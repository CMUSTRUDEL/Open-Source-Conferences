Title: Berlin Buzzwords 2019: Jakub Piasecki & Fabian Hueske â€“ 7 reasons to use Apache Flink #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Jakub Piasecki and Fabian Hueske talking about "7 Reasons to use Apache Flink for your IoT Project - How We Built a Real-time Asset Tracking System".

IoT data poses several challenges to data processing systems. The volume of machine-generated data is huge, users expect timely reactions as soon as real-world events are detected by remote sensors, and connections to edge devices often suffer from varying and often high transfer latencies, resulting in data arriving out-of-order. Apache Flink is an open-source stream processor, that addresses the challenges that IoT data presents. Flink applications run in production at a massive scale at many enterprises and companies, including Alibaba, Netflix, and Uber. In this talk, we will discuss seven reasons why Apache Flink is well-suited for your IoT data project and present how we built a system for real-time RFID asset tracking that is backed by Apache Flink.

Read more:
https://2019.berlinbuzzwords.de/19/session/7-reasons-use-apache-flink-your-iot-project-how-we-built-real-time-asset-tracking-system

About Jakub Piasecki:
https://2019.berlinbuzzwords.de/users/jakub-piasecki

About Fabian Hueske:
https://2019.berlinbuzzwords.de/users/fabian-hueske-1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello and                               fine thanks for sticking around with us                               our so I think that or I assume that we                               have a roomful of Apache think or alt                               fans which is awesome so we've been                               already introduced a microcassette ski                               working for Freeport metrics that I'll                               be presenting for my company today                               co-presenting with fabian Houska                                co-creator of flying working for very                                very very very car                                ok so today first I will talk a little                                bit about IOT data and how IOT                                applications are a bit different from                                other applications then Fabian will talk                                a little bit about Apache flying and how                                it helps with IOT then I will take over                                to Joe to talk just a little bit about                                how we used as report metrics Apache                                flame Frank to build an asset tracking                                system for one of our clients and then I                                will pass it down again to Fabian to                                talk about different oil to use cases ok                                so what is so special about IOT data and                                IOT applications when I started creating                                this presentation the first question I                                asked to myself was how much data do i                                generate consciously or unconsciously                                and it surprised me when I thought about                                it and not not to say they discard me                                but I like to i liked you to walk                                through this exercise with me today and                                think about how much data data do you                                generate maybe you drove through a an                                automated table of other highway or                                maybe you rented an electric scooter and                                when I when I felt like what data I                                produced like more frequently maybe you                                didn't like a self check out at the                                grocery store or maybe you used in GPS                                and your data was was sent to some                                servers to create traffic information                                that is sent to other users or maybe you                                interacted with the smart home device                                and then when you think like what kind                                of data you produce basically every day                                some data this is sent and start outside                                and are processed in some data centers                                and maybe your utility company is using                                smart meters and when you woke up today                                morning and you started your coffee                                machine this spike in energy usage was                                start somewhere or maybe you are using a                                fitness tracker and your pulse is                                measured every five minutes let's say or                                maybe there are applications setting                                your locations in background okay so we                                just covered like some examples from                                everyday life but there is another type                                of IOT data which is industrial data and                                some use cases can be like measuring so                                arm pardonez data on maybe when you are                                right here at Berlin your luggage was                                automatically tracked by a system or                                maybe you can imagine sensors along a                                production line and I believe that we                                will see more and more you know to use                                casings coming because there are more                                sensors all around that some of them                                like more interesting use cases are in                                my opinion like predictive made                                maintenance all the use cases related to                                smart cities and smart buildings there                                is a lot of going down in half course                                well like use like you so like those use                                cases create a very broad spectrum and                                some people may argue that some of the                                examples I presented are not strictly                                IOT data to be honest I spent some time                                before the talk to verify whether people                                consider mobile phone produced data IOT                                data and many don't but all of this data                                sure                                come on properties one common properties                                is that there is just a lot of data                                produced and Cisco predicts that in                                     there's going to be five hundred                                zettabytes                                produce there's a lot of data this                                number may seem abstract what I think is                                important that even if you assume that                                ten percent of this data is useful it's                                still much more that it can be stored in                                data centers so some of this data needs                                to be processed or there's a chance for                                this data to be processed in real time                                and still be useful of course like the                                for a regular application developer                                that's another problem but even on a                                 smaller scale you can see that the                                 amount of data that is generated                                 automatically can be pretty large the                                 second property that is coming to one of                                 the use cases is that you need to deal                                 with latency like latencies and data                                 that comes out of order especially when                                 you transfer it through a mobile network                                 oftentimes in industrial use cases you                                 have Kate with great weight devices that                                 write added own latency and buffer data                                 and of course you need to deal with                                 failures the first properties that flow                                 of the data is continuous which means                                 that you want your system to be                                 available to ingest this data and                                 process it as soon as possible and I                                 think like like for all the streaming                                 applications basically users don't want                                 and sometimes cannot wait data is                                 covered in reward and needs to be                                 processed and influenced the real world                                 what I mean by that let's imagine you                                 have like an valuable piece of equipment                                 the new track so you would rather know                                 sooner sooner than later and then this                                 piece of equipment is leaving your                                 facility out let's say in case of                                 predictive maintenance the sooner you                                 stop the machine that you think may fail                                 the the higher chance of for the machine                                 that it not fail so as we covered the                                 basic properties of IOT data we hand                                 over to forensic and talk about how                                 think absolute yeah thanks so yeah so                                 I'm we right now cover basically what is                                 what are the properties of IOT data and                                 IOT applications and yeah I'm gonna talk                                 about Apache fleeing and how it                                 addresses these requirements so for                                 those of you who not familiar with the                                 Petri flank I have a like brief intro                                 slide here to like give you the right                                 context                                 Apache think is a distributed system for                                 stateful stream processing so it ingests                                 streams of data and process them                                 processes them as the data is basically                                 arriving in this system it's stateful so                                 certain computations require that you                                 keep some state around so imagine                                 disabilities simple as example is you                                 want to count how many how many events                                 you arrive from a stream                                 so there's account some some counter                                 that you need to increment and whenever                                 you get a new event you have to                                 increment the count and then this                                 variable is basically the state that you                                 that this application uses and as it is                                 a distributed system you also kind of                                 like need to ensure that this state is                                 not lost in case something goes wrong                                 so flick applications are also disputed                                 on tens or hundreds of machines so it's                                 very common that something goes wrong                                 and goes wrong and a process occur                                 process goes down so you need to also                                 ensure that this state has been able to                                 to be recovered so it's a disputed                                 system it processes data streams and                                 yeah that's kinda like what Apache                                 flingers and now I'm talking a bit about                                 the that the properties that make it a                                 good fit for IT data so number one is                                 obviously that effectively in processes                                 data with low latency so we are before                                 that many of the applications require                                 timely response to to to certain things                                 that were measured by sensors GPS                                 sensors or I've RFID tag readers or                                 whatever whatever the source of the data                                 is but oftentimes you want the                                 application to respond                                 faster faster this and if link has a                                 couple of properties that make that that                                 make a processing of data possible with                                 low latency first of all a state is                                 always locally maintained so if link                                 does not store state in a distributed                                 database for instance like the count                                 that I was talking about is always kept                                 on the local machine that is processing                                 the events so you don't not call into an                                 remote key value store - hey what's the                                 latest counter you get the RISC result                                 back you increment it and write it back                                 instead all the state is locally                                 maintained either in memory of if your                                 state grows too large you can also put                                 it on rocks to be on the local hard disk                                 the network status is optimized for low                                 latency so we recently improved this tag                                 having a mechanism called a credit based                                 flow control that basically helps the                                 system - - to know from which channels                                 to consume later there's a pretty much                                 pretty detailed blog post that we                                 recently published on the fling block so                                 if you're interested in this I would                                 just point you to this blog post and the                                 way that flink supports state                                 consistency is check pointing the check                                 party mechanism is also designed in a                                 way that whenever the system takes a                                 periodic check point there's are as                                 little intervention in the regular                                 processing as possible and we achieved                                 that by doing these check points in an                                 asynchronous fashion and also support                                 for incommoded check points so the next                                 reason number two is fling City                                 submitted system at its game can scale                                 to large data volumes what this                                 basically means is that data streams are                                 partitioned so it's and therefore we                                 distribute the data and we also                                 distribute the computation to possibly                                 many machines in a computer cluster and                                 basically by just routing the data to                                 two different processes we can scale out                                 the computer                                 flink applications cannot run at very                                 large scale                                 so at ten thousand-plus chorus and also                                 process huge amounts of events for                                 instance Netflix is using fling at                                 pretty pretty large scale processing                                 five trolling events every day which is                                 like about                                                          second and as I said by like disputing                                 the this state we can also like you can                                 think of this that this state is like                                 distributed in the commute classic a                                 similar to a key value store so we route                                 the data to where the state is modify                                 the state locally and then keep on going                                 fleeing is also support for scaling                                 application out and in so that is also                                 possible if in case you your application                                 has some kind of like a pattern where                                 you need to ingest more data or less                                 data over time you can also adjust the                                 parallelism reason number two three                                 as we heard IOT data is often well not                                 of the not of the best quality it arise                                 out of order sometimes sends a produce a                                 bit of messy data flink is able to                                 perform computations in avenged time                                 what this basically means is that fling                                 processes data based on a timestamp that                                 is encoded in the data instead of                                 processing data when it arrives at the                                 processing machine so imagine you want                                 to for instance account something every                                 five minutes or for five minutes you                                 want to know how many how many events                                 you received the easy way to do it is to                                 look at the watch or let the computer                                 check what is the time now and then                                 collect all the events on                                                and then you're done but this technique                                 obviously depends on how much the                                 machine can consume if there is like too                                 much pressure                                 it might get bottlenecked and the                                 correct way of doing this is really                                 using the time stamps in the data and to                                 these computations there is this concept                                 of watermarks which basically control                                 gives is a mechanism to let the system                                 know about the progress of time in the                                 stream so we said that every record has                                 has a time stamp and with water marks                                 Lincoln or a system that uses what I'm                                 asking reason about what time stems to                                 to to expect so it gives some gives                                 gives an application a way of reasoning                                 about the completeness of a stream and                                 by that it can perform a certain                                 computation that it knows okay my stream                                 is now at                                                               computations that received all the data                                 until twelve o'clock and then also                                 discard the state possibly it was linked                                 it's also some support and API                                 primitives that make it very easy to                                 smoothen in a cratering imprecise sensor                                 data like GPS signals or temperature                                 sensors so you can like also smooth that                                 out with it just like one command                                 basically yeah IOT applications should                                 some of these provide mission critical                                 infrastructure and also you basically                                 want them to run continuously and in                                 flink we achieved that by having these                                 checkpoints that was talking earlier                                 about checkpoints basically guaranteeing                                 that the state that an application has                                 is never lost                                 instead fleeing provides exactly once                                 guarantees for the state if something                                 goes down fling buried if link                                 application will basically load this                                 state that is has been written to some                                 persistent storage in HDFS or s                                       load this copy of the state loaded again                                 into the application and then the                                 application will continue as if nothing                                 ever happened                                 so the state also includes the reading                                 positions in the source dreams like                                 Kafka offsets for instance                                 yeah flink is also support for                                 highly available setups and can also run                                 in different resource managers together                                 with zookeeper flink will also be able                                 to recover from massive failures and                                 just deploy the new new processes and                                 again continue as if the error didn't                                 happen this is a very nice feature and                                 often inept                                 coyote applications these these some of                                 some of the applications basically                                 measure real time events and you want to                                 figure out whether or you have a certain                                 pattern in mind in which events should                                 happen for instance your you have an                                 application that receives an order and                                 then the order should be processed into                                 only and finally should be shipped so                                 it's like three bands that you want                                 these to happen in a certain in a                                 certain order and also within a certain                                 time frame then you can define such a                                 pattern in flink has support for so                                 called CP library that is based on the                                 data stream API in which you can define                                 a pattern basically saying hey I want                                 first event a to happen then I want                                 event B or C to happen and finally I                                 wanted an event D to happen so you can                                 define these like reg acts like patterns                                 and when you then deploy such an                                 application if we were like just monitor                                 the stream and whenever the pattern                                 matches you get a you get a new event                                 and can implement an application that                                 just acts on this event there's also you                                 cannot only do this with this CP library                                 there's also a recent extension of the                                 sequel standard so sequel or                                            was this match recognized clause added                                 and this is pretty much exactly this so                                 you basically defined a pattern over                                 some audit relation and                                 can can react to that so there's                                 something like a prime use case for a                                 sequel for four for streaming sequel and                                 flink supports or streaming secrets so                                 you can even implement these these kinds                                 of patterns in a secret theory and the                                 really nice thing is that you can                                 combine it also with data analytics so                                 you can either first have this pattern                                 and then basic count how often did this                                 pattern occur in five minutes but you                                 can also turn it around and first to                                 perform some aggregation and create a                                 new event whenever a certain threshold                                 was was exceeded and then have a pattern                                 depending on these threads threshold                                 exceeding events so I could like to                                 figure out if something violated here                                 SLA twice within a certain time frame so                                 I think this is a pretty pretty good                                 match for from any way to use cases                                 finally or not finally it's number six                                 so it's very connected with many other                                 systems in the in the Big Data space and                                 also with messaging systems like cough                                 car kinases pulsar it is Lincoln right -                                 Cassandra elasticsearch or JDBC                                 databases but also too far it's a                                 different file formats so there's a                                 bunch of connectors available and                                 finally I would argue that data                                 streaming is like conceptually simple                                 and this is basically also I would argue                                 the natural way to think about handing                                 out events so you get a consistent                                 stream of data that you want to process                                 one by one and this is basically                                 something that flink makes makes makes                                 quite easy                                 due to its API the data stream API this                                 has a couple of high level primitives                                 like for instance joins or window                                 aggregations but also allows you to go                                 deep into into into like the core                                 primitives of stream processing like                                 state and state in time                                 and by implementing your application                                 against the status from API you can like                                 scale it out basically to any size all                                 right with this introduction to fling                                 and handing over to yeah coop again who                                 will tell us how he built on as a                                 tracking system with Lincoln thank you                                 so like five introduced a little bit                                 about how we had people metrics bills                                 and as a tracking system for our client                                 ivanhoe a little bit where we are coming                                 from at Freeport metrics and we are a                                 little bit did sell digital products                                 development company so to decipher it                                 repeated software for companies that's                                 out this substrate other companies and                                 over the years we've worked on quite a                                 few projects related to industrial data                                 processing or IOT data some of it were                                 work on projects for solar and wind                                 energy farm analytics we worked on a                                 couple inventory and warehouse systems                                 we worked on automated retail kiosks but                                 I think the important part is that this                                 was our first project with playing and                                 before that we were relying on more                                 traditional ETL approaches or our custom                                 code for ingesting events or on                                 processing them okay so what is an asset                                 tracking system to give you some use                                 cases the most straightforward use case                                 is inventory management so basically you                                 want to track if you have all the things                                 to do should have the second use cases                                 uterine contractions in a warehouse but                                 I think the pretty coal use case is an                                 actually one of our first pallets of our                                 client ease that you hand out RFID                                 wristbands to patients and families in                                 waiting rooms in hospital can see how                                 those patients go for different types of                                 medical procedure                                 and what's the source of data in in an                                 asset tracking system first of all you                                 have RFID taxed and can be many of them                                 like hundreds of of thousands and those                                 tags are tracked by arif a pair of RFID                                 antennas they are sent to gateways and                                 then to the backend systems by besides                                 those tax they are tracked automatically                                 you have also users with handheld                                 barcode scanners and of course obviously                                 like mobile and web interfaces ok so I                                 Fabian is definitely like an expert in                                 the first thing so I will talk about                                 about think that I think was the most                                 useful for us in of people batteries                                 when you just blink and I think this is                                 the computational model of fleeing and                                 what I think the most powerful features                                 event time like Fabian mentioned like                                 you when you have data from multiple                                 sources this data will come out of order                                 in our case we have alpha in the                                 antennas we have users using mobile                                 devices you get all those different                                 latencies from those different sources                                 and to be able to make any sensible                                 results in it order it accordingly                                 the second thing is referring towards                                 fight against that window which is                                 always very a very useful feature for                                 cleaning up your data in in the use case                                 that report on you may have like                                 overlapping antennas so the same antenna                                 is reading so two antennas are reading                                 the same tag like almost at the same                                 time so using window you can apply some                                 heuristics and to figure out when words                                 the tagger really is the second thing is                                 state and I think the powerful feature                                 of Flanagan is that you'd like forces                                 you to partition your state and it'll                                 Casey                                 we could partition all divins related to                                 one assets related to dock to tag into                                 like one let's call it group and but                                 what you get from that it that it is                                 automatically parallelizable and of                                 course it affects performance the second                                 thing about state is when you have                                 stayed you can build state machines and                                 some more complex logic in our case we                                 built a simple version of like a                                 business process modeling tool which i                                 thought let us modeled situations like                                 let's say you have a patient that enters                                 a waiting room and the doctor's office                                 and then recovery area and you can also                                 use time like if ten minutes part we                                 know that the procedure is over a little                                 bit about our experience like developers                                 experience with using a passive think                                 first of all we had a pretty small team                                 to work alongside our clients team but                                 that that were not many of us and so we                                 didn't spend that much time like on on                                 this event processing part thanks to                                 first of all the functionality that Fink                                 provides so that was very useful for us                                 and that has progressed on other parts                                 of the system second created to that we                                 could focus mostly on our our core logic                                 of the application another thing is that                                 flink provides                                 different levels of different different                                 levels of abstractions and even the                                 api's and I also think this is very                                 useful you can work on higher level or                                 you need to cast a customized on                                 processing function level to and adjust                                 it to your use case and needs to say the                                 integration with with with tools and                                 Kafka is very good and the challenge is                                 the interesting part referring to the                                 flink being conceptually simple I have                                 to say like for our team it was like a                                 new way of thinking but I think like                                 it's well worth investing after you get                                 used to this new programming model it's                                 appears like the natural ways of doing                                 things second thing is that it I think a                                 new feature may require serious planning                                 I don't have to convince your                                 discomforts that distributed systems are                                 hard but also in to get understanding                                 from all your team members that the                                 change that may seem simple from user                                 perspective may require like significant                                 adjustment to the system under the hood                                 to give you some examples it is very                                 important what you use to partition the                                 partitioning of your data and/or how do                                 you work with latency in the system and                                 the fact these users that want to see                                 some events immediately or can update                                 application from user interfaces that                                 require cos likeness and internally                                 interaction and the last item is like                                 three years ago when we started working                                 on the system with our clients other                                 than how I would say that access to                                 learning materials was emitted but it                                 cuts so much better in the last couple                                 days because Alaska for last couple                                 years I have to say especially with like                                 all the couple like video materials                                 available and which are all in the last                                 couple months I can say like five and                                 published at work so so so I wish I had                                 fears the girl but that's exactly so I                                 think that it's a good point to hand                                 over to five and again oh yeah thanks                                 alright so we heard about one                                 interesting and challenging use case for                                 for IOT later and the Petri flank I was                                 like when we hit this presentation I                                 said well we also have a couple of other                                 interesting use cases that I want to                                 briefly mention we basically collect                                 most of them                                 oh there's a nice type of                                 we collected a couple of a nice example                                 mostly from the fling forward conference                                 like one of them is in data-driven                                 agriculture by the company John Deere                                 they presented this use case at field                                 forward San Francisco earlier this year                                 John Deere as many of you might know is                                 a manufacturer of machines for                                 agriculture construction and forestry                                 and they also run a data platform that                                 provides like data services for farmers                                 so what they basically do is their                                 machines or at least I guess the newer                                 generation of their machines are                                 collecting lots of data data that is                                 geospatial obviously but also temporal                                 and they measure different different                                 things like for instance how fast a                                 planter is seeding new plants or what's                                 the humidity or whatever so they're                                 collecting lots of lots of data and this                                 is really like at a large scale like a                                 single planting machine produces for                                 instance like more than                                              sensor measurements per second and they                                 collect all of this data basically and                                 then make it available for the farmers                                 to analyze it basically to to                                 investigate is how much in how much does                                 the planting speed influence or the                                 yield in this area and so on so this is                                 really meant to help the farmers to                                 basically increase the years of the era                                 of the fields what they're doing is                                 basically they're apparently using fling                                 con on AWS so they're interesting the                                 data from from kinases processing it                                 with fleeing and then writing it into a                                 into a data like that is based on as                                 three or DynamoDB another use case is                                 stand by by here this is what they call                                 living Maps this is basically static map                                 data                                 gets enriched with real-time data that                                 is collected by cars like for instance                                 they can figure out they basically they                                 have like getting data from the canvas                                 of the car so they can figure out                                 something with a road is slippery                                 weather like this the science changed                                 you to a construction accidents they                                 even measure for for for the side side                                 sends us whether there is a parking spot                                 and can make this information available                                 for somebody who's looking for a parking                                 spot so this also is is a lot of data                                 that has been being being generated and                                 being made available by by here so what                                 they actually do is they have a platform                                 with a data data data marketplace where                                 they offer Frank as a service and you                                 can basically write applications using                                 their data streams to yeah                                 get get the data that you need there's                                 also I think this is a kind of kind of                                 similar to the to the use case that                                 Yakka presented it's a fleet management                                 construction it's a company called Trek                                 unit they're providing telematics                                 solutions for yeah for fleet management                                 mostly in the construction industry and                                 so they basically track where all the                                 all the machinery is how it is used                                 whether it's used or not used and can                                 basically then the customers of this of                                 this service can basically figure out                                 yeah where the machines are basically                                 optimize the usage patterns and also                                 make some inference about the                                 maintenance intervals and finally this                                 is well this is not another real                                 application but it's it's on the AWS                                 blog so I found it quite interesting use                                 case as well this is a basically a blog                                 post that describes how to                                 how to do best fire detection so they                                 kinda like assume like a multi hope                                 network of sensors they basically talk                                 to each other and then basically have                                 some kind of propagation protocol that                                 one sends us broadcasts information and                                 then it goes on and on until to some to                                 some station that then uploads the data                                 to to the cloud and then they have a                                 demo using this CP library that I was                                 talking earlier about basically to                                 figure out whether there is a brushfire                                 or not and maybe making this inference                                 obviously based on temperature in bands                                 and then there is like a visualization                                 with a heat map using using Cubana yeah                                 so the demo you can also try that out on                                 AWS alright so to conclude so like I                                 would say if link this time without a                                 type of meets the demands of IOT                                 applications it's it serves the the                                 latency and and throughput requirements                                 quite quite well a low latency through                                 our local state management                                 volume by scaling out the computation                                 it has good api's to give you like basic                                 control for the for the right level of                                 abstraction that you want to deal with                                 event time helps a lot with dealing                                 dealing with out of order data and it's                                 been used by several IT projects and if                                 you have something in this in this                                 domain you should also give it a try                                 alright so this is a briefly mentioned                                 this this is the book that I wrote it                                 came out I think like one and a                                 half-year month ago I'm signing copies                                 if you have a copy I would be in the                                 booth area if you don't have a copy you                                 can also get one and there it's not at                                 the registration test but within the the                                 booth area as well right after this talk                                 thank you thank you guys and do you have                                 some questions young no yeah present was                                 really cool talk I didn't know anything                                 about flink so we use kappa at Shopify a                                 lot but would you say like the main like                                 like proponents of flink / Kafka and I                                 guess like the other way around and I                                 like yeah yeah so so you're asking like                                 what's the the the difference - - Kafka                                 right so I mean Kafka is a large project                                 which is also doing lots of things flink                                 is not a message message queue right so                                 it's not you you can't use it to route                                 data Kafka and the Kafka project is the                                 sum stream processing with African                                 streams right so flink is I would argue                                 think as I didn't really talk about it                                 because it's a bit maybe no that not                                 that important in the context of of IOT                                 data but flick is also able to do batch                                 processing so it's not only doing stream                                 processing so we actually see well                                 that's a special case of stream                                 processing so you can also do stream                                 processing and lots of the api's or lots                                 of the things where the community is                                 currently working on are going into                                 unifying veteran stream processing and                                 this is like I think an important thing                                 if you look at things like backfilling                                 of results that you want to repair                                 because you identified it back in your                                 somewhere in your in your code you just                                 then run the code also the historic data                                 using efficient batch processing                                 techniques but also deploy for for your                                 life life pipelines you can also do                                 things like statement stripping which is                                 also important so you don't want to                                 start from zero you want to start like                                 with history                                 of like say six-month and bootstrap the                                 state using using this so flink is                                 really like a system that what it is                                 designed or is aiming to like cover the                                 full spectrum of of data processing                                 whereas like Kafka streams focuses on                                 their stream processing power
YouTube URL: https://www.youtube.com/watch?v=JcXWcBA2dx4


