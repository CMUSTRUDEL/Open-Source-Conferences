Title: Berlin Buzzwords 2019: Jon Bratseth â€“ Scalable machine-learned model serving #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Applying machine learning in online applications requires solving the problem of model serving: Evaluating the machine-learned model over some data point(s) in real time while the user is waiting for a response. Solutions such as TensorFlow Serving are available to solve this problem where the model only needs to be evaluated over a one data point per user request, but what about the case where a model needs to be evaluated over many data points per request, such as in search and recommendation systems? 

This talk will show that this is a bandwidth constrained problem, and outline an architectural solution where computation is pushed down to data shards in parallel. It will demonstrate how this solution can be put into use with Vespa.ai - the open source big data serving engine - to achieve scalable model serving of TensorFlow and ONNX and show benchmarks comparing performance and scalability to TensorFlow Serving. 

Model serving with Vespa is used today for some of the worlds largest recommendation systems, such as serving personalized content on all Yahoo content pages, personalized ads in the worlds third largest ad network, and image search and retrieval by similarity in Flickr. These systems evaluate models over millions of data points per request for hundreds of thousands of requests per second.

Read more:
https://2019.berlinbuzzwords.de/19/session/scalable-machine-learned-model-serving

About Jon Bratseth:
https://2019.berlinbuzzwords.de/users/jon-bratseth

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right I'm here                               what I call                               serving and                               that solves this problem in open-source                               so first time be talking about what I                               mean by big data serving what's the use                               cases what's the problems things like                               that and then I'll be talking about                               lesbo which is the open source engine we                                have created to solve these kind of                                problems and then as time permits I'll                                go deeper into architecture and                                capabilities and using this puzzle this                                is a lot of slide so a larder be talking                                really fast or skipping some parts I                                like to start by introduced by talking                                about the big data maturity level when                                organizations start out they usually                                create a lot of data in logs and so on                                but they don't actually use it for                                anything systematically that's what I                                call the latent phase at some point they                                understand that it's useful to look at                                these data systematically to use it as                                for decision-making so that's the                                analysis stage but they're still users                                actual employees looking at the data                                creating reports and so on right then                                you get to the learning stage where you                                try to automatically learn insights from                                your data right which is what people are                                doing with machine learning obviously                                them at last you get to the acting phase                                where you try to make decisions in real                                time based on your data and there's two                                subcategories so that one is what coal                                either stream processing or model                                serving depending on whether you do                                request response or streams and that is                                where it's efficient to look at a single                                data item to make a decision the other                                problem is where you need to look at                                many data items to make a single                                decision and that's what I mean by big                                data serving right so if we do a simple                                example down here which is something                                like Netflix a movie streaming sites at                                first maybe you just                                serve the movies and your log which                                users are looking at which movie then at                                some point you go to the analysis stage                                and you start to do analytics to                                discover what kind of users are looking                                at what movies so that you can create                                curated lists or movies to recommend                                right at some point you want to stop                                paying people to do that job and you                                start using machine learning to create                                those lists for you but you still do it                                offline right you either or module                                persons like that you look to all your                                data and you generate all this list of                                recommendations for different kinds of                                users and then you move into your                                serving system and you serve them then                                the last stage of the acting phase you                                defer the decision about what to                                recommend to every single user to the                                point where the user needs to                                recommendations and there are many good                                reasons to do that right                                if you defer the decision-making until                                the users actually need the decision to                                be made you can use all the up-to-date                                information you have both about the user                                about the data all the information you                                have can go into that decision-making                                it's not Auto date right secondly you                                don't waste computation because you                                never equally compute something that                                you're not actually going to need                                because you do it only in response to                                something happening right for that same                                reason your decision-making can be much                                more fine-grain in the movie example                                right you can move from recommending                                different lists of videos to adults tech                                interested people or whatever to                                creating a separate list for every                                single user right something that would                                be too expensive in most cases if you do                                it eagerly upfront and from an                                architectural point of view if you can                                tweet the system that do the real-time                                decision-making as a black box the your                                architecture becomes much simpler                                because usually to work around those                                problems I just mentioned you end up                                with a quite complicated system where                                 you have some batch processes that learn                                 something and smaller processes that                                 move that data to your serving system                                 but then it's out of date so you create                                 additional systems on top that immense                                 out of date decisions you made with                                 real-time information that you put on                                 top and things like that                                 right so well that goes away if you just                                 commit to doing everything with that                                 right but why aren't everybody doing                                 this it's because it's really hard right                                 and why is it hard it's because you                                 combine a bunch of things that are hard                                 to do on scale into one single thing so                                 we sort of get multiplication of the                                 difficulties so you have you're dealing                                 with state obviously because you need to                                 store the state that you do                                 decision-making over and because you are                                 fanning out because you need to compute                                 over low latency budget over lots of                                 data you need to fan out to multiple                                 nodes working in parallel so you had the                                 problem of doing scale together with a                                 low latency budget and you need since                                 you're on line serving directly to users                                 you need this whole thing to have high                                 availability right so the features you                                 need you need to be able to find data                                 and make inferences in typically a few                                 tens of milliseconds you need at the                                 same time to be able to have real-time                                 updates at a high continuous rate you                                 need to be able to have large request                                 rates over the data because you're                                 dealing directly with end-users and                                 there's a lot more end-users than for                                 example employees right so you need to                                 be able to handle high request rates and                                 as I mention you need to be always                                 available because your end-users out in                                 the world typically won't tolerate                                 downtime or it won't be good for your                                 company and for the same reason you need                                 everything to be able to evolve while                                 it's serving you need to be able to                                 change your schemas change your logic                                 components running in the system                                 change your data change the hardware's                                 running on everything without ever being                                 down right and since this is just a                                 serving part of dealing with big data                                 problems you need to integrate with                                 other parts of the stack like machine                                 learning systems offline processing                                 systems and so on so how can we solve                                 this problem we can use less power which                                 is open source platform open source on                                 restful AI that delivers the features                                 are just mentioned for you best poles                                 actually started a long long time ago                                 and the problem we were attacking back                                 in those days were web search this was                                 before Google so there was lots of                                 different web search engines right so                                 search and in particular web search is a                                 prototypical big data serving use case                                 right because there are close to an                                 infinite number of potentially user                                 queries you can't compute the answer to                                 every query that you'll see up front                                 obviously and because the relevance is                                 really important you need to be able to                                 compute machine learning models over the                                 documents that you're matching so you                                 have that part of the problem as well                                 and you need low latency because users                                 really care about latency once you get                                 past about                                                          right and luckily there at some point                                 starting at the turn of the century                                 there was a lot of money flowing into                                 web search mint which means we could use                                 lots of resources to solve these                                 difficult problems which is why the big                                 data surveying systems go Google to web                                 search right so we worked on these                                 problems in Yahoo search and we made two                                 systems at the same time more or less                                 the same time to solve these problems                                 and they both had the same basic idea                                 rather than looking up data and sending                                 it somewhere for computation which is                                 what you typically do in in two-tier                                 systems that needs to work on data right                                 you create a representation of the                                 computation that you want to make and                                 you ship the computation to the data                                 right so we created Hadoop to do exactly                                 that                                 on the offline side and rest but to do                                 it on the online side right                                 Hadoop was open source from the                                 beginning while Vesper we were unable to                                 support to open source because of the                                 complex IP rights around search but                                 finally about one and a half year ago we                                 were able to open source it so the                                 company I know best which uses Vespa is                                 my own company which used to be Yahoo                                 and is now called Verizon media so we                                 have a cloud system a cloud service                                 running Vespa for all the use cases in                                 this company so it's a couple hundred                                 applications running Vespa we all run                                 them from my team in seven data centers                                 around the world in total we serve over                                 a billion users and about                                         queries per second and one of the use                                 cases here is the third largest ad                                 networks in the work                                 nobody likes ads but they are what makes                                 it possible for poor people to use all                                 the services on the internet for free so                                 they have some episode as well so one                                 example use case actually you can see                                 two here on this page this I talked                                 about movie recommendation and this is                                 another case of recommendation if a user                                 visitors we see it's one of the Yahoo                                 patients he or she gets a list of videos                                 and articles and those videos and                                 articles are computed on the fly by                                 RESPA based on the personalized profile                                 that user and you can also see what's                                 typically called a native ad in there                                 which is a different application of                                 WestBow that do kind of the same thing                                 but additional stuff for doing a                                 real-time auction and so on which is                                 also interesting so so far I've been                                 talking about two use cases so a big                                 data serving platform one may search                                 where the query is typically some                                 keywords and the model machine learn                                 model you want to evaluate is what                                 computes the relevance and the items the                                 data you actually want to return are the                                 ones that have the highest relevant                                 score according to your machine learn                                 real ones model right and then you have                                 the recommendation use case where the                                 query is typically just some filters                                 specifying what kind of data is eligible                                 for that particular user or market or                                 whatever and then particular is also                                 some typically machine learn                                 representation of the user like a vector                                 or tensor embedding and your machine                                 normal is your recommendation model                                 obviously and again you just want to                                 return on items that have the highest                                 score it's actually typically a bit more                                 complicated because you have diversity                                 that you want the factory nothing's like                                 that so those are the two obvious use                                 cases that are keeping us busy at the                                 moment but it's my belief that there are                                 lots and lots of other use cases for                                 this kind of thing where you can compute                                 machine learning models over lots of                                 data items with a latency budget of less                                 than                                                                  have a platform that can solve this kind                                 of problems we can probably apply it to                                 a lot more use cases so I just want to                                 mention one example which is not                                 entirely made up just obvious k to the                                 beat say you have data items which are                                 some kind of assets for example stocks                                 and you have a machine learn predict the                                 price of these assets based on a lot of                                 data about each of the assets and about                                 other data what other things happening                                 in the world now you can create a query                                 that supplies some of the values for                                 this predictive model namely the stuff                                 that's happening in the world and then                                 you can evaluate that machine or model                                 for each of your stocks and then find a                                 new price where given your query which                                 is sort of an update to what's happening                                 in the work right and then you can                                 select the items that have the biggest                                 difference in price predicted from your                                 old world state and a new world state                                 that you send in the query and then as a                                 result you are able to quicker than                                 anybody else compute what's going to                                 happen across all your all the assets                                 that you're interested in the stocks if                                 some event is happening and you can take                                 your event stream with things that are                                 actually happening and find the biggest                                 price mover the the stocks that will                                 move the most in response to these                                 events and do that faster than anybody                                 else and that's actually very valuable                                 so this is just an example that you can                                 if you abstract a problem like this you                                 can probably come up with lots of                                 examples so how to use an engine like                                 this so one question I often get about                                 respo is about is big data serving and                                 RESPA for analytics and to me analytics                                 is different problems it just definitely                                 overlap and you can more or less if you                                 have something that's created for one                                 you can usually make it work for the                                 other because of the OLAP but since the                                 design points are different it will be                                 harder to do one thing or the other and                                 in particular harder to                                 make it performance right so in                                 analytics you typically have response                                 times in low seconds because your users                                 or employees and they can tolerate                                 waiting a bit right while we're big data                                 serving because you're dealing with end                                 users typically your latency budget is                                 less than a hundred milliseconds for                                 some reason you have low workwear rate                                 with analytics usually you are dealing                                 with time series data which means you                                 can use special optimizations that we                                 assume we cannot do at least in respond                                 because we support random writes to all                                 the data and high availability is much                                 more important in big data serving as I                                 mentioned so all of those sort things                                 that are better in something like Vespa                                 but the flipside is that since we can't                                 do a lot of optimizations that you can                                 do with analytics it gets more expensive                                 once you get to the trillions of                                 documents ok so that was a brief                                 introduction to the problem or Big Data                                 serving now I'll move to talking a bit                                 about how Vespa solstice problem so a                                 bit more detail on the features that                                 westphall provides it provides text                                 search and structured data selection at                                 the same time you know                                 represented in a query it supports                                 evaluating machine Luhrmann's scoring                                 relevance inference things like that                                 using natural language features advanced                                 machine learning models integration with                                 terms of flow and so on you can query                                 time organize and aggregate data across                                 all the data that is selected in your                                 query without actually sending all the                                 data to one place and doing the                                 aggregation and organization and you can                                 do all of this while you sustain high                                 wheel time right way so typically you                                 can do from a few thousand to a few tens                                 of thousands of writes per second per                                 node sustained                                 the clusters are elastic and also                                 recovering so if you loosen all the data                                 will automatically rebalance and if you                                 add or remove hardware the data will                                 automatically rebalance in the same way                                 as part of the architecture is also                                 stateless Java container of people used                                 to plug in their own data and query and                                 result processing logic and because                                 these systems can be large you end up                                 with lots of clusters lots of nodes and                                 processes and so on and setting up all                                 of that manually is just way too hard                                 so the clusters are managed and what end                                 users are seeing is more abstracted                                 representation of the system you want to                                 run so architecture on the highest level                                 let's place a two-tier system as I                                 mentioned there's a stateless Java                                 container tier on top where you can plug                                 in your own logic and we're also runs                                 the stateless part of the logic of                                 executing queries sending incoming data                                 and so on and then we have what we call                                 content nodes in content clusters which                                 are all implemented in C++ because we                                 don't really like limitations and                                 problems of working with lots and lots                                 so data in a single process in Java so                                 this part is all in C++ and this is the                                 part where we actually stores the data                                 managers redistribution execute the                                 distributed porch or queries and so on                                 lastly as I mention we have an                                 administration and configuration                                 subsystem which is another cluster over                                 zookeeper that manages the other                                 clusters for you and what the user is                                 seeing is what we call an application                                 package which is a high-level                                 description of the system that you want                                 to run the clusters what features should                                 they have and so on as well as your Java                                 components machine learn models all that                                 so all that is what we got an                                 application package which is deployed to                                 Vespa to the administration and                                 configuration system which will then set                                 up the system for you if you change the                                 application package and deploy again the                                 system will change to accommodate the                                 changes you made without taking anything                                 down or anything like that                                 so the key point the key purpose of ESPA                                 is to achieve low latency computation                                 over data and there are three strategies                                 we use for that which is quite obvious                                 one is parallelization the queries are                                 scattered to a bunch of content nodes in                                 parallel to execute over charts of the                                 data on each of the nodes we dynamically                                 short over many course working separate                                 subspaces of the data while exchanging                                 some information about what subspace to                                 work on next and things like that                                 secondly we prepare data structures at                                 right time as well as in the background                                 to make read time faster and the                                 simplest example that you know well that                                 is reversing this is for text search                                 right but also other examples of this                                 like using a level DB light structured                                 for the raw data so that you end up with                                 more and more sorted data as it goes                                 older things like that and as I mention                                 we move as much of the execution as                                 possible to the data nodes so for                                 example if you want to evaluate a                                 machine learn model that model a spark                                 to your application package when you                                 deploy it the model is copied to all the                                 content nodes so that we can evaluate                                 machine learning models locally on all                                 the content nodes in parallel without                                 shipping the data anywhere for                                 computation right and just to drive that                                 point home here is an example where we                                 compare scalability or using in terms of                                 serving with Vespa for serving the same                                 model the partitions here are different                                 content nodes once we add more content                                 nodes with less power you get more or                                 less linear scaling while with                                 tensorflow what you need to do is to                                 look up the data and Vespa and then                                 evaluate your tensor flow model in the                                 state list here which very quickly runs                                 into the bandwidth bottleneck at which                                 point it doesn't help you at all to add                                 more content partitions so your data                                 doesn't really your computation doesn't                                 really scale with the data anymore while                                 we the best way it does so with more                                 about all Vespa is implemented under                                 hood we use in most cases apart from                                 some specific optimizations which was                                 called document at the time emulation                                 overall the query operators in your                                 query and that's because we want to be                                 able to compute nonlinear relevance                                 models we have two kinds of fields we                                 have index fields which is used for text                                 search where you have positional text                                 synthesis for the old data and we use be                                 trees in memory for the recent changes                                 so we we are not using the old style                                 trick where you have a small reverse                                 index for your changes and then a lot                                 larger reverse index and the new version                                 instead we use B trees which are more                                 like databases for the recent changes                                 and then we flush those to disk in the                                 background and work for structured data                                 we you typically use what's called                                 attribute fields which are in memory                                 forward dense data where you can also                                 optionally have in memory B trees over                                 the data if you want to use them a                                 strong criteria in queries in addition                                 to this we have a transaction log for                                 persistence and replay if you crush and                                 we have a separate store in a level dB                                 similar structure for the actual raw                                 data of all the documents that we use                                 for recovery data redistribution things                                 like that but also for returning the                                 payload data of the response if you have                                 multiple doc schemas we have one copy of                                 all of this for for each chemo so I                                 mentioned that vespa distribute state                                 over the content nodes in a colon                                 cluster that happens completely                                 automatically in RESPA you never ever                                 manually worry about what your shorts or                                 wheesh are doing things like that                                 that's things nobody using best to have                                 worried about for at least                                              there's no way to do offline indexing                                 the reason we used to do offline                                 indexing back in all this is because it                                 was CPU costly while since them CPU cost                                 has grown much faster than sorry the CPU                                 performance has grown much faster than                                 bandwidth performance both water                                 bandwidth and memory based bandwidth so                                 because of that it's just not a                                 bottleneck anymore so it's much more                                 efficient to actually do all the or                                 indexing locally on the node that will                                 serve the data so in West Point you just                                 list the nodes that goes into a content                                 cluster and RESPA will distribute the                                 data over those content nodes we had a                                 certain replication Factory at you can                                 set if you have a high query rate you                                 can also do multiple groups inside the                                 content cluster which each we keep some                                 number of replicas of all the data so                                 that you can do round-robin or load                                 balance distribution across those groups                                 to scale to higher Kweli queries and                                 optionally in some use cases you can                                 also                                 Kolak ate some data with some property                                 on specific nodes which is useful for                                 things like personal search where each                                 of the queries only searches a given                                 users private data and in that case you                                 don't want to scatter all the queries                                 over all nodes you want to co-locate the                                 data on some small number of nodes and                                 only send a query for a given user to                                 that smaller number of nodes where the                                 number of nodes is a trade-off between                                 how much scatter gather you need to do                                 and the latency you achieve right and if                                 you change your configuration to add or                                 remove groups or replication factor or                                 whatever let's probably read this beauty                                 data in in the background while you are                                 serving and handling writes and the same                                 thing will happen if you add nodes or                                 remove notes or some node die on your                                 things like that another big thing in                                 vespa is the inference engine which                                 underlies support for machine learning                                 models and so on so it's part of WestBow                                 we have a tensor data model it used to                                 be somewhat cumbersome to explain                                 tensors to people why we need them and                                 so on but after transform came up it's                                 much simpler so as you probably know                                 tensor is just a multi-dimensional                                 collection on numbers at least if you                                 ask a computer scientist and you can add                                 these tensors to queries documents and                                 models and use them to represent vectors                                 matrixes higher level a higher                                 dimensional data and so on                                 so each tensor dimension in vespa can be                                 either indexed which we use for dense                                 data or it can be mapped which you use                                 for sparse data and then we have a                                 tensor mathematical language that allows                                 you to do to compute much machines or                                 models or even handwritten models or                                 censors and that mouth language contains                                 just six operators which is pretty cool                                 based on those six simple operators we                                 can combine them to provide all those                                 higher level functions that you have in                                 things like neural nets and so on so we                                 have you provide those higher level                                 functions out of the box as well so                                 there's a long list in the documentation                                 which is on the left hand side here but                                 they are all just implemented in terms                                 of those six core functions in Vespa                                 which is very nice for us because it                                 makes it simple for us to optimize                                 because we can optimize those core                                 functions and it will work for all the                                 higher level functions right so people                                 can handwrite these mathematical                                 expressions to achieve whatever                                 computation they want but in most cases                                 people find that too difficult so to                                 solve that problem we also provide                                 integration of that box with tencel                                 outside models so you can have you can                                 store your tensorflow save models                                 directly and respond respond random for                                 you by converting them to mathematical                                 expressions in this language and you can                                 do that the same thing with models                                 inside cake and pie torsion so by saving                                 them in the common or next format which                                 is also read by Vespa so just to give                                 some intuition on that this thing on the                                 left is a simple graph model in terms of                                 flow and on the right you have the                                 equivalent expression in the                                 mathematical language inside                                 just a few words on releases we do all                                 development in the open or WestBow on                                 github so it's on less pangaean slash                                 Westlaw there's no internal thing we are                                 doing and then sinking or and you know                                 that is all in open and we create new                                 production releases Monday to Thursday                                 every week we for each release we each                                 release will first have passed or zuto                                 performance tests and functional tests                                 and so on but they will also already be                                 running the                                                           our own vespa cloud service so that                                 wants to release is in the public it's                                 already proven on all these applications                                 so you can safely use it so I recommend                                 everybody using respite - upgrades                                 create a process to upgrade at least                                 once a week using these releases rather                                 than ending up being behind okay so that                                 was introduction to vespa I'll summarize                                 that and then do just a few more slides                                 and then we'll be done so in summary if                                 you want to use Big Data the best way                                 you need to be able to make decisions in                                 real time over all the data and West by                                 senji nuts optimized for doing this you                                 can find it on RESPA dots AI and there                                 you can also find a QuickStart tutorial                                 that allows you to run it on your laptop                                 or on abs in less than                                                also have a big tutorial that lets you                                 build the blog search and recommendation                                 engine ending with recommendation using                                 neural network that you can follow that                                 takes something like a day but it starts                                 from just a raw data and wins the whole                                 thing which is pretty cool                                 ok so I'll stop soon and move to                                 questions but a few more things about                                 how to use respo veera leaves RPM                                 packages and docker images                                 you install the same thing on all your                                 nodes and you set one variable that                                 points to the administration subsystem                                 and the rest is creating your                                 application package which typically                                 corresponds to something like a git repo                                 representing your application it needs                                 to have three files one file at least                                 describes clusters that you want to run                                 and one listing the nodes you have                                 available and then you need schemas for                                 this and that the schemas also contains                                 models you want to run or pointers to                                 the models you want to run because                                 models are tied to the schema so this to                                 slice is a complete simple application                                 package so we provide HTTP interfaces to                                 do everything towards respond there's                                 also some other alternatives in some                                 cases yeah I won't I'll skip over this                                 yeah maybe I can mention this because                                 there's a lot of search people here I've                                 noticed so typically when you do text                                 search should do supervised learning                                 using the structured data right                                 while in recommendation it's more common                                 to use some kind of vector tensor                                 embedding or both users or whatever and                                 as well as your documents and then                                 evaluate overall your data items and                                 also in as much as possible use                                 reinforcement learning because you                                 typically don't have a good idea about                                 what right                                 ranking is there's a certain movement in                                 search now where you from for text                                 search also are moving away from                                 symbolic computation towards vector                                 representations and things like that                                 which is what people call some people at                                 least call it search - oh it's                                 interesting and in practice the best                                 solution is kind of both at the same                                 time but in any case vespa supports both                                 this kind of use cases and we have                                 interesting applications doing both                                 approaches which are pretty good so one                                 common thing that people are using with                                 text search is what's called gradient                                 boosted decision trees that has been                                 sort of the benchmark for text redlands                                 for a good while you can do that in                                 vespa by writing a mathematical                                 expression like expressing your forest                                 that comes out from the gdt training                                 manually but you can also use extra                                 boost and just put the model directly                                 and respond less probably understand it                                 and convert it for you                                 these expressions are really expensive                                 to run the vespa contains spent a lot of                                 effort into optimizing these expressions                                 to make them faster run so Westfall                                 recognized this shape or mathematical                                 expressions use very                                 specific optimizations for those so                                 that's cool but then we have papers like                                 this which says that you can train your                                 G ability model on them train neural                                 nets that mimics your model and that                                 will give you the same results for about                                 a hundredth of the cost so some people                                 are doing this as well and that sort of                                 makes sense when you think about it so                                 maybe that's what people will do more or                                 in the future in any case you can                                 express both these kind of models quite                                 simply in respond lots of interesting                                 stuff here you can find these slides                                 online if you really want to or even if                                 you just want to a little bit actually                                 but I'll stop there and move to                                 questions we have three minutes if you                                 are any questions hands up if you have a                                 question so I was just wondering I you                                 were showing some of the models from                                 tensorflow and they're actually living                                 in the vespa runtime so how do you deal                                 there with the amount of memory that                                 that model will actually take up it                                 seems to be a user facing it seems to be                                 running a query time if I read it                                 correctly what so you were taking as                                 some deep models and you were putting                                 them in the the rancor yeah and I'm just                                 wondering how does that perform because                                 generally whenever I do this like a                                 models maybe four gigabytes and so how                                 does vespa deal with this oh is it                                 yeah if you need to your your models or                                 more specifically the tensors that are                                 part of the model needs to fit in memory                                 on all the nodes otherwise is possible                                 to run it but it will just be way too                                 slow but typically models aren't larger                                 than what we mentioned a couple of                                 gigabytes and that's fine really                                 so that bottleneck really for at least                                 four simple models the bottleneck become                                 memory bandwidth actually when you                                 evaluate is which is what you want                                 because it's a scarce resource                                 okay I guess everything will Sparkle on                                 my earlier you mentioned reinforcement                                 learning in recommendations algorithm                                 module is it does it contain some sort                                 of Explorer exploit techniques for                                 recommendations right yeah explore                                 exploit so when you do reinforcement                                 learning you want to if you're not                                 familiar with it you want to you have                                 some model and you want to exploit that                                 model to return results but you also                                 want to for some of your traffic explore                                 other options so that you can learn a                                 better model right so that's what                                 Explorer exploited and that's case the                                 question is just west of us support is                                 of the box                                 no we do not we have you can plug in                                 functionality like that pretty easily                                 and I actually wrote a blog post you can                                 find on medium which goes to details so                                 how one use case that is running in                                 production to do comment ranking using                                 reinforcement learning is doing this so                                 if you're interested in technical                                 details you can look at that anymore                                 okay in that case could you all put your                                 hands together and thank John for a                                 really interesting talk                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=tTIATQk-V00


