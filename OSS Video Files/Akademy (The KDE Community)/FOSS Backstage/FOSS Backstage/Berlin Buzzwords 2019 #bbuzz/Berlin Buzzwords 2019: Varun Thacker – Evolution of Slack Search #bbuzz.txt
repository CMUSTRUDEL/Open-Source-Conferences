Title: Berlin Buzzwords 2019: Varun Thacker – Evolution of Slack Search #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	This talk is about building Slack’s Search Infrastructure. Apache Solr powers search across messages, files and many use-cases at Slack. We’ll first talk about how we can build our indexes billions of documents regularly.

How do we support our language internationalization efforts and what that means to search quality and latency. Lastly the talk will discuss some of the biggest challenges we face today and how we plan on tackling them.

Read more:
https://2019.berlinbuzzwords.de/19/session/evolution-slack-search

About Varun Thacker:
https://2019.berlinbuzzwords.de/users/varun-thacker

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hey guys so yeah I'm gonna be talking                               about how we build search it's like what                               are the like intricacies                               and what are the future things that we                               see we want to invest in and I basically                               I'm in the search and discovery team                               here at SLAC and in my previous job I                               worked at lucid works and where I was                               doing search as well so the quick                                summary of what I'm gonna be talking                                about is I'm gonna start off by talking                                a little bit about overall slack                                architecture so that it kind of gets a                                sense as to how such gets plugged in to                                this flow of things then a little bit                                about the indexing side of things how do                                we index data the offline indexes the                                online indexes then something that is                                called enterprise key management and how                                does that play with indexing some                                internationalization and localization                                efforts and how that played how such was                                playing a part in that and then a little                                bit about the query side of things so                                that's gonna be my talk a quick overall                                summary of slack right like so more than                                                                                                      daily basis so people create lots of                                message content which means search is                                very interesting that slack and how do                                we have people find things that they                                type like last month right so that's                                basically what our mission and what we                                want to do and slack brings people                                application and data so what does that                                mean like so people type in a lot of                                messages they share files they share                                links to tickets like Zen desks or                                Salesforce so all of these data we can                                make it searchable so that you can                                reference it in the future and this is                                what the search UI looks like so here's                                a few things that are slightly different                                from other search engines is each user                                has its own unique set of documents that                                they have searchable like you have                                channels that you talk in you have                                private channels that some                                if y'all have access to and you have DMS                                that you taught chat with your coworkers                                so every search is unique in that sense                                because you and your fellow coworker                                will have different access to different                                channels which means results are not                                very durable not very casual because                                they are different per user and that                                also means we have no real head queries                                as such because it's like pretty unique                                in that sense and for the first part now                                we talk a little bit about how a message                                gets sent on slack and then we will talk                                about how that flows into the search                                side of things so this is like a                                simplified architecture of what happens                                when you type a message right so you                                type in a message the message gets                                relayed to the API service the API                                service fans it out to all other slack                                lines that are connected to you and once                                that's done it's persisted in a database                                so that is the flow of how a message                                gets sent out right now in this what we                                want to talk about now is how does                                search play apart so this is like a                                screenshot of how all like each green                                box represents Arshad but essentially                                this is all search indexes within slack                                in some sense where each column what we                                are looking at is called a stripe a                                stripe is basically a combination of a                                computed collection which is basically a                                collection we built offline and a set of                                live collections which are time                                partitioned on a daily basis so we                                talked about seven days of live data and                                a computed index of everything before                                that and how a stripe is basically a                                logical concept on top of that to make                                search across all of it now each slack                                user gets routed to one of these                                   slices or stripes out there so it's                                hosting data for like multiple users                                multiple teams co-locating them and the                                stripe is spanning across multiple solo                                collections now going back to the                                architecture and extending the slack                                architecture essentially once we we                                spoke about in the earlier slide how the                                message gets persisted right like after                                 the message gets sent out to all other                                 clients the message also gets persisted                                 now right after that what happens is the                                 event like a message sent event gets                                 sent on the queue and the indexing queue                                 essentially picks up that event goes to                                 the database and gets the wrong message                                 content again and uses that to index it                                 in do solar so our search indexes are                                 backed by solo and whoops yeah sorry                                 about that                                 so once the message gets indexed into so                                 this is the overall architecture of how                                 a message flows within the search                                 infrastructure now like I showed in the                                 previous slide it's a combination of                                 building an offline index and a set of                                 live collections so we will talk a                                 little bit about how we build each of                                 them and why we do it in that sense so                                 building and offline index involves                                 combining multiple small multiple moving                                 pieces and making the gluing them                                 together because you're basically saying                                 build every single message that was ever                                 sent or slack and have that tooling to                                 be able to do so so we it's think of it                                 as a service that's similar to how you                                 can index via MapReduce and tools out                                 there like the MapReduce er indexer                                 tools so tools similar to those and how                                 you can build large indexes in a Hadoop                                 cluster and then be able to send those                                 claw senders or indexes over to                                 search cluster now how does this work it                                 basically consists of three parts so                                 there's some keep pressing something                                 sorry it consists of three parts we                                 create something called composite                                 dimensions then there's a dog join and                                 then the last is a solar index stage so                                 we're gonna walk through each of these                                 steps to show how the offline index gets                                 billed now the first step in the process                                 is called a composite dimension job                                 you're essentially think of it basically                                 for search we want all the data to be                                 denormalized right not only do we want                                 the message that you send all the                                 reactions that people made to your                                 message which channel was it in what was                                 like was the file uploaded with the                                 message was it a thread and did other                                 people react to that so we want to                                 denormalize all this data so essentially                                 in the composite dimensions job all                                 we're saying is build in memory it takes                                 data from all of these auxiliary tables                                 right like channels to convert internal                                 channel IDs to channel names                                 similarly users was there a file                                 uploaded and basically we partitioned                                 them per team so we take all this data                                 and we partition all the data per team                                 and each team is then assigned to                                                                                                                 all the oxygen tables which are like                                 metadata to the actual message and we're                                 building it into sharding it out by                                     and being able to create this and store                                 this in memory so that when a message                                 when we're indexing messages we can do                                 direct translations to all the metadata                                 for the message and be able to populate                                 it with that                                 so essentially composite dimensions is                                 just providing us a                                                    all the events that took place on the                                 message                                 and we are able to keep this in memory                                 per team like so we load up per team now                                 the second part of the stage is called                                 the doctrine which is basically a thrift                                 structure which combines the actual                                 message with all of this data that was                                 populated before so which channels will                                 the message typed in which how many                                 reactions were there on the message and                                 things like that so we basically take                                 all these input messages we know which                                 team it belongs in we combine it with                                 the team's composite dimension that we                                 had built up earlier and we basically                                 able to create one solar input document                                 at the very end of this step so at the                                 end of this steps we have built solar                                 input documents or in a thrift structure                                 which is not indexed but the raw data                                 has been generated so this runs as the                                 first phase of the MapReduce process                                 right so we are able to now create all                                 the messages out there now what will                                 basically take us we on each message we                                 then shard it and then on we create                                 these shards in another MapReduce so in                                 the reducer phase of the operation so                                 basically we spin up a solar embedded                                 solar and we are able to create each                                 shot at each reducer right so it                                 basically takes this dog shine thrift                                 structure and it's using EMR and it's                                 basically outputting at the very end a                                 solar core which is basically a solar                                 shot at the very end of this process so                                 this is the three steps that we kind of                                 build to have the ability to index                                 messages offline and once these indexes                                 are built we basically push them to s                                  and then the solar query the solar                                 active servers are able to download the                                 indexes of s                                 so this is the indexing process now                                 going back to the same stripes that I'd                                 showed you earlier right you can see                                 that the number of green dots in this                                 section of it is varies in size that's                                 because when we are computing this index                                 offline you know per team or per stripe                                 how many documents are there gonna be so                                 the number of sharks that we create is                                 based on the number of documents that                                 are there so we can dynamically                                 calculate how many sharks should each                                 collection have while building these                                 step well building this offline so at                                 this point we build an index offline and                                 on top of this index we will now be able                                 to interlay seven live collections or                                 seven like time based partitions and                                 index any new data that comes in and                                 overlay them on top and then we can do                                 the same process over and over again so                                 a few advantages of building a very                                 extensive offline system like this is                                 you can make schema changes very easily                                 you can reindex with different field                                 types you can upgrade solar versions                                 like all of these start becoming very                                 straightforward if you have the vast                                 like if you have a tool like this to be                                 able to index every single message like                                 I said it's it's not easy sometimes                                 because a lot of it has downstream                                 dependencies on is this data available                                 to us how is data is will this data                                 bring down by my sequel shards because                                 like you're just reading too much from                                 my sequel so in our case we build                                 backups of my sequel and that gets                                 uploaded to s                                                     offline database something like that and                                 so we are able to build this up now this                                 was like the essential flow behind how                                 indexing works and now we're going to                                 talk a little bit about                                 what are the few features that slack                                 provides and how does that affect                                 indexing so there's a feature called                                 enterprise key management where                                 basically customers control the key used                                 to encrypt the data any customers search                                 index lives on an encrypted filesystem                                 right so each customer's search index                                 also needs to be built separately from                                 everyone else's because that data is                                 encrypted they sit on a different file                                 system so what does that mean that                                 they're they get individual stripes now                                 so each ECAM user has their own stripes                                 which means if you have like thousands                                 of such users you can now have thousands                                 and tens of thousands of solar                                 collections in the same cluster so that                                 propose like that hosts its own set of                                 challenges and how there's the cluster                                 scale when you have thousands of                                 collections out there and even though                                 indexer the offline indexing needs to be                                 running separately from everyone else's                                 so it's like it involved some more                                 tooling on our site to allow something                                 like this to work with search next up                                 I'll talk a little bit about the                                 internationalization and localization                                 and how do we deal with people typing                                 messages in different languages and how                                 do you search for message when a user                                 might type in a certain language but is                                 looking for messages across everybody                                 typing within slack right so Ana Legere                                 is localization is the equivalent of                                 like say having a blog post in different                                 languages right so you can have an                                 English variant or Spanish variant of                                 French variant but internationalization                                 is having the correct block-posts off to                                 you based on knowing where the users                                 coming from what their languages                                 so what are the languages that are                                 currently supported right so you can                                 have German UK English US English and a                                 set of like                                                              support and all of these are                                 configurable within                                                    time you create a workspace or a team                                 within slack the administrator can set                                 the default locale right the second                                 option is your user preference so you                                 can go and use the Preferences and                                 select your locale that you were going                                 to use so we want to use both of these                                 and both of this information to be able                                 to index the message correctly so now                                 let's see if the workspace preference is                                 in English but that is like the user is                                 in speaking in Spanish right so like how                                 does this how would this work right so a                                 user types in a message in Spanish and                                 these two were supposed to be like how                                 messages look like in different                                 languages but essentially what we do is                                 like whenever a user types in a message                                 their locale which in this case was                                 Spanish gets indexed into a Spanish                                 search field and since the team                                 workspace was English we also index it                                 within the English work language field                                 and what happens when we search right                                 okay this give me a second sorry about                                 that                                 it's loading better nari                                 it's feel good okay that was                                 give me a second I don't know sorry                                 into shrinker broken                                 all right yeah this renders much better                                 right okay so essentially like going                                 back to the example like since the                                 workspace best friend says English this                                 is how the message got indexed and the                                 Spanish variant gets indexed with the                                 Spanish field so what happens when we do                                 a search essentially at search time we                                 have to search across all the search or                                 all the language fields because we don't                                 really understand where the users                                 searching and what he couldn't like that                                 person could be searching for a message                                 that would be typed by somebody else in                                 say French right so at search time you                                 kind of have to search across all of the                                 fields and like different fields be like                                 token ax is differently stem differently                                 based on how the language semantics                                 works right so in this case since we're                                 searching across all the fields and in                                 the example I'm kind of just showing you                                 English and Spanish essentially the                                 query term gets matched with in the                                 Spanish field because the word gets                                 stemmed and then the stem form matches                                 the Spanish field but since the English                                 field does not stem the word it doesn't                                 match in this case the English field so                                 in this case we get a match on this                                 Spanish field and when we search we                                 essentially match and then we show you                                 the highlighted term there right so                                 because of this strategy what are the                                 potential improvements that we could                                 make here right like if we can detect                                 the message up front and instead of                                 relying on the language preference of                                 the team or the users language what we                                 could do is have smarter tokenization                                 strategies within Lucene where we could                                 have one-to-one field but have different                                 messages get tokenized differently so                                 that at search time we only search                                 across one field so that would be like                                 something that we want to explore in the                                 coming months                                 and at we could also try detecting the                                 language at query time if we don't do                                 the smarter tokenization strategy so we                                 will not see an explorer can we do                                 something better rather than searching                                 all the fields and hoping they get                                 matched in one of them versus like                                 having a smarter way to do so now those                                 who feels that were like                                 non-indo-european languages now for cgk                                 we do things a little different because                                 CJK essentially we can we have libraries                                 that can detect that this message has                                 been typed in cgk so we don't merely                                 rely on like the workspace and the user                                 local preferences we can also since we                                 can detect the CJK characters so when we                                 type in something in Japanese                                 essentially we index it into a couple of                                 search fields because we know that this                                 message has been typed in cgk so we                                 index it into a Japanese field and like                                 CG k                                                             different tokenization behaviors and                                 they are able to match the search                                 injection so like the two types of                                 language analysis we do for cgk we                                 detect the languages and for all the                                 indo-european languages we kind of rely                                 on preferences out there to index in                                 last up is the query architecture right                                 now we talked a little bit about the                                 localization efforts and all of how the                                 message gets indexed overall how does                                 this stack up right so we have the                                 backend search intra like the back end                                 where all of slack is written in and                                 then we have a search proxy in front of                                 it and in front of that we have the                                 solar services so when we talk about the                                 backend the web application essentially                                 that's written in hat which is a                                 programming language like PHP variant                                 programming languages and a lot of                                 application code of search is also                                 written there because we                                 to translate for that user what are the                                 channels that they have access to what                                 are the conversations they have access                                 to what are the channel when you type in                                 a message like this we need to be able                                 to look up that user's internal IDs                                 where the channel belongs in and be able                                 to translate all of that so all of that                                 happens in the backend search                                 infrastructure and then those get sent                                 over to the search proxy layer the                                 search proxy is like a little written in                                 Java which interacts with solar using                                 solar G proxy now the search proxy has                                 like a few like it plays a few role like                                 it does a few things there essentially                                 now that we have all of like the filter                                 queries and like the ACLs correctly sent                                 from the web application it can now do a                                 little bit on the query rewriting side                                 of things so it can do some detection                                 there add synonyms things like that and                                 then send it to search and when we get                                 back search results essentially we can                                 do rear ANCA within the proxy layer so                                 the proxy layer kind of doubles up as a                                 rear unkingly layer for us as well so we                                 get back like over-over fetch search                                 results from solar and then do the                                 ranking within the proxy layer now while                                 building like a proxy layer like a few                                 things that we found out and like people                                 should know about is when you're writing                                 a proxy layer like sane Java or any                                 other language essentially what you want                                 to make sure you can control is say if                                 you have a few bad or a few slow solar                                 machines out there the proxy layer                                 should not be holding up all the can all                                 of the connections that it can make                                 against only those machines resulting in                                 the proxy not being to serve traffic to                                 the rest of the solar servers so imagine                                 like a bad solar server                                 which is like the see the AWS machine                                 has is in a bad state or something like                                 that but all the connections go there                                 and it's like just a black hole where                                 like connections are being taken up and                                 the like the search requests aren't                                 being processed essentially now say if                                 you have like hundreds of thousands of                                 connections to that machine being made                                 because a lot of searches are there what                                 happens in the proxy layer right all                                 those requests gets held up and at that                                 point like it cannot serve requests to                                 other machines we want to make sure that                                 the proxy layer is aware of say a pearl                                 collection basis or it knows it has some                                 concepts of limiting requests and                                 mitigating requests only so that a few                                 bad solar nodes cannot affect the                                 remaining solar nodes and we gonna talk                                 a little bit like so this is like the                                 overall proxy service that we have we                                 like at some point we'll talk more                                 details as to how do we do ranking there                                 what are the pros what are the cons of                                 doing query rewriting at that stage                                 versus doing it within solar and things                                 like that                                 and we're gonna dive later and some                                 other talk about that so this was like                                 kind of the overall architecture of what                                 we do so if you guys have questions                                 please free to ask                                 [Applause]                                 hi I was wondering when you're searching                                 over the fields for multiple languages                                 do any like stemming or any kind of                                 analysis on the search term so so the                                 way it works is because like so every                                 field type has its own set of stammers                                 so I have abstracted like I've just                                 called it stem Oh filter but like a                                 different field type will have a                                 different stem Oh with a different                                 language attached to it so essentially                                 that message has been indexed with that                                 sort of tokenization and stemming                                 involved so at search time how the query                                 parser so when a query parser takes in                                 the message it for purl field                                 essentially stems it according to the                                 same language so the same query term is                                 now being analyzed differently of                                 different fields when matching in and                                 that makes sense yeah I was wondering                                 like why is the CJK indexing flow yeah                                 quite different to the other european                                 languages because they are just another                                 language so right now we do cgk                                 differently because we have the ability                                 to detect CJK up front so like whenever                                 even if say the users locale is English                                 but he types in a message in Japanese                                 you can we are able to detect that                                 that's Japanese essentially so because                                 of that we know that we can index it                                 into the Japanese field essentially but                                 we don't have the we don't really do                                 that for like say in like Spanish or                                 something like that or somebody types in                                 in Sui like on any other language we                                 detect for cgk specifically so we say if                                 this message is cgk then index at like                                 that otherwise rely on                                 the preferences the other potential                                 improvement which you are suggesting for                                 non-surgical languages is to detect in                                 the same way as cgk yes and then                                 eventually like and we'll only find out                                 when we start actually working on this                                 but like my hope is all of this if we                                 detected at in next time we can even                                 index it into one search field so that                                 different languages get recognized                                 differently but they all land up in the                                 search search field and so that you're                                 not searching multiple fields and paying                                 the cost for that as well do you have a                                 problem when one were have different                                 meanings in different languages and then                                 if we do actually like because in a lot                                 of cases like you can do the wrong you                                 can still find a false positive match                                 because it means something else but                                 because the tokenization                                 matches it doesn't mean that it's                                 actually finding the right thing we like                                 we're still looking at ways to improve                                 the idea yeah I was just wondering how                                 you handle actually the cost of doing a                                 long tight range search when it come if                                 I have kind of like a                                                  okay how do you keep it from being                                 really expensive and having kind of the                                 year ago states have been online so                                 that's a good point so basically if so                                 every time you search like you have a                                 option between recent and relevant                                 search so basically like we we are like                                 by default it's a relevant search so                                 that means we can like search in the                                 lives and then like prioritize the older                                 messages based on that and score them                                 differently as well                                 while recent still has to like still                                 search across everything so we don't                                 really do much around like                                 short-circuiting queries like if we find                                 enough hits beyond a certain range as of                                 now we've not really hit that problem to                                 be honest like like a lot of like search                                 the search system seems to be holding up                                 fine without that so we'll get to it                                 when it kind of becomes more prominent                                 any more questions my question might be                                 a little beyond your presentation but do                                 you have some or do you think about some                                 personalization for your search I'm sure                                 like so like there was a talk recently                                 that another quirk of minded where he                                 talked about personalization in context                                 to how we can do that with keeping                                 messages secure like so like the trick                                 there I guess is like we can't like                                 every team is unique because the data                                 belongs to them and then even within                                 each slack team like different users my                                 type different messages that different                                 users don't have access to so all there                                 was a talk based on how do we index a                                 only public content from that team to be                                 able to do like at least some user                                 behavior and stuff like that so maybe                                 check like that that should be a good                                 starting point I'd say any more                                 questions no okay so I thank you very                                 much for your talk for ruining my round                                 of applause                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=tMNcqTfd7Qc


