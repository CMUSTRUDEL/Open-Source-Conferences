Title: Berlin Buzzwords 2019: Simon Willnauer â€“ What's evolving in Elasticsearch and Lucene? #bbuzz
Publication date: 2019-06-27
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	This talk will walk through the latest developments in Elasticsearch and related changes in Apache Lucene. We will look into significant query performance improvements, frozen indices, usage of the new soft-deletes feature for cross data-center replication and changes APIs and usability changes that makes search-as-you-type almost trivial.

The talk also presents the results of some long standing changes that we made in Lucene which are now paying off in geo-search use-cases and how they are integrated in Elasticsearch. Folks that are interested in distributed aspects of Elasticsearch will also hear about improvements in Zen 2 and Index-lifecycle-Management.

Read more:
https://2019.berlinbuzzwords.de/19/session/whats-evolving-elasticsearch-and-lucene

About Simon Willnauer:
https://2019.berlinbuzzwords.de/users/simon-willnauer

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay cool well first of all thank you                               for having me because I was                               not supposed to speak at this conference                               tat dining was supposed to speak here                               but he couldn't make it and Nina called                               me last week and said like hey can you                               can you like find some slides and they                               would talk and say yeah sure okay                               a couple words to myself I'm a I'm a                                founder of discomfort as well I started                                this with Isabel and Yan in                                             super happy that this conference found                                its place and it's it's like a little                                family I see a lot of faces that I've                                seen like ten years ago and that I                                worked with and that I work with online                                and open source that's great                                I'm a loosing committer since I think                                                                                                   started as well and I found it elastic                                the company together with Shia Benin and                                and two other people                                yeah and I'm I'm still working on that                                stuff it's still fun and let me let me                                show you what we're doing                                elasticsearch is is known for executing                                crews really really fast and under under                                the hood we use leucine quite heavily                                for all our queries that we execute and                                query times will always depend on what                                kind of benchmarks you're running                                they're pretty good and every time we do                                an improvement I have the impression                                like we can't do any better like that's                                it like there's no way we can improve                                this but lately a couple of                                elasticsearch engineers started to look                                into a paper from I think                                               a person that is here from Berlin Stefan                                pol worked on that had a couple of                                really nice ideas about how we can                                actually pre compute the maximum score                                Corey can produce and with this                                optimization we were able to bump this                                up quite significantly in in that sin in                                elasticsearch seven and elasticsearch                                seven and Lucine eight                                yeah and this these these performance                                improvements are just stunning right                                it's like the the                                                 second on a terms query that's just                                insane when we saw the numbers we                                couldn't we just literally couldn't                                believe it                                the reason why this is not like super                                significant of us of course also other                                queries not the queries on here is that                                we we using we don't use this                                optimization everywhere yet like we're                                still working on adding it to more                                queries and how we can bubble it up and                                down the query tree the optimization is                                called the magic wind it's a it's a week                                and optimization and what we basically                                doing there is and I'm trying to explain                                it on a higher level at the entire talk                                could be filled with the details so                                please do bear with me and there's also                                a lot of people here that can give you                                all the all the details after effect and                                afterwards if if you're curious so let's                                look at this great elastic searching                                we've seen and we are basically having a                                function to calculate the maximum score                                each of the query terms can contribute                                to the overall score and in that                                situation when we look at the top                                   documents of a career that's what we've                                seen does it doesn't give you all the                                results it gives you the top                                        very efficiently when we do that and                                collect our doctor documents we can we                                can say like what is what's the minimum                                competitive score that we have to have                                in order to make it to the top                                       the further we collect documents the the                                higher the score needs to be in order to                                get there which allows us to ignore                                entire terms down the road with                                collecting documents which means                                essentially that's why the the single                                term optimization is so high like we at                                some point we can Susie say we skipping                                entire segments or entire shot and we                                don't even need to look at the the data                                under the hood all at some point where                                we know we can stop collecting and then                                pre-emptive query the weekend doesn't                                work all the time unfortunately it                                 doesn't work when you use aggregations                                 on elastic search because aggregations                                 by definition need this                                 every documentary matches but if you                                 have a pure curry that that only is                                 there for matching and not calculating                                 that only the top ten we can't apply                                 this optimization because we're skipping                                 a ton of documents the other situation                                 where we can't do this inelastic                                 searches there's an option called track                                 total hits if you have this little                                 number on top of your websites and like                                 we found                                                               this optimization doesn't work for you                                 because we can give you an estimate of                                 how many documents at least matched                                 that's the default in in in elastic                                 search that's used to be the default in                                 elastic search you get the total hits                                 but now we move to a different option                                 here in lost or seven where we say okay                                 we have at least                                                       if that's something that you can work                                 with that is done of the weekend                                 optimization we'll kick it for you you                                 can set this number that we at least                                 collecting                                                         collecting                                                             go well the less of the performance                                 boost you'll get cheer shapes to your                                 types is a it's it's one of the totally                                 underestimated capabilities of Racine                                 and I've talked to a lot of a lot of geo                                 people and from a lot of different                                 angles I get like machine elasticsearch                                 and all the functionality on top of it                                 is is one of the best you can use for                                    or search these days let me look at a                                 little bit in a history we started and                                 leucine                                                             trees                                                                    be KD trees which is a different data                                 structure represent numbers until then                                 we were stuck with the prefix try that                                 over built - to represent numbers that                                 was working great but it wasn't as                                 scalable as the bikini trees are in the                                 beginning allow search try to pick it up                                 for one dimension for numbers and dates                                 and the further we got in last search                                 was in                                                                 points                                 two dimensions for dates numbers and                                 date ranges calendars and things like                                 that and in Alaska                                                      recent we started to use seven                                 dimensions for geo shapes the what                                 happens under the hood is we like we                                 bullying triangles and try to represent                                 the cheer shapes as triangles and the I                                 got some performance numbers from some                                 users that were able to improve their                                 indexing pipeline by a factor of                                        search performance is much better we can                                 represent shapes that were that they're                                 now indexed in a couple seconds that                                 will not be able to represent before in                                 memory on a                                                          memory efficient there's a little                                 massive improvement it's accurate to one                                 millimeter versus                                                      is much smaller indexing is super fast                                 queries are                                                            Gio points also getting a performance                                 hit here I only have                                                   just moving okay yeah I hope I hope I                                 make it to get some questions the                                 changes API who knows the who has ever                                 heard about the changes API on on                                 couchdb it's it's quite an interesting                                 thing you can just like subscribe to a                                 stream of changes in you getting all the                                 changes that are indexed into or or                                 edits to the to the database and you can                                 consume it like a like a big log file if                                 you will and it's it's one of the                                 changes that elasticsearch wanted to do                                 since we founded a company and we didn't                                 know how to do it for a very long time                                 and after like seven years into the game                                 we built something else called soft                                 deletes in Lucene that we use for cross                                 data center application that made this                                 feature absolutely trivial to do and in                                 contrast to what couch TV does in                                 elasticsearch you will be able once we                                 have to change the API release you will                                 be able to filter the changes as well                                 with any kinds of curry you have that is                                 a super powerful feature that you can                                 say like I know I want to be informed of                                 any changes that are in sir                                 gyah range I want to be informative                                 certain certain changes with a certain                                 category those kind of things full text                                 queries against it they're going to be                                 super powerful we try to not call it                                 send to but I call this end to anyways                                 sin is all we need to filter even if you                                 change the filter yeah it's important I                                 guess but this is unrelated to the                                 elasticsearch change sin is our is the                                 name of the elasticsearch distributed                                 layer that try to figure out like who                                 know which note belongs to the cluster                                 end of force and we have a long history                                 of edge cases and problems and split                                 brains and we reduced the number of                                 spins dramatically but there were a                                 couple of scenarios that were just not                                 fixable with with the model that we had                                 so we spent about two years investing                                 into a new distribution layer that is                                 partially based on on research that is                                 complete formally proven and as very                                 very test well tested and it removes all                                 the pain points that we had before                                 especially with configuration if you've                                 used the elastic search you hopefully                                 have set minimum master notes to a                                 quorum level or to a quorum number of                                 the number of nodes in the cluster well                                 this is important because if you lose                                 one of the nodes in in this scenario                                 will you'll or if you use a majority of                                 the nodes in this scenario didn't they                                 not be able to form a cluster that will                                 prevent your split-brain the problem                                 with this is if you add a new node like                                 you have this moment where you have to                                 change the value to be correct and                                 that's that is actually the moment when                                 you get loose data and what happens in                                 practice is people don't set it people                                 don't set it when you leave when notes                                 leave either so you very likely have a                                 misconfigured cluster at some point and                                 you have problems if there's only one                                 note or one node leaving there is so                                 many situations here that the best                                 option for us is just remove this                                 setting altogether and replace it with                                 sent to where you have to set it only                                 once when you boot step you plus                                 initially you would start your cluster                                 and you say okay I want to start a new                                 cluster with six nodes                                 I set the initial master nodes to four                                 and from then on you can scale up and                                 down and shut down a cluster and we                                 started in elasticsearch we'll take her                                 off the right well used for you that is                                 probably going to remove ninety percent                                 of the of the split pre-order or the                                 data loss situations we had                                 elasticsearch altogether                                 adaptive replica selection is a feature                                 that I think we started to work on in                                 last search five and we had some parts                                 of it already committed in in the five                                 series and it's it was already done in                                 six but not enabled in seven it's the                                 default what is this                                 so in elasticsearch replicas I think I                                 have a pointer here like replicas as                                 well as primaries conserved requests if                                 the coordinating err gets a request it                                 picks one of the replicas either the                                 primary or this replica and fires up the                                 request on this back channel here we are                                 able to collect information how long's                                 the queue how long did it take to a serf                                 the request did we actually have we have                                 we actually been queued on the node etc                                 and that gives us a very good indication                                 if the node is on the load or not and in                                 some situations you might get slow                                 responses and then you actually want to                                 give this node here a pause you don't                                 want to overload that and that's that's                                 to reduce your                                                          latency the adaptive replicas selection                                 is default in the last through seven if                                 you want to try give it a try and if you                                 search use casing you sometimes                                 suffering from spikes you can enable it                                 in last year it's six as well in the                                 history of elastic search we had                                 something called the tribe note the                                 tribe note had tons of problems I don't                                 think anybody has ever used it without                                 having a problem that's something that                                 we realized pretty late then we wanted                                 to replace it with something that is                                 much more stable but at the same time                                 it's also much less powerful we replaced                                 it with something called cross cluster                                 search which allows you to have                                 clusters at different locations these                                 locations are the cities now like                                 searching across data centers is has its                                 own details and its own Devils                                 so you but if you have multiple clusters                                 in a single data center it's only                                 something that that works very very                                 performant Qabbani can now go and talk                                 to the cluster in London and say okay I                                 want to search some indices in New York                                 and some indices in Tokyo and then get                                 it back as a result as if it's from a                                 single cluster the really cool part                                 about this is that we have our                                 compatibility with from the current                                 major to the previous majors latest                                 minor                                 that's what we guarantee so from                                     we'll always be able to search                                          from                                                                    work to search to seek                                                able to have three major versions                                 different clusters without doing any                                 upgrades we're able to search your data                                 can h out if you have retention policies                                 you can HR the entire cluster your                                 upgrade process might be very slick CCR                                 is all the writing part of CCS cross                                 coastal search it's a it's a feature                                 that is it's not free it's a it's a paid                                 as a premium feature in elastic search                                 and it allows you to basically have a                                 leader index and a follower index in                                 different cluster these clusters can                                 also be in different data centers so you                                 can have like a the London sales index                                 be replicated in and across data centers                                 into New York as well as Tokyo or you                                 can have like a central location where                                 you have a copy of the neutral data                                 centers it works both directions it's                                 per index you can do this all together                                 and it's really relatively easy also to                                 configure you can have auto follow                                 follow patterns if you have like data                                 indices and you're rolling over new                                 images every day the the replication                                 will kick in automatically if you                                 specify a an auto following pattern                                 frozen indices it's it's part of our                                 making things slow effort which is like                                 it makes me sad like                                                     I try to make things fast and now                                 everybody's asking for making them slow                                 why do you want to do that anyway it's                                 it's another area where you can innovate                                 and frozen any ceases is is a solution                                 where we try to actually utilize our                                 resources much much better like if you                                 have elasticsearch allocated allocating                                 charlotte's on a note all you only you                                 have a certain amount of disk space left                                 here right not a really good utilization                                 down here in the disk space but under                                 heap you already filled up because the                                 the the each index needs a certain                                 amount of memory in this clip for                                 mappings and segment memory and stuff                                 like this with frozen indices you might                                 be able to fill your disk up completely                                 and the frozen indices are only searched                                 in a serial manner they go through a                                 like a what we call a throttle thread                                 pool which only has one thread so we                                 execute one after another and we lazily                                 loading them into memory to not occupy                                 too much memory that's slower in some                                 situations if file system cache is                                 actually hot it's like only a couple                                 like                                                                  that it makes in insert execution speed                                 while it's recommended to have these                                 indices all being optimized for Smerch                                 having a single segment because if they                                 don't have a single segment and we need                                 to build additional accelerate data                                 structures for like global ordinals to                                 do aggregations which is very costly                                 that ties very much into it that's                                 something that I most research use cases                                 don't really need but if you're like                                 analog in matrix security analytics use                                 cases the next life cycle management is                                 something that that you either do                                 yourself or you always wish that ostrich                                 would do for you in this example you                                 have a hot warm and cold node                                 architecture these nodes might have                                 different discs and different different                                 GPUs and things like that and your                                 indexing into your hot no it's full                                 speed and after every night it at                                 midnight you go and roll over                                 index and then move some of the some of                                 the charts to to the warmth here and                                 then go and shrink them together in one                                 chart that that's the inverse of a split                                 function we can split and shrink in                                 elasticsearch the shrink function is                                 actually very fast because we're using                                 hard links under the hoods it's uh most                                 of the time if you allocate them all on                                 the same hard disk it's a it's an                                 instant operation super quick if you                                 force merge time you're getting smaller                                 and at some point you want to have moved                                 it to the cold tier and then freeze it                                 and that's the what's in X lifecycle                                 management does for you on some point                                 you want to delete it I hope you don't                                 forget about that the leading is the                                 best way to save space if you use lock                                 stash or beats or anything like that                                 they install their their own policies                                 and there's a next life cycle management                                 you eyes in cabana it allow you to do                                 that that's all free yeah nanosecond                                 time steps you would think of                                 milliseconds are enough right but                                 there's a ton of things that can happen                                 in a millisecond right so if you look at                                 this granularity with date nanos you                                 will be able to also have a melissa                                 nanosecond granularity and elasticsearch                                 in you will be able searched with this                                 in similar schemas if you have date                                 nanos or date melees it doesn't matter                                 you'll be able to aggregate on these on                                 these fields full-text searches used to                                 be bread and butter and then we also                                 doing a lot of other things but we we                                 wanted to make things simpler think of a                                 like a searches to type use case how                                 hard it was in the past to build that                                 and how much energy we put in through                                 leucine and like build ffs theses                                 jesters and we'll put them into memory                                 they're super heavy to build they're                                 super heavy too heavy to have a memory                                 either blazing fast but our term                                 dictionary is doing a really good job as                                 well if you do it right in elasticsearch                                 now you have these index prefixes as an                                 option on the field and that will allow                                 you if you index like quick it'll also                                 put qi qi qi c and so forth the prefix                                 is up to like four characters into your                                 index to have really really fast                                 prefix sir                                 out of the box the nice thing is if you                                 do that out of the box it compresses                                 really well like our turn the trees are                                 really made for these kind of things                                 we call these edge engrams and by                                 default now we would with a single                                 second you can do that but for a search                                 as you type you also need the phrases                                 once you finish typing the first word                                 right you need the phrases the same                                 option exists for the phrases and we                                 also doing engrams here but we that's                                 what we in the scene and last we call                                 them shingles or token-based engrams to                                 have really really fast phrase search                                 which will improve your phrase search is                                 quite dramatically if you do that I can                                 in this case you can you can just type                                 these in in the match phrase prefix or                                 in a match phrase query and these                                 queries will actually execute very quick                                 they look at the mappings it's like okay                                 I have prefixes here or I can just go                                 and and use the prefix diagrams and to                                 to do the search and I don't have to do                                 a wild-card search which is basically                                 positioning in the term dictionary and                                 then scanning until your front terms                                 interval queries so it's we we used to                                 have span queries or we still have                                 spankers in the last version you seen                                 but one of the biggest downsides with                                 with them was that there                                 the scoring was quite difficult the                                 extending them was hard like using them                                 with other queries impossible                                 they only taken analyzed text which is                                 also difficult because you're the                                 analyzers are defined in in elastic                                 surgeon you have to like somehow have                                 them in two places and it's very close                                 to my heart because it's one it's a                                    year old issue I started just like                                    years ago and Ellen Thank You Ellen for                                 finishing it up after after standing in                                 the shoulder giants like there had a lot                                 of other things had to that's sometimes                                 it's a very funny anecdote of open                                 source like you have a good idea and you                                 started and at the point at a time it                                 just doesn't work                                 I just dream just missing api's I'm                                 missing other features and then                                        years later you have all these pieces                                 like now now we could actually do it in                                 a nice way and and that's that's one of                                 these things it's you seen                                      eight it's one of these issues that I                                 know by hard and I probably will never                                 forget it okay so what can we do with                                 this if we have text like this we could                                 we can search for like quake in your fox                                 and then the whole thing needs to be                                 near lazy and thought but they need to                                 be at Jetsons right next together within                                 like five characters where the spans                                 this was quite difficult because you had                                 to put like the analyze text in there                                 now we can just put the text in there                                 and do it and yeah that that's that's                                 pretty much it and I think I have about                                                                                                   minutes left                                 oh that's awesome I I can I can talk                                 about a lot of other things too                                 do we have questions do we have                                 questions please all kinds of elastic                                 searching to seeing questions                                 hello yeah so this is a question about                                 the cross cluster replication yeah I'm                                 just wondering under the hood is that                                 document replication I hope not yes it                                 is okay yes yes it's document                                 replication so in elasticsearch we have                                 added something in Luster's                                              sequence IDs or every change has an ID                                 assigned to it it's broken into two                                 pieces it's a primary term the primary                                 term comes from the current active                                 primary of a shard and the sequence ID                                 is a unique identifier per chart and                                 these two numbers together allow you to                                 have a total ordering of we went and we                                 we guarantee you that this total                                 ordering of vince doesn't have even                                 doesn't have holes so we guarantee you                                 go elasticity will always maintain a                                 full history if a document has been                                 acknowledged to the user and cross data                                 center replication                                 makes use of this of this property where                                 we can say and a follower index we can                                 say I have a full history up to sequence                                 ID                                                                    have that's in between your latest                                 highest consecutive                                 sequence idea that's what we call the                                 global versus the local checkpoint and                                 then we can go and catch up with the                                 history that's not just used for cross                                 data's on a replication we also used                                 this for catching up with a replica so a                                 rapid if you shut down and out and start                                 it up again and you've indexed something                                 in in in between like you lost like five                                 minutes of your history the replicas                                 will go to the primaries like hey I'm                                 out of sync give me give me everything                                 since sequence ID                                                        checkpoint and then we will start                                 indexing new documents into it so we                                 could completely complete out of order                                 delivery here and we will try to catch                                 up with the primary now there's some                                 trickiness to it and I said this before                                 we have a feature in the scene it's                                 called soft deletes soft deletes are                                 very similar to delete sin in Lucene we                                 don't really delete a document which                                 market has deleted and when we merge we                                 prune them we drop them for across data                                 center replication and also for the                                 changes API we needed to maintain                                 history we couldn't just like drop them                                 on the floor and remove them we need to                                 hold on to them across mergers and the                                 soft deletes allow you to do that allow                                 you to basically specify a curry that                                 tells the scene which documents it can                                 which of the deleted occupancy can                                 actually prune under under merge in                                 which it has to keep and this allows us                                 to maintain what we call our retention                                 lease on each of the shards and if I'm a                                 replica and and I'm talking to my                                 primary I tell like okay I have a                                 potentially for five minutes and my                                 minimum sequence ID is                                                   the next five minutes that chart the                                 primary will definitely guarantee you                                 that it has the full history between                                    and its global checkpoint Mike this is                                 good okay more questions                                 I is there also involvement in adjusting                                 the highlighter I'm quite struggling                                 with unified and all the other                                 highlighter types list are any I don't                                 think we have any massive new                                 developments along those lines Ellen                                 maybe Ellen can answer this question                                 better oh yes so we the medicine changes                                 in Toulouse scene itself there's a new                                 matches API which will now allow queries                                 to return exact positions of where                                 they've matched something the unified                                 highlighter is beginning to make use of                                 that but we also have Jim Frenchy it                                 works realistic is also working on a new                                 highlighting which will be entirely                                 based on that and will basically remove                                 all of the bells and whistles from                                 everything and just have it now this is                                 a very plain highlighter use this and                                 they will tell you what what matched and                                 both tribes of the cut down on a lot of                                 the they're very complicated settings                                 that the current highlighters have the                                 idea is eventually to hopefully just had                                 that single highlighter and remove all                                 the other ones                                 yes okay two things I think you didn't                                 mention another amazing thing that                                 showed up in leucine and elasticsearch                                 in the past year or maybe my timings off                                 ranges so you can now arrange the                                 first-class field you can put onto an                                 element has a start and an end I think I                                 mentioned it briefly in the when we used                                 how we used the kitty trees that date                                 ranges is here and                                                      that is true like you have like                                 searching data ranges of first class                                 citizen in leucine or and in                                 elasticsearch yeah good this is by the                                 way a lot of these changes originated                                 from developments in elastics which were                                 we try to always add these changes where                                 they belong like there is no friction                                 between elasticsearch leucine and and                                 other people using your scene because                                 like we strongly believe that these                                 changes should be made on the right                                 level and this is one of the perfect                                 examples right this is this whole be KD                                 trees you today we think about is like i                                 we've added it there but it surely                                 started in                                                              that and then like working on the                                 structure for two and a half years and                                 then another two and a half years to                                 actually get here right there is so                                 there's a lot of work and a lot of also                                 cross community like back then when we                                 started Mikey were working for elastic                                 and then no but even do you depart it to                                 your next adventure and we kept on                                 working on it and that is that's a                                 that's that's something that I'm I'm                                 super proud of that's a very cool                                 feature of this community so then an                                 unrelated follow-up question a year from                                 now when you're giving this talk again                                 what are the things you're gonna tell us                                 that happened in the following year what                                 can I say                                 let's let's take a step back and I'm                                 hoping I'm hoping I'm not like Nick's                                 gonna kill me for that but like when you                                 look at it what could you do with vkd                                 trees you could for instance look into                                 and that's why                                 don't take it as a promise right it                                 might be years out look at the data                                 structures that are that can represent                                 graphs like neighbor trees for instance                                 and have a very very efficient graph                                 search like brass search that is so                                 blazing fast that you haven't seen it                                 before that that's something that I can                                 totally imagine being at a base of these                                 data structures but there's there's a                                 lot of ideas and we also what we                                 constantly asking ourselves so what is                                 the next next we've seen major oceans                                 big change like what are we                                 none of these big changes have been                                 planned right it's like they just                                 suddenly show up that that is very                                 interesting like the indexing                                 improvements with documents right upper                                 thread interval queries soft deletes                                 they like we have a problem we try to                                 solve the problem what's the best                                 solution and sometimes it takes a couple                                 years but yeah that's that's that's the                                 outcome this is answer your question                                 Mike okay okay discussions there's one                                 over here we have enough time luckily                                 thank you for the talk I have a question                                 related to growth cluster search yeah is                                 it faster than calling the three                                 clusters in barrel and retrieves the                                 results if it's faster didn't I don't                                 think you have a performance advantage                                 nor downside one thing that you cannot                                 do outside of elasticsearch is reduce                                 the results together if you have                                 aggregations that span multiple serve                                 multiple indices like merging them                                 together can only elastic search can do                                 that otherwise yes it's an obvious task                                 for a client I'm completely with you but                                 if you if you have like a histogram                                 aggregation how do you merge them                                 together if you do it outside of elastic                                 search but it's in there under the hood                                 it works exactly the same as if the                                 connection is within the cluster we in                                 on on the nodes that can do cross                                 coastal search it's a lot like                                 regularity here we establish a one                                 an unidirectional connection in                                 elasticsearch one clusters the star                                 network like every node connects to                                 every other node and and and vice versa                                 cross clusters are just different we                                 only connect in one direction and we                                 maintain a smaller connection pool but                                 these are like keep alive TCP                                 connections and the connections as fast                                 as your network goes yeah this is a                                 question thank you hello and I have a                                 question regarding the Big D trees so                                 currently it's limited to seven                                 dimensions and why it's seven so I see a                                 particular use Kies useful for me is to                                 compute a document similarity for                                 example but usually seven dimensions for                                 that is not enough but seven is hard                                 coded in loosenin you cannot go further                                 so white seven I I think seven is the                                 limit where we were able to make it                                 performance but I probably might need to                                 answer this question the limit is eight                                 in the scene I'm not sure why it's seven                                 in elasticsearch and eight honestly that                                 limit wasn't picked for any particularly                                 good reason it was just placed there as                                 a temporary limit because we didn't know                                 what would happen if we pushed it beyond                                 that but KD trees don't really scale                                 that high if you try to do a very high                                 dimensional KD tree it's sort of                                 degrades into a linear search so there                                 is a point where it's not worthwhile so                                 if that's a limit for your use case and                                 it's compelling to relax it open an                                 issue start a discussion it doesn't have                                 to be eight it could go higher than that                                 at leucine or elasticsearch and the                                 limit is not seven it's just because                                 we're using seven dimensions for that                                 geo shape like we could also use eight                                 but that's that that data starting to                                 fit in there                                 hello my question relates to the prefix                                 searching with the phrase queries I was                                 wondering if that prefix in Graham was                                 enabled by default on like all the text                                 no yeah you have to enable it in a                                 mapping and is the phrase wildcard                                 phrase searching then only enabled for                                 the in Graham fields no no you can you                                 can do that on any field it'll just                                 basically look at the mapping is the                                 optimization enable if the optimization                                 enabled the query that we execute is                                 different okay thank you right so we got                                 the same result but if you enable the                                 optimization will be faster hey so I                                 have a question about the weekend week                                 I'm terribly familiar with that but I'm                                 you mentioned that there are multiple                                 constructs that can actually disable                                 this optimization it doesn't it hasn't                                 propagated to all the types of queries                                 yet you mention aggregations as example                                 and also like tracking total counts                                 results counts and then my question is                                 like if I write my words myself and I                                 maybe use a construct that actually                                 prevents this optimization for kicking                                 in how can I easily found because maybe                                 I can achieve what I want with a                                 different construct which does not                                 disable this optimization is a profile                                 API the right way to look at to find out                                 like so if you get this before this yeah                                 so if you got the relation to be exact                                 in the result you know that we didn't                                 apply the optimization yeah buddy for                                 example maybe I use some because I                                 understand there are more constructs                                 that disable these optimism it's only                                 only this aggregation dirt again                                 yeah you know what I mean but also                                 scripts Corizon might disable this                                 optimization if I have very yes yes we                                 not using the optimization when script                                 store is in the creek that is correct                                 depending on the creek also I guess okay                                 I have also have a second questions is                                 Mike and Michael yesterday had a talk                                 how they usually seen at Amazon and they                                 mentioned multiple times that they use                                 some features that are exposing the the                                 scene but exist new scene but don't                                 exist in the last search near                                 elasticsearch on Ursula one of those                                 features was Lucy searching across                                 multiple segments with different threads                                 is that what we might see one day in the                                 last search maybe yeah maybe okay I mean                                 Mike and I talked about it so the                                 biggest problem is that how do you do                                 the scheduling which quick quest gets                                 the most threads like the concurrency in                                 elasticsearch Lassiter's bill to get                                 concurrency through queries like                                 executing a lot of queries in parallel                                 rather than executing one query in                                 parallel but yeah as a I I started to                                 look into this yesterday and then we I                                 think we have enough constructs in                                 elasticsearch to to allow this on demand                                 like that you know if you don't have a                                 lot of conquering surgeries will give                                 some threats to the search execution                                 there's there's a lot of corners here I                                 guess but this is something that I I'm                                 personally not a huge fan of settings                                 I would like to enable this feature when                                 we can write if you have                                               second on a note it doesn't make sense                                 to use multiple threats but you have                                 three queries the second I don't know it                                 makes total sense it's not a week we                                 know we have this information we have                                 queue length with utilization we have                                 the time as curry spending the queue if                                 it's cute at all we can use the                                 statistic to me to make that work and                                 again that's it's a it's a classical                                 example of like if I would have tried to                                 implement this four years ago I would                                 have been a disaster and very hard but                                 today might be different and then these                                 ideas come up and that's why we come                                 his conferences and talk and yeah so I                                 know another promise but it yeah yeah                                 perhaps the last question because we are                                 running out of time hello hey thank you                                 so still a question about cross cluster                                 search so is is the cross cluster served                                 a new way of distributing the cluster                                 stage like should we use it even if we                                 don't have multiple locations but just                                 to distribute the cluster tip so the                                 cluster state is not distributed with                                 cross cluster search its stateless that                                 is one of the biggest differences                                 between tribe node and cross cluster                                 search course cluster search is a pure                                 read only right you don't have a                                 coordination if you make a change like                                 add a news index or mapping change in                                 one cluster that that is the other                                 clusters were not affected they don't                                 get any information about this there's                                 one of the biggest problems which tribe                                 note is like you have one cluster                                 updates the cluster stayed and it gets                                 propagated to all the tribe notes that                                 connected to it cross cluster search was                                 solely made for this one purpose that                                 you'll be able to search across multiple                                 clusters if these clusters are in the                                 same data center or in different data                                 centers in the same region or in                                 different regions is completely                                 irrelevant if you ask me for the my                                 recommendation I would totally recommend                                 you to have more smaller clusters than                                 one big one for the for a couple of                                 reasons like if you you upgrading well                                 you can do one by one you're learning                                 about the the pitfalls earlier if one                                 cluster breaks down and you still have                                 the other ones to serve requests if you                                 want to etch out data over time you can                                 do multiple clusters the granularity how                                 big one of these closest is is a very it                                 depends question
YouTube URL: https://www.youtube.com/watch?v=17fEK1eTck0


