Title: Berlin Buzzwords 2019: Philipp Schmidt – Large-scale Data Quality Verification (...)
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Philipp Schmidt talking about "Large-scale Data Quality Verification - How to Unit-test Your Data with deequ".

Every day, companies rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises the customer experience and any decision process downstream. Therefore, a crucial, but tedious task for every team involved in data processing is to verify the quality of their data. In this talk, we will show how to continuously verify data quality by defining metrics and constraints, resulting in better testing for data pipelines and machine learning applications.

Read more:
https://2019.berlinbuzzwords.de/19/session/large-scale-data-quality-verification-how-unit-test-your-data-deequ

About Philipp Schmidt:
https://2019.berlinbuzzwords.de/users/philipp-schmidt

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi guys what's up so my name is philipp                               nice three two all of you and                               I would like to talk to you about                               large-scale data quality verification                               with our somewhat newly released data                               quality library called DQ all right so                               the the first question that you that you                               probably that you might be asking                               yourself is okay how is data quality                                relevant to me and and how might it be                                relevant to my team and I'm gonna try to                                motivate it with a few examples and and                                then after that we're gonna have a look                                at a few prominent use cases of our                                library how-to so the obvious case is                                just so you have a single batch of data                                and you want to verify data quality but                                then also we look at the generalization                                of this where you have like ever-growing                                data bases for repetition data sets and                                so yeah our library is open source it's                                hosted on github there will be a link at                                the end of the presentation and yeah                                what I so just to get this out in the                                beginning I think asserting data quality                                before you actually work with the data                                and consume it in downstream consumers I                                think that can we have seen that in the                                past that it can save you from some some                                debugging and error fixing work all                                right so why should we care about data                                quality so the first reason that I want                                to that I want to bring here is because                                data so most of human and algorithmic                                decision decision processes or                                decision-making processes are backed by                                data so that means if the data is just                                wrong or or missing we have the wrong                                conclusions so I think that's maybe the                                most important one because it's so                                general but also what was found is that                                data quality can have an impact on ml                                models and what they found is on a sigma                                                                                                      be considered sort of as the most                                important hyper parameter of an                                algorithm that you have what they did is                                they took an IMDB movie review movie                                review sentiment data set so they tried                                to predict the sentiment of a data set                                and they just took standard out of the                                box I could learn pipeline with stop                                words removal and all of that and HBO on                                top so hyper parameter optimization but                                what they found is so does                                purple bar here is that they actually                                received much better predictive                                performance after they cleaned up their                                data so they had a quality kind of an                                impact on a male models okay and if you                                if you look at so this little box here                                that's the ML code that's usually                                scikit-learn and and all of that but                                usually in a production scenario ml                                models coexist with other very very                                data-centric components and that might                                be just data collection data                                verification and then of course feature                                extraction and as all of these                                components are so they deal with data                                you might consider also looking at air                                quality for these components to have the                                most efficient okay so the last point                                that I want to make is on operational                                stability so everyone I'm sure knows of                                the infamous nullpointerexception and                                they and these except type of exceptions                                can bring any production system to an                                abrupt halt really and this usually                                happens if there is data missing and                                this is a very obvious case right so                                just systems crash but there's also a                                much more subtle one and so I try to                                depict the Machine loading model with                                this cloud I didn't find a better                                picture but imagine this is a machine                                learning model and it is configured to                                be consuming some upstream data source                                and of course this model is making basic                                assumptions about the data for example                                of the scale of an attribute and here so                                it things that it's square meters but                                maybe at an arbitrary point in time the                                data producer might decide okay well I                                want to change it from square meters to                                square feet and if that happens then                                it's very likely that the model will not                                crash but it will rather produce                                predictions that are slightly off and                                 finding these kind of these kind of                                 errors is very hard because they don't                                 yeah they they don't make some services                                 and systems fail really okay so looking                                 at quality assurance so split by if we                                 either look at software systems or data                                 I think in in software systems at least                                 we know how to do this very well so                                 there's a long and well-established                                 practice of how to assure the quality                                 and software systems                                 so depending on the complexity of the                                 component you might just write unit                                 tests and if it becomes bigger                                 integration and acceptance tests and                                 olive and all of that                                 oftentimes what we have observed with                                 data is that data so verifying the                                 quality of your data often has resulted                                 in very repetitive and ad-hoc efforts                                 and having this sort of approach does                                 not really scale in a way and that's why                                 we are working and have recent quite                                 recently you release DQ this is Eddy the                                 melody framework that I'm gonna be                                 talking today about and this allows you                                 so if you take one thing out of this                                 talk please remember you can unit test                                 your data with that so that's the basic                                 catchphrase of this library okay all                                 right                                 so the that's schematic overview of DQ                                 so here in in yellow and there are                                 basically two inputs to DQ so the first                                 one is data of course right and the                                 other one is you as the user interacting                                 with this framework or library and what                                 what the user usually does is the user                                 defines these data quality constraints                                 this is here on the top alright so how                                 does this working let's let's look at                                 the center of this schematic at the                                 center of the queue our metrics                                 computations and they are backed by                                 SPARC and I will talk about it in a                                 minute why that's the case                                 and when when these metrics have been                                 computed then they meet these                                 constraints that have been user                                 user-defined                                 and then depending on the outcome of the                                 of the of the evaluation of these                                 constraint verification the tests might                                 either fail or succeed right and then                                 that will give you an indication of                                 whether the data quality is at the                                 desired bar that you want to want to be                                 a tad mmm all right so a typical unit                                 tests in DQ obviously has to scale to                                 big datasets so that's the reason why we                                 have used Apache spark and most of our                                 metrics are formulated as ask your                                 aggregation queries over the data                                 nothing in our design and acute Isis                                 particularly to spark because you could                                 be just plugging in any SQL                                 client back-end that supports                                 user-defined aggregation functions so                                 okay given that it's scale to big                                 datasets then what it usually does is it                                 computes one or several data quality                                 metrics for example you might be                                 interested in knowing okay what is the                                 how many nodes do I have in this set of                                 columns or this particular column in                                 this data batch and so this would be the                                 completeness metric here there are also                                 and there's a bunch of others and then                                 given given that we have computed these                                 metrics with spark in the backend then                                 we then we will apply your user-defined                                 validation code so you might be                                 asserting then okay I know the the                                 completeness or there are                                            values is that acceptable to me as an                                 engineer or is that already too much and                                 depending on that you can reformulate                                 your verification logic okay okay so                                 let's have a look at some code because                                 maybe this is most actionable alright so                                 in the queue we have we expose several                                 several api's the first one here is a                                 verification suit and the other one is a                                 check API and I've bold-faced all of the                                 checks here that are data being declared                                 here and what this is doing essentially                                 is we we have some data spark data frame                                 in that case that we want to that we                                 want to verify the data on and then                                 we're checking for the completeness of                                 the customer ID and the title column                                 that means we allow for                                                 in these two columns on that particular                                 batch of data and then we also certain                                 the uniqueness of the customer ID that                                 means that there should be no duplicates                                 on this column over all of the rows and                                 then here it becomes a little bit more                                 interesting so we also have you can be                                 passing in so Adam over here this is                                 basically the user-defined validation                                 code so what we do is in the backend we                                 compute the count distinctive titles so                                 the number of distinct titles and there                                 in the database there's there's present                                 that might be known or there might be a                                 value that you would wish for and you                                 could assert with any kind of logic that                                 you want with this and the interesting                                 thing here is that num titles might be                                 stemming from another part of your                                 system you might even be calling and                                 while while the code is executing                                 another service                                 be getting a hold of the most relevant                                 value that you want to assert against                                 okay and then yeah some some more you                                 can yeah you can also assert against                                 histogram values so you can just compute                                 the histogram of a particular of a                                 particular column in this case device                                 type and you want to assert in this case                                 at least that it should be no more than                                                                                                      column and priority should always be                                 either high or low                                 just one more sentence about this year                                 so they in normally checked these checks                                 anomaly checks are a bit special as they                                 are as they are making use of a metrics                                 repository I'm gonna dive into that how                                 to use that and how its how its applied                                 in a minute but essentially what this                                 check is asserting is that the size so                                 sorry so the size of this data set is                                 essentially similar to previously                                 observed ones if you compute that every                                 day for example okay now let's have a                                 look at UM at a concrete example of how                                 you would do that so that would be the                                 the code that I just showed you is to                                 see the singular data better and you                                 want to verify their equality against it                                 but now you have ever-growing databases                                 for example you ingest impression logs                                 and now how do I do that                                 if the data is petitioned in a sensible                                 way and here I'm assuming daily                                 petitions and what we want to do we have                                 basically two objections we want to                                 assert the data quality on every day                                 individually and we want to assert the                                 data quality verify the data quality of                                 the data overall right so here let's                                 have a look at the naive implementation                                 first so today right it's Tuesday and                                 what we have done here we have applied                                 DQ on Sunday we have applied it on                                 Monday and the output of these                                 computations were always the metrics and                                 the outcome of our unit test that they                                 succeed or that they fail and then in                                 the nave of computation for example on                                 Tuesday you might be interested in okay                                 wait hang on a second what is the data                                 quality of Sunday Monday and Tuesday                                 if you if I just Union all of that data                                 so in the naive approach what you would                                 need to do is you would you would need                                 to rescan the data which can be                                 arbitrarily costly right because the raw                                 data can be can be                                 huge and ideally we want to save                                 ourselves some computations on the data                                 if we can that is what the incremental                                 approach is basically addressing so we                                 what we're doing exactly the same so                                 today is still Tuesday we have to apply                                 DQ on Sunday and on Monday however we                                 have not only computed the metrics but                                 we have also computed intermediate                                 states these are these states that are                                 on the bottom here and these states are                                 you can imagine them sort of as summary                                 statistics of the data regarding the                                 metrics that you want compute one to                                 compute and these these states are much                                 much smaller usually than the raw data                                 and you can combine them all so                                 efficiently and then what you can do is                                 maybe I will just show some code so I've                                 tried to boldface here a bit so the new                                 petition that's that there would be the                                 petition of today so Tuesday and from                                 and here in this particular example we                                 are interested in the completeness of                                 the origin column and we compute from                                 the we compute the state from there from                                 the data this is this summary statistic                                 for the completeness metric then in the                                 next line what we do is we load the                                 previously computed States that we might                                 have computed two days ago and one day                                 ago and then we we take the Union you                                 can call it whatever you want you can                                 sum over the states or you can Union                                 them and then we arrive at the overall                                 table state that includes the state from                                 Sunday up up until Tuesday including and                                 then given given the state you can                                 compute metrics from the state's                                 directly and that is probably more                                 cheaper than having to scan all of the                                 data again and having to load all of                                 that so that's the basic motivation for                                 this all right so the the second use                                 case that I did I want to show that I                                 want to show you here is a continuous                                 sort of continuous hands more hands of                                 the wheel solution to data quality                                 verification so imagine the same                                 assumption so you compute data quality                                 metrics every day and let's imagine this                                 value column here I didn't add any units                                 but let's say that's the number of rows                                 of some of some data frame that you                                 inspecting everyday and then this dashed                                 line is your your user-defined                                 so so your defined threshold that should                                 not never be exceeded for example data                                 frames should never be larger than two                                 million rows right but coming up with                                 these absolute numbers can be sometimes                                 very tricky so that's what anomaly is                                 these anomaly based detection anomaly                                 detection based checks are doing they                                 are making the assumption that the data                                 today should be very similar to the data                                 in the metrics space should be very                                 similar to the data that we have seen                                 earlier okay another code sample and                                 here maybe I can spend a minute talking                                 about the metrics repository so this is                                 exactly the same so we still have a look                                 at the verification suit we have a and                                 we declare these checks over the data                                 and here in this particular instance we                                 make use of a file system back to matrix                                 repository and the metrics repository is                                 just your container where you can write                                 metrics to and you can read it from and                                 what this anomaly check here the online                                 normal in this case is doing it's                                 reading the relevant metrics from the                                 metrics repository that you that it                                 needs to be estimating sort of what is                                 the what is a normal data set size                                 because here we're looking at the size                                 metric on the right hand side and given                                 that the the SEM given that this anomaly                                 track has a basic idea of what a typical                                 data set size might look like it can                                 then make the test either fail or                                 succeed depending on whether the data is                                 similar in that space and we implement a                                 number of different very rather simple                                 and straightforward strategies so the                                 one might just be a simple threshold in                                 strategy like laid out here or I'm an                                 online normal estimator so that does                                 just takes text mean of all of the right                                 so it computes the mean of the sizes and                                 the standard deviation and make sort of                                 makes this normal assumption all right                                 so let me let me try to wrap up the talk                                 so I hope that I could have convinced                                 you that human and algorithmic                                 decision-making is mostly data backed at                                 least                                 should be if you want to be objective                                 and for that reason I think we should                                 care a lot about the air quality because                                 that directly impacts our                                 decision-making what we often have found                                 is that data quality verification                                 results in ad-hoc efforts that are very                                 tedious to the engineers implementing                                 that and yeah it is not reproducible and                                 not standardized in a way and for that                                 reason we have released the hue that is                                 basically enabling you to have a                                 framework for all of this where you                                 don't have to write custom scripts to                                 check for the completeness the sizes and                                 all of that and it gives you hope we                                 hope at least a concise API to come up                                 to declare these checks over the data                                 and also as part of the cue as I as I've                                 shown earlier we enable the use cases of                                 ever-growing databases that are                                 partitioned in some sensible manner and                                 then also the more hands of the wheel                                 solution with anomaly detection based                                 checks if you are if you are interested                                 in all of the nitty-gritty details of                                 the experiments that we ran before I'm                                 releasing the library we have a real DB                                 paper please feel free to check it out                                 please feel free to check it out and                                 yeah definitely give the it's the red                                 power look if you want and we also have                                 quite recently actually released the                                 m-                                                                       attention I'm happy to take questions                                 now                                 [Applause]                                 any questions Oh first of all thanks                                 also Philip and as I said DQ'd currently                                 just works on batches of data is there                                 any plans to have it for streaming data                                 yeah that that's a great question I mean                                 so at the moment we don't support it the                                 direct streaming case however I mean you                                 can always batch stream data if that                                 helps but at the moment unfortunately my                                 answer is that we don't support that                                 kind of use case hi we think it's really                                 good work we're actually using DQ and                                 but how does it compare with I mean T                                 effects with the data validation and in                                 particular they impute a schema based on                                 a given training data and they ever does                                 adjacent file and then when new data                                 comes in they'll automatically validated                                 and print out the you know the                                 differences and and and retros that                                 break that give any plans for that and                                 what about Python maybe you take any mo                                 file as a you know I mean any any                                 thoughts on the you know the road map                                 yeah the road map is pretty much open                                 we've had several requests to support                                 also Python based environments for                                 example PI spark there are some                                 components that I didn't talk about that                                 are part of DQ that is the an automatic                                 constraint suggestion that maybe comes a                                 bit closer to what you have access in                                 terms of schema validation because we                                 try to automate the degeneration of the                                 of the constraint verification logic so                                 we look at some data and we do that for                                 for some amount of time and then we                                 probably have a good idea of what all of                                 these constraints should look like but                                 but as I bet as you ask them so the road                                 map is pretty much open and we're happy                                 to to take requests                                 any more questions we have one minute I                                 would like to ask two questions first                                 one is how many people are working on                                 this project that changes from time to                                 time or a couple definitely okay okay                                 I'm the second vision I see two use                                 cases first one it's like because DQ is                                 a unit test library so I could write at                                 the spark job and then then create some                                 artificial data and test and write tests                                 in DQ and check it or I can I can have a                                 like data trigger job which I would run                                 every day for example so what's the the                                 use case what's the primary use case for                                 the it's like to run it's regularly or                                 to run it from time to time and I want                                 to check that my my version correctly                                 yeah so my answer is definitely gonna be                                 you you you yeah you should aim of doing                                 that continuously every day okay because                                 if you do that that way that only then                                 you have the best idea of how your data                                 behaves and yeah so we have seen abrupt                                 changes and data distributions and if                                 you don't do that every day you might                                 just be missing the point and once the                                 data has been basically propagated to                                 all of your consumers then it's already                                 too late right                                 so because then they have may be                                 consumed already this faulty data that                                 might have been caught I'm saying might                                 here but might have been caught too by                                 data quality verifications okay thank                                 you                                 Thanks thank you very much Philippa                                 Thanks                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Y2vzoAJl7ro


