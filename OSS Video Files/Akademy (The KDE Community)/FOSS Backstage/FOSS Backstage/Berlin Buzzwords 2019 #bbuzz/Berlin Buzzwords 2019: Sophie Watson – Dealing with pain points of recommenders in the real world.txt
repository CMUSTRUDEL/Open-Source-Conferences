Title: Berlin Buzzwords 2019: Sophie Watson – Dealing with pain points of recommenders in the real world
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Off-the-shelf recommenders are a necessary building block for developing a personalised recommendation system, but they are not sufficient to solve personalisation problems by themselves. Such systems are designed to use either explicit or implicit data, but never both. They are unable to identify anomalous user activity, and cannot determine when one account is put to multiple distinct uses, for example combined personal and business purchases, or many users in a household sharing streaming media.

This talk will look at the pain points of recommendation algorithms and cover ways to overcome them in practise. We will dive into the example of data drift in recommendation using data from a music streaming service. A user’s behaviour is likely to change over time for multiple reasons: users might discover a new genre they like, they may associate negative memories with a song they used to love, or their taste may simply change as they grow up. Given a profile of a user’s tastes over time, it’s relatively straightforward to recommend content that will spark nostalgia.  However, “nostalgia” comes from Greek words meaning both “returning home” and “pain,” and the songs we loved once may not bring us joy today!

We’ll talk about how to identify changing tastes, find content which will feel like returning home without the pain, and give some simple suggestions for how to incorporate these findings into off-the-shelf recommenders to give a more robust user experience.

Read more:
https://2019.berlinbuzzwords.de/19/session/dealing-pain-points-recommenders-real-world

About Sophie Watson:
https://2019.berlinbuzzwords.de/users/sophie-watson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you so hi I'm Sophie Watson I work                               at Red Hat                               the team our forward                               engineers so we focus on helping                               customers solve their business problems                               using data science and get their machine                               learning and their AI workloads running                               in the cloud over the past year or so                               I've been thinking quite a lot about                                recommendation engines and today I want                                to share with you some of the thoughts                                and ideas that I've had and specifically                                I'm gonna point out hurdles that we've                                had to get across and issues that we've                                seen along the way I'll also suggest                                some fixes and some potential solutions                                many of the services companies and                                products that we interact with on a                                daily basis and are providing                                personalized content to users and giving                                personal recommendations to us and if                                there's companies out there who aren't                                striving to give that personalized                                experience then it's pretty likely that                                they're going to fall behind in their                                market pretty soon so the types of                                recommendations that are made to us as                                users come from a huge range of                                applications Spotify for example curates                                daily playlists for me so if I log in I                                can see a suggestive playlist grouped by                                genre there'll be a pop playlist a                                classical playlist an indie playlist and                                so on and these are a mix of songs that                                I've listened to previously and I like                                from that genre and other songs that I                                haven't heard of before but they fit                                well into that playlist Apple knows that                                last month I bought an iPad so all of my                                adverts and emails are now suggesting                                that I go out and purchase some very                                expensive adapters which mean that I                                could plug my iPad into just about                                anything and video streaming services                                make perhaps what we think of is more                                classical recommendations using our                                listening history or our watching                                history to predict which films we should                                go ahead and check out but underlying                                all of these forms in which I see                                recommendations made to me there's some                                recommendation algorithm so if you look                                at the literature on recommendation                                engines all of the algorithms are                                are evaluated on how well they perform                                in terms of minimizing some error and                                there's an assumption that lower error                                means better real-world predictions for                                users in practice that's not really case                                so are these underlying metrics or                                errors or things like log loss or mean                                squared error and they don't necessarily                                directly correlate with what we as users                                want from our experience they're just                                not capturing the full picture once you                                put your recommendation model into                                production it's true that you can see                                how 'yes' interacts with that and then                                you can go ahead and improve your                                algorithm based on the way in which                                users are behaving and but for today                                we're going to think about the stage                                before production and we're going to                                send think about some things that we                                could do to improve recommendations                                before we get going into production so                                we'll start by talking about the                                problems which recommendation algorithms                                don't usually address and to do this                                we'll introduce alternating least                                squares so this is a very widely used                                recommendation algorithm from there                                we'll see what problems in                                recommendation are not solved by                                alternating least squares will select                                will suggest solutions for some and                                we'll dive deep into the problem of data                                drift we'll talk about a solution which                                uses approximate set similarity to make                                recommendations and we'll see how these                                set representations enable us to make                                sensible and robust recommendations to                                users with an added layer of                                intelligence over that initial                                alternating least squares algorithm and                                we'll see how this is done in a way                                which scales nicely as the number of                                users of a system grows so let's start                                with alternating least squares                                 alternating least squares or als as it's                                 more affectionately known is the                                 industry standard recommendation                                 algorithm and there's off-the-shelf                                 implementations that you can plug and                                 play written in scala - and off                                 in its original form the algorithm                                 itself deals with explicit data so if we                                 stick with the movie streaming service                                 example an example of explicit data                                 would be something like user one gave                                 film five three stars ALS and takes this                                 data and puts it into a matrix like so                                 and you can populate that matrix with                                 any other recorded data that you have                                 from there we're ready to implement the                                 algorithm all in English squares                                 factorizes this matrix into two smaller                                 matrices it does this by some iterative                                 process that for the purposes of this                                 talk we don't need to worry too much                                 about today but if anyone wants to talk                                 in more detail about ALS and how it                                 works then hit me up offline so one of                                 these two matrices represents the users                                 whilst the other represents the items or                                 in this case the films so each row of                                 the user matrix corresponds to one user                                 each column of a film matrix corresponds                                 to one film once the algorithm has                                 factorized the data into these two                                 matrices it becomes really simple to go                                 ahead animate predictions for a given                                 user so if we want to estimate how user                                 one which rate film - we can do that by                                 simply taking the dot product of the row                                 corresponding to user                                                   corresponding to you a to film number                                   and the ability to estimate how a                                 particular user will rate a particular                                 film enables you to do things like we                                 recommend the ten films which a user                                 will most like alternating least squares                                 also allows you to identify users who                                 are similar to each other who have                                 similar tastes if we look at rows of the                                 matrix of users and we compare the                                 similarity of them then we can do that                                 by comparing the vectors in the rows and                                 in the same manner we can compare movies                                 by looking at the columns of these                                 matrices and identify movies which the                                 algorithm deems similar and this might                                 help you save some computational cost so                                 for example if you knew that two users                                 were exceptionally similar and you'd                                 already computed how you one of them                                 would react to a particular film you                                 could just represent that recommendation                                 to the other user so that example was                                 for explicit day sir but what if I data                                 is implicit if we think about the music                                 streaming service as scenario what does                                 it mean if you'd listened to a song so                                 say I listened to a song once do I like                                 it right we don't know would no idea if                                 I listened to a different song ten times                                 you would like your algorithm to be much                                 more confident that indeed I probably                                 liked that song and moving on if I                                 listen to a song                                                        to be even more confident than I like it                                 so there's an implicit version of ALS                                 which works on such data it requires                                 that you define a mapping from your                                 recording to how confident you are that                                 the user likes that item so in our case                                 this would be a mapping that mapped from                                 number of plays through confidence but                                 other than that it's pretty much the                                 same as the implicit the explicit                                 algorithm ok great so now we can deal                                 with explicit data we can deal with                                 implicit data we can quickly make point                                 recommendations to users and we can                                 identify similar users but that doesn't                                 necessarily mean that you should just go                                 ahead and push you're alternating least                                 squares recommendation algorithm to                                 production so in this next section we'll                                 think about what alternating least                                 squares is not providing us with if we                                 go back to this emergency that we've got                                 two matrices one for users and one for                                 products but what happens when a new                                 product is added to the market no users                                 have rated that product yet so we don't                                 know how they're going to interact                                 and because the components of these                                 feature vectors in ALS don't correspond                                 to explainable features we can't just go                                 ahead and generate one for this new film                                 we need that ratings data in order to be                                 able to predict the future vector for                                 the film and thus make recommendations                                 and in the same way that products arrive                                 some products go off the market or fall                                 out of favour with the whole population                                 so this might be for political reasons                                 ethical reasons or it could be because                                 of the media storm or so on and out of                                 the box als doesn't handle this all in                                 English words also doesn't take into                                 account changes in users opinions over                                 time so it's not uncommon for people to                                 have the same accounts now for years and                                 years I've had the same Spotify account                                 since                                                                  but my change in taste in music is very                                 apparent and the music I listen to is                                 much broader now and alternatingly                                 squares just does not consider that at                                 all there's not just one way in which                                 cases can change so we usually think                                 about change in terms of aging and I'm                                 much less likely to listen to club music                                 now than I was when I was                                                also some seasonal changes in taste so                                 this might be seasonal in the                                 traditional traditional sense people                                 only watch Christmas films in December                                 for example and the type of music that                                 is often released in summer and thus                                 that people listen to in summer is                                 tragically different from the things                                 that are released in winter but you                                 might also see changes that indicate                                 seasons of life so there's likely some                                 correlation positive or otherwise                                 between a change in someone's                                 relationship status and their interest                                 in romantic films another thing which                                 ALS can't handle it all is anomalous                                 recordings so these crop up perhaps when                                 you just hit the wrong star rating when                                 you are watching a film or maybe there                                 is some real anomaly                                 in your opinion perhaps you hates all                                 horror films except for one all in                                 english squares is unnecessarily                                 sensitive to these anomalous recordings                                 so in the algorithm anomalies are fed                                 back into the main matrix which is then                                 used to create your user vector and as                                 such all of your future recommendations                                 are influenced by that one anomaly                                 so although OLS can provide us with                                 these personalized recommendations                                 quickly and easily it would be nice if                                 we could solve some of these issues that                                 it doesn't address before we ship our                                 engine to production now some of these                                 problems can be simply solved by                                 post-processing so if we think about the                                 ALS algorithm as our model it takes in                                 data and it returns recommendations to                                 users a post-processing microservice                                 could be placed in between the model and                                 the recommendations and solve some of                                 the issues for example we could use it                                 to filter out these seasonal                                 recommendations therefore not recommend                                 Christmas films in July for example it                                 could be also used to ensure that you                                 recommend any new or any promoted                                 products to the whole population and the                                 outdated products are never recommended                                 now you probably just don't want to                                 remove all of the data you have about                                 outdated products from your data store                                 because these do give you insight into                                 how the user may react to other products                                 so we do just want to filter those out                                 at the post-processing stage what we                                 can't do with post-processing is use it                                 to identify changes in user taste so                                 it's not able to figure out which                                 products or media will spark nostalgia                                 and identify those from the ones that                                 you don't mind if you ever see again it                                 can't tell if you watched a load of sad                                 breakup movies in a very short period of                                 time or whether you're actually just                                 quite like sad breakup movies and you                                 watch them frequently and it's not able                                 to identify                                 anomalous behavior and then prevent that                                 from influencing these recommendations                                 further so lovely for us there's another                                 class of algorithms which we can use to                                 make recommendations and these use                                 composable signatures in this next                                 section we'll introduce the class we'll                                 sure why it solves a few more problems                                 so let's go back to our music                                 recommendations scenario here we have                                 three users the first two users have                                 made me listen to the same stuff so I                                 think I hope you would all agree that                                 it's reasonable to recommend to use the                                 two that they go out and listen to Billy                                 Bragg and recommend new order to use a                                 one but if we look at user three the                                 only house Taylor Swift in common with                                 the other two users so we're much less                                 likely to tell users one or two to go                                 and listen to Spice Girls or recommend                                 that user three goes out and buys the                                 Smiths Greatest Hits and in fact there's                                 a very nice statistic that we can use to                                 quantify this level of similarity                                 between two sets so this is known as the                                 jacquard index and it's computed as the                                 union of those two sets over the                                 intersection so if we were going to                                 compute it fuses                                                     they've listened to the same artists                                 four of the same artists and they've                                 listened to a total of six artists                                 between them and so their jacquard index                                 is four sixths for users one and three                                 they only have one artist in common and                                 between them they've listened to nine                                 artists hence we get a nice and if you                                 compute your jacquard index for the                                 music history of lots of different users                                 with user number one then you can                                 identify users that have similar tastes                                 to number one and as such you can                                 recommend some use music to number                                   which would be songs that the people                                 that they were deemed close to have                                 listened to but they previously have not                                 so now we have a way to numerically                                 quantify the similarity of sets but                                 storing all of the artists that every                                 user has listened to in a set is not                                 very efficient so we can extend this                                 idea of storing a user's music history                                 in a set by storing it in a bit vector                                 so here each bit or box as they're shown                                 represents a band or an artist and the                                 bits are set or colored in if the user                                 has listened to that artist if we                                 compute these for every user then we can                                 compare users by comparing their bit                                 vectors and the jacquard index of bit                                 vectors is computed in the same way as                                 it was for sets so the intersection is                                 just how many bits are set in both of                                 the vectors in this case it's three and                                 the Union is how many bits are set in                                 any of the vectors in this case it's ten                                 hence the jacquard index is                                          vectors provide a much more efficient                                 representation of sets than the set                                 themselves but I probably haven't yet                                 done them justice so one really nice                                 property of bit vectors that we really                                 want to capitalize on today is that                                 they're composable what that means for                                 us is that it's very easy to merge these                                 bit vectors together                                 so suppose user one listens to music not                                 only on their computer but also on their                                 phone and each device keeps track of a                                 bit vector for itself it's really easy                                 to merge these into one global vector                                 for the user and gather all of that                                 information together all perhaps you                                 want to keep these two bit vectors                                 separate so maybe the user listens to                                 podcasts on their commute to work on                                 their phone but listen to music once                                 they're sat at their desk and so you can                                 use those bit vectors that are separated                                 to go ahead and make relevant                                 recommendations on the appropriate                                 device but it's still nice to be able to                                 obtain simply that global overview of                                 the user using the joined vector                                 where this composability really gives us                                 great improvements over alternatingly                                 squares is when we use different bit                                 vectors for different time periods                                 it's these time-dependent bit vectors                                 which allow us to do things like map                                 changes and user's behavior over time                                 and identify the music that you're going                                 to keep coming back to you again and                                 again and ultimately figure out what's                                 gonna stop the spark nostalgia so you                                 could keep a different bit vector for                                 every month every day every week for a                                 user and you can see the changes in                                 behavior there you can catch new trends                                 and you can see what songs people keep                                 returning to now suddenly we can't just                                 stop there this isn't yet a functional                                 solution to our problems for a couple of                                 reasons                                 although the bit vectors take up a lot                                 less space than explicit sets the                                 downside is that you have to visit every                                 bit in the vector in order to perform                                 the set operations the jacquard index is                                 still relatively cheap to compute but                                 the real problem is that to find the                                 most similar users to any given user                                 we'd have to compute an acceptable                                 number of jacquard indexes Spotify has                                                                                                          compute the similarity between just one                                 of them and all the other users it's                                 going to be really expensive so what's                                 next                                 so the algorithm that we're going to use                                 to solve our problems here is called min                                 hash min hash is widely used for                                 identifying similar documents at scale                                 but we'll see that we can use it to                                 determine set similarity and we'll also                                 use it to make recommendations min hash                                 takes that massive vector which                                 indicates the artists that the user has                                 listened to and that sit down to a much                                 smaller structure we call this a min had                                 min hash signature so how do we generate                                 one of these well what we need is n hash                                 functions where n is the size of your                                 min hash signature so here we have five                                 of them                                 each of these functions maps to some                                 large set of numbers to begin with every                                 entry in the pit back in the main                                 hashtag mature is set to infinity and                                 what we do is we move along the bit                                 vector until we find a set bit the first                                 place empty so we carry on when we reach                                 a non empty bit we do the following so                                 what we do is we first look at what                                 number bit that is if I use computer                                 science counting then that fit number                                 one because we start counting at zero                                 apparently so I pass that number in our                                 case                                                                  each of them returns an integer we then                                 update the min hash signature to take                                 the minimum of for each role what's                                 already in the signature and the new                                 number so the minimum of                                                is sweet okay so we keep the infinity                                 out there we put the                                                                                                                             I'm not gonna okay so hopefully you get                                 the idea of how we get started with min                                 hash next we go back to our bit vector                                 and we keep moving along until we find                                 the next set bit so in this case it's                                 bit number                                                             at                                                                       hash functions and we then just compute                                 the minimum of what's already in the                                 vector and what the hash functions gave                                 us so in this case we'd update the                                 second and the fourth rows if I start                                 counting at warn in that case sorry for                                 any confusion so we continue this                                 process and what you end up with is a                                 min hash signature for your user some                                 min hash signatures are by construction                                 shorter than your bit vector so it's                                 faster to compare                                                       than it is the compareto bit vectors and                                 the way in which we compare min hash                                 signatures is that we just can't the                                 proportion of elements which are the                                 same in both of them so in this case                                 we've got two matches there so the                                 similarity here would be deemed                                 two-fifths or not point four now a min                                 hash signature is an approximate                                 representation of a set that allows us                                 to compare those sets it is possible for                                 two sets which are not the same to have                                 the same min hash signature in practice                                 this very rarely happens but it's worth                                 remembering that you're never gonna                                 underestimate similarity with min hash                                 but you may overestimate it for                                 recommendation purposes this over                                 estimation isn't going to cause us too                                 many problems if we think about what we                                 do once we have identified similar users                                 we'd go back to their bit vectors and we                                 take the difference of the two bit                                 vectors we'd look at which songs one has                                 listened to that the other has not and                                 from there we could make recommendations                                 so the pink bits here represent the the                                 artists that we would recommend to the                                 other user suppose we looked at the                                 difference between those two bit vectors                                 and saw that it was actually large in                                 such a situation we'd likely                                 overestimated the similarity and so we                                 take a step back so now we've got a way                                 to compare two users without having to                                 make as many pairwise comparisons and                                 min hash also has that composability                                 property that the vectors had so if we                                 have min hash signatures for a couple of                                 devices we can combine them just by                                 taking the real wise minimum and the                                 process of computing these min hash                                 vectors doesn't actually have to start                                 from the bit vector you could just hash                                 based on the artists names or the song                                 names for example as they're streaming                                 in so this gives you a nice way to                                 quickly update your min hash signature                                 as the user is using as the streaming                                 service it also helps out when you're                                 thinking about the issue of adding new                                 items to the market we don't have to                                 change the size of our min hash                                 signature we can hash whatever we pass                                 in                                 so that solves the problem of us having                                 to compare huge bit factors but that                                 wasn't really the problem that we cared                                 about that wasn't where the                                 computational drain was lying we still                                 haven't made any headway on trying to                                 reduce the number of pairwise                                 comparisons which we have to make in                                 order to find these users that have                                 similar behavior luckily for us                                 something known as locality-sensitive                                 min hash exists so locality sensitive                                 moon hash looks at subsets of the users                                 min hash signatures and if any to have                                 identical signatures in any of the                                 subsets then these users are considered                                 a candidate pair from there you would go                                 on and compute their similarity or their                                 approximate jacquard index by looking at                                 their min hash signatures as a whole to                                 determine how similar they are and if                                 you do want to make recommendations the                                 way in which locality sensitive min                                 hashing works is by splitting the min                                 hash signature into bands so here we                                 have five bands each containing two                                 roles a new hash function then maps from                                 the contents of the Bands into some                                 buckets so for us our hash function                                 would take in vectors of length two                                 since the bands have two roles in each                                 of them if in any band to users map to                                 the same bucket then they're going to be                                 considered a candidate pair and we go                                 back and look at the min hash signatures                                 and see how similar the users are thus                                 we only have to compute similarity of                                 the monastic matures for a subset of the                                 whole population so if you think about                                 the Spotify case if we reduce from                                     million potential users that are the                                 same as mymusictaste                                 even just down to a hundred thousand                                 that's a massive computational saving so                                 Manoj and locality-sensitive min hash                                 solves a lot of our problems but perhaps                                 and surprisingly I haven't given you                                 solutions to all of your recommendation                                 problems of the last half an hour so                                 here's something                                 that we didn't have time to touch on                                 when some suggestions for how to                                 approach these problems going forward so                                 the first is that you might have noticed                                 that min hash example we did was for                                 implicit data we essentially assumed                                 that if someone listened to a song they                                 liked it but earlier we were all in                                 agreement that we're not entirely sure                                 that that's the case so one way round                                 this could be to only record the                                 information in the min hash signature or                                 in the bit vector if a user listens to                                 that artists multiple times so here we                                 wouldn't put song again but we would put                                 song B and song C we're introducing some                                 sort of threshold number and we're                                 saying okay if you've interacted with                                 this product for so many minutes or                                 you've explicitly told us that you do                                 like it then we can make that hash if                                 the data is in fact explicit if the user                                 does say okay I give this film five                                 stars we might want to deal with that                                 slightly differently so one thing that                                 you could do is keep two min hash                                 signatures for every user these are                                 cheap to store because they're                                 relatively small and in one of these we                                 could have things that we know the user                                 likes and then the other things we know                                 the user dislikes I think we could                                 probably all have a reasonable argument                                 that if you dislike some things there's                                 probably some correlation there with the                                 things that you do like so that                                 information is still important you can                                 use it to make recommendations or maybe                                 there's                                 yeah so another problem that we haven't                                 addressed is that of making                                 recommendations in the case where                                 multiple users share one account                                 who here shares some sort of streaming                                 accounts with somebody else and so it                                 might be obvious in this case because of                                 different device usage that you can                                 identify who is using what and make the                                 recommendations accordingly in the same                                 way that we discussed perhaps someone                                 commuting using their phone and then                                 being static and using their computer or                                 you might have to do something more                                 clever like trying to cluster users                                 behavior and when you think about                                 watching movies I think there's an extra                                 layer there so often if you watch a                                 movie with someone else the movie that                                 you watch might be a compromise of what                                 they like and what you like and it might                                 not be the same so from some angle it's                                 your taste but head-on not exactly so we                                 want to see how to deal with this I                                 would argue that this is a bit of a                                 lesser version of an outlier and                                 although outliers don't influence all                                 recommendations when using main hash in                                 the same way that they do in alternating                                 least squares we haven't charged about                                 how to identify them so it might be                                 plausible to just look at the end most                                 similar users to a given user and say ok                                 we've got this new recording for our                                 user did anybody that we deem similar to                                 them also interact with this product in                                 the same way and then from there you                                 could decide whether or not you wanted                                 to record that data and finally we                                 haven't talked about users changing                                 their opinion so suppose I wrote from                                 Haile and then the next year I changed                                 that rating to a dislike can you get                                 some extra information from that so will                                 I now dislike all of the movies that I                                 watched of the same genre in the same                                 time period that I first watched that                                 initial movie or perhaps I need                                 damn wait that behavior from that period                                 until we can figure out what's happening                                 so that's some ideas for the problems                                 that we didn't have time to talk about                                 in detail today but let's review what we                                 did we've seen that min hash gives us a                                 way to obtain summaries of a user's                                 interactions with a set of items and                                 these summaries are composable thus                                 allowing us to capture trends and                                 identify changes in users behavior and                                 identify different behavior across                                 different devices they enable us to look                                 at users listening history over periods                                 of time and make time sensitive                                 recommendations accordingly and this is                                 something that we couldn't achieve with                                 alternating least-squares this is really                                 going to help to make any recommendation                                 service more personable more adaptive                                 and thus more successful if this week we                                 see that one of our users is playing                                 lots of breakup songs we might wait this                                 highly for a while and recommend them                                 more breakup songs until they stop                                 listening to them and then we might want                                 to be quite kind and remove that chunk                                 of music history from their user history                                 I saw eyes do not influence further                                 recommendations and we also saw how a                                 locality sensitive hashing enables us to                                 identify candidate pairs of similar                                 users and this prevents us from                                 comparing every user of a system in                                 order to find similar users so please                                 stay in touch I think we might have some                                 time now for questions                                 I'm also around for the next two days so                                 if anyone wants to chat about                                 recommendation engines then that would                                 be great thanks                                 I think was remarkable any questions in                                 the recommendations there hi sorry yeah                                 thank you for the good talk and like you                                 said in the beginning that the offline                                 measures don't work and a B testing                                 works but a/b testing is like really                                 very expensive and like so there are                                 some learning to rank matrix like                                 procedure at rate K map mean a fresh                                 precision which are used in all                                 recommendation competitions finally they                                 are like they are proved to be useful                                 over time okay                                 and like they also correlate or a good                                 proxy for AP testing okay yeah okay and                                 also in the plain vanilla alternating                                 least squares the matrix factorization                                 there is a way to decay the time and yes                                 in the equation I don't know if you had                                 time to I mean that also handles time                                 decay the matrix factorization in the                                 beginning which you talked about yes so                                 that does get worse over time that's                                 certainly the case yeah yeah let's check                                 out that offline thank you I think I                                 missed something in the presentation how                                 exactly did you tackle the item called                                 start problem there because you talked                                 about it and then yes so the cold start                                 problem is certainly something that                                 alternatingly squares deals with                                 terribly because you don't have the                                 space allocated in your matrices to                                 store the information about these new                                 items for example and                                 whereas so it would the algorithm would                                 stumble if you passed it something it                                 hadn't seen before there are some                                 streaming implementations that are out                                 there and they can be used but it takes                                 a good while to get some good                                 recommendations out of it                                 I guess the advantage of the composable                                 techniques is because of the hashing                                 there's just no stumbling you don't have                                 to worry so much so you would have to                                 increase the size of the bit vectors to                                 acknowledge that a new item had been                                 added to the system but you can use the                                 same Manoj Singh nature that you're                                 already using and just update it with                                 this new hashed item without it causing                                 a problem could they read more about                                 this approach in a scientific paper yes                                 why not drop me an email and I'll send                                 you some citation okay thanks                                 any other questions yes please as a                                 question of the opinion otherwise it's                                 confusing okay actually I implemented                                 something like this for item item                                 recommendation not for a user but I'm                                 curious how do you handle the fact that                                 person never listened or never even seen                                 that there is a stream that you can see                                 they're speaking about goals and sees an                                 item yeah the question is if the person                                 ever found the item how do you deal with                                 it so let me just double check so a                                 person they just don't know that it                                 exists decides yeah exactly                                 so in your case if the person sees it vd                                 for more than two or three four times                                 you say okay this is great the guy likes                                 it yeah but if I never found that music                                 Wow how do you handle with it I'm seeing                                 myself for instance in Netflix I like                                 some some movies but maybe I would not                                 like some other movies but I simply                                 don't know that they exist okay so if if                                 if no one in the population knows that                                 they exist then it's certainly the case                                 they would never be recommended to you I                                 mean hopefully if somebody but similar                                 user behavior has okay so in terms of                                 when you do your factorization in                                 alternating least squares then in the                                 composable okay let's let's chat offline                                 and I'll make sure I know what's                                 happening I think this is a problem                                 about exploration and exploitation                                 because when a new item is in the system                                 you kind of want to force it to get the                                 information from it so you want to show                                 it to some users to get some grades and                                 maybe later put it in a different face                                 like I see so perhaps we could approach                                 it in the post processing situation as                                 well okay so suggest it to many users                                 okay thank you                                 the last one maybe well thanks so a                                 really great talk have you investigated                                 any ways or done any work around                                 potential ways to include context and                                 metadata into this scheme so at the                                 moment it's it's based purely on the                                 co-occurrence which is potentially one                                 of the downsides of pure ALS but there's                                 yeah you have a lot of rich context                                 around you know things like he you've                                 got a little bit of it there with time                                 and with with with devices but are there                                 other ways to include item metadata to a                                 user metadata that you're all about is                                 it just a case of having different mesh                                 structures you know group are keys or                                 are there ways to combine and we'll                                 build them into the signature you                                 thinking like that yes                                 so thanks Nick that's a great question                                 it's                                 absolutely not something that we have                                 implemented but it certainly seems like                                 it would be possible to investigate what                                 the benefits of keeping different                                 Menasha signatures for different genres                                 are for example and so on                                 yeah that would be that would be good to                                 see you sort of only thought about using                                 that extra metadata in terms of                                 post-processing really rather than                                 embedding it into different min hash                                 signatures so if anyone has any                                 experience in this let us know because                                 we'd be interested okay okay thank you                                 awesome thank you very much                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=9o2q88UUmTM


