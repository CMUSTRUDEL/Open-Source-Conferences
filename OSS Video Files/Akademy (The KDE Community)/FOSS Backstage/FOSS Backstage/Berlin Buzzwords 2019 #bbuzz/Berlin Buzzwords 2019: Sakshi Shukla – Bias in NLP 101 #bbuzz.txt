Title: Berlin Buzzwords 2019: Sakshi Shukla â€“ Bias in NLP 101 #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Bias in NLP 101 talks about the biases present in Texts based for Men and Women. For this, a dataset based on movies reviews from India is used and worked on by IBM. The intention is on having various situations where we can check if the model predicts a biased result as based on the stereotypical notions developed in the society for both men and women. 

Since movies replicate the reality in the most significant way their reviews being less in length would get biased. To de-bias the system, a model called DeCogTeller is used which results in the correct form of Gender. This project is a Talk Session on Machine Learning and Natural Language Processing and also brings out the challenges companies like Google, Amazon and Facebook faced while recruiting women. 

Read more:
https://2019.berlinbuzzwords.de/19/session/bias-nlp-101

About Sakshi Shukla:
https://2019.berlinbuzzwords.de/users/sakshi-shukla

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you everyone okay so I'll be                               starting with bias in NLP and I'll be                               covering a very generic part of the                               biases so generally whenever we address                               a bias we we try to in and we try to                               click on the disparity which is present                               in the data or in the input that we give                               so what exactly is a disparity we can                               somewhat relate it to a problem                                statement so there's so many problems                                like we see this bias everywhere in                                various NLP models like you see it in                                movies you see it over books you can see                                it on language translations so all these                                all these examples are which are like                                really close to our old a you know                                lifestyle we witness biases in natural                                language processing models now we will                                see how so generally if anyone here is                                working at Google please don't get                                offended but generally we think like                                Google Facebook Amazon these are like                                really good companies and we think that                                you know these companies are like                                extremely good and we want to be in them                                and we think that they are working very                                well and they don't have as much as                                error they have like error free                                environment but it's not actually truth                                we'll find out how so what happens is                                the Google ads are shown where they                                actually claim with the job scales for                                women and it has turned out that they                                show less paid jobs for the same                                position as what they show to a guy now                                how exactly the model understands that                                it's a guy or a good so obviously on                                prior information which it stores it                                labels through various features it                                understands okay it's either it's a boy                                or a girl and then like that it actually                                proves that you know this is a female                                category based persons so they will show                                them less paid jobs for the same                                position of what a guy would be getting                                and this has been proven similarly                                Facebook is also like that and so is                                with Amazon so there was actually a                                campaign there was rate that was raised                                because you know there was so many                                adds that was shown on Facebook with the                                similar problem that Google was also                                doing and AI model at Amazon was not                                recruiting women it was actually                                scraping out so if maybe let's say I had                                a profile which was exactly similar of                                what a guy was having so maybe the                                profile that was needed a guy would get                                selected and my profile would get                                rejected based on the gender and it's                                not because there's there's an intention                                behind it it's it's happening                                unintentionally and that is the bias                                which is behind it                                so we'll find how exactly you know these                                companies are now working to scrub that                                out so like I said it's not only with                                jobs there with our everyday life                                so it says she is in Polish and he is                                honest if I pronounced it wrongly then                                please pardon me CRI in NASA policia CIA                                sang nurse when I'm translating it it is                                taking up common CR which is he and                                sheets common for both in Filipino from                                English but when I retranslated it's                                actually what it's doing it's the first                                CR which was a she in the first                                statement has been taken as a he so you                                know generally whenever we think of                                corpse whenever you think of Pulis well                                we say policeman we never we usually                                never say policewoman I mean there can                                be a girl also right it's not necessary                                that all all police people are boys or                                our guys so these are the basic biases                                which are actually present in these                                models which we actually don't realize                                so it's something which is there and                                it's occupations are again decided by                                genders                                whenever you know whenever we say call                                the cops we never think that a cab would                                become a police cab would be coming with                                four or five women in it we generally                                think you know male police would be                                arriving I think that's you're in                                Germany as well right right yeah so                                exactly so even Google translation has                                been has not actually worked well it has                                 failed here by by not correcting the                                 right gender so obviously in books also                                 there's bias present                                 like if you see generally you say the                                 girl is gonna take care of the house and                                 the boy will go out and work the girl                                 would cook the boy would do something                                 else maybe eat or whatever so even these                                 basic examples and this is a book for a                                 toddler I mean it's not something that                                 is there for somebody who's like well                                 educated it's even in the beginning from                                 very beginning that's what we're                                 actually consuming and we're not even                                 realizing that it's actually not right                                 there's a bias even here so that's what                                 I said initially that you know even our                                 lifestyle is filled with so many biases                                 with Disney movies I think most of you                                 must have seen even though I see more                                 guys but I think that's ok we generally                                 see that you know maybe it's the white                                 or or or Cinderella in the end the                                 prince comes at the climax and he fixes                                 everything and the girl is suffering                                 maybe she's sisters are not helping or                                 or maybe you know the mother is not                                 helping her so you know it's why do we                                 need to introduce a male character to                                 sort of solve the problem why not we can                                 deal with the with the protagonist                                 itself even though all these feet all                                 these are female driven films all of                                 them have a protagonist who are women I                                 mean Snow White was a woman it was all                                 based on her life with the Indian you                                 know the the prince comes and you know                                 the apple that she eats comes out                                 because of the horse and everything so                                 you know those things are there that it                                 has to be introduced through a prince                                 considering this example now with I'm                                 not too sure if you guys know about it                                 but generally what happens in Bollywood                                 movies like I said in movies so this is                                 a very basic example whatever                                 characteristics we associate for a hero                                 is not generally how we associate for                                 the hero in horizon for the actress and                                 here I'll read out the dialogue Rohit is                                 an aspiring singer who works as a sales                                 man in a car showroom run by mr. Malik                                 one day he meets Sonya daughter of mr.                                 Saxena when he goes to deliver a car to                                 her home as her birthday present so what                                 exactly what exactly is is something                                 that you can                                 find out from here can anyone actually                                 try what is the disparity in this                                 dialogue will obviously discuss it in                                 the further slides but if anyone can                                 still catch the description says Rohit                                 is an aspiring singer and he's a                                 Salesman                                 and he's going to deliver a car and etc                                 and Sonia is daughter of mr. Saxena                                 sorry girls are pampered yeah it says a                                 sales man okay                                 so I'll explain you so here if you see                                 what what is there's an undervalue                                 statement that Sonia Saxena is just                                 daughter of mr. Saxena there's no                                 description given which is daughter of                                 mr. Saxena                                 whereas to both of them are leading                                 people one is the boy one is the girl                                 Rohit is the boy he has been given a                                 really long description that you know                                 he's a Salesman he wants to be a singer                                 he's going to deliver a car etc etc but                                 for the girl it's just that she's                                 daughter of somebody is that description                                 enough to actually make you realize that                                 what is the personality of this person I                                 mean it could have written it in the                                 other way also the true hit is you know                                 through it as a boy and she is maybe she                                 graduated or she's staying there and she                                 sing nothing has been described about                                 her only a simple line has been given                                 that she is or daughter often she's the                                 daughter of mr. Saxena                                 so these undervalued statements which we                                 generally ignore actually help us in                                 finding the bias and the entire                                 description that says is basically that                                 she is getting a present but the way it                                 has been picturised that we not intend                                 on looking on these critical points but                                 we actually focus on the outcomes which                                 is very basic and simple and it's too                                 direct yet we chose to not we still                                 chose to miss out these critical points                                 in the undervalued statements                                 so obviously if had I not been pointed                                 it I I don't think most of you must have                                 actually figured it so it's a difficult                                 to actually detect bias like that so how                                 should one actually try to do it what                                 are the myse so it's very simple to                                 detect bias you need to analyze the                                 basic stereotypes which we actually miss                                 out you need to you need to develop a                                 dataset in such a way that you're able                                 to you know you are able to pinpoint the                                 D bias and you're able to put that as an                                 input so that your training model works                                 accordingly and gives you the best                                 accuracy results so it's very important                                 to make data set which is D biased and                                 obviously then your model would be                                 developed using machine learning                                 algorithm okay so how do you analyze the                                 stereotypes the jhen the broadly three                                 characteristics through words right if I                                 say she's a pretty girl it's very                                 obvious if I just say oh there's                                 somebody she's looking me pretty like                                 that or maybe something it's it's very                                 obvious that you know it's for a girl                                 right whenever we use a guy has to be                                 rich a guy has to be successful we                                 generally don't associate these                                 adjectives these words with with women                                 so these are this is like a very fine                                 line and a base line approach you need                                 to work on the stereo to work on the                                 biasing you need to you need to extract                                 the stereotypes and change it you need                                 to make it unanimously exclusive for                                 both the categories that's one and words                                 are the most primary thing then the                                 second is the actions now generally what                                 we say is like okay she's a girl she                                 must be cooking is a boy must have gone                                 you know out to work so we are giving                                 these these actions these these                                 performance to a specific category                                 without realizing like if you remember                                 the the book that I showed you which was                                 basically for a toddler it had the same                                 example the girls can cook in the boys                                 eat or play was something like that so                                 we are actually not realizing that we                                 are giving these informations ourselves                                 only so we need to take care to to to                                 remove the bias we need to take care                                 that we actually work on these                                 stereotypes so first category will be                                 the words the adjective the second                                 category belongs to the actions that                                 we're labeling like you see if you see                                 here if it's visible to the people at                                 the back also it says that the girl                                 marries enjoyed it says the boy beats so                                 it's it's it's very like regressive but                                 there are some places where it actually                                 happens that you know girls are actually                                 the consider that you know we will just                                 put them inside the house and they'll be                                 working or the boys would go out and                                 work and they they can do not rule the                                 world and everything so we don't realize                                 but these actions are also there so we                                 need to work on that and finally we have                                 the occupation like the example I gave                                 in the translation one that she is a                                 police but it came out to be he's a                                 polis these action these occupations are                                 also play an important role so generally                                 we come across that you know girls                                 generally teachers secretaries we never                                 think of a girl as a sports person even                                 in this example I don't think so they                                 also have it yeah they don't have it                                 it's not even there                                 I mean it's so rare and it's so lesson                                 number otherwise we at least think of it                                 you know okay boys would become maybe                                 less maybe more but at least we have                                 that window open in our heads it's not                                 even open for women doctors extremely                                 less police officers extremely less                                 students we generally think okay they're                                 good you know some of them would be even                                 less interested would not be interested                                 or sometimes we force so that number                                 also decreases these occupations that we                                 you know label are also playing an                                 important part which we don't realize                                 that's one more thing now how do we                                 suffer like I said about the movies for                                 data set generally what what was seen                                 was there were                                     plus movies from                                                       only                                                                    there but for the guys there were                                     sorry                                                                 don't realize it's almost half the                                 number approximately so in general for                                 actors actresses for any occupation the                                 number goes half are the under then if                                 it's a teaching job or if it's a                                 secretary job jobs which are actually                                 labeled for women and these are the                                 things whenever we train a model we miss                                 out on these informations and that's                                 where biases are actually seen in in our                                 model so whenever we are making a model                                 in such a way that it has to be uniform                                 we need to take care of these points                                 because these are some points which are                                 not available you just pick out the                                 information maybe online or you create                                 or while you creating also we generally                                 overlook these and we create the data                                 set and we train a model and then you                                 know afterwards we realize that there                                 are some biases present so it's very                                 important to work with them like                                 considering this data set only it can be                                 found on with this link so you guys can                                 also see it for the same example that I                                 gave I'll be showing you some results                                 later on so if you see the pink one the                                 pink the red one is for girls and the                                 blue one is for boys so there are like                                 around as few Bollywood movies if you                                 see there has been like I think not more                                 than five movies would be you know where                                                                                                        maybe you don't you might not know the                                 context also but even if through the                                 imagery you can't find that okay this                                 much this much has been the                                 participation by a woman and this much                                 has been the participation by a boy so                                 even with these figures we understand                                 that we are actually taking them out                                 even though they are the protagonists so                                 we tend on losing this                                 and of information so if you see                                 basically if you see the percentage                                 it was extremely lesson in initial from                                                                                                       was a strong inclination upwards and                                 then it became again constant and there                                 was an uphill again so why because                                 generally this around this time there                                 were very few female centric movies then                                 few of them like biopics and extract                                 travel made which again is the story of                                 a women it's not basically a common film                                 or a common movie where you're not the                                 guy and the girl is there it's basically                                 it's only dependent on the women that's                                 whether the graph has increased so                                 that's one thing so it's the growth is                                 less than                                                           years now how exactly the bias can be                                 removed which we've seen so far which is                                 very important                                 so this decock teller has been invented                                 by IBM at India I'm not too sure if the                                 German IBM is aware about it but it has                                 been I'm not sure about it Vidya so                                 they've been working because it's                                 basically based on movie data set so I                                 don't think so they they would know                                 about it but what exactly what exactly                                 is your is that they're giving the facts                                 which is very common like how the basic                                 NLP model works you give the information                                 then you perform the word embedding                                 problem you generate the word embedding                                 and then you specify gender based like                                 how we were associating teachers and                                 secretaries okay female look at this is                                 like meal okay that is common can be                                 done by both of them something like that                                 and then what you do is after that when                                 you've given a biased data you extract                                 okay all the information the occupation                                 everything has been extracted then you                                 interchange it and you check the                                 plausibility of it so you are                                 interchange the infant                                 has been interchanging based on the                                 gender and you check how accurate is it                                 and that's how your bias can be removed                                 so generally what we do is we don't                                 follow this step we don't follow                                 anything from your we don't follow it                                 before although the first flowchart in                                 the second one but we don't go generally                                 for the third one and then unless we                                 work it out for both the genders how                                 would we know that there is a                                 possibility for you know females also to                                 be in that part otherwise we'll again                                 have that Google Translation problem                                 statement that was there we'll witness                                 more examples like that so that's one                                 and this is extremely important because                                 this is how that gender thing would be                                 removed if we change the gender you'll                                 understand okay so there is a                                 possibility that even a female can be a                                 police cop so that's very important now                                 how does this algorithm actually work                                 the decock teller you can search it                                 online also there's a paper I'll show                                 the reference also so you generally pre                                 process your data you generate the word                                 vectors after doing after performing                                 this you extract the information and                                 then when you classify what you try to                                 do it you try to do it in both the ways                                 you interchange the information and                                 check how well is it working and after                                 this is generally for the movie data                                 because that's what I've been focusing                                 more on and then you detect bias based                                 on the actions okay so if it was working                                 for both of them then it's fine if it                                 was not working then there's a bias then                                 you work on it and then after working on                                 it your bias has been removed so if you                                 see the gender predictions the accuracy                                 for the movie data set which was there                                 it was following the KN and machine                                 learning algorithm if you guys are aware                                 of it anybody has any doubt okay                                 yeah so the knn algorithm was working so                                 for k equal to one it was like there was                                 a there's a huge inclination angle but                                 for the green one it was initially                                 steady but in the end it it went up the                                 orange one where K was actually                                        orange one was constant it was not                                 constant but didn't it would it steadily                                 increased in the in the end there was a                                 a pill point so it the maximum accuracy                                 that has been achieved is when K is                                 equal to one so that's that's the                                 maximum accuracy you can get for your                                 bias removal for the movie data set and                                 this is for the training data so as as                                 as accurate you can work on your                                 training data it's far more because the                                 testing data the accuracy even decreases                                 so you need to really work on the                                 training data to remove the bias so this                                 is the D biasing system I'm following                                 the same example here as well                                 so Rohith is an aspiring singer who                                 works as a salesman in a car showroom                                 run by mr. Malik one day he needs Sonia                                 daughter of mr. Saxena                                 when he goes to deliver the car at a                                 place on her birthday now here if you                                 see how basically decock Taylor has been                                 working it's basically that he is that                                 Rohit he because it's a Salesman it has                                 already selected it as that he's a                                 singer he wants to deliver he has                                 aspires like he has desires so all of                                 that has been labeled to the blue if you                                 can see this tree they've been assigned                                 the blue label which is figuring that                                 you know all this is somewhat labeled                                 and characterized towards what a guy                                 would like or a guy would be the pink                                 one where it's indicating Sonia is                                 actually just giving that she is a girl                                 because she's a daughter so only that                                 much information has been received so                                 when you D buys this information now                                 what does happen                                 is Sonia sorry Rohit she it has not                                 taken it as she is the saleswoman now it                                 has changed the Jane changed the gender                                 like the work in terms of gender and it                                 says that Sonia is an aspiring sorry                                 Rohit is an aspiring singer who works as                                 a saleswoman so now that name remains                                 the same but the characteristic that was                                 all about the guy has been changed for a                                 girl so here the information has been                                 changed from sales man to a saleswoman                                 and you know that she has dreams and she                                 wants to aspire still remains of that of                                 a boy because that again is not changed                                 the only information that has been taken                                 out is that she is Roy as a saleswoman                                 so because of sales woman                                 it has taken it to be that Rohith is a                                 woman but here again the other                                 information that she's an aspiring                                 singer or whatever has not been                                 converted                                 so there's another bias here also and                                 similarly with Sonia that daughter off                                 has been swapped to son off that                                 information has changed Sonia anyway                                 didn't have much labels she just has it                                 you know she is a daughter which has not                                 been changed to a boy to son off but                                 you're like you see the deliverer and                                 the aspiring feature has not been                                 changed so that this is also again with                                 D biased ex there's another there's                                 still a bias present in that it's not                                 yet biased completely so this is the                                 reference you guys can I think the                                 present the Peabody would be shared I                                 believe yeah so you can go through these                                 papers and you can you know refer it                                 from here                                 it's basically research paper I can show                                 you also                                 oh it's not selecting the entire thing                                 there's a space                                 oh yeah                                 oh it's not taking that thing completely                                 okay I'm not too sure it's not working                                 sorry okay oh yeah                                 finally so okay yeah you can follow                                 these research papers also to know more                                 how to remove bias because it has its                                 this topic was basically on basically on                                 gender bias because we were figuring on                                 the basic level                                                          seen it's a really good paper you can                                 you know refer it and yeah there's one                                 more publication you can see that as                                 well wait I'll open that also                                 I think this one should work properly Oh                                 No                                 not sweet                                 where's this book                                 yeah so this is also really it's                                 basically from Ukraine and Russia you                                 can follow this link as well it's also                                 very handy on also for the data set it's                                 available here you can see that I've                                 mentioned that in the presentation as                                 well so if you want to reach out to me                                 you can that's my Twitter handle                                 LinkedIn you can mail me I have a                                 youtube channel also for various                                 technocrats to come on board and discuss                                 various problems that they've been                                 facing and how exactly they gained                                 recognition so I generally what I do is                                 I do a Q&A series where I call a lot of                                 great technocrats on my channel and I                                 ask them questions how exactly we went                                 there and what are the scholarship                                 opportunities how did you apply maybe                                 somebody's working at who or anywhere so                                 what are the resources to follow so if                                 you're interested you can also warden to                                 yourself as a speaker and yeah that's it                                 thank you we still have a bit of time                                 for questions so if you have any                                 questions about this amazing topic                                 please go ahead                                 I checked with Bangla and it is the same                                 so like Bengali language in English -                                 Bengali Bengali - - English and I                                 realize it is the same even when Bengali                                 is an almost general gender-neutral                                 language so yeah this is an issue now                                 so we generally don't change the                                 interpol arity of these statements and                                 it's like it's very common you try out                                 with other languages also and I'm sure                                 they're going to work like this one it's                                 going to fail I which was familiar for                                 people who are to understand because                                 then Hindi languages the the letters and                                 alphabets would be difficult to                                 interpret by them but Hindi and other                                 most other languages already have gender                                 in their sentences so I would probably                                 understand if machine learning                                 algorithms would take it but Bangla is                                 an almost gender-neutral language so I                                 did not expect that like you we                                 generally say that you know it's                                 specific for a man you know if you say                                 that I am going it's it's probable that                                 I am going would be set by a boy because                                 the verbs are like that but even in                                 Hindi when you like go for like more                                 complex sentences not just I am going                                 because when you train your model you                                 just don't you know input these                                 information you give a lot of                                 information so those informations are                                 also not freely bias-free yeah thank you                                 so you mentioned that the the model you                                 are using that is based you are trying                                 to remove the bias based on other text                                 right from news text in this example but                                 isn't that really dangerous because                                 actually all texts have these biases so                                 you're actually trying to remove biases                                 with bias text it's not that's what I                                 said it's not it has not completely                                 removed the bias if you saw that example                                 the deliver thing and this aspiring                                 thing was still in blue okay so it's not                                 completely removing it but it's giving                                 an intuition that it's possible with the                                 other person as well so it's actually                                 working very well obviously there are                                 few points which can still be worked on                                 like you said that because this was                                 basically on a particular data set                                 it has not been universally accepted so                                 I think I am still working on it but for                                 now this is just based on one product                                 but yeah it's not completely removing                                 and it's still working on that complete                                 pattern I think they will never work on                                 the complete thing because then again                                 it's not right for any an ml model you                                 need to introduce bias also at times so                                 that's where the blue thing was again                                 still there yeah anything else yeah                                 would you say that this this approach to                                 reduce bias is more applicable to to                                 certain problems because we've made the                                 experience led for a lot of models if                                 you reduce the bias in the in the data                                 the model gets worse at predicting                                 because the data you get in still will                                 be biased so if you predict the job of                                 film your mother is gonna get worse                                 where I would see this to be very useful                                 this thing's way it would be really                                 horrible if if this bias creeps in like                                 if you want to have a machine learning                                 algorithm that predicts something for                                 like hiring decisions or something so                                 would you say that certain problems are                                 more prone to this or where this would                                 be more useful thank you for the                                 question so if in the pre in the                                 beginning I showed that you know Google                                 and Facebook all of them had already                                 they had developed a model which was not                                 accepting or was not giving that much of                                 pay scale no we're not accepting them as                                 recruiters or anything if it were a girl                                 or if it was you know as a female                                 candidate so generally this bias is                                 mainly where you know there has been                                 applications where there's been a                                 rejection based on gender it's not                                 universally you know it's not                                 universally accepted for all the                                 applications it's it's restricted to                                 gender based approaches only for now but                                 like you said you                                 it can really work well you know maybe                                 with with cabs and all and we can                                 actually say you know if that's the area                                 which is not very safe for women then                                 you know give them priority maybe there                                 should be a service which should be you                                 know readily available or something like                                 that so these biases are introduced to                                 focus more on how to prevent the imagery                                 of of gender which is not giving that                                 much of coverage anything else anybody                                 has a question yes so this means all our                                 data sets must be clean this way more or                                 less have you an example which is the                                 biggest relevant data set which has been                                 already cleaned or is it just in the                                 future like I said you need to make a                                 data set because not data sets are very                                 raw they're not cleaned even if you're                                 creating your own you need to clean that                                 up so you need to work on that part                                 manually and then you need to obviously                                 you can apply some ml models for that                                 but it the cleaning part and all the                                 considerations has to be done manually                                 considering all the options available                                 yeah because there hasn't been a system                                 to develop the cleaning there has been a                                 system to detect after cleaning but the                                 development part is not yet considered                                 anybody else has a question I think                                 we're done so thank you so you can mail                                 me if there is still some question left                                 so thank you everyone                                 I actually flow down from India to give                                 this talk and it's been a great                                 experience at boiling buzzwords so thank                                 you so much                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=jOXGvLsnYC8


