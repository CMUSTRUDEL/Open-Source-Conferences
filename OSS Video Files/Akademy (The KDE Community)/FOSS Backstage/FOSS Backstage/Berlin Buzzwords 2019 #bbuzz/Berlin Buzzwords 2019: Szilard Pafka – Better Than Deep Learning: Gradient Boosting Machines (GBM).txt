Title: Berlin Buzzwords 2019: Szilard Pafka â€“ Better Than Deep Learning: Gradient Boosting Machines (GBM)
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	With all the hype about deep learning and "AI", it is not well publicized that for structured/tabular data widely encountered in business applications it is actually another machine learning algorithm, the gradient boosting machine (GBM) that most often achieves the highest accuracy in supervised learning tasks. 

In this talk we'll review some of the main GBM implementations available as R and Python packages such as xgboost, h2o, lightgbm etc, we'll discuss some of their main features and characteristics, and we'll see how tuning GBMs and creating ensembles of the best models can achieve the best prediction accuracy for many business problems.

Read more:
https://2019.berlinbuzzwords.de/19/session/better-deep-learning-gradient-boosting-machines-gbm

About Szilard Pafka:
https://2019.berlinbuzzwords.de/users/szilard-pafka

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you for having me how many of you                               have heard of                               learning so yeah                                  much everyone very good forget deep                               learning for now so I'm going I'm here                               to talk about something way better than                               deep learning so before that a few words                               about myself so                                                    doing physics and like most of my                                colleagues I switched to finance and                                then almost                                                        California to do what later and now it's                                called                                data science I also found it then                                organized the first the very first data                                meetup in Los Angeles and I also been                                teaching at two universities one in                                Europe and one in California and I think                                I have some unpopular opinions which I'm                                going to express some of them here so                                don't blame my employer they are really                                nice people and I'm really grateful they                                let me come here talk so how many of you                                have seen this slide from Andrea and                                this is from a couple of years ago so                                Andrea angee is one of the most famous                                person in machine learning deep learning                                ai world and he's been saying this for                                many many years that basically deep                                learning beats every other machine                                learning algorithm especially if you                                have enough data and of course deep                                learning                                combined with with reinforcement                                learning and other techniques had has                                had tremendous success in for example                                beating world champions in various games                                and also we helped that it will solve                                race soon driving so we don't have to                                drive and some people are hoping that AI                                is coming very soon whatever that means                                so I think this is very nice but but the                                truth is a little bit more nuanced so                                indeed deep learning has had a                                tremendous success in computer vision                                and also in somewhat in predicting                                sequences like text lsdm for example and                                also combined with reinforcement                                learning as I was talking about it has                                tremendous success in virtual                                environments like games where you can                                create as much data as you want but this                                is not really the case in businesses and                                also I'm kind of skeptical about its                                deep learning is really AI so I mean why                                some people have have been doing machine                                learning for twenty thirty years with                                practical business applications for                                example fraud detection crazy scoring                                marketing all these areas I've been                                doing machine learning in this for like                                                                                                   people doing this for like twenty                                twenty-five and even more years also                                machine learning has been successful in                                many many other domains like telcos term                                prediction insurance car manufacturing                                or any kind of manufacturing of fog                                detection and many many other domains so                                a couple of years ago I I tried in some                                domains to beat traditional machine                                learning algorithms with deep learning                                and I couldn't so in some domains simply                                other algorithms were better and a bit                                later here is tienkie chen who is author                                of the most popular of one of the most                                popular boosting radium boosting                                implementation has talked at my meetup                                in Los Angeles and basically he                                mentioned that more than half of the                                cattles are not win with deep learning                                but actually with gradient boosting                                machines so this is kind of my answer to                                end ranks like so if you have a problem                                in business we                                you have tabular structural data that                                comes from a relational database then                                it's much more likely that gradient                                boosting will perform will outperform                                neural nets or deep learning or call it                                AI if you really want and even cago CEO                                Anthony has been saying that a little                                bit later that basically in most of this                                kind of tabular data problem problems XG                                boost has been the the winning algorithm                                in cattles but for some of us doing                                machine learning for many many years                                this is not surprising so here are two                                papers from                                                       excellent and still not outdated so                                 these guys have looked at a lot of                                 algorithms on a lot of data sets and                                 have compared accuracy and the top                                 algorithms were random forests neural                                 nets boosting support vector machines so                                 it's not all neural nets and I actually                                 been using random forests and gradient                                 boosting around                                                          machine learning models in production                                 and I could obtain very very good                                 accuracy on various business problems                                 but algorithms not everything so you                                 need to clean your data you need to do                                 feature engineering and if you really                                 want very top accuracy then you need to                                 do many many models and you need to                                 unsampled them so this is how you win ok                                 go for example so you do all this work                                 and ultimately you have to do all the                                 work that's done that's been done many                                 many years ago it was called data mining                                 and now it's called data science so you                                 have to explore the data clean transform                                 the data do the modeling validate the                                 models and all these things and                                 ultimately I would say that if you step                                 back even further then it's not really                                 about the tools it's really about                                 solving business problems so                                 you have to understand the problem and                                 map it in the right way to machine                                 learning so um but if you really and                                 this talk is about the algorithm or the                                 algorithm so if you really want some                                 kind of key quick advice on what kind of                                 problems what kind of algorithms to use                                 taking all this into consideration then                                 here you go if you have tabular data try                                 first                                 gradient boosting or random forests if                                 you have very small data like a thousand                                 observations then almost everything                                 other than linear models some very                                 simple models with overfit if you have a                                 lot of data then GBM some random forest                                 will become computationally too much to                                 do so you are back again to some simple                                 linear models usually with stochastic                                 gradient descent because this is the                                 fastest way to to solve this and it                                 works on very large datasets and indeed                                 if you have image if you have text then                                 yeah by all means do use deep learning                                 so the planning is really the best for                                 that but a better answer would be it                                 depends and I think by now you have seen                                 that the title of my talk was completely                                 misguided it was just to make you come                                 here so gradient boosting is not better                                 than deep learning it's better at some                                 problems and deep learning is better at                                 some other problems for example with                                 tabular data typically gradient boosting                                 is better with images deep learning it's                                 way better than anything else so if you                                 have images or some other problems where                                 the planning shines then by all means                                 use deep learning so another thing would                                 be just try them all so there are like                                 four or five algorithms and like linear                                 models random for is gradient boosting                                 they are related the new Anette support                                 vector machines and maybe logistic                                 regressions so just try them all and see                                 whatever works better                                 your data spend also time tuning the                                 models do ensembles if you each accuracy                                 really matters then you might need to do                                 like                                                                   them and again feature engineering is                                 very important how you map the domain                                 knowledge of your business problem into                                 features into inputs that can be learned                                 by by whatever machine algorithm you are                                 using but ultimately I've been talking                                 mostly about how to get the most                                 accurate model but sometimes accuracy is                                 not like the most important thing so in                                 some cases you might be constrained to                                 do a model that you can explain either                                 because of regulatory pressure or some                                 something else so in that case maybe a                                 linear model is is better so it                                 basically it depends so let's talk about                                 gradient boosting though so gradient                                 boosting is basically the building block                                 is trees and trees you can think of it                                 as like a partitioning of the space                                 which splits by various variables at                                 very split so basically from the data we                                 learn this structure the tree all those                                 plates and also to split variables in                                 the split point so we learned this from                                 data and this is one tree and how do we                                 get gradient boosting from trees here is                                 an example of another tree from another                                 problem and before gradient boosting I                                 would say a few words about edibles so                                 this is this came in                                               builds trees sequentially and we will                                 average them and each tree is trying to                                 minimize the errors that the other trees                                 before made and the way we do this is                                 that those of separation that were                                 misclassified by the previous trees the                                 I've waited so that the next three                                 focuses more on those trees and this is                                 how we build tree after tree and then we                                 stop at some point and gradient boosting                                 is something similar we build                                 iteratively trees I don't expect you to                                 understand this it doesn't really you                                 don't really need to especially in the                                 first round you know in order to to go                                 and use gradient boosting but basically                                 it's it's a gradient descent algorithm                                 in the functional space of these trees                                 more importantly what if you want to use                                 gradient boosting what kind of packages                                 that are open source and are available                                 for you to use so I talked to people I                                 looked up cattle forums I looked up on                                 the internet which are the packages that                                 people are using and this a few years                                 ago when I did this this were the ones                                 that most people were using that were                                 open source and why open source because                                 it's free but not also because of that                                 because there are great communities                                 meetups conferences like this also there                                 is very good documentation nowadays so                                 there are tons of books on our Python if                                 you do data science you can ask                                 questions of Stack Overflow sometimes                                 you get better documentation then with                                 with very expensive paid products and                                 all of these packages are available from                                 our Brighton which most they design this                                 who are using open source are a kind of                                 using so a couple of years ago I started                                 this little benchmark because I couldn't                                 find any internet any information on the                                 Internet that was kind of comparing this                                 packages so I kind of put it on myself                                 but it's still kind of like a limited                                 benchmarks but if you go to this repo                                 you would see the route this kind of                                 scalability in graphs that I'm not going                                 to go in much detail now                                 what's more important is which are the                                 algorithms that are the best and I think                                 that based on this all this work the                                 best are X G boost h                                            something that came later by GBM that's                                 open source by Microsoft and all the                                 three packages they have or libraries                                 they they have an R package or AB item                                 package that you can download and start                                 using it in like one minute so why not                                 spark kind of interestingly there is a                                 big difference between various                                 implementations even the most people                                 implement the same algorithm from the                                 same book still there is like a hundred                                 X and even a                                                          between various implementation when it                                 comes to running time so you're not                                 gonna wait there                                                         better library so I'm gonna give you a                                 little bit later more numbers on the                                 spark but spark is just not really good                                 for gradient boosting but ok so you                                 would say that I have big data so how                                 can I do machine learning on all this my                                 Big Data so maybe you don't have big                                 data or actually there are surveys that                                 what kind of sizes people are using for                                 for analytics and for machine learning                                 the sizes are smaller and especially                                 because you might have bigger old data                                 but when you do machine learning you                                 don't do it on on clicks for example you                                 do it on users so you do some                                 aggregation of the raw data and some                                 pre-processing some refinement and by                                 the time you get to do machine learning                                 you have this model matrix so data might                                 matrix is usually called that it's much                                 smaller is basically the number of items                                 you are doing machine learning on let's                                 say your users and then times the number                                 of columns the number of features you                                 have                                 I don't know maybe if you                                            user                                                                  have billions of human users right and                                 RAM is plentiful and cheap so you can                                 get hundreds of gigabytes of RAM for                                 very cheap also you can just go to the                                 cloud this is one provider and then when                                 I started the benchmarks this was the                                 biggest Ram you could get on one machine                                 so this was                                                              you have                                                                of instance and if you have special                                 customer then they give you also like                                    terabyte of RAM in one single machine                                 and I also looked at how data set size                                 is increased from the surveys and it                                 seems like around                                                     while Ram on easy to do around                                    percent per year so Ram goo just way                                 faster so more and more data actually                                 fits on the RAM on one single server and                                 this was kind of controversial all these                                 things I was saying couple of years ago                                 but I think by now people have realized                                 so this is a survey on Twitter so                                 probably meaningless done about a year                                 ago so I asked what do you care you want                                 your machine learning to work on bigger                                 data or to be faster or you don't care                                 at all so most people want faster                                 machine learning and fast matters we                                 like fast because you need to when you                                 do machine learning you need to do cross                                 validation you might need to do hyper                                 parameter tuning which means run a lot                                 of models maybe hundred models you want                                 an samples again you might need to run                                 hundreds of models so if one model takes                                 a day you that's not very good so the                                 faster the better and                                 forget my old benchmark so that was a                                 little bit too broad so now I                                 concentrated lately on just the gbm's so                                 here's another repo comparing just the                                 best tools not spark and then I made                                 this very easily reproducible so                                 basically there is a docker fire for                                 this so you can just with this command                                 reproduce all my results on and you can                                 run it on your data if you want but this                                 is on some public data set on a million                                 records                                                               see that now late by GBM which is kind                                 of the new a Chinese tool is kind of the                                 fastest also GPUs is a big high because                                 of deep learning but actually it's been                                 successful so for other things like                                 sequel and tabular data and also for                                 gradient boosting so with gradient                                 boosting you don't get the speed-up you                                 get for newer networks so you don't get                                                                                                          and also these tools are kind of a newer                                 so                                                                      mature but I think XG boost by now is                                 the GPU implementation is pretty mature                                 so that's the fastest and unlike larger                                 data sets but still if it fits on the                                 GPU then it can even be the CPU version                                 but all this kind of depends a little                                 bit on your data and your CPU and GPU                                 hardware so these are like decent CPUs                                 and GPUs also it's interesting to look                                 at the memory requirement especially on                                 larger data sets so I opted here a                                 little bit                                                            see large EPM runs on                                                Ram so it's you don't need hundreds of                                 of gigabytes of RAM even for pretty                                 large datasets it's running though for                                 like five minutes so if you need to run                                 like hundreds or thousands of models                                 then you might you can parallelize                                 easily right also on GPUs the best XG                                 boost runs free fast on                                             records on this pretty good GPU and uses                                 only six gigabyte of GPU Ram although he                                 here the RAM of the server is like                                 hundreds of gigabytes one here the RAM                                 of the basically the GPU memories is                                 just sixteen gigabytes so it needs to                                 fit on on the GPU if you want to use                                 extra boost so light GBM and extra boost                                 are great they are the fastest they are                                 faster than a stool yet                                 I like h                                                                 it's very easy to deploy it and to                                 create like a real time that service                                 restful api so basically this is all you                                 need you explore the model you run those                                 things and you build a wharf I and then                                 you run your prediction web service                                 which is listening on a HTTP or HTTPS                                 port and basically you you can get                                 scores with HTTP or else weak requests                                 so that's all that's needed I have here                                 the full example including the training                                 of the model also if you need real-time                                 prediction scoring restful api to                                 integrate with other things then look at                                 h                                                                       it matters not only this kind of feature                                 engineering training                                 unique modeling part and in relating the                                 model but also the deployment and the                                 scoring and then what we do with the                                 scores we we use them in business                                 applications and we have monitoring and                                 all this so I have some spark numbers                                 though so people didn't like that spark                                 I was saying that spark is a hundred                                 times slower so I repeated this                                 experiments with the latest version a                                 few months ago and it's still like                                     times slower than light GPM so you can                                 see that even on larger data set this is                                 large ebn number this is spark run time                                 this is                                                            bigger data then it's this is not gonna                                 get much better anytime soon right                                 so this is like two orders of magnitude                                 and it didn't really improve so it's not                                 gonna get within                                                        it has horrible memory management so it                                 uses about                                                             you run it on the same server even with                                 hundreds of gigabytes of RAM it will                                 just crash it so this was on a server                                 with a terabyte of RAM and for for the                                                                                                    crashed and meanwhile I GBM uses like                                 five gigs of RAM                                 but spark people will tell me that I                                 have to run it on a cluster okay so                                 let's run it on a cluster since he's so                                 slow I'm gonna do only ten trees I'm not                                 gonna wait for hours or days on                                     million records it's still running even                                 with ten trees it's running for like                                 half an hour now it's using a little bit                                 less memory so it's not going to crash                                 on a terabyte if you have a cluster that                                 you have then you have even more RAM so                                 that's gonna be fine however if you do                                 'xg boost or light GBM on just one                                 server                                                                 times faster and it's gonna use just a                                 little bit of memory so it's way easier                                 to scale up this if you need then to                                 scale out an ellipse gradient boosting                                 machine implementation and if you have a                                 spark cluster you can actually beat it                                 with even like an old low-end laptop so                                 on one core                                                       seconds on the smaller data so that it                                 fits on the RAM of one small laptop yeah                                 I told you it's not gonna be popular so                                 what we want him doing machine learning                                 in the distributed setting is really                                 hard so don't blame the developers of                                 spark for this and unfortunately kind of                                 where we are now is that we have tools                                 that have much less features slow and                                 then they're actually also buggy so                                 spark is good for ETL maybes I wouldn't                                 say it's like the greatest thing but                                 it's it's okay for ETL and then if you                                 need Grady boosting or much                                 learning then spark integrates with h                                   and also with XG boosts we can use                                 either of these two things from spark I                                 didn't test this extensively but I                                 tested a little bit and they seemed okay                                 ish so I don't know if they implemented                                 all the features and if it's how stable                                 it is production environment but                                 ultimately what machine learning is is                                 not about big data is more about                                 computation a lot of computation so                                 people who know about internal structure                                 of CPUs in memory and memory hierarchy                                 those are gonna be able to write a much                                 more powerful gradient boosting and                                 neural network libraries then for                                 example something that's been developed                                 with big data in mind and ultimately now                                 we are going to the GPUs and GPUs have                                 proved also very useful for gradient                                 boosting so one thing is the data                                 scientist I want for my tools is also                                 like very high level API so that I don't                                 need to write a lot of code so if you                                 use any of those h                                                     then basically training that gradient                                 boosting machine is one of few lines of                                 code like you couldn't call this one                                 line of code basically so this is just                                 you transform it into some kind of data                                 structure and that's very efficient for                                 machine learning and then this is the                                 training you specify some parameters I'm                                 gonna say few words about that one slide                                 later and then here is you train and                                 then here is you predict on new data so                                 gradient boosting they have a lot of                                 parameters some of the most important                                 are the number of trees I explained what                                 basically we're training trees and we                                 are averaging them the depth of the tree                                 the max depth of the tree the learning                                 rate                                 this prevents somewhat overfitting this                                 is the way how we combine the trees and                                 then something called core they already                                 stopped and I'm gonna mention very soon                                 actually now so with gradient boosting                                 up the problem similar with new Ornette                                 if you train it too long then is gonna                                 over fit so you're gonna think that if                                 you look at your training set you would                                 get better and better accuracy but                                 actually on an independent holdout or                                 test set you would see that basically                                 your accuracy starts decreasing as                                 you're overfitting so you have to stop                                 here basically if you run it further                                 you're gonna and you take this all the                                 strees then you're gonna get lower                                 accuracy and you just wasted all this                                 computation time so stop early it's                                 faster you don't wait so much time                                 especially if you're doing it in the                                 cloud it's gonna cost you money as well                                 and it's gonna get better accuracy and                                 then doing all this stopping in all the                                 stop implementations is very easy so                                 it's just like a few parameters to set                                 up so look it up                                 tuning gradient boosting is not very                                 easy there are very good tutorials so                                 I'm just gonna point two of them so the                                 slides will be available so I'm on                                 Twitter I will post the slides so just                                 follow me on Twitter and you will see                                 where are the slides and you get all                                 these links a little bit more about                                 tuning you can do manual tuning research                                 or random search if you did grid search                                 up to now then read this paper                                 explaining why random search is almost                                 always better than grid search that's                                 all I'm gonna say now and I also have a                                 github in which I've been doing some                                 kind of extensive random parameter                                 search here are the parameters                                 and it gives you an idea of what kind of                                 depths of trees and and what kind of                                 learning rate is is best but you have to                                 experiment with that on your own problem                                 so you would think that if you have a                                 lot of course or a lot of CPUs then                                 these tools will be faster which it kind                                 of depends so here is XG boost on                                    million records so run on one core two                                 cores four cores eight cores and                                    cores on a CPU so you see it gets faster                                 so this is runtime but if you have                                 happen to have a server that has two CPU                                 sockets so on on in the cloud most                                 high-end servers they have multiple                                 sockets it means that you will                                 experience some kind of slowdown and the                                 slowdown is because between the two CPUs                                 the inter socket connection is slower so                                 if the CPU stores on data on this memory                                 bank then it would have to go through                                 the other CPU and it causes a slowdown                                 so XG boost is just not written in a way                                 that it's called Numa aware that will                                 deal with this thing and sometimes you                                 get outrages slow down for instance here                                 is like GBM on one core and here it is                                 on like on machine with                                                 would see that from here to here you get                                 like you get                                                            on all course then if you run it on one                                 single core here the same very                                 surprising so if you think that you just                                 show it to the biggest machine you can                                 find in the cloud and that's not good                                 it's gonna be slower so also if you have                                 hyper-threaded course then it's gonna                                 slow down on these things so what I kind                                 of recommend is basically                                 learn your CPU structure and Van run a                                 training session on just the physical                                 cores of one CPU for instance this this                                 is a server with two CPU socket and this                                 out the hyper threaded course                                 so forget about hyper threaded course                                 and those are the two sockets so you can                                 run like one training session here                                 another one here or you can run                                 different cross-validation folds or you                                 can run different hyper parameters or so                                 something that's independent and there                                 is no communication between these                                 processes and/or threads and those                                 threads because if there is then it's                                 gonna be slowed down just because these                                 tools are not written in a luma aware                                 way but even if you write you run it                                 only on physical cores so you don't get                                 linear scaling so you see here like on                                 one core to core                                                        done on                                                               times speed up I have to speed up a                                 little bit but here basically for                                 instance on XG boost on                                            records on one core if that's the unit                                 on                                                                  times faster and this is kind of the                                 best you can get and like on smaller                                 data is gonna be even less speed-up so                                 mind is kind of scaling things and if                                 you have to run like hundreds of models                                 at the time this is joint work with a                                 friend then we concluded that the best                                 way to parallelize and also to obtain                                 the most reboot if you need to run let's                                 say hundreds of models like in some use                                 cases is to just run one model per core                                 so that's                                 that's how you can train most Hmong more                                 models in the same unit on time on the                                 same hardware so basically here is a                                 comparison table kind of reiterating                                 what I was talking a little bit so if                                 speed is your what you care the most                                 then if you have a CPU then use Lai GBM                                 if you have a GPU use XE boost if you                                 care a lot about this kind of real-time                                 scoring or production then use h                                      this is kind of the same also I've been                                 asking on Twitter what things people are                                 using a lot of my followers using random                                 forests gbm's not surprisingly so does                                 the history there is some bias here also                                 interestingly XG boost is still the most                                 used it became very popular with kegels                                 and it seems like people just don't know                                 about the other libraries maybe but                                 SPARC not very much used and CPU versus                                 GPU so this was a year ago this is like                                 a month ago so basically more and more                                 people are using GPUs for Grady boosting                                 as well and this is I couldn't do this                                 on Twitter because it has only four you                                 can only ask a question with four                                 options so if we included some other                                 libraries then basically this is the                                 poor results again around the hundred                                 people so kind of people know that this                                 we are the top tools and way less people                                 are using the nd are the ones so by now                                 gbm's have been around this                                 implementation for like three four five                                 years and people have figure out what's                                 the best and also people using this to                                 win competitions for example this was a                                 couple of months ago and the story here                                 is the same do feature engineer                                 you slide GBM and XG boost and then                                 neural nets are not as good but use them                                 anyway in an ensemble and then blend all                                 this into an ensemble and basically this                                 is kind of the last slide so if you have                                 tabular data then look first at deviance                                 and not only deep learning and about a                                 lot of other repos I have so this will                                 be posted with the slides and I think                                 I'm just out of time anyway so maybe we                                 have time for a question or two thank                                 you                                 we still have a couple of minutes for                                 questions so please raise your hands                                 even ask something yes I have a question                                 because you compared the speed of doing                                 machine learning for example in AWS and                                 sparked and did you try also there are                                 algorithms provided by AWS they are also                                 always bragging that they're faster than                                 open source one and actually better                                 accuracy so did you try those I think                                 it's with sage maker or mmm-hmm no I I                                 was playing a little bit a couple of                                 weeks ago but I wouldn't say that I                                 really tried it so                                 if you look into tensorflow                                 and installing it on on Pisan you see a                                 lot of Fortran code from the seventies                                 of the last century popping up in the                                 background scipy numpy and synthesis do                                 you believe that this could be also                                 applied by the profession of numerical                                 algorithms and then you get better and                                 maybe complete different results so for                                 gradient boosting what takes most of the                                 time with these implementations and on                                 the data sizes I've been talking about                                 like millions or ten millions of Records                                 is basically doing each split on a                                 variable but instead of splitting on any                                 possible data points all these three                                 best algorithms they are first doing                                 some kind of histogram and then they are                                 trying all only this splits so like                                    plus percent of the time takes in                                 computing these histograms and basically                                 this is different from things that have                                 been done in Fortran in the                                             really big fan on on on not so shiny                                 tech what works and a lot of that is                                 written like Fortran code from the                                     but in this case is not the case because                                 random forests and gradient boosting                                 comes from the                                                         are are not as easy to parallelize as                                 the neural net that's why you don't get                                 the same speed up with GPUs but still                                 it's it kind of helps if you so it's                                 it's faster now and GPUs but also they                                 are improving on the implementation is                                 well but it's not going to get as much                                 of a speed factor as for the new Anette                                 awesome thank you very much all right                                 thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=qjuizRba3ZQ


