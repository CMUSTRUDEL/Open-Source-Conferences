Title: Berlin Buzzwords 2019: Robin Moffatt – From zero to hero with Apache Kafka's connect API #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Integrating Apache Kafka with other systems in a reliable and scalable way is often a key part of a streaming platform. Fortunately, Apache Kafka includes the Connect API that enables streaming integration both in and out of Kafka. Like any technology, understanding its architecture and deployment patterns is key to successful use, as is knowing where to go looking when things aren’t working.

This talk will discuss the key design concepts within Kafka Connect and the pros and cons of standalone vs distributed deployment modes. We’ll do a live demo of building pipelines with Kafka Connect for streaming data in from databases, and out to targets including Elasticsearch. With some gremlins along the way, we’ll go hands-on in methodically diagnosing and resolving common issues encountered with Kafka Connect. The talk will finish off by discussing more advanced topics including Single Message Transforms, and deployment of Kafka Connect in containers.

Read more:
https://2019.berlinbuzzwords.de/19/session/zero-hero-apache-kafkas-connect-api

About Robin Moffatt:
https://2019.berlinbuzzwords.de/users/robin-moffatt

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              oh good afternoon everyone Thanks                               to the dishonor it's a kind of mom and                               so if you fall asleep I shall be                               offended                               put it down to the heat and nothing else                               so I work at confluence that's                               confluence not confluence as always                               causes confusion confluent one of the                               companies who contribute to the open                                source Apache kafka projects and we also                                have confident platform when she's a                                distribution of Kafka so quick show of                                hands who's using Kafka today almost                                everyone who's not using Kafka today                                that's really kinda like the balance is                                now shifted and he's using Kafka Connect                                who kind of thinks they maybe should be                                and that's why they're here but they're                                not using it yet okay and who's here cuz                                it's like some I want hang out                                so Kafka Connect is part of Apache Kafka                                and it gives you a way to integrate                                other systems into Kafka and from Kafka                                to other systems so you can use Kafka                                connects to Poland data from a database                                from a message queue from a flat file                                from anywhere you want to and stream it                                into Kafka and you can use calc connect                                to stream data from Kafka down to                                somewhere else stream data from Kafka to                                elasticsearch to monger to influx to a                                database to wherever you want to put it                                and the thing about Kafka connectors it                                lets you build these end-to-end                                integrations without needing to write                                any code so we're all engineers or                                software developers or techies of some                                various form and we all love to write                                things and build things from scratch and                                as much fun as it is to reinvent the                                wheel each time and kind of build their                                own frameworks to do these things cough                                could connect solves this common problem                                that people have of I've got data here                                and I want to get it to there and then                                I've got data there and I want to also                                get it over there because CAF could                                connect just gives you a configuration                                file so as a as a data engineer as we                                all are nowadays you just set up a                                better JSON you say I want to connect to                                the system I want to pull in this                                information and stream it into this                                particular Kafka topic and it's kind of                                as easy as that                                or it's mostly as easy as that so that's                                all we've got another                                                   about it so people use Kafka Connect for                                different things people use it for just                                very simple fairly dumb pipelines                                we're going to offload some data from                                here and we're gonna go and put it over                                there I've got data in our transactional                                system over here we want to go in                                stick it in a bucket over there or                                because Kafka persists data they say                                well we're going to take the data from                                here we're going to put it over there                                but we're also going to put it over here                                because that's the great thing about                                Kafka being this distributed persisted                                commit log you can reuse your data you                                bring the data in once and they say well                                we'd like to put it over here for this                                use case we'd like to put it over there                                for that use case and we'll put it over                                here as well for something else and you                                can use Kafka connect to say the same                                data and put it in different places to                                suit your purpose you can also use Kafka                                connect to simply say well my                                applications generating data I'm using                                Kafka as a broker between my services                                but some of our data                                I want to write out somewhere so option                                one is we build that into our service we                                write ourselves a service that does all                                this kind of stuff and we say well we'll                                connect to the data store and we'll                                worry about if the networks down or how                                often do we retry and where do we store                                the credentials and we'll worry about                                that because that's a sensible thing to                                try and build into a service or we say                                well we're generating data we're writing                                its cathc or any way we want to get that                                data down somewhere else we'll decoupled                                that responsibility so our application                                our services our service thing which is                                 what it should be doing and then Kafka                                 connectors responsible for saying data                                 in this topic we'd like to go and also                                 put it over there we'd like to monitor                                 what's going on between our services                                 we'd like to put an audit off the data                                 and those topics copy that to                                 elasticsearch or stream those metrics to                                 in flux and so on but you can also use                                 Kafka connect as a way I've started to                                 migrate your architectures away from an                                 older way of doing things may be built                                 around calico a monolith or the database                                 sat underneath it and move it more                                 towards an event-driven way of doing                                 things without having to actually just                                 tear everything up and start all over                                 again so you can say well we've got our                                 existing application is writing to the                                 database the database which is this                                 lovely thing that we've loved and                                 cherished for so long and we can't get                                 rid of that we can't rewrite that                                 application just for the sake of wanting                                 to move to a different way of doing                                 things what we can do is we can take the                                 events out of the database stream those                                 into Kafka and use them to drive new                                 applications that we're building                                 so without impacting the existing                                 application we start to migrate to a                                 more flexible way of doing things by                                 taking those events out of the database                                 using transaction logs and change data                                 capture all that kind of good stuff so                                 we're not impacting the databases for a                                 low impact very low latency we can                                 outdrive new services new applications                                 with those events so there's different                                 reasons why people use Kafka Connect so                                 I want to show you carefully connected                                 action and I honestly really do want to                                 but whether I can or not is up to my                                 laptop so I've done this demo before and                                 it really did crash and burn I've set it                                 running now and we'll see if it's                                 actually going to behave so cross your                                 fingers so first off we've got Kafka                                 Connect and it looks like it's running                                 all this Kurds on gets up by the way if                                 you want to try it out for yourselves                                 and what I'm going to do is simply take                                 data from a database stream it into                                 Kafka and then take that data from Kafka                                 and stream it down to a couple of other                                 places and the points here are it's very                                 easy to do it's just a few JSON files to                                 say get the data from here take the data                                 from there and put it over there and                                 it's also streaming missus event-driven                                 this is not kind of I'm going to wait a                                 for a while and then kind of collect a                                 batch and then maybe tonight I'll send                                 it over there and then tomorrow do some                                 processing as stuff changes in the                                 database we're going to stream it into                                 Kafka and we're going to stream it over                                 to other places where we might want to                                 use it so it starts off with we're in my                                 sequel it's just a relational database                                 it could be any relational database and                                 we've got some data net information                                 about some orders that people have                                 placed who place the order what did they                                 order when do they place the order and                                 so on and I'm going to set a DAT                                 generator running so we set that running                                 and over in my sequel if I reek weary                                 yet you'll see                                                                                                 so there's new data arriving and what                                 I'm going to do there is I'm gonna set a                                 new script running you can see from our                                 cheat sheet what's going on you can also                                 use that cheat sheet to run this for                                 yourselves and we're going to run this                                 which is simply going to echo to the                                 screen the data as it changes every                                 couple of seconds we get a new row                                 coming in which you can see from this                                 create time stamp here now we're going                                 to get that data from the database and                                 stream it into Kafka unto that we're                                 going to use Kafka connect so Kafka                                 connects I'm going to talk about how it                                 works and what connectors are and                                 plugins and all this kind of stuff                                 afterwards but                                 I'm just going to show it you so to                                 start with we just configure it we're                                 going to post it a configuration on the                                 REST API here's the connector that we're                                 going to use it's a connector from a                                 project called Derby's IAM and it                                 connects to my sequel it uses the my                                 sequel bin log the transaction log to                                 get the events out and it also snapshots                                 what already in the database so you say                                 you connect to the data so over here                                 we're interested in this particular                                 table the orders table so I'm gonna copy                                 and paste that over here so that'll run                                 and if I go look at the CAF could                                 connect log you'll see it'll start                                 churning away and actually start firing                                 up that connector and you'll see it                                 doing snapshots and all kind of stuff so                                 if we go back over here if I say to CAF                                 could connect what is the status of that                                 connector it'll say I've got a source                                 it's using the bayesian connector it's                                 running and so are all of its tusks                                 which is really good thanks it means                                 we're getting data into Kafka                                 well we think we're getting data into                                 Kafka if we go and check in my sequel                                 okay here's my sequel here's the data                                 being generated let's put that on that                                 side of the screen on this side of the                                 screen let's have a look at actually                                 what's in Kafka so I'm going to use the                                 Kafka Avro console consumer to say                                 what's the data that's currently in that                                 topic and hopefully any moment now it                                 will say the data in that topic is                                 whatever is in the database there we go                                 so we've got data in my sequel I should                                 probably put it the other way around so                                 our data in my sequel being generated                                 being written to buy whatever                                 applications right into the database                                 streaming in real time into Kafka so                                 that's kind of useful because we've now                                 got Kafka a topic in Kafka which any                                 application can hook up to and say Oh                                 anytime there's a new order I would like                                 to know about it please and the                                 connector itself it says was it created                                 was it updated was it deleted captures                                 deletes as well deletes are also events                                 so you can integrate your database into                                 Kafka using Kafka connect but you can                                 also integrate Kafka into other places                                 and push the data out stream the data                                 out so one use here might be I've got a                                 service that wants to know how a new                                 order has been created our                                 the kafka consumer or the Kafka streams                                 API to read that topic directly but you                                 might also say anytime there's a new                                 order it and I want to analyze that I                                 want to visualize that I want to put it                                 somewhere else and use a tool                                 appropriate for that I want to go and                                 put it into elasticsearch I don't you                                 can put it into a graph database so                                 let's do that so we're going to create                                 ourselves a new connector and again all                                 we say is here's a bit of JSON the                                 connector we're going to use is the                                 elasticsearch link connector you have                                 sources and sinks which are kind of                                 obvious by the name send over to                                 elasticsearch anything that's in this                                 topic here's my elasticsearch host so we                                 copy that in and paste it and then we're                                 also going to stream it over to new or                                                                                                          sync connector and then bits of like                                 connect a specific configuration so                                 here's the cipher information on how to                                 handle a particular data itself so we                                 send those two connectors over to Kafka                                 Connect we say two Kafka Connect tell me                                 about your connectors tell me about                                 their status it says about three                                 connectors that are sync from the sorry                                 source for your database with                                         I've got two sinks and they're all                                 running which is splendid because what                                 it means is we can then gone have a look                                 at this data itself so let's head over                                 we can use the REST API but only use                                 Cabana cuz I like it and we can also use                                 the neo                                                                 which one manages to load up first with                                 the fans on my laptop going up full pelt                                 as well but what's happening is the date                                 is coming into the database divisions                                 reading the binary log of a transaction                                 log streaming into a Kafka topic Kafka                                 connects taking that data pushing it out                                 both to elasticsearch and to neo                                       the key thing here is you've got the                                 Kafka topic and the elastic search and                                 neo                                                          if one connector dies the other one just                                 carries on and that date has persisted                                 for as long as we've told it to so when                                 the connector recovers it will just                                 carry on processing from where it got to                                 we can have a look over here we've got                                 data streaming in in real time it's                                      indeed so the data's coming through live                                 from the day space into cargo being                                 pushed out to elasticsearch okay you see                                 that update there it's updating every                                 five seconds the same thing with neo                                 working on Avalon Kurtz who you bought                                 some cars just show me the first                                    people who bought some cars you got                                 these people here we can start to drill                                 in so here's that person where do they                                 live this is all that person hopefully                                 lives somewhere they live in London who                                 are the other people who live in London                                 and so on so it depends what is it you                                 want to do with the data as to where you                                 put it and this is the beautiful thing                                 about using CAF green your architectures                                 because you can actually say the data                                 goes through Kafka because that's a very                                 sensible place to store events because                                 they what drive our business and then I                                 want to do graph analysis great I'll                                 stream it to neo I also want to do                                 search I'll stream it to elastic I also                                 want to do something else I'll stick it                                 say in flux or snowflake or wherever I                                 want that data you don't have to say                                 well I'll use this kind of one box here                                 that kind of vaguely satisfies all of                                 them but none of them particularly well                                 so without demo done and actually                                 succeeding which is nice let's                                 understand a bit more about what Kafka                                 Connect actually is so Kafka Connect is                                 built on this idea as a modular system                                 as a modular framework it's part of                                 Apache Kafka Apache Kafka is a                                 distributor commit log s an event                                 streaming platform as a producer and                                 consumer API also has the connect API it                                 also has the streams API these are parts                                 of a patrick after if you're using                                 apache Kafka you already have Kafka                                 Connect since version                                                   zero dot ten so at its half it sits                                 between a system where you've got data                                 and Kafka or Kafka and a system where                                 you want to put data if you're                                 integrating with HDFS with s                                           system and you're writing your own                                 possibly you don't actually need to                                 probably you really shouldn't be maybe                                 you should but most of the time not most                                 time Kafka connects what you should be                                 using so it has an idea of connectors                                 connectors are the jar files they're                                 plugins that you can write yourself it's                                 just part of a Java API but the                                 beautiful thing about it is that                                 probably someone already has                                 someone's worked out how did I interact                                 with this database how do I interact                                 with that target place how do I interact                                 with it and they've included that                                 knowledge that source or target specific                                 information of connecting to it into a                                 jar so then you say okay Kafka kudex use                                 this particular plugin and now Kafka                                 Connect knows how to talk to X Y or Z so                                 it can pull the information in and it                                 will stream that through into the first                                 bit of Kafka Connect and all we had to                                 do to configure it to say well use this                                 particular connector connector class is                                 this and for a non programmer like me                                 I'm not really bit of Python but not                                 really I'm mostly like a data engineer                                 what we call nowadays connector dot                                 class sounds a bit scary but that's                                 pretty much as scary as it gets                                 it's just JSON and there's plenty of                                 examples out there so the connector                                 knows how to connect the source system                                 or the target system and it then passes                                 internally and this is all kind of like                                 just under the covers you don't actually                                 see this when you're running it it                                 passes a connect record it obstructs                                 away the idea of actually it's a JDBC                                 record or it says something from the bin                                 log or it's something from a CSV file it                                 obstructs it into I've got some data and                                 I've also got a schema and we'll see in                                 a moment how important schemas are so it                                 passes that internally as a connect                                 record and it passes it to the                                 converters so Kafka connect has                                 connectors Kafka connect has converters                                 and they all begin with C and it gets                                 very confusing but a converter is                                 responsible for saying here is this                                 abstracted idea of some data plus a                                 schema I don't go write that to Kafka in                                 a certain way because your messages in                                 Kafka they're just bytes Kafka doesn't                                 care what it is it's just bytes which is                                 really powerful but it also means that                                 as data engineers using this kind of                                 thing the onus is on us the                                 responsibility is on us to decide how                                 are we going to serialize that data and                                 there's good ways and there's less good                                 ways to serialize your data                                 so if you care about your data if you                                 care about your colleagues if you don't                                 hate your colleagues you'll hopefully                                 bear in mind that the schema that goes                                 with data is pretty important it's                                 really important my colleague Gwen                                 Shapiro has this great expression the                                 the schema is the API between your                                 services it's the contract between your                                 services and whether we're talking about                                 offloading data from a database to put                                 somewhere else for someone to use or                                 taking data from a message queue for                                 someone to write a service against the                                 schema is massively important but if you                                 write a chunk of CSV onto a file server                                 somewhere you're basically sticking two                                 fingers up whoever's using it saying                                 well you figure it out or you're saying                                 well anytime you want to use it you come                                 and ask me and we all know how well that                                 kind of coupling works out so by caring                                 about our schemas by saying we're going                                 to write our data in a fallout which                                 supports schemas then we're actually                                 making it easier to use the data we're                                 making it easier to keep things more                                 loosely coupled so you can use Alfre you                                 can use prototype off there they kind of                                 like the two main contenders here Avro                                 is built into kind of confident platform                                 and elements of Kafka Connect and it                                 kind of makes it easier to use there's a                                 community converter for protobuf and                                 this is the beautiful thing out it will                                 be in pluggable so it ships with an Avro                                 converter if you download come from                                 platform but you can also go and                                 download approach above converter but                                 when you implement one of these                                 pipelines you say I'm going to use this                                 converter I'm going to write my data in                                 Avro I'm going to write it in JSON I'm                                 feeling brave I'm going to write it in                                 CSV as off to you how you write it or if                                 you're consuming it you need to                                 understand from the person who wrote it                                 to that topic well how have you C                                 realized it is it JSON is it Avro is a                                 CSV I hate you it's kind of it's it's up                                 to the people of building these things                                 and obviously it makes an awful lot of                                 sense to standardize so to standard I am                                 biased but I would say standardize on                                 Avro it's very richly supported it works                                 very well at least you share your                                 schemas so for example if you are using                                 Avro then the schema itself gets stored                                 in a schema registry so you get your                                 data comes in which come from a database                                 it's come from a flat file it's come                                 from a message queue it's got payload                                 and it's got a schema                                 so you could say well we'll put the                                 whole thing onto a message and our every                                 single time I get a value I'll store the                                 value I'm the schemer I'll put on the                                 Kafka queue it could do but it's kind of                                 quite a big message Avro takes a much                                 more sensible approach it says here is                                 your payload and then the schemer we                                 will attach that the information about                                 that schemer a reference to that schemer                                 into their payload what they would                                 rights Kafka and I a nice little binary                                 form but the schema itself gets stored                                 up in the scheme registry so then when                                 we come to use that data whether it's                                 Kafka Connect whether its case equal                                 whether it's Kafka streams with us your                                 own Kafka consuming application                                 regardless it can deserialize that Avro                                 data it'll go up to the scheme registry                                 it'll say kind of a schema phrase for                                 this particular ID and then it can reads                                 it can deserialize that data and that                                 data now has a forlorn schema has anyone                                 heard of K sequel a few k sequel is part                                 confluent platform it lets you use                                 sequel streaming sequel over your data                                 and Kafka if you're using that for                                 example you can simply say do a sequel                                 query against this topic and you've got                                 all of your columns and your data types                                 defined if you don't you have to type                                 them all in manually it's that idea                                 schemas are so important to anywhere                                 where you're working with the data so                                 anyway enough about schemas when you're                                 building Kafka connect connector you                                 specify the converter each converter has                                 got its own parameters so if you're                                 using the Afro converter to say well I                                 need to tell it where to store the                                 schema the schema goes in the schema                                 registry URL if you're using JSON you                                 say do I want to use schemas within the                                 JSON and so on and so on so part of this                                 is about understanding how to actually                                 structure that configuration so you've                                 got the key and the value converter                                 because kafka messages are key value                                 pairs and if you want you can use                                 different see relation methods for the                                 key and for the value so here we're just                                 going to use Avro for both which is kind                                 of quite a sensible place from which to                                 start so your value converter the key                                 converter and they've got the parameters                                 for each of the two converters so we've                                 got connectors which specify how to get                                 the data in and out from the actual                                 source and target systems we've got                                 converters which are kind of generic and                                 we can plug in different ones                                 and then we got transforms so the                                 transforms are an optional part of it                                 but let you do transformations on the                                 data as it passes through so you could                                 say as this data comes in i would like                                 to drop a certain field as this data                                 comes in i would like to change the                                 topic name to be something different                                 excuse me as the data goes out I would                                 like to route it to a different index                                 name based on timestamp and so on and so                                 on so you can do light forms of                                 transformation on it it's not for                                 building aggregates it's not for doing                                 highly complex joins and all kind of                                 stuff like that that's what you do                                 something like Kafka streams or K sequel                                 for but doing these kind of light                                 transformation pieces are really useful                                 the configuration is not entirely                                 accessible but it does make sense so                                 here's an example of Trance of two                                 different transformations one of them                                 were going to add the date to the topic                                 one of them we just call label foo bar                                 because we want to make it nice and                                 difficult for people to understand                                 actually what's going on but the point                                 is when you create your transformations                                 you prefix it with transforms and then                                 you say here are the two different                                 transformations one of them is our date                                 topic one of them is label foo bar to                                 make the point these are just labels so                                 then when you actually configure them                                 transforms label dot configuration                                 information so this one here it's using                                 the time stamp router which takes two                                 different parameters this one here we're                                 going to drop a particular going to                                 rename a particular field so we're gonna                                 rename delivery address to shipping                                 address but it's got a light                                 modification of data as it passes                                 through it's kind of useful all of it is                                 extensible all of it you can go and                                 write your own all of its public driver                                 API is you can write your own connectors                                 you can write your own transformation as                                 you create in converters and people do                                 when it's brilliant to see you can also                                 go and take advantage of what everyone                                 else has written and download them so                                 confluent hub is one place to go and get                                 them a bunch of different converters                                 connectors transformations and so on so                                 a brief pause now another cuff and they                                 were carry on                                 excuse me so now deploying so we've                                 learned a bit about kind of what happens                                 underneath the covers just enough to                                 understand water all these configuration                                 items that we're actually setting rather                                 than just like here's something I found                                 on Stack Overflow and I can like tweak                                 it until it works it's useful to                                 understand what our converters                                 particularly converters are what trip                                 most people up with kafka connect and                                 they're fantastically powerful when you                                 understand what they're there for                                 they're a real pain if you don't quite                                 and you just kind of feeling until the                                 damn thing works so you've got your                                 configuration and it's working now you                                 need to know how to actually go and                                 deploy it                                 so kafka Connect is built around this                                 idea of tasks an each task sorry built                                 around connectors and each connector is                                 executed by a task so we've got a                                 connector that's taking data from a                                 topic it's streaming it down to s                                       works actually carried out by an s                                       we've got another connector it's reading                                 and data from a database using JDBC sync                                 it's also got a task or maybe two                                 because kafka Connect can paralyze the                                 work that it's performing depending on                                 the source or target system if you're                                 reading from a single flat file                                 paralyzing that probably isn't gonna                                 make much sense if you're ingesting data                                 from a database you've got ten different                                 tables paralyzing that makes a lot of                                 sense until the DBA phones up and shouts                                 at you but kalfa Connect can do                                 parallelism and that's going to come                                 down to the connector itself so the                                 person who wrote the connector will                                 understand does it make sense to                                 paralyze this kind of ingest or egress                                 so the tasks I want to carry that out                                 and the tasks themselves run within a                                 worker so the work is responsible for                                 actually giving the tasks a place to                                 live and run and the workers write the                                 offsets so kafka connect stores the                                 offsets and this is another of the many                                 reasons why you should use Kafka                                 collectors have been tempted to brew                                 your own because you might say oh well                                 I've got this data here I just need to                                 go to HFS I'll write a spark job on that                                 phone but then tomorrow someone says                                 well can you also write it to s                                      also to neo anyway well that's three                                 completely different technologies I've                                 hard coded all of this to work with this                                 one over here                                 so Kafka connects abstract all of that                                 it has technology specific connectors it                                 has generic things for converting it                                 also tracks where did it get to for each                                 individual task this one managed to                                 write all of the data to elasticsearch                                 this what it fell over because neo                                       something wrong and nod it broke or                                 something                                 so this connector here has only got to                                 this particular offset when we bring it                                 back up this connector knows that I'll                                 find I'll carry on from there so Kafka                                 Connect does all of these good things so                                 it tracks the offsets so you can run                                 carefully connect in two different modes                                 standalone and distributed and this                                 after converters is probably the second                                 thing which causes people the most                                 confusion not always the problems are                                 just about confusion so stand-alone road                                 is just a standalone worker it's just a                                 JVM process that sits there and it runs                                 whether you're running standalone or                                 distributed Kafka Connect does not run                                 on your brokers nothing runs on your                                 brokers except maybe zookeeper and even                                 then some people would argue but connect                                 run separately they can run on a laptop                                 it can run on kubernetes it can run on                                 wherever but it does not run on your                                 brokers so it's a JVM process you can                                 really stand alone but it's standalone                                 it's a single instance it writes all of                                 its offsets to a flat file when you shut                                 it down and bring it back up it will                                 read those offsets and they all carry on                                 doing that once you reach the capacity                                 of that JVM so you're running three                                 different tasks and there's like tons of                                 data coming through from the database                                 and tons a day to going back out you                                 kind of like hit saturation points you                                 can't scale it well you kind of can you                                 could say well we'll just partition it                                 will now run to one of them is going to                                 run the s                                                               run the JDBC work and that's fine until                                 you saturate a JDBC one and then you've                                 got nowhere to go                                 also it's not fault tolerance if that                                 goes bang you're hosed you have to bring                                 it back up nothing happens until you                                 bring it back up so the other way of                                 running kalfa correct is called                                 distributed and it's not as scary as it                                 sounds if you're new to distributed                                 systems if you're new to Kafka                                 distributed worker sounds like oh my god                                 I'll go for the standalone that sounds                                 much much easier but distributed is                                 actually                                 my opinion a much better place to start                                 and this is why you can run distributed                                 on a single node doesn't have to be                                 distributed you can run it on a single                                 node but when you run Africa headaches                                 and distributed mode it stores all of                                 its configuration all of the offsets all                                 of that kind of stuff in Kafka because                                 Kafka persists data                                 Kafka is its permanent store of data so                                 Kafka connectors using CAF good store of                                 that good information which means that                                 if you then want to scale it out you                                 bring up a new worker and that worker                                 says ah I'm part of that same group and                                 it has all of its information its                                 offsets this configuration and so on                                 held centrally in Kafka and Kafka is                                 disputed and fault tolerance etc etc so                                 going from a single node are just like                                 mucking around like this is all good                                 whatever - oh we need to scale out is a                                 case of like well we'll bring up a new                                 worker with the same group ID so all of                                 the learning of where do I find my log                                 files where is it storing the stuff how                                 do I configure it what's the rest API                                 and so on you do all of that once                                 whereas if you go from standalone and                                 standalone you can figure with a flat                                 file over here or something and you go                                 from standalone to like oh crap we need                                 more capacity or we need fault tolerance                                 now you need to relearn and kind of redo                                 stuff so there are reasons why people do                                 use standalone maybe you need kinda like                                 locality specific stuff like you reading                                 from a particular local file which                                 wouldn't make sense to run like anywhere                                 there are reasons for standalone but                                 generally I say use distribution less                                 you've got a reason not to so dis you et                                 gives you an easy way to scale                                 distributed is also fault tolerant so                                 Kafka Connect is like the runtime for                                 these tasks Kafka Connect will say well                                 oh no we lost a worker but we need to                                 make sure that tasks keeps running so to                                 say well I'll bring it back over here                                 and obviously if you don't have the                                 capacity it's going to keep on trying to                                 run it but at least that we running a                                 little bit maybe just kind of like a                                 bits degraded but at least it's running                                 you can also partition your distributed                                 clusters so you can say well I want to                                 isolate these things entirely or maybe                                 I've just got different teams run Kafka                                 connects we don't have to have one great                                 big hairy cluster of Kafka connects you                                 can have one on this team one on that                                 team three on that team however you want                                 to deploy it it's up to you                                 but the key thing is kafka Connect and                                 distribution knows gives you that                                 scalability and fault-tolerance so I                                 made kapha Connect sound a bit scary by                                 saying people have problems with air                                 some people have problems with that so                                 I've gone through enough thoughts what                                 are the things that people actually have                                 problems with and what's the best way to                                 learn to troubleshoot it because kafka                                 Connect is brilliant Khafre Connect is                                 super powerful but it's got a few of                                 these little speed bumps on the way that                                 maybe trip people up and they kind of                                 give up on it before actually giving it                                 a fair chance so if a troubleshooting                                 cough could connect there's a few                                 concepts that I wanna share with you so                                 as we talked about connectors themselves                                 run tasks one or many tasks and you can                                 use the REST API of Kafka connect to say                                 well is it running or not which is kind                                 of a fair question to ask so you say is                                 the connector running it says yes the                                 connectors running you think brilliant                                 but where the hell is my data and kapha                                 Connect says well the connect is running                                 but actually the task isn't you see and                                 it's just one of these funny semantic                                 things so under the covers each                                 connector is executed by one or more                                 tasks if all of those tasks are failed                                 the connector can be running but you                                 ain't get no data so you have to go and                                 check both assuming it says the task has                                 failed and you say well I don't know why                                 it's failed you can use the REST API to                                 actually get a stack trace why did it                                 fail and you can kind of read it off                                 there and it's got all the kind of line                                 breaks encoded within it which is a bit                                 hairy at some point you'll eventually                                 want to go and look at the worker                                 log so as any good troubleshooter knows                                 the logs Jenny where you need to go and                                 find out what's happening it's not                                 enough just say it broke where's my data                                 go and have a look at the worker log the                                 Kafka connect worker log is where all of                                 the stuff gets written to so depending                                 on how you start Kafka connects depends                                 on where you'll find this if you use in                                 conference a li it's confident log                                 compose Kath whatever it's written to                                 stand it out and you can send it to                                 different places so the first thing is                                 you search through it and you search for                                 this the first instance from the bottom                                 of error and it'll say this task is                                 being killed and will not recover and so                                 it manually restarted but which P point                                 people say have found the error                                 what's this mean why is it broken and a                                 bit disappointed when we say well don't                                 know that's just the symptom but                                 just says Kafka Connect saying it broke                                 doesn't tell you why so you then go back                                 up the log and it search for the                                 previous error at which point you                                 actually get the output from the task                                 and the task says here's why I failed                                 here's the stack of trees so you find                                 the error you find out why the task says                                 it broke some of the common errors that                                 we get this one probably the most common                                 one anyone seen this before yeah well                                 not but no too embarrassed to show you                                 and so unknown magic by it sounds fun                                 but it's not unknown magic by it means                                 that you've tried to deserialize some                                 data using the Alpha roadie serializer                                 but it's not Avro because you know I                                 said that with alpha row it stores the                                 schemer it stores information at that                                 scheme like well indicator to the                                 schemer in the scheme registry so this                                 magic byte which is the fun little thing                                 that magic byte has information about                                 what's the schemer ID and if you try and                                 DC rise message which isn't Avro there                                 will be no little magic byte they'll be                                 like a curly brace or something like                                 that says JSON so if you're trying to                                 read JSON data and you've said the                                 converter is out fro Kafka Canet will                                 try and convert it from Avro but it's                                 JSON so just use the correct converter                                 that's a quite an easy one but then you                                 say well it is Avro I promise it's Avro                                 look here's my settings for the thing                                 that's writing to it and it could well                                 be Avro but either from you playing                                 around when you're setting your boss and                                 when I was just playing silly buggers                                 with you someone's writing JSON data to                                 talk to the topic or maybe they started                                 off writing JSON data and then they                                 wrote Avro but either way you got kinda                                 like mixed messages on the topic so                                 Kafka Connect will start from the                                 beginning the topic if it hits a non                                 Avro one you're gonna get the same error                                 but the cool thing with Kafka Connect is                                 it can handle errors it's got dead                                 letter cues so in some instances it can                                 actually say if I fail to deserialize a                                 message either I can kind I just fall                                 over and cry or I can go write it                                 somewhere else and just carry on and                                 it's up to you when you're building that                                 pipeline say well what makes sense if I                                 hit a bad message is that possible is                                 that plausible I'll just carry on or                                 actually stop the world there should                                 never be any bad messages we need to                                 troubleshoot that and not continue                                 default behavior is fail fast fall over                                 throw toys out the pram complain you                                 could also say well the opposite screw                                 it we'll just ignore it if we can't                                 handle it                                 drop it which is kind of brave you can                                 also say well be tolerant don't fall                                 over but if you hit a message that you                                 can't deserialize you can't handle right                                 it to a dead letter q which is a                                 separate topic and that means you can                                 actually process it again so in that                                 example we got mixed Avro and JSON you                                 can say well if you can't deserialize it                                 as Avro write it to another topic read                                 that topic with a JSON converter and                                 maybe you'll have better luck                                 another common problem no fields found                                 using key in value schemas of JSON with                                 C games enable requires schema and                                 payload fails what the hell do these                                 mean it's all about the schema your                                 mother the schema that I was getting                                 quite passionate about before schemas                                 matter and when we're writing data down                                 to a target system that target system                                 may or may not be kind of wanting a                                 schema if we're talking days for                                 elasticsearch we can say well can i                                 we'll just let it guess and quite often                                 that's fine if we're writing data to a                                 relational database the relational                                 database is probably to say well hey I'm                                 a relational database I kind of schemers                                 or what make the world go round so if                                 you try write to a relational database                                 or other systems without a schema you're                                 going to get an error so if you're using                                 JSON sorry if you using Avro data you're                                 in luck because you've got a schema                                 that's the whole point of it so you've                                 got a schema which comes from the scheme                                 registry you got each individual message                                 each individual message picks up its                                 schema and life is good but if you've                                 got JSON data that looks like that so                                 you've just got a better JSON a lump of                                 JSON let's check out a database you                                 might eyeball that and say well it's got                                 a schema it's got like an author ID                                 which looks like a number and a make                                 that looks like a string that's got a                                 schemer                                 but you've not actually declared the                                 schema it doesn't have a schema that's                                 why it's saying I don't have a schema                                 and you could argue that maybe the                                 connector itself could infer it or                                 whatever whatever but it doesn't that's                                 just how it works so one option is you                                 say we're going to use JSON but we're                                 gonna embed the schema which case you                                 must                                 choose that format of it the other                                 option is to use case equal to take your                                 sauce JSON which kind of you can look at                                 it and guess what the schemer is to take                                 that and actually apply a schema Turris                                 and re serialize it this is a really                                 cool thing that you can do with case                                 equal every single message that applies                                 on a topic                                 apply a schema re serialize it someone's                                 mucking around with you they're saying                                 here's a topic it got CSV on hahaha you                                 say well I'll take that data that's in                                 CSV I'll apply the schema and I'll                                 resize it to Avro and now everyone else                                  can use it in a friendly way so the same                                  thing you can use with Kafka correct                                  finally containers so you can run Kafka                                  connect                                  wherever you want to you can run kapha                                  connecting containers I run it on docker                                  it's a lot easier to use like with                                  docker compose and so on there are two                                  different connects images that confluent                                  publish this calf could connect base                                  which is just the base Kafka Connect and                                  then this Kafka Connect which is clue to                                  the elasticsearch connector the HDFS                                  JDBC in a couple of others but when                                  you're bringing other connectors in                                  other plugins in you can get them from                                  confluent hub you can kind of like                                  download it yourself and build it                                  yourself but you can get it from                                  confluent hub pre-built and you need to                                  get that jar into your container you                                  could do it at runtime so you can simply                                  say when I run I'm going to first run                                  confluent hub install and pull down the                                  particular connector and then spin up                                  Kafka codex and run that afterwards or                                  you can build a new image whichever way                                  you want to do it it's up to you depends                                  away on a store your data and so on and                                  you can also automate creating the                                  connectors so you can say well his                                  docker compose I'm gonna install the                                  particular connector I'm in a way if                                  Kafka connected me up and then I'm going                                  to send it the particular configuration                                  the JDBC configuration which so means                                  you can have just docker compose up and                                  it kind of like downloads it runs it                                  configures the connector and if things                                  start running so that is all I have I                                  have two minutes to spare for a few                                  questions                                  and that's my Twitter                                  so where's my toe I'm off is my Twitter                                  if you would like to tweet me tell me if                                  you liked it so if you didn't the slides                                  are on there I will tweet them                                  afterwards as well but Oh before I do                                  questions and there are drinks                                  downstairs                                  I believe confluence like currently                                  sponsoring so I'll hang around I'll do a                                  couple questions now about me downstairs                                  as well if any wants to chat thank you                                  [Applause]                                  how'd I do a quick speaker selfie as                                  well other questions                                  there's one in the center let me come                                  over Thanks yeah would you use tafakkur                                  connector backup and restore compacted                                  topics or could you shoot you back a                                  freestyle kafka cut slovak conducted you                                  can certainly write them oh sorry read                                  them I don't see why not                                  so we can't cuff the Kinect can't create                                  at our target topic but you probably                                  want to pre trace it with appropriate                                  configuration but I don't see why that                                  wouldn't work no okay thank you any more                                  Christians                                  it's a free-to-use is it I see that it's                                  commute confident community license yes                                  and not a party to bundle how is it                                  so Kafka connect is part of a Patrick                                  Africa which is Apache                                                   different connectors have varying                                  licenses so for example the the JDBC                                  connector the elastic search connector                                  HDFS connector part of confident                                  platform I believe they're confident                                  community license but I would need to                                  double-check I can't promise other                                  connectors what may be proprietary                                  others may be Apache to do a kind of it                                  depends on who wrote them thank you and                                  everything so our developer gives us                                  produce Jason without other also where I                                  saw you present is creating a flow                                  schema by K scale so it's any or right                                  is that how you code like how you relate                                  correlated state the cascade connect                                  because you use SQL to create a grow but                                  it's like isolated between calculator                                  it's somehow related to let me                                  paraphrase you have understood correctly                                  so you your interesting this idea of                                  resale realizing data using K sequel and                                  how does it relate Africa correct yeah                                  so k sequel is part confident platform                                  its runs also stomach excuses Kafka                                  streams under the covers but it's just a                                  components that's reading data from a                                  calcio topic and writing data to a Kafka                                  topic just as any other application it's                                  just that it's using the Avro                                  serialize and deserialize Avro                                  serializer to write data that it's read                                  as JSON                                  does that answer your question yeah okay                                  great okay okay                                  I make it six o'clock so going again if                                  you want to hang around here I'll be                                  downstairs as well alright let's do it                                  this way let's take the rest of the                                  questions offline                                  there's reps of all our session for                                  today we hope you enjoyed the buzzwords                                  well let's think we Robin again for for                                  the talk and QA                                  and then let's meet downstairs for the                                  get-together in the paddy is what the                                  hell
YouTube URL: https://www.youtube.com/watch?v=oNK3lB8Z-ZA


