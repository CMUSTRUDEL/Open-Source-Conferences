Title: Berlin Buzzwords 2019: Umesh Dangat – Evolution of Yelp search to a ranking platform #bbuzz
Publication date: 2019-06-27
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Yelp’s core business search was one of the oldest systems at Yelp designed well before the advent of ElasticSearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.

This talk will detail:
1) How Yelp’s search engineers decoupled the search infrastructure from search relevance.
2) The challenges associated with transferring complex custom Apache Lucene based ranking and text analysis functionality to ElasticSearch.
3) The benefits of offloading search infrastructure to ElasticSearch and
4) Finally the dividends it paid by allowing us to further leverage our technological investment in ElasticSearch, by hosting machine learning models in ElasticSearch by using Learning to Rank plugin, and making contributions to it.

Read more:
https://2019.berlinbuzzwords.de/19/session/evolution-yelp-search-ranking-platform

About Umesh Dangat:
https://2019.berlinbuzzwords.de/users/umesh-dangat

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello all thank you for coming to the                               talk as                               introduced my name is Uma Shankar I'm a                               search engineer at Yelp and today I'm                               going to talk about our migration from a                               custom distributed leucine based search                               infrastructure to a one based on elastic                               search before we begin a quick word on                               yelps mission it is connecting people                                with great local businesses here are                                some of the numbers I'm allowed to share                                because these were out in our last                                latest earnings call the important                                things here are as of n of Q                                          have                                                                 are searchable the monthly average of                                unique visitors who was at Yelp in the                                quarter was                                                          website and the app and this translates                                to billions of queries served per year                                by our search all right so this is a                                screenshot of our dub-dub-dub website                                page but the app looks similar so far                                when I say Yelp search or Yelp course                                search I am essentially talking about                                this page here if you see there's a box                                of free text which is used by our                                customers to type in let's say food or                                delivery look up for businesses by name                                like McDonald's maybe and more recently                                also things like services plumbers or                                you want a quote from a plumber or to                                fix something else and there's a                                location component as well which is                                other box here this is going to be                                important of in the talk later as well                                cool so how did our search architecture                                look like until recently one thing to                                keep in mind is Yelp was founded in                                     and I think such codebase was pretty                                much one of the early code bases at Yale                                because those functionalities search                                functionality has existed ever since                                Yelp as existed so these decisions were                                made in like                                                          obviously have elasticsearch or we                                probably didn't have solar either so the                                developers back then resulta used lucy                                and of course we ran to the problem of                                scale so we decided to distribute to                                scene and manage an auto say this                                cluster ourselves                                so here I'm showing the belief search                                nodes what I mean by a leaf search node                                is each node essentially serves one                                Lucene index and very simply put it's a                                master of follower architecture backed                                by arrest so then next in the quest goes                                to a master which takes the rights which                                is written to Lucene index periodically                                which is typically every three to four                                hours we would snapshot this index to s                                                                                                followers which were a pool of followers                                as I will talk about it later they would                                be also restarted periodically pull                                these for leucine indexes from s                                     serve the queries alright and as I said                                before this works only on one node per                                GB M so obviously ardena does not fit in                                one node so what do we do                                of course we shard so this brings                                certain complexities now so like I                                mentioned before our business model the                                search is really naturally sharded on                                geography because you're looking for                                residence near you or in a particular                                city or you looking for a plumber near                                your house or whatever so we would short                                them geographically and then now we need                                to put a coordinator service in front of                                this the task of this is kind of similar                                to the elasticsearch coordinator these                                days where it gets a request it routes                                to the correct indexing node or the                                search node and the rest is the same the                                one detail that is missing here which is                                also taken care of by the coordinator is                                kind of the scatter gather what that                                means is if you see the shard Lucene                                follower for example that is further                                sharded what we called micro shots so                                that allows us some more parallelism so                                the coordinator would send a broadcast                                to like scatter request to all the micro                                shots get back the result from all the                                micro sorts and then do a resort or                                merge sort kind of so as you can see                                this is quickly getting complicated                                 because we could runs potentially most                                 of our ranking in the you know seen                                 followers but then if the results are                                 really tiny the relevant stream could                                 also run some of the ranking or the more                                 expensive one in the coordinator service                                 and they talk about it later how this is                                 not the best thing to do so now that we                                 know a bit about the system what are the                                 issues with the system like I said once                                 initially when we began it was maybe a                                 handful of engineers and the goal was                                 let's get search up and running but with                                 time the team grew we had specialized                                 roles and this became a department of                                 probably                                                                 specific rules for specific people who                                 were motivated to do specific tasks for                                 example the relevant experts their                                 motivation or primary goal was let's                                 make the results look better from a                                 quality perspective and the                                 infrastructure team on the other hand                                 had an orthogonal kind of a primary goal                                 which was availability of the system and                                 performance unreliability                                 so what happens is when your code base                                 is not designed from the start or the                                 get-go to handle these two cross-cutting                                 concerns separately the code is                                 entangled so you try to make change in                                 the relevance it's really hard to not                                 break something or really slow down                                 maybe deleting saifa queries and vice                                 versa if you try to port out let's say                                 some of the shouting logically shard                                 it's really hard to not break some of                                 the relevancy thereby causing a                                 regression yeah a third of this the                                 iteration speed is really slow because                                 for me for example I work on a search                                 infrastructure so for me it's really                                 hard to understand why we are running                                 certain kinds of scores or anchors with                                 these models and it's hard to just                                 isolate my changes from this codebase it                                 was an operational burden of course                                 because                                                              compared to                                                              replicate this the size of data had                                 grown tremendously so our typical fix                                 was ok let's throw more instances let's                                 shout again let's add more replicas for                                 parallelization and or also add                                 microcharts what this means is our                                 cluster gets bigger the code pushes gets                                 lower note how I said that our                                 rankers also ran and those scenes so                                 ever this is a team of multiple number                                 of engineers now trying to push code                                 multiple times a week sometimes multiple                                 times a day we have to do a rolling                                 restart of the entire cluster and the                                 bigger the cluster the by nature of it                                 it's going to be slower                                 and there was more fun besides of just                                 slow code pushes we also loved the heap                                 so we put a lot of stuff in the heap and                                 the first heap such weight I mean it's                                 the same story I guess everywhere                                        cakes                                          hello sub the wall DC's so yeah I'll                                 talk about that too and what primarily                                 was happening but at least the                                 infrastructure team was he was spending                                 more time maintaining the system rather                                 than writing new features another thing                                 that happened was it was really getting                                 hard to add new features so for example                                 business wanted to add things like                                 delivery you want to find out all the                                 restaurants that deliver in the next                                    minutes or you want to book a table at a                                 restaurant for some time without some                                 sort of real-time indexing this is                                 really hard to get one of the other big                                 issues was analyzers could not be simply                                 iterated upon because that meant a                                 complete back full complete back film                                 and some we had to go take the time spin                                 up a battle cluster do all the indexing                                 from source and it's just too much of a                                 chore to do cool so now that we are                                 convinced that we needed a newer system                                 what are the requirements of the system                                 well the one requirement handed down to                                 us from management was since this is                                 mostly an infrastructure project we need                                 the mandate was we cannot cause                                 regressions in the relevance or the                                 quality maybe some edge cases are fine                                 but we really cannot say ok you can't                                 rank the results the same way so that's                                 one thing which meant we had to go open                                 our leaf search node which for the most                                 part we treated as a black box now we                                 have to open it up and look inside it                                 and see what it is doing and how can we                                 put these components out into                                 elasticsearch so I don't talk about the                                 indexing here but it's pretty similar                                 except the ranking part so let's look at                                 a search query a search query comes in                                 again this is a rest rest behind rest                                 API so then we generate a leucine query                                 which is fine we could potentially map                                 this to something in elasticsearch maybe                                 then there's an analysis component which                                 is a bunch of Java classes of course                                 that is the ranking now what is the                                 ranking do we have                                 Lucien's quarters queries and weights                                 and we rely on the Lucille Index but we                                 also rely on like I said before the Java                                 heap okay what does the heap have the                                 heap has things like business field cash                                 yes we still use field cash because                                 again this is old code base and it says                                 we started with leucine - I think we did                                 one port the leucine three and still                                 have the field cash in and it quickly                                 started adding features to it or more                                 fields to it rather and then there is                                 this interesting piece here called the                                 miscellaneous data so like I said before                                 we would have a lot of different                                 engineers working on this codebase                                 typically who would be the users of the                                 infrastructure but in this case they                                 were the ones writing the code that                                 allows engineers so let's say I want to                                 add a new feature called CTR or a                                 click-through rate what they said                                 potentially what is typically means is                                 in the search and kind of adds world is                                 for a given business what are the top K                                 queries that it was clicked for this                                 really helps you like squared up this                                 bedroom now this cannot be represented                                 easily in a field cache so we just put                                 this in a map because why not we have                                 access to maps and this is kept growing                                 school so coming back to what we need to                                 port over this is somebody of the                                 previous slide mean to port over the                                 ranking the analyzers the data on Java                                 heap and some advanced features like                                 highlights and logging by logging what I                                 mean is the kind of you sent back the                                 dictionary of the top key scores and the                                 reason for the score such that let's say                                 it's a dictionary of feature two double                                 where the feature is that will feature                                 and this double is the score of that                                 feature so this helps us our offline                                 training to know how to retrain the                                 model so the other teams that Yelp had                                 been using elastic search for the                                 smaller use cases so this seemed like a                                 natural fit and it was JVM based again                                 so we said ok since we have a lot of                                 code in Java Vlasic so charges plug-in                                 API architecture where we can plug in                                 our Java code so let's see how our                                 components map to the plugins so for                                 pretty much most part there's a                                 one-to-one mapping there's a script                                 plug-in there's a analysis plug-in that                                 is dock values especially the later                                 versions of the scene and elasticsearch                                 and then there is this overarching like                                 search plug-in which pretty much can                                 overwrite anything so let's take a look                                 at of them one by one now it's a custom                                 ranking again we need to house our                                 scoring code in the script plug-in the                                 key here is elasticsearch provides you                                 different entry points and you kind of                                 need it's like a boilerplate code but                                 you need to put your code in these                                 different sections and what's                                 interesting is you need to know your                                 score so the lowest level is                                 elasticsearch will call your for                                 examples search script dot one has                                 double on every single document so this                                 import from performance because you                                 cannot do like expensive operations here                                 then that is code which it calls at a                                 per segment level                                 the other interesting thing here is with                                 elasticsearch one shard comprises sorry                                 of multiple segments and it goes                                 linearly over all the segments so if you                                 can do something just once but query                                 porsche are you would rather do it at                                 the per shard level not a segment and                                 this helps like gain performance so for                                 example we would ship a lot of query                                 parameters over from our service to                                 elastic search and these would be JSON                                 for the most part it could be a parts of                                 your model which we had to be sterilized                                 so doing that at the start level makes                                 more sense and for segments because then                                 it's because if you have shard has many                                 many segments especially if you have                                 real time indexing then it's going to be                                 slow and then there are certain things                                 you can do just once per JVM instance                                 which is like plug-in instantiation                                 against the key part here is you should                                 know the Scopes and yes changes these                                 names and the factories and the leaf                                 factories around a lot so the best way                                 to do this is every new version once a                                 code compiles just turn on your favorite                                 debugger or editor and step through like                                 for a couple of documents the analyzes                                 what some fun to so on the face of it                                 okay we have analysis plug-in and we can                                 use                                 get analyzers and dropper code there the                                 one issue for us was we were using a                                 much older version of the scene now the                                 bright fix which is second bullet point                                 here is let's upgrade all this to the                                 later version of the scene but we don't                                 know how that would actually change the                                 behavior that means now another                                 potential for regression in our results                                 which we didn't want so the quick fix                                 our hack in this case was using shading                                 the shading is like a plugin provided by                                 certain build tools for example maven                                 this is a maven XML taken out of the                                 screenshot taken out of the our                                 symbol where we could relocate your arc                                 Apache Luc in to come yell search                                 old-school ar-ar-ar quick Apache so what                                 can happen then is we take this jar now                                 and drop it on elasticsearch so when you                                 import org Apache l-leucine it's going                                 to use the later one which elasticsearch                                 has but when you import calm Yelp search                                 it's going to use the older Lucene so we                                 had this bridge in analyzers where at                                 the entry point of the analysis code we                                 would have a wrapper analyzer which                                 called a role analyzer now this is                                 tricky and it's quite risky because the                                 bridge definitely like breaks on certain                                 edge cases like it worked for us for                                 highlighting and stuff because things                                 like offsets they just like losing get                                 stricter and stricter with like                                 backwards doing offsets and stuff so it                                 could potentially break I'm not saying                                 Dundas in production but if you want to                                 get something up and running quickly                                 just kind of work for us of course data                                 off the Java heap that was a pretty much                                 one of the big wins of going to es like                                 we just move everything to document                                 values the nice thing was we were able                                 to run a ranker completely oblivious of                                 the back end so the rancor would have an                                 API which would say document dot get the                                 field name and then the implementation                                 was in this case using scripts one                                 interesting thing we had to add to the                                 elastic search as a patch was the CTR                                 data like I mentioned before elastic sis                                 had the ability to store blobs but we                                 couldn't really read them as dock values                                 in scripts so we had to like structure                                 our own data as a                                 before put it in and you could read it                                 out and because it's your own custom                                 format it's really fast too so that's                                 how we cut around like not having the                                 CTR in the heap certain other features                                 like highlights called components these                                 are also implementable you need need to                                 look at some of the documentation is not                                 great but you need to look at the                                 effects of phase look at some                                 implementations and go from there I                                 could spend a lot more time talking                                 about performance but here's some                                 highlights use the profile API it's                                 really cool it tells it order sort of                                 things like okay aggregations are                                 released slow global on tinel's are the                                 cause of it like I said before charts                                 scale up to a point then there is the                                 overhead of just maintaining them dock                                 values is good and doing simple things                                 like just doing J stock if a system is                                 slow will tell you okay what's on top of                                 my stack all the time CMS did not work                                 for us we had a thrust at launch for a                                 few weeks before we went live and every                                 single day CMS caused a lot of issues so                                 we've been using g                                                      we have moved over how does a search                                 architecture what are the benefits of                                 this we don't have to be with the                                 thousands of lines of code now which is                                 managed by elasticsearch our on call is                                 not happier I think and more importantly                                 this is like unlocked other teams at                                 Yelp to essentially run the similar                                 filtering ranking analysis and whatnot                                 on their own data because we can't Rob                                 these plugins on their clusters like ads                                 for example or request a code what is                                 the other new functionality that we                                 added to using yes which would have been                                 harder otherwise one is hosting our                                 models elasticsearch so before so before                                 this we were passing a lot of the model                                 in the query itself and some other teams                                 started doing stuff like let's pull out                                 the data from es hit the network                                 bandwidth                                 keep pulling out till that point and                                 then rescore outside and model service                                 that doesn't really scale so how could                                 we score on elasticsearch luckily we                                 found this plugin colouring to rank                                 developed by OSC the idea here is post                                 your features and models to yes                                 separately offline and then in compute                                 time you can actually calculate this                                 course I'll go into it on this slide so                                 here what we do is we upload the                                 features and models by our REST API                                 annual learning to rank hey my feature                                 one is just my plugin which is a                                 function score query and then it let it                                 do what it wants to do with feature one                                 - another feature could be a painless                                 script a feature could be a leucine                                 derived expression and so on and then                                 you push the model which combines all                                 these features and tells it how to                                 compute in the case of a linear model                                 it's just a linear math equation we also                                 use actually boost in production which                                 is more of a tree form so if every                                 document we do this then it comes back                                 to the plugin it does the final compute                                 and it gives us the result so this way                                 the es query is now really tiny we just                                 need to send in the model name and the                                 query time parameters so as of today                                 Yelp has been a collaborator to the ITR                                 it didn't really work out of the box for                                 us because of some of the advanced use                                 cases and I talked about it in this                                 haystack talk which I gave some time                                 last month and it had some performance                                 issues which we had to fix too but we                                 use it today for many of our critical                                 workflows and if you guys were                                 considering like hosting models in yes                                 consider using this contributions are                                 welcome to finally we are hiring across                                 the board and also for search and this                                 some of our blogs I have written a blog                                 post about our migration too and that's                                 it yes thank you very much                                 [Applause]                                 maybe we can have one short question yes                                 I won't be short I don't know what your                                 answer but D you said you know goal was                                 not to change ranking at all so I'm                                 curious how did you validate that how                                 did we elevate that Oh for the most part                                 we didn't so like I said we we ran into                                 issues like analyzers or highlights of                                 broken so we had to go out this                                 conversation with like product or                                 management okay highlights uh broken in                                 these cases do you really care and it                                 would be like maybe we don't but but I                                 guess what more more important was we                                 had to get a POC out in like a time                                 bound at window well let's say                                           of stuff worked which is what approach                                 worked well and then you could argue                                 about oh but here all the wins we are                                 having you don't have to do management                                 of the cluster on-call is up here you                                 have better                                 there's no attrition people are not                                 leaving the job so that kind of balance                                 out with that cases yes so now that                                 you've learned that maybe you didn't                                 that wasn't such a concern                                 could you make deeper changes in the                                 analyzers without worrying so much like                                 porting them all to newer Lucene are you                                 gonna any way this is I'll ask you                                 alright okay okay note that was short                                 question again let's think wish again                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=ygIEHfEdIfs


