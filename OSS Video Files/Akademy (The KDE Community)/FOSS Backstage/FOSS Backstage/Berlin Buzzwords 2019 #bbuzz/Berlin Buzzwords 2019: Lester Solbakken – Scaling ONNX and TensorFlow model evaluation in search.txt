Title: Berlin Buzzwords 2019: Lester Solbakken â€“ Scaling ONNX and TensorFlow model evaluation in search
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	With the advances in deep learning and the corresponding increase in machine learning frameworks in recent years, a new class of software has emerged: model servers. These promise, among other things, performance and scalability. 

There is however a large class of applications where such model servers are inadequate. For instance, search and recommendation applications must efficiently evaluate models over potentially many thousands of data points as part of handling a query. In such cases the amount of data transferred to the model servers can quickly saturate the network and thus decrease total system throughput and degrade quality of service.

In this talk we present a solution to this problem which is to evaluate the models where data is stored rather than moving data to where the model is hosted. We base our solution on Vespa, an open-sourced platform developed at Yahoo for building scalable real-time data processing applications over large data sets. Vespa has  native features to import ONNX and TensorFlow models and represent the computational graphs in its internal tensor language. 

In this talk we will show how this achieves model evaluation performance at web-scale, and that even if one does not take advantage of specialized hardware such as GPUs and TPUs, the total system throughput can scale much better.

Read more:
https://2019.berlinbuzzwords.de/19/session/scaling-onnx-and-tensorflow-model-evaluation-search
BerlinBuzzwords, Buzzwords, bbuzz, #bbuzz, bigdata, big data, search, scale, store, stream, open source
About Lester Solbakken:
https://2019.berlinbuzzwords.de/users/lester-solbakken

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah hi my name's Lester welcome I'm                               from the vespa team at Verizon media if                               you're not familiar with Vespa don't                               worry I'll I'll try to get back to that                               a little bit later but all you need to                               know is that Vespa has its roots in                               being a vertical search platform hence                               it's a name but it's evolved into                               something more of a kind of a big data                                serving engine at least that's what we                                call it                                Vespas mainly developed by team in                                norway that's wrong I'm in Norway we                                open sourced it                                                   worked on many years before that at                                primarily Yahoo Yahoo was acquired by a                                Verizon a couple years back and we're                                now called Verizon Media Group so what                                I'm here to talk to you about today is                                about using machine learning production                                particularly towards search type                                applications and investigates and solve                                some of the kind of issues or problems                                or challenges that we quickly meet as we                                attempt to scale up these solutions both                                in terms of traffic and in terms of                                contents okay so with the rise of the                                various deep learning platforms other                                recently the last few years we've seen a                                corresponding kind of increase in a new                                class of software model servers model                                servers work from the premise that you                                can take your model that you've trained                                in some of these platforms tensorflow at                                PI torch and so on and you deploy it on                                this model server the model servers                                solves things such as performance                                scalability lifecycle management                                versioning and so on and it kind of puts                                your model behind some interface RPC                                HTTP or whatever and you can put your                                application in front of that inquiry to                                this model server typical use cases                                image classification sending you image                                get back a distribution of objects or                                probabilities of objects and image                                image captioning getting value                                description' what's happening the image                                texts the translation text generation                                impetus sentence game-playing                                impetus abhorred and so on many                                different types of applications and for                                these sorts of applications this                                solution works well for search however                                it's a little bit different in general                                research kind of type applications we                                have a set that works kind of like this                                we have some input some query coming in                                for text search to be like query terms                                and so on for ads and so might be a user                                ID coming in same thing with                                personalization usually have some sort                                of query enrichment going on first                                adding stuff to to the query what do we                                know about the user and user issuing the                                query and so on the query gets sent down                                to a set of search notes for search                                service of content servers and there's a                                lot there there's a lot of computation                                going on depending on the application                                and a ordered list of results are sent                                back again and eventually back to the                                user and lately we've seen a rise in                                interest in doing this kind of ranking                                with this computation with machine learn                                learning such as learning to rank and                                more recently things like neural                                information retrieval and so on and what                                we want to do is we want to evaluate the                                model and each of the results coming                                back from the content server right so                                where previously where we had this model                                service we have one data point in and                                kind of data coming out with this we                                have many data points that we need to                                run through this the mall server and                                that quickly leads to a problem of                                network capacity fairly quickly actually                                the bottleneck becomes the network                                capacity so for instance if you have a                                query you're returning a thousand                                results per query each of these the                                results have it kind of a data of around                                                                                                    reach very quickly with learning to rank                                features or like war and document the                                beddings word embeddings and so on if                                you're running on a                                                   you can sustain the max                                                 second max right then you're using all                                 your network capacity to this bet is                                 generally a bad state of affairs                                 to put this in context a little bit at                                 Yahoo we have multiple application                                 running in the tens of thousands of                                 query per second per Colo so for these                                 kinds of applications this sort of                                 architecture does not work it's not                                 scale at all so the solution to this                                 looking at the equation on the right                                 hand side there or two left there either                                 to send less data for results for                                 instance you can't do that for learning                                 to rank because the features are                                 calculated on the content servers but if                                 you're not doing that you can maybe you                                 know send some of the features to the                                 model servers create your own kind of                                 custom software and top model server                                 there increases memory on the snows                                 maybe have some update issues between                                 what's going on the content and model                                 servers and so on but the problem really                                 is that what's going on in content                                 servers and what's going on model                                 servers might be evaluating this ranking                                 based on the different set of features                                 and that actually lessens the kind of                                 probability of you getting there                                 globally best results that you can                                 because the model service can only work                                 on the results coming from the content                                 servers so there needs to be some sort                                 of correlation between what's going on                                 and that's difficult to do if they're                                 running on different set of features you                                 can also try to return less results per                                 query but again if you don't have good                                 correlation between what's going on in                                 these two two areas and then you                                 decreasing potential quality of your                                 system so the real solution is you know                                 don't move data around but instead                                 evaluate the models on the content                                 servers and that's something we've been                                 working a lot on on vespa by importing                                 these kinds of models that is flow on X                                 and X G boosts and evaluate him directly                                 on the content servers as well                                 okay just to describe a little bit more                                 more about footrest base I mentioned                                 that Verizon acquired Yahoo a few years                                 back has its roots from from from Yahoo                                 we were merged together with AOL and for                                 a while we were called off recently                                 rebranded to Verizon media group and                                 basically working as a kind of a                                 technology producer for a large set of                                 websites on the line but at Yahoo or                                 Verizon media the Spezza long history                                 and it's always been a very popular                                 piece of software inside of Yahoo we                                 have hundreds of US publications running                                 serving over billion users per month at                                 any given time given time it's running                                 over hundreds of thousands of queries                                 per second all over the world over                                 billions of content items so it's in                                 fairly heavy use some small examples was                                 used for and a Flickr image search for                                 instance on the right-hand side there                                 you have the front page of Yahoo it has                                 personalized recommendations for                                 articles also does real-time native ads                                 in between there and those things like                                 real-time bidding and so on and one of                                 the more fun applications that we have                                 is that on all the kind of news pages                                 that you who like Yahoo Finance and so                                 on there are comments sections and as we                                 all know comment sections are generally                                 garbage                                 so there is a piece there that where all                                 these comments are ranked by this by                                 using a neural network and it's kind of                                 difficult to know how you should you                                 rank these comments what's the objective                                 function so we train this using a                                 reinforcement learning algorithm which                                 trains the model PRT periodically                                 and pushes the model to two vespa and                                 based on data it just continues to cycle                                 to improve the model to do all this                                 Vespas a rich set of core features I'm                                 not going to go through all of them                                 there's a talk later today and the mate                                 might go into a few more but three ones                                 I want to kind of focus on a little bit                                 is its elasticity scalability and                                 capacity for advanced relevant scoring                                 so picking a little bit under the hood                                 of a spa we can see how it kind of                                 achieves its performance at scale so                                 whenever we have some query coming in                                 there's always this query handler which                                 is where we do the query enrichment and                                 so on and if that's processed and and in                                 massage a little bit it gets sent down                                 to one or all the content partitions so                                 in any single game content partition we                                 goes through a set of stages so the                                 first is a matching stage we're all kind                                 of relevant or at least somewhat                                 relevant documents related to the query                                 are fetched binary decision then we                                 typically have a first phase ranking                                 function which is a ranking from the                                 function it's efficiently evaluated very                                 cheap one which calls down the amount of                                 documents to be rear and in the second                                 phase and that's typically where you                                 have you kind of computationally                                 expensive computation to be to be done                                 and thus we can reduce the latency or                                 the time spent inside these continents                                 so for instance if you have a machine                                 learn model that would typically be                                 something that's very computation                                 expensive to evaluate so that would be                                 in your second phase ranking but                                 typically we would add in a first phase                                 function to narrow down the search space                                 anyway that's by utilizes all the course                                 on the nodes as efficiently as possible                                 but if you need to reduce latency even                                 more you can add additional content                                 partitions to distribute the workload                                 over a larger number of nodes that's                                 what makes it easy to do that                                 automatically distribute the data among                                 these nodes and and so on                                 so we've been hard at work this last                                 couple years to add this kind of                                 integration with with Vespa tensorflow                                 all necks if you not familiar with all                                 necks by the way it's the open neural                                 network exchange format it's basically                                 the kind of deep learning format all the                                 vendors other than Google are using XG                                 boost is what you want to use if you                                 want to participate in the Kegel                                 competition for instance it's actually                                 useful whenever you create a application                                 for Vespa you recreate you create what                                 we call a application package and it's a                                 kind of a declarative package of state                                 what should the application do how                                 should do it and so on                                 we try to make this using machine and                                 models and that's as easy to possible                                 that easy to use as possible so you                                 pretty much just drop your model into                                 this this application package and when                                 you do that                                 respite takes care of importing the lis                                 models and make them available so you                                 can use them directly in your                                 handwritten a ranking expressions such                                 as this so for instance this allows you                                 to very easily string together different                                 models from different sources and so on                                 so for instance you can have a versus a                                 click probability model trained in in                                 terms of flow may be a dwell time                                 estimator trained in PI torch and so on                                 and you can combine these nonlinearly                                 using extra boost and so on and not                                 saying you should just saying it could                                 but this is a kind of very cool very                                 unique feature that Vesper has I don't                                 think yeah I've seen this other places                                 so when we import these models into to                                 Vespa we don't rely on any kind of                                 external kind of executor for that we                                 execute them in the ranking language in                                 Vespa so a few years back I think two                                 years back we introduced something                                 called the tensor API which is an                                 extension to our own kind of ranking a                                 language which is an extension that                                 handles                                 multi-dimensional data or any                                 dimensional data really and the API was                                 designed to have like a very small set                                 of core features which could represent a                                 large class of different types of                                 computation this is in contrast to for                                 instance tensor flow if you've been                                 working with that you know that the API                                 can be very large very confusing many                                 operations doing the same thing so we're                                 trying to go kind of the opposite                                 opposite direction so here in this this                                 example here on the left hand side here                                 we have a computational graph                                 representing a single layer and a neural                                 network however matrix multiplication                                 between the placeholder which is tensor                                 for speak for inputs weights is the                                 weight strained by your your machine                                 learning algorithm and matrix                                 multiplication multiplication between                                 those two you add in the bias and do a                                 rectified linear unit at the end for                                 instance so this is converted into the                                 expression on the right hand side there                                 and on the back end on the the kind of                                 node where this all this is calculated                                 when it sees this expression it                                 optimizes this so for instance join and                                 reduce its optimized to again a single                                 step so we don't have to introduce                                 temporary tensors and so on so have many                                 of these kind of smaller optimizations                                 and the benefit here is that when we                                 have different models coming from                                 different sources they're all kind of                                 translated to this format so we have                                 kind of one set up at one place that we                                 need to to keep optimizing so we one is                                 the test this a little bit we set up a                                 benchmark for this we wanted to kind of                                 test this hypothesis that sending data                                 around is not the smart thing to do we                                 set up a test we're kind of emulating a                                 recommendation system a blog                                 recommendation system where we have a                                 user representation with a vector and we                                 have a document representation with                                 another vector and we set up a first                                 phase which is basically a dot product                                 between a typical recommendation system                                 and then we have a second phase which is                                 a neural network which we try to                                 evaluate on the content node and                                 alternatively on external model server                                 here tensorflow                                 and additionally we have the green one                                 which is baseline with data which is the                                 during the first phase but sending back                                 the data as if you were going to send it                                 to an external server but not doing                                 anything with it just just adding a                                 baseline and in the model itself is                                 about two hundred thousand parameters so                                 it's a fairly reasonably sized model so                                 these are some of the results that we                                 got on the left-hand side here we see                                 the latency evolving as we increase the                                 number of clients so clients is in a                                 number of kind of clients pushing at the                                 same time acquiring at the same time of                                 two hundred and twentieth which is                                 fairly heavy heavy traffic on the right                                 hand side we see the throughputs or the                                 QPS queries per second how that evolves                                 an interesting kind of thing to notice                                 here is the green line on the QPS there                                 it flats out fairly early actually very                                 early around less than twenty twenty                                 clients and that is a point where we                                 reach network saturation right so we                                 cannot push more data through this                                 network and of course latency increases                                 because of this and you see it tends to                                 flow tracks then below that a little bit                                 and just to be kind of clear here no                                 amount of hardware acceleration that you                                 can put on your tensor flow no to your                                 external motor server via GPUs or TP use                                 or quantum computers or whatever can                                 allow you to push through that green                                 line there right it's the hard                                 scalability ceiling of just sending data                                 around so the blue line there is as the                                 vespa you see that obviously scales much                                 more better you're not sending data                                 round so this is how it looks with                                 running I think it was three content                                 nodes we're sending my with a thousand                                 results per per query we can improve                                 upon this                                 if you want to have a better latency and                                 so on you can add additional content                                 notes this will have the effect of                                 decreasing Elaine see because we're                                 distributing the work around a larger                                 number of nodes this has a kind of a                                 diminishing effect however you can't add                                 infinite number of of content nodes so                                 go down to zero this is on dolls law you                                 might be familiar with and basically                                 says that whenever you're doing some                                 computation you have just a fraction of                                 the work is parallelizable and only the                                 paralyzer part will be affected by                                 adding additional content notes so this                                 has a diminishing effect however there                                 is another way of using these additional                                 computation notes or content notes if                                 you want to and that is actually by                                 having them doing more work so if you                                 can satisfied with your                                 SLA if you're satisfied with running at                                 say                                                                percentile meaning that                                                 queries are going on less than hundred                                 milliseconds you can kind keep adding a                                 work to your content notes to actually                                 rewrite a larger number of documents so                                 the earlier I mentioned what you really                                 want to do with your machine learning                                 models is that you really want to run                                 them on all of the documents that you                                 have available but typically that's much                                 too expensive to do that's why you have                                 to introduce this face to the ranking                                 but by adding additional content nodes                                 you can use the additional compute to                                 actually rewrite a larger number of                                 documents and this is something else                                 feels much more linearly this is a                                 Gustav's Ohm's law is the workload                                 increases as parallelity increases and                                 this is something you cannot do when you                                 think some external model server right                                 because you're just trying to push more                                 data across the network and you're                                 already capped at writes so it's an                                 interesting interesting effect                                 okay so to conclude extra monsters don't                                 really scale that well for when you're                                 using machine learning in search and to                                 to combat that you put the you put the                                 evaluation on the content notes and that                                 gives you many more options to scale in                                 how you can control latency you can                                 control throughput and you can increase                                 the number of rearing results with your                                 machine learning models just leading to                                 higher potential quality of your results                                 multi-phase ranking when you whenever                                 you introduce like a first phase in the                                 second phases so only mentioned                                 previously it's very important with the                                 correlation between these phases                                 unfortunately we see many many times                                 where users of a Vespa particularly have                                 not been really that cognizant of what's                                 what what's going on in the first phase                                 and what's going on in the second phase                                 that's leading to a worst kind of system                                 level retrieval question for me to you                                 if any you have been working with multi                                 phase ranking come find me afterwards I                                 really would like to hear your stories                                 model support in our machine learning                                 model support in Vespa is ongoing work                                 there's some sort of models that we                                 still don't support currently but it is                                 an open source project if you'd like to                                 contribute come find me afterwards as                                 well that was always gonna say thank you                                 [Applause]                                 okay do you have any question for a                                 lessor so one want to ask something yep                                 hey thanks very much for the for the                                 talk it was really interesting and just                                 seeing the talk makes me sort of want to                                 play around with desperate and I'm                                 wondering if you have any pointers what                                 the best place to start would be so                                 you're asking about if you have any kind                                 of simple places to start playing around                                 with this so you see will find many                                 resources at our homepage Vespa AI                                 including many sample applications and                                 the use cases there which we could                                 really just get you started including                                 sample data and so on so there is a                                 tutorial there which builds upon this                                 blog recommendation application that I                                 showed                                 so that has all the data available and                                 so on to recreate those numbers
YouTube URL: https://www.youtube.com/watch?v=wjdxOwQbs2k


