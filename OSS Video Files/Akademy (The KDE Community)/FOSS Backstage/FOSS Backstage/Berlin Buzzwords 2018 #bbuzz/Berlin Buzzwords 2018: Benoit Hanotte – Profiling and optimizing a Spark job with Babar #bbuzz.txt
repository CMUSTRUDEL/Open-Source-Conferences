Title: Berlin Buzzwords 2018: Benoit Hanotte â€“ Profiling and optimizing a Spark job with Babar #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	New data-processing frameworks such as Apache Spark and Flink have made writing Apache Hadoop jobs very easy. However, as the data grows, developers face new challenges: from stability issues to allocating the right amount of resources, large jobs are often hard to tune and debug as their distributed nature and scale make them hard to observe.

Babar, an open source profiler developed at Criteo, was introduced to make it easier for users to profile their Hadoop jobs and their resource usage with little effort and instrumentation. It helps understand CPU usage, memory consumption and GC load over the entire application, as well as where the CPU time is spent using flame graph visualizations.

In this session, we will see how to instrument an Apache Spark job and go through its optimization in order to improve latency and stability, while reducing its footprint on the cluster.

Read more:
https://2018.berlinbuzzwords.de/18/session/profiling-and-optimizing-spark-job-babar

About Benoit Hanotte:
https://2018.berlinbuzzwords.de/users/benoit-hanotte

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello my name is Benoit notes I'm a                               software engineer at Cray do and I we                               talk about providing optimizing spark                               jobs with tools we developed first                               before we can if you world about Korea                               we're in the online advertisement                               business and our motto is connecting                               shoppers who sings and need and love so                               the idea is to to present users with                                relevant advertisement driven products                                rahman services and only present do                                salvat iseman to users that might be                                interested in it and to do so we use a                                lot of machine learning and because we                                use machine learning we need to store                                and process a lot of data and for that                                we've built a Hadoop cluster just a few                                numbers about the cluster it's alpha                                beta bytes of data ingested daily                                   petabytes of data read daily and more                                than four thousand nodes it's close to                                                                                                      makes it the largest I do cluster in                                Europe and because we've got so many                                products we have a very diverse                                ecosystem running on top of it's from                                MapReduce to mapper and MapReduce based                                frameworks cascading scalding to other                                frameworks spark fleeing secret engines                                as well such as Ivan presto and so                                recently teams were refactoring products                                or building new products move away from                                a produced based remarks to no                                alternatives and one very popular                                alternative is spark one main reason                                that it advertised is very fast and                                efficient processing Hadoop so it seems                                like a very good choice right that's                                what they advertise on their website and                                so teams a lot of teams build on your                                products on top of spark the frame that                                we didn't see more efficiency or faster                                processing in fact we saw a lot of                                instability                                between containers lost out of memory                                euros yarn killing of containers on the                                cluster randomly it seems also jobs                                taking way too long for processing                                clearly it wasn't what we expected and                                so jobs were very unstable and teams                                were solving this instability by just                                allocating a lot more memory or                                resources to their jobs compared to what                                we had before which was not                                viable solution for the long-term on the                                cluster because does it cost                                infrastructure cost mainly and so                                clearly we needed to better understand                                what was happening and also root for the                                spark jobs to better be able to tune                                then and optimize them so that they are                                good citizens on our cluster for this we                                decided to build a small tool provider                                for distribute applications on Hadoop                                which we call Baba                                we've it with part but not only it's not                                really tight when you framework visit                                with spark as I said but also sky                                leading I've it should work with link if                                you use blink and there were four main                                reasons who I wanted to develop this                                profiler first I needed to be very easy                                to get started with there exists other                                providers you may have heard of study                                for instance but they are required for a                                structure whether it's database whether                                it's some current code being installed                                on your computer nodes or that was a                                very high barrier countries or country                                for users and that explained why nobody                                was actually using them at creo second I                                need to be made hot stupid applications                                because of this nature makes them very                                hard to observe from start to hand if                                you want to Ricci is agent that entirety                                of your job the profile needs to be made                                for that study to work at scale because                                sometimes everything works fine and                                preparation data on sample data and once                                you move to production data then                                everything goes bad and you need to                                understand what's what's happening at                                that scale so you need to work with                                large scale apps and finally results                                need to be very easy to understand                                because teams don't want to spend a lot                                of time trying to figure out all to                                exploit the results and because it's                                been useful to us we've open sourced it                                 it's on github on our github account so                                 it's a github.com slash creatures raj                                 babbar                                 if you fancy having a look and so if we                                 talk about architecture quickly so Bubba                                 come with a Java IDE Jones so it's a jar                                 that you can attach to the executor JVM                                 and that will instrument the JVM so if                                 we get matrix from the JVM it reads                                 nonpolar so your stack traces and it                                 will get metrics from the operating                                 system                                 if you use Linux so that you can get                                 metrics not only for your java code but                                 also native libraries for compression                                 decompression                                 Python codes that you may run outside of                                 the JVM on your executors and so on and                                 so these metrics would be locked to the                                 local file system and when your                                 application completes yarn aggregates                                 your container logs to a single file on                                 the Hadoop on a GFS and Baba just need                                 to process this file to build a final                                 report as an HTML file so if we look at                                 what a report looks like it's so as I                                 said in HTML files that team members can                                 send through slack archive on on their                                 wiki pages and so on there are six main                                 tabs and this report as the first one is                                 containers so it's interesting to see                                 your containers life side column and                                 containers are running at any given time                                 also the the timeline of each container                                 then we've got memory how much memory                                 you use and resolve on your                                 infrastructure which is hard to                                 otherwise have a good idea of what's                                 happening on yarn and we spoke CPU as                                 well or your executor is using memory                                 CPU well as the bottlenecks garbage                                 collection to tune your memory i/o and                                 finally traces so your stack traces at                                 Bob our samples presented as frame                                 graphs and so let's start with flame                                 graphs if you don't know what frame                                 graphs are it's a visualization to                                 quickly identify expensive code paths in                                 your code base so imagine we've got a                                 very simple application represented by                                 this pseudocode there is the root method                                 a according to method B and C and each                                 of the stream is a diamond cutting a                                 last method D so if we want to build a                                 flame raffle for this application we                                 just have ruth without a as i said and a                                 course like from top to bottom so                                 parents we thought would be at the top                                 Jeremy Todd right underneath that so as                                 I said a scoring method B which in turn                                 is cutting with a D and Wendy and B                                 completes and a scoring C which in is                                 current D and then the application                                 completes                                 so which the vertical axis axis is a                                 call stack and the horizontal axis                                 represents CPU times so the the width of                                 the block is proportional to the CPU                                 times I spend and so because a the                                 written is one word person CPU time and                                 B being only                                                                                                                                would be                                                           interesting because not                                 you can see what which cut paths are                                 used in your application but you can                                 also see all expensive they are on CPU                                 time and so that's very convenient to                                 for instance optimize a job and avoid                                 premature optimization because some time                                 you buy something and you realize at the                                 end or too late that actually it was not                                 the bottleneck of your application so                                 let's apply that to a sample job imagine                                 we are joining two data sets so we spark                                 very simple join we open two data set we                                 read them we join them and we write the                                 result to disk if we provide it with                                 bah-bah we get the frame graphs of the                                 entire application not just one executor                                 but all the executives together and that                                 would give a frame graph like this and                                 so as I said before it's reconvening to                                 see what's happening inside your                                 application if you look at this firm                                 graph we can clearly see the process of                                 this puck join so first we can see we                                 read the data because as you can see                                 it's all the park heads input format                                 code so we can see that reading takes                                 approximately                                                           that the records are sorted map side and                                 then finally stylized for the shuffle                                 and after the show for the data is dis                                 alized drooped so joined and finally                                 written to disk so we can clearly see                                 the process on the cost of each step and                                 here we see that reading is very                                 expensive but we also see that sir icing                                 the sizing for the shofar is incredibly                                 expensive and so that's the first thing                                 we can take from from this frame breath                                 for this joint if we want to if want to                                 optimize something we need to optimize                                 the shuffles the shuffle stage and                                 basically avoiding citation because it's                                 not really network and it's not I use                                 that's expensive that sir ization in                                 this case and if you wanted to convince                                 ourselves of this fact we can have a                                 look at the CPU time spent by the                                 application so Baba can give you this                                 graph which is a midi entire CPU time                                 spent by your executor and boss for user                                 and system mode and if we look system                                 mode is during the shuffle is very                                 insignificant versus user mode so it's                                 really not disk and I Oh which will                                 happen in system mode it's really sir                                 ization in user mode so if once you                                 optimize this job and so we need to                                 optimize probably the shuffle answer                                 ization stage and so we need to pick                                 models accordingly because                                 as we've seen because salvation is so                                 expensive it's often more interesting to                                 pick a data representation for you for                                 your data for your records so the                                 mothers that feels allies in this arise                                 very efficiently in self trying to                                 minimize the footprint in memory and                                 also using specialized sizes can be huge                                 gain in our case our using so for this                                 join using special sizes can be                                         gain time gain not only because it will                                 size better but that so because spark is                                 able to apply further optimizations when                                 it has custom sizes for each of your                                 classes and if we looked at the flame                                 graph we could see that it's really in                                 the sort face that spark is able to                                 optimize by using an i/o based salt and                                 another place where we are not expecting                                 is the models choice to have an impact                                 is in the size estimation of the objects                                 if you know spark spark positions its                                 memory into three different areas use a                                 user memory exaction memory in storage                                 memory and in order to be able to know                                 how much memory how much of this memory                                 is used                                 spark needs to estimate the size of the                                 objects and it does so by going through                                 the object recursively and that can be                                 very expensive we can see here that                                 we're spending in this frame graph we                                 can see that we're spending                                            CPU time just estimating the size of our                                 objects and in some cases it can be even                                 worse we've seen up to                                              spent just estimating the size with very                                 large JSON objects with Android of                                 columns so picking models can be a huge                                 optimization for this kind of joints in                                 this as in this example and so now that                                 we've seen that we can profile to try to                                 optimize or cut path try to remove                                 inefficiencies now in our code we can                                 also have a look at what's happening for                                 the memory allocation because memories                                 of an a difficult topic we spark and                                 sometimes you need to deep dive to set                                 the memory correctly set the memory                                 settings correctly and so if once you                                 see always memory used by our example                                 join on the cluster spark and provide                                 that Baba can provide this graph so                                 first there is a graph about total use                                 memory so here we can see first the                                 total JVM heap memory used on your                                 cluster so the Java heap memory so that                                 is some at any moment in your                                 application of                                 the amount of heat memory used by all                                 your executor receives a true peak at                                                                                                       example join but heap memories not every                                 everything that's on your memory stick                                 it's not everything that's on your                                 physical memory because you will add on                                 top of heap memory you've got some Java                                 overhead and you would have some native                                 libraries or in stands for compression                                 decompression or out of memory and out                                 of heap buffers for the shuffle phase so                                 by bacchanals to show you the physical                                 memory used so we can see that while we                                 are using                                                              we are actually using                                                  physical memory so we need to account                                 for that when when we are dimensioning                                 our job and finally it can show the                                 result memory so that the memory you ask                                 your cluster to give to your application                                 and that's not available to any other                                 application the cursor so it's really                                 important to not over reserved memory                                 because otherwise it's just wasted it's                                 not available to other applications and                                 you're not using it and so it's very                                 important for SPARC jobs and for large                                 large applications actually to dimension                                 and tune your memory correctly and                                 that's often a very difficult topic                                 because if you don't treat correctly                                 your application will die and if you                                 allocate too much you will you will over                                 reserve on allocate memory so first one                                 thing that you need to set with this bag                                 application is the executor memory so                                 that's basically the heap memory setting                                 and so if we look at another graph that                                 Babar provides is a maximum use memory                                 for any container so it represents the                                 largest container at any time in your                                 applications that's very useful to                                 dimension as a memory setting so here we                                 can see that at most the largest                                 container will use at most                                             of heap memory so we can set the                                 executor memory just vs                                             giving a bit self Headroom                                 but not too much so we could give                                    five gigabytes and so that's the                                 executor memory and now we need to                                 resolve the memory on the cluster to                                 accommodate not only for JVM heap memory                                 but for the physical memory as I                                 discussed before and so if we look at                                 physical memory we see that actually we                                 are over seven gigabytes so we need to                                 reserve enough memory so that we can                                 accommodate for the seven gigabytes so                                 we the result memory is a spark is a to                                 memory plus the overhead memory                                 and so we need to set these values that                                 we resolved over seven so approximately                                 seven point five gigabytes of memory for                                 instance and we can see that we have                                 fitting the largest executor tightly                                 inside the reserve memory so we are not                                 allocating too much and not too little                                 otherwise yarn would just kill our                                 applications which is a very common                                 thing when you're first writing your new                                 spark jobs and finally when you're                                 dimensioning memory there is offer the                                 issue of garbage collection because if                                 you allocate too much JVM memory then                                 you don't have issues with garbage                                 collection but then most of this memory                                 is wasted otherwise if you allocate to                                 details and garbage collection will be                                 we've spent a significant amount of CPU                                 time in your application and that's                                 probably a bad thing and that's wasted                                 CPU time somehow                                 so Baba can help you is that as well                                 because we can see the garbage                                 collection metrics for your executors                                 but not only just the garbage fashion                                 values that spark will give you but more                                 detailed more detailed one because we                                 know how much major and minor GC we are                                 doing so that's helpful to dimension not                                 only your heap memory but also the                                 generations inside inside of it and                                 we've seen some times with for instance                                 machine learning algorithms when we've                                 got very large vectors in memory that                                 diamond or Reda mentioning generations                                 is more important than read I'm                                 mentioning the JVM heap memory so as I                                 said we've seen that profiling can help                                 you first understand what's happening                                 inside your applications because these                                 frameworks often black boxes when you                                 write your first bag job you don't we                                 know what's happening underneath and so                                 having for instance the stack traces is                                 incredibly useful so again it's very                                 helpful to optimize what really matters                                 and not optimize something that have no                                 important so reoptimize in for your cpu                                 time provide a profiler is incredibly                                 useful and finally it helps understand                                 the resources allocation and all to                                 better tune it so that your application                                 behaves as good as it can on your                                 infrastructure and so because this to                                 Baba has helped so much we saw                                 everything it can be useful to other                                 people too and we're open source as I                                 said before again it's on github.com                                 slash four slash Baba                                 it's very simple - very nice to have a                                 look at yeah thank you                                 so there are many C's that won't fit in                                 twenty minutes but if you want to dis                                 get about this project or other projects                                 we've got at credo we've got a booth in                                 the next room and feel free to come see                                 us and have a talk with us bunch of us                                 are here in Berlin I think we've got                                 time for questions right hi thanks for                                 the talk so in my experience some jobs                                 use a lot of hip memory and I know that                                 one thing they use it for is for these                                 buffers to read the shuffle files so                                 doing about any other thing that that                                 that uses this hip memory and why it is                                 so large because intuitively I think                                 just                                 buffers cannot be so so big so yes there                                 are few things so buffers for shuffle                                 definitely there is also a native codecs                                 for compression decompression if you                                 write for instance Parkhead files and                                 use a gzip or edit for you can get                                 better performance by using native                                 codecs that's how you can provide they                                 need to be compiled for architecture but                                 they will provide much better                                 performance and they work with our feet                                 memory because they are native code so                                 things that when you use for instance                                 five gigabytes of heap memory Java needs                                 to commit more memory so that it has                                 available memory to grow your hip and                                 this committed memory which is called                                 committed memory and Baba can show there                                 is a graph for that as well                                 will increase the physical memory use                                 even if you don't use this available HID                                 memory it will be committed and it will                                 take some physical memory which is why                                 we system so much Headroom of our memory                                 yep so we don't we don't run it on so we                                 look first we don't run it in production                                 because there is still some overhead CPU                                 time a few percent at most and memory as                                 well and so it's not really desirable                                 what we do is we run it on there are                                 jobs jobs that we are developing and we                                 know they will take a lot of resources                                 we want to optimize them before we                                 release them to production and for                                 production otherwise we uses or tools                                 such as dr. elephants for instance made                                 by LinkedIn which is doesn't take any                                 overhead on your application just                                 processes some logs                                 the final logs yeah so it's more                                 Explorer what's happening before your                                 recent pollution but we run sometimes on                                 production data sorry I haven't got the                                 question maybe it was very similar to my                                 question but can you talk a bit about                                 the runtime costs so especially so I've                                 seen you you're using power for                                 reporting right and also you have shown                                 some profiling so my assumption would be                                 you are not using the profiling in the                                 production system right but I using then                                 the reporting stuff in production and                                 what are the costs there so yeah it was                                 a bit similar we don't choose Baba in                                 production because there is quite some                                 hover we could we've measured it it's a                                 few percent in CPU time and it's a few                                 dozens of megabytes in memory but what                                 Baba provides is probably - too much                                 information - to re run it in production                                 because you won't be able to do much of                                 it we we do use it to optimize the jobs                                 before they go to production some very                                 large jobs that we know will have a                                 large impact on the cluster but for                                 production jobs we use other tools such                                 as dr. elephant from England which just                                 process job contours and will give you                                 reports and can also give you hints                                 based on heuristics yeah sorry we don't                                 have more time for the questions so                                 thank you very much                                 thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Tcg5YFxrVsk


