Title: Berlin Buzzwords 2018: Marton Elek – Docker to kubernetes: running Hadoop in a cloud native way
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Creating containers for an application is easy (even if it’s a good old distributed application like Apache Hadoop), just a few steps of packaging. The hard part isn't packaging: it's deploying

How can we run the containers together? How to configure them? How do the services in the containers find and talk to each other? How do you deploy and manage clusters with hundred of nodes?

Modern cloud native tools like Kubernetes or Consul/Nomad could help a lot but they could be used in different way. It this presentation I will demonstrate multiple solutions to manage containerized clusters with different cloud-native tools including kubernetes, and docker-swarm/compose. No matter which tools you use, the same questions of service discovery and configuration management arise. This talk will show the key elements needed to make that containerized cluster work.

Read more:
https://2018.berlinbuzzwords.de/18/session/docker-kubernetes-running-apache-hadoop-cloud-native-way

About Marton Elek:
https://2018.berlinbuzzwords.de/users/marton-elek
Captions: 
	                              hi I would like to speak about                               kubernetes or and hello but more                               precisely I would like to speak about                               distributed systems and continuous                               environment I'm an Apache cometary                               narrative project which is incubated                               project and this is a ruffed library for                               Java which could be embedded and in fact                               it's embedded in had a new sub project                                in Hadoop this Apache old Apache Hadoop                                O's on an Apache Hadoop HDDs I also have                                a experimental project when I'm trying                                to contain eyes the whole world or at                                least the big data part and I'm running                                the Apache Big Data projects in                                containers in Dockers for kubernetes and                                in docker compose in other ways ok so                                I'm working for the Hortonworks and a                                significant time of my daily job is just                                starting and stopping head up containers                                and I really like it I think it's a it's                                a good job it just depends what kind of                                tools are you used and I'm pretty sure                                that using containers and                                containerization it's a managing Hadoop                                it's more like something like this so it                                could be more robust and more manageable                                but this talk is about head but to be                                honest it could be any other kind of                                animal because it's not strictly                                connected to head you know just I don't                                know if you know the feeling that you                                read some getting started guide and                                everything is fine but with a real world                                application it's not not that the case                                though I use head too because it's a                                very good at this example it's a good                                all the major application so if                                something could work with head and I                                think we know the receipt to start any                                kind of distributed application in                                containers ok next question do you know                                what's this I'm pretty sure you know                                because yeah this is from the European                                Union this is a regulation that every                                household appliance should be tagged                                with some kind of label just to make it                                easier to compare not just based on the                                price but for example based on the                                energy consumption or the noise level or                                something like this yeah it's it's                                pretty smart and what I'm wondering that                                how can I do similar labors to compare                                the containerized environments for                                example cookware that is how can I                                understand the key questions and this is                                what I would like to do in this talk                                just label different contour eyes the                                environment and learn what are the key                                questions which should be sold anyway                                with some kind of tools for example                                kubernetes yeah so for that one we need                                some real world applications or                                environments so I will show multiple one                                but unfortunately we don't have enough                                time to check all of the technical                                details so we'll use a simplified                                methodology just check one good thing                                and one bad thing for for every example                                actually usually I'm more interested                                about the best thing right that's the                                limitation that's wholly how it will be                                failed so that's what we will check it                                ok the next question do you know what is                                Apache Hadoop and so on if you know ok                                most of them okay but to be on the same                                side I have a very short no I have a                                complete head of training actually in                                   seconds this is the way how I explain                                what is Hadoop for what I'm doing for my                                grandparents so it's just very high                                levels so sorry if you don't know the                                head but it's enough for this                                presentation yeah so head hoop is a very                                popular big data to set or big data                                application ok next question what is Big                                Data well this is the same as the small                                later just just in big right so what is                                small data yeah that's that's the                                easiest question I'm pretty sure that                                the small data is an excel sheet right                                so Big Data is an excel sheet which does                                not fit on my computer so it need                                multiple computer and Hadoop could                                handle this yeah the first problem is                                that we need to split it somehow and                                just store one part in one cool node                                that's what HDFS could do and the next                                problem is that we need the same                                 calculation for for example we need the                                 maximum value from the excel sheet so                                 it's pretty easy right we need to just                                 calculate the maximum at every node and                                 after that we need the maximum or the                                 maximum so this kind of calculation is                                 handled by the yarn and other sub                                 project in Hadoop and we can write in                                 the calculation with the MapReduce                                 framework but it's not necessary you can                                 use any ad or other framework or product                                 like sparkling and in fact we have a new                                 or two new Hadoop sub-project ozone and                                 HDDs but there will be a presentation                                 about them tomorrow okay and you can                                 imagine that Hadoop as a lot of master                                 components worker components we don't                                 need to know all of the details about                                 but we have master components worker                                 conference and we would like to contrast                                 an okay let's start with the                                 containerization okay that's the docker                                 file right that the containerization                                 have you ever wrote something like this                                 okay but I think all of you can imagine                                 that we I had a base image I'm adding in                                 a Hadoop and yeah that's all originally                                 the title of my presentation was how was                                 how to quantize Hadoop and I just                                 realized that okay it's very easy that's                                 just one slide right dad yeah the                                 question is that how the containers                                 could work together right we have some                                 question questions which should be                                 answer the configuration management the                                 provisioning schedule looking at all of                                 them so that's not the the tricky part                                 is not the containerization itself but                                 running the containers together ok so                                 let's start with configuration just just                                 as an example so this is the good old                                 head of style configuration I have no                                 problem with that that's eczema but yeah                                 I can do the same type of with XML and                                 llamó files so it's no problem another                                 problem is that in docker environment                                 typically I have a little bit different                                 configuration so the most dope core                                 native configuration is the environment                                 variables right so this is a docker                                 compose definition and you can see that                                 I have a base image hosting port and                                 environment variables in fact I can                                 mount the XML files but it's more harder                                 to manage so what I would like to use is                                 just a set of environment variables so                                 what can I do yeah I can create a                                 launcher script so it's                                 not a big deal right it's very easy to                                 map something and the launcher script                                 could create the configuration files and                                 after that the good ole distributed                                 application could be started for example                                 they had ooh                                 so it's just a few lines of code and                                 after that I could have this kind of                                 docker compose file you can see that now                                 we I have some kind of naming convention                                 and based on the naming convention the                                 stores are skipped will generate the                                 final configuration and based on the                                 extension I can choose the format so if                                 it's an XML it should be converted to a                                 Hadoop XML format okay done                                 let's not learn here is that we would                                 like to manage the configuration value                                 it's not the format format it's it's                                 very easy right we can just convert it                                 from one format or other format but with                                 docker it's easier to manage in                                 environment variables so we can anij it                                 in environment variables okay so the                                 first part is done we we know that if                                 it's noisy or not this freezer okay so                                 we have this the source of the                                 configuration has been sold currently we                                 have no pre-processing and there is no                                 support for change so if I am I would                                 like to change something in the                                 environment variables I need to restart                                 everything manually okay but there is                                 there is an other big question there are                                 two main approaches to to containerize                                 big data application one is just put                                 everything in one container it is the                                 easiest all right just one container per                                 node and the other one which usually                                 suggested by the docker literature to                                 put every application in a separated                                 small container this is usually this is                                 the such a situate but the left side is                                 more easy because we don't need to                                 modify anything any kind of existing                                 application could be started just as it                                 would be another big container so why                                 what is the best and and why that's the                                 question yeah we we need to understand                                 the whys so if we need a freezer or not                                 yeah I prefer the right side and not                                 this is because it is suggested by the                                 literature but I think it's more it                                 seems to be harder                                 but after right it's just easier imagine                                 if you would like to restart for example                                 or or update the hive with one big                                 monolid container you need to upgrade                                 the container and restart everywhere                                 so it's more easier if the container is                                 the unit of the packaging and you can                                 handle all of the application in a very                                 same way if you for example if you add                                 something to the launcher script then it                                 will be available easily for every                                 application so the power of the                                 containerization at least in a local                                 environment or in in dr. Campos                                 environment is inside the launcher                                 script so with launcher script we can do                                 very powerful things just we need to add                                 this simple script and if environment in                                 any environment variable said that we                                 can do some kind of magic okay for                                 example this configuration                                 transformation but we can do any other                                 magic for example waiting for another                                 service or we can download an additional                                 component drawer file we can prepare the                                 ad HDFS Hadoop requires some kind of                                 formatting before the first tarped on                                 the HDFS side but it's very easy to                                 adjust if we need format and if at if                                 the director is missing just do a name                                 not forma yeah we can also setup the                                 credentials enabled promted smart ring                                 withdraw or agent so it's more and we                                 can do more and more complex in just                                 with the launcher my personal favorite                                 is the last one so I don't know if you                                 know what are what is in in the wire                                 between the Hadoop components so you                                 know there is proto buffer and the                                 Hadoop components are just speaking with                                 each other and in our launcher script we                                 have just a simple Java agent it could                                 be turned on because we handle all of                                 the all the applications in the same way                                 and yeah this is a standard output where                                 the log-log standard log just replaced                                 with the messages which are replaced                                 between the name node and data node the                                 master and worker so I can see all of                                 the all of the network traffic with just                                 one environment variable turning on yeah                                 I will                                 I really like it so uh if this is not                                 the good thing then I don't know what                                 but could be the good thing in the                                 Indies dr. Campos based approach but                                 what is the best thing yeah it's it's                                 very powerful then we use it a lot of                                 times during the development or just                                 provide the documentation for the new                                 features but the problem is that it                                 couldn't work with multiple nodes right                                 that's the problem that's the biggest                                 problem we need a real production                                 cluster so what can we do yeah we can                                 start with kubernetes everybody know and                                 everybody knows that kubernetes is the                                 new unicore so we can just try it out                                 but currently we would like to                                 understand the question and compare them                                 based on the main categories so it's                                 very easy to win a competition if there                                 is there is nobody else in your category                                 so I just created somebody else to to                                 compare them with kubernetes and this is                                 a Heshy curve stack based approach well                                 it's more like a do-it-yourself approach                                 because Hoshi Corp is a company who who                                 provides multiple smaller application                                 which are very powerful but you need to                                 assembly or you need to create your                                 solution manually and and just adjust                                 all the components how many of you use                                 console or I don't know mold anybody                                 here ok so one of the one of the famous                                 product is console it's a service                                 discovery server but if you have never                                 seen it you can imagine it ISA as a key                                 value store actually it's a little bit                                 more than for example if the key is a                                 service name and value is an IP address                                 then you can retrieve the information                                 even over a DNS interface so it could                                 work as a DNS server what is an other                                 application again you can imagine it is                                 a key value store where the value is a                                 secret so it's some kind of secret                                 management but it has also some advanced                                 capabilities authentication                                 authorization and dynamic secret                                 generation and Nomad is the scheduler                                 it could run application on a note so                                 that's that's our situation currently                                 right so it's more than one know that                                 that was our problem so and we have a                                 nomad agent did say go long the                                 applications it's very easy to start and                                 my problem is that I would like to start                                 a container somewhere I'm not interested                                 where it will be started it will be                                 started somewhere by the scheduler by no                                 man ok and maybe I need four for                                 container and it will be sorted when                                 there are available capacities ok so                                 this is the Hadoop cluster I would like                                 to start five data node and one name                                 node the master component and I asked                                 know much to start it somewhere I don't                                 know where it will be started so it                                 depends from the available capacity so I                                 don't know how I can set that I need an                                 a data node at every node but the name                                 not really started somewhere okay what's                                 the problem with that                                 yeah the first problem is the networking                                 by default the docker provides an                                 internal networking and no specific                                 networking so between no nodes there is                                 no connection so one worker not here                                 can't see the other worker node unless I                                 do some very magic port mapping which is                                 not very manageable so what can I do one                                 simple solution is just use the docker                                 host network docker hosts per network                                 means that oh doctor is very good I need                                 the docker accept the networking part                                 for the networking part I would like to                                 use the host the host interface of the                                 network you can see that my containers                                 are here and but the IP address and the                                 interface is exactly the same as in the                                 node there are some limitations that I                                 can start the same service twice on the                                 same node because I can't use the same                                 port twice but it's not a problem                                 because it's Hadoop I need just one work                                 I know that every node so it could work                                 and it could be very powerful                                 so container is just the unit of the                                 packaging I don't need to use all of the                                 features of the Dockers                                 ok so these are some other                                 problems which should be checked with                                 every continent so that how can I use                                 the what kind of network could be used                                 between the containers if it's a multi                                 node consider okay but still there is                                 another problem because that was all                                 good on configuration and here I have a                                 host name so what should be returned to                                 here what is the host of the name node                                 well I don't know because the name node                                 this red container will be scheduled                                 somewhere so it could be node                                           who knows                                 I don't know so we need some kind of                                 service discovery because we need to                                 configure the data nodes to access the                                 name nodes okay what can we do yeah and                                 this is the place where we can use                                 console we need just a console at every                                 node and Nomad could save the                                 information the scheduling information                                 to console Steven it's some magic                                 because before the start we need to                                 modify the configuration of the data                                 nodes but we have already already need                                 it right we do it's very similar we need                                 some modification on in the launcher                                 script and we can do it okay so let's                                 check it from closer I have we have the                                 Nomad we have a container we have a Java                                 process ahead of Java process in in this                                 case but it could be any other kind of                                 distributed application we have a                                 console and during the scheduling Nomad                                 we start the docker instance and save                                 the data to the service information                                 service name IP address so when Java                                 process is starting we need some                                 launcher a good example is a is the                                 console template which is an open source                                 project which could render the final                                 configuration because we have good old                                 application which can't speak with                                 console natively and we can render the                                 configuration and just start the Java                                 process after that and with this this                                 setup we have some other space for doing                                 other powerful things for example we can                                 restart the Java process in case of of                                 server change all right because we                                 started the head of Java process we can                                 listen on the service info in the                                 console and just restart the Java in                                 case of configuration                                 and we can also upload all of the                                 configuration to the console because                                 it's a key value store and the console                                 template can just download the                                 configuration render them based on the                                 dynamic dynamic servicing food service                                 name and an IP address and we can render                                 the configuration and save it to the                                 disk and start the Hadoop yeah it seems                                 to be a little bit complex right so                                 don't panic if you if you can't follow                                 it but that's the feeling of this                                 approach actually something like this so                                 it's a very do it yourself application                                 yeah                                 I can also show can it maybe it's better                                 to show from here oh I need a sudo for                                 this oh yeah                                 I have it oh I have it here with more                                 better resolution so that is just the                                 feeling of this approach this is the UI                                 of the console you can see that I have                                 multiple node there at the bottom I have                                 to work or node                                 those are cons or locks of the of the to                                 worker node so this is the console I can                                 see the services I have the name node                                 for example the name of the schedule the                                 Nomad - in that case I can check the                                 configuration all of the configurations                                 are uploaded to here okay this is the                                 HDFS configuration I can check it you                                 can see here is the magic right this                                 will be replaced during the startup ok                                 let's go forward                                 I have other configuration as well yeah                                 for example the log                                                    can just start oh i can modify this for                                 example the info level to debug level                                 and i can just save it that's the same                                 battle and but you can see here that I                                 just clicked to the save in the console                                 and it's automatically reloaded all of                                 the configuration so this is debug logs                                 are                                 so everything works without any                                 modification so I think it's a it's a                                 very very powerful tool                                 oh that's and the to do all of this yeah                                 this this is what I mentioned this is                                 the own change we can restore the                                 restore the application without any                                 modification in the application itself                                 so it's it's very powerful to use yeah                                 it's a little bit do-it-yourself but it                                 it could be very useful and other big                                 advantages of this approach that this is                                 not just cloud native but it's Hadoop                                 native because this is exactly the same                                 way how the Hadoop was tested in the                                 last I don't know                                                        we are using docker host network this is                                 exactly the same network as usually the                                 Hadoop is used so no problem with that                                 okay but let's compare it with we do                                 with the Hadoop with the kubernetes                                 approach if the Heshy Corp version was                                 the do-it-yourself the kubernetes is the                                 out-of-the-box version okay do you know                                 kubernetes or do you use kubernetes okay                                 half of them very good I have a other                                 full kubernetes training in                                            or maybe two minutes okay so what is                                 kubernetes the situation is the same we                                 have nodes right and I would like to I                                 would like to start the contrast and I                                 don't care where they will be started                                 they should be started where I have                                 enough capacity so it could be hundred                                 of nodes okay                                 another application I need two                                 containers they will be started or I                                 need three containers they will be                                 started the big difference is between no                                 mod and an kubernetes that normal is                                 just a scheduler it could start and it                                 couldn't do anything as for example the                                 network problem was not solved right we                                 solved it with the current network and                                 but with kubernetes the kubernetes can                                 see these containers as application so                                 they are tied together and it could                                 solve all of the other problems the                                 network problems                                 yeah it's Bugaboo but we have multiple                                 options to provide some kind of networks                                 between the containers we have solution                                 for the storage                                 that's the out-of-the-box out-of-the-box                                 we have all of these features for                                 example any kind of configuration or                                 secret could be mounted as a external                                 mount point or a file and will be                                 available from all of the container in                                 the in the same application and all of                                 the complexity of the kubernetes is just                                 all of these resources or most of these                                 resources are just definition or some                                 kind of rules to say that oh where this                                 kind of containers should be started                                 for example the daemons that is a rule                                 to start something on every node or I                                 could have a replica set when I say that                                 okay it should be started on just it                                 doesn't matter where but I need five                                 instances so all of this complexity is                                 just in fact just the rule to start the                                 containers somewhere okay seems to be                                 pretty cool right we finished that's the                                 sounds good everybody knows that it's                                 cool what's the problem with that well                                 there is no problem with that                                 exactly but the thinking of kubernetes                                 is different from the thinking of the                                 head tube for example this is a back-end                                 application I defined that I need three                                 instances and you just scaled up good I                                 have a front-end application and I need                                 some kind of connection between the                                 front end and the back end it's a good                                 old front-end application so it it                                 doesn't know if the backend is killed up                                 or not so there is an other kubernetes                                 resource side service which could which                                 works as the load balancer and the                                 problem here is that the service has a                                 network identity a DNS but the ports or                                 the containers itself has no network                                 identity what do we need for the Hadoop                                 yeah we need DNS or it could be turned                                 off but typically we need a network                                 identity because we solved all of this                                 replication problem manually so we could                                 be do it more effectively but we need                                 network network names between the                                 components because we are just managing                                 the replicas and                                 blowing all the stuff so it could be                                 started without DNS but it's not an easy                                 there are a lot of options just turn off                                 turn off turn off turn off then and                                 that's the part where the Nomad based                                 approach was Hadoop native because it                                 could work as before with DNS but here                                 it's more tricky yeah but we have we                                 have the stateful set where we have a                                 kubernetes container or port together                                 with a network identity and it could be                                 used and in fact this is the mostly used                                 resource tie for forehead opened and                                 stateful application a small problem                                 that we can't use the other resources                                 just the stateful set for example the                                 daemon said couldn't be used easily even                                 if it's so cool to store something at                                 every node okay so that was it was not                                 bad sake actually it just a difference                                 between the thinking of Hadoop and                                 kubernetes kubernetes tries to manage                                 all of the good old and dummy                                 applications and we have a very smart                                 distributed application but we can use a                                 restricted set of the kubernetes                                 resources and it could work very well                                 okay so what is the good thing I think                                 it could be defined in in multiple ways                                 but my favorite two things is the                                 ecosystem and flexibility so what what                                 does it mean exactly so let's do some                                 example exercise so do you know                                 promoters do you are promoters is a                                 cloud native monitoring tool and this is                                 something like this monitoring to call                                 Excel above metrics and we can just                                 check what's happening okay so I would                                 like to use Pro Mattel's together with                                 head of my default Hadoop doesn't                                 support Pro metal so what can we do well                                 there are tube providers is it's very                                 simple so it's a server application and                                 by default it poles the components and                                 each component should provide an HTTP                                 endpoint where the metrics are published                                 so we need to think one is a HTTP                                 endpoint here and                                 here and in all the data knowledge                                 Hadoop doesn't have it so we need some                                 magic and the other one what we need                                 that primate house should know where are                                 the HTTP endpoints you know that we have                                 a lot of containers I don't know where                                 they are started so flexibility and                                 ecosystem a start with the ecosystem but                                 I mean on ecosystem that it's something                                 like the get you can start you can't                                 create a new developer to we don't get                                 support I guess right because it is it's                                 it's supported everywhere it's it's very                                 widely used and I think sooner or later                                 it will be the same for kubernetes so                                 the ecosystem is that we have a lot of                                 tools and more and more tools support                                 the kubernetes environment for example                                 comatose because it's it's under the                                 same cloud native foundation it's it                                 supports natively the kubernetes so it                                 can just retrieve the available services                                 from the kubernetes api and check where                                 are the HTTP endpoints are started this                                 is something like this so this is a                                 kubernetes service or definition you                                 don't need to understand all of the                                 things you need to just find difference                                 between the two slide so that's the good                                 thing we need just add two annotations                                 and because pro men-tel's is listening                                 in the kubernetes api if something is                                 started with these two annotations it                                 will be polled                                 automatically so this is the echo system                                 and it's not just the promoters so a lot                                 of other applications and just newer and                                 newer application we will support this                                 kind of we support kubernetes in this                                 way ok so let's go forward still we have                                 a problem                                 so promoters knows that where the name                                 node and data node will be started and                                 where the HTTP endpoints will be started                                 but we have no HTTP endpoint right                                 because it's Hadoop Hadoop has DMX no                                 HTTP endpoint what can we do yeah we can                                 modify Hadoop but thats part is the                                 flexibility you know ecosystem and                                 flexibility so until no I said in the                                 coop                                 natives we have containers that the                                 containers are started yeah that was a                                 little bit high level so in fact we have                                 no containers but boats boats                                 are the basic unity in in the kubernetes                                 world and yeah it contains one container                                 and a few other things for example the                                 volume - volume definition that for                                 example which secret should be mounted                                 to the pod and usually usually one pod                                 is one container but in some special                                 cases we can put two containers in the                                 same pod so two containers will be                                 started in the same host and and it's                                 not just started in the same house but                                 they could see a they can share a lot of                                 things for example the same network                                 interface will be used the same volume                                 or even the same processes so it's a                                 it's an alpha feature but we can turn it                                 on and both of the containers with seed                                 the same processes so this is the                                 sidecar pattern and imagine that this is                                 my good old hadoop application this is a                                 sidecar application which will be                                 started it's an other container and it                                 just check the processes and it will                                 start an HTTP server and publish all of                                 the gmx interfaces and publish all of                                 the gmx monitoring information from the                                 java process so it can check the java                                 process get all of the information and                                 publish here OPA oh something has been                                 happened so again this is this is just                                 two lines right so that's the                                 flexibility so it's it's very easy this                                 is just one example there are other                                 examples to check the kubernetes api and                                 use operator but this is an example that                                 just with two lines without modifying                                 the good old hadoop everything is                                 published to primate house without any                                 any other change and it works not just                                 for the Hadoop this works for every java                                 application so i think it's a it's a                                 pretty powerful thing and back to the                                 ecosystem that ecosystem also means that                                 yeah it includes the fact that it's also                                 a common language so even now most                                 the cloud provider supports kubernetes                                 so it's very easy to run the application                                 the same way on pram or in the cloud                                 okay so we did it I think that's the                                 that's the final label for for                                 kubernetes and I didn't mention all of                                 these things but most of them are out of                                 the box included or we have a external                                 tool in the ecosystem such like the ham                                 which is some kind of package management                                 the config map is an other kubernetes                                 resource that we have this small booty                                 host support with with pluggable this is                                 a pluggable interface and there are                                 multiple options to use network between                                 the containers this is just one set so                                 you can you can do it multiple way for                                 example you can use console for the for                                 the configuration or you can use docker                                 host Network to achieve a more help                                 Native approach so there are just                                 elements and you can use it in multiple                                 ways but what I would like to say that                                 if we are comparing the freezers or the                                 or the washing machines before we buy it                                 then we can compare the containers                                 environment as well and I think these                                 are the most important questions which                                 should be answered somehow anyway ok                                 that was the that was the summary so                                 don't bother checking the label the                                 other one is that the containerization                                 can help a lot so we saw that with this                                 just with three lines or four lines we                                 got some monitoring and this is the this                                 is true for local local action for                                 example it's just a few lines because a                                 lot of tools are available for the                                 containers environment and especially                                 for for kubernetes                                 then then it's it's I believe that it's                                 more effective to use this kind of                                 applications in kubernetes and third one                                 is just it's the question is that how do                                 Hadoop is Hadoop cloud native or or not                                 so it depends but I think it's almost so                                 Hadoop is designed to be and distributed                                 application so it's very easy to start                                 containers there are some limitation for                                 example this DNS in thingy which could                                 be improved but they they are not big                                 architectural change it's just small                                 changes but I think we need to add to                                 Hadoop okay that's pretty much all this                                 is my availability you can check out                                 almost all of the codes not in a very                                 well-documented format but you can see                                 my work all of these docker images and                                 kubernetes definition and I will have                                 another talk tomorrow about I think they                                 I think the title is the same but it                                 will be more about Hadoop and Hadoop                                 ozone and Hadoop HDDs and how Hadoop                                 could provide some kind of persistent                                 storage to kubernetes clusters so that's                                 all is there any question                                 [Applause]                                 first of all very good talk very visual                                 so I like it very much um about the                                 sidecar approach about having Prometheus                                 matrix is a function of having the                                 containers in the pod share processes                                 the Alpha feature as I mentioned have                                 you looked at push gateway or primitives                                 it's a server that you can push metrics                                 and have it exposed since parameters of                                 okay so although also problem also                                 possible to use push but the native mode                                 is just the pool model so usually pro                                 may tell you know easier to use the pool                                 model yeah so that's why the second                                 container the second image that you have                                 like gmx sidecar sorry I'm making you go                                 back to white yeah yeah yeah in where is                                 it it's in exercises I think it's a Nemo                                 configuration kind of so I in fact it's                                 more I simplified a little bit story                                 yeah but if if you are interested                                 oh that's somewhere yeah things forward                                 forward                                 okay in fact you know this is there is a                                 Java attachment API and what's happening                                 here in my container that is checking                                 all of the processes in if there is a                                 Java process a new Java agent will be                                 attached to the Java which contains                                 internally there is this code this is                                 exactly the almost the same as the Java                                 parameters exporter so it just reads the                                 Duramax and will be exported but in fact                                 the HTTP here but I had simplified a                                 little bit story it also could be done                                 with a different way to just read the                                 jam X and publish from here but that's                                 the that's the how it works currently                                 awesome                                 but if ten lines of bash code actually                                 so it's great thanks yeah                                 yeah thank you for the talk questions so                                 you at as a statement or maybe also as a                                 question at the end so a tube as being a                                 first-class citizen and a containerized                                 MIT so my question would be so if I have                                 now I took running have gone there and                                 deploy my naps in that environment so                                 isn't that then a container in a                                 container so why wouldn't I am thinking                                 of pouring my young EPS so that they are                                 then native to kubernetes it's okay I                                 think it's more a it's a complex                                 question but it's it's mostly about here                                 on right the scheduling part then what                                 about the scheduling and yeah that there                                 are two kind of scheduling in in yarn                                 one is just scheduled native MapReduce                                 application another one is a scheduling                                 docker I think it's it's it's a                                 reasonable way to run MapReduce jobs                                 inside at the container in your but for                                 example the SPARC follows another                                 approach SPARC natively could be                                 scheduled jobs on on kubernetes so it's                                 an under approach I think currently the                                 yarn yarn has better scheduling than the                                 kubernetes but I think the kubernetes                                 ecosystem it's it's more advanced so                                 there are multiple options and I can see                                 in the future but one option could be                                 just yeah I think the revalue in in yarn                                 is the scheduling part and not running                                 the containers I think running counters                                 that could be done in a more effective                                 way would kubernetes but this is my                                 personal opinion so thank you yesterday                                 there was a talk by a Dunning who                                 mentioned also kubernetes and one of the                                 key problems he touched there is storage                                 so where do you put the data in in this                                 kind of setup in yeah there are so in                                 this where is I need a Hadoop cluster is                                 this a Hadoop cluster oh let's see                                 this is a new cluster oh not this one                                 this one so by default the Hadoop could                                 so there are different answers for                                 different applications but speaking                                 about Hadoop we have the data nodes who                                 can the data node data nodes can handle                                 all of these storage questions so by                                 default the easiest way just use local                                 storage here because anyway it doesn't                                 matter if it will be lost or not because                                 we have a replication mechanism by                                 Hadoop between the now nodes so we don't                                 need any network storage we need fast                                 local storage even SSD or something like                                 this and one interesting thing about the                                 new head of sub-project for example the                                 Apache Hadoop                                 hdds and Apache Hadoop ozone that the                                 plan is that this storage cluster could                                 provide storage for the MapReduce or                                 small job could provide a s                                              store and could provide network storage                                 for the other container similar to the                                 NFS so the currently it's use ice cozy                                 but later it could be improved so it in                                 that case you will have one storage                                 cluster inside kubernetes and you can                                 use it as a HDFS store you can use it as                                 an object store and you can use it as a                                 kubernetes native block store by add                                 from other containers at least that's                                 the plan                                 okay thanks Martin we anyway run out of                                 time                                 yeah yeah thank you very much                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Fs-zcR-sOJY


