Title: Berlin Buzzwords 2018: Håkon Åmdal – 800 mio. IP addresses to coordinates using Kafka Streams #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Håkon Åmdal talking about "Translating 800 million IP addresses to coordinates each day using Kafka Streams".

The Schibsted Data Platform is the global processing hub for data in Schibsted, and we receive roughly 800 million user behaviour events from more than 40 sites worldwide each day. The Data Platform’s responsibility is not only to collect, structure, and index the incoming data, but also add extra value by adding additional information to the events, known as enrichments.

To offer targeted advertising based on location, the Data Platform enriches all incoming events using an API that will translate IP addresses into coordinates. To do this in real-time and at scale, with sub-second latencies, we utilize Kafka and Kafka Streams.

In this presentation, I will introduce Apache Kafka, Kafka Streams, the Kafka Streams DSL and the Processor API to explain how it can be used for branching, caching, bulking, asynchronous HTTP lookups, and joining. I will also talk about experiences related to operations, performance, and scaling.

Read more:
https://2018.berlinbuzzwords.de/18/session/translating-800-million-ip-addresses-coordinates-each-day-using-kafka-streams

About Håkon Åmdal:
https://2018.berlinbuzzwords.de/users/hakon-amdal

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so I'm here to tell a story a story                               about how we use utilize Kafka and Kafka                               streams in our company to solve                               interesting problems and interesting use                               cases but I think it's useful because                               this is the first time I'm at Berlin                               buzzwords whatsoever so it's interesting                               for me to know who is the audience so I                               have some categories here I was hoping                                people could raise their hands when they                                feel they belong to that category I use                                Kafka but I need to know if Kafka                                streams is something for me okay cool I                                think you're in the right in the right                                room I use other streaming frameworks                                like flint spark screaming or Samsa but                                I'm not particularly familiar with Kafka                                streams some cool I use Kafka streams                                and I'm just here to learn how you guys                                did it nice                                I'm really hoping to meet you guys after                                the presentation too so course come see                                me we have a lot to discuss who's new to                                Kafka okay I have something for you guys                                too so today I'm going to to tell you a                                story I need to provide some context to                                you because I think it matters because                                we you need to understand the scope of                                the problem we're solving and I'm going                                to give a brief introduction to the                                technologies we're using which is Kafka                                and Kafka streams and then we're going                                to build an application here on the                                slide thank you take one line of code at                                the time and I promise you I present                                                                                                        with the Kafka streams API that actually                                solve this solve this problem and then                                we're going to talk a little about how                                to put our app into production how we                                how we put the app into production and                                how like sort of the choices we had to                                make and the problems we ran into and                                then we'll have a really short slide                                about higher the conclusions and what                                we've learned during this presentation                                so my name is fokin humble I'm a data                                engineer in shipstead I'm sorry I don't                                work for confluence even though I have                                this amazing Kafka t-shirt                                and this is as I told you this is the                                first time at Berlin best words and I'm                                particularly found of stream processing                                and you probably noticed that during                                this presentation it's a little hard                                talking about shipstead in Germany                                shipstead isn't a very strong brand in                                itself but it has a lot of local brands                                and I noticed there are a lot of people                                from France there and we do own Leben                                quoi so ship that owns a lot of media                                sites they're particularly big in in                                Norway and Sweden and we do also own                                these marketplaces for classified ads in                                Sweden Norway France Italy Spain and so                                on and we also have some other growth                                concepts and all of these sites they                                implement the shipstead custom tracker                                we have a tracker for iOS for Android                                and for the web trackers and these                                trackers they generate sort of this                                clickstream data we're like you they                                send a signal every time you click a                                link or view an item on these sites to a                                data collection service which we wear                                all the events end up in an AWS key                                Nessie stream here we split the stream                                into two different pipelines we have a                                more traditional batch processing                                pipeline where we run batch jobs mostly                                spark some other stuff as well where we                                use AWS s                                                              for this and the other one is the                                streaming pipeline it comes with some                                different delivery guarantees it's much                                more timely than the batch pipeline of                                course and and this is where sort of                                this is Kafka by streaming platform                                which I'm going to sort of talk about                                today and we do I mean I've seen like                                some sponsors here they brag about like                                half and trillion events each day we are                                not at that scale but we're fairly large                                we collect somewhere between                                            million events each day and that's on                                average so at peak hours it's around                                  million events during the evenings in                                  million events                                each minute and that's during the                                 evenings in in Europe and even though                                 but this number is large enough that                                 we're talking about we need solution                                 that scales we cannot run this on a                                 single thread on a single machine we                                 need a distributed system to solve                                 problems in this scope we use the                                 streaming platform for a lot of things                                 in shipstead and I could talk on and on                                 about all the cool things we do we but                                 to scope this talk we're going to talk                                 about targeted advertising in shipstead                                 because what we do is the end user will                                 enter a page and will get signals from                                 that user entering that page we use our                                 streaming platform to do some some                                 transforms some filtering and sort of                                 forward that data to use your                                 segmentation engine where a user based                                 on its behavior is sort of gets its                                 gender and age and we have some other                                 models running as well get that                                 predicted and that will in turn result                                 in targeted ads for that user so it's                                 really important that this happens fast                                 ideally we want the user to enter the                                 front page say of a news page and by the                                 time we clicked the article there there                                 will be a relevant ad displayed to him                                 at the very next page view and and                                 needless to say this is this is big                                 business this is really really big                                 business so whatever downtime we have                                 whatever data quality issues we have                                 whatever completeness issues we have                                 will affect the total revenue of                                 shipstead ok so the business guys and i                                 mean it's pretty obvious that if you                                 wanna sell stuff location is a very good                                 thing to know about a user because then                                 you can have some location-based                                 targeted advertising so a team in                                 shipstead that's not my team but this is                                 used throughout shipstead for several                                 services we have this location API where                                 you can                                 it's an HTTP service where you can add                                 an IP address or even IP address and                                 coordinates as input and get then get                                 like the coordinates and the reverse                                 geocode components like zip code and                                 country in return and I talked with                                 someone yesterday I think he's here in                                 the room he said why don't you just keep                                 all of this in memory and the short                                 answer is then it wouldn't be as                                 interesting standing here and talking to                                 me today because part of the complexity                                 comes from this API the long answer is                                 there are some some logic there we do                                 run our own models we do need to look up                                 reverse geocoding service that makes it                                 and we use it throughout shipstead so                                 that makes it like it made and that is                                 why it's sort of behind an HTTP API so                                 the idea here is to sort of make                                 something that's something being the the                                 question mark the blue box with a                                 question mark within our streaming                                 platform and I mean we could put                                 everything into the use of segmentation                                 engine but then we would only have this                                 data for only user segmentation if we                                 sort of move it into the central                                 streaming platform of shipstead then all                                 of the the users of the streaming                                 platform would have the benefit of extra                                 enrichments in their events and this is                                 sort of what we're going to to dive into                                 today there are some requirements                                 contradictory requirements even I mean                                 latency is paramount here with the                                 events need to to come in a timely                                 fashion because that will affect the                                 performance of the targeted ads and it's                                 sort of a very crucial part of the event                                 - you like to have this piece of                                 location information and these are                                 contradictory because you can't really                                 have both if the API go down but the                                 essence here is you need a button you                                 can tune the trade-off between the two                                 of them as I told you guys earlier we                                 have a substantial amount of events as                                 not a lot but still a roughly                                 billion events each day is something we                                 need the system we need our system to                                 handle that perhaps more importantly                                 we're in the news business and breaking                                 news do happen this is from inner region                                 in the region site and you can see the                                 traffic increases by five times during a                                 period of two minutes and that that's                                 when a push notification goes out for a                                 particular breaking news event and it's                                 the location API itself because it                                 doesn't scale to look up one event at                                 the time you need to book the incoming                                 events and look them up together you it                                 does it's rate limited so we need to                                 apply some back pressure on the client                                 and it can't be slow and it it can fail                                 okay so let's talk about Kafka                                 Kafka is based on a very simple I'd say                                 a very simple data structure called the                                 log I'm not sure the exact definition of                                 the log but it's something where writers                                 append to you can have multiple writers                                 and then you can have a set of consumers                                 consuming this log in sort of whatever                                 speed they want and they normally start                                 from the beginning and process the the                                 events Kafka offers something very                                 similar to a log or I mean it's it's                                 still a log but they call it it's a                                 logical name called the topic where each                                 topic is a set of independent partitions                                 where sort of the semantics are that                                 have rights within writes and reads                                 within the partition is ordered but in                                 between the partitions there is no                                 particular ordering you cannot guarantee                                 ordering and the reason why you split it                                 up in partitions is is because that's                                 how you make this technology scale                                 because it doesn't work to only have one                                 single log for for a system that                                 produces one billion events each day a                                 log entry in Kafka consists of three                                 items you have a key that can be pretty                                 much anything you want as well or any                                 bites you want I mean it's a value                                 contains the value which also has the                                 same possibility to to contain whatever                                 byte sequence you want and then there's                                 a timestamp and I'll come back to all of                                 these three later in the presentation so                                 Kafka provides these topics these sort                                 of distributed logs at scale and they                                 offer replication for these topics as                                 well and and you normally run this is in                                 a Kafka cluster or I mean you have to                                 have a Kafka cluster to run Kafka I mean                                 and there are a set of applications that                                 sort of uses this Kafka gesture you have                                 producers that                                 some produce and write data to the                                 topics consumers that will consume data                                 maybe build up some internal state or do                                 some processing there's a separate class                                 of of applications that that you can                                 call connectors where you can sort of                                 connect the changelog from your database                                 or the other way around to keep Kafka                                 and your database in sync and then you                                 have this dream process your apps which                                 I would reads and writes to the Kafka                                 cluster and in the same operations and                                 and they can have more cool stuff like                                 joins aggregates group eyes and that                                 kind of thing and this is where you find                                 Kafka streams although so the topic is                                 the topic and the way they distributed                                 the way they scale is sort of that the                                 main thing about Kafka that that's sort                                 of the brilliance of it but there is one                                 one other concept that at least in my                                 own opinion is equally important and                                 that's the that's the abstraction of a                                 Kafka consumer group because in our case                                 on the slide here we have a consumer                                 Kafka consumer that consumes from                                    partitions and they this sort of and you                                 have if you have three and three threads                                 stay even out so they process four                                 partitions each however it could be like                                 this event this breaking news like the                                 event volume increases and you're going                                 to need more power to process this and                                 you can actually just start another                                 consumer thread and that doesn't even                                 need to be on the same machine that can                                 be an entirely different machine                                 entirely different container or anything                                 and Kafka will behind the scenes                                 redistribute the load so and this way                                 you can scale your application as long                                 as you have enough partitions                                 okay so let's that's sort of the better                                 the context that the Technol like the                                 the business case and we talked a little                                 about like the technology we're going to                                 use to solve this problem so let's start                                 developing the actual application that                                 will do this                                 these lookups for us and as you guessed                                 we're using Kafka streams to do that so                                 very very simplified the original set up                                 looks like this we have the firehose                                 containing all of the events that we                                 ingest into the pipeline and we have a                                 filter and transform component that will                                 filter the events and only forward those                                 that are relevant to the advertisement                                 team and the segmentation engine yeah                                 and this app also it's not shown here on                                 the slide it probably should this sort                                 of is the main workhorse of the                                 streaming data platform so this this                                 filter and transform component there's a                                 lot of the work in the data platform and                                 it writes to several topics and have                                 many many use cases so we kind of want                                 to leave that component alone so I mean                                 but that's the brilliant about                                 brilliance about Kafka you can have                                 multiple consumers consuming from the                                 same topic so it felt safe to create                                 something new something that didn't                                 affect the already running system I mean                                 that affects advertising but a lot of                                 other teams as well and we wanted it to                                 look up the API and create a known and                                 known topic with the location data so to                                 set up a Kafka streams application you                                 need to come config parameters you need                                 to set the application name or                                 application ID and you also need to                                 specify the address of one of the Kafka                                 brokers in your cluster then you create                                 this case dream builder class and this                                 is where you create the actual                                 application the actual topology and I am                                 multiple slides on that later but once                                 your topology is ready you create the                                 Kafka streams instance and you start it                                 and then you                                 it's a good practice to add a shutdown                                 hook as well so you can handle shutdowns                                 gracefully and that's it and this is it                                 doesn't require any except for of course                                 having your Kafka brokers up this is a                                 standalone JVM application you don't                                 need any particular infrastructure you                                 don't need yarn or anything to run this                                 it's just a library and that's sort of                                 one of the things I really like about                                 Kafka as cough cough cough cough dreams                                 okay so let's do the first attempt to                                 read the firehose we set up a stream                                 where we where we specify the name of                                 the topic and we specified specify                                 something called the sword a sword is                                 short for sterilizer deserialize ER or                                 something like that and and that sort of                                 is is how Kafka knows how to to sort of                                 translate the bytes on the topics into                                 Java objects and vice-versa so a Kafka                                 stream comes with a built-in cert for                                 Strings but it doesn't come with any                                 JSON support and we use Jason and                                 shipstead I'm sorry so we have to create                                 our own custom jason sword to solve this                                 problem and doing so is fairly                                 straightforward you implement an                                 interface where you need to override in                                 short you need to override the                                 deserialize and serialize methods in our                                 case we use jackson to to perform this                                 for us and yeah that's it we did we                                 specify the code how to go from from                                 bytes to jason nodes and from jason                                 notes to bytes so this is how it looks                                 like now and i can tell you already this                                 doesn't really work I don't expect you                                 guys to read the entire slide here there                                 are some hints of what happened it's                                 some failed to deserialize Jason parse                                 exception pending shut down that                                 so it turns out I had this was my local                                 development moment I had some garbage in                                 my topic that didn't really parse as                                 Jason and you guys might think well but                                 in production don't you always have                                 Jason sort of but I mean you do get                                 garbage in your pipelines and we need to                                 be able to handle that so what we need                                 to do is sort of take back control so we                                 want to handle the parsing ourselves                                 within within the topology so we instead                                 of using this Jason Jason node surd we                                 use a byte cert and then we map the                                 values ourselves but this time we wrap                                 it in a try I mean we do a try-catch                                 operation the Scala functional way and                                 we only include those values that                                 actually parses and so we add a filter                                 and then we we sort of get the results                                 of the successful parts parcels and so                                 here I introduced the map values which                                 you can call in the stream and also also                                 a filter but what they say it when                                 there's a filter and there's a map there                                 is also a flat map and luckily kafka                                 stream provides that to us so we made                                 that a little simpler so now we have a                                 stream of string keys and Jason old                                 values we need to to work with we need                                 to turn these these jason old into                                 request objects that looks like this                                 i told you i was going to show you all                                 the lines of the code i'm not going to                                 show you this it doesn't really matter                                 it's adjacent node to request function                                 that will sort of extract the relevant                                 values so we will map all of these these                                 jason nodes into into request objects                                 using my values and not now i'm going to                                 introduce to you a new function on the k                                 string class which is called peak                                 because I'm interesting in how many of                                 these events we can use so peak is a                                 very nice function to handle                                 side-effects that lets you inspect each                                 like inspect like look at every single                                 element with AK without actually                                 modifying them so this is perfect to to                                 increase metrics for instance to to                                 measure the performance and the quality                                 of the pipeline and then for all of the                                 request objects that actually has a                                 value we do a flat map and we end up                                 with the stream of requests okay                                 let's produce the output so based on the                                 stream with requests we hand it over to                                 a transformer a location API transformer                                 and we're also curious about how this                                 transformer performs like we need to see                                 how many coordinates were successfully                                 looked up and how many like we didn't                                 find and when we're done we have a                                 similar inverse function where we met                                 from from response objects into JSON and                                 then we put it back on the Kafka cluster                                 to the location data topic and now you                                 guys are hold on hold on what's                                 happening here what is this do you                                 remember that I told you we cannot look                                 up one single event at a time we need to                                 do some bulking of requests well so it                                 turns out in order to do bulking we                                 cannot easily use the the sort of                                 functionality that the Kafka streams DSL                                 provides because Kafka streams DSL which                                 I've showed you so far looks very much                                 like how a bright spark code how you'd                                 write in functional programming and so                                 on but in order to do the bulking and                                 work on multiple events items at a time                                 we need to use something called the                                 Kafka streams processor API and the                                 processor API is quite large in itself                                 but one of the interfaces in this                                 process right API is something called                                 the transformer the transformer has four                                 function it has an initialize and a                                 closed                                 function and then it has two methods one                                 it's called transform and that one is                                 called every time on each single event                                 in the stream and the second one is                                 called punctuate which is a method that                                 is called periodically and this is what                                 we're going to use to actually trigger                                 requests to the API so to set it up we                                 store the processor context that that                                 comes into - through the unit function                                 we also schedule punctuate to be called                                 like we just have to pick a number so we                                 picked                                                                   will be called every                                                  and there are also two more things here                                 in the slide that are there's a buffer                                 which is where we'll keep our events and                                 tickle will look them up and there's a                                 location API client which is I mean it's                                 it's out of scope to show you how that                                 works                                 it's an HTTP client where we can that                                 inputs request objects and return                                 response objects the transform function                                 is super simple because every time and                                 we see a new event in the stream we want                                 to add it to the buffer and nothing more                                 we could have returned the value here                                 but we don't want to so that's why we                                 returned no we the only thing we do is                                 we add this tuple to the buffer the fun                                 part is mr. punctuated method because                                 here this is Scala by the way I guess                                 most of you guys already figured that                                 out we take all the requests in the                                 buffer and we group them in sort of the                                 size that is optimal for the API which                                 is in our case is                                                    boat Locker lookup operation and then we                                 do some let me sort of look up all the                                 requests we await the results and then                                 we look up the corresponding keys for                                 for all the requests we have stored and                                 then we call context forward with our                                 response and that way we're emitting                                 data down to like                                 the downstream elements however                                 unfortunately our API only returns                                 successful lookups we need to sort of                                 iterate over all the keys we didn't find                                 and where we omit like these non values                                 because those are really important to we                                 don't wanna we want to forward as many                                 events from this transformer as we got                                 in after we did after we've sort of                                 committed sorry after we forwarded all                                 the events we're supposed to forward                                 then we commit everything and clear the                                 buffer and we return nothing because                                 we've already returned what we wanted to                                 return and I mean this doesn't really                                 matter what's on this slide here except                                 for what I've sort of marked here and                                 that is you have the possibility to                                 forward an arbitrary amount of events in                                 this in this using this process for API                                 and that's and that's how we solve our                                 problem okay just a small recap this is                                 actually except for the more advanced                                 transformer this is how our application                                 looks like and honestly I think it's                                 quite easy to to reason about that this                                 sort of looks right and this is like the                                 development time of such application                                 isn't that long there's one more problem                                 though and we haven't joined the streams                                 like its location data without anything                                 else is useless so let's join Kafka                                 stream supports joints what we do is we                                 create two streams one for location data                                 and one for this relevant user data we                                 call the joint operation on the stream                                 and then we specify the stream we want                                 to join with and this is an inner join                                 that means you need events from both                                 streams in order to to join them and                                 then you specify the function they'd                                 like the joint function in our case we                                 just mutate the JSON object we set the                                 location in the user data object and and                                 inject location event then we specified                                 join window I have a slide on that                                 coming and right next after this so I'll                                 explain it then                                 and then actually we need to specify the                                 search for this join and the reason that                                 the first one is the the search for key                                 and the second the two after that is                                 third for that two values in the stream                                 and the reason why we need to do that is                                 because join is stateful operation so it                                 actually materialized the join in a                                 change log topic behind-the-scenes so it                                 needs to be able to serialize and                                 deserialize these values and then we for                                 relate to this user data with location                                 topic you about join Windows is I'm not                                 sure if you remember from earlier                                 presentation I said each cough country                                 has a key a value and a timestamp so far                                 we've only focused on the value now it's                                 time to like but the join uses the key                                 and the timestamp because of course two                                 keys we leave our keys unchanged during                                 this processing we never touch them so                                 and the keys will be the same for the                                 for both the load like the relevant user                                 data event and also the corresponding                                 location data event however for                                 something to qualify for a join the key                                 obviously needs to be the same but you                                 also need to be within a specific time                                 window so that means the location event                                 needs to sort of arrive within                                    seconds either                                                       which is impossible in our case no it's                                 not by the way or                                                     needs to come in between that window                                 else it doesn't really qualify for the                                 join and the reason why we picked this                                 window is like if we don't see this                                 event after                                                           usable for advertising purposes we have                                 some okay so we now let's put everything                                 into sort of deploy it put it into                                 production and I'll tell you guys what                                 works and what doesn't work at least in                                 our experience so first we put this                                 thing into production and I can't say                                 nothing more that this really worked and                                 it worked quite well I'd say this is the                                 latency                                 measurements with the                                                 punctuated punctuation interval we had                                 the latency between                                             milliseconds extra on each event for                                 this turns out I P addresses tend to be                                 sent to see a lot of the same IP                                 addresses and like the location API will                                 always return not always but for the                                 course of                                                             return the same values for a specific IP                                 address so we implemented a named memory                                 cache using guava and that sort of                                 reduced the latency to                                                 perhaps and then we reduced the                                 punctuation interval to                                                  and now we're down to                                             between                                                                and that's additional to what they I                                 mean this is this is the time it takes                                 for the app I showed you from when it                                 ceased an event like it picks it up from                                 the stream until it writes it back to                                 the stream and I said this is this is                                 absolutely good this is a very decent                                 result we needed to implement the join                                 code I showed you as well and sort of                                 this was a very simple piece of code so                                 well why not use this workhorse of ours                                 to to do to join their great idea this                                 is this is a graph showing the event                                 volume that we work like that is being                                 processed at a time we deployed the join                                 and like the event volume yeah it                                 dropped dramatically and this is very                                 interesting and this is something you                                 should be aware of - when you develop                                 Kafka streams application because what                                 happen it's like we we do this red black                                 style deployment where we have an old                                 cluster and then we deploy a new cluster                                 I mean they have more threads than three                                 each of course                                 okay so when we deploy a new new cluster                                 the kafka consumer group or like behind                                 the scenes will try to distribute this                                 is the fire hose that is represented                                 with these green arrows to the new                                 cluster and I mean this is everyone's                                 happy about this however the new cluster                                 will also start processing the new input                                 topic which is contains the location                                 data and so far so good what happens now                                 is the new cluster see so there's this                                 consumer group it doesn't really consume                                 this new location data topic so it tries                                 to hand it over to the old cluster and                                 the whole cluster it doesn't have any                                 cold even so this thing yeah it goes                                 down and you think we should be fine                                 by sort of you hardly like I mean                                 removing this old cluster completely and                                 sort of now only the new cluster is here                                 but this is super unhappy to about not                                 being able to that the other cluster                                 died so now we have nothing and this is                                 sort of what happened                                 so we did a new attempt we this time we                                 had a control we scaled down our entire                                 old cluster first and we scaled up the                                 new one and that word for ten minutes                                 and yeah we gave up after that we're not                                 touching that up anymore with anymore                                 joints I'll come back to that later mmm                                 but we did see a very interesting fault                                 which I haven't seen in a while it was a                                 segmentation fault I'm pretty sure                                 that's not really what you know their                                 actual error is but we were maybe we                                 were lucky maybe we did some good work                                 figuring out what happened but it's that                                 the problem was with the joint window                                 because even though we have specified a                                 joint mean of ten seconds that doesn't                                 the joint is still has a retention time                                 of                                                                      sees an event it need to browse all                                 those perhaps they have they of course                                 have a more much more a much better data                                 structure than a linear search but still                                 it will have to keep all of those                                     million events in in the join window                                 or in the joint so you can actually                                 specify to reduce the join windows                                 substantially I set it to reset it to                                    seconds so that means after                                            like whatever whatever is in your joint                                 is lost and I mean this is fine for our                                 case and yeah we learned something new                                 also we're not touching this filter and                                 transform application anymore because                                 our team started getting really bad                                 reputation for not providing a                                 sufficiently good service so we                                 implemented the new in a new application                                 and yeah if it worked                                 performance I mean I used to be a little                                 happy about this performance I'm no                                 longer happy about this performance                                 because in what this slide shows is that                                 like the actual a latency of doing the                                 HTTP lookups it's it's very small                                 compared to the overhead of doing the                                 joints and so on and it turned out this                                 solution like we wanted a solution that                                 could handle errors in the location API                                 or in the like errors in the location                                 transformer itself by like splitting it                                 up having one app for joins one app for                                 location transformer but we didn't get                                 any of these flexibility at all because                                 I mean if this crashes then I mean this                                 could crash I mean this could crash                                 separately this location data topic will                                 go dry there will be no events and since                                 we specified inner join this thing goes                                 try to so we've sort of gained nothing                                 about having three applications here                                 another thing is related to cost I'm                                 trying to to show you guys some                                 proportions when it comes to the cluster                                 size because it doesn't matter how many                                 squares are within each application but                                 the important thing is there's                                 proportions so the main workhorse in our                                 streaming application that                                 trance transform the cafetorium                                 supplication would normally run with                                 about eight notes and the much much                                 simpler location API component would                                 have somewhere between three and five                                 nodes on average for however this thing                                 would have                                                             saw                                                                  keeping this join and running this is is                                 I'd say expensive so yeah we we run this                                 for a while and it worked but yeah we                                 fixed it we actually dropped doing the                                 join whatsoever so we would use the                                 location transformer to create a                                 relevant user data with location topic                                 directly from this location transformer                                 and that turn turned out to be much more                                 cost efficient and much more I had much                                 much lower latency as I showed you on                                 the previous slide and to tune this                                 trade-off between completeness and                                 latency we have a custom circuit breaker                                 here rather than trying to tune it at                                 the join so I think concluding this                                 presentation is we can do that in four                                 simple bullets I say that Kafka streams                                 or first sort of this straightforward                                 application development because it's                                 just an app that runs on your computer                                 and as long as you only use the Kafka                                 stream DSL it feels very familiar if                                 you're sort of used to other frameworks                                 like spark or if you're used to                                 functional programming I think it scales                                 pretty well for the basic stuff it's I                                 mean we have to get up to two million                                 events each minute and this application                                 handle it no problem you should perhaps                                 that at least this is something we've                                 learned not only from doing this project                                 but similar project is that the Kafka                                 way of doing things is to have many                                 smaller applications so if you want to                                 go all-in on Kafka streams you should                                 probably make sure you keep the cost of                                 deploying new applications running you                                 applications you should keep that cost                                 as low as possible and in general I'd                                 encourage some ya to be a little careful                                 when it comes to these stateful                                 operations because they work we've                                 proven that they work but they can be                                 very expensive and might not give the                                 flexibility that that you think they may                                 so by that I conclude my presentation                                 and open up for questions                                 hey thanks for showing your learnings                                 yeah when you think about the the nodes                                 and then I'm the application size that                                 is not that is the actual code                                 processing the streams not the casket                                 cluster that's the cold processing                                 littering the Kafka cluster is something                                 different that does                                 Casca streams have an impact on the                                 casket cluster sometimes it does because                                 it can start writing a lot of                                 intermediate topics and those                                 intermediate topics might not have the                                 replication factor that we wanted so                                 that can stall some of the applications                                 if a broker goes offline cool things and                                 I saw that you use fire hose on Kafka                                 fire hoses they a Douglas fir oh no fire                                 hose is sort of my name for a topic that                                 contains everything and the name of the                                 topic is fire hose thanks                                 other questions                                 thanks for a great presentation thank                                 you said that you had some problems were                                 the joints and I noticed that was                                 between two streams yeah and then you                                 fold it for that solution and try                                 something else do you do still use                                 joints between streams and streams or                                 streams and tables anywhere else we                                 don't and yeah no we don't and I tried                                 actually using global K tables for the                                 cash like to store the results for the                                 lookups that didn't work at all okay                                 cool Thanks you know why did you                                 conclude that you need smaller                                 deployments instead of multiple                                 topologies in the same because of the                                 issue with this you know the saw crash                                 and fire on my slides this thing yeah I                                 mean for this like these applications we                                 run in our streaming platform there we                                 don't want any downtime on this and in                                 order to add like multiple topologies to                                 an application you actually need to do a                                 full scale down before you scale up                                 again and that's the reason why we chose                                 like that thank you and and we also have                                 a set up in ships that we're deploying                                 applications is is fairly                                 straightforward                                 so that means it doesn't really add much                                 cost but it adds benefits only the                                 benefits                                 the last question in this last image of                                 the architecture with the circuit                                 breaker um just need to burn some                                 clusters first let me see this is                                 exactly so if if the location API fails                                 is the message still forwarded yes so                                 anything like it's very easy for us to                                 tune that circuit breaker because we we                                 can always like for every one we start                                 trying to look up and then we just                                 timeout and forward the original events                                 so I wanted to use the join to tune this                                 parameter but I couldn't get it to work                                 because the only applicable join with                                 was the inner join if you meet me later                                 I can tell you why a left join doesn't                                 work so by sort of moving that                                 responsibility into the transformer we                                 had a very easy opportunity to to choose                                 how long we would wait for the location                                 I'd be able to respond does that answer                                 your question thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Rxsz0hSG30A


