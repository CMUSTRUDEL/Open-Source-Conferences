Title: Berlin Buzzwords 2018: Philipp Krenn – ElasticSearch (R)Evolution — You Know, for Search ... #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	After the initial release in 2010 ElasticSearch has become the most widely used full-text search engine, but it is not stopping there. The revolution happened and now it is time for evolution. We dive into the following questions:

- How did leniency help the initial adoption, but why and how do we lean more on strictness today?
- How can upgrades be improved to avoid any downtime even when changing major versions?
- How can new resiliency features improve recovery scenarios and add totally new features?
- Why are types finally disappearing and how are we are trying to avoid the upgrade pain as much as possible?
- What are examples for some clever performance improvements?
- How can you shrink and (finally) split shards in a highly efficient way?

Read more:
https://2018.berlinbuzzwords.de/18/session/elasticsearch-revolution-you-know-search

About Philipp Krenn:
https://2018.berlinbuzzwords.de/users/philipp-krenn

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning everybody everybody's still                               awake                               let's see I have a lot of demos so there                               should be kind of some interaction in                               there so can I have some slides please                               maybe otherwise the demos will be very                               hand wavy                               so I will talk about the revolution part                               which will be very short and will then                                mainly spend time on the evolution like                                what has happened in the past few                                versions okay I did not touch any cables                                hey okay cool thank you so this is my                                official role then developer advocate                                I mostly do conferences meetups and just                                speak about the stuff that we do as                                company if you have any questions and                                and you either forget or you don't want                                to ask in public come to me afterwards                                or just use slide or just throw in many                                questions there if I have time at the                                end I will answer them on stage if I                                don't I will tweet out the answers so                                slide or do slash my twitter handle is                                where you can just end or add any                                questions and I'll try to add them or                                answer them at the end so starting with                                the revolution what was the revolution                                like where are we I use DB engines that                                come to say like ok this is where we are                                with data stores today who knows that                                site ok this is surprisingly small                                number so they have like that TOB index                                for programming languages they do the                                same thing for data stores so they                                basically have a ranking you can always                                argue and discuss is this the right                                ranking or how should this be for data                                stores and currently it looks something                                like this so you can see the relational                                databases are still on top but MoMA DB                                is the top no sequel datastore and                                elasticsearch is now on number                                        pretty happy about that so that was kind                                of the revolution and I guess everybody                                is familiar with what elasticsearch is I                                will spare you any introduction by the                                way the only system I think should not                                be in there is the one right below                                elasticsearch which is access and I I                                don't think this should be on any proper                                datastore list but it's still number                                  and we are very happy that we have                                finally overtaken access because that                                was kind of one of the goals we really                                wanted to achieve over time                                and probably it can fade out somewhere                                okay coming to the evolution what is                                kind of what are the trends what are we                                trying to do what are we up dating and                                doing so the first thing is about                                strictness which is basically when you                                are a young project you want to be very                                forgiving and you want to be easy to use                                and people just get started                                when you are getting more mature you                                kind of want to avoid bad stuff because                                bad stuff is not that bad when you're in                                development but it will be very bad in                                production afterwards so we would rather                                fail early and be kind of a pain early                                on then have like that big outage                                afterwards so what we have is we have                                these bootstrap checks did anybody run                                into bootstrap checks and had issues                                with those one not that many we have                                some of these checks these are just                                things where we know this will break at                                some point in production probably and we                                will not start up a node if you're in a                                cluster Abell state and will actually                                fail the node immediately and will not                                let it come up and I can actually show                                that I have three docker containers                                running and one of them is badly                                configured and you can see here I have                                an elasticsearch one node and this is                                the actual bootstrap check that has                                failed and you can see here so on the                                top line you can see we bound to a non                                local host interface and that's when we                                assume this is production mode you could                                form a cluster if you're just binding to                                localhost we assume you're in                                development mode and we will just issue                                a warning we will tell you okay this                                might need fixing but we will still let                                you continue as soon as you're in a                                cluster Abell state will actually fail                                 the node because we saw something that                                 is not right for example here you can                                 see the bootstrap check failed and we                                 did not activate memory locking even                                 though we requested that in the                                 configuration and also that                                 elasticsearch just stops we actually                                 stopped the node you're not able to use                                 it let's quickly fix that so I have                                 explicitly commented out these memory                                 locking settings let's quickly kill the                                 node I will show you my doctor set up                                 while the nodes are actually coming up                                 again so that's basically what I have                                 here is I have three elasticsearch nodes                                 and this is one of these                                 three elasticsearch notes that you can                                 see so it has the name elasticsearch one                                 I use some version in the background so                                 right now I'm using five six nine will                                 do a life upgrade to the upcoming                                     version so I have set some settings                                 basically the usual stuff since it's                                 just my laptop it's half a click of heap                                 and then I have the same setting for the                                 node                                                                     might need afterwards or that we come                                 back to is that I bind so elasticsearch                                 is on port                                                  elasticsearch one binds to nine thousand                                 two hundred one less research to nine                                 thousand two hundred two and three to                                 nine thousand three hundred three and                                 then I have cabana running as well just                                 to make the interaction a bit easier and                                 then I have my data volumes and that's                                 it so this is my entire setup and if                                 you're lucky it has started up in the                                 background already that's not what I                                 wanted so okay it's still starting up                                 and probably somewhere in here we have                                 all the notes started hopefully at some                                 point                                 let's see because there should not be                                 any logging okay cool                                 we have elasticsearch five running to                                 make them slightly larger is that large                                 enough for everybody to read or let's                                 simply change the form setting a bit                                 okay so we have the latest version of                                 five six running you can see we have                                 three notes altogether so we have                                 elasticsearch one two three                                 elasticsearch three is the master node                                 right now and my cluster is green and                                 healthy so let's go back to strictness                                 um the one strictness that we had were                                 bootstrap checks the other one are that                                 we're now checking configurations and                                 parameters and if you have any type of                                 in there in previous versions if you had                                 a typo would simply ignore that type or                                 in ignore that parameter which made                                 debugging very hard because even for us                                 it was hard you have a                                          configuration file you have a type of                                 somewhere and we'll simply ignore that                                 parameter this is probably not what you                                 want by now we will also fail if your                                 configuration is bad we will fail a                                 starter process or if you have a                                 parameter that we don't recognize we                                 will actually tell you instead of                                 ignoring it does anybody see the typo                                 here what is missing in that command yes                                 there is a an e missing here in type so                                 if you run that it will actually tell                                 you okay illegal argument I don't know                                 what you're trying to do and but it will                                 actually try to help you out so you can                                 say this is what you type and then it                                 says well maybe you meant this thanks to                                 the little mints and distance we can                                 very easily guess what might be a                                 similar command if you're using it on a                                 command line you're seeing something                                 similar where it will suggest oh you                                 missed type status or whatever maybe you                                 meant this other command so if you                                 correct that your command will then work                                 as expected this is one of the changes                                 that we added in                                                     fail early be a bit more of a pain in                                 the beginning but make your life                                 afterwards a bit easier the next one are                                 rolling upgrades that we have added this                                 was one of the main point pain points                                 people had because as soon as you had a                                 major version upgrade basically you                                 would shut down the entire cluster                                 update all the nodes then update all the                                 installations and then hope that your                                 cluster would come back up for some                                 people who downtime was acceptable for a                                 lot of people downtime was not really                                 acceptable we are trying to work around                                 that and we support rolling upwards now                                 which probably feel a bit like this and                                 you hope that your little train is not a                                 railing while you're riding it and you                                 just hopefully keep going and keep going                                 and we'll actually try to do that live                                 now so I will need these three documents                                 with three different types we will need                                 them afterwards so I will insert them                                 now we'll come back to those what we're                                 doing first is we want to check how is                                 my cluster doing can I even upgrade and                                 for that in five six or the latest                                 version of the five branch we have added                                 this upgrade assistant and that upgrade                                 assistant will first tell you back up                                 your indices now I will not do that but                                 please for your production use case                                 don't skip the warning or don't blame us                                 that you have lost all your data so back                                 up your data if you have any meaningful                                 data in there and then we have a checker                                 which will basically tell you okay we                                 have one error we could go to the                                 documentation to actually fix that but                                 we also have a helper which can do that                                 for you so basically you can see here my                                 toad Cabana index something in our                                 configuration format changed if I                                 reindex that it will basically create                                 that in the right format for me and then                                 I can do my upgrade so if I head over to                                 my cluster check you can see now I just                                 have notes I don't even have errors or                                 warnings now I can actually do my                                 upgrade so what we want to do now is we                                 want to basically rotate out one node by                                 the net the others so what I need to do                                 is in my environment file I want to                                 comment out five six and I want to                                 switch that over to                                                   released soon but yeah we like to test                                 stuff and I just try to test the                                 upcoming release here as well so                                 basically I'm killing elasticsearch                                 three and replace it by six version of                                 the same node since this was the master                                 node it might take some time until the                                 cluster figures out oh okay the master                                 is gone I need to vote for a new master                                 then the elasticsearch three node will                                 come back up you can actually see                                 somewhere pretty much at the end now                                 probably you can see that elasticsearch                                 three has exited somewhere if I would                                 see it                                 so here you can see we have kill                                 elasticsearch                                                         that closed statement basically we have                                 killed that note the custard keeps                                 running and hopefully it has already                                 elected a new master note and it was                                 quick enough that it has actually joined                                 and this now looks very simple but this                                 is like a big change from what we had in                                 previous versions you can see we have                                 mixed major version numbers we have two                                 notes and five six and one note on six                                 point three already and you can still                                 unclear your data and keep doing                                 whatever you have been doing or want to                                 do with your cluster so let's quickly do                                 the same thing for the other two nodes                                 so you can see okay we have killed                                 another node elasticsearch one is now                                 the master node elasticsearch two is                                 currently being replaced you can keep                                 reading and also writing your data and                                 what i'm talking it will take about I                                 don't know                                                              that until my note comes back up                                 hopefully you can also watch it live                                 here you can see elasticsearch                                      starting it's bound to public interface                                 it is in the state started so it should                                 join my cluster any moment now and it is                                 back so we can actually do the same                                 thing for elasticsearch                                               that let me quickly cheat and copy out                                 one curl command so I'm killing my                                 elasticsearch                                                       Cubana its erroring out why is that                                 because Cubana right now can only talk                                 to single elasticsearch instance and i                                 have set that to elasticsearch                                          now we will need to fall back to the                                 command line so basically i do around                                 the same command with curl just to say                                 like okay give me all the nodes and what                                 we have so we can see elasticsearch                                      now the new master node i elasticsearch                                 one is still starting up and trying to                                 join and now it has actually come back                                 up the one thing that we still need to                                 replace is Cubana and for those of you                                 who have ever complained about java                                 taking a long time to start up wait                                 until that node process comes up because                                 that will be the slowest thing here this                                 will be like a minute or so while I kind                                 of                                 need to keep talking nothing is                                 happening because well there is nothing                                 there so yeah nothing is reachable and                                 in the background somewhere                                 my little note process okay it has                                 exited but it will try to climb up again                                 and it will yeah it will take a while                                 until cabana is back up but we have life                                 migrated you can still run your queries                                 right now your cluster is in a healthy                                 state already so no downtime upgrades                                 anymore everything worked out pretty                                 much as expected                                 while Cubana is coming up let's talk                                 about the next topic and I'll show you                                 that cubanos up afterwards watermarks                                 who is aware of watermarks quick show of                                 hands okay not everybody so we have                                 various watermarks normally when you                                 don't see stuff or stuff is overflowing                                 bad stuff might happen and you basically                                 want to stop from falling over if there                                 is flooding so we had the first to the                                 low and the high watermark we had in                                 previous versions low basically meant                                 stop allocating new charts if you have                                 hit more than                                                         and if you're at                                                       try to migrate charts away from that                                 node if you have space anywhere else now                                 we have added a new high flood stage or                                 flood stage watermark for at                                             values which will basically block write                                 operations so if you have                                               disk space left we will actively reject                                 your write operations you can still                                 retailer you can delete data but rather                                 than writing data and probably being not                                 able to write it everywhere or write it                                 in the right way or corrupting your data                                 somewhere we will just reject the write                                 operations entirely so ideally Cubana                                 has can come back up now and yes looks                                 like we're in luck today                                 I don't want to send statistics now so                                 by the way the coloring scheme changed                                 if you've never seen cabana                                            say                                                                  black                                                                 blue why blue because it's easier to                                 read if you're colorblind like the                                 colorful stuff was not very good for                                 colorblind                                 people but now it should be better so                                 yeah we're on six three the upcoming                                 version which should come out very soon                                 let's hope we don't find any more                                 blockers and well we've upgraded all the                                 nodes that should all be fine back two                                 or three nodes so let's head over to the                                 flood stages I create a new index which                                 I call my flood and I insert one                                 document and afterwards I check how much                                 disk space do I have in my containers                                 and you will see here we have thirty                                 point one gig of this space available                                 here in total and                                                     actually set you can either set it to                                 percent or an absolute value so I will                                 set my watermarks to                                                    I will hit the flood stage watermark                                 immediately immediately after I had this                                 interval that the update intervals so                                 around every                                                          check did you hit any of the floods day                                 or of the watermarks and will then try                                 to act accordingly so now it should lock                                 up my write operations immediately                                 because well I only have                                                 space available and I set that limit to                                                                          you can still read documents this is the                                 document I've just inserted but my write                                 operation should be rejected because                                 we've already hit the high flood stage                                 at the flood stage watermark and you can                                 see this is exactly the arrow operation                                 or the response that you will get an                                 index is read-only and allows deletes                                 this is what all you can do so that                                 works as expected let's revert it and I                                 think it was version                                                  that if you want to reset something to                                 the default value you don't want to set                                 a specific value or not even that the                                 value that is the default but reset the                                 value you can actually set something to                                 now now this will revert all these three                                 settings to the default settings so                                                                                                             update and once it has been updated then                                 I can run my write operations or can i                                 it is expected that this fails                                 it is it's maybe just a bit not what you                                 were expecting because once an index is                                 locked so once we had a shard and we                                 tried to write to a shard where we hit                                 the flood stage watermark we will lock                                 that index and you currently need to                                 manually unlock that index so what you                                 need to do is on the index my flood you                                 need to actually set the read only allow                                 delete to null as well so that you don't                                 write to that anymore                                 and once you've reset that now you can                                 happily write to your index again but                                 you manually need to unset that we might                                 change it in the future that we                                 automatically we see you hit the the                                 flood stage watermark and then you see                                 your K there is more this space now we                                 might unlock that automatically we're                                 not sure what we want to avoid is that                                 we have like a toggling state where you                                 add that                                                                 get a write-in and sometimes you cannot                                 if you delete a few documents again for                                 example so we're not sure right now                                 there is no automatisms you will need to                                 manually actually change and fix that                                 sequence numbers that was one of the big                                 things that we have added as well                                 because sometimes keeping track of what                                 is going on is kind of hard and                                 sometimes you need more than your                                 fingers to count so this is to show you                                 how a sequence numbers work so you have                                 a primary shot and two replicas charts                                 we have a write operation zero which is                                 being replicated to my replicas you have                                 a local checkpoint what you have locally                                 written and then you have these global                                 checkpoints so whenever you do a write                                 operation and the response we will                                 actually tell you okay we're at this                                 local checkpoint and then you can                                 forward the global checkpoint we                                 basically piggyback on the requester you                                 can see we're doing to write operations                                 if they're in the wrong order we know                                 that since there is sequential number                                 only if we have all the rights we can                                 forward the local checkpoint and then                                 tell the primary that we have received                                 all the or replicated all the day then                                 we can forward the global checkpoint now                                 my primary shot will die and you can see                                 right operation four and six has gone                                 through replicas too and the new primary                                 shot has not received that for operation                                 and basically since it's the new primary                                 it will tell the other shot well roll                                 back your fourth right operation because                                 I didn't see                                 I'm setting what is kind of the real                                 estate throat it out and now kind of my                                 state is what you need to get and then                                 they will just sync up on all the the                                 sequence numbers and transactions or the                                 operations that you have had in between                                 so these are the sequence numbers that                                 we have added to keep track of what is                                 going on                                 so secrets numbers look something like                                 this so let's say I'm creating a new                                 index called sequences sequence numbers                                 with one primary shot one replica shot                                 and I'm excluding the elasticsearch one                                 instance why am i excluding the                                 elasticsearch one instance because                                 Cubana is talking to it and i will need                                 to kill notes again and otherwise that                                 we need to fall back on the command line                                 again and i don't want that so these                                 charts can be allocated to the                                 elasticsearch two in three nodes if we                                 check that we can see we have the                                 primary shot is elasticsearch three and                                 the replica shot is elasticsearch to                                 remember that the primary shot is                                 elasticsearch three we will need that                                 information in a few moments so now you                                 can start inserting documents and this                                 looks very familiar or at least this                                 party looks very familiar this part here                                 is what is new the sequence number let's                                 insert a few more documents to see what                                 is happening here so you can see the                                 sequence number is basically an                                 increment for every operation and at the                                 primary term that will change every time                                 the primary shot changes so we can keep                                 track of who is received with write                                 operations we need both of these                                 counters you can write one specific                                 document even if you do an update since                                 I'm writing to the same document ID here                                 even if it's an update it will increment                                 that number if I delete that document                                 will it be another operation will my                                 sequence number increment yes it does                                 so I've deleted a document what if I try                                 to delete the document again will it                                 still increment the sequence number                                 guesses who says yes who says no                                 it does it doesn't change anything but                                 we were keeping track of all the right                                 operations that we do and okay so what                                 we will do now is and we have kind of                                 like the basically three was the primary                                 shot we will kill that node and I think                                 that was also the master node now by                                 accident so this is kind of the worst                                 case scenario we need to vote for                                 another master node and then the nodes                                 need to figure out okay what data do I                                 have where is my new primary copy what                                 do I do here so if you look at that you                                 probably see okay elasticsearch three                                 exited and then my other nodes might be                                 shortly confused but hopefully they will                                 figure out what they need to do so let's                                 see they are still trying to get a new                                 master node so right now they're in a                                 slightly confused state let's see there                                 is a slight timeout for which we need to                                 wait which was thirty seconds you can                                 see okay                                 very scary SEC traces but this is kind                                 of expected they're figuring out that                                 they need to vote for a new master node                                 and then they will also figure out that                                 the primary shot is gone so this might                                 take I don't know a few more seconds                                 hopefully only this will repeat a few                                 times where elasticsearch one and two                                 will try to ping the master node the                                 elastic to three node and after a few                                 pings they give up and then will elect a                                 new master node between themselves so                                 here you can see primary replicas resync                                 so they seem to have figured out that                                 only those two are left now and you can                                 see here they have not figured out that                                 elasticsearch three is actually gone                                 that will take another few moments come                                 on because right now you would try to                                 send your search request to your right                                 operations to a shot that doesn't exist                                 or is not part of the cluster right now                                 it would timeout then your client needs                                 to handle that but in a few moments now                                 they have actually figured it out and                                 here you can see my primary shot now is                                 the elastic search to node my replica                                 shot is unassigned why is it unassigned                                 basically because it couldn't go                                 anywhere else because elasticsearch                                 three is down why does it not go to                                 elasticsearch one because we said                                 explicitly that this index can only be                                 go to elasticsearch two and three why                                 does it not replicate to elasticsearch                                 two because there is no point in having                                 the replica on the same note so there is                                 basically no place where this can                                 replicate so what state will our cluster                                 be in now yellow because all the data is                                 there you can read and write it but you                                 don't have all the replicas that you                                 want to have in your clustering will                                 tell you in the cluster state that is                                 yellow now let's insert a few more                                 documents so you can see my sequence                                 number is counting up and once we're                                 satisfied with that we can actually                                 restart the elasticsearch                                              and well it will take another                                            or so let's keep doing some write                                 operations of the                                                        elasticsearch                                                         again will that be a primary or a                                 replica shot for this index now                                 afterwards replica because we don't                                 change the primaries it will see okay                                 there is an existing primary shot I will                                 become a replica shot and then it will                                 try to sync up all the data that it                                 missed so let's see okay you can see it                                 has joined again we have the primary                                 shot now is elasticsearch                                           replica shot is elasticsearch                                           worked as expected and now to one of the                                 very nice side effects of this we have                                 bless you we have here based on the                                 sequence numbers we have these trans                                 lock ups recovered basically in previous                                 versions we were comparing the leucine                                 segments that we were writing and since                                 they were written independently often                                 these leucine segments were slightly                                 different and then we had to replicate a                                 lot of data everything that was                                 different on disk with this transaction                                 lock that we have we can basically just                                 replay the missed out transactions and                                 here you can see elasticsearch                                   replicated                                                               because those world where the                                            that it missed while it was down and it                                 has done those so any recovery will be                                 much more efficient now since we only                                 to replay those changes okay no we don't                                 want to see that and what do you think                                 happens if we run out of sequence                                 numbers do we have any mechanism to roll                                 those over we don't so our assumption is                                 pretty much this is a long and the long                                 should be enough for pretty much forever                                 also because it's on a per shot level                                 and I can demonstrate that first shot                                 level quite easily so let's throw away                                 the index that we have let's say we have                                 ten primary shots one replica shot you                                 can see lots of shards let's make this                                 slightly smaller so lots of shards and                                 then you can do your write operations                                 for example here I don't provide an ID                                 so it will go to any random chart                                 because we will generate a random ID and                                 you can see okay sequence number zero                                 zero at some point we will have a                                       then it will fall back to zero or two or                                 whatever since it is on a per shot level                                 so this is a write operation on a per                                 shot level if you write to the same in                                 chart because here I have a specific ID                                 it will always hash to the same hash for                                 the writing information so this will go                                 to the same chart so here we have a                                 nicely incrementing counter basically                                 since it's always going to the same                                 chart this is mainly a question coming                                 from Postgres people because they have                                 this global transaction ID which is an                                 integer and if you don't run auto vacuum                                 on a frequent basis and it rolls over                                 you will not be able to do any write                                 operations anymore we don't really have                                 that problem because if I if I                                 calculated that correctly with                                          power of                                                                 write operations per second you still                                 have something like                                                    per shot level or something like that so                                 this is not a real problem that you will                                 run into we don't have any mechanism to                                 roll it over but it will not happen or                                 if it happens something is very weird in                                 your system there's one trade-off I ever                                 we need to keep track of all these                                 transactions so this is again on a per                                 shot level we will keep up to half a gig                                 of transactions                                 or up to                                                                 we will throw out the data but you will                                 need to keep up to half a gig of disk                                 space in addition to every single shot                                 that you have on a server so this is                                 just more disk space that you might have                                 and you need to keep track of that so                                 you won't run into for example the flood                                 stage watermark at some point and be not                                 allowed to write anymore and this will                                 allow some more features so we might or                                 we're currently trying to get to cross                                 data center replication where you have                                 basically independent clusters in                                 different regions and we will just                                 replay operations to another cluster                                 since we have the transaction log we can                                 do that in an async fashion we're just                                 fixing some stuff in or adding features                                 we need on a leucine level and once we                                 have those we will add them in                                 elasticsearch we're not sure about the                                 version maybe some later six to the X                                 version or sometimes in seven only but                                 we're working on that so that will come                                 as well types are going away who knew                                 that types are going away who who is                                 very surprised okay                                 there is still one or two hands and so                                 first off why are we getting rid of                                 types um kind of because we lied and                                 okay something is falling down                                 why multiple types don't really exist on                                 a leucine level and a half like if you                                 have different types you would assume                                 they're kind of independent but they are                                 not because for leucine they still map                                 to the same field so for example if you                                 have two different types and you have                                 the field disabled on two of them and                                 you might think one of these fields is a                                 boolean because I disabled somebody and                                 the other field is a timestamp because                                 that's when I disabled somebody this                                 will not work out because it needs to                                 map to the same data type sparsity even                                 though improved in leucine seven is                                 still not a great thing and also scoring                                 is not on a per type level but within                                 the index which is kind of confusing and                                 the elasticsearch team wants to get rid                                 of types for a long time so what are we                                 doing in five you could opt in to having                                 a single type in six by default you can                                 only have a single type anymore you can                                 change it in a configuration but don't                                 do that because it will only be more                                 pain later on you can still use multiple                                 types if you have imported data from                                 five that is why I have inserted those                                 documents at the beginning in seven you                                 cannot create multiple types anymore and                                 the type will actually be optional in                                 the API and in eight basically there are                                 no more types so this will take quite a                                 few years but this ensures that we don't                                 have any breaking changes and you don't                                 have any major upgrading pains there so                                 what does this look like you remember I                                 have created that index types with three                                 different types so you can see I have                                 here type                                                                them have the ID one by the way just to                                 make it a bit more tricky now I want to                                 create a new type with the underscore                                 doc type which is the one we recommend                                 in                                                                 without the underscore we changed it we                                 are sorry use underscore doc now that is                                 the one we recommend unfortunately on                                                                                                      underscore doc it doesn't work go to                                     directly and then use underscore doc to                                 avoid any upgrading pains around that we                                 just figured out like underscore is                                 normally what we use for internal stuff                                 so this is the one we want to use now                                 though you can pick any type as long as                                 this there is only a single one so I'm                                 inserting one document with this type                                 then I try to insert another document                                 with another type what will happen it                                 will fail of course and it will actually                                 tell me you were trying to have two                                 different types which we don't allow how                                 do you migrate your data I mean this                                 squares okay you need to change the type                                 how do you migrate your data data I'm                                 using the reindex API and basically                                 taking the data from the types index                                 with multiple types and play it into the                                 node types index and then I'm changing                                 with this script here I'm saying the ID                                 is a concatenation of the type and the                                 ID since all my documents in the                                 different types at the ID                                   I need the concatenation I set the                                 underscore type field this internal one                                 to my custom field type and then I said                                 the underscore type field to underscore                                 doc and if I play that it will actually                                 take my three documents and put them                                 into the no types index and you can see                                 I have all of them here all of them have                                 to type underscore doc and you can see                                 this one was the one                                 insert it directly and these are the                                 ones that I've replayed with this ID and                                 with a filter you can get back to pretty                                 much the same behavior that you have if                                 you were using multiple types you just                                 need to have well the filter to filter                                 down on the type one for example here                                 and then you should get a single                                 document back with that custom type                                 field - kind of more performance                                 optimizations are automatically resizing                                 we've added that which is kind of clever                                 you can set my target response time for                                 searches for example is two seconds and                                 elasticsearch will figure out that right                                 now it is serving                                                        second so your queue size is a hundred                                 and and if you try to add                                           element it will actually be rejected and                                 then your client can figure out do I                                 want to try a different node or do I                                 want to take some other action but                                 rather than queuing up search requests                                 for longer than those two seconds or                                 approximately two seconds then we'll try                                 to actively reject operations to have                                 avoid that the other one is adaptive                                 replica selection so right now we are                                 doing round robin in between primary and                                 any replica shots to do read operations                                 what we are so it was added in six one                                 needs to be enabled explicitly it will                                 be on by default in seven it's based on                                 yeah a funky paper what we're trying to                                 do is basically we're figuring out                                 trying to figure out which is the least                                 busy node and then we'll try to go to                                 that node so we're not trying to go to a                                 node that has already a very queued up a                                 lot of requests and might be slow or                                 that is I don't know experience some                                 garbage collection we'll just try to                                 route around busy nodes and go to the                                 less busy node for faster requests if                                 you have a busy cluster this will help a                                 lot if you have a not very busy cluster                                 the change is still not hurting you                                 shrink and split those are two of the                                 API so people have requested for a long                                 time so well shrink is kind of obvious                                 you try to combine multiple shots                                 together which will look something or                                 yeah which will look something like this                                 so I have a shrink index where I insert                                 one document so you can see                                 many shots will I have five primary five                                 replicas what I didn't say is I want to                                 have one primary or shot replica shot on                                 the elasticsearch                                                     sure that I have at least one shot or                                 one of the shots on one note and then so                                 if you show that you could we could see                                 that elastic so three has now one copy                                 each and now I take my shrink index                                 called the underscore shrink API and                                 store the result in the shrunk index                                 which only has one shot and if you run                                 that you can see now we only have two                                 shots one primary and one replica and                                 this is very fast because this is hard                                 linking the files on the file system                                 that is why we needed to have one copy                                 on the s exert three instance of every                                 short so we could hard link them                                 together and from now on you can just                                 work with this one copy the other                                 operation so the document is still there                                 the other operation is split so what I'm                                 doing here is I'm saying I want to                                 create and split index with one shot but                                 I want to have a routing shot a number                                 of routing shots up to                                              split into any number of or any factor                                 of that so let's create those if I enter                                 the document you have one primary one                                 replica shot this is pretty much what                                 you expect                                 you can block write operations and then                                 I basically say take the split index                                 called the underscore split API endpoint                                 on that I couldn't find a better name                                 unfortunately but this is called split                                 in five now and I'll split this one shot                                 into five primary shots which is also                                 hard linking those and then you can see                                 okay now we have five primary and five                                 replica shots again you get the same                                 data back and if you run the underscore                                 settings API on that one it will                                 actually tell you where this data has                                 been coming from and has been split out                                 okay so we have covered those things the                                 one thing I still wanted to mention                                 we're changing the default number of                                 sharks because                                 is not a good number you know if I want                                 to shrink I need to split our shrink                                 into a factor and                                                        any prime number is not a very good                                 choice what do you think would be a good                                 number of primary shots and we've had                                 this discussion for years internally any                                 guesses or any guesses which number of                                 shots we will pick as a default for now                                 it will be                                                        default because where we have teen                                 seeing too much over shouting and also                                 someone says that use                                                chart until it blows up and the Jewish                                 diamond is here but oh yeah hi Simon                                 yes he will actually say that to you if                                 you ask so yeah we're getting there and                                 the other thing that we're adding or                                 probably do this is still under                                 discussion we might do JDK                                            version for the upcoming set                                            are can't use this upgrade and what                                 versions of the Java client do we                                 require then which will basically mean                                 transport client is deprecated so we                                 don't care anymore the low-level client                                 doesn't have an elastic search                                 dependency so that can stay on                                          want to keep that on                                           high-level rest client to use the                                 elastic search                                                         the JDK                                                                use the high-level client in version                                   and get all the features from                                          need the newer versions or new features                                 you will need to fall or fall back to                                 the low-level client to actually run                                 those queries but that is what we had in                                 mind this is pretty much it and I think                                 we're out of time                                 if you haven't seen enough elasticsearch                                 stuff we have a meet-up here                                 organized by the community we will have                                 one on Thursday so if you want to dive                                 more into search things I will do some                                 workshop style search thingy on Thursday                                 even though I think it's pretty full by                                 now um yeah you might still get a space                                 with that do we have any questions                                 oh and before you run off I always try                                 to take a picture normally so that I can                                 prove to my colleagues that I've been                                 working I guess this is not necessary                                 today smile everybody                                 you may even wave awesome do we have                                 time for questions                                 oh thank you for the first and uh let's                                 take one short question and then we'll                                 have a short break together is there a                                 short question I don't take questions                                 from my coworkers there's one here are                                 your coworker no he's fine just a quick                                 question you were talking about types                                 and like the fact that each change and                                 you have only one type and it's the                                 underscore dog now so what's what advice                                 would you have for those that have                                 indices that have more types so what                                 should I do and will there be some                                 helpers on top of elastic to help                                 migrating that yes basically run the                                 reindex that I have shown where was it                                 something like this I mean this might be                                 slightly simplistic but this should give                                 you the idea basically take your old                                 index with multiple types play it into a                                 new index with one type and then you                                 probably need to change the ID if you                                 have like shared IDs or overlapping IDs                                 between the types remove the type                                 underscore type field and set it to                                 whatever you want and probably keep that                                 type information as a separate field if                                 you need to keep that information                                 something like this just through the                                 replay reindex API                                 I hope that answers it thanks a lot                                 all right speaker again                                  you
YouTube URL: https://www.youtube.com/watch?v=oYDrWDHeHTk


