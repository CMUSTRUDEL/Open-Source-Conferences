Title: Berlin Buzzwords 2018: Michael Noll – distributed stream processing for everyone with KSQL #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Michael Noll talking about "Big Data, Fast Data, Easy Data: distributed stream processing for everyone with KSQL, the streaming SQL engine for Apache Kafka".

Modern businesses have data at their core, and this data is changing continuously. Stream processing is what allows you harness this torrent of information in real-time, and thousands of companies use Apache Kafka as the streaming platform to transform and reshape their industries. However, the world of stream processing still has a very high barrier to entry. 

Today’s most popular stream processing technologies require the user to write code in programming languages such as Java or Scala. This hard requirement on coding skills is preventing many companies to unlock the benefits of stream processing to their full effect.

However, imagine that instead of having to write a lot of code, all you’d need to get started with stream processing is a simple SQL statement, such as: SELECT* FROM payments-kafka-stream WHERE fraudProbability greater than 0.8, so that you can detect anomalies and fraudulent activities in data feeds, monitor application behavior and infrastructure, conduct session-based analysis of user activities, and perform real-time ETL.

In this talk, I introduce the audience to KSQL, the open source streaming SQL engine for Apache Kafka. KSQL provides an easy and completely interactive SQL interface for data processing on Kafka - no need to write any programming code. KSQL brings together the worlds of streams and databases by allowing you to work with your data in a stream and in a table format. Built on top of Kafka's Streams API, KSQL supports many powerful operations including filtering, transformations, aggregations, joins, windowing, sessionization, and much more. 

It is open source, distributed, scalable, fault-tolerant, and real-time. You will learn how KSQL makes it easy to get started with a wide range of stream processing use cases such as those described at the beginning. I cover how to get up and running with KSQL and explore the under-the-hood details of how it all works.

Read more: 
https://2018.berlinbuzzwords.de/18/session/big-data-fast-data-easy-data-distributed-stream-processing-everyone-ksql-streaming-sql

About Michael Noll:
https://2018.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so thanks again for turning up it's my                               third time here at Berlin bus routes                               last year I've talked about Kafka                               streams with I think                                               Celsius it's so cool to be here without                               sweating all the time oh it's the same                               for you so I'll talk for about the next                               half hour about K sequel which is a                               distributed streaming sequel engine for                                Kafka and first a few words about myself                                so I work at confluent and if I start                                who knows conflict who knows what                                content is ok that's almost everyone for                                the few of you who don't a confluent is                                the company founded by the credits of                                Apache Kafka we're based in the United                                States I am actually based in                                Switzerland working from remote so we                                are pretty distributed team and I joined                                confluent about three years ago on the                                engineering team but since then have                                moved to product management and now                                responsible for stream processing with                                Kafka and confluent and specifically                                this is Kafka streams which I talked                                about last year and case ago which is                                what I cover today so that was a few                                bits about myself let me know you a                                little bit more who of you is using                                Kafka yeah yeah almost everyone again                                who of you is using Kafka streams okay                                who of you is using or has learned that                                there is something called case sequel ok                                a lot who of you knows sequel like the                                traditional sequel like my sequel post                                grad and so on ok everyone cool so this                                is cool because one of the things that I                                want to talk about in this session is                                the world of streaming with Apache Kafka                                as shown here and the world of databases                                and one thing I will talk about in                                particular is that there was a very                                close relationship between the two they                                like each other very much so and to set                                the stage for this                                we're pretty international audience                                you're Grillin buzzwords so in order to                                get here you typically would have to                                poke a fly it reserved a hotel order a                                cab or taxi probably listen to the music                                on your way here                                maybe you select to your colleagues                                right here because you still have to do                                some work and as it turns out a lot of                                these daily activities are nowadays                                poverty by kafka behind the scenes                                whether you know it or not so it's there                                anyone here that wants to say like                                    cents about how they're using Kafka                                maybe one of the speakers around here                                that don't feel shy to speak up okay                                some people are real hesitant but they                                don't want to talk about it                                okay bye Devin escape hatch so since we                                tech people tend to be introverts here                                is a an example that I think should                                still be relevant to most people here at                                least if you're working for or like a                                company in the private industry so a lot                                of times what you do at a company is                                that you're getting a lot of signals                                about your customer through a variety of                                internal channels and through a variety                                of external channels so in what                                typically you would like to do any                                companies to aggregate all this                                information that you know about your                                customer and then create consolidated                                customer profiles that could be used for                                a variety of things such as if you're                                doing fraud detection you want to know                                that a person is currently in Berlin so                                it seems to be fraudulent activity if                                the credit card of the person is                                suddenly new somewhere in Argentina and                                there a variety of scenarios where these                                things can help and as you can see here                                what we're looking at at a very high                                conceptual level is you will have a lot                                of input data in this example in the                                form of streams and you want to process                                that in real time into a table something                                like a normal database so we talk about                                that later on in a variety of different                                facets so here is where our case seeker                                enters the picture so as I mentioned                                before case lists the streaming sequel                                engine for Apache Kafka and at a very                                high level here is how you would use                                 that so you have your data in Kafka                                 and we're like in the motivating example                                 informations and signals about the                                 customers and then you want to process                                 their data you want to analyze it peek                                 into it and so on and with Kay sequel                                 you would run                                 k sequel and I show you what it means to                                 run K sequel at the perimeter of your                                 cuff tear cluster and then we'll talk                                 across the network to a Kafka cluster in                                 order to read data write data and so on                                 so very much like running Kafka streams                                 application for those of you raise the                                 hand earlier that there isn't this                                 either for writing Java applications or                                 scale applications and that's also all                                 you need right you don't need anything                                 else you don't need to install and I do                                 cluster because you want to have fault                                 tolerant stream processing that's why I                                 need HDFS and so on that's all you need                                 and I won't go through this thing here                                 in detail but in case you want to take                                 like a reference picture here are some                                 of the cool properties of k sequel in a                                 nutshell i will talk about primarily the                                 top half the top part and a little bit                                 of the bottom half but I will skip over                                 exactly once processing I skipped over                                 Kafka Security's of anyone here's in                                 finance or some other heavily regulated                                 industry there is not much information                                 in this talk but feel free to talk and                                 to talk to me afterwards so just some                                 things that I will cover in the next few                                 minutes so before there was case equal                                 the way that you would process your data                                 in Kafka through Kafka streams would                                 look some somewhat like this so here is                                 a skull application like an entrance                                 calaf lubrication that reads from Kafka                                 and you know apply some simple fraud                                 detection logic on input data in real                                 time and then writes the results back to                                 Kafka so apart from the import                                 statements this is literally the                                 application that you would have to write                                 in this scenario and it would run on a                                 single machine signal container it could                                 run on dozens machines dozens containers                                 and even more even so a lot of people                                 that we work with and a lot of you that                                 have used Kafka said that this is still                                 quite a high barrier for people to use                                 either because they're not Java experts                                 or Scala experts for that matter or                                 because they are so busy doing other                                 things that it just takes them too                                 to implement Java Scala applications and                                 then you know deploy them and so on so                                 with case ago all of these lines of                                 codes and you know I'm I'm a developer                                 by trade so for me I would be okay with                                 the above we've only just filled before                                 but all of that collapses down to this                                 single single statement so you don't                                 need to write any JavaScript you don't                                 need to embed those sequel statements                                 inside some other application or some                                 kind of person job that's all you need                                 and one of the nice things of that is                                 that you have a much faster and more                                 interactive workflow so with Kafka                                 streams or no similar tools you would                                 write your code in Java Scala and you                                 know compile it package it run and                                 deploy it and so on versus with case you                                 just write your statement and that's it                                 this is super cool if you're you know                                 sitting down at lunchtime you have this                                 cool idea and just let's just let me see                                 what happens if I do this and then you                                 know do very quick iterations on your                                 idea this is super cool for that we've                                 seen that also being used for some of                                 operations people so as our East I'd                                 want to figure out why was this message                                 and a process so take me look at let's                                 take a look at all the data that                                 happened in less today and see whether                                 there was any thing you know                                 particularly problematic in a message                                 that we got from this particular                                 customer during this time period and so                                 on so when I say Kasich how would you                                 use that so I showed the Kasich or                                 command-line interface here there are                                 three ways you could use case equal at                                 the moment one is through the CLI you                                 know a bit like an icicle or phosphorous                                 pronged route and type in your queries                                 you can use a modern WI for that shown                                 in the middle and you can also use the                                 REST API and that is for people that                                 like the sequel part but they still want                                 to drive it through their favorite                                 programming language like you know go                                 micro service or no JavaScript for                                 example here's a simple REST API example                                 how that would look like and what you                                 can do with the REST API is either what                                 I'm showing here is use you know sending                                 a query and the results are streamed                                 back in real time to the client but you                                 can also submit                                 statements such as creating a table                                 creating a stream and                                                    query to KC Qin and then it keeps                                 running behind the scenes for you                                 so that was a quick introduction and                                 high-level overview of K sequel so what                                 can you do with that take a sip it's not                                 as hot as it was last year but it's                                 still hot so one of the first things                                 that people typically with kasey kahne                                 our experience is that they just enjoy                                 looking at their data so it's very cool                                 if you'd want to explore your data in                                 Kafka so you know they're raised for you                                 to see the topics that value in your                                 Kafka cluster you can peek into what is                                 actually inside a topic so you know this                                 is actually the one that I want to look                                 at because I have this idea about you                                 know combining customer activities with                                 you know progress that we get from some                                 external partner and of course you can                                 do the normal sequel style of your                                 selecting your data and so on what you                                 can of course also do is you can use it                                 to enrich your data so you could say                                 that I have some a payment stream of                                 incoming financial transactions and I                                 want to enrich that with customer                                 profile information like you know the                                 very motivating example at the beginning                                 and then I can make more informed                                 decisions                                 downstream whether or not I would flag                                 for some of this transactions for the                                 land or not goes back to this idea if                                 the customer is currently in Berlin and                                 the trench action happens in Argentina                                 probably this is an indication that                                 something fishy is going on so what                                 Kasich will support here is no joints                                 like stream table joints and so on where                                 you can't combine data sources in real                                 time you can also use it for things such                                 as streaming e-tail or real-time ETL so                                 use it to filter data expense data so if                                 those of you in the room for like me                                 received dozens of emails about hey this                                 is our updated privacy notice there's                                 this thing called gdpr please read out                                 your privacy policy so a lot of these                                 companies are also using a sequel in                                 order to make sure that no data is being                                 anonymized of studentized appropriately                                 in order to comply with shitty power and                                 of course there's a bunch of other                                 things that if you're working in that                                 space you have probably had a whole lot                                 of fun in the past few months                                 similarly you can use it for anomaly                                 detection so a very simple example was                                 from the beginning here's another one                                 straight forward so I can fit it on one                                 slide so what we're doing here is we're                                 aggregating the raw input data then                                 we're making some you know heuristics                                 are some thresholding on the aggregates                                 that we have just computed in real time                                 and then saying okay if more than X or                                 front authorisation attempts failed in a                                 certain time period then this looks like                                 something bad is going on so then you                                 want to learn we want to follow up for                                 people that work in an IOT space whether                                 it's like connected cars you know                                 earthquake sensors whatever that is you                                 can do the same thing here so we can                                 look at the data in real time aggregated                                 and then alert follow-up you know create                                 actions and trigger things off of that                                 as well and of course it can be used for                                 more mundane tasks so oftentimes in                                 Kafka what you like to do is you want to                                 convert data for example let's say your                                 input data is in JSON but you want to                                 edit have it in a fro or you want to                                 reap rotation your data because you want                                 to scale out that it's very easy to do                                 with case you could we know with                                 one-liner we're saying it I want to I                                 have for some of this many partitions or                                 I want to have this output data format                                 in this case we are converting whatever                                 form of the input stream is into JSON                                 format now where's Kasich will not not                                 such a great fit a case equal is a                                 streaming sequel engine so it is not                                 optimized for you know random lookup of                                 arbitrary field in your data so for                                 example if you're looking for one                                 specific message in a stream of Kafka                                 messages it will not return this in                                 constant time like probably database                                 with indexes would and for the same                                 reason it's also not such a great fit                                 for another traditional VI tooling well                                 because as I mentioned there are no                                 indexes yet in case people also because                                 there is no JDBC driver yet well there                                 is a community one but not one part of                                 the case of the project but also because                                 a lot of these tools and in this space                                 are not yet working                                 with continuously updated streaming                                 results so they are not good at working                                 with your data in real time that is                                 another reason which goes beyond what                                 case you could does or does not so how                                 does case equal work if you were here                                 last year when I talked about Kafka                                 streams I talked about how Kafka streams                                 was usable in production right from the                                 very beginning because                                                   it had to solve were already solved by                                 kafka the foundation of Kafka streams so                                 Kafka she was standing on the shoulders                                 of the Kafka giants or the streaming                                 Giants case you could ask the same thing                                 so case equal itself is built on top of                                 the Kafka streams API which in turn is                                 built on top of the Kafka producer and                                 consumer API so which then begs the                                 question well this looks actually pretty                                 cool across the board but when should I                                 use either of those can I combine them                                 but first yes you can combine them so                                 it's pretty common that for example                                 someone is using Kafka streams to know                                 massaged the input data and I give an                                 example for when you would want to use                                 that and then hand it over to a case                                 sequel based workflow and then this is                                 then being taken into another Kafka                                 streams workflow and so on so people are                                 doing that a lot and the way I would                                 just oppose them is the higher up you go                                 in this pyramid the more you get                                 ease-of-use at the expense of                                 flexibility in what you can express in                                 your application so for example if                                 you're using the core consumer or the                                 producer you can really work on like the                                 nuts and bolts of Kafka so you can                                 subscribe to Kafka topic you can peek                                 inside the Kafka topic etc so it's like                                 you know your soldering iron working                                 with Kafka if you're going for the up                                 the stack you get Kafka streams which                                 gives you actually two AP is one is a                                 functional programming style API called                                 the Kafka streams DSL and one is an                                 imperative style more like you know                                 event at a time processing way called                                 the processor API so what you can do                                 here is know this example is that ezel                                 it's a very Scala collections like you                                 know                                 a flat map and so on and then on top of                                 that is K sequel which is Eng gives you                                 a secret express your processing logic                                 and that is also how I would Chuck                                 suppose those in terms of when you would                                 you want to use them so imagine this                                 example here                                 so here's case you let the table and the                                 same kind of logic at the bottom for                                 Kafka streams now where would you not                                 use case equal because it looks pretty                                 simple in the rich icky sense like no                                 not a lot of moving parts right there's                                 this one thing that it does a good                                 example that I like to use is for                                 example if you have to implement a                                 finite state machine it's something that                                 doesn't really flow naturally in my                                 opinion in any kind of sequel tool but a                                 repro comanche is pretty good at that so                                 we've seen some customers that work on                                 network telemetry data where they are                                 getting raw network traffic packets and                                 they're stitching together TCP sessions                                 and in TCP there is a finite state                                 machine that tells you here's an a                                 connection that is being established and                                 then it's continuing and at the end it's                                 being closed and that is something that                                 I think it's more naturally into a                                 program language where you can build                                 your finite state machines and so on                                 this is not something that I think                                 that's very well into the secret world                                 in terms of architecture and how does                                 that all work behind the scenes so with                                 Kasich oh just just just one thing                                 really that the dust work and that is                                 the case achill server which is a JVM                                 process and the server has two parts                                 there is the case equal engine and the                                 case covers the API the rest API allows                                 you to interact with the server and you                                 start the server with a simple command                                 and that could be on a physical machine                                 like your laptop when you just download                                 it and want to play with for the first                                 time can be a docker container it can be                                 on pram it can be in the cloud can be                                 public or private cloud it doesn't                                 matter works as well as open chef on                                 open ship as it does somewhere in event                                 the emperor based setup on GC PE or with                                 Google Google Cloud or confluent cloud                                 the actual processing happens in the                                 engine and as I mentioned earlier this                                 is based on Kafka streams so the engine                                 in case equal use utilizes the Kafka                                 dreams API to do the processing and as I                                 mentioned earlier there are a couple of                                 ways they can use these Kasich observers                                 interactively either for UI for a CLI or                                 by driving the REST API directly in your                                 favorite programming language so and                                 just to stress the fact again that I                                 mentioned earlier because I think this                                 is super important case you could just                                 like have costumes it runs everywhere so                                 wherever you can deploy a JVM process                                 you can deploy this and that means it is                                 equally viable for a super small scale                                 use case like a proof-of-concept or                                 prototype or for very large-scale                                 production setup and that I think is                                 pretty cool because typically we have                                 other you know particular like the big                                 data tools that originate in the big                                 data world is you have to reach this                                 minimum threshold of pain that you want                                 to go through before you take the job                                 and start you know for something using                                 Hadoop or spark here you can use the                                 same tool from your initial testing with                                 test data locally all the way to                                 large-scale production on you know                                 dozens of machines and more and to                                 showcase how that work is then how to go                                 from like a single container for some to                                 distribute it set up as a reminder case                                 equal service read and write to Kafka                                 across the network they are not running                                 inside the Kafka cluster they're not                                 running inside the Kafka procas you can                                 run one server or many of them and if                                 you run many of them they automatically                                 form a case equal cluster behind the                                 scenes what they're doing is they're                                 forming a Kafka consumer group if you                                 know a little bit about how Kafka                                 consumers work behind the scenes that                                 means if they are in the same cluster                                 they're in the same consumer group and                                 first they collaborate Lee and in                                 parallel start processing your data and                                 there is nothing you need to do that                                 there's no coordinator they need to run                                 etc there's no master node or anything                                 all of this is being handled by actually                                 the Kafka bag and behind the scenes so                                 if you need to have like five servers                                 running you run five containers if you                                 want to have two running just stop three                                 and talk about that in a second                                 and similarly because it's so                                 lightweight udeploy and what you                                 typically see and also what we recommend                                 is that actually you deploy not just                                 humongous Kasich a cluster inside your                                 company so actually quite the opposite                                 because you want to deploy a case equal                                 cluster and cluster size very heavy                                 weight you could say like maybe a case                                 equal deployment or case equal                                 application per project or per team or                                 per use case that also allows you to use                                 different version of case equal against                                 the same Kafka cluster so some teams                                 prefer to use only like Troodon tried                                 versions of case Eagle that they have                                 been using for six months only then they                                 go to production with it they can stick                                 to these you know older versions or and                                 then you can have you know the very                                 innovative teams in a company that can                                 wreak havoc make a lot of mistakes or                                 quickly they can use the very latest                                 version or maybe even running of case                                 equal master the master branch directly                                 in production so you could D cup of                                 teams and ten lines very easily which is                                 super cool if you're in a company that                                 has maybe more than ten people working                                 on that problem so if you're like a                                 small start-up it's super cool because                                 you can get up and running like in a                                 minute with this but in a bigger company                                 often the problem is the organizational                                 tooling and the organization processes                                 around that so that also allows you to                                 be couple of teams and timelines very                                 easily and if you want to use case equal                                 there are two ways you can do that                                 here's the interactive usage and I'm                                 Jack the poster to in a second with                                 interactive usage you start in one or                                 more cases called servers and then you                                 can interact with those servers along                                 with your classical deployment through                                 the CLI the UI or the rest of API                                 directly the second way to deploy case                                 sequel is in headless configuration when                                 you're doing this the case you'll                                 service disable any interactive access                                 like the REST API is completely disabled                                 and the way they know what to process is                                 through a sequel file that you give them                                 and in this set up like these three case                                 code servers they would fall in our case                                 ago deployment ok second cluster they                                 would only run those queries that are                                 predefined in a sequel file that you                                 give them and that is pretty cool for                                 companies that want to have in a clear                                 audit trail what is being pushed to                                 production what is not that                                 can roll back if need be and that in                                 general should fit into their existing                                 CIC D pipeline where you want to prevent                                 human mistakes happening in production                                 so this allows you to lock down k sequel                                 and minimize any mistakes you know for                                 human operators so an example journey                                 often looks like this so people have an                                 idea or they want to try this out so                                 what they do is they have an interactive                                 case achill a set up where they're just                                 you know typing their queries seeing                                 what works with dozen and then they're                                 making good iterations on that and come                                 production you know then you know what                                 you've got what you want to do you also                                 have an idea about like the capacity                                 that you would need to only like one of                                 them don't need like ten of them etc                                 then you would run a headless case' call                                 deployment within your predefined sets                                 of queries it's a very simple and you                                 can combine them as well that said there                                 of course also other people that use                                 interactive case acrylic production as                                 well so also an option if you're fine                                 with that                                 we've seen that not just for people that                                 work in the line of business that are                                 actually like building the product or                                 I'd say the fraud team the                                 personalization team or whatever but                                 also people that work in operations                                 where they have a way to use case we'll                                 to look into the flows of data in Kafka                                 so now let's talk about something that                                 is hopefully even a bit cooler if you're                                 interested in the the techie and and                                 conceptual side of things so something                                 that we already stressed with Kafka                                 streams in Kafka and sometimes we're                                 also stressing case equal is what we                                 call the stream table duality now what                                 is that and why should you even care so                                 here's an example from the previous                                 slides maybe it was hard to notice but                                 in some cases we actually showed                                 sequences that created streams and in                                 other cases we created tables okay oh                                 and is that right what what is the                                 difference there so I'll talk about that                                 in a second the most important point is                                 in practice most use cases that you                                 implement and even you know the infamous                                 word count is an example of such a use                                 case you need both streams and tables so                                 if you have only streaming tools                                 you built the table part yourself if you                                 only have the database tables you build                                 all of this reading part yourself and                                 that kind of sucks                                 I mean I've before I joined confluent I                                 was you know using those technologies                                 not building them and it was really                                 really hard like all of this you had to                                 implement yourself and that's why for                                 Kafka we decided and for Kasich oh we                                 decided now this is some that should be                                 working out of the box like the                                 batteries should be included and let me                                 tell you why this is so important first                                 thing is what is actually a table so if                                 you look at this example here you have a                                 table on the left side that is                                 undergoing mutations from top to bottom                                 and what we can do is we can do change                                 data data capture on the table which                                 means we're recording the mutations                                 against that table that gives us a                                 stream a changelog stream and the center                                 column and then we can use that                                 information this stream to reconstruct                                 the table for any particular point in                                 time cool part is that you can also do                                 that on a different machine in a                                 distributed environment so I plugged                                 about that like the the bottom link is I                                 talked about that in length recently                                 we're use Scala code to juxtapose that                                 including kafka streams code in case you                                 code to explain the relationship between                                 those concepts here's a sneak peek so                                 here's a stream and you're turning that                                 into a table of the latest user                                 locations for a user so the stream is an                                 input of user locations you know Ellis's                                 and paris now editors in Rome and you                                 see how the table is changing over time                                 as two input data is being processed now                                 we're looking at the same example but at                                 the top there is no the change lock                                 screen for the table and as you can see                                 here is in this specific example the                                 stream at the top is a copy of the                                 stream at the bottom and by the way with                                 Kafka's in case of course no they                                 realize that and they don't duplicate                                 your data on necessarily but here's                                 about the concepts                                 so this looks a bit trivial right                                 because the data looks almost the same                                 top to bottom so let's look at something                                 different what we're doing here is the                                 same input data the same input stream of                                 users two locations and now we are                                 counting how many locations the                                 particular user has visited no more the                                 trivial example could be a frequent                                 fliers computation if you're an airline                                 for example and what we can see here the                                 changelog stream for the table at the                                 top is not a bit different now we're                                 seeing in numbers for users being                                 capturing that changelog stream so there                                 is a clear relationship between a stream                                 as a table as the stream as a table and                                 with case equal you can explore that in                                 your applications and the core                                 realization of that is if you have a                                 stream you can get to a table by                                 aggregating that stream that could be                                 for example accounting it could be                                 summing it could be top K analysis and                                 so on so to get to get to a table you                                 take a stream and aggregate it or in                                 something like you interpret that stream                                 in a particular way and if you have a                                 table you get back the stream or just                                 looking at the changing of that table                                 and that's also why these two have such                                 a close relationship there are two sides                                 of the same coin that's also why Apache                                 kafka and databases are so much related                                 and if you've read up on on that that                                 concept you know the idea of turning the                                 database inside out this is turning the                                 database inside out but now what does                                 that mean for you as a user so let's                                 take a look at what you can do with that                                 in case equal for example imagine you're                                 doing change data capture from your my                                 sequel or you know whatever already be                                 nice you're using database into Kafka so                                 you get for some customer information in                                 a real-time stream data Kafka through                                 Kafka Connect so there are connectors                                 that you can use for Oracle MySQL                                 post-crash you know whatever to get data                                 flown in real time                                 Kafka then you can use case equal to                                 process those changes to your customer                                 data in real time and then send it to                                 somewhere else with Kafka connect again                                 like elastic so can build an elastic                                 search base dashboard or some other                                 downstream roughly off of that similar                                 example let's stick to the customer                                 table at the bottom which is a table but                                 you can also have things like you know                                 connected cards in an IT scenario which                                 are writing updates continuously as they                                 happen in real time to Kafka as well                                 what you can then do with K sequel is                                 you can then join the stream and the                                 tables in real time and then you know                                 sent them wherever they are needed none                                 of that with minimal effort so that is                                 example so I've showed on previous has                                 already some code so I'm going to do                                 that here again how you do that with you                                 know case code statements but this is                                 how you benefit from this as a user but                                 in order to also explain how case you                                 could dog fruits itself with the stream                                 table duality let's take a closer look                                 at what happens behind the scenes with                                 case equal and we're going back to                                 deployment or operational parts so a key                                 challenge in any distributed system or                                 specifically for stream processing is                                 for tolerance and for stream processing                                 it's primarily about fault tolerant                                 state so what is that imagine you have a                                 computation that does any kind of                                 stateful processing typically examples                                 are you know Giants and aggregations but                                 also windowed applications particular                                 very saying that I'm doing five minute                                 averages of something or I'm doing a                                 stream to stream join over an                                 overlapping period of let's say                                    minutes all of this requires state and                                 managing that state behind the scenes                                 for you so in this case you know I used                                 a blue database icon to represent the                                 state of the server that is because you                                 asked it to do some kind of stateful                                 processing you know like it table join                                 or aggregation now going back what we                                 showed very early that you know a table                                 as a change                                 the table and so on what is happening                                 behind the scenes that case circle is                                 kind of streaming Becky doing a                                 streaming backup of any state changes                                 from the case achill server that runs at                                 the perimeter of the kafka cluster to                                 the kafka cluster itself now whenever                                 that server happens to go down like the                                 machine crashes or communities decided                                 to move the container somewhere else                                 then the replacement container of the                                 end server whatever will say I will not                                 restore the state of the failed machine                                 to exactly the point where it was when                                 it died and only then I would resume the                                 processing so that is super cool like                                 there is nothing else and you do all                                 that happens behind the scenes and                                 automatically for you and to show that                                 in an example where there are a bit more                                 servers around imagine your three                                 servers one of them dies and why because                                 they're doing stream processing and                                 stateful stream processing work the                                 failed state from the failed machine is                                 moved over and split to the remaining                                 ones and I'm oversimplifying here a                                 little bit we're gonna say X split and                                 merge later on so ignore that I don't                                 want to make it too complicated your                                 ending a little bit of time that we have                                 so what happens then behind the scenes                                 is if this happens like server three                                 Dyas behind the scenes because kcq let's                                 build on Kafka streams which is built on                                 Kafka producers and consumers a Kafka                                 consumer probe rebalance is triggered                                 which is essentially the event saying                                 that oh something happened with one of                                 the things that we're doing the                                 processing and as a result of that                                 processing like the logic plus the state                                 of the processing is being migrated to a                                 new place in this case is being migrated                                 to the live service that are still                                 running and similarly if you frame back                                 up server number three the work is split                                 again across the three instances the                                 three service and they start                                 collaborating and share the work again                                 what happens behind the scenes again is                                 another rebalance is triggered and then                                 the logic is and including the state is                                 being migrated so that was the fault                                 tolerance view which is things happen                                 technically and you rather have them not                                 to happen but it's the same it's not a                                 side of the same coin because it's also                                 relevant for elasticity and scalability                                 essentially no difference between them                                 really if you look at it so you can also                                 add remove or restart a circuit service                                 term live operations right so if you                                 think that your retailer and there was a                                 Black Friday coming tomorrow you can add                                 more machines to your live application                                 as it is running to prepare for the                                 onslaught of you know customers buying                                 things from your website on Black Friday                                 and once it's done you just stop some of                                 the instance that you no longer need                                 because you know you don't want to pay                                 for them on AWS for example and it just                                 continues working there's no data loss                                 there's no duplicate processing and so                                 on so how would that work very similarly                                 you decide well we need more processing                                 power so just start additional case                                 achill servers what happens is that the                                 existing ones will start sharing some of                                 their work including sharing some of                                 their state to the new ones which do the                                 processing and again what happens behind                                 the scenes is rebalance event status                                 beam are created and if you are done and                                 you can scale down again you can do the                                 very same thing you just stop some of                                 them and they scale down gracefully so                                 which is super cool and this is also why                                 you can easily run not just case ago but                                 also kafka streams application that's a                                 containerized on kubernetes                                 because there's not much you need to do                                 it just start as many or as little                                 containers as you need so as I mentioned                                 I have to apologize a bit because it's a                                 bit not simplified so I didn't talk                                 about like threading model or what                                 stream tasks are because that is where                                 Kafka makes assignment between input                                 data stayed and the processing so I                                 ignore all of that here because it's                                 otherwise a bit complicated to show on                                 on one slide but you can read up on that                                 in the Apache Kafka documentation how                                 that all works behind the scenes so to                                 wrap up                                 so if there is only one thing that you                                 should probably remember for today is                                 that Kasich Ulis is trimming sequel                                 engine for Apache Kafka so if you like                                 to use sequel in order to express what                                 you want from your data Kay sequel is a                                 great tool if your data is in Kafka it                                 has a bunch of nice properties some of                                 them I just alluded to ignored that it                                 can do exactly once processing I ignored                                 that it integrates natively with Kafka                                 security so it can encrypt data in                                 transit between you know the case achill                                 servers and your Kafka cluster you have                                 authentication authorization                                 particularly now here in Europe when you                                 want to prevent people from you know                                 accessing sensitive data in your Kafka                                 installation you can do that also with K                                 sequel it fits right in I didn't talk                                 about event time processing but this                                 allows you for example to reprocess data                                 with K sequel without case we were                                 thinking that oh you're telling you're                                 reading back like one month of                                 production data and if you're not having                                 event time processing support case agree                                 with otherwise think that oh this just                                 happened right now like one month of                                 traffic happened right now                                 oh god alarm bells DDoS attack fraud                                 whatever right so and with the meant                                 time processing what you can ensure is                                 that data is actually being processed                                 tomorrow as it was being processed today                                 so hope that was a good summary if that                                 was interesting to you here are some                                 pointers so the case with your project                                 is on github feel free to you know go                                 there take a look even more encouraged                                 to contribute some code or ask questions                                 tell us what works what doesn't and if                                 you're really interested we are always                                 hiring a confluence so if you're                                 interested in that and not just working                                 on distributed systems but also working                                 in distributed team you'll just come                                 talk to me or come talk to us more than                                 happy to chat thank you                                 and I think we have time for like wastin                                 few questions so whenever you do an                                 operation that requires some kind of                                 state some aggregation as you were                                 saying you're generating a table now you                                 have local rocks DB that's holding your                                 state store essentially is there every                                 time you do an operation                                 it sends data to Kafka and then Kafka                                 reads it back as the changelog to build                                 the store state this is that's what's                                 happening I tried to reformulate your                                 question because I think you're asking a                                 question underneath that and let me go                                 back to that slide this slide I think                                 you're kind of referring to correct                                 which is when we're doing stateful                                 computation and K sequel and like                                 sectionals and Kafka streams what is                                 happening behind the scenes in order to                                 ensure fault tolerant processing so                                 right you don't want to miss to process                                 a transaction like a payment if the                                 machine fails and also you don't want to                                 processes more than once                                 what is going on there and then I think                                 this is also part of your question is                                 this efficient so of course this is like                                 the high-level overview what is                                 happening is that all the computation                                 that happens on a particular Kasich                                 server happens locally so you refer to                                 that as no we're using behind the scenes                                 works to be as a local storage engine                                 for doing Network yes so that is what is                                 happening well then it's also happening                                 which is the second part of your                                 question is that for fault tolerance                                 reasons these changes to work stee-rike                                 the state and the storage part is being                                 backed up to Kafka and is this efficient                                 said yes this is pretty efficient so                                 there are a lot of optimizations that                                 are being done behind the scenes so that                                 nobody or not if you're getting                                   million input mate records that are                                 mutating a table you're not sending                                   million records to the changelog topic                                 there so there is some let's say it's a                                 compaction being done for these updates                                 so that this is actually rather                                 efficient the feature that you would                                 look up                                 or in the documentation would be record                                 caching which allows you to tune you can                                 also choose the encasing about for most                                 use case you don't need to look at that                                 this allows you to say how many of these                                 let's say intermediate updates                                 intermediate results would need be                                 compacted like squashed away before it                                 is being sent into such a topic or                                 before they sent to a downstream                                 operation as well so that that is                                 optimized a lot behind the scenes                                 because you're right this requires a                                 round trip across the network or at                                 least like one run trip across the                                 network hi first of all thanks for the                                 talk I would like to know if it's                                 possible to use case equal when your                                 messages are encoded with formats like                                 protobuf and especially when you publish                                 a schema update so not sure whether the                                 microphone is record or not so the                                  question was about supported data                                  formats particularly if they now have                                  schemas are involved at the moment case                                  acute supports three broad categories of                                  data one is no delimited thing our CSV                                  data one is JSON and one is afro ever is                                  an example of no read from the schema                                  wrote above is currently not supported                                  so no there's no protobuf support the                                  reason why that is not is because                                  currently we are wrapping up work on                                  integrating struct support so a very                                  good support for nested data once we                                  have that then we're looking at no                                  opening the kind of forms to have like                                  pluggable data formats including                                  protobuf so thank you for your                                  presentation cool so we run out of time                                  if you have more questions you know just                                  let me above - yeah where if you want                                  you can meet him later thank you very                                  much thank you                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=nf4enboASio


