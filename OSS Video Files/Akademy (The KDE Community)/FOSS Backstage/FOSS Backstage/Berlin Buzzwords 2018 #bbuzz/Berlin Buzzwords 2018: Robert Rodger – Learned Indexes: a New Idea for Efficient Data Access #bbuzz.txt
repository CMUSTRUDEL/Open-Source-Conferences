Title: Berlin Buzzwords 2018: Robert Rodger â€“ Learned Indexes: a New Idea for Efficient Data Access #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Indexes are what make efficient access for our data storage systems possible. Though traditionally implemented with highly-optimized tree-based data structures, this past December a group from Google proposed a novel idea: replace certain types of index structures with trained machine learning algorithms. 

After all, an index is nothing other than a model that maps a key to the position of a record; in this light, exchanging, say, your B-tree search with a deep neural network prediction seems at least possible, if not practical. Surprisingly, doing so can often lead to significant performance improvements, in terms of both time and memory consumption.

In this talk we discuss how learned indexes accomplish this. We focus on neural networks, and in particular how recent trends in processor architecture design make them computationally competitive against tree search. We then have a look at how machine learning algorithms can be applied to the task of range indexation, how they can deliver error bound guarantees, and how their accuracy can be honed by layering them recursively. We finish with a review of the Google group's results on three realistic datasets and a brief mention of how machine learning can be applied to other indexation tasks.

Read more:
https://2018.berlinbuzzwords.de/18/session/learned-indexes-new-idea-efficient-data-access

About Robert Rodger:
https://2018.berlinbuzzwords.de/users/robert-rodger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone and thanks for coming to                               my talk my name is Thomas ed is Robert                               Roger                               I am a American but I am based in                               Amsterdam where I work for a small data                               science and engineering consultancy                               called go data driven and now of the two                               of forementioned career tracks at my                               place of employment I am a data                                scientist which is to say I'm not an                                engineer so what then is data scientists                                like me doing on the store track stage                                well the answer is this paper the case                                for learned index structures written by                                Tim Kroshka who I think at the time was                                a brown took a year off went to Google                                is now at MIT along with a team of                                collaborators over at Google it made a                                splash at the data science water-cooler                                back in December because it proposed                                what I think is a really novel idea that                                machine learning something we data                                scientists no one care a great deal                                about I had the potential to replace                                indexes and certain types of database                                systems something I think and hope you                                hardcore storage systems engineers and                                the audience all know and care a great                                deal about now not only was this paper                                interesting in a general sense because                                at first glance the two use cases don't                                really seem to match up on the one hand                                you have statistical inference which is                                for learning patterns for the purposes                                of making predictions based on the                                future input and on the other you have                                data structures which are useful for                                optimized lookup but that's of                                information which you've already seen                                but it was also interesting for me                                personally because while I feel like I                                know a fair amount about machine                                learning                                despite their crucially in my daily                                professional life I know next to nothing                                about database internals and wanting to                                understand this paper gave me a really                                good excuse to dive in and learn all I                                could about the subject so I want to                                share with you today this idea and let's                                hope that Doug Turnbull had it right                                yesterday when he said that these sorts                                of autodidactic experiences tend to lead                                to great talks                                whether or not that's true here's my                                plan so first I'm going to talk about                                database indexes and while I'm sure this                                will be old hats and most of you in the                                room I want to do it anyway both to                                ensure that everyone is on the same page                                but also to reframe how we think about                                just what it is that database indexes do                                secondly I'm going to talk on again                                relatively high level about what it is                                that machine learning tries to                                accomplish and how this can be adapted                                to the database index domain and lastly                                I'm going to show how machine learning                                can be utilized to replace database                                indexes and three different types of                                tasks one range index is                                                and                                                                    let's go let's start with analogy a                                database is like a haystack and I hope                                this analogy is not too english centric                                for the crowd but as these databases                                consist of thousands millions or even                                billions of records I think it would not                                be unfair to say that the challenge of                                finding specific records in our database                                is very much like finding needles in a                                haystack                                and if we were to follow the naive                                approach every time we need to look up a                                record in our database we would have to                                comb through our haystack one entry at a                                time until we either find what we are                                looking for or convince ourselves that                                what we're looking for isn't to be found                                and this might be acceptable for                                databases with a few hundred records but                                even in moderate scales waiting for                                answers from our database whether it's                                me doing some sort of analysis or you                                and the audience trying to populate the                                catalog of your online retail shop that                                would take far too much time and what we                                would rather have is some means of                                knowing precisely where our desired                                 record is stored in the database and                                 then skipping over all the other records                                 to just such that one so because of this                                 database systems often compared with an                                 ancillary system called an index whose                                 function it is to tell us more or less                                 where in the database our desired record                                 is and to do it in an efficient manner                                 and how it works is roughly as follows                                 say each record in our database is                                 identified by a unique key an index is a                                 black box or and this is the insight                                 we're going to need for part two of my                                 talk a model where the unique key goes                                 in and a prediction for the position of                                 the Associated record comes out by the                                 way this is very much like how card                                 catalogs in the library used to work at                                 least for those of you like me old                                 enough to remember having to do high                                 school research projects using one now                                 that's a rather abstract view so we                                 should ask ourselves now if we were                                 actually to implement such a system what                                 types of black boxes or models could be                                 used and to begin this discussion with a                                 concrete task what could we used to                                 handle say range requests that is                                 locating all the records whose keys fall                                 between two specific values now anyone                                 who's ever taken an algorithms course or                                 perhaps recently sat for a tech                                 interview would probably think well if I                                 can order my records by these keys then                                 I can probably use some sort of binary                                 search to find each of the endpoint                                 records and then grab everything in                                 between at least as a first guess and so                                 let's follow up on that hunch we want to                                 facilitate binary search and so for our                                 model let's use a binary search tree now                                 for those of you who haven't seen one                                 before                                 the way a binary search tree works is as                                 follows for each record key you make a                                 node and that's uniquely identified by                                 the same ID as the record and also                                 carries the position of that record in                                 the database and then these nodes are                                 stored in the tree structure where every                                 node can have at most two children which                                 themselves are trees and you have                                 requirement that all the keys and the                                 left child tree will be smaller than the                                 current node ski and those in the right                                 child tree will be larger than the                                 current note ski and if you further use                                 one of the variations on a binary search                                 tree that also do their best to keep the                                 bottommost nodes all in the same depth                                 then you get a guarantee as well this is                                 supposed to be an animation oh there we                                 go is attempting to demonstrate that the                                 maximum number of nodes that you have to                                 examine during a look                                 is on the order of log base two of n                                 where n is the number of keys in your                                 index so for example if you had a                                 million keys this number turns out to be                                 around                                                                  dealing with                                                           improvement over brute search and you                                 might rightfully ask yourself well why                                 stop at two children per node or even                                 one keeper knows and if you continue                                 with that line of thinking and again                                 think of some clever rebalancing rules                                 to maintain and even tree depth you'll                                 eventually discover improvement to the B                                 tree seen here ignoring the costs of                                 scanning the keys inside an individual                                 node which well because the number of                                 keys and the note is much much smaller                                 than a total number of keys in our                                 database should be justifiable we've now                                 guaranteed ourselves lookup times on the                                 order of log base K of n where K is the                                 number or the maximum number of children                                 allowed for node which means if we go                                 back to a previous figures and we say we                                 allow                                                                million records were down to a tree                                 depth of three and for a billion we have                                 a solid five and with this we seem to                                 optimize the look of complexity of the                                 problem but having a node for every                                 record also can perhaps be seen as                                 overkill and likely there's room for                                 optimization in terms of space                                 requirements and this light it's s to                                 the third and final improvement we can                                 make which works as follows so we take                                 our sorted records and we divide them up                                 into continuous groups which we call                                 pages of a fixed size which we call the                                 page size and then we store in our index                                 not every key of every record but only                                 the first key of every page and then                                 this allows a great deal of space                                 efficiency and though our index now only                                 points to a page and not to a record                                 meaning that after we finally descended                                 the index we still have to perform a                                 search on the page since we choose the                                 page size to be again tiny in comparison                                 to the size of the number of records on                                 the balance of increased computation                                 versus decreased storage we still come                                 out in the black and with this third                                 improvement we now have modulo some                                 optimization                                 involving caches arguably them the most                                 common type of reined index out in the                                 wild and it's this last improvement                                 making our index sparse I think really                                 makes the analogy of index as model of                                 work and the reason is when we think of                                 the word model we really we typically                                 think of something that makes                                 predictions and then these predictions                                 have associated errors and with binary                                 search trees and beech trees we had the                                 notion of prediction that is where we                                 could find the record but now with                                 sparsity a model predictions gained also                                 the notion of error as the prediction is                                 not of the exact location of the record                                 but of its page further these errors can                                 come with hard guarantees after all the                                 record if it's in the database is                                 definitely not to the left of the first                                 record on the page and it's definitely                                 no more than page saves page size                                 records to the right and all in all we                                 have now a very nice system and that                                 we've got hard guarantees both on                                 prediction complexity and sorry compute                                 complexity and also on error magnitudes                                 and seeing as how B trees have been                                 around since                                                             no nothing could really work better                                 because if there were then that newer                                 technology would have long ago replaced                                 the B tree as the model of choice for                                 range indexes except they're not                                 necessarily the best option out there                                 and here's a simple counter example                                 which I should mention also comes                                 straight from the paper say your records                                 were of a fixed size and the keys were                                 to be the continuous integers between                                 let's say one and a hundred million then                                 we could have a constant time lookup in                                 our database simply by using the key as                                 an offset perhaps minus one and of                                 course is not the most realistic example                                 out there but it serves illustrate the                                 following point that the reason why B                                 trees are so widespread and generally                                 available database systems is not                                 because of the best model for fitting                                 your data distribution but because                                 they're the best model for fitting                                 unknown or what's called the average                                 data distribution of course if your                                 database                                 engineers were to know your exact data                                 buta data distribution they could                                 engineer a tailored index but this                                 engineering costs would likely be too                                 great for your project and would also be                                 unrealistic to expect from a database                                 available for general use so Thank You                                 Redis is thank your Postgres is etc                                 which leads us to the following wanted                                 ads what we want we want an index                                 structure tailored to your particular                                 data distribution which can be                                 automatically synthesized to avoid                                 engineering costs and which comes with                                 hard error guarantees as otherwise the                                 performance gains we get at prediction                                 time might be lost at seek time so what                                 could fit the bill well as you might                                 have guessed the answer according to                                 Koshka at all is machine learning and as                                 for the general reason why recall that                                 what we want the index to learn is the                                 distribution of keys in the key space                                 this distribution is a function and it                                 turns out that machine learning is                                 really good at learning functions in                                 particular the class of machine learning                                 algorithms called neural nets which form                                 the basis of all the deep learning                                 you've been hearing about for the past                                 four years in fact that machine learning                                 algorithms are so good at learning                                 functions that data scientists and other                                 practitioners of machine learning                                 typically introduce restrictions on the                                 allowed complexity of trained machine                                 learning models simply to keep them from                                 learning the data too well and here's an                                 example of what I mean say the function                                 you want to learn is here represented by                                 the blue curve and machine learning                                 problems you don't know a priori what                                 that function is but you can make                                 observations of that function here                                 represented by the yellow dots by the                                 way the reason why those yellow dots                                 don't actually coincide with the blue                                 curve is because these observations are                                 in machine learning problems corrupted                                 by some sort of noise and if we can sort                                 of make this grounded in reality here's                                 an example let's say we're trying to fit                                 the function of apartment prices here in                                 Berlin                                 and our observations would include                                 information about the number of each                                 rooms or the area in square meters or                                 the distance to the metro for each of                                 these apartments and then together with                                 the actual historical sale prices of                                 those apartments whose deviations from                                 this latent price function could be                                 explained by let's say the prejudices of                                 the buyer sorry the seller or time                                 pressures experienced by potential                                 buyers etc so now our machine learning                                 algorithm would then make a guess about                                 what this function could be then used                                 the observations together with some                                 measure of loss to calculate an error on                                 that guess and then consequently use the                                 error to make a better guess and so on                                 until the change in error between                                 guesses falls below some tolerance                                 thresholds so we try to fit curves of                                 varying complexity to these observations                                 the most simple shown here in blue well                                 we see that even with the best selection                                 of the parameters for that function the                                 resulting curve is unable to approach                                 the vast majority of observations                                 machine learning practitioner would say                                 that in this case the model is under                                 fitting and this arises when the allowed                                 complexity of the model is not                                 sufficient to describe the function                                 underlying the observations the most                                 complex curve here the green one this is                                 also doing a terrible job but for a                                 different reason                                 this one is trying to pass through all                                 of the points no matter how illogical                                 the resulting shape and this phenomenon                                 is called overfitting and what's                                 happening is that the machine learning                                 algorithm is fitting not to the latent                                 function but to the noisy observations                                 of that function and remember this                                 because that'll be important later so we                                 need some curve whose complexity                                 somewhere in between the blue curve and                                 the green curve which is here shown in                                 orange and actually finding that perfect                                 balance between under fitting and over                                 fitting is one of the hardest parts                                 about doing machine learning ok so that                                 was an example of using machine learning                                 algorithm to fit a simple function but                                 actually machine learning algorithms are                                 capable of fitting immensely complicated                                 functions of hundreds of millions of                                 parameters                                 Google translates Facebook's facial                                 recognition software and deepmind's                                 alphago these are these all boiled down                                 to machine learning systems that have                                 learned incredibly complicated functions                                 so we see that machine learning is                                 useful for learning functions how do we                                 apply this to the database domain so                                 let's say the situation is the following                                 one our database records each have a                                 unique key and all and the collection of                                 all these keys is orderable to our set                                 of records is held in memory sorted by                                 key and it's static whereby static I                                 mean actually static or perhaps only                                 updated infrequently so we have let's                                 say a cold storage or a data warehouse                                 or something like this and lastly we are                                 interested in read access in this                                 database and we're interested in range                                 queries so given these conditions here's                                 another function learning situation more                                 along the lines of what we're interested                                 in doing say we have our keys a little                                 difficult to see here we have these pink                                 lines representing keys and key space                                 and they're spread out in some way                                 amongst the allowed values what we're                                 learning is interested in learning is                                 this it's the key distribution and                                 please forgive the lack of rigor in my                                 illustration so now machine learning                                 algorithms could learn this naked                                 distribution perfectly well it's a                                 machine learning task called density                                 estimation but actually from an                                 engineering point of view the function                                 we would rather learn is the cumulative                                 feet key distribution that is a say we                                 want to give our model a key and we want                                 to have it predict say that this                                 particular key is greater than                                        all the keys according to their ordering                                 because this way we immediately know                                 that we would skip the first                                            records in our database to retrieve the                                 record associated with that key now what                                 I just described about learning                                 distributions could be termed normal                                 machine learning however there's a very                                 important difference between our                                 database indexing Aereo and normal                                 machine learning in normal machine                                 learning you learn a function based                                 annoys the observations of the function                                 and then make predictions for input                                 values that you haven't seen before so                                 for instance going back to our Berlin                                 apartment pricing model we were fitting                                 this model based on historical prices of                                 sold apartments but actually the reason                                 we want to use this model is not to                                 explain apartment prices in the past but                                 rather to make predictions of the price                                 of an apartment in the future whose                                 exact combination of features we haven't                                 seen before but with an index model not                                 only are you observe a shion's the keys                                 noise-free                                 but when it comes time to make                                 predictions you're actually going to                                 make predictions on inputs the model has                                 already seen before namely the keys                                 themselves and then this break with                                 normal machine learning methodology                                 means in fact that the situation the in                                 this situation our observations and the                                 underlying function we're trying to                                 learn are one in the same that is                                 there's nothing really distinguishing                                 the blue curve and the yellow dots which                                 in turn means that in our previous                                 example we actually would have preferred                                 the highly overfitting model that wildly                                 jumps about because it always predicts                                 what it had seen before and because                                 there are no values of the function that                                 the model hadn't seen before                                 additionally this break with traditional                                 methodology gives us hard error                                 guarantees on our predictions because                                 after training our model will only be                                 making predictions on what the model has                                 already seen and because the training                                 data doesn't change to calculate our                                 error guarantees all that we have to do                                 is to remember the worst errors that the                                 model makes on the training data and                                 that's it now I mentioned earlier that a                                 machine learning algorithm particularly                                 adept at overfitting is the neural                                 network so to test their idea the                                 researchers took a test set of                                     million web server log records trained                                 in neural network index over their time                                 stamps and examine the results and what                                 did they find                                 well they found that the model did                                 terribly in comparison with the standard                                 b-tree index                                                            slower for making                                 addictions and two to three times slower                                 for searching the error margins                                 now the author's offer a number of                                 reasons for the poor performance much of                                 it could be attributed to their choice                                 of machine learning framework for both                                 training and for making predictions                                 namely pythons tensorflow tensorflow was                                 optimized for big models and as a result                                 has a significant invocation overhead                                 that just killed the performance of his                                 relatively small neural network this                                 problem however could be                                 straightforwardly dealt with you simply                                 train the model with tensorflow and then                                 you export the optimized parameter                                 values and recreate the model                                 architecture using a faster language                                 let's say C++ for actually making                                 predictions but there's another problem                                 less straightforward which was that                                 though neural networks are comparably                                 good in terms of CPU and space                                 efficiency at overfitting to the general                                 shape of the cumulative data                                 distribution they lose their competitive                                 advantage over B trees when going the                                 last mile so to say a fine tuning their                                 predictions so put another way with a                                 sufficient number of keys from                                        feet up in the air the cumulative                                 distribution looks relatively smooth but                                 when you zoom in and you see that the                                 distribution appears relatively grainy                                 now the former situation when the curve                                 appears moved that's really good for                                 machine learning but when it's quite                                 grainy like this on the right that's                                 quite bad so the solution of the authors                                 was to replace that single monolithic                                 model with something that looks like                                 this which they called their recursive                                 model index the idea is to build a model                                 of experts such that the models at the                                 bottom are extremely knowledgeable about                                 a small localized part of key space and                                 the models above them are really good at                                 steering queries to the appropriate                                 expert below                                 no by the way that this is not a tree                                 structure multiple models at one level                                 can indeed point to the same model at                                 the level below now this architecture                                 has three principal benefits one instead                                 of training a single model based on its                                 accuracy across the entire key space you                                 now train multiple models each                                 accountable only for a small region of                                 the key space which has the net effect                                 of decreasing overall loss number two                                 complex and expensive models which are                                 better capturing the overall general                                 shape can be used at the first level of                                 experts while simple and cheap models                                 can be used on the smaller mini domains                                 so in this case we can use a neural                                 network at the top to make the first                                 initial assignment and then afterwards                                 beginning something similar like linear                                 regression and three there's no search                                 process required in between the stages                                 like in a beech tree remember I sort of                                 glossed over this but when you're                                 searching a beech tree every time you                                 hit a node you still have to search the                                 keys in that no before you figure out                                 what child you have to go do which means                                 that model outputs are simply offsets                                 and as a result the entire index can be                                 expressed as a sparse matrix                                 multiplication which means that predicts                                 occur with constant complexity instead                                 of on the order of log sub K of the                                 number of keys in your index so I should                                 mention up to now that we've been                                 discussing the beech tree database                                 indices as though they were strictly for                                 looking up individual records and while                                 they are adept at that their true                                 ability lies in accessing ranges of                                 records remember that our records are                                 sorted sort of free predict the                                 petitions of the two end points of our                                 range of interest we very quickly know                                 the locations of all the extras of the                                 records we'd like to retrieve so a                                 logical follow-up question could then be                                 are there other index structures where                                 our machine learning could also play a                                 role and that's the subject of the third                                 section of this talk like to now talk                                 about two additional types of index                                 structures                                 hashmaps and bloom filters so let's                                 start with hashmaps in contrast to tree                                 based sorry be tree based indexes which                                 can be used to locate individual records                                 but whose strength is really to quickly                                 discover records associated with a range                                 of keys the hash map is an index                                 structure whose sole purpose is to                                 assign individual records to and loader                                 later locate in an array so let's call                                 it a point index viewed as a model we                                 again have the situation where key goes                                 into the black box and record location                                 comes out but whereas in the previous                                 case the records were all sorted and                                 adjacent to one another in the point                                 index case the location of the keys in                                 this array is assigned randomly I'll be                                 the deterministic hash function so what                                 typically happens is that multiple keys                                 are assigned to the same location a                                 situation known as a conflict thus what                                 the model points you may not in fact be                                 a record of all but let's say a list of                                 records that needs to be traversed and                                 now an ideal situation there are no                                 conflicts and then lookup becomes a                                 constant time operation and no extra                                 space needs to be reserved for the                                 overflow but in the situation where a                                 number of keys equals the number of                                 array slots simply because of statistics                                 collisions are unavoidable using the                                 naive hashing strategies and collision                                 avoidance strategies cost either memory                                 or they cost time so what we want from                                 our hashing model is to make location                                 assignments as efficiently as possible                                 there's say we want to fill up every                                 available slot in our array so to do so                                 the proposal of Kroshka at all is to                                 again have the machine learning model                                 learn the cumulative D key distribution                                 that is to say the model predicts that a                                 particular key is greater than say                                     of all keys and then the hashing index                                 tries to insert it                                                      the available array slots and of course                                 should there be a closed                                 which is bound to happen if there are                                 fewer slots then keys the regular                                 collision of resolution techniques could                                 still be applied the point is that by                                 avoiding empty array slots in the first                                 place these costly collision resolution                                 techniques will have to be useless                                 frequently so that's hashmaps moving on                                 to bloom filters we are now interested                                 in an index structure that indicates                                 record existence so let's call it an                                 existence index specifically a bloom                                 filter is a model which predicts whether                                 or not a particular key is stored in the                                 database with the additional requirement                                 that a prediction of no have an error of                                                                                                     error but this air can be deterministic                                 we mitigated typically by giving the                                 model access to either additional                                 compute and/or additional memory from a                                 machine learning perspective this seems                                 like a job for a binary classifier that                                 is a model which predicts a percentage                                 between                                                               value such that predictions above that                                 number are classified as being in the                                 database and predictions below are                                 classified as not being in the database                                 and so just as in the range and point                                 index scenarios we have to break with                                 standard machine learning methodology                                 but this time we do it in a different                                 way specifically usually when we train a                                 binary classifier we feed the model                                 examples of both classes but in this                                 case we only have examples of the                                 positive class that is to say the keys                                 which are in a database so the first                                 trick we have to use is to just make up                                 some fake keys that is values which come                                 from the allowed key space but are not                                 actually used by our records and in                                 these fake keys we add to the collection                                 of real keys and then we use this                                 combined data set to train our models                                 the second trick we use is to adjust the                                 threshold value to match our desired                                 false positive rate remember the set of                                 keys is static so                                 we just keep changing that threshold                                 until we reach that desired number now                                 well of course still be left with a                                 false negative rate which remember we                                 need to get down to zero so trick three                                 is to actually just make a separate                                 bloom filter a traditional one which                                 will be applied to all the keys                                 predicted by the machine learning model                                 to belong to the negative class as a                                 double check just to ensure that we get                                 that negative false sorry that false                                 negative rate equal to zero and while                                 that might be seen as a sort of cop-out                                 and talk about using machine learning to                                 replace database structures we still we                                 still greatly reduce the resources                                 required to implement our existence                                 index in particular because bloom                                 filters scale linearly with the number                                 of keys they were responsible for and                                 given that the number of keys our                                 overflow bloom filter will be                                 responsible for scales with the false                                 negative rate of our machine learning                                 model even if the binary classifier has                                 a let's say                                                         we've managed to reduce the size or                                 bloom Fister we need by half so I've                                 told you about machine learning models                                 and how they can be used to supplant or                                 complement B trees hash maps and bloom                                 filters for the purposes of range point                                 and existence indexing respectively what                                 I haven't told you is how well machine                                 learning based index systems held up                                 against their classical counterparts so                                 now how were crash Grinnell's                                 benchmarking results all the results                                 were good at least according to Google                                 and while I don't have time today to go                                 into the details of the benchmarks and                                 nor am i as a data scientist really on                                 solid enough footing to be able to                                 evaluate the appropriateness of the                                 tests that they performed I think I can                                 confidently say that this idea has                                 opened up the possibility for new                                 research programs which especially given                                 the increased likelihood for the                                 inclusion of machine friendly machine                                 learning friendly GPUs and potentially                                 even GPUs in commodity hardware                                 may very well result in the adoption by                                 future database systems of the ideas I                                 talked about today rounding up I'd like                                 to thank Tim Kroshka Alex bill tell EDG                                 deft teen and Niklas Bowie Soltis for                                 their novel idea I'd like to thank my                                 employer go data-driven for flying out                                 here and let me speak to you on company                                 time and I'd like to thank you the                                 audience for your attention and now if                                 there's any time for questions I'd be                                 more than happy to feel them we've got                                 some time for questions are there ones                                 there's one here in the Nova and it's                                 not here hello texting interesting talk                                 so one of the things you mentioned is                                 that your models and all the things in                                 your database but in a usual production                                 use case                                 you're always inserting records what                                 does that mean every time you insert a                                 new record you need to be retraining                                 your model that's correct                                 so the scenario given in this talk is                                 that as you say that the it's it's                                 static for some time frame the authors                                 of the paper do address the concern of                                 doing either inserts in the middle or                                 pens on the end yes when you have new                                 data you have to retrain your model so                                 they propose a couple solutions one you                                 either have place or like an overflow                                 buffer and then only periodically update                                 to your database in the first place only                                 needed to be updated let's say one time                                 at night and so that wouldn't really                                 interrupt your system and three they                                 also consider the idea that you could                                 insert but then you don't just insert a                                 single record like you do in a tree you                                 have to insert let's say some additional                                 block of space which you determine based                                 on your previously calculated cumulative                                 key distribution so you make the                                 assumption that this distribution is the                                 same regardless of how many keys you                                 enter in the future                                 can you please pass this down Thanks so                                 a little bit similar question the data                                 set was static but what happens with a                                 cold start so you still need to have                                 some sort of distribution at the                                 beginning to train the model and usually                                 when you have a use case I'm starting I                                 have a business model but no data how I                                 will build the index without the data                                 the point is that you don't so this                                 particular use case is when you already                                 have some body of data and then want to                                 build an index on top of it nothing                                 the authors don't propose that this is                                 the solution for every you case they                                 just say to here the specific you cases                                 where it could be advantageous so maybe                                 I'll ask another question                                 is all of this still purely theoretical                                 or was they'll be like a database                                 outdoor and development that will be                                 able to take this kind of like modeled                                 index yeah put it to use so I said this                                 paper came out in December so December                                                                                                          and the public knows this is all                                 relatively theoretical aside from their                                 their tests and then part of what they                                 encourage in the papers and believe that                                 this should become a research Avenue                                 that said it's Google and I wouldn't                                 think that they would release such                                 information so lightly so my assumption                                 is that there's at least some                                 experimentations going on deep in the                                 bowels of Google trying to implement                                 this in production but that's just the                                 speculation on my part all right there's                                 one more here what kind of objects were                                 used for the keys right so the they had                                 a number of test data sets which they                                 spoke of they used let me see if I                                 remember so they made up a set of                                 integers based on a log normal                                 distribution they have a set of                                 longitudes based on some map data they                                 had a set of document IDs which were                                 strings and they had a set of time                                 stamps which were also integers now I                                 didn't want to talk too much about the                                 results but the results show that for                                 integers this performs my                                 better strings are when you have                                 difficulties and that comes largely                                 because of the challenges in                                 representing variable length strings as                                 keys and then uniform way but yet so                                 direct answer is index and strings what                                 they talk about integers and strings                                 okay let's take one more question and                                 that is here to the left                                 so those model and indices they are                                 being calculated on top of the data like                                 from the data but has there been any                                 research of actuary let's say mapping an                                 existing classic like B                                              model like try to create a model based                                 on existing indices no tomatoes now all                                 right let's think this peak again                                 you
YouTube URL: https://www.youtube.com/watch?v=0q9mxMekBeE


