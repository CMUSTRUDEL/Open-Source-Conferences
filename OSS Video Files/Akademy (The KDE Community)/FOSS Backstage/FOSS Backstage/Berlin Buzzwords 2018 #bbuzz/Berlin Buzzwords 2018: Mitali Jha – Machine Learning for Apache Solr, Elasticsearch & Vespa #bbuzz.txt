Title: Berlin Buzzwords 2018: Mitali Jha â€“ Machine Learning for Apache Solr, Elasticsearch & Vespa #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as "learning to rank" (LTR) or "machine learned ranking" (MLR) etc.

In this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.

Read more:
https://2018.berlinbuzzwords.de/18/session/machine-learning-apache-solr-elasticsearch-vespa

About Mitali Jha:
https://2018.berlinbuzzwords.de/users/mitali-jha

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi everyone I'm Natalia and I work on                               machine learning projects in a company                               called crossover so I'm sorry for all                               the experts here because my talk is                               geared more towards the beginners of                               learning to rank that is the developers                               who have already built an a search                               engine application but I get to leverage                               the machine learning capabilities for                                ranking now this is just a very                                high-level introduction on how to use                                learning to rank and it's not intended                                for the audience who has leveraged the                                machine learning capabilities so what is                                learning to rank now learning to rank is                                a class of machine learning algorithms                                that outputs models that are used for                                ranking the query search results based                                on optimization of feature values now                                generally what happens is that when a                                developer starts building a search                                engine application he normally Tunes the                                ranking parameters in a way so as to                                achieve maximum achieve as relevant                                results as possible however with the                                machine learning capabilities capturing                                user behavior and logging so better and                                more optimal results can be achieved                                without even having to have the domain                                expertise by training the machine learnt                                model and using this trained model for                                banking so now I would walk you through                                the demos of the three search engines                                starting off alphabetically from                                elasticsearch so the LTR plugin for                                elastic search comes as a third-party                                plugin oh it happened okay comes as a                                third-party plugin which is provided by                                open source connections this was                                developed by dr. bull so if you have any                                queries you can always reach out to him                                he must be here somewhere then the basic                                concepts here are feature stored now                                what is a feature store a feature store                                is actually a place where different sets                                of feature sets get stored say for                                example                                a team identifies defines a set of                                feature sets which gets stored in a                                feature store and at the same time of                                different team defines yet another set                                of feature sets which goes and sits in                                the same feature store at the same time                                living side-by-side parallely so during                                training we need to select one of the                                future stores from here which goes ahead                                to trainer model the next concept is to                                is a model store this is similar to a                                feature store but here the algorithm                                which has so the algorithm must have                                generated a model which needs to be                                uploaded into a model store onto the                                search engine now while rewriting the                                query results we need to select one of                                the model one of the models from this                                model store and which actually helps in                                which then goes further to Arirang the                                feature value scores the third concept                                is query D scoring now what happens is                                that a search engine originally read                                ank's its query results on the basis of                                what is called as native ranking or                                native scoring but in order to achieve                                more optimal results there's a concept                                of query scoring which then read ank's                                which is also called as second phase                                ranking which reruns the feature value                                sets in order to achieve better results                                so this is the place where query D                                scoring comes into picture then elastic                                ltr' supports libraries like rank clip                                and extra boost and rank lip has got a                                set of algorithms like it supports                                linear regression logistic regression                                then decision tree based models then                                lambda mat etc so for the purpose of                                this demo I have used the movies                                database tmdb dataset which comes as a                                toy example with the learning to rank                                elastic search demo it consists of                                movies for further reference you can go                                to this link and documentation so                                I now walk you through the demo                                elasticsearch is running just okay so                                the first script is the Python script                                for utils which is actively accessing                                which is used for accessing the                                elasticsearch cluster it comes it                                contains parameters like elasticsearch                                 host authentication password hostname                                 and the port so we run the script the                                 next script is the python script for                                 prepare which actually downloads the                                 tmdb data set and the rank chevron jar                                 file so the interest of time and the                                 fact that internet usually doesn't work                                 during demos I'm not going to run this                                 file we move on to the third step so the                                 very first step after starting up                                 elasticsearch in LTR demo is to Index                                 this tmdb data set so we run this script                                 and we see here that elastics are                                 elasticsearch Klein gets initialized and                                 reached the team DB data set which then                                 gets indexed on to elasticsearch since                                 this data set contains lot of records                                 almost about                                                             to wait for the indexing to complete                                 rather move on to the next step which is                                 to train a model so training a model                                 involves some helper methods like                                 judgments taught by collect feature stop                                 I and load features taught by so we also                                 see here that a path gets created where                                 features are stored in order to be used                                 for our models and we go back and see if                                 the indexing has been completed or not                                 okay so indexing has been completed and                                 we see that poison is the last movie to                                 be indexed which is also the last thing                                 to have in life if you ever choose to                                 have it so yes we run the script                                 and then move on to the Python script                                 for train so this is a very important                                 step because this involves training the                                 module this starts off by connecting to                                 elasticsearch and the very first step is                                 to create an initial default feature                                 store for the purpose of this demo we                                 have identified certain features like                                 title and overview we see here that                                 these features are identified by                                 ordinals so the main idea here is that                                 the user defined key verse should match                                 the title for a particular query which                                 then becomes the feature score for this                                 feature title                                 similarly the overview match for this                                 query becomes the feature score for the                                 feature overview now a feature in                                 elasticsearch is an elastic search query                                 and it can be anything like say how old                                 a movie is or anything that correlates                                 to the user's sense of relevance and                                 feature scores that I yielded here are                                 then used for training and evaluating                                 the module so we define a feature store                                 here and then this is an empty feature                                 store obviously and then the second step                                 is to load these feature scores back                                 into elastic search then comes the                                 concept of judgments list so I show you                                 what a judgments file looks like ok so                                 this is a three tuple judgment list                                 which actually tells us how relevant a                                 document is with respect to a search                                 query it transits of grades query ID and                                 the query itself so we see here that the                                 movie Rambo for query ID                                            Rambo receives a grade of                                               that the user found this movie to be                                 extremely relevant for query Rambo                                 similarly the movie first daughter                                 receives a grade of                                                     which means that the user found the                                 first Auto movie to be not to be                                 relevant at all for the query tampo and                                 henceforth so we use this judgment a                                 sample judgments dot txt file now in                                 order for a ranch lab model to work it                                 needs this judgment file along with the                                 two more columns which have the feature                                 score values which are extracted from                                 the elasticsearch so we run this and we                                 see here that the ranked clip this                                 training is happening for each of these                                 ten modules and train module simply uses                                 the sample judgments with features file                                 and the rank clip then generates a                                 module and save model so the model                                 generated from Lang clip is then                                 uploaded into the model store of                                 elasticsearch we learn this and see that                                 each of the ten models are getting                                 trained so while this takes some time we                                 move on to the next script with which is                                 actually acquiring acquiring the results                                 and re ranking them on the basis of this                                 trained model so we see that this is a                                 simple elastic search query across the                                 two fields title and overview and this                                 is an important step here which we're LT                                 our plugin comes into picture so this is                                 actually d scoring or re-ranking or the                                 second phase of ranking of the query                                 results based on the parameters keywords                                 and model we see if all the models have                                 been trained and yes all the ten models                                 have been trained we quickly see what                                 one of the model files look like okay so                                 this is for lambda mot model and we this                                 is an XML format we see that the                                 features core value for feature one                                 which is title is ten point three six                                 something and for feature two which for                                 overview the feature score value is                                 eleven point nine five okay so we run                                 the Python script for search taught by                                 and then we run the final query which is                                 searching for Rambo query in the lambda                                 math model and we see the order of the                                 movies so this is the order in which the                                 feature values of the movies have been                                 read and of what the model thought                                 should have been the best order so this                                 pretty much covers the demo for                                 elasticsearch then we move on to the                                 learning to rank for solar                                 so the LTR plugin for solar comes out of                                 the box as a contrib modules which was                                 initially provided by Bloomberg the                                 basic concepts remain the same feature                                 store model store and query D ranking                                 and it supports libraries like libous                                 feelable inia rank a partial because it                                 supposed only few algorithms like lambda                                 MOT being one of them and deep learning                                 for J the support of which is going to                                 come                                 so the ltr' plugin for solar demo comes                                 with tech products which you can find                                 here you can go through this                                 documentation but for the purpose of                                 Apple to Apple comparison I have used                                 the same tmdb data set which I used for                                 elastic search so we'll go to insomnia                                 rest client and the very first step is                                 to create collection with shard                                       the name of the collection is tmdb ok                                 then we index all the data on to solar                                 this again is going to take a while                                 because it contains about                                               so while the indexing is happening I'll                                 walk you through the next set of steps                                 which involves enabling the LTR plugin                                 so there are three main components for                                 enabling the LTI plugin one is to add                                 the ltr' query parser the second is to                                 add feature cache feature cache is                                 on top of the feature store which is                                 used for caching features and the third                                 step is to add LTI transformer now we                                 see whether all the documents have been                                 indexed okay so all the documents have                                 been indexed here then we add the ltr'                                 query parser add the feature cache we                                 can also set the size for this feature                                 value cache and at the LTR transformer                                 now we add the features here so just                                 like a feature in elastic search is an                                 elastic search query feature and solar                                 is a solar query so now we hit the query                                 to extract features which is going to                                 extract all the features core values                                 from solar ok so we see that the value                                 for the title feature is                                          something and                                                        overview feature which is almost similar                                 to what we saw in case of elastic search                                 since the feature values scores are                                 similar the models will also be similar                                 hence I'm not going to retrain the model                                 rather use the same model which was used                                 in case of elastic search and I add the                                 model here                                 this takes about                                                      add these feature value scores we can                                 plug into the judgments list and hand it                                 over to Tran clip to train and rank the                                 model ok and then we run the final ltr'                                 query for the strained model so the                                 re-ranking scored so it uses the                                 rehanging parameter with the default LCR                                 model and it rewrites the top in                                 documents which is                                                      arirang stir query results and we see                                 that the order is the same as what we                                 saw in case of elastic search so this                                 completes the demo for solar LCR and                                 then we move on to Vespa so Vespa offers                                 the first-class support for LTR basic                                 concepts include tensors so like we saw                                 that in elastic search or solar the                                 feature value scores are scalar                                 quantities in case of Vespa it can be                                 multi-dimensional quantities as well                                 which are called tensors so tensors can                                 be anything ranging from scalars to                                 matrices to higher dimensional values                                 and then models of course and the first                                 and second phase ranking expressions so                                 second phase ranking is required on top                                 of the native ranking to achieve better                                 results then Vespa supports tensorflow                                 library okay so we use real world data                                 here from one of the panel competitions                                 which handles the block-post about                                     million blog posts are there and                                 captures users preferences for lighting                                 those blog posts you can always go to                                 this documentation for further                                 information so for the purpose of saving                                 time and that the fact that I've got a                                 very low handle laptop and the data set                                 was huge                                 I have pre-recorded the demos I'm going                                 to run through them okay so I start off                                 by starting vespa through a docker                                 command which assigns                                                 memory                                 the application is huge so especial was                                 already running in the background so I                                 did not have to start it and then                                 ensuring that Vespa has started properly                                 the application status is                                              good and then the third step is to                                 deploy the sample application now what                                 is a sample application it consists of                                 models Kweli profiles search definitions                                 and a definition of which hosts the                                 application will run on search                                 definitions the schema of the                                 collections the collections here are                                 blog post and users now a document of                                 type blog post has the following fields                                 date in the gmt format language title                                 author etc apart from this schema it                                 also includes the ranking profiles first                                 phase ranking and the second phase                                 ranking okay then we see whether the                                 sample application has been deployed or                                 not so in other words whether the                                 collections have been created                                 I missed something okay                                 okay then the next step is to split the                                 data set in training and test data sets                                 the test data sets comprises of about                                                                                                  comprises                                                             computed the data in the training and                                 test data sets and also pre generated                                 the ten cells for training so I did not                                 have to run this again then the next                                 step is to train a model using                                 tensorflow library so training a model                                 took about                                                               to pre train this model and here we see                                 the output of this trained model so the                                 sample application comes shipped with                                 this pre trained model so we don't even                                 have to deploy it again then the next                                 step is to index the blog posts and use                                 a data into vespa using this big script                                 okay so once the data has been indexed                                 and the model has been deployed we now                                 see them in action so first we run a                                 query search for Pegasus we see here                                 that the Vespa query format is very                                 similar to SQL from sequel and is called                                 YQL we run this query and see the titles                                 or the interview they                                                 run the same query but with the point of                                 view of a logged in user for user ID                                 this                                 and see that the titles are different                                 I do believe or whatever because these                                 these results are more geared towards                                 users interests hence we see a                                 difference here so this pretty much                                 covers the Vespa demo                                                                                                       mind which relates which is in relation                                 to the Vespa demo is say for example                                 user hits a query for Munich and the                                 search engine comes up with results                                 relating pertaining to Munich's weather                                 or the buildings or whatever but say if                                 the if that user wants to see is more                                 interested in sports and specifically                                 football then with the help of machine                                 learning capabilities the search engine                                 should show the results say some some                                 football team in Munich so this is                                 pretty much it yeah all right let's                                 think the speaker take it I suggest to                                 take the questions offline and we now                                 conclude this first session of today and                                 the Casa house and break for lunch lunch                                 break is about an hour and taken I've                                 just taken just next doors in the patio                                 we reassemble here to or enjoy your                                 lunch and see you later thank you thank                                 you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=DyfkzOTFpDc


