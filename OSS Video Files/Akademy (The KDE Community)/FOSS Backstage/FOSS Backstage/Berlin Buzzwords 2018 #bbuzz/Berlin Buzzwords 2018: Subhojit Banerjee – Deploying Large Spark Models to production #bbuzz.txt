Title: Berlin Buzzwords 2018: Subhojit Banerjee â€“ Deploying Large Spark Models to production #bbuzz
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Subhojit Banerjee talking about "Deploying Large Spark Models to production and model scoring in near real time".

1. How does one build a pyspark model and deploy it in a scala pipeline with no code rewrite - Solving the greatest fights between data scientist who want to code in python and data engineers who like the tried and tested type safety of the JVM.
2. How does one beat the spark context latency to serve spark models in milliseconds to handle near realtime business needs
3. How does one build a ML model, zip it up and deploy it across platforms in a completely vendor neutral way i.e. build your model on AWS and deploy it on GCP or vice-versa.
4. How does one leverage the years of efforts spent in software engineering and use it directly in building data science pipelines without reinventing the wheel and pain.
5. How does on build a completely GDPR compliant machine learning model with 0.88 on the ROC curve.

Read more:
https://2018.berlinbuzzwords.de/18/session/deploying-large-spark-models-production-and-model-scoring-near-real-time

About Subhojit Banerjee:
https://2018.berlinbuzzwords.de/users/subhojit-banerjee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning as the boys clear                               modulation do I have to speak up speak                               down is this can everyone hear cool                               thanks                               firstly a note of thanks to the                               organizers and thank you guys for coming                               to this talk the intent of this talk is                               to share my war stories and hopefully                               you guys don't have to go through the                                same pain of deploying large spark                                models in production so so big leader is                                not easy and I apologize for the term                                big data for let's for our definition                                say data that doesn't fit into a single                                node and the reason big data is not easy                                is because it sets at a confluence of                                three very diverse and very deep fears                                machine learning distributed systems and                                domain of business understanding now you                                need to merge and mix these three                                different fields to make profits or                                revenue which is a very difficult task                                in itself so Gartner found that only                                    of companies have machine learning                                models in production and the top five                                vendors are in losses and ninety percent                                of all data leaks fail so does Big Data                                promise that was given to us you know                                it's failing in some ways where is it                                failing Gutner tried again to find out                                why this is failing the top the top two                                are very organizational related reasons                                so last yesterday mention about the data                                being in silos and it's very difficult                                to get the data out of those silos                                also companies do not yet know how to                                take advantage of big data but this talk                                is not about the first two slifer's two                                points this talk is about the last point                                that is how do we deploy machine                                learning models quickly to production                                the slide that you see here this is the                                ML code and this is from the paper                                hidden technical debt in machine                                learning this was                                nips                                                                   code is the machine learning code right                                and the the color gradation is based on                                the complexity of the code so this is                                possibly the most complex part of the                                code and this is possibly the easiest                                part of the code but the size of these                                boxes represent the amount of code that                                you have to write most of the media                                concentrates on this part of the code                                but to ensure that the entire data                                pipeline the production pipeline goes                                into production you know you need to                                talk about configuration data collection                                feature extraction process management                                analysis tools serving infrastructure AV                                testing and the entire kitchen sink as                                per the paper                                                        machine learning code the remaining                                    is the glue code the way I structured                                this presentation is I'll be going                                through a business use case I'll present                                the first solution what problems have                                faced I'll mention about a better                                solution and the most interesting part                                the demo hopefully they'd ever go                                observe with us today I'll present the                                conclusions and then we'll be having                                questions so key takeaways at the end of                                this presentation hopefully you can take                                this along with you                                a better than random model is has                                revenue generating propensity from day                                one and I'm not talking about medical                                grade machine learning models you know                                for that you possibly want                                        possibly an ROC of                                                     but you know for standard business                                related machine learning models a better                                than random model has a better                                propensity of generating revenue right                                from day one pi spark models can be                                deployed in scalar pipelines so now                                there is a war going on between data                                scientist and data engineers you know                                data scientist want to write their code                                in Python and R and push it over the                                wall to the data engineers who have to                                again learn what the data scientists                                were thinking and write it into a Java                                code or a scalar code you know and in                                this process of throwing the models                                across the wall                                certain assumptions that the data                                 scientists made just falls apart                                 and so it becomes a huge pain for the                                 data engineers who don't know about the                                 data science process to write code in                                 Java that mimics the entire complex data                                 science pipeline data science or the                                 machine learning models right so the                                 intent here would be to show you how                                 data scientists can directly without                                 data engineers directly without seeing                                 any part of the data science code can                                 push the models into production or have                                 a pipeline for that spark models can be                                 scored in near-real-time                                 so spark you know it good it's very good                                 for large processing but when you have                                 to process a single row of data spark is                                 not that good because of the distributor                                 tax and I'll go into it how we can solve                                 that problem spot models can be dock                                 rised and once you docker is a model so                                 docker today is the currency of scale if                                 you can juggle as a model you                                 immediately get the advantages of CI c/d                                 a/b testing scale without you needing to                                 put any effort on that when I say you                                 it's the data scientist and all this                                 above can be done in a matter of minutes                                 and I'll show you how in the demo and if                                 you are in Europe gdpr has already come                                 in and so if you have a data scientist                                 you need to take care of personal                                 information                                 suta anonymize data using pseudo                                 anonymous data it is possible to get                                 good machine learning models and I'll                                 show you how I got a result of                                         the ROC curve in one of my gates so                                 roughly in March last year business came                                 to me and said that boogy we have this                                 website people come to this website and                                 buy stuff in real time can you even                                 before the session completes can you                                 identify whether the user is going to                                 buy or not buy a product you know so                                 that if he's if you know the user intent                                 we can show him certain discounts so                                 that we can change the intent of the                                 user right so it's a very standard                                 problem nothing complex trivial it's a                                 real-time segmentation                                 of user into a real-time classification                                 of the user into by versus differ but                                 what makes this problem tough is it has                                 to be real-time and it what it makes                                 even more tougher is that it has to be                                 gdpr compliant                                 right so use a segmentation on data on                                 using personalized data that is it's a                                 solved problem you have it everywhere                                 right but on an anima is data that is                                 something that makes this problem                                 interesting but first we need to collect                                 data right this is the real time even we                                 need to collect the events and I need to                                 also mention about the constraints so                                 that I was the only guy who was doing                                 both the real science at the data                                 engineering and I wanted to limit the                                 DevOps because then if I don't limit the                                 devops then I have to do the divorce as                                 well you know so from day one I had to                                 use all managed infrastructure as much                                 as possible so that I can concentrate on                                 the good parts so the solution that I                                 chose was snowplow who have you who has                                 used snowplow before snowplow is a                                 skinner based even collection and                                 processing tool the good thing is it's                                 just plug plug and play so you have the                                 website you can collect events from the                                 website in real time you can then                                 transform and enrich it and then dump                                 the data into s                                                          your machine learning api's on the                                 events as they are coming in and each of                                 those components that you see they                                 collect the transform these are                                 individually scalable so you can have                                 multiple copies of this of these same                                 and then you can scale up and down                                 depending upon your needs so this is                                 your standard lambda architecture                                 nothing fancy you have all seen this                                 this is the data comes in the real-time                                 evens come in it this is based on AWS                                 it's hitting the kindnesses roster from                                 the kindnesses roster it comes into the                                 kindnesses enrich and also it goes to                                 the batch processing pipeline here you                                 have a schema schema registry or a clue                                 server that basically disambiguates the                                 good events from the bad events and the                                 good events from the good even so you                                 can actually hook it up to elasticsearch                                 or a machine learning api to actually                                 get                                 or the bad events where you can actually                                 process it later on the on this side you                                 actually have the events getting                                 collected and once it lands on s                                  you can use EMR or any big data                                 processing tool to create models out of                                 it I don't and this is a conference of                                 scale people have shown huge events but                                 this was at the start of when I was                                 starting of the project we just had                                    million events per day it's not very big                                 but that doesn't mean that we didn't                                 have the we shouldn't have the                                 discipline of ensuring that it can be                                 auto scaled because the traffic was                                 pretty volatile so we collected about                                                                                                       initial machine learning model building                                 and now we have collected a lot of data                                 once this pipeline has been set up and                                 the cost was pretty cheap you know six                                 to                                                                      so the first solution I tried to do a                                 performance test and just I doubled the                                 load on that just to check whether my                                 where my pipeline is able to take the                                 real time even so not and most of the                                    percentile I was able to get with less                                 than half a second with no errors which                                 was good enough for me                                 for to get this process started ok so                                 now that I have the business use case                                 how can we have a real-time pipeline                                 that we can train and screw around with                                 ml models on the first model that I used                                 was a Markov chain model and remember                                 the GDP our compliance thing I cannot                                 use any of the person's ADA so what I                                 took was the sequence of the web pages                                 visited and then I created a Markov                                 chain model out of on that it was I made                                 the model in our I uploaded it to s                                      using lambda I exposed it as an API                                 right but there were some problems faced                                 AWS lambda gives you a restriction of                                    MB compressed size so on my first try                                 to hit                                                                  model so I had to hack on it I brought                                 it down to                                                      recommend anyone hacking on the core our                                 libraries are is not supported on AWS                                 lambda so I had to create a Python                                 wrapper on top of it so that I can use                                 my AWS lambda on it                                 but here comes the last problem and this                                 is the final coffin in the nail for the                                 Markov chain model every time front-end                                 would change the sequence of webpages                                 visited would change and hence my Markov                                 chain model that I built on the old data                                 is not valid anymore right                                 so it's stall again we hit the cold                                 start problem for every time front-end                                 would change so we needed a better                                 solution but before a better solutions                                 have a few learnings you know the effort                                 was well worth it because I could see                                 almost my practice the economies of                                 Solace and this is from John McAfee                                 sorry for the name dropping this                                 gentleman had way back in                                               that computing is one day going to be a                                 utility so you just like you open your                                 tap and you you just pay for the amount                                 of water that you use you could just                                 start using computing and you could just                                 pay only for the compute that you can                                 use so it was really nice to see in real                                 life on the what the dream was and how                                 it actually materialized one fact though                                 was that John McCarthy was a Turing                                 Award winner and I had married three                                 times no correlation between those two                                 though requirements so we had to come up                                 with a better solution the better                                 solution was I had to decrease the time                                 of the models to move from notebook to                                 production right that's very important                                 for me that'll be super scalable and the                                 PI spark models I should be able to                                 deploy it with minimal or almost zero                                 code change and serving our inference                                 had to be super fast because this is a                                 real-time use case that we are talking                                 about technical analysis of this user                                 requirement for model super fast for                                 model serving to be super fast we had to                                 take it out of the super of the spark                                 context because of the distributor tax                                 of spark                                 it has to be completely vendor-neutral I                                 I mean if I'd build my model in the my                                 own datacenter I should be able to                                 deploy it to AWS or I build it on a                                 tablet I should be able to deploy it on                                 GCP and it has to be truly portable that                                 is can I see realize the model and send                                 it to my coworker and he scores the                                 model in his own ecosystem you know so                                 this was the stringent requirements I                                 had but wise pass slow in scoring spark                                 it's very good for large data processing                                 you know but while processing a large                                 amount of data it creates a stack it                                 creates this execution plan and it                                 creates this lineage for large data                                 that's fine because this distributed tax                                 is being spread across to this entire                                 large the entire data set right which                                 runs into gigs that's fine but when you                                 are scoring a single row of data this                                 distributed tax becomes very cumbersome                                 for one row of data to actually use for                                 real-time processing so what were my                                 available options that I had to bring                                 the spark model out of the spark                                 ecosystem I had a few metrics and I had                                 a few options these slides will be                                 available to you later on so you can                                 actually go what are the strengths and                                 weaknesses of it but finally I chose                                 Emily because it had support for the                                 languages that I was using and the most                                 important part was my entire pipeline I                                 could basically deploy it outside the                                 spark ecosystem so that was not                                 available in the other products and also                                 I need to mention that data breaks has                                 its own DB ml which also does this but                                 at that time it was only available to                                 the data breaks customers and I wanted                                 something that was open-source first                                 things first                                 Emily was is possible because of the                                 good work of Holland Wilkinson mikowski                                 minute what is a B it's a common                                 serialization format for executing                                 machine learning                                 blinds it supports sparks chi can learn                                 intensive law and once you see there is                                 a model you can run it on AWS GCP so you                                 get the model out as a bundle as a                                 serialized model and you can just run it                                 anywhere using a docker container for                                 most parts you don't have to change any                                 of your internal code and it's open so                                 you can go under the hood and change the                                 code yourselves in fact I had to change                                 a part of the code and I'll show you                                 that part so that I could deploy it on                                 areas so this is just a visual                                 representation of how this looks like so                                 you have the data layer you have spark                                 and on top of it you have Emily which                                 creates MD bundle which is the                                 serialized model and then using the                                 Emily brown time you can actually create                                 an API using the MVP runtime on top of                                 it this is a quick components that it                                 has the the main thing here is the                                 linear algebra library that is very                                 important because the linear algebra                                 library it that is the main thing that                                 routes you back to the spark ecosystem                                 so if you have one-to-one mapping                                 between M leap and spark you can move                                 all these models away from this particle                                 system to the new ecosystem I mean if                                 you look at the complexity of what we                                 are trying to achieve                                 you're trying to move their entire spark                                 models out of spark so all the data                                 frames transformers estimators that you                                 have in the spark ago system should also                                 be available in the new ecosystem the so                                 this bit the bundle ml this is one of                                 them cool components and what it does it                                 provides you a common serialization for                                 your spark and Emily it is                                             wrote above and JSON based so it's so                                 you can deploy it anywhere and because                                 most of the supporting systems today                                 have a support for protobuf and json and                                 can be written to zip files and it's                                 completely portable so yeah and this is                                 the m leap pipeline so I would one of                                 the so the in the demo what I'm going to                                 do is create a model in spark use the M                                 leap processing                                 to convert at the spark model and                                 serialize it out and then once it's been                                 serialized I can expose it as an API and                                 so this is a similar sort of diagram so                                 I'll be building the my model in a                                 notebook using spark and Hadoop and then                                 I'll be using my Emily transformers                                 basically I'll create a bundle out of                                 that and then expose it so about that                                 and expose it as a API right and all                                 these containers can basically use the                                 REST API to read the data these are the                                 available power transformers and                                 estimators that we have in Emily                                 it supports almost everything that you                                 have on spark the last time I checked it                                 didn't have support for Els so but the                                 thing is you you could do your custom                                 transformation and you'll just get pull                                 away from it okay so now the demo so we                                 are going to build a PI Spock model                                 using cable data in a chip eater                                 notebook we are going to seal as a model                                 in a JSON and protobuf format we are                                 going to load the serialized pipeline in                                 a docker container for near real-time                                 model scoring and we are going to solve                                 the docker container from a scalable AWS                                 REST API in minutes now I'm using AWS                                 but you could use any container based                                 platform you know manage kubernetes                                 works fine as well as long as it can                                 just run docker you know okay let's                                 shift now to some code is this visible                                 is this visible yeah okay                                 um you do you want me to enlarge it a                                 bit it's it good yeah so this is the                                 cattle there's one cattle computation                                 where you have to predict house prices                                 this is a sample I can't use production                                 data so I'm just using this data to just                                 show you this is the house sales data                                 and in real time so basically I'm just                                 going to run over and quickly create a                                 model the intent of this notebook is not                                 the actual data processing or creating                                 the model but how once you create the                                 model how do you expose it out you know                                 that's the part that you're most                                 interested in so so this is your                                 standard thing that you would do you                                 know find out the features find out the                                 correlation between them and then                                 basically identify which of these                                 features gives better bang for your for                                 the time spent and finally you come up                                 with the features that are the most                                 important you know so this is where it's                                 time for spark to actually use this                                 features that we have generated into                                 this pipeline and ensure that we can use                                 them leap from this right so this is                                 where the actual I'll start my notebook                                 chromium so I'm just using the standard                                 libraries you know the spark libraries                                 that you love and creating custom schema                                 out of this and this is based on the                                 fields or the features that I'll be                                 using I'm reading the CSV file and then                                 I have the continuous features and the                                 categorical features here and I've run                                 this notebook earlier so that because we                                 have less time here but the the main                                 part I'm going to run it and I'm going                                 to show you this is just to show you the                                 flow of thought as we are going forward                                 and then I scale the features the                                 convenience features and the categorical                                 features so I run my transfer of the                                 standard transformers on that and then I                                 create a pipeline so let me just show                                 you this so once I create the pipeline                                 with both the pre transformers and the                                 post transpose                                 transformer Sonic I run a linear                                 regression on this and what I do is                                 basically I am going to transform the                                 data set it's a very small data set so                                 that I could just show you very quickly                                 as part of this demo and yeah so yep so                                 you have that and it's pretty quick                                 let me just chime this in because it's a                                 local data set let me just time this and                                 just show you how much it how much this                                 takes and it's close to two hundred and                                 fifteen milliseconds this is not a                                 distributed data set you know this is                                 just a in house on my laptop or an Spock                                 takes for a very small data set you know                                 it's two hundred fifteen milliseconds                                 now comes the important part okay so                                 till here your data scientist can do the                                 job okay till this part your data                                 scientists do it and then this is where                                 they would throw it to you to actually                                 run your to basically change the entire                                 code to into Java but they don't have to                                 do it all that they need to do is                                 basically run this bit of code just two                                 lines import em leap and the serializer                                 and what you need to do they do is just                                 see they are seedless this bundle out                                 that's all                                 so your entire machine learning pipeline                                 including the Transformers including the                                 pre and post transformers everything is                                 now out in the PI spark LR except let me                                 just quickly show you that                                 this is visible yeah this way okay let                                 me just show you so we created this                                 right now the PI's Park Allah does it                                 okay now what I did was I moved the                                 spice Park a lot except to separate                                 folder here under models and I had                                 created it yesterday night and so this                                 is the model that we'll be using                                 okay now what i'm doing here is no no no                                 I can't shut down the okay um am i back                                 to this okay                                 so this guy over here is the M leap                                 serving server it's running in the                                 background and it's waiting from pushing                                 the models and also it acting it's                                 acting as an API server so that you can                                 get you can push your models that is you                                 can push your not only the models but                                 also your API to score in real-time what                                 I'm going to do is now I'm going to post                                 the models we just now created so this                                 was the model we had okay let me just                                 so this was the model that we had so the                                 only server that is running I need to                                 push this model the newly created model                                 to the M leap server so that's done and                                 now I need to see ok very quickly though                                 let me also show you the JSON file which                                 I will be sending across                                 so this is the JSON files that I'll be                                 sending this has you know the standard                                 schema where basically the number of                                 bedrooms bathrooms                                 waterfront everything is going to these                                 are the features and this is the actual                                 data that I need to score this is just                                 one row of data that I'll be sending                                 across okay so the moment of truth                                    seconds that was two hundred two hundred                                 and twelve seconds of course that was                                 scoring a little bit more data but I'll                                 show you in the later part also that                                 with one row of data also it's actually                                 sparkles taking quite a long but here in                                 forty forty four milliseconds you have                                 the entire scoring done okay but this is                                 entirely sitting on my laptop right I                                 need to move this into a more scalable                                 platform remember the five thousand                                 ninety five percent but the presentation                                 slide that I showed you earlier in order                                 to really push the model into production                                 I need to have that                                                   code also and the more managed services                                 are used the better it is for me because                                 then I can concentrate on the real                                 business rather than solving issues                                 associated with their DevOps so some                                 time back Amazon came up with something                                 called as Amazon sage maker and this                                 basically provides you a platform where                                 you can create your models and also                                 deploy your models so that you can                                 create an API in real time so before                                 that what I did was the docker container                                 that I created I think it's better I                                 also show you the docker file that it's                                 a pretty simple docker file so here the                                 main thing though is the EM leap serving                                 this is the bit of code that I had to                                 change a little bit to change it                                 according to the needs of the amateurs                                 on Amazon Sage maker docker requirements                                 but other than that there's no change at                                 all                                 so I created the docker container and                                 what I did was I uploaded it to the                                 elasticsearch contain the elastic                                 container service of Amazon it's a fully                                 managed service and this is basically                                 the container that I uploaded it to ok                                 and using Amazon sage maker what I did                                 was I created so this is the model I                                 created the model from here I created an                                 endpoint configuration and using that I                                 also created an endpoints to save time                                 okay so one good thing though here is is                                 this bit you know the auto scaling you                                 can set it on so your API is you you                                 don't know when the volatility hits you                                 so you can just buy a click of a button                                 you can actually put on or off the auto                                 scaling you can also turn on you can                                 increase so you have used a very small                                 instance but you can use a larger                                 instance as well and of course you can                                 increase the instance count but the good                                 part about this is the logs the metrics                                 and you could also create an a/b test                                 all out of the box without you having to                                 change a build any part of that code                                 anyone who has built that part of the                                 infrastructure knows how difficult it is                                 to build and then maintain it ok so now                                 that that endpoint has been created does                                 it work so this is the JSON file that I                                 showed you earlier                                 okay                                 this is the JSON file we also send it to                                 the docker container if you remember in                                 the command line this is the same JSON                                 file and it's the same one single row                                 what I'm doing is I am sending this to                                 hopefully if I can get there                                 so there you are I send that request and                                 I'm not sure if you can see here you                                 know very quickly it I could just send a                                 request off to AWS get back the results                                 in um I can't see the but I think it's                                 can someone tell me what the number was                                 there                                             so that was the request sending part to                                 it to Amazon and getting back the                                 results it has the same results that we                                 we also saw earlier and yeah that's                                 that's that I have to change the keys                                 though so let's go back to the                                 presentation so we build a PI Spock                                 model we see realize the model out we                                 created docker container out of it and                                 then we pushed that model to AWS for                                 real-time model scoring coming back to                                 the GTO compliance and the model how                                 quickly what sort of results can we got                                 by using anonymized data so this was                                 something that I had to run for one of                                 my clients and using gdpr comes and                                 features for the same use case that I                                 was started off you know with the                                 real-time user segmentation I was able                                 to achieve                                                               entire model using M leap I could score                                 the model in                                                        gates of TM and this is based on their                                 exes                                                           clickstream is basically as it follows                                 the seasonal patterns you need a bunch                                 of features to identify the seasonal                                 patterns and this is the features that                                 gave the best results and I just wanted                                 to check if this is just a one-time hit                                 or is it applicable to all use cases so                                 I tried the same the same model and the                                 same sort of technique and the same sort                                 of features on this talking data at                                 tracking fraud detection challenge it                                 was just recently concluded on cable and                                 although I did not win the competition                                 but this is I wanted to check how far in                                 a single model                                 q because this model was the random                                 forest model and if I could get good                                 results here I can immediately deploy it                                 using the model framework that I showed                                 you earlier there I was able to get                                      on the ROC curve so the                                                 in production was not a fluke because                                 it's giving you the same sort of result                                 on publicly available computation data                                 as well okay conclusion this is a                                 timeline of man's possibly man's effort                                 to control electronic chaos on the top                                 part you have this line that is that                                 shows the history of computer science                                 and on this side is your data science                                                                                                     machine                                                               came out but I'm pretty sure that they                                 lost the code Midway before that                                      there was a World Wide Web                                          pretty interesting                                 you had AWS you have the Solaris zones                                 and also the MapReduce paper came out                                                                                                                                                                                more and more attention and                                             coined the term ml was quite machine                                 learning I'm using the electronic                                 trading part here because possibly                                 electronic trading is where the first                                 instance where data science was used to                                 make money so this was the first                                 instance where electronic trading                                 started                                                                 model got the economics Nobel Prize and                                 people realize that yes now we can use                                 mathematics to make money but these two                                 has two separate lines that were going                                 across you know and now is the time                                 where machine learning has matured                                 itself to go into product                                 now we could build this we could                                 reinvent this entire pipeline ourselves                                 or use the effort that was done using                                 computer science that computer science                                 has already invented and fine                                 found out and fine-tuned by using a                                 darker base infrastructure or by                                 changing a model into a docker container                                 we immediately can use the same currency                                 which the computer science pipeline can                                 use by using docker based machine                                 learning model the entire shebang that                                 is associated with your machine learning                                 pipeline you know the                                                   was showing you earlier that without any                                 effort a very minimal effort you can                                 achieve it anecdote I would like to end                                 this with a small anecdote this is from                                 the book art and fear and the the                                 ceramics teacher came into the class and                                 divided the class into two bits into two                                 halves one half say was going to be                                 evaluated on the number of actual                                 articles or ceramic pieces that was made                                 you know so slowly based on the quantity                                 the other part of the class was going to                                 be evaluated on the quality of the task                                 at the end of the day what was found out                                 was the highest quality was produced by                                 the group that was being graded for                                 quantity so this was like pretty                                 interesting like why why how is that                                 possible it was found out that while the                                 the group that was geht was going to be                                 evaluated on quality was where I da ting                                 on the best piece that had to come out                                 the guys who were working on the                                 quantity peace with continuously                                 building the ceramics pieces learning                                 from the mistakes and then trying to                                 build a better piece from that now if we                                 bring that same similes to the machine                                 learning world you know data science is                                 an iterative process data the shape of                                 your data changes and new models come in                                 all that is needed is an infrastructure                                 that BR allows you to create                                 end-to-end models quickly so you would                                 build a model quickly push into the                                 production check whether it's working                                 fine or not and then it would have an                                 expiry date by which you push out the                                 models and pull in on your model so                                 hopefully in this talk I could give you                                 a rough sketch of how to do it in                                 production so that's that questions okay                                 anyone have any questions                                 you talked about cost how much does it                                 cost to deploy your new model creating                                 the model                                                               hours I used about                                                       cost me about                                                          of data but this is the air map if you                                 run it on GCP it were to be much more                                 cheaper we still have time for one more                                 question anyone                                 so did he use any historical features in                                 the classifier so something like the                                 user has injected with those pages the                                 last five pages and how would you model                                 such thing in a leap library                                 so having historic not only the events                                 current data like user agent browser and                                 some stuff like this but also historical                                 orders or something                                 is it possible to package this in the ml                                 leap transformer so right now it has                                 linear if you're talking about online                                 machine learning models are you talking                                 about online no just about historic data                                 data coming into into the classifier so                                 where did you use it so like in the                                 historical database like I know dynamo                                 was something where you look up                                 historical interactions of the profile                                 yeah so this entire the hundred gigs of                                 data that I was talking about that was                                 all historical data the model was built                                 on using that hundred gigs of historical                                 data that had come till that point I                                 build a model on top of it and then                                 expose it okay so um what I meant is the                                 data you're using at the point of                                 friction time if you also have access to                                 historically                                 within the same session data of the                                 profile yeah I just took the current                                 event data frame yes so only the current                                 event only the current even that is                                 coming in using only that bit because                                 that was generating the field that was                                 good enough for the features and those                                 features were sent in to the model and                                 that was called in real time so here the                                 model is not an online machine learning                                 model I created the model offline and                                 then expose the model as an API and the                                 real-time data or the real-time even                                 server coming in that was getting scored                                 in real time and being exposed out okay                                 thank you I just have a question about                                 the response time I you mentioned it's                                 about                                                             produce this time using larger instances                                 in the AWS or is it some fixed                                 number yeah so they did a benchmark on                                 Emily and there are a lot of                                 presentations around on the web you can                                 look at it they were able to get in less                                 than eleven milliseconds as well it also                                 depends upon the sort of model that you                                 are using random for us you know                                 depending upon what sort of model if you                                 have a larger model with more number of                                 branches it takes a little bit more time                                 but again if speed is your criteria you                                 might want to choose a easier model too                                 so it's all based on fine-tuning your                                 needs based on the use case and what                                 sort of accuracy you want okay thanks                                 thank you and I think we ran out of time                                 thank you so is it for the talking the                                 demonstration                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=6xVvfTU_aag


