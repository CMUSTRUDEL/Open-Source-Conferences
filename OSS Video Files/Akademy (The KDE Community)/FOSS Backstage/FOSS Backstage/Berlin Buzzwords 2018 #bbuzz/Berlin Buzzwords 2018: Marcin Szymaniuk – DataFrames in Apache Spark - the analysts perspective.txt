Title: Berlin Buzzwords 2018: Marcin Szymaniuk – DataFrames in Apache Spark - the analysts perspective
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Are you a data analyst who works with Apache Spark and often gets confused by failures you don’t understand? Have you seen a bunch of presentations or blog posts about Apache Spark performance but you are still not certain how to apply the hints you have been given in practice?

Apache Spark is commonly used by people who are not experts in programming but they know SQL and sometimes basic Python. They treat Spark as a tool for getting business value from the the data. And that is how it should be! Although it’s common that queries they run do not work for any obvious reason. This talk is designed for such Spark users and will be focused on common problems with Spark (especially DataFrames and SQL) which can be solved by anyone familiar with SQL. You don’t need to read bytecode to understand the techniques presented and apply them in practice!

This talk will be a case study of multiple DataFrame queries in Apache Spark which initially do not work. I will not only explain how to fix them, but we will go through the solution step-by-step so you will learn what to pay attention to and how to apply similar techniques to your codebase!

Read more:
https://2018.berlinbuzzwords.de/18/session/dataframes-spark-analysts-perspective

About Marcin Szymaniuk:
https://2018.berlinbuzzwords.de/users/marcin-szymaniuk

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello my name is marching today we are                               going to talk about data analysts                               perspective on data frames                               I'm a data engineer at Santos data                               Atlantis data will help clients with                               basically everything data related so                               from plaster installations through                               application architecture development                               training supporting teams of data                                analysts and data engineers and I have                                been doing all of that but on daily                                basis I act as data architect and data                                data developer during this talk first of                                all I am going to introduce you to what                                I mean by data analysts or data                                scientists perspective what and why they                                are using spark then we will have a                                quick walk through spark execution model                                so how SPARC execute the program in the                                distributed environment and then we will                                have a walk through multiple use cases                                multiple problematic queries which fails                                initially and then we will have a look                                at why they fail so at the end you know                                how to fix such kind of queries and the                                query selection is based on the fact                                that they are very common so I will show                                you very common failures but at the same                                time there are they will be quite easy                                to solve by by anybody who is not who is                                not very good at networking or garbage                                collection so that makes it perfect use                                case for for data scientists data                                analysts and at the end I will very                                quickly summarize the lesson learned                                from these problematic use cases so                                let's start with data analysts left                                scientist perspective so what do they do                                they look at the data and they try to                                explain what has happened and why and                                the next step would be to use that                                knowledge in order to improve the                                product in order to drive a decisions to                                be made by a company and even steps                                further would be to build machine                                learning models so you can automate                                these stuff so you can you can make it                                better                                and this kind of things has been done                                for a very long time so the traditional                                approach is that you take the data from                                the source you move it to maybe some our                                server or to your local machine you do                                the analysis and you get the result but                                if you have large data set you early you                                have to you have to limit it you have to                                either take a sample of your data or an                                aggregate of of your data set and that                                means you might end up with these blind                                men and an elephant problem so a few                                blind men are touching an elephant and                                they get completely different idea about                                what an elephant is and something very                                similar might happen to your data if                                you're looking at just limited data set                                on the other hand if you use spark you                                can analyze the whole data set because                                because it distributes the load you can                                analyze the data set faster you get                                faster feedback loop and also since                                spark kind of brings the the analysis to                                the data not the other way around you                                don't end up with extra data copies and                                that might be quite important from the                                point of view of data privacy gdpr and                                stuff like that so coming back to data                                analysts perspective the end result of                                what you do is probably something quite                                quite sophisticated algorithm something                                might be quite complex but the reality                                is that in order to achieve that you                                really have to run a lot of ad-hoc                                queries to understand the data to come                                up with come come up with reports and so                                on and also to do the data cleanup so                                you come up with with good feature set                                for your machine learning model so this                                is a diagram I have found in the                                internet and it says over                                              machine data scientists time is spent on                                data cleanup probably you don't like                                that but what you would like to achieve                                at least is that you run these kind of                                ad hoc queries as fast as possible                                so spark lets you express all kind of                                ad-hoc queries so even if you are not                                familiar with it if you are familiar                                with sequel then you see group by                                drawing select you already know what                                 this program is doing but the problem is                                 that sometimes queries just fail for not                                 obvious reason so imagine you are going                                 to the office on Monday and you have                                 like                                                                     we understand the data better and you go                                 further from there and you hope to run                                 them by the end of the day but then you                                 realize okay it's Wednesday already and                                 I'm stuck with query number                                            not a cluster problem because all the                                 other guys around me are running them so                                 what is going on what do you do with                                 queries which fail or which never finish                                 so what we are going to do we are going                                 to do a quick deep dive into how spark                                 executes the program so you have better                                 disaster recovery plan than this so                                 asking for help is completely fine but                                 sometimes is good if you if you can                                 solve the problem so let's do the deep                                 dive the very first thing you have to                                 keep in mind that when you work with                                 data frames it feels that you are                                 working with some local collection you                                 are not exposed to any details about                                 machines and stuff like that but the                                 data set is actually partitioned and it                                 lives in multiple machines of your                                 cluster and SPARC do two types of                                 transformations on top of on top of the                                 data frame it do narrow transformations                                 which work on single partition at the                                 time so the simple simplest operation                                 ever and it does wide transformations                                 where it has to exchange the data over                                 the network it has to exchange the data                                 between multiple partitions so just very                                 quick example let's say you want to                                 calculate hash of a user ID and then                                 reverse a column string                                 and maybe some up to different columns                                 and you will end up with such an                                 execution plan so what that means is                                 pretty much that spark will read the                                 input data it will squeeze all these                                 operations together it will do the magic                                 code generation to run it in a                                 performant way and it will write the                                 write the result so the point is that                                 these kind of operations can be squeezed                                 together and they're kind of executed                                 locally to each partition and your                                 execution plan might look slightly more                                 complex but the point is as long as                                 these operations are in context of                                 single stage they will be executed kind                                 of locally but sometimes you want to do                                 something more complex so for instance                                 you would like to group your events                                 based on user the ID and maybe do some                                 further analysis so what spark will have                                 to do it will have to pull all the                                 events with user ID one to one place all                                 the events for user ID two to another                                 place and so on so we have many arrows                                 nodes are talking to each other and a                                 lot of a lot of data is being exchanged                                 here so here are samples of operations                                 which require to exchange data between                                 partitions here our sample of operations                                 which requires to do a shuffle and we                                 are going to focus on join and                                 repartition                                 so I will show you a few use cases where                                 where where we use them so this is like                                 just to give you an overview what it                                 will look like in in spark UI when you                                 write a program which does a drawing of                                 users and events this is what Sparky Y                                 would look like and these arrows are                                 indicators that the shuffle happens                                 these URLs indicates that that spark                                 will exchange the data between                                 partitions so in order to understand how                                 spark                                 paralyzed the load I'll show you the                                 simplest scenario ever first so we reach                                 some events from from from HDFS and                                 these events contains some timestamp :                                 and out of this timestamp column you                                 would like to calculate the Year month                                 and the day when the event happened and                                 spark will squeeze all the calculation                                 all these three column calculation into                                 into single operation inside one task                                 and one task will be reading a single                                 block from HDFS or a portion of your                                 data and eventually it will write out                                 the result and if you have let's say                                 your data set in                                                         will create                                                             will have many of them all of them will                                 look exactly the same all of them are                                 independent there are no arrows between                                 them there is really nothing go going on                                 between them so it's very easy to                                 parallelize so it's very simple so let's                                 have a look at small extension of this                                 use case so we would like to organize                                 our data set by the date so we have we                                 get better better performing queries so                                 if you are doing a lookup in your not                                 organized data set based on the Year                                 spark will have to scan the whole the                                 whole data set and then and filter                                 filter out the events you are interested                                 in as on the other hand if you organize                                 your data by the years of                                              one directory                                                            on you will end up with structure like                                 this and the queries for a given year                                 will perform much better and of course                                 it's not a rocket science it has been                                 there for quite a long time and what you                                 would like to do you'd maybe like to                                 organize it by not by year by but by a                                 day and since this is very well known                                 approach to to optimizing the queries                                 Parc supports that out-of-the-box you                                 can call partition by you specify which                                 columns you want to partition the data                                 by your partitioning it here by day                                 month and a year and in many cases it                                 works but I'll show you an example where                                 it does not so you run a program like                                 this then you go to spark UI you can see                                 that there is only one stage going on                                 and spark tells you that all the tasks                                 has finished and you don't trust it so                                 we would like to go deeper and look at                                 the task overview and you can see many                                 many tasks and all of them are marked as                                 succeeded but the job is still up and                                 running it's still marked as up and                                 running but you don't trust me we think                                 the UI is somehow off so you go to HDFS                                 and you list the directory where you                                 expect the the output to list the                                 content of this directory for instance                                 and you want to count the number of                                 files there and you get resolved                                        like one minute later you run it and you                                 notice that there is a difference is                                    and after some time you run it and and                                 you can see that basically the number of                                 tasks in the output directory is still                                 growing so what is going on in order to                                 understand what's going on you need to                                 understand how the partition by method                                 is implemented so let's say you have                                    days of data in your in your in your                                 input directory each task will read                                 block from HDFS it will calculate the                                 columns and it will write out single                                 file file pipe per day because that's                                 how you wanted it to partition by and                                 then you will have many of these tasks                                 because you have many blocks as an input                                 data and by the end of the day if you                                 have                                                                    blocks in HDFS                                 you will end up with these many blocks                                 as an output so you do the math but the                                 point is it's quite quite large number                                 and all of these files will be small so                                 it doesn't make that much sense and the                                 consequence will be that which will be                                 slow it will be slow because HDFS is not                                 really optimized for for handling many                                 small files is right it's rather                                 optimized for for batch workload and                                 what has happened here is that spark has                                 created the output in temporary                                 directory but now it's kind of moving                                 the temporary directory to the actual                                 output location and it's hammering name                                 node kind of a central piece of of HDFS                                 it's hammering the name node and                                 everything is just just slow but in                                 extreme cases you might even kill the                                 cluster and this is what your cluster                                 admin will look like when he noticed and                                 the problem is he actually knows who you                                 are so we want to fix that so mmm the                                 way to fix that is actually quite simple                                 there is a method called repartition and                                 that can reorganize our data based on                                 what we want to but without touching                                 HDFS so we call the repartition we                                 specify the exact same columns we used                                 for partition by and then when we look                                 at spark UI it will be slightly more                                 complex we'll have two stages now and                                 what these stages will do is stage                                 number one a task in stage number one                                 we'll read the data from HDFS it will do                                 the current calculation and it will                                 organize the data by day but it will                                 save it just to the local disk and all                                 the other tasks will do exactly the same                                 and the rule here is that if task if day                                 number three is going to the green                                 bucket it will always go to the green                                 bucket so it's consistent so task number                                 one in stage two can pull all the data                                 for day one                                 task number two will pool day two and so                                 on so here we are processing single day                                 or maybe multiple days but the whole day                                 is processed by the same task as the end                                 result so we will end up with just one                                 file in HDFS per day so we have limited                                 number of files initially when we when                                 we were running just partition by we the                                 job was really slow so you have to know                                 how the partition by works but once you                                 know that you definitely should consider                                 we partitioning the data when you are                                 writing it to HDFS and in order to                                 decide whether to use it or not you need                                 to know your data distribution so                                 repartition is the rescue here but is it                                 some kind of a silver bullet well no                                 because I'm going to show you another                                 use case where it arc when it actually                                 fails after we partition in so let's say                                 we run exactly the same code just on                                 different data set so I will show you my                                 small example I have run it                                 I have run it on some small data set the                                 the query plan looks exactly the same                                 and when the stage number one has                                 finished so it has organized the data by                                 day it's a it's stored it locally but                                 then you go to stage number two and you                                 want to see what is going on there and                                 what you can see here after sorting your                                 tasks based on the amount of data they                                 have read you can see that three of your                                 tasks have read like three gigabytes of                                 data but the other ones have read                                 nothing and they are already succeeded                                 so they are just so lazy and then you                                 refresh the page and you can see these                                 three tasks are growing and growing and                                 growing so the reason for that is I in                                 my data set had just three days of data                                 and I'm repartition                                 by day the rule is that the whole day is                                 processed by single tasks after                                 reporting and you end up with such kind                                 of problems so imagine you have                                          of data per day then your execution plan                                 we will look exactly as before except                                 single tasks will have to process                                 hundred bits of data and most often this                                 is not what you want to do most often                                 you will end up with one of these                                 problems so first of all spark might try                                 to avoid out of memory and it will spill                                 the data to disk but it will be slow and                                 there are other possible problems which                                 you can end up with depending on what                                 exactly you're doing after the partition                                 so what what could you do                                 you could try to split the data further                                 so you could try to repartition by hour                                 and that will what what that will do so                                 if we are if you are partitioning the                                 data by day then that means day number                                 one will go to one task and day number                                 two will go to another one but if                                 instead you are reproducing the data by                                 something which splits the data further                                 if you have such a common our number                                 eleven day one will go to one task our                                 number one will go to another one and so                                 on sometimes it might be a little bit                                 skewed but still you are better off with                                 doing that and after running my job I                                 can see many more tasks which are                                 actually doing something and this is                                 what I want to achieve so when you are                                 dealing with the partitioning of your                                 data first of all you really need to                                 have you really really need to know what                                 your data looks like how the keys are                                 distributed so you you do it smart you                                 you make sure your executors are getting                                 enough load and what you want to achieve                                 you really want to have to see all your                                 tasks doing something at least so                                 so you benefit from the fact that spark                                 is a distributed system okay so we have                                 an idea of how shuffle works for                                 repartition so let's have a look at join                                 something slightly more complex so you                                 want to join events with users based on                                 user ID : spark will create three stages                                 stage number one will organize your                                 users based on the user ID stage number                                 two will organize the events based on                                 the user ID so the key you are using for                                 the join and it will do it in a                                 consistent way because it's consistent                                 then stage number three will pull the                                 bucket one for users and the bucket one                                 for events so they are matching and it                                 will perform the partial join it will                                 just perform the partial join here                                 start task number two will perform                                 another part of the join and so on so                                 this is what it will look like in the                                 spark UI and I would like you to pay                                 attention to this number we have                                     here and that means the the join will be                                 processed in two hundred tasks so there                                 will be two hundred tasks which will be                                 pulling the data and and processing part                                 of it and when I run some program                                 performing the join I can see that my                                 tasks are all of them are kind of busy                                 and all of them are reading some data                                 but I am concerned about the amount of                                 the data I can see already almost ten                                 gigs and I'm just doing simple join and                                 again this might be might be problematic                                 so let's assume you are processing ten                                 terabytes of events which are uniformly                                 distributed across your users and you                                 are joining it with just one gigs of a                                 few tzer's or some small user data set                                 the join will look exactly the same and                                 the question will be how many tasks in                                 stage                                                                    will split the data into here is the                                 answer it's controlled by this sparks                                 equal shuffle partitions parameter and                                 that's why you saw                                                      are dealing with                                                        you performing the join and if we do a                                 quick back of the envelope calculation                                                                                                         gigs per task this is too much you want                                 to keep your executors small for for                                 this kind of jobs so this is too much so                                 how do we how do we solve that                                 so this is what what it will look like                                 each task will get                                                     we end up with                                                           one of these shuffle problems so first                                 of all you have to understand your data                                 you have to know the size of your data                                 and do the actually do the back of the                                 envelope calculation so you understand                                 why it happens to you but then once you                                 understand that it is actually quite                                 simple you can just control the level of                                 parallelism so use this parameter and                                 increase the number of tasks so you have                                 more tasks each of them will be                                 processing place data because they are                                 uniformly distributed a similar problem                                 might happen to when you do any other                                 shuffle shuffle operations so if you are                                 doing free partition you can also                                 control number of partitions it will it                                 will create so as the result after                                 changing the the parameter we can see                                 two thousand tasks here oh my god really                                 we can see                                                              them will be just processing smaller                                 amount of data but what if our data                                 what if our events are not really                                 uniformly distributed so let's have a                                 look at case number form for the skewed                                 join sometimes it happens that when you                                 run your job on this on this like                                 overview level it looks exactly the same                                 as before but when I click here I can                                 see that many of my tasks are not doing                                 that much they are just processing some                                 reasonable amount of data but there is                                 this one task which read much more data                                 this is one gig so far it's not bad but                                 it's still growing and eventually it                                 explodes so what if you are about to                                 join ten terabytes of events with one                                 user who produced one terabyte of events                                 so one user has produced most of your                                 events and you are joining by by the                                 user ID again the way it will be done is                                 exactly the same but one of your tasks                                 will have to process one terabyte of                                 data and this will definitely explode so                                 what do you do if you have a skewed                                 joint problem first of all check if your                                 data is correct it might be that the                                 data delivery mechanism is off and it's                                 producing some now IDs and so on so that                                 means you have to change your data                                 delivery then check if the logic you you                                 just created may be the way you are                                 building the data frame is just off                                 maybe you have a bug in your logic and                                 that means you have fixed your logic but                                 sometimes it's completely fine you have                                 data like this and you would like to                                 still perform the join                                 so how do we do that on the left hand                                 side you can see events on the right                                 hand side you can see users and this                                 user ID number one happens to be happens                                 to appear quite often even we would like                                 to split that because you will go to to                                 the same to the same task so what we do                                 is we generate a salt column where we                                 randomly children and narrate some value                                 here we are just generating three                                 different values and what we do with                                 users is we duplicate all of them with                                 every single possible salt value so from                                 one two three                                 each user appears three times and now                                 when you do the join you join by not                                 only user ID but also less salt so user                                 ID one salt one goes to one place salt                                   is going to another task and salt three                                 is going to one more task so we have                                 splitted that into three buckets usually                                 what you want to split it into into more                                 so how do you do that first of all you                                 have to calculate the salt at events so                                 the the events with salt : and you do                                 that by adding an extra column where you                                 basically create a hundred different                                 random values then you need to calculate                                 the users users salted so first you                                 create some dummy data frame with                                 hundred different values and then you do                                 Cartesian join of users and these                                 hundred dummy values and then you join                                 the event salted with user salted not                                 only based on user ID but also based on                                 the salt value so at the end you won't                                 have your task won't have to process                                 this much data you limit the amount of                                 data per task and this might sound us a                                 little bit artificial problem for you                                 but this is something                                 can see very often when working with                                 clients and it's actually quite powerful                                 technique and of course you can optimize                                 it you can generate salt for only only                                 the user which is problematic and so on                                 but but make sure you keep keep in mind                                 that these kind of techniques so the                                 quick summary of the use cases I have I                                 have shown you is that first of all you                                 have to know your data and that's really                                 important size of your data the                                 distribution of your keys you are using                                 when when joining and so on then you                                 really have to understand how the                                 operators you are using are working and                                 you should pay attention to those ones                                 which are triggering shuffle so join is                                 then the most common one and what you                                 want to achieve in general is you want                                 to keep all your tasks busy so we                                 benefit from the fact SPARC is a                                 distributed system and you want to make                                 sure that your tasks are not processing                                 too much too much data so so maybe                                 controlling the level of parallelize                                 will help and that there are many other                                 challenges but I would really recommend                                 you to go back to these examples and                                 make sure you understand them maybe play                                 around with them because for instance                                 when you are when you want to do                                 broadcast variable the whole point of                                 broadcasting is that you avoid shuffle                                 so you have to know when shuffle happens                                 and what is the consequence but if you                                 would like to discuss any of these of                                 these problems or any problems part I                                 would be happy to do that afterwards and                                 the last comment I would like to I would                                 like to make is that if you are a data                                 scientist or data analyst you definitely                                 want to know how to fix the most common                                 problems but on the other hand there are                                 problems with spark which which are not                                 really related to due to your expertise                                 which might be related to what happens                                 in the cluster with the general cluster                                 state or it might be bugging with                                 so if you feel that you are completely                                 stuck and it's going outside of your                                 expertise make sure you know where to go                                 to make sure you know where the data                                 engineers are or where your dev ops are                                 so day they unblock you I would be happy                                 to answer your questions now I have                                 actually posted the slides on my on my                                 Twitter and I I also have have posted a                                 blog post about one of the use cases I                                 will be describing all of them soon so                                 follow me on Twitter if you are                                 interested so we have ten minutes                                 question interesting hook thank you what                                 happens if you put some of sparks equal                                 inside your data frame is it good or bad                                 to do that tied to doing things by hand                                 well so behind the scene it will do                                 exactly the same it will use the same                                 the same optimization engine and so on I                                 like myself I'm not a fan of sequel                                 because it's just much easier for the                                 test for me and it's much easier to                                 split that into into the coding blocks                                 but I know like if your data scientist                                 data data analyst a they really many of                                 them really like do it but and the point                                 is behind the scene it will work exactly                                 the same so if you are writing a sequel                                 and and if you are performing a join the                                 same thing will happen so whether you're                                 using this interface or sequel it's                                 still the same it's still the same                                 pretty much                                 hi so what is your usual debugging                                 workflow when you notice that a task is                                 taking way too long longer than it                                 should be how do you best in your                                 experience go about debugging it okay so                                 I actually try to in a very quick way                                 reproduce it here so first of all I look                                 at at the spark UI and I see an overview                                 of this stage which is problematic then                                 I try to narrow down how this stage                                 matches the code and which part of the                                 code is is the problematic one then if I                                 suspect for instance we have a skew                                 I actually run an analysis on top of on                                 top of it and see if it's if it's really                                 a problem also it's worth knowing that                                 where your logs are so like if you are                                 using spark on top of HDFS the dialog                                 should be automatically aggregated if                                 you want to really go deeper and deeper                                 but for I would say for most for most                                 queries and knowing your your data                                 distribution and understanding                                 understanding how to read the data from                                 the from the SPARQL how to read some                                 some hints should be enough at least                                 four for a data scientist if you want to                                 go deeper definitely looking looking                                 into logs looking into into the query                                 plan spark shows you how it will go it                                 will execute the query but yet generally                                 it depends on on the use case but this                                 is the most common one                                 still have some time I believe so                                 okay thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=4Lngj22dFiA


