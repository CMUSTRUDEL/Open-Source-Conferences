Title: Berlin Buzzwords 2018: Houston Putman â€“ Relevant Data Analysis: Apache Solr Analytics #bbuzz
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Data scientists now have access to vast amounts of data; therefore, it is important to ensure the data they analyze is meaningful and relevant. Search engines are used to find relevance in vast volumes of data; however, until recently, they could only do basic data analysis. We will investigate new features recently added to Apache Solr that make search an attractive option for data scientists. The talk will focus on the Solr Analytics Component and other complementary features offered by Solr.

This talk is presented by Bloomberg.

Read more:
https://2018.berlinbuzzwords.de/18/session/relevant-data-analysis-apache-solr-analytics

About Houston Putman:
https://2018.berlinbuzzwords.de/users/houston-putman

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone as you said my name is                               Houston Putman I'm a software engineer                               at Bloomberg and the creator and                               maintainer of the Apache Solr analytics                               component which is a control I'm here to                               talk to you today about relevant data                               analysis and unsurprisingly how the                               solar analytics component can solve your                               needs so first I'll just give a brief                                overview of what we do at Bloomberg we                                are a provider of financial governmental                                law and news data for a variety of                                professionals our strength is accurately                                and quickly providing this very relevant                                information to these clients and since                                we have professionals and finance any                                instability can really cost our clients                                money so stability is key we employ over                                                                                                    these are tasked with providing this                                very relevant data and since we have so                                much data this data isn't necessarily                                useful to the clients raw so relevant                                analyzed data I mean                                it kind of ties into what we're talking                                in here that is not what I wanted to do                                there yeah so today we're gonna start                                off talking about relevance and                                analytics and kind of how to merge these                                two worlds and then talk about how solar                                analytics is one approach to this                                problem and then how the solar analytics                                works in a distributed mode because it's                                not as trivial as I would think after                                that we'll talk about performance                                considerations which unsurprisingly ties                                into the distributed nature of the                                component and then additional features                                that have been introduced over the last                                year and at the end we'll briefly talk                                about kind of how this is used within                                Bloomberg and what use cases this is                                really at that solving so relevance and                                analytics as I said Bloomberg and many                                other people really have this use of                                finding relevant data and then analyzing                                it and there are a lot of establish                                search engines such as elastic and solar                                and a lot of established analytics                                engine such as spark and Hadoop but they                                don't really play together that well                                there are solutions to take data out of                                solar after a query and then ingest that                                into sparker dupe to perform data                                analysis but really the benefits of                                these external analytics engines come                                when you are processing a vast amount of                                data so like hundreds of millions of                                records billions of records and taking                                all of that data out of search engines                                is pretty slow yet sparking I do have a                                very like a vast kind of universe of                                tools around them because Dana                                scientists have built them over the                                years because they needed them with                                these systems so they do solve a lot of                                problems solar analytics is trying to                                take a chunk of these problems and make                                a better solution so instead of having                                this external search in this search                                engine talked to this analytics engine                                and kind of having to manage both of                                them have an analytics engine reside                                within of the search engine which kind                                of fixes a couple of problems one taking                                all the data out of solar doesn't really                                need to do that because if you have your                                analytics within solar the analytics                                engine can read directly from the index                                which is obviously much faster and since                                no one really likes managing too many                                dependencies just relying on a search                                engine is much easier than managing a                                search engine and analytics engine and                                how they talk to each other kind of the                                last point is that solar's is live as                                the data ingested into it and therefore                                a lot of clients need analytics to be on                                this live data and if you're using an                                external analytics engine it's pretty I                                know it's pretty common to cache results                                from solar cache the results these                                analytics which causes your results to                                not be as live as they could be if you                                were getting your analytics straight                                from solar so let's bring it back a                                little bit and say it kind of asked the                                question why do we need to analytics and                                how did this guide the creation of the                                 solar I know                                 so kind of a warning I am a very big                                 baseball fan I'm sure most feel don't                                 know anything about baseball but all the                                 examples will be in baseball and you                                 don't need to know anything about than                                 the fact that baseball data is very                                 easily broken down into individual                                 events of documents in the search                                 engines and we'll be talking about the                                 Astros who won their first World Series                                 in                                                                   from that so let's say that we have a                                 search engine that's full of baseball                                 data and this is very convenient because                                 we can search for results for certain                                 team a certain year a player etc but                                 however like when we get these results                                 back it doesn't really tell us much                                 because as most it's just a decisions no                                 one piece of data doesn't give you                                 insight into like the whole corpus of                                 data so even the worst hitters will hit                                 a homerun and even the best hitters will                                 strike out in order to find meaning                                 within this data we need to analyze it                                 this means combining it a grenading it                                 and we do this through analytical                                 expressions you can see these throughout                                 your life we use them every day probably                                 but they've been doing this in baseball                                 for over                                                                 called on-base percentage and batting                                 average which are probably the two most                                 popular ways of expressing how good a                                 player is batting and these are very old                                 concepts that not very easily into                                 analytical expresses a very new concept                                 so analytical expressions are a very                                 good way of mapping real-world problems                                 into problems that can be solved with                                 the analyst component so let's just get                                 an example out there since we're using                                 solar we need to first query for some                                 results let's ask for some Astros                                 results which gives us a list of players                                 in their plate appearance back first                                 we're trying to calculate on-base                                 percentage as you can see on the right                                 and we're going to build up an                                 expression to calculate this so first we                                 need to map data within each document                                 together and Sylla looks as I said                                 before                                 a MapReduce framework much like sparking                                 Hadoop so the first step of any alpha                                 analytical expressions taking the fields                                 from your solar index and mapping them                                 together to combine data so for our                                 addition function we're trying to add                                 all the times that a player got on base                                 per plate appearance once the mapping                                 has been done we need to then reduce the                                 data because that's where the analytical                                 magic happens we reduce data across all                                 the documents to find the aggregation so                                 we take the count of plate appearances                                 sum up all the times that they've been                                 on base and then we finally do our last                                 mapping to map the results of these                                 reductions together to get an overall                                 result of on-base percentage which is                                                                                                    mapping reducing framework has kind of                                 built on the same principles as Dupin                                 SPARC and allows for as much                                 parallelization as possible for these                                 things so mapping can be done say per                                 shard because it's done on a per                                 document basis the reduction can be done                                 using groupings such as spark and a dupe                                 and kind of aggregated together at the                                 end                                 so this MapReduce framework really makes                                 the component as extensible as possible                                 to further parallelization so I've                                 showed you building up the on-base                                 percentage animal expression now I'll                                 show you average as well so all these                                 integral expressions start with                                 constants and fields which are from your                                 index from these fields and constants                                 you can map them together on a per                                 document basis and then reduce those                                 values together and after the reduction                                 has been done across all documents map                                 the results of those to get your overall                                 values so as you can see we can                                 calculate analytics over entire result                                 sets but how often do people do that                                 very rarely because the thing you want                                 to do with these values is compare them                                 I don't really care the batting average                                 of everyone in                                                       batting average                                 of each player in                                                    break up the data calculate results                                 right based on those groupings and then                                 return them                                 so in solar we how I already have this                                 idea facets using the fast component and                                 so we can extend that into the analytics                                 component to break up the data and                                 baseball you can I think of groupings                                 such as group by to see which players                                 are the best and worst which years in a                                 player's career were they good which                                 were years were they bad which teams are                                 good which teams are bad                                 basically as many ways you can break up                                 the data the analytics component                                 lets you do that so we'll go through the                                 different types of fastest available                                 first we have value facets which are an                                 extension of solar field facets which                                 let you group the data by the value of a                                 field or a mapping expression will go                                 further into the value facets later but                                 here on the right you can see that                                 players have been grouped the data has                                 been grouped by players so that we can                                 calculate analytics on a per player                                 basis next we have range facets which                                 work exactly the same as they do in the                                 FAFSA component which you can break it                                 down by dates such as May and June or                                 value such as innings                                                innings                                                                  we have data from inning                                              not included because it wasn't in the                                 defined set of ranges provided to the in                                 the request solar also allows for query                                 facets so the Analects component also                                 provides query facets so you can ask for                                 cold games hot games if you have an                                 additional query to send a solar to                                 break of the data by so I mentioned                                 value facets earlier and how they're                                 kind of an extension of field facets but                                 using some of that analytics magic in                                 their value fastest let you give a                                 mapping expression and when we were                                 breaking down these expressions earlier                                 that's any expression that doesn't have                                 a reduction function in it because you                                 can't really break down data by the                                 reduced value it doesn't make any sense                                 and so for an example if we didn't have                                 what stadium played appearance at                                 curtain we could say that I make an                                 expression that says if that play                                 at home at their home stadium returned                                 that team or else returned the opposing                                 team this kind of gives you that data                                 without having to store it Valley facets                                 also allow for more complex sorting than                                 a given in the FASTA component and solar                                 so you can sort by multiply multiple                                 criteria be it expression or facet value                                 and then set a limited offset of course                                 so this is just an example of how to                                 express it in the analytics framework                                 and this is what I explained earlier I'm                                 not gonna get too far into it                                 pivot facets are what kind of the same                                 ideas they are in the fast component                                 allowing drill down of different                                 fascinating drill down over multiple                                 mapping expressions and so it's like the                                 solar and the solar fasting component                                 pivot facets but instead of fields                                 mapping expressions much like valley                                 facets complex sorting is enabled for                                 each pivot individual individually                                 unlike the fasten component and then                                 results are calculated at each pivot                                 level let's go over an example so say                                 that we wanted to calculate how each                                 team did a compensable league so we can                                 see the Astra get the results from the                                 Astros calculate their analytics then                                 see how they did against the Yankee's                                 against the Angels see how the Yankees                                 did against the Astros the Red Sox and                                 the Blue Jays and since the Dodgers                                 haven't played anyone yet they won't                                 have any children pivot to make this is                                 just how you would express it in the                                 analytics language so how does this                                 mapping and reducing framework work with                                 facets here required for the same set of                                 assets results and we're calculating                                 on-base percentage again we first map                                 the documents with it the values within                                 each document and this is unchanged from                                 before since we haven't started to                                 reduce so once we reduce instead of                                 having one reduction at the end we need                                 to reduce for each value of                                 the facet that we're calculating so                                 we're gonna in this example break up the                                 data for each player once the reduction                                 has been done we can then map the result                                 from each of these facet values to get                                 an on-base percentage for each of the                                 players pretty straightforward so I've                                 given you a lot of baseball examples and                                 I understand it must be L don't know                                 baseball here's an example of how we use                                 this app Bloomberg this is the screen                                 merchant mergers and acquisitions which                                 basically tells you about companies                                 buying other companies and here all the                                 data is provided by the solar analytics                                 component and you can see here we have                                 analytics expressions just the count of                                 deals the minimum maximum deals eyes                                 median and then volume deal count etc                                 and then facets that this data is broken                                 up by such as payment type being cash                                 doc equity and whatever target multiples                                 is I'm not really sure so as you can see                                 this real world applications of this                                 another component so distributed                                 analytics why is it needed                                 so in solar collections can have                                 billions of documents however shards are                                 limited to do two billion documents and                                 since a lot of users need to be able to                                 analyze lots of data that two billion                                 documents per shard really limits                                 users and so in order to solve this we                                 need to have data spread across multiple                                 shards and this causes some problems                                 when try to compute analytics what is                                 that issue so obviously let's go back to                                 our MapReduce framework mapping is                                 really easy since you can just map on a                                 per document basis and documents are on                                 one shard each so mapping isn't affected                                 by this shard egg however when you start                                 to reduce the data down you'll get one                                 set of reductions per shard because the                                 data you can't really when you're doing                                 this reduction you can't talk across                                 machines very easily so what do we do                                 with these three                                 that's a reduction at the end to get one                                 overall set of reductions for some                                 things is a very easy task so                                 associative reduction functions such as                                 sum count min and Max this is trivial                                 because you can take the sum of each                                 chart and then take the sum of all those                                 results and you get the overall sum but                                 for non-associative reduction functions                                 is pretty hard because you require all                                 the data to be in one place such as                                 percentile and median and unique                                 percentile median require a sorted list                                 of all the data and unique requires a                                 unique set of all the data unless you                                 want to if you want accurate information                                 so the solution is since every reduction                                 function needs a different set of data                                 sent across shards the reduction                                 functions are in charge of exporting and                                 merging that data in the original node                                 and so for men the min function sins all                                 the minimums from each shard and can                                 define the minimum of all those values                                 at the end unique since the unique set                                 from each shard creating an overall                                 unique set medians and sorted lists                                 creates an overall sorted list and some                                 you understand so as you can see each                                 function here is in charge of sinning                                 and merging its own data making it very                                 easy to create new functions in this                                 framework so let's just briefly go over                                 how a distributed request is sent inside                                 of a solar cloud the request is sent to                                 an analog component which finds one shot                                 one replica of each shard to send the                                 request to the request isn't fair all                                 the mapping is done on each of those                                 shards and the initial set of reductions                                 is calculated that reduction data is                                 sent back to the originating shard which                                 is then merged and then the facets are                                 sorted and limited and then the response                                 is sent out so the takeaways from this                                 is that distributed analytics does let                                 you speed up aggregations a lot because                                 that mapping phase can be paralyzed                                 across the shard and done at the same                                 time and for associative reductions                                 which are really easy to distribute                                 across shards you can really speed up                                 your aggregation by as many shards as                                 you have in your cluster                                 meaning that if you have say                                           it will perform roughly                                                  it's just having all of your data in one                                 shard and since we need we need the                                 request interface to be the same across                                 single charted and multi search                                 collections so that no features are lost                                 in between and the users don't really                                 care where their data is so let's talk                                 about some performance considerations                                 but first let's talk about how a request                                 is processed in the analytics component                                 and basically how we can speed things up                                 so first obviously the query that sent                                 to the Analects component is executed                                 and a result set to calculate the                                 analytics over is found for each of                                 those documents it's looped the analyst                                 component loops over each of those                                 documents reading it from the document                                 from the index and filling the reduction                                 data for expressions and pivot and value                                 facets since query and range facets are                                 done by sending extra queries to solar                                 those have to be done in a different                                 step which is a new step in so the                                 queries are sent the queries are                                 executed which kind of returns to sub                                  and populates additional reduction data                                 for those facet values once all the                                 reduction data is found it's sent back                                 to the originating node and the overall                                 the results are calculated the facet                                 results are filtered and the results are                                 sent back to the user so what things can                                 we do to make this faster so a lot of                                 the times and these request you'll sit                                 you'll send some of the same sub                                 expressions multiple times so in these                                 set of four expressions you can see                                 multiple things that are referenced at                                 more than once such as count PA is                                 reference twice add homerun and BB walks                                 is reference twice and homerun is                                 reference three times instead of                                 calculating all these individually we                                 can I basically share overlapping                                 expressions so that we only had to read                                 plate appearances once from the index we                                 only have to read home runs once we only                                 had to add home runs and locks once                                 and this really speed things up the                                 speeds things up for large analyst                                 requests um let's go back to the                                 distributed reduction solution and see                                 how we can speed things up there since                                 as you can see if you're trying to                                 calculate the median over a billion                                 values that can be pretty slow because                                 you have to send billions of values                                 across the network to the originating                                 node um so what we can do is not reduce                                 that amount of data because to get                                 accurate results you have to send all                                 the data back but we can make sure that                                 we're not sending the same data multiple                                 times for example if we're trying to                                 calculate the median of inning the                                      percentile inning in the                                                 inning                                 all of those reduction functions require                                 a sorted list of innings so instead of                                 sending that sort of list three times                                 once for each function we can have these                                 functions share the data and we do this                                 through reduction data and so instead of                                 having reduction functions merge and                                 export data we have reduction functions                                 Reserve Reduction data which manage                                 collection exporting and merging so here                                 we can see that median and percentile                                 would reserve a sorted list of their                                 expression and unique set with a unique                                 would reserve a unique set and this can                                 really give performance improvements for                                 non-charged collections so let's go to                                 the let's give an example of this and                                 say that we want to find the median of                                 inning twenty percent of inning sixty                                 percent of inning some of home run and                                 mean of home run what reduction data                                 would these reduction functions require                                 and as you can see they only require                                 three sets of reduction data that's                                 count of home runs sum of home run and                                 sort of list of inning once this has                                 been found out we can just ask each                                 shard to collect these three bits of                                 data and then merge and export export                                 and then merge that in the originating                                 node and once that's happened we each                                 production function can take the data                                 from the reduction data it's reserved                                 and calculate its value instantly so as                                 you can see here all five of these                                 functions only require three                                 it's a reduction data which vastly                                 improves the performance of sending data                                 across the notes especially with the                                 slow Network cool some other performance                                 various performance considerations think                                 of our adding new expressions don't                                 necessarily increase the amount of time                                 the query takes to calculate and I would                                 very much suggest that people add the                                 things that they want to be calculated                                 and see if it affects performance                                 because the way that the analog                                 component kind of solves the overlapping                                 expressions and the sharing of                                 production data can really mitigate a                                 lot of fears of calculating the                                 performance overhead of calculating                                 expressions and it should also be noted                                 that the non-associative reductions such                                 as median and unique in percentile                                 require a significant amount of memory                                 for large results at sizes so obviously                                 if you're trying to compute the median                                 of a billion values starting this                                 billion values and memory to is a hefty                                 hefty charge and then all fields used in                                 these expressions must have dock values                                 enabled that's just kind of a caveat                                 because that's how it reads from the                                 index and if you use the old analytics                                 component the new one that has a much                                 lower memory consumption for high                                 cardinality facets so facets with                                 hundreds of thousands of values so some                                 additional features I'll just breathe                                 through these have been added in the                                 last year so expressions over multi                                 valued fields are now supported by the                                 component so for example expressions                                 functions that used to take in a single                                 value and return a single value such as                                 log in negate now take in multiple field                                 with multiple values and return multiple                                 values for each value in that input and                                 you can see how the mapping works it's                                 pretty straightforward functions have                                 taken a single VAT two single valued                                 parameters and return a single value are                                 a little harder to map this multi valued                                 expression world but it works pretty                                 simply so you can take a single value in                                 the first parameter and the multi value                                 in the second program                                 it returns a multi multiple values                                 one for each mapping mapping the first                                 parameter which each with each value of                                 the second parameter and this works the                                 other way around taking a multiple for a                                 multi valued parameter in the first slot                                 and then a single value parameter in the                                 second these functions also can take in                                 a variable length parameter so just                                 concatenate add and multiply on this                                 very easy to see work in a multi valued                                 expression world by just taking in one                                 multi valued parameter and returning                                 that single value so instead of taking                                 in like add a one and two just say add a                                 value that contains one in two and it                                 returns that's same exact value we have                                 new supported mapping functions such as                                 logical ones comparison and conditional                                 and you can you've seen these in the                                 examples I've given previously so                                 variable functions are a new feature                                 enabled too and so they allow you to                                 kind of put business logic into your                                 request and not have to write out the                                 same thing over and over again since we                                 know copy and pasting can lead to lots                                 of errors so using variable functions                                 you give a variable name a function name                                 with the variables it takes in and then                                 it uses and then an expression that uses                                 those parameters so if mean wasn't                                 enabled in the Analects component you                                 could say mean of a is the division of                                 some of they encountered a and then use                                 me as if it was a built-in function and                                 solar it also accepts variable length                                 parameters since analog functions also                                 take in variable links parameters so if                                 you put in CSV of one and two it would                                 expand out to concatenates can cats set                                 of comma with one and two which results                                 in one comma two we so in the first                                 example I gave it wrapped a with sum of                                 a and count today not all if you were                                 given a variable length parameter                                 wrapping it could not work because if                                 you wanted to like fill missing each                                 value within                                 just a feeling filled missing a with na                                 doesn't work because that would film                                 missing just takes two parameters so we                                 have a framework of kind of lambda                                 functions that say for each value of a                                 fill missing that value within a and so                                 if you put CSV of                                                     map out to can cats F of fill missing                                 one and a and then fill missing in                                 Noland a yeah if you have any additional                                 questions about this I know it's a                                 little confusing there's much more in                                 the solar reference guide about it and                                 you could ask me after the talk so on to                                 use cases when does the solar I know its                                 component work for you and as you can is                                 you've probably seen here solar Analects                                 only provides first-order analytics and                                 first-order analytics are analytics that                                 are based off of the underlying data set                                 second-order analytics are analytics                                 that rely on that underlying data set                                 and first-order analytics and                                 third-order etc so if you right now the                                 analytic component only supports                                 first-order analytics there are plans to                                 support in order analytics in the future                                 but for now if you're use case most use                                 cases really only need first-order                                 analytics so this shouldn't be a problem                                 but it's something to keep in mind so                                 how do we use this in Bloomberg I start                                 this off by saying that solar Analects                                 doesn't fit all use cases that what I'm                                 trying to say Bloomberg still uses a                                 dupe and spark heavily and there are                                 several teams that use other analytic                                 internal analytics engines such as                                 streaming expressions and JSON facets                                 solar analytics is used heavily within                                 our team sun as a member of the search                                 infrastructure team with hundreds of                                 clients within the company we know what                                 teams are using what features and the                                 analyst component is used heavily within                                 Bloomberg and it has some place many                                 high priced external solutions and                                 custom in-house code that we can now                                 deprecated the use cases ranged a lot                                 like from analyzing a hundreds of values                                 to hundreds of millions of results using                                 one shard versus dozens of shards and                                 then not using any facets to using                                 facets with hundreds of thousands of                                 values so this has been test very                                 heavily using a                                 I eighty of different use cases there's                                 a lot of future work that we want to                                 enable such as as I mentioned early                                 supporting in ordered expressions                                 natively we also want to add integration                                 with streaming expressions                                 since both calculated analytics would be                                 nice if they could talk to each other                                 also the ability to pivot over different                                 types of facets so say you want to value                                 facet pivoted by query facets much like                                 it works in JSON facets and then finally                                 the ability to add custom functions in                                 the schema so that you can write your                                 own functionality and not have to                                 actually modify solar source code and                                 build your own package in conclusion on                                 the Analects component provides complex                                 data introspection without spending the                                 resources on managing external analytics                                 engines it's running in production at                                 scale at Bloomberg and it's available                                 starting this all the features that I've                                 talked about are available starting with                                 the cellar                                                              since solar                                                           new features are in solar                                   documentation in important bug fixes                                 were included in solar                                           basically if you have a solar cloud and                                 need analytics check out solar analytics                                 because it could possibly fit your needs                                 very easily if you have any questions                                 there's a section on the reference guide                                 about it we are hiring at Bloomberg for                                 search professionals so please can talk                                 to us if you have any questions about                                 that                                 we have some current work of moving the                                 Analects component from the country as a                                 contribute to moving it to core and we                                 have a history of the different solar                                 tickets if you're interested in that                                 cool any questions                                 hello you talked about the concat                                 function for multivalued yields but do                                 you have for example a count function                                 yes most so those are just like examples                                 in the reference guide it gives all the                                 default functions and what type of                                 parameters they allow multi-diode or                                 single valued oh I think all the                                 reduction functions allow single or                                 multi valued fields like it doesn't                                 really care so if you're doing a count                                 you can do a count of documents which is                                 like if you have a multi valued field it                                 would just count the number of documents                                 that have a value or count the number of                                 values in each field if that makes sense                                 so you counted the number of values in a                                 multi valued field and after that can                                 you sort of your results on this base                                 are you mean sorting like facets you get                                 you can search facets on that count this                                 yeah so you would like to see the                                 results in your facets with the number                                 of v.i.c seven values and six yeah you                                 can sort by any analytical expression                                 that you give it and counting is yeah                                 one of those Thanks all right great talk                                 Thanks my question is about I've come                                 across a couple of use cases where                                 people want dynamic facets so they don't                                 know the buckets at the start and they                                 want to derive it from the data as they                                 go through have you thought of any of                                 those use cases so her in the original                                 analytics component                                 did support that but it was a bit tricky                                 to do given the just like whenever we                                 had a distributed support because you                                 need to calculate all the results then                                 go back out to the shards to get more                                 data so that's not supported right now                                 but when we add in order and the in                                 order expressions that should be it I                                 would imagine that would be enabled                                 using the results of previous                                 expressions to facet over the new ones                                 yeah questions                                 okay if none thanks houston for the talk                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=2N94JVacGaE


