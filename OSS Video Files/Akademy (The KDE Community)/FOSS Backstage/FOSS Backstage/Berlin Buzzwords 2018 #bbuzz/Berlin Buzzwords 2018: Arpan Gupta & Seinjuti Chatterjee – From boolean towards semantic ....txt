Title: Berlin Buzzwords 2018: Arpan Gupta & Seinjuti Chatterjee – From boolean towards semantic ...
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Arpan Gupta and Seinjuti Chatterjee talking about "From boolean towards semantic retrieval models".

The default boolean retrieval model in generic full-text search engines is usually not the best choice for domains like e-commerce because of the lack of decidability about optimal combination of conjunctions and disjunctions of terms that would yield a significant number of relevant results. To improve this, Apache Lucene/Solr introduced the minimum match criteria, where a specified minimum number of query terms must match. But the problem is to decide and tell Solr which are the most important terms (the most salient query theme) that “must” match.

We present a framework to identify (a) important weighted terms in queries (called Must-have Tokens or MTs) and (b) augment them using synonyms. A dependency parser learns weights of tokens to build the MTs list and a neural net embedding and word sense disambiguation is deployed to learn the synonyms specific to MTs in the query context. The models to determine them are built at scale on Apache Spark by analysing clickstream and catalog data across various domains. A custom query parser for Solr is used to augment the queries with these MTs and synonyms.

Read more:
https://2018.berlinbuzzwords.de/18/session/boolean-towards-semantic-retrieval-models

About Arpan Gupta:
https://2018.berlinbuzzwords.de/users/arpan-gupta

About Seinjuti Chatterjee:
https://2018.berlinbuzzwords.de/users/seinjuti-chatterjee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good afternoon everybody I hope you had                               a wonderful lunch but you're still awake                               the topic for today's discussion is from                               bouillon - its semantic retrieval models                               I am Cindy working as a data scientist                               at unboxed I am urban and I am working                               at as an engineer at unboxed right so                               little about unboxed what do we do so we                               are a leading machine learning platform                                for e-commerce search and we are present                                across verticals like fashion hardware                                electronics groceries home improvements                                and we are integrated with a lot of                                platforms and this enables us to collect                                data at the scale of                                            interactions per month which enables our                                machine learning algorithms some of the                                product offerings that we have an                                unboxed site search which are the search                                services and this powers the client                                websites optimizes their queries track                                CTR and C we are the second offering we                                have is intelligent storefront so this                                is a tool that we give to our                                modernizers to control the merchandising                                what promotions to put what deals to                                control and also analytics on that the                                third offering we have is product                                recommendations and this is where we use                                ml learnt                                personalized modules where we can                                recommend products depending on the                                user's intent mentality and tastes now                                coming to the problem statement boolean                                retrieval what is it and why do we need                                semantic retrieval given a query for                                example black bomber jacket what the                                search engine gives you is a parameter                                or a novice called minimum match                                criteria the minimum match criteria can                                be set at                                                            any of the tokens in the query term and                                this corresponds to the union of the                                three tokens and the retrieve sets now                                if we set the minimum match criteria to                                                                                                   means retrieve the documents which have                                exactly these terms and this represents                                the set intersection area that you see                                there now if we set the minimum match                                criteria to a value in between                                say                                                                 three tokens but in that case a black                                bomber it can be a good query but we                                know that bomba jacket is probably more                                important here so there is no way we can                                control the boolean retrieval pertaining                                to the weights of the query tokens and                                that's exactly why semantic retrieval is                                important now there are two important                                relevance measures as we all know                                precision which is the number of                                relevant documents out of the retrieved                                set which shows how accurate your                                results are showing and recall which is                                a coverage measure out of the total                                actual relevant documents how many of                                them have you been able to retrieve with                                semantic retrieval we try to optimize                                these two parameters the first part of                                the retrieval recognizes must have                                tokens which is a way to read the query                                tokens and this makes the results if                                precise but it might so happen trying to                                make it precise we have a low recall the                                number of retrieve documents can we                                really know that's what the second part                                of the algorithm kicks in and that is                                called                                the synonym augmenter so finally what we                                have in semantic retrieval is a query                                which is a diction disjunction on an all                                Clause of the must-have tokens and                                they're synonyms I invite my friend here                                to now let us walk through the details                                of these algorithms all right Thank You                                Cindy                                so what you see on the screen right now                                is there a typical product description                                page it has a product image and a lot of                                attributes that describe this image now                                I have a question for all of you the                                question is which noun according to you                                best describe this product yeah do I                                hear a jacket here right so I am pretty                                sure that most of us would recognize                                this other jacket but the fashionistas                                the longest might say that it is a                                bomber jacket so this is exactly the                                 intuition behind the must-have tokens we                                 tend to think of must-have tokens as the                                 best representation of the product now                                 sometimes it so happens that the query                                 issued is something like Christmas                                 pajamas when the catalog has other                                 variations of it like festive PJs or                                 Christmas PJs                                 when the query does not exactly match                                 the contents of the catalog in search                                 cases to bridge the gap between the user                                 query and the catalog if we make use of                                 synonyms so there are broadly three                                 categories of synonyms that we make use                                 of the first our conventional synonyms                                 which are which could be something like                                 pullovers and sweaters then there are                                 very strongly related words like                                 printers laser jet inkjet and so on                                 there are spelling or lemma variants so                                 this is a language phenomena where there                                 are multiple ways of conveying the same                                 thing for example wireless enabled phone                                 and phone with Wi-Fi they can be the                                 same thing and they will lead to similar                                 result set then Packers T so Packers T                                 and Green Bay Packers t-shirt they would                                 they mean the same thing and will also                                 lead to similar result sets T and t                                 shirts are the llama variants here so                                 now let's talk about the query                                 understanding clear for a query like                                 black bomber jacket when it first hits                                 the per-unit is standing layer it is                                 first intercepted by the must-have token                                 recognizer in such case bomber jacket                                 would be recognized as the must-have                                 token and then it will be augmented by                                 synonyms like motorcycle jacket biker                                 jacket hooded jacket and so on by doing                                 so we not only show one precise result                                 which is the reversible bomber jacket we                                 actually expand our result set into six                                 highly relevant products now let's delve                                 deeper into the nitty gritties of the                                 generation of must-have tokens we make                                 use of the top queries from our domain                                 query logs the first step in this                                 generation is to generate a dependency                                 parse of the queries so dependency                                 parser basically takes a sentence as an                                 input and it returns the syntactic                                 relation a syntactic structure of the                                 sentence so in this case it tells us                                 that black is an adjective and it is                                 connected with jacket which is a noun                                 and this relationship between them is                                 the adjective modifier relationship it                                 also tells us that bombard which is a                                 noun is connected with jacket and it                                 also describes jacket                                 this relationship between two nouns is                                 called a compound relationship so Jacket                                 is actually the root of both these                                 relationships now we observe that among                                 our top queries more than eighty-five                                 percent of them had the adjectives                                 modify relationship while more than                                     of our queries had the compound                                 relationships so we make a heavy use of                                 these relationships we are particularly                                 interested in the root of the adjectives                                 modifier relationships so in this case a                                 black jacket have the adjectives modify                                 relationships and jacket is the root of                                 it so we will add jacket to the list of                                 candidates then for bomba jacket which                                 are connected through the compound                                 relationships we will add both of them I                                 mean both the nouns as phrases to the                                 list of candidates now once we have this                                 exhaustive and comprehensive set of                                 must-haves tokens the next step is to                                 assign importance to each of these                                 candidates we do so by assigning a score                                 to each of them the first the this code                                 is dependent on two major factors so                                 it's a composite score the first factor                                 here is to look at the number of queries                                 that each of these candidates will                                 actually cover and among all the queries                                 that they cover we look at the queries                                 where they actually are the root of a                                 mod or are part of compound                                 relationships now let's be realistic we                                 will always get grammatically incorrect                                 queries like jacket bomber black where                                 dependency parser would tend to generate                                 incorrect dependency power structures in                                 such cases black might be added to the                                 list of must-haves tokens but this is                                 handled here because the count of the                                 queries where they actually are the root                                 of a mod or a compound relationships                                 will be quite low now my friend will                                 tell you about the synonym generation                                 pipeline right thanks so now the first                                 part of the problem was to weigh the                                 query tokens figure out what are the                                 important must-haves                                 instead once we have that we move on to                                 the second stage the synonym generation                                 the first step here is to build a local                                 corpus and a local purpose consists of                                 the catalog and a click stream the click                                 query logs for a client or a particular                                 domain the second step is to train what                                 vector emitting of this and we were                                 really talking about that in the next                                 few slides once we have the entire list                                 how we generate synonyms is a two-prong                                 process so we feed the list to a global                                 corpus open source English concept tools                                 like concept net and word net and we get                                 the candidates from there the second                                 step is to feed it to the local word to                                 vector eminent and generate the                                 neighborhood the nearest neighbors of                                 these words now sometimes it so happens                                 that the context of the word is not                                 established so as a result we might have                                 different senses I have called sin sets                                 across the board so to figure out the                                 correct sense of the word and to prune                                 out the noisy ones we use an agar that                                 we called word sense disambiguation and                                 we'll be talking about that in the next                                 few slides finally once we have the                                 candidates the winners we apply spell                                 correction and stemming to figure out                                 the final since it now language so                                 language has various nuances it can be                                 confusing as you see here I died under                                 standing in front of the ATM machine the                                 kid walks up to him asks him dad what                                 are you doing he says just checking my                                 balance but did he mean that he was                                 checking his posture with his hands high                                 up in air or was he talking about the                                 amount of money in the ATM so it is                                 difficult to understand language but                                 nowadays there is a lot of tools which                                 are generating so many meals and                                 documents and texts that we need to                                 teach a machine how to be able to                                 classify these documents and cluster                                 them that's exactly where the word                                 vector a meaning helps so it is a way of                                 representation in a machine format of a                                 vector of high dimensions which can                                 embed lexical ambiguity semantic                                 relationships and concepts so this                                 concept was in                                 by Hinton who said that there is should                                 be a distributed representation of                                 symbols for example cat as a symbol                                 means not much but if we can add the                                 neighborhood of cat and its different                                 concepts that might be more                                 representative and that is the intuition                                 between what vector and weddings it's a                                 function it's it's a function where a                                 word is represented by the contextual                                 words or words in its nearest                                 neighbourhood and the optimal dimensions                                 capture most of the nuances of the                                 modeling which is context ambiguity                                 semantics and different things now there                                 are two popular neural network                                 architectures to learn this embedding                                 one is the SIBO models were given a                                 context you're trying to predict the                                 missing words and the second                                 architectures keep brand models we are                                 given a word you're trying to predict                                 the context for the product that we have                                 released we have used boogers were to                                 make algorithm which is a combination of                                 the C Bell and skip REM to give you some                                 examples of what it can do so if we look                                 at a keyword called jacket and we look                                 at the word vector omitting space and we                                 do PCA just to project it in a lower                                 dimensional space to improve our                                 understanding what we see are different                                 flavors of jacket evolving so on one                                 hand you have one ba jacket leather                                 jacket windbreakers on the other hand                                 you have winter wear like a nut jacket                                 puffer jackets and the third type is                                 cardigan sweaters and things like that                                 to give you another example the word                                 shoes has different concepts in the a                                 meaning space so it can range from                                 running shoes sneaker sportswear to                                 flip-flops and sandals and regular wear                                 kind of shoes so these are the different                                 concepts of the word shoes now we use                                 this word vector omitting and we do it                                 on the catalog space that gives us the                                 set of synonyms that you saw in the                                 slides that open explained                                 so for bomber jacket we end up with                                 biker jacket moto jacket a leather                                 jacket as good recommendations for                                 synonyms similarly if you see the urines                                 and the nearest neighbor you see the                                 different stylings of earrings drop                                 earrings hoop earrings stud earrings                                 evolving                                 on the catalogue and also related                                 products like necklaces and bracelets                                 now if you do the same operation in the                                 user query space it so happens that                                 users are also looking for synonyms like                                 denim truckers and softshell if we do it                                 on glass the different kinds of concepts                                 of glass of all like plumpers lipsticks                                 lip liner lip color bombs etc so having                                 learnt the word vector omitting the                                 second part of the application is to do                                 word sense disambiguation the word                                 orange if fashion domain means a color                                 but if you feed it to an open-source                                 corpus like concept native word net it's                                 going to return you the four different                                 senses of the word orange first insect                                 indicates color the second one is fruit                                 third one is a tree fourth one is a                                 pigment now how do you teach a machine                                 to pick the correct cluster here which                                 is the first one that's where we can                                 also use word vector embeddings so we                                 take the entire word cloud represented                                 here projected in the embedding space                                 take the vectors and figure out the                                 centroid and then we measure the cosine                                 similarity with the M in English of the                                 original key which is orange and the one                                 which minimizes that distance is the                                 winner so in this case the winner is the                                 color sunset and we get good                                 recommendations as synonyms like orange                                 red salmon blonde all of all indicating                                 colors now to summarize the things that                                 we explain here firstly we get a query                                 we do a dependency parsing on the query                                 a kind of scoring that gives us the                                 must-haves tokens we learn the word                                 vector embeddings on a local corpus we                                 treat the MDS as keys and use the word                                 vector a meaning on the catalogs and on                                 the click locks to generate the synonym                                 candidates we also use it as a key and                                 fit it to an open source concept net                                 wide net api is and we figure out the                                 synonym candidates but we prune the                                 noisy ones using word sense                                 disambiguation and finally the query now                                 becomes an all Clause a disjunction of                                 empties and synonyms and this is finally                                 effect to the EDA's max solar query so                                 we have written the custom solar                                 lagoon's which can convert this to a                                 format that solar understands all right                                 so conclusions and future work since                                 dependency path sits at the core of our                                 must-have token generation we want to                                 train our own dependency parser using                                 dependence using deep learning and we                                 intend to extend the must-have token and                                 the synonym dataset from one client to                                 other clients and finally over one                                 domain we intend to improve and simplify                                 the vector algebra operations so what I                                 mean by that is just like mu coleauxv                                 established gender as a vector in a                                 meeting space we want to do the same for                                 a synonym a vector and you want to                                 improve the training times for a word                                   vector aining by making use of MapReduce                                 and that finally we want to implement a                                 feedback mechanism to auto create the                                 good synonym and must-have token pairs                                 by automatically pruning out the noisy                                 pair so currently our accuracy sits at                                 around                                                             hundred percent or as as close to                                 hundred percent as we can thank you for                                 listening we will take your questions                                 now thank you so much                                 [Applause]                                 discussions for speaking                                 hey thanks a lot for the presentation                                 really impressive that you implemented                                 all the stuff and have it running in                                 production and did you ever measure like                                 how yeah how much slower queries get by                                 the pre-processing because dependency                                 parsing is very slow as far as I                                 remember maybe for short queries not but                                 did you measure that or the performance                                 impact right so we we are hitting those                                 problems introduction so what we do is                                 we train it offline so we do the offline                                 analysis on the top queries daily or at                                 an hourly basis we learn the dependency                                 parse and I must have tokens and we put                                 it in a cache and that's how we can do                                 the online part because we did notice                                 that the dependency parsing on a online                                 part is quite slow and that's the timing                                 solution for any latency issues that we                                 might see I didn't quite or didn't                                 completely understand what you do on                                 disambiguation so what we try to show                                 you is to expand the synonym candidate                                 sets so other than the local corpus we                                 also want to use open source tools like                                 ordinate and concept net and that's                                 where we are getting a lot of synonym                                 candidates but the word orange in                                 fashion means color and therefore                                 applying a synonymous of fruit wouldn't                                 make sense there so that is exactly                                 where we do the word sense                                 disambiguation so we get the cin sets                                 from word net and then we I trade over                                 each and for each one we look at the                                 word cloud and we take the local and                                 bearings and that's when we figure out                                 the distance so no the orange fruit                                 seems very far from what we're looking                                 at and this color seems more intuitive                                 because it's distance is minimizing to                                 the vector of orange and that's how we                                 kind of figure out the right sensit and                                 take it forward and second question the                                 queues somehow measure the improvement                                 of the recall and position you get by                                 applying all these methods definitely so                                 there are two ways that we are trying to                                 do that so a lot of it was manually                                 generated must have tokens or synonyms                                 put in place so                                 after we moved to this automated method                                 there has been a huge uptick because lot                                 of clients we can do in parallel and we                                 are trying to do a/b testing between                                 this manually generated set and the                                 automatic set and we are seeing an                                 improvement of                                                         the clients eyes and things like that                                 any more questions um so are you                                 harvesting synonyms both from the corpus                                 and the query logs as well as wordnet                                 and yes so on the sort of word embedding                                 query log document based it's very often                                 that you'll find nearest neighbors that                                 are that are really not the same ideas                                 but that are used in similar contexts                                 right I think one of the speaker's gave                                 an example of like you know confirm and                                 cancel tend to be used in a similar                                 context and they're very different words                                 can you talk a little bit about how you                                 end up pruning or dealing with these                                 sort of false positives that you might                                 find if you're if you're looking at                                 nearest neighbors and an embedding space                                 right so so the way we are trying to                                 solve that and this is work in progress                                 is uh we use some feedback mechanism now                                 from humans and we are trying to build a                                 classifier on top of that synonym set                                 generations right now the accuracy that                                 it puts                                                                  is that this to any person and their                                 feature vectors and if we can do                                 something like a relevance feedback                                 where we automatically reject the                                 suggestions when it is in the boundary                                 cases so that is something which is                                 working progress and that's how we plan                                 to solve the problem thank you alpha and                                 situ T I think we can take a break for                                 coffee so again we'll meet back in                                 another                                                      you
YouTube URL: https://www.youtube.com/watch?v=n_nR5m8rZRM


