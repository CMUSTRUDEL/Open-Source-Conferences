Title: Berlin Buzzwords 2018: Doug Turnbull & Tommaso Teofili â€“ The Neural Search Frontier #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Is search the next industry to be revolutionized by deep learning? Lately researchers have been applying neural networks to search applications, with impressive gains. Search users use different language than what's contained in the corpus. For example, doctors create articles discussing jargon like 'myocardial infarction' but patients search use lay-terms like 'heart attack'.  

Mapping vocabularies using expert created taxonomies or word embeddings (word2vec, LDA, etc) can help. Manual approaches can take a great amount of work or don't map between searcher and document vocabulary. When clear associations between relevant documents and queries can be made, neural search can learn the patterns between query and document language embeddings, with tremendous gains on text search. Such embeddings can also be used to provide alternative representations of the user queries in order to better capture the user intents.

Read more:
https://2018.berlinbuzzwords.de/18/session/neural-search-frontier

About Doug Turnbull:
https://2018.berlinbuzzwords.de/users/doug-turnbull

About Tommaso Teofili:
https://2018.berlinbuzzwords.de/users/tommaso-teofili

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so as as you saying I'm Doug Turnbull I                               wrote the book relevant search here's                               all my information I work for a company                               open source connections where we help                               search teams reach their highest                               potential in terms of mostly that means                               relevance and delivering higher value                               for their customers as in consulting and                               my name is tamas that they awfully                                worked for Adobe as a computer scientist                                and also an Apache Software Foundation                                member in yeah I'm very interested in                                the search field and topics and you're                                writing a book yeah yeah it's a it's a                                meet now so I hope to be that to be                                finished by end of this year so yeah I                                mean playing paper it's exciting when                                you get it in paper ok so today we're                                going to be talking about deep learning                                and search and can deep learning be a                                thing that can really have a big impact                                in the search landscape and I want to                                talk to start by talking about I'm gonna                                run through some slides and for me                                personally this is doing this talk has                                been an educational opportunity and one                                of the things I always try to imbue to                                people is you don't have to be a deep                                expert in a topic like deep learning or                                machine learn an LP to give a talk and                                often the best speakers are people and                                bloggers and that sort of thing are                                people actually learning as they're                                going so I want to I say that to                                encourage other people to feel like they                                should feel safe giving a talk even even                                when they're becoming familiar with the                                topic so there we go so what often                                happens with search this is a very                                common problem in search there's kind of                                two specific things that I'm going to                                talk about what is this matching problem                                almost all of must search clients that                                we work with have a vocabulary mismatch                                problem where you have searchers and                                this is like somebody this is basically                                how my dad would search the law                                searching for a dog catcher and you have                                a bunch of laws that say animal control                                law right and and and this is a common                                problem in search and we spend a lot of                                time trying to work through                                these problems and I really like                                Giovanni's talk this morning that was                                about a lot of the you know manual                                search management that we do and a lot                                of our work we see the same kinds of                                things where it's actually one of the                                secret bullets of relevance is to have a                                great taxonomist or librarian who's                                manually curating the mapping between                                how searchers consider talked about                                their content and how the corpus talks                                about the content and I have a talk                                taxonomical semantical magical search                                that I gave that if you're interested                                this is uh obviously one of the things                                I'm curious about is I also have many                                clients who just don't want to get                                involved in this kind of work or they or                                the domain is too large to really build                                a this kind of controlled vocabulary                                taxonomy to solve this problem and so                                deep learning is exciting to me from                                that point of view because you see this                                is sort of a feature engineering thing a                                manual feature engineering thing and one                                of the themes of deep learning is for                                text and images there's all these layton                                rich features why are we creating all of                                these manual features for these things                                when deep learning helps us learn these                                abstractions right ranking is sort of                                the other side so you think I'm matching                                and ranking and of course ranking is                                very challenging we have all of these                                ways of querying our our search engines                                so learn elasticsearch they have a very                                rich query DSL we are always fine-tuning                                these things right there's all these                                weird heuristics tf-idf stoller                                elasticsearch I mean tf-idf is just some                                like thing that came this you know so I                                was on a Twitter thread once for someone                                was saying people who don't really                                understand how search works they do all                                these hacks but I'm pretty sure that                                everyone including me when they're                                actually solving a real evans problem                                 everything feels like a hack and that                                 goes for tf-idf - I mean tf-idf s is                                 statistic that just came out of people                                 playing with numbers until it turned out                                 it was the best given some relevance                                 judgments and all of these things are                                 kind of like heuristics for getting                                 close to what the user wants whereas in                                 deep learning deep learning helps us                                 sort of maybe get deeper than that                                 and of course we have learning to rank                                 and learning to rank as a bit is a                                 different field than deep learning but                                 learning to rank isn't perfect either                                 learning to rank comes with its own                                 problems it's only as good as your                                 features and those features of course                                 are just based on this stuff so if your                                 features aren't mapping between dog                                 catcher and animal control officer                                 you're still going to have problems it's                                 the first thing the first stop on our                                 deep learning journey and and when I                                 look at these and again inspired by what                                 you see in deep learning for NLP deep                                 learning for images are there ways to do                                 to learn better feature representations                                 that aren't based on things that are                                 sort of in a layer of abstraction that                                 maybe a machine would learn but aren't                                 in a language that we could access and                                 how many people have used word to Veck                                 before I think we're do Veck so so how                                 many people have used layton let's see                                 if I can say this right darisha lay                                 allocation Lda yeah so you have these                                 sort of these sorts of topics and you                                 can build what are called embeddings out                                 of them and embeddings are great because                                 what you're doing with them is you're                                 remembering something about the context                                 that those words tend to appear in and                                 you know I swear I did this without                                 knowing what the keynote talk was but                                 cryptocurrency is hyped question mark                                 appear you know Bitcoin hype seemed to                                 appear in similar context and one of the                                 great things about word embeddings is                                 they're they're recording this                                 information you have these situations or                                 cancel and confirm are actually                                 appearing in similar contexts so weird                                 embeddings are really vectors right                                 they're n-dimensional vectors in a some                                 vector space and of course they're                                 probably not two-dimensional vectors                                 they're probably often                                                 vectors is a number that jumps out a lot                                 but when you map them in a vector space                                 you can see words that occur in similar                                 contexts occur together this is the goal                                 of a good embedding and say you see that                                 in both situations                                 of course search is also based on a                                 vector similarity right we have our                                 query up there and we try we have our                                 query terms you know regulation we're                                 trying to find the documents that are                                 about regulation we're trying to find                                 the documents that are about Bitcoin                                 right and upper right you know sort of                                 represents our query the farther in the                                 upper right we get the more about the                                 document is to our query and so we sort                                 of have this notion that aboutness is                                 tf-idf for each of these terms and so                                 the closer our doc gets to having more                                 of each the more it's going to be rated                                 relevant because it's closer to where                                 our query is this is also a kind of                                 vector similarity and of course the                                 question that often comes out this is a                                 hypothesis and I've claimed it I say it                                 as a hypothesis because I want to be                                 clear that you know I have going down                                 the chain of saying I'm going to take my                                 data straight out of Word avec latent                                 semantic analysis Lda                                 and just put it my search engine and                                 everything will just be fine right and i                                 pathi says is that this embedding                                 similarity where we have maybe we've                                 done the same thing with our documents                                 and it has you know we have an embedding                                 for a document but this embedding                                 similarity is somehow related to this                                 aboutness similarity that relevance                                 tries to capture and this is a                                 hypothesis that works out sometimes you                                 can see on the bottom right                                 it appears to be working however in some                                 this hypothesis does not always hold and                                 this is something important to take away                                 you know this is a great example I                                 actually heard this you know chat BOTS                                 do a lot of support and this is a very                                 related field is that uses a lot of                                 embeddings answering the question I                                 would like to confirm my my reservation                                 and you respond with oh are you sure you                                 want to cancel your reservation is is a                                 bad that's a bad time you're having                                 so this hypothesis doesn't always hold                                 and it's important to know that it's                                 hypothesis it's also important to know                                 that tf-idf itself is a hypothesis                                 measurement of about Ness                                 I should say that you know that often                                 doesn't work out too so context alone                                 isn't enough and I think it's good to                                 point out and this comes up a lot in                                 Tomaso's book there are ways just in the                                 same way we do a lot of data modeling                                 and search engines there's actually a                                 lot of things that you do before your                                 data gets to word go back one thing that                                 comes up that's actually in the word                                 defective at paper is they make sure to                                 pull out the entities the phrases that                                 you should really consider one thing so                                 Boston Globe is treated as one word in                                 that vocabulary that's in the original                                 word to vector box so that you're not                                 confusing Boston Globe the newspaper                                 with Boston the city or Globe they're                                 treated as separate so and there are                                 other things to do we could we could use                                 other factors of our words the sentiment                                 of the language you know confirm and                                 cancel usually will have different sent                                 sentiments with cancel obviously maybe                                 someone's yelling at the search engine                                 or something synonyms and all kinds of                                 other ways of doing this parts of speech                                 is another one so when one low-hanging                                 fruit is do you want to talk about this                                 library sure yeah so I think one of the                                 key things in this talk is also I mean                                 getting his two things one thing is                                 getting a general understanding of how                                 deep learning can be useful in the                                 context of search but the other thing is                                 also kind of get a few quick ideas or                                 things that you as the audience can try                                 out in your projects so that that's the                                 idea but the so-called low-hanging                                 fruits so so if the probability                                 distribution of words in the corpus can                                 provide enough information                                 to predict whether two words appears in                                 similar context and that's exactly the                                 slide that Doug just showed up then we                                 can kind of use them as synonyms or I                                 mean not a strictly synonyms in a way                                 they told us to school to think about                                 cinnamon synonyms but basically things                                 that make sense to see the words that                                 make sense to be used to replace one                                 another and that's kind of the query                                 expansion kind of thing so I think                                 that's one thing where you can use kind                                 of adjusted versions of word Tyvek for                                 example or basically word embeddings                                 that also can takes take in in account                                 sentiment or other things that can                                 mitigate the problem about for example                                 cancel and confirm and then you can you                                 can use them to learn query expansion                                 very very effective query expansion                                 algorithms that you can use under the                                 hood so that's the idea about so                                 cryptocurrency and Bitcoin I mean you                                 cannot type you can you may not type                                 cryptocurrency in the in a query but the                                 the search engine would turn it into                                 Bitcoin on the road and fight two                                 different queries you know to                                 queries with cryptocurrency and Bitcoin                                 using that and this is kind of expected                                 to raise the the recall yeah so there                                 can often be still a mismatch because                                 when people use word avec or all of                                 these tools when they're building                                 embeddings they're still fundamentally                                 using their corpus to do this for the                                 most part usually they're running it                                 against a corpus and they're learning                                 similarities and legalese and not Laye                                 speak so still we have the same problem                                 and may not mean I solve that this the                                 problem for everything and it depends a                                 lot                                 everything depends and I'm a consultant                                 so I say it depends it depends on what                                 your corpus is                                 and how how much overlap there is                                 between your search language and your                                 corpus if you're supporting lawyers may                                 be great if you're supporting non                                 lawyers may not be great so one thing I                                 ask myself a lot is can we build our                                 first step on the neural network trained                                 is can we build better embeddings can we                                 build embeddings that get closer to our                                 searchers notion of relevance                                 you know maybe so you know and I think                                 it's a good this is a good thing I think                                 everyone I'm gonna go through exactly                                 how this stuff works I sort of feel like                                 they should pretty soon they'll be they                                 should be teaching this stuff in high                                 school like this machine learning stuff                                 it shouldn't just be about coding we                                 need to get to this stuff in high school                                 and I think it's anything something                                 anyone can really understand so the word                                 Tyvek model is relatively                                 straightforward and it's a great thing                                 to start to start to learn about when                                 you're doing this kind of machine                                 learning and once you learn about it                                 then you can hack it and do crazy things                                 with it you basically have this very                                 simple table that's storing and                                 embedding for each term in this case                                 energy and Bitcoin and we're taking                                 those terms we have a dot product and                                 you can imagine this is initialized                                 randomly this may not even be it's a                                 just a poor bottle take a dot product                                 which is just that math up there a                                 sigmoid which really just forces                                 everything to zero to one and if it's                                 close to one its the prediction of the                                 model that this is a true context but                                 this is something these are true context                                 words close to zero the prediction is                                 it's a false context it's not an actual                                 context word so maybe if Bitcoin and                                 energy were in the same context the                                 model predicted                                                         we have some it's close to you know if                                 we think about this prediction as a                                 probability of being in the same context                                 of occurring in the same context then                                 you know we're two-thirds of the way                                 there and the goal is the arrows got a                                 little misaligned but the goal here is                                 as we now that we have maybe we randomly                                 set this up but we know the real answer                                 when we're training the goal is to shift                                 the stuff that's in the true context up                                 and move the stuff that's not actually                                 sharing in context down we want this                                 number this prediction to go down we                                 want that prediction to go up                                 so our model becomes more accurate and                                 this is using I think this is using an                                 interesting approach called negative                                 sampling which i think is the easiest to                                 get into and what negative sampling does                                 is say okay I've got this term Bitcoin                                 I'm trying to learn about I've got a                                 true context word for my training data                                 and I'm randomly selecting some negative                                 words that I know don't occur with that                                 and what's interesting about this is                                 learning to rank also has an issue where                                 it's very important to have lots of                                 negative training data and so there's                                 there's some like interesting symmetries                                 here or something in the universe that                                 we're learning about and the question is                                 how do we tweak this and so we have this                                 big scary math thing and you don't need                                 to be intimidated by this big scary math                                 thing but what is happening is and I'm                                 just gonna go ahead and these that this                                 stuff in here that's just the model that                                 I just told you about                                                    something is a sigmoid VC are a dot                                 product of two embeddings in this case                                 it's the true context and we take away                                 you know this is what we want to go up                                 the true context because this is what                                 our you know this is what we want to go                                 up we want this value to go up in these                                 values to go down and so this whole                                 function is designed to be maximized ok                                 so you know how do we how do we maximize                                 that and I'm not going to get into the                                 to the weeds of gradient descent or in                                 this case ascent but the idea is if we                                 take the derivative of that function we                                 told you about we know which direction                                 in each of our the V here is the Bitcoin                                 in each of our bitcoins direction to                                 push that number to make that that the                                 that loss or that likelihood higher that                                 we're getting something more accurate                                 and then you talk about this idea and                                 Deve learning of back propagation so                                 back propagation is where we take that                                 that thing we learned about about this                                 loss function and we propagate it back                                 to the other weights and deep of course                                 you may know that deep learning                                 that's deep it when we do deep parts of                                 it we actually have several layers of                                 things to keep propagating back and back                                 and hopefully keep maximizing or                                 minimizing when you say minimize the                                 loss in this case that's maximizing                                 something so that's what when you hear                                 people talk about back propagation it's                                 really just sort of doing this kind of                                 learning and there's not magic here it's                                 just math it's math that I feel like if                                 more of us understood we would all make                                 better decisions and our jobs in our                                 lives and fall for the silly AI                                 advertising a bit less because people                                 will say this is this AI of course we                                 can do the same kind of thing with                                 documents in this case this docked avec                                 uses what's called paragraph embeddings                                 and a paragraph could be a whole                                 document it could be a subset of a                                 document and we can do the same kinds of                                 things so we can tweak these guys you                                 know here is Bitcoin here is a true                                 document that it occurred in we can                                 tweak that prediction to be up here's                                 Bitcoin and a document it that is                                 unassociated with it and we want that                                 prediction to go down and in this COI                                 and in this model you know right now                                 this is still tweaking the Bitcoin                                 vector but you're you are learning both                                 the document embeddings the paragraph                                 embeddings and the document the same                                 time if you notice I'm leaving you                                 Easter eggs in the upper right if you                                 get these slides you can click on links                                 and follow to learn more stuff so you                                 want to take this one way so these are                                 the low-hanging fruit so similar word                                 embeddings lie close to one another                                 I mean similar in semantics so that's                                 the the kind of results of word of word                                 to like and other kind of similar                                 algorithms about word embeddings and the                                 same stands for paragraph factors                                 and document embeddings in general so an                                 other research have shown that words                                 appear often close to the documents that                                 they are more represented into so if                                 again if we think about two to Lda and                                 how we can we can kind of think of using                                 that together with word embeddings to                                 discover topics about about documents                                 but just looking at the position of                                 words into this embedding space and so                                 but as a search engineer if you think                                 about your search project we've always                                 or I mean we mostly use our time                                 thinking about terms and on analyzers                                 and tokenizer z' and filters and stuff                                 like that just to speak the ricean api's                                 in the ricean api's so is this going to                                 change all all of this so did anyone say                                 terms when it comes it comes to                                 retrieval or frequencies when it comes                                 to rent Kings so how these things work                                 together with the statistical models we                                 are like now we have for example leucine                                 uses beyond                                                            statistical algorithm for ranking so                                 this is not really low hanging fruit                                 it's not so low and the the thing we can                                 do and the experiments have done by the                                 way I've started kind of the same way as                                 TAC started so is the there is this hype                                 about word Tyvek and the embeddings and                                 the neural networks and stuff like that                                 so does that really work in the context                                 of search so let's see and I encourage                                 you to try it out and think about not                                 just as these                                 tools as a kind of magic but as things                                 that we need to understand in order to                                 make them useful for for us at the end                                 of the day and to jump a bit forward I                                 think in terms of my experience in terms                                 of retrieval is that terms and vectors                                 and frequencies and distances in the                                 embedding space work well together                                 so the the old knowledge and the old                                 models also to say models work work well                                 in conjunction with this stuff and you                                 may realize that each of these models                                 feel also gaps in the algorithms from                                 the other model great so I think some                                 things I think about and again this is                                 the frontier so we're thinking about how                                 we can push this and I actually like                                 sharing I could go ahead and probably                                 similarly get a patent for some of these                                 ideas I'm sure but I'm sure I'm not the                                 only one who's thought of this stuff but                                 I really like to share if I have an idea                                 I want to share it out there as much as                                 much as possible and one of the thoughts                                 you know crazy idea I had is can you                                 take this and build relevance based                                 embeddings can we take our get to like a                                 single vector space so often you know we                                 keep talking about vector spaces can we                                 use deep learning to learn the not just                                 the document vector space or the vector                                 space of the searcher to get one vector                                 space that sort of a similarity will get                                 tell you query document similarity in in                                 one space and one thing you think about                                 is can we use this negative sampling to                                 get sort of a relevance based embedding                                 because we have a lot of supervised data                                 right often if you're doing any wanting                                 to rank you might know that this                                 document is relevant and these are                                 irrelevant can you learn can you do the                                 same negative sampling technique I just                                 showed you to learn to push push this                                 one this vector space for Bitcoin the                                 document so that it will when exposed to                                 a query that's relevant for it will                                 return closer to one and when it's                                 exposed to a query it's not relevant                                 work for it will return something                                 closer to zero you could of course the                                 challenge is these embeddings are not                                 generalizable they're based on queries                                 and documents we've already seen we're                                 maintaining a giant table of these                                 things so we hope we need to have a lot                                 of data to store okay all of the Bitcoin                                 nettings and all the document embedding                                 is that sort of thing which if you're                                 doing embeddings anyway perhaps you're                                 already doing that                                 can we pre train you know if we pre                                 train with our corpus or our sessions                                 and then start to tweak that's another                                 idea so maybe we start with almost like                                 a Bayesian prior of this is what our                                 corpus says and then keep making having                                 an evolution maybe even an evolutionary                                 approach over time to move our                                 embeddings away from the document space                                 and towards the searcher space it's                                 another thought I think to one thing you                                 can do is just take our your query                                 vectors and sort of average them closer                                 you know the documents that are relevant                                 for them and just move them into the                                 document space so if you know that                                 bitcoin hype is in this in this area                                 then you can average the relevant                                 documents and get a push it into sort of                                 the document embedding space so one                                 thing I wanted to mention and I thought                                 of this as I was in the mobility talk he                                 talked about ranking it as a that's a                                 sort of an LTR                                 learning neural network LCR                                 implementation well one thing I do want                                 to mention this is sort of another                                 low-hanging fruit is in learning to rank                                 implementations embeddings are often                                 some of the best features that people                                 get gained from I've heard that from                                 multiple people in teams independently                                 so I want you guys to you know realize                                 this you may be able to take this data                                 and use it in the context of learning to                                 rank along with traditional features                                 like Tomaso says in traditional                                 relevance work to get something to help                                 yourself so let's see if we can do this                                 in                                                                   models so language models are about                                 predicting text and what I'm going to                                 talk about and this I might in the next                                 session is you know                                 beddings are great way to get started                                 but language models I think especially                                 recurrent neural network based language                                 models I think in five years because                                 should are going to be things in every                                 search engineers toolbox and I'm really                                 excited about them and what language                                 models do is basically if you're not                                 familiar they're predicting text eat                                 pizza and you may say well this will                                 obviously make such as if I'm texting                                 someone I want auto suggests but also                                 make sense if you have existing text and                                 you're like what goes with this texts                                 okay                                 you know you can almost imagine a graph                                 of this thing might inspire this next                                 thing which might inspire this next                                 thing so there's a lot of capability                                 there for for doing some expansion and                                 when people teach about recurrent neural                                 networks they often start with saying                                 why regular deep learning doesn't work                                 well for it but I think it's actually                                 maybe because I'm come from a search                                 background it's actually easier to start                                 from a traditional language model like a                                 Markov language model and a Markov model                                 basically you can think about it as just                                 a transition matrix if we start with eat                                 there is a high probability of eat pizza                                 there is a very low probability of eaten                                 nap and a slightly higher probability of                                 each chair just because it's a noun                                 right cat high probability of nap low                                 probability of cat and hopefully no                                 one's eating cat pizza so you can                                 imagine this is like if I have a word I                                 can predict the next one you can also                                 then Te'o if I do eat pizza I might say                                 okay well what comes after pizza and I                                 could say oh maybe there's another word                                 and you can keep like generating text                                 forever and back in the early                                       there was a lot of silly web things like                                 automatically generating research papers                                 that I think more or less were based on                                 this kind of approach now the next                                 question is you start to think what if I                                 represent my word as an embedding so                                 here's the eat embedding can I build a                                 transition matrix that gives me the                                 embedding of the next word                                 and indeed with a Markov model if you                                 have you can actually sort of do this                                 you you get kind of weights and you                                 might learn these weights through back                                 propagation of what's the best way and                                 again if we're gonna do basically a dot                                 product if we look at if we have this                                 kind of embedding and we say okay how                                 much should for position zero                                 how much for position one of the source                                 of embedding count and so on and so                                 forth and so you can you take this and                                 you and you can multiply it through and                                 get the embedding of the next word so                                 transition matrices need not strictly                                 apply to to that and both of these cases                                 okay that's interesting but in both of                                 these cases really require context so                                 far we've just looked at the word before                                 and the weakness for these language                                 models is we don't know that the race is                                 on eat dust right                                 so if we have some context we know dust                                 is very likely to be a word in this                                 sentence so how do we get context so an                                 old embedding this is just what I showed                                 you if we have the eaten bedding we can                                 sort of predict the embedding of the                                 next word so this is sort of a the                                 transition we're going from input to the                                 next word okay so this is just what I                                 showed you before and the next question                                 becomes can I use this information about                                 the past state to do something sort of                                 with to predict the next state and okay                                 we get the next word pizza maybe comes                                 through here input again this is another                                 transition matrix                                 I just put dot dot dot to a new                                 embedding to predict what might go there                                 right everything we've done so far and                                 then this kind of interesting thing                                 happens if we could think about almost                                 to transition matrices                                 go from previous context to new context                                 that also bleeds in the current what we                                 just fed in as input we could somehow                                 learn the right way through machine                                 learning to balance those together and                                 come create the next context and so on                                 and so forth                                 and I haven't used the word neural                                 network yet and then you could have sort                                 of a prediction the output embedding and                                 need not even be embeddings you could                                 actually just have a large set of                                 vocabulary but embeddings are usually                                 used and at any state you can say okay                                 what's the likely word at this point                                 given what have you seen before and you                                 can keep doing this pattern over and                                 over and you know what the right                                 embedding should be because you know the                                 words that go in each spot so with that                                 right knowledge at each output you can                                 do back propagation to learn all of                                 these weights to get your model                                 increasingly accurate in other words we                                 might have this simpler view races on                                 eat and we do all of these things here's                                 our X - hidden X - hidden X - hidden and                                 then finally we output something and we                                 have our transitions at each hidden                                 state and this is really just what a                                 recurrent neural network is it's really                                 a set of Markov models chained together                                 where you learn all the transitions                                 through back propagation now it for                                 search another low-hanging fruit is                                 it's obvious to imagine how we might                                 inject contextually relevant items into                                 here using this sort of model and of                                 course it's not necessarily a silver                                 bullet because we're still in this                                 situation we've we were able to model                                 our corpus well or maybe our Searchers                                 vocabulary but we're still in the                                 situation where we're not we're not                                 learning the ideal sort of language                                 model that connects both of them and                                 that's where we get to and I think this                                 is really where this stuff gets really                                 exciting so of course we can make our                                 sessions documents right this is a very                                 common thing to do                                 you could build embeddings just in that                                 space and I sort of already said this we                                 could have our document search term and                                 we could do this sort of LTR sorts of                                 things but what is really interesting is                                 this new new exciting field of neural                                 translation and this is an encoder                                 decoder a framework you hear this sort                                 of thing and we have like the races on                                 and there's German should I try it Renee                                 renin East in good ok and what what                                 you're doing here is you start and you                                 get sort of this hidden state between                                 one neural network and another that's                                 primes the next neural network when                                 you're doing recurrent neural networks                                 and that actually is enough information                                 it turns out to begin to translate this                                 information you do enough training with                                 the actual translation and it this                                 neural network becomes a translator of                                 sorts and it's kind of its kind of                                 amazing how how that works and by the                                 way I'm talking about recurrent neural                                 networks                                 there are many architectures of RN ends                                 and I'm just not going to get into in                                 this talk there's a great blog article                                 seek to seek the clown car of deep                                 learning so you can imagine could we                                 translate our documents to queries could                                 we take a document animal control law                                 and we know what queries might come out                                 and we can sort of do this sort of                                 translation between the two languages                                 and can our ranking be a sense of how                                 likely this this is going to happen how                                 likely this is going to be relevant or                                 not relevant I think I think we can I                                 don't know if any was really doing this                                 anywhere the other thing is can we use                                 graded judgments and of course these                                 predictions are sort of and you can                                 think of those word embeddings for the                                 words we're predicting or they would                                 actually be the vocabulary probability                                 distribution of our large vocabulary and                                 you can imagine we might                                 to wait our training samples so what                                 should come out at any given point based                                 on some notion of relevance to say these                                 these are actually this is very                                 important for this document these are                                 moderately important and that sort of                                 thing so that's another possibility I                                 think that could be exciting the next                                 level is this idea of thought vectors                                 that you might hear about and this is                                 something called skip thought vectors                                 which is a kind of sentence to Veck and                                 this is interesting because queries                                 often come in this form of this stuff on                                 the right there are a little there a                                 couple terms they're not huge and you                                 can even think about this as possibly a                                 way to look before and after what the                                 user is typing in a query box we type we                                 send in its fleece was white as snow and                                 the idea with sentence to Veck or you                                 seeked these vectors with a sentence                                 before in sentence after what this                                 becomes is sort of a embedding for the                                 sentence it's sort of encoding the                                 semantic meeting before and after and I                                 think about this as like can you look                                 across queries across user sessions and                                 do a similar kind of thing could we take                                 one user's query that occurred before or                                 after and sort of see some kind of                                 relationship there or relationship                                 between the document and those queries                                 and I don't know I think there's a lot                                 of this is about the frontier right I                                 encourage all of you to to work on this                                 so I'm not sure we're actually have                                 we've got a try ok go that's what I mean                                 it and we're gonna Tomas is gonna do a                                 quick demo we should have scheduled four                                 hours for this talk so as I said before                                 I think the core idea here is that we we                                 are not sure yet if that's going to be a                                 silver bullet so the demo is about                                 bringing some kind of a scientific view                                 on this thing with experiments and the                                 other thing is about showing things out                                 the things work out in                                 life with real data so I've basically                                 used stuff from this project which is                                 called listen for IR which from from a                                 university is basically losing a set of                                 tools to to experiment with do                                 experiments with listen and but there                                 are more than this out there that is                                 also this tool called an cerini which is                                 basically as the same the same idea so                                 I've used this the data sent from the                                 association of computer machinery which                                 is a small one it is just three thousand                                 knocks and basically I've made running                                 all these ducts indexing and searching                                 using different ranking models so                                 basically using the m                                                   other similarity classes that are                                 available in loosen together with word                                 embeddings and particle factors and                                 since we don't have time I will directly                                 go to the results and it's all up on                                 github so this show Suites up very well                                 but basically what we have is that there                                 is skip the second line for a second                                 it's the class they're basically the end                                 ECG column is the second column is                                 basically between basically the higher                                 is the is is the better and we have that                                 classic similar similarity which is                                 tf-idf basically performed better than                                 anyone else                                 so it's surprised it's not a silver                                 bullet beyond                                                          from randomness which is another model                                 it's kind of low as well and paragraph                                 vector and were were vectors ranking                                 models                                 are slightly below classic similar ideas                                 so it doesn't work but it's not really                                 the case so we have                                                    really not enough for I mean in this is                                 kind of true for most of machine                                 learning we need quite some some stuff                                 quite some data and so this first result                                 was was generated retrieving the first                                                                                                          for each query and for such a small data                                 set this was probably not not the right                                 choice so let's see what happens when we                                 have                                                                     and we top                                                               the classic is slightly over paragraph                                 factors but not that much anymore and                                 surprisingly enough this LTS thing is a                                 small regression model that use a neural                                 network to basically use statistics                                 about about terms to learn to score in                                 an unsupervised way and actually that                                 was the turn out is to perform the                                 better but as I said at a certain point                                 the best comes with mixing the old                                 knowledge so to say with the new                                 knowledge and so mixing the paragraph                                 vectors ranking together with language                                 modeling delayed similarity performed                                 best slightly below what vector with BM                                                                                                         encourage you to do when it comes to                                 your search projects so really try out                                 different things think about the data                                 you have and what kind of things might                                 sense or might not sense to use so in                                 this in this case probably three                                 thousand dots is not enough for                                 paragraph vectors and word back                                 vectors based rankings to outperform                                 along the basic models so I try to make                                 it as quick as possible so if the other                                 thing is we've prepared a small demo                                 using flink                                 to ingest tweets about those tags                                 they're burning bots were burning boxes                                 mm hatre seen deep learning whatever and                                 store them into continued continuously                                 index them and run a pre fixed query                                 against them to see how the relevance                                 changes over time when and what kind of                                 results we have and with my ugly jQuery                                 capabilities I kind of made this ongoing                                 CSV basically so you can see that so the                                 query run is a something lappa about                                 neural search deep learning and at first                                 we have that PM                                                        the as the top tweet and class classic                                 which is tf-idf deep learning and fonts                                 and document rankings returned the same                                 as beyond                                                                we have that the UH                                                   learning and document embeddings Michael                                 Jordan not the one for the basketball                                 player words us that deep learn is going                                 to be lead to bad medical whatever so                                 it's and this is the kind of things that                                 is important to do in real life so as                                 the data changes see how the relevance                                 changes and what kind of documents you                                 get if they are relevant or not and if                                 you look again language model Dirac led                                 basically returns the first top results                                 that it used to return at the first time                                 so it doesn't adapt it seems it is less                                 flexible than the document embedding                                 ranking to the to the data that changes                                 in the index so that's something that                                 for example let's                                 that's one quick thing to learn from                                 this quick experiment and since we have                                 no more time or kind of no more time I'm                                 going to skip the other parts yeah so                                 two gotta go to the end yes and I think                                 we'll get clapped off soon so basically                                 I think some takeaways for us for this                                 are you know mixing there's no silver                                 bullet you know there's no magic                                 cognitive search unicorns that are gonna                                 come and save your everyone's jobs or                                 take everyone's jobs yet one thing that                                 I think is important to take away is                                 really showing tokenizing text and any                                 thing can really be in an embedding                                 space deep learning is good at                                 representations of all of these things                                 and vectors and I talked about queries                                 and documents queries can mean users we                                 can talk about images all these things                                 can be vector and in relevant search we                                 say everything can be tokenized and I                                 think those things very much dovetail                                 and solar in the S community need to get                                 better at first-class vector support                                 raise your hand if you know about Vespa                                 so some people ok Vespa does this very                                 well and so learning s sort of it's kind                                 of painful to get that work so in the                                 India API you can see there is it's very                                 hard to see how can I fit a vector into                                 that API so I mean that's something that                                 probably will will or will not evolve                                 over time but it's something to think                                 about so how to make these changes also                                 in the software we use is this is this                                 whatever you think we've done is in                                  supervised learning is this gonna be the                                  future and we haven't really how much do                                  you do with unsupervised learning versus                                  learning Trank where there's cliques and                                  you have a lot of labeled data can we do                                  better with unsupervised learning I mean                                  all of these things are questioned and                                  finally I there's this slack community                                  that we have we talk about search                                  relevance issues I really encourage                                  everyone to join we talk about plugins                                  there's even a jobs board there's                                  conferences we of course hope that                                  everyone knows about haystack our                                  conference that what the community did                                  book authors are there to talk about                                  directly at these issues so please join                                  us there if you                                  questions so okay thank you uh sorry                                  we're going to Rupp you                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=33UR9uQlCpE


