Title: Berlin Buzzwords 2018: Holden Karau – Working with Tensorflow from the JVM #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Holden Karau talking about "Working with Tensorflow from the JVM: How Big Data and Deep Learning can be BFFs".

Tensorflow is all kind of fancy, from helping startups raising their Series A in Silicon Valley to detecting if something is a cat. However, when things start to get "real" you may find yourself no longer dealing with mnist.csv, and instead needing do large scale data prep as well as training. 

This talk will explore how Tensorflow can be used in conjunction with Apache Spark, Flink, and BEAM to create a full machine learning pipeline including that annoying "feature engineering" and "data prep" components that we like to pretend don’t exist. We’ll also talk about how these feature prep stages need to be integrated into the serving layer.

This talk will also explore how Apache Arrow impacts cross-language development for big-data including things like deep learning. Even if you’re not trying to raise a round of funding in Silicon Valley, this talk will give you tools to do interesting machine learning problems at scale (or find more cats).

Read more:
https://2018.berlinbuzzwords.de/18/session/working-tensorflow-jvm-how-big-data-and-deep-learning-can-be-bffs

About Holden Karau:
https://2018.berlinbuzzwords.de/users/holden-karau

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you                               I am also joined by my co-presenter boo                               she travels with me everywhere so yeah                               my name is Holton my preferred pronouns                               are she or her it's tattooed on my wrist                               in case you or I forget mornings can be                               a little rough sometimes I'm a developer                               advocate at Google it's nice they pay me                               money and they work on open-source                                software                                it keeps me happy and I'm on the spork                                PMC and I contribute to beam and a lot                                of other Apache projects as well                                previously I was at IBM Alpine dataworks                                Google Foursquare on Amazon I'm a                                co-author of two books the second one I                                realize that you can negotiate royalties                                with publishers so if you're looking to                                buy a book about SPARC and you don't                                really care what's inside definitely buy                                the second one you can also follow me on                                Twitter and if you happen to be curious                                about like how ASF code reviews go in                                really large projects                                I'm live-streaming some code reviews as                                well I don't know I think it's fun and                                then I actually set aside time to do                                code reviews which is always a challenge                                when you're busy in addition to I am                                professionally I'm trans queer Canadian                                and part of the leather community this                                isn't super important or directly                                related to deep learning but I think                                especially for those of us who are going                                to be working on machine learning                                related problems it's important to                                remember that we all come from a variety                                of backgrounds and we should work                                together to make sure that we build                                systems that work for everyone and we                                don't just build crap that reinforces                                our existing crappy systems and we can                                make more awesome things that being said                                I give you know techniques to do that so                                good luck ok yep this is boo she also                                uses she/her pronouns and she is the                                author of learning to bark and                                high-performance barking you can also                                follow her on Twitter she does not                                currently have a twitch but maybe maybe                                one day soon and why does my employer                                care and this is in response to                                something that I was asked this morning                                by someone who was like so I get that                                you work on open source but why does                                Google care about these systems don't                                they have like awesome other internal                                systems                                and yet we have a lot of really awesome                                internal code but we also think it's                                really important to support all of the                                wonderful tools in the Big Data                                ecosystem                                things like Apache beam and Apache spark                                we have on Google cloud with hosted                                solutions and even if it's not a thing                                where we have this hosted solution it's                                fun because you can just run it on                                Google cloud and then we make money or                                at least that's something along the                                lines of what I vaguely understood from                                our business plan for like five minutes                                of it that I like was paying attention                                and then I walked out because it was                                getting kind of confusing and so that's                                not official but just TLDR somehow this                                results in money for my employer don't                                worry about me                                I will continue to get paid okay so                                hopefully you're nice people I am really                                curious how many people here are spark                                users okay how many people are flink                                users because I'm in Berlin that is less                                than I was expecting maybe there is                                another flink talk happening at the same                                time okay how many people are beam users                                that is about what I expected that that                                was three but it's cool bhima's beam is                                awesome                                and it's gonna get more awesome so I'm                                gonna talk about big data outside of the                                JVM in general because tensorflow is                                sort of we can just think of it as a                                special use case of trying to do big                                data outside of the JVM and then we'll                                look at tensorflow on spark and then                                we'll also look at tensorflow on Beam                                and we'll talk about how Apache Aero                                can totally change these things to no                                 longer resemble a did you have the Ford                                 Pinto here is that no okay it's an                                 American car that was notable for                                 catching on fire and so will make this                                 better than our v                                                currently catches on fire okay                                 so PI spark actually how many people are                                 PI spark users in the house not that                                 many so I apologize to all of you PI                                 spark users it's getting better but PI                                 so spark is the Python interface to                                 spark it's the general technique used                                 for a lot of other language on top of                                 spark and it's also the same thing that                                 is used to power tensor flow on spark we                                 pretty much looked at the possibility of                                 writing                                 our own bindings and one that's hard oh                                 but we already know how to talk to                                 Python yay and the performance is bad                                 but that's that's okay so why is the                                 performance bad for all of these things                                 we use this thing called pickling which                                 is about as performant as that pickle                                 which is not very fast sockets to                                 communicate data and then because that                                 wasn't enough we decided we'd also use                                 UNIX pipes and then over top of that we                                 were like you know what this really                                 needs is another format for interchange                                 so we threw JSON on top of that yeah                                 that didn't go so well and it looks kind                                 of like this we essentially end up                                 copying or lambda expressions from                                 Python into the JVM and then on the                                 workers we end up receiving these lambda                                 expressions and our data and we take the                                 both the data and the lambda expressions                                 and send them through to Python and even                                 if you're gonna work in Scala and you're                                 gonna use tensorflow                                 oh they told me not to move yeah                                 whatever you're gonna miss like this                                 part's gonna go away so PI                                             be in your life and you'll be happy                                 about that but this part is still gonna                                 be in your life and I'm really sorry                                 camera person I just don't have a                                 pointer thing uh yeah and in flink it's                                 pretty much the same thing except with                                 slightly different formats so how does                                 this impact big data systems besides                                 SPARC it makes double serialization                                 costs make all of this very expensive                                 and slow and that's fine I'm a cloud                                 provider I sell you resources by the                                 hour if it takes twice as long to run I                                 make twice as much money                                 I think I'm not super sure on how our                                 business model works but that seems                                 probable the only downside is it turns                                 out that you might go like wow this is                                 really slow I'm just not gonna do this                                 and then that's sad because that's money                                 that I'm not making and it might also be                                 said for you because it's a problem that                                 you're not solving there's a bunch of                                 other things that are bad the error                                 messages make no sense but if you're                                 working with tensorflow you're already                                 experiencing that and the dependency                                 management makes limited amounts of                                 sense and this is amazing because we                                 take a system that is designed to                                 distribute Java packages and then we                                 make it                                 tribute native code and then that goes                                 about as well as that sounds which is                                 poorly and so I want to be clear a lot                                 of times when I talk about how PI spark                                 and flink work people are like wow those                                 both sound terrible                                 what should I use instead and I'm here                                 to tell you everything else sucks too                                 some of the systems even looked at that                                 and when you know what this needs                                 XML it did not need XML but that's fine                                 so don't worry everything sucks and this                                 same general approach will apply to the                                 other systems even the ones that use XML                                 so ok tensorflow one spark let's let's                                 do the hello world part and we're gonna                                 train em NIST yay and so there's there's                                 a tensor flow on spark package which we                                 can just use out of the box we have to                                 use it in Python though but under the                                 hood it's Python calling Java calling                                 wife calling Java again calling Python                                 calling C++ code and so we get this like                                 sort of turtles in the middle situation                                 it's not quite all the way down at the                                 bottom you always find C++ or Fortran                                 there to give you a helping hand and a                                 segfault and so this this works                                 surprisingly but you might not be very                                 excited about this because you're                                 probably Java users so one of the things                                 that we should do in addition to                                 exposing this from Java is make it not                                 bad the performance of this is really                                 terrible                                 and there's some cool things which have                                 happened which now make it possible for                                 us to make this performance really cool                                 and this cat is very happy about this                                 new performance paradigm and Apache                                 arrow will allow you to transfer data as                                 fast as this cat is switching universes                                 not a guarantee and the nice thing is it                                 supports SPARC and GPUs and R and Python                                 it supports arbitrary Java libraries so                                 if you're not a spark user you could                                 totally add support for this to                                 whichever a Java library that you're                                 working in and actually deal                                        support for arrow for example but you                                 know it's going to take a little bit of                                 work if you want to say add it to flank                                 and so there's this really nice                                 performance graph from someone                                 which implies this will be                                           faster that's a lie but it will be                                 faster probably and and at least for for                                 our purposes it will be we're probably                                 not even gonna get this three times                                 faster but it does it does make the                                 stuff go a little better and so why why                                 we're talking about this so we're                                 talking about this because we're in Java                                 and we want to get our data into                                 tensorflow and we want to do that in a                                 way which isn't incredibly slow and                                 Apache arrow is pretty much the only                                 option right now for that we could also                                 write files out to disks and TF record                                 and that that's slow disks are not fast                                 even SSDs and so instead we can use                                 arrow as this fast interchange library                                 and in the future maybe we could go                                 directly into tensorflow rather than                                 going Java arrow Python tensorflow but                                 that would be work and other projects                                 are using a to write all of our friends                                 are jumping off of the cliff so you                                 should join us in this party that logic                                 works and so not just spark and so if                                 you actually want to integrate other                                 things besides tensorflow you should                                 definitely check that arrow and see if                                 it fits your project and so to rewrite                                 our code in spark to use this we                                 essentially take our register function                                 and we just call pandas UDF instead and                                 we can see here we're doing very                                 advanced numeric computation we're                                 adding two integers together which                                 honestly Java a little touch-and-go                                 write a numeric computation in Java I                                 don't know and so with the fact that we                                 can add two integers together is a                                 really good sign of how we're gonna be                                 able to power tensor flow and strangely                                 enough that's what we're gonna do and so                                 we can we can do this to TF on spark the                                 the core of TF on spark is this thing                                 which takes for each partition and                                 starts tensorflow on each of the nodes                                 and feeds it the data that it needs from                                 your big data system and that's that's                                 cool and that's kind of fast it's a                                 little unreliable and we'll talk about                                 this and so we can make this train                                 function and we can rewrite it into this                                 UDF which returns zeros because for we                                 forgot to add support for returning                                 nulls but that's okay this this sad hack                                 for now is probably the one part of the                                 code which will survive                                 stun all of my experience and this just                                 ends up taking our input data as a                                 narrow thing which is converted into                                 pandas behind the scene and then we                                 convert it into tuples and then we feed                                 it to tensorflow                                 and if that sounds like a lot of                                 conversion it is but it it's a bit                                 faster and so this this design looks                                 like this and so you can just go ahead                                 and make this change in your own like                                 private Fork and then your magic em                                 mystical all the way back here will                                 suddenly get faster it will also get                                 slightly less reliable but that's a                                 trade-off everyone's willing to make                                 right right okay so the TLDR is the                                 SPARC scheduler has some issues when it                                 comes to scheduling deep learning jobs                                 in that we assume that we can just                                 restart any individual partition oh and                                 now the mood lighting takes effect as we                                 move into okay right and so we swap                                 pickles for arrow batch records and now                                 we have a little panda which is much                                 cuter than that circle and that panda                                 can go into tensorflow and that's fun                                 yay happy pandas okay so what could we                                 do how can we make this like actually                                 awesome so the first thing that we could                                 do is we could go back look at this                                 lambda where we're turning it into                                 tuples and go that sounds kind of                                 unnecessary and that's totally a thing                                 that we could get rid of but there are                                 reasons why doesn't work we could start                                 using memory-mapped arrow so right now                                 we still put arrow records over top of                                 unix sockets and so that was like a                                 great plan memory mapped could be better                                 since we're not actually sending a lot                                 of data back though who knows if this is                                 going to make a big difference and the                                 other one that we could do which is                                 really exciting and we're looking at in                                 spark as well is if we sorry that I keep                                 jumping back we can see that arrow can                                 read directly from park' and one of the                                 things we could do is if we know that                                 we're running essentially just a tensor                                 flow job one raw park' data we could                                 just cut the jvm out although that might                                 not be so exciting for the people in                                 here that like Java                                 but for those of you who don't we could                                 get rid of Java um and that's cool okay                                 so that's all fun and good but you're                                 here to access tensorflow from the JVM                                 so now we have to create multi-language                                 pipelines limited excitement okay                                 No so multi-language pipelines are                                 amazing because the alternative is that                                 I learn how to rewrite tensorflow in                                 Java and that does not sound like fun I                                 just want to use tensor flow from Java I                                 don't want to have to make it that is                                 way too much work and I am not paid by                                 the hour anymore and so we actually have                                 these things and we can kind of make                                 them work in spark and in the future                                 will maybe be able to make them work in                                 beam and right now in practice it's                                 really painful                                 but let's go look at the pain and see                                 how we can do it yay okay so sparkling                                 ml is the project where I've done this                                 you can go check it out it's on github                                 sparkling pandas and it supports other                                 things besides tensorflow                                 turns out no one gives a about the                                 other things besides tensorflow if                                 anyone's really into NLP it does some                                 cool NLP stuff but come find me later                                 NLP friends so okay how do we how do we                                 make this work we use our good friend                                 Piper J and we make a Java class for                                 representing the interface for what our                                 Python code is going to be and we allow                                 it to call into that with arbitrary                                 parameters and if you really want to                                 look at start-up about pie you can                                 definitely go look on the github but                                 this is the short version of it is we                                 put a bunch of functions in here and                                 then we say this is the class that we're                                 implementing and then we only allow Java                                 to call us Java will give us the spark                                 session information the name of the                                 function that it wants to call and a                                 bunch of parameters and we'll just                                 evaluate those parameters as an AST                                 literal what could go wrong many things                                 but provided that you have no malicious                                 users ever this is fine if you have                                 malicious users this is an excellent way                                 to execute arbitrary                                 code okay so the the Java boilerplate                                 looks like pretty much the the inverse                                 of this it's also pretty boring and it's                                 the the full details are in a few files                                 but now we can use it for NLP I promise                                 we'll use it for tensor flow shortly but                                 the NLP example is funner and simpler so                                 the first thing that we do how many                                 people are familiar with Spacey yay for                                 people's um this example is going to be                                 great for them for the rest of you oh                                 wait I'm in Europe you you experience                                 languages oh and I'm in Germany you                                 experience languages where space                                 tokenization is perhaps not ideal yes                                 sometimes and so perhaps using the                                 standard word count example that we all                                 see you get a bad count of the words and                                 so this is exciting we can rewrite our                                 word count example to use Spacey take a                                 really effective tokenization and we all                                 know this is big data so word count is                                 our use case so we we take in an input                                 series and this is essentially we can                                 take in pandas dataframes or we can take                                 in series when we don't have structured                                 data because I'm just getting a list of                                 strings we're just gonna take in a                                 series Spacey magic get is essentially a                                 whole bunch of which you really                                 don't want to look at that just                                 initializes Spacey on the workers and                                 make sure as everything is happy and it                                 allows for reuse                                 if you end up tokenizing a lot of data                                 and then inside we call our happy little                                 function and because I did this in                                 Python to seven because reasons we are                                 today I do it in Python                                                  it in something and it was painful so I                                 have to explicitly called Unicode I                                 remembering which virtual line of I have                                 active when I'm like writing code for a                                 slide as hard but this is this is pretty                                 cool and we can tokenize our text and                                 it'll go fast and on the JVM side we we                                 just call it like a regular spark ml                                 pipeline stage we specify our input                                 columns and our output columns and our                                 language which is English because it                                 turns out that I don't speak any other                                 languages besides English so I assure                                 you this probably works better for                                 German but                                 to be fair I don't know but it probably                                 does and that's good enough okay and so                                 here's here's this very fancy diagram we                                 can see oh okay seems like people are                                 really excited about the tensor flow                                 stuff and not so much NLP that's fine so                                 spark deep learning pipelines are yet                                 another way to do deep learning on top                                 of spark and they have some limitations                                 that you can read about if you're                                 particularly interested in them but we                                 can expose the spark deep learning                                 package from Python into Scala in pretty                                 much the same way how we do with                                 everything else in sparkling ml and so                                 we have to write this kind of not so                                 pretty bit because the the first                                 function that we were looking at back                                 here is a really simple function it it                                 takes in one column you know it's                                 totally fine I can write that as UDF                                 really simply but not everything can be                                 directly defined as UDF some things                                 actually need to take in a data frame                                 and this is especially true for deep                                 learning things where we want to look at                                 a bunch of things on the data frame at                                 the same time so we taken a data frame                                 we make all kinds of happiness the Scala                                 side looks pretty similar the model name                                 yeah woo okay                                 everyone's very excited by the Skylar                                 boilerplate code ferrites no ok the                                 front row is just like no I don't care                                 ok and so this is this is this this is                                 the part with the actual sadness where                                 we take our our Scala parameters and we                                 serialize them as strings to give to                                 Python this is bad but it's not that bad                                 ok yes and so whatever you can just if                                 there's more parameters that you want to                                 access in the model you can just add it                                 here and do that because I got lazy and                                 I added the minimum number of parameters                                 required for this to work but you can                                 come and add the parameters that you                                 care about so you can access them and                                 set them on your model and have happy                                 fun times so you can sort of set model                                 name you can have other                                 there too and in a magical possible                                 future our data will not have to flow                                 through Python first that magical                                 possible future requires a lot more code                                 than exists today though so this magical                                 possible future is like definitely a                                 patches welcome scenario so let's not                                 focus on that okay cool so that is how                                 to make sure work with spark and the                                 various Python based systems and                                 exposing the Python systems into the JVM                                 but there are other ways to do this too                                 we could use DL for J it also uses arrow                                 probably I'm like                                                      reading their code it looks like they do                                 but the DL for J code is kind of gnarly                                 so I'm not a hundred percent sure where                                 they use it this leads us really well                                 into our next point you you might find                                 yourself trying to train a deep learning                                 model and then finding yourself needing                                 to do this thing called feature prep how                                 many people will spend their time doing                                 feature prep there are less hands than I                                 expected how many times how many people                                 spend their times doing really cool ml                                  that is not feature prep there is                                 two hands interesting interesting I want                                 to talk to you about your jobs later but                                 so there's a really good chance that the                                 people who didn't raise their hand just                                 aren't using this yet and it turns out                                 that as as cool as this deal for J and                                 all of these fun systems are our data                                 has to be in a format that we can do our                                 cool deep learning on it and so we've                                 got two different options and probably                                 there are more that I just don't                                 remember                                 there are pre-built packages that are                                 designed to allow us to do our feature                                 prep in a way that it can be reused at                                 run to at serving time and another                                 option is we could just write piles of                                 custom code and then we could just keep                                 it in sync by hand from training time to                                 serving time and it definitely wouldn't                                 get out of sync and start returning the                                 wrong results and there's one person who                                 finds that very amusing I want to know                                 what results you were predicting anyways                                 so another possible future is we could                                 use Apache beam and those three people                                 in the audience                                 would be very excited and we could use                                 tf-x on top of beam on top of spark or                                 tf-x on top of beam on top of link and                                 then yeah happiness and so if we did                                 that we could get access to things like                                 TF transform and this allows us to                                 represent all of our like kinda gnarly                                 feature prep stuff and have it                                 automatically compiled in for serving                                 into our graph it's really fun it runs                                 on top of a patchy beam and it currently                                 doesn't work outside of Google cloud                                 platform um if you're a Google customer                                 it's great and that's lovely but I I                                 also want other people to be able to use                                 it and we're working on it but if you go                                 home and try and use this today you will                                 be sad but that being said let's look at                                 the code and so yeah we can scale to                                 zero one strings to ants all of the                                 Commons sort of feature prep stuff and                                 for example like scaling to                                           computing the mean I mean that requires                                 that we actually see all of the data                                 first right I can't just do this with                                 like a hash function and hope for the                                 best                                 and so TF transform runs this sort of                                 analyze pass on our data and it outputs                                 pure tensor flow constant tensors that                                 can then be used in your same serving                                 graph and you can use the fun happy you                                 know tensorflow serving ecosystem and                                 there's a whole bunch of things in there                                 for pretty much any use case that you                                 want and so let's let's focus on the                                 limitations of this so non JVM beam does                                 not work so well outside of Google in                                 its environment so if you want to make                                 something in production today you're                                 kind of stuck with one of the other                                 things that we've been talking about or                                 becoming a Google cloud customer which                                 is great you could we use gr PC and                                 protobuf instead of Aero performance is                                 more or less equivalent it's just not                                 invented here syndrome I guess would be                                 the not so polite description but it                                 predates Aero as well                                 and there's exciting new work if there                                 are Python                                                            don't support Python                                                    you can come join me there is a sort of                                 tracking JIRA where we have lots of fun                                 things it kind of there's a hacked up                                 prototype that you can look at and you                                 can even run go on top of it the go part                                 is completely unrelated to tensorflow                                 but kind of fun except in that it allows                                 us to run native code and therefore we                                 could theoretically run tensorflow                                 stuff on top of it that way - are there                                 any go users whoa come talk to me about                                 running go on big data with beam if                                 you're so inclined and you can run it on                                 top of Google Cloud today or sort of                                 Apache flink kind of my very much kind                                 of and in the future maybe also spark                                 which would be really convenient because                                 I don't want to have to learn a lot of                                 aflink I'm pretty lazy and as a San                                 Francisco kid you know I learned spark                                 first so yeah ok cool so how does how                                 does this stuff relate to tensor flow so                                 tensor flow is in Python kind of and if                                 we support multi-language pipelines and                                 beam we can do the same tricks that we                                 did inside of spark to make this stuff                                 work and then we can actually be                                 portable and we can use cool libraries                                 like TF transform so we don't have to                                 write like giant piles of custom code                                 and write custom model exporting code it                                 doesn't work today or tomorrow but                                 eventually here's a whole bunch of                                 resources if anyone's interested in like                                 playing with these things                                 sparkling ml is not production ready                                 either it's like a project that I work                                 on with some friends who occasionally                                 show up but it probably works and if it                                 doesn't you could fix it and I mean it                                 passes its test suite which is pretty                                 promising - we're gonna get to my most                                 important slide which is                                 high-performance spark it is completely                                 unrelated to this talk but I do get the                                 highest royalties on                                 so I strongly encourage those of you                                 with a corporate expense account to                                 purchase this book and not return it                                 cats love it and if you buy the printed                                 copy your cat will love the book                                 although definitely buy the e-book copy                                 as well because I get double royalties                                 on the e-book hmm okay I will be talking                                 about dealing with contributor overload                                 later on this week if you want an excuse                                 to go to New York I'll be there later on                                 this month and you can come join me at                                 any of these other fun events and                                 otherwise I think happy question time if                                 people have questions or okay I know he                                 has a question are you not gonna ask me                                 that question I it's up to you tonight                                 perfect hi so you were talking a lot of                                 all these confirmations and all this                                 software and so on but what about the                                 last resort in terms of performance                                 since here we need to go from this                                 platform to this framework and so on                                 what about my last result of it I mean                                 is it going to take more time to                                 actually get what it wants or yeah I                                 mean every time we copy data is                                 expensive                                 that being said is so this theory let's                                 go all the way back                                 magic slide magic slide come on there's                                 a lot of slides okay yeah the theory                                 with with Apache arrow is that                                 well we'll be jumping between all of                                 these systems if our data remains in the                                 same format it's not so expensive for                                 today right now we still end up copying                                 that from the memory of these different                                 systems but we could use shared memory                                 buffers it's just that every attempt of                                 that so far is mostly resulted in a lot                                 of exceptions and a few sec faults but                                 it's not inherently not going to work as                                 a shared memory buffer and once it                                 starts being passed around as a shared                                 memory buffer the cost of switching                                 between these systems will guide a lot                                 less expensive                                 does that answer your question yeah the                                 present is bad the future is better                                 maybe not a guarantee oh yeah okay I had                                 a question about your runs for bean so                                 you have Python go Java and God knows                                 what come what's coming next and you                                 have fling spark epics blah blah blah                                 yeah and so the different permutations                                 on the combinations so spa Python SDK                                 with flink runner and go SDK with spark                                 Runner how do you regression test all of                                 these in a release it's kind of seems                                 like pain yeah okay I mean it's it's                                 really slow and annoying to test a whole                                 bunch of different things but it's okay                                 cloud computers magic we can just run a                                 whole bunch of computers and test the                                 matrix so the next the next question is                                 about the beam in the flink run a lot of                                 time to just start up the workflow and                                 you know for the pipeline to start                                 executing yeah I know it's experimental                                 it's so what is happening in between the                                 distinct like five to six minutes just                                 to load up on my laptop or the pipeline                                 to start up on my laptop                                 that's a great question um so it does a                                 whole bunch of things it starts a bunch                                 of containers which takes just a little                                 bit of time and then once it started the                                 containers it also starts flink waits                                 for flink to finish starting and then                                 start some more containers and waits for                                 those containers to finish starting as                                 well and so pretty much it's just                                 booting a whole bunch of like mini VMs                                 and oh and the other part is there there                                 was like a race condition and so we                                 fixed that in the traditional way which                                 is just waiting a long time and hoping                                 it doesn't happen so yeah my question                                 knows more of it the Python SDK and the                                 ping fling Python is to hear or know oh                                 yeah so we for the for the Python SDK                                 though like we start a different                                 container with extra happy things inside                                 of it                                 and the extra happy things take a while                                 to start because we so with the with the                                 Java direct runner right now it's not                                 actually starting a whole bunch of                                 different containers and doing the our                                 pcs over them it's just like I'm in Java                                 I'm awesome and so the overheads a lot                                 lower to start a new pipeline for now                                 the last question sorry let's see if I                                 have a beam with a fling runner and in                                 fling if I was just writing my job in                                 Flint I can paralyze each operator and                                 you know the number of parallel I can                                 set the parallelism for each operator I                                 kind of find that missing in beam and                                 you know trying to translate that from                                 beam to fling how does it all work at is                                 it still working yeah okay so that's                                 that's not super well exposed in the API                                 yeah I think that is an area of active                                 work would be V but it's not like people                                 aren't aware of this sort of like                                 challenge with the the decreased                                 knowledge of what's happening inside the                                 box and it's trying to find ways to                                 expose enough information about the the                                 runners without making it so that this                                 pipeline that you've written which is in                                 theory like portable across all of these                                 different runners becomes locked into                                 that one runner right we don't want that                                 to happen and so it's it's complicated                                 to do well so it takes time cool are                                 there any non beam questions yes I have                                 a question do any of these problems that                                 you present it go away if we don't need                                 to train any models with tons of flow                                 but just want to use model and just just                                 use it for making predictions I mean if                                 you just want to serve your tensor flow                                 models do you want to serve your tensor                                 flow models with like Big Data mm-hmm                                 or do you want to just serve it                                 streaming streaming oh yeah I mean the                                 first is don't touch the JVM just                                 forget all of this and just use                                 like the happy TF serving library so we                                 have some legacy kudos oh well then that                                 depends on your legacy code I guess is                                 the short answer I would have to take a                                 look at it and I really don't want to                                 any other question yes thank you again                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=zFwQm58EIpo


