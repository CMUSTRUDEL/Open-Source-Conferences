Title: Berlin Buzzwords 2018: Jim Dowling – TensorFlow: Hotdog or Not with Toppings #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Jim Dowling talking about "TensorFlow: Hotdog or Not with Toppings (Training/Inferencing/Security/Distribution)"

Have you seen the Silicon Valley episode where they trained a neural network on a mobile phone to recognize whether an object is a Hotdog or not a Hotdog? We're going to build that app, and more, in just a few hours - we will do everything from training a neural network to having an app that can take photos, send it to a deployed server to test it against your trained model (model inferencing). All this will be done with end-to-end TLS security. 

This workshop is aimed at data scientists and data engineers who are eager to learn about deep learning with Big Data. We will write, train, and deploy a convolutional neural network in Tensorflow/Keras on the Hops platform as a Jupyter Notebook. We will show you how you can scale out training to reduce training time. 

We will also show you how to embed Deep Neural Networks (DNNs) in production pipelines for both training and inference using Apache Spark. We will base our tutorial on exercises given to students in Sweden’s first graduate course on Deep Learning – ID2223 at KTH.

Read more:
https://2018.berlinbuzzwords.de/18/session/tensorflow-hotdog-or-not-toppings-traininginferencingsecuritydistribution

About Jim Dowling:
https://2018.berlinbuzzwords.de/users/jim-dowling

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay thanks this is a hands-on workshop                               so I've been giving out some stickers if                               anyone doesn't have any there's some                               more on the table over there there's a                               URL on the back of them so we're gonna                               go through this particular app we're                               actually going to do an end to end                               workflow from training a model to                               serving it to having a client make calls                                on the model okay so I'm with logical                                clocks and I'm gonna talk about our                                platform that supports tensorflow                                I'm gonna do an example which is hot dog                                or not and this is the URL that's on the                                back of your stickers so if you want to                                know what you're talking you can just                                register an email address it doesn't                                even have to be valid email address if                                you don't want to there's some people in                                Stockholm we should validate these while                                I'm talking here and then you should be                                able to so you will need to have in your                                password eight an uppercase letter so                                capitals and and a number so just                                remember that and you do have to                                unfortunately put in a what's your                                favorite pet or your mother's maiden                                name all right just a couple of words                                about our platform our platform is                                called hops it's the world's fastest                                Hadoop platform it's the only Hadoop                                distribution that supports GPUs in the                                cloud and we did some work with Spotify                                in Oracle where we got sixteen times a                                throughput of a dupe but I'm not going                                to talk about that so I'm going to talk                                we're gonna work with this platform this                                is the hops data platform we call it                                hops works and what it is is it's a user                                interface and you have access to all of                                these different open source services                                behind that user interface and the user                                interface also is a REST API so if you                                want to use the platform                                programmatically you can do it this                                little shield thing up there means we're                                basically built on TLS certificates not                                on Kerberos so you can integrate with                                this externally with your IOT apps or                                anything else outside of this and the                                it'll integrate still with Active                                Directory if you're if you're running in                                a Kerberos kind of environment so for                                the who here is a data scientist who                                actually writes Python                                all right I normally talk to data                                scientists ok so so this is M slightly                                off track so all of you will probably                                know spark and you're familiar with                                flink and things like that which we have                                in the platform but when you get into                                data science you typically will use a                                lot of Python right so python has become                                pretty much the de-facto language for                                data science we have or but if you're                                going to do deep learning and today                                we're going to look at deep learning                                Python is is the the name of the game                                and if you're going to do deep learning                                on Python there's a number of frameworks                                and you can see there's a big TF there                                for tensorflow that's pretty much the                                dominant platform                                you also have Kerris which is an API on                                top of tensorflow it's extremely popular                                and then this little droplet there                                represents something called PI torch                                which is getting quite a lot of                                attraction in the research community but                                what our platform basically does is it                                it's a it's you can think of it as being                                like a Hadoop distribution it'll run on                                the cloud or on pram but you can rise in                                Jupiter that's what we're going to do                                today                                we're gonna write a notebook in Jupiter                                and we're gonna run us and it'll run on                                top on yarn actually helps yarn and you                                can also write spark applications use                                Kafka and then we use a lot of other                                services like influx graph Anna elastic                                and Cabana for from monitoring                                applications and giving feedback but it                                is a kind of Hadoop distribution so you                                still have hive one feature that's                                currently under development and it's not                                in the version we look at today is we're                                using kubernetes for scalar model                                serving so we will do model serving but                                 it won't be elastic today                                 okay so who's heard of coop flow no one                                 - - ok so at this point with data                                 scientists they'll say well you should                                 be doing coop though what's the story                                 so cube flow is a recent framework that                                 Google released for doing machine                                 learning on kubernetes and if your data                                 scientists you have to write your mo                                 file defining your structure your                                 infrastructure so you'll say I need                                 these containers I'd like a container                                 with this label that has GPU and and so                                 on and with your infrastructure you need                                 to then define a docker file because you                                 want to install libraries                                 when a Python libraries in there and you                                 you basically write a docker file and                                 then finally you have a tubular notebook                                 enough you can go into your machine                                 learning so hops is an alternative model                                 to this what we're trying to do is make                                 it easier to use some couvreux and I'll                                 show you how in a minute so what we                                 introduced in hops is we introduce some                                 abstractions that we we think it helps                                 make it easier to do machine learning so                                 the main one is something called a                                 project so everyone is familiar with                                 github so you know good hope you have a                                 project or a repository and in your repo                                 you can add members and you can give                                 them roles inside that repo so we have                                 the same abstraction and we think this                                 is the right way for machine learning                                 people to work you have data engineers                                 and data scientists in the same projects                                 and they take hand of machine learning                                 pipelines from end to end so data                                 scientists aren't expected to do                                 everything by themselves and data                                 engineers aren't expected to do all the                                 data science they work together as a                                 team now the project abstraction is is                                 security by design so we have                                 certificates behind us                                 it's very self-service it's designed for                                 ease of use so you'll see that                                 everything is basically point-and-click                                 for data scientists and at the backend                                 it's scale out deep learning so because                                 we have GPUs and our cluster here you                                 can run the jobs that we're going to                                 look at today on one GPU on ten GPUs on                                                                                                   question is well why would I care about                                 many GPUs well we want to do parallel                                 experiments if you're I'll talk about                                 this in a bit but parallel experiments                                 is when you start doing some data                                 science and you want to maybe build a                                 model architecture for your deep                                 learning system you don't just write the                                 code once and run it I mean nobody has                                 ever written code once and compiled it                                 and it worked and was fine uite really                                 improve it so you may change the hyper                                 parameters you may run some small                                 experiments to see if this converges                                 better and these parallel experiments                                 you can actually run many of them in                                 parallel there's no reason why if you                                 have one GPU you have to sit there and                                 wait for each experiment to finish                                 before you start a new one if you have                                 like                                     you just run                                                             second reason why scaler deep learning                                 is important is because if you have a                                 large data set and your training it can                                 be very compute intensive and can take a                                 long time so there's well-known data                                 sets like image net with a million                                 images and we work with some vehicle                                 manufacturers and they have millions of                                 images and if you train them in a single                                 GPU you're looking at weeks and that's                                 not a good use of your data scientist                                 time if you're paying them you know                                 relatively high salaries so distributed                                 training will enable you to train your                                 models much faster on larger volumes of                                 data ok the next reason I I think it's                                 interesting is that we have this is why                                 it's easier to use hops is that we have                                 persistent Python in the cluster so                                 remember we have a github project this                                 project abstraction and inside your                                 project you have your own counter                                 environment and that counter environment                                 will live on every machine in the                                 cluster so when you want to install                                 something into your project a Python                                 library you're installing it just for                                 your project not for anyone elses so                                 each project can have its own version of                                 Python it can have its own version of                                 every libraries and we think it's pretty                                 easy for data sciences to use this and                                 why all this can come together and work                                 well in an enterprise environment well                                 we don't use Kerberos we use certificate                                 certificates are the the main mechanism                                 we have to enable multi-tenancy in this                                 platform to have many different projects                                 coexist on the same platform and still                                 be protected from one another so we have                                 for when you're a member of a project                                 you'll have a certificate created for                                 every member of a project we call those                                 per project user certificates so if you                                 think about how you have two projects                                 like project a and project B I can't                                 copy data from project a to project B                                 because I have a different user identity                                 in project a than they have in project B                                 so there's a different certificate for                                 each user and a user a may have you know                                 access rights for some data set and user                                 B doesn't so user B can't read from that                                 data set                                 so it's not enough just to have                                 certificates identifying an authorizing                                 users you also need to have certificates                                 for the services things like the name                                 node resource manager caf-co brokers                                 hive server etc another thing we have                                 that's this is if you're familiar with                                 Google this is this is basically the                                 same as Google's internal security                                 architecture for Google acquired every                                 application you run will generate a new                                 certificate for that and the advantage                                 of that is then we can track what                                 particular files users have read and                                 written and we can save that and make                                 that available for auditing ok so then                                 the the platform is completely                                 open-source but we are a company and we                                 do have a you know a enterprise version                                 that supports certificate revocation                                 renewal and reloading                                 okay so GPU in yarn so we have support                                 for GPS and yarn is in the explicit                                 resource you can basically say give me                                 ten GPUs on this host four GPUs on this                                 host you can use node labeling and yarn                                 to say give me GPUs only on hosts that                                 have InfiniBand support and the actual                                 GPUs we only support in video for now so                                 those of you who are familiar with GPUs                                 know that Nvidia are the dominant player                                 and deep learning right now so it's CUDA                                 but you can have a mix of different                                 types of NVIDIA GPUs so you may buy the                                 commodity                                                             the P                                                                 all of them in the in the same cluster                                 and it works okay obviously if you're                                 gonna train you would and you you don't                                 want to have a mix if you're if you have                                 half of your GPUs are very high-end ones                                 and half are low-end that may affect                                 your training time because the slow ones                                 may slow down the the faster ones so                                 when you're training you should use                                 things like no labeling to make sure                                 that you're training them on the GPUs                                 you want to you to train them up ok I                                 can actually at this point I think I'm                                 talking quite a lot so have people                                 manage to log in create accounts and                                 logged in one or two shaking their head                                 I will I will it's on these stickers                                 that we have are                                 around here but it's the URL is hops io                                 /tf hops don't I Oh /tf so if you type                                 that in so you can register any email                                 address just remember you just try a new                                 email address with a new password and                                 just remember it and then try and use                                 that to log in and abettor minus-- okay                                 so i'm gonna go over this is where you                                 get to when you do that this is kind of                                 the basic instructions for the the                                 workshop so if you click on this hops                                 here link you come to actual hops                                 if you're curious tis basically running                                 on it on a large VM and Google Google                                 Cloud and so if I go back I just opened                                 up the other one actually house it IOT F                                 okay so what what what I'm gonna go                                 through today I'm going to go through a                                 bit more background just before we get                                 started but when I'm gonna go through                                 today is we're going to do a tensorflow                                 tour and end-to-end machine learning                                 workload and you're basically going to                                 create a project called tensorflow demo                                 and in that project you can train an                                 export a model it's quite hard to read                                 that text from down here so we're gonna                                 train an export a model this will                                 basically use that the hello world of                                 deep learning columnist many of you will                                 be bored with this but many of you may                                 be new for you as well so when that                                 models trained it will be a file a                                 protocol buffer file and that file will                                 live in our distributed file system HDFS                                 and then we'll start a tensor flow model                                 server number B and from that we can                                 then load the the protocol buffers file                                 and serve it and if you're familiar with                                 thanks for serving you'll know that this                                 will be using G RPC as an API and then                                 we can what we need to do is basically                                 copy the IP and port from our serve                                 model and paste that into another                                 notebook called a serving client and                                 with that it we can then run the client                                 and it will it will then send G or PC                                 requests to the model server and it will                                 classify the images that it sends and it                                 will get back responses so if you're not                                 familiar with                                 I'll tell you a                                                          it em nest is hello world for deep                                 learning it's it's a data set with tens                                 of thousands of hand-drawn numbers you                                 have the numbers from zero one two three                                 all the way up to nine there's ten of                                 them so what you do is you train the the                                 images that come in are very small                                 they're                                                             grayscale                                 so you're basically turning them into a                                 big vector it's                                                       long and you basically feed that into                                 the neural network so neural network can                                 be a convolutional or a network of                                 feed-forward it's quite easy to train a                                 good prediction model on em nest and                                 once you've trained us you'll get out                                 the network itself you can save it as a                                 protocol buffers file so once you've got                                 this trained model I can take any                                 different                                                             number                                                                 to the train model and say tell me what                                 number this is is this a                                                it nays okay I'll tell you what I'll do                                 I'll give you a quick look at this                                 before we actually because you're gonna                                 hot dog or not let's have a quick look                                 at it before we we go in and start doing                                 the other practical work so this is the                                 hot dog or not notebook and what it                                 basically does is I took a data set that                                 I found on a kegel competition and I                                 took a notebook by this guy Magnus                                 Peterson he does really good tutorials                                 much better than the official tensor                                 flow tutorials and his and this is                                 actually this is a I would say it's a                                 sophisticated notebook at some level                                 what it does is it takes the raw images                                 now we've already cropped the images to                                                                                                         notebook for that but with those raw                                 images we're going to turn them into                                 what we call tensor flow records has                                 anyone heard of tens for records yeah                                 couple of people so tempo records are                                 the native file format for training                                 tensor flow models and the reason why                                 you want to use tensor flow records is                                 because if you have GPUs and you're and                                 we decide well I'm going to read the                                 files I'm going to decode the images I'm                                 now going to convert                                 them into my you know twenty by twenty                                 pixel or twenty by twenty array and                                 there's three color child's here so it's                                                                                                  compute-intensive                                 and it will slow down training so your                                 GPUs will be waiting a lot of the time                                 for data to come in so you know if you                                 have ten GPUs on a server and you have                                                                                                          those images fast enough and fetch them                                 to feed those GPUs and keep them busy so                                 in a production pipeline you should be                                 doing your feature engineering and data                                 wrangling in something like spark if its                                 large data it can be just answer flow                                 but then if you save your output in                                 product in tensor flow records then you                                 can make sure that you use those to                                 train the models directly so this                                 notebook just a brief overview it it                                 basically it it we you can see that                                 we're gonna have to import the images                                 from a shared data set and that's doing                                 it there and there's some images and                                 then we're in this case defining it as a                                 data set so reason it days a tape you                                 know in in tensorflow and then we have                                 some images here it's not a huge data                                 set it's only a thousand images and we                                 can plot a few of those images you can                                 see here this is a hot dog true and this                                 is labeled these are all labeled true as                                 hot dogs actually and then we convert                                 them here into tens for records and once                                 we have our tents for records we can                                 then store them in HDFS and then finally                                 we'll define a parse function that our                                 data set API will use to feed tensorflow                                 and then we have another input function                                 which will pull the records out of that                                 and then finally we're training it down                                 here so okay first we have to define our                                 features but then we we do our training                                 down here we're doing it on a very                                 simple pre count the estimator this is                                 the estimator framework in tensorflow                                 if you're familiar with SPARC you you                                 may have seen the data flow or the SPARC                                 summit last week ten                                 data data brakes just released a                                 framework that looks exactly like this                                 an estimator framework for doing deep                                 learning from spark but this is the one                                 that's native to tensorflow okay so once                                 you've trained your model you can see                                 you can get your prediction accuracy I'm                                 on this conned estimator not getting                                 great values and then we have some                                 predictions and so on so you know if you                                 want to take a picture of a hot dog                                 while you're here and add it to the data                                 set feel free to do that you know it                                 should work but let's go back and see                                 that's where we're gonna get to by the                                 end of this at this point do we have                                 questions feel free to play around if                                 you're in there and start on the tour                                 and do a tenth of a little tour and go                                 ahead okay write a little bit of                                 background then a little bit more Theory                                 so why is why do why do we think                                 distributed tensorflow is interesting                                 why why do we want to actually use lots                                 of GPUs to you know make our models go                                 train faster or or do more                                 experimentation and parallel on them                                 well you can do it quite easily in in in                                 SPARC actually we use SPARC as the                                 framework to do data parallel training                                 and data parallel experiments so if you                                 imagine a set up for you like this one                                 where you have a hundred GPUs we have a                                 hundred GPUs all connected to our                                 distributed file system HDFS what we do                                 when were training deep learning models                                 is we do what's called an iteration an I                                 duration is when you take a batch in                                 this case will be files corresponding to                                 each image or tensorflow records which                                 we're actually going to use so let's say                                 we have a batch of to three thousand two                                 hundred tens of flow records well we                                 want                                                               thirty-two into the next and each GPU                                 will get                                                                then do what we call a forward pass in                                 in deep learning and then it would                                 calculate the changes in the weights in                                 our network we call those the gradients                                 and each GPU will do it for their                                    samples in this batch and they'll send                                 their gradients up somewhere you can                                 think of this abstractly because what                                 we'll see later on is the parameter                                 server that you think you should send                                 these to is actually not the scale the                                 model will be using in a few years we'll                                 be using a ring so in the traditional                                 parameter server model you would send                                 them all up to a server who would                                 aggregate all of these gradients compute                                 a new model so the changes and weights                                 from all of these                                                       together change the weights and send                                 this new updated model down to all the                                 GPUs that is one iteration now if you                                 imagine that you have                                                  have three million two hundred thousand                                 samples well then we're going to do an a                                 thousand of those iterations to cover                                 all of the                                                             you've done all taosenai durations we                                 call that an epoch and typically                                 training models requires tens of epochs                                 you know                                                                 this is this is a cycle that's gonna go                                 round and round and round now everyone                                 here I guess is familiar with SPARC at                                 some level so you can imagine pretty                                 much how this is going to work in spark                                 our GPUs will basically have an executor                                 each in them and we're gonna have to                                 have some way of kicking off the                                 training from the driver so this is kind                                 of how we do the parallel experiments in                                 hops in that style of architecture our                                 spark driver will start executors the                                 executors will have one GPU each they'll                                 have a full copy of the model on them                                 and then they'll communicate with each                                 other by reading and writing to the file                                 system so that's so you know if you're                                 doing distributed deep learning at any                                 sorts somehow the nodes need to                                 communicate with each other and Google                                 also aspires the use of a distributed                                 file system is the easiest way to do                                 that because you need to do things like                                 you need to collect the logs when you're                                 training if we're doing parallel                                 experiments each of these executors will                                 run a different full experiment it's not                                 the distributed training now they're                                 gonna say I'm gonna do I'm gonna run all                                 of the training for this particular                                 model with these hyper parameters on                                 this input data and they're going to                                 write the output to the file system and                                 all the executors will do that they'll                                 write all their I puts to the file                                 system what we'd like to be able to                                 visualize that at the end was tensor                                 board you know we'd like to be easily                                 able to get our model service to pull                                 out the Train models we'd like to be                                 able to checkpoint our models and have                                 easy access to them                                 so there's a lot of reasons for using a                                 distributed file system when you're                                 building these systems okay let's have a                                 look at the pipeline and we'll have a                                 look at it at an abstract pipeline and                                 then we'll look at some examples that                                 you can you can play with while we're                                 doing this                                 so the typical pipeline you'll have is                                 that data will have to come in from                                 somewhere now if you're Ober uber has                                 something called a feature store and the                                 feature store abstracts out whether it's                                 going from a Kafka topic or a hive table                                 or just CSV files in HDFS or even s                                     many others will have to actually read                                 from those sources directly and you know                                 you might decide well you spark for this                                 and have data frames representing the                                 different features that I want to                                 combine together to train my model but                                 you still have to do all the work                                 related to data wrangling so transform                                 your data in our case we want to decode                                 the images I want to take these hot dog                                 images and and their JPEGs we need to                                 decode them and turn them into vectors                                 we want to maybe extract features from                                 them so we need to know obviously the                                 labels of the images and the next phase                                 is quite a big box there it's                                 experimentation so we then will need to                                 play around with different networks and                                 say ok what would be a good Network for                                 this what will be good hyper parameters                                 and once you've decided what good hyper                                 parameters and good network is then you                                 can train the model and then finally                                 you'd like to test it and service now                                 each of these boxes has way more detail                                 than I can go into here things like                                 serving we've had a number of talks at                                 this conference very good ones as well                                 about some of the challenges of how to                                 you know handle serving but I can't go                                 into all the detail in all of them but                                 if you have specific questions we can we                                 can take them so as a programmer if you                                 want to write this pipeline and you're a                                 Python programmer or your Python team                                 because we're gonna have a team remember                                 who are going to be responsible for this                                 pipeline you can you can decide and use                                 tensorflow and python the whole way if                                 you want to if your data is not that big                                 it's completely feasible                                 underneath it what we provided services                                 to help you build these pipelines or                                 things like Kafka hive HDFS or we call                                 hops of s and therefore serving we have                                 early support for kubernetes what we                                 typically would espouse though would is                                 that people should use PI spark instead                                 of tensorflow because obviously as you                                 know pi spark can handle larger data                                 volumes and can see a light but                                 tensorflow then becomes just a library                                 that's run inside a larger PI spark job                                 that we'll see later on                                 so again you can run on the same                                 infrastructure so I'll do the first demo                                 and you can try and follow with me is                                 anybody not logged in and then you've                                 got problems do you want to tell me why                                 or create a new that's what you should                                 do is create a new email address no you                                 don't receive an email you just try and                                 log in sorry                                 you sorry you yeah hmm                                 okay tell you what to do you should if                                 you go back here there's this update                                 your Google spreadsheet just write your                                 email address that you created I have                                 some guys on slack and I'll ask them to                                 to just register the account so if you                                 just write in your email in this                                 spreadsheet down here anyone has any                                 problems go to the house today or /tf                                 okay                                 okay so they should take care of this so                                 just update this right in your and                                 they'll and they'll fix that hopefully                                 within a second okay I'm gonna log in                                 and what I'm gonna do is I'm going to I                                 think a log in here actually have two                                 accounts open so for those who you                                 haven't logged in so you just put in                                 your email and password there is support                                 for two factor authentication there is                                 of course HTTPS but in this case I just                                 disabled so if you haven't run this tour                                 you can just click on the Tor tips and                                 enable that it's quite annoying you'll                                 need to disable it later on because it                                 just keeps popping up and then click on                                 tensorflow                                 I've already done this so I'm just going                                 to go in and look at tensorflow                                 so this is what these are the projects                                 here on the right-hand side that I                                 mentioned before and if I want to create                                 a new project let's call this one                                 Stockholm account spelled Sacco okay so                                 creating a project is relatively quick                                 thing you can see it takes a couple of                                 seconds and that project so I if I take                                 my project here the demo tensorflow                                 if you feel friendly you can talk to                                 your neighbor and say well you know                                 maybe I'll add your my neighbor to my                                 project so you can go in here and I can                                 add in this case I can add myself to the                                 project and I can have myself as one of                                 two roles data scientist or data owner                                 so if you talk to your neighbor you can                                 say well I'll add you whatever and now                                 what I'm going to do is I'm going to do                                 this first part of the pipeline which is                                 data collection data transformation and                                 feature extraction I'll just show you                                 one one feature that I think is quite                                 nice which is something called Google                                 facets so what Google facets is is a                                 plugin to to Jupiter and it allows you                                 to take any data set that you can read                                 up in a panda's data frame and there's                                 kind of a practical limit of a better                                 gigabyte or two gigabytes for this and                                 it will then allow you to visualize                                 both in numerical and categorical                                 features you'll be able to see things                                 like what columns have missing volumes                                 used what are the average values for                                 columns and there's another one called                                 Google dive which allows you to actually                                 look at individual records or rows in                                 your in your data frame and you can then                                 look at different distributions so I'll                                 do a quick look at that just to show you                                 how it works let's go back here so I'm                                 going to start a Jupiter notebook for                                 this you can just press Start Jupiter                                 and it should just work if you're                                 curious you can play around with this a                                 bit I've set the limits of the amount of                                 memory to about five or something                                 gigabytes I don't remember number of                                 parallel executions I think it were                                 limited to two right now and this is                                 like you can add jars and things and so                                 on to your program if you want to but                                 I'm just going to run it so the                                 libraries that will be available to this                                 notebook that I've just started will be                                 will be visual you can find them here in                                 the Python this these are micro-services                                 on the left hand side so if I go into                                 the Python one I can see which libraries                                 are installed so these ones are                                 available so pandas is already there                                 facets is a is a plugin for Jupiter it's                                 already enabled and tensorflow is pre                                 installed you can see as is hops and                                 HDFS connector called PI dupe so I'm                                 going to go back to Jupiter so if you                                 get this far to Jupiter and if you want                                 to try out facets you just click on the                                 facets folder and you can try facets                                 overview or facets dive I will go with                                 overview and you can see it's going to                                 start a normal Python kernel so it's not                                 running PI SPARC yes this is just normal                                 Python and this will only work if you                                 have Google Chrome if you're running                                 Firefox it won't work I'm afraid it's a                                 Google product what can I say but it's                                 quite nice if you have Google Chrome so                                 I'll just explain what this code is                                 doing just so you can see it it's                                 basically taking a number of features                                 defined in a list here so we have these                                 are the features that defined in a data                                 set called the                                 census data from you from the US and                                 it's reading it from HDFS so it's going                                 to take the path to your project there's                                 a folder in there called test job data                                 census idle top data                                 it's the CSV file and it's going to read                                 that up into this pandas dataframe train                                 data and we'll do the same thing with                                 the pandas dataframe test data so if                                 you're curious about that data see this                                 adult data and adult test we can go back                                 to hops works and I'm going to go back                                 here to data sets and we had this thing                                 called test job here so I'll click on                                 that                                 and inside there I'm gonna follow this                                 path data census and then we have adult                                 test and adult data if you're curious                                 you can download it or you can preview                                 it and you can see some of the values                                 there let me show you small amount of                                 the values but I'm just gonna go back                                 and show you what we have in here and                                 facets so what we have here is we can                                 see what we call numeric features so if                                 you have a panda's data frame and it has                                 in this case I think it has                                           not correct if not mistaken                                                                                                                     are integers and                                                         but data scientists don't like the terms                                 integers and strings so they called in                                 numeric features they don't like the                                 term columns or the column features so                                 what we would call a column events they                                 were called a numeric feature and what                                 we would call a column of strings                                 they'll call a categorical feature now                                 if you're if you know Python who                                 programs Python here a bit just has a                                 few okay so if you're curious about do I                                 trust the values here you can see I was                                 just checking earlier on I said well                                 there's no missing columns here in the                                 numeric features but if we look in the                                 categorical features                                                  work class features of missing values                                 and countries of some missing values so                                 what I did earlier on was I basically I                                 did the following I just printed out                                 which columns I want to check which                                 columns have no                                 you can see that it says work class                                 occupation country have no values in                                 there so that seems to to coincide with                                 what we can see here in the in the                                 visualization now the point of this                                 visualization is if you've done machine                                 learning before and deep learning you'll                                 know that we have what's called training                                 data and test data or validation data                                 and you train on the training set and                                 when you're happy that you've trained                                 you then validate your training see how                                 accurate the model is you've trained on                                 this holdout data that you haven't used                                 before but one of the things you want to                                 do when you when you split your data                                 into training and test data is you want                                 to make sure that the data distribution                                 that you're learning is the same in both                                 the training data and the test data                                 because if they're different you'll                                 train on one data distribution and then                                 you're going to try and validate it on a                                 different distribution then you won't                                 get good results so this is a tool which                                 allows you to you can see here that we                                 can see some of the the mean values and                                 the max then the median and then you can                                 see some other visualizations of this                                 with histograms so you get a pretty                                 quick feeling of whether that's your                                 that you're happy with your data now if                                 we got a python dive it's it's quite                                 similar you can run it as well if you                                 have a Google Karl and it's gonna read                                 the same pandas dataframe but it's going                                 to read it into a into into fastest dive                                 so you can see it looks a bit different                                 if you click on any point here on the                                 right hand side you can see the it's                                 gonna show the row the contents of that                                 row so what this particular what's                                 happening right now is that it's                                 actually faceting the data by we can do                                 by this is education status I think so                                 you can you can you can basically change                                 how you would like to view the data you                                 can say well I'd like to view the data                                 by my education status or by age and it                                 will show you then the different                                 distributions of people so we can see                                 most people in this dataset are in these                                 particular age brackets and then you if                                 you think one of the data points is a                                 little bit off like this one at the top                                 we can get one up here we can see this                                 one has an age of                                                       details about them okay so this is what                                 we call feature engineering I'm going to                                 shut down this notebook I am the way you                                 should have down when you come out here                                 is you can just press this button here                                 which will shut down your jupiter                                 notebook for you I didn't actually close                                 it here sorry for that does anyone have                                 issues that aren't being handled right                                 now you can write them into the to the                                 into the spreadsheet                                 any questions on facets no these enough                                 right so let's move ahead yeah so if                                 you're a Python person I just wanted to                                 impress on you that the fact that your                                 data is in HDFS just means that you get                                 this one different line of code change                                 sorry we go back here this right so it's                                 not a big deal so the fact there many                                 Python frameworks like tensorflow pandas                                 PI spark they support HDFS natively                                 which we do too okay so the next part of                                 our pipeline and the pipeline's up on                                 the top right hand side if you want to                                 know where we are is we're still in the                                 data perhaps the the you know the data                                 wrangling phase and tensorflow since                                     i believe in core they introduced the TF                                 data api it's a new data set api and in                                 it you can do you can put in map                                 functions to you can pre-process your                                 data in pipelines in tensorflow the only                                 problem with it is that it's a bit of a                                 mess right so if you like nice clean                                 pythonic code this is not the code view                                 right if you want to write a for loop                                 you don't write a for loop and this is                                 one of the reasons why for example pi                                 torch is gaining adoption because it's                                 very pythonic but there's no need to                                 have to worry much about this this will                                 only run on a single server you need to                                 care about things like how many threads                                 are used for pre batching how many                                 parallel batches are used if you want                                 this thing to work efficiently and but                                 just our advice would be forget about it                                 use PI spark                                 and I put your data as TF records right                                 because this will be efficient that will                                 scale with the number of executors and                                 then when your data is how you you will                                 have a another job to orchestrate these                                 different parts of your pipeline in our                                 case we use spark to orchestrate two                                 different jobs but maybe different                                 people in different frameworks will use                                 other other tools so this is what the                                 code looks like in in pi spark for                                 taking for example images and resizing                                 them and cropping them that's what we                                 want to do with our hot dog images I've                                 done this already by the way you can see                                 the code is very clean right you're                                 basically reading images the path can be                                 in HDFS here's a really nice feature                                 this sample ratio so if you're in the                                 experimentation phrase and you have                                     gigabytes of data you don't necessarily                                 want to read up                                                       into a data frame you might want to read                                 up just you know                                                        your sample ratio is                                                 data frame you can now apply a                                 transformer this is using something                                 called FML spark from Microsoft it's an                                 open source library they have for                                 machine learning on spark and they have                                 a transformer for manipulating images                                 and images support is now in spark                                     and then Yahoo have a framework called                                 tensorflow on spark and they have some                                 libraries for taking data frames and                                 exporting them as TF records so that's                                 that's how you basically get to the                                 point where you're now ready to play                                 with your models and tensorflow this is                                 where I get back to the distribution                                 issue that when you're training a deep                                 learning model you think well it's a                                 loop a training loop but there that's                                 the inner loop right the outer loop is                                 when you actually need to look for                                 things like a good model architecture a                                 good set of hyper parameters and no                                 matter what way cuts that takes a lot of                                 time and a lot of experimentation and                                 you can we think you can cut that down                                 massively with distributor distribution                                 running them in parallel so we look at                                 that little way of doing that in in hops                                 if you're curious and want to follow                                 and do this you can are going to jump to                                 the code is this hyper parameter                                 optimization in hopes there's a notebook                                 for this so if we go back here and start                                 Jupiter this one is going to take a bit                                 more time                                 because you're going to do quite a lot                                 of training so if you go to tensorflow                                 and CNN and we have two of them here one                                 is called hyper parameter search on C                                                                                                          would will kind of skip that I look at                                 grid search on fashion M NIST so fashion                                 Ebony's came from from Berlin actually                                 it's a it's a data set to replace the                                 classic MS amnesty                                                                                                    different classes of clothing and it was                                 released by by these guys I think what's                                 the name of your company again                                 her brain's gone dead I slander yes                                 Lando released this data said thank you                                 you saved me okay so I'm gonna run this                                 but like so what happened when I run                                 this I didn't I didn't go into any                                 details you can pick a number of                                 executors that you want to run in                                 parallel if you have I'll just show you                                 what happens when you run it in this                                 case you just define a dict with the                                 combinations of learning rate and                                 dropout rate these are hyper parameters                                 in the deep learning model in this case                                 it'll run six different spark executors                                 each of them will run just one                                 experiment the first one will have a                                 learning rate of zero zero one the                                  second will have a dropout rate of                                      and then and so on                                  the second one will have a learn a                                  learning rate of zero zero five and the                                  dropout rate is zero five so when all                                  that's run we'll get the results in in a                                  in a nice tensor board that we can                                  visualize and it kind of looks like this                                  you'll see your learning rates here and                                  then you can look up here to find out                                  the best combination of learning rates                                  and but there's another thing that I                                  said don't press the button on because                                  this is very compute-intensive it's a                                  new area of my love not just research                                  but development which is                                  can the best computers find better                                  models than the best machine learning                                  experts and the answer is yes it's a                                  resounding yes so Google this year and                                  last year they released a two different                                  frameworks one on reinforcement learning                                  and another one on genetic algorithms                                  and they improved the state of the art                                  for training image classification on a                                  well known data set called image net so                                  they beat the best results gotten by                                  humans by having computer programs                                  reinforcement learning agents and                                  genetic algorithms search for a better                                  model architecture so you know a good                                  you know the people in the area talk                                  talk in the valley that you know a good                                  data scientists and get a million                                  dollars well if the good data scientist                                  is just picking good combinations of                                  hyper parameters and designing good                                  models in terms of which layers it                                  should have and how many it should have                                  if the computer can do that by basically                                  searching at using many GPUs in parallel                                  well I don't know which will be cheaper                                  in a few years make hay while the Sun                                  shines                                  is what we say okay so this this                                  particular example we we have genetic                                  algorithms and hops we have something                                  called evolutionary search this is it                                  will basically run for                                                 it's doing genetic algorithms and you                                  can specify mutation rates and crossover                                  rates and how many executors your                                  population size and number of executors                                  inspark you're going to have and then it                                  will search using different combinations                                  of those hyper parameters okay so this                                  is showing you just how it runs because                                  it takes a little bit the time so the                                  first run see far ten has ten different                                  image classes so if you do random if you                                  train around the model you'll get a                                      chance of being right so this is                                      chance of being right but after one                                  generation you can see that we get some                                  improvement and genetic algorithms will                                  pick out the best ones and it will                                  generate mutations from the best one so                                  within one more generation we get a                                  quite a large improvement in the model                                  accuracy and then you know we got even                                  more improvement so you know you're                                  gonna have at some point you're gonna                                  have diminishing returns on this but                                  it's a good way of it's an                                  promising new ways autumn Auto MLS                                  called of having computers design the                                  best models for your for your                                  architecture okay so that's the                                  experiment phase let's go back in and                                  see how this thing was going right so I                                  started this running earlier on didn't I                                  the grid search so you can see it's it's                                  that you still get a star going on here                                  is anyone running this no one okay                                  alright so I'll just show you when it's                                  running if you want to look at how its                                  proceeding here and you can just start                                  it running now and and have a look at it                                  what will happen is this button will                                  appear here it says a spark UI for your                                  session so if you press that button you                                  can actually get to spark UI we can look                                  at how many executors are running I                                  think I only had one executor but maybe                                  I had two you have only one executor and                                  the driver so it's only gonna run one                                  experiment at a time but you can have                                  six you know if you had six executors                                  then they'll run six in in parallel and                                  you can look at the tensor board while                                  it's training when this image when this                                  run finishes the tensor board will                                  disappear so it only appears for a                                  little bit and you can see that you know                                  it's it's getting quite good accuracy                                  it's up to                                                              at logs this is an issue that you have                                  in yarn so if you're working in a yarn                                  environment you'll know that logs are                                  only aggregated when when the job                                  completes which is no good so if you                                  click on the Cabana boat and you get                                  your logs in real time and you know if                                  you want you can even do graphing a                                  based on your logs and then we have some                                  metrics for the job in coming in and                                  graph Ana here and then there's also the                                  yarn you are if you're interested in                                  that for that so you can you can                                  basically we go back here you can                                  basically track the progress of your                                  jobs not just in here and the notebook                                  but also using this these set of you                                  eyes here and you can break them out                                  into new window if you want to                                  okay anyone any questions or points                                  we're not very interactive at this point                                  okay so what I suggest is who has run                                  something right who's run any tensile                                  notebook well and a few over here okay                                  so let's everyone try and who's in here                                  let's try and just do the first part                                  which was serving a model and then                                  having a client run on that train model                                  so yeah yeah                                  moon is my friend but I I would say                                  cause Apple is not so good                                  Jupiter the development effort that's                                  gone into making Jupiter better in the                                  big data space in the last two years has                                  been huge so we're using something                                  called spark Colonel here for Jupiter                                  which is quite nice and you obviously                                  with the Python kernels Zeppelin we                                  haven't and we and and we wrote a                                  connector for Jupiter to put the                                  notebooks in in HDFS so anybody logged                                  in from any server anywhere they'll see                                  the same view of all the notebooks                                  the problem with Zeppelin is that it                                  we've had stability issues with it and                                  isolation so it's it's designed to have                                  people be able to collaborate on the                                  same notebook but in practice we don't                                  find that it works that well so we have                                  where that we're limp we've limbs that                                  we have support or Zeppelin but it's                                  really we're encouraging people to use                                  things like hive and looking at sequel                                  in there rather than writing spark jobs                                  so we're moving more off to Jupiter okay                                  let's do the model serving thing because                                  I have one running all right I'll just                                  stop that one and we'll do a new one                                  okay so I'm going to start you there                                  notebook and we go back here so anybody                                  wants to follow along just go into your                                  demo tensor flow tours start your                                  Jupiter notebook and I was like killed                                  my grid search one there oops we go back                                  and kill this I'm gonna open it again                                  you can click this button to open it                                  again so if you click on the serving                                  this is in the instructions actually so                                  in the instructions that we had at the                                  beginning which were here we're gonna                                  follow these first instructions so you                                  can follow along here you're basically                                  gonna start a Jupiter notebook and we're                                  gonna run this serving train and export                                  model notebook so I'll go back here and                                  we go to serving and then going to open                                  this train and export model notebook so                                  if I run this this is not a spark                                  notebook this is a Python notebook you                                  can see that we're kind of moving                                  between spark and Python notebooks quite                                  quickly                                  I want getting an error because I think                                  the models already there so have to put                                  in a different directory so so I've                                  already trained this so let's go back                                  out so this will work fine for you I                                  need to I would need to delete that but                                  I'm just gonna skip the deleting part                                  leave and then once you've run that that                                  takes that goes very quickly I'll delete                                  this one here and then you key then                                  you'll come into this particular page                                  here the model serving page so you can                                  just click on this button here create                                  model serving and from here you can then                                  select the model that you trained so the                                  models will be in this data set called                                  models and the one we trained is go                                  amnesty we can click on that there we go                                  so I no need to still press the create                                  saving button and you can enable                                  batching for it to make it for improve                                  throughput at the cost of latency and                                  then just press run that's it and then                                  your notebook pottier tensorflow serving                                  server will start serving that model and                                  you can see the port and host IP address                                  that it starts running out has anyone                                  gotten anywhere here without we're                                  trying okay let's let's go back in there                                  we'll do the client and we'll see how we                                  do the client so I've copied the port                                  from from this particular page here I                                  copied that port that the server's                                  listening on and now I'm going to paste                                  it in here on line                                                        IP and port then here so if I just run                                  this what it's going to do is this is a                                  Python program and you can see it's                                  importing numpy which we have already                                  it's using tensorflow we've got HDFS                                  because we're going to read some images                                  from HDFS it's basically a gr PC                                  application it's a gr PC client and it's                                  a bit messy to be honest                                  alright so there's you know you've quite                                  a bit of work to setup your client so                                  the good news is that the other bad news                                  is that it only works at Python                                         if you enabled a project with python                                      this won't work                                  but the good news from yesterday is that                                  tensorflow serving                                                       that supports a REST API so now it'll be                                  much easier to use that so here we go                                  the client ran it it ran a bunch of                                  images it sent them to the central                                  server who's going to classify them and                                  we can see it got an accuracy of                                     percent okay so that was the amnesty                                  that we sent some images to and it said                                  you know I think this is a                                           percent of the time it was correct and                                  this will handle you know it's pretty                                  efficient model server tens we're                                  serving it can handle hundreds of                                  requests per second and you can leave                                  these running and you can monitor your                                  logs but obviously you want have more                                  hooks in here and we've arrested the I                                  to this so you can get up this data if                                  you want to for example do some                                  retraining of your model so I'm going to                                  go back to the slides because I didn't                                  talk about distributed training but                                  we'll get will we'll have a quick look                                  at that now so I mentioned earlier that                                  you know we're working with some people                                  they have a million images of of                                  self-driving vehicle situations and if                                  you want to train a model on those                                  million images you know many people will                                  say use transfer learning take a trained                                  model and then do this on top of us but                                  you know if you want to be the best in                                  your industry and have the best                                  prediction models you will not do that                                  you'll train from scratch because you'll                                  get better results so the problem is on                                  a single GPU this inner loop of training                                  it can take weeks for that kind of data                                  set so you want to get weeks down to                                  minutes and that's kind of where the                                  Facebook's and Google of this world are                                  so the the imagenet data set which takes                                  two weeks on a large GPU to train google                                  recently got a training time down to                                     minutes on                                              Facebook had one hour on the p                                            last year they're actually kind of                                  competing with each other and see who                                  can do the best have the you know                                  quickest training time so the problem                                  with distribute training is that                                  if you do it for example in a                                  multi-tenant cloud environment you get                                  all of the problems of you know network                                  i/o from other jobs causing your                                  training job to slow down so the                                  Facebook's and Google's do these on                                  dedicated networks network IO becomes                                  one of the main bottlenecks now I                                  mentioned earlier that the parameter                                  server model is not the future and it's                                  not what people are using to get                                  high-performance distributed training                                  the model that people are using is what                                  we call ring all reduce it's a it's an                                  algorithm from high performance                                  computing and because we're Network IO                                  band here what happens with the width we                                  can see what's happening in the ring                                  reduce out and it's just sending all the                                  gradients between all the nodes so                                  they're using both the upload and                                  download bandwidth of all of the nodes                                  efficiently in the parameter server                                  model the GPU servers are basically just                                  sending their data up and getting some                                  down they're not necessarily using all                                  their available bandwidth and they're                                  doing it quite synchronously so what                                  we'll see is some there are is some                                  optimizations in the ring reduce                                  algorithm that when you're computing                                  gradients on the backwards pass through                                  the layers you can send those gradients                                  on a per layer basis while you're                                  computing the gradients for the next                                  layer so you don't need to just wait                                  till you get to the bottom and send all                                  the gradients you can do it as you                                  progress down through your network and                                  if you have a res net                                                                                                                   opportunities for overlapping i/o and                                  compute there so network bandwidth is                                  the bottleneck for a distributed                                  training this is me playing up two                                  cliches factor the parameter server is a                                  bottleneck and the the gradients flow                                  freely like the guiness on the ring all                                  reduced model now we've done some                                  experiments on some of our GPU servers                                  this is to GPU servers with ten GPUs                                  each their ten ADT is we this is the                                  cheapest infrastructure that if you want                                  to build it on Prem this is what you do                                  and we bought InfiniBand                                             cards they were I think seven of them                                  for                                                                    the cards himself cost                                                                                                        price but we got extremely good                                  performance I mean we're getting about I                                  think we're getting                                                     GPUs and about                                                         this is what ResNet training this one                                  here's inception but we get similar                                  results for res not on and this is on                                  the image net data set so it was                                  interesting what that was I was at a                                  talk recently by a large corporation who                                  on their cloud on InfiniBand said well                                  we got                                                                 high-end Nvidia cards v                                                   were getting the same on                                                which are the commodity cards so the                                  reason why we're getting good results                                  here is that you can see the network is                                  this is over                                                            training between all the nodes but the                                  GPU utilization is extremely high now if                                  you look at the parameter server model                                  this is what it looks like it's a bit of                                  a mess GP utilization goes up and down                                  as nodes are waiting for the model to                                  come back from the parameter servers and                                  network bandwidth is only used                                  sporadically so then if you're convinced                                  that distributed training is the way to                                  go and you'd like to do it what does the                                  code look like well horev odd is a is a                                  layer that you write tensorflow code                                  inside of and you basically just have a                                  couple of primitives to work with you                                  have this HVD object it allows you to                                  wrap your optimizer so that the                                  gradients get sent between all the GPUs                                  you can use some MPI primitives like                                  local ranked number of nodes in the                                  system the H or size H V data size and                                  you know in ranked                                                       I'm going to check my models in the in                                  the GPU at rank                                                       ones I'll do whatever so this will just                                  be boilerplate tensorflow code that                                  you're putting in here and if you go                                  with the distributed tense for model                                  you'll find the code is quite complex                                  it's a bit messy so the API that we have                                  has it has a good bit more than just the                                  things we've seen so far there's support                                  for Kafka and SPARC its link                                  we've seen some of the support for                                  things like tensor board and HDFS so I                                  showed already this example the model                                  serving example and but what I didn't                                  talk about was well you know once you're                                  serving your model what do I do now                                  right how does the loop get closed we                                  start by ingesting data you know we go                                  all the way to experimentation training                                  and serving models but then you need to                                  monitor your models in production so you                                  need to look at the difference for                                  example between your performance your                                  prediction performance when you were                                  training versus when you're actually                                  serving so you need to maintain                                  statistics so if a prediction comes in                                  you need to then say well this                                  prediction was made for this input data                                  and this was the actual results and we                                  save that data and we can then use that                                  to compare it with what we trained on                                  and say well there's some SKUs happening                                  here some training serving SKUs                                  happening it's now time to start a new                                  round of training we need to train a new                                  model on because we've new day the                                  training data available and you know we                                  call this covariant shift as the method                                  that you would use to to signify when                                  you when you want to do training now                                  there may be other reasons why you                                  wanted to retrain your models maybe you                                  know some event has happened that you                                  know about that affects the models maybe                                  you just want to do periodically but you                                  need to have the tools to do that so in                                  our case you can use the REST API and                                  the data stored in HDFS to decide on                                  when to train now let's go back and                                  let's let's look at this this actual                                  practical part so we kind of did the                                  first part which was the tour and we did                                  our end-to-end pipeline for                                          let's have a look now ask the hot dog                                  example unless anyone has any questions                                  no ok I'll aim to finish a couple of                                  minutes early as well so that we can                                  enjoy the fine weather and fine cuisine                                  of Berlin okay so let's go to this                                  Python notebook for hot dogs so what you                                  actually have to do here                                  is you have to right-click on it to                                  download it so I'm just gonna save us                                  I'll save it to this Documents folder so                                  the you know I could have included this                                  in your in your tour but I just wanted                                  to give you experience of how easy it is                                  to get data into the system so if I want                                  to create a new notebook let's say I'm                                  gonna create a new notebook we'll call                                  it the TF er what I want to do is I want                                  to upload that Python notebook that I                                  downloaded into this project but I also                                  want to get the hot dog data set into                                  the project so I can start with one or                                  the other and this is this thing here is                                  telling me I should start with searching                                  for the data set hot dog and imported                                  into my project and I also need to                                  enable Python                                                            things                                  I'll start by enabling Python                                             I'm inside the new project I created TF                                  ER and we go to Python and now you can                                  see this didn't appear when you run the                                  tour cuz when you run the tour it                                  automatically enabled python                                              can see that you can choose here to                                  activate python                                                           ready-made                                  environment and unaccounted environment                                  as a Yano file you can just use that and                                  import it and then that will set up your                                  own account environment with the                                  libraries that you need but I'm going to                                  enable                                                                   can see that it brings you to a new                                  screen and you have to wait for this bar                                  to disappear now we have a relatively a                                  much larger clustered mess with like you                                  know                                                                     minute or two for environment to be                                  created on all the all the hosts the way                                  it actually works is that we have a base                                  environment that we clone on the hosts                                  so an account allows you to clone them                                  and you can see here that we have a                                  condor channel so defaults is quite a                                  well-known channel but you also have                                  something called condor forge if you're                                  familiar with Python                                  you you will probably know that counter                                  Forge but you can search in here for                                  libraries I'm going to search for a                                  couple will go back to the filter                                  because I need a couple in here so if we                                  go back to our instructions it will tell                                  me that I need matplotlib pillow and                                  numpy so you can see it's still not                                  completed so it takes about a minute we                                  let that continue going so what I'm                                  going to do well that environment is                                  being created is I'm going to take the                                  next step which says find your hot dog                                  data set and import it into the project                                  so if you go up here and type in hot dog                                  we can make a mistake and just go like                                  that hot dog and we can see that there's                                  lots of things so a lot of people have                                  created projects called hot dog here and                                  then we have something called public                                  data set and cluster hot dog so if you                                  click on that you can see that this is                                  the hot dog data set and there's a bit                                  of information about it and here's some                                  good links there's a full standalone app                                  if you want to look up that later                                  there's a Kaggle computational hot dog                                  or not in this app I touch boy torch                                  notebook as well for us so what I'm                                  going to do is if I press this button up                                  here it's gonna say add this to one of                                  your project so I'm gonna add it to this                                  new ti                                                                   select my project and I press add data                                  set so it doesn't copy the data set so                                  if you're working in a kind of a you                                  know a big enterprise environment and                                  you want to move data between projects                                  as such you often end up copying the                                  data in this case it's not it's linking                                  the data in so if I go into my project                                  and click on data sets you can see that                                  it appears in here as data sets : : hot                                  dog and the owner may not be you and                                  maybe someone else in this case it is me                                  and if you want you can go in and look                                  at the data and we can click on this one                                  hot dog here and I can just pick some                                  random image here and preview us and it                                  kind of looks like a hot dog it's hard                                  to tell I think it might be a bit warped                                  and there in the aspect ratio but you                                  can have a look through your images                                  there and I'm gonna go back and check if                                  python is finished it has and I'm gonna                                  install the libraries that I'm                                  to install so I'm going to take my plot                                  lid first I just take the latest version                                  you can pick any version you want but it                                  should be compatible one so I need to                                  install matplotlib pillow and numpy so                                  let's do it                                  I think numpy is already installed but I                                  can do it again just to be sure to be                                  sure to be sure so you can pick your                                  version here and then you just go                                  install and if you want to watch you can                                  see that they're ongoing here there is a                                  button retry failed up so if you've used                                  hip in particular we when we first                                  started with this scheme we I taught                                  this in a course at the university kdh                                  in Stockholm with a hundred and thirty                                  students and I gave them Conda and I                                  said they got hers counter you got can't                                  afford you have counter defaults and                                  immediately it was like heckles of well                                  I want this library opening eyes not                                  encounter this is not there so you have                                  a lot more libraries available in hip                                  than you do encounter but the problem                                  with pip is that it's a bit of a jungle                                  so you can install one library and hip                                  it will pull them one dependency and                                  then install another library and the                                  dependency may conflict with another                                  library and problems can arise Conda is                                  much better curated as a so let's have a                                  look okay so I was gonna install the                                  last one which was pillow so pillow is a                                  library for manipulating images in this                                  case we need to decode the images we're                                  going to read JPEGs in and we want to                                  decode them and store them and TF                                  records okay so that's my environment                                  setup it's I just need to wait for that                                  one to finish but while we're waiting                                  for it to finish we can go back and see                                  what I'm supposed to do next I have to                                  upload the the the notebook yes so let's                                  do that so we can go to data set here                                  and I'm gonna upload it in directly into                                  Jupiter because when you open up Jupiter                                  this is it shows this as your base                                  folder now you can navigate around but                                  I'm just gonna load it directly here it                                  was this one                                  and you can see it's pretty quick so now                                  I can start Jupiter and we can have a                                  look to what do I want to do I'm going                                  to click on tensor flow and that looks                                  okay to me                                  let me start up Jupiter again okay so                                  now I can open my hotdogs notebook is                                  quite big and again this is Magnus                                  Peterson who did the original version                                  and he is very entertaining videos if                                  you want to look them up on YouTube so                                  what I'm gonna do is this is what you                                  end up doing in in Python notebooks if                                  you're not familiar with them you end up                                  just executing each paragraph one at a                                  time I just want to make sure that                                  everything is installed and you can see                                  that pillow is still ongoing so when if                                  I get to pillow before its installed                                  maybe we'll have an issue but it's still                                  ongoing                                  let's go so we're gonna this one's gonna                                  use matplotlib which we have imported                                  already should be okay again this is a                                  it is not a PI spark notebook this is                                  just a Python notebook some of them are                                  PI spark and some are Python typically                                  PI spark is when we want to do                                  distributed things this is not                                  distributed but you could make it                                  distributed if you wanted to we're using                                  tens for                                                                  for this month until another few weeks                                  pass and then we'll have                                               just explain this code here at the                                  beginning so all we did at the beginning                                  was we imported some libraries so the                                  first thing we're gonna do is we're                                  gonna read the data set from HDFS and                                  it's into this data tier here and then                                  we have a training directory and a test                                  directory there are sub sub directories                                  of the data directory so you can see                                  that the path given here is HDFS : :                                  projects data sets + hot dog so I could                                  just change this to look like this and                                  that would be correct maybe slightly so                                  where did I get this path from well this                                  is the shared data set I imported and if                                  I go back here we can go to our data                                  sets and you can see this particular if                                  I take the test for example you                                  see the path is written up here to that                                  particular folder so I can just copy it                                  to my clipboard paste it in here and                                  then I get that path because this is a                                  shared directory if you have datasets                                  inside your project you can use a                                  relative path which is HDFS get                                  underscore project path which will give                                  the relative directory to your project                                  but this data set is not in my project                                  and unlinking it in from another project                                  called data sets so I need to just give                                  the full path to it okay so what this is                                  going to do basically is define some                                  variables it does an execute much code                                  there's gonna be two classes of image                                  hot dog and not hot dog and then what                                  we're going to do is just look at what's                                  in the directory just to make sure it's                                  okay it's using pi tape to do it and it                                  looks a bit messy but you can see that                                  yeah those folders are in there that's                                  correct and then the next thing we need                                  to do is we're getting into data science                                  here now so I'm gonna I'm not gonna go                                  into huge detail on this but a few                                  questions just shared at me and what                                  kind of say okay what's going on here so                                  this one this particular paragraph is                                  quite long it's defining a class called                                  data sets and data sets has some paths                                  will give you a the path to your files                                  your training data your test data and in                                  fact what you might expect at this point                                  is you might say well my training data                                  is all the images you know I want to                                  make a data set with all the images but                                  this is actually going to make the                                  training data will be the file names for                                  the image it's not the actual images                                  we're going to just set up a data set                                  containing all of the file names and the                                  first part will basically get our we'll                                  have something called one hunt encoding                                  our labels will be one huntin coded will                                  basically say whether an images is a hot                                  dog or not so you'll have a one when                                  it's a hot dog zero if it's not and vice                                  versa if it's if if it's the case so now                                  we've defined that data set object we                                  point it to our images and you can see                                  it prints out basically the some of the                                  directories there and then we can get                                  the class names inside that data set so                                  we've got hot dog now hot dog in our                                  days that has all these file names and                                  we can just have a look at that to                                  basically print out                                  the first filename so this is going to                                  print that the training path for the                                  first image and that's this one here you                                  can look it up in the in the UI if you                                  want to it's this one let's say we can                                  we can do it briefly if we have a quick                                  look it's that one there and if I go                                  back to here we can just see which                                  folder was in it's in the training data                                  osa it's in the hot dog folder okay                                  it's in here so this is the image that                                  it's just gonna show and it kind of                                  looks like that so if we keep going down                                  we can see that we can get this is if                                  you're not familiar with Python this                                  next syntax here a method this method                                  get test set for the data set object it                                  can return basically a list with three                                  three elements so we can then assign                                  them in a single statement so we can get                                  it'll return back the image pass test                                  the the class labels for them the                                  classes for them and then labels for                                  those those files and we can look at the                                  first file it's that one and the data                                  set itself has                                                          other ones you can go look in the net                                  you can probably augment this data set                                  it's not it's not a big one and we'll                                  see later on the result of that it's not                                  big the next paragraph is basically                                  going to just be a paragraph to plot                                  images with so we can view them later on                                  and it uses matplotlib I believe it does                                  and then we have a method a helper                                  method to help us load scenario two                                  paths into we're going to use this I am                                  path I am read path to read and the                                  images so so let's plot a few images to                                  see if they're correct and it's running                                  here you can change these numbers here                                  if you want to plot a few different ones                                  did I run them                                  okay let's make it hopped over a couple                                  of these let's run them again so I get a                                  start from this beginning I'll clear the                                  APIs                                  I'm just gonna jump back to where we                                  were before okay so we're plotting                                  images load and finding loading images                                  and they were plot a few images here                                  okay so if you want to just try to check                                  it a few different images just to                                  convince yourself that this is actually                                  doing it we can do it like that and then                                  we get a bunch of different images okay                                  so now we have some basic code to read                                  up the the filenames for all the images                                  we want to turn them into tensor flow                                  records and we have we're defining a                                  path that we're gonna store them in it's                                  called resources train TF records and                                  we're gonna store the the test TF                                  records in the same folder and this data                                  tear folder so that's our data tier that                                  we can look at the data tier here it's                                  in it's in here it's called resources                                  it's this one here                                  so we can see there's no TF records                                  there and what we'll do is we'll define                                  some functions to print progress as                                  we're generating them and this is where                                  we get into tensorflow code it's very                                  messy so if you're using the TF record                                  file format and data frames you have to                                  define each feature you can see this is                                  our feature and we have to take the                                  value and wrap it inside an int                                          and that's itself in wrapped inside a                                  feature so you have multiple layers of                                  nesting there which is not easy for                                  understandability so what this code is                                  going to do is going to decode the                                  images you can see it says from pill                                  import image and at some point here                                  let's have a look in IM read we're going                                  to decode the image so that's going to                                  take a compressed JPEG and turn it into                                  a bigger expanded array so let's run                                  that code because it takes a few seconds                                  to run now we're getting an error path                                  train records not defined                                  okay I must have skipped over that okay                                  I think this one here okay so I skipped                                  over one of the paragraphs so we can see                                  what it's doing is it's now reading all                                  the files it's converting them into                                  tensorflow records it's it's doing this                                  on the local file system because Pihl                                  doesn't support HDFS which is a bit                                  messy and then the copies a results back                                  into HDFS so we'll have them available                                  in our in our UI so we can go into our                                  UI and actually just make sure that the                                  file is appearing there someone's                                  finished we can start converting the                                  next one the test data you can see it's                                  kicking off so if we refresh here we can                                  see that our training records appeared                                  in here and if you look at the size of                                  it it's                                                                  bit bigger than the training data set                                  which was I think sixteen or twenty                                  megabytes so when we decoded the images                                  and expanded them they became a bit                                  bigger so now we're converting our                                  training data and that's nearly done                                  don't and now what we want to do is is                                  we're going to use the estimator                                  framework in tensorflow this is what                                  Google use internally and this is being                                  used in production in a lot of places                                  the estimated framework is a way of                                  trying to abstract out training so that                                  you just basically say here's the                                  training data inside a data set in the                                  data API and tensorflow here's my test                                  data here's my evaluator function here's                                  a function to give you the records one                                  at a time here's a function to parse the                                  records before you if you you feed them                                  into tensorflow and and you put them all                                  together and then you can run it as a                                  single abstraction so i'm gonna define                                  some of the functions for our estimator                                  this one is basically going to decode                                  our raw image data from from the from                                  the TF records examples it's going to                                  decode those images it needs to turn the                                  the the int the                                                        float                                                                     you know have                                                            point numbers and then we have to                                  extract the the labels and the and the                                  bytes for the images so we return them                                  back to the topple and then the next                                  thing we do is we define this input                                  function which allows you to when you're                                  training you might be want to repeat                                  going through the data set when you're                                  testing you don't want to repeat and                                  then we we need to pass back an iterator                                  or we need to get nitrate err to our                                  data set this is part of the the data                                  set framework and with that ID Reiter we                                  can get out an image and label and in                                  this case we want a batch of them not an                                  individual one and then we have to                                  return them as addicts which is a little                                  bit messy so this is our train it is the                                  input function which will take our input                                  file names and return back some records                                  to the to the estimator and this is for                                  the test function so let's do some                                  training and we're going to do training                                  on a pre count estimator so there's                                  there's some pre-built ones that you                                  don't write yourself this one is called                                  the DNN classifier a deep neural network                                  classifier                                  I'll just started running here and then                                  we can we can see what it looks like so                                  the kind of unfortunate part of this                                  framework is there's a lot of                                  boilerplate code for each column or each                                  feature you have to wrap your values you                                  can see there in the feature image of                                  feature columns this is part of the                                  neural network how it's going to look                                  it's going to have                                                      one layer tune or                                                                                                                              the at the estimator that we define we                                  say that here the future columns are                                  training with the network architecture                                  structure we're going to use a Rallo                                  activation function and there's going to                                  be two classes it's a binary classifier                                  so once it's train is finished training                                  and this one is still training it should                                  finish it's                                                              already passed and step on                                  one but it takes a little bit of time                                  then you can then you can evaluate so                                  the the estimator framework allows you                                  to supply an evaluation function and                                  we've just it's just been executed now                                  and we can see what kind of results                                  we'll get out of it                                  so it takes a few seconds and we can see                                  we're getting                                                           random this is not what we want now you                                  can say well why is that well we have a                                  data set of only a thousand images we                                  can augment it you can do this is an                                  exercise for home you can use transfer                                  learning to take an existing train model                                  we could rotate the images and deform                                  them to make the data set larger                                  probably you'll need on the order of a                                  few tens of thousands of images to get                                  this to work really well and the best                                  option would be probably to use a pre                                  train model and then train on top of it                                  with these so there is another one down                                  here we can see with another estimator                                  that's defined I'm this takes a bit                                  longer to run it's a convolutional                                  neural net that was just a a                                  feed-forward neural network and comp                                  nets are better at doing image                                  classification and this one I ran                                  earlier and we can see the results we                                  got was                                      I'm not going to train here cuz it takes                                  a bit more time which is a bit better so                                  there's a signal there that if we run                                  and train that for a long time and we                                  augmented and rotated our images that we                                  should get at least some better                                  predictions from okay I said I promised                                  I'd finish up a little bit early so with                                  that I'm just gonna say that look this                                  is the framework it's open source it's a                                  European project we come from the                                  University but we have a start up we're                                  trying to push this out there if you're                                  interested in doing deep learning and                                  particularly on large volumes of data                                  with lots of GPUs come talk to me and a                                  lot of people have worked on this and a                                  lot of people are still working on us                                  and it's a European platform for deep                                  learning with big data called hops with                                  that I'll take questions and I have a                                  hand up over here on the side                                  is it possible                                  so can we upload our datasets two hops                                  Hadoop and you know run our satellite                                  images for example of course I mean we                                  have a cluster in Lille I can give you                                  access to with pet abideth and is there                                  a storage limit like how much that I can                                  upload we can we're flexible ok thank                                  you with us thank you                                  alright more questions anyone thanks for                                  sticking here cool thank you one more                                  quick we'll take it anyway so are we                                  going to see pie-dar john harwood in                                  hops sort of soon yes is the answer                                  ok looking forward yeah I'm looking                                  forward to a good model server or for                                  trained PI torch models that's the big                                  big kind of missing picture piece for                                  fir for pi tours right now you know but                                  all right thank you                                  ok thanks                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=9haYBKMU4mM


