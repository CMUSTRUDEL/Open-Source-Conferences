Title: Berlin Buzzwords 2018: Igor Mazor - Deep Learning Recommender Architecture: Master Thesis to ...
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Igor Mazor talking about "Deep Learning Recommender Architecture â€“ From a Master Thesis to Production".
 
The vast majority of the recommender systems nowadays are based on algorithms which are good at representing linear relations between users and items, or items and items. With the growing popularity of deep learning applications, it was quite natural for us to experiment with a more advanced recommendation engine which could represent more complex, non-linear relations. With this we were able to generate much more relevant recommendations to the users.

The new recommendation engine was developed by a student as part of his master thesis, lead by a PHD colleague from the mobile.de data team. The engine is based on deep learning and combines 3 sub neural networks. The engine showed really good results compared to our current recommendation engine, what brought us to the decision of trying to deploy it into our production system. Once we started to review the new engine, the following main challenges were raised:

 - We are using Java/Scala in our production system: Is there a possibility to deploy a Tensorflow model, which was trained in python, in Java/Scala?
- How would we be able to deploy new models into the production system, without any  downtimes ?
- The model contains 3 sub neural networks, each sub network is responsible for different  functionality: Would it be possible to isolate each of those sub networks and deploy it  separately in order to scale out the entire engine ?
- How could we generate recommendations in real-time without pre-calculating the recommendations for each user/item over night ?
- Does such complex system can scale at all?

The main focus of this talk would be on our journey towards the design and implementation of a scalable architecture, giving all the mentioned above requirements, which could support the deployment of the new Deep Learning Recommender in production. During the talk I would try to present the different architecture components and how each component helps in solving the mentioned above challenges. In addition, I would try to describe how the different components of the Deep Learning Recommender could be reused and help us to improve also our search functionalities.

Read more:
https://2018.berlinbuzzwords.de/18/session/deep-learning-recommender-architecture-master-thesis-production

About Igor Mazor:
https://2018.berlinbuzzwords.de/users/igor-mazor-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so my name is Igor I am working for                               Villa in the last almost two years I                               join will be left for building                               specifically as a real-time                               infrastructure and a few months ago I                               got a really interesting project which                               is around deep learning recommender the                               project started as a master thesis by                               Marcel Karofsky and I was leading by                                florian vedyam a PhD employee from one                                of my team colleagues and they build                                something really interesting but it was                                part of his master thesis and they came                                to me and asked me ok how can we make it                                to production and then we started a                                really interesting journey with a lot of                                challenges which I want to discuss with                                you in short today how we actually were                                able eventually to bring this master                                Jesus into production so today a bit was                                the agenda I will start a bit with                                motivation about the recommender why we                                needed in mobile specifically some of                                the components and the deep learning                                approach and architectures that we were                                using so a little bit numbers for                                motivation mobilize Germany largest                                online vehicle marketplace we have                                almost around                                                        and its keep changing each day updated                                constantly                                we have around                                                        covers almost all the dealers in Germany                                so we are really the leading platform                                for that we have almost around                                         have                                                                  visit visitors here users that come in                                each day and we allowed a lot of changes                                to the items on the other hand we have                                also a lot of interactions of the users                                we have almost around                                                   that users generate each day by                                interactions with the platform events                                such as item views such as searches                                saving to favorite list ok but when you                                look on that so                                                       us it's quite a lot but separately each                                of those events do not tell us anything                                and we had to come up with a way that                                you can reduce those                                                   to something more useful something more                                meaningful that can help us to somehow                                help the users to find a better car                                something that fit for their preferences                                so eventually the idea was to create                                something in the form of user profile                                so each day we take all of those                                    million events we crunch them we                                calculate them and we reduce them                                eventually to around                                               preferences so as you can imagine on one                                hand we have                                                           other hand we have millions of different                                user preferences so then the question                                and the real challenge is eventually how                                can we bring get together how can we use                                the part of the user preferences and the                                part of the items to match between them                                and to help find the users the sort of                                the perfect car so if you look for                                example on this firefighter the perfect                                car that will be for him this is the                                firefighter truck right so this is what                                we want to do for all of the users to                                find help some findings or perfect car                                find perfect match so we try different                                approaches and one of the approaches                                that we tried was using deep learning                                and you may ask yourself okay but why                                deep learnings are many different ways                                of recommender and we tried other                                recommenders as well but deep learning                                if you think about it it can first of                                all capture a nonlinear relationship                                which is really important because most                                of the algorithms today for                                recommendations whether is matrix                                factorization or any other type of                                recommenders usually have some limits                                about linear relationships second of all                                it until some degree reducing                                engineering effort feature engineering                                effort because deep learning in the way                                how it build it it can just take and                                combine different features together and                                you get in something yeah the only                                problems that it's harder than to                                understand exactly what is output why we                                getting some specific results but in                                 general it should improve also                                 predictive capability so this was a                                 motivation for us to try deep learning                                 approach so speaking a bit about the                                 components                                 what kind of components we have that are                                 part of the major deep learning                                 recommender first of all our items so as                                 I mentioned we have two million items                                 and each item have more than                                     attributes and just to make clear item                                 for us it's some sort of car it can be a                                 car a motorcycle it could be a truck                                 okay so each item have more than                                     different attributes such as price color                                 a number of doors number of previous                                 owners condition seats                                 many many we're currently using only                                    attributes and so we examining the model                                 only with twenty attributes but                                 definitely in the future we will try to                                 see how we can take more of those                                 attributes and leverage them and our                                 items are updated constantly by the                                 dealers most of the time it's the price                                 but still it's really dynamic it's not                                 like items for example in our B&B or                                 ebooks or Spotify that they have some                                 something that is more static that once                                 user upload the song most of the time                                 nothing will be changed for that song                                 anymore                                 the other component that I mentioned is                                 the use of preferences and in our case                                 user preferences is actually modeled                                 through the bayesian statistics so                                 actually we take all the interactions of                                 the user such as for example save to                                 favorite least contact specific dealer                                 view search and so on and we try to                                 build the statistics about that user we                                 try to see what his preferences towards                                 specific item attributes for example you                                 can see that user Express                                          colors and                                                            prefer                                                                and so on and the price distribution is                                 around                                                                   to build that kind of statistical model                                 and in addition we calculate also the                                 average user the preferences of the                                 average user and eventually using the                                 Bayesian statistics we weighting the                                 specific use of references against the                                 average user preferences okay but how                                 does that really help us then to build                                 our recommender so imagine if we would                                 had a way that we can represent our user                                 preferences and our item in some                                 abstraction some sort of maybe                                 mathematical abstraction way and then we                                 can take the abstraction representation                                 of the user preferences and abstraction                                 of the item put them in some sort of                                 black box and the black box will be                                 smart enough to give us a ranking and                                 that ranking will be sort of probability                                 of user to interact with a specific item                                 so based on that probabilities we can                                 select the items the next items that we                                 want to show to that user and that black                                 box is actually as some of you already                                 guessed this                                 is a deep learning approach so how                                 actually that the black box looks inside                                 so he have a deep learning model which                                 is trained with SAP with three sub                                 different networks user net item net and                                 rent net the user net is actually                                 responsible to taking those user                                 preferences and converting them to some                                 sort of embedding in another world the                                 embedding is a vector of                                                mathematically represent that user the                                 item net on the other hand taking the                                 attributes of the car and calculates                                 also embeddings which is also a float a                                 vector of                                           actually in the training process what we                                 try to do is to bring the item                                 embeddings of the the item in banks and                                 the user in bindings vectors to be on                                 the same space because that will allow                                 us later to do a direct comparison for                                 example using equality and distance once                                 we have the item in banks and the user                                 in benning's we can pass them together                                 through the rank net and what the rank                                 net do is actually give a score between                                                                                                      also negative sampling method so                                 actually what the rent net give us is                                 the probability of the user to interact                                 with a specific item for a higher level                                 view imagine that we have the user                                 preferences we have multiple different                                 items we convert them we take the user                                 embed ink we take multiple item in                                 benning's we pass it everything through                                 the rank net we give we get scores and                                 then based on the scores we can decide                                 which item we're going to present to the                                 user all right so our first challenge                                 started with our data scientist our data                                 scientist are really smart guys but as                                 most of our data sign many other data                                 scientists today they prefer to work                                 with cerebus Python and I wish Scala                                 with Python and most of the                                 proof-of-concept that they build are                                 with Python and they were using                                 tensorflow to train our deep learning                                 models and obviously they done into this                                 Python so when they came to me and they                                 told me ok we need to bring it to                                 production we had a bit and a problem                                 because our production system is only                                 Java or Scala this is the policy                                 currently it's not looking very we not                                 that we have anything against Python                                 I actually think that we should try to                                 bring some Python stuff to production                                 but most of our experience of the                                 developers and of the DevOps are around                                 Java and this way we didn't want to have                                 any sort of surprises now with Python so                                 we had to think how actually we can take                                 the Python stuff put it in Scala because                                 it is my team works mostly with Scala                                 without rewriting the code from scratch                                 and this is what's really important                                 requirement after doing some search and                                 research I found out different blocks                                 that most of them actually were more in                                 the Python but I had enough ideas and                                 understanding how we can convert it to                                 the Scala world so the interesting                                 things that tensorflow comes with it                                 stands for floss serving which is a                                 specific C++ implementation that allow                                 us to load tensor flow models tensor                                 flow models which can be exported and                                 saved via protocol buffers and tens are                                 serving is the C++ implementation that                                 can simply load the model and you can                                 use it to serve predictions or do                                 anything that you want around this                                 tensorflow models the only problem is                                 the tensor flow model was sorry viz the                                 tensor flow servant was that because                                 it's C++ and because it's Google and                                 Google always special they using only                                 the protocol which is called G RPC G RPC                                 communicate only via protocol buffer                                 which means that you cannot you cannot                                 call tensorflow serving with a simple                                 rest comma with a simple rest service or                                 with a simple JSON interface okay but                                 all our ecosystem around the services is                                 the rest obviously so we had to think                                 how we overcome it so the idea was to                                 take a docker container which allow us                                 to ice with resources and to deploy in                                 the same docker container the tensorflow                                 serving and side by side a scale                                 application right the scarification                                 would be able locally inside the docker                                 to communicate with the tensorflow                                 serving and other services that need to                                 get prediction we will call actually the                                 Scala gateway and in such way we were                                 able to scale it quite well quite well                                 as well because we didn't had to deploy                                 separately any sort of tensorflow                                 serving cluster the applications that we                                 scaled was each application have a                                 docker and inside the Scala together                                 with tensorflow serving once we figure                                 out how to make Scala and tensorflow                                 work together we could build we could                                 start thinking about the                                 flows how we can divide our problem to                                 multiple parts so we can so we can put                                 everything together in production so the                                 first workflow so before the first world                                 who actually is the general concept                                 would be as following we have a tensor                                 flow modules that we train in Python                                 once we finishing to train it in Python                                 we can export the the module to protocol                                 buffers file which is really a simple                                 file once we have the protocol buffer in                                 some place that assumes some sort of                                 object storage okay it doesn't important                                 git repository we can actually then                                 build a docker image and once you build                                 this docker image the docker image will                                 include inside the scala gateways that                                 we built the that Scala gateway is able                                 to communicate locally via G RPC                                 protocol with tensorflow serving and we                                 will be able also to load the protocol                                 buffers model to that docker and what                                 nice with tensorflow serving that is                                 actually have a place a library sorry a                                 directory where it listens for new                                 incoming protocol buffer files so if you                                 have a new version of the model you can                                 just push it to that specific folder and                                 tensorflow serving automatically will                                 catch up and start serve it so it's                                 actually allow you first of all easily                                 push new models updates without                                 downtimes                                 and it allows you actually if you will                                 call to the tens of serving with a                                 specific module version to test it we in                                 a be really easily different model                                 versions ok so for the next part we had                                 the challenge of our items as I                                 mentioned our items are constantly keep                                 updated the dealers constantly update                                 the items sometimes they change small                                 attributes most of the time to change                                 the price but we need to catch up                                 constantly we need to be able to                                 calculate those item embeddings based on                                 the attributes of the item constantly in                                 real-time we cannot allow to ourself do                                 it in batch again as I mentioned because                                 we are not like Airbnb for example as                                    or somebody upload department or                                 somebody upload the song or a book and                                 this is quite statically and it will not                                 change once the dealer constantly keep                                 updates or deleting for example                                 uploading new ads we need to be able to                                 catch it because otherwise if we will do                                 it calculation of the embeddings once a                                 day it could be that we will serve                                 something that's already deleted and not                                 up                                 to date also our dealers are able to                                 solve to sell cars quite fast so it                                 could be a situation that we have a new                                 car we didn't catch it up we will                                 calculate the embedding only for the                                 next day but that car already gone so we                                 want to be able to catch up as fast as                                 we can so the idea was that we have a                                 kafka topic that includes all our items                                 and we constantly consuming that kafka                                 topic and using the configuration of the                                 Dockers that we have the Scala get away                                 with the tensile serving we can actually                                 constantly keep generating item                                 embeddings and we push them to a new                                 Kafka topic by pushing those item in                                 banks plus some specific item attributes                                 to a new Kafka topic we can also allow                                 other teams later to leverage it and to                                 use it for any use case that they want                                 we can then later to persist those                                 embeddings to elasticsearch to radius or                                 whatever and I will actually touch that                                 point the third challenge which was one                                 of the biggest actually is how we can                                 actually generate the recommendations                                 themself so having the process of being                                 able to calculate items embeddings in                                 real time it's cool and after that we                                 figure out how we can combine Scala and                                 G RPC and the tens of serving in the                                 same docker container it was also cool                                 but the major challenge remained                                 actually how can we eventually generate                                 those kind of recommendations so ideally                                 in a perfect world scenario we have only                                 two million but imagine if you had five                                 million so ideally each time we getting                                 a user input we're getting from some                                 user preferences service we can get also                                 the Preferences of the user and then in                                 real time we need to be able to                                 calculate the user embedding so imagine                                 we could do it without a problem but                                 then we need actually to rank that usery                                 beddings against all our item in banks                                 against all the two million item in                                 banks to get the rank for each item and                                 then to take for example the top ten of                                 the top                                                              request ranking through the rank net                                   million item embedding sits it will not                                 scale it will not work it will take us                                 more than a minute or two minutes to                                 submit recommendations for the user and                                 I guess nobody will be really much happy                                 with such performance                                 so the idea is that we need first to                                 filter those items and we need first to                                 try to find out some way that will allow                                 us to select the more relevant items for                                 that user                                 for example let's filter out and select                                 approximately only                                                       candidates and based on those candidates                                 we will use only the embeddings of those                                 candidates to pass them through through                                 the rank Network and rank on is those                                                                                                          done that we can take the top                                           top                                                                   actually how actually we can do that                                 kind of filtering especially with two                                 million so our first approach was to try                                 this elastic search so as I mentioned we                                 have the kafka topic right and we can                                 use a Kafka topic in order to write some                                 sort of elastic search indexer and we                                 would index our item embeddings to                                 elastic search together with some of the                                 attributes so for example we will in                                 indexed elastic search the curve price                                 and the color and number of previous                                 owners and so on but in addition we will                                 index L so the item embeddings and to                                 index the item in Bennigsen elastic                                 search it's actually quite                                 straightforward it's just a vector a                                 list of                                                                 time when we get a request from a user                                 we need to calculate the user we need to                                 get the user preferences we will need                                 also to have some of the user embedding                                 and we will execute based on the user                                 preferences a simple query against                                 elastic search which will do filtering                                 based on those preferences so for                                 example if we have a user that prefer                                 red or yellow we will ask a sick search                                 to filter all the items that have red or                                 yellow and we will feel we will ask                                 elastic search to filter items in a                                 specific price range okay in a specific                                 car model for example BMW or Audi once                                 we will do that we will take the top                                 hundred the sorry the top                                                of the result from elastic search we                                 take on his item embeddings of those top                                                                                                     container which have inside the ability                                 to run it through the rank net                                 so some of the problems with that                                 approach first of all our item                                 embeddings and our user preferences                                 already contain implicitly inside all                                 the information about the user                                 preferences about the color about the                                 price and so on it's just that the deep                                 network learned it in a more obstructive                                 way in a more mathematical                                 representation so actually if we will do                                 that kind of filtering before it's until                                 some degree I will not say it's resident                                 but then it's it's useless and to use                                 the deep learning because the the real                                 power of making better recommendations                                 is actually to use the embeddings                                 directly and if I do this kind of                                 filtering it could be that I'm actually                                 missing something and I will remain with                                 items that are really not the one that                                 fit for that user so this is why it was                                 a bit problematic second of all and the                                 biggest problem scalability so that                                 approach is not scanning really well                                 executing thousands of those queries per                                 second against elastic search to be able                                 to do this kind of filtering and then                                 taking only the                                                         works really hard and we saw a lot of                                 issues and okay currently we have our                                 elastic search I think is around                                    knots cluster but still we had a lot of                                 issues to scale it like beyond already                                                                                                         the ideal solution for us then we                                 decided to try a more crazy idea and I                                 think eventually it's actually also                                 remained only in the matter of of idea                                 so the idea is as following I will try                                 to explain it clearly so we have all the                                 item Ebanks item Ebanks are just vectors                                 right so the operations that we can do                                 on top of those item embeddings is                                 clustering using even the simple k-means                                 so we can cluster all the two million                                 item embeddings two different clusters                                 let's assume we will cluster it to                                    until                                                              clusters we can assign each item to                                 which cluster it belongs based on the                                 distance calculation between the item                                 embedding and the centroid of the                                 cluster so then what we can do in the                                 next step when we index our items into                                 elasticsearch                                 we can also index together with the item                                 embedding the item attributes                                 the century the cluster to which that                                 item belongs and why it's good how it                                 can help us so on the next step when                                 we're getting a request from a user to                                 generate recommendations for we can                                 generate the user embed ink and then for                                 that user embedding we will do almost                                 something similar we will calculate a                                 karidian distance between the user                                 embedding vector and all the centroids                                 of the clusters that we calculated                                 before and then we will take for example                                 two or three clusters which are the most                                 closest to that user okay so once we                                 have the user embedding we can calculate                                 what are the cluster centroids that                                 represent item embeddings closest to                                 that user embedding and we can use it as                                 a filter in elasticsearch so then in                                 elasticsearch instead of filtering by                                 user preferences we actually will say                                 okay give us all the items that belong                                 to cluster one two and five because we                                 believe that this is the clusters that                                 have the items most closest to that user                                 okay but that not enough we want still                                 to leverage our embeddings even more so                                 we could also on the Spree filtered                                 results to use a quality and distance to                                 calculate the distance between user                                 embedding and item in veining now it's                                 not really a quality and distance it's                                 more approximation of a quality and                                 distance because then you don't need to                                 calculate the route square and then it's                                 make life easier from elasticsearch also                                 because we don't really need the precise                                 distance for the quality alone we just                                 need something that give us relative                                 sorting and the approximation of                                 equivalent distance is good enough to                                 give relative sorting in order to                                 implement the quality and distance                                 calculation in lastik searched was not                                 so straightforward because elasticsearch                                 out-of-the-box support all kind of those                                 functions only with vectors for two                                 dimensions like Gyo Gyo queries but not                                 with                                                                  elasticsearch                                                            scripting language called painless and                                 that language is eventually when you                                 write a script is compiled directly to                                 Java bytes so actually the execution is                                 as fastest as it can be for native                                 elasticsearch functions already so we                                 actually had to write a really simple                                 function which was just really a simple                                 loop that you would switch a item in                                 each of the vectors and just do sass                                 obstruction and some                                 chillie another way to do it is with                                 plugins and we actually thought about                                 maybe leveraging some of the Java more                                 linear algebra core computational                                 numerical libraries I tried it I done a                                 benchmark but for this kind of vectors                                 substraction of                                                         in Java is was the fastest really so                                 going for numerical libraries was                                 overkill                                 so once we have that we could execute                                 sort of a risk or query inside                                 elasticsearch is calculating equality                                 and distance and then taking on his                                 items which have the closest one for                                 example the top                                                         do it a bit we saw that this also have a                                 lot of problems you need to reduce the                                 month and number of items quite                                 drastically because the calculation of                                 the quality and distance since it's not                                 supported natively by elasticsearch was                                 quite problematic and it was not scaling                                 as well so finally the approach that we                                 came up was the nearest neighbors search                                 approach approximation actually and we                                 found a really good library enoy which                                 is coming from Spotify and what if I                                 using this library for some of the                                 recommendations and the full name is                                 approximate nearest neighbors per year                                 and the name is funny but the library is                                 really good and what it was really                                 actually good in that libraries that it                                 is a C++ implementation and it's allow                                 searching points in spaces that are                                 close to a given query right so for                                 example we have the user embedding                                    floats all our items are also                                           so the user embeddings will be the input                                 query for our nearest neighbor search                                 and then it it will help us to find the                                 most closest item embeddings which we                                 can then later tag and pass through the                                 rand network well it was extra really                                 fast the library works really fast                                 it keeps an index mam it keeps an index                                 file in memories actually it have a                                 special technique that I will not go too                                 deep into it which use random                                 projections and it created a special                                 index                                                                  they use some sort of approximation to                                 find the nearest neighbors what was the                                 best that is support Scala and Python as                                 I mentioned we didn't want to rewrite                                 the code so we want to have something                                 that the data science can check on his                                 site from Python perspective and we can                                 check in scar                                 and that you have exactly the same                                 behavior and the fact that the memory                                 that the index file isn't memories also                                 helped a lot to boost up the performance                                 especially since at least in our case we                                 have only two million so putting it in                                 memory it was like around                                              which is definitely not a problem today                                 in any environment in any production                                 environment I know a library supports                                 multiple different distance calculation                                 like a collodion Manhattan and Harmon                                 concern we had to we needed on his equal                                 ideon and it works best with vectors                                 under                                                                performs quite well also these vectors                                 up until                                                               dimensions it was perfect for us so how                                 we make it to work eventually so back to                                 the flaws that we had about how we                                 update in real time our item embeddings                                 right so we have still the Kafka topic                                 with item embeddings and we have the                                 docker configuration with Scala gateway                                 and tensorflow serving we calculate the                                 item embeddings but eventually since                                 we're putting them as remember I told                                 you since we put them in a separate                                 Kafka topic somebody else can just take                                 that topic consume it and to do whatever                                 you want so in the first attempt we try                                 to consume from the topic and put in                                 indexing in elasticsearch on the second                                 attempt what we're doing is actually                                 we're running a schedule job in jenkins                                 each hour and that job each hour consume                                 all the items from that Kafka topic and                                 again since we have only two million we                                 can afford it and it takes around                                    minutes to consume the entire topic and                                 it builds that kind of annoy index file                                 once we are finishing to build that                                 annoy index file we need to push it to                                 some object storage which will be used                                 later by some the core application to                                 load it into memory and to use for                                 submitting recommendations anything                                 chemists know okay                                 so then the next question after we                                 solved how we select candidates using                                 the nearest approximate neighbors                                 approach and how and how we were able to                                 calculate item in banks and update them                                 in real time and that we figure out how                                 we're dealing with the problem of Python                                 and Scala the next question is actually                                 okay but how do you eventually generate                                 those recommendations and again we need                                 to be able to generate recommendations                                 in real time we don't want to generate                                 those recommendations in a batch once a                                 day so each time a user getting a                                 request we want to calculate his                                 embeddings and we want to calculate what                                 items can feed him for that for that                                 point the most updated items not waiting                                 for the night to do that kind of magic                                 so we have another docker container                                 which is the main the core applications                                 the core deep learning application and                                 that docker container have three major                                 components so it will have the Scala                                 gateway that have all the logic and all                                 the coordination between all the                                 different pieces and networks it have                                 tens of serving tens of serving again                                 the same configuration but this time we                                 have two different nets that we are                                 holding in that docker we have the user                                 net in order to be able to calculate                                 user embeddings in real time and we have                                 the rank net in order to make the                                 ranking between the user net and the                                 item in benning's and actually we also                                 need the annoy index file so the scale                                 application can look inside that annoy                                 index to find the nearest neighbors the                                 candidate so let's have a look on the                                 flow what's happening when we getting a                                 request of a user and we need generate                                 recommendations so first of all we need                                 to get the user preferences and the user                                 preferences we can get from the user                                 preferences service it's a completely                                 separate service and actually I gave a                                 talk about it last last year I also                                 recommend you if you want to see how you                                 can calculate user preferences in real                                 time                                 the trick is sync with the user                                 preferences is like item embeddings user                                 preferences are calculated in real time                                 so when some user come to our platform                                 he search he view his he saved something                                 to his favorites list his preferences                                 updated almost immediately which means                                 that the profit that the Prados                                 preferences are not                                 statics are dynamic and that means that                                 we really cannot calculate user                                 embeddings overnight as well because                                 it's keep changing constantly                                 this is why we need to take the zero                                 user preferences those statistics of                                                                                                            of                                                                  those in real time to be able to                                 calculate the user embedding so once we                                 get into the Preferences the next step                                 will be to calculate the user embedding                                 user using the user net once we have the                                 user embedding we continue to the                                 following step which is finding the                                 k-nearest items using the Neue library                                 right there are no index files the                                 nearest neighbors approach we take we                                 take approximately now I think today                                     candidates once we have those                                     candidates which are we believe the most                                 closest to that user we using the other                                 net rent net to give a score for each of                                 those item embeddings and sort of saying                                 this is a probability of that user to                                 interact with that items and finally                                 from sauce we take top                                              depend on the sec holder in the service                                 you can define I want                                                                                                                       eventually our recommendations ok so the                                 general view how everything looks                                 eventually in production so we have one                                 side that is consuming items                                 calculated constantly the item                                 embeddings putting them to Kafka topic                                 which have those embeddings we have a                                 job that runs each hour calculate                                 generates a new annoying index file and                                 push it to some object storage                                 something like a                                                       private cloud so it's something based on                                 OpenStack on the other hand we have                                 another docker which is the core deep                                 learning application it is able to                                 communicate with the user preferences                                 service to get the Preferences each time                                 it's needed it's able to calculate the                                 user embeddings in real time each time                                 it's needed and to do the ranking and                                 also he need to load as a no index file                                 from that object storage so when we                                 deploy this application on the                                 production at the beginning there is a                                 sort of part of the code that knows ok                                 now before I start the application and                                 it's available for all the stakeholders                                 I need first to go and grab                                 I know index from the object storage the                                 other question is how actually but then                                 you put each hour the new update is                                 annoying index file to that application                                 and here's our multiple ways one of the                                 ways is for example to build a mechanism                                 inside your application that constantly                                 keep checking each minute for example                                 the object storage to see if there is a                                 new file and if yes it's loaded and it's                                 doing swap in memory on the fly so                                 really without any kind of down times                                 this is one way another option is to                                 send actually notifications to the                                 application saying hey I have a                                 calculated a new file please grab it                                 from the please please get it from the                                 object storage and this you can do in                                 two ways either you have a Kafka topic                                 which the application consumes messages                                 from Kafka topic and through the Kafka                                 topic you keep sending messages hey I                                 have a new file and then the application                                 will know to grab it either actually                                 what we decided to do is a bit more                                 easier we have something called console                                 from Hoshi Corp it's it's a specific                                 software that allow self discovery of                                 services but it have also a lightweight                                 key value storage and it's actually have                                 a specific functionalities that you can                                 listen to changes in that specific key                                 value so each time we actually push a                                 new index file to the object storage we                                 update that key value the other                                 application constantly listening to                                 changes in that key value and once there                                 is a change it will take the new URL and                                 will download the new index file and                                 keep it in and switch switch it in                                 memory all right so for some final notes                                 bring data scientists down to earth                                 really important because when I came to                                 start working with the data scientist                                 about this project they had the Python                                 and they had this implementation and                                 there was some exotic library here and                                 exotic libraries there and many                                 requirements and it's all look guys this                                 is all nice it works fine it was really                                 beautiful in your thesis but we need to                                 bring it to production production is                                 completely different environment we                                 cannot allow ourselves too much playing                                 there and failing so you need to see                                 what data science bring you and then you                                 need to conform with what is actually                                 possible in reality ok but don't start                                 from but don't stop from yourself to                                 being creative                                 don't say hey this is really hard to                                 implement I don't know this exotic I                                 don't know how to put it in                                 the Scala world so I will not put it                                 know try to listen and try to to see                                 what you can do this is why it died you                                 need to take time to do the research and                                 read it took me some time to read                                 different blogs and researchers                                 especially even not related to Scala and                                 but more to Python to get to get idea                                 how we can actually do it in Scala as                                 well for that project actually I think                                 that basic understanding of deep                                 learning concept was enough I am really                                 not a deep learning expert as you see                                 this talk was not about how to choose a                                 proper deep learning model or framework                                 or whatever but how you can put the                                 pieces working together in production                                 and basic concepts understanding is                                 quite important really important at                                 least for our case was to find sky                                 equivalent libraries to Python because                                 we didn't want to do code rewrite since                                 we have already is that kind of                                 experience from previous projects so for                                 me it's really important otherwise you                                 have a really he'll maintain two                                 different code bases and as I mentioned                                 create creativity and out-of-the-box                                 thinking it's quite important don't give                                 up because you think something is too                                 new or too exotic                                 maybe there are still some really nice                                 solution somewhere that you can leverage                                 thank you so we have five minutes                                 question so hi                                 you mentioned that you're running an                                 hourly job to recalculate the index for                                 the no library right does it mean that                                 you are still running into the risk of                                 having like outdated index basically huh                                 yes but we did some analytics and we                                 tried to see what are the risk for that                                 and we saw that they are constantly                                 items that are updated but still we                                 decided that during that hour we can                                 sort of take that risk because otherwise                                 it was between not doing it at all or                                 doing it sort of you can say just in                                 time right so real-time definitions are                                 what is real time for use just in time                                 I'm a I'm curious I'm curious if you                                 looked at the elasticsearch learning to                                 rank plugin for implementing crank net                                 because they have a rank net                                 implementation again the there's a                                 elasticsearch learning to rank yeah we                                 had a look on that but this is actually                                 not the one this it works a bit                                 differently than that one it will not                                 help us actually I didn't had time to                                 put it on the slides here but we had                                 resently workshop with West I don't know                                 if you heard about it it's open source                                 presently from Yahoo and actually we are                                 now evaluating it to see if this can                                 replace entirely that stuff because it's                                 exactly for that use case when we and                                 well we evaluated the skull and the                                 peyten implementation of an OE we found                                 that the skull implementation was very                                 slow did you have any did you look into                                 the performance of this maybe we use the                                 wrong library so you mean scallion                                 permutation of tensorflow I know II I'm                                 using this color implementation of an IE                                 and did you do any evaluation of the                                 performance versus the python                                 implementation we I don't remember if                                 you've done any comparison but it was                                 working come in fine and we compare                                 these data science to see that you                                 getting the same results and there is                                 actually two implementations one is a                                 scholar pure implementation and one is a                                 pinning to C++ that using Zano itself so                                 it's depend which one you take thank you                                 hi I'd be interested to know how you                                 measure the improvement of your                                 recommendation system after you                                 implemented this introduction good                                 question unfortunately currently I                                 cannot answer it because this is still                                 going to the a/b test scenario and most                                 of the evaluation was done actually in                                 sort of offline manner this is why we                                 really try to bring it now properly to                                 production with proper rate so it's in                                 production is just waiting for a B tests                                 to run properly so we can know the                                 result so it's quite fresh out of the                                 oven so but the answer would be a be                                 testing in that case so currently we try                                 to ib test against the current                                 recommender but in general the idea is                                 to build the system that keeps                                 constantly monitoring the performance of                                 that so then it's also can trigger the                                 training of a new model when it's needed                                 hi one question so tensorflow serving is                                 now out when version                                              something so what's your if you look                                 back having it in production for real                                 you case with a lot of data how is it                                 working how it's monitoring capabilities                                 how stable is it so with part of the                                 item in benning's which will calculate                                 from Kafka constant it's in production I                                 think already for three or four months s                                 the other part took us a bit more time                                 to bring to production I didn't show any                                 problems I keep monitoring the                                 performance a quite good sum from two                                 until three milliseconds for making me a                                 prediction on the                                                   problems that I do know this tensor                                 serving is that you can submit sort of                                 for example                                                           the same time this will be fine but if                                 you will try to submit thousands of them                                 then terms of serving start to be a                                  problem and this is for example why                                  Vespa decided to sort of rewrite their                                  own terms of serving alright thanks so a                                  question about rank net so the two                                  inputs is one the embedding of the item                                  and embed embedding of the user so and                                  then rank rank that is kind of black box                                  which could learn any function but                                  further you use as assumption that the                                  Euclidean distance is like a good thing                                  to filter for so how the question is how                                  do you make sure that this assumption is                                  is true okay I will try to answer it                                  because as I mentioned I am NOT coming                                  more from the data science part but                                  first of all the rank net is more using                                  a sinc function similar to logistic                                  regression some sigmoid to do the                                  classification with some negative                                  sampling as this one the second thing we                                  we try to punish the training of the                                  model between the item embeddings and                                  the user embeddings i think is using                                  causing distance so he tried to do some                                  sort of punishment over there in order                                  to force the item embeddings and the                                  user embeddings to be in the same space                                  so this is why we believe that the                                  collision distance can be a good enough                                  approximation to find close items we                                  also tried different methods                                  to do the evaluation and if you thought                                  actually that collinear distance works                                  quite well this combination with the                                  rent network but yeah user preferences                                  are dynamically changing the source are                                  quite stochastic the item embedding are                                  quite static so it's it's based on the                                  same attributes rights of the car but                                  from one point we try to bring it into                                  the same space from another point a car                                  born to be read but user have some                                  preferences which is not always read so                                  but still the performance was quite well                                  thanks last question yep I was curious                                  if there is an advantage to rank net                                  versus a gradient boosting method if you                                  looked at that so I do you know what                                  really much understand the second one                                  this probably because I don't know it I                                  can ask one of the data scientist and                                  tell you I am purely in that engineering                                  so no more question last one how do you                                  know that you profile the same users you                                  don't have to login into your site to                                  perform the search and since users don't                                  have to outright authenticate how do you                                  distinguish different users from each                                  other it's a kind of a basic questions                                  but I'm curious how you do it GTP are                                  related question not sure I can answer                                  it was that lawyer but we just use the                                  cookie ID and for those that are login                                  it's cool but most of our users actually                                  not login so it's just the best effort                                  that we can do it could be situation                                  that you are coming at mobile and                                  tomorrow you'll be on a desktop and you                                  will see you as two different users so                                  we try to do the best as we can in you                                  know in the limits of the law                                  okay so thank you                                  and you can enjoy your lunch next door                                  [Applause]                                  [Music]
YouTube URL: https://www.youtube.com/watch?v=C6gB4ROjTXI


