Title: Berlin Buzzwords 2018: Fabian Hueske – Leverage the power and simplicity of SQL on Apache Flink
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Fabian Hueske talking about "Why and how to leverage the power and simplicity of SQL on Apache Flink".

SQL is the lingua franca of data processing and everybody working with data knows SQL. Apache Flink provides SQL support for querying and processing batch and streaming data. Flink’s SQL support powers large-scale production systems at Alibaba, Huawei, and Uber. Based on Flink SQL, these companies have built systems for their internal users as well as publicly offered services for paying customers. In my talk, I will discuss why you should and how you can (not being Alibaba or Uber) leverage the simplicity and power of SQL on Flink.

I will start exploring the use cases that Flink SQL was designed for and present real-world problems that it can solve. In particular, I'll explain why unified batch and stream processing is important and what it means to run SQL queries on streams of data. After discussing why and when you should use Flink SQL, I will show how to leverage its full potential. 

The Flink community is developing a service that integrates a query interface, (external) table catalogs, and result serving functionality for static, appending, and updating result sets. I will discuss the design and features of this query service and how it will enable exploratory batch and streaming queries, ETL pipelines, and live updating query results that serve applications, such as real-time dashboards. The talk concludes with a brief demo of a client running queries against the service.

Read more:
https://2018.berlinbuzzwords.de/18/session/why-and-how-leverage-power-and-simplicity-sql-apache-flink

About Fabian Hueske:
https://2018.berlinbuzzwords.de/users/fabian-hueske

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah thank you thanks for showing up for                               the last talk of the conference yeah so                               I was already introduced here with the                               title of the talk this talks about why                               you should run sequel on flink and it's                               how you can do that a few words about                               myself I'm a PMC member of a petra fling                               started basically started building up                               this project since day one since -                                enough years working on the                                relationality ISIF link I'm also trying                                to write a book about stream processing                                with the Petra flink it's going so and                                so working progress still and I'm also a                                co-founder of data artisans which is a                                startup founded by the founders of                                original creators of a Petra flink and                                we're providing the the a platform which                                is a bundle of open source Apache flink                                and the application manager and the                                platform basically integrates fling with                                and puts it into into context they can                                use it where it eases the operation                                operations of streaming applications so                                it's integrated with logging metrics CAC                                ICD it uses kubernetes to to deploy                                flink applications and the application                                manager takes care of the managing the                                lifecycle of applications so you can                                stop an application you can resume it                                you can scale it out scale it and you                                can update the code microwave                                gamification to another cluster and all                                these things so for who has heard about                                the flash effective link or nose or so                                most of you so I guess I can cut this                                rather short so Apache flank is yeah a                                stateful stream processor it covers the                                full bandwidth of streaming applications                                starting on the batch side so a                                processing like finite streams so to say                                going to traditional stream processing                                but also to the edge of event-driven                                applications where your applications                                that consume events and perform certain                                computations and react on them                                flink has a couple of nice features it's                                as I said it can process real time and                                historic data streams it is a true                                processor with very low latency it's                                photo and so you can get exactly ones                                semantics for your state it does lots of                                a memory processing it features event                                time processing so you can really have                                the nice semantics in your applications                                and it scales very well so people use it                                at very very large in very large                                deployments so a few of the users here                                are fairly big big enterprises so we                                have for instance Netflix which uses                                fling to process three trillion events                                per day has jobs in production with                                   terabytes of state there's also Alibaba                                who built a stream processing platform                                based on based on flink ing is using it                                for a fraud detection and they are also                                a couple more users or so this is just a                                selection of where flink runs in                                production flink has a couple of                                programming api's that you can use which                                are kind of layout so you can start at                                the bottom end on the on the process                                function level which is an interface                                that gives you access to to control time                                and state so which are like the basic                                basic building blocks for building                                streaming streaming applications on top                                of that is the data stream API which                                gives you nice nice shortcuts for very                                common stream processing operations like                                windows window windows for instance or                                also doing a synchronous calls against                                external data stores and                                                are the high level API is for for                                streaming analytics sequel and on the                                table API and this is also where this                                talk is is about                                so if link has these two types of                                relational api's there is and                                sequel so standard sequel we're not                                using some kind of work streaming fight                                sequel here so this is a very simple                                query which just groups clicks clicks                                tabla per user and computes account and                                then there is the table API which is a                                links that API link stands for language                                integrated query language and here you                                basically embed the query in your                                application so you see you have some                                 kind of a table environment here on                                 which you can call them at that scan and                                 then you say group I select and this way                                 you basically build up a query that is                                 here is doing exactly the same as the                                 secret theory                                 so both api's are unified api's for                                 veteran stream processing for veteran                                 streaming data and this means that a                                 query will speci                                 or a query specifies exactly the same                                 result regardless whether you run the                                 query on on batch data on a file for                                 instance or whether you run it on an all                                 streaming data such as a Kafka topic                                 Curie's are translated in this flow so                                 we integrated the translation of the                                 table API and seeker queries into one                                 flow so both query representations are                                 translated into a logical logical                                 execution or logical plan we're using                                 care set for the secret passing and also                                 for the logical teammate for the                                 optimization so secret furious end table                                 API queries are translated into a                                 logical plan um then we apply                                 optimizations on that again using                                 carrots idea Kerr's optimizer and                                 depending on whether the query is                                 executed in a streaming or in a batch                                 context it is translated into a into a                                 data set if link data set plan which is                                 flings a better API or into a data                                 stream plan for for streaming theories                                 so the bottom left part in that case the                                 data is a spa                                 it's pitched and in the other bottom                                 right part we would create a plan for                                 for streaming data so what happens if we                                 now want to run this very simple query                                 on on our table clicks that is where the                                 table clicks represents a file so in                                 that case we basically read the data                                 from the file we would give all the data                                 somewhat into the query processor and we                                 would get a get a result so the data                                 input data is read once and the result                                 is also produced at once so this is like                                 very very simple the the standard way of                                 evaluating queries in batch data so but                                 what happens if clicks is a stream so                                 here we represent it as a as a stream                                 could be a Kafka topic could also be                                 data in kinases or whatever other stream                                 you you you have and in this case this                                 table of course also has some kind of                                 the schema but the records appear over                                 time and as new records arrive we can                                 evaluate the query and incrementally                                 compute the result of the table so we                                 got a record here for Mary and another                                 one for Bob so both counts here are one                                 if we now get another one for Mary we                                 can increment the counter for Mary to                                 two and if we then get another one for                                 this we add another row for this so in                                 this case the data is continuously read                                 consume continuously interested and the                                 result is continuously updated however                                 in the end the result in both cases is                                 the same so we have the same input data                                 we run the same query and we get the                                 same result so why is it important that                                 this this is this property of stream bed                                 unification so first of all it's of                                 course a usability issue if since flink                                 implements NC sequel syntax there is no                                 no no custom stream sink sequel syntax                                 that anybody would need to learn and                                 there's also no hidden semantics so we                                 and exactly the same semantics as you                                 would expect from from a bad fear                                 processor this also increases the                                 portability of queries because now you                                 can run the same query and bounded and                                 unbounded data but also on recorded data                                 and on real-time data so in case there                                 is some kind of outage here you use stop                                 receiving data or whatever happens you                                 cannot continue processing the query you                                 can take exactly the same query and run                                 it on on the recorded data set again you                                 can also use this feature to bootstrap                                 state you can it's also great feature if                                 you want to explore a data set or design                                 a query on a small sample of batch data                                 and later deploy it on a live stream but                                 how can we actually achieve this the                                 secret semantics when when running                                 theories on streams well that's actually                                 not something that is very new in fact                                 database systems do that for quite some                                 time and the corresponding feature in in                                 database terms are materialized views                                 these this feature on materialized views                                 are kind of similar to regular values                                 but they are persisted in this in memory                                 and whenever the input tables or the                                 base tables of the view definition view                                 definition change they also reflect                                 these updates in the materialized view                                 so if you think about it the updates to                                 the base tables like the the records                                 that you receive by a stream so that                                 those are the updates the view                                 definition query is this trimming theory                                 that you're evaluating and the result is                                 the materialized view so if you think it                                 think about it that way                                 like this this type of incremental query                                 evaluation is not that new at all                                 however in the context of link we're                                 putting it into a distributed stream                                 processor this rooted stay for stream                                 processor this is yeah something that                                 has not been done so off before                                 so in the context of a fling we have                                 this concept of a dynamic terror which                                 is a table that is changing over time                                 and with these dynamic tables you can                                 these these dynamic tables can produce                                 the input as well as well also as well                                 as the output of a query so if you have                                 an input dynamic table you apply a query                                 on it and then the result of this query                                 will be another dynamic table so                                 whenever something changes in the input                                 table these changes will be will be                                 reflected in the output table by having                                 an incremental theory evaluation                                 mechanism so what does it mean for when                                 when running these continuous queries on                                 dynamic tables where can I get a such a                                 dynamic Taylor from well first of all                                 usually you have a stream that you                                 somehow want to conceptually convert                                 into a dynamic table and there is                                 different ways how you can translate a                                 stream into a dynamic table and also                                 back for instance there are append                                 conversions where each record that is                                 sent by the stream is treated as an                                 insert to your dynamic table so whenever                                 you get a new record from the stream you                                 just appended to to your dynamic tail                                 another mode is the absolute conversion                                 where you basically each the the schema                                 of the stream has a certain key                                 attribute and whenever you get a get a                                 record you look up in your dynamic table                                 whether there's already a record with                                 this key and then you update the record                                 and if there is no record for such a key                                 you insert the record so this absurd                                 conversion and finally the most generic                                 one is the change low conversion where                                 for each record you have something like                                 a flag which tells the system hey this                                 is a record that you should insert or                                 this is a record that you should remove                                 from the table so there's no key                                 involved there's simply the notion of                                 hey at this and remove this and this way                                 you can basically it is the most generic                                 so kind of like most expensive way of of                                 treating updates in a table or treating                                 treating up generating tables from a                                 firm stream and but this is very very                                 very generic so what kind of operations                                 does Flickr just fling                                 support in flink                                                    released a couple of weeks ago so                                 there's of course all these simple                                 things like select from where                                 so projection and in filters we also                                 support group and having classes on non                                 windowed on one window to a group by                                 classes but also have these shortcuts to                                 define tumbling hope and search windows                                 in in the group by clause there is a                                 certain subset of joints supported                                 windowed joints where you have a record                                 from one side and you say hey I want to                                 join this with everything that is ten                                 minutes earlier in                                                     the other stream these are these window                                 joints that we support there's also                                 non-winner joints these are joints that                                 typically have to materialize the full                                 table because there's no time time                                 constraint on the table on time                                 constraint in the in the joint                                 attributes at John predicates sorry                                 we support quite a quite a few different                                 types of user-defined functions so you                                 can also plug in your custom custom                                 logic into sequel queries we support                                 scalar functions aggregation functions                                 and also table valued functions and in                                 the last release we added CLI client to                                 to play around with the API to submit                                 theories to to to fling cluster there's                                 a few few operations that are only                                 supported in either on either streaming                                 data or batch data such as the set                                 operations which are only supported in                                 batch at the moment and over windows for                                 for streaming data so what can you build                                 with these tools at hand or with this                                 feature set of of sequel                                 well first of all you                                 can build of course a simple ETL                                 low-latency TLO data pipelines where you                                 ingest data transform it aggregated may                                 be filter it and then pipe data from one                                 stream into another or you can ingest                                 data from a stream right into a                                 distributed file system or into a                                 database system you can also use it to                                 to run stream a batch analytics both on                                 historic data but also live data using                                 the same query and you can also use                                 these queries to power our live - BOTS                                 so you basically define a query on a                                 stream that generates basically a                                 materialized view which is live updated                                 as theta of on the stream arrives and                                 then have a dashboard viewing this table                                 and visualizing the data so I'm going to                                 demonstrate here like a few queries                                 using this new si Lang client and the                                 data set that we're using is the New                                 York right take New York City Taxi                                 rights dataset we we stripped it down a                                 bit so in our case we only have five                                 attributes which is an ID for a ride                                 it's a start whether this is an event                                 that represents the start of a ride or                                 an end of a ride                                 we also have the longitude and latitude                                 values for where this event happened in                                 case of a start event this is where the                                 passengers entered the taxi and in case                                 of the aunt event this is where they                                 left the taxi and then there's also of                                 course the time attribute for when this                                 event happened so if we now would like                                 to have this this use case here                                 basically we would like to compute for                                 for every location every five minutes                                 the number of taxi rides that that that                                 departed and arrived at a certain                                 location within the last                                                 this is a classical hopping window or                                 slightly also swoops oh it's a sliding                                 window this is defined here in the group                                 by clause                                 where you say hop row time because we                                 interested in the timestamp here we say                                 interval five minutes so we want to                                 compute every something every five                                 minutes over the last fifteen minutes we                                 also interested in the number of                                 departing and arriving taxes so that's                                 why we put the is start attribute flag                                 odds in the group by clause and we are                                 grouping on a cell which we computed                                 using a user-defined function - cell ID                                 which takes the longitude and latitude                                 converts that into a basically in a                                 discretized grid and using that as a                                 grouping key because if it would                                 obviously a group on longitude and                                 latitude we would not get very                                 meaningful results here so so we're                                 grouping on the on the only area here on                                 the start and on the time using the                                 hopping winner definition and then we                                 say simply we also select these fields                                 and add a current aggregation that then                                 basically counts how many taxes arrived                                 or departed at each location within the                                 last every five minutes in the last                                    minutes another use case could be two to                                 join the start and end rights on the on                                 the right ad and then compute the                                 average right duration per pick up                                 location so basic for each location we                                 want to we want to know how long does it                                 take the read last in average when when                                 it starts at this location and the query                                 here would join these two sub queries                                 again we're discretizing the                                 longitudinal attitudes to two cells here                                 in this case we're filtering on a start                                 so we get all the start events and we                                 join that with all the events where is                                 that is false so these are all the end                                 events are joining to start events with                                 the end events and the interesting part                                 is the join clause the joint predicates                                 first we turn on the right ID and then                                 we have an have we bound the time on                                 which we want to join them here where                                 they start time the end time should                                 between the start time and the start                                 time plus one hour so we only joining we                                 are assuming here that it takes the                                 right would not take longer than one who                                 work and finally we can compute the and                                 finally we can compute the average time                                 by computing the diff between the two x                                 times so with these two it's at hand we                                 could of course also ingest data from                                 from Kafka right into an elastic search                                 and then use for instance Cabana to just                                 visualize the data because the query                                 takes care that in elastic search we                                 always get get the data aggregated in                                 the in the right way so how can you use                                 it well unfortunately until until                                 recently all these secret theories had                                 to be embedded in Java or Scala code so                                 there was no way to simply send a                                 secretive link and let it run the run                                 this query you basically had to                                 implement a Java class or a Scala class                                 and then define your query in this class                                 send it to a fling cluster for execution                                 however this also means that or it                                 doesn't does not occur automatically                                 mean but the nice thing about this is                                 that the table API and sequel are                                 tightly integrated with the data stream                                 a data set API so whatever other                                 libraries you're using can be can be                                 used together together with a sequel or                                 or all the table API for instance on the                                 on the better side you could use the                                 sequel API to four-for-four ETL to get                                 the data in the right shape and then                                 apply a djeli job on this data                                 Delia strings graph processing library                                 and on the streaming side you could                                 first run a certain pattern using fling                                 CP library and then evaluate the result                                 with seeker however since since flink                                                                                                        more on making the API or easier to                                 easier                                 suppose exposing the api's to two users                                 in a more friendly way so we were no                                 working also in catalog services on                                 better let us offer support for table                                 sauce and table things and this CLI                                 client that I'm going to show you is not                                 the first first version of having having                                 it to where you can simply use fling to                                 analyze your streaming a batch data all                                 right so this is no demo charm so we can                                 you read that or should I increase the                                 font size later okay just to be sure                                 alright so so what I did here is                                 basically I started a few docker                                 containers a cough car a fling cluster                                 and also this fling seal a client and in                                 the background there is a threat that                                 pushes data into into a Kafka topic so                                 we have here exactly the same table that                                 I that I've shown you before                                 so this taxi rights table with these                                 attributes right ID is that attribute                                 longitude latitude in row time and we                                 can simply spirit that with the most                                 simple theory checking out what's in                                 there and                                 so this is now the data that's as it                                 basically flows into the into the cuff                                 cut topic and here in the flink flink                                 web UI we also see that this query is no                                 no no running this is the operator that                                 ingests the data from Kafka and this is                                 the thing that sends it back to the to                                 the sink to the CLI client so we can                                 also do a little bit more fancy stuff                                 [Music]                                 simply aggregating the data no Priscilla                                 D is basically something similar as                                 we've done before so we say again to                                 silletti                                 to discretize the data say count star                                 from taxi rides and group by to several                                 ID longitude and latitude and now we see                                 these are the IDS of the cells and now                                 we see how we incrementally compute the                                 count and again there was there was a                                 query started here something that looked                                 a little bit more complex here we have                                 the source again reading the data this                                 is a Hef's partitioning to send the data                                 to the right grouping operator a group I                                 that computes the count and then again                                 the sync which sends the data back to                                 the client                                 all right so that's a nice tie but can                                 you use it for anything serious well                                 obviously not this is just a sea light                                 line for for playing around you can use                                 it to to look into the data in your                                 streams but not much more                                 therefore the fling community started                                 this what we call flip flink improvement                                 proposal for query service and we                                 envisioned this to be a rest service                                 where you could submit queries to and                                 submit and manage secret theories so                                 select theories theories that directly                                 write into a new engine into a new table                                 using insert into select and it should                                 also be able to serve the results of a                                 secret theory back and this is actually                                 basically really difficult on difficult                                 parts here start it should also                                 integrate with turbo catalogs like                                 educator log or schema registries and                                 the use cases for for such a service                                 that way you can basically send Cebu                                 queries to using rest and then either                                 directly receive the data or right into                                 write it into a enter into another                                 storage system would be either data                                 exploration using notebooks like Apache                                 a Zeppelin you can get real-time access                                 to data in your application and also be                                 able to easily route data from one one                                 topic to another or have an easy way to                                 define ETL ETL pipelines the challenge                                 here is basically the the serving of                                 these dynamic tablets the tablets that                                 are dynamically updating right so                                 because unbounded input also means                                 unbothered results and whereas in the in                                 the batch case serving bounded results                                 is not that hard right you have the                                 results you send it back and then you're                                 done of course this can also be very                                 large but in principle if you're if                                 you're working on a stream they are they                                 actually unbounded and depending on the                                 query returning results can be either                                 very hard or not                                 that hard if you look at the career on                                 the left-hand side which is a simple                                 simple select from where query with a                                 simple filter this query if applied on                                 an on a stream with an append table what                                 result also in an append result table                                 right so you would only get new records                                 appended to the result and in this case                                 the results rows that you've emitted                                 will never change and therefore you can                                 the the challenges in basically                                 buffering the records until they are                                 consumed from the read from the consumer                                 whereas in the other hand side like this                                 is the query that we had in our example                                 a group a query with a congregation we                                 have a table that is continuously                                 updating and in this case we somehow                                 need to be able to tell the consumer                                 downstream that that the results did                                 change right and the result of it also                                 needs to be maintained somewhere so                                 serving these results in the online                                 streaming theories on continuous queries                                 is a little bit difficult so the design                                 that we envision for this query service                                 looks looks like this so we have an                                 application which uses rest to                                 communicate with the query servers which                                 is in the middle the crew service again                                 has a rest interface it has a catalog to                                 to which it that can be connected to an                                 external catalog it also has obviously                                 the optimizer to to optimize Nick Fury                                 and a component that we call here result                                 server when you have a query the                                 application would submit the query via                                 rest today to the queue service the                                 optimizer would then compile the query                                 into a streaming job submitted with the                                 Flint cluster it would start the query                                 on the fling cluster this is typically                                 this is a stateful Kaveri stateful                                 stifled screaming drop so it would read                                 the data from an event log or a database                                 or ever and then write the data again                                 into an event log or database and from                                 there on the curio service which fetch                                 it and serve it back to the application                                 so in this case all the results are                                 served by arrest so this is a very nice                                 property if you're in certain cluster                                 environments however in this case all                                 the data flows through the results                                 server so this might in this case the                                 query service might become a bottleneck                                 right depending on how many queries you                                 start or run reading all the data or the                                 result data from a Kafka topic or from a                                 database and serving it back to an                                 application might not work with so well                                 because of that we also thought about a                                 solution to this to this problem again                                 we could submit a query to the query                                 service the query service submits the                                 job to the fling cluster on the                                 left-hand side the data again flows in                                 some kind of a storage system but                                 instead of returning the result directly                                 from the query service we return a                                 certain result handle and then there is                                 a serving life serving library in the                                 application that could then connect to                                 to the storage system and fetch the data                                 from there and serve it directly within                                 the application so all of these design                                 decisions are not final yet so if you                                 have have a good idea or a certain                                 feature that you would like to have for                                 security service let us know as I said                                 the effort is called flip                                              happy to hear your your ideas about this                                 feature                                 so to summarize unifying pattern stream                                 processing is important for our for                                 various reasons first of all being able                                 to use the same query on streaming and                                 batch data makes it makes it very                                 portable you can just if something goes                                 wrong run it on on historic batch data                                 as well                                 fling sequel solves many of the                                 streaming and bad use cases                                 it runs in production at Alibaba uber                                 and in others so these companies build                                 internal platforms which are powered by                                 flink sequel and the community is                                 working currently working on improving                                 the the user facing side of these of                                 these features                                 I'd also like to mention there is the                                 Fink forward conference happening in                                 September begin of September                                 exactly here at Cooper aye if you're                                 interested in that sign up early early                                 bird prices are still available until                                 June                                                                thank you                                 small promotion of my book here as well                                 you can get it on early release of                                 O'Reilly already and yeah thank you all                                 the questions let's take them                                 hi from what I understand when you do                                 like grouping by something right and                                 then you need to maintain estates of                                 every aggregate so some aggregates like                                 averaging to maintain like intermediary                                 aggregates like denominator and                                 numerator basically can you access those                                 as well as a user what kind of a so                                 let's say that you're calculating like                                 arithmetic mean yeah so you from I                                 understand you have to maintain like                                 intermediary never for the denominator                                 and numerator right can you access those                                 as the user as well you can you can                                 certainly do that it's not very                                 efficient let's put it that way so the                                 way this works in in in in flink it                                 exactly as I said you have to maintain                                 all the state and since this is an                                 incremental incremental operation where                                 you can also potentially have have need                                 to remove something from an aggregation                                 again you need to maintain and basically                                 all the values that is true like for for                                 if you want to want to compute the mean                                 you have to put all the values                                 interstate this isn't very expensive                                 operation in this case yes but it's it's                                 possible you can do that but it's                                 consumes a lot of state                                 hi for the user interfacing have you                                 given any thoughts of having a driver                                 the yes we thought about that the                                 problem with the JDBC or having Mike                                 so the problem JDBC is that it works                                 better for for bounded datasets right as                                 soon as you have to update the result                                 this doesn't work so well anymore also I                                 mean undaunted returning unlike a table                                 that is unbounded upend might work                                 because you just have an infinite                                 results that we can just fetch data from                                 that would certainly work that that we                                 could do but it would fail for a certain                                 theories that do for instance like this                                 group I user count theory at that point                                 JDBC is not is not working anymore and                                 we would have to come up with something                                 on our own yeah                                 I have one small problem with it which                                 is that I think sequel sucks                                 is it just my I'm at fault or the sequel                                 have fundamental flaws in this world                                 yeah so I mean sequel has its problems                                 yeah that's where we also have the table                                 API so which is like a more embedded way                                 of specifying your your your queries so                                 both api's are basically our equivalent                                 since since we are there they're going                                 through the same translation paths right                                 so we have if you know such a fan of                                 like specifying the queries as a as a                                 string and putting them into an                                 application which I also think it's not                                 the nice way to it to do to do it you                                 can also use as I said a table of the                                 the tabular API and then compose it with                                 methods in an incremental way and when                                 you                                 if we look at the example that I had                                 here it also solves this like what some                                 people made where is it                                 consider as an issue with sequel where                                 you have to read basically did the query                                 from the middle down and then up so                                 parsing the cure is not that simple                                 where's in this case you can basically                                 start from the top you say okay I scan                                 my data I group it under then I project                                 it instead of hey whereas maybe they                                 come from or it comes from the middle                                 and then okay I do a group I and then                                 I'm jump to the top to see what I get                                 yeah kind of following up on the past                                 two questions you showed the sequel                                 interface and the rest interface are you                                 working with any of these tablished                                 players for for dashboards for BI tools                                 not yet so we're not there yet                                 Oh simple might suck but it's a                                 significant market space and yeah no                                 we're not yet the the idea here would be                                 to probably integrate here at that point                                 when you basically return a result                                 handle and then have basically have a                                 have a query that consumes from Kafka                                 writes the data into like a relational                                 database and then when you return the                                 handle you basically say hey this is the                                 table this is the JDBC connection on                                 which you can fetch the data and then a                                 - what solution could use this ready BC                                 connection to interactively run queries                                 on the data that is updated by takfeer                                 why they where the continuous query runs                                 make sense what what I see many                                 applications are now not producing and                                 consuming applications but there are                                 applications that put data inside and                                 then there are others that read so yeah                                 not                                 what has been done by a JDBC both ways                                 is now done decoupled sense to think                                 about how to take any more questions                                 there's one left middle hi what I don't                                 really understand so if fling is fed by                                 a stream of data right and this queries                                 can be submitted at hoc how do you know                                 how much data you have to preserve in                                 flink because it's very different if i'm                                 asking you only about the last hour and                                 you can drop a lot of data out of the                                 state but if the next user asks you like                                 for account since the dawn of the data                                 then you already have lost all of it                                 right yeah so this kind of depends on                                 the on the way that a table would be                                 defined in fling so usually if you're                                 for instance if you take the example of                                 a Kafka topic right the data is in Kafka                                 and then when you when you define the                                 query you can say hey I want this table                                 to be represented by reading from the                                 beginning or you say hey I want to start                                 at the offset from from the current                                 offset so to say if you then the the                                 proper way of restricting your data then                                 if you say okay one with the data from                                 the beginning from the early ons or at                                 least as long as I have it in Kafka the                                 then you don't have to do anything                                 because the table is defined as starting                                 from the current offset so it goes on                                 from there and if you say hey I want                                 only want the first day from from the                                 first record the first day then you                                 basically have to set something like                                 like a predicate where my row time is                                 smaller at this timestamp and then it                                 would automatically it cut it off that                                 that point all right let's take one last                                 question from here thank you for your                                 talk the I honest empathy the nose on                                 the bound bounded streams must be                                 different but                                 the meaning of a window in secret has a                                 very specific meaning which is bounded                                 bound to bounded set and so but now it                                 seems to me that if you use the same                                 term on unbounded streams that the                                 meaning a little bit changes why did you                                 choose to use the same term instead of                                 another word yeah yeah so we we're kinda                                 like distinguishing so you're coming                                 from the from the secret windows that                                 you use like a nova cross right so yeah                                 we we tend to distinguish them by over                                 windows and group by windows basically                                 to specify at which at which place there                                 are used if you if you use like a group                                 by window which is one of these Tumbo                                 hopper session you basically this is a                                 constraint how you group your data so                                 it's like actually a grouping we use                                 like a nova window we also support those                                 or a subset of those then you define the                                 window in the over clause or in the                                 window close and use the reference in                                 the over so we support both of them but                                 yeah on the other side we these this                                 group are windows or a week we call them                                 group a window size or group I witness a                                 library like what people coming from the                                 stream processing space usually                                 understand as windows so it's kind of                                 like a like a trade-off like to get both                                 communities somehow in on the same                                 semantics and that's why we call them                                 the one windows group I witness and the                                 others over and us hope that makes sense                                 [Applause]                                 [Music]
YouTube URL: https://www.youtube.com/watch?v=qo7tKGuib8o


