Title: Berlin Buzzwords 2018: Michael Stockerl â€“ Ship your Machine Learning Application #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	A classifier labeling Van Gogh drawings as invoices and a chatbot insulting users on Twitter are only two examples of Machine Learning (ML) models, which went wild as soon as they hit production. Although evaluated on a test set, in the face of unseen data machine learning models oftentimes behave in an unpredictable way. Depending on the application, such a model may lead to decreasing revenue, bad reputation or even a threat to the health of people.

To ensure a stable rollout of new models into production, we have to promote machine learning models to first class citiziens in the Continuous Delivery pipeline. Kubernetes and Apache Kafka are two great tools to support the rollout of new machine learning models in a (semi-) automated way. I will show a pipeline built with these tools, which will lead to more confidence in your deployments and happier users.

Read more:
https://2018.berlinbuzzwords.de/18/session/ship-your-machine-learning-application

About Michael Stockerl:
https://2018.berlinbuzzwords.de/users/michael-stockerl

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi so my name is Mitch token and I'm                               here to tell you how we screwed up                               basically two years ago and what we                               learned out of it I was working there                               back there in a company and it's                               basically the biggest webpage of Germany                               and we had a simple task we should build                               a recommender system for our webpage and                               we thought yay cool let's try out all                                the cool new technology and build a                                recommender with the amount of theta we                                have and we came up with one and after                                two weeks implementation we delivered                                our first recommender with an a/b test                                everything looked good we even had a                                like a increase time on the page of                                    and the people were clicking like crazy                                on our links so we went to all my                                naturist and told them ok release it we                                need it now we are losing money when we                                don't do it however after releasing it                                some friends of us we're asking us ok                                what are you doing there it's totally                                 and you're only showing porn stuff                                wave form and there was real content                                now it's porn and we were totally under                                estimating the uses with our user base                                to recommender another story is I was                                working for another company where we had                                a task classifying documents so when                                there is an invoice we should classify                                it as an invoice if there's a contract                                which were classified as a contract and                                so on and we basically had only one big                                customer and this customer was their                                German mail delivery and the CEO of this                                German mail delivery was testing our                                system and we prepared like crazy                                shipped a new model right before their                                milestone and then their user again the                                CEO did what we didn't expect it it used                                it in a way that we wouldn't assume and                                basically he uploaded some drawings of                                Franco to our system and we were                                classifying everything as an invoice                                because our                                in learning algorithm basically ever                                fitted to invoices because we wanted to                                make sure that the invoice case works so                                the managers again we're not super happy                                with us                                in both cases the brawler was basically                                our test data did not really match our                                life data so we had our sumption x' in                                our test set which basically the user                                don't care about and we have to make                                sure that the live data is tested and                                not all tested itself another problem                                especially in the first case was that we                                deployed something to production and it                                been there for a while user could make                                some stuff and then it broke totally and                                until we got the real feedback from a                                user that it's broke it was too late for                                us to see ok this was a commit we made                                which really broke our whole system and                                the last problem was that we were not                                able to reproduce what we did before we                                deploy the stuff so basically it was                                cowboy style we hacked a model together                                deployed it and hope the best so we                                totally forgot with which test sets we                                used we forgot which hyper parameters we                                trained and so fixing a barque was                                always redoing it at all all over again                                yeah now I'm at the Nemo tomb and I had                                time to think about ok how could we                                improve the situation for us and also                                for our startups shortly about on                                Dyneema tomb what isn't anymore to stand                                a Titan a claims that we are the                                Stanford of Bavaria basically we are a                                non-profit entrepreneurship Center which                                tries to help startups in every stage to                                become successful so we help them with                                their first idea I'm teaming up until                                venture capital and help them to build                                secure scalable companies                                in addition to that we also work                                together with a lot of big corporates to                                help them to stay competitive and                                innovative for example some of our                                partners our BMW time layer and Audi and                                also in the tech side like Google and                                Facebook are our partners and we have                                some cool projects like Hyperloop we                                build some robot this stuff like that so                                if you're interested in high-tech check                                out on the NEEMO tune so how do we                                 actually release a model with our                                 startups and I will use it as an example                                 of the document classifier as an example                                 to go through the whole process so the                                 most important step in the machine                                 learning pipeline is always a                                 pre-processing so we have to make sure                                 that the data is clean enough and fits                                 to our model we use Kafka to store our                                 training data and we have like a first                                 one which stores the raw data and we                                 process it in two ways like one for                                 training set and one for test set and we                                 named that topics with the git commit of                                 the preprocessor to make sure that we                                 always know which one we use and also                                 that we can't roll back and we don't                                 have clashing names of our topics this                                 works pretty good as long as you don't                                 change anything in the preprocessor if                                 you change the preprocessor everything                                 has to recalculate it the whole pipeline                                 has to go through again so this is kind                                 of one of the bottlenecks we facing                                 right now and so somebody pushes some                                 code to production or near to production                                 but to get and we trained our candidate                                 out of the training data set this is                                 pretty normal                                 our data scientists or engineers can do                                 that also locally so they have access to                                 the training topics so they can build                                 with the real data we using also for the                                 build process locally which makes it                                 much easier for them                                 to feel like they would work in                                 production as soon as we've got our                                 candidate model we use the live model                                 which is like currently life and rerun                                 the whole test again just to make sure                                 that it's like order to have a baseline                                 because our test set is always involving                                 we're using different test sets all the                                 time                                 to make sure that we don't our fit in                                 one direction and so we create another                                 baseline for our new model to see                                 whether it's better or worse than the                                 old one so we just rerun everything                                 again and then we do the same for the                                 new model so we we evaluate the new                                 model and then we compare because we                                 don't really know how good is good in                                 our production system but we assume                                 everything which is better than the old                                 one is really nice to have so when it's                                 better than the old one we go with this                                 one if it's slightly or comparable                                 perhaps we just try it out and when it's                                 worse then we don't even try it out so                                 we go on with that like in the case when                                 it's it's good enough we we tag it in                                 our get history as a good model and we                                 publish it to our object storage as also                                 IBM is one of our partners we're using                                 IBM for that but you can use basically                                 everything like s                                                    that okay until now this is pretty basic                                 and even the research is doing that                                 so the using a training set that using a                                 test set they compare it to see which                                 one is better and this is why basically                                 what we also done before why we want to                                 test our model in production and see how                                 it behaves there for that we built a                                 real service with the model so we                                 decoupled the service from the model                                 itself to make sure that a data                                 scientist can work independently                                 from an app developer and this again                                 works as long as you don't change the P                                 processor but it gives you some kind of                                 in dependencies between the two                                 disciplines so when either the app                                 developer or the data scientist wants to                                 push something we merge the branch we                                 build a docker image                                 pull there the model and then we run                                 basic unit tests and integration tests                                 then we make sure to include some corner                                 cases we expect for example create a van                                 Gogh drawing and to see whether it's                                 always another other document and not an                                 invoice because some corner cases should                                 not happen in production and it's                                 basically a real unit test but most of                                 the time you cannot really test your                                 model with unit tests or integration                                 tests because they behave sometimes                                 strange not always as predicted so you                                 will have a lot of failing tests if you                                 do test everything with unit tests so we                                 just make sure that like the basic                                 functionality stays the same if I would                                 say lat                                 I would make sure that their highest                                 paid ad is always recommended for                                 example when all the tests pass we                                 publish the model to our continual                                 registry again on IBM and then it's                                 accessible for our kubernetes cluster                                 and we simply run a deployment for our                                 service in the kubernetes cluster so the                                 deployment makes sure that there is                                 always three instances running and when                                 we change the model or the service that                                 it's done in a way that the user doesn't                                 notice so it first removes one instance                                 starts a new one see whether it works                                 and goes to the second one and so on and                                 the classification is pretty simple so                                 we have an incoming topic where older                                 data is stored in a pre-processed way                                 then their service itself is just                                 classifying the stuff and I'll put it                                 into a life topic to test a new model we                                 deploy a cannery instance and it                                 basically does the same it just                                 published stuff into a cannery topic so                                 the life talk types will surf the web                                 page and the cannery will be like there                                 is nothing to surf from but both topics                                 basically contain the same information                                 it's just a model with the tablet                                 prediction which is different and to                                 find like examples where we have a                                 classification where we are not sure                                 about we join the two topics and have a                                 spot checker input queue and also like                                 again spot checking is pretty simple we                                 take the examples where the two models                                 cannot agree on a talk type and we show                                 it to the one who deploys or if it's                                 more domain knowledge involved we show                                 it to somebody who can classify the                                 stuff so in this case a human being                                 should tell me if it's an invoice a                                 contract or other and we go through a                                 couple of them it always depends how                                 many documents we need to see a                                 difference between the two models in                                 that way we kind of build up our test                                 data automatically so with every                                 deployment our tests say the test data                                 set grows which is cool but it does not                                 work if you have to comply to some data                                 privacy issues so as long as no user                                 data is involved this can work to build                                 up your test dataset                                 and then it's basically just a decision                                 based on a dashboard so the one who                                 deploys looks at a dashboard and makes                                 an educated guess whether it's better or                                 or not so there's just like a part of it                                 here but we for example money tour                                 what is the precision on the live system                                 based on on user feedback so whenever we                                 see a drop and like a precision of user                                 feedback we might want to do something                                 when we deploy or in beginning of the                                 deployment we also check whether the                                 cannery and the life prediction have the                                 same distribution on a test set because                                 we know the distribution on the tested                                 and it should not really be different on                                 the two models if there is a big                                 difference we better don't deploy the                                 stuff because this was the case with our                                 invoice classify everything as an                                 invoice we would have seen it on a test                                 at distribution already and this is                                 basically the last step these are the                                 classifications of the spot checker and                                 as we taking mainly examples where the                                 two differs you break quickly see which                                 one is better on the tested on their                                 life data and in this case it was pretty                                 safe to deploy the new version although                                 we didn't have like a lot of examples                                 running and then we deploy it                                 we still monitor it because then the new                                 one is in production and all the                                 production metrics applied to this one                                 and like after a couple of hours we                                 assume that it's okay depends                                 always on the traffic so with startups                                 it's most of the time pretty difficult                                 because they don't have too much traffic                                 but you cannot destroy too much                                 startup because they don't have uses                                 anyway not all of them like it's working                                 pretty good with most of their like use                                 cases where we're using it but there are                                 some limitations as I already mentioned                                 when you like change the preprocessor                                 it's kind of difficult to make sure that                                 it's still working that not one of the                                 classifiers is crashing the data privacy                                 might be an issue like in our case it                                 was not a problem because we anonymize                                 every document anyway right in the                                 beginning this was like one requirement                                 of the German mail delivery service so                                 we could store just analyze features and                                 that's good                                 and it basically only works for                                 classification use cases where a human                                 being can tell whether this is this                                 class or that one so we have another                                 application right now it's about                                 predictive maintenance for trucks and we                                 do it based on the canvas data and we                                 want to predict two weeks in advance                                 whether this truck will break down or                                 not and I have no clue about Campos data                                 and when I see it it's basically nothing                                 for me so we cannot really tell based on                                 sample samples whether the                                 classification was right or wrong we can                                 only tell after two weeks and when a                                 truck breaks down after two weeks with                                 some precious load and we didn't know                                 that might be not good for our startup                                 so we're currently working on that and                                 if you want to have more details there                                 is Anastasia who is basically working on                                 that so ask her if you want to get more                                 information about this use case and                                 that's all I wanted to tell you like if                                 you have more question come afterwards                                 or reach me out on email or Twitter                                 thank you                                 [Applause]                                 [Music]                                 other questions I I'm just curious about                                 if any what effect type arrives what                                 happens to this system like in case when                                 contract invoices and maybe some other                                 artifact type arrives so we changed the                                 entire processing the thing and I mean                                 that entire pipeline changes                                 you mean when we introduce you classes                                 or like what happens when we introduce                                 new classes or do you mean something                                 else yeah maybe                                 okay when we introduce new classes                                 basically we start all over again                                 because most of the others documents                                 might be one of these documents and if                                 we train with like other is this type of                                 document it will have some British                                 precision problems so at least we try to                                 find there the classes in the other                                 documents and we run like create all the                                 topics new                                 and what's the same with their with the                                 web page where we build a recommender we                                 had to classify whether content is good                                 or not and first we trained with three                                 classes like good bad and medium and                                 then our product owner wanted to have                                 good media and bad and delete and it's                                 really hard to like strange stuff if                                 everything is marked is bad                                 if there's a new delete class perhaps                                 there is some overlapping stuff going on                                 there                                 last question cool thank you thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=16oIh0fAs0Q


