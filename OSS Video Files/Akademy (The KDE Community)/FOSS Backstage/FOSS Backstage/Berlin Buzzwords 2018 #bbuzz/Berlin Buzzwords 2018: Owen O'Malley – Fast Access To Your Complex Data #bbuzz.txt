Title: Berlin Buzzwords 2018: Owen O'Malley – Fast Access To Your Complex Data #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Owen O'Malley talking about "Fast Access To Your Complex Data - Avro, JSON, ORC, and Parquet".

The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Understanding your use of the data is critical for picking the format. Depending on your use case, the different formats perform very differently. Although you can use a hammer to drive a screw, it isn’t fast or easy to do so.

The use cases that we’ve examined are:
- Reading all of the columns
- Reading a few of the columns
- Filtering using a filter predicate
- Writing the data

While previous work has compared the size and speed from Apache Hive, this presentation will present benchmarks from Apache Spark including the new work that radically improves the performance of Spark on ORC. This presentation will also include tips and suggestions to optimize the performance of your application while reading and writing the data.

Read more:
https://2018.berlinbuzzwords.de/18/session/fast-access-your-complex-data-avro-json-orc-and-parquet

About Owen O'Malley:
https://2018.berlinbuzzwords.de/users/owen-omalley

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so am i I've worked on Hadoop for longer                               than I should admit but I was the tech                               lead for MapReduce I worked on adding                               security and more recently I've been                               working on hive and orc I've worked on a                               lot of the different file formats I've                               done work on sequence file our C file                               orc file of course T file does anyone                               actually know what T file is aha C it's                                buried in the Hadoop source code and                                it's not very often used although they                                just put in a thing for log aggregation                                that uses it and so I had one of our                                engineers come on and look me up because                                he I was the only name on the design doc                                that he recognized and he was trying to                                ask questions from something that I                                touched like                                                           exciting and if I worked on the Avro                                requirements back when Doug was getting                                started with that so what was the goal                                for this I had previously done a                                benchmark comparing the different file                                formats in terms of how I've used them                                but we've been doing a lot of work                                actually one of my co-workers named Dong                                Joon has done a lot of work speeding up                                the spark access for orc and so I wanted                                to take the previous benchmarks and                                extend them so that they would work with                                spark and test it the way that spark                                accesses your data because that's always                                been part of Hadoop's central goal and                                promise is that once you put your data                                into the system you can access it the                                way that makes the most sense for you                                and so testing how different people are                                accessing the data can help all the file                                formats get better this really was a                                science experiment right we didn't know                                what we didn't know and so we wanted to                                find out not only what performs the best                                but also where are they failing what can                                do better and so on I wanted to use real                                and diverse datasets                                I've seen some benchmarks before that                                either use t PCBs which is a very common                                data set and very well known but it's                                all synthetic data which leads to some                                insane properties of the data that can                                mess up your benchmarks and so having                                real data was important to me and I                                wanted the benchmarks to be open sourced                                and in a public place so that anyone                                could come in with feedback or                                suggestions and see exactly what what it                                does okay so I wanted to talk about the                                file formats a little bit                                Avro was designed by Doug cutting it's a                                cross language file format for Hadoop                                one of its early it was the first one to                                really do schema evolution in a big way                                and so that really was one of its                                defining characteristics the schema was                                a great read from the data                                unlike protobuf or thrift which was                                because it was originally designed as a                                file format it really wasn't designed as                                a messaging or RPC level interface but                                even though that's how it's typically                                used now and it's a row major format                                which means each row is written out                                together                                Avro by the way does anyone know what it                                what the name came from it was the name                                of an airplane company that Doug son                                liked actually of course the most                                popular one of his projects Hadoop got                                its name because of that was the name                                his son's name for a stuffed elephant                                Jason of course is incredibly common                                it's a serialization format for HTTP in                                JavaScript it's a text format with many                                many parsers the schema is completely                                integrated with the data so you                                basically if someone gives you a JSON                                document you basically have to read the                                whole thing in order to figure out what                                types are in there it's a row major                                format each rows are in together and                                usually compression is put in on top or                                core or C was originally started as part                                of five                                to replace our C file and now it's a                                top-level project this schema is                                segregated into the footer so you just                                have one copy of the schema it's a                                column major format so that you can read                                individual columns without reading the                                 other columns we'll see why that matters                                 a lot in a little bit and it's got a                                 rich type type model and it's stored                                 top-down it's got two integrated                                 compression index and stats so part of                                 what our C file did in particular was it                                 treated each of the columns as a blob                                 that made sense originally but it meant                                 that you couldn't do anything                                 higher-level you couldn't have any                                 understanding of what the data was in                                 the file without a lot of outside                                 information that turned out to be a                                 serious mistake and that's a lot of what                                 we're trying to fix while we were                                 designing or C the guys at Twitter were                                 designing park' and they based it on                                 Google's Dremel paper which we looked at                                 - they also segregated the schema into                                 the footer                                 it's a col major format they have a much                                 simpler type model where you don't                                 represent the types as precisely oh you                                 wanted to change the mics and they push                                 all the data down to the leaves excuse                                 me well we have some technical changes                                 here                                 whoops                                 okay so all the data was pushed to the                                 leaves of the tree so what that meant                                 was that intermediate columns if you                                 have structures or a list of struts in                                 the middle of your data all that                                 information is pushed to the leaves                                 which means that you gain some                                 advantages but it means there's some                                 duplication there okay so what data sets                                 did we look at the first one was the New                                 York taxi data set                                 every time someone takes a taxi in new                                 york new york publishes a data set with                                 a row for each ride they tell you where                                 they got picked up where they get                                 dropped off what time it was                                 and how much they paid in fares and tip                                 so that really awesome data set actually                                 there's some great data analytics that                                 was done at that URL and it had some                                 really interesting characteristics like                                 even though the data is totally                                 anonymous there's nothing in there about                                 people they you can tell who some people                                 are because when they get dropped off at                                 a house out and not in the city because                                 the city is pretty has a lot of people                                 overlapping but when they go out to the                                 suburbs then you can tell who someone is                                 and so you can pick out individual                                 writers and you can see a lot of                                 patterns so we've pulled                                                 out of that now the next one is                                 unfortunately generated data but it's                                 based on a customer that I've worked                                 with a lot and I used their real schema                                 and it's because there really wasn't                                 anything that I could find that was                                 public that matched the same kind of                                 sales data we use the properties from                                 their real data to generate the random                                 data that we used for this there's a                                 little bit of structures mostly                                 timestamp strings Long's billions and we                                 picked arbitrarily                                                    kind of match you to other data sets the                                 third one that's kind of fun is the                                 github blog archives github makes                                 available to Google and then at Google                                 Howe                                 the data where you get one row for every                                 public action on a public repo so you                                 can see oh oh and did a commit to or                                 core only did a commits on hive that day                                 and there'll be a roof to show it now                                 this data is insanely complex it's got                                 seven hundred and four columns with a                                 lot of structures and a lot of nulls and                                 just a half month of data still gives                                 you ten million rows now one of the                                 problems with that is the standard one                                 actually that comes up a lot in                                 companies of github doesn't really                                 provide the schema for that data so I                                 pulled a few million rows and I was like                                 what types are in here and so we ended                                 up writing a tool that couldn't go                                 through the Jason and discover the                                 schema and that's what we used for this                                 at first I was gonna put the schema on                                 the slide but then the schema is huge                                 and so that was not a good plan okay so                                 which versions of the software did I                                 want to test you spark two three one I                                 used Avro one eight two I tried to use                                                                                                          problem with using the newer version of                                 Avro I used orc                                                       all the way around it was part K not                                 Avro yeah it was okay and then Sparky                                 Avro four zero zero now it's really easy                                 to say that in the slide getting it to                                 actually work and build a single jar                                 with all those things was not fun I was                                 tempted to call it maven help but in                                 reality maven just made it possible to                                 do so it wasn't maven thought it was                                 just the fact that I was trying to                                 combine a bunch of different software                                 that have these huge dependency trees                                 and make them all work together okay                                 there were a couple configurations that                                 were really important the first is that                                 orc probably can push down is turned off                                 by default in spark and so you need to                                 turn that to true and dungeons work                                 actually                                 hasn't been set as the default yet and                                 so you need to set native equal to true                                 otherwise you'll get the older                                 implementation that goes through hive                                 input format finally to get Avro to work                                 you need to set a config but not in the                                 spark config but in the associated                                 Hadoop config to tell the Avro Reader to                                 not ignore files that don't end in Avro                                 unfortunately when I tried the oh sorry                                 let me back up sorry the benchmark uses                                 sparks sequels file format interface                                 because that provided all the                                 functionality we needed Jason or canned                                 park' are all in spark that's like great                                 this is exactly what I need                                 afro didn't have one but then data                                 breaks the people who work on spark made                                 one available awesome unfortunately when                                 you tried to run it you get the plane                                 crash and it doesn't support all the                                 spark types basically Avro doesn't have                                 a time stamp field or a decimal field                                 and so hive uses in                                                  sets a flag saying oh this is actually                                 timestamp or this is actually a decimal                                 and the Avro spark reader didn't handle                                 it so it crashed so ignoring Avro for                                 the rest of it oh and I actually talked                                 about it a little bit so first I wanted                                 to go through and just generate the data                                 and why does the data size matter well                                 because you're still storing this right                                 you've got three copies of each of your                                 data files in Hadoop and when Facebook                                 moved from HDFS or moved from our C file                                 to ork they saved I think it was                                     terabytes and so they were mentioning a                                 lot of servers because they suddenly                                 didn't need them anymore the it's also a                                 big factor in in the read speed HDFS                                 read speeds are typically about                                    megabytes a second in the real cluster                                 the HDFS guys like the clay claim it's                                                                                        and it's not that only works if you've                                 got a completely empty cluster that's                                 only running your benchmark which I                                 don't know how many of you have empty                                 clusters if you do come talk to me I can                                 get you some workload to run on them but                                 that so                                                           realistic number or can park a both used                                 run length encoding in dictionaries and                                 all the formats have general compression                                 with general compression you have a                                 trade-off                                 there's Z lib which gives you tighter                                 compression but it's slower and snappy                                 that is some compression is faster okay                                 so here we've got the different file                                 choices by the way if you're ever doing                                 benchmarking and someone talks you into                                 doing a matrix it's a really bad idea                                 because all it takes is for file formats                                 across three data sets and then across                                 three different compression formats to                                 make a really complicated chart and a                                 lot of benchmarking so you can see the                                 jason with no compression was the                                 absolute worst parquet was e lib did the                                 best and the other ones are in the                                 middle so don't use Jason I had one                                 someone once tell me I don't know who                                 this Jason guy is but clearly he's a bad                                 guy you should be using the snappier z                                 lib compression Avro has a small                                 compression window which hurts and                                 parchesi lib is is the smallest okay on                                 sales on the other hand we got a very                                 different picture Jason is still bad see                                 bad Jason don't store your data in Jason                                 and we'll really see that when we get to                                 the read speed instead of just the sizes                                 orc did the best park' did the next best                                 and then Avril is at the back just                                 before Jason this is actually musing                                 because the customer that the scheme was                                 based on uses orc extensively and so                                 it's good                                 that it works on their test case                                 although it's probably a feedback loop                                 or we fix the things that don't work                                 well for them so what happened here we                                 had a lot of columns with small                                 cardinality so we got dictionaries lots                                 of timestamp columns or it worked as                                 well and doubles doubles actually turned                                 out to not encode is tightly with ork                                 partially because we didn't run Lincoln                                 code                                 although we're looking at that for the                                 next version of the format and also                                 because our performance engineer detuned                                 them so that they don't gzip is hard                                 either because he realized he could make                                 the whole thing faster if he detuned the                                 Zeeland compression for doubles and                                 finally for github this one we get some                                 interesting results Jason none is still                                 huge but look at the best one that's                                 average Aysen                                 or Avril with Z live inland Jason with Z                                 live is just after it followed by orc                                 wizzy lib so the reason that that                                 happened is basically what happens in                                 this file or this dataset is you have so                                 many columns the columnar compression                                 actually doesn't work very well because                                 you've got                                                               new zeeland stream at the top and so                                 instead of remembering you've got HTTP                                 once and saying oh just refer to that                                 which happens in in Jason and Avro it                                 instead needs to relearn that over and                                 over again and so we need to investigate                                 using C standard with a pre-loaded                                 dictionary so that we can get better                                 compression okay so what use case is the                                 first one we just want to read all of                                 the columns and all the rows one of the                                 things that people often worry about is                                 whether you can assign different workers                                 to work on different pieces fortunately                                 all the format's are good with that                                 except for Jason when it's compressed                                 and with spark                                 one of the characteristics that wasn't                                 obvious until we started hitting it is                                 that when you're reading data through                                 the file format interface spark will use                                 columnar batch which is a faster                                 internal representation if all of your                                 types are primitive types so of ours                                 only taxi fits into that category                                 okay so first notice that I've put it on                                 a logarithmic scale so each of those                                 lines means it's twice as fast as the                                 one above it and these are seconds as                                 you did the read so you can see that the                                 lowest one is Park a park a is a little                                 faster orc is next and then Jason is                                 really slow like really painfully slow                                 he's it's                                                          seconds so really painfully slow that's                                 why you don't use Jason bad Jason Jason                                 slow of course because it needs to do a                                 lot of string parsing and park' is the                                 fastest I suspect although I haven't                                 verified it yet that it's because the                                 even the faster orc reader which is in                                 fact much faster than the old one is                                 still going through an extra level of                                 indirection it's going through the                                 vectorized row batch to the or extract                                 and then to call an or batch we've                                 written some code that gets rid of that                                 so it'll go straight from the vectorized                                 row back to the columnar batch which                                 will make it much much faster we just                                 need to get that code committed and then                                 released now sales we got a little bit                                 different picture so here orc did really                                 well partially because it's smaller and                                 partially because the data is better                                 suited for what work is fast at park' is                                 a little bit slower and jason is still                                 the slowpoke                                 so here the read performance is                                 dominated by the format it makes the                                 most difference which format you've                                 encoded and less about                                 the particular compression picked okay                                 now github times here actually Jason had                                 its first and only when you can see that                                 the fastest was Jason github now granted                                 it's huge but it's still fast work is                                 next in parque is is worse now when I                                 talked to the Twitter guys the guys who                                 started Parque they said oh yeah don't                                 use Parque for that case I was like                                 really that's the use case that it was                                 that was dremel was made for but yeah                                 they say don't use it like that now one                                 of the other characteristics is that                                 because there are so many columns the                                 stripes are actually ending up pretty                                 small and so if you're writing your                                 application you really should configure                                 a bigger stripe size for these very wide                                 files you'll do much better now we're                                 gonna add something in work to say to                                 define the minimum number of rows per                                 stripe because if you have too few rows                                 then the the optimizations don't work                                 well alright so the next use case is for                                 column projection often when you're                                 running your query or writing your                                 program you only need a few of the                                 columns so it's not that uncommon to                                 have                                                                   or three or five of them and so this is                                 where the columnar formats actually                                 change you can actually just read and                                 decompress the bytes you need for those                                 columns and you don't need to read the                                 rest and spark-gap file format makes it                                 really easy you get to pass in your                                 desired schema and the file format is                                 required to process that and figure out                                 which columns it needs and not read the                                 rest Jason and Avro obviously do that                                 read the read first and then just drop                                 the columns or can parque don't bother                                 reading the data                                 because it's faster okay so this is the                                 percentage of the data that or can park                                 a red in the different cases for the                                 different you sets so I sorted by which                                 use case and then the format and then                                 the compression so you can see for                                 github ork was reading about four                                 percent typically and parquet was a                                 little bit higher but about the same                                 this is still a logarithmic scale so                                 it's doubling oh no this is the one                                 that's not logarithmic this is just the                                 percentage for sales is somewhere                                 between                                                                bit bigger up at                                                     taxi data set Parque went crazy and                                 somehow read twenty percent of the data                                 and for the two columns that I asked for                                 and the benchmark and so I'm not quite                                 sure what's up with that                                 but it's repeatable it so you can see                                 that obviously if something was a                                 hundred percent then you don't have                                 column projections so if you put Avro or                                 Jason on here it clearly be at a hundred                                 for all of these so all of them are much                                 much better on the x come down                                 correspondingly now predicate pushdown                                 predicate push down is a nice                                 characteristic of the more advanced                                 readers when you're querying for example                                 when you just want the names for                                 employees that were hired between a                                 given set of dates you express your                                 sequel query like that the reader gets                                 passed down oh I wanna hire date between                                 those two dates and given its given to                                 the file format during via filters and                                 this is primarily useful on sorted                                 columns we'll talk a little bit more                                 about that on the next slide so orc and                                 Parque index their rows with min and Max                                 values for a whole set of ranges so                                 sorted data means that it can use those                                 men and maxes to                                 say okay I don't need to read those rows                                 I won't bother parsing it at all the now                                 if you don't have sorted data for                                 example I've got one customer that has                                 wants to do queries on customer names                                 customer names can't really be the sort                                 key because they need to sort by time                                 and so they need to query on customer                                 names all the time but it it can't be                                 the sort key so for them we've made                                 bloom filters and so when you set up a                                 bloom filter it records a probabilistic                                 answer to whether specific values are in                                 this set of rows and so they do take                                 more space but they let you answer the                                 question of okay do I need to look at                                 this set of rows at all and only or                                 caste that the readers for a predicate                                 pushdown can either filter out the                                 entire file the stripe the section of                                 the file that you work on or in orc you                                 can also get row groups down with                                        rows now after this after you get the                                 results back you still need to have the                                 engine check the filter at the row by                                 row level this will just say yes you                                 need to read these                                                       may be stuff in there that you don't                                 want so you still need to check it all                                 right now when we did this obviously we                                 only did with or can't park a the dark                                 wine is the total number of records the                                 blue line is how many records Parque red                                 and the green one is how many orc red                                 and notice those lines are logarithmic                                 again so in this case you've got                                        for the taxi data that orc red versus                                 the                                                                     what's going on here is that the Parque                                 wieder decided it didn't know how to                                 deal with timestamps so in                                 of the predicate push down and so it                                 silently said okay I'm just going to                                 read everything and and did actually                                 that's also what happened on the github                                 side is it read all the records now                                 sales it had an integer so the predicate                                 push down actually worked                                 but because orc has the indexes at the                                 ten thousand row level it was able to                                 read just ten thousand rows                                 Parkay ended up reading sixty times that                                 which is the size of its stripe and of                                 course that that's still a hundred times                                 better than the the total number of rows                                 so you can see why this predicate push                                 down is a big deal                                 a few years ago when Yahoo was still in                                 existence                                 instead of being oh so now they were                                 benchmarking hive against SPARC and they                                 were running their queries and there was                                 one query that was coming back really                                 fast on hive they thought it was broken                                 but it turned out that part of what was                                 going on was exactly doing this                                 predicate push down and so the hive                                 running out of HDFS just needed one file                                 in one section of one file so I was                                 reading basically ten thousand rows and                                 then it had its answer                                 SPARC even once it was a memory had a                                 hundred terabytes in memory so it took a                                 lot of X executors looking through all                                 of its memory to figure out oh okay                                 here's the one row I need so so hive was                                 much much faster than spark even once                                 SPARC had everything cached in memory of                                 course now with el ap el ap is the new                                 execution engine for hive that caches                                 data aggressively and so it would be                                 much much faster actually we've had                                 queries run against el ap where you're                                 testing against a table with six billion                                 rows to come back in less than a second                                 really amazing actually                                 okay so as I said parquet doesn't push                                 down timestamp filters spark defaults or                                 the predicate push down off you need to                                 turn that on the small orc stripes or                                 github meant that we ended up with a                                 small read of less than                                                 because it's an optimization the file                                 formats aren't very good about telling                                 you when it's been turned off that's                                 something we should actually get better                                 at it took actually looking up the data                                 of how much data had been read before I                                 was sure okay the predicate push out                                 actually happened and then you start                                 looking for cases about why it didn't                                 happen the benchmark actually uses a                                 tracking file system so that it can see                                 how many bytes got read or written okay                                 another advantage of Orkin parquet is                                 that they store some additional metadata                                 in the file footer so they have the file                                 schema but they also have the number of                                 records in the min/max and count of each                                 column and so if you need any of that                                 it's actually pretty easy to get it in                                 one access okay one of the most                                 important things here is everything                                 changes right open source systems are                                 continually evolving right the                                 environments change and so things are                                 constantly changing right if you did a                                 similar benchmark a couple of releases                                 ago for spark it would have been a very                                 different experience and so you actually                                 need to take that into account                                 the benchmarks will change but only when                                 people come up with suggestions for what                                 to do better the other thing is to                                 really evaluate your needs                                 most people really need column                                 projection and predicate push down is a                                 nice to have but you definitely want to                                 be in either work or parquet most of the                                 time you want to determine how to sort                                 your data we had some customers that                                 trying to partition their data by both                                 country and their product and that made                                 it easy to query you could filter out                                 things but it really meant that you                                 ended up with some little teeny                                 partitions right if you look at how many                                 people in Guatemala want to use Product                                 X that's a really small number compared                                 to like the number in Germany for                                 example that would be much larger for                                 them it's much better to sort the data                                 and then let predicates push down to                                 work the data and then considering                                 balloon filters is another really good                                 case if you have equality bloom filters                                 don't work for a less-than or                                 greater-than                                 they only work for strict equality so                                 but if you have a lot of equality test                                 where you're looking for point lookups                                 they're really useful okay questions                                 just think the speak up first oh come on                                 someone have a question there's one when                                 you read the data in SPARC you actually                                 need to do an action in order to                                 something happen so how do you separate                                 the action time from the actual read                                 time so that's part of I was using the                                 the file format or rather the benchmarks                                 are using the file format interface so                                 that it doesn't launch tasks to read it                                 so it wasn't just the declaring the task                                 it was actually accessing the file                                 format directly basically the point was                                 to make micro benchmarks that would                                 exactly isolate out the reading from the                                 processing that would happen downstream                                 give time you said that you don't really                                 know what that okay is going crazy on                                 this any explanation what is different                                 than the data recommendation when to not                                 use pocket okay so okay technically what                                 I said Parque was going crazy it was                                 just                                 the predicate push them and that wasn't                                 on the github data that was on the the                                 sales data I think wasn't it actually                                 that was the Cullen projection one right                                 when we back up now the okay there                                 should be a faster way to do this sorry                                 yeah that was actually the taxi data                                 where the column projection on the taxi                                 data now the the Twitter guys and                                 actually the results here also pointed                                 out that for highly complex data for the                                 github case you don't want to use Parque                                 and when I looked at it it mostly seems                                 to be about out memory allocation it's                                 basically creating a lot of temporary                                 objects and then throwing them away so                                 it's great GC thrash there was a                                 question here                                 so I guess best in your presentation                                 actually when I have a specific use case                                 it will be really hard to look in the                                 Internet advantages disadvantages of                                 each of the formats the better way is                                 just to take a sample of data set and                                 use a benchmark do you have any                                 inundation steeps so how I decided is                                 not how to do proper benchmark okay well                                 first of all I haven't pushed this code                                 up yet but I will shortly the earlier                                 benchmarking code is already in the orc                                 project so you can actually download the                                 github and it lets you down it has a                                 script to download the data that you                                 need or you can put your own data in and                                 then process it you just need to fill in                                 the benchmarking code the benchmarking                                 code uses JM h which is java micro                                 benchmark harness which is integrated                                 into open JDK and does a really nice job                                 of both letting you specify variants and                                 measure the execution time and other                                 attributes you're interested in                                 so absolutely download the benchmarks                                 put your real data and obviously for                                 open talks and for the public benchmarks                                 I can only use data that's publicly                                 available right obviously the the                                 customers that gave me this get the                                 schema for the the sales table wouldn't                                 even let me use the real column names I                                 basically had to completely change the                                 call names because they were that                                 worried about anything leaking out about                                 what they're doing but ya know it'd be                                 relatively straightforward to plug it in                                 and use your real data so absolutely so                                 hi thank you I will be interested you                                 talked about customers a lot I'm about                                 the amount of adoption of column uniform                                 odds like or cos mm-hm                                 because I got the feeling that with the                                 direction of the kafka Evaro marriage I                                 would say that a lot of people just dump                                 their every records into a tree because                                 again and if you have a view from the                                 market what's how often you see people                                 really using something like                                 Co actually mostly we do because                                 they stream through it often they use                                 Avro like you said to put it through                                 Kafka but then once they land in hdfs                                 then they want to put it into a columnar                                 format because the performance                                 advantages are really really high and                                 you write it into HDFS once and then you                                 read it a lot and so usually most                                 customers find that yeah streams through                                 their Kafka as as Avro but then once it                                 lands in HDFS they they translate it                                 over thank you                                 thank you I would like to ask about our                                 C version you is there a timeline for                                 finalizing everyone what can we expect                                 from it um so trying to be realistic                                 it'll probably be out later this year we                                 have it's still relatively early in the                                 design at this point we're figuring out                                 what needs to be there and what we want                                 to change in the format so effectively                                 ork had to format two versions of the                                 format the first one that was in the                                 very first release of hive that ork was                                 in which was hive                                      then in the very next release                                         came out with what we now call version                                   and version                                                            we know a couple pieces that we                                 absolutely need to fix we need to make                                 time stamps better we screwed up the                                 time zones oh my god the person who                                 decided to ever put any time zone                                 information and was was a really bad                                 plan but I wish we could just use like                                 standard UTC all the time but at least                                 for programming that would make life                                 much much better but yeah I'd say by the                                 end of the year we should have at least                                 out in alpha thank you                                 let's take one more last question so for                                 data with a complex schema                                 like for instance github in a machine                                 learning application where most of the                                 columns would be used anyway for feature                                 extraction do you still see an advantage                                 to use columnar formats like marquee or                                 if you're really just going to use it                                 all the time and it's highly structured                                 Avro does work pretty well so with the                                 with the restriction that for spark the                                 Avro adapter I'm not very happy I'm                                 about ready to file a bug report on the                                 Avro descriptor saying what the hell but                                 that but that aside Avro does work very                                 well for the highly structured case                                 where you don't need to do column                                 projection but the columnar ones are                                 still better than Jason even in this                                 case yes basically Jason is always                                 really slow is really what it boils down                                 to and if you've ever tried to actually                                 look through the details of a JSON                                 parser you'd understand why thank you                                 very much                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=aIcxFIyL6xo


