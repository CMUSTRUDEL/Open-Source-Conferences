Title: Berlin Buzzwords 2018: Tomás Fernández Löbbe – New Replica Types in Solr 7 #bbuzz
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	For the majority of cases, the current SolrCloud distributed indexing works great. There is a subset of use cases for which Solr's legacy Master/Slave replication may be a better fit, like cases where NRT is not required, and where read availability is more important than consistency. Solr 7 now ships with three different types of replicas to choose depending on different consistency and availability needs. 

With a combination of replica types, one can create a SolrCloud cluster that behaves like the Master/Slave architecture from Solr lower than 4.0 and provides separation of responsibilities (search vs index) while still getting most of the SolrCloud benefits, like high availability of writes, replica discovery, collections API, etc. 

This talk will be a deep dive into the new Replica Type feature: reasons for implementing it, differences between the types and how/why one would choose to use them, and implementation details of the feature.

Read more:
https://2018.berlinbuzzwords.de/18/session/new-replica-types-solr-7

About Tomás Fernández Löbbe:
https://2018.berlinbuzzwords.de/users/tomas-fernandez-lobbe

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah thank you for coming for my talk to                               my talk this is about replica types a                               new feature added in solar                                             agenda for my talk first for context and                               history maybe we're going to start                               talking about scaling in solar how it                               was before solar cloud how it is in                               solar cloud                               why or what things are we trying to                                address with replica types then with                                which replica table types were added                                then one of the use cases that you can                                achieve with replica types which is                                having some sort of master slave                                architecture in solar cloud then how to                                use them and some to do some future work                                or think things that needs to be done or                                that can be done yeah so before we first                                saw a cloud the first order for the way                                to scale solar was by doing a                                master/slave architecture where all the                                updates would go to the master and all                                the queries would go to the slave then                                in the background the slave would be                                doing a segment replication and copying                                the latest updates from the master to                                the slave this works                                this works pretty well to some extent I                                had some issues before I go into that                                just some key concepts required to                                understand segment replication solar is                                built on top of a bachelor since most                                people already know that the way solar                                stores in stores the index is by writing                                segments as you continue adding                                documents to solar at or to the scene it                                will ask every time you open a new                                searcher or you do a comment or every                                time the memory buffer fields is going                                to write a new segment to disk and the                                important part is that that segment once                                it's written in never it never changes                                okay there's going to be another thread                                that's going to be choosing segments and                                merging them merging those creating a                                new bigger segment and then dropping the                                old ones so solar uses these                                characteristic of the Lucian indexing to                                to perform segment replication and the                                way it works is that the slave server is                                going to ask the master hey which                                segments do you have the master is going                                to explain                                to say I have segments in this case for                                example it would say I have segment                                    &                                                                    segments that the master has versus what                                will suffice and download only the                                updated the new segments and since since                                he knows that the segment wanted to                                didn't change he doesn't need to                                download memory again this strategy does                                not support near-real-time so you cannot                                open a any real-time structure on top of                                of those segments until so you need to                                Hart commit get the get the latest                                segments and then open a searcher and                                that's when it's available for search so                                this worked very well                                it has some issues a lot of all the                                configuration for master slave before                                solar cloud was manual you had to                                specify who was the master who was the                                slave also there was no high                                availability for rights so if the master                                go down the updates would fail the good                                thing is that the queries would still                                succeed and then you would have to get                                go fix that master or add some other                                node to become a master but that again                                is manual work so then with solar for                                solar cloud came solar cloud is                                essentially just the set of features of                                or capabilities added to solar to have a                                high availability no recovery no                                discovery automatic load balancing all                                those things one of the main features                                added to solar cloud was distributed                                indexing so you don't know no longer                                needed this segment replication in order                                to send the documents to the different                                replicas of a particular chart the way                                it works with distributed indexing is                                that when a user adds a document it will                                go to the leader if the leader doesn't                                receive it immediately with whoever                                receives that document is going to send                                it to the leader and then the leader is                                going to add that document locally and                                 then send it to all the replicas of the                                 shard all the followers each of those                                 replicas are going to add the document                                 locally and then respond to the use to                                 the leader and then the leader will                                 respond to the user we have your                                 document the good thing about these                                 strategies that now this supports me a                                 real-time get sorry near real-time so                                 those those replicas they can open a                                 search                                 and since they have everyday all the                                 documents they can provide search for                                 all the latest documents that it                                 received another big feature added to                                 solar cloud was a transaction log the                                 transaction law is a certified that                                 essentially it contains all the                                 documents updated since the last commit                                 and this these file is needed for for                                 reals and get and it's also needed for                                 recovering and also another feature was                                 self recovery of notes right so what                                 happens if in this situation where we                                 had the distributed indexing as I was                                 explaining before before but in this                                 case the replica too for whatever reason                                 couldn't respond to the update for                                 example because it was either with some                                 Network issue or the replica was down or                                 there was some garbage collection or                                 whatever reason it just can't respond to                                 the leader what's going to happen is                                 that replica one is going to proceed the                                 same way and the leader is going to tell                                 that replica that it needs to recover                                 okay in some way so that replica is                                 missing data and essentially it can't                                 now that it's going to be in recovery                                 state it can't provide search you can                                 respond to search from users anymore and                                 it needs to recover the way recovery                                 works is that the replica once it's back                                 up it notices that it's it's being put                                 place in recovery so it's going to go to                                 the leader hey I'm ready um I need to                                 become active again the leader is going                                 to start sending first every new update                                 is going to be going to this replica                                 that's intercutting                                 and the replica instead of indexing that                                 document is going to start buffering                                 them and at the same time is going to                                 start the first phase of recovering                                 which is called piercing the piercing                                 essentially is where the replica is                                 going to ask the leader hey which                                 documents did I miss this is the last                                 document I got how many did I miss okay                                 and the leader is going to say okay I                                 have already this number of documents                                 that you don't have is that number is                                 small by default is less than                                          the leader is going to start sending                                 those documents individually to the                                 replicas the replicas will update those                                 in the in                                 next and then replay the replay the                                 buffer and then it becomes active but if                                 the number of documents missing there is                                 more than                                                                that to some other number to your                                 particular number the way the replica                                 will recover is by doing the same                                 segment recovery that we were talking                                 about in before he master slave                                 essentially it will commit to the leader                                 is going to start replication is going                                 to download all the segments after that                                 is going to replay the buffer and then                                 become active so if the if all of these                                 works then why do we need replica types                                 in master slave one of the things that                                 that was interesting was that the the                                 indexing process and the searching                                 process were were separated right so if                                 you had like a expensive documents or a                                 spike in number of updates they would                                 not affect the search the search traffic                                 the search latency or throughput or the                                 other way around if you have a very                                 expensive query and that would not                                 affect the updates right this was not                                 possible in solar cloud because now                                 every replica does everything so                                 expensive updates will affect queries                                 and the other way around too so one of                                 the nice things of the distributed                                 indexing is that its support near                                 real-time but some use cases don't                                 really need near real-time some cases                                 are okay with serving data that's mean                                 it's old or I don't know seconds even                                 even that may be fast                                 also I need initiator recovery that                                 recovery state can become a problem                                 let's say you have a big cluster and                                 there's some sort of network partition                                 and the leader tries to send the update                                 to its I don't know hundreds of replicas                                 or whatever and one third of the one                                 third of the replicas are cannot respond                                 so all these replicas are going to go                                 into recovery state state and that means                                 that those replicas cannot serve such                                 traffic anymore and that may be a                                 problem for you also like like every                                 replicas doing everything can sometimes                                 be wasteful so let's say if you have a                                 three node cluster a three replicas                                 shard then you may be you may want that                                 every replica has everything ever                                 I can be leader and they have all the                                 updates but if you have a                                            shard um that may not mean it you may                                 not need that the                                                        doing everything                                 also in that recovery that I was showing                                 with the piercing and the segment                                 replication um with cases with higher                                 throughput the piercing has little                                 chance of succeeding because if it is                                 like                                                                                                                                           coverage like slow garbage collection                                 can can can cause this right and then                                 especially again in high up high                                 indexing throughput case is the number                                 of segments that the replica needs to                                 download maybe too many because it                                 changes all the time right and this                                 problem can happen in when recovering                                 doing a segment replication so I show                                 you before how in master slave the                                 replication is somewhat incremental only                                 the new segment sites are downloaded but                                 let's think about this case where you                                 have three nodes a B and C is the leader                                 and C is in recovery let's assume for                                 now that the that the piercing fails so                                 node C is going to go and say the leader                                 hey which which segments do I need to                                 download and the leader is going to say                                 I have segments a                                                   going to say okay have segment c                                      then not a is going to say now that's                                 fine you don't need that one throw it                                 away download in a                                            essentially is doing a food segment a                                 full index replication okay that he did                                 Rob Nazis Achtung now it's working some                                 time passes and then it goes into                                 recovery again now the leader is not                                 being there was a leader change and                                 nobody has its own segments because                                 every every rep guys I think is indexing                                 locally is merging locally so each each                                 replica would have its own its segments                                 right so not be so don't think it's                                 going to go to the leader and say hey                                 who which segments do I need and nobody                                 is going to say you need b                                            okay I have a                                         throw those away just download the full                                 index again yeah and                                 this can be a problem so to address some                                 of these issues we are the replica types                                 three types of replicas we're added NRT                                 that stands for near-real-time                                 Dillo that stands for transaction log                                 and pool because it pulls in this inner                                 team replicas is not really a new type                                 this is essentially the name that we                                 gave to the existing types in solar                                 cloud and this is exactly the same it                                 will do that the replication is the same                                 it will handle the updates the same way                                 that it was doing before there's no                                 change again this is the replica that                                 existed today when a document is added                                 the leader is going to add it locally                                 update the transaction log send it to                                 all the new real-time replicas and then                                 they are going to index the document                                 update the transaction log and respond                                 and then respond to the user so for now                                 let's not think about what type of                                 replica the leader is in this case let's                                 just think about the replica on the                                 right yeah so new real-time replicas                                 they support everything that solar costs                                 firstly they support soft commits they                                 support real-time gets they can become                                 leaders the this the Telegraph Rica is                                 one of the new types added and                                 essentially it works it works very                                 similar to near real-time the difference                                 is that when a telegraphic are receives                                 an update it will update the transaction                                 log but it won't update the index ok it                                 will respond to the leader I have the                                 document but it won't be available for                                 search until the till of replicas will                                 do a periodic replication from the                                 leader ok so essentially the tiller will                                 only will you continuously do                                 replication and it will only update the                                 transaction log whenever Hanna updates                                 if she is received because it works like                                 this Tillich replicas do not support any                                 real-time and they also do not support                                 real-time gets the third type of                                 replicas is pull replicas the pull                                 replicas are slightly different because                                 what they do is that they don't even                                 receive updates from the leader the only                                 the only thing they do is like a                                 periodic replication the same                                 replication that the tea logs to                                 understand replication that master slave                                 used to do                                 so pull replicas cannot support the soft                                 commits do not support real-time gets                                 and they actually cannot become leaders                                 because there are missing they don't                                 have all the latest data so why are they                                 good for essentially the best thing                                 about that the pull replicas is that                                 they cannot go into leader initiated                                 recovery so because the leader is not                                 trying to contact them for updates that                                 replica is not going to in place in                                 recovery even if there is some sort of                                 network issue between the leader and the                                 replica as long as the rep as the full                                 replica still connected to zookeeper is                                 going to remain active that means of                                 course that it can become out-of-date                                 so it could be if this if this problem                                 between the leader and the pull replica                                 remains there for a long time pull                                 replica could be serving all all data                                 for some time so in summary near                                 real-time replicas they write to the                                 index they write to the transaction log                                 they receive every update and they                                 replicate periodically sorry and they do                                 not replicate periodically tillich                                 replicas do not write the index they do                                 write the transaction though they                                 receive every update and they replicate                                 periodically and see how there is an                                 asterick in the till log doesn't write                                 to the index and the reason for this is                                 that when the telegraphic ayah becomes                                 leader it will behave exactly as a near                                 real-time so it will the telegraphic are                                 that is leader is going to write to the                                 index okay so that every one every other                                 telegraphic I can replicate from pull                                 replicas do not write to the index do                                 not write to the transaction log they                                 receive they do not receive every update                                 and they replicate periodically from the                                 leader so to show the data flow again                                 let's say you have these two log two                                 telegraphic ass and pull replica                                 when an update is received it will go to                                 the index and the transaction log of                                 note a which again is a telegraphic abut                                 because it's a leader its indexing it                                 will go to the transaction log of all                                 the node all the other T log replicas                                 and then the index is going to be                                 replicated for all the T log or full                                 replicas so what do they support near                                 real-time replicas again it supports                                 everything that solar cloud supports                                 right now                                 soft commits real-time gets they                                 can become leader and they can go into                                 leader initiated recovery Telegraph                                 bigges do not support off commits do not                                 support real-time get again there's an                                 asterisk here they can become the leader                                 by apply so when a deal of replica needs                                 to become a leader essentially it will                                 apply everything that's in the                                 transaction log and it has the most                                 recent data and they can go into either                                 initiate a recovery and again the                                 asterisk here is because that's not                                 super real-time get as long as you are                                 not the leader if you had a deal log and                                 you're the leader and you do variate and                                 get it will respond and pull replicas do                                 not support soft commits they do not                                 support yet then get they cannot become                                 leaders but they can also not go into                                 the canticle go to leader initiated                                 recovering yeah so now when you create a                                 collection in solar you can specify the                                 number of replicas that we want of each                                 type right right or when you wanna add a                                 shard but not all the combinations of                                 replicas types are supported or                                 recommended these are the the                                 recommendations for combinations of                                 replicas types first it's all in real                                 time that again this is the default if                                 the existing before seven and if you                                 don't specify replica type is going to                                 go into this configuration and this is                                 the only configuration that would                                 support any real-time and otherwise is                                 also recommended for small to medium                                 clusters or when the indexing throughput                                 is not too high you can also choose to                                 have all telegraphic ass and you would                                 use this combination when there's no                                 need for any real-time when there's high                                 update throughput or when there's medium                                 to high medium to big cluster but you                                 still want every replica to have every                                 document and then set of T log plot plus                                 a set of pool replicas that's also a                                 good combi combination and you would use                                 it when you don't need near real-time                                 when the update throughput is high when                                 from medium or to big clusters or with                                 high or high throughput but also where                                 you prefer search availability over near                                 real-time or consistency this last two                                 combinations also have another benefit                                 is                                 that issue would recovery that I was                                 mentioning before it's not a problem                                 anymore because let's go to that example                                 again but now let's think about not a                                 and not B being telegraphic us and not                                 sieving either till o or Apple replicant                                 recovery so yeah the note C is going to                                 ask the leader hey which segments we                                 have I have segments a                                                don't know that but see how in this case                                 no B has also a segment a                                           because it's essentially also                                 replicating all the time and then not C                                 is not very reliable and we will go to                                 recovery again and in this case the                                 leader is not V but nobody has a segment                                 a                                                                        a                                                                        need to copy those so it it's back to                                 being incremented in that sense a couple                                 of combinations I didn't mention in the                                 recommend that ones are near real time                                 with either T log or pull replicas or                                 all pull replicas so that issue with the                                 recovery that I mentioned before it                                 becomes much worse in if you mix near                                 real time with pull replicas or                                 telegraphy cos because it doesn't even                                 need to go to and no doesn't even need                                 to go to recovery it's replicating                                 periodically from the leader if the                                 leader changes that means that all the                                 pool or telegraphy cos that you have out                                 there in the next replication phase they                                 are going to download the full index so                                 you have to be very careful and all pool                                 replicas because um because pool                                 replicas cannot be leaders if you have                                 all full replicas you essentially have a                                 little less charge and this actually is                                 a combination and it's not supported so                                 if you try to create it solar is going                                 to complain so I mentioned in the agenda                                 that one of the feed nice features that                                 you could have by using combinations of                                 replica types is a master slave                                 architecture in solar cloud and it's                                 very easy to think about how to do it                                 you just need to have a tillich replica                                 that's going to be the leader and then a                                 bunch of pull replicas to do queries                                 from and you can do even better now you                                 can have a set of D lock replicas that                                 are going to be writing and then                                 of pool replicas for queries right and                                 you get the benefits of solar cloud so                                 if the leader goes down now you                                 automatically get a leader leader                                 election another now is going to become                                 a leader and then the replication is                                 going to continue and you won't even                                 notice from the search or from in the                                 updates you may have a small leap but                                 then it will continue to work                                 automatically so you get you get to                                 prefer search availability over                                 consistency or near real-time about                                 using something like this you get to do                                 that separation of responsibilities that                                 was Gooding in master/slave but you also                                 get most of the benefits of solar cloud                                 you have higher value high availability                                 of rights financing no recovery and most                                 of this Oracle cloud features and this                                 also of course I'm showing it as a                                 single index but essentially you since                                 you can have multiple collections or                                 multiple shots for a collection you can                                 you you could use auto scaling group so                                 that some some nodes only get                                 telegraphic cuts and some other nodes                                 only get full replicas and that gives                                 you that separation of responsibilities                                 completely right because if you have                                 multiple shots and then in a node you                                 have the leader of one and then and then                                 pull replicas they there's still no                                 separation because if there is a spike                                 in updates they're probably going to go                                 to all the shots and then you wouldn't                                 get that separation so you can use auto                                 scaling rules to make sure that some                                 types of replicas only go to some notes                                 and some other types to some others so                                 how do you use them how do you use                                 replica types it's actually pretty easy                                 and essentially when you create a                                 collection or when you create a shard                                 you can specify the number of replicas                                 for different types that you want for                                 for that channel also when you create a                                 replica you can specify the type that                                 you want if you don't this is backward                                 compatible so if you don't and you                                 create a collection without specifying                                 the types of the different number of                                 replicas for different types and you                                 just say replication factor that's going                                 to translate automatically to                                 near-real-time which is the existing                                 type before                                 seven the same with AD replica if you                                 don't specify the type it's gonna be                                 near real-time and there's of course a                                 v                                                                       Jane so if you create a collections                                 using solid J then you just those                                 numbers there are the number of charts                                 number of near-real-time replicas still                                 out and pull out and pull replicas and                                 when you create and when you add                                 replicas you can specify the type that                                 you want it to be as I was saying before                                 you can use the auto scaling policy                                 framework so that you can choose which                                 types of replicas goes to different                                 notes and the rules would look something                                 like this to identify the types of                                 replicas the type replica is the way the                                 way to do is is by looking at the at the                                 cluster state there's going to be for                                 each replica that the type key that's                                 going to tell you which type it is and                                 also there's a small but very important                                 detail that now when you create the                                 collections via in the occupations API                                 the core is going to include the type of                                 replicas hosting in this case it's T                                 that stands for T log while it would be                                 n for near-real-time                                 or P for pool and why I think this is                                 important because solar includes in the                                 logs for the majority of the log system                                 it includes the core name of the well                                 it's logging so you can say okay this is                                 that this is a tillich replica that's                                 why it's it's logging about doing                                 replication that makes sense there's an                                 I'm cheating a week here because this is                                 not yet committed but this is in India                                 that's soon to be committed by by rocket                                 and eating so there will be in the ape                                 in the UI you would be able to see which                                 type the replicas are then so another                                 interesting part is you can choose you                                 can set the preference of which replica                                 types you want to respond for a query so                                 you can say for example in this case I'm                                 going to start send this query and I                                 want it to                                 respond by replicas of type of pool and                                 as long as there is one and you can set                                 reference that's much more complicated                                 like this you could say first pour and                                 then active and the idea is that we                                 could keep we could add more rules here                                 there's you can choose also location of                                 the replica like like a IP prefix or                                 something like that and the idea is that                                 this is going to sort which replicas are                                 available for a shot and then if there                                 is at least one that's pool and is                                 active is going to be used for                                 responding your query the important part                                 is that if there isn't one that's that's                                 applying to to this rule then any other                                 replica is going to be used in this case                                 the same way I mean it will default it                                 will it will go to the default which is                                 to take any replica there's this other                                 feature that's also not committed but                                 soon that you can use to filter types                                 and in this case what you want to want                                 to say is that if there is I want to                                 query only a replica of type bool and if                                 it's not there and if there's no active                                 pool replica for a shot just give me an                                 error I don't want to care query in any                                 other replica and the reason why this is                                 also important is that if you are doing                                 that master slave architecture and you                                 don't really want to go to the leaders                                 you can you can do that so what's next                                 to do this is some of the stuff that I                                 think can be done there's more that                                 people can think of this is so one of                                 the main thing that came during the                                 development of this feature was that                                 issue that I mentioned before that if                                 the pool replicas cannot talk with a                                 leader for a long time it will continue                                 to be serving search traffic and that                                 such traffic may be may be may be                                 responding with a very old data right so                                 there's options there's this JIRA to                                 address that the options that were                                 discussed are either let's let's make                                 the replicas we can't go to recovery if                                 it fails X amount of                                 vacations or if it phase for this amount                                 of time another option that that we were                                 thinking is including the response the                                 time of the last rapid successful                                 replication so that the client can                                 choose okay this day guys way to all I'm                                 going to do another query another                                 another thing that needs to be done I                                 think is that right now replication is                                 always going to happen from the leader                                 and that's not really necessary because                                 all the all the telegraphic a sample                                 replicas are going to contain the same                                 segments the the leader will have the                                 latest one but if you are ok with being                                 a little bit more behind you could say                                 okay replicate from any Telegraph Raqqah                                 um and that would I mean because the                                 otherwise they can be a bottleneck in                                 the leader if everyone is trying to                                 replicate from it integration with the                                 CLI is not done so if you use being                                 solar to create collections                                 you can't specify right now how many how                                 many replicas you want for each type so                                 the Sharpe preference is also only                                 supported in the multi shard case it's                                 yeah in the yeah in the multi sharp case                                 if you have a collection with a single                                 char right now it won't it won't it                                 won't support that parameter the reason                                 is that if you have a single shard                                 collection the the code that is that                                 decides were word query is in the client                                 so we need to we need to make sure that                                 this code can run in the client too                                 and there's an open JIRA for that but                                 that that's not done yet another thing                                 that could be done is that as I                                 mentioned you we don't mix near                                 real-time replicas with pool or tea logs                                 and the reason is that if they need to                                 replicate from one and then and then                                 there's a little change and the new                                 leader has a completely different set of                                 segments that that would cause a full                                 download of the index from for every                                 kilo or pool of replicas but so what we                                 really need is that if there's near                                 real-time replicas let's just make sure                                 that they don't become leaders so if                                 there is a use case                                 for the same collection you want to do                                 some near real-time queries but some                                 others are are okay with having some                                 stained data then you could have a                                 cluster where you mix all the different                                 types you just need to make sure that                                 the near real-time replicas do not                                 become leaders and yeah this is not this                                 is not supported right now another thing                                 that would be interesting is that as I                                 said solar doesn't support any real-time                                 when doing replication and the reason                                 for this is because when I think the                                 replication was implemented way long ago                                 and soft communities not exist then and                                 then with Sora cloud that's stopped                                 mattering because the replication was                                 only used for recovery but now that                                 we're going to use replication for for                                 updating the index we may want to be                                 closer more more real-time right and                                 loose in actually the replicator module                                 supports near real-time replication on                                 the software means but it so that's not                                 yet supported in solar that's something                                 that could be added and the Telegraph                                 because the full replicas could get more                                 up-to-date data so yeah that's my talk                                 thank you very much make sure you're                                 rooting for the right team in the workup                                 that's why I'm putting that there                                 brain storm brain don't ribbit ok I                                 think we have time for questions here                                 I see mentioned small medium and large                                 sized clusters what what are they in                                 your view sorry                                 well I'm small medium and large what do                                 they mean yeah that's like that's a good                                 question I don't have an answer actually                                 it really depends on each use case right                                 because it's not a single I mentioned                                 two off big too smart to be it's also                                 the number of replicas the number of                                 nodes the traffic that you are that you                                 are using so I I've heard people using                                 telegraphy cos it's much more stable                                 even for not so big clusters and then                                 yeah there's people that uses                                 near-real-time replicas for very big                                 clusters and still good for them so it's                                 there's no I mean it's kind of a blurry                                 thing that you you will have to design                                 with your define with your own data hi                                 question about the t lock replica you                                 said it when when object comes in and                                 the transactional updated in t log it's                                 actually not such a bully it's not                                 committed until leader initiates                                 replication or what what sugars she                                 looked actually make the change is                                 searchable sorry I didn't Oh                                 in order to make the change searchable                                 in the dialogue yeah exactly                                 it the tillow is actually not going to                                 use at all unless unless it needs to                                 become a leader so the the information                                 is not going to be searchable until the                                 replication happens is there any way to                                 trigger that application like force it                                 to happen without being becoming leader                                 replication or or indexing from the                                 tillow index from key lock no in fact                                 it's something that you don't want again                                 if you have more than one node writing                                 index at any point in time you can cause                                 a lot of trouble because anyone will                                 have a different set of segments and                                 then every everyone needs to download                                 any more questions I'm actually trying                                 to understand the practical use case of                                 the polo replicas if you can explain                                 learn more the practical use case of                                 what of full replicas yes the main the                                 main use case of that is that it doesn't                                 need to go into leader initiated                                 recovery so if you have a big cluster                                 and you are afraid of losing a part of a                                 cluster because of some blip in the                                 network those replicas are still going                                 to be active as long as they don't                                 disconnect from zookeeper and the                                 timeout for that is much longer than                                 just a single update failing at a                                 particular point in time you know did                                 yes yes yes poor replica parties                                 reading search yes and there they are in                                 fact that for that particular for that                                 thing yes they still have time for some                                 more questions yeah I talked a lot about                                 leader initiated recovery is it possible                                 to have a replica initiated recovery                                 where it knows the desire of day instead                                 of a leader kind of a replica initiated                                 initiating recovery for what so they                                 know if the replica knows it's out of                                 date it can go and recover as opposed to                                 the leader initiation know so the the                                 replica won't go into recovery                                 automatically as long as it doesn't                                 disconnect from zookeeper any questions                                 so how often does the T log replica I                                 think that was part of someone's machine                                 replicate from the leader who's writing                                 the index so right now that's not                                 configurable but it's set to have the                                 outer commit time so if you the way to                                 the way to configure right now is to set                                 the auto commit time and half that time                                 is going to be used for replication but                                 then if the that is just how when the                                 replication is triggered so if you is                                 there's really no strong definition of                                 how long it's going to take to see the                                 new data because if for whatever reason                                 one replication takes X amount of                                 seconds and that's more than the Neary                                 at time than than the auto commit time                                 the new replication is not going to                                 start until the other one finishes and                                 there could have been multiple commits                                 on the leader during this time so                                 eventually it will that it will be there                                 but it's not you don't know exactly one                                 are there any more questions okay if                                 none thanks Thomas Fitz take it off                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=C8C9GRTCSzY


