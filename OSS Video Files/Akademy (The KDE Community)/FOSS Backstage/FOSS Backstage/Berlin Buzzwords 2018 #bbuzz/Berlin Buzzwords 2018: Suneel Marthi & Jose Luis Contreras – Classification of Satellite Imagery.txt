Title: Berlin Buzzwords 2018: Suneel Marthi & Jose Luis Contreras – Classification of Satellite Imagery
Publication date: 2018-06-20
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Suneel Marthi and Jose Luis Contreras talking about "Large Scale Landuse Classification of Satellite Imagery".

With the abundance of Remote Sensing satellite imagery, the possibilities are endless as to the kind of insights that can be derived from them. One such use is to determine land use for agriculture and non-agricultural purposes.

In this talk, we’ll be looking at leveraging Sentinel-2 satellite imagery data along with OpenStreetMap labels to be able to classify land use as agricultural or non-agricultural. Sentinel-2 data has a 10-meter resolution in RGB bands and is well-suited for land use classification. Using these two datasets, many different machine learning tasks can be performed like - image segmentation into two classes (farm land and non-farm land) or more challenging task of identification of crop type being cultivated on fields.  

For this talk, we’ll be looking at leveraging Convolutional Neural Networks (CNNs) built with Apache MXNet to train Deep Learning models for land use classification. We’ll be covering the different Deep Learning Architectures considered for this particular use case along with the performance metrics for each of the different architectures.

We’ll be leveraging streaming pipelines built on Apache Flink for model training and inference. Developers will come away with a better understanding of how to analyze satellite imagery and the different Deep Learning architectures along with their pros/cons when analyzing satellite imagery for land use.

Read more:
https://2018.berlinbuzzwords.de/18/session/large-scale-landuse-classification-satellite-imagery

About Suneel Marthi:
https://2018.berlinbuzzwords.de/users/suneel-marthi

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi everyone so I am vanilla Marty and                               this is Jose Luis Contreras he'll be my                               co-speaker today attainder today we'll                               be talking about satellite image                               classification for land use so how many                               of you here have to deal with satellite                               images or you have a use case something                               like that okay this one and two okay                               so the agenda today is we'll have a                                brief introduction of the data set and                                the image description and satellite                                images yeah you have cloud covers so how                                do you classify cloud cover images and                                segmentation we will be talking about                                semantic segmentation here and the whole                                thing once you train your models the                                whole thing would be put in a streaming                                pipeline and we had used Apache beam for                                that a small demo and future work coming                                up so the goal here is to identify tulip                                fields from Sentinel to satellite images                                so this is an example if you look at                                this image here yeah this may be a tulip                                field and this may be it really feel to                                this band here so that's what you're                                trying to identify from satellite images                                so this is the typical workflow                                basically you download your images you                                filter it for cloud you figure out the                                images that have cloud cover and then                                you segment your images for Philippines                                [Music]                                okay okay so before we go into the                                detail slide deck let me just run                                through a quick notebook here yeah so                                let me run through this notebook so now                                we'll be doing the data acquisition and                                we're in there actually downloading the                                satellite images Sentinel                                       Sentinel to satellite and we have                                something called a web mapping service                                that's what WMS is for the Sentinel                                  images will be downloading the data                                using the web mapping service instance                                and so let's do that                                once we do that we overlay the images                                with the tulip fields what the tulip                                fields are so so we are using a package                                called I buy leaflet for overlaying the                                image with the different layers so let's                                look at an interactive display so for                                this particular demo we're actually                                taking a map of Den Helder in                                Netherlands a political map and                                overlaying that with a satellite image                                so this is Den Helder in Netherlands                                okay so this is the political map now we                                don't know what is here in this area and                                this area so that's what we get from the                                satellite images                                so this satellite image is coming from                                the web mapping service for central do                                so let's add the layer so once you                                overlay your a political map with a                                satellite image this is how it looks and                                what we are trying to do here is to                                identify the tulip beds which are across                                this image so let's do that in order to                                do that we need some ground truth as to                                what the images is and whether to live                                better and that data is coming from                                signage aside from geo pedia so yeah                                this is what you would see as output so                                that's where the tulip beds are so                                that's the goal of this presentation                                today how do you identify the culet beds                                from satellite images let me go back to                                slide deck ok so from here Jose will                                talk to talk talk us through the process                                for doing this so first of all the kind                                of data we are getting the satellite                                images are coming from Sentinel                                        is a satellite mission from the European                                Space Agency and it has two satellites                                which revisit each each region it's five                                days the data is coming from                                            bands ranging from RGB to up to the                                shortwave infrared but we will only be                                using RGB here for different reasons we                                can cover later the spatial resolution                                for this images it's                                                    for the RGB bands for the other bands                                some of them have                                                   some of them                                                        they follow a free and open data policy                                which makes it pretty convenient for us                                ok so to download the images we use a                                tool which is very similar to the one                                shown it just showed you it was                                 developed very material and basically                                 what you have to do you select your                                 polygon and it downloads for you the                                 different tiles with all the satellite                                 images these data is in RGB as I                                 mentioned before                                 and the images we are downloading our                                 chips of                                                                 here we many times have traveled with                                 lovely images this image is nice but the                                 other one is super cloudy so that's a                                 problem we have to solve go ahead that's                                 a problem we have to solve because                                 otherwise there's not much segmentation                                 we can do if all we can see is clouds so                                 how do we filter these clouds                                 we are gonna we are gonna train a neural                                 network to filter images as clear or                                 cloudy and for that we are going to use                                 resonate                                                              networks because they are one of the                                 best networks for computer vision                                 problems and also we found out well we                                 didn't find out there is an art there is                                 some articles that they tend to transfer                                 learning better which will be useful in                                 this case as we will see later so this                                 is the basic building block of our resin                                 it here the most interesting part maybe                                 is this connection here which is a an                                 identity mapping which basically will it                                 allows resonance to be deeper than other                                 networks so you can have resonate                                    which is                                                                 layers and even deeper networks so one                                 problem as always with neural networks                                 is how to find training data for this                                 network because we could label data                                 manually from our data source but it                                 would be pretty time-consuming so we                                 found this dataset in color which is                                 coming from this planet                                 understanding the MSM from space                                 competition and it contains                                        images which are labeled as clear like                                 this one hazy partly cloudy or                                 completely cloudy so apart from that                                 data we also took                                                   Sentinel to download it with our tool                                 and we had to hand label them there was                                 no other approach so from these images                                 we have like T                                 the percent of them which are cloudy and                                 from the Cal competitions we have like                                 thirty percent of them are cloudy I told                                 you before that there are four classes                                 in the Cal competition but here we will                                 only consider to clear and cloudy and                                 the cloudy class in globe's all the                                 three other classes like he is partly                                 cloudy and cloudy we will consider them                                 all as cloudy because basically if we                                 have any kind of clouds in the image it                                 could be disturbing for our segmentation                                 network so we prefer to filter them out                                 so this is how we are gonna distribute                                 our data we first of all have the out of                                 which we use                                                        resonance and we save the final quarter                                 for validation and choosing a model and                                 then from the                                                           we use again                                                          the network we chose and the last                                        test the system and evaluate the                                 accuracy so these are the results we got                                 as you can see both resonate                                        resonate                                                                 of performance what you can see here on                                 the right the a box is the the number of                                 training epochs we we had to convey for                                 each of them as resonate                                             simpler it takes less time to converge                                 so instead of                                                        train it for                                                          you see there is the number of epochs on                                 the fine-tuning data set we'll run them                                 on so given that we have almost the same                                 accuracy but it's a bit better for                                 ResNet                                                                   less less time to train it makes sense                                 to choose less than                                                    example of the result these are as you                                 can guess these are the cloudy image                                 clear images sorry and the other ones                                 are the cloudy so the thing is after                                 this whole process of filtering out the                                 clouds we don't have that much data we                                 have like                                                            there are many clouds in the Netherlands                                 so we need to you do some data                                 documentation to                                 have more data for our segmentation                                 Network which we will see later so for                                 this we use augmenter which is this                                 Python library which I highly recommend                                 as you can see it super easy to define a                                 pipeline you define a couple of                                 operations and then you can get your own                                 mental images the transformations we                                 each charge q share in flips and                                 rotations so we can see them in the next                                 slide here on the here you have the                                 original image and that's the result of                                 applying some random transformations to                                 it to troupe some changes in                                 perspectives and rotations but we can                                 still identify the two defeats from it                                 so now on to the segmentation part which                                 is where we will identify the tulip                                 fields so this is an example of the                                 expected outcome of the segmentation on                                 the left you have the image with the                                 tulip fields which are pretty                                 identifiable on in red and what we                                 expect to get are the polygons defining                                 the bounding boxes of these tulip fields                                 so how do we do this we use a unit which                                 is a state-of-the-art                                 CNN for segmentation it's gotten like                                 the best results in most competitions                                 and in most applications where it has                                 been used it was originally designed for                                 biomedical images but we found that it                                 works pretty well for this too you can                                 see the paper is pretty interesting this                                 is the architecture of the network which                                 has a contracting part here in which we                                 get a smaller smaller layers each time                                 but with a higher number of features and                                 then we have an expanding part on the                                 other side which is here in which we                                 observe all and also the most                                 interesting part is this skip                                 connections here which map directly                                 information from the first layers to the                                 last ones so that we will kind of keep                                 some spatial information in the end of                                 the network then now we have there we                                 have two slides about the implementation                                 of the building blocks the most                                 interesting part to see is the                                 convolutional block which is the basic                                 block                                 which contains a convolution about                                 normalization and they are real wacked                                 evasion and then we have the down block                                 and in the next slide the a block in                                 case you're interested next one because                                 we don't have that much time okay so for                                 the training data of this unit we are                                 using the satellite images we have but                                 we also need ground truth right so the                                 ground truth is coming from geopdf from                                 synergize like Sony showed you before                                 and it comes in the form of these images                                 right here where we have the white                                 polygons representing the tulip fields                                 and the rest of it is blood so I will                                 before about one more perfect the                                 instead of starting by the loss function                                 let's start with the evaluation metric                                 which we used intersection over Union                                 it's probably the most used metric for                                 segmentation tasks and as you can see                                 it's basically the rate here between the                                 intersection of what you predicted to be                                 tulip fields in this case and the union                                 of what you predicted to be tulip fields                                 and the ground truth it's also called                                 the jacquard index and it's pretty                                 similar to the dice coefficient which is                                 the function we will be using a slot                                 which is this one here so this one is                                 the short dice coefficient loss it's                                 similar to the dice coefficient is                                 similar to intersection of a union but                                 it measures kind of if you use it on a                                 whole dataset it measures something more                                 similar to the average performance                                 instead of the worst case performance                                 which would be the intersection of a                                 union somehow so that's why we chose it                                 and some interesting things we can have                                 here prediction which we have there has                                 the form of a probability that's why we                                 call it soft dice coefficient instead of                                 the regular one and it's the softmax                                 output of the unit we have before and                                 yeah well we have a minus because we                                 need to make it our loss function                                 so the results we got on this we got an                                 intersection of a union score of                                      after training for                                                    this in context normally intersection of                                 a union you consider                                                   was considered to be a good result right                                 now                                                                  standard good result and to compare our                                 results with some other state-of-the-art                                 results we found the most similar thing                                 we found is this the stl Cargill                                 competition which was also satellite                                 image segmentation and the best                                 researcher was getting                                                   over union but the difference here is                                 that they were segmenting crops versus                                 buildings or water or different things                                 in our case we are segmenting tulip                                 fields from any other kind of thing                                 which can also be corrupt so it's                                 difficult to compare the results but                                 yeah that's what we have now Sammy will                                 until yeah so once you have trained your                                 deep learning models the resonate and                                 unit so obviously you want to deploy                                 them in production and you want to go in                                 for streaming inference and most people                                 any frameworks that we have today are                                 python-based for whatever reason I wish                                 it was JVM based so how do you put this                                 in a streaming pipeline and obviously                                 one approach was to maybe make an RPC                                 call from a fling fling pipeline to a                                 Python model yeah sure you can do that                                 it's doable it's doable                                 the other approach was to go with the                                 Google bean to go with Apache beam which                                 has a Python SDK and a Java SDK so what                                 is Apache Beam                                 it's a agnostic unified batch in                                 streaming programming model as many of                                 you know it's called support for Java                                 Python and go SDKs and yeah it's quite                                 several backhand runners for Flinx Park                                 and Google dataflow and a local data                                 runner                                 so via Apache beam why Apache beam so                                 number one it's both portable portable                                 in the sense that you're writing your                                 code against the beam API and you can                                 run it on a fling core a spark executor                                 or any other executor that's supported                                 by beam and the second reason is it's                                 got a unified batch and streaming API                                 and the third reason is a bit burn model                                 SDK it's quite extensible model and SDKs                                 so you can use the custom SDK to write                                 your custom syncs and sources so the                                 Apache beam mission has been that again                                 Apache beam is a project from Google                                 that's been open sourced into Apache and                                 the vision is you have a different                                 categories of users if you look at this                                 the architecture of a beam you have a                                 beam Java Python and other languages now                                 they have go as of last week I believe                                 or maybe last month so the end users                                 create pipelines in a familiar language                                 it could be Java or Python and then you                                 have that different category of users                                 like SDK writers the folks who write who                                 want to who want to create new language                                 packs a new language support for being                                 so the counts you can make that                                 available for different languages here                                 and the most important part is the                                 runners the backend runners so today we                                 have fling spark dataflow and epics so                                 you could have a different streaming                                 engine as for this so if you look at the                                 beam programming API if I write my code                                 in Java beam API using the Java Beam API                                 you create your pipeline using Java Beam                                 API and then you specify your runner you                                 want to run it on fling a spark or do                                 you want to run it locally and if you                                 look at the beam API they have something                                 called FN or function runners and these                                 function runners are defined in the                                 high-level language and there that's the                                 theological language and they're                                 translated for each of the specific                                 runners linker spark and executed on                                 those frameworks so overall for this                                 project this is your inference pipeline                                 so jose trains the models jose trains                                 resonate model in the unit model and how                                 do you plug that into a streaming                                 pipeline so this you ingest your                                 satellite image data you call the                                 resonate                                                                unit for segmentation all of that is one                                 beam                                 and we are running it on the flink                                 executor any questions so far so this is                                 for this particular code this is how the                                 beam inference pipeline looks like you                                 specify your pipeline options which                                 would be your input output the model                                 folder and this is the typical beam API                                 with beam pipeline SP you read the                                 images first and then I'm creating a                                 window of all the images that kind of                                 create a mini batch for inference and                                 then you filter out the cloudy images                                 and once you have the filter images you                                 take the filtered images and segment                                 them for unit classification so this is                                 the typical beam pipeline so obviously                                 if you look at this flow again yeah each                                 of this is a beam function each of this                                 steps here is a beam function and coats                                 and beds for these so this is for a                                 cloud classifier and do Flynn is it's a                                 beam convention every function is a                                 doofen and do you specify that as a part                                 of a power tube which is again the beam                                 API so for you are filtering out the                                 cloudy from this is filtering with the                                 cloudy images so I'm loading I'm loading                                 my resinate here as part of the                                 initialization process and then I have                                 once I have that resonant image I am                                 calling yeah I'm calling the making the                                 predictions here for the resume for the                                 resonant images I guess a resonant model                                 and and that's you go get you up and I                                 get a collection of all the clear images                                 the ones that are not loading so the                                 next step of once you have your clear                                 images the next step is Jose explained                                 was to segment those images for tulip                                 fields so this is the beam API for that                                 so you have a clear clear images which                                 is a collection of file names and then                                 then you create a mask mask image and                                 then you save that as your output so                                 let's do a big demo                                 so here okay so the one challenge that                                 we had been building this verse be may                                 appear suppose only Python                                     as of today okay it's not on Python                                 three eight they're working on it                                 nevertheless and yeah so the some of the                                 code was written in Python                                                                                                                 Python programmer it was kind of like a                                 living hell for some time trying to get                                 this working and the other challenge was                                 the flink Runner for beans Python SDK is                                 still experimental and thanks to the                                 Google folks they really got it to work                                 in some shape for this talk hats off to                                 them                                 so let's this is on Python                                              me run this so it's going to take some                                 time it's take some time to start up and                                 load reload the deplaning models so well                                 that's running well that's running let's                                 talk about some important links so most                                 of the satellite image data is available                                 on it appears at AWS Earth and you can                                 get this at you have data sets from the                                 sentinel                                                        different satellites here both an                                 infrared band in the forex near-infrared                                 as well as in RGB format and to                                 understand better what unit is at how                                 semantic segmentation works there's a                                 very interesting medium.com post by this                                 gentleman it's really interesting and                                 explains very well how unit works and of                                 course the papers on resonate and unit                                 you could look at yeah and of course the                                 beam api's and this light tag here so                                 let's go back and see how our                                 so this is going to take some time so                                 let's how are we on time okay so this is                                 going to take some time and yeah this is                                 something I was I was kind of like maybe                                 I should just put some German messages                                 and you know translate them in English                                 using a noodle machine translation model                                 where this was running sure okay so                                 let's say leather keep running but this                                 is a sample output that you would see so                                 here you have this satellite image do                                 you see any tulip fields here or nothing                                 so the output of that would be a blank                                 black screen okay so let's look at the                                 next image yeah do you see any tulip                                 fields here yeah there is something here                                 and something here and here so that's                                 that's the output that you get so using                                 a unit segmentation so that's where it                                 identifies the way that you lis fields                                 are and again let me remind you we are                                 only using three channels GP channels we                                 are not using the near-infrared and you                                 know further bands which would provide                                 much more information now the last                                 sample here so this is seems to be a                                 small chilly field and so that's the                                 output of that but do you really think                                 it's actually field it doesn't look like                                 one to me it's a house yeah it's a                                 building and the reason again for that                                 is because we are only using the small                                 RGB three channels three bands if you                                 had been using more infrared band then                                 maybe we had more information that we                                 could go against so yeah it's still                                 running okay give it some time so given                                 this image which kind of you know brings                                 brings which kind of serves as a segue                                 to what the future work would be so                                 classify rock formations so we are only                                 using the RGB bands but if you are using                                 the short shortwave infrared images                                 which are kind of in the                                            nanometer range all l appearing in                                 remind                                 the images that we have are in the                                    meter range resolution if we can go with                                 further resolution like                                                point                                                              classify rock formations well we all                                 know that plants don't grow on rocks so                                 that's a data point that we could yield                                 leverage and also all the objects rocks                                 they emit radiant flux it's called the                                 amount of energy that they radiate out                                 per unit time which is given which is                                 known as radiant flux and that's the                                 equation for that you could use that as                                 information to classify whether this is                                 a rock or a building or it's a crop                                 field the other the other future use                                 case would be how do you measure crop                                 health so for this you can actually go                                 with the near-infrared radiation                                 images so plants obviously have                                 chlorophyll and missile fill and other                                 pigments and each of those they give out                                 you know they give us some radiations                                 infrared radiations and the amount of                                 chlorophyll content differs between the                                 plants as well as in the various growth                                 stages of the same plant so depending on                                 the infrared radiation that you get from                                 the plants we can determine the health                                 of the plant as well as classify this                                 plant as dooley fields versus a Rose bed                                 and of course the very last use case is                                 just go with red band so have you ever                                 wondered like if you look at an image if                                 you look at any satellite image without                                 there being a clear clear demarcation of                                 the boundaries between the countries or                                 the different regions yet just by a                                 visual looking at it visually you are                                 still able to figure out you know this                                 is this could be this is Netherlands and                                 this is Germany and this is where                                 Germany starts and Netherlands stops                                 this is Amsterdam versus Berlin have you                                 ever figured out why you and I will to                                 do that even though you're just looking                                 at the infrared in just a plain image                                 it's because most of the images are in                                 red band which is more visually                                 appealing to humans and it's kind of                                 like a clustering what you're doing is a                                 clustering oh this this particular                                 region is Amsterdam versus this is                                 Berlin                                 so yeah that's definitely a possibility                                 going forward okay great I have this                                 running                                 [Music]                                 so this is the beam that's running okay                                 beam API so if you look at this these                                 are the different transformations that                                 are happening and yeah that it's trading                                 about that I'm reading about ten images                                 and you know calculating the mask for                                 those ten images okay so we are done                                 with that let me switch to                                 so let's look at this guy it's hard man                                 so this one do we see any may Julie                                 feels here yeah I see some Reds but is                                 that really fields maybe not let's look                                 at the mass for this divulge it to my                                 mother                                 okay so this is the mask for that this                                 is the mask for the images or before                                 it's a nice black screen so it's a nice                                 black screen which means there's no                                 tulips there so let's try a different                                 one                                 yeah yeah there's simply habit to leap                                 bed there let's try a different one                                 maybe not no okay let's go with this so                                 this is another sample image so it's got                                 something here could that be truly bits                                 yeah maybe so let's look at the mask                                 this is your original image and this is                                 the mask so the original image had some                                 tulips here and up here and there's                                 something here yellow band so this is                                 what the mask came up with which means                                 yeah those are possible to look fields                                 okay so this is kind of an example of                                 you know this is all running in a bean                                 streaming pipeline so you can have we                                 can actually get the images live from                                 satellite and you have your train models                                 you put your train models in a streaming                                 pipeline and you can make an inference                                 this way                                 so yeah this code is running on a flink                                 runner by the way when the beams beam                                 python is decaf link runner let me                                 emphasize that it's still experimental                                 but nevertheless it's working in some                                 shape okay switching back to PowerPoint                                 okay so we talked about this so that                                 kind of concludes the stock and some of                                 the credits for this Jose of course and                                 kelan and Matthew from Amazon Berlin                                 here who have done this earlier bus from                                 Frankfurt he was the guy who came DUP                                 came up with some of the ideas that we                                 had used here and he's an expert on                                 computer vision and land use                                 classification and of course the Apache                                 beam folks from Google they have been                                 very helpful to get this Python SDK run                                 off of link in some shape working and of                                 course a few other folks from Amazon                                 here Pascal and Jade Thunderball Jet                                 Center Bell is the guy from Amazon who                                 maintains the who maintains the data set                                 and AWS the one I had pointed to earlier                                 so Earth on AWS is maintained by jets                                 under wall and of course a few of the                                 open NLP folks and others who have been                                 helping out with this slide tech at                                 least one being the slide deck so with                                 that concludes our talk any questions                                 [Applause]                                 hi thanks for the talk so I'm just                                 curious do roofs cause a lot of problems                                 for detecting two tulip fields whether                                 roofs just out of curiosity roofs of                                 houses is a kind of a square which is                                 more or less red so should they                                 definitely do and the reason for that is                                 we are only using so for example this                                 one the exact reason for that is we are                                 only using three bands                                 red green blue whereas if you have been                                 taking more infrared bands and                                 additional bands you would be getting                                 better resolutions for example this one                                 this not it you Liefeld it's a house                                 it's a building and it's a roof of the                                 building so it's kind of you know                                 classifying that as actually filled                                 because it's red in color sexual talk my                                 question is how long this check to try                                 train the units and which harp I did use                                 two trains we were using a media Teta                                 and V it didn't take too long it took                                 like nine hours to train it                                           and then we chose the best performing                                 epic which was after                                                 train it in                                                              another thing for training the deep                                 learning models what we had seen is you                                 train for different deep learning models                                 with the different type of parameters                                 and you get those models average them                                 out train the fifths model with average                                 values that works the best okay it works                                 the best that's it so it's kind of like                                 an in-sample training hi thanks                                 so just a follow-up on that point Sunil                                 if you're saying you run for models and                                 get an ensemble of them here right SJM                                 air so you know my intuition for hyper                                 parameter searches that you would run                                 many experiments maybe grid search or                                 and you know nowadays people are doing                                 model architecture search would you have                                 benefited from a firm of GPUs where you                                 could do large-scale so                                 before you then start your                                            training yeah definitely yeah                                 and I totally agree with you Jim and                                 that you're right                                 and one other point that we do not                                 mention here was the number of epochs we                                 are chosen for the training the models                                 so we kind of tried with different                                 epochs number of epochs and we kind of                                 plotted the graph and the accuracy of                                 the different one and we just went with                                                                                                        kind of found that                                                  elbow happens in the graph when you plot                                 the graph versus accuracy number of                                 epochs was the accuracy and similarly                                 for                                                                    accuracy was about the same we just went                                 with the resonate                                                   model compared to                                                       unite we didn't have it yeah unit I                                 think we went with                                                       your question Jim could have benefited                                 with a GPU cluster so this most of this                                 stuff was trained trained on a single                                 GPU incidence Titan no more questions                                 I'm just curious for them for the                                 recognition of tulips it it looks like                                 it's a lot based on color did you ever                                 try to to build a model that just looks                                 using open CV and look looking basically                                 using statistical approaches for for                                 energy or object detection yeah no we                                 did not but yeah that's when I                                 approached to definitely consider well                                 the idea for this the idea behind this                                 whole project was to actually use deep                                 learning from the word go                                 and so we've okay well thank you                                 everyone for coming and thanks to our                                 presenters here and we're gonna make a                                 short quick                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Sosu9zKMPzo


