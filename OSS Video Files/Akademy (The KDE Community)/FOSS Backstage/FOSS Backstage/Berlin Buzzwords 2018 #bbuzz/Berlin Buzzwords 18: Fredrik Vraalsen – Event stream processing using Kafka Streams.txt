Title: Berlin Buzzwords 18: Fredrik Vraalsen – Event stream processing using Kafka Streams
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Further information: https://berlinbuzzwords.de/18/session/event-stream-processing-using-kafka-streams

This workshop will give you hands-on experience using Kafka Streams to solve a variety of event stream processing problems. The examples and exercises are based on real-world usage from our stream processing platform in Schibsted, used to process over 800 million incoming events daily from users across the globe. We will cover topics ranging from the basics like filtering and transforming events, to how you can use Kafka Streams for data routing, aggregating events, and enriching and joining event streams. To solve the exercises we'll be using Java 8 and/or Scala.

If you like this video why not share it ▶ https://youtu.be/OwA_gpWt3xA

If you're new, why not subscribe to our channel!

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right thank you welcome everyone to                               this workshop as I saw I already had                               this slide up on some getting started                               notes if you want to follow along with                               the exercises or do them on your own                               later and you can clone them from this                               repository normally I've done this                               workshop as more of a half day long                               thing so I'm not sure we'll see if we                                get much time for the exercises but if                                someone wants to stay after the talk and                                work on that together with me though                                that's fine as well we can find                                somewhere to to sit down hopefully we'll                                have some time to walk through some of                                them at least the github URL will be at                                the bottom of the slides moving forward                                so quick agenda for today just a quick                                introduction and then go into a little                                bit about why you want to do stream                                processing we'll dive into a bit more on                                Kafka and Kafka streams and then we'll                                go into more detail on the Kafka streams                                API and how to develop using that we'll                                cover some of the basics like filtering                                and transforming your data and routing                                it to different places and then we'll                                look at also some examples of                                aggregations and time handling of time                                all right so first of all Who am I why                                am I here to give this talk so my name                                is Frederick Wilson                                I work as a data engineer in shipstead                                in the Data Platform team there where we                                work with all these kind of cool                                technologies a lot of stream processing                                using Kafka we also use spark for                                example for our batch processing most of                                you are probably not heard of shipstead                                but it's it's a company that owns a lot                                of classified sites and media sites                                across Europe and other parts of the                                world as well so for example leboncoin                                in France block it and Finland Sweden                                Norway are some of the big ones but we                                also others are in other parts of the                                world so these are kind of three main                                business areas okay all right we also                                have tech hubs in multiple places in                                Europe we have about                                                 spread across mainly these sites so in                                my team in the data platform team we are                                sort of responsible for gathering                                event from all of these different sites                                and making them available to our                                analysis teams so during the course of a                                week you can see this this kind of                                pattern that we have so we receive                                around                                                              clickstream events mainly but also other                                types of events from around                                         sites across the world as you can see                                the load varies a lot across the day so                                we need to be able to scale to vary                                traffic by the way feel free to stop me                                if you have any questions along the way                                and just raise your hand I'll try to to                                catch it so briefly going through our                                data pipeline like I said we receive                                events from various sources from our                                apps on iOS and Android our web sites of                                course through our own tracking solution                                and from a set of back-end systems or                                components as well like for instance                                messaging events payment events and so                                on all of these go into our micro                                service named collector aptly named                                where we then push all those events onto                                amazon Kinesis for that's kind of our                                first stable storage for the events from                                there we pick up the events and pipe                                them into our two main pipelines so one                                going to s                                                       pipelines where people where we sort of                                gather all the events into our early                                data sets and people can do various                                analysis jobs on top of those and then                                we have our streaming pipeline which is                                what we'll be talking about today                                using Kafka and Kafka streams our team                                is mainly as I said responsible for                                making the data available to the                                different consumers so what we use kafka                                streams for in our team is mainly to                                sort of distribute the data split it                                into different data sets in here we get                                everything into one event firehose so we                                need a sort of d-max it again in two                                different types based on the event type                                 and what site comes from and so on for                                 example then we send it to various                                 downstream consumers                                 we support the main services in Amazon                                 sending it to classes an s                                            for messaging based handling or various                                 third-party tools like amplitude for                                 example which is an analytics tool that                                 we use for some of the analysis purposes                                 right so in addition to our the team the                                 pipeline of our team has built in terms                                 of doing this processing for the stream                                 stream processing we also have other                                 teams that have built on top of our                                 platform using kafka streams so one a                                 couple of examples are one team has                                 built this data quality analysis tool                                 that we use for our events to ensure                                 that the events contain the right kind                                 of data that are useful for the                                 downstream analysis teams checking the                                 formats of the events the content and so                                 on and making this so we can get this                                 closed loop in terms of the developers                                 on the sides can look at these - person                                 see whether the events they sent to us                                 are ok as expected as I mentioned we've                                 done integration with an analytics tool                                 so we have our own tracker solution for                                 for the web and our apps name pulse and                                 and so instead of having to use many                                 different trackers on the sites we try                                 to then instead send the events from                                 from our tracker into multiple analysis                                 analytics tools doing transformations                                 along the way into the format that they                                 require and so on so this is allowed us                                 to easily set up new integrations with                                 third-party tools sort of negotiated                                 with amplitude for example and how to                                 how to send events into their platform                                 this is allowed us to onboard new sites                                 and components into these analytics                                 tools very easily we also created a                                 number of other data-driven applications                                 on top of this ranging from sort of                                 experiments experimentation tools like                                 for a/b testing for example maybe you                                 want to talk about my colleague                                 yesterday on geoip                                 enrichment of the events so that's one                                 of the things we were built on                                 of this as well we have image feature                                 ization applications that take images                                 are uploaded for example to the                                 classified sites and do them image                                 analysis on those and post this using                                 events coming from from the streaming                                 pipeline another another example is                                 messaging intent analysis so you can                                 tell based on a conversation between                                 users what their intent is and then                                 offer sort of specialized UI for example                                 for handling some of the replies or                                 something like that                                 alright so why you want to work with                                 streaming well I mean typically you want                                 to use stream processing systems for                                 getting more real-time analysis results                                 right or in the lower latency maybe you                                 are familiar with the lambda                                 architecture where this is sort of                                 proposed - a couple of years ago where                                 you have the idea at the time was that                                 you had you typically had your batch                                 jobs processing events every hour or                                 every day but you wanted to have more                                 online more real-time results as well so                                 you added this stream processing layer                                 here or the speed layer to get more                                 up-to-date results and then you would                                 sort of merge the results from the two                                 layers in when you create for for data                                 the idea here was kind of your speed                                 layer was giving you faster results but                                 they were also more inaccurate you                                 didn't have all the results or you might                                 not have the same guarantees for                                 processing and so on the process of this                                 batch layer was that you had all the raw                                 data you could do reprocessing quite                                 easily and so on did any of you catch                                 the talk by large albertson talking                                 about the ten failures of data                                 engineering yeah so you talked about                                 this sort of it typically have more                                 tools support for doing reprocessing and                                 stuff on the batch side if you have                                 errors in your code you can fix them and                                 redeploy and reprocess the data more                                 easily but I think it has sort of                                 changed a bit in the last couple of                                 years so it used to be the case that you                                 didn't have the same kind of guarantee                                 when it came to the accuracy for example                                 of the stream processing but this has                                 changed with the new frameworks are are                                 available now you're much better at                                 gangees and in some ways stream                                 processing is actually now sort of a                                 superset of what you can do in batch                                 talking about stream processing what do                                 we kind of mean well first of all when                                 you talk about batch processing for                                 example you typically talk about bounded                                 datasets you work on an hour or a day of                                 data or so on whereas in stream                                 processing you have unbounded datasets                                 there's sort of potentially no end to                                 the data you always have new data coming                                 in so this is a feature of the stream                                 processing frameworks that they support                                 handling this kind of data you might                                 also have more need to handle unordered                                 events events come in out of order or                                 delayed for example this can also happen                                 in a branch layer though I mean you                                 might have a device that is offline and                                 then suddenly comes online if you're for                                 example traveling by plane suddenly you                                 have events that are several hours late                                 coming in and and most batch pipelines                                 that I've seen don't really handle that                                 if you haven't events come in that late                                 they are either discarded or included in                                 some later data set or something                                 so stream processing has sort of                                 built-in features to deal with this kind                                 of things and in the newer things like                                 Kafka streams and flink and so on and a                                 newer versions of the other ones as well                                 you have much better correctness                                 guarantees you have guarantees for at                                 least ones processing or even exactly                                 once processing of events you have                                 consistency in terms of storage and                                 handling of failures and so on and you                                 have more built in ways to deal with                                 time and especially as I mentioned for                                 example handling late events typical                                 example is that you you you I got group                                 dates and now use in in time windows and                                 then later on you have events coming in                                 to that actually belong to that same                                 time window the stream processing                                 frameworks and then typically allow you                                 to publish and updated results for that                                 time window so you can have                                 incrementally                                 more accurate results as you as you move                                 along and it gives you the sort of the                                 knobs you need to tune to deal with this                                 alright so let's get into cough grab it                                 how many here are using Kafka that's                                 about half or so cough cough dreams                                 anyway a few people more or less the                                 same people I think alright so just a                                 brief intro to cough Carden                                 so there are quite a few people we're                                 not using it so Kafka is essentially a                                 log based system so you can think of in                                 principle Kafka writes events to an                                 immutable log you start at the beginning                                 you just append events to the end and                                 you can essentially keep the events for                                 how long you want consumers of the                                 events can then read this log at their                                 own pace and sort of so if one consumer                                 is reading from here the other is                                 reading from here they're totally                                 independent of each other and they can                                 read the same data you can add new                                 consumers that can start consuming you                                 can also do things like rewind and                                 reprocess the data quite easily just                                 move the point or essentially like if                                 you were here you can just reset to the                                 start of the log in reprocess for                                 example if you have fixed a bug in your                                 processing algorithm then you can just                                 reprocess and recreate the more correct                                 results in addition in order to achieve                                 higher higher performance                                 what Kafka does is it splits this log or                                 a topic as it's called in in Kafka into                                 multiple partitions that are spread                                 across the Kafka cluster so typically                                 each each server in the Confessor will                                 have one or more partitions belonging to                                 a topic and this allows you to spread                                 the rights to a topic out across the                                 cluster the other side of this is that                                 you you lose sort of global ordering                                 here but you still have ordering within                                 each partition so if you are smart in                                 how you assign data to the different                                 partitions                                 keys in the events then you still have                                 can achieve the ordering that you need                                 typically and of course these topics and                                 partitions are replicated across the                                 cluster so that if one of the nodes in                                 the cluster goes down then you can just                                 resume considering from another server                                 or writing to another server so on the                                 consumer side you have sort of the same                                 picture where you have used these                                 partitions for to achieve parallelism                                 but you also have this concept of                                 consumer group which so a single                                 consumer I can have multiple threads                                 consuming the data from a given topic                                 and what Kafka will do then is we'll                                 essentially just automatically divide                                 the available partitions in this case we                                 have twelve partitions to our topic and                                 three consuming threads so each of them                                 we will get four partitions to consume                                 and these threads can be running on the                                 same machine or on different nodes in                                 the cluster or whatever depending on                                 your what kind of hardware you use and                                 and so on now if you need more                                 processing power you can easily add                                 another consuming thread and Kafka will                                 automatically distribute the workload                                 among them so you don't have to deal                                 with this at all and it because of how                                 it takes care of offsets and                                 checkpointing and that and such you will                                 ensure that you will always have still                                 always process everything that's needed                                 you can tune this in terms of the                                 guarantees that you want I think the                                 default now is set up for at least once                                 delivery of events but with the new                                 features in Kafka on the row and Kafka                                 streams now you can also have it do                                 exactly once processing in events and                                 transactions across partitions and so on                                 all right                                 so so what are the events that we have                                 talked about so far the Kafka Kafka                                 events are essentially a key value pair                                 but Kafka doesn't really care what the                                 keys and values are - cough no it's just                                 a byte array so it's up to you to bestow                                 meaning up on this and use a sort of                                 serialization and deserialization                                 whether you but Jason or strings or                                 Avram messages or whatever on the Kafka                                 topic Kafka doesn't really care the only                                 thing it sort of cares about is this key                                 that determines which partition your                                 message will end up on so for instance                                 if you are if you have a set of events                                 for a user you might want to use the                                 user ID as a key so that all of the                                 events belonging to the same user end up                                 on the same partition and and in order                                 right you don't have ordering guarantees                                 across partitions but if you use same                                 key for events belonging together then                                 you can achieve that that ordering                                 looking at stream processing frameworks                                 to then actually process this data this                                 data there are a number of options so                                 here are some that we have evaluated                                 when we were building our platform we                                 were looking at spark streaming for                                 example since we were already using                                 Street as part for our batch processing                                 and things like acha and Samsung and                                 flink which you probably have heard                                 about or even are using but we ended up                                 going for Kafka's own solution which is                                 kafka streams and this is something they                                 released a year and a half ago or                                 something maybe - it was fairly new at                                 least when we when we picked it up one                                 of the main reasons for for us to use                                 this was that it's essentially just a                                 library a lightweight Java library that                                 you can run and include in any java                                 application and you just deployed as a                                 regular time application and the kafka                                 mechanisms takes care of all this                                 distribution of the partitions and so on                                 to your consumer so there's no need to                                 set up like a cluster that you need to                                 deploy your application into and stuff                                 like that                                 this is nice for us as you could see I'm                                 on my first slice we had very sort of                                 varying amount of traffic along the day                                 so we we wanted to be able to use auto                                 scaling mechanisms in Amazon Cloud for                                 example and that allowed us to do this                                 suggest deploying as a regular job                                 application and on                                                      scaling cluster and based on the load                                 you could just remove or add notes as                                 needed and the Kafka library took care                                 of distributing the work among the                                 available notes like most of the stream                                 processing frameworks you have this                                 notion of both of streams events and                                 also tables so what you can do is you                                 can use various aggregation mechanisms                                 for example to to get aggregated values                                 for for your entities and store them in                                 tables that you can then look up and                                 join with and so on and we'll have some                                 we'll have a little bit of look a little                                 bit about that later on also you have a                                 high level DSL which looks very similar                                 to the stream API if you start in Java                                 or if you use functional programming                                 another language just like Scala for                                 example but you also have access to the                                 low level sort of processing topology                                 and and framework if you didn't really                                 need that and we have used that in a                                 couple of cases as well so Kafka streams                                 trials I saw this graph from there one                                 of their documentation to trying to sort                                 of hit this simplicity to become a                                 simpler to used and some of the other                                 frameworks that you have while giving                                 away a little bit of power but still you                                 can do quite a lot of stuff using just                                 Kafka streams so some of the features a                                 comfort of streams provides very similar                                 to a lot of the other stream processing                                 framework so of course is that you can                                 you can of course filter your data you                                 can use this to of course tailor the                                 stream to just what your consumer wants                                 this can also be useful in for example a                                 privacy setting if you want to filter                                 out events for a user status set that                                 they want don't want to be processed and                                 so on                                 you can transform the events bringing                                 them into the form that's expected by                                 downstream consumers for example or                                 whatever you just removed the data that                                 you don't need also from a privacy                                 standpoint that's quite nice if you want                                 to do data minimization so you only keep                                 the actual parts of the events that you                                 need for your further processing can of                                 course compute various aggregates all                                 the typical stuff like counting and                                 creating a computing sums and so on but                                 also a lot more and you can deal with                                 this aggregation typically in terms of                                 time windows so you can aggregate the                                 number of clicks on the site per hour or                                 per minute and so on Kafka handles the                                 aggregation or the time windowing done                                 for you and of course you can also join                                 streams together and also streams on                                 these I could get tables together to                                 enrich your data by joining multiple                                 data sets together or streams together                                 alright let's go look at some code so                                 the basic anatomy of a caustic extremes                                 app alright so it consists essentially                                 of three parts if you write a kafka                                 stream sort of pure Kafka streams up you                                 have some configuration that you need to                                 sort of the minimal complication is you                                 need to tell it where to find the Kafka                                 cluster and also then an ID or a name                                 for your for your application the second                                 part is that you need to build your                                 streaming stream processing topology and                                 we'll get into that more in the                                 following slides and finally you need to                                 of course start your Kafka streams                                 processing so essentially it is create                                 an instance of Kafka streams give it the                                 topology and configuration and tell it                                 to start and typically also registers                                 some kind of shutdown hook so that it                                 shuts down properly when when your                                 application is terminated so these are                                 the three main things that you need of                                 course you might need more configuration                                 to deal with setting up timeout values                                 and stuff like that but I don't want to                                 go into details on that right now                                 but let's have a look in more detail on                                 what building a topology looks like so                                 that's where the fun is right that's                                 like that's the actual actual work so we                                 have this dreams builder let's see what                                 we can do or stream builder I think                                 that's a typo here actually                                 they changed the API and naming of                                 things in one dollar or slightly so we                                 have a stream builder and as I mentioned                                 Kafka doesn't care what the data is two                                 guys just bytes so you have to tell it                                 how to serialize and deserialize the                                 data and it uses this concept called a                                 third short for serializer deserialize                                 err and it has a bunch of them built in                                 so you can this relies string byte                                 arrays into strings for example in this                                 case so I have here I'm building I'm                                 consuming a stream or a topic called                                 articles and I'm saying that consuming                                 this with a key type of string and value                                 type of string and this gives me back a                                 case stream which is sort of the basic                                 component in Kafka streams and okay                                 since this is hello Kafka streams                                 example what do we do with it we print                                 it out to stand it up and this will show                                 something like this so here you have                                 sort of the the keys comma the values as                                 you can see the keys are not in order                                 because they are spread across multiple                                 partitions in this case I have only one                                 consumer so it will consume all the                                 different partitions in one but it will                                 receive sort of random it will be random                                 ordering between the partitions so                                 that's why you don't see one two three                                 and some one here but you'll have the                                 evidence coming from the different                                 partitions interleaved as you can                                 probably also see the actual events or                                 values here are seem to be Jason so if                                 we want to consume the events as Jason                                 sort of strings we can also tell                                 Kafka seems to do that so what we need                                 to do then is create it doesn't provide                                 this by default so we need to create                                 this JSON absurd but it's essentially                                 four lines of Java code and a bunch of                                 boilerplate and if you clone the                                 repository you'll see an example of how                                 you can do that                                 so it's essentially just using jackson                                 in this case Jackson Jason framework and                                 it's three or four lines of code so and                                 then since I then specify that the value                                 type is now of Jason I get a case stream                                 of string type for key and Jason node                                 for my value and the printout still                                 looks the same since it's still Jason                                 oftentimes you will have the same it                                 will repeat the same sort of key and                                 value types a lot in your application so                                 instead of having to specify this                                 explicitly you can also configure the                                 defaults here so you can in this                                 configuration section that we showed                                 earlier you can add configuration of                                 default serve classes for your keys and                                 Alice if you want to and then you can                                 just say create a stream from this topic                                 and leave out the consumer all right                                 time to move on to some more more code                                 more examples of what you can do any                                 questions so far or no everything our                                 own keeping keeping up that's good                                 alright so I mentioned you can do                                 filtering so we we have our our stream                                 of article data from a various news                                 sources and I want to fetch just the                                 ones that come from BBC now if you if                                 you know this the the stream API in Java                                 for example this will look very very                                 similar you do a call to filter this                                 will take in the key value pair that you                                 can do the filtering on and you just                                 return true or false depending on                                 you want Evan to be filter out or not or                                 sorry the opposite if you don't keep                                 this keep the event or or not so in this                                 case I'm just looking for the events                                 where BBC equals the site of the Jason                                 event and this will give me them back a                                 nuke a stream with the same type but                                 with only the events that I expect if I                                 print this out I'll get only these three                                 events we can do more of course we can                                 do not just filtering it also transforms                                 so if I want to have just not the entire                                 event but just the titles for example                                 how do we do this so in this case I can                                 use the method called math values which                                 takes in the value of the event and just                                 returns a new value out so in this case                                 I extract the title event the title                                 value from the jason and return that as                                 a string or its text now you can see                                 also my case stream type the type of my                                 value has changed from jason of the                                 string that's expected now if i print                                 this out of course i get just the titles                                 the same keys though i haven't changed                                 the keys is any of you are any of you                                 using Scala with a few couple so yeah                                 they've made this for some reason even                                 though Kafka itself is implement in                                 Scala it's a bit hard to actually make                                 the API work nicely from Scala but                                 they're fixing this in in Kafka                                     they're adding a separate Scala API and                                 that's actually supposed to go into code                                 freeze today with the release candidate                                 so we'll see hopefully alright of course                                 you can also join these two together                                 like you can with a regular stream API                                 in Java so you can just have this as one                                 expression where you do the filter and                                 map as a chain of operations now we're                                 still just printing this stuff out and                                 that's probably not what you want to do                                 right you want to put this data back                                 into                                 into your system so that you can do                                 quite easily writing it back into Kafka                                 by using this two method on your case                                 stream just provided with with the topic                                 that you want to write to and then also                                 specifying how should you deal with what                                 the data how do you convert it back to                                 byte arrays and then if I from my                                 command line I I can use the command                                 line interface for consuming data from                                 from my topic and it will show me these                                 three titles from from the data that I                                 had alright so this is the map values is                                 sort of this implicit transform you can                                 do you also have sometimes you need to                                 convert one input event into a series of                                 output events for example if you extract                                 a list of data from inside of your event                                 object so converting from one input                                 event to too many or zero in that case                                 sometimes if you again if you know                                 functional programming or the stream API                                 you can use the flat map operation to do                                 this and that's essentially that that                                 takes in the value and you return an                                 iterable of whatever type output type                                 you have so this works nicely for                                 collections and arrays which are                                 iterables in in Java sometimes you work                                 with iterators and that is a little bit                                 more hassle in particularly in the case                                 here of of Jason so I want to extract                                 not the title of the article here but                                 the list of authors so there could be                                 one or more authors of a newspaper                                 article for example and this will                                 actually fail because this returns an                                 iterator are not an iterable and the API                                 doesn't support that so but it's quite                                 simple to create an iterable because                                 what an iterable in java is                                 essentially just a function or a class                                 that returns an iterator and we already                                 have the iterator so we can do that                                 quite easily but there's a simpler way                                 to do this since this is a single                                 abstract method we can replace this                                 whole thing with just an operation or                                 anonymous function that a that has no                                 input parameters but returns an iterator                                 so you get this kind of funny-looking                                 structure here so you have your input                                 value and it actually returns a function                                 here so we have a function that returns                                 the iterator and that works so that's                                 how you can extract multiple events but                                 if you if you already just if you have                                 something the returns a collection                                 directly you don't need to have this                                 sort of funny-looking operation here yes                                 yes since I'm doing flat map values I                                 don't change the key you have similar                                 operations called flat map and and just                                 map that allow you to also change the                                 key of the events I'll get back to that                                 a little bit later                                 but I sort of as I mentioned the key                                 determines where your events end up                                 right so if you change the key you need                                 to move them around                                 essentially it we'll talk a bit more                                 about that later the final sort of basic                                 thing that I wanted to mention is that                                 you have support for branching of events                                 if you want to split your events into                                 multiple streams you can do that so                                 essentially you just call a method                                 branch and you provide it with a set of                                 predicates and it will return an array                                 of streams so in this case I have I want                                 to split pi events into streams for                                 articles from BBC CNN Fox News and then                                 whatever is not covered by those so I                                 will get back four streams and I can                                 these are just regular k streams so I                                 can write those out to different                                 the topics written and the things I've                                 gone through now are essentially what we                                 have used to build up our mainstreaming                                 pipeline the filtering transforms and                                 this branching is what we do to                                 essentially the building blocks that we                                 use to to move all of our incoming                                 events to the different downstream                                 consumers in the format that they expect                                 and so on using these simple building                                 blocks so I think we're pretty good on                                 time so have have you all been able to                                 download and get the code from the                                 github repo or at least some of you I                                 figured if you don't have it then if you                                 want to work something together with                                 someone that's also good I won't spend                                 that much time on this right now because                                 I think we I can go through some of the                                 examples later on and then you can also                                 work on this on your own or we can hang                                 out after after the talk and go through                                 more details sound good so I'm thinking                                 we spend about                                                        then do a short break and I'll go                                 through some of the exercises after I'll                                 go if people have questions please raise                                 your hand I can go around and help out                                 there are also some example code showing                                 the different things so let me actually                                 open up my IDE to show you let's see if                                 I can get this window here where has it                                 gone I display no mirroring all right                                 oh can you it's a bit small maybe let me                                 change the font size here so is that                                 readable I hope so if you open this                                 repository you will see that you have                                 examples and exercises here and the goal                                 of the exercises is to essentially                                 they're written as tests that initially                                 will fail so if you run the Gradle build                                 you'll see that you have have like                                 sixteen failing tests or something and                                 the goal is to actually have the tests                                 pass by filling out what I pass in is                                 this stream builder that we saw earlier                                 and you're expected to create enough to                                 topic containing some particular data as                                 described in the in the in the sort of                                 documentation above each of these                                 methods so there's one sort of just                                 basic example here which just passes the                                 events through unchanged just to give an                                 idea                                 there are also some example code in the                                 neighboring package here that you can                                 have a look at as well if you are                                 adventurous and want to do this in Scala                                 there are similar examples and exercises                                 in Scala but you need to change a value                                 here in this exercise base to uncomment                                 the Java exercises and sorry I've I've                                 already done that but so you need to                                 switch these two if you want to use the                                 scholar once so that's not good alright                                 let me know if you I'll walk around we                                 spend about fifteen or twenty minutes on                                 this I think and I'll go through some of                                 the examples and exercises after and                                 then continue to continue with in the                                 second part of the talk so raise your                                 hand if you want to if you have                                 questions are any any issues on head                                 around                                 all right just one thing which I forgot                                 to put up here is that if you don't have                                 the Scala plugin for IntelliJ it might                                 not be able to compile so in that case                                 you need to compile from the command                                 line using Gradle you can still use                                 their ID or any text editor to edit it                                 but I think it requires the Scala since                                 the tests are written in Scala                                 so I I got a question about the slides I                                 just uploaded them earlier to                                 slideshare.net so you go there and                                 search for Kafka streams workshop and                                 Fredrik you should probably find them                                 hopefully otherwise I'll put up a link                                 after                                 all right I think I'm gonna start just                                 going through some of the exercises show                                 you some of the solutions and we want to                                 unfortunately have time to go through                                 all of this now but I see at least a lot                                 of you have gotten started and that's                                 good as I mentioned earlier if you wanna                                 hang out later and work on more of the                                 exercises that's I'm happy to do that or                                 you can also do it as homework of course                                 all right so let's see I'll just go                                 through a couple of the first ones to                                 show you what it was the what's expected                                 here in the git repository there's also                                 a solutions branch that you can check                                 out and have a look at if you want so                                 you can see some suggested solutions                                 here most of these examples require only                                 like three or four lines of code so what                                 we want here if we look at the exercise                                 we get in the set of strings on lines                                 and we want to return just a length of                                 that string so that means sorry we need                                 to create our input stream first from                                 the topic text and that is consumed with                                 strings and and strings right as before                                 so we have our our input stream we can                                 call that lines so now we have we input                                 our input data can people read okay in                                 the back there I'll I can even collapse                                 this one and make it a slightly larger                                 [Music]                                 like so so on lines we want to do map                                 values then since we are transforming                                 our value from the text into the actual                                 length of the text so we have a line in                                 this case as input and we are just we                                 just want to return the length of that                                 so we can do that or as IntelliJ is                                 happily reminding me instead of doing                                 line lengths here I can replace this                                 with a method reference if I want                                 just referring the the length method on                                 the string I am kind of partial to                                 actually using this form so but up to                                 you lengths and finally we want to write                                 this out to our output topic with the                                 two method the topic name from here line                                 lengths and then produce not concerned                                 with strings as the keys still but now                                 our values are our integers so we need                                 to use the different insert which so the                                 cells are provided up here and here you                                 can also see this Jason node sir that I                                 mentioned earlier if you want to look at                                 how that is implemented you can navigate                                 to that and you can see that there's a                                 bunch of empty boilerplate stuff the                                 next section handling but it's                                 essentially essentially just like four                                 lines of actual code anyway back to this                                 and we can of course we don't have to                                 assign everything to intermediate                                 variables here so you can inline a bunch                                 of stuff and typically you will see                                 something that looks kind of like kind                                 of like this that's fairly common to see                                 a format for it now words per line it's                                 very similar but instead of counting I'm                                 gonna start with the same code                                 essentially just to get a bit faster                                 here instead of counting the number of                                 characters we only count the number of                                 words easiest way to do that is to split                                 the line                                 spaces and then just do the length on                                 that and that will be words per line now                                 let's see if we can actually run some of                                 the tests here                                 no okay                                 exercises then I'll just find my                                 exercises here why is it no no here                                 here's the actual tests don't need to                                 run run so if you run this from the                                 Gradle command line actually that works                                 too but it will output quite a lot of                                 information so sometimes it is if you                                 see in the in the IDE what their stuff                                 is working or not but as I mentioned                                 requires that you have the Scala plug-in                                 installed so if you don't have that and                                 use the grade one now                                 it's bit small here but maybe you can                                 see here now that the three first tests                                 are green and the rest are still red and                                 that's the remainder of the exercise                                 also I think I'll show one or two more                                 just to show you some examples so here                                 all the words is an example where we                                 want to split one line of text into                                 multiple events so we can start out with                                 the one that we had up here again                                 perhaps but now instead of splitting and                                 returning the length we actually just                                 want to return the words themselves but                                 this doesn't work with map values of                                 course because then we will instead                                 return our data type with the array of                                 strings but we wanted still the output                                 to be strings just individual words and                                 not the whole lines so this is where we                                 want flat map values and it doesn't                                 let's see split this is work again sorry                                 good oh yeah so the trick here since it                                 doesn't handle arrays directly but it                                 does handle collections you can use the                                 built in arrays                                 health care class and do as list and                                 that will give you and of course our                                 output type is not in spot its strings                                 and the compiler is happy the only thing                                 we need a fix is the name of the output                                 topic so now if I rerun my tests                                 hopefully we should have one more green                                 test and yeah we do get all the words of                                 screen ok yeah if you guys as I said                                 when I go through more of this after the                                 talked and I'm very happy to do that but                                 I think we should move on with some more                                 of the content and I also had some                                 questions and in the break here that I                                 wanted to cover at the end if we have                                 time so see if I can switch back here I                                 wanted to turn off mirroring back into                                 presentation mode alright so we covered                                 and you saw some examples and worked a                                 little bit with some of the basic                                 features that allow you to do                                 essentially kind of ETL like the                                 behavior using kafka streams just taking                                 your events moving them from somewhere                                 to somewhere else and possibly doing                                 some transformations along the way but                                 sometimes you want to do some                                 computations on your data as well right                                 and that's where the aggregations come                                 in so if we stick with our example of                                 news articles I want to compute I don't                                 want to see just the raw stream of                                 articles I want to compute the number of                                 articles that are published first site                                 and this is very similar to what we do                                 when we get our click stream events and                                 you want to count the number of visitors                                 for example parasite and so on what you                                 can do then is instead of doing your map                                 and and so on the case stream itself you                                 want to create something called a cake                                 stream so what you do is you essentially                                 you take your input events and you tell                                 it to group your event by some by some                                 value so you based on your key and your                                 value you can group your events in this                                 case by the site of the article and what                                 this does is essentially create a new                                 key for your events so you're you are                                 rekeying your events so that all of the                                 events that belong together end up in                                 the same partition I had to actually                                 sort of borrow this this picture is                                 borrowed from a spark documentation or                                 talk but what you want to do is in order                                 to be able to to do aggregations on a                                 set of data you need to ensure that all                                 of the events that belong together are                                 ending up on the same node so that you                                 can process them together so that makes                                 sense so that's kind of similar to what                                 you do when you do is read partitioning                                 and/or inspark where you move data                                 around and you can spark there's a lot                                 of this sort of under the hood for you                                 in a more sense but in conference rooms                                 you need to be more explicit in terms of                                 what you want to group by or how you                                 want to group your events so this what                                 happens then in in Kafka streams is that                                 it will actually create an intermediate                                  topic for you that you write to with                                  this new set of keys so that the events                                  end up together that you want to do                                  aggregate this repartition will happen                                  not only by this group by operation but                                  by a bunch of different operations as                                  well as we talked already looked at map                                  valleys and flat map values they those                                  don't touch the keys so that they did                                  not trigger this tree partitioning but                                  if you use map which allows you to                                  change both the key and the value and                                  similarly for flat map or any of these                                  other operations by select key group by                                  key and so on they will trigger this                                  repartition and that enough sort of move                                  data across your gopher cluster so that                                  you you get everything together all                                  right so back to our code so we've done                                  this group by operation here you                                  back a group dream and this has a new                                  set of message that we can apply and the                                  simplest one in this case is we can do a                                  call to account an account will return                                  not the stream but a K table you                                  remember we mentioned briefly that you                                  have this duality of kind of streams of                                  events and tables with aggregated state                                  so in this case we will create a table                                  which aggregates the state for each key                                  the string is the keys string here and                                  it's this site ID we will have an                                  increasing count of articles as we get                                  new events coming in so this here we                                  have sort of all of their events here                                  you will have a stateful store inside                                  Kafka stream so that you can query and                                  do things to it                                  and also join with other data streams so                                  we can again we can sort of how can we                                  look at this well first of all we can of                                  course also put this thing together as a                                  single stream of transformations if we                                  try to print this out it will typically                                  show or something like this so it will                                  not necessarily show us all the                                  intermediate states as it also                                  periodically output new aggregated                                  values I mentioned this is a duality                                  because you can actually look at the                                  tables and streams as two different                                  viewpoints on the data so if you have a                                  database table you can easily create a                                  stream of the changelog events for your                                  data right so here we changed a row or                                  add a row so we can output this as a                                  changelog and then we add a new row we                                  can output this as a change drug event                                  now we change an existing row to new                                  values so we just output a new event                                  with the new value for that key and so                                  on and this is very very much what your                                  actual databases do right there are                                  transaction logs and so on so if you if                                  you think that your database actually                                  has everything stored in tables like                                  this it's typically that at least the                                  latest changes are in a log kind of like                                  this but you can do the opposite as well                                  given this stream of change events                                  you can recreate your table right that's                                  the whole point in a database                                  transaction log is that you can recreate                                  your table States so you can sort of                                  view these as two different views on the                                  same data the difference is that in in                                  the K table and typically in the                                  database you only see the last version                                  of your state but in your stream of                                  events or you change log you have every                                  change to that happen to your data so it                                  can be both can be very useful and but                                  this ability to go sort of back and                                  forth is kind of core to have a kafka                                  streams works all right yes I wanted to                                  also talk a bit about windowed                                  aggregations because those are very                                  powerful so you can do aggregations just                                  like across all time but typically you                                  want to group them into some time                                  windows so we've got streams lets you do                                  that very easily you can specify                                  different types of time windows you can                                  have tumbling windows you could have                                  time windows that overlap kind of like                                  you just you have a                                                       you move one minute ahead for example                                  all the time and and it will                                  automatically down together the records                                  that belong together and you can do the                                  aggregations within that time window so                                  you can count the number of events per                                  five minutes for example in this case or                                  do some other kind of aggregation maybe                                  you want to the count or some are sort                                  of the simple ones but you can                                  essentially do any sort of reduction                                  operation that you want or aggregation                                  operation you want on your your data as                                  you do get event access to the raw                                  events and you can combine them however                                  you want so if you want to concatenate                                  strings or do whatever build up a map of                                  values seen in a given hour or something                                  that's totally doable and then have that                                  sort of output                                  as a new changelog but with this time                                  window information on a new Kafka topic                                  right so let's have a look at what this                                  window will look like so instead of                                  having the simple count that we had                                  earlier we want to have this count a                                  number of articles seen per hour and                                  instead of so we still do this group I                                  hope raishin when you serve doing a                                  count we will on our grouped stream we                                  do a windowed buy and we give it a time                                  in it in this case one hour and then we                                  tell it to count and then we need to                                  give it a state store so what it will do                                  it will keep an internal state for this                                  aggregation in an internal key value                                  store essentially so by default Kafka                                  streams uses rocks DB as its internal                                  state so you can configure it to just                                  store stuff in memory or you can have it                                  actually spill to disk if you are having                                  large amount of state for example or if                                  you wanna have more durable durability                                  but even even if your the cool thing                                  about this is that been sort of behind                                  the curve behind the scene here Kafka                                  streams will create not only this                                  internal state store in the key value                                  store but it will create a change log                                  for that key value store as a stream of                                  events so you can always sorry if even                                  if your note goes down and you need to                                  take it back up again                                  you can just consume again this change                                  log and build up your state store and                                  continue consume consuming firm from                                  where you left off                                  yes yeah so it's both represented the                                  question was is it is it both                                  represented as a table and as a topic                                  yes that's exactly what it is so                                  internally it's represented as a table                                  in a key value store and as a changelog                                  topic with the changes to that table and                                  we saw when I showed briefly example of                                  printing it out and that that was                                  essentially that the changelog it                                  printed out from this table okay so                                  alright so we have this state store but                                  what can we do with it                                  well we can we can query it we can                                  connect to it this is just a again it                                  you can include kafka streams in any                                  regular java application so typically                                  what you will do is you will have an                                  application with kafka streams embedded                                  in it and then you can just query that                                  using for example a regular REST API                                  that you provide and that REST API can                                  query the internal state stores inside                                  of your Kafka streams app so remember we                                  gave we materialize this thing as a kit                                  as a as a state store and we can then                                  also query that so I can ask my Kafka                                  streams runtime for the state store                                  named articles per hour of type window                                  store and I will get back a query Ball                                  State store so from this I can say okay                                  I want to look at all the accounts of                                  articles for BBC from the start of time                                  which is in this case is January                                                                                                             will give me a list of all of the                                  aggregated states it has in the in the                                  state store so this iterator                                  I can iterate over using the regular                                  mechanisms in Java so for each key value                                  I can extract the key and the value and                                  just print out for example in this case                                  or I could respond to the REST API call                                  with a set of JSON objects so                                  whatever in this case I just print out                                  two standard out so I only have one time                                  window populated here so that's all it's                                  gonna give me but if I had not full ones                                  it would show me a list of up it so you                                  can you can have your rest the API then                                  fetch the state stores query for data                                  and respond with with that now this is                                  simple if you have all of your data on                                  one node right but typically you will                                  have a cuff custom zap or very often at                                  least if you'll have it distributed                                  across multiple nodes so the challenge                                  then is that your state will actually be                                  distributed as well since each node                                  along you know about the data given from                                  the partitions that it is consuming so                                  you might need to query across your                                  consumers and this is kind of where                                  Kafka streams doesn't really give you                                  everything for free but you still have                                  API so you can query to see where is the                                  data located for this given key and so                                  on but you might need to implement                                  things like retries and stuff if a node                                  suddenly goes down as you're trying to                                  query it and move data moves around so                                  you need to deal with those kind of                                  things on yourself yourself but if you                                  have something that lives in just a                                  single node and it's quite easy to to do                                  this and it depends on the amount of                                  data that you need to process an                                  aggregate of course it's yes                                  yes exactly so the state for a given key                                  if you if you add multiple consumers or                                  if you add additional consumers or                                  remove some your data or state will kind                                  of move around right that your yeah                                  excuse me so when you go to build your                                  state store again your state stores                                  gonna come from Kafka and rebuild itself                                  with the partitions that are associated                                  with that particular client yeah so what                                  the the new node that it's added for                                  example it will read this changelog                                  building up its internal states or and                                  when that is ready it will sort of take                                  over processing that that partition I                                  believe and of course then the client I                                  was the query needs to ask Kafka where                                  or they use the Kafka API to figure out                                  which of these nodes has that                                  information I'm interested in does that                                  make sense                                  cool now I have a certain set of                                  exercises but I don't know if we should                                  go into those right now I think that we                                  can let me see I have actually have a                                  couple of slides I wanted to go through                                  in addition to this I would rather spend                                  time I think if you guys have questions                                  and stuff but so another thing is in                                  addition to just occurring the state                                  stores for example you might want to get                                  data into other systems or out of other                                  systems                                  cough-cough streams itself only supports                                  Kafka right so you can only consume from                                  Kafka and produce to Kafka this is                                  different from a lot of the other stream                                  processing frameworks that support                                  multiple data sources and data sinks but                                  but they've chosen to keep Kafka streams                                  as simple as possible only the image                                  Kafka and they have other mechanisms for                                  getting data in and out of Kafka itself                                  so in particular confluent and i Kanaka                                  open source project as well                                  provides this component called Kafka                                  Connect which allows you to connect the                                  different data sources and things so it                                  could be a database it could be s                                       example and so on and it will you can                                  deploy various connectors into this                                  Kafka connect cluster and it will take                                  care of pushing data in and out of Kafka                                  for you and it will for for certain                                  setups it will also sort of give you                                  these kind of guarantees that you have                                  in for exactly once processing for but                                  that I think only if you're using Kafka                                  mostly Kafka intend with some additional                                  support but but this works very nice I                                  think for for connecting to various                                  sources and sending data to downstream                                  consumers that are not consuming from                                  Kafka the other alternative instead of                                  using Africa Connect is of course to use                                  the other parts of the calf ecosystem so                                  Kafka consists of many things you have                                  the cluster the server part of it then                                  you have sort of the regular Kafka                                  producers and consumers which are Java                                  client so no they I believe they also                                  have support for a number of other                                  languages you have the stream processors                                  which is what we have looked at now                                  which are kind of combined producers and                                  consumers they just they both consume                                  data from craft companies to it so                                  that's the Kafka stream API and then you                                  have this this connector API that I just                                  mentioned so you can write things using                                  regular producers and consumers and this                                  is actually what we are doing in our set                                  up is a we had a legacy stream based                                  solution already supported a number of                                  the things in such that we wanted so we                                  just sort of migrated that code to use                                  Kafka clients to consume from from Kafka                                  itself and that's quite simple it                                  most of our our connectors are just one                                  or two pages of code essentially just                                  going in a loop fetching data from craft                                  girl riding it somewhere else so that's                                  totally doable                                  of course you still have the option of                                  using Kafka Connect if you want to                                  Avista so the full full ecosystem I                                  really you had a question about s                                         if you wanted to move data to s                                       example so caf-co connect supports that                                  I believe there are also some other                                  other third-party solutions to do that                                  but I believe those are using the old                                  Kafka library or old Kafka versions but                                  what we are doing is essentially just                                  fetching a batch of events from Kafka                                  and writing them as a micro batch into                                  s                                                                        think we have time to do anything useful                                  here but if you are interested in more                                  details about our platform you can watch                                  a couple of talks that are available                                  also this is my colleague large Mario's                                  he did talk at Java Sohn so actually two                                  years ago but it still covers most of                                  our platform how we ingest in process                                  seven or eight hundred million events                                  per day from our clickstream data and                                  then some of you already saw my                                  colleague who cool yesterday he was                                  talking about doing this enrichment of                                  the events using IP to geo coordinates                                  look up he also I'm assuming that that                                  the version that he gave yesterday will                                  be available at the video from that as                                  well soon so see I'm actually quite                                  ahead of time but there's a bunch of                                  further reading that I would recommend                                  if you are looking to go in to start                                  using stream processing I would highly                                  recommend these two articles the bat the                                  world beyond batch they have a go into a                                  lot of detail on some of the aspects                                  that I've talked about today in terms of                                  handling of time how you handle                                  events that are coming in late and so on                                  and and going into very good                                  explanations I think on how that works                                  confluent the commercial package behind                                  Kafka also have a very good blog I think                                  with a lot of good articles on how to                                  use cough cough cough cough screams I                                  didn't talk about joints now but you can                                  if you guys want we can we can go into                                  some examples of that as well I have                                  some slides I can I can show if you want                                  because the talk by my colleague                                  yesterday showed some examples of how                                  that didn't work so well for us but I                                  can I can show some examples of how to                                  do that's in the streams API I just                                  unhide some of my slides here yeah so                                  joining streams and enriching your data                                  since we have about ten minutes left I                                  think that's a good topic to dive into a                                  little bit so don't use dreams do not                                  cross your streams right all right but                                  sometimes you do want that and and it it                                  has very powerful mechanisms for kind of                                  joining data together you can do                                  multiple types of joints in side of                                  Kafka's room so here I'm not going to go                                  into detail on all of this because                                  that's a long blog post on that content                                  plug-in itself but you can join                                     streams together or you can join various                                  combinations of K streams and K tables                                  and you have also this thing called                                  global K table which I haven't touched                                  on today really but it's essentially a K                                  table that has the same state across all                                  nodes so if you have some sort of                                  metadata that you want to enrich                                  everything all the air events with you                                  will put that isn't that big you will                                  put that in the global                                                   of your notes will have access to the                                  same same data and you can do different                                  types of joins here if you're familiar                                  with joins from this from sequel and you                                  know like you do in your Joyce left                                  joins and so on depending and so an                                  inner join will                                  join the events if they are if you have                                  events that match that coming from both                                  both sides a left join will join                                  anything coming in on the left side                                  whatever it has on the right side and an                                  outer join will join all combinations                                  together so giving you potential null                                  values here and there but depending on                                  what you want that's that's what's                                  available                                  so that's dime into short example here                                  so staying on the same kind of topic                                  with our news articles I want to see so                                  I have a set of click events essentially                                  articles being read and I have some                                  information about my users my logged in                                  users in the table so I want to join                                  these two together to enrich these                                  article event or click events with some                                  more information so in this case I'm                                  trying to get the reads the number of                                  eats by country right so I know which                                  country my users are are from so I can                                  do this                                  I can do a join of this table with this                                  stream so what I will have done is that                                  I will have article and user events                                  coming in to your ni I need to tell it                                  where yeah what is the output of that                                  join and in this case I'm just                                  interested in the users the country of                                  the user and then I can take this reads                                  by country and do what we saw earlier I                                  can do a group by that                                  the country name and the account right                                  the the trick here to be able to do this                                  join these two have to have the same                                  keys to be able to fork of code to be                                  able to join them you have to use the                                  same keys in both your left and right                                  hand side here so so what I need to do                                  potentially here if it isn't already                                  using the user ID as a key I will need                                  to do this kind of reelect key operation                                  I mentioned briefly before so that I can                                  get a new stream that has the same keys                                  as my user table because I'm making                                  first of all it will ensure that all of                                  the events that are supposed to be join                                  again end up on the same processing node                                  similar to when we did the aggregation                                  operation but once that is satisfied it                                  will do this join out automatically                                  output in this case all of the the                                  countries from this from for each event                                  coming in here it will look up in the                                  table find the appropriate user object                                  and I can extract the country from there                                  and produce essentially a stream that                                  just contains a list of countries right                                  so for this I see normally Norway Spain                                  Germany France and so on and then I can                                  group by that and do the count an                                  aggregation does that make sense                                  hopefully                                  joints are quite powerful and and it's                                  taken me quite a while to sort of                                  understand how they how they work how                                  they actually do this based on since                                  essentially when you're dealing with                                  streams of events of course it's not the                                  same as joining a database table where                                  all of your data is present the events                                  come in at different times right so when                                  you're joining this K table with K                                  stream that's kind of simple because it                                  will only for each event coming in or                                  the stream it will just look up the                                  corresponding value in their aggregate                                  state store but if you're joining two                                  streams together you need to deal with                                  what is the time window I'm looking at                                  for a join what is my join time window                                  and so on so you can configure that in                                  your joints saying I want to join every                                  matching event that comes in within five                                  minutes for example style minibike great                                  yeah I was asking how do you make sure                                  that an instance of your application                                  gets the same keys from the partitions                                  from the different topics yeah that's a                                  good question so how do you ensure that                                  the the same so the articles for a user                                  match up on the same note as the user                                  data for that user essentially yes so                                  the trick is of course there's two                                  things you need to do first of all you                                  need to ensure that you have the same                                  keys on the events as I mentioned so you                                  need to rekey your article read events                                  with the user ID for that event in                                  addition you need to ensure that your if                                  you're joining two streams for example                                  you need to ensure that both streams                                  have the same number of partitions                                  otherwise if one stream has five                                  partitions and the other has seven and                                  it will not be able to sort of match                                  them up so so that's a those are the two                                  kind of considerations you need to to                                  make there and and you can do this kind                                  of inside of Kafka streams                                  in the rear partitioning right there was                                  the question up here as well yeah so do                                  each one of these operations emit a                                  subsequent stream to Kafka so in the                                  joint operation for the joint operation                                  here will emit essentially one event per                                  event coming in on the stream here but                                  if you are joining two streams together                                  you can have multiple events as there                                  can be multiple matches inside of the                                  time the join window so and then the                                  output of the join is that another                                  stream yeah that's another stream                                  exactly here it's just a case stream it                                  may be materialized inside Africa                                  streams or inside your calculus or not                                  it might just be internally in your                                  Kafka streams application although I                                  think it will actually have it be                                  materialized                                  so there's I'm not going to go into                                  details on the joints themselves but                                  there's again these articles I mentioned                                  there's another one here that I had in                                  my read list goes into a lot of detail                                  if you want to figure out how joints                                  work this has a lot of examples on what                                  is the incoming events what is the                                  output depending on whether you doing an                                  inner stream drawing or an outer join                                  our table join and so on so that's yeah                                  that's the topic for a whole talk on our                                  workshop on its own enrichments so you                                  can also of course in rich events with                                  the information from external sources                                  and this is what my colleague talks                                  about yesterday because essentially your                                  spark code or sorry with your costumes                                  code it's just regular Java code you can                                  do whatever you want inside your Mac                                  values for example you can call out to a                                  third-party service fetch some data and                                  add that to your event and that's your                                  enrichment you also have this more                                  low-level API that I mentioned that it's                                  called the processor API or there are                                  two of them there's one called processor                                  and one called transformer they're very                                  similar but sort of different levels of                                  the API and you can use that for more                                  backstory nted processing where you have                                  you can collect a set of events and you                                  have some punctuation calls and stuff                                  like that so you can you can do more                                  better ain't it                                  if you want a for example batch events                                  up before you call the service instead                                  of calling it for each individual event                                  so I believe that's what the one we are                                  actually using now for our GOI B                                  enrichment instead of doing the joints                                  we're doing this this kind of enrichment                                  instead alright so yeah we went through                                  this thing already there's a lot of good                                  documentation on the confluent site                                  confluent a tile site on Kafka streams                                  and Kafka in general you also have                                  similar documentation on the Kafka                                  apache site as well so with that I say                                  thank you for listening                                  I think we have three more minutes if                                  there any more questions at the end                                  thank you this one here yes I'm involved                                  in the Apache agent project and I was                                  sort of while you were talking I was                                  sort of comparing it with what we're                                  doing there so it seems to be a perfect                                  addition to Kafka streams because                                  especially with IOT and edge devices and                                  usually don't have a cluster to work                                  with yeah yeah so having almost the same                                  syntax just without the key concept it                                  seems like a perfect addition to sort of                                  build your IOT applications yeah so the                                  first project I worked with the Kafka                                  was in an IOT setting where we had                                  sensors sending in events from                                  households about power usage and stuff                                  like that so it was a quite perfect fit                                  for that actually yeah any more                                  questions yeah I don't quite understand                                  Oh                                  rapid stream would work without being                                  window in time                                  don't you need a concept of start and                                  end to be able to aggregate data you can                                  do aggregations in time windows but you                                  can also sort of just your continuous                                  aggregation so from from the first event                                  that you consume until and so what will                                  happen in both global aggregations and                                  in the time window aggregations is that                                  well it Kafka will periodically produce                                  new incremental results right so even in                                  the time window once as I mentioned how                                  do you handle out of time other sort of                                  late events coming in that actually                                  belong to an earlier time window so you                                  can configure how long you are able to                                  deal with those events how long should                                  you keep that time window around and if                                  it sees new events coming in it will                                  actually publish an updated version of                                  that time window to its changelog so                                  when you query the state store for an                                  older time window you will get a new                                  results and it's kind of similar for                                  global aggregations without time windows                                  it will just publish new results                                  periodically for the                                  does that make sense yeah cool we still                                  have time for some questions if not I'll                                  happily stay around outside here or                                  somewhere and have a chat with you if                                  you want to ask me more stuff so oh                                  thank you again                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=OwA_gpWt3xA


