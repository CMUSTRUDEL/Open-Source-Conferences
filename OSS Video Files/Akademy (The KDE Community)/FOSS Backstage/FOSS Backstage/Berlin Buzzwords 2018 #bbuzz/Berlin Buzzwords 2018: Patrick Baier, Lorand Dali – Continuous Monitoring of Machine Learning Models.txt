Title: Berlin Buzzwords 2018: Patrick Baier, Lorand Dali – Continuous Monitoring of Machine Learning Models
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Patrick Baier and Lorand Dali "Continuous Live Monitoring of Machine Learning Models with Delayed Label Feedback".

The usual steps of developing a machine learning model are: training on a training set, tuning on a validation set and evaluating the performance on the test set. Often this is the end of the story. However, if the model is particularly good, it will be deployed to serve predictions in a production system. In this talk we present what happens to a machine learning model after it is deployed in production at Zalando Payments. We focus on the precautions we need to take to ensure that a model’s predictions always stay at the high quality we expect. 

The stakes are high, particularly for models that directly touch the revenue stream. Since we cannot afford to let a drop in prediction quality pass unnoticed, we need to continuously monitor our deployed machine learning models. As we operate in the fraud detection domain, one additional challenge we face is that we only know several weeks later if a customer paid his order at Zalando and if our predictions were accurate in that case. 

This makes the simple solution of monitoring the prediction accuracy impractical, because by the time we notice the problem, it is already too late. In this talk, we present our solution, which consists of monitoring the similarity between the distributions of features in the live traffic and the distributions of features in the test set on which the model was evaluated. This allows us to immediately detect if the conditions under which the model was evaluated have substantially changed, which would invalidate the conclusions we drew in the initial testing. We describe how the mentioned changes in feature distributions are automatically detected using the TDigest algorithm, and how alerts are raised. 

Further, we delve into the technical implementation decisions: First, we describe how we collect the live traffic of a mission-critical service in a non-intrusive way, in order to avoid interfering with the normal operation of the service. Secondly, we present how the collected data is processed in a scalable way using Apache Spark. Finally, we show how we automate everything with AWS Data Pipelines.

Read more:
https://2018.berlinbuzzwords.de/18/session/continuous-live-monitoring-machine-learning-models-delayed-label-feedback

About Patrick Baier:
https://2018.berlinbuzzwords.de/users/patrick-baier

About Lorand Dali:
https://2018.berlinbuzzwords.de/users/lorand-dali

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so yeah welcome everyone to our talk                               today which is about monitoring of                               machine learning so we are both from                               solando payments which is kind of sub                               company of solando so I think people                               that live in Europe have heard of                               solando before it's the biggest online                               fashion shop in Europe and this talk we                               want to give you a brief idea what we do                                in terms of data science there and then                                we go over to this main topic which is                                why we should monitor machine learning                                models then we I will talk about a                                little bit about prediction monitoring                                in general and law and in the end we'll                                give you some insights about our                                implementation of these topics so who we                                are and what we do so                                I'm Patrick and here's Laurent he will                                catch up later                                so we both work as data scientist at                                solando I'm there for three and a half                                years low understeer for one and a half                                years yeah and we have different                                backgrounds maybe basically in data                                science machine learning computer                                science so maybe more interesting what                                we do at solando so as I said before                                solando is the biggest fashion online                                shop in Europe so maybe for people who                                didn't heard it before it's like Amazon                                but only in Europe and only for fashion                                right so what happens typically in such                                online shop there are people they chop                                they put items in the cart they want to                                have this shirt and they spend and then                                we ship them the stuff and then they pay                                so we have a very special thing here in                                Germany it's called invoice payment so                                it's like when you buy something we                                would allow you not to pay immediately                                but only after two weeks right so that's                                some people not from Germany they they                                also they always think that's kind of                                weird but it's actually what our Germans                                love they don't trust internet very much                                they want to have something before they                                pay so but the problem with this                                approach is more or less that this could                                happen right so we would ship stuff to                                the customers but they will not pay so                                this is obviously not so attractive for                                solando to to ship them something that                                they don't pay so our job is kind of to                                prevent this okay so we are in the                                payment fraud department I can give a                                little bit more detail throughout the                                course of this talk but I will not talk                                like in detail how we actually exactly                                do this and how we prevent just payment                                fraud but at least you can get a idea I                                think so what we do for this we have a                                machine learning model so we will like                                charge every order and we'll kind of try                                to estimate a probability of someone                                pace or not right depending on this                                probability if it's very high probably                                we don't ship the order or I have other                                measures in place but the interesting                                thing for you is like we have orders in                                the shop and for every for every order                                we have to attach just for probability                                that's that's the end of our story                                that's what we do and how we do is if we                                like have data are lying on AWS three                                AWS we have and it's kind of a                                collection of all order data in solando                                and it's quite a huge big data set                                because solando is out there now for                                around ten years so we basically have                                access to all these orders and we can                                just check who paid in the past who not                                we try to build a machine learning model                                with apache spark on top which then use                                the stuff like locust regression random                                forest GBG neural networks whatever we                                have always tried to find new features                                try to find new model to make this                                product more accurate okay so it's a                                very classical binary classification                                problem you want to predict for one not                                for zero the thing we have to do which                                goes a little bit beyond machine                                learning is that we also have to                                kind of supply our predictions in the                                production system so we actually want to                                evaluate the fraud risk for every order                                very timely so other teams depend on it                                and we have to provide it within one                                 second so someone orders we have to say                                 fraud sir not fraudster and for this we                                 also built module a rest service which                                 contains this module that I showed you                                 just one slide before so it basically is                                 fed by some features X which of course                                 top secret and it puts out a four                                 probability so this rest service is run                                 in Scala Play and since SPARC is also in                                 Scala you know we can just load there                                 the models and do the prediction so no I                                 think that's that's what you need to                                 know about this slide in general what we                                 use in our team is quite diverse so if                                 we interview people it's kind of always                                 difficult to to like ensure that they                                 are full of this stuff but we actually                                 cannot like as we cannot have fun people                                 who know all of this but probably will                                 ask for people no Scala because we write                                 machine learning code in Scala and of                                 course they should know machine learning                                 but on top of this you know we use all                                 this other stuff like Amazon AWS for                                 deploying stuff Jenkins for production                                 stuff spark is very important our set up                                 and if we do experimentation we rely on                                 our pies and so whatever the data                                 science people prefer so this was this                                 introduction role so now we come to the                                 topic why we should monitor and I will                                 first give you some rather abstract                                 example which could happen like this in                                 an online shop but it's not very                                 attached to what we do but it could be                                 that you are working in such a data                                 science team in an online shop and                                 there's someone coming to you and say I                                 am deploy model for fraud detection in                                 online shop so you should basically                                 build this binary classification model                                 that I just described and deployed                                 production so there's different steps                                 you have to do obviously so the first                                 one is you have to somehow collect the                                 training data you have done to train a                                 model and finally you deploy to                                 production so let's look at these steps                                 in the bitchery                                 so at least what we do we kind of                                 collect our data training data in one                                 central store which is MSO nostri so not                                 sure I guess most of you know what it is                                 it's like on a big key value store which                                 is very cheap and you could just put a                                 lot of data there so if you have done                                 this online shop like solando you would                                 run around through all the systems                                 because you need features from                                 everywhere and some of this data may be                                 only accessible it locks because some                                 some other team had a live system which                                 produces these locks but doesn't write                                 it to data base so you have to fetch                                 this locks if you are a little bit more                                 fortunate it may be in a database where                                 you can just call this database or even                                 a data warehouse so but actually what I                                 really recommend is that you collect                                 this kind of proactively because what we                                 had in the initial stage is every time                                 we learned a model we went just before                                 learning to all the systems and try to                                 extract the data but this did not really                                 work well because the database is a live                                 database right so you should not collect                                 you in business high peak business time                                 their data and the log data is maybe                                 also not available so what we do we                                 collected offline during the night for                                 every day and put it to s                                                have some kind of dating training data                                 set so you have to be if you have smart                                 data scientists they will come up with                                 features which would kind of indicate if                                 this is fraud or not so one thing                                 probably could be that someone who is a                                 fraudster has some kind of a bot running                                 who does all the orders for you right so                                 maybe one good feature is the time to                                 order so what is put here is just the                                 time in seconds when someone comes to                                 your homepage and then the Tremonti                                 orders and                                 if what in this example here I have                                 several cases where they are like all                                 these secret features and we use but one                                 of them is time to order and you see                                 like there's a fraud case here and it                                 was only five seconds so maybe it was a                                 bot because no one could probably order                                 in five seconds                                 and most not most of the not fraud cases                                 are around between                                                 there's other one fraud case which for                                                                                                        not but just remember there is this kind                                 of time to order feature may be helpful                                 for our model so if we look now at this                                 feature and we would just plot some kind                                 of a histogram or kind of a empirical                                 distribution of it yeah we would see if                                 we just put plot this here with past and                                 it would be somehow normal distributed                                 around                                                                reasonable right so maybe most of the                                 people use actually this time when we                                 now go live there at leasts probably in                                 most cases there will be another service                                 which feeds you this features because                                 you a kind of machine learning data                                 science team and all this stuff that we                                 collected before offline for training                                 needs to be available life we would                                 really fast so probably there's other                                 team who sends you just exactly this                                 data and then you just have to predict                                 and put out this P fraud so that's how                                 it works in our case and once we alive                                 we get features X and over by different                                 microservice in real time so well and                                 mostly this team probably is just                                 services built by a smart software                                 engineers smart Big Data engineers but                                 not part of your team so now we come to                                 this monitoring part so you deploy this                                 new model and X come in P fort goes out                                 and now we all know microservices you                                 have to monitor all right you have to                                 look at stuff like CPU usage memory                                 usage latency all this kind of classical                                 stuff from the software engineering                                 world and if we say this is all cooled                                 and probably this thing works okay but                                 that's only one part of the story at                                 least if you have a machine learning                                 model because then this code happens so                                 some weeks later people are angry and                                 they say tell you that you did not                                 detect the fraud and the business is                                 ruined okay so maybe one point about                                 this what we have especially in our                                 domain if you want to take payment fraud                                 and there is as I told you earlier                                 there's this like this delay so people                                 have two weeks time to pay or not they                                 kind of you're only will detect if they                                 paid after two weeks right so if you're                                 if something goes wrong in your model                                 and it's producing now crappy                                 predictions you will find out two weeks                                 later if you have two labels right and                                 in this time your business could be                                 ruined so this is the problem we kind of                                 want to tackle and now you start                                 investigation because this business guy                                 just comes to you and say something is                                 really wrong look at your system and                                 then you maybe do the same thing again                                 that we just did and you say huh looks                                 fine I don't know what happened but now                                 comes this part where you not only look                                 at this this classical stuff but you                                 also could look at X and now we look at                                 X and we now see these values right and                                 they're kind of different to what we saw                                 in the training data and all if we plot                                 them now on this empirical distribution                                 you also see like there's a different                                 number which is now                                                 happened the mean shifted from two                                 hundred to two hundred thousand and then                                 you go poby to this team who built this                                 micro service feeding your prediction                                 system and then you find out that the                                 feature is not sent to us in militia in                                 seconds but in milliseconds right so I                                 think this is not really I did not                                 really happen so far our T but I think                                 it could happen because you know data                                 scientists they may be thinking seconds                                 and                                 everything is milliseconds right but the                                 obvious thing is now if this feature                                 tells you that then all these                                 predictions that you produced our                                 garbage or could be garbage if this                                 feature super important because now our                                 model is like very sensitive to detect                                 BOTS by saying if the small the time to                                 order is very small then it's probably a                                 thought so but it now gets feed all                                 these values it virtual say ah no BOTS                                 will like take like three hundred                                 thousand seconds to order anything right                                 maybe it's very undecided customer who                                 spends a lot of time on our homepage so                                 the the key point is here this is not                                 good so there are several problems we                                 lost we lost a lot of money and we did                                 not detect it in time and we could have                                 detected it in time and provided a fix                                 if we looked at this feature                                 distribution much earlier not only after                                 two weeks so the conclusion of this is                                 that we need to make sure that the                                 distributions of input features are                                 always the same as in the training data                                 because we train our model on data with                                 certain distributions and we only can                                 rely on these predictions that they are                                 really doing what we want if they would                                 be same the same kind of data although                                 in production and this brings me now to                                 prediction monitoring so I will like now                                 give rough overview and then in the                                 second part log and will tell you some                                 implementation details how we did it so                                 the first thing we want to monitor is                                 failing features so consider that once                                 you have this feature time to time to                                 order this could also be null a lot of                                 time so we are living like in an                                 uncertain world and this team that could                                 collect these features and send it over                                 to you they could also just put a null                                 in this field and what would you do in                                 your model you can not predict on a data                                 point where there is a null so what is                                 typically done you do some kind of                                 imputation right so you take some kind                                 of the median or average that you saw on                                 training data but this will still                                 kind of corrupt your prediction so one                                 very important thing in our world is to                                 monitor how often a feature fails and                                 there's some kind of natural failing if                                 it's only a few percent of predictions                                 but if you see this list so if this time                                 to prediction would be                                                cases then you should go to this team                                 that sends you this over and I should                                 ask them why this is the case something                                 now we come to the more elaborate thing                                 is what you really should do and what                                 what would have actually taken up this                                 case I introduced earlier is if you                                 compare the distributions of every                                 feature between the test data and the                                 live data so this picture just shows                                 show each small picture is a feature and                                 this is how it was distributed in tests                                 or training and how was it distributed                                 in life so I put their test data because                                 you could also do it on training but                                 typically the test data is the data                                 where you're very confident because it's                                 where you generated this performance                                 measurements like area under a curve and                                 thing and you want to behave your model                                 in life like you saw it when you                                 measured it right because this is where                                 you said it's good so you would compare                                 it on the test data and I have someone                                 plot here which we actually generate and                                 you have the three things so the red one                                 is the live data the blue one is the                                 test data and the green one is the Train                                 data so now you see like actually in the                                 live data there are much more values of                                 point nine say or not so but what does                                 this picture tell us is this is this                                 still good or not and the answer is that                                 there is no easy answer for this right                                 so what we kind of did is we monitor                                 continuously and we compare this                                 distribution to each other's and they                                 are like tons of come out of                                 distribution comparisons in statistics                                 one of them is a KS test which kind of                                 compares the cdf slow and we'll say more                                 about it but with this with how we                                 implemented us we can really say okay                                 this feature was looking and test like                                 this in life it's looking like this and                                 that's the difference this number is the                                 difference between this distribution and                                 if it was really high then it's really                                 bad and depending on your business you                                 have to define what is bad and what is                                 good because there's no Universal answer                                 to this so if somebody says okay no                                 features really it's a little bit                                 different but it's still okay there's no                                 like business effect we saw in the past                                 did not really suffer from this                                 distribution maybe it's just kind of                                 seasonality that's okay but this is the                                 thing you have to stretch all these                                 values and then you have to find out                                 your own what is acceptable and what not                                 and there are some other things so now                                 imagine we have notice system which kind                                 of compares even in certain time frames                                 the current feature distribution to the                                 test so there have kind of camera                                 parameters you can tune and the first                                 one is our big should be the size of                                 your aggregation window right because if                                 you want to do build this distribution                                 block you have to window it somehow and                                 count the number of occurrences and one                                 could be you take it where is very small                                 one then it means your I only look at                                 one out last hour and compare this to                                 the test distribution the good thing is                                 this is like you can detect anomalies                                 very fast but if you have seasonality in                                 the data you know that people in an                                 online shop behave differently in the                                 morning than in the evening then you                                 would get a lot of false positive alarms                                 and maybe the other extreme so as you                                 look at your affiliate gate data from                                 the live system over                                                   kind of aggregate out all this short                                 term seasonality s at least during the                                 day but you're slower of detecting                                 anomalies because you notice the sliding                                 window if there's an element in now the                                 sliding window will slide over it and                                 only if this anomaly makes most of the                                 window you will see like a difference or                                 something                                 the same as yeah you often should you do                                 this and this is kind of if we go live                                 here and then you have this                                         window you could maybe do this you know                                 every hour you could look back on the                                 last                                                                  for this so though more often you do the                                 more quicker you can define it come                                 detect anomalies but it's very complex                                 you know you have to it it costs money                                 if you do it at least on AWS and then                                 you have to do it more often so it's                                 more complex and it's also kind of                                 complex operation or you do it less                                 often then you don't have to spend                                 enough money but maybe you also have to                                 steal a again so actually I think there                                 are some approaches I think there was a                                 twitter paper where you actually could                                 really do this in kind of real-time so                                 with every few data points they will                                 kind of detect this anomaly but at least                                 for our system this kind of approach                                 works very well so we don't have to                                 build a very complex system to master                                 this okay and this live monitoring last                                 slide for me is why why we do it and                                 what we had in the past so the number                                 one thing is this technical problem so                                 the data that you get from other teams                                 is just not the data that you thought                                 your in test and this could be that they                                 have a problem on their side collecting                                 data that they have different units that                                 they implemented maybe this data                                 gathering feature different than you and                                 for this it's very useful since I'm kind                                 of out of time I will now give over to                                 Lauren for the implementation part I                                 will continue with going a bit more into                                 detail of how we implemented the things                                 that are at risk describe you so we                                 start with a central problem of                                 disappearance nation is how to measure                                 the difference between two distributions                                 automatically so in this picture we have                                 two distribution represented as their                                 probability density functions                                 or histograms so we have the one which                                 which is a blue and the yellow one so                                 the blue one has a peak around the                                 middle and the yellow one has two peaks                                 on one to the left and one to the right                                 so looking at this visually you can see                                 that is quite different but doing                                 something very simplistic like just                                 taking the average the average would                                 probably quite close to each other so we                                 define this distance measure between the                                 distributions which is just two                                 integrals so the denominator the                                 normalizing factor is just the area of                                 everything like this overlap                                 distributions and the numerator is the                                 area of the parts which which differ so                                 in practice we don't compute these                                 integrals we just take sample points and                                 we measure how big they are like for                                 instance there there is an example of                                 one one one difference and then we sum                                 them up and normalized and to be between                                 zero and one so if two distributions are                                 completely identical then the distance                                 will be zero if they are completely                                 different a distance will be one so we                                 do it like this except that we in we                                 actually don't keep track of the of the                                 histograms but we use the cumulative                                 distribution functions so this is not a                                 problem because cumulative distribution                                 functions and probability density                                 functions are kind of equivalent you can                                 go from one to the other by                                 differentiation or integration and the                                 reason why we prefer cumulative                                 distribution functions is that if you                                 have the histogram and the most common                                 implementation of histograms is that you                                 would take the minimum and the maximum                                 value that you see and then chop chop up                                 the space into equal size buckets and                                 then count how many things are there                                 so if the maximum is pretty far out on                                 your on your left then then most of the                                 data will be just in in one or two                                 buckets and it will not be very good so                                 so the cumulative distribution function                                 gives us kind of percentiles so what it                                 what it tells us is that for certain                                 value let's say zero how many how many                                 of the values are smaller than it so is                                 here we see about                                                      are smaller than                                                      for every value and conversely we can                                 see okay what is our sixty percentile or                                                                                                         like this some outliers so the                                 disadvantage of using percentage is that                                 percent is not so easy to compute like                                 histogram but we computed use it just                                 approximate approximately compute this                                 percentile and for this we use very                                 useful technique that is called tea                                 digest so I have put here some scholars                                 code just to show how easy it is to do                                 this so first we just import that that                                 library and then here we have two                                 functions both create and the first one                                 can create a tea digest which is a                                 summary of a cumulative distribution                                 function and it creates it from a in                                 memory collection so we just get these                                 numbers which is sequence of doubles and                                 then we just create the T digestant and                                 we just add each of the numbers there                                 and we we have the we have the digit                                 digest basically and from here we can                                 get percentiles and and such and if the                                 data is really big and we can store it                                 as a distributed collection for instance                                 inspark so we have this RTD of double                                 and it's not much more complicated than                                 than this the RTD will be will have                                 several partitions and for each                                 partitions we basically do this and                                 and we have a lot of different tea                                 digests so this happens in this                                 segmental operator and then we we have a                                 tea digest for every partition and to                                 digest a nice property that it can be                                 combined so all the digests of all                                 partitions are combined to the final                                 result in in the combining operation so                                 now let's look into how the data is                                 actually collected so before when we see                                 when we saw how we we measured the                                 distances we assumed we just have these                                 distributions but this values of these                                 features are collected from from                                 production so this is a schematic                                 overview of how our prediction service                                 looks like so we have a rest service                                 there is a prediction engine there there                                 are some machine learning models which                                 were trained previously and they are                                 loaded from s                                                        then prediction is made and the answer                                 is is sent out so this is working very                                 well and if we want to build this data                                 collection on top of it one thing we                                 want to take care is that we don't mess                                 up this system very much so we just add                                 them some components to the to the right                                 so we just add an S qsq and another                                 process which collects the data so this                                 if I'm not mistaken is also called                                 something like lumberjack pattern so you                                 log everything that you get into the                                 system and that you respond so you get                                 log all of this your request and the                                 feature that you have computed and and                                 then you send just send this off to an                                 sqs queue the the reason is that this is                                 not a critical process we are allowed to                                 fail in collecting some of the data                                 points here but we never want to be                                 delayed or failed because this this go                                 wrong goes wrong so we move this a                                 collection and saving of the data logic                                 to a different service to this                                 lumberjack process                                 and this process of collecting                                 collecting the logs here I have we have                                 simplified the code a bit that it can                                 fit to a slide so by throwing away all                                 the error handling and cleanup and other                                 stuff but basically is it still captures                                 the the main ideas so what we have here                                 is a function this go code so we have                                 this sqs interface then we have a dump                                 size like how much how much locks we                                 want to handle together then we have a                                 channel on which the process can be                                 interrupted and then we have like a                                 callback function once we gathered                                 enough locks this upload function will                                 do something with it put it into a                                 database or in s                                                    start to initialize these buffers and to                                 the timeout and then we have this                                 infinite loop which rich on the                                 interrupt channel if in case you get a                                 message there it has to stop in case the                                 time a timeout happens then also in that                                 case we upload the data or otherwise we                                 just read receive new messages from from                                 the queue append it to the buffer and                                 when the buffer reaches a certain size                                 we just upload the data so what what do                                 this upload function does in our case                                 it's just it creates a file and then it                                 it uploads it to s                                                     the World Cup on football starts I will                                 also mention that our systems are called                                 after football players and the name of                                 this system is platinum                                 so now putting it together in a SS data                                 pipeline so then the pipeline looks like                                 this so before this lumberjack process                                 collected all the logs and put them on                                 s                                                                       jobs which do these aggregations over                                 let's say this                                    our time window that Patrick showed                                 before so we take the locks and there's                                 some spark job which process this locks                                 it groups it by the model so there are                                 always several models in production                                 running and then failed features are                                 computed like how what percentage of                                 feature is failed then we create this                                 tea digest and we create the histograms                                 and then we measure the distances                                 between the different features and then                                 we save this to like a common data                                 format which can be used later to create                                 some reports where you can look visually                                 and compare or some some distributions                                 and look at some tables of which                                 features are missing or more useful we                                 can directly do some alerts thresholds                                 on something on the on the distance and                                 make alerts because usually there is no                                 time to look at a lot of this a lot of                                 these reports so one lesson that we also                                 learned is that is it was not good to to                                 make this job create the report or the                                 alerts directly because the thing about                                 reports is is that when you look at them                                 always you want to change something and                                 measure something else and so on so so                                 if we have an intermediate step then we                                 can easily just change the reports how                                 they look like what they show and                                 everything so the final notes if you                                 have a machine learning system in                                 production you obviously have to monitor                                 it somehow and this monitoring is                                 especially important if the performance                                 feedback comes with a large delay like                                 in our case because you cannot just add                                 at prediction performance and you have                                 to find some ways of seeing if your                                 system doesn't work beforehand so                                 usually there is a lot of research and a                                 lot of related work with which have a                                 very very complex way of monitoring but                                 it's better to just start simple and try                                 to not interfere with production systems                                 too much                                 because it's no fun running this by hand                                 all the time                                 it's better to automate as much as                                 possible and also if they are there                                 start to come out some best practices                                 around monitoring and if you want to                                 measure how far you are then I recommend                                 to answer the questions in this Google                                 paper what's your ml test score so                                 although we worked on it quite for quite                                 a while I think we're only about halfway                                 through so there's a lot more things to                                 do so                                 thank you very much I'm happy that you                                 came in such a large number and I hope                                 it is also useful for you so we are open                                 to questions now or later in the coffee                                 breaks Thanks                                 any questions hi thank you very much so                                 I'm wondering so I think it's great                                 what are you doing and I are also taking                                 initiatives to reduce this online                                 offline serving skew in the organization                                 away so because when I look at the                                 slides you had like the data team could                                 access everything like locks and the                                 data warehouse and just grab feature                                 somewhere and all the other code to                                 deploy there to develop the real-time                                 features scattered around the company so                                 other efforts to solve the problem from                                 the start and not from the end and                                 looking what went wrong you know what I                                 mean - like don't realize something or                                 have a data with feature repository or                                 something yeah I think it's a difficult                                 question so it's easy to let's say                                 easier to find out the problem where it                                 happens but some team that creates some                                 some data it's hard to foresee every way                                 it will be used so maybe this code was                                 written years ago and and our team maybe                                 didn't even exist then so it's hard to                                 like in theory this should be done but                                 in practice is quite hard to pull this                                 off so it's better to have each team                                 especially because if when they work                                 autonomously that each team monitor very                                 well their system and their their use                                 case is monitored so then hopefully                                 we'll each the whole is also working                                 well if the parts are working well yeah                                 ok thank you                                 more questions                                 if not thank you very much for your                                 presentation
YouTube URL: https://www.youtube.com/watch?v=h1ewJRLefhk


