Title: Berlin Buzzwords 2018: Jaeik Lee & Qin Tang â€“ Elasticsearch index management for paas style logging
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Jaeik Lee and Qin Tang talking about "Elasticsearch index management for paas style logging system".

In this session, we will introduce large-scale log management system called NELO used in Naver corp and mainly discuss how to maintain Elasticsearch indices for paas style logging system. Naver Corporation is an Internet content service company which operates Korea's top search engine Naver and manages global mobile services such as the mobile messenger LINE, video messenger Snow, and group communication service BAND. 

NELO is handling various different kinds of logs and more than 3 billions of logs are incoming every day. As backend storage and search engine, we are heavily depending on Elasticsearch. Because the number of logs and variety of logs is increasing, managing indices in Elasticsearch clusters are more and more complicated. In the beginning, we only created one index every day, but as scales are growing, we suffered mapping explosion issues and performance issues. By introducing index management service inside NELO, now we have resolved mapping explosion issues and supported custom type and custom retention time, etc. 

In this session we will explain our first and recent index model and how to resolve mapping explosion and how to support custom type. From this information, users will be able to understand difficulties of maintaining large scale Elasticsearch cluster and index model for multi-tenant log management system which can cover many different kinds of logs with different mappings.

Read more:
https://2018.berlinbuzzwords.de/18/session/elasticsearch-index-management-paas-style-logging-system

About Qin Tang:
https://2018.berlinbuzzwords.de/users/qin-tang

About Jaeik Lee:
https://2018.berlinbuzzwords.de/users/jaeik-lee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning everyone this is Jay and                               working for neighbor in Korea                               it's good money I am counting also                               working for neighboring China so today                               we will talk about elastic elastic                               search index management for pass style                               logging system first I'll briefly                               introduce nello and how we use elastic                               search in yellow and some problem or                                first design and then how we resolve the                                issues in second page and index manager                                details is design so nello is in house                                logging system in neighbor for neighbor                                developers and some we provide some                                asset case to port logs and collecting                                logs through some various protocols and                                we provide a lot in your time and                                scheduled way so some users if there's a                                Magister a lot if some rules of a lot                                meet then we send some notification and                                we also provide kind of crash slope this                                implication and the of obvious case of                                you stating that means for example in                                mobile application sometimes application                                crashes then it generates crash log                                basically that crash slow is that Shuman                                lead over so in our peg and some for                                example if user register their binary                                and some when there is some crazy log                                some                                generated in applications an application                                for to our system through our SDK then                                in our system we some transform that not                                human kneadable some crash log to human                                readable format and we developed own web                                application and we provide dashboard                                just given a dashboard and some case                                user want to customize their some law                                with stored in yellow so we provide open                                API to make some data available to those                                kinds of users we have to some vein                                design considerations first one is we                                need to support various log format in                                neighbor we support some we provide many                                applications and those applications are                                developed based on some various platform                                such as Android and iOS and Mac OSX                                windows and some depending on                                publication they also some use some                                frame of to Saint Louis                                so depending on platform or frame ups                                and some terrorism common part for some                                different hard talk local format we need                                to cover those cases and also meanwhile                                some application they also want to                                define some own some custom for some                                field so basically we have some inside                                of our companies and some hours and                                client who want to some pro by some                                their own local format so we need to                                cover                                this is a current our scale Ovilus                                search we have four hundred ninety s and                                note and eleven clusters in nine                                instances and a total number of logs                                seven point six terabyte and total size                                of logs currently one point six petabyte                                at the beginning of our product we just                                start from training data node herbalist                                search and we just have some                                half-billion laws so we our some product                                itself is keep growing so we need to                                concern of of scalability so this is our                                overall architecture so in client-side                                we support many different kind of sdk                                including some open source logging agent                                client applications and log to our                                collector and in collector size if some                                incoming logs are growing then we can                                easily add some collector and some some                                registers some collector addressed to                                the air for then it is very easy to                                scare out and most of data stored in                                Africa and some depending on some local                                type and event we defined some some                                different topics and in the backend                                depending on this topic we process and                                most of some back-end processing we                                implemented on turbo stone and crash                                collector is aggregation crash data and                                store to last asserts and most over logs                                lead from Kafka and some                                some in-depth through sand storm                                topology and for allotting we use                                populate the feature                                simulator I already explained you but                                implemented it on top of stone                                previously we use a different types of                                different kind of frame oh but for                                 maintainability we unified to use a                                 stone and some most of our features are                                 available and featuring some data is                                 available through open API and users are                                 basically interact through our web                                 application and today I remain nice and                                 talk of elasticsearch so enforced faith                                 this is a elasticsearch closed                                 architecture we had three dedicated                                 master node and client node and so most                                 of some such requests or indexing                                 Roquette always some cones routes and                                 coordinating those to the data node and                                 so we use some kind of hot ohm                                 architecture so using some SSD machine                                 we solve some hot data and through some                                 HDD machine we some solve some warm data                                 I think is kinda popular architecture                                 and for index motor at the beginning we                                 defined someone in depth Pole and day so                                 every data for one day means all some                                 project data stored in one in depth and                                 and each project we define some                                 different mapping and to support                                 retention time basically some old                                 project in this structure all projects                                 have same detention time because they                                 are sharing some index so if there is                                 some requirement for example some some                                 organization want to keep some data for                                 two years or five years that we splitted                                 instances and how we search or in depth                                 and to optimize search performance we                                 use a custom routing so for example for                                 small project we like to store data only                                 why short but for big project to                                 paralyse processing we store some data                                 to all chart in in depth so for example                                 here that - actually this decide whether                                 the project is more or at the moment we                                 just use some static value some                                    millions some logs per day if some                                 projects have dead much off-site then we                                 some consider it as big project some                                 others and project have lower value then                                 we consider it as a small project so                                 here's a line application is a big                                 project so let's say is more than                                    million data is coming every day then we                                 store some data                                 - all short and for Nate music and                                 some map application we store only                                 tutors one specific chart but in this                                 model determinism problems first problem                                 was a so-called mapping explosion more                                 projects created that means more mapping                                 critic creatives as well so for example                                 in neighbor instance we have more than                                                                                                       mapping the pint in one in depth size                                 was on more than six megabytes basically                                 you know last search synchronized the                                 mapping of an index among all node with                                 single thread so that means some size of                                 mapping increases and number of a node                                 increase then some takes more time to                                 update mapping so sometimes entire cloth                                 blocked by this mapping update mapping                                 event so for example this was a kind of                                 log so some some update food mapping                                 took some more than five minute so if we                                 see this graph of index root food so if                                 we see some lead arrow every some some                                 moment some kind of Sun in the same                                 group for dropped because of that some                                 update mapping event and another problem                                 is a imbalance of short sides because we                                 are using the routing custom loading so                                 you know velocity search decide the                                 chart based on hash value of that                                 loading value so if some project have                                 same                                 some hash value then some those projects                                 stored in same chart that means                                 sometimes some chart bigger than other                                 chart and it means - art is busier than                                 others so in this graph this is came                                 from the earth data actually or other                                 problem is a shared site itself quite                                 big so here's a most of shared size is                                 around between                                                          is more than                                                         affect some entires and close the                                 performance and another thing is a                                 impact of big project so basically some                                 among more than                                                                                                                               size big project and they send but they                                 are sending more than                                                    know some because they are stored in                                 same in depth some some so I shard has                                 shared together so that means some                                 shirring lizards together so all                                 paintings and Obamas of remaining some                                                                                      another one is a as our product is                                 service keep growing some organization                                 keep asking us to some kind of custom                                 mutation like one or two years or                                 sometimes almost forever they want to                                 kill data in this case basically we                                 don't support readings and ones and less                                 third cluster essen                                 and their different retention time in                                 our structure so at the moment we need                                 to split instance means different                                 elasticsearch cluster and I mean some                                 some different entire narrow so it means                                 we need to some give some mores and                                 maintainability cost because we need                                 boards and some note and we need to                                 maintain more servers another problem is                                 at the beginning honest we support                                 string time but you know in dashboard -                                 oh great                                 k theta numeric data have more valuable                                 to some show some very different kind of                                 application result but you know from                                 last                                                              mapping is different but if some                                 mappings and this resides in within one                                 I mean same in this then if field name                                 is same type also should be same so for                                 example let's say I want to use side                                 field but side field with some integer                                 some type but other project already is                                 and defined the side field as string                                 then there can be some type conflict so                                 here question is to define new some                                 design do we need to create separate in                                 dice for every project                                 in the case we have                                                    we need to define                                                        means any                                                                if some if there are two leaf flickers                                 that means hands out on the chart every                                 day created so it's not scalable to                                 resolve this problem we improved our                                 architecture we have several instance                                 but there are some some big instance                                 special neighbor and line instance so                                 for those instance we splitted some                                 cluster to keep some some metadata                                 cluster smaller and we splitted indices                                 per day previously we defined only one                                 in this we defined multiple some indices                                 but some none like some some define                                 entities for every project with by                                 splitting some indices with sub we can                                 support some custom type and the change                                 question retention time and previously                                 we only some using some static already                                 some specifies and chart number but in                                 new model we sent island it dynamically                                 asked me some number of shard some by                                 some history of some log size so we keep                                 organized on number of shard and cluster                                 so                                 in terms of elastic search across the                                 architecture we introduced it at rhyme                                 note you know try note with try note you                                 can search with for some multi cluster                                 so what I mean by that is even if some                                 data is stored in different charged with                                 criminal                                 you can search together and but there is                                 some limitation with trying knows you                                 cannot update some metadata so from six                                 point X alas search oh fisheries and                                 support                                 cross cross desert so if we use some                                 recent button we can use cross cross the                                 search and in previous model we just                                 have one cluster even for hot here and                                 one here we just use some comment to                                 move data from hot to warm but it with                                 fretted clusters so we in our case we                                 use a HDFS snapshot so when we move some                                 data we first some store snapshot from                                 this hot cluster and restore to some one                                 cluster and for index mode we separate                                 indices to different kind for small                                 project still we store some common                                 indices means one in this for one day                                 because                                                                  in one in depth but for big project we                                 stored in dedicated indices so we wide                                 this way because we try to some                                 optimized on the number of chart so in                                 this example some be project like band                                 or neighboring or line they                                 some some have some dedicated indices                                 but others have some share some common                                 index in this model to surgeon in depth                                 is more simplified because we you start                                 using areas here so previously for                                 searching in there's some we need to                                 aware about whether the project is big                                 or small                                 because for big project we don't specify                                 loading and just use default routing but                                 possible small project we need to                                 specify some routing value but here some                                 some for indexing searching don't need                                 to care about it just use just need to                                 aware about an alias name alias name nor                                 is project name and some date of loss                                 and for indexing name for dedicated                                 project we give some post piece some                                 project name so application only care                                 about areas but within areas it just                                 knows and point to specification L in                                 depth or specific chart with routing                                 this kind of job is done by index                                 measure so touching can you introduce                                 about in the Spencer thanks Kay okay                                 next I will describe was a detail of how                                 we managing our new index models and the                                 component of which is rooster tomb                                 performance a management work is called                                 the index manager                                 what is index manager it is responsible                                 for manic is a life circle of all                                 indices in a newest is                                 every day Yasuda pre create all the                                 indices and the new SES for our project                                 and it needed to delete all expired                                 English is and the Angels is everything                                 besides of this as we have multiple                                 clusters so it will show the response                                 Bravo transfer indices is among these                                 hot tire clusters according to the index                                 retention time besides all the scheduler                                 task index manager also handles some                                 instant the project a monumental event                                 as this event they may bring some                                 changes to our existing index such as a                                 create project and a delete project so                                 this is a modules and the tasks of the                                 index manager inside the index manager                                 some tasks that just needed to be                                 scheduled or once every day but others                                 they needed to be scheduled every time                                 there is a real-time management event                                 comes for scheduled the tasks they are                                 responsible for maintaining the existing                                 projects in this is an dangerous is they                                 are for kind of scheduler tasks first is                                 that is the creating index in angel                                 strobe in this drop it is responsible                                 for creators of all projects indices and                                 Indian angels years of tomorrow                                 and it also needed to transfer the oil                                 indexes from one cluster to another                                 cluster in the backup job related to a                                 create indexes wrap short in HDFS and                                 this is for use the fulfil over backup                                 and in the clean up drop the index                                 manager should be responsible for                                 removing all the expiring indices and                                 aeneas s according to the retention time                                 so here is a scheduler task another kind                                 of task is world ham task                                 there are four kinds of a real-time task                                 first when the user created a project as                                 raloo web app then the create project                                 handle et sudha and as projects a nurse                                 to the existing in this then well                                 Theatre Project event comes in each                                 project handler should delete the                                 project dedicated the index was angels                                 from common index besides it means to                                 delete all the history data of this                                 project as vivid project editor is a                                 time consuming pro pro the process so                                 you to do it in a synchronous another                                 you printer is a customized project                                 mappings every time when this task                                 receives the event it will read as the                                 project mappings from we call the                                 customized dub mitra indexes then we                                 build as a program Baptist in updated                                 index and the for displayed project                                 event the handle release to create a                                 dedicated a index for this project and                                 it removes all angels and mappings from                                 the coma antics here is a architectural                                 view over the index manager as index                                 manager manages all the indexes of a                                 yellow system so we won't allow any                                 downtime instead of a deploy on single                                 road we deploy it into the index major                                 cluster in this cluster we have one                                 master and several snips the each node a                                 is collected to the root keeper once the                                 master is done remaining snails will                                 elect a dual master and the continue to                                 perform the Magnum                                 to work as we described the in previous                                 page the index mangers managing broker                                 can be triggering in two ways first is                                 the scheduler task it will be triggered                                 by the timer every day and once the                                 timer is triggered trigger that                                 different task as a way of performing                                 the monkey mental work differently for                                 example for the pre create job the first                                 stage will create all the projects                                 angels and mappings in the hot cluster                                 and as a nurse and for the transfer job                                 it needed to transfer the indexes from                                 the hot cluster and the tool on cluster                                 and we use HDFS as is a story meter and                                 for the clean up drop it needs to remove                                 all expiry indices and any alleles are                                 from all these clusters and the fourth                                 backup job it also needed to create a                                 snapshots of every indexes in HDFS                                 Furious is a scheduled tasks another                                 trickery is a real-time event every time                                 there is a real-time event or from web                                 app then the real-time handle a virtual                                 event from the Kafka and handle the                                 differently maybe create read edit take                                 Katie the index were a dangerous to the                                 coma Enix were updated projects map is                                 like that after talking about these                                 tasks this come to see the index manager                                 config a stick worthy describe the                                 previously we have a steering instance                                 is each instance their skill is                                 different their hardware may be also                                 different                                 and so and for example we need a                                 different configuration for the shop                                 number notice odd number the shorter                                 size so and we also need to define                                 different rotation days so this is our                                 son Telemachus our meters we can                                 configure the in is an index manager                                 config besides for all of the tasks you                                 can see we interact with many many                                 component like a zookeeper years and                                 HDFS any task can be failed to arena                                 because accouting officer transaction so                                 well if the transaction is fail and when                                 index memories recovers and just work                                 may be changer from from the older                                 master to a low electric master so how                                 we do how we know we recover from where                                 so we recorded the index managers                                 runtime information in your index code                                 index manager meter this is also an area                                 index which is used the tool stores the                                 project's customized mappings okay next                                 I will introduce how we perform some                                 important tasks first a users creating                                 index in Angels is a where they know how                                 our new index model is but before we                                 create into tomorrow's index I think                                 there are several problems we needed to                                 solve first is in our new index model we                                 allocated a big project in dedicated                                 index in the small project in common                                 index                                 the problem is what kind of project is a                                 big and what kind project is a small can                                 we decided to dynamically and not only                                 use a static number                                 and another problem is for a index                                 prover snake we use a static a sharp                                 number but currently can we determine it                                 to the logically every day and what is                                 the appropriate number and if the sub                                 number is too big then it will overload                                 you just the cluster and it is too small                                 it will bring down our performance                                 another problems for the Angels in                                 common index which angels should have                                 routine and which angels should have not                                 routine how it is that how it decided it                                 dynamically maybe come here you are they                                 low when needed to estimate as a size we                                 needed to estimate it as project styles                                 and as the index test then compared with                                 some threshold then we determine the                                 dynamically here is how we could do it                                 first we estimate is our project size                                 using the average of a last Testament a                                 snogs                                 burn then based on these projects ties                                 we compare it with a Spoleto style                                 threshold then we determine whether this                                 project is the allocated in common where                                 did he take hidden index is if we                                 exercise is a smaller than this ratio                                 design you should begin common index                                 only if it's bigger than that then it                                 should be in dedicated index then for                                 then we have multiple witnesses every                                 day for each index will lead to decide                                 as a short number the Angels is a nice                                 markings for the short number as we were                                 the nose protocol size in this index so                                 we can get the stint exercise simply by                                 some of the projects tests and the sub                                 number we just estimate it using the                                 number of index size divided a stroller                                 style a shot                                 and the for the agencies in the common                                 index we also use as a project size to                                 compare with other routines that is the                                 threshold then we determine whether                                 where you team were not so currently we                                 only a disease ratios can figure a                                 config file so maybe in the future we                                 can determine it dynamically using some                                 tool and you run it in our production                                 environment then we can get this baby on                                 her medical this is how it transferring                                 in the exists in our new model for some                                 very big instance we have two clusters                                 for each cluster we have different                                 orientation types of the index so this                                 retention diet is configure the Inza                                 index manager config file                                 besides over this every times                                 transparent drop works yellow will                                 record as at last to transfer the index                                 of each cluster then next time it runs                                 from these two kinds of information it                                 allows on each cluster which indices we                                 need to move then for each index index                                 we just create a snapshot in HDFS and                                 store the index from this lab shortening                                 the target cluster then when the latest                                 snapshot and indexes in frost cluster                                 after that we have updated metadata here                                 is how we do backup the backup task is                                 the almost same wheels as a transferring                                 drop it also uses HTTP as is a storage                                 media and every day the backup job it is                                 includes it will finds which we see in                                 Texas we should create a snare                                 short as every time we record as an                                 artist nap short date in the index mind                                 emitter so we know where to start then                                 below from the start date to yesterday                                 we all needed to create indices and for                                 every one we create a snapshot and                                 update as a metadata this is how we                                 removing indices and a newest s as the                                 chick already state we have many                                 projects through styling the one                                 instance and every project they have                                 different retention time so we live to                                 clean this project data differently                                 according to the recent change in time                                 and in index magnetometer we also start                                 each projects nested deleted angels were                                 indexes so from these two kind of                                 information we can find for each project                                 which index were angels we needed to                                 remove then for for the project's angels                                 in the common index we just remove the                                 angels but we didn't remove the data as                                 deleted by quarry is very snowy years so                                 we just leave the deleted data and -                                 when the common indexing expired and                                 when the see this is how it do is in the                                 common index coordinator if the protocol                                 is in dedicated the index then we just                                 remove the index and the updated                                 metadata okay this is what we wanted to                                 see re thank you                                 has anyone asked a question questions                                 thanks very much for the presentation                                 that was really nice to see how you                                 scale to this amount of documents and                                 and scaled many questions but one of the                                 last ones that I occurred to me it it                                 revolves around how you decide which                                 index you're gonna use so if it's gonna                                 be a big project or a small project and                                 you said in order to make that decision                                 then you look at seven days of data but                                 that doesn't that mean that you've                                 already made a decision and then you run                                 the project to run for seven days and                                 then you make another decision am I am i                                 understanding that correctly yeah all                                 right                                 so you start with a small index yes I                                 say okay and then you run you run it for                                 seven days and then you make a decision                                 okay now we need to upgrade it to a fee                                 okay all right and my second question                                 was with regards to the shards size                                 threshold what is that can you say a few                                 more words on that and currently we just                                 restate it according to our experience                                 for example in our big most a big                                 instance we just just stated to                                    gigabytes oh god yeah                                                    pair it depends on some vacation and                                 some machines power actually you need to                                 some test about it actually there is                                 some document of insanity searchable                                 okay so there is observe the test that                                 you can run in order to figure out what                                 is the optimal size for the short story                                 or the application yeah actually we need                                 to improve that part or until a peace                                 and justice specify some pace very short                                 okay thank you very much thank you thank                                 you okay thank you very much
YouTube URL: https://www.youtube.com/watch?v=lM4Um50R7c4


