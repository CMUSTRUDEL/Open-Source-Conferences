Title: Berlin Buzzwords 2013: Sylvain Lebresne - On Cassandra's Evolutions #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Apache Cassandra is a fast moving project, each release bringing it's fair share of new and exciting features. This has been particularly true in the last major release, Cassandra 1.2, with the addition of vnodes, CQL3 (and its new binary protocol), request tracing, atomic batches, a number of performance improvements and more.

Read more:
https://2013.berlinbuzzwords.de/sessions/cassandras-evolutions

About Sylvain Lebresne:
https://2013.berlinbuzzwords.de/users/sylvain-lebresne

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right so um hello everyone I'm                               silver glen i will talk about apache                               cassandra today i'm i'm a commuter on                               apache cassandra and in fact my my day                               job is to develop the Apache Cassandra                               project and i work for company called                               datastax that that let me do that and so                               what I want to talk about is ought to to                               show you a few of the recent feature                                that when in Cassandra basically some of                                the the at least some of the important                                ones more does it work it works so very                                briefly uh hopefully you've all over                                Earth of Cassandra and cassandra is a                                fully distributed database database                                that's been built to scale and that in                                practice is massively scalable so that's                                kind of a it's so old graph that was                                done by Netflix to like see all rights                                were scaling when you add nodes so                                that's on ec                                                           here is to show that it does scale                                freely linearly or in the number of                                nodes which which is nice for it to have                                it's also highly performant because if                                you scale but you're very slow it's                                still nice but but it's nice to have a                                base that's not too slow so thatthat saw                                a graph or let's not talk about                                benchmarks took me be clear on that but                                just to graph from vldb paper vldb the                                database conferences that we're a                                benchmarking a number of database and                                the one line here is cassandra so well                                on that graph it's definitely better                                than the other but the point is to say                                that it's not it's not very it's not a                                very slow database to start to it and it                                does scale right so it's serious                                database and the last maybe not the last                                one but one of the very important                                property that we are care about in                                cassandra is the reliability and                                availability right so it's a database                                that is build so it's always run you                                will never have the data that's down                                unless you do something                                really wrong and so that that's a few                                sweet few tweets of user that had good                                experience with that and there is older                                but I mean it's used by all of people                                and it's something that's been proven                                over and over again right but it's also                                a project that uh that I think evolved                                fairly quickly all right and so and so                                today I want to show you some of the                                fairly recent features but before going                                on to to those one I want to just go                                back over the last word Alaska version                                of Cassandra and one of the big feature                                that went in just to give you an idea of                                that evolution of Cassandra and so two                                years two years and a half ago actually                                we released Cassandra                                                   of the first on what I consider that as                                the beginning of the usable Cassandra                                before that you cannot if you didn't                                read the source code it was probably                                kind of our to ensuring everything image                                was the beginnings and with their seven                                you start to have something that was                                really usable and in                                                    like dynamic schema creation what that                                means is that before there are seven you                                want it to add a table to your database                                you have to take a node shut it down                                update the xml file with your new                                definition and then restart the node                                right so that's so this should be the                                database so you have more than one node                                you have to that on every node you could                                that in a row like fashion so that                                didn't meant that you had downtime but                                nonetheless it was fairly a crappy right                                so we added we added a query that just                                you know create table basically so you                                can trade a table dynamically and then                                we are the things like expiring co on so                                you can you can go insert some column in                                the database and you can set it to TL                                and it will expire automatically when                                that TTL or a lab so that that there's                                number of use case and we are the things                                like seven or indexes so before that                                release we didn't add cylinder indexes                                so we only add like primary indexes that                                were                                the one you declare with the table but                                sometimes secondary indexes are pretty                                 useful so to be clear in Cassandra you                                 may add sometimes that secondary index                                 can be slow and they're not always                                 useful sometimes you have to use other                                 tricks because they can be slow for some                                 use case but there are some time really                                 useful so that's one night and then six                                 months later where the new can make                                 major version so we had minor version                                 between those but with a new major                                 vation and that version at it well it                                 added distributed canners so there are                                 there things that count stuff they are                                 useful to do our real-time analytics                                 kind of thing and did they have a number                                 of done sighs too so you want to read                                 the duck carefully if you want to use                                 them but I as it happens I did                                 orientation of that damn in Berlin                                 before two years ago so I think the                                 video is somewhere online so if you want                                 to know more about that and then we also                                 added something that is called CQ all                                 the customer query language we added the                                 first version there and I'll skip to                                 that quickly because I will talk about                                 that more later and then we had things                                 like automatic mem table-turning so in                                 Cassandra the name table is where we                                 actually are buffer the insert before                                 writing them to disk on and so that one                                 of the important structure in memory                                 that Cassandra ass and before                                            to configure a number of things for all                                 of your table on when we need to flush                                 those on many operation you need to us                                 in there was a number of setting and                                 they were pretty hard to guess right you                                 know where you had to do a lot of                                 experimentation and what was even worse                                 that if you were a new table you had to                                 kind of recompute all of them because                                 your your machine has so many memory and                                 so if you are more table then you have                                 more mem tables so you have to recompute                                 those those thing so we added a lot of                                 automatism so that it's much easier to                                 use now and now all of that is done with                                 Cassandra I just tell tell it how many                                 memory you have and it well or like that                                 works and then all in                                 still more more than one year and a half                                 ago we we at customer one that's zero so                                 at that point we can consider that Oh                                 Cassandra was used by the lot of people                                 in prediction so we decided it's                                 probably time to you know other                                    version so that that because people do                                 use it in production so it is ready for                                 that it doesn't mean at the end of                                 Cassandra but it means that we've                                 reached a milestone and in                                            few things like on on this compression                                 so we compress now that the stable on                                 disks and we have something that we call                                 the veldt compaction so compaction it                                 when you write that are you prefer                                 eating in memory and then Cassandra will                                 flush those assets those those those                                 name table we called at SS table and you                                 have a compaction a background process                                 that actually contact those file on disk                                 regularly and the way you compact impact                                 performances in many ways so we added a                                 new compaction that add some good                                 priorities for some use cases so anyway                                 and well the a year ago we actually add                                 the customer                                                       something caught they called love row                                 level isolation so we always add row                                 level atomicity in Cassandra in the                                 sense that if you do insert into the                                 same row you were guaranteed that either                                 all of the insert or none of them would                                 be persisted I put you we had an                                 isolation is that you could have a race                                 between write and read so that the                                 reader can actually see a partial                                 updates and we fix it that in                                        then there's concurrent schema changes                                 here it's kind of a a link to that                                     we actually a little ways to dynamically                                 create khan family but you weren't able                                 until                                                                   concurrently right you have to be                                 careful that only one operator we're                                 actually doing updates to the schema and                                 in fact here the to change this part is                                 important because in                                                     do                                 current table creation you can do                                 updates so the table in concern have the                                 number of setting that you can change                                 you can do concurrence changes to the                                 stable but you can't create them                                 concurrently and that's kind of a bug of                                 one wine in a way and that's fixed in                                 one to our quick word at the end and                                 then we do things like I improve the                                 support for mixing SSD and HDD so it's                                 some people want to put tables that are                                 more of an access or that I've well need                                 for better Latin see anything like that                                 on SSD but still for you know cust                                 reason I'll keep some of the table and                                 on our drive on rotational hard drive so                                 we made that possible and then some more                                 self-tuning so really in two year and a                                 half we went from something where you                                 weren't able to actually create a table                                 dynamically to you know going into                                 fairly more specific features and now                                 what I want to talk about in more                                 details is about casino                                                  is the current stable version of                                 Cassandra it was released in January and                                 we're at like                                                         released and that was a that was a                                 version that has quite to be the fairly                                 big feature for for my generation                                 Cassandra and I want to talk about all                                 four of them there is more i will just                                 quickly list then at the end but the one                                 I will talk about is the virtual no tql                                                                                                       protocol and tracing because i think                                 it's a nice feature even though it's a                                 very simple one so the first one is                                 ritual at it before going into religion                                 will let me recall the way that data                                 distribution works in Cassandra so you                                 have a number of nodes and you have to                                 distribute that outer death note the way                                 Cassandra does is this we take a sh                                 function that's md                                                      function and you actually consider the                                 domain of that hash function there is                                 like                                                              six values so anyway you consider the                                 government of that and you consider that                                 in a ring you make that trap and that's                                 what we call a ring the next step is                                 that you actually peak value on that                                 train so here the dot separation here                                 are value on the ring and you pick one                                 for each node so another one for example                                 we we will have this value that's what                                 we call a token of the node so every                                 node in a canonical sir i would call                                 token the token is a value on the ash                                 domain that we've picked and for node                                 one that could be this one for no to                                 that could be this one not three etc etc                                 and those token are actually creating                                 data ranges right that is you create                                 that I ranges because there's a ranges                                 of data so that the ash of that data end                                 up in this range here right so that's                                 that's what we called that arranged and                                 well that's how we actually do                                 distribution we say everything between a                                 node token and the next token on the                                 ring will end up in the node which token                                 it is so that's not one token so the a                                 range will end up in another one right                                 and then well the next one be will                                 anything that be because that's the net                                 to token and the C and D and E and F                                 right but we also want to do replication                                 so every node will have more than one of                                 those range they will actually have in                                 that case I have                                                         a replication feather of                                            something you pick and so in that case                                 where we do something fairly simple we                                 say not one as this a and we'll just                                 take the two previous ranges and then                                 we're done we have three replica right                                 and we'll do the same for B so B will be                                 B a and F and C will have CB and a x                                    right so you can actually also change                                 that and make things even more                                 complicated on oh you actually picked up                                 the remaining replica but the basic                                 principle is this one okay now in                                       introduced something called virtual                                 known and what that does in practice is                                 just that instead of an one token for                                 every node we have more than one token                                 for every node and in fact i think the                                 default when you want to switch vinod                                 which                                 by the way is optional you don't have to                                 switch to it but if you want to switch                                 your cluster to Vinod you can you can                                 set multiple tokens / nose and the                                 default I think if my memory is correct                                 is                                                                      three you want a bigger number of tokens                                 but in a way except for giving more                                 token to each node the rest of the                                 algorithm is kind of the same you have                                 each node with of multiple tokens so                                 maybe not one and I think I don't                                 exactly there's too much stuff to follow                                 it but not one would have maybe this                                 token maybe one here and maybe one here                                 and then every node will have tokens                                 that way that are along the ring and and                                 you will still have this property that                                 the node one will have the range between                                 one of his token they can be war than                                 one but for each of these token you will                                 have the range lies between East Oakland                                 and whatever is the next token on the                                 ring right so that way you will have                                 more small ranges but those ranges will                                 be more distributed over there that are                                 ushering and you still do the                                 replicating so if you have not if you                                 have a another one you will FP and oh I                                 don't know if that's real a schema but                                 that's assess the principle um and so                                 why do we do that well the main reason                                 we do that as to do is repairing nose or                                 adding new nodes in factories saying                                 that is if you say you had five nodes                                 right and you had a fifth one or that                                 could be maybe that that first node was                                 actually existing but it's actually died                                 I think that's what the the schema                                 depict here and when I said I'd I mean                                 rather like explode right the machine                                 just exploded so the disk the art drive                                 doesn't exist anymore you buy a new                                 machine and you say hair replies replace                                 that node so the data that wearing that                                 node need to be rearick ated because it                                 has been lost right it doesn't be lost                                 for the user because there is a replica                                 but concretely we need to rebuild the                                 nil right and we have the same thing                                 when you add a new node except that on                                 top of                                 that you have to redistribute the data                                 but that still the same thing and when                                 you actually build the data because you                                 have you have a small number of ranges                                 because you only have one token for a                                 trench so it's really create as many                                 ranges that there is no din the in the                                 cluster it mean that this node can                                 actually pull data only from a small                                 amount of nodes in that case three and I                                 think that's not true because it could                                 actually pick ah you could actually pick                                 your range here but it wouldn't be                                 useful because it has only three ranges                                 to pick so you can pick them from only                                 two three nodes and if you have a much                                 bigger cluster with an android node                                 that's still true you want to rebuild                                 the node it can only pull that up from                                 three nodes now the problem is this when                                 you do that when you do that transfer                                 here all those nodes are serving data                                 are serving reads and writes your                                 application is still running all that is                                 is live so you don't want that operation                                 to actually costs too much to that node                                 you don't want to use too much I owe for                                 that so don't slink here they won't                                 necessarily are going you know full                                 speed so you will throw all those the                                 stream because you just don't want to                                 put too many I'll load on those nodes                                 right and even if you don't do it you                                 have a limitation for the cable right so                                 if you have an Android gig of data to                                 transfer here it can take time                                 especially if you throttle which is                                 something you want to do often so                                 rebuilding a node will just take a long                                 time just because you're using only                                 three link and those link are limited                                 because you want to do it so it's slow                                 now if you actually use virtual nodes it                                 gets much better because you have a lot                                 more small ranges and again in practice                                 you will have a lot more injuries                                 because you have you don't have like                                 three token per node you have like more                                 than                                                                 pick tokens from pretty much all the                                 donors enduring all the road in journey                                 all the node in the ring could be used                                 to repair that node and what that mean                                 is that you have much more of those link                                 that you can use and you will rebuild                                 that node                                 much faster without necessarily you know                                 using too much resources on the note                                 that actually are sending the data the                                 data on that note note that this one                                 will get much more data quickly but we                                 don't care because that's the one that's                                 not yet doing real work is just                                 rebuilding it's only as that to do so                                 you want you want this to be the fastest                                 possible and so that that's that's why                                 we we won't veer but that's one of the                                 reason one virtual note probably the                                 most important one and that's actually                                 matters because when you rebuild to note                                 you may have a lot of that at transfer                                 and if you start having a bigger cluster                                 well that that can take a lot of time so                                 that that does make a big difference in                                 practice that can make a big difference                                 pray so that's virtual nodes and I'll                                 for it for this presentation I'll move                                 to something else but before before that                                 so first thing is like let me just say                                 very quickly that we call them virtual                                 nodes in Cassandra but there is                                 something called virtual mode kind of                                 literature that's not exactly the same                                 concept or we have a lite version of                                 virtual nodes so if you have already                                 heard of your tool no did you like                                 that's not where they're doing then                                 that's kind of true but we still call                                 them that way so really all these does                                 is we assign multiple tokens / nodes and                                 update the distribution algorithm for                                 that though the domain are the main pros                                 of that approach that you have faster                                 will build off of as I've discussed but                                 there's a few other perks like it'll                                 lose it original nodes because when I                                 say that you have multiple tokens per                                 node you can actually have a different                                 number of token for veneered right so                                 you can easily give more token to a node                                 and naturally all because you distribute                                 your token evenly you will have more                                 load on what node so you can have nodes                                 of different capacity lately you could                                 do that before with the token but that                                 was much more painful to actually do and                                 well those are ones like when you when                                 you actually add node it's it's simpler                                 operational                                 uh with with virtual notes because you                                 actually have less data to move around                                 or just because the way it works so I                                 don't have time to show you that in the                                 schema but you can take a paper inch and                                 you will realize is better anyway so                                 moving on to the other one I think is                                 another fairly big feature of Cassandra                                                                                                   language and to be very clear here I'm                                 talking of the version                                                   so i said earlier that in                                               first edition of secure and so we we                                 tried a little bit to get something that                                 was working for Cassandra that takes                                 some time so we had version one we had                                 version                                                                 and we I strongly believe that it's the                                 version that that we wanted from from                                 the beginning so I don't think we'll                                 have a version for in the next release I                                 really don't believe we we will and                                 that's what I will describe here and and                                 it's fairly different from version                                      be perfectly honest right so it's a                                 relatively major revision and in my                                 opinion amor amor on vicious one so the                                 goal of that Cassandra query language is                                 really can to provide a simpler and more                                 attracted interface to interact with                                 Cassandra as a developer right before                                 that we have or we always avin we                                 continue to have the street interface                                 which is called the Swiss Winter phrase                                 because it's used the thrifter pc a                                 framework but that one is it does a                                 number of problem it's hard to use its                                 art to make evolve with version if you                                 want to add new features it's art to                                 make it evolve in a way that don't break                                 the clients and well there was a number                                 of recent and and more importantly it's                                 not obstructed right so it exposed very                                 directly Oh Cassandra work internally so                                 as a developer you are exposed to a                                 number of implementation detail you                                 shouldn't have to care that right and if                                 you're if so if your views Cassandra for                                 a number of years                                 you don't care anymore because you know                                 all those quirks and that doesn't matter                                 but for new users that just painful that                                 just the number of small things to                                 understand and that don't necessarily                                 make sense except from the fact that's                                 the way the implementation work and on                                 the other side it's also mean that                                 because the API exposed very directly on                                 the way the internal storage engine work                                 it's also mean that is it's a little bit                                 order to actually do a number of                                 improvement to the storage engine                                 because you may break the API that                                 expose irrelevant details so anyway that                                 was kind of the motivation for that a                                 new API and if you want a quickie d of                                 what the question the customer a query                                 language is if I should describe in tour                                 that we'd say that it's denormalized SQL                                 so it looks like SQL if you read it but                                 you still it's still Cassandra so when                                 you when you use Cassandra you actually                                 do normalize that I you don't did you                                 don't normalize like in general you                                 doing SQL or traditional relational                                 database which you will be normalize I                                 give a quick example so really that's an                                 SQL but that's truly so what I call                                 strictly real-time oriented and by what                                 I mean here that we have no join so it's                                 not as cool at all we don't do join                                 there is no syntax for joint there is no                                 sub queries there is no aggregation                                 there is no there's a there's some order                                 by but it's limited you can do arbitrary                                 order by so that that's limited to what                                 Cassandra does or quickly I would say so                                 I'll give two example to give you fill                                 out oh it looks like and my example my                                 running example will be like storing                                 songs and playlists so say you're                                 writing a music player whatever and you                                 need to store songs in that so well the                                 way you do it is fairly well it's if you                                 know SQL we should probably do it's                                 fairly simple you create a table that's                                 called songs and you will create each                                 song we have an ID so the ID will be a                                 UID which is also in case we                                 Castro is uid because that's an easy way                                 to get a unique ID without having                                 summarization so we don't use auto                                 increment ID for example because that's                                 hard to do in a distributed store so we                                 use your ID but that just a unique                                 identifier first song and then you may                                 have a title some arty some album maybe                                 your track number of that song in the                                 album and then insecure we do support                                 some collections so we support sets list                                 and maps so you can have tags there you                                 know sets of texts then you can insert a                                 bunch of song and you have some data                                 here that will look like that so it's                                 kind of boring because everyone                                 understand what that means but but                                 that's a good singing our opinion one                                 remark though is that those two here                                 there's two operation here they are bus                                 absurd insecure so the artists update or                                 insert so really the insert insecure is                                 like syntactic sugar for an update we                                 have both sing tags because people are                                 used to both syntax so why not but if                                 you do an insert and the songs already                                 exist the database won't complain you                                 can still in your code choose to use                                 insert wherever that's supposed to be                                 inserted updates wherever that supposed                                 to be updates but the database won't                                 complain if you use it the wrong way and                                 that's due to a number of technical                                 reasons so that that example was like                                 fully trivial and interesting and the                                 more interesting one is what if I want                                 to store playlists for users so for a                                 user I want to tour playlist that as an                                 N and the number of songs if I have if                                 I'm doing a relational database and                                 traditional kind of SQL I'll probably                                 have a table that will be fairly simple                                 it may have a user ID probably the                                 playlist name and then it will just have                                 the song ID and whenever I want to do a                                 query like well give me the                                            of a given playlist for a user I will                                 just do a join and that's it and we                                 don't do join Sandra but                                 instead what we do is we do normalize                                 and so the normalizing is just saying                                 that in that playlist we just include we                                 just duplicate some data on the on the                                 song in that table here as you can see I                                 don't I don't put all the property of                                 the song because maybe I don't need them                                 and the trick here is that that table                                 will depend a little bit on what your                                 application do so when you're modeling                                 Cassandra you start from the query and                                 then you decide oh you actually model so                                 maybe here in our example I I used I                                 I've decided that I only care about                                 those three value to be to do whatever I                                 need to do with my playlist and maybe                                 the song guide's so that maybe I will                                 only display that on the pages that                                 shows the playlist and maybe if someone                                 clicks on a song then I will use the                                 song ID but that's another request to                                 have the details of that song okay so                                 there that that can change that the way                                 it works so the one thing however that's                                 interesting in sick you on that changes                                 from from SQL is the primary key here                                 the primary key here as the same meaning                                 that a SQL it does those are those                                 columns identify uniquely a row in that                                 table however there is not there's                                 additional meaning insecure                                          values and the special meaning are those                                 one the first value in the primary key                                 here is what we call the partition key                                 so when I said the first value it can                                 actually be as in this case multiple                                 columns but that is why of the                                 additional number of parentheses here                                 because the kind of the first element is                                 the primary key will be the partition                                 key for Cassandra and I can group them                                 with an additional set of parentheses                                 right and the partition key is what                                 determines on which node the row will go                                 so in another way another way to say it                                 is that it's it's on those column that                                 the ash value that we used to the                                 distribution will be computed right                                 so what here that mean is that all the                                 songs for a given playlist of a given                                 user will be in the same on the same                                 host right so those two lines here are                                 guaranteed to be on the same host which                                 in practice mean I can request them                                 quickly because I know that I will only                                 have to eat one machine and note the                                 world cluster even if in die playlist I                                 have a lot of songs here I have just two                                 songs which is trivial example for you                                 know my slide are only that big but if I                                 have a lot of songs here I'm still                                 guarantee they will be on the same post                                 for a given playlist which belong to a                                 given user in that case and then the                                 second part of the primary key which is                                 the rest of the columns here they are                                 taken together they are doing what we                                 call the clustering key insecure and                                 that that clustering key actually                                 defines the ordering of the rows on disk                                 so as some of the SQL database has some                                 some very similar notion of clustering                                 that's where the names come from but in                                 any case in general what that mean is                                 that the here those are those two lines                                 here will be in that order on disk                                 because L is before em right basically                                 for a given playlist all the song will                                 be alpha bacchetti alphabetical order by                                 title and if two of them are the same                                 title which usually don't happen but                                 that can happen then the album will                                 decide what is the actual sorting of                                 those two rows so that's what that                                 what's allows you to do it you know                                 efficient query in the order that's                                 defined by the clustering key so with                                 that schema you can do things you can do                                 very efficiently queries like you know                                 give me for a given play DS with me all                                 the title that's star between B&C for                                 example that can be done very                                 efficiently because that will eat only                                 one box and that will read pretty much a                                 contiguous part of the our drive that's                                 a little bit more complicated practice                                 but that still                                 Canady idy it's efficient and you can of                                 course of the                                                        title right and so depending of the                                 order that you need you may need to                                 change the way you actually starting but                                 then again is the way it works in                                 Cassandra and that's why a particular                                 three years a little bit short but                                 nonetheless obviously give you a bit                                 filling now continuing with the features                                 that are we added in                                                   cql three we actually added what we call                                 the net chief protocol which is actually                                 binary transport protocol for secular                                 three and so as I've said the ddr kind                                 of legacy API for Cassandra shrift                                 because it's used it's reached every                                 framework so the IPI is both a thumb you                                 know some function some air pc calls but                                 the thrift also enters the transport for                                 us so it does the transporting the query                                 to to the server and then transporting                                 back data to the client but for cql we                                 don't really need that anymore so                                 initially in the first releases we've                                 reused the three transport for secure                                 we've added just one code that's called                                 execute it takes a string and indeed                                 execute server side and it's sent back                                 the data using thrift but ultimately you                                 wanted to a not dependence raved for a                                 number of reason but some of these                                 reason is that by doing a binary                                 protocol ourselves we can we can make it                                 well we can optimize it for ticular                                 three and we can have some property that                                 you cannot really easily average thrift                                 like are being completely as synchronous                                 so that one is not completely to                                 frustrate to be perfectly honest but                                 shrift is air pc-based so the call to                                 the thrift metas are blocking by design                                 so thrifty is doing a number of tricks                                 number of tricks internally so you can                                 so you don't have to have one connection                                 for every Cyril if you want but                                 nonetheless the FBI is kind of                                 synchronous by default and the native                                 protocol is really a synchronous all the                                 way so you can have on the same                                 connection you can pipeline multiple                                 queries on                                 on it and you can have an LP I that use                                 that to exposure a truly asynchronous                                 API and the other one that you actually                                 at my to my knowledge cannot do is with                                 our three is that we have server                                 notification with that no transport so a                                 client can register on the connection to                                 receive some events and to be perfectly                                 clear so far the only events that we                                 actually send are like cluster generic                                 thing like an unit was added but it's                                 still fairly useful because for clients                                 library that you use it means that                                 whenever you start new nodes because you                                 have new loads and you need more nodes                                 in your cluster then they will the                                 clients themselves will be notified and                                 they will out the new nose to your pool                                 of connection and then you will use them                                 in your queries and you will load                                 balance better you Aquarians and                                 everything like that and there's other                                 events for Chima changes a few things                                 like that we don't you have like a user                                 type or you you cannot as a user                                 register for notification but at least                                 having a protocol that allows it we may                                 allow it to one day we don't know if we                                 will but we can fairly easily do it so                                 anyway if you want to have a feel of all                                 that works i encourage you to test you                                 like data structure by driver that sound                                 get up it's open source and it's                                 probably the more mature driver that use                                 this new protocol and you know secure                                 three know the kind of last feature that                                 I want to discuss our                                                watching request tracing it's a fairly                                 simple thing it's just that you can                                 execute the query and just say I want to                                 trust that trace that query and I hear                                 this would we do secular age and what                                 you will get give you is this so the                                 colors are mined they just said that all                                 all this is what happened in one node                                 and the blue one is in another note but                                 what what these are tells you is that                                 the first node actually received the                                 query and it actually                                 send it to the second node because the                                 second light is probably ripped gap for                                 that query while the first one was on a                                 replica for the query and then the first                                 one actually applied it and you do it                                 does a bunch of stuff and then send back                                 the answer and you have the time for                                 that so it actually helps you up see                                 what's taking time in your query and to                                 illustrate all that youthful I I want to                                 i'll show you some antipyrine Cassandra                                 so i will show you something that you                                 shouldn't do it gasps Anna are you                                 should probably not do it at least not                                 do it that way but the first time you                                 learn about Cassandra you may be tempted                                 to do it and that's a message queue                                 right so only one way that would seem                                 easy and fairly efficient to do mr.                                 Shue's concern rise to create that kind                                 of table you create a table cute and you                                 will have a number of q that will be                                 identified by an ID so that's identify                                 my queue and in the queue our of                                 messages that that they will be created                                 at a given time that's what identify my                                 message I ignore the fact that I can                                 have multiple decision in the same time                                 stamp it's easy to solve but for my                                 example is not relevant and then you                                 have a value of your message whatever                                 that means so here the partition key                                 here would be the ID which means all the                                 elements of Mike you would be on the                                 same post so i can query q fairly                                 quickly but i still distribute all night                                 Hugh on my cluster and then the                                 clustering key is a created ad which                                 pretty much guarantees that the queue is                                 order by the events time so it's                                 basically accused ordered by when the                                 events why was pushed and the way I will                                 make so what what that give me is that                                 if I insert new element i use the                                 current times the current time here and                                 I will have things added at the end of                                 the queue and I want to process an                                 element where I will read the head of                                 the queue I will process the element                                 that I will remove it and i will                                 continue i will read the new the new                                 head of the queue process it and and                                 erase it there is of course like                                 concurrency issue if you do that nicely                                 you may process twice the same message                                 but for number of use of cute actually                                 perfectly fine so at least your                                 guarantee that you will consume every                                 messages though that looks fairly a nice                                 and I said that the sorting here was                                 guaranteeing somewhat efficient queries                                 so why shouldn't that be efficient well                                 if you actually use that cue a little                                 bit and then you Trey you trace the the                                 queries that actually are grabbing the                                 head of the queue so here we're grabbing                                 the one element of the queue and I order                                 by created add to have like the smallest                                 one so the head of the queue I'll get                                 these trays and the important one is the                                 red one that I've I liked it for you                                 here but what it says say that it has                                 really on the on the actual note that                                 actually executed the query is it has                                 read one life cell and like it's like                                                                                                        summarize the way we do with delays so                                 for a number of technical reason why we                                 delete Cassandra will actually inserting                                 something that say that column is                                 deleted and we keep that Tom stone for                                 sometimes right we have to keep it for a                                 few days basically otherwise we could                                 have problems with a resurrecting stuff                                 and whatever so anyway so that the way                                 we do is delayed and because of that                                 when you actually deletes elements that                                 you have processed those stone stone                                 will be there for sometimes until they                                 are actually reclaimed so in practice                                 when you read the the end of the queue                                 you you have to skip all those deleted                                 stuff that are actually still there                                 before reaching one life cell which is                                 kind of the current head of the queue                                 and well skipping all of that is going                                 to be slow so here there's a third time                                 that here and with tracing you can very                                 quickly understand what is going on of                                 course you do need a little bit of a                                 mystery Oh Cassandra works but at least                                 that that points you very clearly where                                 the problem here                                 and well they did there is ways to make                                 it better in Cassandra being a little                                 bit smarter but that that you shouldn't                                 need that way for that reason so anyway                                 that concludes like the feature that I                                 want to present in details but in                                       are whether bunch more features                                 including things like so that's the last                                 part of the concurrent schema thing                                 which is in our in                                                       create a schema table concurrently right                                 before you can only update them and know                                 you can trade them then we are we                                 improved the support for just a better                                 of discs so Catherine already do                                 replication so a lot of people don't                                 want to use things like right                                            red                                                                    lose all of your disks so it can be                                 annoying this is known to rebuild and so                                 it may not be worth it so there's a                                 number of user that actually use just a                                 matter of dry of disk and before                                        problem was that if the disk was failing                                 Cassandra was trying to read it over and                                 over again and that's not what's not                                 very efficient so we will fix it that                                 we've moved the number of structure of                                 eep so i won't go into details but well                                 if you know java you know why we do that                                 and the the the last one that's worth                                 noting maybe we have a new partition ER                                 and here that's kind of the ash function                                 i was talking about and I said it was                                 md                                                                     to four new cluster you can use a new                                 ash which is based on mer                                              faster ash and that's good enough for                                 the purpose of distributing data so                                 there is that and so very very quickly                                 in conclusion I want to mention the next                                 question rallies so the next customer                                 losses will be call Cassandra                                        will be released in July when I don't                                 know but in July hopefully and uh what                                 do we have for for you in that hole is                                 well we                                 a number of stuff we have a number of                                 improvement to the c ql three-hour show                                 them more and even more updates to the                                 native protocol actually like things                                 like a paging query automatically so                                 that's something that we reach to do                                 manually it could be a little painful                                 and we've made that automatic with that                                 new protocol that's also something that                                 the new protocol allows us to do and and                                 one thing that's that will be fairly                                 Andy is we've added a compare and swap                                 operation so in practice in secure all                                 three to look like that you will be able                                 to update a table or setting a number of                                 columns and adding an if conditions here                                 that could be if not exists if the row                                 doesn't exist or it could be if the                                 column was previously some other value                                 so that's why it's really kind of a                                 compare and swap and we guarantee that                                 this is executed atomically right and                                 the way it works internally is use paxos                                 and then we'll have some support for                                 triggers don't really think about SQL                                 trigger you'll have to implement a class                                 for now but that trigger will be able to                                 do something each time and in syrtis                                 does in ease down on table oh it will be                                 experimental but you will be able to                                 play with it it's experimental because                                 we know that will probably change some                                 of the API or that that you have to                                 implement with your trigger class but                                 you can start work using them if you                                 want to try it and we will have                                 something called eager retries and you                                 can root rise on the goal here is to                                 have a better like                                                       and when Cassandra you do read we're                                 really always only asking one replica                                  for the data even when you have like                                  consistency level quorum or more we                                  still only asking that out in one node                                  and the user knows actually send us like                                  digest are just there to check that we                                  can fulfill the consistency level you                                  ask and the reason is to save network                                  bandwidth but the problem is that if you                                  have a student event like a node dies                                  and that was the one you just asked for                                  data then your patient will timeout it's                                  not a huge deal                                  self because the client will retry the                                  liberation but in practice it's not very                                  good for your latency so that the idea                                  here which actually comes from some                                  Google presentation that normal number                                  which one is just to retry                                             the first query and if you don't earth                                  back after a fairly short time which                                  could be based on your mean latency for                                  example will do will query another                                  replica and maybe that's useless because                                  maybe we'll are back from the first                                  query but even if we do in general it                                  will it will we won't retry every time                                  but still get a better latency and                                  that's all i have for you know yeah we                                  have performance improvement but to                                  always do that so and we have more and                                  thank                                  you                                  you
YouTube URL: https://www.youtube.com/watch?v=HuUu2MgbiT8


