Title: Berlin Buzzwords 2013: Ted Dunning & Michael Hausenblas - Apache Drill Implementation Deep Dive
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Apache Drill is an exciting project that aims to provide SQL query capabilities for a wide variety of data sources in an extensible way.

But the technologies underneath the implementation are also very exciting even outside of the context of Drill itself. These ideas can be repurposed for a wide variety of other uses either by directly extracting code from Drill, or by using the philosophies and ideas in new forms.

I will talk about how Drill goes about several key tasks including: forming a DAG of operations and binding these operations to real code. Drill does this using a JSON concrete syntax so that the DAG can be created or executed in a variety of implementation languages including Java and C++. moving schema-free or flexible schema nested data through an execution DAG as efficiently as data with a rigid relational schema. Drill uses a novel column oriented format with a new batch adaptive schema technology. This allows the inefficiencies associated with non-columnar data or with flexible schemas to be confined to the data sources where the penalty is only paid once. 

Read more:
https://2013.berlinbuzzwords.de/sessions/apache-drill-implementation-deep-dive

About Ted Dunning:
https://2013.berlinbuzzwords.de/users/teddunning

About Michael Hausenblas:
https://2013.berlinbuzzwords.de/users/mhausenblas-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I'm Michael right so it's all about                               apache drill so let's roll into that                               first a little question for you what                               kind of workloads do you see ya we know                               that one batch processing right HBase                               Cassandra fold that kind of stuff string                               processing what what kind of streams we                               can process IOT social media streams                               whatnot of course very prominent here                                sergeant information retrieval                                elasticsearch solar and so on but what                                about interactive at hagakure at scale                                well these days we pretty much have                                these options not very interactive and                                what we want to achieve what we want to                                get at is these guys right so talking                                about interactivity having a human                                sitting in front of a box clicking                                somewhere in expecting a result within                                seconds or less right so let's have a                                look at two of the selected use cases we                                had a look at before we started with                                that imagine you're in a position where                                you want to attract suppliers so they                                might have for you know shipments and                                whatnot and you have very specific                                questions there such as you know how did                                that one supplier ACM how did that guy                                do in the last                                                        data might in fact be you know good old                                relational databases and some shipment                                logs in chasing for example right                                another                                                               detection whatever you could imagine                                that MapReduce style you generate a kind                                of heat map here and the human operator                                looks at that and sees oh there is                                something going on here something                                reddish right and now needs to drill in                                that's where the name comes from right                                so you need to understand what is going                                on in terms of the transactions there to                                see if the one transaction here is in                                fact a fraudulent one right a couple of                                requirements we derived from these use                                cases there many more out there are that                                it's essential to being able to support                                different kinds of back-end different                                kind of data stores we want to be able                                to have different kind of query                                interfaces really obviously we're                                dealing with low latency systems so kind                                of yeah synergetic to to the badge                                MapReduce style of doing things we want                                to be able to support ad hoc queries and                                hoc queries are really important because                                the knowledge I have about the data set                                or the priorities change rapidly so I                                need to be able to ask questions in an                                ad hoc way and hopefully get answers                                pretty quickly and of course as usual                                it's got to be scalable and reliable                                something completely different kind of                                backdrop where it comes from we heard                                today already from own regarding dremel                                                                                                     nice research paper and highlighted some                                of the more interesting pieces of the                                abstract there dremel system is a                                scalable interactive ad-hoc query system                                for read-only nested data right so if                                you keep that in mind and that hopefully                                survives the after party today then my                                child is not coming back to drill so                                some of the key facts that you might                                want to yeah consider if that is                                important for your application as I said                                I try to point out drill the drill                                design is based on on largely based on                                Google's dremel we aim for sequel                                     support recent being that a lot of the                                deployed bi tools out there natively                                speak sequel                                you want to be able to plug that in and                                directly work with that if you think                                back of the requirements the pluggable                                data sources so if you want to put                                there a fine couch HBase whatever you                                have right we treat nested data as                                first-class citizens right the schema is                                optional so if you have one to provide                                one that's great otherwise we're trying                                to explore it and find out how this                                we're going to talk about that in                                greater detail is ugly to me personally                                what is very important about the overall                                activity is that it's a true community                                driven open source project so at the end                                of the day not Ted or myself but you                                 guys decide how far we go how quickly go                                 and what we can achieve right the high                                 level level architecture of pet seadrill                                 essentially down here you have the data                                 sources as already mentioned pretty                                 flexible regarding that can support                                 whatever you like there I come back to                                 that in a moment here in userland you                                 have a number of i would say in Apache                                 Hadoop ecosystem almost standard                                 interfaces arrest in the face                                 command-line interface so you have a                                 shell where you can interact with drill                                 you have the native API it's written in                                 Java and they're important jdbc odbc                                 right so that's what the end user                                 essentially sees no matter if it's a                                 necktie programmer or more down to earth                                 hecker like we are right so a number of                                 terms i'm going to use are we going to                                 use in the in the following you might                                 want to remember the source query that                                 something comes in sync sequel and tells                                 us what we want to do the logical plan                                 that's our core piece there is a very                                 abstract way to say what we want to do                                 in a language agnostic way the physical                                 plan is actually the plan that says how                                 we want to do things and the execution                                 plan is                                 all about where we do things in a                                 distributed setup and that leads us to                                 the career execution and now we will                                 already benefit from all the terms which                                 has learned here comes to source query                                 we have a parser open Parsa API which                                 means if you come up with a new domain                                 specific language whatever that's fine                                 we can you know just yeah supportive                                 right out of the box translate whatever                                 comes in here into the logical plan                                 looks like chasing actually is chasing                                 and if you want to you can serialize                                 that externalize it as chasing and                                 actually you know print it out put it in                                 the wall whatever then comes the                                 optimizer the interesting part here is                                 that as soon as you run in a distributed                                 mo dinner in a cluster this optimizer                                 here has a real job to do right trying                                 to figure out how to partition things                                 and and spread out things and then we                                 have the scanner API and there you can                                 plug in any kind of data source really                                 now going a bit deeper there in the wild                                 level architecture so what we have                                 imagined you have a four node cluster                                 and each of these nodes has a drill bit                                 that's kind of process running there                                 locally and storage process running and                                 the entire coordination the Career                                 Planning and so on is actually done                                 distributed and how that is performed                                 we're going to talk about that in a                                 minute and just remember each of these                                 drill bits can act as a as a format can                                 take on a query and yeah drive that                                 entire career process number of things                                 here what we use internally is correct                                 her which is Netflix curator kind of                                 wrapper around sue keeper so Cooper is                                 great but it's it's nicer to deal with                                 it through through crater right and we                                 use a distributed cache called hazel                                 cast and that keeps track of all the                                 metadata locality information stuff like                                 that so as I said the originating drill                                 bit here is the formant that                                 the query and thats how it looks like                                 right the drill bit where I send in the                                 query that turns out to be the root of                                 this multi-level execution tree that's                                 one of the innovations introduced by the                                 Google engineers and then distributes to                                 clear it down to each of the workers                                 here that actually scan a part of your                                 data and then report back the results                                 there you can imagine that this works                                 well when the input data size and the                                 output data size are not equivalent                                 right so if you have a huge like five I                                 don't know exa piles or whatever of                                 input data and your result set is also                                 in that same dimension that might not                                 probably be the best idea but if the                                 result set that's your thing back of the                                 use cases the very specific questions                                 and very yeah the result set then                                 hopefully is rather small compared to                                 the input data number of technologies we                                 use here I remember someone saying in                                 the morning pointing out Nettie yes one                                 of the great things we heavily rely on                                 optique trillion height of pent house is                                 working and contributing on that almost                                 there and now number of other things as                                 I said hazel cast pretty pretty                                 important curator as a wrapper around                                 zookeeper yeah quite some of the good                                 shiny things here and as I said or kind                                 of summing up to that part the key                                 features full sequel support nested data                                 is first-class citizens optional schema                                 and extensibility points which in my                                 understanding really sets drill apart                                 from others things like you know custom                                 operators UDF you can do quite a lot of                                 things in the optimizer any kind of data                                 source you can plug in via the scanner                                 API so this is really where we're drill                                 shines very flexible very extensible and                                 yeah of course we were working also on                                 the user interfaces                                 and one question I often get is how does                                 it compare to Hadoop or or is it a                                 Hadoop killer no it's not it's it's in a                                 sense it's complementary and if you want                                 to learn more about that the Google                                 bigquery team put together a very nice                                 white paper and bigquery is essentially                                 the user facing the service as a                                 software version of dremel that you can                                 use online and now I believe it's time                                 to hand over to Ted so what I want to                                 talk about here basic them Oh what I                                 want to talk about a bit here are some                                 of the key aspects of extensibility in                                 drill as Michael mentioned one of the                                 unusual characteristics is that you can                                 actually inject code in at several                                 levels you can provide a sequel parts or                                 sequel query which is parsed to the                                 logical plan but you can provide the                                 logical plan to drill to provide to                                 execute different things you can also                                 record the physical plan for a                                 particular query or you might guide the                                 optimizer a little bit until you get a                                 physical plan that you like and you can                                 you reuse that physical plan you can                                 modify it for your own purposes so                                 there's deep integration points which                                 are well specified and usable there i                                 also want to talk a little bit about how                                 that physical plan actually execute and                                 some of the ways that drill is able to                                 do this processing with very very few                                 copies and no more serialization                                 deserialization overhead than is                                 strictly necessary I'm going to skip                                 over the basic demo sort of things but                                 here what you can see we have here in                                 illegible type on this on this presenter                                 I'll try to read it or it's so small                                 here that there's no way let me see if i                                 can adjust                                 oh okay so what we have there is a data                                 source which has a record that describes                                 a particular kind of donut and a few                                 different variants on that and here we                                 have a logical plan again difficult to                                 read it has starts with a query it is an                                 operator an operator sequence sequence                                 is a higher order function and it takes                                 functions internally and it                                 automatically wires those up here is one                                 operator which is a scan operator and it                                 references a donuts input which we don't                                 show here and it has a source which is                                 interpreted by the the scanner and it                                 selects particular data from there so                                 we've pushed down a selection into the                                 logical plan and now we also have then a                                 filter which has an expression Donuts                                 ppu less than two dollars and this dot                                 here is an extension that exposed is                                 exposed back through this sequel                                 interface and that allows us could look                                 into hierarchical elements directly but                                 it also in an interesting way when it                                 gets down to the physical plant lets us                                 look into this hierarchical elements                                 without deserializing the high the top                                 level elements we only deserialize the                                 low level elements so that's the logical                                 plan there and drill then produces an                                 output of JSON format like that which                                 are records now the use of JSON here is                                 interesting because this illustrates                                 late schema binding as you read a JSON                                 file you have no idea what data you're                                 going to see you have no idea which                                 fields you're going to see you know have                                 no idea what sort of data you might be                                 executing in there and drill is going to                                 handle that or does handle that right                                 now in an interesting way what it does                                 is it reads the input in batches and                                 potentially in parallel batches each                                 batch then has a derived schema if the                                 input is JSON so on the fly it invents                                 schema for this batch since this batch                                 is finite and small we can do that very                                 easily we don't have to say anything                                 about the future we just say something                                 about the present what we see right now                                 and so if we see integers in a                                 particular field now we can say that's                                 our schema if later we see floating                                 point numbers or strings we can adjust                                 the schema later and so what then the                                 execution unit does is it takes the                                 schema for the batch generates code                                 based on that so that it gets primitive                                 level performance against the batch that                                 we see if the next batch has the same                                 schema then all as well we don't have to                                 regenerate code is the schema changes we                                 do read generate code now for cases                                 where it's not JSON but where there's a                                 strong schema we get that schema up                                 front we do the same sort of operations                                 it's just that the caching of the                                 generated code works well so let's go in                                 a little bit more into how that works so                                 here is the original sequel that were                                 interested in a particular case some                                 unusual characteristics we might say                                 that this is out of an H base or an H                                 base like table this happens to be an m                                  table which exposes in HBase API it                                 could have been HBase could have been an                                 Oracle table we're going to group it by                                 name which is presumably although we                                 don't know it yet a column in this table                                 and we'll group it there order it by                                 total sales in a descending order and do                                 a limit so the next step transforms this                                 into a logical plan here we have the                                 scan it's now recognized that this                                 storage engine is m                                                    HBase compatible storage scanner and it                                 has pushed down a little bit into that                                 of the query just to say which table                                 very little push down at that point then                                 we have an operator which does certain                                 projections these select different                                 columns we have an operator here which                                 does segmenting which is the primitive                                 operation in a group by then we have a                                 top Singh aggregate which looks for                                 elements in succeeding records until a                                 flag column in this case the carryover                                 name their changes and so it knows to                                 reset the aggregator at that point and                                 here are the aggregations being done the                                 aggregations allow a symbolic form kind                                 of a formula that formula is of a                                 limited subset of a Java like language                                 and that is actually what gets generated                                 into native code on the fly during the                                 execution this is handled by a specific                                 grammar so whatever form that you have                                 in your surface form has to be                                 translated slightly their sequel for                                 instance requires almost no                                 transformations other languages might                                 require if you're translating cloture or                                 kaska log or something into this                                 primitive language you would have to                                 regenerate those expressions we then do                                 an ordering which is going to resort the                                 sequence of records that are gone and                                 here are the specifications of the                                 ordering and then we're going to store                                 that data so this is all the logical                                 plan this is a very very tight semantic                                 description of what's happening but it                                 does not express how the computation                                 will be done it merely is a restatement                                 of the sequel into the terms specified                                 by the logical language the physical                                 plan on the other hand changes things                                 around a lot and very commonly for for                                 normal sorts of queries that you would                                 expect to see the physical plan is much                                 much simpler than the logical plan and                                 this is because we can do things like                                 push down the projection operator                                 directly into the scanner we don't need                                 to pull all the columns out of the m                                  table and so we don't so we've pushed                                 down that we also then inject this which                                 is a and at what's called an exchange                                 note it's a marker for this is a point                                 where parallelism can occur or data                                 would be exchanged from one node to                                 another if we were to paralyze groups of                                 operators                                 in exchange notes this operator here is                                 the sorting hash aggregate which does                                 the group by does the the aggregation                                 and does the the final ordering and it                                 carries all of the aggregation formulas                                 from before back in and so all of the                                 three or so operators in the previous                                 slide we go back here this segment the                                 collapsing aggregate and the ordering                                 all collapse into that one physical                                 statement and this is common that we                                 will have optimized combined operators                                 to do a set of common things so the                                 exchange defines where we have a                                 parallel as a boundary and all of this                                 down here can then be executed in                                 parallel except of course the screen                                 which drives back to the terminal and                                 that is an inherent serialization point                                 the the boundaries between exchange                                 notes is a good example of something                                 that is the parallel equivalent of a                                 basic block in normal code generation so                                 then the execution plan is generated                                 from the physical plan taking into                                 account where data is actually located                                 how many nodes that we have to execute                                 on and it generates fragments it doesn't                                 generate a single query like we saw in a                                 logical or physical plan it generates                                 fragments that are then sent to the                                 various nodes and the fragments include                                 addresses of the place to send output                                 and so on and how to interconnect them                                 and which actual little fragment to                                 execute the execution unit then takes                                 that tiny fragment and generates code                                 for that so if we look at how these work                                 we have leaf nodes which are doing the                                 raw reading and initial aggregations or                                 things like that initial filtering then                                 we can pass those back up to                                 intermediate nodes the reason we have                                 multiple intermediates instead of one is                                 that this is not just a tree                                 aggregation but it's dag based execution                                 so the output of a drill program could                                 be much larger if necessary than the                                 input not the normal pyramid style                                 aggregation where the output is supposed                                 to be small the intermediate fragments                                 don't actually start until they start                                 receiving data and all of the transfers                                 here are in the form of those batches                                 that I referred to previously there's a                                 tight wire level spec for how batches                                 get transferred but it's not tied to the                                 execution of drill itself the format is                                 designed it's in proto buffs but there                                 are large uninterpreted blocks which                                 actually described the particular                                 columns this this protocol feeds also                                 designed specially so they could be hand                                 interpreted on the fly the first few                                 bits specify the structure of what's                                 coming up from there we can actually                                 direct the socket to off heap memory so                                 that we can move that data very                                 efficiently once it lands there into off                                 heap memory for a particular execution                                 unit it stays in that place and in that                                 form we don't transform that and move it                                 into the heap at all and if we can avoid                                 it what happens instead is we have two                                 things one is a row wise bitmap which is                                 how we encode filter operations that's                                 stored on the heap and is associated                                 with the current set of records and that                                 expresses which records are actually                                 active we do a push through architecture                                 inside there so we push the entire set                                 of live columns into a native code                                 executor which does whatever operations                                 are going to happen updates the filter                                 mask for the set of rows and only DC                                 realizes rows that are necessarily                                 involved in the selection or the                                 operation and in some cases can even                                 push down selection into the compressed                                 form so we never deserialize the data at                                 all on output                                 the columns that are being used                                 unchanged if we haven't had any                                 aggregation points inside this execution                                 unit are copied verbatim out along with                                 any new columns that have been generated                                 in this execution unit if we did a                                 filtering operation at the output stage                                 we applied the filtering operation if it                                 results in very few bits being set or we                                 carried along for the next stage if it                                 results in most of the records being                                 happened so the effect of this is we                                 have one copy coming in directly from a                                 socket to off heap memory and we have                                 one copy going out for most operations                                 and that's about as little as you could                                 get we also have no deserialization of                                 data we're just carrying along for later                                 units and we have no deserialization for                                 filter operations that can operate on                                 the compressed form that means we can                                 minimize deserialization andrey                                 serialization of data as it moves                                 through the execution units and many                                 execution units then can run at full                                 wire speed in and out this is an unusual                                 operate characteristic for java                                 executors and here's some some of these                                 fragments and we can start seeing how                                 this code generation can happen here for                                 instance we have a leaf fragment which                                 is the hash partitioned cinder so that's                                 part of the group by step where we've                                 got things being sent and we have a                                 child operator here which has a mock                                 scanner which calls from there and                                 mostly all it's doing is a projection                                 operation here's the original that this                                 one is pulling that many records that's                                 telling there here we go here's the                                 actual filtering and there's the                                 expression applied to that this is fully                                 cast fully type managed because we now                                 know the schema as we generate that and                                 so we can send from here to our                                 destination                                 ri that's not shown but the destinations                                 here would be where we send our data                                 going out this therefore expresses a                                 very flexible dag and a very high                                 performance tag the optimizer that we                                 use is not complete it's beginning to                                 produce interesting plans the place                                 that's in place that's very complete                                 right now is the sequel to logical blam                                 that gets down to nessa joins and and                                 interesting operations like that the                                 part that's not very well fleshed out                                 are the physical optimizers right now                                 but it's a fully general rule base re                                 cost-based optimizer so it has a lot of                                 flexibility that can be applied in those                                 last stages and it uses the same rule                                 set its uses the same representations                                 all the way from logical plan through to                                 physical fragments and so the                                 transformations that work at any given                                 level can be adapted to be work can work                                 at later levels this is one of the                                 hardest problems in the system of course                                 because we don't have some of the data                                 that we would have in a traditional                                 system we don't have as much in the way                                 of statistics we're hoping that the                                 thesis of Dremel that statistics and                                 that kind of optimization are not as                                 important in large table scan sort of                                 applications will will come through and                                 so far we do see that most of the                                 optimizations are predominant you have a                                 particular form of optimization that                                 just totally dominates the others based                                 on the simple statistics that we do have                                 there's also some early talk in in drill                                 that we could replan as we saw different                                 batches so the batches themselves would                                 provide early statistics for later                                 optimization and so there are still                                 hooks for the potential of we would                                 rewrite the dag on the fly during                                 execution we don't have that in plan at                                 this                                 there will also be a lot of caching of                                 query optimizer steps so that once                                 you've optimized a particular query for                                 a particular data set you'll have that                                 optimization result available and you'll                                 also have statistics from previous runs                                 so you may be allowed to adapt that or                                 you may want to expose those previously                                 optimized forms to users this would be a                                 form of query hinting that's much more                                 expressive you might say then sequel                                 level for reentering which is quite                                 difficult in a in a language where there                                 are no schema or direct our index system                                 to work with the execution engine itself                                 provides is provided by a single JVM                                 that heavily threads internally it uses                                 a very small huge space the initial                                 prototypes are using heap space is on                                 the order of                                                           means that GC is a non-issue it's                                 milliseconds or less of GC overhead when                                 it does happen and that's because all of                                 the big data that's moving along is off                                 heat and so we have a few data elements                                 being materialized when the heap arrives                                 as it comes in we have a little bit of                                 code generation components being                                 generated at that point but those then                                 tend to live for the life of the query                                 across many batches and so the amount of                                 garbage collect created is very small                                 and it's created over a period of                                 seconds which is the processing time for                                 a fairly large set of chunks which is                                 again not a very big load for the GC we                                 have callbacks for each message sent                                 that allows us to have detailed metrics                                 as things are coming through and as I                                 mentioned before the protocols are                                 heavily designed to minimize copying and                                 Surrey costs we think that that's based                                 on our experience with other levels in                                 the map our system these these copying                                 costs one of the dominant overheads that                                 apply in these high performance system                                 and in fact these these tricks with the                                 proto buff where we do the direct                                 allocation and redirection of components                                 of the RPC directly into those are taken                                 directly out of the map our file system                                 those techniques the query set up in                                 fragment runners are all managed via                                 queues and thread pools and so we                                 inherit the the good characteristics of                                 job at that level we break the data into                                 batches and this is how we're able to do                                 this late binding schema every batch has                                 its schema as a collection of fields and                                 it has particular data types we have two                                 expressions of the data types one is at                                 the network level where we have things                                 like small and I'm sorry we have things                                 like four byte quantity we have a                                 secondary type system at the execution                                 level itself where we understand that                                 that for by quantity is a small integer                                 instead of a long integer and that                                 allows us to abstract away the internal                                 data structures the internal                                 interpretations so that the network                                 layer doesn't have to change as these                                 internal interpretations of the data                                 change so if we add new kinds of                                 compression as we add new kinds of data                                 structures or alternative in coatings                                 for things like strings we don't have to                                 change the network layer or the batching                                 layer or the overall execution framework                                 at all these value vectors are where we                                 store these things and these are facades                                 into off heat byte buffers this is an                                 exciting and probably reusable construct                                 it's been designed to be relatively                                 independent of all of the execution                                 environment we I was talking to sac                                 earlier and we've been talking at map                                 are not necessarily on the drill topic                                 yet that some of the the need for                                 coprocessors in systems like HBase there                                 there are two needs one is for core                                 committer                                 to write new code for HBase the other                                 source of need for these coprocessors is                                 for user level constructs the user level                                 constructs are very poorly served right                                 now in h by spy code processors what we                                 need is an arm's-length coprocessor that                                 doesn't have visibility into HBase                                 internals value vectors or something                                 very much like them would provide a very                                 high performance method for building                                 these out of JVM arm's length HBase                                 coprocessors for instance they could be                                 used in other applications as well where                                 the idea of batch small batched                                 computation would be valuable and the in                                 memory of each value vector is well                                 defined and language agnostic and you                                 have to hurry up a little bit because I                                 want to give some question asking time                                 the font is getting smaller as we drill                                 into these systems as you can tell and                                 the level of detail is getting higher                                 and higher but the the key point from                                 this slide is that the particular style                                 is that we want to push as much of the                                 work away from the execution framework                                 so that we can solidify that early and                                 drive most of the implementation effort                                 into the creation of a large number of                                 operators these operators will be of the                                 form small scalar functions and scalar                                 functions here could operate on complex                                 elements or they will take the form of                                 things like that collapsing aggregating                                 scanner they might be special-purpose                                 scanners that pull in parts of the query                                 for a particular data source you can                                 imagine an Oracle scanner could pull in                                 quite a lot of the query an HP scanner                                 might be able to pull in some of the                                 skin scanner are some of the query for                                 some operations and so that style is is                                 of building a large number of operators                                 is key to this design another key in                                 terms of performances that we're sizing                                 all of these batches to be propitious                                 for execution they're going to be                                 sighs oriented probably a small sub                                 multiple of the l                                                       batch lands on a processor will be able                                 to process that extremely fast and we                                 won't be blowing cash and we won't be                                 blowing cash for other course either                                 going to skip through most of this                                 information is useful I think if you're                                 reading these slides offline as opposed                                 to trying to catch it as it goes whippin                                 by as I talk faster and faster as the                                 phone gets smaller and smaller and it's                                 got a professor and higher so get higher                                 information content as I speak input and                                 output goes via the storage engine                                 interface one of the interesting things                                 here is that the API for storage engines                                 is partly in the form of optimizer rules                                 so every scanner if I write a new                                 scanner for my new whiz-bang data store                                 what I will give to the system is a set                                 of optimizer rules which tell the system                                 how to give me specialized operators                                 based on the structure of the logical                                 plan and that gives a huge amount of                                 flexibility and lets us we've been                                 looking at JSON we've been looking at                                 HBase at sequel engines and splunk and                                 at things like the storage engine                                 as as case studies and this style allows                                 the special purpose qualities of each                                 platform to really shine and it's the                                 only way we've been able to find that                                 lets the the peculiarities of certain                                 storage platforms to be taken advantage                                 of in a general-purpose environment and                                 it turns out it's not terribly hard to                                 do that you just basically say if you                                 have a where clause with these variables                                 I can handle it and then the optimizer                                 handles rewriting things to give you the                                 sort of things that you need to run well                                 and you then we'll be getting a record                                 reader or the record writer sort of                                 calls and there it is one of the key                                 things here and I'm going to let Michael                                 continue on this is that we want to be a                                 very very open very very aggressively                                 includes                                 group that means that you guys are going                                 to be a big part of this effort and I'm                                 going to talk let Michael summarize here                                 thanks a lot cat right current status we                                 have already some bits here in place so                                 you can play around with basic demo and                                 other things we're working a number of                                 things in parallel sequel support and so                                 on also in May yeah back to the future                                 and there that's probably a very                                 interesting slide for you so even if                                 you're not a coder you can actually                                 contribute in you you are encouraged to                                 contribute coming up with test data test                                 queries use case scenarios or probably                                 even starting the documentation around                                 that and so I really like to encourage                                 people who don't have that coding                                 background probably in this crowd that's                                 not that important but you can yeah and                                 you should let me give an example Ellen                                 hold your hand up ellen has been                                 watching mailing list and just making                                 notes of important things that have been                                 happening she attends the weekly google                                 hangout and she writes it down which                                 means at the end of the three-month                                 period it's a piece of cake to write the                                 quarterly report that's a contribution                                 because otherwise what happens is you                                 get to the end of the three months and                                 you go wow man I don't remember what we                                 did we did a lot and that doesn't make a                                 very good quarterly report to the board                                 probe patchy so there's a contribution                                 that's decidedly non Cody she's also                                 been writing some stuff around that has                                 contract with the Riley to write a book                                 on it just the beginnings of it good                                 examples people can code people can                                 provide provide non-code support for                                 drill in a lot of ways yeah little shout                                 out to our colleagues have contributed                                 so far the core and probably the most                                 important slide engage you has a lot of                                 options here might be on Twitter good                                 old mailing list every tuesday at                                 six o'clock your time here we have a                                 Google+ Hangouts very good you know the                                 chance to interact with with the entire                                 team and we have a little blog here                                 reporting in a roughly weekly basis and                                 i hope we still have a couple of minutes                                 for some questions right all right let's                                 thank the speaker first because
YouTube URL: https://www.youtube.com/watch?v=KsihNaCdEJg


