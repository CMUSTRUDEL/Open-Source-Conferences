Title: Berlin Buzzwords 2013: Dan Filimon - Clustering of Real-time Data at Scale #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Last year at Buzzwords it was reported the Apache Mahout project had a new kind of clustering algorithm soon to be available which promised extraordinary speed. Since that time, that promise has been filled. This new algorithm is extraordinarily fast, possibly the fastest production clustering algorithm available. It also has many unusual characteristics which can make clustering applicable in new ways.

This talk is a report on the progress of this new kind of clustering. I will describe the theory behind how this algorithm works and how it is able to provide high quality clustering with only a single pass through the data. Mostly, however, I will focus on practical results of this algorithm.

Read more:
https://2013.berlinbuzzwords.de/sessions/clustering-real-time-data-scale

About Dan Filimon:
https://2013.berlinbuzzwords.de/users/danfilimon

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so it looks like we have an audience                               here what I'd like to do is introduce                               Dan philemon he popped up on the mailing                               list unannounced just said hey is there                               something cool to work on it was almost                               a year ago nine months ago turns out                               he's an undergraduate student he was an                               undergraduate student I guess that's a                               was now still AM no still out at                                University of Bucharest and he's done                                some excellent work over the last nine                                months in getting this streaming k-means                                ready for inclusion in mahout we have a                                release coming up and this is the                                centerpiece of that release in the new                                                                                                     talk about how this stuff works okay                                thank you ted so um here's what we're                                going to be talking about basically what                                data is you probably all know with that                                clustering k-means the problems with                                k-means but we can do that with those                                problems how do we do k-means tooth                                large scale well a bunch of stuff                                basically this was actually a                                         talk initially I tried giving this like                                a week ago that failed miserably so this                                is hopefully the short version that                                everyone will like and has more pictures                                if you want to go and see the full                                version it's there that link and                                obviously everyone will have these                                slides by tonight or tomorrow or                                whenever okay so what is data well data                                is anything you want it could be like                                documents webpages images users but we                                don't really care about that we                                essentially just want to have vectors                                preferably real-valued vectors but                                pictures with numbers and then these are                                the feature vectors that you generate                                from the data in some way you just                                encoded and then we can cluster it so                                what is this clustering thing well we                                basically want to find some chunk some                                clumps in the data I guess the thing                                that clustering is it's not really one                                problem like I'm going to talk about one                                way of doing clustering but there are                                really more ways of doing it and the one                                I'm going to be talking about it starts                                with the following assumptions you have                                these n data points                                and they are in the air in D dimensional                                space and you want to group them into K                                disjoint sets so you want to do this and                                the whole point of it is to minimize                                this total cost which is this formula                                here which is I promise this is the only                                formula in the entire talk okay so                                doesn't look intimidating I hope it's                                just so the idea is these disjoint in                                sets we call them capital X x                                         way to XK and Excise the i'th cluster CI                                is the centroid of the i'th cluster the                                centroid is just the mean of all the                                points in that cluster and xij is the                                jade point from the i'th cluster and                                essentially what we're doing is we're                                summing up the distance from each point                                to its centroid and summing all of those                                up and that that entire thing is called                                the total cost and we want that to be                                minimized so that's the problem that                                we're dealing with with with some                                distance metric like the distance                                medical there is the l                                                  something else potentially but a                                distance anyway okay that was all of the                                math for today so let me just show you                                how key means works so here are the                                points we want to cluster so these are                                points these are vectors I use these                                type these words interchangeably for an                                other points right they look like points                                and say we want to cluster the data into                                three clusters now the point here is                                that um right here you might not see                                three clusters and indeed it like in                                general it might not be the case that                                you actually have K clusters sometimes                                you you can't really tell how many                                clusters you want to have in advance so                                you just pick some cave and we do it and                                we'll see what it gets us what kind of                                quality we can get there so these points                                are in                                                             k-means does is pick some initial                                centroids so say it's these guys so we                                 pick some points as the initial                                 centroids these aren't really the means                                 of any clusters because we haven't                                 computed any clusters at this point but                                 we just pick some points in multiple                                 there are more ways of doing this but we                                 just pick some points let's just say                                 they're these points okay and so the                                 next thing we're going to do is for each                                 point we're going to drop we're going to                                 find the cluster that's closest to it                                 the centroid that's closest to it right                                 and this will create this partition that                                 you see there I just made the partition                                 up like it might not be very accurate                                 but                                 go so this is the first iteration and                                 we're assigning each point to a cluster                                 and this is the assignment that we get                                 and so the first step is cluster                                 assignment and after that we adjust the                                 centroids so the centroids were here and                                 the centuries are now there they moved a                                 bit okay so after we do this is one step                                 right but we will probably do this                                 multiple times so the next step is this                                 the Soviet the assignment got changed a                                 bit like you can see the lower the point                                 there in the lower um and the lower part                                 of the screen change from being in the                                 leftmost cluster to the other one right                                 so there's that that might not be very                                 accurate but let's just say it is and                                 then we adjust the centroids again and                                 you can keep doing this until well until                                 you stop and stopping is really                                 something you do it when the it couple                                 you do it when it confront the when the                                 code converges basically which is either                                 for some number of iterations or no                                 point changes cluster we actually need                                 to talk about these details so first off                                 since the profits so okay for everyone                                 who's done wishing learning this is an                                 unsupervised learning problem which                                 means that we don't really know what the                                 right answer is the problem itself as I                                 defined it is np-hard like the formula                                 that the minimum is at the minimization                                 so we can only do approximations here                                 and we define a bunch of metrics and say                                 one of them is the total clustering cost                                 the thing that I talked about at the                                 beginning that formula to some but there                                 are others in general we want to have                                 clusters that are compact and well                                 separated and there are a bunch of other                                 indices that you can look up on                                 Wikipedia which measure this in some way                                 like they're not perfect but you know                                 better than nothing so this is how we                                 measure quality and when so we stopped                                 when quality reaches a certain level                                 basically that's one option or after                                 after some number of steps then one of                                 the biggest issues that we face that is                                 like pretty much determines the quality                                 is centroid initialization so well I                                 told you that we're going to pick some                                 number of points the K points we can                                 pick them at random that's the most                                 straightforward thing to do but there                                 are some problems if we                                 picked them at random and we're really                                 unlucky like you see in that picture                                 over there we're going to get two seeds                                 that are really close together in the                                 same cluster which which will pretty                                 much break everything because those                                 clusters won't be able to be pulled                                 apart which will make one of them be                                 empty at one point so you get you get                                 crappy clustering like that so what the                                 constraint sort of fails in that case                                 and one typical way of addressing                                 failures is just trying multiple times                                 so we can do multiple initializations or                                 there's this fancy way of initializing                                 called k-means plus plus where you                                 actually take into account the distances                                 between the points you can you can look                                 up more details in the other                                 presentation but for now we just random                                 initializations as the most simple thing                                 another issue is outliers it it turns                                 out that real data is very messy and                                 sometimes you can't tell if some of the                                 points should really be there or not                                 those three points over there they might                                 be outliers they might be a different                                 cluster who knows but the thing is                                 assuming they are outliers they're                                 messing up that cluster over there he                                 wanted on the top top right so a better                                 centroid I think would be the one where                                 I pointed the arrow but because of those                                 three points it got shifted to the                                 shifted down so we'd like to avoid that                                 and one way of doing it is using bulky                                 means which is basically just instead of                                 using every single point in a cluster to                                 recompute the centroid you just use like                                 a ball around the centroid but wikipedia                                 basically but it's not that complicated                                 like instead of doing that you just say                                 make a ball around each of these imagine                                 there's a ball and it doesn't cover all                                 the points and for those that it does                                 cover you compute their mean that's                                 basically it ok so there's quality we                                 talked about these two issues the                                 initialization and the outliers but                                 other than that we also care about speed                                 the main point of this project is also                                 that clustering works faster so what can                                 we do about the speed so first of all                                 what what is the complexity of this well                                 the biggest thing that determines the                                 complexity is really the cluster cluster                                 assignment the part where we go through                                 each point and to each cluster and                                 compute the distance to that cluster and                                 then we take the minimum so that's going                                 to be n so if you have in point                                 and K cluster that's going to be n time                                 scale right there and then times G for                                 the distance calculation so why don't we                                 of getting getting a better speed is to                                 simply not go through all the clusters                                 so we're going to reduce k so I'm sort                                 of doing the accurate search where you                                 go through every single cluster we're                                 going to do an approximate search now                                 there are multiple ways of doing this                                 but pretty much everything we do is                                 based on random projections so random                                 projections is basically a fancy name                                 for a very fairly basic idea here's a                                 nice picture so suppose that line is a                                 vector it actually has a back to ride so                                 we're talking about to lease space here                                 right you basically are going to sample                                 some vectors from someone from a more                                 normal distributed distribution you're                                 going to normalize them you're going to                                 get a direction say that is the                                 direction you're just interested in the                                 direction really at this point and it                                 looks like a line if you want to think                                 of it that way then you have those                                    points and you can think of them so it's                                 a their points but they're also vectors                                 as in the other end would be in the                                 origin of the system and you can see how                                 they project onto the line right so                                 those are the projections of the vectors                                 onto that direction the cool thing about                                 it is that it will give you a total                                 ordering so normally you have these D                                 dimensional vectors you can't really                                 compare them right you can compare                                 complex numbers you can compare these                                 things but if you project them you will                                 just have scalars that you can compare                                 and if you sort these the cool thing is                                 you can bind a research so you're given                                 a new query vector that you want to                                 search for and you can simply search for                                 it in this sorted set and that's going                                 to be a log K block a time operation and                                 there you go you can actually get                                 multiple directions if you want better                                 quality results but two or three works                                 fine so that's one approach like there's                                 a lot more detail in the other talk butt                                 plug this is probably ok for now so this                                 is one approach that you can use to make                                 it faster and the other one is just well                                 if the data is too big like there are                                 too many dimensions and these vectors                                 just make them smaller and they're too                                 well the one you probably have used is                                 principal component analysis                                 and the other one is just have lots of                                 random projections and you can use those                                 random projections of those vectors as a                                 matrix which will get you smaller                                 vectors the point is you apply linear                                 algebra it just works ok that's                                 basically it um ok so k-means in mahout                                 does not do that like that it doesn't do                                 any of that it doesn't do the random                                 production search you can potentially do                                 it has this video so you can reduce d                                 but um it doesn't do the smart                                 initialization with cammies + + doesn't                                 do any of that so what does it do what                                 how does it work like you I've sort of                                 explain to you what the basic algorithm                                 is but how does it look like as a                                 MapReduce because this is supposed to be                                 a scale talk right so k-means is a                                 MapReduce is pretty much well first off                                 you want to paralyze it right you have                                 the mappers and reducers you've all used                                 MapReduce so the thing you would want to                                 do is to split it somehow but you have                                 this outer loop which is the number of                                 iterations why do you go through each                                 iteration and you do the cluster                                 assignment and then you do the                                 readjustment of the centroids but the                                 problem is you cannot split the outer                                 loop right because you basically just                                 have data dependency there right you                                 need the results of the last step to do                                 this current step so that's a no go so                                 you must express one actual step of the                                 of the k-means algorithm as a MapReduce                                 which basically just means that the                                 mappers will be responsible for cluster                                 assignments and the reducers will update                                 the centroids now this works as an this                                 is exactly what whoo-hoo does and mahout                                 basically does but I just described with                                 using random initialization and with                                 some number of iterations and told                                 converges now this so as I said this                                 works but it's not ideal we really don't                                 like having multiple map reduces for one                                 you shuffle more data round you pass                                 through the date of multiple times but                                 then there's also the question of well                                 you can't really do some things that you                                 would really like for a better quality                                 clustering so say you can't really do k                                 means plus plus for instance you do                                 reservoir sampling the other random                                 points but how do you get reservoir                                 camis + + is a lot more complicated than                                 that you need to dynamically adjust the                                 probabilities of the points                                 you're sampling from so we can't really                                 do that and so other other issues are                                 just like in terms of implementation we                                 don't implement fast searching and stuff                                 so there's that but we can do what we                                 can use a totally different approach                                 though and this wholly different                                 approach is not that totally different                                 it turns out we actually you so if you                                 went to the Kino you already know what                                 it is it's basically just make small                                 make large data into small data so this                                 is based on an idea in that paper but                                 this is their multiple papers about this                                 but this is the one we actually using                                 the implementation fast and accurate key                                 means for large data sets the idea is                                 instead of clustering the large data set                                 that you have you want to cluster a                                 smaller one right so what you want to do                                 is to build a sketch of the data a                                 sketch of the data is basically just you                                 want to call you you want to do a                                 clustering but with more than K points                                 probably and each point each cluster                                 that you're going to have each                                 intermediate cluster is going to be a                                 representative for the points in that                                 cluster each and remain intermediates                                 Android sorry so in the paper it                                 basically says that you need to have of                                 K log and intermediate clusters for                                 there to be any sort of guarantees about                                 what the K clusters you're going to get                                 at the end will be like and the key idea                                 here is we want to collapse these                                 clusters so that the resulting data set                                 will fit into memory because we can do a                                 lot more things when it when we have the                                 points in memory but here's what                                 streaming k-means looks like here's what                                 it does so you have streaming k-means is                                 an online i would write that's that's                                 the entire point that's why we can do                                 just one iteration through the entire                                 data set and still get the clusters so                                 here's what it does so you have the                                 first point which comes in there's no                                 other centroids so you might as well                                 make it a centroid right then you get                                 another point which is fairly far away                                 say now this point being that far away                                 it will become a centroid because well                                 it seems fairly far so it sort of makes                                 sense it's just intuitively points that                                 are really far from essentially should                                 peace and joy then you get another point                                 which is fairly close to the closest                                 other sentry that you have so being                                 fairly close we're going to say well                                 this is close enough so rather than                                 making this a new centroid we're going                                 to just put it in that cluster so                                 the cluster becomes these two points and                                 their centroid is that red dot that                                 moved a bit so we merge it in that case                                 now we get another point this point is                                 fairly close right it's really close to                                 the right class with the closest cluster                                 but um you see here's the part that's                                 nice because we could merge it but then                                 it turns out that here we don't merge it                                 we make it up we make another centroid                                 now there's a real reason for this it's                                 more it's more of a probabilistic choice                                 so the main idea here is that as points                                 are further apart further from there's                                 closest century they should probably be                                 a separate sent as a separate centroid                                 themselves if they are close enough then                                 they will with some probability that                                 increases as the distance decreases they                                 will be part of that cluster that                                 they're close to so this is the                                 pseudocode this is like the horrible                                 slide with the text in it on the basic                                 thing that you want to look at here is                                 this this trunk do then if an event with                                 probability proportional to blah blah                                 blah occurs the idea there is that                                 that's where you compute the distance                                 right the distance to the closest                                 cluster and if that is close enough then                                 we will want to add it to the cluster                                 otherwise we're gonna create a new                                 cluster and you can see that there's a                                 way to W they're included and what                                 that's saying is well if there are lots                                 of points in that cluster represented by                                 that centroid we're actually okay with                                 not really making it a different without                                 really merging it because we would sort                                 of lose too many points that we were                                 clumping stuff together which we                                 shouldn't really be doing and okay we                                 did that but you're getting points one                                 by one like I've shown you in the other                                 slides like you're getting points one by                                 one but you're going to be making lots                                 of clusters right you need to stop at                                 one point because we said we want to                                 have all of K log n so um when do we                                 stop well we don't really stop we just                                 continue going on until we have too many                                 clusters now this too many is fairly                                 loose you can you can look at the paper                                 the code but basically it's supposed to                                 be within some constant of that K log N                                 and when that happens what we're going                                 to do is instead of                                 we need to collapse them somehow right                                 there there's way too many of them so                                 we're just going to do another pass of                                 the streaming k-means but instead of                                 adding new data points and clustering                                 those we're going to use these centuries                                 that we have that are way too many we're                                 going to treat those as points and we're                                 going to just collapse those together so                                 that's but we just applied recursively                                 sort of so that's streaming k-means                                 basically now what this allows us to do                                 is this allows us to do just one pass to                                 the data which is really useful and how                                 would this work with you know the big                                 MapReduce idea well the idea is you have                                 all of your points right and instead of                                 having that problem with the outer loop                                 that you can split you don't care about                                 that anymore you can just get all of the                                 points and you're going to split them                                 across your mappers you're going to run                                 streaming k-means on each mapper they're                                 going to get some intermediate centroids                                 from every single mapper and what you                                 want to do with those is you're going to                                 collect them in just one reducer and you                                 might potentially apply swimming k-means                                 again if there are way too many of them                                 but there should be there should be                                 enough points so that they fit into                                 memory right that's the main idea and                                 once you get few enough points then                                 you're going to be able to just apply                                 bulky means and but bulky means just                                 basically the ordinary key means we                                 talked about previously with some fancy                                 tricks like the k-means + voss                                 initialization which you can do because                                 the points will fit into memory this                                 time so you just do that in the reducer                                 basically and that gives you the                                 approximate key means basically that                                 that's how you get the K clusters now                                 this is for MapReduce right um you can                                 also do this in storm sort of so how                                 many of you have you storm do I need to                                 like tell you about what it does yeah                                 have used to people okay um so basically                                 and install you have this network of                                 nodes which you call bolts and you you                                 build this this network of well of these                                 nodes basically and you start from                                 points college spouts that they so those                                 omit the data that goes through this                                 topology the graph right and in each                                 node of the graph you're processing                                 those streams somehow the streams of two                                 the data that's coming through this is                                 some as the storm is supposed to do                                 online processing so what we can do for                                 storm is as our points are streaming in                                 like literally screaming in streaming                                 k-means makes a lot more sense it's not                                 doing batch processing as as MapReduce                                 does so we can just do streaming k-means                                 in one bolt where each each point comes                                 in through the bolt we're going to                                 cluster it with what streaming k-means                                 does and periodically or based on some                                 signal or something we're going to omit                                 the approximation that we have that's                                 basically the use case and you can do                                 that you can do anomaly detection or                                 stuff using this but um you could let                                 the code will actually do this but I                                 have it and it's in a good hub rep oh                                 it's it's just you copy and the mahout                                 code and change a few things it works                                 fairly well and so the end that is tried                                 in which is and which is basically a                                 higher level storm thing but I'm not                                 going to go there so now that I talk to                                 you about all that how does this                                 actually perform well let's let's set                                 our expectation straight i guess so what                                 we want to do is we want to get                                 something that is approximately the same                                 as key means right both of these are                                 approximate algorithms so we are it's                                 not as if you're gonna get you know the                                 optimal solution or anything but ideally                                 with this other approach since it's                                 supposed to go faster that's the main                                 point we wanted to be to also have                                 decent quality so we're going to compare                                 the quality and the speed now what I did                                 for measuring the quality is I picked                                 these small data sets off you see I did                                 you do machine you've probably seen                                 these already some of them are really                                 small like iris has just                                                 four dimensions each these are just iris                                 species but then there's power which is                                 like a lot larger has like twenty two                                 million points with nine dimensions but                                 still these fit in memory on just one                                 machine so they're not very big and what                                 I did to compute the quality here is                                 simply compute the total cost which is                                 the thing that we mentioned that we want                                 to minimize so that's that's good if                                 that small but also these three measures                                 that you see there the done index the                                 day baseball tennis and you just rant                                 index so what what these do is the first                                 to basically try to tell you if your                                 clusters are compact and well separated                                 that's essentially it I could I could                                 put the formulas on the slide but you                                 can just go to Wikipedia and see what                                 what they look like the idea is for the                                 done index that you want it to be large                                 and if its large then that means you                                 have a good clustering and the Davis                                 bolded index is supposed to be small now                                 they're just random mix on the other                                 hand is supposed to be a measure of the                                 similarity of two clusterings so say you                                 have                                                                  the same data and what you do is so I'm                                 going to just show switch the next slide                                 which is an actual example here so this                                 is the iris data set right the small                                     point example so you can you build this                                 matrix table called the confusion matrix                                 or if you've done classification this is                                 probably familiar so what you do is for                                 each example that you have you're going                                 to find the closest clusters to that                                 it's closest to write in the two                                 clusterings in clustering one and then                                 considering two and so suppose it's                                 closest to cluster I in clustering one                                 and a cluster j and clustering to then                                 you're going to go to the to the to sell                                 IJ in this matrix and increment that by                                 one by that point and then you simply do                                 that for every single point that you                                 have and you compute the total cost                                 you're going to get a matrix that looks                                 something like this so what this matrix                                 tells you is that the cloth strings are                                 fairly similar right because so ball and                                 streaming k means that that new approach                                 is is the columns and my food key means                                 what we had initially is the rose so if                                 you add up the Rose you're gonna get the                                 the number of points in the mahout                                 clusters in the k-means clusters and if                                 you add up the columns you're going to                                 get the ball streaming k-means clusters                                 so you can see that cluster to for                                 example in mahout has                                                 and that's exactly the same as cluster                                   in ball streaming k-means right and then                                 you can see that oh if you look at                                 clustering cluster                                                      that has                                                                he means cluster                                                        can see that they might be / muted but                                 they're the same right you have the                                 exact number of clusters so this is why                                 the adjusted R and index is one in this                                 case the adjusted rounding index goes                                 from                                      one and you can think of this as that                                 sort of like the percentage in which                                 these things are similar that's just an                                 intuitive idea of what these things do                                 and so in this case you got the exact                                 same clustering hooray right it worked                                 just fine using just one pass now I ran                                 I actually did                                                        how it works and it sometimes doesn't                                 work that well from Burma hoots k-means                                 for just normal k-means and the problem                                 is in this in this in this confusion                                 matrix you're seeing the effect of a                                 poorly chosen seed so look at the first                                 cluster a mahout k means that basically                                 has the second and third ball in                                 streaming k-means clustering together so                                 with                                                                   class that's probably because of the                                 seat of a see that was picked you know                                 by the random sampling you can't really                                 do anything about this other than run                                 the algorithm again and again until you                                 get this now you're not going to always                                 get you know the perfect clustering                                 exactly the same class ring but here you                                 go the so this would not ever happen                                 from for ball streaming k-means just                                 because you're doing the smarter                                 sampling so there's that at least we're                                 not proud of that problem right okay                                 then I have a box plot this is for the                                 seeds data set and so you could this                                 this basically plots the distribution of                                 distances within each cluster right and                                 it sort of like plots that across all of                                 the runs all of the five runs so the                                 black bars are like the median of that                                 distribution so you be as km is the ball                                 streaming k-means thing and km is the                                 k-means thank right you can see that                                 they're fairly similar and also the                                 means are pretty much the same four                                 point three verses four points too so                                 the average distance is pretty much the                                 same and even the distributions look                                 fairly similar now this is probably                                 pretty hard to visualize so there's this                                 other plot for some other data set                                 called movement this will act this                                 actually shows you for all five runs                                 what the distance is in each cluster                                 look like well the average distance is                                 anyway so you can see that they tend to                                 look fairly similar right like k-means                                 and paul streaming k-means now some of                                 okay the empty circles are both                                 screaming gaming's of the full ones or                                 k-means so                                 you can see that they're pretty much you                                 know about so instant around the mean                                 but then there's also some outliers                                 saying the second run in the red wonders                                 of a red k-means da like way up there                                 you know this just happens like it                                 doesn't happen all the time and the                                 first two runs I think should look                                 pretty similar and in this case actually                                 ball streaming k-means is actually a bit                                 worse it it has mean distance                                            than                                                              confusion matrix but this is                                             that's not as easily cluster abell so                                 the thing is you you have your data and                                 you want to cluster it right but you're                                 probably not going to get like it                                 doesn't really make sense to cross                                 streets sometimes or it doesn't make                                 sense to cluster with as many clusters                                 as you want it to so sometimes you're                                 going to get totally random                                 distributions of clusters although you                                 can sort of see that no-good k-means                                 tends to split up the split of the                                 clusters to some extent likes a cluster                                 three and ball streaming k-means is with                                 across clusters                                                        for in this example the adjusted R and                                 index that measure of similarity between                                 the two clusterings is only point seven                                 so not as good so basically what I'm                                 saying here is that if if the data                                 itself is in class isn't as easily                                 culturable like there are there aren't                                 exactly you know how many cultures you                                 wanted say the                                                           there then that basically means that the                                 two algorithms will sort of diverge in                                 what clusterings will get you but that's                                 just I mean if they're very if they are                                 if you actually have like this three                                 clusters that we had here they do agree                                 but if the data isn't cluster abell not                                 so much okay now here's another all pod                                 well I gave these really crappy names                                 this is another plot for the power data                                 set this is this was somewhat larger                                 this one is neat because first off you                                 can see that the median for k means is a                                 lot smaller but then you have those dots                                 at the top which are the outliers so you                                 have like some of some point some                                 clusters have really large distances                                 also the y-axis is log scaled so those                                 are very large and they and these just                                 look very similar very                                 similar so you might think that k-means                                 is actually doing a better job here but                                 you know yeah we'll see and here's                                 basically another one that's for the                                 exact same distribution is for the same                                 data set as this one as power and you                                 can see the outliers on this one as well                                 like the blue and green ones the one we                                 run green runs especially right and the                                 means are pretty much the same here ok                                 so I showed you all those pretty                                 pictures but to summarize in a table the                                 blue result the bold results are better                                 so if the dunny legs is better that's                                 bold if the davis building is this bowl                                 is both that's better so you can see                                 basically that for the two first ones                                 the iris and seeds the clusterings are                                 fairly similar like you have                                            or and point                                                           there they do agree felt to a large                                 extent on the other hand though it's                                 funny because the done index for iris is                                 nine point something but for ball                                 streaming k-means just six points                                 something this actually goes to show you                                 that these indices tend to be deceiving                                 sometimes it's that large because in                                 this case where it actually gave you a                                 really crappy clustering the done index                                 of this is actually really large so you                                 know all of these indices should                                 probably be used with some skepticism I                                 guess you should probably just use more                                 metrics to see if the clustering is                                 really good or not so yeah that's why                                 you have a nine-point something there                                 but for seeds you get fairly similar                                 results and you can see that ball                                 streaming cammies is slightly less good                                 right like these quality indices are                                 slightly worse except for iris again the                                 average cost is less because of that                                 mistake that k-means made because of the                                 faulty initialization but interestingly                                 enough once you go the larger data sets                                 particularly controlling power you can                                 actually see that something really weird                                 is happening at least in power like both                                 screaming k-means is a lot better than K                                 means i am not sure why this is really                                 it should be fairly similar but i just                                 guessed that um well it came he's                                 probably got unlucky with some                                 initializations and that was just it so                                 and you can see that for these last few                                 days that's the closest rings don't                                 really agree like you just have a point                                                                   average round index so yeah not as                                 clear-cut but the clustering should be                                 okay now then that's the quality I think                                 I hope at least that I sort of convinced                                 you that the quality of this approach is                                 the same or maybe sometimes better but                                 basically the same now on to the main                                 reason of implementing this namely speed                                 so what does this be like so first off                                 here's the speed for the threaded                                 version of streaming k-means this plots                                 the time to cluster one point versus the                                 number of threads and you can see that                                 the speedup is nearly linear and so                                 that's basically as good as it gets                                 right it scales linearly across as as                                 you get more threads here well and of                                 course is zooming out the chords okay so                                 this is a threaded example but still not                                 really large enough we talked about                                 Matthews that's what we wanted to do so                                 for MapReduce I use a larger data set                                 which I generated myself which is                                 probably you know prob not the best idea                                 I guess it since it was fairly small                                 anyway so what I did was I got say                                     million on that were of magnitude points                                 that have                                                               from the corners of a hypercube it was                                 just a multi normal distribution in each                                 of the vertices and I simply got                                       them that were the actual cluster that I                                 was supposed to be getting and I just                                 sampled from those basically and yeah                                 you have like                                                          is like seven gigabytes of data on a                                 four node cluster it was initially a                                 five node cluster provided by map are                                 but I've messed up and killed one node                                 and I don't know how to start it up                                 again so for node cluster and this is                                 what the performance looks like so this                                 is a case where you are using you want                                 to have                                                               k-means and you can see that it needs                                 two iterations to converge right three                                 map produces and each of them takes                                    minutes raz few min k-means takes                                       it's like a bit longer than that so the                                 reason here is that                                                   really that many clusters so you won't                                 really get any advantages from the                                 nearest neighbor search that you're                                 doing in                                 streaming k-means and also there's the                                 issue of well you're also doing more                                 things in a bulky industry Mickey means                                 job you're doing the streaming he means                                 and you're doing the bulky means at the                                 end and actually we're not really just                                 doing one bulky means we're actually                                 doing multiple initializations there and                                 we're trying to get the one that has the                                 best cost so it's actually doing more                                 things there but still it's basically                                 the same as one iteration of my foots                                 k-means so this is pretty good now                                    clusters again isn't that much so what                                 if we go to see a thousand clusters this                                 is this is where um k-means not not                                 doing any sort of fast search kind of                                 breaks down this is where boots key                                 means kind of doesn't do the right well                                 it does the right thing but there's very                                 inefficient so for this case with a                                 thousand clusters you can see that it                                 finishes in two iterations right which                                 is because you're asking for a thousand                                 clusters when you really only have                                    clusters so it will figure out what the                                 what the assignment should really be                                 fairly fairly quickly but streaming                                 k-means on the other hand it finishes                                 what mahout k-means did in four hours in                                                                                                       faster and also just one iteration is                                 all is a lot of past                                                 it's doing the approximate nearest                                 neighbor search so yeah there's that and                                 basically what the whole the conclusion                                 of all this is you can have essentially                                 the same quality of the clusters and                                 it's a lot faster like it should be at                                 least as fast as one it should be about                                 as fast as one iteration of mahout                                 k-means of the above the basic one but                                 if you have lots of clusters then you                                 know it's going to be a lot a lot better                                 so okay i hope i convinced you about                                 that and the code is in my food right                                 now it's going to be part of the point                                 eight release so you can start playing                                 with it these are just a few classes so                                 balky means and streaming gaming's are                                 like the main classes that do the                                 clustering then there's a bunch of                                 fasteners neighbor searchers like                                 projection search for example and then                                 there are some quality metrics that                                 calculate all these indices that I                                 talked about and of course the map you                                 do                                 is and there is a storm prototype on                                 github which you might use but then it's                                 sort of like I mean I'm not going to                                 guarantee that that works properly so                                 okay that was it if you have any                                 questions hope you do have questions                                 because I sort of skipped over lots of                                 slides well in the full one okay so                                 questions                                 you                                 you
YouTube URL: https://www.youtube.com/watch?v=RRBu9shemWk


