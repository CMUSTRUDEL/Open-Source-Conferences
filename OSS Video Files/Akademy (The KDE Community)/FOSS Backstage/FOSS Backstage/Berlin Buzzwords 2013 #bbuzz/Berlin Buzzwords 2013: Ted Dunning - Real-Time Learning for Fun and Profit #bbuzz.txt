Title: Berlin Buzzwords 2013: Ted Dunning - Real-Time Learning for Fun and Profit #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	I will describe how real-time Bayesian learning can optimize real processes. These processes can be web-sites, user interfaces, ad-targeting servers, back-end server farms, search engines or physical systems. Google, Microsoft and Yahoo have all adopted the algorithms described in this talk for ad targeting because they produce superior results, but you don't have to be on that scale to benefit as well.

In this talk, the audience will learn the basic ideas behind effective real-time learning, but also see detailed implementation techniques and learn how to architect effective testing systems. I will also cover methods for starting gently without massive system upheavals and how to build consensus around how real-time learning and optimization. The focus throughout the talk will be on practical methods that can be applied in real life.

Read more:
https://2013.berlinbuzzwords.de/sessions/real-time-learning-fun-and-profit

About Ted Dunning:
https://2013.berlinbuzzwords.de/users/teddunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              ok                               I'd like to talk                               about a little bit of work that we've                               been doing on real time learning there's                               a lot of people interested in this                               lately let's get the yes right slides so                               these slides are already up on                               SlideShare they'll be up on the Berlin                               buzzword site very shortly i'm sure but                                what I want to talk about is how can we                                do real time and long time together dan                                talked a little bit about how these                                recent streaming k-means could be used                                in real time but that doesn't                                necessarily help us if we want to                                process also a very long archive or                                multiple scales of time where we need to                                reset these there are other things that                                we may like to do where we decide our                                analysis has changed so we need to catch                                up the real time against long time while                                we're still processing in real time and                                so we have this problem we have two very                                nice tools at least two tools we have                                Hadoop which is very nice for long time                                batch computations and we have tools                                like storm or s                                                         real time but neither is good for what                                the other one is good at Hadoop is just                                by design not good at real time and                                storm is by design not intended to do                                batch large-scale computation and so it                                sounds like this is a terrible thing but                                as in all ways with computer science                                it's not actually a problem it's an                                opportunity we have the chance to become                                famous and here's the the picture of the                                problem we have a timeline it goes from                                very long ago to very much now and we                                have the most recent full period of                                analysis might be one hour might be a                                day that's the latest full period and                                then it takes some time after the end of                                that for Hadoop to get its job done and                                that means Hadoop can process if right                                there is now                                Hadoop can process all of this green                                region but it leaves this red region                                unprocessed it cannot process the last                                full section until that full section has                                finished plus time for it to do its work                                and so we have that missing bit and what                                we would like to do is have storm handle                                that last piece and to exactly but these                                two analyses together it's not good if                                we overdo this because we'll have over                                counts that suddenly resolve we will                                have situations where the the number of                                unique people for the month in the                                middle of the month goes down if we                                overlap these things if we under lap                                them if there's a gap in there we'll                                have similar inexplicable things we                                might have a traffic pulse we see a lot                                of traffic for the month suddenly we say                                none and then we see it again as it                                crosses the gap neither of those are                                very good solutions especially if our                                boss is looking at the statistics and                                and bosses and CEOs have this unnerving                                tendency to wake up at three in the                                morning and go look at the statistics                                exactly when the gap appears I don't                                know how they do it but they do it and                                so what we want is a blended view                                between this real-time thing and long                                time and we want that blend to be                                perfect we don't want any gap we don't                                want any overlap so the trick is how can                                we do that one traditional way is people                                put some sort of search engine producing                                logs and then a consumer and then they                                blew the no sequel thing de jour                                Cassandra or HBase or something like                                that and they say see we have real-time                                processing and we throw the data into a                                big pile so we have long time but there                                there is no sense of accuracy in                                deciding what's in this and no sequel                                database and what's in the longtime                                database you can do long time queries                                and you can do real time but you cannot                                join them easily there is no provision                                for that boundary being well defined so                                we have these problems that's simply                                dumping things into a no sequel engine                                is just not sufficient                                and very often the insertion rate is                                bounded so that we can't deal with that                                 we don't have load isolation so if one                                 part gets too busy it can't do load                                 shedding and catch up later and the skin                                 performance is typically relatively low                                 as well we might very much prefer to                                 have flat files and it's very difficult                                 as i said to set boundaries now recently                                 there's been a lot of talk about                                 something called the lambda architecture                                 lambda architecture conceptualizes all                                 of history up to the current moment as a                                 single very large data element and we                                 think of every version of that history                                 starting to say you know yesterday                                 yesterday plus one hour yesterday plus                                 two hours each version of that history                                 as an immutable object that's usually                                 kind of true that that the past is                                 immutable especially if we wait a little                                 while but then there's some function of                                 the past everything up to now which is                                 defined and we want to compute that that                                 that's the general framework that you do                                 now that's infeasible to recompute on                                 the fly in one second some function of                                 all of history if you have to look at                                 all of history and so the lambda                                 architecture decomposes this computation                                 into the fundamental layer and a speed                                 layer which is kind of the very last                                 most cash bit but it provides no                                 provision for making this butt splice                                 this connection between real time and                                 long time be seamless it provides the                                 intuition that this is a good thing but                                 it doesn't really provide the crux of                                 the architecture so it needs more work                                 and if you talk to people at twitter or                                 something and you ask them about this                                 they say oh yeah you know the statistics                                 do strange things but it's good enough                                 for now it really depends on what you're                                 doing if you care about accurate numbers                                 that's not good enough it is good enough                                 as a first cut but not as good as we can                                 do so let's take in the simplest case                                 counting and let's let's work out                                 architecture which gets this exactly                                 right and counting is a very nice thing                                 I mean partly because people want us to                                 do it partly because for people like me                                 it's incredibly difficult to do                                 correctly as you saw with the books                                 recently counting to four I missed it                                 was difficult but but the nice thing                                 about counting is it exhibits properties                                 associativity and commutativity which                                 allow us to decompose counting into                                 pieces it's an online algorithm it's                                 it's very nice that way and so if we                                 build a good architecture for counting                                 that really does work correctly then we                                 should be able to generate lies it to                                 any associative online algorithm and                                 there are some very interesting ones the                                 streaming k-means is it is an example of                                 that so here's the rough idea we're                                 going to start on the left where the                                 data sources are we're going to throw                                 them into a queueing layer the queuing                                 wire merely provides us a little bit of                                 elasticity so that we can be doing                                 maintenance downstream without losing                                 data upstream and it will put it into a                                 proto spout meaning proto buffer spout a                                 storm construct a thing that reads data                                 we're going to log it because we always                                 log raw logs in in Big Data world we're                                 going to also pass it now to the counter                                 bolt the counter bolt is the beginning                                 of the boundaries between real and long                                 time it is the real time part of that                                 and it will keep logs internally it will                                 acknowledge tuples as they come through                                 and it will send semi aggregated values                                 for the data downstream into another                                 lager and here is where the boundary                                 between long time in real time is it's                                 at this snapshot so we will make a                                 snapshot of the data we're aggregating                                 it bit by bit as we go by keeping a log                                 and somebody will snapshot that and                                 inherently because of the way the                                 snapshot works the last moment in the                                 snapshot will be a parent                                 because the last semi aggregated point                                 in that snapshot is in it and the one                                 after that is not if we have a correct                                 snapshot and that defines our boundary                                 so let's focus in first on the counter                                 bold how can we make this work correctly                                 well let's focus in earlier on the                                 catcher protocol this is not a big deal                                 you can say hello you can you can log                                 messages the cluster of those is felt                                 tolerant because they will forward you                                 can cash the results and then redirect                                 the proto spout is simple it simply                                 reads from files which have been                                 appended by the catcher and now the                                 counter goal so the counter ball takes                                 incoming records and it holds on to them                                 as quickly as possible it puts it into a                                 replay log that log is persisted and is                                 appended to so it's very fast to write                                 to and as soon as it's persisted to the                                 log there it can acknowledge that tuple                                 upstream inside of the inside of the                                 spout that gives us a replaced semantics                                 which is safe anything that's in the                                 replay log the counter bolt has taken                                 responsibility for and will work even                                 over failures and restarts of the                                 counter bolt anything that has not made                                 it into their the storm architecture                                 must resend if we're to get it we can                                 also put hashes on those so that if                                 things go into the replay log multiply                                 we can handle that correctly the counter                                 bolt then we'll also periodically drop                                 out the semi aggregated results and at                                 that moment it will start a new log for                                 replay purposes and the invariant there                                 is that the semi aggregated results have                                 the aggregations up to a certain moment                                 in time and the replay log the live                                 replay log will have all transactions                                 beyond that time not aggregated in pure                                 form and so what that allows us to do is                                 restart by looking at the last send the                                 aggregate and then replaying from that                                 point and and then start handling things                                 and it also means that the replayed log                                 will always stay quite small                                 those semi aggregates are what gets                                 snapshot it and that snapshot divides                                 the long time from the current time and                                 all of the data in the snapshot is by                                 definition long time and the semi                                 aggregation strategy means that we can                                 take only the semi aggregates in the                                 long time we can do any sort of long                                 time analysis we want and then we can                                 reliably combined with new semi                                 aggregates that were not in the snapshot                                 and finally the replay log so in a very                                 short time we can get any arbitrary                                 associative and online algorithm                                 competed against all of time and it will                                 be a perfect butt splice so the                                 guarantees that we have are that the                                 counter output is relatively small and                                 that's because it's counting and the                                 aggregates are counts counts are one of                                 the best compression mechanisms there                                 the persistence must provide certain                                 guarantees it has to be a right                                 consistent medium now sadly that leaves                                 out HDFS but since I'm wearing a red hat                                 I'm allowed to say that it leaves in the                                 map our implementation and so that's                                 good enough for that layer we do have to                                 have some layer that provides good                                 persistence guarantees good right                                 ability guarantees and good flushing                                 guarantees the presentation layer now is                                 fairly simple it takes the last of the                                 long time the latest semi gagra gates                                 and it replays the log itself to get a                                 composite view that was the blended view                                 that we wanted at the beginning and so                                 that's the basic idea is that online                                 algorithms allow these aggregation type                                 behavior to be done they allow quick                                 replays against that last aggregation                                 and they allow aggregates at least if                                 it's an associative online algorithm                                 aggregates to be combined and that if we                                 could do this with counting we can do                                 this with any of these other algorithms                                 and there are a number of them so we can                                 do it with k-means clustering we can do                                 k-means clustering back to any                                 degree of time so that would allow us to                                 define that we wants a k-means                                 clustering of any time period say within                                 five minutes resolution from any time in                                 the past years ago up to the present                                 time or any point in between that's                                 difficult to do in a fraction of a                                 second with any other approach that we                                 can do that with other things we can do                                 count distinct the hyper log log                                 algorithm or the the the K men value                                 hashing algorithms give us an                                 associative count distinct operator                                 where we can store the state of that k                                 MV or the hyper log log and we can                                 combine those an associative way and we                                 can add in new transactions those are                                 the requirements for the replay so we                                 can do how many uniques were from this                                 five-minute thing three years ago to ten                                 minutes ago in a very very short amount                                 of time in basically log of the total                                 time if we have a pyramid scale of                                 aggregators and the time should be less                                 than a few hundred milliseconds which                                 again is stunning we can also do top K                                 values that's trivially associative as                                 long as we have some bound on K we can                                 do the top k of count star what are the                                 most frequent elements this is                                 approximate operation what we can do in                                 approximate what are the top k for any                                 time period over a long time or real                                 time or any blended thing this is kind                                 of cool that that we could do these                                 things which would normally take queries                                 that would take hours like that any one                                 of those can be done implemented and in                                 this way and we also know that as the                                 data ages and moves from real time into                                 long time will get exactly consistent                                 results relative to what we want and of                                 course in the part two of this what I'd                                 like to talk about is a very exciting                                 sort of real time learning called basing                                 bandits even that because at its heart                                 it has counting as its internal                                 operation that's the learning that goes                                 on in that algorithm even that can be                                 applied long time real time a little bit                                 of time young and naive models can                                 compete with old and wise models and we                                 can restart those accurately at any time                                 for any time in the past so any                                 questions on the first part of this talk                                 so it makes sense i think somebody in                                 the back I'll try to repeat your                                 question go what happens when that's                                 part I heard so he asks what happens                                 when we get new data well we start new                                 semi aggregates well we don't drop it we                                 store it into the the file system and                                 and so when we restart that and store                                 that one and we build a new semi                                 aggregate later we can recombine those                                 because of the associativity of the                                 online algorithm and get a combined                                 result or we can add some of the replay                                 log and get those things accurately set                                 up so it's a straightforward thing but                                 but those get reset to zero at every                                 micro time period and then we will                                 eventually probably using a MapReduce                                 mega Phi those into longer time periods                                 so he keeps saying the final one                                 overwrites know each one is stored in a                                 separate location so the semi aggregate                                 for                                                                  location that's my aggregate                                            is stored in a different location every                                 semi aggregate is stored in its own                                 place they're not overriding their                                 appending okay so let's move in so now                                 the next question is what can we do                                 something more interesting than counting                                 counting is kind of boring people can do                                 that people can do count unique things                                 like that but what about a/b testing can                                 we do that or what about contextual                                 learning which allows very advanced                                 targeting sort of operations can we do                                 that with this combined                                 real time and long time format now if                                 we're going to do learning we have to                                 start talking about probability and so                                 on now it may be that some of you can                                 predict what's happening before but                                 that's because you've had some training                                 so if we're going to talk about                                 probability the simplest thing to talk                                 about is coins okay this is a coin this                                 is a                                                                    for audience consensus here what is the                                 probability of it coming up heads                                     fifty percent sounds like the same                                 number okay that's good that's good now                                 that's partly because you don't know                                 that it has two heads or not that's                                 heads so that's good the probability of                                 heads is at least non-zeroes Ted's again                                 that's cool heads again now what's the                                 probability of heads heads again what's                                 the probability on the next flip point                                 five this these people are very                                 difficult to convince of anything heads                                 again five times no big deal heads again                                 seven times heads again tails tails okay                                 we just we're unlucky so so anyway                                 here's another question if I asked you                                 when it's before I flip it it's before i                                 impart momentum to it and so it's got                                 this uncertainty here but after i impart                                 it when it's don't throw it up so high                                 excuse me I'll be able to see in a                                 moment I'm very lucky to have caught                                 that but when it's in the air let's just                                 do this as a thought experiment what's                                 the probability of coming up heads isn't                                 that already determined because you know                                 it's it's flipping and my hand is in one                                 place and so on isn't it already                                 determined would the probability be                                 different                                                             after it's already landed in my hand it                                 is one way or it is the other what's the                                 probability of heads                                 he keep saying you know this hedging he                                 doesn't actually believe what he's                                 saying he says according to the current                                 model fifty percent what about somebody                                 have a strong opinion fifty percent                                 thank you I like that sincerity heads                                 but it was already determined but you                                 still said fifty percent now here's                                 another experiment I flipped it and I                                 look at it and I ask you what is it how                                 could that be I looked at it I know what                                 it is but what would I say would I say                                 the same number no no not at all okay                                 one just last time what's the                                 probability of heads be bold point five                                 what's probability of tails point five                                 what's the probability of something else                                 okay so now we have a philosophical                                 conclusion here without telling you this                                 i have just converted you all to                                 bayesian those of you who were not i                                 apologize your parents may be unhappy                                 with me but your auld now bae ziens                                 because you all seem to agree that                                 probability r expresses how much we know                                 if i knew and you didn't you still said                                 fifty percent once i showed you that                                 there was no coin you changed your                                 answer this is very good this is very                                 adaptive this is very human and                                 reasonable behavior it's only                                 unreasonable in the                                                    and so you are now all bayesian and                                 agree that probability is expressed as a                                 subjective sort of thing and more than                                 that I think that you begin to express                                 that you it could be over a whole range                                 initially if you didn't really know what                                 a coin was and I asked what the                                 probability of heads and tails was you                                 go I don't know and you would kind of                                 say anywhere between zero and one                                 this horizontal axis is a probability                                 and then the vertical axis is what's the                                 likelihood you would answer a certain                                 way and it could be anything if we did                                 five heads out of ten throws not ten                                 throws out of                                                         might say well I think it's kind of in                                 the middle if we did two heads out of                                    throws you might say oh I think that the                                 probability of tails is higher but I'm                                 still somewhat uncertain now if I asked                                 you for a specific number you would do                                 something like an average there's the                                 mean of that thing approximately that's                                 where that vertical line is that's the                                 single number that you would give me to                                 minimize your chance of error you know                                 there's a big thing to the left but a                                 long tail to the right and so you would                                 kind of fit something in the middle                                 there but that single number denies the                                 uncertainty it says this is a single                                 point but you agreed that it wasn't a                                 single point that there was uncertainty                                 in there and so even if we put error                                 bars on the sort of thing we still don't                                 really understand from the error bars                                 and that mean what that distribution                                 looks like the distribution is better                                 described by me asking you several times                                 what's a plausible value so you might                                 say oh point force plausible but you                                 would more often say something like                                 point                                                                                                                                            me plausible values one after another                                 you might sample from a random                                 distribution with the correct                                 distribution according to the evidence                                 you've seen and that would give me the                                 idea of the actual distribution that you                                 have in your head much better than a                                 single number would and that's the crux                                 of this algorithm we want to find the                                 best of any of the options were                                 presented with the best alternative web                                 design the best thing to give to you the                                 best top in ranked                                 comments to show on a blog the best ad                                 to show somebody I want to find the best                                 of any of these with uncertain to                                 information we don't have much data yet                                 and so we have something like that but                                 we know that the information that in                                 these distributions in our uncertainty                                 is distributional so this algorithm says                                 compute the distributions based on our                                 experience and then sample the                                 probabilities that we will get something                                 or sample the expected value don't                                 estimate it sample it and so if we go                                 back here sample it means give me some                                 value from                                                            point six with the centered court of                                 bulk of it in that middle part so sample                                 p                                                                       highest sample this is a way of                                 converting an uncertain thing that                                 encodes distributional information into                                 a deterministic algorithm so that i can                                 give you one ad to see one presentation                                 of comments on a blog one thing that you                                 can then give me a yes or no whether you                                 like it or not through your actions that                                 converts uncertainty into deterministic                                 testing in a principled way and in fact                                 against one of the highly accepted                                 things this is a bayesian bandit the                                 gray thing there the score vertically is                                 called regret and as you might guess up                                 is bad and this is a bayesian bandit                                 with very very weak assumptions it's                                 just saying you might get a reward with                                 something between you know                                             the                                                                      zero and one if we'd given it stronger                                 estimate knowledge of the world it would                                 have cut that error even more and                                 epsilon greedy there is one of the ones                                 that's commonly recommended for this                                 sort of problem and it's about four                                 times worse in terms of total                                 accumulated regret and and as I said you                                 can make that even better so here we                                 have and very simple how many lines four                                 lines of algorithm                                 that beats most of the best things I                                 have a video if anybody wants to see it                                 do you want to see it yeah good this                                 video will show one of these things                                 working and show it learning assuming                                 that the quicktime player will come up                                 there it is okay so this is very much                                 like what we saw before this horizontal                                 axis is probabilities from                                           converting of getting a positive result                                 and let's make this there we go and so                                 initially we have five different                                 alternatives from purple blue light                                 green yellow and red red is the best                                 it's the furthest to the right and after                                 just a little bit of data we have very                                 uncertain estimates except that purple                                 and green have never shown us any                                 success in a few trials and red has                                 shown us a few successes and a few                                 failures yellow a few more failures and                                 yeah blue is kind of in the middle here                                 so at this point if we sample from                                 yellow or red either one could be bigger                                 it's very unlikely however that purple                                 would be the best alternative and so                                 down here this represents how many of                                 the trials what the probability that                                 each trial will be offered red yellow                                 green purple and so on and so we see                                 that yellow is already got a fair bit of                                 bandwidth red is up there and purple has                                 very little this is the regret from that                                 earlier slide it's declining as we learn                                 and watch the system learn so at this                                 point it's learned that red and yellow                                 or up here and the others are all down                                 there and so it's giving very little                                 bandwidth to anything but blue                                 and red and yellow are duking it out red                                 has a slight advantage and the regret                                 has decreased substantially now we can                                 see yellow is making a quick run it past                                 red for a moment we see the statistical                                 noise in the sampling and you can see                                 that red is getting most of the                                 bandwidth which is good every so often                                 you'll see blue take a small kick                                 because it'll change it'll get an                                 opportunity and learn a little bit the                                 uncertainty on red is decreasing because                                 it's getting most of the bandwidth the                                 uncertainty on yellow is not decreasing                                 very much because it's not getting very                                 many trials and the idea here is that                                 the system is testing the possible                                 winners far more than the very likely                                 losers so it's optimized its cost of                                 learning against its uncertainty and at                                 this point it has very little                                 uncertainty left it's almost certainly                                 read something like a ninety-five                                 percent chance it thinks that red is the                                 best and a five percent chance that                                 yellow is the best in a negligible                                 chance the others are better this is                                 sort of algorithm we could throw a new                                 option in there at any time it would                                 have high uncertainty it would get                                 traffic for a little bit and it would                                 either be eliminated because its worst                                 or it would survive and it could we                                 could take alternatives away we could                                 have contextually appropriate things                                 sometimes we'd only allow blue and green                                 because people are only allowed to see                                 that so this sort of algorithm can learn                                 these things very effectively let's go                                 back to slides here's the our code that                                 generated those images except for the                                 part that generates the images I mean                                 the spark that learned and right up here                                 right here is the learning and very                                 nicely it's addition and because it's                                 addition this whole thing is associative                                 and online so we can do this exact same                                 form of learning in that same form of                                 modified lambda architecture which has                                 these wonderful properties of exactitude                                 and perfect transition between real-time                                 long time so we can do these sorts of                                 learning over any sorts of time periods                                 we like and the basic idea for this new                                 learning algorithm is that it can be you                                 can encode a distribution by sampling                                 and that that can give you Keith key                                 optimality results and also very                                 exciting is that you can extend this too                                 much more general response models where                                 I know about you the guy who's yawning                                 in the front and I know where he comes                                 from where his browser comes from and I                                 know other things about him historically                                 and that provides a context where I can                                 make a much more complex learning                                 algorithm and that complex learning                                 algorithm could be run partially in long                                 time parsley in real time so that i can                                 use this architecture to do that now we                                 may need a little bit more complicated                                 things we may need context about time of                                 day and source and content and all that                                 this all still works and so this                                 real-time architecture can be used to                                 train some some very complex systems                                 like an optimizer for content or                                 recommendation engine and it can do it                                 long time from one second out two years                                 time frame so there's the contact and                                 such late tonight is early today so                                 that's nice I usually have to say it's                                 tomorrow but in this case it's already                                 up                                 you
YouTube URL: https://www.youtube.com/watch?v=74eOmR22fuU


