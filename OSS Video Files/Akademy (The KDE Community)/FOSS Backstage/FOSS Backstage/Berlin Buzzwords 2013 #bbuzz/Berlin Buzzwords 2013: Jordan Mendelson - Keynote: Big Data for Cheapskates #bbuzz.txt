Title: Berlin Buzzwords 2013: Jordan Mendelson - Keynote: Big Data for Cheapskates #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	The general topic will be around utilizing open data and cloud computing resources so that everyone can benefit from modern big data methods.

Read more:
https://2013.berlinbuzzwords.de/sessions/keynote-big-data-cheapskates

About Jordan Mendelson:
https://2013.berlinbuzzwords.de/users/jordanmendelson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello hello Berlin buzzwords it's great                               to be here my name is Jordan Mendelsohn                               I work for a small open data nonprofit                               named common crawl whose skull it is to                               provide an archive of the web for anyone                               to download play with run analytics on                               or whatever you want to do and today I'm                               going to be talking about big data for                               cheapskates so what is a cheapskate the                                dictionary definition is a person who is                                stingy and mize early I admit when I                                name this presentation I did not                                recognize this this is the name this is                                kind of a negative thing I actually                                always thought of somebody as a                                cheapskate as someone who made cool                                stuff with very little money someone who                                would rather build something than by it                                and imagine from this group there's                                quite a few other people here who have                                possibly built things rather than buy it                                show of hands who here has has spent                                some time building something rather than                                buy something off the shelf good good                                welcome cheapskates of all kinds I feel                                not so lonely today so about me I have                                always been a cheapskate uh mostly out                                of necessity my first company I formed                                in                                                                  source writer that served the east coast                                the United States was very poor very                                poor I ran out of a closet in the back                                room of a movie rental store                                the whole thing was a converted desktop                                machine that ran slackware Linux hooked                                up to a bank of modems that we went out                                to the local electronics store and                                purchased and everything ran open-source                                software and this is at a time when not                                everything was entirely stable I believe                                we were running like Linux                                            point Oliver web server you know patchy                                was new like we were in n ntp servers                                crazy things like fax to email gateways                                but we built some cool stuff and by                                diverting that money that we would have                                spent buying you know giants park sun                                sun sparc machines and son Wes we were                                able to buy a second internet connection                                upgrade the speed and pretty soon we had                                a pretty good business my cheap skated                                pneus followed me I mean if that's a                                word uh to Napster where you know we                                were a group of five guys we did not                                have much money and what we had users                                not a lot of users then but we had users                                and they were growing very very rapidly                                so we did what we we knew how to do we                                bought and made some cheap linux servers                                we're an experimental copies of linux                                kernel that were constantly being                                patched and hacked up thankfully                                partially by alan cox who every single                                time i had cinema colonel oops would                                send me a new driver for my ethernet                                cards which are just constantly going                                out and we were spending a lot of our                                time just trying to get you know an                                extra thousand users on every machine                                and then in an extra ten thousand users                                an extra forty thousand users and even                                when we had money and you know we did in                                the end end up with a fair bit of money                                it all went to legal bills none of it                                went to none went to hardware certainly                                not any that i noticed personally so it                                wasn't until i join linkedin in the data                                analyst group that i had the luxury of a                                idle cluster of machines and and i don't                                know if you can understand how this                                feels but i had begged pleaded and                                borrowed every ounce of cpu power i                                could find i was using desktop machines                                inside the office to run jobs because i                                had nothing else you know nowhere else                                to go i we built a screen saver                                literally to go and while people were                                home going in and and crank out a little                                extra power but by the time I got to                                LinkedIn I realized something my cheap                                skated pneus it was was too far                                ingrained I was hopeless I didn't know                                what to do with all this extra machinery                                luckily in the area area of big data                                that we're now we're in the kinds of                                skills that cheap skates like me use on                                a regular basis are now back into demand                                you know so we're going to talk about a                                little bit with this with this talk the                                 overarching message here is that anybody                                 here anybody at all can do big data                                 analytics on a very very limited budget                                 so we're going to talk about a little                                 bit about getting data we're going to                                 talk about tools we're talk about                                 planning which is extremely important                                 when you're talking about big data on a                                 budget we're gonna talk about some                                 techniques and then we'll have some time                                 towards the end for questions and                                 answers so getting free data                                 I admit that a few years ago I thought                                 that big data was something that only                                 big organizations enterprise                                 organizations did you know when your                                 facebook or google or your yahoo and you                                 have tens of thousands of machines just                                 sitting around their purchasing you the                                 newest greatest things all together not                                 only are they giving you the hardware                                 but your systems generating terabytes                                 and terabytes of data on a regular basis                                 that you don't have to go looking for                                 free for data so I thought you know even                                 if i had the compute power what was I                                 going to use it for but in the era of                                 open data that we have today there's                                 been tremendous growth in the variety                                 and the size of data that you can                                 download and use publicly for free open                                 data movement has has really grown by                                 leaps and bounds my own company common                                 crawl provides for free a                                              archive of the web from a crawls from                                 the last five years you know that you                                 can go and do whatever kind of weird                                 analytics stuff that you would only be                                 able to do if you were somebody inside                                 of a search engine like Google you as an                                 individual can do but there's other                                 organizations there's the thousand                                 Genome Project thousand Genome Project                                 has put up a                                                         genomic sequences for over                                       individuals that you can go on directly                                 through their website or through                                 Amazon's public data sets and just use                                 you know if you know something about                                 genetics which I I don't but it goes it                                 goes beyond that the the US government                                 and the UK government's plus cities and                                 states like Berlin have opened up data                                 portals they include everything you can                                 think of if you want crimes                                                                                                        it for you if you want air pollution                                 measurements it's there you want water                                 pollution measurements it's there the                                 Berkeley earth project has aggregated                                 hundreds of data sources out there if                                 you're interested in climate change they                                 have                                                                    measurements made since the                                            tens of thousands of weather stations                                 around the world if you're interested in                                 image processing you can go to image net                                 they will hand you an archive but                                    million images to do whatever you want                                 with and I could go on and on and on but                                 the bottom line is we live in an era of                                 abundance and through this you can do                                 some really really cool stuff without                                 paying any money to get the data itself                                 so i was going to plaster the slide with                                 URLs and logos and you know free data                                 open data movements a little fractured                                 there there as I said governments have                                 portals cities have portals individual                                 projects have portals there's there's                                 only a couple catalogs out there and                                 they're not very complete but this one                                 Cora a link actually provides a better                                 starting point than anything I could put                                 up on the slide and don't worry about                                 writing it down this deck will be online                                 after the talk but it really does a                                 really great job so let's talk a little                                 bit about tools and I'm sure many of you                                 are aware there are some really great                                 analytic tools out there both free and                                 commercial unfortunately a lot of the                                 really good ones you know sigh pie and                                 and the like are really built for an arm                                 things like that are really built for                                 smaller data                                 that's when you start talking about                                 processing terabytes of data and they                                 kind of fall down in the commercial ones                                 and if you're a cheapskate the                                 commercial ones are just the probably                                 outside of your reach you know as much                                 as I love I mean I love splunk as a tool                                 i use splunk at a previous company right                                 now as an individual I don't have a                                 couple thousand dollars to plop down on                                 a log processing tool as great as it is                                 so what can you do as as an individual I                                 personally believe that Hadoop is the                                 best tool out there for big data batch                                 processing I mean it's free which is                                 always great there's a lot of support                                 tools that exist out there a lot of                                 layers you can put on possible but the                                 fact of the matter is is that Hadoop is                                 not easy this is not even as easy as a                                 database server which has a pretty high                                 bar of usage for some things                                 non-technical I have a friend of mine                                 who runs a analytic consulting company                                 who mostly deals with giant enterprise                                 organizations and he told me this story                                 about a fortune                                                          funny a CEO fortune                                                      I've been following big data he had gone                                 to I think he went to strata or                                 something it's sold by all the slick                                 people there bought a gigantic I do                                 cluster beautiful systems you know                                 everything you could ask for fast                                 networking all the RAM you could ever                                 use as as a street SSD drives all over                                 the place you know more cpus than I've                                 ever seen in my life to analyze all the                                 data that was coming out of his                                 brick-and-mortar stores                                 and he calls it my friend a couple weeks                                 later a couple months later excuse me uh                                 and says hey you know nobody ever told                                 me that it would take                                                  me a general report he came from a world                                 where somebody went in they open up                                 Excel you know they took their high-end                                 numbers and then they plotted some stuff                                 that was that was how the general                                 reports but now no matter you know even                                 with this fancy-smancy machine and                                 insulation it still took a lot of people                                 a lot of people to do some of the things                                 that they wanted to do and these                                 problems are magnified when you're                                 talking about running Hadoop in a                                 limited resource environment if you                                 don't have you know                                              machines with you know                                                   of state drives and                                                      everything it's it's harder but the                                 thing is Hadoop works it really does                                 over the past couple of months I have                                 processed petabytes of data through                                 Hadoop it has reasonable reliability and                                 I say and it's reasonable it is is                                 pretty resilient errors if your nodes go                                 down it will try its best to come back                                 up again it generally speaking doesn't                                 lose data and it really really shines                                 really shines been talking about                                 processing a lot of messy data mean if                                 you're talking about open data which is                                 often pretty messy I find that the dupe                                 works better than anything else I've                                 used in the past and thanks to the                                 contributors from the Apache project and                                 outside the Apache project it's just                                 getting better every day                                 and and I expect in a few years a lot of                                 the problems that exist today will be a                                 bad memory so let's talk a little bit                                 about this you have your data and you                                 are a cheapskate you don't have you know                                                                                                         your data on and if you did somebody                                 else is probably using them so what do                                 you do well for most of us that means                                 choosing a cloud service now up here                                 I've just listed a couple of the major                                 clouds next to them I have some prices                                 the prices are mostly there to help you                                 understand the range of types of                                 machines that exist for each of these                                 clouds but the fact of the matter is is                                 price per machine per hour is not the                                 only thing you look at when you're                                 choosing your cloud each cloud service                                 has its ups and downs some have much                                 much much faster networking in some                                 cases there's at least one service who                                 for Hadoop will cluster their machines                                 for your Hadoop cluster all on the same                                 switch right next to their blog our s                                 three types storage system to make it                                 much much much faster in the case of                                 Google Google is one of the unusual ones                                 Google will after ten minutes start                                 charging by the minute whereas everybody                                 else rounds the nearest hour and some                                 take seconds to bring up an instance                                 some take tens of minutes to bring up an                                 instance so in any case though the all                                 charged for outgoing bandwidth and they                                 rarely I don't think any of them charged                                 for incoming bandwidth so what that                                 means is you need to choose your cloud                                 very carefully because once you set the                                 data there it's going to cost you money                                 to get it back out again                                 so I'm just going to bring up a couple                                 tips on deploying Hadoop inside of these                                 client kinds of clouds this is a big one                                 if you are using Amazon and I cannot                                 stress this enough use spot instances I                                 mean I seriously came up here thinking                                 that I could say this one slide drop the                                 microphone and walk off stage it's that                                 important through spot instances you                                 will look to save about ninety percent                                 off of what you're on demand instances                                 cost which is which is pretty major with                                 though no real work if you are willing                                 to work weekends which I'm sure not many                                 of us are um if you're willing to work                                 they ken's or at least start your jobs                                 and weekends the spot pricing will                                 rarely fluctuate past fifteen percent of                                 the on-demand price and due to                                 inefficiencies in how Amazon's bidding                                 system works you can often get some very                                 very very big instances from them with                                 zero contention nobody else wants them                                 everybody wants like the mid-level m                                  extra-large instances and if you look at                                 the charts of price histories for their                                 cluster giant cluster compute instances                                 the price is completely flat so use spot                                 instances it is it is it'll make a huge                                 difference and once you once you are                                 using spondence if you're on amazon roll                                 your own Hadoop and I say this because                                 the way amazon has a service called                                 lastic MapReduce beautiful in terms of                                 ease of use it does everything you want                                 to do it brings up spa tenses as if you                                 want it brings up on demand and senses                                 if you want the thing is is it's priced                                 you will usually pay more money for EMR                                 then you will for your spot into this                                 because they're priced as a percentage                                 of the on-demand price to give you a                                 better idea of this this is this is some                                 pricing and this is for the biggest                                 cluster compute system that Amazon                                 provides and with spot instances with                                 with with you know your own hadoo you're                                 paying about two euros an hour for you                                 know                                                                   know                                                               storage if you were to pay if you were                                 to go and get the exact same thing with                                 on-demand instances you load it up with                                 EMR you're paying twenty five point                                 eight euros an hour which is obviously a                                 big savings the same thing is true with                                 the smaller instances this is a this is                                 the m                                                                 that you've actually end up with more                                 discs which is kind of funny previous                                 instance you get                                                         get about Giggy so there's some slight                                 differences there you'll pay you know                                 one euro                                                                and you can do a lot with that you know                                 that that that's a it's a pretty big                                 cluster so is the last thing I want to                                 talk about in terms of deploying Hadoop                                 in the cloud is deploy your Hadoop                                 instances only as necessary now this                                 might be really obvious to people here                                 you're all a bunch of smart people but                                 I've had a lot of conversations with                                 people who treat cloud computing as they                                 would servers they purchased and put in                                 their data center what I mean by that is                                 they start up their servers they get                                 some reserved instances they start up                                 their servers you configure the Hadoop                                 cluster and they just let it run                                 doesn't matter if it's used as a matter                                 of the type of innocence they chose is                                 wrong they just let it run until                                 something breaks and somebody goes in                                 and and and fixes it the thing is is you                                 can spin up                                                         Amazon in less than six minutes a                                 master-slave the whole kinga boodle so                                 if you treat your Hadoop deployments as                                 transient you reap a lot of benefits and                                 some of them all I'm going to talk a                                 little bit about later but they're                                 mostly relate to you know the cost                                 you're not paying well they're not                                 running and the flexibility to choose                                 the type of instance you want for the                                 job you're going to run so no talk about                                 a dupe can be complete without planning                                 planning is something when you don't                                 have a lot of money is something that's                                 paramount honestly it requires a lot                                 more restraint working with big data                                 than it does with hacking up a web app                                 for instance I am somebody who's a                                 little quick and dirty I like to                                 experiment on production machines I am                                 one of those people I like to throw                                 something up see if it works we optimize                                 it throw it up see if it works this is                                 not work in the world of big data where                                 every single time you run something it                                 costs you real money so I like to ask                                 myself a couple questions what am I                                 going to do with this data frankly when                                 I when I think of this question I list                                 out every single job I ever expect to do                                 with this data just just you know long                                 list of things so that I can do these                                 next couple steps                                 and then how often is this data updated                                 if you're talking about third-party data                                 it can be updated a bunch of different                                 ways they can give you just your new                                 data if you're talking about things like                                 the Department of Labor Statistics they                                 revise their old data so if you                                 processed it before you have to                                 reprocess it again so it's very                                 important to pay attention you know how                                 your dad is updated finally you know how                                 often does this job need to be run are                                 you in a financial services company                                 where you're doing day by day you know                                 moving averages or is it just something                                 you just run one off it's what i like to                                 do is like to build charts and your                                 chart might not look anything like this                                 this is mostly fit for this slide but it                                 you know I list all my jobs you know                                 statistically impossible phrases which                                 is fun to do page rank which is not so                                 fun to do list out you know the                                 frequencies and finally I list the                                 intermediate data that I need between                                 them all and I list the intermediate                                 data because a lot of my jobs tend to                                 use the same kinds of data and I will                                 treat usually intermediate data is                                 something to throw away you know it's                                 it's in my it's in my pipeline it's                                 something exists and then once it goes                                 into the next pipe I just toss the data                                 but if you are running jobs frequently                                 or a lot of jobs use the exact same                                 intermediate data you might choose to                                 keep it around and which will save you a                                 lot of time and energy in creating the                                 exact same intermediate data with                                 various types of code so let's talk a                                 little bit about techniques meditate                                 techniques i'm going to scribe or the                                 opposite of what some people might                                 suggest especially those with large                                 clusters of machines and there's a very                                 good reason for this my techniques are                                 not                                 optimized for speed they are optimized                                 for cost and you might think that speed                                 and costs are directly related but in a                                 cloud environment nothing could be                                 further from the truth the way clouds                                 price instances means that you can run a                                 lot longer on us instance with very very                                 little memory and get in and get charged                                 almost nothing and run the exact same                                 amount of time on a bigger instance with                                 more memory and get charged a lot more                                 so very important and finally though I                                 just want to say that when you're doing                                 these kinds of optimizations always be                                 cognizant of of what you're doing                                 basically I have found myself in the                                 position of optimizing for the last one                                 percent to get that last one percent of                                 my performance because you know I knew                                 how to do it it would just run that much                                 faster and I could save that much more                                 money and then three days later went out                                 okay I was very happy with myself and                                 then I worked out how much I saved and                                 it was less than the costs of the coffee                                 that I used to build the optimizations                                 themselves optimization is an exercising                                 exercise and diminishing returns so                                 always optimize you know to get your big                                 stuff done first and if you're really                                 bored work work towards this the small                                 stuff so before really going really into                                 specific techniques I thought this was                                 very important to put up your ultimate                                 goal with big data is to turn it as                                 quickly as possible into small data your                                 goal is not to generate more big data                                 your goal is not to have big data around                                 Big Daddy is difficult to process the                                 types of algorithms you can use on big                                 data are limited and frankly it's                                 impossible to digest internet itself so                                 you should not be afraid of shrinking                                 big data to something more usable small                                 data on the other hand is extremely                                 flexible you know and if you can get                                 small data small enough you don't even                                 need a Hadoop cluster process it you                                 know loaded infant and r and be happy                                 you no spit out all the charts and                                 graphs and everything analysis that you                                 want to do once you turn your your giant                                 data into something smaller personally I                                 like to shrink it to something like this                                 and this in this first line represents                                 my input it's like job zero and I like                                 to shrink my data by about eighty                                 percent or more on my first pass and you                                 can shrink it in a bunch of different                                 ways you get filtering segmenting                                 summarization you can even use                                 compression tossing data and just using                                 random samples I mean there's there's                                 lots of ways of going about it but a lot                                 of the techniques I'm going to talk                                 about are essentially based on this idea                                 getting the data small quickly so let's                                 talk about the first technique remember                                 I said to spin up your job instances as                                 necessary and especially for the job                                 type you're going to run well the reason                                 for that is that cloud providers the                                 pricing they use for instances is vastly                                 different between instances that are                                 optimized specifically for memory where                                 specifically for CPU or specifically for                                 networking or specifically for storage                                 then they are for general purpose                                 machines okay so what that means                                 is that you can take the exact the steps                                 of your job okay so you know you have                                 some specific job split it up into CPU                                 heavy pieces memories heavy spieces                                 network heavy pieces io heavy pieces and                                 then split them into individual jobs and                                 run each individual job on completely                                 different instance types that are sized                                 just right for the job you're going to                                 run also what you can do is take the                                 exact same job and segment your data                                 itself to run on different segments use                                 many different instances for that                                 segment of data say you have a piece of                                 data that it takes a long time to                                 process                                                                memory or a lot of CPU and the other                                 eighty percent requires very little time                                 because they're really small records or                                 everything's ones or something simple                                 let me just give you an example to help                                 clarify that as I said I work for                                 non-profit we build and maintain an open                                 archive of the web and we deal with a                                 large amount of data and I said we have                                                                                                   processing stages of building that                                     terabytes we exceed several petabytes of                                 data you know during all of our stages                                 so I just put up here a couple of the                                 jaws we run one is web crawling                                 obviously we have to get this data                                 somehow web crawling takes weeks you                                 know to run uses very little io uses                                 very little ram use a little CPU and use                                 a little networking reason why it                                 doesn't use a lot of networking is                                 because our crawler is usually VIP                                 they're very very polite and in order to                                 not flood web servers with requests we                                 back off pretty pretty heavily so                                 on the flipside parsing which is what we                                 do so that we can prepare extracts of                                 data like you know raw text that goes                                 along with this web page minus all HTML                                 tables or for pdfs whatnot and do                                 language identification run various                                 hashing algorithms and and whatnot uses                                 a lot of CPU and it uses a lot of memory                                 and it uses still a moderate amount of                                 i/o and a modern matter of networking so                                 we could have easily combined these two                                 jobs together to test together after all                                 when you are downloading a webpage the                                 web page is already memory you know if                                 we went and did and parsed up right                                 there that means there's no disk i/o                                 that we'd have to do there's no                                 networking I all we'd have to do we just                                 plop it in run it parse it and be done                                 the problem with this is the crawl takes                                 weeks whereas the par step takes days                                 and in order to do this kind of thing we                                 would have had to run much bigger                                 machines for weeks in order to do our                                 crawl to handle the excess memory the                                 parsing does uses whereas if we split                                 them up we can use very very small                                 machines and for instance on Amazon we                                 might run like m                                                      pay two point six cents per hour per                                 machine to run meaning we could do a                                 crawl of a couple billion pages for a                                 couple hundred dollars and then run our                                 persing step on much bigger machines for                                 a couple days we can also go past this                                 and split our person job up into two                                 pieces so it turns out parsing is really                                 really expensive if you're parsing                                 really big documents but most of the                                 documents and common crawl are really                                 small so if you're parsing things that                                 are over about                                               it uses sometimes in the order of                                    times more memory then you're parsing                                 things that are under                                                  what we can do is segment or data run                                 our our small parses on again small                                 machines and then run our big parses                                 which you know are ten to fifteen                                 percent of our of our data on big                                 machines and save ourselves overall                                 thirty to fifty percent and costs with                                 very little engineering effort and this                                 is you know widely accessible to I'm                                 sure many of the things people are doing                                 but let's just go into another another                                 interesting thing we talked a little bit                                 a little earlier about keeping some                                 intermediate data this is a difficult                                 one as I mentioned earlier storage                                 services charge for storage sorry sorry                                 cloud services charge for storage and                                 you have to think to yourself well you                                 know is it worth it for me to keep some                                 of this intermediate data around the                                 there are a bunch of different factors                                 involved you have to keep an account the                                 constant lowering of prices for cloud                                 services which happens literally every                                 six to nine months in the case if you're                                 in Europe you have to worry about the                                 Euro fluctuation you have to worry about                                 the time it takes to generate the data                                 versus the time it takes the cost to                                 taste and store the data but you know we                                 all generate the same kind of                                 intermediate data and sometimes they                                 speed up our processing quite a bit and                                 and that's and that's something you                                 really have to take in an account when                                 thinking about whether you're and keep                                 it around if you do keep it around                                 sometimes it's worth it or throw away                                 the original data so you know spend some                                 time looking at your intermediate data                                 deciding whether or not it's worth it to                                 keep it around                                 and if it is great and you'll probably                                 save yourself a lot of time and money                                 down the road the final technique I'm                                 going to talk about is paring down your                                 data which which I did mention earlier                                 which would you do you know by sampling                                 segmenting filtering and in summarizing                                 excuse me sometimes though that the                                 algorithms you use to filter down your                                 data are actually quite expensive you                                 know certain classifiers are pretty                                 expensive some clustering algorithms are                                 pretty expensive especially we are                                 talking about running it on                                            billion records so I like to use an                                 algorithm a technique of running really                                 really fast but dumb algorithms first                                 followed by really slow but smart                                 algorithm second and I could debate up                                 here you know the benefits of various                                 classifier algorithms or whether using a                                 you know be bit min hash is better than                                 a sim hash the fact of the matter is is                                 it sometimes you can get by with using                                 something that you would never ever use                                 on a real set of data to just shrink                                 your data set down to just the stuff                                 that's likely to be caught by your                                 second filter so let me just give you an                                 example from coma crawl so we do do dee                                 duplication of data so every single time                                 you run a crawl we D duplicate things so                                 we don't have three million copies of                                 some coming soon webpage which nobody                                 wants to see I for                                                                                                                 is about                                                          starting this this was all fetch                                 choosing Apache notch so you know that                                 we've been experimenting with there's a                                 bunch of algorithms you can use for                                 deduplication of near near neighbors                                 near close duplicates sketching some                                 hash locality sensitive passionate                                 variants like that they have their                                 pluses and benefits generally speaking                                 in order to get these things to go fast                                 you have to trade off something usually                                 its memory in other cases you end up                                 with a you end up with what's the word                                 I'm looking for data dad explosions                                 excuse me so for instance the sim hash                                 in order to smash really efficiently                                 there's a bunch of ways one of the ways                                 is to shingle the heck out of it and so                                 you end up with like like                                               fingerprints and where you had one                                 before and then so if you were talking                                 about doing it on                                                     we'd have                                                           hashes then you have                                                   we like to do our deduplication against                                 not just our new data but then against                                 our existing data as well so it just                                 blows up and blows up so instead what we                                 do is the world's dumbest algorithm that                                 works has huge false positive rates I                                 mean truly truly terrible false positive                                 rates but it is really really really                                 fast so what we do is we take the top                                 and words for a document we alphabetize                                 them and then we hash them that's it                                 it's it's fast it's easy it can be used                                 on the slowest machines amazon provides                                 outside of their micro instances and i'm                                 working on getting them to work on                                 micron's as this so i don't have to pay                                 for it at all                                 I then what that does is that it shrinks                                 are two billion hashes down to about                                     million it goes beyond that it shrinks                                 each bucket each key down to at maximum                                 about a hundred thousand on average                                 about six to eight thousand so all of a                                 sudden doing a sim hash is totally                                 doable and extremely fast and what that                                 kind of looks like is this we have                                 ourselves a giant input you know it's                                 what I right here it's about                                    terabytes of data that we turn into two                                 billion sim hashes we should do the same                                 hash calculation and the the really dumb                                 hash calculation at the same time                                 because it might as well it's just there                                 and the very end very very very end last                                 bucket we end up with                                                  olds that are duplicates that we can                                 then pull out of our index and we do all                                 this for you know a couple hundred                                 dollars you know and and this is this is                                 a lightweight stuff actually I forgot we                                 have                                                                   very important for those of us who are                                 dealing with a bad bad external data or                                 data can be corrupted data this giant                                 and probably has corruptions in it and                                 that is to avoid errors avoiding                                 specifically catastrophic errors but the                                 chances of your code breaking increases                                 proportionally with the size of the data                                 you're processing this is seems to be an                                 inevitable fact and this usually happens                                 this usually happens when processing the                                 last record of a                                                     know why it just happens like you or                                 when you walk away are you starting on                                 weekend                                 it happened mr. actually be starting on                                 a weekend it was it was it was the first                                 record and you just didn't notice it so                                 there's some obvious ways of mitigating                                 this problem I like to process the first                                 wreck last for her occurred first I'm                                 just kidding although I've considered it                                 seriously in the past though I cannot                                 stress how important it is though to                                 always test your jobs on sample data                                 first and as somebody who is kind of                                 fast and dirty and loose and doesn't                                 like to test things very frequently ah I                                 have been beaten into submission into                                 doing this after losing many many many                                 many days to jobs that fail for all                                 sorts of reasons and I am talking about                                 things like Miss compressed files this                                 compressed files that somehow blow up                                 the decompressor HTML pages that cause                                 my parsers to get into infinite loops                                 I'm talking about just just I honestly I                                 believe stray solar particles hitting my                                 memory on Mike machine at the wrong time                                 that's what I'm convinced of because I                                 haven't been able to trace it down it                                 seems the most likely the next thing you                                 can do though this is a little bar                                 regular is a skip over bad records and                                 skip over slow records there are going                                 to be edge conditions that you can't                                 predict okay so to minimize your chance                                 of a bad record taking down your long                                 job just skip over log it skip over it                                 be done with it and if you're using a                                 third-party library if you're using a                                 third party                                 library process your data I highly                                 recommend setting a limit a time limit                                 for the time that jaw that probably                                 records allowed to be processed and if                                 it hits just canceling it just                                 continuing on your way and there's a lot                                 of ways of doing this you can use Hadoop                                 built-in stuff this is all written                                 Python it's not accurate it's it's my                                 pseudo code so but uh you can use my I                                 do is built-in way of doing it it's a                                 very not it's really not efficient with                                 Hadoop works is essentially if your map                                 or reduce or crushes it will go in and                                 if you flip these blogs on it will then                                 skip the record you were processing so                                 that next time you will continue on your                                 way but what this does mean is that it                                 goes and reprocess all the other records                                 that you've already processed the                                 alternate way is to you something like                                 threads or processes to stick a proxy in                                 between your actual data processing a                                 process and your due process and then                                 just kill it you know if if it's going                                 wrong undo the changes log the whole                                 thing and be done of course you can do a                                 lot more elegant things than what I've                                 done here but frankly they're not that                                 hard to do they just require time and                                 effort to set up your templates so in                                 summation I'm going to just talk about                                 what we've talked about today plan your                                 jobs really very very important stuff                                 filter your data as quickly as quickly                                 as possible either through literally                                 filtering or through sampling or through                                 summation or cleaning oppression                                 whatever you need to do to the shrink it                                 as quickly as possible if you're using                                 Amazon instances you spot you know bro                                 you're on Hadoop start Hadoop is                                 necessary                                 and be flexible with instance types that                                 you run with your jobs keep some                                 intermediate data uh depending on                                 whether or not benefits you or not and                                 be cognizant about the type of internet                                 intermediate data you keep around use                                 algorithms wisely again sometimes the                                 best algorithms are not going to be the                                 cheapest so be the cognizant of how your                                 particular deployment affects your                                 algorithms and of course avoid errors                                 when possible and finally just to wrap                                 this whole thing up I would like to say                                 that anybody here anybody can do big                                 data and I fully expect that you guys                                 can all go home log on to the coral Inc                                 start up your Hadoop instances we have                                 some great tutorials at common called                                 org and you know MapReduce in like five                                 seconds with for less than twenty twenty                                 hero you know you could do whatever you                                 want and really really do some of the                                 stuff that only larger organizations are                                 doing today thank you                                 you                                 you
YouTube URL: https://www.youtube.com/watch?v=vWa9CUsNzdw


