Title: Berlin Buzzwords 2013: Friso van Vollenhoven - Dirty data: dealing with substantial volume (...)
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Full title: Dirty Data: Dealing With Substantial Volume External Sources

At a large European publisher, we had the challenge of receiving external data from 10+ different sources, adding up to tens of GBs per day. Add to this that the file formats will sometimes change without notifications and that sometimes connections go bad or files go missing. This is while trying to maintain that at least the amount of data is near correct in an environment where the 'correct' amount of data for a source is often a difficult to predict number somewhere between 20M and 50M records for a particular day.

We built a extracting and loading pipeline to get data into Hadoop en expose it via Hive tables, which includes scheduling, reporting, monitoring, transforming and, above all, the ability to respond to changes very quickly. After all, responding to a file format change within the same day or adding a new source in a day are very reasonable user requests (right). We were focused on developer friendliness and rely on a fully open source stack, using Hadoop, Hive, Jenkins, various scripting languages and more.

Read more:
https://2013.berlinbuzzwords.de/sessions/dirty-data-dealing-substantial-volume-external-sources

About Friso van Vollenhoven:
https://2013.berlinbuzzwords.de/users/friso

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning Thanks this talk is not                               about the elastic search I'm sorry for                               that so my name is Pizzo i'm on twitter                               and I've email I work for a company                               called go data-driven we provide                               services for people that want to do                               interesting things with their data that                               means that apart from building solutions                               we have to do a lot of kind of grunt                                work like the mundane things that need                                to happen when you do things with data                                and that that's basically what this this                                talk is about it's about the the mess                                you get when you use other people's data                                to do something interesting so I'd like                                to start out by we're giving you I see                                some some examples of what we're talking                                about so this is a simple plot of a                                number of unique visitors per day of the                                week you can see monday till Sunday on                                the horizontal axis and well weekdays                                are busier than weekends did this by the                                way is in in millions so you see we have                                reached at about                                                        and Friday of all the weekdays is not                                exactly the busiest day but there's                                these two interesting things there in                                the top you can see them right so those                                were really good friday so you start                                looking into that and then it turns out                                well Friday's in February are great for                                some reason it's the bar over here so                                you start thinking what happened there                                because what's so special about Friday's                                in February this is just the fridays and                                you can see this there's even a downward                                trend for this particular site but still                                that's februari so you start wondering                                about these things and you you wonder is                                this a data issue or were they actually                                exceptionally good days and then when                                you look beyond that you can see that                                February's                                actually doing quite okay all together                                and then you have to you know look                                outside and see that in that month some                                particular interesting events were                                happening in the European football                                league and that it actually makes sense                                that it's there but very often is the                                other way around you get twice the                                amount of data that he should be getting                                just because and you have no idea where                                it comes from or you get fifteen percent                                more than you should be getting and and                                that's you know if you know upfront some                                kind of hard count on which you should                                be expecting that that's even not so                                much an issue because you can just build                                in a simple check but what do you do if                                the right number of records that you                                should be receiving in the day is                                somewhere between                                                  depending on the weather and what are                                there were sports events and those kind                                of things that influence a news website                                for example so that's that's one one                                example so you can have to too much or                                too little and it'll come in does                                anybody know this particular XKCD comic                                yeah so you would hope that this by now                                would be kind of general knowledge right                                we have an easel standard and and then                                I'm not even complaining about the lack                                of time zone information right because                                we know everybody keeps their servers in                                Zulu time right so a lot of people think                                this is perfectly fine for a date format                                and you get this a lot and and the the                                kinds of things that we ingest this is                                not only large sets but there's also                                people that just deliver us like small                                pieces of stem data and it comes from                                excel files somewhere and it comes from                                I don't know what somewhere else and you                                get this stuff a lot of it so it's also                                there you have to do something and then                                the other day the guy that does the                                Excel trick to make these things is on                                holiday and somebody else takes over                                right and they have their excel in a                                different locale so the thing will break                                at some point hmm a lot of people push                                data through s                                                      fails of course it does but also a lot                                of people push a lot of really small                                files to s                                                            web service call for getting us getting                                a file actually fill about one in every                                                                                                      take about three thousand files per day                                 then                                                                  fail so that's that's something that you                                 don't really want to nurture yourself                                 right you don't want to be pressing the                                 button every every time that happens                                 because it will happen anyway you have                                 to be some I prepared for that I'll just                                 this is another one this is funny what                                 is the difference between a zero by gzip                                 file and a                                                          header so the zero byte one obviously                                 breaks the codec right and the                                         one is actually the empty file but                                 there's a lot of people that generate                                   by gzip files who knows what this                                 character is Ralph does so so this is                                 the Thorin character or the Icelandic                                 Thorin character and it is encoded as a                                                                                                        notably less than one and a Windows                                 encoding which is exchangeable what                                 windows uses and at some point somebody                                 sends you with TSV file right so you                                 think that's a TV file um it is okay so                                 so okay it's a separator right and                                 people choose to use                                                  it's kind of ok still although we would                                 like to have our separate or somewhere                                 you know nearer to                                                     separate the characters are nicer for                                 sorting order and those kind of things                                 but anyway it's a separator you can                                 split on it it'll probably work yeah you                                 know that this is assigned bite in hive                                 but you need to use for that                                 so um so so so these are these are some                                 some interesting examples of the kind of                                 stuff that you need to ingest if you                                 work with these external parties to                                 delivery data and you have to somehow                                 work with this all right and and this is                                 of course not an entirely new problem a                                 lot of people have this problem and                                 everybody's trying to solve that in                                 their own particular way and if you're                                 in a large organization you even have                                 these kind of problems internally so                                 with your own data and because first for                                 many people this is an internal problem                                 and especially for for you know                                 corporate organizations this is an                                 internal problem so what do they do they                                 create a business out of it hmm and they                                 invent mechanisms for it in the infant                                 you know activities that you should be                                 applying and then everything becomes                                 right right and well some call it                                 differently and then actually there's a                                 role if you can have she can be a data                                 custodian is anybody that for me maybe                                 data stewards yeah so what the idea is                                 here is that if you apply all these                                 principles you follow the rules then                                 you're your data is going to be in great                                 shape every throughout the whole                                 pipeline and everything is going to be                                 nice and all your use cases will work                                 right but the issue that I at least have                                 with that is that the people that you                                 know do all this they don't actually                                 know about the use case and so what they                                 were going to do is giggle upstream and                                 say you need to deliver the data and                                 this in this format and this in this                                 shape and this in this quality because                                 otherwise I'm not going to accept it and                                 then all the way down the line there may                                 be data analyst who says yeah but I just                                 want to get a rough estimate of this and                                 I don't care if half of it is wrong I                                 just know I want to know whether the                                 phenomenon exists or whether this                                 happens ever at all or something like                                 that so by making it basically an                                 organizational problem you kind of lock                                 yourself out of a lot of things that you                                 could potentially be doing if you just                                 get everything there is but then again                                 if you get everything there is then it                                 does become your                                 problem to solve it and in the end it                                 turns out that is not really that hard                                 so if you show this chart this simple                                 plot to a data analyst somebody who                                 works with the actual data every day                                 they do their hive queries and they do                                 there are magic and then what not and                                 you just shut on this plot for the                                 import over time then they will                                 immediately spot those two and they will                                 immediately be able to say okay this is                                 wrong or this is wrong for me right now                                 so in essence this is all the bad things                                 that will happen the format will change                                 we've had a lot of sources that had a                                 little worse esv like and it would just                                 add two columns at some point in time                                 and of course they would add them in the                                 middle so you have to be careful with                                 these things but then again if you're                                 going to accept all data from anybody                                 and you make it your problem you will                                 have to be able above all to respond                                 really quickly to those kinds of changes                                 right so the import breaks I changed                                 some code I deploy it I run it again and                                 I have new data and it works this time                                 hmm so um doing these kinds of plots for                                 the actual and lists for the business                                 users combined with a number of tricks                                 that we use in order to do etl and                                 scheduling those kind of things                                 basically goddess quite a quite well on                                 our way there so so what I what I'm                                 going to present this is not so much                                 something that we did on like a single                                 project but it's it's more over a                                 general way that we attack this problem                                 and for some customers we have slight                                 deviations from it but in the general                                 sense this is kind of how we attack it                                 there's there's no rocket science in                                 here this is not a very sexy subject                                 it's you know making sure that your data                                 gets into your store where you want to                                 use it but at least we did it in what we                                 think is a developer friendly way and we                                 use we use open source tools all the way                                 so what we want is we want to be able to                                 simple                                 had to have simple deployments of ETL                                 code meaning that the import breaks I                                 fix it takes me                                                        should be ringing immediately without                                 hassle okay scheduling obviously is                                 important that needed to be scalable I'm                                 going to have a lot of jobs where we're                                 talking                                                                 different outside parties and I want                                 those to be as independent as possible                                 because dependencies between jobs is                                 nice if everything just happens and you                                 fire off the one and all the others will                                 just work but then the first one                                 breaking you have nothing at all but                                 maybe all the other ones can do their                                 job partially so if that's all                                 independent and if some of these things                                 can do their job partially or break                                 halfway or something then we could end                                 up in some kind of you know corrupt or                                 inconsistent state in the data store                                 itself the thing that the analysts use                                 so any debt to be fixable in hindsight                                 so instead of you know upfront creating                                 a whole process that is going to make                                 sure that everything just works I'm                                 going to make sure that afterwards if we                                 notice that something went wrong and I'm                                 going to make very sure that people will                                 notice then I should be able to                                 basically fix the problem in hindsight                                 redo the thing incremental where                                 possible some people send us Excel files                                 but other people send us                                                 a day if it breaks halfway because of                                 one EZ as                                                            know redownload the other five gigs I                                 just want to have it there amazon being                                 as fast as they are anyways and metrics                                 is important as you need to have these                                 things because otherwise nobody will                                 notice when stuff is wrong so what we're                                 essentially talking about this is etl                                 getting data changing it into something                                 that's usable for us like the funky date                                 formats into something that's a real                                 date format and and then you load it                                 into whatever data story you use we use                                 hive                                 and from there on business people can                                 use it okay so who's familiar with ETL                                 tools like these ones so what these                                 things do is they give you this                                     environment where you can kind of                                 connect the different steps in a process                                 and you basically just connect the dots                                 and then it will compile down to                                 something that runs in some run time and                                 it will execute the thing right it makes                                 a lot of sense when you think about it                                 because it's nice you get a few of                                 whatever your your ETL workflow should                                 be doing it should be able to do                                 notifications when something breaks and                                 it be able ideally to handle whatever                                 you come across by putting these little                                 magic squares in your flow and say                                 change this to that or something change                                 the funny date format to real one so it                                 turns out that at some point these                                 things get a bit hairy and and difficult                                 to work with and because what happens is                                 your ear you're limited in which you can                                 do with these things right many have                                 some kind of scripts interface that will                                 allow you to maybe do javascript against                                 whatever data runs through or some other                                 scripting language that they have                                 embedded but then you need to somehow                                 you know connect all the things that                                 first it extracts the date format from                                 the file name because that happens a lot                                 that the files don't actually have any                                 date information but it's in the file                                 name and even before that you need to                                 check on HDFS what is already there and                                 then unless three what is already there                                 and you do the diff and you only                                 download the ones that are new and then                                 at some point there's a provider that                                 you kindly ask about what are the dot                                 staging dulces add files in there                                 because we've been importing those and                                 they say oh no you shouldn't do that                                 that's our testing data and loving you                                 have to obviously modify this thing to                                 you know do the inverse graph for the                                 dot staging somewhere                                 so this becomes mess but luckily their                                 Savior because all of these tools at                                 some point have this little magic square                                 all right and then we're back then we                                 can do anything because this thing turns                                 out to be this thing and this is it Brad                                 this is                                                                  the problem there was that well we have                                 the shell square we write the scripts                                 the script goes into the tool somewhere                                 and then well you have a version and you                                 push it to whatever machine is actually                                 running those things and somebody else                                 will need to at some point make an                                 alteration and you need to version                                 control would be nice it's basically the                                 issue there and and many of these tools                                 have version control ish but but not a                                 real kind so so so there there's some                                 work to be done and also maybe bash is                                 not the nicest thing here because as                                 powerful as bash is it turns out not to                                 give you the most readable type of of                                 code especially when you do these kind                                 of things right and and grab a knock I                                 know are the best tools ever but maybe                                 encode it in some other code it looks                                 nicer but on the upside for t                                           bash is way better than this already so                                 we're improving here so this is our our                                 best attempt and there's one more                                 problem here so what I'm going to do                                 here for each file is actually put it to                                 Hadoop and many of these files will be                                 small and there's the Hadoop move and                                 sometimes you need to create a directory                                 so these are three Hadoop comment line                                 usages and that's fine if you do three                                 of them but if you do three of them x                                                                                                         be                                                                                                        you process so your import just got one                                 hour longer just because of firing up a                                 JVM so this is the new version of the                                 same thing anyone gets the language it's                                 almost by them so what do we do here is                                 typically the same thing but you can see                                 we have created some library code here                                 like I want to have a HDFS file list or                                 I want to have an FTP file list and I                                 will just grab me the thing I need and                                 if I need to do other things I can                                 expand the library but the core of the                                 etl will still be something like this                                 like a very concise script and if you                                 look closely then we instantiate some                                 classes here and we we talked to some                                 kind of HDFS API like HFS path and then                                 a constructor parameter and and some                                 other things so what is this is actually                                 actually Jason jonathan is nice for this                                 because it talks to Java so it took to                                 Hadoop natively you don't have the JVM                                 start up overhead it has Python syntax                                 so you don't have to implement the                                 filter interface with an anonymous                                 implementation to you know filter a list                                 of files but you can just use well                                 almost nice Python the item is up to the                                                                                                      don't get list comprehension and                                 dictionary comprehension speak you at                                 least have loved us and some functional                                 constructs there so so we we went for                                 Jason we can use the Python standard                                 library and also any java library that                                 we can find for for our purposes hadoop                                 obviously being one of them but also                                 there's a couple nice ftp libraries in                                 the in java and some other things that                                 you may want to do their performance is                                 fit for what we are doing right the                                 actual heavy lifting is just moving                                 it's around and that happens even in in                                 Java and the the Hadoop API and the rest                                 of it is well we compare lists of file                                 names of a couple of thousands so                                 there's nothing too exciting there so                                 this is nice now let's do it every day                                 or every hour or every whatsoever so                                 obviously once you're working with code                                 the first instinct is to just crawl it                                 right code goes in version control I                                 check it out in some kind of directory                                 Ike run it and it will run every now and                                 then so who's ever had the pleasure of                                 managing a really big con tab so yeah so                                 that's that that wasn't too pretty                                 especially if you want to also at some                                 point you know kind of trigger these                                 things manually every now and then                                 because it fails or whatever happens and                                 you get these descriptive chrome emails                                 they're also not the greatest to read so                                 um enter Jenkins next piece of the                                 puzzle Jenks is a build server right                                 it's continuous integration thing but it                                 turns out it's a really great scheduler                                 as well and also the very nice property                                 is that it saves all of your output                                 whatever comes out of the job you can                                 get that to                                                              you want to be using this it will give                                 you sensible emails on error and it will                                 actually give that really long output of                                 your job as an attachment to the email                                 which is really nice so you can open it                                 in something well that's more                                 appropriate for watching stack traces                                 than your email client Jenkins can scale                                 the multiple notes you can have worker                                 nodes in Jenkins if your ETL becomes                                 heavy you can add Jenkins workers and                                 make the work happen over there this is                                 very nice it will give you a status                                 monitor and it integrates with first                                 control obviously so we can put up this                                 wall and we did and then everybody who                                 would work with data can just when they                                 enter the office already see if                                 something may possibly be wrong with                                 their particular data so that's that                                 that's very nice                                 they come in they see that and if they                                 see a rat thing they come to your desk                                 and say hey what happened and you can                                 warn them about well this and this                                 happened what are you gonna do if it's                                 that you're fine if you want everything                                 to be up to date to the last hour it's                                 not there I'm sorry because this is what                                 happens right the the data feed is                                 supposed to be delivered at eleven to s                                  and then sometimes it's not there                                 sometimes when you check out                                         still you know getting there and then                                 what do you do I mean you can wait but                                 for how long but also you can just start                                 working and grab the rest of it the next                                 day so how do you deploy that's easy                                 because Jenkins will just pick it up and                                 and and this is by far the most                                 interesting thing right something                                 changes in the outside world you have to                                 respond to it and instead of starting to                                 click like an idiot in one of those                                 things that has                                                   whatever is going everywhere I change a                                 couple of lines of code maybe and I do                                 we get push and then it's there again                                 jenks will run tests and if it works                                 it'll pass through so one of the other                                 points that we talked about having these                                 jobs independent and once more or less                                 there's no real magic here it's just we                                 want to make sure that if I do one thing                                 today and I do the other thing that                                 doesn't matter necessarily if they just                                 so happen to go out of order and maybe I                                 schedule one thing every hour because I                                 like to have the incremental downloads                                 but I want to do the processing maybe                                 twice a day for some data source because                                 nobody's interested in the individual                                 hours and does kind of constraints and                                 if you create threes of dependent things                                 and then um they always have to happen                                 in that order and with those                                 dependencies in mind so what we try to                                 do is make everything independent of                                 each other and that whenever you were on                                 the job it'll do work if there is any                                 we'll just you know quietly quit without                                 error if there is none so by far the                                 easiest thing to do here and which we                                 obviously did and this is not a new                                 trick we put stuff in HDFS and we use                                 move to make it go from staging folders                                 to another staging place to eventually                                 where the raw source data ends up moves                                 our atomic or atomic enough in HDFS                                 meaning that if you do a file listing in                                 a directory that everything you see will                                 be there so that means that if we upload                                 we upload files to HDFS with a like a                                 upload extension and then when the                                 upload is completely done we move it                                 into place to his real extension and so                                 everything that you know watches that                                 space in HDFS needs to ignore the things                                 that are made flight that are in                                 progress of uploading then there's                                 there's transformation jobs right many                                 of them are Roma produce because we do                                 non-trivial parsing against some of                                 these formats what happens there she                                 basically do the same trick again you                                 grab everything that's in the staging                                 area move it out of there to some other                                 place do the processing the result of                                 that you move into another place and the                                 other place is typically where we make                                 things go into hive so we don't write                                 five tables directly from the                                 transformation jobs but rewrite whatever                                 comes out of it usually even just text                                 based separated formats and then we                                 actually use hive to map that as an                                 external table and insert it into a real                                 hive table and where the real hive table                                 is something that has like RC file or                                 some other culinary storage engine that                                 works nice for analytics worthless I                                 think these days we have parquet and                                 work file but they were in the runt all                                 right they aren't having been around for                                 that long and then when things are into                                 hive as basically the main entry point                                 for for the for the analysts they use                                 some some proprietary tools maybe                                 against it like the click view or to                                 blow or something else that will connect                                 to JDBC but also people write our code                                 against whatever comes out of hive and                                 they do a lot of you know I've queries                                 so the nice thing here is i can run any                                 of these things at any point in time and                                 they won't interfere with each other the                                 only thing that they d anything how they                                 influence each other is of course what                                 data is there alright so um if you do                                 these things out of order you don't                                 really know what is actually in the hive                                 tables at any point except I you know                                 for the things that you put there but                                 then how much of the total volume is                                 that so instead of you know trying to                                 guard against that up front we said this                                 won't happen anyway people are going to                                 be late with their data feeds and and                                 sometimes they miss days and then the                                 next day you get the day before and                                 today and and there's all these these                                 external factors that somehow influence                                 whatever you can potentially be                                 ingesting but if you hold it back                                 because not everything is there then                                 you're going to hold the the analyst                                 people from doing their jobs for a full                                 day which is also not not optimal for                                 for well bottom-line oriented places                                 like online advertisers and who knows                                 what so it will happen anyway so what we                                 do is basically we say you should always                                 ask what is the use case for this                                 particular piece of data and only then                                 decide what the quality should actually                                 be and then you can verify that before                                 you actually run that piece of analysis                                 and and this is where you use metrics to                                 show the the people that do the analysis                                 what is actually in there instead of                                 saying the data didn't make it today                                 because it doesn't meet the quality                                 needs so you get nothing of course then                                 when the metrics do show that something                                 is severely wrong you have to be able to                                 make sure you fix it somehow and fixing                                 in                                 fixing on our set up basically means                                 drop a partition and I've regenerate                                 whatever should be going in there and                                 I'll refill it it's basically this is                                 repairing things afterwards because the                                 other option is make everything                                 transactional and have it you know Phil                                 all together or succeed all together and                                 this is what all of the ETL tools like                                 the graphical once try to to do for you                                 but it's just a really hard thing to do                                 and it works nicely for smaller                                 workloads but if you're ingesting tens                                 of gigabytes a day from from slow                                 connections from outside sources you let                                 you just want to go as far as you can                                 and you can fix it later on and of                                 course we have these kinds of metrics                                 that allow people to see whether what is                                 in there is of enough interest to them                                 to do their work basically sorry about                                 that so of course from metrics you can                                 use one of the many things that's there                                 already I you know about graphite and                                 maybe you can push data to ganglia or                                 something if you want to go that far and                                 have a lot of stuff you can do things                                 like open TSD be or maybe there's a lot                                 of metric solutions there the only thing                                 is that most of these things are there                                 for time series and what we have is not                                 exactly time serie because in the end                                 we're going to be pushing things into                                 hive and at that point potentially we're                                 going to say okay what went in there is                                 not complete I drop it all together and                                 I'm going to reinsert it into that                                 partition but now it's the full set                                 right so for all the importers like the                                 downloading you have a metric i                                 downloaded so many gigabytes today                                 that's okay you don't have to do                                 anything there because it was missing                                 before then it adds up and then the                                 metric in the end is complete same goes                                 for the processing but you can actually                                 do the transformation maybe two or three                                 or four times depending on whether you                                 know something went wrong or whether                                 indeed the upstream source delivered                                 additional sets that you should still be                                 getting and then of course you you drop                                 a partition and you really create it or                                 you refill it basically so your metrics                                 also need to be able to forget about                                 certain things and none of these tools                                 really do that because well it's the                                 time series so we did is basically                                 create a really simple web service and                                 all the different processes that do some                                 kind of importing they can call this                                 through an endpoint or through an API of                                 course that you have for the for the                                 Jays and scripts because you have a                                 library there and they call the web                                 service and basically say what they did                                 and then when you decide to manually                                 drop something every created or manually                                 say this import happened but I'm doing                                 it again right now you can fix the                                 metrics in hindsight and then in the end                                 provide at the unlist with a total view                                 of what is actually in there and that                                 means that this is the stuff that is                                 actually in there and on these days the                                 upstream provider just lost it because                                 while that happens as well sometimes                                 they have to be robust about that now                                 all right this is the first time I ever                                 did this talk so I didn't have a clear                                 feel for the timing of this but it turns                                 out i think that we have                                                questions yeah thanks                                 you                                 you
YouTube URL: https://www.youtube.com/watch?v=qYshvXej004


