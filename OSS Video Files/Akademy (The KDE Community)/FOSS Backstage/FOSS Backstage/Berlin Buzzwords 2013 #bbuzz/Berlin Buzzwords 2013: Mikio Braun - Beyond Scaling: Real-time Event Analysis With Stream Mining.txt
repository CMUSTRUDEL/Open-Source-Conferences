Title: Berlin Buzzwords 2013: Mikio Braun - Beyond Scaling: Real-time Event Analysis With Stream Mining
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	High volume event streams are an important case of big data applications. Dealing with millions of events per day is a huge challenge, in particular for batch-oriented scalability approaches like map-reduce. 

In this talk, I will discuss an alternative approach based on stream mining algorithms, which have been developed in the mid 2000s in the data mining community, but have to yet make it into the mainstream. Instead of relying on scalability and parallelization alone, stream mining allows you to trade accuracy for resource usage, resulting in robust algorithms with performance guarantees. 

I will focus on two classes of algorithms, counter based algorithms for identifying so-called heavy hitters, and sketch based algorithms to estimate activities of different event types. While these algorithms seem pretty basic at first, in the last part of the talk, I'll discuss how these algorithms can be used for more advanced analytics, for example, trending, probabilistic modelling and outlier detection, clustering, TF-IDF and related relevancy reweighting measures, and classification.

Read more:
https://2013.berlinbuzzwords.de/sessions/beyond-scaling-real-time-event-analysis-stream-mining

About Mikio Braun:
https://2013.berlinbuzzwords.de/users/mikiobraun

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay hello                               time event processing with stream mining                               and as you can see beyond scaling so                               this is actually why I got referred all                               these talks about a doob and you know                               everyone tells you that you have to put                               all your data in the cluster first you                               need a big data to handle all the data                               but actually if you if you look into it                                there are algorithmic approach is beyond                                scaling which which allow you to deal                                with quite an a big amount of data in                                real time and I'm going to talk a bit                                about bad ok so a lot of the the data                                you if you look at big data alright a                                lot of the data is actually event data                                right the question with big data is                                always how did it get so big and one of                                the possibilities is that you have                                events and you have lots of the events                                ok and there are many different areas                                for example if your financial data or if                                you have like an gaming app and you                                collect all the events of all your the                                people who play the game then you can                                get a lot of data very quickly and                                usually what you're interested in is you                                know you have all this these events and                                they're sort of unstructured and you                                want to know okay what's actually                                happening in there so for example is                                there anything which occurs very often                                or very rarely and and what actually                                what are the the people how many people                                are playing my game what is the average                                time right so basically the first thing                                you you usually want to extract are very                                like basic statistics from your data                                okay and there are basically two                                problems with with the event data and                                one is that you have a lot of data so if                                you if you start with only let's say                                    events per second which is really not                                much then you end up with about                                    billion events per year and if each of                                the event has I don't know a kilobyte or                                so then you already have a lot of data                                to store and also to process ok another                                problem with data is that potentially                                your events are talking about many many                                different kinds of events for example if                                you have tweets or something like an                                English and if you just want to count                                how often a word occurs then you already                                have the problem that you have                                potentially a large number of distinct                                words right or even if you                                I don't know if you have a website and                                you look at all the people who come to                                your website might easily be that the IP                                addresses which originate the accesses                                there on the billions and so on right so                                you have to so even if you just want to                                do simple statistics you already have                                the problem that you have to count                                millions of distinct objects and also                                for many many different events ok so how                                do you do this usually and usually what                                people tell you is you should just scale                                right so I mean so here try to sort of                                picture the the real-time landscape so                                at the top SQL so this just stands for                                like the traditional approach you just                                put it everything in the database and                                then if you want to count you just do                                some select count whatever right and                                then you get out the the statistics                                you're interested in but the problem is                                that of course is it's it's quite slow                                because it's like one huge disk and                                probably it's too much information and                                doing this aggregation takes a lot of                                time and people have developed different                                kinds of approaches to deal with that so                                one problem with this SQL in the                                traditional centers that often like each                                row and the database is stored in in                                memory so if you're just interested in                                aggregating over one of the columns you                                have to do you have to do lots of seeks                                to actually extract this column right                                because for each row you go in there and                                say okay just give me that and then give                                me that and they're all scattered over                                the database and people invented for                                that so that's like the first on the                                left here so there are things like                                columnar database which are actually                                organized in columns and then you can                                 just do a seek over a column very                                 quickly because it's just like continue                                 it's continuously on the disk so that's                                 one thing and there are many products I                                 would say you say ok we do real-time                                 event analysis but actually they're just                                 a columnar database then the second                                 problem is of course is you could say ok                                 I store everything on disk which is the                                 main problem why it's so slow so let's                                 just move to in-memory right and then                                 there are also products also say ok no                                 we do like MapReduce on in-memory and                                 we're also real time then there is the                                 problem always that if you first store                                 the data and you want going to analyze                                 it later you have this the computation                                 takes some time so even if you scale or                                 it takes some time between you you issue                                 the query and you get the results so                                 there are people the approach is to say                                 ok                                 don't just store the data and then                                 process it later but we just process it                                 at the country so we have something                                 that's the extreme processing and then                                 there even people say okay stream                                 processing is too complex I want to have                                 a nice DSL where I can just issue some                                 things like a stream query and then I                                 get the aggregates out and then you end                                 up with something that complex event                                 processing okay so I mean all these are                                 I mean of course you also have                                 combinations you can have like a                                 columnar database which is in memory or                                 you can have the stream processing is                                 always a memory yeah but I mean so like                                 if you talk about real-time they                                 actually many different approaches and                                 and even if you say this is still not                                 enough I can always scale out right and                                 then you add up to something with it                                 like a duper strong but there's another                                 out but there's an eternity of route                                 which I'm going to talk about now and                                 that is not to scale but actually to                                 like say okay I just have one computer                                 and I don't want to buy more but instead                                 to see whether there are algorithms                                 which allow me to to deal with their                                 data yeah still like on a single machine                                 okay and the key to that is that you                                 start to approximate so you say if I                                 want to count                                                            want to store                                                            to focus on that which is which is                                 important to me so that's the basic idea                                 okay so stream mining stream mining                                 algorithm so this is a set of algorithms                                 which have been developed in the field                                 of data mining in the mid-                                              basic idea is that you have an infinite                                 stream of data down here okay and you                                 have an analyzer which has borrowed                                 resources so the small normally means                                 you only have a fixed amount of memory                                 but so you're still interested in                                 answering certain stream queries and the                                 most basic ones are for example how                                 often does an item appear in the stream                                 right well as the stream and I just want                                 to let's say it's IP addresses and I                                 want to know how often did a certain                                 RPLS occur in the stream other questions                                 how many distinct elements are in the                                 streams are not interested in like                                 counting but I just want to know how                                 many different ones are in there so it's                                 more like                                                                then probably the most complex of these                                 basic things is what are the top top K                                 most frequent items so I just not only                                 want to count like individual                                 items but I want to I also want to get                                 like a ranking of them right on all with                                 these bonus resources okay so how our                                 algorithms okay and the so obviously                                 right if you want to count items but you                                 only have bounded resources you have to                                 you have to throw away some some piece                                 of information because otherwise it                                 can't work right and usually so one way                                 to picture this is this triangle here so                                 on the top we have big data so you want                                 to be able to process lots of data you                                 want on the lower left side you have                                 that you want to be fast right so in the                                 sense of you can have a lot of data and                                 also get results and a short amount of                                 time and then the other and we have we                                 have exact and sort of MapReduce and                                 friends there they say okay we are big                                 data and exact but you can't be fast so                                 this is like this this cap theorem which                                 says you can only have like two sides of                                 the triangle and streamlining or the                                 other hand says okay I want to be able                                 to process losses a large amount of data                                 and I want to be very quick but I don't                                 care so much about the exactness of the                                 results okay so actually I start with                                 the with the last province of this top                                 case so how does these do these                                 algorithms look like and the interesting                                 thing is about these algorithms is that                                 they're often quite simple so for                                 example with these heavy hitters the                                 idea is as follows so you want to count                                 often an item occurs and you have a                                 fixed table of counts right so you only                                 have two fixed amount of memory and                                 these are just the keys you are counting                                 and these are the counters here and now                                 I have a new element occurs in your                                 stream you have to have two choices so                                 either it's already in the table and                                 then you just update the count or if                                 it's not in the table then you just                                 remove the entry which has the lowest                                 count and your but you also use this                                 count as a starting point for the new                                 elf is a new item right so here and then                                 I'm okay and that way so this is                                 something like like a worst case lower                                 bound on the number of times I might                                 have seen that item but missed it                                 because right because it has already                                 been in the table but then it got                                 deleted from the table in order to curse                                 again but this is sort of like the lower                                 bound and based on that you get some                                 performers go                                 tee's okay and of course the larger the                                 table is the more excites exact your                                 results are which also depends on the                                 distribution of the data so it's quite                                 simple I will but it yeah you can very                                 quickly get the like the top K elements                                 from that so another thing a problem                                 often is that you are not really                                 interested in having these top k or                                 these heavy hitters for the stream for                                 for all of the stream but only for a                                 given time window right so it's not like                                 you start the algorithm and then after                                 two years you still have like the top                                 items for the last two years but only                                 you're interested in the last week or                                 last month or so and if you want to do                                 that exactly that's also something which                                 can be quite difficult because right                                 because you really have to at least                                 remember all the events for the time                                 frame you're considering because even                                 like if you you count up when they come                                 into the time window and then you count                                 down when they leave the time window you                                 still have to keep keep this data                                 somewhere and if you're aggregating over                                 a month or so this takes a lot of time                                 an alternative is to use exponential                                 decay okay so instead of just like going                                 doing one up and one down when the event                                 occurs actually the the count of the                                 event decays over time so after let's                                 say after an hour so when it occurs you                                 have a count of                                                        goes to down to                                                         imp like that ok so it's not exactly                                 like over this time window but you know                                 but you still get an aggregate if it has                                 a certain time scale with it ok and the                                 nice thing about this is that it's                                 actually you can implement it in a very                                 efficient way so ok when you when you                                 write this exponential decay down                                 basically you get this formula here so                                 this is the like the decay part where                                 you have the time differences and this                                 is the score which you put in in there                                 and the nice thing about the sum is that                                 is actually recursive so in order to get                                 the score for the next element you have                                 like the count and then you have                                 something which is like a time shift                                 term which which which you put on the on                                 the score you had before                                 sort of you don't have to aggregate keep                                 all the data and aggregate all the time                                 but you only have to remember the                                 timestamp of the last event and the last                                 score you have so only two numbers okay                                 so if you write that down in pseudocode                                 basically you have two tables to count                                 us into time stamps and then you have                                 two operations one is the update when an                                 event occurs and the other is in order                                 to get the score you get something like                                 this right and you just you always just                                 have the account and you have this                                 waiting term which is like a rewriting                                 overtime okay                                 so I've come I come back later as an                                 example on for that another I'm sort of                                 a class of algorithms are so called                                 sketch algorithms which sort of try to                                 approximate a histogram over certain                                 elements with a fixed amount of memory                                 and so they work in a way a bit like                                 these bloom bloom filters I don't know                                 if you know them but the ideas that you                                 have like in this direction you have n                                 bins and you just n different hash                                 functions for each of these rows right                                 and if you get an item you compute all                                 these different hash functions which                                 gives you in different index for each of                                 the rows which sort of is like a                                 fingerprint for that item and then you                                 count up all the cells which these hash                                 hash functions have indicated and of                                 course you will get collisions but the                                 idea is now if you if you do the query                                 you take the minimum over all these                                 entries and this sort of minimizes the                                 amount of collisions you have and                                 therefore you get a good good estimate                                 of your good data okay so this is                                 something which is very efficient if you                                 want to approximate counts or a very                                 large spaces was very it was very few                                 memory but compared to this thing with                                 the heavy hitters you don't get this                                 order in here right because you don't                                 really know what's in there it's just                                 like this bit field and you don't you                                 don't store the the keys which are in                                 there but if you want to query you can                                 just carry anything and get an                                 approximate answer here okay so right so                                 these are two two algorithms which are                                 fairly simple but also somehow very                                 boring right because they just do                                 counting over a certain time frames                                 I mean in a way getting the top most                                 active items is already useful if your                                 analyze some analytics kind of                                 application and you just want to know                                 what the most active things that this is                                 already nice but what else can we do                                 with it and it turns out so this is the                                 second part of the talk I'm going to                                 give you a few examples on that you can                                 actually do quite a lot of stuff with                                 simpler counting because if you think                                 about it counting is statistics okay so                                 if you so I've written down there are                                 three basic statistical quantities like                                 like a mean so you just want to estimate                                 the average or a correlation or even                                 like principal component analysis so                                 this is something where you if you have                                 Victoria data you want to know what the                                 what the main directions are in that                                 space and if you look at it right so                                 it's always in the end you always                                 aggregate some sums okay and you could                                 do this not only just with overall time                                 but you could also end insert it's a                                 time scale in there and also this                                 weighting so that you get get these                                 statistics based on data which belongs                                 to a certain time frame so you could                                 send for example say what is the the                                 average of that quantity over the last                                 hour average over the last minute over                                 the last day or something okay and that                                 you can already get with the based on                                 these algorithms which without doing a                                 lot of computation because you just                                 update each event as it comes in and for                                 each item type you only need to store                                 two numbers okay so just slightly more                                 complex example is the least squares                                 regression so this is a simple method                                 for doing predictions on real valued                                 functions so basically what you have to                                 do is you have to compute these better                                 hat right so this is just in matrix                                 notation it's a very simple you just                                 have to write down this X transpose X                                 matrix and invert it and if you don't                                 have too many features that is probably                                 okay because it's just a d x team D                                 matrix so if these few hundred or so                                 you're okay and these are the entries of                                 this matrix right but this is again                                 some which could be used but you could                                 also just as I said use the this                                 exponential decay to aggregate these                                 sums over certain time frame right do                                 these two updates for each element you                                 get and then you can sort of online                                 learn this these predictions based on a                                 certain amount of data right so you                                 could have you track something which                                 which changed a bit over time and you                                 could have a prediction which is always                                 based on the like last hour or last day                                 of your data without actually having to                                 go back and crunch all the data because                                 you just track these numbers okay so you                                 could also just learn probabilistic                                 models based on the data just again                                 because these are just numbers okay and                                 if you have that so for example if you                                 compute the mean and the variance you                                 could already do something like outlier                                 detection right so the idea is that you                                 compute some statistics so this is the                                 data you've already seen so far this is                                 like the year future data and this                                 number this line here is the like the                                 weighting factor which says how much                                 this data contributes to your estimate                                 and then you have a probabilistic model                                 if you get a new point and you can ask                                 so help so given the data I've seen and                                 the model I have inferred from my data                                 how probable is it that I see this point                                 and if this probability goes below                                 threshold you can do outlier detection                                 so you say okay this looks very                                 improbable probably something's not                                 happening here and this is something                                 could use very well for monitoring other                                 kinds of approaches okay but I mean you                                 can even do more so there's this there's                                 a paper which describes how you can use                                 these countenance caches to aggregate                                 all the information you need in order to                                 do online clustering so in clustering                                 the idea is basically you get new points                                 and you have like centroids I mean like                                 like Centers for each of the classes and                                 then you just assign the new point to                                 the closest class right and then you                                 update the class so we you get the                                 points are sort of like you start with                                 random points and if they you just                                 assign new points to the closest center                                 you have and over time you just                                 the classes which describe your data                                 well and this is the problem if the lag                                 if you're clustering text documents your                                 spaces can become very large because you                                 have these potentially                                                   using this continent schedules you can                                 actually could keep this these                                 statistics in a very compact manner and                                 in-memory okay right so and if you start                                 to think about it that way okay it's                                 always only counts and if you dig in and                                 look at especially statistical methods                                 like like classical statistical methods                                 which have been around for quite some                                 time they're all based on simple                                 statistical estimators like numbers so                                 so Frankston tf-idf which is a measure                                 which you use to to weight words based                                 on how how often they occur right so a                                 word like that because the occurs in in                                 all documents is very high probability                                 it's really not that surprising if the                                 occurs very often in a document and                                 that's something you can so you can also                                 track in real time they make based on                                 these algorithms because it's just it's                                 just counts right so for each word you                                 would do like the update for the word                                 and you would do another update to count                                 the number of documents you have and                                 then you get this IDF score based on                                 just from the statistics you've                                 collected and the interesting thing                                 about this is that this is something                                 which automatically tracks the data you                                 actually have right so if people                                 suddenly start talking about a certain                                 topic after some time you've just                                 adapted to that to these statistics and                                 you actually start to to down wait these                                 words so they are not that surprising                                 anymore because like in the last hour                                 there have been yeah very very frequent                                 in your data right and again this is                                 something which you can just track I                                 mean even for words you don't you just                                 need a few megabytes hundred megabytes                                 um something it's not something where                                 you actually you will have to store the                                 data first and then you have to crunch                                 like the day of tweets or whatever to                                 get this kind of thing you can just do                                 it an online session                                 so just to give you an idea how complex                                 these things can get you can even do                                 classification based on naive Bayes so                                 naive Bayes is like the like a baseline                                 method so if you ask a machine-learning                                 guy they say I mean naive Bayes isn't                                 that like the simplest thing you can do                                 so it's always used as a baseline to                                 compare your own method against but                                 actually it's quite it's not so bad as                                 people saying right so basically this is                                 the formula you use to do the prediction                                 right you see again it's a sum and most                                 importantly the the numbers which are in                                 there they are all also very simple kind                                 of statistics like how often the word                                 occurred in the document a number of                                 times the word appeared in a certain                                 class so things like that and then there                                 is also this a nice paper tackling the                                 poor assumption of naive Bayes                                 classifiers where they present presents                                 or                                                                       Bayes perform almost as good as a                                 support vector machine on for text                                 classification and these are all again                                 things which you can just compute in                                 just the same way using doing these                                 updates all right so you can just stream                                 in your data for example you could use                                 this to predict hashtags on Twitter                                 right so you just see the hashtags which                                 you have and you can then and use these                                 as training examples in an online                                 fashion so that you can also predict                                 hashtags for tweets which don't have                                 hashtags or which don't have these                                 special hashtags and this is again                                 something which automatically tracks                                 based on the yeah whatever x squared                                 you're interested in right so this is                                 not something where you like you store                                 your data first and then we're like once                                 a week you crunch it and you you learn                                 for a long long time and then you have a                                 new predictor but this is something                                 which just updates we just update it all                                 the time ok yeah so these are were all                                 like very simple methods based on                                 statistics but if you take something                                 more advanced like a support vector                                 machine the question is could this                                 technique also work there it's actually                                 not so not so simple so the function you                                 learn in the support vector machine is                                 something like that and actually that's                                 the sum of all the points you have                                 and this is not something which you can                                 so it's not like in the example with the                                 linear regression where you only have                                 like a fixed set of statistics of things                                 to track but here actually you have a                                 big matrix which come which contains all                                 the data and for training you need to do                                 a lot of computation with that data they                                 can't really do it but you could still                                 use stream mining for example to cluster                                 and then to extract and representative                                 tester data set okay okay so if you want                                 to play around with that so actually we                                 have built something called stream drill                                 which is exactly that so it gives you                                 this basic layer not the statistic                                 methods I talked about but the basic                                 layer to do this counting so if you want                                 to play around with this kind of                                 technology this is something you can you                                 can just download there you start it and                                 then there's a rest interface where you                                 can talk with your thing and basically                                 what you pipe in are tuples so you have                                 to predefine beforehand what the                                 structure is but then the events are                                 just tuples where you say okay this word                                 and this would occur together or                                 something and then you get these top k                                 trends for different time scales which                                 you can just query and for example we we                                 did a simple demo based on this based on                                 these stocks at birth so i don't know if                                 you've seen this before but actually                                 there are people who are using the like                                 the the stock ticker symbols on twitter                                 to talk about certain companies and what                                 we did is we extracted those and then                                 for example looked at combinations that                                 co-occurrences between these symbols and                                 certain keywords right so here you would                                 for the symbol apple and the keyboard                                 apple okay that's not so surprising you                                 will get this number of occurrences over                                 the last day and then you can do                                 something like this year so in this                                 graph what we've plotted are the the the                                 symbols and the keywords and the                                 connections between them okay and this                                 is again something and then you would                                 see over time how this bubbles and                                 things go it gets smaller and larger and                                 it's really something which which can                                 work in real time okay so the summary so                                 so although like everyone says okay you                                 first need a dupe cluster and you have                                 to scale if you want to do big data                                 there are actually alternatives where                                 you can just take some data pipe in I                                 don't know a few                                                   second and actually see something which                                 is in your data without you know doing                                 this initial investment and stream                                 mining is a class of algorithms which                                 which work pretty well for that and the                                 idea as always that you say okay it                                 doesn't have to be exact so it's okay                                 with the proximity but for many                                 applications actually is right                                 especially if you're just interested in                                 them in the most active ones but you're                                 not really interested in the exact                                 numbers it's okay to get approximate                                 results and you still can sort of do                                 this use this kind of approach to get                                 something in real time and then if                                 you're really interested in the exact                                 numbers you can still crunch it later                                 but you don't have to to do this and try                                 to scale your Hadoop cluster to a size                                 where you can actually handle event                                 streams in real time because it's                                 something which is which is very                                 expensive so it's better if you have a                                 cluster which you can like run once a                                 day and use something like this in front                                 of it to get your real-time results out                                 there okay and if you want to play with                                 it you can look at stream drill which is                                 something we've built to help you get                                 started with that okay thank you very                                 much                                 you
YouTube URL: https://www.youtube.com/watch?v=u08tNk2jgPY


