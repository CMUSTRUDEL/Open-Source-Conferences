Title: Berlin Buzzwords 2013: Radu Gheorghe - JSON Logging with ElasticSearch #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	This talk is about aggregating loooots of logs - searching of seriously big data. We'll look at how, where, when, why, and what to log. We'll show how to use ElasticSearch as a data store for logs and what the benefits of doing so are. 

We'll discuss advantages and disadvantages of logging in JSON, which is easily processed by computers, over traditional logging, which is easily processed by humans. Finally, we'll explore how you can get your logs - JSON or not - into ElasticSearch, run searches and statistics on them, and create pretty graphs you can't stop staring at.

Read more:
https://2013.berlinbuzzwords.de/sessions/json-logging-elasticsearch

About Radu Gheorghe:
https://2013.berlinbuzzwords.de/users/rgheorghe

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              um hello and thank you for being here my                               name is Raju gergely i work at semi text                               or got someone text again i'm also                               writing at this book called                               elasticsearch in action other things I                               like to do I like to dance with my wife                               there's me there without a head and I                               like to sing with my son there's me                               there with long hair I'm here to talk                                about centralized logging so you have a                                bunch of servers they're all logging and                                you want to get all those logs into a                                central data store and then be able to                                use that data when you want to search                                through all of it when you have multiple                                servers multiple applications and to do                                statistics so we can get metrics from                                those logs and make graphs out of that                                so we'll have a look at how that can be                                done and we'll start by looking at the                                central piece the store then we look at                                how you can send your logs to it and                                then about searching and statistics and                                finally we'll zoom in a bit into the                                logs themselves because some of them can                                be you know structure down structure we                                look about that at that a bit later so                                earlier this this year we did a poll                                when we asked devops people if you                                centralize your logs where do your logs                                end up and most of them said elastic                                search and i think the reason                                elasticsearch is so popular for storing                                logs is that it's fast and it's scalable                                and another reason for that is because                                there are lots of tools that can help                                you do that you might recognize some of                                them here I'll touch some of those so                                basically these are the ones that you I                                mean they're not the only ones but you                                know the ones I've I'm aware of that you                                can use to ship your logs to                                elasticsearch and the ones over there                                are the ones that you can use to                                search and to do searches and analytics                                and the way most of them works is by                                using elastic searches rest api so you                                send a json over HTTP with your log or                                logs and you get back adjacent reply                                saying hey I've indexed your logs and on                                the searching and analytic side it's                                more of the same you send the JSON with                                your query and it get back AJ song with                                with the results so I said oh my gosh                                anyway I said elastic search is fast and                                it's fast because it indexes your log so                                being indexed you can get them really                                quickly but a lot of people ask okay how                                much can I expect and I'll just say if                                you get like a five-hundred-dollar                                laptop I would expect you to be able to                                store something like                                                    average apache logs into it and still be                                able to do searches and statistics and                                get your reply back in under a second                                this was this is what I would expect and                                I also would expect you to be able to                                get logs in at a rate of                                                per second I was supposed to say                                something here so that bear over there                                is the burning bear for those of you                                don't know so                                                          the whole building x is your website                                every day you would be able to store                                those logs for two months now                                           logs per second means if the whole                                building gets to a website within six                                minutes with that laptop you should be                                able to to hold that load and but if you                                do that if you do that at                                           second you're                                                  obviously not be two months it will be                                something like six hours so that's when                                if you want to store more logs you have                                to add more servers um                                so that's where scalability comes into                                play and by default when you when you                                send your logs to elasticsearch and                                let's just assume use a single container                                single index elasticsearch by default                                will break that data into five shards                                and we will also try to replicate that                                data once but if when you have only one                                node that makes no sense so it just                                doesn't do it but as you add more                                servers to your cluster and they would                                automatically see each other by default                                via multicast then it will allocate                                those replicas and it will also move                                 around all those shards and replicas so                                 that you end up with a more or less                                 balanced cluster where you have your                                 data sliced across the let's say for                                 nose and I'm just going to show you a                                 small screen cast of how this might work                                 so here on the right side I have                                 elasticsearch head which is a nice                                 little tool that you can use in your                                 browser to watch on elasticsearch now                                 there's nothing here because i have no                                 cluster right now I've just downloaded                                 the elastic search from the website and                                 unpacked it so what is going to happen                                 now is that I'll start elasticsearch and                                 you should see it in there but we have                                 no data so so now i'm using the the rest                                 api to just send something that you know                                 might be a log in to elasticsearch and                                 elastic search will will take care of                                 pretty much everything for me so it will                                 create a new index it will detect that                                 my message field over there is a string                                 so i don't have to pre configure                                 anything to get started so you see this                                 is our first five shard index these are                                 the replica is replicas which are not                                 assigned yet because we only have one                                 node so now I'm starting a second node                                 and notice on the same computer but you                                 know in production you'll probably use                                 more and it joins the cluster it detects                                 it via multicast and                                 now you can see that the replicas that                                 the content of our index get replicated                                 to the second node so now I'm starting a                                 third node should be there in a minute                                 less than a minute one and some of the                                 shards will automatically go from the                                 two nodes to this to this third node                                 everybody will it will be more of the                                 same when we start start the fourth node                                 some awkward silence so now you can use                                 the REST API as I said for for shipping                                 logs for doing queries but you can also                                 use it for doing a lot of administrative                                 tasks like changing some configuration                                 settings and what i'm doing here is i'm                                 using the rest api to shut down notes so                                 i'm shutting down the last note i                                 started i should leave the cluster in a                                 moment you see the teenage warhead there                                 now it's gone and because of the                                 replicas even if one node leaves you                                 still have at least a complete set of                                 data so elasticsearch will automatically                                 replicate the data that it is missing                                 because of the left node to the                                 remaining nodes so you can scale both up                                 and down I mean that up and down like                                 this and we're back to where we started                                 so how do you send your logs to                                 elasticsearch one of the tools I really                                 like for doing this is called log stash                                 and you might have heard about it                                 yesterday if we went you went to the                                 open source logging talk I really like                                 it because it's really DevOps friendly                                 and it you can see what it does it has                                 it has three threads and once the input                                 and you can use lots of inputs you can                                 make it receive this law you can make it                                 tell your log files you can even do                                 stuff like make it watch your inbox and                                 send                                 the message is there as logs you have                                 some filters which you know you can do                                 you can change your logs in there so for                                 example you can if you have Java stack                                 traces it will mesh them in a single                                 event so you don't have to look over                                 them and you also have lots of outputs I                                 mean there are lots of each of those                                 they're like                                                            of them is obviously elasticsearch you                                 can also use send the data to different                                 data stores for example lot of people                                 use gchat loss the word anyway read this                                 okay you've read this for for buffering                                 so you have your your server that's                                 logging use log stash to with the Redis                                 output to send your logs to read this                                 and then on the other side you have                                 another log stash that gets your logs                                 from readies and puts you to                                 elasticsearch so you have a buffer there                                 in the middle just in case your elastic                                 search cluster goes down for example                                 then you your logs get buffered in rebus                                 and sent to last research whenever that                                 becomes available so let's look at a bit                                 let's zoom in a bit more so for example                                 if you have your application that's                                 logging to syslog for example you can                                 out of your sister or demon you will get                                 the things you you log from application                                 and also stuff like the day the host                                 name is so on and that's a standard                                 format and log stash gets that syslog                                 and puts it in a nice JSON and sends it                                 over to elasticsearch and once you get                                 your logs in it's time to search and                                 again if you've been to the talk                                 yesterday you might have seen cubana in                                 action cubana is a real nice tool that                                 you can use for both searching and                                 analytics so this new version out that                                 you can use you can use it like a                                 dashboard it's actually called dashboard                                 somewhere so you can you can add widgets                                 to it you can add a search widget where                                 and you know like you have this table                                 here and you can search for what you                                 want but you can also add all sorts of                                 graphs which is realize you can also                                 have multiple dashboards and stuff like                                 that okay so now that we went to                                 searching let's look a bit more at the                                 logs and I guess many of you have seen                                 logs that initially make no sense like                                 that thing Mike                                                      that mean and you might ask the                                 developer read some documentation and                                 you'll find out that for example this                                 means the user ID Mike spend                                            on our website ended up buying a mouse                                 and did so successfully and now this is                                 a very compact format and once you get                                 used to it I think it's you know it's                                 easier to read now there might be some                                 glitches when you search so for example                                 if you search for mouse you might get                                 back a la guerre mouse was the name the                                 user ID and stuff like that and when you                                 want to do analytics it it might be more                                 of the same how do you know what's the                                 average time you have to parse those                                 logs so I think for most purposes a good                                 thing would be to structure those logs                                 and to index them in a structure format                                 like JSON like elasticsearch talks JSON                                 and you can you can have fields from                                 your logs in each field of the JSON and                                 one of the very good tools to do that is                                 log stash you can use a filter that's                                 called grok which is like user-friendly                                 interface to regular expressions and you                                 can say okay this my first word is going                                 to be you the field user ID and the                                 second one is the number and it's going                                 to be in the field time and so on and                                 log stash will happily parse that for                                 you and we is able to construct your                                 JSON that you can then send to                                 elasticsearch so then when you search in                                 Mike the user ID you know that you won't                                 get a item that's called Mike or                                 something like that okay                                 but this might be a problem if the                                 format changes so if if you get a new                                 field their that's the browser for                                 example then you would have to adjust                                 your configuration because otherwise                                 parsing would fail and I think a good                                 thing to do here if you especially if                                 you have a format that's changing all                                 the time is to log in JSON in the first                                 place and if you log into JSON then you                                 won't have to make logstash use regular                                 expressions which is also expensive in                                 terms of CPU but you can also use other                                 tools and i'll come back to syslog here                                 for example for example just rsyslog and                                 syslog ng which are the most popular                                 syslog demons they both use this this                                 standard called siiii in which because                                 if you take a syslog message that's a                                 free format you can put everything                                 anything in there but you can put you                                 can start the message with the cookie                                 SCE there and you can put a JSON in the                                 rest of this of the syslog and you can                                 have your syslog demon parse that for                                 you and i'll put it in whichever format                                 you want what's more if you use our                                 syslog for example our sister also can                                 output to elasticsearch so for example                                 if your application you can you can have                                 your application log in in JSON and you                                 can have our syslog to say okay is this                                 JSON if it's not then you can output it                                 to elasticsearch just like any other                                 cislo i mean you have the day to have                                 the severity and then the message which                                 is the format that you choose or you can                                 say my message is this JSON and it can                                 parse your JSON and you can say you can                                 output it to elasticsearch with all the                                 fields that you define from your                                 application which is useful because most                                 of the time I mean future custom                                 application then you already have the                                 variables in there you know you can out                                 you can make a structured                                 document very easily if it's not most of                                 the applications you can configure a log                                 log in format right like Apache does you                                 can make Apache look JSON that there is                                 no problem in that so then you can you                                 don't have to parse those logs anymore I                                 mean you have but you don't have to                                 guess what field is which so yeah this                                 is it we look at some of the tools we                                 also use them in this SAS product that                                 we have which is called log scene and                                 you can check it out our booth if you                                 want to we're also hiring and that's it                                 you
YouTube URL: https://www.youtube.com/watch?v=yDLtyLi6Ny8


