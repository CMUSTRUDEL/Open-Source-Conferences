Title: Berlin Buzzwords 2013: Christian Moen - Language support & linguistics in Lucene Solr ElasticSearch
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Full title: Language support and linguistics in Lucene/Solr/elasticsearch and the open source and commercial eco-system

In search, language processing is often key to getting a good search experience. This talk gives an overview of language handling and linguistics functionality in Lucene/Solr/ElasticSearch and best-practices for using them to handle Western, Asian and multi-language deployments. Pointers and references within the open source and commercial eco-systems for more advanced linguistics and their applications are also discussed.

The presentation is mix of overview and hands-on best-practices the audience can benefit immediately from in their Lucene, Solr or ElasticSearch deployments. The eco-system part is meant to inspire how more advanced functionality can be developed by means of the available open source technologies within the Apache eco-system (predominantly) while also highlighting some of the commercial options available.

Read more:
https://2013.berlinbuzzwords.de/sessions/language-support-and-linguistics-lucenesolrelasticsearch-and-open-source-and-commercial-eco

About Christian Moen:
https://2013.berlinbuzzwords.de/users/cm

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay thanks everybody for coming my name                               is Christian and I'm going to talk about                               language support in and linguistics in                               lucene solar and elastic search and the                               ecosystem which is a very very broad                               topic each of these topics I think                               deserve a a talk of the road but in this                               talk out tried to give an overview of                               some of these things and also dive a bit                                deep by showing you some code for                                specific things that I think is                                important a little bit about myself my                                background with the search is from a                                company called fast that was acquired by                                microsoft i was there                                              years in R&D and five years in japan                                doing all sorts of things when cut when                                microsoft bought the company i left to                                start a tilaka and for a small company                                and focus on search natural language                                processing and big data analytics based                                in tokyo in japan but our customers are                                pretty much everywhere so even though                                we're a small company our customers are                                typically very very big companies i'm                                also a newbie Lucy non-solar committer                                I've mostly been working on on Japanese                                language support and I've also recently                                been doing some work on Korean so we not                                talk today and these are the things that                                will cover we'll talk a bit about basic                                searching and matching I went reduce                                some of the challenges with natural                                language and such talk briefly about                                some basic measurements for search                                quality and then we'll go into how some                                of these and will be things work in in                                lucene elasticsearch and apache solr and                                we'll also talk a little bit about the                                NLP ecosystem so i'll also try to do                                some hands-on demos and i'll try to do                                as many of these as as we have time for                                all of them the code for this and how to                                to run these things will be available on                                github after this talk so if you're                                interested you can go there and have a                                play yourself                                okay so just have a look at what the                                search engine actually does at the very                                very very high level so let's say we                                have these two documents so she is very                                tasty in Japan visiting this giddy fish                                market is very fun so let's say we want                                to to search search or make this content                                indexable so will you first do we take                                this source text and we segment the text                                into tokens we get something like this                                and then we normalize these tokens by                                lower casing them because we usually                                want to search in a case insensitive                                manner so then we take our tokenized                                documents with normalized tokens and                                then we index them people then inverted                                index not there again at the very high                                level it looks something like this so                                each we map each term to each document                                that contains that term there's more                                going on with positions and so on but                                for the purpose of this let's keep                                things at the high level so sushi occurs                                in document one is a curse on document                                one and two very in document                                            so on so let's say we want to search                                using this inverted index we're                                searching for this this is our query                                very tasty sushi we can tokenize this                                into very tasty and sushi and then we                                can interpret this past Korean various                                ways we can look at this as an or query                                or an and query or perhaps even                                something else in this case we're using                                an aunt query so we're taking these                                terms and we're looking them up in our                                inverted index with finding these                                documents are matching but since our                                query is an aunt query we need all the                                terms and our query to to exist in the                                documents that that that we get back                                when we look up in this inverted index                                so we see that this term very doesn't                                existing in documents one document one                                and then we                                get this search result on which that's                                what we return so simple enough let's                                try another query visit fun market we do                                the same thing we look up market and fun                                and find that document two matches but                                this this visit and this visiting we we                                don't have the entry visit in our                                 inverted index so these things don't                                 match and with this being a nun query we                                 get no hits so the key point here is                                 that search engines are not magical                                 answering machines they match terms and                                 queries against terms in documents and                                 all the matches by rank and text                                 processing effects search quality in a                                 big way because it affects matching so                                 if you put low quality things into your                                 search engine you will get low quality                                 things out and I believe that the magic                                 often some search engines provide is                                 contributed by high quality text                                 processing the Rossum yeah so I will now                                 introduce some specific cases with some                                 languages to sort of give you some                                 real-world example of where some of                                 these complexities like and these                                 languages are English German French                                 rabuck and Japanese so this is an                                 english english example so this sentence                                 if we want to search this sentence there                                 are some considerations that we should                                 make perhaps how do you want to index                                 this this word and also should i search                                 for style match styles should ferment                                 match fermentation with german how do we                                 want to search these characters these                                 are things we somehow need to decide                                 what about search for home stats do we                                 want that too much                                 learn this upstart would be nice perhaps                                 but French we can have these characters                                 that we need to deal with do we want to                                 normalize them to allow perhaps people                                 not so familiar with French to get good                                 search results or get any search result                                 and however you want to search do you                                 want to search for aoc to match up with                                 asean division control it which is how                                 is usually abbreviated and in this case                                 anyone read Arabic here good so it gets                                 easier as long as you know that you                                 start from here and so in this text how                                 do we want to search this word which is                                 a this word has been elongated it has a                                 touch wheel character in it which is                                 just a stylistic thing so that this word                                 gets elongated so if we search without                                 this title character we would match                                 unless we had some sort of normalization                                 in place to do it                                                        do we want to normalize their critics                                 these like beautiful squiggly things on                                 these characters so if we remove them we                                 get something like this which is more                                 how contemporary Arabic is is written by                                 doing that we also change well we                                 introduce some ambigua shin by removing                                 them that's not a problem in practice                                 for rabuck because the meaning of these                                 are words even after normalizing                                 diacritics is still implicitly                                 understood by a native speaker because                                 of contexts and also in arabic it turns                                 out that is very hard for for for many                                 users of the language to actually type                                 it correctly on the computer                                 so people often right wrong things and                                 they're very common they use a glyph                                 that looks like something in Arabic that                                 might be from Farsi for example that has                                 a unit different unicode code point but                                 the glyphis this looks the same so these                                 normalizations are hard japanese we have                                 a language here that's where that's not                                 white space segmented so what are the                                 tokens in the sentence and what do we                                 index so their implicit so we can                                 segment this using technology in the sea                                 so we get these tokens but then Japanese                                 language is highly inflected so in verbs                                 are highly inflected also adjectives so                                 maybe want to normalize do some                                 morphological normalization maybe we                                 want this is this word for beer it can                                 be written using half with characters                                 and full with the characters you have                                 different character widths and Japanese                                 for some scripts and maybe some will                                 also want to have be match this beer so                                 this is I think in in iOS this they use                                 some unicode code put forth for this                                 emoji character so even though these                                 languages are very different there are                                 some common traits we need to take                                 source text and segment that text into                                 tokens we need to deal with non space                                 separated languages and also space                                 separate on languages and when you need                                 to handle punctuation in both and with                                 German and another European language we                                 have issues with compound nouns like                                 this London sub start needs to be dealt                                 with or should be dealt with and to do                                 that we usually apply some linguistic                                 normalization kannamma lies characters                                 you can apply morphological                                 normalization spelling                                 synonym stop war zone so we have been                                 looking at just a few languages but we                                 also have lots and lots of all the                                 languages in the world that not so                                 people use and each language is                                 different and each language has their                                 own set of complexities that needs to be                                 dealt with and good search need /                                 language processing but how that                                 processing is being done is in many                                 cases application specific okay so let's                                 talk very briefly about search quality                                 measurements who here is familiar with                                 the notion of precision and recall good                                 so basically for those who are not                                 precision is the fraction of the Rapide                                 documents that are relevant so if you                                 don't have if you have a lot of unwell                                 event documents in your search results                                 you have a precision problem recall is                                 the fraction of relevant documents that                                 are retrieved so if your search engine                                 cannot find the documents that are                                 relevant then you have a recall issue so                                 should i optimize for precision or                                 recall we believe both but usually get                                 one than not the other and I think that                                 largely depends on your application so                                 for compliance applications litigation                                 stuff where you don't want to miss any                                 potentially relevant hit you optimize                                 for recall other applications where                                 multiple answers might serve the users                                 information need and you have lots of                                 different good answers then practice to                                 optimize for position but in practice I                                 think a lot of tuning work is about                                 improving recall without hurting                                 precision too much                                 okay let's move over to linguistics in                                 in lucene so this is a very very                                 simplified lucene architecture seen from                                 a from a query and                                                    point of view so have a document or                                 query that is being processed through a                                 leucine analysis chain often called an                                 analyzer of rap together as an analyzer                                 and these elements of an analyzer they                                 are processed in a pipelined fashion and                                 in lucene we do processing on a per                                 field basis and this is a key plugging                                 point for linguistics in Lucy so before                                 we index something we we process in the                                 content or the query so what do these                                 analyzers do they basically take text as                                 input and turns them into a stream of                                 tokens and tokens are produced by                                 tokenizer but tokens can be processed                                 further by chain of token filters                                 downstream so we have a conceptual model                                 that looks something like this we have a                                 reader that provides the the text that                                 we want to process and those can have a                                 associated char filter with them we                                 won't go into much discussion of that                                 but that can be used to normalize the                                 sum of the sum of the cap there the                                 characters we we saw earlier in in some                                 of the german and french examples then                                 the tokenizer segments text provided by                                 the reader into tokens and these are the                                 processing further downstream or curse                                 I'll show you how some of these things                                 work in code in a bit but before doing                                 that let's see how how leucine actually                                 processes this French sentence                                 okay we start with this sentence Lucien                                 pang a protégé par in appellation                                 d'origine contrôlée if that sounded                                 approximately French I'm happy so we                                 first run the standard tokenizer to to                                 to analyze this text and we get these                                 circuits and then we apply a bunch of                                 filters so the first filter is this the                                 relation filter that remove this did                                 these characters indicated in red hair                                 they normalize this by removing them                                 then further on we do a lowercase filter                                 so this capital L becomes a lowercase l                                 and then we run a stop filter so remove                                 remove very common words and these words                                 gets removed and then we do some light                                 stemming and in this case the stammer                                 also does some character normalization                                 so we get this so we start from this and                                 this is what what we actually typically                                 would index so that's the output of the                                 French and Eliza so the processing model                                 for these analyzes is that they provide                                 a token stream and it can retrieve this                                 by calling a method token stream with a                                 field under reader and then this token                                 stream typically bundles together the                                 tokenizer and filters that that you need                                 to do the processing that the analyzer                                 is responsible for doing an input is                                 advanced by calling increment token you                                 can you can add information about tokens                                 using so-called token attributes when                                 you when you build the stream so there                                 are attributes for the term text that we                                 typically index we have offsets and                                 token types and Japanese we have a ton                                 of stuff and these token attributes are                                 updated on calls to increment token so                                 let's see how there's actually                                 works in code okay let's see so have an                                 analyzer an analyzer test case here that                                 basically prints the results of analysis                                 I'm not sure if you can read this all                                 that well but let me try to make this                                 bigger so have a test case here for each                                 of those language that we discussed                                 earlier English German French Arabic and                                 Japanese we are providing the text in                                 the example and we are applying a a they                                 just default in the scene the English                                 analyzer and the German analyzer French                                 analyzer right rabuck a loser and                                 Japanese analyzer here so there's a                                 analyzer printer that does the work so                                 let's have a look at what it does so                                 this print terms method it takes an                                 analyzer and some text to process and                                 then it creates a token stream using a                                 field here in this case that's just a                                 dummy feel that we don't we don't really                                 use but we need to provide that I will                                 just wrap this text that we want to                                 process in a reader then we reset the                                 stream before we can start consuming                                 tokens and we add a character term                                 attributes to this to the stream so we                                 get the term text and then we print it                                 out and we just call increment token                                 until the until the end of input and                                 then we print out the term attribute as                                 we as we go along so we can run this                                 we get output let's see if we can make                                 this readable so this is our english                                 text and these are tokens that are                                 searchable that we typically with index                                 and be searchable this is the output for                                 German so seeing that some character                                 normalization is going on here and also                                 some stemming stop worrying and so on so                                 is easy to use so that's the basic                                 analyzer API if you want to to look a                                 bit what one of these analyzers actually                                 does like this French analyzer here we                                 can open it up this is the leucine code                                 in                                                                  these tokens strip components that                                 string things together is typically are                                 there then the last method of these in                                 the in disc in these analyzers so here                                 we do some matching if we're all on the                                 after version                                                        tokenizer and we string together there                                 the filters that we want to use and                                 that's being that's being returned okay                                 so that's that moving on synonyms so                                 really commonly used technique to                                 improve recall so if you don't get                                 matches for things you want to have                                 matches for it is often good to add                                 synonyms for them think those are widely                                 used and so important that I'd like to                                 cover them particularly here there are                                 two types of synonyms in in the scene                                 there's a one-way synonyms and two-way                                 will wreak level it equivalents in the                                 nips so with a one-way case here a                                 search for sparkling and why would also                                 match champagne if you use this sort of                                 definition or two-way equivalents like                                 in this case that these AOC and this                                 appellation d'origine contrôlée those                                 are equivalent terms one match is the                                 other and the other matches the one                                 synonyms can be applied both index time                                 or query time so it apply them on on                                 either one of them don't deploy them on                                 those typically i recommend people to                                 apply them on the query side because you                                 can then update synonyms without having                                 to to reindex it also allows for easy                                 testing of synonyms it's possible also                                 to apply them on on the on the indexing                                 side it depends on if your queries are                                 very heavy and you're ok we indexing                                 maybe you should to do it on the query                                 sir but I think for most deployments we                                 won't do these things on there on the on                                 the query side and not the indexing side                                 so we can also have a quick look at how                                 this looks in code so here we're going                                 to make our own French and synonym                                 analyzer so here's our test code so                                 these are our synonyms yes we have a                                 synonym here this is just a bogus                                 synonym but this means that terms hello                                 and AAA are equivalent this is funny                                 okay so we have the two synonyms here                                 sparkling wine is should match champagne                                 and we have a appellation d'origine                                 contrôlée and and AOC should be they                                 should they should they should be                                 equivalent and then we have this synonym                                 analyzer here so this is our analyzer                                 code so this synonym analyzer takes a                                 reader with them synonyms and then it                                 initializes these synonyms and turns                                 them into a synonym map that is backed                                 by an FST and fast things that Mike                                 McConnell's has been working on and our                                 components is we have a standard                                 tokenizer we have a locus filter we have                                 a synonym filter and the French light                                 stemmer so this is a sort of simplified                                 version of the analyzer with that we                                 that we looked at earlier but it has                                 synonym support so let me just try to                                 run this oh one other thing we are                                 calling another matter than this                                 analyzer printer so we're calling a                                 matter that method that also prints more                                 details about tokens so previously we                                 have this charm term attribute but now                                 we're also adding offsets very input                                 this token starts and stops and we also                                 have a type attribute so let me just try                                 to run this                                 so here we have all our our tokens is                                 our original text and we have the tokens                                 here they are type type alpha                                          the mail phone um but then down here we                                 are seeing that we have synonym tokens                                 so these are palacio gets stem to a pail                                 and it starts at all set                                                have the region controller in stem forms                                 and we also have this aoc thing here and                                 it also starts at offset                                                at                                                                       ends so it means that if someone in this                                 case searches for aoc since we have this                                 offset information in a highlighted                                 results a search for aoc will actually                                 highlight a policy on the machine                                 controller if that was in the original                                 text and that's pretty pretty nice the                                 nice feature of of the scene okay so                                 i'll also give you a quick intro of how                                 you can do some in linguistics in                                 elastic search so just as a little                                 overview elastic uses lucy and analyzes                                 tokenizer sand filters so that all the                                 things in lucene are are generally                                 provided so and these analyzers are made                                 available through a provider interface                                 and i'm not sure if it's a hundred                                 percent complete and contains all the                                 things that has that is there in the                                 scene but if there's something that's                                 not there I think the elastic search                                 guys will fix that for you very quickly                                 some analyzers are well blast plugins                                 kuramochi is my Chinese analyzer and so                                 on and analyzer can be set up in your                                 mapping and i think i think the previous                                 talk also disgusts how analyzes can be                                 used with elastic search one useful                                 feature is that unless can also be                                 chosen based on a field in your document                                 such as a Langfield and that's what i'll                                 show you now                                 so we have a power mappings so we have a                                 we have documents with a title and a                                 body and there's a wiki tag and there's                                 a there's a there's a wiki feel then                                 there's a long field and here I've set                                 up that the analyzer chosen by                                 elasticsearch matches whatever this                                 whatever valid is Langfield has so if                                 this lang field is English like in this                                 document when we index this document                                 that English analyzer will be chosen                                 this the German analyzer will be chosen                                 and so on so Japanese support is                                 available through as a plug-in called                                 cool emoji and here I've added a                                      demonstrate how some of these analyzers                                 can be defined in the mappings I defined                                 and analyzed the Japanese here that just                                 uses the coup de moda tokenizer and                                 doesn't lower casing so let's try to                                 index some of these documents in                                 inelastic let's see here                                 let's see i have a list of commands here                                 we go so in case i have an index zero                                 already which i think i do i will delete                                 that and then now i'll create an index                                 with this with this mapping that we that                                 we just had a look at all right here we                                 go we have a new index now we can try to                                 analyze some french do it like this so                                 this is the comma learn to do to do that                                 we specify and I r and Eliza like this                                 in our texts and we get this sorted                                 results so these are available right off                                 the bat can also do Japanese I guess                                 it's all works beautifully we can then                                 post some documents so we're posting                                 some of the documents with the example                                 sentences that we that we talked about                                 earlier let's just post those and                                 they're searchable immediately so here                                 we can search Japanese for Shinjuku                                 using the Japanese analyzer and here we                                 go I get one match and we get too much                                 for this token mode don't show                                 highlighting here but yeah that's the                                 token that matches                                 so it's pretty straightforward to use                                 I'll also talk a little bit about how                                 some of these things works in solar                                 solar of course also uses leucine                                 analyzes to organizers and filters and                                 processing is defined by field type in                                 schema xml a different processing can be                                 applied on both the indexing and and                                 querying site if decide same can lastic                                 search and a really rich set of                                 predefined and ready to use her language                                 field types have been provided for you                                 so Robert Moore did an excellent job in                                 in                                                            definitions in so for example this is an                                 M this is the field up for French the                                 filter for rabuck you could just start                                 using these in infield definitions you                                 can just use the text and the score AR                                 type and you can can start working with                                 with the rapid content these are the                                 field times that are available out of                                 the box today in                                                      korean in place which will be good so                                 I'd like to get a little bit in depth on                                 how solar processes a data at indexing                                 time so this is a very very simple solar                                 architecture from a from a document                                 processing point of view so here we have                                 a document in XML format and we post                                 that to an update request handler that                                 receives the document in XML converts it                                 to a solar into a document and activates                                 an update chain so this update chain is                                 a chain of so-called update requests                                 processors that processes documents at a                                 time the whole document and this is a                                 useful plug-in point if we want to                                 manipulate things at the document level                                 so we can add fields and so on so for                                 example we process this first guy                                 processes document passes on to the next                                 guy and so on maybe this third guy can                                 have a look at the text in the title                                 field and the text in the bottle the                                 body field and then infer what language                                 this document is likely to be in and                                 then other to the document does as meta                                 information we're also do field mapping                                 based on what was found so for example a                                 document in Arabic could be mapped to a                                 field that has a text and the score a                                 our field type so that the correct                                 linguistic processing in lucene is                                 applied so we can we can do that using                                 update request processors but the                                 leucine processing it's important to                                 understand the difference between the                                 two because leucine processes these                                 things on a per field basis North on a                                 per document basis so releasing we don't                                 have we analyzes they don't know about                                 other fields so then ID is processed we                                 do not don't do much processing on ID we                                 do processing on title and so on body                                 and so on and then language perhaps we                                 don't do much processing there and then                                 all this stuff goes into the sea on the                                 query side we have a search handler that                                 handles the query and then we have                                 searched components and then we have                                 analysis chain we get some results back                                 perhaps that are both passed also                                 through research components before they                                 handed off by a search handler so these                                 are also useful plug-in points for                                 linguistics in in in solar so I got to                                 show you how some of the some of these                                 things that we just talk through how we                                 can use a title field on a body field in                                 solar to in                                 for the language of the content without                                 having that pre tagged and then search                                 using that content so all this stuff                                 will be available on github so you can                                 have a play if you'd like so here my                                 configuration would like to show you a                                 couple of things my schema so define                                 some field types here everything that                                 all fields that ends with it and                                 underscore AR does it has arabic a field                                 type and so on for German French                                 Japanese Norwegian and so on and in our                                 solar config and we have defined a an                                 app that request handler that how it                                 uses an update chain called multi                                 language that uses an update request                                 process called lang detect language then                                 to fire update processor factory there's                                 one that's called this and there's                                 another one for tikka but this one has                                 much better quality than watson tikka so                                 basically this guy looks at titles and                                 body in in the in the input documents                                 and then it populates a language field                                 based on what sort of languages guesses                                 this content to have and then it remaps                                 those fields so something title in                                 english becomes title underscore en and                                 so on so have documents in these                                 documents are wikipedia documents                                 thousand wikipedia documents in Arabic                                 German English French Japanese in a                                 widget so I've index these already but I                                 can just post them again here                                 so we can then see how this guy                                 haarahld's these these these are this                                 language detection let's see if asset we                                 can do a fascist query on wiki we can                                 see that then that will let's see here                                 we are thousand documentaries of these                                 types we have a field language here we                                 can do we can do faceting on that see                                 that in the Japanese we could be pedia                                 set with the thousand documents in all                                 of these look in all this edge of                                 documents we have                                                     are Japanese to are not perhaps oh yeah                                 or more but we can then see to find that                                 the documents here that are in the                                 Japanese Wikipedia and the faceting on                                 language so there are two documents here                                 that are in fact English and not                                 Japanese and we can see what those are                                 well and at which yet so there's one                                 document here that has some English text                                 some Japanese date looks English                                 reasonable to tie that as English then                                 we have software license open source                                 software license that's perhaps not so                                 useful from the Free Software Foundation                                 how that here so we have seen that this                                 this is just some example that this                                 guy's actually does a pretty good job of                                 analyzing or inferring language based on                                 the content so there's a I think Dave                                 advice recently ported a Python language                                 detector to java I think it's on leucine                                                                                                    that's something that that might make                                 its way into into two solar said                                 so just wrap up some challenges with the                                 multiple languages how do we take                                 languages accurately it's feasible to do                                 on the on the indexing side which is                                 hard to do on the query side because                                 their ambiguity so how do we deal with                                 on the query side then is usually best                                 to have that supplied if it's not supply                                 then we have these options but not                                 knowing the query term language is most                                 likely affect your overall ranking                                 negatively a little bit about the NLP                                 ecosystem is a company called basis                                 technology there are customer hours they                                 have really good NLP technology they                                 also support the compounding for a range                                 of of European languages that's                                 something that's missing in lucene today                                 but patches are welcome there's also                                 open NLP which offers a range of of NLP                                 support that's perhaps not only related                                 to such to do sentence segmentation to                                 do part-of-speech tagging to do named                                 entity recognition John text chunking so                                 if you're interested in these sort of                                 applications I recommend having a look                                 at open LLP so we also have a a tutorial                                 on that I'll just run it quickly so it                                 basically shows how to how you can do a                                 sentence segmentation and part of speech                                 tagging and it just outputs these these                                 things here so this text input text it                                 can segment this into sentences in a                                 good way these are the visual tokens in                                 each sentence for this sentence here it                                 extracts person names in English so                                 these are Bill Gates Paul Island already                                 identified as person levels by a                                 statistical model these are part of                                 speech tags with probabilities and so on                                 all that stuff will be on github if                                 you're interested in that so these are                                 other ecosystem options if you're                                 interested in doing NLP that are not                                 directly related to search the                                 our open source frameworks that I                                 recommend having a having a having a                                 look at just to wrap up getting language                                 right i think is a hard problem and                                 doing this right really helps search                                 quality so there's a wide range of                                 language support available out of the                                 box in you see in elastic search and                                 solar considerations need to be made on                                 the indexing side and query side on what                                 sort of features to use the scene                                 analyzes work on a per field level but                                 Solar's opted request processors work on                                 the document level Solarize                                 functionality to automatically detect                                 language this is also available for                                 elastic search is a plug-in but you                                 still need to do the do the language                                 detection showed up outside of elastic                                 set before submitting a documentary                                 elastic set I think that is by design                                 and I've heard some rumors that somebody                                 might that there there are some plans to                                 maybe expand that through the use of a                                 percolator interfacial but i'm not i'm                                 not completely sure talk to the elastic                                 search guys if you need that some                                 practical advice when working with                                 language is really important to sort of                                 have your users need and your and their                                 content needs in mind should also                                 understand the language that hand and                                 what their issues are if you have issues                                 will recall consider synonym stemming                                 compound segmentation for european                                 languages things like word limit to                                 filter phonetic matching so on issues                                 with precision consider using answers a                                 divorce consider improving content                                 quality perhaps search few fields if                                 some content is more important than                                 other content or language consider                                 boosting that some thank yous our                                 example code will be available here with                                 some some read me details thanks so much                                 you                                 you
YouTube URL: https://www.youtube.com/watch?v=QI0XEshXygo


