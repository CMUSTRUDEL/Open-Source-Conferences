Title: Berlin Buzzwords 2013: Jan Lehnard - The CouchDB Implementation #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	CouchDB is a fully-featured database server written in less then 50.000 lines of code.
We'll take a deep dive into the implementation of CouchDB's core features and look at the design principles that guided the developers.

CouchDB's internal architecture is a great showcase for how the different layers in a complex system can be designed. We'll learn what makes CouchDB CouchDB and how all the little details come together to make a compelling whole.

Erlang seems an odd implementation choice at first, but for what we need CouchDB to do, Erlang's strengths can play out nicely. We'll look at the strengths and weaknesses of the Erlang platform and how it affects CouchDB. And don't worry, you don't need to know any Erlang to follow this talk

Read more:
https://2013.berlinbuzzwords.de/sessions/couchdb-implementation

About Jan Lehnard:
https://2013.berlinbuzzwords.de/users/janl


Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi                               I me on my name is John Leonard                               from Berlin welcome to Berlin i hope you                               enjoying the city who here is not from                               Berlin right I hope this is a great day                               for you um oh also thank you I have a                               little bit of help with this whole event                               so it means a lot for you to just buy a                               ticket and be here and enjoy the show                                thank you for coming um um if you've got                                any questions there is a little bit of                                time for Q&A afterwards and i'll be                                around so we can chat while you're all                                here but if you think of a question                                after we've all gone home here's my                                email address feel free to email me I                                like receiving email and if you do the                                Twitter you can do follow me as well if                                you want I'm one of the core developers                                of Apache cardi B and i'll be talking to                                you about a couch TV implementation um                                meaning that will do a rather hardcore                                technical deep dive into what makes                                couch to be work and I hope to show you                                that catch to be incredibly clever in                                the way it does things and I hope you                                agree at the end but we'll see who here                                well who you know who has heard of                                couchdb probably everyone right right                                that wasn't all hands anyway and who's                                using CouchDB write a few that's cool                                who's looked into the internals one and                                a half two and a half yeah cool that's                                good so this should be good for everyone                                here um just to reiterate really quickly                                on why we should care that something                                like CouchDB exists um I'm showing you                                the new project motto motto I'll be                                you'll be one of the first people to                                hear this this brand new slogan that                                we've got and it's crazy revealing right                                CouchDB is database that replicates its                                deceptively simple and this we only want                                to convey a few things with this so                                couchdb is a database if you and I tea                                and you hear something look something                                says DB it's probably a database so                                that's kind of given but we do take data                                storage very seriously if you if you                                trust your data to couch to be a couch                                to be                                everything and can do to honor that                                trust and give you the data back                                whenever you asked for it it also does                                clever thanks to the data that you can                                find it in very efficient ways and do do                                clever things with it and then there's a                                bunch of features on couch maybe that                                are really really cool error that have                                certain advantages of other things or                                have interesting trade-offs but the one                                feature that everybody agrees on who                                work somehow should be or he uses                                couchdb is its replication feature so                                that's why we put it in the model                                ketchup it's a database that replicates                                and our replication can mean a lot of                                things to a lot of people in CouchDB                                land it means synchronization it means I                                can replicate a database to a second                                database and this could be on the same                                machine a different machine copy on the                                phone could be on the other end of the                                world can go back and forth and that is                                Carol jeebies true killer feature and to                                help to elaborate that a littler to make                                this a little bit more understandable                                visible is I prepared a quote for you                                that's also very exclusive think of                                couch to be as get for your data epic                                out for your application data see I                                screw up my own quote sorry so think of                                couch to be as good for your application                                data and get not in the way that you                                have a bunch of versions and you have                                the big merging algorithms and dipping                                and you can rewrite history and                                everything but get in the way that you                                can have data in in a location where you                                want to do stuff with it so say I                                develop a some new software on my                                computer over here I'm writing the                                software and I'm showing it over over to                                Steve and he clones the repo and he can                                look at my code and say God I found a                                bark i fixed it so he pushes or I pull                                the the fakes from him I keep working                                and he shows it to Mary and Mary then FX                                is two more bucks and I she pushes them                                 to me and eventually we're all happy                                 with the with work and then I push it to                                 github and our continuous integration                                 server sees oh there's a new change on                                 github so I maybe run I go and check                                 that out and run it through our                                 integration test suite and that all                                 passes so the CI services hey this is                                 good code i push it to production or a                                 staging environment and mark there's                                 good code the magic here is that not                                 that you can do all this crazy                                 would get but that you can have the data                                 locally way you want to do a specific                                 task with it and cut should be gives you                                 that but for your application data in a                                 more specific example                                 say we have an office in Berlin and you                                 have a little bit of like internal                                 office software that your your employees                                 use to manage customer data and things                                 are going to store stuff and couchdb                                 it's all like it's just a server in your                                 in your office or cool and things are so                                 successful because of this really great                                 internal tools that you've written that                                 you need to open in New York office over                                 there and maybe you you put the the                                 local server in Berlin on a public IP so                                 the New York people can access it and                                 then they complain was too slow because                                 it has to do whatever                                                 delay for every single request and user                                 interaction and really cumbersome to use                                 kind of like github on our end because                                 it's in California and maybe you upgrade                                 to a bigger line but then your internet                                 provider craps out for an hour and then                                 the people in New York can't work                                 anymore and it's like after hours here                                 after I was here so we can't get                                 technical sport coats all that sucks in                                 the people New York can't do any work                                 anymore either so that's all bad you can                                 put it in the cloud somewhere but then                                 you have to trust all the data to the                                 cloud which is a questionable thing to                                 do at this point eventually though you                                 realize oh I have a catch to be here I                                 can just have a full copy of my data in                                 the New York office and you set up                                 another server and the application runs                                 there and all the New York people can                                 access the data locally its fastest on                                 the local network gigabit and everything                                 and it can replicate the data to Berlin                                 so we can all work on the same data and                                 with there's an internet outage well we                                 don't really realize because we all have                                 access to the local data over here and                                 when the internet comes back up we can                                 replicate and go again and this works                                 really well as well so that you have to                                 open an office in Tokyo we love Tokyo                                 right and then you can set that up as                                 well and you can replicate things around                                 so that is one of the scenarios where                                 weathered catch to be replication is                                 really useful another one for example                                 you have a bunch of couches and the data                                 some data data center somewhere maybe                                 that's a big cluster that holds a bunch                                 of user data and then your users or                                 maybe have you have a website and you                                 have per user data on the website and                                 maybe you have a mobile app on iOS app                                 an Android app and you want to use the                                 same system and that's entirely possible                                 there's two                                 pieces of software that complement couch                                 to be in that whole scenario the one is                                 pouch TB the other one is touch TB we                                 are very creative with names I know but                                 it mean people call it the star ouch                                 family of databases but we don't like                                 that pouch TB is implementing the whole                                 replication protocol which I'll explain                                 later in JavaScript for the browser and                                 it may sound crazy but if you ever used                                 a web app in the subway in Berlin or on                                 the plane with no Wi-Fi you kind of like                                 you kind of know why it would be useful                                 to have a local fully available local                                 replicating data store in your browser                                 so approach to be is that and allows you                                 to build fully offline web apps they                                 replicate to a big cluster in the back                                 end and touch TB it's the same thing                                 again yet again written or it is it's a                                 different implementation yet again and                                 written in objective-c and Java                                 respectively for iOS and Android so you                                 can have the same scenario and you're in                                 mobile apps if you want so this is quite                                 cool so this is what this is the cool                                  that couch to be does and there's                                 other stuff is a document storage                                   users jason for storage HTTP for                                 transport interesting indexing solutions                                 and everything but this is this is the                                 cool stuff when this just as an intro                                 for four people maybe haven't looked too                                 deep into a couch um now let's jump into                                 the technical bit if we look at any                                 database just pick any database and then                                 we make a cut through it and look at the                                 layers in the database every database                                 any database looks like this and may                                 actually look like this that's operating                                 the operating system somewhere down here                                 there's a file system which still part                                 of the operating system and we're                                 finally getting to the database part                                 over here where as file system access                                 and on top of that are the core data                                 structures that power everything and on                                 top of that of the core features that do                                 things with the core data structures to                                 provide something to the user or provide                                 the value and then on top of it is the                                 API that makes makes every                                          gives you access to these features on                                 top of the data structures and                                 everything so this is every database                                 which me and you may know why I'm into                                 databases I love this cake                                 so this is my daily bread and butter                                 it's cake I like it sorry and couch to                                 be in particular is no different with                                 file system on top of that sets a module                                 called couch file then a module college                                 be tree and a bunch of modules couch dog                                 how to replicate how to introduce and                                 all the free core features of catch TV                                 sit on top of that and then yet the the                                 upper level is a big module called couch                                 httpd which is the the user facing API                                 implementation so and we'll be looking                                 at the middle three bed I'll start in                                 the middle jump down and up again                                 because that makes the most sense in my                                 narrative so far any questions I don't                                 think so I don't know who made that cake                                 um let's look at the core data                                 structures um couch abuse core data                                 structure is a b-plus three and if                                 you've if you're done CS computer                                 science you should you know probably you                                 know i'll be plus                                                       special form of the b-tree which is not                                 a binary tree but be tree it's very it's                                 a trick question in that kind of sense                                 be trees are very wide shallow sparse                                 and the plus variety means that it                                 actually has all the the using the usage                                 data only in leaf nodes of the bee tree                                 it's a generic picture of a tree that is                                 what we think of tree which is this is a                                 very actually two trees that look like                                 one and a half but a bee tree looks more                                 like this very thin in the beginning and                                 it spreads out really wide and all the                                 actual stuff is in the top leaves over                                 there I'll go into more details of that                                 so the this beat post tree is the core                                 data structure for most of the important                                 things for the most important things in                                 CouchDB at first and simplest this                                 employ also uh sorry one more thing the                                 b-tree obviously gives us an ordered                                 access to data so I i can look something                                 or i can find something in the bee tree                                 but also can ask for a range or the                                 order of stuff that's important here the                                 first thing that we implement with this                                 bee tree is and by ID index                                 kind of looks like this big table                                 basically and has a bunch of slots and                                 when we were writing any data for                                 example document G it goes over there if                                 we write document d it goes over there                                 and then we write document k it goes                                 over there so far so clear but in                                 reality of course we build the data                                 structure as we start out so it actually                                 looks like this we don't have empty                                 slots it's not a hash table so it's like                                 a mini tree over there DGK stored in                                 order and clustered together so this is                                 very basic if ever the sorry um the the                                 smallest unit of data and CouchDB is                                 called the document which is a JC                                 realized JSON object and every one of                                 those has an ID and this ID goes into                                 this by IDs index ok whenever you have                                 an idea of an object or document you can                                 look it up in the by D index very easy                                 there's a second index though that is in                                 the core data storage called by sequence                                 and it is designed to answer the                                 question what happened since and it                                 looks like this it has the same data                                 document dgn k but in a different order                                 more specifically in the order that they                                 were inserted into the database so the                                 index is actually the one two three                                 that's the index and then the value for                                 that for the value one well the index                                 entry one is doc G then for to doc d and                                 so on and what we can do with this is oh                                 what we do what this does is record the                                 history of the database since the                                 beginning of the database I know which                                 document was written up now in order of                                 the in order of its existence or                                 relative to the other one so ah it's                                 like a get lock or svn log for your                                 database and this is indexed which means                                 it's very efficient to look up data in                                 there so we can answer the question what                                 happened since the beginning of time for                                 the database and get a list of                                 everything that happened but we can also                                 say hey what happened after three so                                 after two basically in this case so we                                 would get back just the row with a                                 number three over here and this works                                 over the entirety of the database which                                 is really nice and I'll show in a few                                 examples how this is the actual cool                                 core data structure of couchdb before I                                 do that                                 we do another jump down to the file                                 format it's so interesting the file form                                 is also afternoon your minds are totally                                 full with other cool technical stuff                                 that has been going on yet but bear with                                 me file format is actually very neat so                                 the setup here is that couchdb is the                                 database management system there's                                 multiple logical databases per system so                                 I have databases that have different                                 names or managed by the same server just                                 like every other database as well and                                 then each database the core storage of                                 the database is represented by a single                                 file in the file system and then if we                                 have secondary and tertiary indexes                                 these are stored in in different files                                 but the cord storage is single file and                                 the two indexes i just showed the by DN                                 by ID and the by sequence index are both                                 in the database file along with the day                                 that are interleaved and i'll show how                                 that looks in a second it is also an                                 append only file structure for various                                 reasons that are also showing a bit so                                 we'll only ever store stuff to the end                                 and CouchDB uses a multi version                                 concurrency control mechanism or mvcc to                                 allow or it uses that and a as a queue                                 of what's it called a serialized queue                                 of rights for the database so that                                 rights can happen at the same time that                                 reads can happen and we can have any                                 number of read request to the database                                 happen in parallel without rights ever                                 having to wait or the other way around                                 so it's a fully highly concurrent age                                 structure and we have full control over                                 F sync f sink is the magic thing that                                 makes database is secure so colonel or                                 ego yeah and what's it called a system                                 call and it by default care should be                                 opted to be very secure but if you don't                                 want to be as secure maybe speed up a                                 little you can relax the f sync                                 guarantees of CouchDB maybe maybe for an                                 index file that you want to write faster                                 but then it's okay if you lose it may be                                 so you can review can rebuild it quickly                                 so there's a bit of trade-off to be made                                 by default though all safe                                 and yeah since it has both indexes it                                 can answer two questions give me a                                 document for a given ID and answer what                                 happened since and everything else is                                 built on top of that each other let's                                 dive in let's have a look time                                 progresses from left to right is                                 indicated by this very fancy arrow we                                 created new database that creates a                                 header a header has a bunch of metadata                                 information the name of the database in                                 some such and pointers to the indexes                                 and currently these are nil pointers                                 because there's no indexes yet so we                                 created a database and not much happens                                 that's pretty cool there's also check                                 some in there that make sure that we                                 have a valid visa valid data structure                                 and then we start writing a document                                 document a i will just append it to the                                 file that's just the raw value of the                                 document gets stored there and then we                                 update the body index for the document a                                 and we restore that at the end of the                                 file and then we update the by sequence                                 index for documentation will store it at                                 the end of the file and then we write a                                 footer a footer is the same thing thing                                 as a header but it goes at the end has                                 the checksums and the pointers and now                                 these points actually point to the                                 indexes and which in turn point to the                                 document over here so and then things                                 get everything then committed to disk                                 and are safe and never touch it again if                                 we write some more we can add document                                 be will write it to the end of the file                                 behind the footer update the index for                                 be the by ID index for be updated by                                 sequence index for be and then write a                                 footer that has the updated pointers to                                 the updated trees and those then in turn                                 point to the previous versions of the or                                 the divergence down here which then in                                 turn point to the document a down there                                 and so this is a data structure that                                 always grows to the end of the file                                 never touches data that has been                                 committed to disk this is very important                                 for safety so this is the generic set up                                 and now you say while updating to                                 indexes for every single document update                                 sure seems expensive and you're sure                                 right how should be supposed to think of                                 bulk operations it does about Battle of                                 batching into bugs by default so a bunch                                 of rights going into the same database                                 actually will patch them for you and                                 make a book commit for you but you can                                 also send it back shows that you may                                 made yourself then it looks like that we                                 rewrite sorry we will write a batch of                                 document a and B and in one operation it                                 gets flushed appended to the disk we                                 update the index for a and B in one                                 operation it's more efficient than doing                                 it once and then again the byte sequence                                 index in one operation and finally                                 append the footer the situation is the                                 same as before basically but all the                                 intermediate steps are way more                                 efficient and of course the batches are                                 bigger and depending on your heart where                                 your memory your disk i/o and all of                                 that can never like patches of thousands                                 or ten thousands and these get committed                                 way faster than every single one of them                                 like and you create a tighter data                                 structures that are more memory                                 efficient and as good as catch to be can                                 it will do that for you I'll show you                                 one more thing hunter while we're here                                 how to delete the document and things                                 may not happen as you expect so we want                                 to delete document a so will will write                                 a new the new value of the document ace                                 now I'm deleted and then we update the                                 index a to no longer at the ID index for                                 a to no longer contain a and the by                                 sequence and next we write down that we                                 delete a document a over there and then                                 we write another footer so instead of                                 removing a piece of data we added four                                 more pieces of data how is that a delete                                 this is mark as deletion because like I                                 said we don't ever touch anything that                                 has been committed to disk how we deal                                 with actually removing stuff I'll show                                 you in a bit so this is this is roughly                                 how the code works and this has a few                                 consequences one it's very efficient                                 with storage consider a spinning disk                                 you all know this disks there's two                                 parts to spending to a hard drive                                 spinning disk in an arm that finds the                                 position on the disk to find the spot                                 where to write the data it's really hard                                 to make this and this really market so                                 and the more expensive operation in this                                 case is finding the position on the disk                                 it's something like for                                                  think in modern systems and then once we                                 found the spot we can write hundreds of                                 megabytes of data to the disk or read in                                 either direction it's really efficient                                 if we don't have to move the head if we                                 have to seek all over the place then                                 we'll be waiting ha tens of tens of                                 milliseconds per single request and your                                 CPU can do millions of cycles in the                                 same time so you're just wasting time                                 over here that's if you have a big I'll                                 wait that's where it comes from socal                                 should be doesn't do that it positions                                 the reap the head of the right head and                                 a certain position and then just keeps                                 flushing the data to disk so if you                                 write a lot of stuff to catch TB you                                 only ever have one seek and then ride                                 with near disk capacity right capacity                                 to the hardware so this is really cool                                 and if you're sitting here like while we                                 on spinning disks this so                                                this is dino get with the program that's                                 also cool if you know how they work at                                 least the current generation or the                                 current storage class due rights by                                 allocating buffers in this in the                                 storage layer and if you can allocate                                 bigger buffers in row that's more                                 efficient than having to allocate random                                 buffers all over the place so sequential                                 writes are even faster on SSDs than                                 random writes on SSDs of course they're                                 not they're still way faster than on                                 spending discs but the operational                                 consequence for the the data structure                                 we chose in the way we use it is that                                 it's both more efficient than not using                                 this kind of thing on both SSDs and                                 spinning disks so that's kind of cool                                 it's cash friendly in the sense that I                                 explained be trees like very shallow                                 very sparse and then it goes really wide                                 each node in the bee tree has about                                                                                                        millions of entries we have a street                                 depth of like four or five I keep                                 forgetting and even for billions it's                                 just six and the nodes while they have                                 sixteen sixteen hundred values in there                                 are tiny enough to fit in any ram                                 situations like even efficient on the                                 phone and that kind of scenario what                                 that means though is that your file your                                 operating system has a file system cache                                 that catches all the upper levels of the                                 b-tree so whenever we have to jump into                                 the b-tree to read or to write and we                                 can jump through the different nodes in                                 memory and then only maybe only for the                                 last jump we have to actually make a                                 secon this to get the data out so this                                 is very efficient again                                 pretty cool as well so and that's also                                 the reason why care should be has no                                 caching layer built in on the file                                 system layer because we the file system                                 cache is always smarter if you have read                                 the thing between from the guy who did                                 varnish on yl                                                            read can't be smarter than the file then                                 the operating system in this case at                                 least at this layer oh this is my                                 favorite one who use in two backups                                 everybody all the hands up everybody                                 cool so to back up a couchdb you need to                                 install this very complicated tool that                                 we've written for no no no to do to back                                 up a couch gb you have the dot couch                                 file which is the core data storage and                                 UCP it to your backup location and                                 that's it you don't have to stop your                                 database you just copy the raw file to                                 the backup and the operating system will                                 just walk through the file and copy it                                 to the new destination when it's done                                 it's over there so and that is pretty                                 neat and i'll show you i explain what                                 that works with a second bit data                                 operation is very safe in CouchDB and                                 already explained about three hundred                                 times that we're in a pet tyler pen                                 storage always goes to the end of the                                 file and you remember when we wrote the                                 to document and wrote the footer and                                 everything if we close the database and                                 open it again for reading later or if i                                 like ya later we would open the database                                 and read from the back instead from the                                 front and we would find the footer and                                 the footer has valid checksum so we                                 believe it's true and we can walk the                                 pointers to find our data and I now                                 consider we are in the midst of a                                 writing operation so we wrote the                                 document we updated one of the indexes                                 but not the other one and we didn't                                 write the footer and then hardware                                 failure power failure software failure                                 couch failure don't care something                                 treasures the we lose that operation                                 just fails and we have half written data                                 to the database so one we never                                 acknowledged to the client that that                                 data was safe on disk so we're good and                                 second when we restart CouchDB it will                                 open the database file the next time and                                 read from the back and we'll see will                                 read from the back and look for a four                                 and we know the footer is checksum so we                                 know what it looks like and we can                                 guarantee it's it's it's consistent and                                 when it finds like a half-written index                                 and then a bit document data and then a                                 footer it'll just take the footer that's                                 valid and discard the rest of it it will                                 not the half written data cannot know                                 how correct that is so it will discard                                 it or not just not consider it and then                                 we will start writing to the database                                 from the last committed footer going                                 forward this means catch DB has no                                 equivalent to a database check some file                                 because the database on right is                                 guaranteed to be correct as long as the                                 file system or cosmic race don't flip                                 the bits on the file system and then if                                 you something like ZFS then you even                                 have secure security against that and                                 catch me just check something on read so                                 at least you know when something's                                 broken on reading that happen outside of                                 the controller carriage to be so ya know                                 fix up face so if something crashes car                                 should be just restarts instantly and                                 that's one of the reasons why catch to                                 be doesn't have a shutdown procedure                                 there's no okay now shut down the server                                 what you do is just kill the pig like                                 UNIX has plenty of ways to destroy a                                 process and catch some sticky I just                                 expect that to have to happen at any                                 time and they're still guarantee safety                                 so stop couch gb kill the pit you're                                 good maybe don't kill this nine but kill                                 the pig I'll kill their sign is also                                 safe but that has other consequence will                                 just kill the pit and you're good so                                 it's very safe so um there's of course                                 one big trade up in a bunch of your                                 sitting like it that you when is he                                 going to say about this we can't have a                                 file that grows forever my disk will run                                 out of space on my god and I keep going                                 on a normal on the positives the one big                                 trader for this is that we have we need                                 to run the process that we call                                 compaction other databases call this                                 vacuum the same it's the same                                 consequence so same theoretical things                                 garbage collection will have data that                                 is no longer relevant in the database                                 file but since we never touched the                                 database file we can't we can't like a                                 selectively remove it so what we do                                 currently so yeah is we will walk the by                                 a sequence index and                                 copy out all the latest data into a new                                 database file and when that's done we'll                                 swap out the files so the database                                 starts using the new database file and                                 we'll throw away the old one there's a                                 stupid display of doing compaction but                                 it works really nice we're currently in                                 the process of rewriting that that the                                 copying phase is a lot smarter faster                                 less training on Io to make that a less                                 of a problem and we'll see how far that                                 goes there's other things that we're                                 implementing that that makes compaction                                 a little bit more localized as well so                                 you have not as much compaction going on                                 but until we know how that actually                                 plays out for people we have a mechanism                                 inside couchdb that that's your schedule                                 compaction that fits your use case                                 mostly I think the deep sorrow mmm we're                                 tracking what's it called fragmentation                                 so the ratio between actual data and                                 stuff that has to be has and you can say                                 hey run replication whenever I have more                                 than seventy percent fragmentation on                                 that database but don't do it between                                 eight and five because there's our peak                                 hours so catchy will wait until whatever                                 your main users are gone and you can do                                 the compaction afterwards that will work                                 you do not have to stop the database for                                 this so this is something that like the                                 backup just runs while the database is                                 running but of course it uses I oh so                                 that's the one big trade-off that we're                                 currently working on improving on but i                                 think the number of positive features on                                 the side make it well worth it so let's                                 look at a bunch of core features                                 replication indexing the changes feed                                 and compaction and compaction is an                                 technically a feature but it shows how                                 we're using the by sequence in next and                                 just to hammer this home all the cool                                 features are powered by the by sequence                                 index that's the actual cool thing and                                 CouchDB and the next slide i'm really                                 skipping because that actually got in                                 there so let's look at replication we                                 have a database down here we write a                                 bunch of changes and write a bunch of                                 documents in here and we're we will                                 refer to them by their update sequence                                 so it goes to from one to four and then                                 we create another database over here and                                 we'll replicate and sorry oh okay sorry                                 will replicate                                 will tell database be replicated from                                 database a and the database be will                                 check its own records hey I've ever                                 talked to database a like oh no and asks                                 database a hey every ever talked and                                 these I should know and then database                                 pieces okay give me everything since                                   so database a will send over                                           over to database be afterwards and then                                 replication is over afterwards we write                                 a few more changes to database a five                                 six seven and eight over here and then                                 database we tell database a hair                                 replicated from database a database be                                 will have remembered last time we talked                                 I was at four do you have anything more                                 give me everything since four and then                                 database a sends back five six seven and                                 eight so we have an incremental update                                 of the replication that's the                                 fundamental thing that happens over the                                 network there's a little bit more detail                                 to that which I will get to after this                                 bit but let's look at indexing next we                                 have a database a familiar scenario will                                 write a bunch of changes and you have an                                 index over here so maybe actually has a                                 mechanism called views which allows you                                 to do one dimensional secondary indexes                                 derive from the original data there's a                                 thing called geo couch which does                                 spatial or                                                            way there's a thing called CouchDB                                 leucine that integrates with that same                                 story and then there's a solar connector                                 that also works the same way so this                                 just works really nicely when lindex a                                 notice is hey I need to be updated and                                 I'm derived from database a it asks                                 database a hey last time we talked oh I                                 actually we never talk just give me                                 everything so database a will just send                                 out one two three over to the index and                                 then we do some more changes on database                                 a you get the idea right and then index                                 I needs to be updated again and it asks                                 database a hey and what happened after                                 four and then in database a sends all                                 this over this means a few things again                                 the incremental update to which is like                                 we don't have to look at all the data                                 against very efficient and to em what                                 was my point um last my point in the                                 middle of the thought                                 um give me a second oh yeah with                                 whenever we make changes to database a                                 we don't have to also update all the                                 secondary and tertiary indexes in the                                 same operation so write operations only                                 ever talk to the database and then                                 whenever we have to update an index that                                 happens lazily and if we had a lot of                                 changes they go to get all updated in                                 bulk taking advantage of the black                                 operations such held earlier so this is                                 way more efficient than doing it one by                                 one when writing this is also a couch to                                 be it's nice and fast third the changes                                 feed the changes we looks like this we                                 have a database we make a bunch of                                 changes and I can make an API call that                                 gives me the list of changes this is a                                 user facing feature there's no more to                                 this slide so everything I just showed                                 you you can use in your application for                                 for your own needs and purposes this is                                 really nice for things like activities                                 again changelogs any kind of activity                                 streams that kind of stuff and then                                 finally compaction and here i show a                                 little bit more detail we have a                                 database we write document a we write                                 document be document see no surprise                                 here but next we train we add a new                                 revision of document a and at this point                                 we no longer point to document a on                                 index number one so at this point the by                                 sequence index starts to and then moves                                 forward so each document is only counted                                 once in the by sequence index and then                                 we add another document day and another                                 document be and boom number two gets                                 great out as well so we started three                                 now and so on and if we have to do a                                 compaction we started in a new file over                                 here and we just walk the by sequence                                                                                                       database file that's smaller than the                                 previous one compaction is done if the                                 update of a for example was delete that                                 was knocked over here at the point of                                 compaction that data is actually removed                                 so this is how we get around the                                 particular thing so with this will jump                                 to another data structure that's also a                                 tree but not the bee tree and it's tiny                                 and the operational consequences are                                 insignificant for here but this is this                                 is the thing that makes the replication                                 magic work                                 and it initially didn't want to show                                 that but I thought it was a good thing                                 to do so i'll i'll try my best have a                                 nice white slide again and we'll start                                 with database eight will write a                                 document and this time we also show the                                 revision ID of the particular document                                 and revision ID start with an integer                                 that's incrementing as the revisions go                                 forward and then a hash of the document                                 and I abbreviate the hash with a over                                 here so the revision of document a at                                 this point is                                                    revision of the same document we're only                                 looking at a single document here the                                 new revision was called to be but we                                 also carry around the information that                                 the previous revision was                                               to be came after                                                   around the actual data that was in there                                 but the information that that revision                                 came before and we write a new revision                                 called                                                                and                                                               replicate this to database be we only                                 ever replicate the latest version of the                                 document version or revision potatoes                                 potatoes so this is just a pristine copy                                 of the same document with the same                                 history now on database be and we make a                                 change of database be because encouraged                                 to be you can write to any match up to                                 any database as if it were a master                                 database and will write version                                        has the same history as the previous                                 version of course and then we replicate                                 that back to database a and at that                                 point couchdb sees that the history of                                 the region                                                            the previous version here so we can in                                 get terms fast forward this document to                                 the next one and all as well so and this                                 is the fundamental procedure i just                                 showed only two nodes here but this can                                 be any number of databases and they'll                                 all work in the same way and then the                                 history tracking all makes it work                                 nicely but like so this is a list i said                                 revision free show me the tree the tree                                 is interesting when we have to deal with                                 conflicts and a conflict is a state of                                 data that is natural in a distributed                                 system if you write to multiple                                 locations data can be in conflict end of                                 story and there's anything bad to it you                                 just need to know how to deal with it                                 and i'll show you how casually deals                                 with it blank slate again                                 database over here we Mike our three                                 edits to document a same situation we                                 replicated over to database be this is                                 same as before so again                                                 over here we'll make a change still the                                 same but then database a also we make                                 also a change and we call it for k                                 instead of                                                            through editor it's both the full three                                 vision on each note and either node but                                 it's a different one make a different                                 change in both sides so no logically                                 this document is in conflict but the                                 nodes don't know anything about this                                 until we replicate back from B to a or                                 the other way around but I show back                                 from B to a next step here is catch to                                 be detects that these two documents are                                 now in conflict and keeps around this                                 thing the history tree for                                              in the history tree for                                                so this is this is where it's actually a                                 tree now we have two heads in that tree                                 or two branches in that tree and                                 couchley be a smart enough to see that                                 the history is mostly the same so                                 actually it stores this so it knows only                                 the region's                                                           and now Koch gb tells us this document                                 is in conflict by I updating the JSON                                 structure in there it does a few more                                 magic things but the next step that                                 we're going to do as users is resolve                                 the conflict like in version control                                 when you have the big equal markers that                                 are guaranteed to not compile that you                                 need to resolve this before checking                                 code back in or your friends and                                 coworkers are going to be mad at you so                                 there's three scenarios how we get out                                 of here first we say yeah that thing is                                 actually cool remove the brackets so                                    it's actually the revision we want                                       say this is a previous version we'll                                 just keep it in the history but we don't                                 have to have the values anymore so we'll                                 resolve this to                                                          say for DS actually what we want and it                                 should supersede                                                       can write that so one of those or we                                 take both                                                        application specific logic merge the two                                 files and as you know from merging                                 source code this isn't at a certain                                 point no longer possible to do this                                 automatically it can be                                 very clever freeway emerges and                                 everything but at a certain point a                                 human has to decide which data is                                 correct so in this case we can do a                                 merge or even even if the computer                                 decides what to merge actually that                                 works too we can create a new revision                                                                                                        for both                                                              there if we replicate the last one one                                 of one of the last ones back to be couch                                 we will see                                                          these logical child or successor to the                                 last version i have so i can fast                                 forward as well so this conflict is now                                 resolved and can be replicated into the                                 whole system moving forward this again                                 uses two nodes but you can have any                                 number of nodes and we can have any                                 number of conflicts within per document                                 and that tree thats like great thing                                 here or the one on top of it will just                                 have more entries in the brackets or                                 more brackets depending how deep the                                 conflicts go this is modeled as a tree                                 so when you start resolving it you could                                 just start on one and it basically doing                                 in fix pre-fit I think it's prefix                                 prefix traversal of the conflict tree of                                 the revision tree and then you can                                 resolve all the conflicts eventually so                                 this is a deterministic problem solution                                 possible that's what makes the                                 replication magic work have like six                                 more minutes I think briefly KGB's                                 written in Erlang who he writes Erlang                                 all right a few that's cool but that's                                 part of the Father problem we picked                                  early early on because CouchDB had a                                  very small development team the number                                  of developers of most of couchdb is one                                  one guy wrote most of CouchDB back in                                  the day and it shows Erlang because it                                  allowed him as a single person to write                                  a fully capable database system at that                                  point at alone around                                                  code so obviously is very smart too                                  erling is a functional programming                                  language that means code tends to be a                                  little bitter while still being very                                  expressive it's very efficient to use in                                  small teams they're still around                                                                                                                tiny for database current code count is                                  about                                                                  tiny for a fully capable database that                                  has this and many more features learning                                  has a few more other things that are                                  pretty cool that you may that have made                                  their ways into other programming                                  languages it implements an actor model                                  before there was an actor model so                                  there's little tiny processes that run                                  independently that has a few                                  consequences again one of this an error                                  in one of the processes and none of the                                  other processes are affected there's a                                  thing called the supervision tree which                                  connects all the processes in a way that                                  you define and one supervises the other                                  so if you tell someone to do a job that                                  will actually say okay i'll i'll do the                                  job and then will actually tell somebody                                  else do the job and when that fails and                                  that gets reported and if it's an error                                  that i will just try again and ask                                  another person to do the job then that                                  succeeds we'll refer back to the caller                                  but if the arrow over here is one that                                  we can't recover from at this point will                                  just fail as well and report the arabic                                  this is kind of very efficient                                  management structure for for building                                  fault tolerant systems it's really cool                                  and then because everything runs in                                  independent little processes the online                                  virtual machine can spread that whole                                  computation load over as many nodes are                                  CPUs and in course as your system has                                  this is really neat as well so I think                                  the current limit is a couple hundred                                                                                                        will wear you if you add a core will                                  just get so many percent faster linearly                                  and then its implementation details in                                  the vm that make that go away but if you                                  have machines with more than                                             then it's okay if that's a limit for now                                  but the vm team keeps working on this                                  the nice thing is the language and the                                  implementation doesn't have to change so                                  your code just this is the same whether                                  runs on the single core two chords                                     cores                                                                    faster magically actually magically and                                  it has a portable runtime so it runs on                                  all the UNIX is the windows is the max                                  it even runs on phones if you want to                                  embedded systems like the Raspberry Pi                                  which is pretty cool and the downsides                                  of Erlang sadly it's kind of hard to                                  recruit for you just saw the few hands                                  here this is a rather specialized                                  audience so I'm kind of hard to get                                  people for it it's a bit if you're new                                  to it's kind of hard                                  the tents if you've been infected with                                  Java first it's kind of hard to                                  get your head out of this sorry Java                                  people there's other as well                                  that gets you hard that's kind of like                                  the big one it's a bit of an operational                                  black box and this is actually like a                                  like a what do you call it it's not a                                  it's actually not bad there's a famous                                  story you may have heard from the                                  British Telecom that made a study on how                                  good their Erlang routers were working                                  and they came up with this magic number                                  of nine nines of availability meaning                                  the servers were only down less than a                                  second per year which means they were                                  never down and that led to things like                                  the operational engineers never having                                  to fix anything which led to never                                  learning how to fix anything so you're                                  in a bit of a chicken egg situation here                                  you want to be prepared for when                                  something is wrong but you never get to                                  train because never any nothing goes                                  ever wrong so they have to inject random                                  failures into the system to actually                                  learn troubleshooting which is an                                  interesting management problem and yeah                                  that's about it there's a few things we                                  can do better in CouchDB but i'll spare                                  you the details and leave you for                                  questions if you want so far thank you                                  very much this it                                  you                                  you
YouTube URL: https://www.youtube.com/watch?v=edbi9jJZkpg


