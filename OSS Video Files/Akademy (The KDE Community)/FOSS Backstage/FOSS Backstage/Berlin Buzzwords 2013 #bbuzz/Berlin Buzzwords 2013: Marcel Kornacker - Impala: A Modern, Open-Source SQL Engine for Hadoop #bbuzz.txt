Title: Berlin Buzzwords 2013: Marcel Kornacker - Impala: A Modern, Open-Source SQL Engine for Hadoop #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	The Cloudera Impala project is pioneering the next generation of Hadoop capabilities: the convergence of fast SQL queries with the capacity, scalability, and flexibility of a Hadoop cluster. With Impala, the Hadoop community now has an open-sourced codebase that helps users query data stored in HDFS and Apache HBase in real time, using familiar SQL syntax. 

In contrast with other SQL-on-Hadoop initiatives, Impala's operations are fast enough to do interactively on native Hadoop data rather than in long-running batch jobs. Now you have the freedom to discover relationships and explore what-if scenarios on Big Data datasets. By taking advantage of Hadoop's infrastructure, Impala lets you avoid traditional data warehouse obstacles like rigid schema design and the cost of expensive ETL jobs.

This talk starts out with an overview of Impala from the user's perspective, followed by a presentation of Impala's architecture and implementation. It concludes with a summary of Impala's benefits when compared with Apache Hive, commercial MapReduce alternatives, and traditional data warehouse infrastructure.

Read more:
https://2013.berlinbuzzwords.de/sessions/impala-modern-open-source-sql-engine-hadoop

About Marcel Kornacker:
https://2013.berlinbuzzwords.de/users/marcelk

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi thanks for having me I will be                               talking about Impala and open-source                               modern sequel engine for Hadoop alright                               a little overview I'll start out with                               the goals why we created Impala and what                               Impala looks like from the users                               perspective and then I'll go over some                               performance benchmarks that we did                               internally and then talk about the                                Impala architecture and finish with a                                comparison against existing systems so                                impala was designed to be a                                general-purpose sequel engine running                                inside of Hadoop meaning that it's                                supposed to work for a variety of                                workloads so not just for ad-hoc                                analytical but also for transactional or                                single row workloads so if you have a                                storage manager that supports that                                meaning you could do single row lookups                                or range scans you would be able to use                                that use Impala against that you would                                also be able to use it for a workload                                that consists of a large number of                                smaller queries such as our typical of                                production environments that do use                                databases for instance for web serving a                                second goal was to create something that                                runs directly within Hadoop so unlike a                                number of other systems that have come                                up in the past ye or months this                                actually runs directly against the                                widely used for storage formats file                                formats that you find in Hadoop so it                                can read those natively it can talk                                natively to the popular Hadoop storage                                managers and I'll go into a little more                                detail on which those are and then it                                also runs on the same nodes that run the                                Hadoop processes so you do not need a                                second cluster in order to run Impala                                inside of your Hadoop environment a                                third goal from the right from the                                get-go was to have a high-performance                                system and to that end impala was                                written in C++ instead of Java so                                you don't incur the Java Runtime                                penalties in particular garbage                                collection it utilizes runtime code                                generation to get rid of execution                                overheads that you often find in                                interpretive query engines and it was                                written from the ground up so it does                                not reuse MapReduce and it also does not                                reuse existing open source existing open                                source equal engines such as postgres                                from a user's perspective Impala is a                                distributed service meaning you have                                demons running in the entire class that                                you talk to it via ODBC and jdbc and                                there's also CLI or a GUI which is                                called Hugh and so you can point your bi                                tools at it and they interact with it                                through odbc or JDBC when a query comes                                in it is distributed and runs on all the                                nodes and the system that have relevant                                data at the moment Impala is not fault                                tolerant meaning that if a query fails                                you have to rerun it it is not                                automatically restarted or pieces of the                                query are not saved to disk in order to                                integrate well with the hive sorry the                                Hadoop ecosystems we decided to utilize                                the hives metadata storage system which                                is called meta store so if you're                                already using hive in order to map your                                physical data onto logical relational                                tables you will be able to Ryu ttle eyes                                that and keep currying those tables in                                Impala as well and I mentioned before                                that empower supports popular storage                                format in particular those are a new                                color format called Parque and I'll talk                                a little more about that later there's                                also sequence files and RC file with                                various compression codecs and Avro data                                files and of course uncompressed and lzo                                compressed text files                                Impala is a sequel engine and the sequel                                support that you find an Impala is very                                similar to what hive supports at the                                moment so in essence it is sequel                                   minus correlated sub queries that means                                you have select project join you have                                aggregation there is a union statement                                and there is an order by clause which in                                this case requires a limit there is also                                the insert statement you can run bulk                                inserts by doing insert select and                                 there's also limited ddl support so you                                 can create tables through Impala add                                 partitions all the tables etc there are                                 some functional limitations in                                 particular right now there is no support                                 for UDS user-defined functions or                                 civilization deserialization objects                                 meaning you cannot have file formats be                                 pluggable impala right now is also                                 limited to flat relational tables so                                 there's no support at the moment for                                 arrays structs and nap valued columns in                                 the tables and an operational limitation                                 is that all joins are executed as in                                 memory hash joints which means in                                 practice that the right hand side table                                 or the right hand side input I should                                 say of a join is required to fit into                                 the aggregate memory of the machines on                                 which the joint is running                                 Impala supports both HDFS and HBase as                                 underlying storage managers and what I                                 just described in terms of functionality                                 applies to HDFS in particular with HBase                                 the support is a little different                                 because HBase gives you different                                 characteristics than HDFS in particular                                 HBase does allow you to do single row                                 lookups and short range scans so against                                 HBase tables you also have support for                                 its select statements bulk insert but                                 also single row insert or multiple row                                 insert through the value Clause if you                                 map your table your HBase table in a                                 particular way into your meta store                                 table you can use predicates in the                                 query that are formulated against the                                 column that map's the roki those are                                 turned into start and stop rolls so that                                 you can actually do single row lookups                                 as well as small range scans via Impala                                 against HBase if you have predicates on                                 other columns those are mapped into what                                 the HBase API calls single column value                                 filters and the goal is to push as much                                 of the filtering down into HBase in                                 order to minimize the amount of data                                 that needs to be retrieved through the                                 HBase API the mapping that Impala                                 requires right now is patterned after                                 what hive exposes through meta store and                                 a particular that means that all data                                 needs to be stored as scalars and an                                 ASCII which is a fairly severe                                 functional limitation and it performs                                 limitation and the Rope he needs to be                                 mapped to a single string valued column                                 in the metal store table those are                                 things that cause a lot of friction and                                 is not really particularly usable for                                 real-world applications so I'm just                                 going to describe what is on the road                                 map for the HBase integration this is                                 not supported right now but we're                                 planning on doing this in particular of                                 course we would like to support update                                 and delete state                                 and also you should have the ability to                                 create and declare composite Rocky's                                 that you can map into an arbitrary                                 number of columns in your meta store                                 table in particular columns that are                                 appropriately typed not necessarily                                 string columns but that express the the                                 actual type of the data you're mapping                                 and then of course you would like to use                                 structured data instead of simple                                 scalars in your HBase cells in order to                                 minimize the storage and the access                                 overhead mmm this basically concludes                                 the user view of Impala and I'll talk a                                 little bit about results of benchmarks                                 that we're an internally so in this case                                 this is a benchmark based on                                            from the T pcds the standard TBC                                 decision support benchmark mmm scaling                                 factor of one terabyte and we bucketed                                 it into the cruise into three categories                                 depending on how much data they access                                 if they access something like a month                                 worth of data we called it interactive                                 if it's several months we call it                                 typical of reports and then something                                 that scans all data we call deep                                 analytics this was executed on a cluster                                 of                                                                       each of which has                                                      the I hope you can see it the Impala and                                 hive numbers this is hive                                               I've                                                                     at the time and you can see that what we                                 label interactive you can actually run                                 interactively this is the the geometric                                 mean of all the queries in that bucket                                 and this is single user execution and                                 you end up somewhere in the                                        second range and you can see that the                                 performance gap to hive is fairly                                 dramatic and it keeps on going in the                                 other categories along those lines as                                 well in summary this is what we've                                 observed the speed ups that we've seen                                 and obviously really short running                                 queries are most heavily impacted by                                 hives very                                 a extensive start up overhead and                                 general processing overhead but even                                 long-running crews full table scans etc                                 show remarkable speed ups what I showed                                 you was single user performance numbers                                 here we also did multi-user benchmarks                                 where we ran the same queries the same                                 benchmark queries against the same data                                 set but now with multiple clients and                                 parallel you see the graphs here show                                 the response times again the geometric                                 means of the response times in the                                 various buckets and how they scale up                                 from                                                                    and as you would expect obviously as the                                 cluster saturates and you run things                                 with a high degree of concurrency                                 response times go up but as you can also                                 see that for instance the what we call                                 the deep analytics is even with                                    clients in parallel you end up with mean                                 response times in the                                                 that compares favorably still against                                 hive with just a single client right so                                 even a single client hive will still run                                 roughly three times as slowly as impala                                 with                                                               another view of multi-user performance                                 in this case we're plotting throughput                                 again multiple clients and at some point                                 you can see throughput go up and at some                                 point you're saturating the cluster at                                 which point throughput levels off and                                 stays constant now this is actually a                                 good result because that means that the                                 system doesn't waste work even as you                                 scale up and do more have more                                 concurrent users in the system you are                                 not you don't see throughput total                                 throughput go down this is concludes the                                 performance section and now I'll talk                                 more about the architecture itself so                                 impala shows up as two binaries one is                                 the Impala demon that I mentioned before                                 that does the actual lifting and curry                                 execution                                 and handles both client requests and                                 also all internal requests there's also                                 a thing called the state store of which                                 there's only one instance and which is                                 used by Impala to provide a name service                                 Impala itself internally is architected                                 much like standards MPP database system                                 so it is typical of what a parallel                                 database system does once the request                                 comes in it is handed off to a planner                                 the planner then takes this and turns it                                 into a collection of plan fragments                                 which are then handed off to a                                 coordinator that initiates execution on                                 all the remote nodes that have relevant                                 data during execution those notes                                 exchange data in particular they screen                                 intermediate results between them so                                 that you can materialize results as                                 quickly as possible meaning there is no                                 intermediate storage into HDFS of                                 intermediate results here an example of                                 a running system consisting of a cluster                                 of three nodes each of which contains                                 runs and hdf test data node and also                                 hbase.regionserver.requests on metadata                                 among other things that it gets from                                 hives meta store as well as the HDFS                                 name node the data nodes and the state                                 store once the planning is done it hands                                 it off to the coordinator the                                 coordinator and initiates execution in                                 parallel to all Impala demons that sit                                 next to relevant data and while the crew                                 is running the demons perform the scans                                 and do the data exchange and results are                                 then streamed back to the user                                 hmm I'll want to talk a little bit about                                 the career planning and what we call                                 actually the front end when i said                                 earlier on that Impala is written in c++                                 that was actually partially a lie the                                 front end which comprises the process                                 analyzer and the plan itself are written                                 in Java and the reason was that those                                 interact fairly extensively with the                                 rest of the Hadoop ecosystem in                                 particular they need to talk to the two                                 hives meta store and no surprise and all                                 that is available service policies that                                 are in job also there's no phone Starla                                 we experienced a two-phase process you                                 the first phase produces a single node                                 plan which is the typical left deep tree                                 of plant operators if you've ever used                                 my sequel or Oracle and run an explain                                 plan you will see basically the output                                 of that is similar to what comes out of                                 this one the second phase is the                                 partitioning phase which takes the                                 singer impress my friends for her the                                 goal of this conversation                                 the partitioning is special paralyzation                                 of all career operates currently the                                 Impala planner does not do join our                                 optimization so whatever you specify in                                 the front claws is the order in which                                 the joins are being executed we're                                 planning on changing that in the future                                 and adding a cost-based optimizer here's                                 an example of a query that does three                                 joins it joins two large tables at HDFS                                 resident with one small table that is                                 HBase resident applies some predicates                                 and then does a grouping followed by an                                 order by with a limit clause and here is                                 the plan that would come out of that                                 right you see the to join operators in                                 the middle fed by scans followed by an                                 aggregation followed by a top-end                                 operator                                 the second phase does the partitioning                                 of the single node plan into multiple                                 plant fragments and so the overall goals                                 are to maximize scan locality you really                                 want to read all data and locally and                                 process all data locally as much as                                 possible in order to minimize data                                 movement and data basically network                                 transmission overhead another goal is of                                 course to fully distribute all query                                 operators so in particular you want to                                 paralyze joins joins are paralyzed                                 either as broadcast joins in which case                                 the right hand side input of the join is                                 is broadcast to all nodes that take part                                 in any of the joint processing or it is                                 done as a partition join in which both                                 tables are partitioned on the join                                 columns and so that the right hand side                                 input is then partitioned among all of                                 the nodes that do the right hand ed that                                 through the joint processing the                                 decision between whether it does the                                 joins in one mode or the other is based                                 on the estimated cost of the data                                 movement and to that end Impala uses the                                 column statistics that have been added                                 to hive relatively recently it basically                                 use those to estimate the total cost of                                 the data transfer aggregation is also                                 paralyzed into a pre aggregation step                                 that is done at the point where the data                                 is first materialized followed by a                                 merge aggregation step that is also                                 distributed by repartitioning the output                                 of the first aggregation step on the                                 grouping columns the top n operators                                 also distributed by initially doing the                                 top n again at the point where the data                                 is first materialized followed by a                                 single final top and operation in a                                 single plan fragment                                 so any example going back to this                                 example this contained the first join                                 did join between two large tables the                                 second one joined the input with a small                                 table and then it contains an                                 aggregation on top m and this is                                 paralyzed and partitioned into these six                                 plan fragments so you can see first of                                 all that all of the scans are done in                                 their own separate plan fragment and all                                 of the maximize scan locality the first                                 joint is between two large tables so a                                 partition joint is more appropriate and                                 the outputs of the scans are repartition                                 Don the join columns which and get this                                 case our IDs the second join is between                                 is to a small table on the right hand                                 side this one is done as a broadcast                                 join so that the output of the HBase                                 scan is then broadcast to those joint                                 operators in the same node the output of                                 the second joint is then used for the                                 pre aggregation step which then is we                                 partitioned on the grouping column and                                 the merge aggregation is done in a                                 separate plan fragment followed by the                                 initial phase of the top n which is then                                 followed by the final phase which is the                                 final top n which also then sends back                                 the results to the client I mentioned                                 metadata handling before and so metadata                                 comes in from a variety of sources one                                 is of course hives meta store and which                                 contains all the logical metadata such                                 as table definitions columns about                                 things about users etc create table                                 parameters there's of course also the                                 HDFS name node which contains the                                 metadata about the physical data for a                                 particular directory it can tell you                                 which files exist and it can tell you                                 the block replicas of the blocks that                                 make up the files the data nodes                                 themselves contain also contain                                 interesting metadata namely the volume                                 IDs of the end                                 roblox which we use for improved io                                 scheduling unlike hive Impala caches                                 metadata we did not want hive basically                                 does synchronous metadata API calls meta                                 store API calls whenever a query runs                                 and of course that adds a lot of                                 overhead if you want to run things at                                 subsequent sub-second latency so we                                 decided not to do that in power and                                 instead cash all the metadata which                                 means you also need a cache invalidation                                 mechanism this right now is done                                 directly and through a command the                                 Refresh command we are planning on                                 changing that so that metadata is                                 distributed through the state store                                 component and which then would then also                                 be responsible for propagating updates                                 to the metadata now there's also edge                                 catalog which was not available at the                                 time that Impala was first started so                                 we're planning also planning on                                 supporting that standard the Impala                                 execution engine itself is written in                                 C++ as mentioned before and an explicit                                 goal was to design it from minimal                                 execution overhead so to that extent it                                 is using a optimized in-memory format                                 that in particular puts fixed with data                                 at fixed offsets so that if you have                                 operations on that data such as adding                                 two integer columns you would actually                                 be able to compile that into a single                                 edition into single an add instruction                                 without having to do any point out                                 resolution Impala also makes extensive                                 use of intrinsics and special CPU                                 instructions of which the most recent or                                 even looking a few years back new                                 processor models have quite a few in                                 particular SSE instructions are useful                                 for text processing or things like hash                                 value computation as for instance the                                 crc                                                                 performance story of Impala is runtime                                 code generation                                 and this happens at the level of the big                                 loops that you normally find in a                                 typical sequel execution engine so when                                 you look at the operator tree produced                                 by the planner each of these operators                                 typically operates on a batch of rows                                 and it loops over that badge and                                 performs a single operation or a number                                 of operations repeatedly and you know                                 almost all the processing is done in                                 those big loops and so the way it's done                                 typically and something like postgres                                 which uses an interpretive approach                                 incurs a lot of interpretive overhead                                 which in the form of function calls but                                 also something like switches on the data                                 types itself and all of those things                                 conspire basically to to obliterate the                                 performance of a modern CPU modern                                 pipeline CPU where a lot of the performs                                 is gained through instruction level                                 parallelism so to that end Impala uses                                 tool called llvm which is an open source                                 tool kit basically compiler toolkit to                                 do runtime code generation and generate                                 these big loops in a form that gets rid                                 of all unnecessary branches and all                                 function calls you basically end up with                                 something that is fully in line and                                 minimizes dead code and branches I                                 mentioned the state store before Impala                                 uses that it's a central component                                 Impala uses it as a name service right                                 now so all all Impala demons in a                                 running cluster know about each other by                                 propagating their membership information                                 through the state store the goal is to                                 use the state store as a general-purpose                                 distribution mechanism for all                                 interesting cluster state such as                                 metadata or anything that is scheduling                                 relevance the state store itself is soft                                 state meaning that it is not the final                                 store of record for any of the data that                                 is flying around the system so if it                                 goes away all the data is replicated all                                 the Impala demons if the state still                                 goes away the stir will keep running and                                 functioning because all the data is                                 replicated                                 and if you bring the state store back up                                 it reinitialize 'as itself from by                                 re-registering with the running and                                 power demons or having the Impala demons                                 reregister with itself it is scalable by                                 relying on pushing data and using                                 heartbeats to the Impala demons so that                                 it does not get overwhelmed in a large                                 cluster by the Impala demons hitting the                                 state store itself synchronously we                                 often get a question of why not use                                 zookeeper for this purpose we actually                                 looked at zookeeper and it turned out                                 that zookeeper is not particularly good                                 at what we really care about which was                                 the pub sub component in particular we                                 looked at the communication patterns and                                 it turned out that it would have                                 required n by n communication in our                                 case which would have been prohibitive                                 in very large clusters at the same time                                 zucchi but there's a number of things                                 that we don't care about such as                                 civilization and persistence so we ended                                 up deciding that zookeeper would add                                 more overhead and be more of a liability                                 and require more management then if we                                 simply added a very simple and                                 lightweight components such as a state                                 store this concludes the architecture                                 part and I want to talk about competing                                 systems so one is of course dremel                                 Dremel has become a very popular and                                 talked-about since the initial                                 publication in                                                        step back what is dremel a dremel really                                 contributed a colonist storage format                                 for data with a nested structure so                                 inside of Google everything is stored in                                 initially in protocol buffers and                                 protocol buffers naturally I let you                                 expose and express a nested structure so                                 that there was a need to come up with a                                 fully shredded format for a for nested                                 structures which was accomplished with                                 the column i/o format and dremel also                                 basically uses distributed scalar                                 aggregation algorithms on top of that                                 corner structure in order to get highly                                 scalable query execution                                 those were to a large extent actually                                 borrowed from the existing techniques                                 that have been invented for peril                                 databases so what are the equivalents                                 and Hadoop ecosystem there was at the                                 time and we talked about this there was                                 no particularly usable corner formats so                                 we created Park haein when I say we this                                 is actually a collaboration between                                 Twitter and cloud era so there's a new                                 columnar format called parquet which                                 I'll talk about in the next slide in a                                 little more detail and then of course                                 Impala which runs on parkway as well as                                 other file formats gives you the                                 distributed aggregation piece so the two                                 of them together are actually a superset                                 of what dremel provided as of the                                      publication I'm not talking about the                                 most recent versions of demo so a little                                 more about Park hey what is it it is a                                 columnar format that is a container                                 format designed as a container format                                 that is suitable for all serialization                                 formats so it is not tied to any                                 anything like Avro thrift or protocol                                 buffers that can be used for all of                                 those it is the successor to Trevin II                                 which the cutting worked on and as I                                 mentioned before it was co-designed by                                 Cloudera and twitter so twitter also                                 expressed interest in this so it ended                                 up we ended up collaborating on this and                                 it is open source and hosted on github                                 right now and there are actually a                                 there's a third company working on this                                 so the goal is to have parkade support                                 native parquet support across all four                                 of frameworks in Hadoop so in particular                                 it'll be natively supported for pig                                 MapReduce hive so pig and MapReduce is                                 being done by Twitter hive there is a                                 company in Paris called crit he'll                                 working on the hive                                                  Cloudera is working on the impala                                 integration and Twitter's also                                 interested in a cascading integration in                                 terms of features it is a fully shredded                                 format and in particular it uses the                                 notion of repetition and definition                                 levels similar to basically the same as                                 generals column a                                                       the nesting structure                                 you're nested relational data column                                 values are stored in their native types                                 so unlike our C file which does                                 everything integers and even as ASCII                                 this does everything in the native types                                 and also has support for extensible                                 value encodings so there is run length                                 encoding that is dictionary encoding etc                                 and also support for index pages for                                 fast lookups if you look at hive in                                 particular how is hive different from                                 Impala obviously hive is based on                                 MapReduce and very similar to Google's                                 Tenzing project hive breaks down a                                 sequel query into a number of MapReduce                                 jobs which comes with a host of features                                 the most notable of which is fault                                 tolerance obviously MapReduce gives hive                                 and sent a nice fault tolerance but it                                 also comes with a high price namely the                                 high latency and low throughput that you                                 normally get from using MapReduce by                                 having to store things right things and                                 read things back out from HDFS                                 repeatedly hive also includes a very                                 extensible architecture and it has a an                                 almost excessive layering architecture                                 inside which means that you can extend                                 it through UDF sensories but it also                                 means that it comes with extremely high                                 processing overhead itself Impala in                                 comparison is obviously does direct data                                 exchange doesn't contain fault currents                                 at the moment and has an execution                                 engine that was initially designed for                                 low runtime overhead so it cuts out as                                 much processing overhead and layering as                                 possible in particular it doesn't do any                                 function calls inside the main                                 processing itself so very shallow stacks                                 what I talked about so far was what                                 Impala is right now they're obviously                                 the question is what are we going to do                                 in the future one of the there's                                 obviously additional sequel that Impala                                 doesn't support that is desirable and                                 one of the most requested features is                                 support for udfs so that is definitely                                 some                                 thing on the roadmap there's also sequel                                 authorization and more ddl so beyond                                 additional alter table statements as                                 well as grant and revoke statements and                                 sequel style table and common level                                 permissions that users of relational                                 databases expect from something that                                 looks like a relational database system                                 there's also the desired have an order                                 by clause without a limit and analytic                                 and window functions of course so                                 basically the the rolling window                                 functions that are part of sequel                                      another thing that is also interesting                                 for a lot of users is support for                                 structured data types so having maps                                 arrays and structs inside of inside of                                 inside of individual columns I already                                 mentioned improved HBase supports and                                 they're also number of runtime                                 optimizations that we're going to be                                 working on such as straggler handling                                 and join our optimization I mentioned                                 that before obviously you want that to                                 be cost based and also improved cash                                 management right now when you're running                                 out of memory you are actually utilizing                                 the OS buffer cache and running out of                                 the OS buffer cache comes with a number                                 of overheads that are that we would like                                 to cut out compared to what a relational                                 database system does which contains its                                 own buffer cache and we're a basically a                                 page read quote unquote out of memory is                                 nothing but basically a pointer transfer                                 obviously you can't quite get that by                                 reading out of the OS buffer cache so                                 there's there things to be improved                                 there's also the desire to do data                                 colocation for improve joint performance                                 this is also a typical technique of                                 parallel database system where you                                 partition your data and in particular Co                                 partition tables that are joined                                 typically on the join columns and so                                 that all joins are transferred into                                 local joins i mentioned by the metadata                                 handling and particular distribution                                 through the state store and there's also                                 the goal to have better                                 sauce management in particular to be                                 able to run exploratory and production                                 workloads in the same cluster without                                 the exploratory workloads being able to                                 steal resources and impact the                                 production workloads this basically                                 concludes the talk Impala                                             been released about five weeks ago it is                                 available in for several prepackaged for                                 several operating system distros you                                 probably run one of them and impalas                                 also impalas open source and hosted on                                 github and there's a user list that is                                 actively monitored by the entire team so                                 if you're running it and have questions                                 you should address them to the impala                                 user list this concludes my talk alright                                 if you stay away
YouTube URL: https://www.youtube.com/watch?v=GgJKxrFgWIw


