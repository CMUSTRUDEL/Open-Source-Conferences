Title: Berlin Buzzwords 2013: Gary Dusbabek - Blueflood: Simple Metrics Processing #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Rackspace needed a metrics system that could ingest 30 million signals generated from the Cloud Monitoring system. It had to offer custom data retention levels and still be able to offer graphs to customers in real time.

How We Did It:
We created a distributed system of shared-nothing nodes that split the responsibilities of: ingesting data, processing rollups and servicing data points for reads.Depending on the need, nodes can be easily reconfigured to support all or some of those functions.

What You Will Learn 
- How putting the user experience first drove requirements for this system. 
- How we leveraged excess processing power on existing storage hardware so that no additional hardware was required for this service. 
- Techniques for scheduling rollups and still maintain numerical accuracy. 
- How we handle non-numerical data points. 
- How we utilized open-source technology (Apache Cassandra, Scribe, Thrift, and Node.js) to deliver relatively quickly.

Read more:
https://2013.berlinbuzzwords.de/sessions/blueflood-simple-metrics-processing

About Gary Dusbabek:
https://2013.berlinbuzzwords.de/users/gdusbabek

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              sound I don't there we go okay hi                               everybody welcome to this presentation                               my name is Gary deuce Babic I work for                               rackspace in San Antonio Texas currently                               I work on the cloud monitoring team we                               make a monitoring product that people                               can use to monitor external things or                               install the legit and monitor things                               inside their servers and I've worked on                                that project for a couple years now and                                before that I worked on Cassandra                                full-time I'm a project committer and                                quite familiar with the ins and outs of                                Cassandra so if you have Cassandra                                questions later you can you can hit me                                up for that but that's a little bit                                about me and what I do the outline that                                we're going to follow today is first I'm                                kind of kind of give you a description                                of what blue flood is and then I'm going                                to talk about why we built it I'm going                                to demonstrate or explain to you some of                                the primitives inside of blue flood and                                then kind of spill the guts of the                                project and we'll proceed that way but                                first things first you know this is an                                open source conference you know we've                                used a lot of open source to build a                                blue flood and along it was my goal and                                intention to make this open source and                                Rackspace is making it a little bit                                easier for us to do this we've been                                given permission to do it but we need to                                go through the code and take out some                                specific branding things and make some                                things a little bit more obvious and                                we're working on that so my goal my                                personal goal is that by the end of the                                month of june that we should have blue                                flood available for you to download off                                of github use some good documentation                                and hopefully it'll be be be ready by                                then are our goal you know lots of times                                when projects go from being closed                                source to open source you know they can                                they can turn out one way this isn't                                what you really want this is a this is                                bad some projects you've seen they just                                packaged their code up and kind of throw                                it over the wall and then kind of                                abandon it or forget about it and that's                                that's not what we want to do we want to                                spend some time make sure the                                documentation is good do what we can to                                foster community and I think it's going                                to work out better that way and so if at                                the end of the presentation any of you                                are kind of interested in learning more                                about this either sync up with me in                                person or on Twitter and we can can find                                a way to kind of a expose exposed boo                                fled to you so what is blue fled it's                                basically three things or at least it's                                supposed to do three things well the                                first thing is ingest metrics metrics                                come into the system it persists um and                                then we've taken the approach that we                                want to roll metrics up and this is for                                a couple of optimizations mainly along                                the read past so that we can guarantee a                                constant service level for users the                                last thing that it does is it allows                                users to query those metrics and so                                those are those are the three things it                                does and we've tried to make it a simple                                project so those are the only things                                that does we've tried not to to leak out                                too many things in this presentation                                you'll hear me use the word metric and                                signals kind of interchangeably so when                                you see a signal that's the same thing                                as the metric I might even slip up and                                say stream or something like that but                                they're all the all the same thing blue                                fled is written in Java mainly because                                the engineers that we had tasked to work                                on blue flood were that was their of                                their primary language on cloud                                monitoring we have it really is a                                polyglot system we try to pick the best                                language for the task we've got things                                written in node things written in lua                                things written in c and i think for this                                project java really was the the best                                choice mainly because of the libraries                                that are available to us for doing the                                concurrent things and that's mainly what                                we we ended up going with for storing                                the data we currently use cassandra the                                 reading and writing constructs within                                 the code are going to be easy enough to                                 extract interfaces out of so that other                                 backends can be used to store the data                                 and so you know whatever whatever you                                 want I think that we're going to be able                                 to do that I don't think that's going to                                 be a problem we use Cassandra mainly                                 because we have a lot of in-house                                 experience with Cassandra we know how to                                 how to make it do these                                 it turns out that Cassandra is a pretty                                 good time series database right out of                                 the box I'll get to some diagrams a                                 little bit later that'll explain that to                                 you and make that a little bit more                                 obvious some of the reasons why we built                                 this is our first in primary objective                                 was we wanted to have fast graphs for                                 the user the system is built to hold                                 data for many customers we didn't want                                 the volume of data for one customer to                                 impact the service level for another                                 customer and so we we optimized                                 everything so that we could make the                                 graphs as fast as possible already                                 talked about how we have multiple                                 tenants we wanted this to be kind of a                                 cheap system keep in mind that our                                 primary product is the monitoring                                 software it's not generating pretty                                 graphs or pictures or dashboards for                                 people and of course we want it to be                                 maintainable we have quite a bit of                                 servers for the project and we don't                                 want to burden our operations people so                                 I'll go kind of deeper into each of                                 these right now so fast graphs as i said                                 before our primary job isn't isn't to                                 generate dashboards and and graphs as                                 the cloud monitoring but for blue flood                                 that's that's its main thing and so we                                 have to be able to get at that data                                 quickly the the way that we do that is                                 we optimize the read path so that we                                 don't do some things as as often as                                 might be done other other ways we want                                 to return as few data points as possible                                 and along that same line we want to make                                 sure that as much as possible we                                 precompute things when you when you look                                 at I guess the big picture there's a                                 sweet spot in a graph you can only                                 contain it that any graph can only                                 contain so much information before it                                 just becomes too dense if you've ever                                 tried to put a hundred thousand or even                                 ten thousand points into a single graph                                 that might be                                                     quickly realize well I can't really see                                 all those data points you know and we've                                 found that in practice most the time you                                 can see between three and four hundred                                 distinct data points on a normal sized                                 graph that appears in our browser and so                                 we've optimized the long                                 lines we want to make a system that it's                                 easy to pull out for any given period of                                 time something that represents                                         to                                                                   then turn those into into graphs as I                                 said before you just can't fit much much                                 else into a graph supporting multiple                                 tenants Cassandra makes this pretty easy                                 for us because it gives us a per column                                 TTL and so different customers you know                                 depending on how long they want to                                 retain their data for can can pay to                                 have it kept for longer and this is a                                 you know breed pretty pretty                                 customizable for the most part you know                                 a standard time to live is used but for                                 those who want longer retention we've                                 made it easy and possible for them to do                                 that so the next step is well we wanted                                 something that was kind of cheap we                                 realized that we have this new medium                                 sized Cassandra cluster and it was it                                 was using a lot of disk but it wasn't                                 really using a lot of the CPU well pre                                 computing this data rolling it up is                                 very CPU intensive and really not not so                                 disk intensive until the end where it's                                 where it's written so we decided to go                                 ahead and co-locate are our blue flood                                 instances with the cassandra instances                                 and so we get a little bit of a bonus                                 there because the cassandra reads go to                                 local hosts or the cassandra reads and                                 writes go to local host for the                                 cassandra as the cassandra can                                 controller the rights may end up going                                 all around the cluster but that first                                 hop is always a local local host hop and                                 of course it's you know if local host                                 for whatever reason isn't available i                                 will try other nodes in the cassandra                                 cluster but this is this has worked well                                 for us we also theorized i'm pretty sure                                 that blue flood would also work good in                                 a like a hybrid cloud environment where                                 you host your cassandra on real hardware                                 and you host your blue flood instances                                 on virtual hardware because it's I mean                                 it's mainly you just need CPU and some                                 degree memory                                 as far as maintaining the the blue flood                                 cluster we wanted it to be something                                 that is we spin it up and then we kind                                 of forget about it let it just do its                                 job we didn't want it to have a lot of                                 tunable knobs that our admins have to                                 worry about or that we have to worry                                 about we did spend a lot of time making                                 sure that it's well instrumented so that                                 we can just pull things up in a jmx                                 console or no we're also getting the                                 metrics into stats d and and graphite we                                 wanted to be able to make sure that we                                 have a good view of what's going on                                 inside the system and so we've heavily                                 instrumented it quite a bit again we                                 just we don't want this to be a burden                                 for our operations people we want                                 something that that can easily be tuned                                 and it scales horizontally I'll get into                                 details about how it skills later on but                                 basically you just add servers make a                                 few small configuration changes and                                 you're on your way when it's time to do                                 a scale one of the other things I think                                 that helps with maintenance is as I said                                 before blue flood has three jobs ingest                                 roll-up inquiry we decided to build this                                 into one process and one code base and                                 so when you spin up a blue flood                                 instance you can just tweak its                                 configuration and it can it can do                                 either all of these tasks or some of                                 them or even one of them and that's fine                                 in our in our own cluster we have we                                 have a couple query slaves a couple                                 ingestion slaves and then the bulk of                                 them the ones that we've collect                                 co-located with the Cassandra nodes are                                 roll-up slaves only alright the concepts                                 so we ended up borrowing a lot of                                 vocabulary from other things and in some                                 cases it may not make sense but I'll do                                 my best to to try and and help you to                                 understand that the first thing is the                                 concept of a metric there are certain                                 things that attributes of a metric and                                 as I said the metrics also known as the                                 signal first thing above all is that a                                 metric obviously has some day                                 and then it's going to have a little bit                                 of metadata in for associated with it                                 the first thing is that it can have type                                 information associated with it we treat                                 integers differently than floats and we                                 treat strings and boolean's even                                 differently than we treat integers and                                 floats and those are the the basic types                                 of data that that booth but has if the                                 if the metadata or the type information                                 isn't sent with the metric when it comes                                 in it does its best to try and figure it                                 out and usually that's I mean pretty                                 obvious something else that can be                                 associated with the metric is unit                                 information this comes in handy on the                                 query end users really sometimes like to                                 know whether this thing that they're                                 querying represents bytes megabytes                                 milliseconds things like that again this                                 is a this is an optional piece of meta                                 information that can be included with                                 the metric something else about a metric                                 is it has to be uniquely identifiable it                                 has to be addressable because it ends up                                 getting stored in a single row in                                 Cassandra and we need to be able to look                                 that up and the way that we've we've                                 called that that primary key a locator                                 and so throughout the codebase you'll                                 see you know look the this concept of a                                 locator it's basically just a primary                                 key into the system and some things to                                 the system it's just an opaque string it                                 just treats it like a string but what we                                 do and what we would encourage anyone                                 else who uses blue flood to do is to                                 stuff all kinds of data in that string                                 as much data as you want as much as as                                 much as makes sense a good example this                                 is one that we you might see in our                                 system the first there's a tuple it's a                                 tuple separated by a colon the first                                 element it represents a tenant ID or a                                 customer or something like that the next                                 part represents a host and a disk in the                                 host and then some attribute on that                                 disk and so there might be multiple                                 attributes on the disk and then again                                 multiple disks and hosts and tenant you                                 get the idea it goes on and on so it's a                                 way for us to kind of and this is                                 probably a good pattern to practice                                 because it                                 just it makes your data easier to find                                 later on an example of how metrics look                                 as a row so we have the the key with a                                 locator and that goes in along with the                                 first column so each each unit of data                                 has a time stamp associated with it and                                 that is the time stamp the data was                                 generated which is different than the                                 time on the server or the time that it's                                 actually getting ridden this is the time                                 that the the generator that the data                                 came from whatever it was collected from                                 as we get more data in it just gets                                 appended again this is a you know simple                                 time series database concepts and if                                 you've done this kind of thing before                                 it's probably pretty pretty easy to                                 understand we've also broken a blue                                 flood cluster up into shards now shards                                 are simply just a way to break the                                 metric space up into parts that can be                                 processed individually every metric oh                                 yeah so you determine how many shards                                 you want to have call that n and then                                 every every metric hashes to one of                                 those shards and that's just a way of                                 knowing who is going to be responsible                                 for rolling up that metric later on our                                 current system uses                                                     coded in it may make sense to make this                                 configurable setting at some point                                 there's not a lot of penalty that you                                 would get by increasing your shards                                 later on you would have to kind of just                                 either destroy and let naturally                                 repopulate some state information or do                                 something to to repopulate it yourself                                 you know some kind of manual manual                                 trans transition but you should                                 generally pick something big enough you                                 know to where you think it might be the                                 maximum nodes you would ever have to                                 have processing your shards and so we've                                 got it set to                                                         thirty two nodes processing shards and                                 they're split into two data centers and                                 so each set of                                                        the shards and we have some mechanisms                                 that we'll talk about later that we make                                 sure that we use some locking constructs                                 to make sure that only one                                 no it is processing a shart at a given                                 time each node is going to own one or                                 more nodes so when I want to spoke about                                 our instance we have                                                   shards are divided up evenly on so each                                 one processes so many nodes shards                                 themselves can be owned by one or more                                 nodes you have to have at least one node                                 to own a chardon or else that those                                 those keys those locators will never                                 ever get rolled up but it's okay for a                                 shard to be owned by multiple nodes we                                 use a zookeeper to make sure that you                                 zookeeper to make sure that only one                                 note is processing it at at a given time                                 shards have nothing to do with query and                                 very little to do with metrics ingestion                                 so they mainly exist for the for the                                 sake of rolling the data up I think I'm                                 about to explain roll-ups but I first                                 talked about how we we use zookeeper to                                 kind of manage this zookeeper is                                 something that right now it's a it's                                 kind of optional there's no penalty                                 imposed by letting multiple nodes                                 process the same shards is just that                                 multiple work is going to be getting                                 done and why not save yourself the                                 hassle but zookeeper is kind of a pain                                 in the neck dependency I mean we've also                                 we've already got a dependency on                                 Cassandra which is kind of onerous I                                 have this dependency on zookeeper so                                 this is something i'd like to see go                                 away and i think with a little bit of                                 work that this will be this will                                 probably go away eventually like i said                                 before it's okay for either zookeeper to                                 fail or to not be there in the first                                 place there's a simple configuration                                 setting that says everyone just process                                 your shards pretend like there's no                                 zookeeper but if sue keeper is acting up                                 then it fills in a safe manner and                                 multiple nodes will process the same                                 shards and that's fine all the roll up                                 operations in blue flood our identity                                 and so we don't have to worry I mean you                                 can roll up the same range of time over                                 and over again and you're going to get                                 the same result unless new data is                                 coming in concept of granularity                                 granularity is a way that we use to                                 divide up time and this is something                                 that that it's hard to explain so I've                                 got a picture when data first arrives                                 arrives at full resolution it can arrive                                 spaced evenly in time but that doesn't                                 have to always be the case they can                                 arrive sporadically full resolution kind                                 of doesn't get divided up into slots but                                 what happens is as the roll-up slaves                                 start doing their processing the first                                 step is to process at five-minute                                 granule area so the granularity goes                                 from full                                                                minutes                                                        five-minute roll up we'll take all data                                 that's received in those five minutes                                 condense that down to one data point and                                 it will wait for the time that it has to                                 come and do that again now encoded in                                 these data points are a couple things we                                 include the count the max the min the                                 count max min the variance and the                                 average we thought about including some                                 more statistical information in there                                 and we reserve the right to do that in                                 the future the the data structure as I                                 get saved to the database is versioned                                 and so we can change it later on to add                                 more data if it proves useful but we                                 figured by storing those things that we                                 can do quite a bit of statistical                                 analysis on just that data and that's                                 that's pretty simple that's you know                                    data points and coat it up into these                                 little roll-ups so anyway we get to the                                 end and eventually it's time to start                                 creating                                                                happens is the                                                           composed of one two three four five                                 minute roll-ups we don't use the full                                 resolution data to to create that                                 there's nothing that says that we can't                                 and maybe that's a better idea that's                                 something we just have an experience                                 that experimented with and the                                           roll-ups worked like as you expect and                                 then the same thing with the one-hour                                 roll-ups three                                                        condensed into a one-hour roll up and so                                 inside that one data point you're going                                 to have the count of all these things                                 average of all those things a good                                 approximation of the variance of all                                 those things and also the men and the                                 max of all those things and also it get                                 the same process happens for four hour                                 and                                                                    arbitrarily chosen it may prove that                                 that we don't need some of them at some                                 point but these are the ones we're using                                 right now and it offers us enough                                 flexibility to to make sure that we can                                 have a consistent service level for for                                 our users as far as returning them the                                 right number of data points for their                                 queries so the concept of a slot so if                                 you can just imagine a two-week period                                 and it's divided up into five minute                                 chunks so innocent each chunk yeah as I                                 said is five minutes long so in a single                                 day you're going to have                                              and in that two-week period you're going                                 to have                                                                of those a number you know starting with                                 zero they end with                                                   those these slots reset every two weeks                                 so they go back to back to zero like                                 that I really like using keynote so this                                 same concept is repeated for the other                                 granule areas except there's just going                                 to be less slots for them because they                                 represent bigger swath of time so for                                 five minutes we have                                                     as time as the granularity gets more                                 course the number of slots in them gets                                 less and so finally it ends with                                    hours which obviously have                                            slots in them every time stamp which is                                 when a metric was received or recorded                                 hashes into one of these slots and so                                 then we can use that information to know                                 what slots are kind of dirty or what                                 slots are active so we know which time                                 periods that we then need to go and roll                                 up so as I said that's how we know if a                                 metric is active you keep track of that                                 slot in a data structure that gets                                 serialized into Cassandra that's a                                 little bit of a                                 eight and other nodes periodically pull                                 that out and update their state and it's                                 okay if it's a little bit out of sync                                 the worst thing that can happen is an                                 extra roll up or two happens and that's                                 totally fine with us we track these this                                 state information in a column family                                 that has a                                                             if we stop receiving a metric we're                                 going to realize that                                                    we'll stop rolling it up and so that's                                 that's convenient that's how we know the                                                                                                     anymore Cassandra makes that really                                 handy with tie it with TTL columns so                                 data coming into the system it I                                 probably already done a good job                                 discussing what attributes and metric                                 has basically those those things yeah                                 we've talked enough about collection                                 time typing unit as I said before are                                 just optional metrics arrive into the                                 system somehow our current                                 implementation uses thrift we're going                                 to ship it when we first release it open                                 source with an HTTP transport but it's                                 going to be flexible enough for you to                                 swap those out they pass through a                                 series of transforms like for us we get                                 the t the TTL information from like an                                 account API that goes into rack space                                 systems pulls the TTL out and then                                 augments the metrics of that they don't                                 arrive with the TTL and we do also some                                 other things to kind of guess what the                                 the units are we use a heuristic ille                                 approach where we based on what the                                 metrics name is we assume whether it's                                 like bites or milliseconds or hours or                                 something like that those that series of                                 transforms will be customizable or you                                 can implement your own so that you can                                 you can transform the data to the way                                 that you want before actually gets                                 written to Cassandra and then of course                                 it gets written to the to the full                                 resolution column family it also gets                                 written to the discovery database and                                 that's how we know what metrics are                                 active for a given shard so we know                                 which metrics to query when it's time to                                 roll up a sharp and then also I'm                                 I discussed the the slot state before                                 that's how we know that a specific slot                                 is dirty and is in need of being rolled                                 up ingestion is designed to be pluggable                                 we want it to be extensible so that you                                 can customize it to fit your needs and                                 so that's that's one of the things that                                 we're working hard to make sure that's                                 right before we actually release the                                 code and another thing that we're doing                                 is as we modify the code to make it open                                 source we're continually deploying this                                 in our production and staging                                 environment so we want to make sure that                                 we don't we actually open up the source                                 that we don't you know deliver a turd to                                 everybody we want something that we can                                 say hey we're using this in our in our                                 production environments this is                                 something that we trust with our data                                 you can trust it with your data too if                                 you're coder this is going to be pretty                                 easy just swapping your transport swap                                 in your transforms and you're good to go                                 or you can just live with the defaults                                 when we when we finally ship it I think                                 we're just going to have the HTTP as the                                 default and they'll be a default safe                                 set of ttls for all the data something                                 about ingestion is sometimes you've got                                 collectors that are just kind of light                                 to the to the game with their with their                                 data late data is okay up to that                                 two-week period where the slots reset                                 themselves if if data is later than that                                 we discard we actually in our system                                 discard it we don't allow data older                                 than                                                                     over                                                                 make more sense for you to write a short                                 little java program to use some of the                                 libraries that blue flood comes with two                                 bulk load the data yourself and in back                                 feel that way but um late data is okay                                 what it just means is it just marks that                                 slot dirty and the scheduling algorithm                                 picks up that slot and then just rerolls                                 it up and this time includes the updated                                 data and that trickles on up as the                                 granularity is get more course all the                                 way up to                                                             truly that late discuss that all right                                 the process of rolling up the data is a                                 little bit complicated                                 conceptually it's something I think                                 that's pretty simple but the actual code                                 that does it is some of the trickier                                 parts so when a slot hasn't been updated                                 for more than five minutes it gets                                 scheduled to be rolled up typically the                                 Delta between when it's scheduled when                                 it actually is rolled up is quite small                                 but if you're in a system where you know                                 you're constantly getting data that's                                 just a little bit late a couple seconds                                 late if new data arrives while that slot                                 was scheduled and it's for that same                                 slot the slot will become d scheduled                                 and we'll have to wait you know for it                                 it's a five-minute timeout period to                                 happen again until it can be rescheduled                                 so we try to be smart about it that way                                 yes so when we realize that a slot is is                                 five minutes old hasn't received any                                 data we select out all the locators or                                 the metrics that were updated during                                 that slot and we get that from the the                                 state database and then for each locator                                 we get all the data points that                                 correspond to that range that's again a                                 Cassandra read operation and then we do                                 the math like I said we can recompute                                 the count them in the max the variance                                 and then the average and those things                                 get serialize into a little object that                                 then gets written to Cassandra and then                                 this process gets repeated you know on                                 up the up the chain / granularity we                                 tried to something that it's tricky is                                 scheduling for example if I include this                                 example or not yes so we wouldn't want                                 to roll up a one hour slot or range if                                 all of its                                                            been computed yet because of course the                                 one hour roll up is computed from the                                    minute rolfe and so the scheduling                                 algorithm has to be smart enough to kind                                 of realize parent-child relationships                                 between slots and it does that and it it                                 works pretty well for that something                                 else as far as planning out what the                                 blue flood cluster does is we want to                                 make sure that it doesn't get behind so                                 roll-ups get scheduled every five min                                 if you find that you can't compete                                 complete all of your roll ups within                                 five minutes well then your cluster or                                 at least that particular node is under                                 provisioned which means it's probably                                 just processing too many shards the                                 thing to do there is just to take away                                 some of those shards from that server                                 and give them to another server and so                                 that's a pretty pretty simple operation                                 all right the query API this is a this                                 is something that probably will change                                 quite a bit in the future it's a it's a                                 simple part but it's a part that no no                                 it's it's pretty simple there's there's                                 a lot of room for improvement here to be                                 honest and that's why I think it will                                 there's going to be changes there's two                                 methods that we use to query the API the                                 first method is basically get by points                                 so we pass in the metric ID a time range                                 and then how many points you want so you                                 know if you're generating a graph that's                                 going to go on a little dashboard                                 webpage you'd pass in the the metric ID                                 that could be whatever it could be one                                 day it could be a year and then the                                 number of points that you want and so                                 you'd say I want                                                       points i want                                                         then the code then chooses the best fit                                 granularity to deliver you that many                                 points and you know depending on certain                                 factors it could be pretty close or it                                 could be it could be off because it just                                 tries to find the one that makes the the                                 best fit for what you've asked for but                                 sometimes it's it's a the best fit isn't                                 really that close to what you want the                                 other query method is the one that has                                 more control you just get everything by                                 resolution so you pass in some of the                                 same things the metric ID a range and                                 then a resolution and so this is the way                                 you have the most control but you run                                 the risk of of just returning too much                                 data points that's fine from a Cassandra                                 perspective it's just one simple read                                 operation but you know you'll find that                                 the more data points you feed to these                                 graphing libraries the longer that they                                 take to generate their graphs and users                                 in our experience don't tolerate slow                                 graphs                                 at all they just they get the heck out                                 of there so just you know watch out for                                 that that's something pretty pretty easy                                 as I said before we optimize the system                                 around fast reads and consistent reads                                 because our users they have to have a                                 guaranteed experience we're currently                                 using this in the in the rackspace                                 control panel if you have if you're                                 using claw Rock space and use cloud                                 monitoring either external checks or                                 agent checks you can see the graphs                                 generated by this system in your in your                                 control panel right now that's                                 everything that I have are there any                                 questions I'm happy to to answer any                                 questions you can throw sticks at me to                                 ask why you did that or or anything I'm                                 happy to hear anything is there some                                 sort to follow team or just poetry some                                 sort of what other thing alerting yeah                                 yeah cloud monitoring does all the                                 alerting and so I mean blue flight                                 itself exposes a number of metrics that                                 we could then feed into our alerting                                 system and keep in mind that metrics                                 reporting and alerting systems even                                 though they all are underneath cloud                                 monitoring are completely orthogonal so                                 if our monitoring system is broken we're                                 still going to be ingesting metrics and                                 so I mean it's sometimes when systems                                 are related you have a problem with one                                 dog foods off the other well if one                                 fails you don't know if it's which ones                                 causing it we would we we use a                                 different different tools for monitoring                                 we currently use graphite and we feed                                 our metrics in roofied log statements                                 into gray log we have it set so that                                 gray log if it returns so many types of                                 errors will get some kind of alert                                 nagios also gets a lot of those blue                                 flood metrics and can then send us                                 alerts so we've set thresholds that we                                 think are tolerable and again this is a                                 new system to us so we're kind of figure                                 out what's what's right as far as the                                 different metrics that we have inside                                 blueblood okay how do you handle delays                                 or data corruption in your input data                                 garbage in garbage out so if if the data                                 coming in is bad you know Lou flight is                                 it's a dumb system it doesn't know how                                 to judge the data whether it's good or                                 bad there is a certain level of                                 verification that has to take place                                 certain things that the metrics have to                                 have or else they kind of they get                                 booted out of the system we log that at                                 a warning or an error or something like                                 that we also have a concept although we                                 haven't implemented it yet of annotating                                 data streams and so for example what we                                 want to do is we want to have a concept                                 of system annotations and then                                 user-defined annotations and these                                 basically are just going to be string                                 metrics and so when we detect like bad                                 data coming in well we would generate a                                 system annotation there so that                                 hopefully you've got something that's                                 reading this and say oh no I'm                                 generating system annotations because I                                 changed the data type or something silly                                 like that which we do see a lot because                                 we have users who write their own agent                                 plugins to return data off of their                                 their their internal systems and we get                                 all kinds of stuff and so we try to we                                 try to do a lot of validation to make                                 sure that the data before it goes into                                 the database is correct and report some                                 kind of error if it's not                                 okay so maybe my question what the first                                 slot is not rode up because signals are                                 stealing coming and I'm queried for the                                 spot for the data I'm sorry could you                                 repeat that yeah okay I've got this law                                 that data signals for the slots are                                 stealing coming so understand it is not                                 rolled up but I want to query for the                                 values it so the question is well what                                 if the the query period is for a part of                                 time where the data hasn't been rolled                                 up yet we currently address this on the                                 curry level by automatically rolling                                 that up we don't write it to the                                 database but we just roll it up make the                                 computation and give it to the user you                                 said you're starting from zero every two                                 weeks yes are you doing that by rotating                                 column family kiss or something more                                 sophisticated so those shards wrapped                                    every two weeks and the main reason we                                 did this is that to limit the number of                                 rows that end up in the Indus Tate                                 tables we could have gone to some other                                 kind of hashing scheme where as time                                 goes forward the shard ID kind of                                 marches forward with it in in five                                 minute increments but we we just wanted                                 something that wraps so we would have a                                 predictable number of rows and the data                                 you know it just gets overwritten as a                                 as time wraps around I have a question                                 if you compare this system like from                                 performance point of view or resources                                 to some analytical database is some                                 columnstore elastic surge or something                                 else like what what's the difference                                 there thanks difference as far as                                 performance the general use of resources                                 like if you were to store this as an                                 elastic surgeon queried real time don't                                 pre computed like is there any benefit                                 there we haven't done any like                                 comparisons between other other storage                                 systems we we kind of had an idea of                                 what we wanted to build when we when we                                 started out there there weren't a lot of                                 things that were off the shelf ready and                                 did the kinds of things                                 we wanted and so there wasn't a lot to                                 compare to at the time we started this                                 project i'd say probably last last june                                 and so that's to be honest we haven't                                 done a lot of comparisons we're                                 currently pushing on our on our                                         Cassandra cluster i believe it was seven                                 million cassandra operations I don't                                 want to lie it was either permitted or                                 per hour but it's I mean it was a it was                                 quite a bit of load on our Cassandra                                 system our Cassandra cluster which was                                 handling that fine and so we're doing                                 quite a bit of work our goal was for                                 this system to be able to handle                                    million metrics are                                                    per minute and so that's that's what we                                 built for and I'm pretty sure that it's                                 going to be able to handle that and more                                 everything that we've done in our                                 staging testing indicates that we're not                                 going any problem scaling to that level                                 yeah okay did you consider it and try to                                 calculate all these mods on the flight                                 when signals no coming but understand do                                 you do that about computing but maybe                                 will be also work of you know                                 calculating among the flight and have                                 all these values in place immediately                                 yes we did consider doing all the                                 computations on the fly but we would run                                 into the problem that we couldn't                                 guarantee how fast those computations                                 were going to take some queries might be                                 for just a you know three hours worth of                                 data and which would be a really quick                                 computation and some queries might be                                 for two years worth of data which would                                 be you know computation that would that                                 would take more time and so that's why                                 we opted for taking the roll-up approach                                 was to guarantee a constant predictable                                 read death do you have any plans for                                 federating these clusters to support                                 higher right loads I'm sorry could you                                 repeat that so if i wanted to write more                                 than                                                                  how could i do that right what you would                                 end up doing is bumping the shard count                                 to some bigger number you know for                                 probably however many nodes you plan on                                 having say                                     or                                                                  shards to to your nodes so maybe if you                                 wanted to start with an eight node                                 cluster with                                                                                                                                  you give per node and then as those                                 nodes start to as you realize that those                                 nodes are taking too long to process                                 their shards you would add note add more                                 nodes and then give some of the shards                                 to them so it it scales along the number                                 of shards I wanted one cluster if I                                 wanted one cluster in Europe and one in                                 the United States is that somehow                                 possible or do you plan for that to be                                 possible that is possible but right now                                 the problem with that would be probably                                 with zookeeper zookeeper doesn't                                 tolerate are we have our zookeeper I                                 believe it lives in one data center                                 right now and it's just the lock latency                                 of like going from locks in London to                                 Chicago or wherever our zoo keeper is it                                 it's just sometimes we have crappy                                 results so probably once we get the                                 zookeeper locks away that will be able                                 to do something like that cross                                 continent we currently have the the                                 cluster split out between Chicago and                                 Dallas and that's fine we haven't                                 haven't seen any problem with that and                                 everybody any further questions                                 you
YouTube URL: https://www.youtube.com/watch?v=pCAFX2SvMvs


