Title: Berlin Buzzwords 2013: Kris Geusebroek - Mapreduce a Neo store: Creating large Neo4j Databases ...
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Full title: "I Mapreduced a Neo store: Creating large Neo4j Databases with Hadoop"

When exploring very large raw datasets containing massive interconnected networks, it is sometimes helpful to extract your data, or a subset thereof, into a graph database like Neo4j. This allows you to easily explore and visualize networked data to discover meaningful patterns.

When your graph has 100M+ nodes and 1000M+ edges, using the regular Neo4j import tools will make the import very time-intensive (as in many hours to days).

In this talk, I'll show you how we used Hadoop to scale the creation of very large Neo4j databases by distributing the load across a cluster and how we solved problems like creating sequential row ids and position-dependent records using a distributed framework like Hadoop.

Read more:
https://2013.berlinbuzzwords.de/sessions/i-mapreduced-neo-store-creating-large-neo4j-databases-hadoop

About Kris Geusebroek:
https://2013.berlinbuzzwords.de/users/krisgeus

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi I'm Chris Chris Heisey Brooke from                               the Netherlands and I am I going to talk                               about how we created a large neo                                 database with the help of our cluster                               the agenda is looking like this I'm                               going to tell you what we did why we did                               it especially how we did it and a little                               bit at the end who did it and who I am                               so I'm starting with the two                                technologies we used use Hadoop and                                neo                                                                  than Nia Jay this this conference assume                                we all know what I stole the Wikipedia                                definition to get a little picture of                                what the technologies were so we use                                Hadoop HDFS and the MapReduce frameworks                                and we use neo                                                        create pretty pictures like the ones                                below and the use case is that we wanted                                to create large new                                                    that multiple times so it's not a                                one-time exercise where you basically                                import your initial data and then add a                                few notes in a few edges but we wanted                                to look at the data differently from                                time to time and recreate the complete                                database the test setup we worked with                                was a near forge a database with                                   million notes and                                                  between them notes had                                                  each edge for properties and that all                                was thought in the year for J for query                                you might want to know why we did that                                we we used neo                                            visualizations because Excel does a                                lousy job there with with large                                databases and we wanted to use that data                                in New                                                        exploration have a look at one person in                                the graph and see it was a financial                                network see where the money went from                                that person how many steps were between                                this person a and person B and we needed                                to have not one way of looking at the                                data about multiple ways and because we                                at one time we wanted to look at it on a                                transaction basis the other one the                                other time on an aggregate basis so we                                wanted to recreate the database a lot so                                we start with the database do some                                exploration and then throw it away and                                use another database with different data                                and have some exploration there and you                                have multiple ways of creating a new for                                a database you can use the Java API the                                REST API you can use cipher that's kind                                of SQL for graph databases to create                                nodes and edges you also have a better                                import functionality but that comes with                                with the new fridge a database and you                                have my way of doing it and that's what                                I'm explaining to you now first the more                                normal ways of creating a database is                                just adding nodes and edges one at a                                time but new for Jay being transactional                                that has a lot of transaction overhead                                every note you you insert and with the                                                                                                     there was a lot of overhead so that's                                not a good way to do it the batch                                importer                                there they have left out the                                transactional part so the overhead is                                less so that was a good way to start so                                first we started with the normal bet                                importer you write some code like this                                creating creating a note with a map of                                properties that you have read from from                                the input text file CSV tab-separated                                something like that when we created that                                large database it took us about                                         waiting time on our desktop machines so                                the first improvement we did was we had                                a new cluster so we stole one of the                                data notes and use that one for creating                                database and that improved it took                                   hours or so on that machine the problem                                was only that you have to get it to the                                machine one year for j is running and                                that took another two hours but since we                                had a Hadoop cluster we had a Hadoop                                cluster of                                                             don't you know don't we use that dead                                horse power and the other advantage of                                doing it with MapReduce was that I could                                learn me some may produce paradigms                                along the way so how did we how did we                                get going I want to make clear that new                                for da it doesn't have to be running we                                 don't need new for J we are building the                                 database from the ground up and we're                                 doing that by creating the underlying                                 file structure neeraj a has a                                          different files where each file has its                                 has its own purpose there are files                                 storing the used IDs file storing the                                 nodes separate files for the edge                                 information and the properties are                                 stored in separate files                                 and by if Nia Jay has access to this                                 file structure it can become a running                                 database so we had to investigate what                                 was the internal representation of a                                 node and an edge in those files we                                 didn't have to do that all from the                                 beginning Chris during he wrote a blog                                 post on the file structure in of j                                                                                                              to be used but he did did some work                                 before us explaining how the file format                                 looked I'm going to I don't want you to                                 read this blog post right now so I'm                                 going to repeat some of the important                                 information from the post and that is                                 that a note here in the top has a                                        reference in the file so it has a use                                 flag where you can which is used for                                 deletion it has a pointer to the first                                 relationship and a pointer to the first                                 property and together with the                                 relationships where you have a pointer                                 back back to the note and other pointer                                 to the to note a type of the                                 relationship and the previous and next                                 for both the from manda to note that                                 gives you essentially a doubly linked                                 list on on disk since pictures say more                                 I created this little graph it's me                                 being a speaker at Berlin buzzwords                                 there are properties between the curly                                 brackets so the place Berlin the venue                                 go to browser I but also a property the                                 date at the relationship and with this                                 little graph you can see                                 a little how it works so the red arrows                                 are the pointers from the notes to the                                 first relationship and the blue arrows                                 are the pointers to the first property                                 of a note and then within those                                 relationships you have a pointer to the                                 next and green lines are we are pointing                                 back to the notes again and the black                                 arrows I only drew them at the property                                 level there you can see that doubly                                 linked list elements pointing back to                                 the previous and pointing forward to the                                 next property so this is the file format                                 that we wanted to create and the                                 position in the file is important                                 position in the file tells you which                                 note you are working on so this is a hex                                 dump of the note property store the                                 colors i did add myself to so you can                                 see the nine bytes in the bluish color                                 or the note number with the ID                                       then you have nine bytes for node number                                 one and so forth so since the position                                 in the file is really important we                                 needed the kind of row number generator                                 the data we had had a key with which was                                 a string some account number with string                                 data character data in it so you can't                                 use that as a row number so you have to                                 create it yourself unix has a row number                                 generated built-in cat and so if you                                 could put the main page on you can see                                 the minus n it puts a number before the                                 output lines that helps but we needed to                                 distribute it get mines in so we could                                 convert the parts the input data on the                                 left and put the row number in front of                                 it we wanted to do that with our Hadoop                                 cluster and the easy way out is doing it                                 with one reducer at the end which puts                                 in the numbering but it's not fun right                                 so we did it in a real distributed way                                 where we had the mappers amid the                                 records as is and have access to the                                 hashing algorithm where the mapper would                                 know to which reduce or the data is                                 going and it could increment a counter                                 for that reduce at the end when the                                 mapper is done with the data it could                                 emit also those counters I'm going to                                 lead you to an example to make it more                                 clear so the mapper first initializes                                 the counters when starting reach the                                 input amidst the input edges and                                 increments the correct counter based on                                 the hashing code it gets from the                                 petitioner and at cleanup it emits all                                 counters that it incremented for for the                                 data its or during imp this is some                                 pseudocode but I have here a slide with                                 sugar coat and some data so on the left                                 you see again the the small input with                                 with the hashes at the end and you start                                 by reading ABC and emitted and                                 increments hero she wrote counter and                                 then d e f and it increments the                                 following counter and you have a                                 separate mapper for the data after the                                 split and it does the same and emits                                 also the counters then you have the                                 reducer and reducer processes all those                                 counters at first to calculate the                                 offset so the first reducer reduces zero                                 doesn't have to it can start with zero                                 so it doesn't have to process those                                 those counters and while emitting the                                 input data it adds the                                 counter to it and then increment the                                 following reducer reducer number one                                 processes first from the two mappers it                                 got several values first two and one so                                 the offset is three and it starts                                 emitting at three and then four and so                                 on and reducer to gets different values                                 higher values and so this way you have                                 to distribute it get minus n oh is that                                 all no you have to do something between                                 the mapper and reducer face so we have a                                 custom partitioner that partitioner                                 calculates dash to know based on the                                 input key to its reducer it goes and you                                 have to make it available to the mapper                                 code and a group in comparator to be                                 sure that in the reducer you have only                                 one call to the reduced method and some                                 custom writer balls to make sure that                                 you can put those counters in front                                 front of the values after the putting it                                 to the MapReduce job of the row number                                 generator you have your original data                                 with with a row number and then you can                                 do the following steps to generate the                                 real neo                                                        simplistic here in the total is it                                    MapReduce jobs following each other but                                 we start with the properties do output                                 of the properties and the nodes and                                 edges again join nodes and edges to get                                 from the functional ID to the row number                                 ID and then after that i'll put the rest                                 so starting with the properties you have                                 to start with the properties because you                                 every note has a pointer to the first                                 property and they optimize the property                                 store a lot so one property record can                                 hold multiple properties so it's not                                 that simple as saying well the first                                 property is my node ID times the number                                 of properties i have that was the first                                 mistake we may                                 but we learned so then we started with                                 the properties and then you can do the                                 optimization and you really know what                                 was the first property you would would                                 encounter and another advantage of doing                                 the properties first is that after you                                 have created the new fuji a property                                 files you can leave that information out                                 so you only have to keep the node ID and                                 the functional ID and the rest of the                                 properties you can leave out so you end                                 up after that first job with smaller                                 intermediate data which has can work on                                 after that the output of the properties                                 we also output the node ID with that                                 first property ID and with the edges for                                 the same and you leave out the real                                 property information then you have to go                                 from the functional ID to the real row                                 number ID so we have to join the node I                                 information with with the H information                                 if you have done that then you can                                 output the real note file for neo                                   because you know which was the first                                 relation so you have that reference and                                 you already had the property reference                                 and you have to do some self joining                                 with the relationships to know which one                                 is really the first one but I simplifies                                 it what simplified it for foolish                                 presentation and the last thing is that                                 you can output the DHC information with                                 the previous and next pointers and stuff                                 like it after doing those MapReduce jobs                                 it's just simply do a Hadoop minus ket                                 on the data the parts that are Britain                                 and concatenate them but we have a                                 working file for the new a database                                 it's a little more more times because we                                 store the relationships in separate                                 separate directories and the note files                                 but this is                                                            from my gfs to the machine where neo                                   can be started afterwards okay just a                                 little bit about myself this was the                                 technical part I've brought a little                                 cipher query to let you know who I am                                 what my email address is I am part of                                 the Dutch Hadoop user group meetup and                                 the graph database meet up so there I                                 can combine these these two technologies                                 also much better handle is Chris clothes                                 and I'm not on github bubble so to end                                 this presentation a little earlier than                                 i was expecting but that's no problem I                                 guess being the latest one in the in the                                 row just a little list of things that we                                 need to do to make it complete new                                    uses internally lucena indexes would be                                 great if we could create those leucine                                 indexes on a few of the properties                                 during the MapReduce cycles we do we                                 skip the array properties the new fee is                                 all primitive properties and string and                                 array properties the array properties we                                 skipped because it was work and it and                                 we can kind of basically do the same as                                 we did with the string properties but                                 it's not not in there yet the guys at                                 neo technology are not resting so they                                 created or are busy creating neo                                      the dough and there the file structure                                 is changing so we have to keep up with                                 them so that's another thing to do and                                 at the last session of the graph to be                                 meet up somebody and told me that it                                 would be great if we could do it also                                 the other way around so we have the new                                 forge a file structure and create                                 something relational or tab separated                                 from that to do graph global analytics                                 only a dupe cluster so those are the                                 things that we can work on the code is                                 on get up so if you want to use it                                 please do and if you have requests of                                 things you want to be billed in there                                 let me know and time for questions if                                 you have any okay thank you to the                                 speaker                                 you
YouTube URL: https://www.youtube.com/watch?v=D5NMVwtRYm8


