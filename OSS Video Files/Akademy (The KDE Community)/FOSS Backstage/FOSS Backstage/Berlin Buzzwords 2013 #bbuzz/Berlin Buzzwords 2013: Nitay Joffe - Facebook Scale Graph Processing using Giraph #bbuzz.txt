Title: Berlin Buzzwords 2013: Nitay Joffe - Facebook Scale Graph Processing using Giraph #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	This talk will discuss the design and architecture of Giraph - an offline distributed graph processing system built on top of Hadoop. We have scaled Giraph to process very large graphs. For example at Facebook we run PageRank on a 400 billion edge graph in a matter of minutes. A similar workload in Hive or Hadoop takes many hours and requires an order of magnitude more machines. 

The talk will go through the design decisions we made in order to keep Giraph simple to use yet expressive and powerful. We will dive into the architecture that allows Giraph to scale to very large data sizes. Giraph utilizes Hadoop for job scheduling, resource management, and checkpointing, among other things. We ended up customizing core functionality for efficiency wins. In particular, we built on our own completely in-memory message passing system and use Netty I/O with a lot of caching for performance.

Read more:
https://2013.berlinbuzzwords.de/sessions/facebook-scale-graph-processing-using-giraph

About Nitay Joffe:
https://2013.berlinbuzzwords.de/users/nitayj

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi my name is Nate I I wear                               book I'm working on the graph processing                               team just part of the larger sort of                               data infrastructure organization so                               before I begin how many of you have                               heard of giraffes of Apache giraffe oh                               nice okay that's about half of you and                               how many of you have actually used it                               like I've run a job I've processed                                graphs like three of you all right                                hopefully we'll change that so my talk                                today is going to be focusing on how we                                scaled it how I got it to the point that                                it is today and you'll see that we've                                made some tremendous improvements and                                gotten it to very large scale so first                                I'll give a bit of background so that                                everybody can go on the same level and                                everybody will be able to raise their                                hands then I'll get then I'll dive into                                all the scaling work that we did then                                follow up with some results and show                                some benchmarks and things like that and                                finally some Q&A so first some                                background so what exactly is giraffe so                                draft is a nap Apache open source                                project that's based on a paper by                                google called prego so as many of you                                probably know Hadoop is based on gfs and                                MapReduce and HBase is based on BigTable                                similarly giraffe is based on prego one                                of the things i want i want to point out                                right away it's very important is that                                so facebook is one of those sort of at                                least currently one of the larger                                committers to this project but we don't                                actually maintain any internal fork we                                don't have our internal code base or                                have anything like that we're working                                completely off the apache code we work                                with JIRA we work with all this all the                                great tools that's what we use so                                everything that we use you guys can use                                we work off the trunk so draft supports                                I do hi HBase cumulant lee sort of                                adding a lot of other things we're now                                working on tinker pops and a few other                                things and specifically for Hadoop we                                actually support a wide variety of                                Hadoop all the way from like zero                                   without security to cloud errors head                                due to yarn to a bunch of things                                basically we try to be as agnostic as we                                can about Hadoop so giraffe specifically                                it's built on a BSP model and BSP stance                                or bulk sync parallel which is this pair                                hello programming paradigm that                                essentially takes a large computation                                that you have splits it up into epochs                                and each epoch the work is divided                                amongst all the workers each one does                                they're sort of sub computation they                                send messages to each other but and this                                is a very very crucial point the                                messages don't actually get delivered                                until the end of the epoch and they'll                                be available for those notes during the                                next epoch we we specifically call you                                park super steps you may think of it                                like an iteration in the graph and                                because of this because of this sort of                                nature when you send the messages and                                the ordering and all that doesn't really                                matter anymore there's no it sort of                                removes all the constraints of total                                ordering and partial ordering and all                                these sort of concurrency issues that                                you'd normally have giraffe has a bunch                                of tools and things that you maybe maybe                                familiar from Hadoop like combiners with                                aggregators that let you compute things                                globally on the whole graph so you're                                able to to basically infer something                                from from the graph as a whole you can                                mutate any part of the graph during the                                computation you can add edges remove                                that just add nodes completely wipe away                                and like put probably in a new graph you                                can do pretty much anything you want and                                all we actually impose on the user is                                basically specifying to us what is your                                graph look like so you tell us what do                                you want the store what's the idea of                                the note how do you define the note how                                do you find it what valued is stored at                                each node what value stored in each edge                                and what are the messages that are sent                                between them and all those are types                                that are completely generic their java                                 parameters that you pass in and the only                                 thing that we that we enforce is that                                 they have to implement Hadoop writable                                 and that's so that we can serialize them                                 and move them amongst but other than                                 that completely up to you what you want                                 to do so very importantly also what is                                 direct not the first one this is one                                 that we get confused but all the time on                                 the list is it's not a graph database                                 it's not neo                                                        compared with that because it's trying                                 to approach a clearly different problem                                 which is very large scale processing                                 batch processing not this sort of live                                 interactive query any sort of stuff it's                                 also not a completely asynchronous                                 generic mpi system so because we know                                 that you're working with a graph and                                 because we know the graphs are sparse                                 and we know that most the time you only                                 really want to send messages to your                                 local neighbors that allows us to do all                                 the various cool things that we do and                                 allows us to give the user a much                                 simpler API to work with but it also                                 means it's not just some arbitrary MPI                                 system and finally most importantly it's                                 not a slow tool this maybe was true                                 maybe a year or two ago but as you'll                                 see from this talk it's no longer the                                 case so one question will get all the                                 time especially in facebook being                                 humongous hive users and why not just                                 use hive and basically the reason is                                 that in hive everything breaks down to a                                 MapReduce job and as we all know from                                 MapReduce map tasks read from disk do                                 some key value compare do some qre                                 transformations right back to disk do                                 some shuffling sorting then you have                                 reduced tasks and that writes it out                                 that in and of itself wouldn't really be                                 that bad the problem is if you're going                                 to do some graph algorithm every single                                 step in your graph algorithm is going to                                 become an iteration in each iteration is                                 going to become its own MapReduce job so                                 if you have some graph computation that                                 need to do a thousand steps well guess                                 what each of those is its own MapReduce                                 job so you know I have at least two                                 thousand or so disk seeks and reads and                                 writes and whatnot so giraffe breaks                                 down into a few components each draft                                 job has a master it's very important to                                 understand that when you launch it a                                 giraffe job these components spin up and                                 when you take it down that's it giraffe                                 is gone we don't maintain our own server                                 or anything like that we run on top of                                 on top of Hadoop we launched as a                                 MapReduce job so to speak but really all                                 we do is just take the map portion do                                 our work and that's it there's no                                 reducer there's no nothing else that we                                 basically use Hadoop for resource                                 management for launching jobs and at                                 that once were cross that point which is                                 the very beginning we all say okay we'll                                 head to thank you very much we're going                                 to do all our stuff now so the master                                 synchronizes the super steps it's                                 basically the one in charge of making                                 sure of enforcing that BSP model it's                                 the one that tells all that that make                                 sure all the workers are doing their                                 computation and sort of blocks at that                                 barrier so that everybody is finish                                 their computation at the same time it                                 also between super steps is the one                                 that's ah that's responsible for                                 assigning the partitions between the                                 workers so we talked about mutability                                 briefly if you change the graph in some                                 drastic ways and all the                                 sudden one workers really unloaded you                                 can move around the partition so that                                 you rebalance the load the workers are                                 the guys doing the actual computation so                                 each worker is going to be assigned some                                 set of vertices from the graph as you                                 see here from this from this picture                                 there is sort of four regions four                                 partitions in this picture there                                 specifically they're sort of clustered                                 together but they don't particularly                                 need to be the simplest partitioning we                                 have is just half space partitioning in                                 which case would be just scattered                                 throughout and each worker handles also                                 the input and output reading various                                 splits and then writing eventually                                 writing them out back to wherever your                                 output this each draft job also has a                                 zookeeper that we use in this is to                                 maintain to coordinate to help                                 coordinate everything and maintain sort                                 of a base level global application state                                 that keeps things running smoothly when                                 you launch the job you can you can tell                                 us that you have a zookeeper cluster                                 already and we'll use that but if you                                 don't it's fine and will spin up our own                                 zookeeper so basically we'll grab a                                 couple of the map tasks and they'll just                                 run zookeeper so the data flow of a                                 giraffe job when the job spins up and                                 this should look very similar to                                 MapReduce II type of things because we                                 tried to make it very intuitive for                                 users coming from from Hadoop the master                                 takes the day input format splits all                                 the other the info coming in into it                                 into a whole bunch of splits assigns                                 them to the workers the workers each in                                 turn go and read those splits and                                 they're also assigned which vertices                                 which partitions they own so as they                                 read the split they read let's say                                 vertex at a time they know hey this                                 vertex is mine I'm going to put it in my                                 sort of data store or actually this                                 vertex belongs to this other guy I'm                                 going to send it over to him so they                                 exchange vertices amongst themselves                                 until everybody's done and everybody has                                 the vertices that they're owning that                                 they're in charges next comes the main                                 phase of of the job and at this point                                 this computer and iteration point there                                 is no more disk i/o that's it we've                                 loaded the graph everything from here on                                 out is in memory its network and that's                                 it the partitions of the vertices are as                                 we said our own our own by workers each                                 one does their computation of                                 over that and then they send stats back                                 to the master so that it sort of knows                                 what's going on again does the                                 rebalancing that we said and can move                                 can precede us as it needs one thing to                                 notice there's no sort of correlation                                 necessarily between splits and port and                                 the partitions you can add five                                 partitions but five millions plates or                                 they're unrelated concepts finally when                                 the computation is finished each workers                                 and is in charge of writing its                                 partitions back out to the disk and then                                 there is an apple computer similar to to                                 Hadoop we actually use sort of the same                                 naming format but instead of like a key                                 value input format you'll have a vertex                                 input format instead of the like a key                                 value outperforming other vertex output                                 formats so instead of knowing how to                                 write a key in a value this user                                 specified class knows how to write a                                 vertex so another result of our                                 presentation to give you the state                                 diagram sort of things this is the same                                 the same as the data flow slide you see                                 here you have input you have the super                                 steps looping and then the two things I                                 can possibly help the computation is                                 either the master says okay I'm done and                                 basically it just holds the entire                                 computation for everybody or each vertex                                 when it's done with it's sort of local                                 computation can say okay I vote to halt                                 and when all the vertices are voted to                                 halt then the competition also comes to                                 to an end and we'll see some examples of                                 how that works so each vertex has it's                                 sort of life cycle where everybody                                 starts out active when they vote to halt                                 they become inactive which means that                                 the next time around if they're inactive                                 and they have not received any messages                                 from anybody else they're basically                                 won't be computed over but if you're                                 inactive and somebody sent you a message                                 then it's your job to sort of wake up                                 and sell and compute over that and see                                 if there's something some new piece of                                 information so let's see how this plays                                 out so we have a very simple example                                 which is I have this basic graph of                                 three nodes and I want to compute what's                                 the maximum value of in this graph and I                                 basically want every node by the end of                                 the computation to have this maximum                                 value so in the first super step                                 basically the logic add a single node is                                 I'm going to send my value to all my                                 neighbors and then when I received                                 messages if the messages are received if                                 any of them are bigger than my value I'm                                 going to store that value and I'm going                                 to pass it on otherwise I'm just going                                 to go to sleep I'm going to vote to halt                                 essentially assume that I                                 the largest value so we see after the                                 first computation after the first super                                 step that the top node goes to sleep it                                 basically says all right I have the                                 biggest value because all it gets is the                                 to the middle node is is a value                                      gets the five and it's a way where the                                 fight is bigger all right I need to                                 store that the bottom node gets the one                                 and it's see that that's smaller than                                 the two so it actually goes to sleep and                                 here's where the interesting part                                 happens now there's there's another that                                 the blue lines are the barriers now that                                 the five gets sent down to the to the                                 two node it wakes up again it says hey                                 wait a minute here's a larger value                                 stores that and then finally all of them                                 have halted and at this point the                                 computation stops so this is the actual                                 code to cut to rerun this so not some                                 made-up thing this is in the giraffe                                 codebase you can go you can look this is                                 it this is all you have to write and                                 actually this example is not so                                 contrived because you can use this code                                 this exact code to compute what's called                                 connected components on grass and                                 connected compliance basically tells you                                 if you have a big graph and it's                                 actually made up of sub graphs you can                                 essentially label them basically by the                                 largest vertex in that one and this is                                 can be used to basically find                                 communities another popular algorithm                                 that people always talk about is page                                 rank so this is popularized by Google                                 basically it's to find the importance of                                 web pages in a graph or some or some                                 documents with link types of things and                                 ideas that each each vertex tends to its                                 neighbor is an equal fraction of its                                 page rank that's it's assumed that every                                 link is sort of the same the same weight                                 so you can go you can arbitrarily walk                                 go to any link and then you compute your                                 new page rank according to this formula                                 that pic that page rank as we can an                                 interest of time I'm going to sort of                                 reach through this but if you're                                 interested but I can talk about in more                                 detail and one of the interesting things                                 that we looked at for this particular                                 one is all right well in the existing                                 systems how would I how would I                                 implement this if I didn't have giraffe                                 so one of the popular ones that                                 everybody knows about is mahout and we                                 looked at mahout then this is the code                                 to the to implement page rank it's great                                 it works it does it does a decent job                                 but it's not the most user intuitive it                                 also has various other issues of                                 performance this is the giraffe code and                                 this is it and if you look at the                                 formula                                 look at the code you can pretty easily                                 map this to that right you can actually                                 see the formulas right there you can see                                 everything before that is sort of                                 summing up all the messages and you can                                 see everything after that it's sending                                 it out that's it so let's talk about how                                 we got the scale how we agreed ref so if                                 one of those sort of first things we ran                                 it across is as you draft jobs get                                 bigger and bigger naturally it run with                                 more and more workers actually one of                                 those workers is bound to crash so                                 giraffe has a system called                                 checkpointing for essentially you can                                 decide every few super steps to actually                                 write out the state and the state the                                 state is saved in case a worker fails                                 then it can restore from that state and                                 continue on so that if if there's sort                                 of a constant a trade-off between how                                 often be right because obviously this                                 adds some some hit to the runtime but                                 you can sort of see how often your                                 machines die and and and choose the best                                 number for you similarly the master can                                 crash and for this we use zookeeper so                                 zookeeper has a great recipe called the                                 queue and essentially the way it works                                 is when we spin up a whole bunch of                                 notes say when we actually initialize                                 the job a whole bunch of notes come up                                 and say hey I want to be the master I                                 can be the master and one of them wins                                 one of them becomes the master and if                                 that master during the computation dies                                 then the next one in the queue kicks in                                 and it takes over and actually all the                                 state that the master actually needs to                                 know is fairly small so we actually keep                                 it in zookeeper itself what this means                                 is the master is completely stateless                                 essentially and if it dies and the other                                 one picks up it can just read that state                                 back from zookeeper get everything it                                 needs to know and compute onwards keep                                 going sorry so to the be more specific                                 about this the last so that's like what                                 we talked about this we compose the                                 master work and zoo keeper zookeeper                                 itself is a quorum that's that's fault                                 tolerant and so putting all three of                                 these together we now have a system that                                 that has no single points of failure if                                 any of these new phones feel the job                                 still continues the job goes on so                                 another thing that we ran into very                                 quickly is that most graph algorithms                                 even though we allow the user to do                                 whatever they want and you can you can                                 put entire maps and collections and huge                                 things and tight inside your vertex                                 values and edge values most algorithms                                 actually fairly primitive in terms of                                 the data types that you need most of                                 them have you know if you're doing like                                 shortest path you have doubles on on the                                 nodes you have simple integers on sorry                                 doubles on the edges simple integers on                                 the nodes themselves similarly for like                                 network flow if you're just counting                                 some some degrees you might not even                                 need anything on the edges at all so a                                 lot of graph algorithms use very simple                                 types integers long doubles and when                                 you're growing and you have and you have                                 each each worker you know processing on                                 the order of billion or so vertices and                                 edges this becomes a huge hit because                                 you because collections of primitive                                 types basically have a lot of overhead                                 to them if the box and unbox every                                 single time there are no longer just in                                 just an int it actually becomes an                                 object that object have like Chavez                                 internal monitor has basically all these                                 other things that you don't really need                                 it one the solution here was to use a                                 library called fancy tool we've stuck                                 this in it's basically a collection of a                                 whole bunch of primitives collections                                 that don't adhere it all to the Java API                                 because to adhere to the Java API you                                 have to use generics you have to return                                 capital long capital but because they                                 don't they are super efficient they use                                 primitive types that use like the actual                                 bites everywhere they save this a huge                                 huge though still even with this we                                 still had a problem of many many objects                                 so one specific example we can look at I                                 came up with this for this talk is let's                                 say we had a problem we want to find out                                 during this Berlin buzzards conference                                 who's sleeping during these talks so                                 what I mean by that is let's assume that                                 each user has some labels of basically                                 some potential label of what he thought                                 of a given talk so it was amazing it was                                 boring as enticing it was confusing as I                                 slipped through it whatever it is and                                 you labeled here each user that the ones                                 that you know about so I may know that                                 the ones in the front most like you                                 thought it was okay but the ones in the                                 back they may be sleeping and I build                                 some graph where basically the                                 connections between the graphs are let's                                 say maybe their friends or maybe                                 somebody's sitting next to each other                                 basically things that I may think those                                 things may influence somebody so I'm not                                 it's not clear to me what the people in                                 the middle how they                                 about the talk and I run this algorithm                                 called label propagation which basically                                 takes these initial labels of people                                 that I know sort of how they how they                                 behave probably get it through and then                                 you see sort of at what probability that                                 the unknown nodes didn't have each label                                 you basically end up with this                                 probability distribution so when you do                                 this and this this graph is very small                                 but but what we wanted to do is get this                                 type these types of algorithms to a very                                 very large scale we're talking a billion                                 notes                                                                   numbers so this particular algorithm                                 would with this data to take this                                 example we have a billion vertices                                     billion edges let's say we're spreading                                 it out over                                                           there's a billion edges per worker and                                 each edge as we can see here is a double                                 so that's one object / edge that's one I                                 want primitive or whatever it is so that                                 means per node with the initial sort of                                 data structures we were using just a                                 basic old list of edges that's around                                 our sorry that should be                                           objects similar that we have sort of a                                 similar formula for the vertices so we                                 had a map that contained the vertex data                                 so we have five about five million                                 vertices per per worker ten objects in                                 the end the vertexes I the reason I say                                 ten is because as I was saying when you                                 when you get the final result you get                                 sort of a distribution so we see like                                 node                                                           percentage likelihood of thinking that                                 it's amazing because it's because it was                                 connected to node                                                      some probability of thinking it was                                 boring because of node                                                  we want to keep say the top five of                                 those so each of those labels is one                                 object and the probabilities another one                                 so ten objects that's just throwing out                                 numbers so then the thing that sort of                                 gets heavy is the messages themselves                                 because in the message you have to also                                 send out that distribution to the guy                                 that's ahead of you so that also is                                 going to be                                                            and on average you're going to basically                                 send one one message per every single                                 edge this is one one interesting point                                 about this type of algorithm labour                                 propagation versus like the connected                                 components I talked about is this                                 algorithm on every single super step and                                 everybody always sends a message whereas                                 with connected connected components the                                 vertices go to sleep and that means that                                 as                                 algorithm proceeds and necessarily does                                 less and less work that's not true with                                 this and this in this type of a grande                                 duration time stays roughly the same                                 each time so we have this formula here                                 at the bottom that basically tells us                                 that this this total thing you use is                                 about on the order of                                                    which first for a single node for Java                                 is huge at this point java GC just                                 starts going crazy and we have this note                                 that tells us sort of roughly how much                                 the objects ah are used given given a                                 certain data set so a solution here that                                 we came up with was basically to throw                                 everything out the door in just go back                                 to basics and replace everything with a                                 binary store all the vertices store all                                 the msgs store all the edges and just                                 one ginormous by Tori and on top of this                                 we put an iterable interface with this                                 thing that we call it representative                                 object so that basically as you're                                 reading it end is this the orange sort                                 of Pentagon that you see is this                                 represent about this so that would be a                                 vertex or an edge or a message as you're                                 reading it in you would write the user                                 so behind the scenes is writing to that                                 vertex and then we write it to the                                 binary then you write to the next one                                 and rewrite it to the binary similarly                                 for the interface when the user is                                 computing / messages we give them an                                 iterable of messages that's that's the                                 interface that giraffe has and that                                 iterable of messages behind the scenes                                 actually only has a single message that                                 single message when you call next reads                                 in the next data from the byte array and                                 now you have that data what this means                                 is if you were to say in your                                 computation store each one in the list                                 at the end of the day you'd have a                                 billion or                                                             that are all the same reference so the                                 user sort of has to know this and then                                 if they really want to store every                                 single message they're going to have to                                 make a copy almost the algorithms that                                 we've seen you only really need to do to                                 touch a message once because most likely                                 you're aggregating some data at the end                                 of the day what this meant is that we                                 could take everything down and all the                                 men all the men all the memory they cut                                 like I used on on the system sort of                                 came down to this big old capital V                                 which is the total number of vertices                                 which is enormously small compared to                                 basically the thing that we found was                                 most degraff algorithms are limited by                                 the number of edges because on average                                 the number of edges is saved on the                                 order of                                                           number of vertices                                 so some once once we got sort of the big                                 capital e out of this equation is a                                 serious serious performance key so as                                 beautiful as all of this is and it                                 really is very cool everything that it                                 did it also had an additional savings of                                 not only that it reduce the number of                                 objects but it also reduce the actual                                 membrane which wasn't even necessarily                                 like originally the intention but but it                                 but it did that as well because again it                                 got rid of those like monitors and all                                 those things that objects have it just                                 kept just the raw data that we want in                                 the vertex so this sort of created a new                                 problem for us which is that we used to                                 work with these lists of vertices and                                 maps of edges and and the you the type                                 of the user one it was the type that we                                 already had in memory now suddenly                                 everything's in by terrain we have to                                 serialize and deserialize constantly so                                 that suddenly becomes an issue of like                                 well how do we do that fast so we                                 started looking at all these things with                                 the interface that we provide to the                                 user is this data input data output                                 which is what comes with writable with                                 Hadoop writable so we looked at very                                 serialization schemes cairo thrift etc                                 and the thing that we came up with is                                 this thing called unsafe you guys                                 haven't not heard of unsafe it's a sort                                 of hidden piece of the java api and when                                 i googled it this is what i found out                                 about it it's dangerous and has no                                 formal API it's volatile it's bound to                                 break its non-portable it's only in the                                 Sun Oracle JPM and it's it's so extreme                                 to this point that it's even hard to                                 like actually get at it and this is the                                 code that you have to write when you                                 google unsafe everybody has this code                                 you basically have to use this                                 reflection magic to actually pull out                                 where is that freaking object so all                                 that said this thing is awesome just                                 this is as fast as it gets I mean if you                                 dive down deep into like Cairo and into                                 the serialization libraries and you find                                 the really really good ones you'll see                                 that somewhere they have this code like                                 grep I'm telling you go to these go to                                 github and grep for the unsafe and                                 you'll see what serialization libraries                                 are good and the reason for this is                                 because basically this is as native as a                                 guess essentially what it's doing under                                 the hood you can think of it the way I                                 like to think of it it's essentially                                 like a c pointer cuff you have a byte                                 array it's okay it's actually a long                                 pointer i'm going                                 you reference that and get it out I have                                 a friend of mine on the team who sort of                                 like a JVM expert and he tells me that                                 these days the JVM is so good at doing                                 unsafe that it actually becomes                                 basically a no op for those of you                                 they're curious unsafe also comes with a                                 lot of other things for example you can                                 allocate memory that's off the heat not                                 off the heap sorry off the JVM of the                                 garbage collector and this is how things                                 like direct dog byte buffers they                                 actually use unsafe under the hood to                                 allocate the memory you can also do                                 really crazy stuff like you can actually                                 inherit from a final class so you can                                 make your class inherit from like string                                 let's say that one I really don't                                 recommend so another one that we ran                                 into I mentioned this idea of                                 aggregations aggregations that you                                 complete thing on the whole graph so                                 originally aggregations were sort of                                 used more for tiny little things like                                 for example just saying during like a                                 pagerank computation how many notes                                 change their page rank value so each                                 node would just sort of fire off a                                                                                                               level master would aggregate those                                 values and you just get one single                                 integer and there's no problem with that                                 but eventually aggregations got bigger                                 and bigger and bigger specifically                                    algorithm that was interesting was when                                 you have an algorithm like k-means                                 clustering k-means is sort of an                                 unsupervised machine learning technique                                 that you might use to do something like                                 for example finding similar emails so so                                 an email to basically you have the whole                                 bag of a mouse like imagine we're                                 talking on the order of like all of                                 gmail or something and you want to sort                                 of classify which emails are like each                                 other so that eventually when you know                                 that this set of emails when they should                                 know that this set of emails as spam                                 then you know that this whole class of                                 emails was actually all spam because                                 they're like each other so you might                                 create all these features that                                 describing all the length of the email                                 the number of times the word and large                                 become its appears in it all these sorts                                 of things that you might think of might                                 might be useful sort of features and                                 then you get this big feature space and                                 you get this high dimensional thing I'm                                 not a machine learning guy but that's                                 about all I know about it and you want a                                 cluster on that there is in that cadamia                                 there's sort of a lot of different ways                                 of doing k-means clustering in with in                                 parallel there's there is actually some                                 message passing                                 nincs and what not but there's actually                                 another technique where basically you                                 don't actually send any messages at all                                 you just consider those the clusters the                                 centroids as the aggregators what this                                 means is that each HR gator itself is                                 actually a high-dimensional point so                                 then at this point that aggregates                                 themselves become very very big so                                 initially we stored the aggregators in                                 zookeeper that obviously broke down very                                 quickly because we keep her notes I                                 think can only store one megabyte or                                 something like that step that very                                 quickly broke down so we moved to                                 storing it on the master and that worked                                 pretty well for a while but then again                                 it got larger and larger and larger and                                 the master became the bottleneck you can                                 suddenly everybody sending to this one                                 guy that's one guy suddenly getting                                 gigabytes of data and has to do                                 something with it and then it has to                                 send it all back so real solution we                                 came up with a skilled with us sort of                                 to the future is that actually the role                                 of aggregating data should actually be                                 owned by the workers themselves                                 basically when the when the jaws fins up                                 we randomly assign each aggregated to                                 workers and in this picture each of                                 leaves the green red line or sort of the                                 green red and black lines are sort of                                 different aggregations and as you see                                 they're owned by these three workers and                                 so during the computation when when I                                 that when a worker wants to aggregate                                 some data he knows which who owns that                                 data aggregation he sends it to that                                 worker instead of sending it to the                                 master it workers then the workers that                                 are in charge of the aggregations to die                                 aggregations themselves they send the                                 final value to the master because                                 there's actually an interface that we                                 give you in the master where you can do                                 something on an aggregation for example                                 like in PageRank pagerank might take                                 millions and millions of cycles                                 Melanctha millions of steps to actually                                 converge fully and sort of the notion of                                 convergence to you might mean like an                                 epsilon of whatever it is so you might                                 make that decision in the master say                                 like oh k epsilon reached within this                                 value stop the computation so the                                 workers sounded to the master the master                                 then sends it back seat each worker                                 that's in charge of it but that data is                                 very small because that's the final                                 result that's not die aggregation of                                 every single value and then the worker                                 that's in charge of it sends it back out                                 to everybody else so that everybody now                                 has that state                                 so okay so another thing that we came                                 across was that basically in there in                                 the bsp model a sort of naive way to                                 look at it is again that initial picture                                 that I have we split up the work each                                 each worker does its tasks then it sends                                 the messages so this is sort of what                                 intuitively what what it looks like if                                 you take away that red box and just have                                 compute and network this is sort of like                                 the intro level way to describe it                                 obviously for us we know that all that                                 time in the network is just time spent                                 waiting you're not computing anything                                 you're not you're just waiting for                                 messages to get delivered and                                 furthermore initially when we built the                                 system we were basing it off like                                 Hadoop's RPC and a few other things that                                 really just didn't fit with the model we                                 knew that we were what we're really                                 doing is message passing so clearly this                                 sort of RPC system and whatnot was not                                 was not great clearly synchronous calls                                 are not allowed we just want to send a                                 message and fire-and-forget basically so                                 the solution here was to put in this                                 library called Nettie and that is a very                                 high performance asynchronous system                                 that allows you to build sort of network                                 services on top of it and specifically                                 one of the things that we watch for all                                 the time that we track the metrics of is                                 we we try and send when one of the                                 workers sorry when a vertex says hey I                                 want to send this message to this other                                 to this other node we send that method                                 as soon as soon as we can we start                                 sending messages as soon as we can so                                 that we enter leave the computation with                                 the network and we constantly track that                                 wait time that that red box to reduce it                                 as much as possible and basically what                                 that ends up meaning is that we tune the                                 buffer sizes of the of the neti system                                 to Hamid how many messages we buffer up                                 before we send and we also tune that in                                 correlation with the number of threads                                 that were used to run the actual                                 computation one thing that we found is                                 those tunings along with basically                                 almost any other thing you might imagine                                 that we tuned like like the GC settings                                 itself are actually very job specific so                                 we inside the RAF sort of pick like the                                 smart sort of intelligence that the best                                 sort of intelligent things that we can                                 pick but most likely for your particular                                 job if you really want to squeeze out                                 that extra performance out of it you'll                                 have to do a little tuning yourself                                 because we don't know your work                                 where were sort of this generic system                                 so what does this all mean what are the                                 final results here so this is a graph                                 that's showing that within a given a                                 fixed data set and that the data set we                                 chose here was                                                        billion edges and                                                     means that per worker we have                                            running those that compute function of                                 on on vertices in parallel and what we                                 did was we increase the number of                                 workers and we ran the exact same job                                 same sort of random data set and we saw                                 how does that scale this graph might be                                 a little deceiving because it looks like                                 it sort of gets worse as it goes but                                 this is actually very very close to                                 ideal if you look at if you look at the                                 trend of that and you look at you pick                                 two numbers say like                                                     see that the values that are actually                                 very close to like half of that this is                                 basically linear it's essentially as                                 good as you can get similarly here we                                 have the opposite we fix the workers we                                 fix the computation power that we had                                 and we grew the data size so with                                    workers                                                           increase the data size but by increased                                 data size something specifically than                                 usually the number of edges sometimes                                 the would divert the vertices too but                                 mainly it's the number register you care                                 about mainly that's that's the thing                                 that really is the bottleneck and you                                 see here that the iteration time                                 basically grows fairly close to linearly                                 and I don't recall exactly what that                                 data point is but essentially it's it's                                 huge it's billions and billions and                                 billions of edges so finally what did we                                 learn from all of this and I hope that                                 sort of you guys can take some of these                                 lessons that we've learned and not                                 repeat our mistakes and all the all the                                 things that we did that to get to this                                 point this is there's a sort of general                                 knowledge will be learning from building                                 this distributed system so first one of                                 this this one maybe maybe obvious so                                 basically doing coordination doing                                 thought would fault tolerance and                                 resiliency and all this is really a zoo                                 and personally I think zookeepers                                 actually one of the most amazing like                                 open source projects there is in terms                                 of things that are based off of Google's                                 Google papers I think zookeeper space                                 stuff chubby or something it really is                                 amazing it's it's like the piping that                                 allows you to build you know these                                 it's and everything on top of it                                 secondly doing something efficiently on                                 the network is really really very hard                                 and there's just no better library than                                 Eddie if you're doing something and you                                 care about performance on your network                                 you should stick in meddy play with it                                 thirdly is the one that I mentioned if                                 using a collection with primitives                                 you're going to get primitive                                 performance there's just no better way                                 around it and this is something that is                                 unlikely to basically ever change                                 because job is very stuck in its                                 collections API and because primitives                                 have to behave like objects in order to                                 be used in a collection so if you care                                 about performance again check out a                                 library like fast util force byte array                                 though it may seem simple and it may                                 seem sort of like CTU and it may seem a                                 little a little scary when it comes down                                 to it if you're going to create billions                                 and billions of objects you should                                 really think of this I think actually                                 HBase does a similar thing and again                                 think and turn the lane their data                                 structures that use a lot of byte arrays                                 to next being unsafe sometimes is                                 actually a good thing again especially                                 with things like the byte array and                                 finally if you have a graph if you try                                 out direct so lastly sort of the end-all                                 result we compared it with hives we are                                 data scientists use hive all the time to                                 do graph computations and sort of on the                                 on the average job this was the the                                 result that we saw the                                                  but more importantly around                                       elapsed time speed up this means that a                                 job that used to take                                                around nine minutes and specifically for                                 those of you that just speak English not                                 math at facebook data scientist                                 basically these jobs that they would run                                 on the entire facebook graph used to be                                 called weekend jobs and now basically                                 their coffee breaks and i actually have                                 a personal story of this that was very                                 funny to me where when internally                                 started we were just drop was picking up                                  and we were starting to get more teams                                  using it we helped a lot of teams sort                                  of port their hive jobs to draft and i                                  had one guy come to me and he was so                                  used to the sort of the mindset of being                                  a bit being a weekend job                                  that he we finally sort of got the                                  giraffe job working and he launched it                                  on on like a friday before he went                                  before he left for cuz he was used to                                  this like all right I'll launch it i'll                                  check on him in the main monday then                                  friday like                                                              text at page from him that's like                                  man listen I think I messed something up                                  the whole computation is going to be                                  screwed we're going to have to start it                                  over like the whole weekend is screwed i                                  text them back I thought them your sink                                  finished four hours ago like it's                                  already done so then we reran in and it                                  was done by midnight well that's it                                  you                                  you
YouTube URL: https://www.youtube.com/watch?v=TsovQfkmG9Q


