Title: Berlin Buzzwords 2013: Clément Stenac - Dataiku Flow : data-driven scheduling for real-life data ...
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	The life of a data scientist is full of traps. After you've vanquished the daemon of file formats and charsets and the hydra of data enrichment, mastered Pig, Hive and other processing tools and found your way through the maze of Machine Learning, you are faced with an even greater task.

Your precious insight must now be productivized, computed each day from many sources through dozens of processing steps, and integrated in the target system. You must assemble together disparate systems that were never meant to communicate. Errors will happen, you'll have to handle them, make sure that no day of data is missing. And you'll suffer.

Dataiku Flow is an open source data-driven scheduling tool for complex data pipelines. Instead of scheduling tasks and actions to perform, you describe your data pipeline, no matter how large, as dependencies between datasets and tasks. You then simply request your output data: "Compute the aggregated visits info for 2013/06/03". Flow computes all required dependencies and partitions, and performs all computations in parallel. It relies on the real state of the system to know what data is already present and what must be refreshed. Flow manages a unified schema across tools.

Flow has builtin support for many tasks : files syncing across many protocols (filesystem, HDFS, S3, Google Cloud Storage, FTP, ...), Pig, Hive, SQL, Python, R, ... Custom task kinds can easily be added.

The talk will describe the concepts, architecture and features of Dataiku Flow and detail its unique capabilities. We'll explain the rationale for a new tool, and how it compares to and completes existing tools like Apache Oozie, HCatalog and ETL solutions. We'll also give some roadmap information.

Read more
https://2013.berlinbuzzwords.de/sessions/dataiku-flow-data-driven-scheduling-real-life-data-pipelines

About Clément Stenac:
https://2013.berlinbuzzwords.de/users/cstenac

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so good morning everyone and thank you                               for attending this session about that a                               coup flow so my name is chemin cynic and                               I am the CTO of data a coup and that a                               coup is a French startup founded this                               year we have a software editor and our                               goal is to make software that makes the                               life of data scientist easier and more                               productive and to try to fix the pains                                that data scientists fill and there are                                many pains for a data scientist some are                                related for example to ending data                                basically just getting data in the                                system so passing all kinds of data                                formats doing data cleansing                                normalization enrichment going from raw                                data to stuff which is really usable for                                real data science so basically doing                                some advanced reporting's doing some                                machine learning etc and one of the                                pains which is associated to that is                                ending data pipelines and data flow for                                example this this is what could look                                like the data pipeline of an e-commerce                                company so you have lots of silos with                                input data so you have all the CRM data                                all the web tracking or the search                                engine data so all of that in really                                different format and there are a lot of                                analysis that you want to perform to in                                the end get a few outputs so a new                                product recommendation some reports some                                data for search engine optimization and                                you have ready a large number of stage                                if we zoom just on one thing which is a                                recommender here just a recommender is                                actually first you have to to start with                                your tracking logs the page views your                                CRM orders and customers table and you                                have to make at least a dozen stage just                                to go to one thing which is the product                                recommendation for my people who like                                that product also like this one                                so this is a real life that a pipeline                                and what we call realize that the                                pipelines have many characteristics and                                the first one is that you have a very                                large diversity of tools and of type of                                task so there is no single tool and I                                read on things that there will be any                                single tool able to do all of that you                                will use a Hadoop for some of them                                mostly pig I've for many of these tasks                                you will want to use Python because                                Biden has excellent support for many                                data analysis test you might want to use                                our for statistical processing so really                                lots of things and in a real-life                                company all of this will evolve                                extremely frequently you will have new                                steps to add new data and you processing                                you you are going to change your the way                                your website works and you are going to                                have to change the data pipeline to                                accommodate that and due to this large                                ever you tivity what we call exceptional                                situations are really the norm so                                exceptional situation is really I've                                changed my input data for my filtering                                stuff and so I need to recompute all the                                previous data to bootstrap my new                                algorithm if I don't recompute                                everything will be skewed etc or this                                day my provide my tracking provider or                                add a failure so I didn't get the log so                                I will need to reprocess it etc etc and                                there are many pains associated to all                                of that like sharing a schema you have                                as I said lots of tools each one has its                                own view of what my data is how it                                destroyed our the schema is entered and                                just that is a mess and you also want to                                do some efficient incremental                                synchronization incremental computation                                of course when you were working on the                                tracking rocks you want tronic compute                                date or your data on two days or                                actually yesterday's tracking log and                                you want to                                track clearly the state of each data set                                we think the evolution of all of that is                                really similar to what we've seen in                                build system bit system started with                                shell scripts you basically just said                                what you want it to do but quite quickly                                some things happen or killed McPhail and                                most importantly notion of dependencies                                so Meg depths and if it's already                                   years ago that we started to say or if I                                 have already compiled this while I will                                 check the time zones and not recombine                                 them and then much much later we started                                 to have a new tools so mostly sconces                                 sconce see make and math maven in the                                 Java world where you start it to have                                 some more high-level tasks so you you                                 don't nowadays you don't anymore or say                                 you don't have to write ok to compile                                 this type array I have to compile each                                 file one by one then call AR to make an                                 arc eyes and ran names and strip etc you                                 have higher level tasks that are                                 packaged and you have really better                                 dependency management because the tools                                 are aware of what they are doing it's                                 not they don't just execute them tasks                                 so the evolution that we've seen on                                 build go in the sense of better                                 dependency management and higher level                                 tasks built-in known by the tools and we                                 think there is the same evolution for                                 data pipeline and big data systems where                                 when Hadoop was released in                                           everybody just wrote shell scripts then                                 we started to have some tools like                                 Hadoop make an uzi which provide a                                 higher level and scheduling uzi knows                                 how to run a pic script it just doesn't                                 run the pig executable but we think we                                 still can go further and put more                                 intelligence and more dependency                                 management stuff in a data pipeline                                 scheduler and that's what we are trying                                 to achieve with the tool called data a                                 flow                                 which is a data-driven orchestration                                 framework for complex data pipelines                                 this might sound a bit so what                                 do we mean what do we mean by that you                                 want to manage data you don't want                                 managed steps tasks what you want is                                 data and that's exactly what you have in                                 build systems you want all the pains                                 that we discussed about so all the                                 maintenance rebuilds the situation being                                 entered natively by the tool and you                                 want all of the pains specific pain                                 points of complex data pipelines handled                                 so basically flow is a in its concepts                                 quite simple you have one concept which                                 is simply a data set what is the data                                 set simply put it's like a table so it                                 has a schema it has records and data set                                 an important thing is that in can be                                 partitioned must so partitioning is                                 really semantic split of your data set                                 so most often it will be time based                                 partitioning you are processing                                 something which which is logs basically                                 and so you partition by day but you are                                 also some value partitioning like                                 partitioning my user data by country etc                                 and data set is a logical view over any                                 back-end basically so a back-end could                                 be a sequel database obviously many no                                 sequel stores can be viewed as a data                                 set and obviously HDFS file or any cloud                                 storage file for processing by Pig hive                                 or MapReduce and then now that we have                                 data sets we need tasks simply put a                                 task is something that that as input                                 data sets and that computes new output                                 data sets and where it's interesting is                                 that the task the simple fact that we                                 define this task define dependencies                                 between these data sets                                 the daily aggregation the data set                                 depends on the customers data set and                                 some tasks are built in into flow with a                                 strong integration we'll see what that                                 means later but you can also run some                                 quite custom tasks so just with these                                 two notions we can create of what we                                 call a flow so flow is really a graph of                                 data sets and tasks so that's a simple                                 one to compute sessions and visits                                 statistics for an e-commerce website and                                 the first distinctive point of flow is                                 that it's what we call data oriented so                                 in many scheduling systems do you manage                                 a dag of tasks so the fact that you have                                 a graph of tasks allows you to have                                 dependencies between tasks and so you                                 know that if that task failed then I                                 cannot run this task I have to retry it                                 or I have a narrow path etc but you                                 still only manage tasks in flow you                                 manage boss tasks and data set and                                 that's what makes it data oriented so                                 the thing is you are not going to ask                                 run this task you are not even going to                                 ask run this task and then all dependent                                 asks what you are interested in is the                                 data so what you ask is this data set                                 this output data set it's what I want                                 it's the output of my data pipeline and                                 I wanted computed from this particular                                 day from this particular partition and                                 then it's the system that will manage                                 what tasks are needed based on the                                 current state of the system and the                                 tasks to go to the output so you don't                                 execute tasks you compute data set and                                 that is still very similar to what                                 modern build systems do in scones for                                 example you type scones the name of the                                 output executables that you want to                                 generate and one important thing in this                                 dependency management stuff is what we                                 call partition level dependencies so it                                 is a very simple example I have some web                                 tracking logs so really the logs of all                                 actions of my users on my website and                                 always the first thing that I will do is                                 that we I will clean up I will normalize                                 it i will enrich it to make it usable                                 and that makes what i call the clean log                                 data set and then i want to make a big                                 aggregation of all visits on the last                                 seven days to computer sliding                                 statistics and for that i decide to use                                 pig because pig is extremely well-suited                                 to this kind of tasks so both my input                                 and middle data set our partition by day                                 each day of the day as the data for this                                 particular day but the weekly                                 aggregation stuff needs not only the                                 data of the day that it's working on but                                 also the previous data and the previous                                 data must be up to date it must be clean                                 it must have been verified and for that                                 what you do in flow is that you declare                                 a partition level dependency between the                                 between the data sets so you will say                                 there is a sliding days kind of                                 dependency between these data sets and                                 then if I ask flow okay now compute my                                 weekly aggregation for this particular                                 day it will compute go back the chain of                                 dependencies compute that I I need a                                 clean lock for                                                       going back until it reaches the surface                                 and then use time stamping and check                                 something to check okay i have already                                 computed clean log two days ago but for                                 some reason the web tracking log has                                 been really teed since so i need to                                 recompute it and it will do all of that                                 automatically and in parallel and it                                 will do all the web tracking log and                                 then only then it will do the pig                                 aggregation so we have automatic                                 parallelism because we can compute the                                 global dag of all activities and then                                 once we have done that we we are immoral                                 it's more like traditional schedulers                                 you use the graph of data sets and tasks                                 to compute the dag of what you have to                                 run exactly and that gives you that                                 gives you were a list of steps a list of                                 stages and wherever you can paralyze and                                 where you cannot so in the previous                                 example all the seven daily aggregations                                 daily cleanup tasks can actually be                                 running parallel they have no                                 dependencies between them but then the                                 final one can only be on when the seven                                 previous ones or I've succeeded and of                                 course it's quite simple on such a small                                 example but when you have a huge                                 pipeline with fully dozens of tasks you                                 cannot manage all of this parallelism                                 manually anymore it just becomes quite a                                 mess and you have to make some shortcuts                                 or not not optimized enough so this                                 dependency management building and                                 partition level dependencies I really                                 score different she talks that we think                                 flow as it has a few additional nice                                 things we think the thing the first one                                 we talked about is a schema data sets                                 have a schema which is expressed in a                                 common language and which can be                                 translated in all the tools for which                                 flow as the deep integration and also                                 what we call data validity checks the                                 idea is that it's very often you can                                 have a task which succeeds okay you run                                 the task or rhetorical d                                                ok but actually no dieter has been                                 produced or completely wrong data has                                 been produced                                 this is extremely easy to get it wrong                                 especially with all that looks like logs                                 so txt files where suddenly a column can                                 be messed up it's extremely easy to get                                 it wrong and so we want to have dieter                                 validated checks which are executed                                 automatically you declare some                                 validation holes and each time a data                                 set is refreshed validation rules are                                 executed so it can be a check that a                                 given the graph that I know must be                                 always if is present or it can be                                 something more clever like check that                                 from one day to another there is not a                                 huge variation in the size of the output                                 dataset because it means that something                                 is off and waiting it's extremely useful                                 because it produces automatic tests for                                 data pipeline so we all know that in                                 software engineering automatic tests are                                 all good and there is no reason that                                 it's not the same case for data biplanes                                 we think a dupe is a great ecosystem and                                 flow as some night native integration                                 into a loop and that's mostly when we                                 talk about a loop in the context of data                                 analytics is mostly piggin I've so we                                 have a native knowledge of ping and I've                                 schemas loader storage formats which                                 allows the user to concentrate on                                 writing in salgo ISM and not just                                 repeating the schema of each of each                                 data set however as I said earlier you                                 cannot do everything in Hadoop so if                                 Hadoop is a first class citizen the very                                 important thing is is it's not the only                                 citizen and you can very easily have non                                 Hadoop tasks in a flow and you also have                                 deep integration of nonnative tests so                                 that means sequel and that also means a                                 synchronization automatic                                 synchronization from source systems and                                 to output systems for example after                                 executing a complex pick a gradation you                                 often want to low                                 your data to vertica for example for                                 quick analysis so you might not need                                 that in the future with Impala drill or                                 stinger but from the moment you can't                                 use Vinnie you can't really use hive for                                 a real-time processing so I guess the                                 question might come what about whosay                                 and had h catalog uh no sorry Z Susie                                 Susie for those you would not know is a                                 workflow scheduler for Hadoop it has an                                 extremely nice integration into a dupe                                 so with Susie you can really simply                                 execute pig tasks hive tasks MapReduce                                 and to some extent some custom dust we                                 think we can go further by really                                 putting some notion of dependencies and                                 putting the notion of data set at the                                 center and H catalog is is a schema                                 management for Hadoop so it provides a                                 centralized schema we would like to use                                 H catalog in the long term for the                                 moment we have preferred to develop a                                 very small very simple schema                                 centralized schema management thing                                 which we think is a more simple but in                                 the long run we might want to switch to                                 H catalog so what is the status as I                                 said we just started so we have a the                                 cove engine with all the ideas that I                                 exposed up and running and we are still                                 in the active development for having                                 some betas and we have a small website                                 where you can subscribe to get more                                 information I guess there will be the                                 question so i will that i will answer it                                 is it open source as i said we are an                                 extremely young company so we are still                                 experimenting we are still testing                                 models business models etc so nothing is                                 certain yet our come on                                 plan is to have that as a open core so                                 like actually most of the open source                                 technology is driven by a company                                 meaning the core engine being open                                 source and some additional modules                                 additional tasks additional enterprise                                 stuff being closed rocks but we are                                 still young that might change a bit so                                 just to make you wait we have one last                                 thing to show have you ever been annoyed                                 by transferring data between systems                                 chance is that you if you are a data                                 scientist in somewhere answer you've                                 already used some or most of these tools                                 just to do some data transfers and make                                 data read ready to be usable by another                                 system and we felt that pain and it                                 annoyed us so we created dc-dc the data                                 cloud transfer client which is an                                 extract from flow and which allows you                                 to manipulate files across a large                                 number of virtual file systems with very                                 normal and intuitive command so you can                                 do reading writing copying of data                                 incremental synchronization transparent                                 and Ling of GZ PNG zipping zipping                                 unzipping etc doing some splitting of                                 final editing editing editing remote                                 files directly in your favorite editor                                 we things DCTC is very useful and you                                 can get it right now it's available on                                 DCTC dot IO and we'd really like to have                                 some feedback thank you very much let's                                 thank the speaker                                 you
YouTube URL: https://www.youtube.com/watch?v=feKrI4hZApM


