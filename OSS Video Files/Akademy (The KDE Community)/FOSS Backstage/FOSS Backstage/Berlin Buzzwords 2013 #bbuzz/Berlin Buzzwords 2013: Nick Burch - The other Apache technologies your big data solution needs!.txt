Title: Berlin Buzzwords 2013: Nick Burch - The other Apache technologies your big data solution needs!
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	You've gone to the talks on Hadoop / SOLR / NoSQL / etc, and now you're ready to start building your own solution on top of that! Before you go diving in though, we want to make sure you don't accidentally end up re-inventing the wheel while building your system...

In this talk, we'll take a look at a range of projects from the Apache Software Foundation that can help in building your big data solution, alongside the headline projects. While we can't cover every project at Apache (there are just too many these days!), we'll take a tour through some of the up-coming and lesser-known established projects out there that should prove very helpful to you in building your big data solution. We'll see that Apache is more than just the webserver, Hadoop and Lucene, and with any luck point you at projects that'll save you time!

Read more:
https://2013.berlinbuzzwords.de/sessions/other-apache-technologies-your-big-data-solution-needs

About Nick Burch:
https://2013.berlinbuzzwords.de/users/gagravarr

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              ok hello and welcome talk I'm giving                               today the other                               technologies your big data solution                               needs so I work for a company called                               quanta Kate and we're just starting to                               do things with big data and as part of                               that I've been doing some vestigation                               Xin to various Apache technologies out                               there that we should be using and in                                general I do help out with the apache                                software foundation and I've been trying                                to learn quite a bit about the different                                projects that are at Apache now I'm                                going to try and give you a whistle-stop                                tour through some of the the projects                                that you should know about so when                                you're building your solution you can                                avoid reinventing the wheel or at the                                very least avoid giving it only with                                three sides so the asf apache software                                foundation there are currently                                    top-level projects                                              projects so that's a lot of different                                projects out there and if you were                                thinking of bringing one to the                                foundation why is the letter to go for                                it's the only one in the alphabet that                                we don't have a project starting with                                and we try and do meritocratic                                community-driven open source so the                                foundation has been growing which means                                every time I give this talk it gets                                harder because there's even more                                projects involved and but we're now up                                to about                                                           there not all of them relate to big data                                but a surprising number of them do so                                what are what am I not covering in order                                to fit this into                                                     are a few projects that are already                                being covered here at berlin buzzwords                                they've got their own session so i'm                                skipping those so they're mostly the the                                big-name ones not covering any of those                                or any of those if you want to know                                about those projects need to just look                                through your schedule and find the                                sessions on them so having excluded the                                very biggest projects that does still                                leave quite a lot to be done so I've                                tried to break the list of projects up                                to give a bit of structure so firstly                                I'll be looking at some of the projects                                that make it possible to load data in                                and query it and look at some of the                                projects make it easier to build                                MapReduce jobs                                going to have a look how to deploy and                                build for the cloud the big data                                solutions looking a bit at some of the                                server technology related stuff then                                finally a number of the other projects                                that that help when you're building the                                the complete solution now I've not got                                that much time there's a lot of projects                                so I'm not only going to give you a bit                                of an overview it's not a complete                                introduction but it can hopefully whet                                your appetite and let you know which                                sort of things to be looking at so I                                need a bit of a show of hands to work                                out which projects to focus on on which                                ones gloss over so if you consider                                yourself a developer can I see hands up                                okay if you consider yourself to be a                                sysadmin few of you and okay and if you                                consider yourself to be kind of in                                management project management anything                                like that a few of you okay so i will                                try and mention almost all of the                                projects i've got slides for but I'll                                dwell more on the developer related ones                                for the others that tend to be more so                                sad mini or that sort of thing you will                                be able to download the slides later and                                have a look through and then come and                                find me at six o'clock with a beer and I                                will happily talk to you a bit more                                about those projects right now about                                them so slides will be available I'm                                going to cover an awful lot I would've                                vines not trying to take notes I did                                want to see someone dry and it wasn't                                pretty just try and remember the                                projects that seem interesting at the                                top of all the slides I'll try and give                                the project URLs go away later have a                                look see if they're still relevant to                                you learn some more from that so first                                 up loading and querying um pig                                 originally from yahoo I went through the                                 incubator now Quitely you quite widely                                 used and it's an easy way to query your                                 data and in Hadoop MapReduce jobs                                 typically I'm told it's about a                                 twentieth of the line of code that you                                 would involve if you were to be writing                                 in Java and it takes about a fifteenth                                 of time which is good but it's not                                 sequel right which can be an issue if                                 you're trying to give it to be I people                                 data scientists people who are unused to                                 sequel like ways it's usually nearly as                                 fast as writing the the Java yourself                                 but just a lot quicker for you you to                                 write and so there's a scripting way                                 through the shell or embeddable Java and                                 it has a local mode for development                                 which is important so that you can test                                 what's actually going to happen before                                 you let it loose on your cluster and                                 have it run for a week and it's got                                 built-ins for a lot of the common stuff                                 you're going to do when you need to load                                 in data will save it out and filter it                                 and then it's got hook point so you can                                 write your own custom udfs and inject                                 them and do your logic that way similar                                 range of functions to sequel but it                                 doesn't look like sequel so and it is                                 possible for BMI people to use it but                                 they they need a little bit raining a                                 little bit of hand-holding convince them                                 that it is worth persevering with gora                                 is an ORM framework for no sequel which                                 maybe seems a bit odd and but it since                                 you think well it's not sequel like why                                 would you try and map it like that but                                 it does seem to be useful for people who                                 want to deploy to multiple different                                 systems or want to have a layer of                                 abstraction originally it came from                                 Apache notch who found themselves                                 wanting to separate that out I currently                                 supports HBase Cassandra and hyper table                                 on the column storefront it also has                                 some support for key value stores and                                 flat files the way it works is by                                 encoding it into the data into Avro and                                 then it gives you ways to query it and                                 through java java api but you can also                                 query it with pig lysene hive and a few                                 other things and so this is just a                                 little example if you wanted to get                                 started you just define some fields ever                                 been and then just nice simple Java API                                 and away you go so if you are not sure                                 about which columnstore family you're                                 going to be targeting or anything like                                 that and you want to get up and running                                 you can maybe use something like gora to                                 abstract that away and then you can                                 change the back end while you firm up                                 your plans maybe later on you'd want to                                 move to a native API but in the early                                 days that could be helpful                                 accumulo is distributed key value store                                 built on top of Hadoop zookeeper and                                 thrift you'll hear a lot the projects i                                 mentioned i'll be built on technologies                                 like that inspired by big table but                                 they've tried to fix some of the design                                 issues with it and the important one                                 that they've tried to introduce is                                 permission the original work came from                                 the NSA so you can see why they might be                                 interested in doing some permissions on                                 top of these kind of things so you can                                 have cell level commissioning to control                                 what data someone can read and then you                                 can also have right constraints you can                                 say a particularly user can read but not                                 right and then it's got some nice little                                 server side hooks that mean that you can                                 manipulate the data as it's going in or                                 coming out and it supports very very                                 large rows so it can work with data                                 that's too big to fit into memory all in                                 one go how's high-availability using                                 zookeeper and it's got some nifty stuff                                 we've distributed right ahead logs and                                 that allows it to be high availability                                 and only right to HDFS later and it                                 supports logical time which is good and                                 so that if different machines have                                 different ideas about what now is you                                 can still get some sense out of it it's                                 a lot cheaper than what Google have done                                 which is just go and buy a few atomic                                 clocks for every data center that does                                 work but budget is kind of a bit                                 different and very testing friendly                                 they've got lots of mock objects you can                                 use functional tests scaling tests                                 random walks and if you were in here                                 earlier hearing you talk about some of                                 the work that lucene have done with                                 random testing you see that that's worth                                 scoop it doesn't have a you you will get                                 confused to not find the website if you                                 try typing in you as I did and it's a                                 bulk data transfer tool for getting data                                 in and out of big data systems so for                                 example if you've got a giant relational                                 data warehouse and then you've got your                                 shiny new Hoodie cluster and you want to                                 make the to talk to each other you can                                 make use of scoop and define very                                 quickly a way to pull the data across                                 and then when you're done doing all your                                 comparisons and calculations on Hadoop                                 you can then push it back in                                 so that your be eyes can make use of it                                 and work with it in the tools that they                                 used to chakra it's been in the                                 incubator a while so it's it does seem                                 to be having some issues with getting                                 enough traction but it's a log                                 collection analysis framework for Hadoop                                 for pulling in all sorts of logs from                                 many different systems you might have                                 out there getting them into Hadoop so                                 you can analyze them it's all very well                                 having a thousand web front end nodes                                 that are all logging away but if you're                                 only mechanism is a ssh into every                                 machine and running a grep to find out                                 what's going on not going to get a lot                                 of value out of those so using one of                                 these log collection frameworks you can                                 suck it all into somewhere and do the                                 analysis and get useful stuff out of it                                 so it lets you collect and aggregate                                 logs from many machines and then put                                 them up into chunks in HDFS so that you                                 can query and analyze them later and you                                 need to have a little agent running on                                 every single node that can collect from                                 all the standard to kind of sysadmin                                 friendly logging sources and then it                                 sends it to a special collector running                                 on the Hadoop node and that can do                                 transformations and you'll often want to                                 also write it out into HBase so that you                                 can do pretty visualization and then                                 after that you can do MapReduce jobs and                                 pig and so on Netflix one of the big                                 users of it and they do some nifty stuff                                 where they have all of their web                                 front-ends logging that goes through                                 chakra chakra does some transformations                                 into thrift packets saw some in HDFS and                                 also stores them in hide and then people                                 can go and run nice queries against hi                                 and see what's popular at that moment in                                 time on their front ends an alternative                                 trying to solve the same problem but                                 taking a different approach it's floom                                 floom concentrates on getting the data                                 off of the remote servers that                                 generating the log data and into Hadoop                                 as quickly as possible so it's all about                                 the quick throughput and getting it                                 ready to be analyzed and typically you                                 would actually have it going into                                 several different stores HDFS so that                                 it's there for bulk MapReduce put it in                                 hives you can do nice visualizations                                 full text search so you can query it and                                 find out what's been going on and it's                                 got slight different terminology and but                                 you have sources which suck in the                                 original events and then channels which                                 is where it puts it pending delivery and                                 then sinks which are the things that                                 actually write it out into Hadoop or                                 whatever it can do one too many many to                                 one all that kind of thing it's very                                 easy if you want to add in new sources                                 so if you've got your own custom logging                                 source that's that's easy to put in but                                 if you want to do really really custom                                 things you might be better off a chunk                                 or flume is much better for just how do                                 i deploy this get it out the box and I'm                                 working patchy blur incubating project                                 and it's trying to make it easy to                                 search structured data and a few of the                                 lucene people I've mentioned it to you                                 have raised eyebrows but people do seem                                 to be interested in it and it's a way of                                 being able to do indexing and searching                                 across large volumes of data that you've                                 already got stored in your Hadoop                                 cluster so it runs MapReduce jobs to                                 suck in the the data off of HDFS and put                                 it into thrift and then index that and                                 then distribute the shards across the                                 cluster so it's all nice and fault                                 tolerant and chard failover and so on                                 and then finally gives you a web base to                                 query interface where you can send                                 inquiries that will get distributed out                                 to the different shards and run against                                 the shards and get your answers back so                                 if you've got lots of data and you want                                 a relatively easy way to be able to                                 search through that it could be worth a                                 look Falcon is trying to give a complete                                 data life cycle management for the data                                 in Hadoop rather than you just saying                                 well and have these jobs they're going                                 to write into Hadoop some time later we                                 might think about what to do with them                                 or maybe we'll just buy an extra cluster                                 cuz it's cheaper to buy another rack of                                 machines but you have this in the work                                 out what the hell to do with the data                                 we've already got if you use Falcon you                                 can try and bring some order to all of                                 that rather than you try                                 to implement a life cycle and retention                                 policies and so on your coat you can use                                 falcon which is got its own set of unit                                 tests chose that largely works and then                                 just hook into the components and that                                 so you say well data coming in initially                                 needs to be replicated then don't bother                                 replicating until we finish processing                                 it but once the process is finished i                                 also need a rep for copy of that and i                                 need to keep at least one copy around                                 for a year for regulatory purposes so if                                 you've got those kind of data flow data                                 life cycle things don't roll your own                                 badly use something like that hammer is                                 an alternative to something like                                 mapreduce on her deep rather than                                 implementing MapReduce it builds bulk                                 synchronous parallel processing there's                                 some good papers on that if you're                                 interested you go and learn a bit more                                 about it but it works well for matrix                                 operations graph algorithms and network                                 algorithms and it also generally works a                                 lot better than trying to do the message                                 passing stuff yourself so if you've got                                 the kind of problem that's not mapping                                 well onto MapReduce but is a bog thing                                 and you've got those kind of algorithms                                 have a look at may be doing it with                                 hammer and BSP rather than doing with                                 MapReduce miracle and RQL but kind of                                 nice name it's a large-scale data                                 analytics engine that can run on top of                                 Hadoop with MapReduce jobs or on top of                                 hammer with the BSP jobs it has a sequel                                 like query language which I gather is                                 important for the bi people who get                                 scared when it's something that's not                                 sequel and and it has a lot of complex                                 types and constructs that are there                                 already so whereas with pig you might                                 end up having to write a little bit of                                 MapReduce stuff yourself when the                                 standard query won't do it in theory                                 miracle should have all that there and                                 and it can do the page ranking k mean                                 and all those kind of things quite                                 nicely very small sets of code I think                                 this is pronounced Tahoe maybe taiyo not                                 sure it would be great if more projects                                 have a little pronunciation guide on                                 their site                                 would also avoid all those issues about                                 is it gif or gif or whatever they                                 announced it up front so data warehouse                                 system for Hadoop which is trying to do                                 low latency queries some of the other                                 things that designed more for very bulk                                 parallel operations this one's much more                                 about how do I get a quick turnaround                                 for my queries so it uses HDFS for                                 storage almost all of these systems use                                 HDFS the Hadoop file system to store                                 their data some of the interesting                                 things come about when they come to                                 query it or analyze it so we're tired                                 taio they've got their own query engine                                 that does the executions and                                 optimizations and it will run apparently                                 native sequel queries it will be                                 filtering grouping sorting joining and                                 so on so most sequel queries will run                                 against it and then it will query                                 against your big data stored in HDFS so                                 now on to building MapReduce jobs if                                 you've decided MapReduce job is the way                                 forward let's have a look at some of the                                 things you can make use of avro is one                                 of the two language neutral data                                 serialization projects that Apache                                 thrift being the other one the idea with                                 these is when you're storing the data                                 into something like HDFS you want to be                                 able to query it and get the data back                                 out from lots of different languages if                                 you only have one language that can read                                 and write it you restrict what other                                 teams can do with it you restrict for                                 interesting uses you might better come                                 up with for it later so an avro uses a                                 JSON based format to do the storage and                                 then it has a binary data representation                                 from that so you define it in JSON and                                 it gets stored in a binary structure and                                 if you're using dynamic languages you                                 can skip the cogeneration step if you                                 want obviously if you're using                                 statically typed languages you're going                                 to have to have code generation for all                                 the standard getters and setters and so                                 on it does support our PC and the the                                 data format always includes the schema                                 so over time your data structures change                                 in you add new fields and so on you can                                 always figure out what the structure                                 walls at the time fire was written                                 which is often handy if at the start                                 you're not sure what it is exactly                                 you're going to be storing going forward                                 see schema is always present which means                                 you can do dynamic typing and dynamic                                 languages supports a fair range of                                 languages and the different languages                                 can talk to each other and make RPC                                 calls by passing the data structures                                 around and I'm told that it's often                                 faster than thrift and proto graph but                                 the downside is that everything has to                                 be loadable into memory there's no                                 streaming support so if you've got a                                 small amount of metadata and then multi                                 gigabyte video files it's not                                 necessarily going to be the best fit for                                 you because you've got to be able to                                 buffer it all up into Ram before you can                                 do anything with it thrift takes a                                 slightly different approach thrift                                 originally came from facebook but is now                                 very widely used and it does have our pc                                 support and it has streaming support so                                 if you need to be at a work with only                                 part of it and direct the data off it                                 might be a better fit and but it                                 compiles down into code so there's less                                 metadata available with the data at the                                 time of use and if you're interested in                                 any of this stuff there white paper is                                 really accessible and really interesting                                 if you want to understand more about the                                 data structures and the way of making it                                 or work H catalog is trying to give you                                 a table structure on top of war HDFS                                 files so rather than just having HDFS                                 files that contain random binary blobs                                 that you can't make sense of you can                                 just put a bit of metadata on the front                                 it actually uses hi its metadata format                                 pop that on the front of the file and                                 then you can more easily query it so                                 from your Pig jobs or your map produce                                 jobs you can then load in these these                                 blobs make sense of them process them                                 that way and it doesn't require                                 additional data store so it can still                                 work on top of                                                        you do is why putting this metadata on                                 the front its then easier to work with                                 mr unit is unit testing which is                                 important there are be a few people                                 around who maybe over a beer can tell                                 you some horror stories of what happens                                 if you don't properly unit test your                                 MapReduce jobs and then you discover                                 that this job has been churning away at                                 the entire cluster for two week                                 so no one else has been able to get                                 anything in and at the end of it you get                                 Duff data out cuz now untested it that's                                 bad so it is worth important through                                 some unit testing there was a talk this                                 morning on some really advanced unit                                 testing that you can do you can use mr                                 unit for some more basic stuff and it                                 gives you all of the the objects you                                 need to test all of the different                                 components individually without having                                 to spin up a huge Hadoop cluster just to                                 work out that you've got one of the                                 inputs wrong and it what you can because                                 you've got wheel kind of fake cookie                                 bits rather than are having a load of                                 brittle mock objects you can actually                                 play with all the different bits                                 individually and check that they glue                                 together and this is an example to show                                 how easy it is to get a simple unit test                                 up we've got about                                                       a that's a Hadoop unit test Hadoop                                 development tools are it's it's a very                                 new project and just come into the                                 incubator the idea is to provide a load                                 of Eclipse plugins to make it easy to do                                 map produce development and development                                 in Eclipse one part of that is being                                 able to work with multiple copies of                                 Hadoop in a given Eclipse instance the                                 other one is to do a Clips integration                                 for a lot of the common tasks that you                                 need to do as a developer how do I query                                 the class to find out what it's doing                                 how do i submit my job once I've got it                                 compiled and building in Eclipse for one                                 of something that's the cluster how do I                                 do that and importantly when my job's                                 going a bit wrong how do I attach the                                 Eclipse debugger on my current machine                                 to that Hadoop cluster over there when                                 my codes breaking to work out what's                                 going on not all of that is there yet                                 but it's something to keep an eye on                                 hopefully in                                                          that will be in place and you'll be able                                 to work within eclipse if that's your                                 preference but then work against a deep                                 Suzy is a workflow and data workflow                                 scheduler in dependency manager for                                 Hadoop jobs so let's say you've got some                                 raw log files that you're going to need                                 to do some initial analysis on and then                                 you're going to split the out into a                                 keyword analysis and a user agent                                 analysis and then you're going to do a                                 count job and slice it up wait                                 some of those have common parents and                                 you don't want to have to rerun the                                 whole process when you come to do the                                 second and bit of analysis if you can                                 define a workflow in a data flow then                                 you can use easy to spot when part of                                 it's already been done and we use those                                 intermediate steps recalculate the whole                                 thing if it's changed so it lets you                                 define data pipes and then say dear                                 Hadoop cluster please make that happen                                 and give me some nice results out at the                                 end of it big top is one possibly a bit                                 more further siblings out there and it's                                 a way of letting you build and package                                 Hadoop based systems for example you                                 would want HDFS and Hadoop court and                                 zookeeper and for your given set up and                                 then you need to deploy that out for                                 cluster if you use big top you can then                                 generate all the packages for that you                                 give to your sis admins that they can                                 push out with something like puppets but                                 then equally you can say well what                                 happens if we upgrade to a newer version                                 of zookeeper all that breakout setup                                 it's not still compatible with the old                                 version of HDFS that we're using and it                                 will let you do integration testing                                 across the whole stack to work out                                 whether or not an upgrade is going to be                                 a trivial thing where it's going to be a                                 massive amount of work ambari is                                 monitoring and lifecycle for a whole                                 Hadoop cluster so if you want you can                                 use something like big top build up the                                 packages and then deploy them using                                 puppet and your sis admins can have full                                 control over it and bari is trying to                                 take it up a bit of a step and then have                                 a REST API where you can say and Bari I                                 need a cluster with zookeeper and pig                                 and HDFS core please go and make it                                 happen so it's bringing it up stack to                                 make life easier you get less control                                 over it but then if you've got                                 developers you need to deploy code you                                 don't want to be messing around with                                 puppet and building packages and things                                 then I'm Bari might be better it just                                 depends on the kind of sea serpents                                 skills you've got in a house kind of up                                 dead skills you've got in-house there's                                 the speed of changes that being made but                                 this potentially can let you spin up                                 particular set of machines a bit more                                 quickly so moving on so looking at some                                 things for sort of cloud scale                                 deployment and very large Big Data                                 deployment provider independent cloud                                 api's sounds a bit of a mouthful but you                                 probably want one of these if you're                                 going to be deploying to public clouds                                 so it lets you provision a managed and                                 query without using an API native to                                 that provider so when that provider does                                 something silly or changes their pricing                                 or someone new comes onto the scene you                                 can then just change your configuration                                 file and say today we're going to be                                 talking to rackspace because they've                                 given us a good deal rather than saying                                 okay to change an ECG to rackspace we've                                 got to rewrite everything so typically                                 they do that all the basic stuff create                                 start/stop reboot destroy tell me what's                                 running lib cloud is aimed at scripting                                 it's mostly in Python and you just write                                 a                                                                    cloud instance running on ec                                     Rackspace or whatever so that's one                                 option if you like writing scripts to do                                 it the other option is Delta cloud which                                 has an XML REST API and without you can                                 just post a snippet of XML to it and it                                 will spin off a machine and then you do                                 a query and you get back to mex ml                                 telling you what's currently running                                 when you start playing with these things                                 it's very easy to accidentally run up a                                 huge credit card bill the querying is                                 important and having a cron job that                                 periodically emailed everyone says do                                 you know that we've currently got                                    large instances running on ec                                           had for four hours is that deliberate                                 rather than the credit card provider                                 ringing you up and going I'm sorry but                                 you've reached your limits because                                 someone called Amazon services EU has                                 just charged six thousand dollars to you                                 that one's a bit more problems so even                                 if you're not going to use anything else                                 from it maybe have one of these running                                 in the background warning you that you                                 finally load of ec                                                     gone off on holiday and forgot about                                 them                                 um provisioner tries to be a higher                                 level than all that those to tend to be                                 more aimed at people thinking about                                 individual machines provisioning them                                 bringing them up stopping them                                 parishioner tries to come higher level                                 and you go to prisoner and say I need a                                 cluster with                                                             to have this kind of structure and then                                 it builds on top of puppet and spin                                 crates and machines and then uses puppet                                 to deploy all the code to them and it                                 handles all the retries and the                                 throttling and everything like that to                                 make your cloud just come up and use as                                 a provider independent cloud API so you                                 can say please bring up a cluster split                                 into two parts one part on Amazon in                                 here at one part in rackspace in Texas                                 and bring the whole lot up or you can go                                 further down and use something like                                 cloud snack to run your own private                                 cloud it's got all the software you need                                 to do your own IaaS now compete platform                                 much the same as your own little mini                                 amazon and it works with all the                                 standard hypervisors it's got nice                                 restful interfaces web interfaces CLI                                 tools and it can pretend to be easy to                                 if you've got some code that's already                                 written to talk to easy to you can just                                 change the the IP address that it's                                 going to talk to and it can talk to                                 cloudstack so there's a lot of tools out                                 there that will let you pick the point                                 in the snack that's going to be right                                 for you to target if you've got lots of                                 siblings lots of DevOps people maybe you                                 can go quite down low if you've mostly                                 got lots of developers and some VC money                                 but notice admins and you just want to                                 throw a credit card at the problem you                                 can look at some of the technologies                                 that bring it higher up the stack and                                 you can say look here's a credit card                                 and I need to tend note Hadoop cluster                                 and I needed it                                                        make the problem disappear for me so                                 some service related stuff um zookeeper                                 is a whole load of low-level services                                 and building blocks that let you do                                 configuration and naming and                                 synchronization so when you start trying                                 to bring up a whole load of different                                 machines it's not very good to have this                                 mingo round and log into a machine and                                 say right I need to start this service                                 on here on this service on here and it's                                 just broken so let me log into over here                                 make some changes zookeeper you can                                 define up front what should be going on                                 and then your systems can automatically                                 come on so when we were looking at the                                 start at some of the solutions a lot of                                 those you zookeeper so that they can do                                 the high availability and the automatic                                 naming and numbering and all that sort                                 of stuff so the system can just come up                                 but if you're not going to use one of                                 the standard tools but you are going to                                 bring together lots of different                                 components into a big cloud system you                                 should be using something like dookie                                 but don't try and write your own                                 distributed system for working out which                                 one's going to be the leader you're                                 almost certainly going to get it wrong                                 and have bugs kept one that someone else                                 is already bug fixed so zookeeper                                 distributed highly reliable it's not                                 single point of failure and it has all                                 the primitives you need to do consensus                                 tracking and groups and subscribe and                                 that sort of thing it's often described                                 as a central nervous system for                                 distributed and absent service and it's                                 got binding through all the languages                                 you want to use alternately go a little                                 bit higher art and use curator which is                                 a keeper for zookeeper so it provides                                 out-of-the-box implementations for a lot                                 of the stuff you'd end up writing on top                                 of zoo keeper and it and deals with                                 retries nearer cases and that sort of                                 thing and it's got some very nice test                                 support the allow you to bring up a                                 little unit testable in memory so you                                 keep the cluster and work with that so                                 if you want to be doing a lot of the                                 standard stuff potentially you zookeeper                                 or possibly now the curators out there                                 what you should be doing is going to                                 curator and using one of the standard                                 operations built into curator and have                                 it deal with zuki before you and just                                 say I need the machine to come up and                                 connect to the cluster and work out what                                 its number should be and just write two                                 lines of curator code and underneath w                                 couple hundred lines of zookeeper code                                 and underneath that it'll works don't                                 try and write your own distributed stuff                                 for this it's really really hard if you                                 are smart enough to do it go and join                                 one of the existing projects airavata is                                 a toolkit that lets you build manage and                                 monitor very large scale applications                                 but its workflow driven so you can                                 define a workflow he actually uses the                                 activity workflow engine to do a lot of                                 it and it was originally there for a                                 scientific processing and it's expanding                                 out and end users can define the                                 workflows and then and then launch and                                 have the data processing happen were                                 guru from Hadoop and it again it's                                 you've got some high-level services that                                 you define and say look just go off and                                 make this stuff work for me for more the                                 Java developers out there I've been                                 mentioning some of the tools that you                                 can use for deploying whole Hadoop                                 clusters but if you need to deploy some                                 quite complex java stacks and especially                                 if you're using osgi then you can use                                 ace to deploy out eats osgi itself but                                 it can work for most things and think of                                 it as a puppet at the OSGi level so it                                 does all the kind of stuff that puppet                                 would do let you track and configure and                                 roll forward and back and all that sort                                 of thing and but it's there for a high                                 level if you're using osgi for your                                 large-scale java stuff apache river is                                 toolkit for building distributed systems                                 it's a little bit like zookeeper it's                                 been around for longer and it uses ji ni                                 for a lot of things so if you're in an                                 organization that's already doing a lot                                 of java stuff like java naming stuff and                                 the way that zookeeper names and                                 addresses things is looking over alien                                 to you you should have a look at maybe                                 using river if you're coming to it fresh                                 may be better off sticking with                                 something like zookeeper or curator                                 which newer and seems have a bit more                                 momentum behind them helix is cluster                                 framework cluster management framework                                 which is based on a state machine see                                 you say and define states and                                 transitions and define how it should                                 handle if things start to go wrong when                                 you things come up                                 and then you can define jobs that should                                 happen at certain points in time you                                 should say every six hours this job                                 needs to be run once on the cluster or                                 once inch regional zone so you can                                 define how to bring up an entire Hideo                                 cluster have it run what to do if                                 machines start falling over and then                                 also make sure that the backup happens                                 once not zero or two times or your                                 maintenance so lets you have a sort of                                 globalized brain for the whole system                                 Knox is and think Fort Knox security and                                 so on so if you're spinning up random                                 Hadoop clusters on public cloud systems                                 there's a bit of a security issue there                                 which is if you've got your Hadoop                                 publicly available to everyone then                                 anyone can come along and login to the                                 admin system and it's still got the                                 default password and that's bad so the                                 idea with Knox is that you have your                                 hoodie plus to come up all on private                                 addresses and then knocks acts as the                                 interface between the outside world and                                 your cluster CUA user will connect to                                 Knox authenticate to Knox and then                                 they'll submit their job through and                                 fetch the data back so it's such oak                                 point for Hadoop level Application                                 Firewall me sauce is trying to do                                 virtualization on top of your cluster so                                 you've got your cluster there and                                 running and you've got the engineering                                 group who want to be able to run the                                 latest version of Hadoop to test out the                                 code and you've got the bi guys who want                                 to have a nice stable hooded cluster                                 running one from six months ago and then                                 someone else wants to try out an MPI job                                 you can use measles to deploy in the the                                 different versions and on top of the                                 same machines and then have different                                 clusters running on top of the physical                                 machines and people doing different                                 things so if you've got sort of two                                 backs of your own servers and lots of                                 people calling to do different things on                                 it one option is to virtualize the whole                                 thing and then spin up a new virtual                                 servers on top of it and let people                                 deploy their own things alternately you                                 can use something like me Zeus and                                 virtualizing applications on top of the                                 machine you can slice it different ways                                 depending on what seems to work best for                                 you                                 and then finally then got about five                                 minutes and they run through some other                                 projects that might help you round out                                 your solution so if you're dealing with                                 binary documents that are coming in from                                 different places teka lets you get out                                 the text and metadata from them it's                                 often used if you're building search                                 based systems because the internet and                                 vile stores are full of random PDFs and                                 word docs and images and you want to get                                 the metadata and text out and he can                                 give you a consistent interface to all                                 those documents you EEMA lets you get                                 useful information out of unstructured                                 blobs of text you can do that with C++                                 or Java it's the main technology that                                 was at the heart of the IBM jeopardy win                                 so they just had dumps of wikipedia                                 dictionaries and all that sort of thing                                 and you email let them pull out the text                                 and then they had all sorts of                                 interesting code on top that weighted it                                 all and made it work if you've got lots                                 of texts that you need to try and get                                 information from you mean there could be                                 worth a look or alternately open NLP                                 which does natural language processing                                 and if you need to be tweaking and                                 you've got academics you want to play                                 with different models open NLP might be                                 better if you just want something that's                                 more turnkey you eNOS probably the the                                 one for you and if you've got clinical                                 data which is something close to my                                 heart with my current company see takes                                 sits on top of both you email and open                                 NLP and tries to pull out useful                                 clinical data from the unstructured                                 notes from clinicians and try and map it                                 into different bits and there probably                                 are other projects out there that do the                                 same kind of thing for other knowledge                                 areas so if you have got lots of                                 unstructured text start looking at                                 extensions onto you EEMA an open NLP                                 that can turn that one megabyte block of                                 unstructured text into something you can                                 work with and then analyze and then                                 throughout your Hadoop cluster Mina is a                                 framework that lets you do highly                                 scalable network apps so if you've got a                                 really really big Hadoop cluster that's                                 doing lots of stuff and need to talk to                                 external systems you don't want to have                                 a single threaded network server                                 somewhere in the middle because that's                                 all just going to melt when the whole                                 hoodie cluster tries to talk to it so if                                 you do find yourself writing                                 network services for a big scale meaning                                 could be worth looking out for a lot of                                 the basic building blocks to use or                                 alternatively H is a slightly higher                                 level thing trying to solve the same                                 problem with H you write a formal                                 description of the messaging framework                                 and then it compiles it down into the                                 high-performance network code for you                                 you're probably going to need queues at                                 some point one option is you can use                                 some of the primitives and something                                 like zookeeper or alternatively pick one                                 of activemq Cupid or snaps based on the                                 languages and the way it slices down if                                 you think you need a cue go and read                                 their websites and pick the one that                                 looks right for you they're solving the                                 same problem but it's the way they                                 attack it may be a good fit or a bad fit                                 for you and putting three message                                 brokers on the same slide is definitely                                 those were compliant and Kafka is doing                                 push-pull again trying to pass messages                                 around and queuing takes a very                                 different approach but could be worth                                 looking at how are we doing almost at a                                 time s                                                               continuous streams of data do processing                                 on the stream rather than buffering it                                 all into memory working your way through                                 it and spitting it out trying to do                                 real-time work on the stream Apache                                 Commons has huge numbers of libraries                                 and let you do a lot of the stuff that                                 you're going to need to do the ones that                                 might be relevant to the big data one                                 will be CLI for doing the argument                                 passing when you bring it up codec for                                 working with the different encodings and                                 compress and then if you want to do your                                 own and processes that going to start                                 and stop and windows on Linux and                                 everything then demon could be worth a                                 look jmeter lets you do load testing so                                 if you've got external servers that                                 you're going to be talking to jamies to                                 let you work out whether or not that                                 will cope before you have seven racks of                                 machines or trying to talk the same                                 thing with melted into Sansa shouting if                                 you've got repositories document                                 repositories doctor management systems                                 Apache chemistry gives you a rich view                                 of the data in them wholly different                                  languages they can talk to but it'll let                                  you connect to your document system                                  navigate around check the permissions                                  check the metadata                                  or alternatively manifold CF it's just                                  sort of Hoover for a lot of these                                  document management systems that's just                                  aimed at ingesting or the text as fast                                  as possible into something like solar so                                  use chemistry if you want a highly                                  structured view on the data or manifold                                  CF if you just want to buy the whole lot                                  up put it in your deep cluster and                                  analyze it so you've got about three                                  minutes left for questions you're                                  probably all stunned but as i said i                                  will be putting these slides up online                                  in about five minutes time have a look                                  through try and see the ones that maybe                                  interest you find me later on if you                                  want and i can try and tell you a little                                  bit more about them and also point you                                  out more information about them i don't                                  know a lot about many of these it's                                  trying to give you an overview but                                  hopefully it's picture interest giving                                  you some things you can go home read                                  about learn about and use to build your                                  solution rather than reinventing the                                  wheel with only those pesky three sides                                  so any questions in two minutes or do                                  you just want to go and breathe and                                  think i'll take the stunned silence as a                                  no come find me at six o'clock bring                                  your beer and i'll talk to you more                                  thank you                                  you
YouTube URL: https://www.youtube.com/watch?v=iMf24-gGR_0


