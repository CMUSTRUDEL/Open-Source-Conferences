Title: Berlin Buzzwords 2013: Michael Stack - All That's New in Apache HBase #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	Come hear about the latest developments in Apache HBase. Learn about all the good stuff our diverse community of contributors -- HortonWorks, Intel, Facebook, Salesforce, Taobao, Yahoo!, Cloudera, and others -- have stuffed into our fat new HBase 0.96 release and what these contributors are busy working on next.

Read more:
https://2013.berlinbuzzwords.de/sessions/all-that%E2%80%99s-new-apache-hbase

About Michael Stack:
https://2013.berlinbuzzwords.de/users/stack

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello that's loud so                               you see oh this one's up moving okay                               you see his friends sorry technical                               difficulties let me see if I put this                               one over I can                               I can get something to work am I too may                               spit rough but bear with me so those                               faces a scalable distributed data store                               a modeled on Google's BigTable paper                                we've implemented most of it I suppose                                we have different nomenclature still                                some stuff to do but roughly there we                                grew up out of a patchy Hadoop project                                we were originally a contrib now a                                top-level Apache project you'd use HBase                                one you want to add random read/write to                                your to your your head you'd stack                                that's what you'd want it kind of a key                                value store sort of thing we're on top                                of a how do you say a distributed file                                system HDFS and then we use things like                                zookeeper fur cluster coordination who                                uses it here's a few of the people that                                use it all these people are interested                                in scale that's why they're using a                                HBase Facebook they've talked a lot                                about their HBase use it's like used for                                like their metrics the ODS system there                                you know the eyes and ears of what's                                going on at Facebook it can't go down                                who else is in there runs on a Yahoo's                                grid there's a people like the flurry                                people they they all your mobile apps                                they all report home to hbase cluster                                that's the biggest that we know of the                                biggest contiguous cluster that we know                                of it's like north of a thousand nodes                                and then that note that cluster itself                                is replicating into two other data                                centers that are also of a similar size                                but it's been all about scale how does                                it used a teaser tower just dump it out                                on on one slide DM there's this there's                                the data store is the dumping ground all                                kinds of user all kinds of you know                                whether it's like a website like say                                stumble upon you the page when they                                serve a page is made                                of multiple features those features                                could be written by different engineers                                the data set that they're pulling from                                can have different characters but                                they're all the work they have the one                                backing store behind all Yahoo similar                                when they run HBase on the grid have all                                kinds of tenants doing all kinds of                                merit data scientists kind of things and                                people like sales for Salesforce well                                you know give customers a table to do                                what they want with developers generally                                like this because they don't have to                                worry about scaling somebody else is                                doing it for them they just get the tap                                on the shoulder from ops if they do                                something really stupid such that they                                disturbed co-tenants um people like eBay                                you know when you search for items in                                ebay it's the data that goes into the                                search engine is comes out of a HBase                                reconciliation stored is like actually                                many databases that they pull from and                                then they put it all into a single HBase                                cluster at the center of their indexing                                pipeline they like it because there's a                                time dimension so they can do take a                                snapshot of the of what's for sale at                                any particular time then most people                                around HBase they'll also maybe use it                                as the backing store for their metrics                                things like open TSD be the application                                that gives you like you know nice graphs                                and what's happening in your you know in                                your cluster is your machines you get                                your applications to to right into the                                metrics store this is what facebook's is                                OD OD s is and this is like the first                                production use case of HBase at                                Salesforce this is what it's doing is                                they have their own metric system so                                they can see what's going on across                                their many clusters so hey space is it                                plays in many positions is doing a                                people are using it for all kinds of                                things not all necessarily always                                appropriate we're doing like OLAP and                                oil TP if you try to do both at the same                                time we still have some work to do so                                who runs the project                                Pachi Pachi project as i said earlier                                 but the main committer teams come out of                                 these companies until have quite a they                                 have they have a stake in the game now                                 have three committers there's Facebook                                 have a big team to run their internals                                 Cloudera where I work we have five or                                 six people seven maybe hortonworks                                 similar the point of trying to make                                 years there's a bunch of people driving                                 the HBase project forward what's the                                 current state we are current stable                                 offering is                                                             last week                                                              is settled down nicely release every                                 month each one more stable more                                 performant some features but the bulk of                                 my talk will be about                                                    summer the thing that we've been working                                 on for the last year our summer                                 blockbuster I have like a thousand fixes                                 or more so far but let me talk about                                 some of the main focus that has been                                 behind the                                                              time on mean time to recovery and once                                 this mean in the HBase context so in the                                 old cap triangle HBase favors                                 architectural favors a consistency over                                 availability so there will be times when                                 you're down is offline so when a server                                 crashes or maybe were rebalancing or                                 stuffing split so there's these phases                                 during which will you know and when the                                 data is offline that we as these phases                                 of recovery that we've been working on                                 trying to make the window smaller like                                 you know detection repair notify each                                 server runs a right ahead log at the                                 right ahead log of whatever's in memory                                 and you know to get before you can get                                 the data back online if you want to give                                 a consistent view you need to replay all                                 those edits the way works at the moment                                 is the edit                                 the all the war logs have farmed out                                 over the cluster to be the split                                 distributed in a distributed fashion so                                 you know improving our mttr has been in                                 this been there's been big strides made                                 in                                                                    HDFS fixes like if a data node hasn't                                 checked in in the last recently then you                                 know don't ask it to do help you during                                 the repair process just skip it all                                 these windows have been I've been shrunk                                 things like I talked about the                                 distributed log splitting be previous                                 fees to keep the data offline while that                                 was going on but now we actually bring                                 the data online immediately so you can                                 start taking rights again but you can't                                 take reads HBase comes out of them like                                 I say hey chase came out of Hadoop we                                 use a lot of the basic how do                                 infrastructure in a space and one place                                 in one thing we use in particular is the                                 serialization format it's a you actually                                 handwrite how you want something object                                 to be sterilized and how you're really                                 back in again this is actually a problem                                 when an object to contain an object                                 another one contain an object and they                                 all revolving at different rates and                                 then if you want to put that whole thing                                 over over the wire do an RPC with it                                 that itself was versioned we it made us                                 so we couldn't evolve we could evolve                                 servers independent of clients and we're                                 trying to be an online serving system                                 that we were bound up we were looking                                 for the freedom to evolve so we moved                                 there everything over to protobuf                                 tseverywhere we persist we do in proto                                 bus we're insistent tables file systems                                 up on zookeeper or only put stuff over                                 the wire we also inherited the Hadoop                                 RPC and over the years there are PC is                                 accumulated an awful lot of croft uses                                 things like java proxy there was a                                 pluggable engine for whether you wanted                                 security or not or did so you could                                 choose different civilizations                                 and we just got rid of all that and we                                 just moved to protobuf service the whole                                 thing is just cleaners and clean it's so                                 clean now we wrote a spec to describe it                                 but again this is all about freedom to                                 evolve we have snapshotting in                                          schlep snapshotting it's good primitive                                 for doing things like backups                                 replication or offline processing                                 processing processing you go against the                                 snapshot rather than maybe the heat face                                 API our equivalent of listings optimized                                 has had a bunch of work done on it it's                                 pluggable we have different                                 implementations it's probably my                                 favorite thing is we have a whole suite                                 of integration tests we borrowed freely                                 the netflix chaos monkey you can you                                 know let it run loose in the cluster                                 that's running either in on a single                                 node or you can size it so that the same                                 test is being run on and in n nodes or                                 and you can ask it to run for like the                                 weekend or                                                              really great at finding issues                                 miscellaneous smarter load balancing                                 revamp metrics you I it'll run on how do                                                                                                          in HDFS go local avoid the data node tax                                 if the data is local this notion of                                 favorite nodes it's really important                                 keeping data locality this thing came                                 from the lads of Facebook the when you                                 write your replicas you you dictate                                 where they're going to be so that if                                 there's a crash when you come up again                                 you come up on one of the favorite nodes                                 and your data remains local I'll talk a                                 bit about ecosystem HBase ecosystem you                                 know there's stuff in HBase for you know                                 so hooking up your Python or your your                                 Ruby or nice you eyes that are better                                 than ours or but there's been a bunch of                                 energy expended in                                 one particular area a bunch of ecosystem                                 energy expanding one particular area and                                 arielle i like to call the chasm this is                                 me last weekend out for a ride and so                                 this is this is my this is my                                 electricity bill actually comes out of a                                 every time i get a bill my usage over                                 time is shown this is provided to to my                                 power company by this                                                  of HBase of the stuff coming out of a                                 HBase cluster the so you have this nice                                 little application over here and then                                 but then over on what you on the right                                 hand side is the is our dirty raw bytes                                 API that that's all you have an HP so                                 how do you write applications against it                                 how do you make it easy to write                                 applications against this raw API                                 friends of ours we be data they've been                                 there their expert living experts in                                 teaching people how to write big data                                 applications they've been doing it for a                                 while and they came up with this                                 framework the kiji framework it's you                                 know it's the                                                          choices they are baking their experience                                 teaching people how to do big data apps                                 and into this framework there's a those                                 kind of basic edit these entity centric                                 simple model policy realizations done                                 with Avro now what kinds of types                                 compound types each cell is versioned                                 and so and that's useful when you know                                 if you want to change versions the                                 framework takes care of it for you the                                 same framework works whether you're                                 going to the data via rest or V over                                 there MapReduce they'll make sure that                                 you have the same view all over open                                 source and production deploys as people                                 have been made you know jump in the                                 chasm using sequel for a long time                                 yesterday we heard Michael and Ted                                 talking about apache drill and myself                                 was talking about impala this is a chevy                                 impala i couldn't find the logo for                                 holla sorry Marcel and but these the                                 promised here is that you'd have a you                                 have a sequel query and it doesn't                                 matter what the data sources are and                                 these lads have been making working on                                 so that you know hey Tris can be                                 first-class data source clustering are                                 the people at Salesforce they've been                                 writing this Phoenix it's like the skin                                 hey space as a sequel skin of a HBase it                                 some exploits our triggers and stored                                 procedure facility our coprocessors and                                 custom filters you just get the H space                                 for your JDBC driver so you know sqi                                 line or are a squirrel HBase shows up                                 it's nice but a lot of energy has been                                 expended in this area making apps easy                                 to run on each base or just write apps                                 on HBase what's coming next it's about                                 time we call it one point oh it's been                                 around for long enough we kept getting                                 beat up on this faster scan so maybe                                 like slower writes better compacting so                                 that you know it comes read time and get                                 more out of it some inspiration corner                                 stores latency resilience I Jeff Dean                                 calls it latency tolerance but this you                                 know bring home the outliers if there's                                 a bad disk or running on ec                                              be so extreme in HBase world what this                                 means is if if you're if you're trying                                 to read from one replica and it's not                                 coming back fast enough start up the                                 second read against another replica                                 that's a reit I'm on right time make                                 multiple wall writers me people with                                 spending time here and related is just                                 quite giving quality of service                                 guarantees it's hard in a HBase given                                 that you know where such you were you                                 know we're running on HDFS and then HD                                 messes you know it's the hardware and                                 all those things if you wanted to go                                 real qos guarantees there's a lot that                                 has to be                                 up but we should be able to do something                                 primitive so that if you're running that                                 MapReduce job against your HBase cluster                                 you can steal some aspect of low latency                                 serving out the front end without                                 without it being affected something                                 course in here i will bring that design                                 principles book and the way we we offend                                 in many ways he traced project does but                                 this one kind of jumped out the it took                                 they talked about the flexibility                                 usability trade-off I think when fellows                                 add features to hbase they usually had a                                 bunch of configuration as well                                 configuration that nobody really looks                                 at I just in general I think the for the                                 project we want to move towards like                                 mirga nomics like the JVM just you know                                 adapt to the workload ourselves                                 internally and just in general be more                                 forgiving I just the general take away                                 as we've been doing a load of work over                                 the last year and then                                              landing maybe july and divorce or middle                                 of also so please keep an eye out for it                                 just try it out thank you                                 you                                 you
YouTube URL: https://www.youtube.com/watch?v=8hMVFST7Wuw


