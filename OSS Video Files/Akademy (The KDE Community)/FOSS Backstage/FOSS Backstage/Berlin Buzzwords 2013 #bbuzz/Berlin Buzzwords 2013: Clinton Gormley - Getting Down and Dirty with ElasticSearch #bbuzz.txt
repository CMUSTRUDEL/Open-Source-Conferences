Title: Berlin Buzzwords 2013: Clinton Gormley - Getting Down and Dirty with ElasticSearch #bbuzz
Publication date: 2013-06-19
Playlist: Berlin Buzzwords 2013 #bbuzz
Description: 
	We've had talks about how ElasticSearch can scale search and analytics to hundreds of nodes and terrabytes of data. We know it is up to the job, big or small. The question now is: How do I use it? How do I go from a list of requirements to a functioning performant application?

This talk will take you from the beginning -- how to get your data into ElasticSearch -- through the process of tweaking the schema to support your search requirements. We will talk about how to construct complex queries using the query DSL, and the tools available for understanding search results. Finally, we will talk about using facets and suggesters to help your users navigate your data.

By the end of the talk, you should have a solid understanding of the tools available to you in Elasticsearch.

Read more:
https://2013.berlinbuzzwords.de/sessions/getting-down-and-dirty-elasticsearch

About Clinton Gormley:
https://2013.berlinbuzzwords.de/users/clintongormley

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi everybody I'm Clinton Gormley I work                               for a last                               was elasticsearch his first user about                               three years ago I went live with version                                                                                                  bugs and improvements since then and for                               those of you haven't heard of                               elasticsearch or don't know much about                               it it's a real-time search and analytics                                engine there are a lot of buzzwords                                associated with it it is distributed                                out-of-the-box                                start                                                                  a cluster and with that you can scale                                massively                                the clustering also gives you high                                availability so it can survive node loss                                the you speak to it using a very simple                                restful api and json over HTTP so this                                is something that you can do from the                                command line or from any HTTP client in                                any language that you want to use it is                                schema free you don't have to set                                anything up to start using it                                multi-tenancy you can host multiple                                applications with it like you would                                expect on any database open source and                                based on Lucy okay so this is all cool                                and but this is why we use elasticsearch                                this is the thing that makes you decide                                to download it unzip it and start it up                                but then what you've got this that you                                don't even notice if it's up so this                                talk is about getting you from zero to                                understanding the fundamental concepts                                in elasticsearch and becoming useful                                with it it's not going to make any of                                you an expert through this but it'll get                                you a long way down the road so how do                                we use it we said we could talk to it on                                the command line and the way we                                represent all of the interactions in the                                documentation so on is using curl and                                this is a very simple request the first                                thing we do is specify a verb and it's                                restful so get put post delete head                                though the things that you used to from                                any restful application we'd say which                                node we want to talk to and you can talk                                to any node in the cluster                                but we're just running this on local                                hell                                so that's one will talk to                                             default HTTP port and we specify the                                path to this restful api and any query                                string parameters well most of this                                stuff doesn't change between queries so                                I'm just going to show you the bits that                                do change so if we just request the                                route path we get a response back and                                it's been pretty printed because we                                specified pretty this tells you the node                                is up and running the version in sass                                and so on okay that's cool we have a                                running cluster containing one node and                                I'm not going to get into clustering at                                all this is about how you use it rather                                than how you scale it so where do you                                start well you've got a bunch of data                                first of all and data is represented as                                JSON documents so this is a simple tweet                                document we've got the content of the                                tweet the username of the person is sent                                it the name of the person the date they                                sent it the retweet count and the                                geolocation of in this case me how do                                you put it into elasticsearch you put it                                you need to say where you're going to                                put it and you have to specify the index                                which is like a database in the                                traditional database we'll imaginably                                call up my app then what is it well it's                                a tweet and which tweet is it we'll just                                call it tweet one and if you don't give                                us an ID then elasticsearch will                                generate one for you but this ID is                                unique in this index and type then we                                just pass it the JSON body and what I've                                done here this is a curl parameter that                                says post this body ok or in this case                                put this body and we've just thrown this                                at elastic search we haven't created an                                index we haven't said what fields we've                                got specifically before indexing it                                we just throw it and we get a response                                back it tells us that it has been                                created and gives us a bunch of metadata                                which is the same thing that we passed                                in plus this version number which I'll                                 come back to a bit later                                 to get the document back up it's gonna                                 be no surprise you get it and you get                                 back all the same metadata including the                                 source field which contains the original                                 JSON document that you passed in okay                                 that's all good and well we can tell is                                 it exists using the head API and you                                 don't get a body response back you just                                 get an HT to be a response code                                                                                                               and this does an atomic delete and put                                 okay so you keep adding documents and                                 the old Dilys versions get removed over                                 time but it's really indexing a new                                 document and you get back at                                                                                                                   already existed the version number has                                 been incremented - to delete the                                 document you just delete it and again                                 the version numbers being incremented so                                 what's up with these version numbers                                 well this is running in parallel                                 multiple changes can happen via multiple                                 processes at the same time and what you                                 don't want to do is lose changes                                 actually at all the time you don't                                 really care but in situations where you                                 do care you want to be able to control                                 that elasticsearch uses optimistic                                 currency controls to do that where it                                 just keeps a version number that gets                                 incremented for every change that                                 happens which allows it to happen                                 without any locking because locking in a                                 distributed system is is a failure so if                                 we know which version we wanting to                                 update we can specify it and as long as                                 it's the current version it all work if                                 we try to update an older version we'll                                 get a conflict and that change won't be                                 committed so at this stage you need to                                 handle this in your application but it                                 allows us to be safe about the changes                                 that we're making you can also update                                 documents in place here's an example                                 where we use the update endpoint and we                                 pass the script which says increments                                 the counts filled in in the source                                 document                                 actually this is doing exactly the same                                 thing as we did before it gets the                                 document it changes it and tries to put                                 it again                                 the difference is that this is happening                                 local to the shard you haven't got the                                 network round-trip to take into account                                 which reduces the chances of there being                                 a conflict but you can still have                                 conflicts and here you can tell us how                                 many times you want to retry this                                 process until it eventually fails back                                 to the client lastly if you have if                                 you're trying to make lots of changes                                 it's cheaper to do it in bulk you can                                 send thousands of log entries than in a                                 single request rather than having to pay                                 the price of network overhead for every                                 single request similarly you want to get                                 a whole bunch of documents back by ID                                 you can use multi get to get them back                                 I'm not going to go into this because                                 this is the interesting part of the talk                                 this is just how to use elasticsearch to                                 store your data most of the time you'll                                 have your data somewhere else already                                 it'll be an existing application and you                                 have it sitting in some other data store                                 and that's fine we can work with any                                 data store you need to mirror the data                                 from your store to elasticsearch but all                                 can work hunky-dory from there if you                                 are starting any application and you                                 don't want to use another data store                                 talks elasticsearch directly up to you                                 okay so now we index a bunch of Doc's                                 we've got them sitting in our elastic                                 search index how do we search them let's                                 start with the empty search just give me                                 everything you've got                                 we get a response back that tells us how                                 long it took in milliseconds whether it                                 timed out or not that's an optional                                 thing that you can set if you so desire                                 how many shards were successful now I                                 said I wouldn't go into any of the                                 clustering or any of the distributed                                 stuff but it's worth knowing that an                                 index is actually just a virtual                                 namespace that points out a number of                                 different shards and these shards are                                 where your data actually lives and                                 because these shards are independent                                 they can be moved across different nodes                                 in the cluster allowing you to scale out                                 so searching an index actually means                                 searching all                                 the shards involved getting the results                                 back producing that and then returning                                 it to the user so we don't expect shards                                 to fail but you know if too many nodes                                 go down you're going to have missing                                 shards and elasticsearch tells you how                                 many try to hit how many were successful                                 and from the ones that were successful                                 it gives you the data it can so if you                                 do lose a shard it's not necessarily the                                 end of the world you can still serve                                 partial data not optimal but it can help                                 then you have the hits element which                                 tells us that we have a total of                                    documents in in our results there's the                                 max goal which I'll explain in a minute                                 and then a list of the top ten documents                                 that matched our query each entry in                                 this hits array gives us exactly the                                 same metadata you've seen before the                                 original document which means that you                                 don't need to fetch it in the separate                                 phase if you've got the whole document                                 there already for you to display and                                 finally the score for this particular                                 document that school says how well this                                 document matched the query that you                                 specified we didn't specify a query so                                 all documents are equally relevant and                                 they all have this neutral score of one                                 you can search across multiple indices                                 and multiple types I said that searching                                 a single index was already hitting all                                 of the shards of that so searching                                 multiple indices is exactly the same                                 process but extended to more shards how                                 you can specify it here you specify you                                 can search a single index or multiple                                 indices or even specify them using                                 wildcards and then you can limit it                                 further to the types within those                                 indices or multiple types wildcard types                                 or all types or all some types in all                                 indices so it's up to you how you                                 combine them but this is what the paths                                 would look like we get view the top ten                                 results by default and those top ten                                 results are sorted by relevance normally                                 but if you want to page through                                 the second or third pages you can use                                 size to specify how many results are in                                 each page and from how many results                                 should be skipped before we start                                 returning so our page size is five pages                                 one two and three which started from                                 zero five and ten okay it's all pretty                                 simple now about specifying queries we                                 have something called search light well                                 that's what I called and such light is                                 pretty simple we can specify the query                                 in the query string and this query                                 simply looks for the word John in the                                 name field and we get a results back                                 search light can actually be pretty                                 complex this one says the word foo and                                 the treat field must exist the word John                                 in the name field must exist and the                                 date field must be greater than the                                 beginning of May that's all well and                                 good unfortunately this is passed in the                                 query string so once you encode it                                 correctly it's slightly less readable                                 it's also quite fragile because this is                                 a mini language and if you get the                                 syntax wrong you'll get a syntax error                                 so this is not what you should be using                                 to run your queries in production okay                                 this is a development tools that you can                                 use from the command line to do                                 something quickly and easily and it is                                 useful for demonstrating things as I'm                                 about to do but at this it will come to                                 what you should be using in production                                 later so here we're searching for Mary                                 but we haven't specified a field and we                                 get back a user whose name is Mary                                 tweets sent by Mary and a tweet                                 mentioning Mary so what's happening here                                 well it turns out that when you don't                                 specify a field it uses the default all                                 field and the all field is this field                                 that is magically added when your index                                 documents takes the string values from                                 all of the other fields and sticks them                                 in here again a really useful feature                                 for starting out later on you're going                                 to want to                                 specific fields with the exact things                                 that you want but this allows you to                                 explore your data quite easily but                                 there's some oddities when we tried some                                 of these searches out if I search the                                 old field for                                                            if I search for the specific date                                      no                                                                       that my data has only got one date one                                 tweet on each date so that seems wrong                                 if I search the date field for the same                                 date I correctly get one result but if I                                 search it for                                                         okay so something is weird                                 here and it's worth understanding why                                 this happens it is due to some                                 difference in data types so let's look                                 at what the how what the data file type                                 is how are our fields defined and to do                                 that we check the mapping                                 here we get the mapping for index my app                                 type tweet and it gives us something                                 like this there's a lot here and I'm                                 going to be coming back to the rest of                                 it but the the important thing to look                                 at here is that the date field has been                                 recognized automatically as type date we                                 know the all field although it wasn't                                 mentioned is type string so there's a                                 difference between how dates and strings                                 are handled which kind of makes sense                                 you might expect that but it's actually                                 more than that it's the difference                                 between exact values and full text exact                                 values are what they sound like whole                                 numbers floats dates billions and it can                                 be strings as well where capital foo is                                 different from lowercase foo and full                                 text is the kind of thing you'd have and                                 the tweet content and an email body in a                                 book a document like that                                 the in full text you expect to be able                                 to search within the text you seldom                                 look for the whole exact value you want                                 to find brown fox if you search for                                 foxes you won't search to understand                                 your intent so foxes should still find                                 Fox                                 oops should find jumped how do we                                 support this the this is handled easily                                 by traditional databases this is the                                 territory of search engines we build a                                 structure called an inverted index and                                 the inverted index is a thing that makes                                 full-text search fast so there are a                                 number of phases in it and we'll use                                 these two documents as an example first                                 we separates each string into words or                                 terms as we call them then we created a                                 sorted unique list of these terms and                                 then specify which document it contains                                 each term and you end up with something                                 like this sorted list of unique terms                                 and the document occurrences now a                                 search becomes quite easy a search for                                 quick and brown means we just look at                                 those two terms document two has one                                 document one has sorry I ducked into as                                 two doctrine one as                                                    likely to be more relevant cool but now                                 this search which says you must have                                 quick and you must have Foxes gives us                                 no matches because we have quick in one                                 document and foxes in the other document                                 no document contains both but that                                 doesn't seem right the both of those                                 documents look pretty related so we need                                 to improve how we are indexing these                                 terms so that we can improve recall case                                 shouldn't matter capital quick lower                                 case quick same thing same for there so                                 we'll merge those dog dogs Fox Foxes                                 they're the same root word same intent                                 well we'll just index the root word dog                                 and Fox jumped and leap well they're the                                 same idea they're synonyms so we'll just                                 index both of them is jump suddenly we                                 have a more compact index that is better                                 able to serve our search requests this                                 normalizing of terms is very important                                 but our query still doesn't match                                 we don't have the capital quick in this                                 index and we don't have foxes in this                                 index so we need to normalize the terms                                 in the query two terms in the index and                                 the terms in the query need to go                                 through the same analysis process                                 because you can only find what is                                 actually in your index quick becomes                                 lowercase foxes is stemming to Fox and                                 suddenly our query works this process is                                 called analysis and it consists of the                                 tokenization in two separate terms and                                 the normalization of those terms and                                 this is handled by analyzers and                                 analyzers are just a package which                                 consists of a tokenizer                                 and                                                                     bunch of analyzers that can built into                                 elasticsearch the default of which is                                 called the standard analyzer this starts                                 out using the standard tokenizer which                                 divides these words according to an hour                                 rhythm from the Unicode consortium into                                 what we think are separate words in                                 pretty much any language that you can                                 throw at it then passed through the                                 lowercase filter and then it applies the                                 stop words filter and we don't know                                 anything about what language you're                                 using so we apply the English stop words                                 by default that removes those they're                                 also a bunch of language specific                                 analyzers that can built-in so the                                 English analyzer has the same token as a                                 lowercase filter but then it can use the                                 English stemmer to convert jumped to to                                 jump and apply the list of English stop                                 words similarly if you using French and                                 lies in the Spanish analyzer it would                                 apply a rules appropriate to that                                 language and you can also use these                                 tokenizer z' and filters to build your                                 own custom and analyzers and we'll come                                 to an example of this later on so back                                 to why we have this data mismatch we                                 said data's type date all is type string                                 so that means a date is an exact value                                 and or the old field contains full text                                 date contains that exact date and the                                 old value contains three terms through                                 the string terms                                 top that                                                                for                                                                   term searching for that date is actually                                 searching for any of those terms which                                 also match the searching for the exact                                 date in the date field correctly gets                                 you one result and searching for this                                 exact date doesn't give you any because                                 that exact value does not exist in the                                 date field okay                                 so field mapping is important                                 understanding how your data is being                                 indexed and telling elasticsearch about                                 your data so that it can index it                                 appropriately there a list of coalfield                                 types that are supported by elastic                                 search string date numeric floats                                 boolean and object which is like object                                 hash map associative array depending on                                 the language you used to there are a                                 number of others a couple of which we                                 will touch on later on but this is what                                 you're working with when you throw a new                                 field elasticsearch it uses dynamic                                 detection to try to guess what that                                 field contains so if it looks like a                                 string or it's a string unless it's a                                 string that looks like a date in which                                 case it's a date                                 whole numbers become Long's                                 floating-point become double boodles                                 become billions and objects are mapped                                 as objects we haven't mentioned a raise                                 but there's no special mapping for                                 arrays in elastic search any field can                                 contain multiple values so it just takes                                 the type of the first element in that                                 array and uses that to figure out what                                 type of data that field contains which                                 also means that you can't mix data types                                 in an array an array is a list of the                                 same data type by far the most important                                 setting for field mappings is the type                                 and for exact value fields like date and                                 long generally you don't need to touch                                 it that's the only thing you care about                                 here we can see that that our                                 geolocation has been incorrectly                                 interpreted as an object and that's the                                 first thing we should takes the this                                 isn't not                                 object this is a specific type that we                                 want to index it's a Geo point so we're                                 going to change that to TAC geo point                                 when am looking at string fields we need                                 to distinguish between full-text and                                 exact strings and we do that using the                                 index parameter the three options for it                                 the default is to set it to analyzed in                                 other words string fields are treated as                                 full text by default if you don't want                                 that to be the case                                 set it to not analyzed and then it will                                 stall and query the exact value if you                                 don't want to search word all you can                                 set it to index no it'll still be                                 available in your source document that                                 you stored but it won't be searchable so                                 our neck is not full text it's a user                                 name let's change that to a not analyzed                                 field for analyze string fields these                                 one more important setting and that's                                 which analyzer to apply so we can say                                 that our tweet field we are indexing                                 English tweets so let's apply the                                 English analyzer to that field okay                                 we've got our mapping that we want to                                 use how do we go about changing it you                                 can add new fields to an existing                                 mapping using the mapping endpoint so                                 index type mapping endpoint and then you                                 can specify the new fields that you're                                 wanting to add you can't change existing                                 fields because the data that's ready in                                 the index comes from the old settings so                                 you probably not be able to circulate it                                 properly if the settings change so you                                 need to re index you later delete the                                 existing index create a new index                                 specifying the new mappings and proceed                                 to fill it up with data again this                                 sounds like you can't store anything in                                 elasticsearch but actually they're very                                 easy ways to copy index data from one                                 index to another and have your                                 application be blissfully unaware that                                 anything has changed on to full body                                 search and this is what you should be                                 using for your search this is the                                 powerhouse of elastic search                                 call request body search because you                                 pass it a request body and you can pass                                 parameters like the pagination                                 parameters and the query that you want                                 to run and this structure is called the                                 query DSL it's a rich flexible query                                 language that can do anything that's                                 equal could do and more so start off                                 with the basic that your first kind of                                 go to query the thing that you should                                 use on any field is the match query                                 there are lots of others but this is a                                 good starting point and this is saying                                 find the word search in the tweet field                                 match knows that the tweet field is                                 analyzed with the English analyzer and                                 so it will analyze the search field                                 maybe the the query string before                                 searching in the tweet field if you were                                 to search the NIC it would know it's not                                 analyzed and treated as an exact value                                 match you pass this to the query                                 parameter in the search api and get your                                 results the query IDs are actually                                 consists of two parts the filters and                                 the queries and they're very similar in                                 nature but they have different purposes                                 filters only do exact matching queries                                 do as well but they also do full-text                                 search and that's what they specialize                                 that filters just tell you yes or no the                                 document either matches or it doesn't                                 there's no other information about it                                 queries are more subtle they go well yes                                 this document matches but this document                                 matches better than that one it gives                                 you a relevant score which it has to                                 calculate filters tend to be fast and                                 queries generally tend to be heavier not                                 least of all because they have to do the                                 relevant scoring the output from filters                                 can be cached which makes them really                                 fast to use and queries can't be cached                                 so generally you want to use queries for                                 full-text search or anything involving                                 scoring and filters for everything else                                 okay so we've got our query now we want                                 to limit it oniy to tweets by Mary so                                 we're looking for the exact value at                                 Mary in the nick field using                                 the term filter and how do you combine                                 these the query parameter only in search                                 only accepts one argument so you have to                                 map wrap this somehow into a single                                 query that accepts both of these things                                 and that query is called the filtered                                 query and it takes a query and a filter                                 parameter then you can pass that query                                 the filtered query to the query                                 parameter in the search api perhaps you                                 want just a filter just give me tweets                                 by mary                                 well that's equivalent to specifying a                                 match all query and in fact you don't                                 need to specify at all it's the default                                 but if I'm searching for Mary's tweets                                 there's no relevance so just I'm given a                                 bag of tweets to have them ordered                                 perhaps recency is a use for ordering so                                 we can sort by the date field and                                 perhaps I want to limit it just to                                 tweets from the month of May and I can                                 use the range filter here on the date                                 field to specify a lower and upper                                 bounds so so far we've looked at a                                 finding individual documents but elastic                                 search is also good at learning out and                                 giving an overview of what is in your                                 index and these aggregations these                                 summaries are called facets in this                                 particular case we're looking for of top                                 tweeters who are the people who have                                 sent out the most tweets in our from the                                 data that we have in our index and for                                 that we use the terms facet and we                                 basically bucket by the values in the                                 neck field so every time we have a                                 document tweeted by Mary that the                                 account for Mary gets added up and this                                 will give us our                                                        combine it with the query so who's                                 tweeting about elasticsearch these                                 facets are calculated over the documents                                 that match this query and to be clear we                                 could have billions of documents and                                 this is still calculated within                                 milliseconds this is massive scale                                 perhaps we want to count the number of                                 tweets we have in every month and for                                 this we can use the date histogram facet                                 and we look at the date field and take                                 each date and slot it into a month so                                 now let's put some of these things                                 together we mentioned a lot of concepts                                 this is one that people often want to do                                 how do i do autocomplete somebody starts                                 typing in a name I want to suggest                                 things to them so they type in Joh and                                 we'd like to give them Johnny Smith                                 Johnny Depp Lyndon Johnson the only                                 problem is that Joh doesn't exist in the                                 index and if it's not in the index you                                 can't find it okay so we need to index                                 partial words engrams are like a window                                 on a word and you specify an Engram of a                                 particular length so an Engram of length                                 one would just give you each word a each                                 letter length two you'd move letter by                                 letter and I'll give you two letters at                                 a time and so on this is good for                                 partial matching but it will match                                 anywhere within a word what we need for                                 this for our particular requirement is                                 edge engrams that are a specialization                                 of engrams they are anchored to the                                 beginning of the word so John Smith as                                 edge engrams                                 would produce these tokens and that is                                 perfect for autocomplete now it's been                                 it takes a few steps to actually get                                 this implemented but we'll go through it                                 bit by bit the first thing is that we                                 need to pass our terms through an edge                                 Engram token filter so each term will                                 get broken up by their Jen Graham token                                 filter into something that looks like                                 this we specify enough and I'll show you                                 how you put all these chunks together                                 later on but first in the filter section                                 we call this our or two complete token                                 filter it's type edge Engram I want                                 lengths of anything from one to twenty                                 characters now we apply analyzers to                                 field so                                 need to specify which analyzer we want                                 to use first for the name field you                                 don't really want stop words a a mill                                 would have the A's removed which doesn't                                 make sense for a name field so we're                                 gonna have the name analyzer which just                                 uses the standard analyzer but then                                 we'll configure it to not use any stop                                 words for autocomplete will have the                                 name autocomplete analyzer which is a                                 custom analyzer we still want to use the                                 standard tokenizer and the lowercase                                 token filter but we add in our                                 autocomplete token filter at the end of                                 that now we have to tell elasticsearch                                 which field to apply this to and we do                                 that by changing the mapping for the                                 name field we're using the name field                                 for two purposes ordinary name matching                                 and this autocomplete and to use a                                 single value or a single field for multi                                 purposes we change it to a multi field                                 and we give it the two subfields that we                                 need to use this name field which has                                 the same name as the top-level field is                                 called the main field and it can be                                 referred to by the top-level name or as                                 name name and for that we just want to                                 add the name analyzer for autocomplete                                 it's a sub field and it can only be                                 referred to as named autocomplete and                                 this is a little more complicated we                                 said earlier that you want to apply the                                 same analyzer at search time and index                                 time but in this case you don't you want                                 to store all of the engrams in your                                 index using the autocomplete analyzer                                 but when you search for joh you only                                 want to search for joh you don't want to                                 search for jjo joh and so we'll use our                                 standard name analyzer at search time we                                 delete the index and create a new index                                 is specifying our analyzers and filters                                 in the settings section and our new                                 mappings in the mapping section and now                                 we after we put the data back in we are                                 ready to query it and the query is this                                 simple we search the name door                                 complete field and passing whatever the                                 user has typed in and that is going to                                 match but John will also match Jonathan                                 so if we want to the school whole word                                 matches better than partial matches we                                 will actually want to run two queries                                 one against the autocomplete field and                                 one against the normal named field so                                 the way you combine multiple queries is                                 using a bull query and bull queries take                                 must must not and should parameters                                 clauses that must match clauses that                                 must not match and if these match then                                 it's a better match it's more relevant                                 but they don't have to match so we put                                 our autocomplete into the must match                                 column because if the autocomplete                                 doesn't match then nothing's going to                                 match and we do a simple match against                                 the top-level name field in this should                                 close and this now gives you a nicely                                 weighted autocomplete function if you                                 just match the the partial field this                                 Clause will be scored but any whole word                                 matches will be bumped up in relevance                                 so I've lost a slide okay and that was                                 the end of the main section and I've got                                 a few more sort of common examples that                                 I could show you I put them in as as                                 extras because I didn't want to go over                                 time and I think I've got                                                                                                                         rank frequently retweeted tweets higher                                 than tweets that nobody who cares about                                 so we want to take the value of the RT                                 or retweet field into account here's an                                 example where we can do it a query on                                 the tweet field and then use the script                                 to just tweak the the the score for that                                 document based upon the value of this                                 retweet we don't completely overwhelm                                 the score that's come from the main                                 query this is just an adjustment                                 so we can take the value of the retweet                                 field and we can use this sort of                                 smoothing function so that it doesn't                                 overwhelm things I just multiply it by                                 the existing score from the query and                                 we'll get a slope boosting of popular                                 tweets to the top of our results ah that                                 was the slide I was looking for and                                 perhaps you want to show tweets that are                                 only within                                                          well we've got a geolocation field in                                 our tweets so we can apply the Geo                                 distance filter to it here find                                 everything within                                                        central point which happens to be this                                 room or close enough and or perhaps you                                 don't want to exclude the things that                                 are outside that's                                                    just want to give more importance to the                                 ones close to Berlin and we said that                                 queries are useful relevance and filters                                 are used for scoring but there is an                                 efficient way of using filters for                                 scoring as well and it's called the                                 custom filters score query first it runs                                 our query and then it looks at any                                 filters we specify and if the document                                 matches those filters you can adjust the                                 score accordingly so in this case we're                                 using the same Geo distance filter and                                 we go yeah if it's local just boost the                                 importance of this thing a bit and                                 that'll increase the relevance of local                                 tweets without completely excluding                                 those from outside and that's all I've                                 got for you I hope that this has given                                 you enough to at least get started to                                 find your feet and go from not having                                 any search to being able to do the                                 simple things easily                                 any questions                                 you
YouTube URL: https://www.youtube.com/watch?v=52G5ZzE0XpY


