Title: Lester Solbakken – From text search & recommendation to ads & online dating
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Full title: From text search and recommendation to ads and online dating; approximate nearest neighbors in real world applications

Anything can be represented by a vector. Text can be represented by vectors describing the text's meaning. Images can be represented by the objects it contains. Users of a system can be represented by their interests and preferences. Even time-based entities such as video, sound, or user interactions can be represented by vectors. Finding the most similar vectors has all kinds of useful applications. 

There are many libraries to choose from for similarity search. However, in real-world applications, there are additional complications that need to be addressed. For instance, similarity search needs to scale up while ensuring that data indexed in the system is searchable immediately without any time-consuming index building in the background. Most importantly, however, additional search filters are often combined with the similarity search. This can severely limit the end result's quality, as post-filtering can prevent otherwise relevant results from surfacing. 

In this talk, we'll explore some real cases where combining approximate nearest neighbors (ANN) search with filtering causes problems. The solution is to integrate the ANN search with filtering, however, most libraries for nearest-neighbor search work in isolation and do not support this. To our knowledge, the only open-source platform that does is Vespa.ai, and we'll delve into how Vespa.ai solves this problem.

Speaker: 
Lester Solbakken – https://2021.berlinbuzzwords.de/member/lester-solbakken

More: https://2021.berlinbuzzwords.de/session/text-search-and-recommendation-ads-and-online-dating-approximate-nearest-neighbors-real
Captions: 
	                              all right                               uh thanks for that thanks for tuning in                               so as sergey mentioned my name is uh                               is lester uh i work at verizon media                               group                               previously uh yahoo and as as sergey                               also mentioned we're soon to be called                               yahoo                               yet again um i primarily work on                                machine learning solutions and today                                i'll be talking about                                nearest neighbor search um specifically                                some                                practical issues to think about when                                using a ns in search applications                                uh because there's been a lot of buzz                                around the nearest neighbors in recent                                years we've seen a lot of libraries and                                platforms appear um                                but not too much talk about some of the                                more you know practical problems that                                need to be solved and i'll try to                                exemplify them using some some real                                applications                                uh so as as you mentioned at verizon i                                work on on the vespa platform                                and if you're not familiar with it vespa                                is                                the open source platform for low latency                                computations over large evolving data                                [Music]                                it's a platform we've been developing                                over many years has roots spanning back                                to                                you know research and wars of the late                                                                     but we opened sources a few years back                                so while it may seem                                new to many people it's been battle                                tested over many years                                and today serves hundreds of thousands                                of queries per second uh at any                                given time all over the world uh serving                                hundreds of                                applications hundreds of millions of                                users each month and so on                                um there's a debate i think on thursday                                uh which compares vespa to elasticsearch                                and lucien which will surely be great                                to check that one out if you're                                interested in invespa                                um so vesp is fairly feature-rich for                                many types of applications                                and its features all included and                                tightly integrated with each other                                each other uh such as you know advanced                                relevant scoring with tensors machine                                learning models                                one of the really cool features of usb                                is ability to you know string together                                or pipeline various machine learning                                models from various platforms                                uh together across different phases to                                calculate relevant scores and so on for                                queries but                                but that's another talk for this talk                                we'll be looking at                                nearest neighbors um                                so with nearest neighbors uh we need to                                start with uh representations                                um so pretty much anything can be you                                know represented by a list of numbers                                uh or more formally a point or vector in                                high dimension space                                you know text images um this is an image                                of my dog                                he's not really sure what's going on uh                                he doesn't like his nearest neighbors                                they're mostly cats but                                even you know things like based on time                                such as sound or video or even user                                interactions that can be represented by                                you know such representations                                so i'm not really going to talk about                                how we generate these representations                                representation learning is a talk of its                                own but it's had great progress recently                                with deep learning                                um there's however one thing i'd like to                                say and that                                we've seen great results when using                                representations that are you know                                trained or adapted specifically for                                for the use case but these                                representations don't necessarily work                                all that well when transferred out of                                the domain so for instance for text                                we've really seen                                we haven't seen great results when using                                 things like you know universal sentence                                 encoders in relation to                                 you know more classical approaches such                                 as bm                                                                  i'm                                 digressing that's that's another talk                                 but anyway these representations that we                                 generate in some way they map objects                                 into some high                                 high dimensional space um                                 this allows us to you know find or                                 search for nearest neighbors for                                 instance to some some query points                                 um the nearest neighbors are measured by                                 some you know distance functions such as                                 you know geometric distance or or                                 euclidean                                 uh distance uh we have cosine angles                                 which                                 work well for sparse representations of                                 particular texts                                 manhattan distance having distance from                                 working bit vectors and so on                                 um to actually you know find the nearest                                 neighbors what we can do is just scan                                 through all points                                 in a kind of brute force uh manner and                                 determine the closest ones                                 according to some some this distance                                 measure um                                 of course you know this brute force                                 doesn't really scale that well so we                                 need some sort of index structure to                                 help us                                 you know navigate this space um                                 unfortunately there aren't really any                                 known methods of doing this exactly so                                 we need to resort to                                 approximate solutions which hopefully                                 gives us good enough results                                 there are a few different methods for                                 this and i basically divide them into                                 whether or not they can be                                 represented with traditional inverse                                 indexes                                 those are can such as k-means or product                                 quantization and its relatives                                 locality sensitive hashing basically                                 create groups of centroids or buckets                                 for similar items and you can kind of                                 find the most similar bucket or                                 centroid to the query and prune away a                                 lot of the search space you know that                                 way                                 uh the other group of methods is those                                 that you know don't use these inverse                                 indexes                                 such as you know hierarchical and how we                                 call small worlds or hmsw                                 which i'll get back to a little bit                                 later but basically this is a graph                                 structure that we search through instead                                 but you know for most cases this is a                                 little bit you know too low level                                 instead we use                                 libraries to help us do this                                 and a great resource for that is a                                 nnbenchmarks.com                                 this site compares various libraries                                 across many different data sets that                                 have different distance measures and                                 vector lengths and so on                                 and they're graphically compared in                                 figures such as this which compares                                 recall to queries per second                                 so up and to the right is is                                 is better uh you know giving higher                                 recall with with higher qps um                                 just kind of short where in the                                 measurements here you know these these                                 graphs are all single cpu                                 single threads so so they measure the                                 latency and don't really measure the                                 potential for throughput or                                 or maximizing the queries per second uh                                 using for instance multiple threads                                 which is just as                                 or maybe even more important from many                                 types of applications so                                 uh when choosing a library this is                                 something you might want to think about                                 at least it's something we care a lot                                 about that                                 at vespa anyways um here i turn just a                                 few libraries                                 to declutter a little bit uh notable                                 ones are scanned from from google                                 uh files from facebook and away from                                 spotify and and of course                                 vespa um so we have a pull request into                                 an                                 a nnn benchmarks to include vespas and                                 an implementation                                 even though it's not really available as                                 a standalone library                                 but we wanted to measure how good our                                 implementation is against the others                                 and as you might see it's not right at                                 the top but it's competitive with others                                 we compare                                 ourselves to which is if if you kind of                                 able to see that are other libraries                                 that use this                                 hmsw approach                                 and the reason for that is that vespa's                                 implementation is of course based on h                                 and sw for                                 a few good reasons um one is of course                                 you know it performs well during                                 retrieval but perhaps more importantly                                 is that it allows for                                 incremental modification and now most of                                 the                                 other approaches i mentioned they build                                 indexes offline you know in batches                                 uh so the index is kind of immutable and                                 requires some                                 or sometimes lengthy indexing periods                                 you know particularly for for large data                                 sets uh but essential design of                                 vespa is that data that's indexed should                                 be you know immediately available for                                 searching                                 uh and for many types of application                                 that's that's really important                                 and it's directly supported by the say                                 nhsw algorithm                                 and now we've done some modifications to                                 this algorithm such as allowing removes                                 as well which is equally                                 important removing stale data                                 so in the previous slide we saw some                                 libraries that do                                 vector similarity search um they're also                                 higher level platforms such as you know                                 perhaps                                 nilvis we v                                                           offers a vector search service that you                                 can call out to from your serving stack                                 um which library or platform you                                 ultimately choose of course depends on                                 what type of system you're developing i                                 mean background                                 index building might be perfectly                                 acceptable solution for                                 applications where data doesn't really                                 change that often                                 however when it comes to you know search                                 and and recommendation applications                                 um there is an another important aspect                                 uh which isn't touched upon uh that                                 often                                 and that is uh filtering on metadata                                 so what i mean by that is that most                                 often in                                 a search application each item in the                                 data usually has                                 associated data with it or side                                 information or metadata                                 and now this metadata is usually stored                                 separately from the library or service                                 implementing the nearest neighbor search                                 and                                 this can become problematic as we'll see                                 shortly so                                 the first example is a semantic search                                 for text                                 now again assume you have some way of                                 encoding text such that you know                                 documents or paragraphs or sentences                                 you can encode them to two vectors uh                                 one common way these days is to use                                 transformer models such as purge and so                                 on to                                 and use one of the token vector                                 representations there as an embedding                                 vector                                 um any way you do this you can you know                                 visualize these points                                 in some high dimensional space                                 now assume also that you have some way                                 of                                 encoding a query so that it maps into                                 the same space                                 so that the closest documents in this                                 space is the most relevant to the query                                 now queries usually have some different                                 properties and documents                                 for one they're typically much shorter                                 um so they typically don't use the same                                 encoding model                                 but to ensure that they kind of map into                                 the same space we can use like                                 a two tower configuration which includes                                 a distance function                                 and propagate the tuning process but uh                                 anyway                                 mapping the query into the same space we                                 can find                                 what here for instance the                                         relevant documents to                                 to the query and this works well                                 however search applications might allow                                 the user to kind of drill into the                                 results by for instance                                 here filtering on the publication year                                 of the document                                 now since we've first retrieved the top                                 then                                                    nearest neighbor search and then we                                 filter for the year                                      we only actually here retrieve the three                                 relevant documents to the query                                 i mean the other documents the one in                                 orange here that are in this space are                                 not recall at all                                 even though they are irrelevant to the                                 user now this is kind of a                                 constructed example but you know clearly                                 shows the problem                                 we need to solve so again to repeat that                                 i mean the nearest                                 neighbor search returns the                                            items to the query                                 and applying the filter                                                                                                             away leaving only the three that                                 actually pass the filter                                 which is ultimate result of this search                                 so naively you know to return the top                                    documents that                                 actually do pass the filter we need to                                 search in this case for you know                                 something around                                                                                                          don't really know                                 how much to search for beforehand so but                                 we'll see later how we can                                 improve upon this                                 so another example recommendation so                                 recommendation is similar to search but                                 the query                                 is the actual user profile                                 we want to search for items to to                                 recommend given what we know                                 about a user so classically we'd use                                 approaches such as matrix factorization                                 and these days of course use more deep                                 learning approaches                                 such as neural collaborative filtering                                 and its descendants                                 these figures here by the way they're                                 from a news recommendation tutorial we                                 have                                 uh for westbound in case you're                                 interested                                 in any case we create this user                                 embeddings and item embeddings and                                 find the items to recommend by doing a                                 dot product or                                 or inner product search between the user                                 embeddings and the item embeddings                                 this is called maximum inner product                                 search uh and it isn't really you know                                 directly implementable                                 in the approximate nearest neighbor                                 algorithms so we need to convert the                                 mips problem to for instance the                                 euclidean distance problem which which                                 allows us to do so                                 now for recommendation services such as                                 youtube and tick tock and so on                                 there are a lot of you know inherent                                 filters we need to consider                                 such as you know age appropriateness                                 band content                                 region availability language and so on                                 and there might be more dynamic filters                                 such as business rules for                                 diversity and deduplication and                                 so on but all these kind of serve to                                 potentially you know filter away a lot                                 of otherwise relevant items                                 so that the effect is that we recall far                                 a few items than what we otherwise                                 could have                                 add search another example similar                                 application really                                 um the image here to the to the right                                 here is taken from the front page of                                 yahoo.com                                 uh which to the large degree runs on                                 vespa and you have this kind of infinite                                 scrolling                                 stream uh new stream which has these                                 kind of native ads kind of hidden in the                                 stream                                 um likewise users have profiles                                 as as do ads and the dot product between                                 these two you know correspond to the                                 kind of interestingness so they add to                                 the user                                 at least hopefully um now these these                                 ads also have a lot of metadata such as                                 you know target group uh                                 target location geographic location and                                 so on and so on                                 um but one very important piece of data                                 is the ads or                                 or ad campaigns budgets so the                                 advertiser might                                 pay for each impression and we don't                                 earn any money if we show an ad that has                                 suspended budget so so likewise if we                                 first do a vector search and then                                 perform filters afterwards there's a                                 real possibility that we aren't really                                 you know retrieving the most interesting                                 or most profitable ads                                 to show to the user                                 uh so in online dating                                 you have your uh your profile of of                                 interests                                 um now that's what you are interested in                                 it could be like pets and travel and                                 food and tv shows working out etc etc                                 now this is your your representation um                                 when searching for                                 potential matches you also have a set of                                 preferences or filters such as you know                                 geographical location                                 you might want to meet people close to                                 you a gender age range                                 are they at least six feet tall and                                 other filters                                 particularly for services such as tinder                                 where you                                 filter away users where you already                                 swipe them left or right                                 uh so again if if first retrieving                                 profiles using                                 nearest neighbors there's a big chance                                 that the people that you know best match                                 your interest don't pass all of these                                 filters                                 so many potential matches are are not                                 retrieved and                                 no date for you                                 shopping another obvious example really                                 this example here is from a use case in                                 our documentation for using                                 a vespan shopping the use case sample                                 appear even comes with a front end this                                 can be seen here                                 now products can have their                                 representations and queries can be                                 mapped to the same vector space as the                                 products and                                 products have a lot of metadata of                                 course the price category brands                                 even rating probably one of the                                 important ones is where not the item is                                 in stock                                 getting results for items that can't be                                 purchased because they're out of stock                                 might be a                                 poor user experience                                 and again by by first retrieving                                 products using nearest neighbors and                                 filtering using                                 metadata would likely lead to a lot of                                 relevant products not                                 not being retrieved                                 uh finally um local business search                                 um let's say i want to search for                                 restaurants near me                                 and we've implemented some sort of                                 personalized search so that my                                 preference profile is used to                                 retrieve the restaurants that best fit                                 my profile now uh                                 assume that my profile uh retrieves only                                 the best restaurants in the world                                 uh of course it's it's unlikely that                                 they're close to me                                 at least for the small town that i live                                 up north which is the red dot up at the                                 top there                                 well it's not actually not true we do                                 have some great restaurants here we do                                 have a few michelin stars                                 here but anyway restaurants that best                                 fit my profile                                 globally are pretty unlikely to be close                                 to me                                 meaning that you would very likely not                                 return any results at all                                 so this is this is kind of an extreme                                 example obviously you wouldn't do this                                 but                                 but the point stands                                 so uh the point being uh is that you                                 know post                                 filtering of metadata doesn't really                                 work all that well so what can you do                                 about that                                 um one obvious approach is to                                 recall more and just return more items                                 from the nearest neighbor search                                 um however that can become quite                                 expensive you know both in processing                                 and                                 in time especially if these need to come                                 across the network                                 and still this isn't really guaranteed                                 to work                                 another option is to you know set up                                 multiple indexes                                 with one per filter for instance in a                                 text search example                                 one index per publication year now this                                 might                                 work if you have a small number of                                 filters and can avoid the explosion of                                 combinations but but still                                 this adds a lot of complexity um                                 now the solution really is to do a                                 nearest neighbor search only among the                                 items that have                                 survived filtering is pre-filtering and                                 the approaches that i mentioned                                 previously you know the libraries and                                 platforms they don't really support this                                 as the nearest neighbor index and                                 metadata usually are you know in                                 separate systems                                 however in vespa we've modified the                                 basic hmsw to to support this                                 by introducing eligibility lists                                 this causes the algorithm to effectively                                 skip items that are not                                 in this list so in this figure again the                                 orange are the items that pass the                                 filter the blue                                 items that we want to skip so the                                 algorithm effectively                                 dynamically increases the search area                                 according to the number of items that we                                 want to retrieve and the items in the                                 eligibility list                                 and this is nice because we only return                                 the exact number of results that you                                 want to the solving you know the problem                                 of post filtering                                 um there's perhaps one thing that should                                 be mentioned and that there's a                                 still a slight cost to navigate the                                 graph for items that are skipped                                 so if the filter is strong meaning there                                 are relatively few items in                                 in this eligibility list the cost of                                 searching the graph                                 can be high and to solve this problem                                 vespa falls back to                                 exact nearest neighbor scan only through                                 the list of eligible items when this                                 happens                                 because that's actually more efficient                                 than navigating the graph                                 so and this threshold is of course                                 configurable and that's one of the kind                                 of                                 nice things that the best adds to to the                                 table                                 so uh to kind of uh sum up um                                 vespa innovates a little bit with                                 approximate nearest neighbors uh                                 search to fit in with you know many of                                 the different aspects that are important                                 in search applications                                 one is the dynamic modification of the                                 graph by you know particularly                                 supporting                                 removing of items use efficient data                                 structures and methods to                                 increase the performance when building                                 indexes                                 and it modifies h and sw to to support                                 pre-filtering um                                 now the implementation we chose                                 to support a ns are really a consequence                                 of fitting in with                                 design philosophy of vespa and it's                                 important to note that vesp is much more                                 than a vector search engine and all the                                 examples that we saw previously are                                 actual applications using                                 using vespa so some other use cases here                                 in                                 text search using both classic                                 information retrieval techniques such as                                 bm                                   and more modern approaches such as                                 vector similarity and transformers such                                 as bird etc                                 recommendation and personalization using                                 again vector search machine learned                                 models true partial updates and so on                                 another example which you have a pretty                                 good sample app for is question                                 answering which uses                                 a ns to select passages with answers to                                 questions and uses burp models to                                 extract the                                 exact answer to the question and and we                                 have many more examples but                                 even finding love um okay cupid the                                 inspiration for the online dating                                 example is a great blog post on why they                                 chose                                 vespo ver elasticsearch                                 so that pretty much sums up my talk uh                                 if you want to know more about vespa                                 check out these these resources uh we                                 have an open source version                                 and the cloud offering as well um which                                 currently has a free trial if you want                                 to check us out                                 and other than that tune in to the the                                 search engine debate                                 on thursday to to learn more uh                                 so that thanks i'm going to open the                                 questions                                 thank you for the great talk and uh what                                 is the way to finish it up with                                 finding the love also this vespa right                                 yeah                                 exactly very nice so we got a few                                 questions from the audience um so                                 first question would be how customizable                                 is vespa can we deal develop and plug                                 custom relevance algorithm                                 it's like you mentioned a bit of like                                 custom models right but how actually                                 complicated is that to                                 you know develop a new similarity model                                 or like new algorithm right and                                 it's very customizable so basically uh                                 is divided a little bit in two we call                                 it the kind of stateless                                 front end and we have a kind of a                                 stateful backend which                                 where you have all the content and all                                 the kind of that's where all the                                 computation is done                                 so on the back end you can write your                                 custom                                 ranking functions to do pretty much                                 whatever computation you want to do                                 using tensors and                                 combining them with the machine learning                                 models and so on                                 in the front in the stateless layer you                                 can add your custom java code which                                 pretty much can do any form of                                 processing both when                                 handling queries or ingesting data and                                 so on so i would say this is very                                 customizable                                 nice very good and uh kind of like a                                 similar question but from other                                 direction                                 um does vespa also support traditional                                 relevance algorithm                                 for example bm                                                  how does it work right can you combine                                 them right or you need to decide like                                 it's either one                                 or another one no as i as i mentioned                                 in just a few slides ago um bm                                         least you know these traditional                                 information retrieval techniques                                 uh they're very well supported that's                                 the kind of basis really of of vespa um                                 and                                 they're supported both in forms of                                 matching and uh you know the first phase                                 and you're searching for all documents                                 and using them in ranking as well                                 so so that's very flexible and how you                                 actually match these documents                                 or or rank upon them as well so yeah the                                 these are definitely uh supported but                                 one of the nice things is that is that                                 you can kind of combine these more                                 traditional information retrievable                                 approaches                                 with more modern informational                                 retrievals as well and and                                 adding machine learned models to like a                                 later phase model                                 or um later phase expression or uh                                 calculation as well so uh yes vespa is                                 very flexible in in that regard                                 so essentially you can get like you know                                 uh top whatever and right from a                                 neighborhood right and after sword by                                 whatever your like tfidf that you were                                 actually before right as an example                                 right                                 yeah exactly true okay nice um                                 and uh what else i have here with                                 questions there's like more questions                                 popping up that's why like                                 okay so first question or like third one                                 actually um what are the core features                                 of lucien                                 that are not supported by vespa yeah i                                 guess like people will be like hey this                                 is like so amazing so why do i still                                 need to have this him                                 to be blatantly honest uh i                                 don't work too much with lucine in that                                 i'm a fan of vespa                                 so so that's my focus and to to get a                                 good answer that question                                 uh i'd uh refer to the great debate on                                 thursday                                 uh which will have experts both on vespa                                 and                                 lucine and on elasticsearch to answer a                                 lot of those different questions there                                 sounds good some people also asking                                 about performance this comparison i know                                 i read but i think the benchmarks that                                 you mentioned would be like really the                                 ultimate answer for those right so                                 just like go to the website and people                                 can find it right uh definitely                                 so uh so again um we've added a                                 pull request to add the the performance                                 of the a ns                                 implementation of usb                                           benchmarks uh but                                 we don't really you know uh provides the                                 n                                 code invest as a library as a                                 stand-alone library it's part of you                                 know the the bigger system                                 um still we want to see how how well it                                 performs                                 but it's um important to note that it's                                 it's working in the context of a larger                                 you know search system which does a lot                                 of other stuff                                 as well so so um that needs to be taken                                 into consideration too                                 yeah so that's good um another question                                 actually from my side so you mentioned                                 this uh example with a budget right and                                 me coming from e-commerce i kind of like                                 seen it like across the board right                                 so do you see because i mean the easy                                 solution would be saying like hey if                                 budget is like out right you just like                                 stop showing the things right                                 but it also means that there's a website                                 like the yahoo for instance right                                 you can have like you know like lots of                                 impressions right and after suddenly                                 they go like to zero                                 is there like any ways that you can see                                 that it's like still scalable                                 and you can do it more dynamically right                                 so you can you know like a boost                                 visibility a bit of like last right                                 and kind of like a dual for uh                                 controlling algorithm like in the way                                 right                                 but there's like a lot of budget right                                 you have this ability if not like it's                                 kind of like becoming like less and less                                 but not drastically right from one to                                 zero                                 yeah so uh i guess there's different                                 ways of of uh of handling that                                 um you can kind of bake that into your                                 kind of ranking function in a way and                                 then you have a                                 sort of kind of a graded decrease and so                                 on you don't want to you know                                 suddenly come into the position where                                 you you you only have                                 uh ads to show which all of them have                                 used used up their budgets and so on of                                 course                                 so um we have different teams um                                 that that work on this specifically and                                 those are just one of the kind of many                                 applications that are in vespa                                 and um i i really don't have full                                 view into how they handle exactly this                                 question but they definitely have to                                 solve that so that's good thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=-NjETJIe-Xs


