Title: Lars Francke & Sönke Liebau – Building a new Big Data distribution based on Kubernetes with a twist!
Publication date: 2021-06-23
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	The need for companies to deploy and operate Big Data infrastructures hasn't gone away but their options to do so have dwindled in the past few years. That's why we decided to build a new Open Source Big Data distribution.

It includes the usual suspects like Apache Kafka, Apache Spark, Apache NiFi, etc.

We asked around and were told it's a crazy idea but we did it anyway: We implemented a Kubelet in Rust that uses systemd as its backend instead of a container runtime. We also started writing Operators that target these special kubelets.

This means we can deploy hybrid infrastructure (partly running in containers and partly on "bare metal") using the same stack, the same tools, the same description languages, the same knowledge, etc. getting the best of both worlds.

In this talk we'll share what we learned about writing Kubernetes Operators (in Rust) as well as gain insights into our new distribution.

Speaker:
Lars Francke – https://2021.berlinbuzzwords.de/member/lars-francke
Sönke Liebau – https://2021.berlinbuzzwords.de/member/sonke-liebau

More: https://2021.berlinbuzzwords.de/session/building-new-big-data-distribution-based-kubernetes-twist
Captions: 
	                              hello everybody my name is                               zhang lieber i work with stackable and                               together with lars franke                               i'm here today to give you a brief                               insight into what our                               vision for the future                               yeah thank you zunker and yeah my name                               is                               uh las franca and as jungkook mentioned                                i'm also working at stackable                                and yeah we're talking about um a new                                kind of big data distribution                                as as zinke already mentioned uh                                obviously i don't know any of your                                backgrounds i don't know                                anyone who's listening so i'll give a                                super quick introduction                                uh what what we mean by big data                                distribution and                                and uh yeah what we're talking about so                                the whole big data                                ecosystem started around                                          with the apache hadoop ecosystem i                                assume that some of you or most of you                                already know tools like apache kafka                                apache spark apache nifi                                apache hbase and so on and there's                                there's lots of tools out there in this                                big data space like if you want a                                process store uh visualize                                lots lots of data with open source tools                                you have to use                                more than more than one tool often tense                                or so                                so what what what happened is around                                                                    the first distributions happened so                                these are companies                                and these companies started building                                distributions basically they packaged                                the software                                or all the um all the open source tools                                they packaged them together made it easy                                to manage made it easy to deploy upgrade                                and                                and sell your support um                                these numbers might be off a bit it's                                not super easy to find find the                                find the exact yes but as you can see                                like back in                                        the                                                                  five different companies offering these                                kind of                                distributions um you can see that intel                                dropped off fairly early they invested                                in cloudera instead                                later ibm dropped off and pivotal                                dropped off they                                merged or migrated to hot works so these                                are all companies                                that that did offer or still offer um                                commercial distributions around there                                around the hadoop ecosystem                                now as you can see since                                             only single distribution left so there's                                only                                the cloudera and they have their their                                own distribution and that's the only                                commercial distribution left for like                                on-premise big data stuff                                out there there is an obvious                                competition and that                                that are all the cloud vendors like you                                can obviously go to amazon and uh google                                and microsoft and                                and buy these things um uh in the cloud                                but you can't get it you can't get it on                                for on-premise in one package other than                                cloudera                                so cloudera and hortonworks merged in                                                                        what happened then is is this this is                                not to scale it's just illustrative                                and the pricing for these distributions                                usually was like per node like if you                                had one node you paid this amount of                                money if you had two nodes                                you paid this amount of money and so on                                and                                the prices remained relatively stable                                for most                                of the of the time but when only a                                single company was left the prices                                skyrocketed from                                i think two thousand dollars per node                                per year to                                uh ten thousand per per node per year                                obviously before discounts and so on but                                anyway                                 and both cloudera and hortonworks                                 actually did provide a free version of                                 their distribution                                 up until                                                     but after the merger the free version                                 was removed and these pricing changes                                 came                                 came into effect and                                 the fact is for people using this stuff                                 on premise                                 or even in the cloud obviously things                                 have gotten much more much more                                 expensive                                 um so this is where we come in um while                                 we're building what we're building                                 uh both zuka and i already had like a                                 small company with an                                 open core like a big data consulting                                 company                                 and what happened in                                                  got contacted by lots of our customers                                 and they asked what what other what are                                 your other customers doing now like we                                 we used to pay this amount now we pay                                 two                                 three four five times or we used to use                                 the free version and now we have to                                 pay a lot of money out of a sudden so                                 what what's everyone else doing now and                                 what what we did is we organized a                                 meeting                                 or actually three or four this was                                 before corona and it was in person                                 and um yeah we organized a couple of                                 meetings with like                                                                                            in germany and everyone was unhappy with                                 the situation and so we decided to found                                 stackable in                                      and um yeah we we are basically                                 basically building our big data                                 distribution                                 but um yeah that's that's that's our                                 super quick history                                 uh the distribution that we're building                                 is open source and um                                 the things yeah we're building it on and                                 our                                 yeah foundation pieces are what we're                                 going to talk about now and with that                                 i'll hand over to zunker                                 with what we actually do yes thank you                                 guys                                 so i have the same problem as last i                                 don't know who's watching or what your                                 background is so i'll start off with a                                 very very brief introduction into                                 kubernetes and i'll keep it really brief                                 i promise                                 because most of you probably know more                                 about it than me                                 um so when we started off we took a good                                 look at what we actually need                                 to to build a distribution of big data                                 tools and                                 it was fairly obvious that we would need                                 something to sort of act as the central                                 instance to a whole state                                 but also something to decentrally manage                                 the servers that we want to roll out                                 these products on                                 the the old or existing distributions                                 all have some form of management tools                                 so cloudera for example has the cloudera                                 manager                                 which has a web ui and you can click and                                 add services and configure them                                 for hortonworks it was ambari and every                                 distribution pretty much had                                 something to to manage and define your                                 services in                                 and um well we took a good look around                                 at what's out there                                 and kubernetes currently is fairly hard                                 to miss it is                                 one of the bigger projects in the uh                                 pretty much                                 any space currently so um we did take a                                 good look                                 hard look at kubernetes and um we've                                 we've                                 just pulled out a few of the basic                                 principles of kubernetes here to give a                                 brief introduction so um it's                                 it's an orchestration framework to                                 manage services                                 um which is a fairly uh                                 abstract thing to say it is declarative                                 by nature so                                 you don't tell kubernetes please do this                                 please do that please do this                                 but you simply tell it i would like my                                 world to look like this                                 go and make that happen so basically in                                 in our world you would tell kubernetes                                 i'd like a kafka cluster please                                 on all nodes that match this descriptor                                 and then you trust kubernetes to go and                                 figure out what needs to happen to do                                 that                                 and to support that kubernetes offers a                                 few basic data types so kubernetes has a                                 few built-in data types things like                                 ports which describe a workload to be                                 executed                                 nodes which describe you know a server                                 that's out there that's ready to run a                                 workload and                                 lots and lots of other things around                                 that like deployments and stateful sets                                 and there's the secrets to store                                 passwords and things like that                                 so kubernetes offers lots of                                 abstractions                                 but and that's where it becomes really                                 interesting kubernetes also offers                                 something called custom resource                                 descriptions                                 which allows you to extend kubernetes to                                 help you with what you exactly want to                                 achieve so for us                                 for example that's obviously we need to                                 come up with a custom resource                                 description                                 to describe how an hdfs cluster should                                 look like or a kafka cluster                                 or going a bit more abstract for example                                 a kafka topic                                 or acls on a kafka topic and then those                                 those resources can can relate to each                                 other so for example you could define                                 a kafka topic and an acl that relates to                                 that topic                                 and then deploy that to separate                                 clusters so it's a very                                 fairly flexible data model that you can                                 build                                 on top of kubernetes and i was listening                                 to a podcast recently which there                                 someone said comedy kubernetes is less a                                 list of a deployment technology or                                 something but it's a platform to build                                 platforms                                 which sounds a bit cheesy if we're                                 honest but it is actually not too far                                 from the truth                                 because um using these abstraction                                 layers you can actually                                 just use the kubernetes control plane to                                 create                                 platforms of your own and um i actually                                 had to look up the description of                                 platform and it                                 what i found was uh it's a layer of                                 technology that makes software delivery                                 possible                                 which is pretty much exactly what we are                                 trying to achieve so                                 that that fits the build fairly well for                                 us                                 so if you take a look from very high up                                 at kubernetes um what kubernetes is is                                 at the center of it is the api server                                 that's the central instance that you                                 always interact with and                                 you as the user on the left you simply                                 declare                                 what you want your world to look like                                 and give that to the api server                                 and the api server then persists that                                 into lcd and at cd here it's                                 a storage technology for for all intents                                 and purposes um could also be a                                 relational database so pretty much                                 anything else                                 um it is something with specialized for                                 kubernetes but                                 it is more of an implementation detail                                 that                                 we don't really need to talk about too                                 much and what the api server then does                                 is it informs everybody who's interested                                 about what just changed                                 and it doesn't really tell them what                                 exactly changed it just tells them what                                 the new state is so the cluster is now                                 i don't know if if i change my cluster                                 from three to five nodes it would simply                                 tell everybody who's interested                                 this cluster now should have five nodes                                 but it won't tell them                                 that it used to have three nodes so the                                 the sort of general principle in                                 kubernetes is always that you shouldn't                                 know what has changed you should only                                 need to know                                 how does it look like now and that's the                                 only input that your                                 basically control plane should need and                                 what the control plane then does is it                                 triggers a so called reconciliation                                 which means it                                 tries to reconcile what is with what                                 should be                                 and then that reconciliation probably                                 triggers stuff that gets written back to                                 the api server again which gets                                 persisted in hcd and                                 then it just goes back and forth for a                                 while probably because                                 those changes will be node will notify                                 the control plane again and                                 that'll trigger changes again and so                                 that reconciliation can                                 happen quite a few times until it's                                 actually done                                 and at some point in time something will                                 fall out of that that actually needs                                 executing to make something happen and                                 that will usually be a pot that is                                 assigned to a cubelet and the cubelets                                 then are the the decentralized                                 things that run on servers to actually                                 execute your workload                                 in a container and                                 i know everybody who knows anything                                 about kubernetes will probably                                 be aghast at this slide and and how                                 simple this makes it look but um                                 drilling going going very very very far                                 back this is                                 at least to me the the underlying                                 principle of what kubernetes is about                                 so after having taken a look at                                 kubernetes of course we needed to define                                 our relationship with kubernetes so um                                 our first sort of um                                 goal or mission that we came up with and                                 we pitched this to the customers that                                 last mentioned in the workshop was all                                 right we'll just write a couple of                                 operators to roll out these                                                                                           there are these big data products deploy                                 them to an existing kubernetes cluster                                 and then everybody can be happy                                 because we add the least amount of                                 effort because kubernetes takes a lot of                                 a lot of work off our hands                                 and you have your data big data tools                                 running in a kubernetes cluster                                 but most of our customers actually were                                 not too keen on that                                 because um and a lot of                                 what i'm now saying might be prejud                                 prejudice but that's                                 as you really is probably a little bit                                 of a kernel of a truth in there as well                                 kubernetes is a fairly complex piece of                                 software                                 deploying kubernetes there's a reason                                 why there's many kubernetes                                 distributions and no one                                 deploys it by hand anymore so                                 it's got a lot of moving parts and and                                 things that                                 have grown over time have changed over                                 time even people who work                                 with the community and have been around                                 from the start still say                                 it's tough to keep up and on top of that                                 kubernetes always                                 also always has the sort of people                                 always think that um                                 it costs performance and some of our                                 customers actually run a few tests and                                 in their scenario the overlay network                                 actually cost them lots of performance                                 it might have improved since then but                                 results like that stick with you i think                                 so our solution to that was well we'll                                 just use parts of kubernetes we'll use                                 the control plane which gives us the                                 back end that we need to orchestrate all                                 our stuff                                 but to run the workloads maybe we can                                 simplify that a little bit                                 and not use containers because again                                 containers out of the equation                                 makes the the performance problem                                 if i want to call it that go away and it                                 also                                 allows you to to reuse your existing ops                                 know-how because                                 you can actually ssh into the machine                                 you can log it uh                                 you can tailor log file you can use vi                                 to inspect the config file                                 stuff like that so it's um it gives you                                 everything that your ops people might be                                 used to                                 but on the other hand it also gives you                                 the option to use existing kubernetes                                 tools and know-how because                                 kubernetes does give you in theory the                                 chance to orchestrate                                 and and control and manage your entire                                 stack                                 from one place which is the kubernetes                                 api server                                 and by sticking very close to kubernetes                                 standards                                 we make that possible to with with our                                 tools as well                                 and we'll actually have a demo later on                                 where i'll show how we deploy                                 our workloads on bare metal and i do                                 realize that bare metal is                                 probably a bit of a loaded term in this                                 in this context but                                 we've somehow somehow grown used to it                                 so we keep using it                                 um and looking a bit into the future                                 what this also makes possible is hybrid                                 architectures because for                                 for some of our tools for example if we                                 take hdfs as an example                                 htfs runs two types of nodes it runs                                 data nodes and it runs name nodes                                 and data nodes are very very state-heavy                                 they have terabytes of data                                 that they that they manage ideally                                 locally                                 and if one of these data knows fails you                                 wouldn't really want kubernetes to spin                                 that up                                 three machines over in a different data                                 center on a different rack and then                                 having to move all that data over there                                 as well plus hdfs has redundancy built                                 in so if that node fails                                 it doesn't really matter because htfs                                 has been built to accommodate that and                                 simply                                 keeps trucking along the name nodes in                                 https on the other hand those                                 don't have too much state they simply                                 have metadata in memory that says well                                 that file's there that file's there he's                                 in charge of that                                 if those die it they very much make                                 sense to spin them up somewhere else                                 because they can recover                                 their state fairly easily and                                 so that's having these hybrid                                 architecture sort of allows                                 us to do kubernetes where it makes sense                                 just not everywhere and that's a very                                 good thing                                 so if you come back to the picture that                                 i showed earlier from the kubernetes                                 from orbit i think it was called                                 from outer space um where do we fit in                                 here                                 so this is the unchanged picture what we                                 did was we wrote a couple of operators                                 which should hopefully now be uh                                 exactly um up top it was the sort of                                 unchanged                                 original idea we just write a couple of                                 operators that                                 write stuff out to kubernetes and manage                                 these tools and                                 i'll leave those to last you'll say lots                                 about them later on                                 but what we also did was we wrote a                                 a replacement for the kubernetes cubelet                                 which is called the stackable agent at                                 the moment                                 we suck at naming things if anybody has                                 a good idea what this should be called                                 based on what it does                                 and you'll learn what it does in the                                 next five minutes                                 please let us know because we we can't                                 come up with anything better                                 and what the stackable agent does is it                                 gets pots assigned just like the cubelet                                 but then it goes and downloads not a                                 container image but actually a taji zip                                 file which currently is the the                                 official apache convenience binaries                                 that are released                                 and downloads those extracts them and                                 then it creates a systemd                                 service that runs off those binaries and                                 with config files that it actually                                 creates on disk so                                 if you want to have a look at how your                                 server is doing you can actually ssh                                 into the machine look at a config file                                 the one that is actually running                                 currently not one that's                                 hidden away in a container somewhere and                                 you need to understand the init script                                 that created it                                 and also have a look at the logs                                 so that sort of gives you both both                                 worlds                                 in the same at the same time                                 and then if we move on                                 um of course that you could say that                                 this is the worst of both worlds                                 we have the complexity of kubernetes but                                 we don't have the flexibility of                                 containers                                 and to that i would say yes and no                                 because kubernetes as i said                                 if you take a look at the very core of                                 it it's a very simple thing                                 the api server is pretty much boils down                                 to a rest web service that stores data                                 and informs you if it changes that's                                 that's not a terribly complex thing                                 the complexity comes when you take a                                 look at the control plane which has tens                                 or even hundreds of controllers or                                 operators                                 and has this this complex web                                 of objects that are dependent on each                                 other                                 trigger changes everywhere else but if                                 you if you take that away                                 the very core is pretty simply very very                                 simple data model                                 so what we said is that we explicitly                                 limit ourselves to a very very small                                 subset of the                                 of the data model of kubernetes so we're                                 using a port because that's                                 pretty much no way getting around the                                 port we're using a node because that's                                 what you need to register                                 so the agent registered itself as a node                                 we're using a couple of config maps                                 because                                 that's where we generate config files                                 from and secrets                                 um but all of those are fairly fairly                                 simple and                                 low level abstraction so we're                                 explicitly not using things like                                 deployments or stateful sets or demon                                 sets or things like that because                                 that's that's where complexity starts                                 creeping in                                 um and we actually spoke to a team that                                 develops uh an                                 uh well-known operator and they said yes                                 we are using deployments in stateful                                 sets and                                 we wish we hadn't taken that decision                                 early on because                                 what that does is it gives you quick                                 benefits                                 but once you move past that                                 and you become you start looking at more                                 complex workflows then it                                 also puts you into the core set of doing                                 stuff                                 exactly the way that kubernetes wants                                 you to do stuff                                 and sometimes it can actually be very                                 yeah it can be good to not use them                                 because it gives you more freedom to do                                 things                                 exactly the way that you want to do them                                 taking this even even a step further we                                 could actually use something like                                 kcp which is i think it's called the                                 minimal kubernetes rp server                                 it speaks the same api as the kubernetes                                 api server but it doesn't know any of                                 the objects that the the kubernetes api                                 server knows so it doesn't know about a                                 part it doesn't know about nodes                                 you'd need to create all of those as                                 custom resources and that that would                                 actually allow you to                                 really really stay very minimal in what                                 you do                                 and kubernetes itself is actually                                 thinking about i'm not sure if they're                                 actually actually considering moving                                 into that direction but there's been                                 talk about                                 if they'd be doing it again they would                                 implement everything as a cid                                 pretty much namespaces and then maybe                                 labels would be                                 built in objects but the the tendency                                 towards simply doing everything as a cid                                 and not having anything pre-built and                                 treated in a special way                                 as something that kubernetes the                                 kubernetes community is talking about as                                 well                                 and by doing that and limiting us to                                 this this very very small subset of                                 things                                 and doing a lot of things ourselves                                 which yes that created a bit of overhead                                 in the beginning                                 but it also gives us lots of freedom to                                 implement things                                 exactly the way we want them in our                                 operators                                 and for those operators i'll hand back                                 over to lars and he'll give you a brief                                 overview of what we are doing there                                 yep thank you sir um                                 yeah so what we do at stakeholders you                                 can mention this we basically write a                                 whole bunch of operators and as                                 also mentioned these operators are                                 basically pieces of software that                                 control other pieces of software                                 so um that's this is what it looks like                                 at the                                 pseudocode level um                                 but because what we're doing is                                 basically a we extract the                                 the knowledge that that you like this                                 admins ops people devops people human                                 operators have                                 and we try to extract all of that                                 knowledge into into                                 one piece of software and this piece of                                 software is then responsible for                                 managing a single thing                                 like we have a kafka operator we have a                                 kafka topic operator we have a kafka                                 whatever operator                                 so all of these extract extract the                                 knowledge                                 from humans so you don't need to do                                 these repetitive tasks over and over                                 so an operator                                 basically as zinc also explained already                                 watches the resource so we've got a                                 kafka operator and that watches the api                                 server for kafka cluster objects                                 that you as an end user store on the                                 system and then the operator takes over                                 and it updates and or creates like parts                                 config maps and so on so                                 what it does it translates these high                                 level concepts like in                                 cluster into parts conflict maps                                 whatever                                 so when we started on this journey we                                 wanted to know like how do other people                                 do                                 this like what are best practices and                                 there are some blog posts out there                                 there's some documentation                                 google has a best practice page the red                                 hat has one                                 and this talk is a bit too short to go                                 into all the details                                 and you'll find them out there and what                                 we found is that in in                                 reality in our experience they all of                                 these                                 guides fall a bit short like they stop                                 where it gets interesting                                 most of the time and so what we found is                                 that most people                                 write a single operator or maybe maybe                                 two like for one or two projects and and                                 that's it so there's                                 very little uh there's little code reuse                                 there is some but not not a lot and um                                 our problem is that we have to run away                                 problem we like it                                 but our problem is that we have to write                                 dozens of operators and they                                 should all be and feel consistent to the                                 end user                                 so what we found when looking at                                 existing operators                                 is that all of them do things slightly                                 slightly                                 slightly different so all of these                                 operators                                 uh consist of a reconcile loop and i'll                                 go into that in the next and the next                                 slide                                 and then and what we like to do and what                                 doesn't happen in in the                                 broader ecosystem is we want to have                                 common labels for everything so                                 everything should be common                                 have have common labels for pots objects                                 and whatever                                 we want of common nomenclature that like                                 a role or conflict over there it's                                 called the same in all our operators                                 common status and events they should                                 they should                                 feel similar because they will plug into                                 your into your monitoring                                 the configuration and the cids should                                 feel the same they should feel similar                                 monitoring tracing metrics all of that                                 stuff should be the same so it                                 whether you manage a knife cluster or                                 kafka cluster that shouldn't really                                 matter                                 um these things should have the same                                 names and so on                                 so basically common stuff like                                 across the operators makes it easier and                                 less surprising for the humans using                                 this system and as                                 also mentioned we also have common hacks                                 because what we found is yes declarative                                 is nice but                                 in reality sometimes you do just need to                                 restart this one stupid server over                                 there                                 and kubernetes makes that pretty hard                                 and we there's other people out there                                 who've uh                                 invented workarounds for like please                                 start their postgres                                 backup uh i like this is a this is like                                 a command                                 that's hard to do in a declarative way                                 or like a restart                                 like the declarative this would i mean                                 my goal state is started                                 uh but just from started to start it i                                 want to restart                                 and so we we we worked around these                                 things and our distribution should feel                                 consistent so we decided to write our                                 own framework around these things                                 um okay the reconcile loop this is                                 uh something i looked at over                                    different operators in in three                                 different programming languages and i                                 found that there are two                                 alternative styles to write this record                                 side loop one is the                                 one on the right with huge steps like                                 the reconcile is called                                 something changed my kafka cluster                                 object changed now i'm going to do                                 whatever work i need to do reconcile                                 part one and i                                 wait until it's uh stored and up and                                 running then reconcile part two wait                                 until it's up and running                                 this is good um and it means that at the                                 end of this loop                                 i basically have um i don't know exactly                                 what the state is like that the my                                 target state will be                                 the desired state and that's it right                                 because                                 nothing can interfere i just do start at                                 the beginning and and                                 end at the end but if you have like a                                                                                                       take time like restarts or these kind of                                 things so                                 this can take half an hour or so to run                                 this one reconcile loop                                 and if you change the object in between                                 we won't notif uh notice because we we                                 are not interrupting ourselves anywhere                                 so what we can do instead is have this                                 left uh the small step um approach where                                 we do one thing                                 and then check whether something changed                                 and we need to re-queue                                 this is um what's what what's depicted                                 here so we reconcile part one and then                                 check has something changed                                 if if yes we re-queue which re-queue                                 basically means                                 read my object from kafka from from                                 kubernetes again and then start over                                 so if something changes in between we                                 can immediately react to changes                                 so we decided to go with this small                                 steps approach these names are crap and                                 we haven't come up with better ones as                                 in goods that                                 we have a huge naming problem but yeah                                 this is a                                 a style that we found in in lots of                                 operators one one or                                 one or the other is used in almost all                                 of them                                 so what we did is we have written a an                                 operator framework                                 that's pretty like yes there are some                                 generic parts in there but it's pretty                                 tight to                                 uh to our company and our operators it                                 it can be reused                                 but yeah we haven't especially                                 uh tried to make it reusable                                 for people outside but yet um so what                                 we added uh like lots of convenience                                 functions and structures                                 like retrieve all existing parts uh all                                 existing parts that match a label                                 retrieve and set conditions make sure                                 that the conditions transition                                 transition to the states that we want                                 them remove parts and so on                                 and often used higher level abstraction                                 things like remove all pots that                                 uh like orphaned that we don't need                                 anymore find nodes                                 that that should have a pot that that                                 don't uh                                 these kind of things this requires a few                                 conventions about                                 uh like what our object should look like                                 but as i mentioned before this is                                 exactly what we want because we want                                 consistency across all our operators                                 so um if you know how to use kafka                                 you will also know how to use the knife                                 operator                                 so how does it look uh or work in real                                 life this                                 is uh almost real code like we i just                                 removed                                 a tiny bit of boilerplate in between but                                 basically                                 we have these these two things or these                                 uh the                                 the yeah we can basically change these                                 commands                                 everything in blue comes from our                                 framework for example so we can                                 basically                                 create our own chain the way that we                                 want it red is something you need to                                 implement yourself                                 at the moment we're working on                                 extracting more and more functionality                                 but here for example say in its status                                 um and then                                 please delete all illegal parts that                                 don't match the required labels wait for                                 terminating ports if there are any pots                                 still                                 terminating uh please just re-queue and                                 we'll we'll wait                                 wait for running and ready pots this is                                 for things like a rolling restart we                                 want in a rolling restart scenario                                 we don't want to kick off that i create                                 new                                 new servers new new parts while there's                                 still others                                 that are that are restarting and then                                 delete access ports                                 like you change the uh the specification                                 saying                                 i don't want any more kafka clusters                                 kafka brokers on that rack                                 so now they are access delete them and                                 so we'll delete them                                 and next time we will probably end up in                                 the wait for terminated pots                                 state again and we q we req until we're                                 done                                 and this um like we're all the time                                 extending this framework basically                                 adding more and more methods to this                                 framework which allows us to plug and                                 play uh mix and match                                 these these operators together so we                                 only have to implement the core business                                 logic                                 and don't have to implement all the                                 stuff around it                                 and we found that uh lacking there's                                 some some frameworks out there which do                                 parts of it but we haven't found                                 anything that that does all of what what                                 we need                                 and as i also mentioned we're using                                 these lower level abstractions like                                 parts and deployments and so on that                                 makes it a bit harder to use                                 but again we are moving lots of these                                 things uh into                                 into of our framework which gives us the                                 full flexibility                                 while still being able to uh yeah                                 easy to use so i haven't talked about                                 any programming languages but                                 for for those of you who do know they                                 might have recognized that this is not                                 go this is actually                                 rust so we decided to use rust for all                                 our operators                                 so that's the second twist basically the                                 first is that we're running kubernetes                                 and running                                 this not with containers but with this                                 system d and the second one is we                                 decided to use rust for all our                                 operators and the agent as well                                 so kubernetes and most third-party                                 operators are written in go or java                                 why did we choose rust well when we                                 started this whole thing                                 this was very much a community driven                                 thing everything that we're doing is                                 open source as                                 i mentioned we should have put a link up                                 somewhere um                                 we didn't know either language so we                                 didn't know the go we didn't know rust                                 our background was java the whole uh                                 apache big data ecosystem is java so our                                 background was java so we tried both we                                 tried both languages                                 and we looked at the library ecosystem                                 and we like rest and we liked it                                 better end of story there's not much                                 like there's not no                                 hate for for go it's just we like rust                                 we like the error handling we like the                                 enums we like generics no garbage                                 collection                                 security if it compiles it'll probably                                 work and we're pretty happy with the                                 decision                                 so some things like go structural typing                                 is pretty sweet so it's not like                                 yeah we don't we don't hate go it's just                                 we decided to go with the rust and we                                 are pretty happy with it                                 so the rust kubernetes ecosystem there                                 there are three                                 crates is what we're using uh k                                   open rps                                                            build                                 if you build things and rest on top of                                 a kubernetes this one you'll probably                                 use because it generates                                 uh rust structs and so on from the open                                 rp spec for kubernetes                                 which is super useful they feel native                                 to rust everything works                                 works with generics and so on so this is                                 pretty pretty good                                 the next one is cube rs and that's                                 probably                                 the most important one because this                                 actually wraps                                 the whole k                                                            client around it like uh this is yeah it                                 gives                                 gives you a as it says here a more                                 generic client                                 a go library because yeah generics exist                                 in rust i know they do and go as well                                 now but um                                 it's it's really really easy to write                                 generic things using cube rs and it's                                 fantastic                                 it's being developed all the time the                                 community is super responsive every once                                 everyone's super nice things that we                                 were missing have been added super fast                                 so if you want to build something in                                 rust on top of kubernetes                                 this is the way to go and the last one                                 and the last slide for this presentation                                  as well                                  is something called crosslit and this is                                  uh by the folks over at these labs                                  which belongs to microsoft um that's                                  called crosslit as the name implies it's                                  a cubelet written in                                  in rust and they initially developed it                                  to run                                  web assembly stuff so you can basically                                  run webassembly stuff by assigning it                                  pots as well                                  but it is generic so it doesn't it                                  doesn't matter you can plug                                  plug in a different backend so we                                  plugged in this our systemd backend and                                  that works                                  perfectly fine they actually as it says                                  here crosstalk                                                     they actually i think in the process of                                  donating this whole thing to the cncf um                                  so this will become a more or less                                  official project hopefully soon                                  but i can't make any promises i don't                                  know yeah so this is                                  the quick summary um of what we're doing                                  so we're doing a bit                                  a few things uh special but we are super                                  happy with the choices we made it's                                  it's fun we learned a lot um rust and                                  kubernetes have been treating us well                                  and with that                                  i'll be handing over to zinke because i                                  think we're a bit short on time but yeah                                  okay please take it away                                  yeah that works i can stretch or shorten                                  the demo as necessary                                  so um what you're looking at here is                                  pretty much just to set this up                                  this is our infrastructure that we'll be                                  using to deploy things on so we have                                  an external network which is connected                                  to the internet with one edge node                                  and that also has a wireguard vpn which                                  i'm logged into so that i can address                                  all of the machines                                  in the internal network just as if if i                                  was in there as well                                  and then we have the the orchestrator                                  machine so this is running k                                    as a lightweight kubernetes replacement                                  actually it is like a kubernetes it's                                  not a replacement                                  um so we'll be using that to as our                                  control plane                                  and then we have worker one two and                                  three and                                  main one those are the the actual like                                  proper machines that we want to manage                                  and deploy                                  services on um if you want to play                                  around with this we we do offer                                  um a we run a public rest service where                                  you can download the terraform and                                  ansible playbooks to                                  stand this up just give us a shout after                                  demonstration and i'll uh i'll send you                                  a link where you can download things to                                  to stand this up on your own and play                                  around with it                                  it's um fairly fabulous to be quite                                  honest                                  um so as i said our agent                                  will register in kubernetes as a regular                                  cubelet more or less                                  so um these are the four machines that i                                  just discussed uh this is k                                               it's a                                  command line interface to kubernetes uh                                  you could also use                                  lens or oculus octane it's called or                                  pretty much                                  that that's that's a fabulous thing you                                  can use whatever floats your boat                                  and talks to kubernetes so um this one                                  is                                  this is actually a regular kubernetes                                  cubelet which                                  has been deployed by k                                                 coexist quite nicely                                  and then if you look in here you can see                                  we have a type crosslit                                  and then there's there's a few tanes in                                  here which are pretty much just there to                                  make sure that we don't get any                                  regular kubernetes spots sent our way                                  because our agent wouldn't know what to                                  do with those                                  those taints more or less control that                                  we only get the ports that                                  our operators write out                                  and if i now want to deploy something                                  a any of the big data services um what i                                  would need to do                                  is head over here so this is our                                  also public this is the documentation                                  repository and we have the demo requests                                  in here so if you want to play around                                  with those                                  please by all means go and check them                                  out                                  and this is a zookeeper cluster                                  definition                                  what i was talking about earlier it's uh                                  at this point in time it is fairly                                  simple                                  we simply say we want one instance per                                  per server                                  which you know we want for instance um                                  we want a zookeeper in version three                                  five eight                                  and please uh deploy that on anything                                  that matches this                                  which as we just saw should be worker                                  one two and three                                  and if i now apply this to the                                  kubernetes cluster we should see down                                  here                                  that's been created and                                  i can now over here check out the                                  the the object that we just created and                                  if we go all the way down then you'll                                  see in the status                                  we are tracking what's happening in the                                  background so this is currently doing an                                  initial installation to version three                                  five                                  eight so if we                                  have a look at the pots over here we can                                  see two of them have already been                                  written out and started                                  and the third one is in the product                                  process of being                                  being uh started outside                                  and if you wait a little bit yep so now                                  that's running as well                                  so what the the agent has now done in                                  the background is it's gone and                                  downloaded                                  an apache zookeeper targets that file                                  extracted that and i'll show that in a                                  second                                  and install that on the servers and then                                  started a systemd                                  service that runs that zookeeper                                  and we can actually also uh                                  use the kubernetes locks function so if                                  i                                  if i want to look at the locks and                                  humanities what the agent does is it                                  talks to journal control which is the                                  systemd logging component on the machine                                  and extracts the log and streams that                                  back to me                                  on my management machine over here so                                  this is one of the instances where we                                  have                                  sort of the best of both worlds because                                  i can in my kubernetes                                  ui work with the logs and and see from                                  one central point                                  everything that's going on but i can                                  also head out to the machine and do                                  journal control minus you                                  default zookeeper and see if the lock's                                  uh going on here so you can see                                     oh no yep that's what the two hours utc                                  thingy                                  that should be our current logs                                  so i can do it the old-fashioned ops way                                  but i can also do it the                                  the way cool kids do it and use cube                                  control logs                                  and then just to give you one more                                  example uh i'll also                                  deploy a nifi cluster because this shows                                  um                                  our idea of sort of service composition                                  so life in its cluster mode needs a                                  zookeeper to                                  orchestrate how their cluster nodes talk                                  to each other so                                  in our spec here we give it a zookeeper                                  reference and this is                                  this name the simple is actually                                  something that we'll recognize from                                  over here so if i now deploy this nifi                                  cluster                                  the operator will go out it'll go talk                                  to kubernetes ask for the zookeeper                                  cluster of name simple                                  and retrieve the zookeeper connect                                  string and then use that to configure                                  nifi to find that                                  zookeeper and use that for for its own                                  orchestration                                  so we should now see yep here's a couple                                  of um                                  of knife pods that have been written out                                  this is fairly quick because i did the                                  demo early on and the the download has                                  already been done so um                                  usually this would take a little bit                                  longer but i figured no need for you to                                  wait around for that                                  uh what we can now do is                                  head over to worker one stackable demo                                                                         as usual forget the slash nifi                                  and then you can see that we do have a                                  nifi deployed that should hopefully show                                  us                                  a canvas any second now                                  and up here you can see uh it actually                                  did form a cluster of three nodes                                  that's using the the zookeeper we                                  deployed just before that                                  to coordinate with each other                                  and with that i believe i'm actually                                  already one minute over time and we'll                                  wrap it up                                  thank you yeah                                  you
YouTube URL: https://www.youtube.com/watch?v=0ptB1REVKLw


