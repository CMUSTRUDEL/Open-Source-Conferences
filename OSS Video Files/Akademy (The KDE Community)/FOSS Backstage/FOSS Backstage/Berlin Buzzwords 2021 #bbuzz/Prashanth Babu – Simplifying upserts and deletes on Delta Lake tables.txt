Title: Prashanth Babu â€“ Simplifying upserts and deletes on Delta Lake tables
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Data Engineers face many challenges with Data Lakes. GDPR requests, data quality issues, handling large metadata, merges and deletes are a few of the tough challenges usually every Data Engineer encounters with a Data Lake with formats like Parquet, ORC, Avro, etc. This session showcases how you can effortlessly apply updates, upserts and deletes on a Delta Lake table with a very few lines of code and use time travel to go back in time for reproducing experiments & reports very easily, how we can avoid challenges due to small files as well. Delta Lake was developed by Databricks and has been donated to Linux Foundation, the code for which could be found at http://delta.io. Delta Lake is being used by a huge number of companies across the world due to its advantages for Data Lakes. We will discuss, demo and showcase how Delta Lake can be helpful for your Data Lakes because of which many enterprises have Delta Lake as the default data format in their architecture. We will will use SQL or its equivalent Python or Scala API to perform showcase various Delta Lake features.

Speaker: https://2021.berlinbuzzwords.de/member/prashanth-babu

More: https://2021.berlinbuzzwords.de/session/simplifying-upserts-and-deletes-delta-lake-tables
Captions: 
	                              thanks for having me here                               uh today we are going to discuss uh                               simplifying                               upsets and deletes on delta lake so                               that's our topic for today                               so a brief agenda is                               first initially we'll talk about the                               topic and then we'll discuss a bit about                               challenges with data links uh and also                                we'll discuss about                                next comes the features of delta lake                                and how it helps                                tackle solve the challenges with                                the data lakes usually and finally                                we get into the meat of the topic like                                update delete and upset on a data lake                                table and how                                how easy it is and how simplified it is                                to run                                any of these commands and ensure your                                data lake is fine and general data lake                                is pristine                                and could be used for downstream uh                                analytics and also machine learning                                i'm also touching a bit about optimize                                and vacuum                                these are essential for ensuring you can                                delete some data and also                                ensure you can pinpoint the files and so                                on and so forth and                                i round it up by couple of demos uh                                these are                                i'm showcasing uh update delete absurd                                uh in two different notebooks and                                finally uh optimize and vacuum also is                                also part of                                a notebook so that's about uh our agenda                                for today                                and finally i leave a number of                                references so that you can you can                                go through them in detail i mean this is                                kind of a                                lightning talk i would say a                                          session                                there are enough ample number of videos                                and                                blog posts which are rare in my                                references section which would help you                                to                                dig deep into any of the information                                which                                you might be interested in uh with that                                i'm getting into the topic                                so uh very brief about me i'm prashant                                i've been with databricks for almost                                like three years now                                uh i'm uh email practice lead for rsa                                rsa stands for recent solutions                                architect                                and my linkedin profile is i showcased                                on under                                on the slide here i would be i would                                love to interest                                i would love to connect with you with                                any of you and all of you                                i'm a very very very brief about                                databricks if you are not aware                                databricks is a                                platform to unify data                                machine learning and a workloads                                basically                                you can do everything in a single                                platform which is where i'm going to                                showcase                                my demos as well and databricks you                                might be aware already are the original                                creators of spark                                data lake ml flow and coils and we                                almost have                                                           the globe using                                our databricks um again                                one simple slide to explain what data                                break series and what lakos platform is                                you might have data in aws also or gcp                                the three main cloud vendors in                                structured format                                unstructured format are semi-structured                                formatter streaming i mean these are the                                kind of workloads which could be                                processed                                with data breaks for data science and                                engineering or bi and sql analytics                                machine learning and finally real-time                                data applications also so this                                all this is underpinned mostly with                                delta lake                                uh which is what we are going to talk                                about a bit more bit in detail uh                                in today's session um these are the                                 uh i mean this is a very very simple                                 sample of                                 uh our customers and you can see couple                                 of uh                                 german heavy customers also represent                                 like uh                                 daimler for example or zelando et cetera                                 so on so forth so                                 these are that is a very brief about uh                                 data breaks and finally getting on to                                 the topic uh what are the typical                                 challenges with                                 data lakes and how data breaks delta                                 solves these challenges is what                                 we are going to discuss in the next few                                 slides so                                 um as you can see most of the                                 uh i mean as per many servers in fact                                 mit's law on management review says                                    percent of cos say                                 ai is a strategic priority at the same                                 time gartner says                                                                                                     could be created by ai                                 by end of next year                                                     here                                 but but there are there are a bit of                                 problems here like                                 it is very hard to get right and it is                                 just not evenly distributed so                                 the same gardener which predicts                                         uh trillion dollars business value also                                 says                                                                                                      venture build also says                                    of data science projects never make it                                 into production and                                 and some companies like uber google                                 amazon                                 etc are making are having huge huge                                 success                                 but a lot of them struggle and most of                                 the reasons would be around                                 the data are the data which is                                 hit sitting in the data lake and which                                 is causing some challenges and we are                                 going to drive down                                 drill down and discuss in deep dive on a                                 couple of challenges with                                 data lakes the first one is something                                 very very simple like uh appending new                                 data using spark                                 into a data lake but at the same time                                 some                                 other processor or some other pipeline                                 is also trying to read                                 the same data so that usually                                 essentially causes a ton of issues                                 and user users usually want all the data                                 all their changes to appear all at once                                 this is                                 very very hard to achieve making                                 multiple files all appear at once                                 or even a single file to appear in full                                 form and                                 it's not supposed to be was not supposed                                 to be work or supported out of the box                                 with uh data lakes that is uh the first                                 and foremost problem with uh                                 data lakes the second problem is of                                 about modifying existing data                                 is very very difficult i mean take the                                 classic case of gdpr where                                 some someone sends a request for                                 deleting their data                                 i mean from any of the organizations and                                 that implies you have to read all the                                 data                                 and then filter that particular row or                                 those particular rows                                 from the data and then rewrite the data                                 into the data lake                                 so that is again a huge a big big                                 problem i mean gdpr run ccpa for that                                 matter                                 so there are many many manual techniques                                 which are applied and which are very                                 very unreliable                                 one of which we are going to discuss                                 here today uh in the demos                                 and the third and and third option the                                 third challenge with                                 data lakes is jobs failing midway i mean                                 most of the big data pipelines and spark                                 pipelines you would easily                                 understand half of the data appears in                                 the data                                 and rush might be missing so it is                                 usually a problem with                                 jobs failing midway which call which                                 caused this particular challenge                                 um another another problem is mixing of                                 batch and real time that is usually                                 turns out to actually heal usually and                                 it is very very tough to                                 uh mix them and it leads to a lot of                                 inconsistencies and                                 and one of the variations of the first                                 problem is with appends but at the same                                 time                                 streaming also adds a bit more                                 inconsistency and                                 basically you're reading partial results                                 if i can say so                                 um fifth topic uh fifth challenge being                                 it is very very costly to keep                                 historical versions of the data i mean                                 usually all the regulated organizations                                 need                                 some or many of the versions of the data                                 to be available in the data lake that                                 is bound to be costly as well as                                 leads to a lot of governance issues as                                 well i mean auditing and                                 governance issues as well and it is very                                 hard to do                                 uh sixth challenge is about uh                                 difficulty to handle                                 large metadata i mean if i used uh                                 hadoop hdfs for that matter where you                                 would                                 have a huge amount of data in your                                 hdfs that internally causes i mean                                 that's because                                 a huge amount of data implies large                                 metadata to be stored at                                 name node for example and all such                                 problems uh would                                 magnify the moment you use petabyte of                                 data in the data lake and                                 it's very very tough and even the                                 metadata itself                                 lands into gigabytes and gigabytes of                                 data                                 um this is one of the most classical                                 problems i would say                                 like too many small files uh too many                                 files i mean                                 because of your using streaming for                                 example that implies                                 too much of data is landing in and you                                 are processing the data                                 at a very very breakneck speed like                                 every                                                                   example or even                                 every minute and you are saving the data                                 into the data lake that implies                                 you are storing two tiny small files too                                 many                                 small files are some are sometimes                                 gigantic files also i mean either of                                 them are usually a big big challenge and                                 most of the time is spent by spark just                                 opening and reading the files rather                                 than                                 opening and closing files rather than                                 reading the file usually                                 on the same note it is very very tough                                 to get                                 great performance and it is it has to be                                 manually done                                 and it is error prone to get right                                 partitioning and ensuring                                 manual techniques applied to get a very                                 decent performance not not so great                                 performance i would say                                 it's more of getting a decent                                 performance here                                 and finally data quality issues like                                 usually uh i mean it's happened that                                 data evolves                                 and that implies as the schema evolves                                 the underlying storage                                 would either have to read the data or                                 store the data and that implies the                                 downstream pipelines would have problem                                 in reading that particular data which                                 has                                 different metadata different columns                                 compared to the earlier data so                                 all these are usual challenges which you                                 will face                                 uh with any of the existing data links                                 which are stored in                                 any of the formats like pake for example                                 so this is where                                 our delta comes into the picture and                                 i'm going to explain why delta lake                                 solves these particular problems                                 whatever we                                 discussed the name challenges and how it                                 solves also                                 is what we are going to discuss now so                                 first and foremost it is                                 based on open format and it is open                                 source you can find all the code                                 of data at delta dot io uh it is                                 basically                                 an opinionated approach                                 for building a robust data x and what i                                 mean                                 that is like it has its own transaction                                 log mechanism                                 i mean it we i will briefly show in the                                 next slide itself where                                 how the transaction log looks like so it                                 brings                                 both the best of data fire housing and                                 data lakes all together into one single                                 format                                 and it helps for ensuring the downstream                                 reading                                 is perfectly fine even when you're                                 writing some data to the same table                                 same table same location databricks                                 delta adds                                 reliability quality and performance to                                 data lakes                                 how it how it does is what we are going                                 to discuss in the next few slides                                 um delta lake is comprised of                                 only three three important topics one is                                 uh delta tables which is where the                                 data is stored and the delta                                 optimization engine                                 which is where the it allows to do                                 mergers upsets                                 deeds it allows to do vacuuming and                                 and optimizing so on and so forth and                                 finally delta x storage layer so these                                 are the three                                 uh components of delta lake now                                 to add on top of what i briefly                                 mentioned before                                 uh telelac offers all these important                                 features like                                 asset transactions and spark i mean you                                 can                                 can ensure that whatever you're writing                                 to delta table                                 that will not be read by another                                 pipeline which is reading at the same                                 time i mean that's how                                 that is all transaction isolation is                                 maintained on tilde lake                                 it allows for unifying streaming and                                 batch                                 with the same table you could write to                                 the same table                                 and a bad chance can also write to the                                 same location and also streaming also                                 can write to the same location and it                                 allows                                 uh both uh both the both the patterns to                                 be to be done at the same time                                 so basically lambda architecture being                                 resolved just by using                                 uh delta format and it also talks about                                 uh schema enforcement and where required                                 you can also enable schema evolution                                 which is                                 what we have is simple demo uh                                 showcasing that particular feature today                                 uh it also allows you to do time travel                                 like you can go back in time                                 and look at the data and how it who                                 processed it who added it and                                 on which cluster on which date so on and                                 so forth could also be seen uh                                 on using time travel uh upsets and                                 delays are one of the major                                 major important factor important options                                 of                                 delta uh suggestion support also is a                                 palatal                                 so just going back to the all the nine                                 challenge challenges                                 uh how delta tackles those challenges is                                 what we'll discuss in the next few                                 slides basically so                                 there are asset transactions and all                                 these                                 the first five channel challenges are                                 are resolved by delta lake by using                                 asset transactions                                 each and every table whenever you write                                 data it sits                                 in the cloud object storage or hdfs for                                 that matter                                 and there is a small uh metadata folder                                 which gets created like as you can see                                 here                                 in the uh in the location slash path                                 slash table slash                                 slash uh underscore delta underscore                                 that's the                                 folder location i mean wherever you                                 write a table                                 uh say say customers table for example                                 and in within that customer's table                                 there will be a subfolder created                                 by name underscore delta underscore log                                 and within that you would have for each                                 transaction there will be a separate                                 file created a json file connected                                 so which is what the is the heart and                                 soul of                                 uh delta basically so whenever you write                                 any entry any any                                 row or delete or merge or do anything on                                 a particular table                                 all that is recorded as transactions in                                 uh in that particular table                                 so going ahead uh yeah                                 and finally whenever uh the number of                                 transactions increase                                 what uh database delta does is it                                 checkpoints them as a packet file so                                 that is also done                                 implicitly by databricks you wouldn't                                 need to do that you wouldn't need to                                 worry or bother about that so                                 this is how it does all this like it is                                 hard to append data and all these                                 problems are resolved just by using                                 uh data and as we discussed a bit about                                 time travel                                 yeah it allows time travel and all the                                 transactions are recorded because of the                                 transaction log                                 and it will allow you to go back and                                 play                                 basically play play forward um                                 difficult to append uh difficult to                                 handle large metadata all large metadata                                 as i mentioned                                 it metadata is stored in open pocket                                 format and                                 and it is resolved by just by reading                                 the file and also                                 portions of it can be cached and                                 optimized for faster fast access                                 um this is a huge huge problem usually                                 too many small files or                                 profiles this is where things get get                                 very very very interesting like                                 with delta you can just write a simple                                 command optimize so once a table                                 that implies it will bring back all the                                 entire file in                                 entire data in that particular folder                                 where possible to                                           one file each i mean and it works within                                 the partitions also so                                 this way we are going to uh we are going                                 to resolve the problem of too many small                                 files as well                                 uh finally data quality issues like                                 schema validation and                                 evolution um delta supports                                 schema validation as well as evolution                                 even in merge and                                 merge scenarios mergers and absorb                                 scenarios as well                                 so this is what we are going to discuss                                 today i mean that                                 exactly the same topic we are going to                                 talk today updates deletes and upsets on                                 a delta lag table                                 so um the after the nine challenges we                                 discussed                                 uh i'm going to touch upon heart open                                 data                                 modification of existing data being uh                                 difficult                                 and finally too many small files uh uh                                 to main file                                 small file problem as well as poor                                 performance so                                 what are the sample use cases for                                 updates deletes and                                 upsets so first and foremost would be uh                                 whenever you want to do a delete or a                                 merge                                 there might be a problem there might be                                 a case where someone sent a request for                                 uh right to be forgotten so gdpr                                 compliance should be the                                 might be the one of the simplest use                                 cases you would you couldn't imagine                                 and uh duplication deduplication you                                 would like to read                                 your entire data lake even that is                                 simply easily possible with                                 delta i and finally what are the                                 challenges with                                 with this without delta lake is                                 it is inefficient probably possibly                                 incorrect                                 and it is very very hard to maintain and                                 unreliable any of these upsells                                 mostly more so with merges it is very                                 inefficient                                 and it is very very manual to do it so                                 which is what will come to the first                                 topic which is update                                 update on our delta table now if you see                                 the syntax it almost                                 looks like exactly what you would do in                                 a rdbms                                 uh query rdbms any of the rdms                                 rdbms you might have used so the key                                 features are                                 updates any column for the rows that                                 match a predicate which is what                                 it's a pretty simple statement to do it                                 and similarly for delete exactly the                                 same                                 like what you would do in a rdbms delete                                 from so-called                                 customer table where some column                                 predicate is what you would provide                                 i mean in both the cases updates the                                 column values for the rows that match a                                 predicate but if you don't                                 provide any predicate it updates all                                 values for all rows whatever                                 your you have mentioned here like update                                 languages set name is called python                                    specified without a predicate                                 it will just blindly update everything                                 so that's the same thing even for delete                                 if there is no predicate given then                                 deletes all the rows                                 and which is finally we come to the                                 important topic of                                 upsets so merge without data lake would                                 be very very painful to                                 just to walk through the simplest                                 possible approach which is approach to                                 uh with with merge analyzing the                                 updates on a table and find out the                                 partitions to overwrite                                 that will be the first step and read all                                 the data in the relevant partitions in                                 the target table                                 then joining these two tables overrate                                 all those partitions in exchanging                                 location                                 and then atomically publish this is what                                 it would look like                                 i mean emerge will look like without                                 delta lake and how merge is resolved                                 with data lake                                 it is a pretty pretty simple uh                                 statement to do it                                 like if you do merge if you have a                                 customer's table and if you have an                                 update stable                                 now you would like to do update                                 some customers whose customer id and                                 source id are present                                 and you have a new address for all those                                 customers you can just do this like                                 it in this case we are both doing an                                 update                                 set and if it is not available then we                                 are inserting so basically                                 up cert update and insert update                                 or insert is what is happening and we                                 can also do uh in fact delete also in                                 this                                 so behind the scenes what merge does is                                 it                                 basically does an inner join between                                 update and                                 target and it is not doing it on the                                 entire data                                 it's actually going and looking at the                                 min and max files of                                 max values of the file and getting those                                 values                                 and trying to do uh some intelligent                                 analysis there                                 so i'm not working through everything                                 here but                                 i'll just walk through so that i can get                                 the demo sooner                                 um optimize and vacuum are very                                 important concepts as i said                                 it is bing packing compact compaction                                 and also                                 it allows data skipping with these with                                 optimized events where                                 so date and zero duraby and                                 similarly vacuum you have vacuum it is                                 pretty simple                                 to do vacuum and so on so table it will                                 clean up                                 all the old undropped untracked files of                                 delta                                 so that it can limit the storage cost                                 i will quickly jump on to the demos it                                 is pretty pretty simple demos                                 i'm using this is a simple cluster                                 i'm using this is database platform by                                 the way and                                 i'm using a simplest possible                                 use case here sorry i think i'm sharing                                 wrong screen                                 so i'm showcasing uh update uh columns                                 of a delta table                                 and delete rows away delta double so                                 basically what i'm trying to do here is                                 uh                                 i have a small data set like i have                                 i have a small data set where i i have                                 spark                                 database and data i mean by mistake                                 someone wrote a code                                 which ensure which cost the pipeline                                 uh to fair to to uh right                                 to write incorrect values like as you                                 can see here it is                                 data now this is a over simplified                                 example per se so that we can walk                                 through                                 uh the use case to to explain what                                 databricks filter does so                                 what i'm doing here is i'm just writing                                 the data to                                 a data table in the data format and                                 providing a path so that implies                                 it is writing to the uh external table                                 basically                                 now i'm displaying the data in from the                                 delta table here                                 as you can see it is it is it is coming                                 as                                 data as you can see now as we                                 saw earlier now what i'm trying to do                                 here is i have                                 a id column which i know there is a tree                                 and                                 for which there is data here now this is                                 where we are going to do                                 some magic like updates onto table set                                 some column is equal to some value where                                 condition i mean you can specify                                 either this condition or this condition                                 but basically both are exactly same                                 and it is atomically doing everything                                 behind the scenes so                                 it it is also showing number of affected                                 rows here                                 this is how delta performs uh in real                                 world so                                 you can see the value got changed and                                 the same thing which i'm going to do                                 here is i'm going to delete that                                 particular row                                 uh from from delta so again it shows how                                 how many rows it got affected i mean                                 this is your own simplified                                 example but you can get the gist out of                                 what i'm trying to do here                                 and uh let me display the table                                 again so you get a spark and data breaks                                 because we                                 deleted uh id                                            behind the scenes as i mentioned                                 databricks is maintaining a transaction                                 log which is                                 what would look like and this is the                                 visual representation of                                 transaction log every operation i mean                                 this is my email id and this is my user                                 id and this is the time stamp                                 i'm based out of london so it is showing                                 gmt basically                                 here and you can see the what kind of                                 operation was done                                 and what are the predicates what the                                 operation parameters                                 etc etc all this information is at a                                 single snapshot like                                 a single single source of truth                                 basically and if i want to do                                 some time travel for example i can go                                 back in time and play                                 play on the time data like you can see                                 here i                                 initially have uh ingested data as data                                 so which is what it is showing                                 now the next step is showcasing uh                                 delta so if you see in the third when                                 we deleted the third row we can see                                 delta is not present but if we go back                                 in time we can see that                                 and finally the most recent version is                                 dead                                 you can see the most recent question so                                 this is how it is                                 so pretty simple and name and easy to do                                 uh with delta update delete and all this                                 very very simply so let me go to my next                                 notebook which is more schema evolution                                 probably i'm zipping through because i                                 just have five more minutes                                 um so this is schema enforcement during                                 merge                                 and the the use case here is                                 i have two columns id and name the                                 latest data has                                 three columns id name and year so a new                                 column has been added to the data set                                 year which is what we are showcasing in                                 this particular case                                 but the same time my requirement my                                 business case says                                 i want to insert all the data merge all                                 the new data to the existing delta table                                 but also enforcing the schema so that                                 implies                                 i need to discard the new column in the                                 delta table                                 so if i can quickly run through the                                 entire notebook                                 what i would do here is i'm just running                                 the entire                                 notebook and i have couple of tables                                 source and target                                 same as before i have three rows here i                                 mean again                                 over simplified example so that it is                                 easy to explain                                 now i wrote the table into data into a                                 data table                                 and i'm displaying the data here now                                 the new data frame after couple of days                                 assume after a couple of days                                 has a new column as year now there are                                 two use cases here one                                 you want schema enforcement strictly                                 adhered schema validation is done and                                 it shouldn't add this new column into                                 the data into the                                 data link so which is what we are seeing                                 here so basically                                 i am doing a merge into some table using                                 a source table based on a condition of a                                 predicate condition of target.id is                                 equal to                                 source.id so i have id                                            already present in the                                 data in the new data frame id                                         column                                         now when i run merge syntax merge                                 command on this table because this                                 syntax you can see                                 it's pretty simple it will look at all                                 the rows                                 and populate uh and update rows where                                 it's required                                 and where it is uh that particular row                                 is not present it will insert the row                                 so going back and you can see                                 it is schema is enforced strictly so                                 there is no new column here                                 and if i go to the third notebook then                                 my final notebook here                                 here is where i am doing a schema                                 evolution                                 it's exactly the same uh notebook with                                 no changes at all but                                 only change i'm doing here is there is                                 an option                                 if you want schema evolution to be                                 available even in the                                 merge you can you just need to enable                                 auto merge                                 as syntax the moment you set this config                                 what delta is doing behind the scenes is                                 it is allowing you like                                 as mentioned before it's exactly the                                 same uh data frame                                 the first data frame as well as the                                 second data frame second data frame                                 also has a year column now now                                 with the same exact statement what i'm                                 doing                                 here is because i enabled schema                                 evolution                                 you could see here is coming out to be                                 the value what we ingested so basically                                 we have replaced databricks earlier                                 value                                 with                                                                    happening                                 behind the scenes you can you can also                                 uh                                 look at look at the way it is                                 working and finally let me go to the                                 last last thing here                                 which is uh showcasing time travel and                                 optimization                                 vacuum so this is uh as                                 i mentioned create table and merge which                                 is what we are we are doing                                 at the same time we can do a time travel                                 i mean we can go back to zero and one                                 zero                                 didn't have the year column here but now                                 the latest version has                                 here column and finally there is an                                 option called                                 optimize as i mentioned optimize you can                                 see the beauty of optimize in a single                                 command like                                 here i have number of files as three the                                 moment i                                 run optimize command if i do a describe                                 detail again on the same table                                 to optimize it will optimize all the                                 files into a single file i mean                                 in this case we are using a smaller file                                 but that's how it is                                 and finally the last thing which i                                 wanted to showcase is vacuum                                 now before i run vacuum there are so                                 many files                                 the moment i mean database doesn't allow                                 you to run vacuum as is                                 so you have to enable uh if you want to                                 run it as zero                                 ratings you have to enable sp special                                 flag                                 once you enable special flag all the                                 untracked files                                 will be deleted from uh the the cloud                                 object storage are                                 the local storage so this is these are                                 the three                                 uh notebooks three use cases which i                                 wanted to showcase                                 and uh uh if you have any questions or                                 uh any anything you would like to know i                                 would be very happy                                 to answer them and these are i am                                 leaving further references                                 so you can take a look at it this is a                                 very                                 new book like uh the three early release                                 chapters were released just last week                                 uh you can take a look at this uh the                                 new book on delta lake                                 and finally learning spark also has a                                 chapter on data lake so                                 there are a couple of uh docs and talks                                 and                                 webinars and so on and so forth i'm                                 leaving all this uh for your reference                                 please do uh let me know if you have any                                 questions uh and                                 thanks for having me uh thank you very                                 much for uh giving me                                 uh time today                                 um hi thanks again for for your                                 presentation uh                                 it's really interesting to see all the                                 things that delta can do it's pretty                                 interesting format and                                 well that's what you show is pretty cool                                 i'm just going to check to see if there                                 are some questions the last                                 they were not so maybe i could start                                 with just one from mine                                 uh what are i mean well we talk about                                 all these really nice things that delta                                 can do what are some limitations that                                 that has                                 at the moment or things that you plan to                                 improve or add in the future                                 uh delta e is evolving as we as we                                 continuously like i mean because mergers                                 usually cause                                 a lot of problem i mean they create                                 multiple small files so                                 as time goes on their database is adding                                 more and more features into delta like                                 low shuffle merge for example i'm just                                 giving one simple                                 example um which will allow which will                                 not                                 remove the ordering which is there in                                 the local files i mean the files                                 which are red so so that it will not                                 relate the files                                 in a different order rather it will                                 retain the exact same order                                 which was there before because of the z                                 ordering it will help                                 for data skipping for example so as time                                 progresses                                 delta is adding i mean databricks is                                 adding more and more features into                                 delta like change data feed is one more                                 new feature which is coming in                                 which is in private review to be correct                                 you
YouTube URL: https://www.youtube.com/watch?v=BcOImrLimWw


