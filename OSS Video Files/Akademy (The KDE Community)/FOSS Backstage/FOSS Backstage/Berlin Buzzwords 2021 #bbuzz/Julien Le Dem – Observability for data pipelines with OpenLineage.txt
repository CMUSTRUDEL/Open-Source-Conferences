Title: Julien Le Dem – Observability for data pipelines with OpenLineage
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Data is increasingly becoming core to many products. Whether to provide recommendations for users, getting insights on how they use the product or using machine learning to improve the experience. This creates a critical need for reliable data operations and understanding how data is flowing through our systems. Data pipelines must be auditable, reliable and run on time. This proves particularly difficult in a constantly changing, fast paced environment. 

Collecting this lineage metadata as data pipelines are running provides understanding of dependencies between many teams consuming and producing data and how constant changes impact them. It is the underlying foundation that enables the many use cases related to data operations.

The OpenLineage project is an API standardizing this metadata across the ecosystem, reducing complexity and duplicate work in collecting lineage information. It enables many projects, consumers of lineage in the ecosystem whether they focus on operations, governance or security.

Marquez is an open source projects part of the LF AI & Data foundation which instruments data pipelines to collect lineage and metadata and enable those use cases. It implements the OpenLineage API and provides context by making visible dependencies across organisations and technologies as they change over time. 

Speaker:
Julien Le Dem – https://2021.berlinbuzzwords.de/member/julien-le-dem

More: https://2021.berlinbuzzwords.de/session/observability-data-pipelines-openlineage
Captions: 
	                              hello                               i'm julian i'm the ceo and co-founder of                               datakin                               and today i'm going to talk about data                               lineage and observability with opening                               it                               so to start um i'll start by talking a                               bit                               about the need for metadata and state                                the problem we're solving here                                and then i'll talk about the solution um                                in particular talking about open lineage                                the open standard for linear collection                                and mark as its reference implementation                                and i'll finish by uh discuss a bit of                                how that works in practice                                and show you a couple of examples of                                usage of open lineage                                [Music]                                so first i'll start by talking about                                the need for metadata                                and this raises from the fact that                                we are consuming and producing more and                                more data and it's not                                necessarily about the volume of data                                even though like there are more and more                                different data sets and tables and                                different                                locations they are stored in but it's                                also about                                um an organization uh growth                                like in a company in an organization                                there are a lot of different teams that                                consume and produce                                data and within their own                                team their own practice they're very                                they know fairly well how it works and                                how they depend on each other but across                                teams                                it's often where their friction happens                                um and this is you know like people talk                                about                                data ops data mesh uh different of those                                practices and really treating                                the data as a product and                                if we draw a parallel to service service                                oriented architecture                                or microservices you know you have                                different teams                                that expose interfaces and                                in a service world the interface is an                                api in the data world the                                interface is the data set                                so it's really important to start                                understanding                                where the data you're consuming is                                coming from of who's consuming                                the data you're producing and have                                visibility across those teams                                otherwise every time something changes                                it's hard to communicate and it's hard                                to understand how we depend on each                                other                                and so today very often we have a                                limited                                context right we know we have to compute                                um and consume data but it's                                you know it's not always clear where is                                the data source what is the schema                                who owns it how often it's updated                                uh where it's coming from was using the                                data but that's changed right like some                                like                                where is it coming from where who's                                consuming the data i'm producing                                what the schema supposed to be how fun                                is it changing                                all those things are very unclear which                                makes it very difficult                                uh to consume data and transform it and                                produce metrics some                                models or value things in a reliable way                                and so that leads us to i've borrowed                                this maslow's                                heart key of needs and you know the                                original master's hierarchy of needs it                                starts with                                before you start looking into happiness                                right                                there's some basic needs you need before                                you can even like                                have a live a happy life first you worry                                about                                um food and shelter right once                                your food and shelter you worry about                                safety                                um and then and so on right and once you                                those basic needs                                are fulfilled then you can think about                                 happiness uh how do i become the best                                 version of myself                                 or whatever goals you set for yourself                                 and                                 so in this data hierarchy of needs                                 there are lots of basic things that need                                 to be covered                                 before you can even think about                                 taking advantage of your data of uh                                 getting value out of it                                 first the data needs to be available you                                 know you need to                                 wherever it's coming from you need to be                                 able                                 to collect this data                                 second it needs to be updated in a                                 timely fashion                                 right like if you have steel data you                                 can't make good decision                                 and third it needs to be correct                                 right you need your data to be available                                 to show up on time and be correct and                                 once you have all those things achieved                                 then you can start into okay what are                                 business optimizations we need to do how                                 do we find new business opportunities                                 how do we actually get                                 value out of our data and this is                                 really getting those three first levels                                 uh correct is really what's going to                                 help you get your                                 head above the water so that's why i was                                 drawing this line here it's like                                 as long as you're spending all your time                                 figuring out why                                 you know your pipelines are broken or                                 are they taking a long time                                 hawaii is making sure your data is                                 correct                                 you are not actually spending your time                                 on                                 actually getting value out of your data                                 and so that's why we started building                                 open lineage right                                 and in the past uh you know so obviously                                 i've um been the co-creator of parque                                 and later on i've been involved in um                                 the start of the apache aero project                                 um and really building those standard in                                 open source                                 and some of the things we did for                                 apechiero is to reach out to a wide                                 group                                 of people who were interested                                 in a standard having a standard                                 representation to enable                                 exchange of data in a performant way                                 between various systems                                 and so we already took a page out of                                 this                                 for the open lineage project and so it                                 came from the realization                                 well if we want to enable observability                                 and really enable collection of lineage                                 for data pipeline lineage and all the                                 related metadata                                 this needs to be a standard that can be                                 shared across the ecosystem and so you                                 know open lineage                                 draws a parallel with open telemetry                                 which                                 open telemetry is this project part of                                 the cncf                                 about collecting traces and metrics for                                 services                                 and that helps understanding                                 dependencies between microservices for                                 example or any service oriented                                 architecture                                 and old and building observability for                                 that so opening edge is really                                 the same thing for data pipelines and                                 data pipelines work                                 in a fairly different way so we started                                 the open lineage                                 by reaching out to a wide group of uh                                 people in the data ecosystem um                                 and we've been and talking to them                                 um                                                people agree that yes we need a standard                                 for collecting                                 lineage and why don't we have it already                                 and so                                 well the reason we didn't have that yet                                 is because                                 we need to get together and actually                                 start it                                 right like if nobody starts defining a                                 standard for                                 collecting lineage in the ecosystem it's                                 not going to happen by itself                                 and so by getting together and really                                 discussing how we can define this this                                 standard and make it happen                                 rarely get this effort started and so                                 that's what we did last year                                 um in starting this open lineage project                                 and getting contributors and creators                                 of many of the important open source                                 data project                                 to contribute to the spec                                 and so the purpose of open lineage right                                 so                                 defining it as an open standard for                                 metadata a lineage collection by                                 instrumenting data                                 pipelines as they're running and so to                                 make it simple                                 you know how in your digital camera or                                 smartphones today when you take a                                 picture it actually saves                                 a lot of metadata in the picture itself                                 right it's going to save the                                 gps coordinates where the picture was                                 taken when it was taken                                 some other information about the                                 characteristics of the lens of the mode                                 being used and the best time to capture                                 this metadata but the picture                                 is when it's taken and so you know                                 that's what open lineage is                                 open lineage is the exif for data                                 pipelines                                 it is standard to capture the metadata                                 about the job at the time                                 it ran and so                                 and this is the best time to understand                                 you know the characteristic of the job                                 what was the state of the data set when                                 you consume it                                 uh what was the schema some other                                 runtime information                                 and so on                                 [Music]                                 and and having this starter itself                                 really an architectural problem right                                 there are a lot of projects                                 that are interested in consuming lineage                                 and there are a lot of project                                 that may produce lineage or have                                 inherently                                 lineage because they're transforming                                 data                                 and before upper lineage there was a lot                                 of duplication of effort                                 each project that cares about metadata                                 lineage has to instrument                                 all possible types of job to extract                                 information                                 so this picture on the left here where                                 whether your data catalog or a                                 governance product                                 or a discovery a data discovery                                 any type of system of operational system                                 you need to integrate with all those as                                 transformation framework processing                                 systems                                 warehouses to collect the lineage                                 and so introduce a lot of duplication of                                 effort                                 each project has to instrument all the                                 jobs                                 and also it makes it the whole thing                                 very brittle                                 because integrations are external                                 and can break with new versions not only                                 you have to deal with spark versus                                 bigquery versus snowflake versus presto                                 you also have to deal with evolution of                                 all those things                                 all the versions of spark and so on                                 right                                 and so depending on the internal of each                                 of the system to understand lineage                                 becomes very brittle                                 very expensive very complex so with open                                 lineage                                 there are a lot of multiple advantages                                 one is that the effort of integration is                                 shared                                 right once we expose lineage in the                                 common standard                                 you can leverage that across multiple                                 projects                                 that consume lineage and second                                 integration can also be pushed in each                                 project instead                                 now open lineage becomes a standard                                 interface right so you can pull that                                 into each of those projects and instead                                 of having                                 external integration that pull the                                 lineage from each of the system by                                 enters                                 understanding their internals you can                                 have each                                 of those lineage producing system                                 to expose it in the open lineage or                                 presentation                                 so that it evolves with them right you                                 don't have to worry about                                 evolution of the internal because now if                                 the                                 opening exposure of the lineage evolves                                 with each of the project                                 so it's really kind of you know in                                 software design                                 this is known as inversion                                 of dependency right we instead of having                                 each of those                                 uh lineage interested consumer project                                 depend on each of those projects all of                                 those projects depend on open lineage                                 which uh remove the dependency and                                 everybody depends on open lineage                                 and it removes this uh n square                                 so to speak dependencies um here                                 so to clarify the scope right open                                 lineage is really like                                 open telemetry it is the spec                                 and the integration and the ability to                                 expose lineage                                 it is not any of the storage or                                 use cases that you can build on top of                                 the lineage so                                 in the consumers of course marquez is                                 the reference implementation                                 of the open lineage spec                                 and so openlineage defines how you                                 collect metadata how you represent it                                 and then projects like marquez de la                                 hoya munson in nigeria                                 can consume open lineage um                                 and um to leverage it to make lineage um                                 available in their system and that here                                 i'm putting examples                                 of the open source world but really                                 um that's applicable to many                                 to any other um lineage                                 focused uh system                                 and the interesting part party here is                                 like when you talk about lineage                                 um different people will have different                                 definitions                                 of what lineage is and so very often                                 you have dedicated products                                 or projects or uis for various projects                                 right                                 you may be interested more in operations                                 uh reliability of your data data quality                                 you may be interested more in data                                 discovery data catalog                                 you may be more interested in governance                                 compliance                                 um or in privacy and like                                 enforcing things like gdpr or ccpa                                 and all those use cases are very                                 different but they all                                 rely in some way on also understanding                                 lineage and                                 some of them may define lineage as                                 understanding how this column is derived                                 from                                 other columns in car data set it could                                 be understanding                                 as oh the update of this data set                                 depends on the execution of this job                                 that consuming those also data set or it                                 may be interested in                                 knowing how this um                                 pii user private information column                                 is being consumed and where is being                                 copied in your warehouse                                 so if we look at the core model of um                                 open lineage open lineage is defined                                 as a json schema spec so which                                 as a core model of understanding jobs                                 and runs and data sets                                 and from that we can attach various                                 pieces of metadata around it                                 so the core model is very simple you                                 have a notion of job                                 and a job is a recurring transformation                                 that can happen                                 it could be a sql query it could be a                                 spark job it could be a flink job                                 um it could be a python script                                 that reads data and produced data and is                                 recurring                                 um is running um multiple times                                 a run is a particular instance of this                                 job running                                 so it could be running at a specific                                 time                                 for a specific schedule                                 it has um it could have specific                                 parameters                                 and so on the data set is                                 um a representation of data so it could                                 be                                 a table it could be a folder in the                                 distributed file system                                 something like s                                                 and so we have this core model of                                 lineage where you will have a job                                 is running for a specific run id and                                 it's reading and writing specific data                                 sets                                 so that the core of the model and                                 dc some of the very important things                                 here                                 is how we build consistent naming for                                 jobs                                 based on what scheduler is scheduling it                                 so typically you can build                                 hierarchical name from scheduler job                                 tasks                                 and for data set in a similar way to be                                 able to stitch the lineage back together                                 understanding consistent names for data                                 set is really important                                 and that's part of the spec how do you                                 consistently name                                 jobs and data set so once you have this                                 core model                                 of knowing that there's a particular run                                 for a particular job that                                 read and wrote to dataset then the                                 notion of asset                                 is what enables attaching values uh                                 aspect of metadata to this core model                                 and that's where you could have                                 run facet job facet dataset facet that                                 are about metadata                                 specific to each of those elements and                                 i'm going to                                 go in the details about facet in a                                 second                                 and really the the only thing that open                                 lineage depends on                                 is having some asynchronous protocol so                                 it could be                                 http it could be a kafka queue                                 it could be something like that                                 basically you're going to start                                 send a start event when the job starts                                 and a unique run id is going to help                                 correlate the start event                                 and the end event um                                 so when the job starts you may be                                 sending a start event saying this job id                                 uh is starting a run with this run id                                 and it's reading from those data sets                                 and then you start a complete event um                                 that will tell you that the job is                                 finished                                 um and you may capture uh what was the                                 input data set                                 what was the output data set version and                                 schema the time the job ran                                 and really making observation about what                                 happened this particular job                                 and this is a version of the source code                                 that                                 was run uh possibly this particular                                 sequel                                 those were the parameters um this is the                                 query profile                                 uh that we collected when the job was                                 finished here's the schema of the input                                 at the time it                                 it ran here's the schema of the output                                 at the time it run                                 and so on and so that helps really                                 understand what the run did and                                 collect all the media data around it                                 and so this notion of asset is really a                                 mechanism                                 uh to make the model flexible                                 right one of the goals of the opening                                 each spec is to avoid having this big                                 monolithic spec                                 that takes a long time to build                                 consensus on and                                 instead by having the notion of facet                                 each of those facets                                 is its own atomic spec that focuses on                                 one specific                                 aspect of the metadata for example                                 you can have a data set facet that                                 focuses on the schema                                 and we model the schema of a data set in                                 a certain way                                 and that's one facet you may have um                                 another facet which is                                 what's the query plan for                                 the particular job you can have a facet                                 which                                 is what was the version                                 of the input uh if you use um                                 underlying storage format like delta                                 lake or iceberg                                 that actually capture the version of the                                 data set                                 and so it's really make this extensible                                 model                                 that it's really easy to focus on                                 various aspects of the metadata                                 and we may have a conversation about                                 what's a data quality metric facet                                 uh what's the schema facet what's the                                 model quality                                 metric facet and it's different for                                 machine learning                                 or a lot of those different aspects of                                 metadata we may want to capture                                 and so it's really makes it easier to                                 have focused discussion                                 and optional aspect to the data and it                                 also makes it easier to specialize                                 uh the spec for different type of                                 storage for example                                 a streaming data set like a kafka topic                                 with a very different facet                                 from a table data set like a table in a                                 warehouse                                 or a folder uh in the distributed file                                 system so that lets you                                 facilitate adding a various pieces of                                 optional metadata depending of the type                                 of underlying                                 job or data set and it has lots of                                 flexibility                                 it's also decentralized because                                 to help people experiment and be able to                                 create facets without having to go                                 through a validation                                 on a building consensus process                                 you can make your own custom facet all                                 you have to do                                 is publish adjacent schemas                                 for it and then you can add custom facet                                 to the spec and start collecting them as                                 part of the model                                 so this is really about making it really                                 easy for people to adapt                                 and experiment with open image                                 and to give you some examples of assets                                 you know at a data set level you may be                                 interested in capturing statistics                                 for data quality for example um you know                                 mean max number of nulls distribution of                                 a column for example                                 you'd be able to capture schema so that                                 you can can capture how the schema if a                                 data set changes over time                                 the verb the version if the underlying                                 format                                 is version the column level lineage of                                 that important to you                                 at the job level you may be capturing                                 what was the version of the job at the                                 time it ran                                 what were the dependencies or the                                 versions of the dependencies                                 um where is it stored in source control                                 what was the query plan                                 at the run level you may care about                                 the schedule time the batch id if that's                                 the thing                                 in their scheduling model what was the                                 query profiles or what parameters were                                 passed                                 to the particular run or hyper                                 parameters for example                                 and so that's upper lineage making                                 observation about a job that's running                                 and marquez                                 is a reference implementation that                                 consumes                                 those open lineage events and keeps                                 track of all the changes                                 so really the way you use markets                                 is this uh you have a marquez instance                                 and you may be monitoring like building                                 observability for                                 an entire data platform right so if here                                 i'm making this hypothetical example of                                 a data platform                                 so typically you have an ingest layer                                 where data comes in and you ingest data                                 and then you would have storage and                                 compute layer so typically in your                                 storage layer you have two tiers                                 one streaming you may be using kafka and                                 one                                 for data in motion and then one data at                                 rest storage where data gets                                 archived into a distributed file system                                 something like s                                  hdfs gcs you might be using iceberg                                 for making sure you have a good                                 uh underlying table abstraction for your                                 data storage                                 and then in the compute layer you may                                 use something like flink or spark                                 streaming                                 uh for streaming and the batch side                                 you may be using something like spark or                                 a warehouse like snowflake or bigquery                                 or redshift                                 and maybe probably python                                 pandas different type of transformation                                 as well and might be                                 using a scheduler like airflow to                                 schedule all those things                                 and then typically you have a bi layer                                 or you may also use your data into your                                 product                                 as through machine learning                                 recommendation engine and so on                                 and so open lineage is about collecting                                 lineage information and metadata about                                 everything that's happening                                 in this platform and marquez                                 is a central way to store it and keep                                 track of what has changed                                 so you'd see that the market law reuses                                 some of those same notion                                 but where opel lineage is making                                 observation and modeling the job that                                 he's writing is                                 it's telling you well this job is                                 running and it's reading from this data                                 set and running to this data set                                 markers is really about keeping track of                                 how that changes over time                                 right so every time the job runs                                 and writes your data set it's going to                                 create a new version of the metadata in                                 the engine                                 because this job has run and it's                                 creating a new version of the data set                                 and we're going to keep track of the                                 metadata so if                                 this particular run of a job changes the                                 schema of its output                                 we're going to capture it but then the                                 run will also have like the facet                                 information about performance like                                 the query profiles for example and the                                 job will keep track of what was the                                 current version of the job                                 uh what's the current git cha for                                 example and so on so you keep track of                                 everything that has changed                                 as things are running                                 and so open lineage is about collecting                                 lineage information markets keep track                                 of all the changes                                 and what we do at dedication to kind of                                 situate all those things                                 is just build on top of marquez to                                 enable                                 understanding of operational                                 dependencies                                 impact analysis troubleshooting and like                                 really building this lineage analysis                                 and enabling troubleshooting what has                                 changed since the last time it worked                                 where is the bottleneck why is my                                 pipeline slow why                                 is the data showing up late and it's all                                 leveraging                                 all the data collected is through                                 opening engine markers                                 and so now i'm going to go a little bit                                 over now we talked about the problem                                 and why we built open lineage and where                                 it's coming from                                 i'll talk a bit about what you can do in                                 practice                                 with open lineage                                 and marquez using this metadata                                 and so today as of integration that are                                 covered                                 with open lineage you can integrate with                                 apache airflow                                 as part of that you will have                                 integration with warehouses like                                 snowflake bigquery                                 redshift um and                                 uh we also have integration with apache                                 spark                                 and so all those things are covered out                                 of the box and so if you're using those                                 tools you will really be able to                                 see lineage um and understanding how                                 things depend on each other and how                                 things change                                 over time what's currently in beta and                                 uh being added is the great expectation                                 and dbt coverage so dbt                                 is his newer framework um                                 for building sql transformations                                 um that's becoming very popular and so                                 there's a currently um beta                                 integration with dbt in progress                                 and great expectation is a popular                                 library                                 to build data quality right you can                                 define                                 assertions build expectations on your                                 data                                 and validate them and at the same time                                 it also collects                                 um statistics and so you can keep track                                 with opening asian markets                                 of the evaluation of those statistics                                 over time                                 so it's kind of like the current                                 ecosystem                                 and of course there's a lot of                                 integration that are being planned or                                 people want to                                 contribute things like prefect                                 um other data quality libraries like dq                                 um other processing framework like flink                                 or looking into streaming as well and                                 dealing with kafka and so on                                 and so that's kind of to give you an                                 overview of uh                                 what already works with the project at                                 the current time                                 so when using with airflow um the way                                 this work is you just                                 install the marquez airflow                                 uh python package and so to add                                 um marquez as a plugin so to speak to                                 your                                 airflow integration and you can just                                 configure it to point to your marquez                                 instance and start collecting                                 uh open lineage event                                 when you use the spark integration                                 there's a similar                                 mechanism when you can use the spark                                 driver extra java option                                 to add the markers integration and point                                 it to                                 um the end point where you're going to                                 be able to                                 call it clean it                                 and so metadata being collected uh                                 across the board so of course lineage                                 for each job what are the inputs and                                 outputs                                 um what was the data volume in the input                                 and outputs and that's very important to                                 start                                 building you know the first step of data                                 quality and keeping track of                                 how much volume was produced and if                                 there's any suspicious change                                 and so that works across things like                                 spark and bigquery for example                                 it keeps track of the logical plan uh                                 for example                                 you can keep track of what the plan was                                 for bigquery what the plan was for                                 um spark and see how that changes over                                 time                                 if that impacts your performance um                                 it keeps track of the schema to see the                                 schema the inputs and output has changed                                 in what particular run of a job                                 has changed the schema of the output how                                 long it took to process and so on                                 and so when we look at the model there's                                 also                                 often a notion of nested jobs                                 right if we think about airflow airflow                                 will have a notion of dag which is the                                 top level job in airflow and then                                 individual tasks                                 inside your airflow dag are executed                                 as independent run instance                                 and then when you run a spark job for                                 example you may have a python                                 task in airflow that spawns a jvm                                 process which is your spark job                                 and this process will itself run                                 multiple actions right like                                 every time you write to something in                                 your spark job it's it starts a                                 different job                                 so in the open lineage model each of                                 those actions                                 is collected independently and we keep                                 track of this nesting notion                                 through uh this notion of parent job                                 and keeping track how this nesting works                                 and so to give you an example you know                                 since we                                 thanks to this consistent naming of                                 jobs and data set we can stitch back the                                 lineage so you may have multiple                                 workflows or multiple spark jobs that do                                 multiple transformation                                 and that helps teaching these things                                 back together and build this lineage                                 graph                                 and so some examples and because we keep                                 track of                                 you know i talk about how we're keeping                                 track of the road count in the output                                 you can start keeping track of the                                 evolution of the size of the data set                                 over time right and for each of those                                 updates it's actually connected to the                                 lineage graph                                 so you know exactly uh if the number of                                 rows drop                                 in a data set for example you know                                 exactly what particular run                                 of the job produce this drop and so that                                 will help you                                 actually investigate and keep track of                                 that one level lineage                                 to know exactly where this problem is                                 coming from and you can                                 easily correlate this to the upstream                                 dependencies                                 that what data set you depending on                                 is there a correlated drop in number of                                 rows                                 in an upstream data set for this                                 particular version                                 and um so that's you know the rock on by                                 count is what is collected                                 um across the board through the regular                                 integration and if you use something                                 like great expectation                                 on top of that you can start also                                 collecting                                 column level metrics and start keeping                                 track                                 uh for each individual method so not                                 just like having a course growing data                                 quality matrix like number of rows                                 but also adding uh                                 information like the distribution of a                                 column a number of nulls                                 and so on and keep track of those                                 changes                                 over time                                 and so that's for the information we                                 collect so of course you can join                                 the conversation open lineage and                                 marquez are part                                 of the lfi and data foundation                                 uh which is uh you know a sister                                 foundation of the cncf they're both                                 under the umbrella of the linux                                 foundation                                 um which really enforces that                                 those projects have a clear governance                                 they're not owned by anybody and it's                                 really kind of                                 a community driven effort and it's                                 really                                 owned by the community for the community                                 so you can check out those projects on                                 github                                 um you can join the slack if you have                                 questions if you're curious about                                 learning more                                 they also of course have a twitter                                 account                                 um and so we are looking forward                                 um to see you                                 join the conversation                                 and on this um this is it for my                                 presentation today                                 okay so i am just checking within a half                                 question so far so maybe                                 i will start with one question um i'm                                 really excited to see that                                 somehow it's like in the last decade we                                 have been trying to solve                                 uh how to do that uh in a massive scale                                 but                                 not resolving how to do data proper data                                 engineering like the top level                                 of this and and finally it's nice to see                                 that this is starting to happen so this                                 is pretty cool                                 uh my question is about because since                                 this is observability                                 what sort of synergies you see with the                                 other observability projects from the                                 cncf site                                 because some are similar like well                                 in the same space let's say                                 oh um yes so i think                                 clearly to me the only way we can solve                                 this is through                                 open source and having this kind of                                 project that helps defining a standard                                 because otherwise there's so much                                 complexity in all those projects and so                                 much fragmentation                                 you really want to have uh develop this                                 standard and really enable                                 everybody to expose the lineage in a                                 standard way                                 so which is what we're doing and really                                 yes there's lots                                 like in many ways the service space                                 is a lot more mature like service                                 observability                                 is a lot more mature than data pipelines                                 observability                                 like you were mentioning right we've                                 been focusing on this big data aspect                                 and people were caring about                                 how do we scale processing how do we                                 consume produce                                 so me so much data but the other problem                                 is not just the the volume of data                                 aspect but it's like                                 how many data sets we have like the                                 volume of many tables many jobs many                                 transformations                                 and many people being involved in it                                 right so it's kind of like                                 the big data problem is also a size of                                 the organization of like the lack of                                 visibility                                 a number of data set problem which is                                 what observability                                 um helps solving and so right that's why                                 opening it's really his name it's really                                 taking                                 like top open telemetry as a template                                 right                                 um it's like open telemetry standardizes                                 collecting traces and these traces are                                 really                                 traces of services is really the lineage                                 of data                                 and so eventually you know you ever                                 even would uh connect both right when                                 you                                 um collect your lineage if you're caring                                 about the sql level lineage                                 and then you go up to the data that as                                 it comes in it may be coming from a                                 kafka topic                                 and a disk topic some service is writing                                 to it because you're instrumenting them                                 and the service is writing to this kafka                                 topic because it's receiving a request                                 from another service                                 so really the lineage goes all the way                                 upstream in services as well and that's                                 where you start                                 you could start connecting open                                 telemetry traces                                 with the open lineage uh lineage events                                 and start like understanding lineage                                 from a click on the website                                 all the way to your tableau dashboard or                                 you know whether you use tableau or look                                 here or                                 all of those things right so it's really                                 the eventual                                 goal of open lineage is to                                 really be able to cover this whole                                 picture and connect everything                                 uh together and understand from an                                 observability standpoint every                                 everything that's happening and where                                 it's coming from and why it's happening                                 okay                                 thank you i'm just going to check that's                                 another question maybe                                 yes there is more i'm going to ask first                                 one who has three votes                                 any challenges specific to streaming how                                  jobs would be identified by                                  for instance yeah so you know in                                  streaming                                  i think the the the intuition is like a                                  streaming job                                  is a constantly running job right versus                                  a bad job that runs like every hour or                                  something                                  but actually a streaming job still has                                  discrete                                  runs right so you can identify a                                  streaming job and the high level lineage                                  still works the same like as a streaming                                  job will have a                                  unique name that identifies it and                                  kafka topics can be identified you know                                  you have a topic in a broker so you can                                  still                                  build those data sets out of them so of                                  course the metadata is different                                  then when you start your streaming job                                  you'll have a start event                                  that says like oh this particular one of                                  the streaming job is starting                                  are you interested in capturing what's                                  the version of the source code                                  um words started reading in the topic                                  you may be                                  an on the kafka level for example you'd                                  be interested in capturing the offsets                                  of where you started reading                                  and then at some point in the future                                  you're going to stop this job                                  and maybe upgrade dependencies or like                                  update                                  and redeploy it and so it's going to                                  stop and you                                  are going to redeploy a new version of                                  the code and start it again and it's                                  going to resume where it stops reading                                  from                                  the kafka topic so typically when you                                  map it to open lineage you would think                                  about                                  having a start event and an end event                                  and capturing where you stop trading in                                  a kafka topic like by capturing the                                  offsets for example                                  from an observability standpoint uh and                                  then we start again                                  with a new version and possibly it might                                  be changing the schema                                  in its output if you updated your logic                                  and you're writing different information                                  so depending of what type of data                                  modeling you use                                  you may be using avro you might be using                                  protobuf you might be using a schema                                  registry                                  you know this is all sort of metadata                                  you would want to capture                                  at the time of the job restarted is                                  starting writing this new schema                                  right and make those observations and                                  starting setting up a lineage event                                  and opening each event don't have to be                                  just the start at the end                                  so you can send more event along the way                                  for capturing information about oh the                                  schema has changed at this particular                                  point in time                                  and keeping track of those things um                                  as part of the metadata so um                                  opening it started more on the batch                                  side but definitely it's in the scope of                                  capturing streaming                                  and those are some like thoughts uh we                                  had                                  about where streaming modeling could be                                  going to be part of that                                  you
YouTube URL: https://www.youtube.com/watch?v=HEJFCQLwdtk


