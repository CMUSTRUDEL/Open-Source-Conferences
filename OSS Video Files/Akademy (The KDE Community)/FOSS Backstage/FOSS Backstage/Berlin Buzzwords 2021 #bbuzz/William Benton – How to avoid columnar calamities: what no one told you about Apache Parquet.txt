Title: William Benton – How to avoid columnar calamities: what no one told you about Apache Parquet
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	If you're dealing with structured data at scale, it's a safe bet that you're depending on Apache Parquet in at least a few parts of your pipeline. Parquet is a sensible default choice for storing structured data at rest because of two major advantages:  its efficiency and its ubiquity.  While Parquet's storage efficiency enables dramatically improved time and space performance for query jobs, its ubiquity may be even more valuable.  Since Parquet readers and writers are available in a wide range of languages and ecosystems, the Parquet format can support a range of applications across the data lifecycle, including data engineering and ETL jobs, query engines, and machine learning pipelines.

However, the ubiquity of Parquet readers and writers hides some complexity:  if you don't take care, some of the advantages of Parquet can be lost in translation as you move tables from Hadoop, Flink, or Spark jobs to Python machine learning code.  This talk will help you understand Parquet more fully in order to use it more effectively, with an eye towards the special challenges that might arise in polyglot environments.  We'll level-set with a quick overview of how Parquet works and why it's so efficient.  We'll then dive in to the type, encoding, and compression options available and discuss when each is most appropriate.  You'll learn how to interrogate and understand Parquet metadata, and you'll learn about some of the challenges you'll run into when sharing data between JVM-based data engineering pipelines and Python-based machine learning pipelines.  You'll leave this talk with a better understanding of Parquet and a roadmap pointing you away from some interoperability and performance pitfalls.

Speaker:
William Benton – https://2021.berlinbuzzwords.de/member/william-benton

More: https://2021.berlinbuzzwords.de/session/how-avoid-columnar-calamities-what-no-one-told-you-about-apache-parquet
Captions: 
	                              here                               buzzwords is one of my favorite                               conferences                               and today i'm going to talk about one of                               my favorite topics with computing myths                               and it's actually one of my favorite                               kinds of computing myths                               and that people can do can they                               recognize                                breakdown and how they're false so                                here's a little bit about me                                um i'm currently working on data science                                product strategy at nvidia                                in strolls i've done data science and                                engineering management and emerging                                technology                                in sort of the programming languages                                distributed computing                                and space so i came to                                machine learning data sound that's gonna                                sort of                                influence the background that i provide                                in this talk                                as well so um that's that's sort of the                                high-level view of who's talking to you                                i should mention that while i work for                                nvidia i don't speak for nvidia these                                are my own opinions in this talk um                                but i want to start with a question and                                um                                anyone can provide their                                own responses but i want to think about                                it                                and that's some of them about computing                                that we still believe                                and to give you an idea of the kind of                                thing that i'm uh                                i'm thinking about here uh one of these                                one of these such myths is write once                                run anywhere                                for java right it's mostly true it's not                                entirely true                                but it's true enough to be useful                                right and once you understand why it's                                true and also where it breaks down                                you can really get a lot of the benefits                                out of the java ecosystem                                so think about the myths that you've                                learned were myths and that you still                                believe anyway                                and we'll talk about a couple of these                                in this talk so here's what the rest of                                the talk is going to look like                                we're going to start with some                                background sort of explaining                                how computer systems work and                                talking about talking about why                                things have to work the way they do to                                be efficient and will sort of motivate                                column number formats in general in that                                prologue                                then we'll talk about why you might want                                to use apache parquet                                in particular uh we'll look                                at how parquet gets                                good performance uh both in terms of                                space and speed                                and then we'll see some of the limits of                                one of the myths                                that we'll look at about part k which                                are some of the potential                                uh performance and interoperability                                problems you might run into                                using parquet in a polyglot environment                                and then finally um you know with it                                with a story we want a good denouement                                we want a happy ending                                in this talk we have a demo instead so                                we'll look at a demo of sort of how to                                identify these problems and work around                                them                                so i want to start by talking about why                                commoner formats in                                general and the title of this talk does                                include the phrase what no one told you                                about apache parquet i have to apologize                                because i'm going to start                                by level setting with some things that                                people may have told you already                                about parquet about columnar formats                                more generally                                and about some background about computer                                systems to illustrate why these formats                                make sense                                but first i have some more questions if                                we were all in the same room                                this would be easier but this is                                sort of something to think about for                                those of you who have python background                                 i'm going to look at this code code                                 example                                 we're going to see                                 two ways to change                                 every element in an array                                 the first one is to loop through                                 explicitly                                 and multiply each element by                                 loop through each element by                                 index and then                                 multiply each one by four                                 the second way is to use a vectorized                                 operation                                 in python and multiply the entire array                                 by four at once                                 uh those of you who are python                                 programmers any thoughts about which of                                 these is going to be faster                                 so it's nice when it works out this way                                 but the shorter code is also faster in                                 this case because since we're not doing                                 explicit looping                                 we can operate on essentially every                                 element                                 in the vector in one library call                                 and we get a lot of benefits and we can                                 take advantage of parallelism and                                 hardware and all sorts of other                                 advantages as well so this is a question                                 about which is faster this is one for                                 people with a python background maybe                                 maybe the data scientists in the                                 audience                                 i have another one for the data                                 engineers in the audience                                 which is if we're looking at an analytic                                 database                                 which one of these kinds of queries is                                 going to be more common                                 and we have a table with three floats                                 and a string                                 in it and the first option is that we're                                 going to do some                                 aggregates on one of the float columns                                 and we're going to group by                                 the value in the string column                                 the second option is that we're going to                                 combine                                 every value in a row in some way                                 into a single value or that we're going                                 to use every value in a row                                 so i mean you might say neither of these                                 is more common these these both sort of                                 look suspicious but the question is are                                 we more likely to operate on every value                                 in a few columns in an analytic context                                 or are we more likely to operate on                                 every value in a row and                                 the answer is really going to be it                                 depends but i think in a lot of analytic                                 contexts you're much more likely to                                 operate on every value in a column                                 than you are to need to worry about                                 every column in an individual                                 row so                                 with that context of sort of why we                                 why we want to worry about doing                                 operations a collection at a time                                 and that we think that maybe operating                                 on                                 columns at a time might be faster than                                 operating                                 dive a little deeper and see why                                 this is faster on computer systems to                                 operate this way and why                                 we can benefit from these things being                                 more common on computer systems                                 so to do this we're going to look at                                 the way memory is organized in a                                 computer and we have                                 in any computer we have a memory                                 hierarchy where we have very small                                 but very fast memory at the top of the                                 hierarchy and we have very large but                                 very slow memory                                 at the bottom of the hierarchy and what                                 this looks like is this                                 and this is sort of adapted from a                                 contemporary                                 real processor but the values are sort                                 of turned into ranges                                 in in some cases so that it's a little                                 more general                                 but these are these are recent numbers                                 from within the last few years                                 so at the top of the memory hierarchy                                 your processor has a bunch of registers                                 these are individual locations that hold                                 individual values and you have hundreds                                 of these per core so you have a few                                 kilobytes of registers                                 per core and you can access                                 a register in a single cycle                                 so if you have a three gigahertz                                 processor this is a third of a                                 nanosecond if you have a four gigahertz                                 processor this is a quarter of a                                 nanosecond this is extremely fast this                                 is as fast as you can do anything                                 some registers take more than one cycle                                 to access                                 again we're sort of dealing at a high                                 level here                                 the next level of the memory hierarchy                                 is level one cache                                 um where we have tens of kilobytes you                                 know                                                             um we have instructions uh so we program                                 and data in separate caches                                 and these caches are organized in what                                 we call lines                                 so this means that the cache is                                 addressed                                                    now this is not as fast as the registers                                 but we have more of it                                 so it takes four or five cycles so again                                 between one                                 and you know one and two thirds one and                                 two nanoseconds to access                                 a value in the l                                                        want to call out in particular about the                                 l                                        is this aspect of it being organized in                                 lines and what this means is that you                                 can't just                                 load a single value into a cache if we                                 want to load a value from memory and                                 have it in our cache so we can put it in                                 a register or operate it on it quickly                                 we're actually going to load not just                                 that value                                 but the                                                                 value                                 right we have to load at that level of                                 granularity but again this is very fast                                 and this is why it can be very fast                                 because we have these restrictions on                                 how we use it and because it's small                                 so next up we have the l                                            modern processors have rather a lot of                                 this they have you know around a                                 megabyte per core                                 and you know you have a computer                                 architects can choose whether or not to                                 include the values in the l                                           not                                 in many designs they do include the                                 values in the l                                        again these are organized in lines but                                 they're                                 you know about two to three times                                 slower than the l                                        now in a modern processor we don't just                                 have one core we have several cores                                 um and these these diagrams are more or                                 less to scale at this point                                 um we have we have several cores and                                 we have a cache that's shared across all                                 of those cores and that's the l                                        so in some designs this includes the                                 values that are in                                 the smaller caches above it in the                                 memory hierarchy                                 um in many designs it doesn't but in                                 those designs it can include what's                                 called a victim cache which is that if                                 something                                 is put out of the cache it will land in                                 the l                                                                back                                 because sometimes things that expire                                 from a cache                                 may get put back in pretty quickly and                                 we can access this                                 still relatively quickly but you know                                    to                                          slower than a register or i'm sorry uh                                                                                                       cache and and                                 so so                                                                                                          registers                                 um when we look at main memory                                 the memory on our cpu looks tiny by                                 comparison                                 um in a typical workstation or a desktop                                 computer or laptop or                                 even a cell phone at this point you have                                 tens to hundreds of gigabytes of main                                 memory                                 obviously your main memory is shared                                 across cores in most conventional                                 consumer computers and you can address                                 this memory                                 a page you know in pages a page is                                 you know a unit of memory that the                                 operating system and                                 the processor cooperate to manage and                                 this is much slower right                                 accessing a value in main memory takes                                 between                                                    so the way to think about this is that                                 if you have a four gigahertz processor                                 and everything you need to do requires                                 loading a value for memory                                 you're only going to be able to use                                                                                             right if it takes                                                      main memory                                 so we really want to have as many values                                 as we operate on in the caches                                 we want to we want to exploit the fact                                 that we're loading data that are close                                 together                                 in those caches and then finally                                 memory itself is even dwarfed by the                                 kinds of disks that we have                                 and the disk can be hundreds of                                 gigabytes or terabytes                                 um but the uh the disks are actually                                 um the disks are actually measured in                                 milliseconds rather than in nanoseconds                                 so there's a typo on this slide it's not                                                                                                        so this is                                 this is a million times slower than                                 than memory at this point or then than                                 than caches so                                 we really need to be careful about                                 accessing the disk if we're going to                                 access the disk                                 all the time um you know we're really                                 not going to be getting                                 the best possible performance we can get                                 from our computer system so we need to                                 access the disk in a way that's                                 effective                                 now what is effective well you might                                 think well a disk                                 i mean i have files on my disk i can                                 access wherever i want i can seek to                                 some point on the disk and read and i                                 can seek to some other point                                 but that's not the most efficient way to                                 do it and                                 an interesting quote from almost two                                 decades ago from                                 database pioneer jim gray in an                                 interview                                 um i think still holds true today                                 and you know this is this is again                                      context but                                 gray was looking forward to a future                                 where we might have                                                   in commodity hardware and he says well                                 if you have                                                              disk and if you only read                                 a few kilobytes every time you hit the                                 disk it'll take you a year                                 to read all the data on that                                             disk but if you                                 go to if you go to sequential access and                                 you read more of the disk at once and                                 you read the disk in order                                 you can actually read through that disc                                 in a day rather than a year                                 so it's it's really uh it's really a                                 remarkable                                 advantage you get from treating the disc                                 like a sequential access device like we                                 like we were talking about with the                                 caches where you're reading contiguous                                 blocks of memory you want to read                                 contiguous                                 blocks of your disk gray's takeaway here                                 was that programmers have to think of                                 the disk as a sequential device                                 rather than as a random access device                                 and you might say well we have we have                                 ssds we have nvme now                                 this point actually still holds uh maybe                                 to a lesser extent than it does with                                 spinning disks but it still holds that                                 you're going to get the best performance                                 by accessing things that are in order                                 and close together                                 this is sort of a fundamental principle                                 of computer systems it's easy to predict                                 what's going to happen next                                 it's easy to do the right thing with                                 what what happens so                                 how does this apply to data processing                                 well                                 let's look at our example an example                                 data set that we'll use for the rest of                                 the talk and our case study here is for                                 a hyper local payment service                                 for places that are within walking                                 distance of alexander plots                                 so if we look at this data we have time                                 stamps we have user ids                                 we have transaction amounts and we have                                 the neighborhood that the tran                                 transaction took place in and                                 if we think about these logically as a                                 table we might want to picture them like                                 this                                 now if we had these in a row oriented                                 format where we're going to pack these                                 on the disk a row at a time where values                                 in rows                                 are close to one another values in rows                                 have that sort of locality                                 then it might look like this so                                 in this row oriented representation our                                 data are packed in together pretty                                 nicely                                 um this probably doesn't take up as much                                 space as it would to sort of                                 have a have a more human-friendly                                 representation                                 but let's see how this representation                                 works for running                                 an analytic query of the sort that we                                 want to do in a data processing system                                 so this is just a very simple one this                                 is how much money has each                                 user spent and in order to do this                                 transaction we have to scan through                                 the whole file and for every row we have                                 to get the user id                                 and the amount and then we add up those                                 user ids for each for                                 those amounts for each user                                 so we're going to see something that                                 looks like this where we're only                                 accessing                                 some subset of the data                                 now this is actually sort of worse than                                 it appears                                 because remember all of those things we                                 just talked about with the memory                                 hierarchy right                                 you're reading data sequentially so                                 you're reading a lot of data that you're                                 ultimately not going to use                                 and in fact you're reading data into                                 caches that you're not going to use                                 so you're necessarily accessing your                                 fastest most precious memory                                 in a really wasteful way so we can't                                 just read the bytes that we're                                 interested in we have to read                                 the disk sequentially and to get that                                 data from main memory into our cpu we                                 need to read cache line size chunks                                 so in this case the representation of                                 our first row                                 is                                                                  cache lines we're going to use                                 three cache lines to read five rows and                                 nearly all of that very fast memory is                                 going to be wasted                                 because we only care about the                                          i've highlighted here                                 that contain our user id and the                                 transaction amount                                 i hope this seems like we can do better                                 right and in fact we can and one of the                                 ways we can do better is to transpose                                 our data                                 so instead of storing a record for each                                 row we store a file of records                                 for each column and that will look like                                 this                                 so if we're implementing a query that                                 accesses only two of these columns                                 we don't need to read the other values                                 um                                 and we don't need to care about them                                 right we're accessing values we care                                 about we're accessing them sequentially                                 and we're doing everything basically as                                 quickly as the system will allow at                                 every level this is what computer                                 systems were designed to do                                 so if you remember nothing else about                                 columnar storage from this talk                                 remember this because analytic queries                                 are more likely to do something to every                                 value in a column                                 than to do something to everything in a                                 row column or formats can be                                 far more efficient for than row oriented                                 formats for analytic databases                                 so there are lots of other advantages to                                 columnar formats we'll talk about those                                 soon                                 but for now let's talk about the high                                 level value proposition for part k let's                                 look at the parquet myths                                 and there are two parts to this myth                                 that i want to call out                                 the first one is that par k is                                 ubiquitous if we think about a typical                                 data science discovery workflow                                 you have a lot of different stages from                                 sort of deciding whether or not                                 you even have a problem to solve to data                                 engineering and model training                                 to finally building a production system                                 that has to sort of live and evolve                                 with the data that you're seeing in the                                 real world                                 and you also have a bunch of people                                 working on these systems                                 you have data scientists and business                                 analysts working on the sort of problem                                 defining and                                 exploratory analytics part of the                                 problem you have data engineers focused                                 on that early stage of making the data                                 accessible                                 available clean and efficient                                 then you have that sort of inner loop of                                 machine learning model development that                                 a lot of people focus                                 on and then finally we have a production                                 deployment                                 and in each of these phases people are                                 going to be using different tools                                 that work well for their environment so                                 a lot of data engineering jobs                                 happen in the jvm and that sort of big                                 data hadoop ecosystem                                 a lot of data science is happening with                                 tools like python                                 r and julia and in the production                                 environment it's really the wild west                                 it's going to be a combination                                 of a lot of these things as well as some                                 specialized                                 tools that are you know maybe written in                                 c                                 plus that that are for high latency or                                 low latency                                 serving for example now the fact that                                 parquet is available in all of these                                 environments is a huge                                 selling point and that's sort of the                                 myth right that you can use parquet                                 everywhere                                 we'll see where this myth breaks down in                                 a little bit                                 another advantage is that parquet                                 creates smaller files and is thus more                                 efficient                                 so here's a recent example from my own                                 work that's pretty representative if we                                 have                                                                                                          data a little more interesting schema                                 than the one we saw in our example                                 that's about two and a half gigabytes of                                 csv                                 and under                                                           csv to part k is a totally unfair                                 comparison                                 because csv is a textual format and                                 there's way more overhead to store                                 numeric values and you know all kinds of                                 values but the amazing thing is that                                 even if we compress                                 this csv file with gzip the output is                                 still bigger than the parquet file                                 so even if we're exploiting redundancy                                 in the text of our records                                 parque is going to come out ahead and                                 that part k representation is going to                                 be directly useful for supporting                                 queries                                 whereas the gzip csv is not and those                                 smaller files will lead to faster                                 processing                                 so let's look at how parquet actually                                 accomplishes some of these things                                 again consider our tabular oriented data                                 that we've transposed into a columnar                                 format                                 so if we're doing analytic queries with                                 these data we've already gotten some                                 benefits just by separating things into                                 columns because we only have to read the                                 values we care about and they're next to                                 each other so they're spatially local to                                 one another                                 but there are more things we can do as                                 well the first thing we can do                                 is if we have repeated values in a field                                 like time stamps as                                 this is a very common case instead of                                 storing each one explicitly we can store                                 them as runs so i can say instead of                                 having this first time stamp twice i can                                 say two and then the value                                 and so on for these other examples this                                 can save us a lot of space and time                                 the second thing we can do is to store                                 low cardinality values in a dictionary                                 and replace each value with its                                 key in the dictionary these keys are                                 typically going to be a lot smaller than                                 the value they're mapping to                                 especially if we're talking about                                 strings so this can save us a lot of                                 space and here we've done this with                                 neighborhoods                                 so instead of storing each neighborhood                                 specifically                                 we keep an index of neighborhoods and we                                 just store                                 a dictionary of neighborhoods we just                                 store the index of each particular                                 transaction neighborhood                                 in the column now                                 whether we've used these encoding tricks                                 or not we can also use a general purpose                                 compression                                 algorithm to compress each column so                                 that we're saving                                 some additional space another thing                                 parquet can do to improve                                 performance is what's called predicate                                 and the idea behind predicate put                                 metadata for each of these                                 indicating some copies that are in them                                 so if                                 this if we partition file into                                 files and we if it looks like this want                                 to consider                                 subset of the time i don't even need to                                 save our data we can ignore that                                 grouping similarly if we want if we're                                 interested in a value that only appears                                 in a subset of our data like in this                                 case we only have kreitzberg                                 in the set of records on the left                                 we don't need to look at the set of                                 records on the right                                 so i said logical file and this is this                                 is a little bit of a white lie                                 in some cases we can actually have                                 multiple logical files in a single                                 physical file and part k calls these row                                 groups                                 means the columns for a subset of rows                                 in a data set as well as the metadata                                 for each so if we had a single parquet                                 file                                 we might have two row groups in that                                 file                                 and this doesn't change anything about                                 the way predicate pushdown works because                                 we can                                 treat these as logical files the query                                 engine can seek to a particular part of                                 the file                                 and read the values that we're                                 interested in                                 sequentially another wrinkle is if we're                                 dealing with part k files generated on a                                 cluster from a spark or hadoop job                                 in this case we actually will have a                                 directory full of parquet files that                                 we can treat transparently as a single                                 parquet data set                                 and each one of these is going to                                 correspond to row groups generated from                                 a partition                                 of our original data set                                 so we can inspect the metadata of a data                                 set stored in parquet with the parquet                                 tools command line utility and this is a                                 command that provides                                 several sub-commands to examine metadata                                 encodings compression and actual data in                                 a parquet file or directory                                 it'll provide a ton of output we're                                 going to look at a little bit at a time                                 so here we can see that we're dealing                                 with a per k file that has multiple                                 parts                                 that was generated with apache spark and                                 that spark stored some metadata about                                 the schema                                 the next part of this file shows us                                 parquet's schema for this file                                 and as we read on in the output we have                                 the metadata for each row group                                 this has a lot of useful detail in it                                 and it can tell us about how a query                                 engine can handle our data efficiently                                 by examining that metadata                                 first up is the row count and the total                                 size of values in the row group                                 and then for each field we have the data                                 type                                 compression type and compression ratio                                 and we can see in this case that the                                 time stamping coding                                 saved us over                                                         relative to the wrong values                                 we also see the column encodings here so                                 we see that we have dictionary encodings                                 for                                 a lot of these                                 and finally we have some metadata about                                 the kinds of values that we can take                                 that we have in each column                                 so these things put together can really                                 provide us some                                 some benefits as as query processing                                 engines so let's see where the myths                                 break down though                                 and we really want to focus on this                                 handoff                                 between data engineers and data                                 scientists                                 where parque's ubiquity breaks down and                                 if we think about taking from a jvm                                 based                                 data engineering pipeline to a python                                 based feature engineering pipeline we                                 might be thinking about going to pandas                                 so the first problem you run into is                                 availability so                                 pandas has an api method to let you read                                 a parquet file                                 but you'll have to install some other                                 libraries to use it                                 now pandas gives you two options in this                                 error message                                 fast parque and paero uh there are                                 trade-offs between each                                 in practice i've found piero has been                                 the best for my projects and that's what                                 i'll focus on in the rest of the talk                                 another problem is the capabilities of                                 your implementations so                                 this is the compression types that                                 parque supports                                 um some of these will get you really                                 great results if you're in an                                 environment that supports them                                 but snappy and gzip are going to be the                                 ones that are most widely available and                                 those are a good safe bet                                 until you know about what the                                 environment you're running in                                 is going to support another problem is                                 that if we're reading                                 a parquet file into something like a                                 pandas data frame                                 these dictionary encoded strings can get                                 materialized on read                                 so instead of this nice compact                                 dictionary encoded representation                                 which we might want to use as a                                 categorical in pandas                                 we have this long list of strings right                                 this is a problem that we'll see how to                                 work around                                 in the demo the last problem                                 is related to parquet tools itself                                 parquet tools is super useful                                 but it's been deprecated upstream and                                 removed from the parquet repo so if you                                 look for it this is what you'll get                                 as a workaround you can pull an older                                 version of parquet tools from maven or                                 install it via the homebrew tool                                 and there's also an aero-based version                                 of parquet tools that runs in python                                 under development                                 so i want to quickly go through and see                                 some of these problems in                                 action with our demo                                 and what we're going to see                                 is loading a data frame from spark                                 and then seeing how pandas                                 inappropriately materializes                                 these um                                 these dictionary encoded fields and how                                 we can work around that                                 so here we have a                                 parquet file um in spark                                 and we're gonna look at the schema here                                 and then look at the first                                             see if it makes sense                                 so we have our amount we have our                                 neighborhood our timestamp our user id                                 we have everything we expect to see                                 there                                 now if we look at the first                                         again this is looking about like we                                 expect we have a few more neighborhoods                                 in this data set                                 um and but the data the data look                                 look pretty sensible so now we're going                                 to go down                                 and read we're going to look at the                                 parquet                                 metadata again with parquet tools and                                 we're going to go ahead and read that                                 into                                 pandas                                 as you can see we have the size the                                 value count                                 the sort of field metadata and the                                 encodings                                 all that we expect to see there                                 now when we actually read these into                                 pandas though                                 um so we see crucially we note that our                                 neighborhood is dictionary encoded                                 right and when we when we read these                                 into                                 when we read these into pandas we're                                 going to see an issue                                 we notice that the neighborhood actually                                 takes up less space than the value count                                 because uh                                 because of the dictionary encoding                                 so that's uh that's a real advantage                                 there and when we have the metadata for                                 what values are in                                 in that column                                 we'll see that the time stamp again is                                 is compressed because of the run length                                 encoding                                 okay so if we see what happens when we                                 try and read these into pandas                                 we get a pandas data frame like we would                                 expect                                 with the read parquet method i have pi                                 arrow installed here                                 that looks looks pretty good but                                 these strings have all been materialized                                 so instead of having a nice                                 pandas categorical which we could use                                 more or less directly for exploratory                                 analysis or to train a model                                 we are actually storing these as python                                 objects so they're even taking up more                                 space than they would as strings                                 in this case so we thought we've had a                                 problem with the round trip here                                 so we could actually convert that column                                 to categoricals                                 which we're going to do here and we see                                 that if we if we save the categorical                                 valued column from pandas                                 we actually can recover that type                                 information                                 that we're interested in but this sort                                 of defeats the purpose of having an                                 interchange format right if you say well                                 i'm going to hand this file over to a                                 data science team and they're going to                                 have to rewrite it to make use of it                                 efficiently                                 so if we look at this this round trip                                 here we're going to get the types we                                 expect we see that this is a category                                 instead of an object so                                 we can use pi arrow the pyro api                                 directly to sort of have more control                                 over how we read                                 this in                                 and as we can see we can we can read a                                 table                                 with pi arrow                                 and if we look at that table we have um                                 a string rather than a python object                                 which is a step in the right direction                                 and the                                 the arrow is a columnar representation                                 so we know that that's going to be                                 dictionary encoded as well                                 but if we convert this to pandas again                                 we've gone back to an object                                 so what we want to do instead                                 is                                 that we want to read some columns and                                 preserve that dictionary metadata so                                 i'll show you what this looks like here                                 and we're just going to specify a list                                 of columns that we're going to read as a                                 dictionary                                 so now if we look at that table                                 we see that it's a dictionary if we                                 convert it to pandas                                 we're going to maintain that categorical                                 type                                 we're running a little short on time so                                 i have some more code                                 that's available from a blog post that                                 you can see on how to                                 inspect parquet files but we'll skip                                 that part of the demo and we'll just go                                 ahead to our conclusions                                 so thanks everyone for your time um i                                 hope that                                 hope that hope that you're                                 thinking more about some computing myths                                 that have uh                                 that you've discovered are false and                                 that                                 that you realize are useful anyway and i                                 think that parque                                 um is not perfectly ubiquitous and it                                 doesn't always get you perfect                                 performance but if you're careful about                                 how it works you can get really                                 excellent results                                 so here's what we talked about today                                 we first talked about the organization                                 of computer systems we talked about                                 these principles of locality                                 we talked about how these things fit                                 together                                 and how to get the best performance out                                 of computer systems by reading things                                 sequentially                                 operating on values that are close                                 together                                 and                                 we saw how a row oriented format can                                 have really bad performance                                 for analytic queries and for caches                                 and i hope everyone is noticing that i'm                                 actually choosing a column-oriented                                 format for these                                 summary slides uh                                 the next thing we looked at was how                                 parquet                                 can get good performance by techniques                                 like column encoding                                 run length encoding and dictionary                                 encoding and then predicate push down                                 to only consider the parts of files that                                 a query is actually interested in and                                 then finally we looked at some                                 interoperability                                 challenges um that uh                                 you might run into taking part k from                                 an environment like the jbm                                 to an environment like the python data                                 ecosystem for example                                 and we saw some solutions by using the                                 arrow apis                                 directly to sort of work around some of                                 those challenges                                 so um again i didn't i cut the demo a                                 little bit short                                 but the the full uh interactive notebook                                 version of that demo is available                                 from a link on my blog                                 chatbo.freevariable.com                                 if you search that site for parquet                                 you'll get a link to the                                 github repo with that notebook and                                 there's a buzzwords branch                                 on that repo that has the berlin                                 payments data set                                 um on it so please keep in touch                                 uh twitter and github are are great ways                                 to reach me i'm at will be                                 on both and you can send me an email at                                 will                                 will be nvidia.com and i think                                 uh since we have a break now i think                                 uh the moderators are telling me we have                                 time for a couple of questions                                 so i'd i'd appreciate any questions you                                 have folks                                 thanks again for your time thank you for                                 the great talk was definitely a great                                 introduction on different pieces of this                                 tech as well                                  and we do have a few questions while we                                  have like a break after this one                                  so first question is basically what is                                  your opinion on                                  orc versus parque there's like another                                  format                                  that's that's a good question so i mean                                  from my perspective                                  um there there are advantages to both                                  there's                                  uh the question is orc versus parquet                                  and                                  i know um i know some people have had                                  great success with orc                                  i've i've used parkade because of its                                  ubiquity                                  um you know really in in more                                  applications um                                  and i think i think parquet has sort of                                  a little bit better coverage                                  in the ecosystem but you know certainly                                  there are                                  there are performance advantages to orc                                  in some applications and it's always                                  worth measuring and checking                                  yeah sounds good and maybe uh another                                  question from my side                                  now we're talking about synchronously                                  right so you mentioned you know like cpu                                  and caches and everything else                                  i've seen some companies also um                                  advocating and saying like hey                                  your data computations is going to be                                  like on gpu right and we're not talking                                  about like machine learning kind of                                  computations but like a normal                                  search and indexes and sums right what                                  do you think about that is it's                                  something that makes it faster                                  or overhead of gpu is actually like                                  taking it away                                  so i mean this is a great this is a                                  great question like i mentioned that i                                  work on                                  i work on apache spark and um you know                                  some of what i work on at nvidia is                                  is actually the product strategy for                                  apache spark on gpus                                  i mean again i'm not speaking for my                                  employer um i think that                                  columnar formats really open up                                  accelerated computing to a lot of these                                  applications right and it's it's                                  i mean it's a case where gpus can                                  accelerate what we think of as                                  traditional database work                                  and i mean it's one of these things                                  where in a lot of cases the devil is in                                  the details right you have to be careful                                  about how you use things but                                  if you have data that are in columnar                                  formats if you have queries that operate                                  a column at a time and if you can do                                  enough work                                  i mean there is some cost in getting                                  data from main memory to gpu memory and                                  back if you can do enough work                                  over a task to amortize that out you can                                  get good results so                                  um you know i've i've seen in in                                  realistic                                  uh you know spark workloads uh                                  acceleration of                                                           jobs                                  right with spark on gpus so                                  you
YouTube URL: https://www.youtube.com/watch?v=kgBlZYH2Yp8


