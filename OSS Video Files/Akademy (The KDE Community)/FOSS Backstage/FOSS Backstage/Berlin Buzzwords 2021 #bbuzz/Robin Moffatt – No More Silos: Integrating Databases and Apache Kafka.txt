Title: Robin Moffatt – No More Silos: Integrating Databases and Apache Kafka
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Companies new and old are all recognising the importance of a low-latency, scalable, fault-tolerant data backbone, in the form of the Apache Kafka streaming platform. With Kafka, developers can integrate multiple sources and systems, which enables low latency analytics, event-driven architectures and the population of multiple downstream systems.

In this talk, we’ll look at one of the most common integration requirements - connecting databases to Kafka. We’ll consider the concept that all data is a stream of events, including that residing within a database. We’ll look at why we’d want to stream data from a database, including driving applications in Kafka from events upstream. We’ll discuss the different methods for connecting databases to Kafka, and the pros and cons of each. Techniques including Change-Data-Capture (CDC) and Kafka Connect will be covered, as well as an exploration of the power of ksqlDB for performing transformations such as joins on the inbound data.

Attendees of this talk will learn:
- why events, not just state, matter
- the difference between log-based CDC and query-based CDC
- how to chose which CDC approach to use

Speaker:
Robin Moffatt – https://2021.berlinbuzzwords.de/member/robin-moffatt

More: https://2021.berlinbuzzwords.de/session/no-more-silos-integrating-databases-and-apache-kafka-1
Captions: 
	                              my name is robert muffett i work at                               confluence one of the companies behind                               the                               open source apache kafka project and i                               wish i was in berlin today it's a                               glorious sunshine in yorkshire but i'm                               sure it's the                               glorious skies like this in berlin too                               and but we're stuck at home at the                                moment                                but today i'd like to talk about getting                                data                                from a database into kafka and i'm going                                to talk about                                why we want to do that i'm going to talk                                about how we go about doing that because                                like it's important to understand                                both sides of this so let's start off by                                thinking about like why would you even                                want to get data                                from your database into kafka because                                like perhaps that data is quite happy in                                the database perhaps it doesn't want to                                be                                in kafka but it turns out that getting                                data from your databases                                into kafka powers a bunch of really                                useful things that we can do with it                                so sometimes people want to say well                                i've got data set in my database and i                                want to build                                analytics against it i'm querying that                                database directly perhaps it's kind of                                like                                overhead and performance all those kind                                of things why we don't want to do it on                                the database                                so you say well let's offload it from                                that database and we'll put it somewhere                                else                                and nowadays that's probably going to be                                a cloud data warehouse or a cloud object                                store or somewhere like that                                but we can use kafka as the piece in the                                middle to kind of get that data and                                buffer it and                                store it and push it out to the places                                where we want to use it                                so that's one very common use case data                                sat in our database                                we want it somewhere else kafka acts as                                the perfect broker a medium platform                                for doing that data transfer like                                getting the data out                                and pushing the data somewhere else in a                                reliable and scalable way                                another thing we want to do with the                                data is not just go and shove it                                somewhere else                                but actually use that data in our stream                                processing                                to enrich other data that we've got                                flowing through our streams                                so perhaps we've got a microservice                                somewhere there's writing information or                                orders that have been taken in a service                                or whatever and that's writing onto                                a kafka topic and we want to enrich that                                information that we've got                                about the orders being placed with                                things like well who is the customer                                who placed that order we probably know                                it's customer id                                   but what's their email address or their                                shipping address or their loyalty club                                status or                                all that kind of stuff that we tend to                                keep on databases                                so by pulling in data from a database we                                can stream it                                into kafka and use stream processing to                                join it to events                                as they arrive from other places and we                                can use that to drive                                applications we can use this drive like                                real-time dashboards and things like                                that                                about the orders that are being placed                                the other thing we can do                                with getting data from a database into                                kafka                                is to actually start to evolve our                                existing systems                                towards a new way of building them so                                many                                many um systems nowadays are built as                                monoliths                                nothing wrong with monoliths but                                sometimes people decide that's not                                 actually                                 the way they want to continue with it                                 but instead of going to just like                                 ditch all of that and start afresh we                                 can say well we've got our existing                                 monolith our existing application up                                 here in the top left                                 and we say well almost certainly it's                                 gonna have a database underpinning it                                 something happens that monolith it gets                                 written to the database                                 well we can capture the events out of                                 that database                                 into kafka and so as stuff happens in                                 the database like                                 record at a time those are events which                                 we can capture into kafka                                 we can use that to drive new services                                 and applications that we're building                                 so that we can start to chip away at                                 bits of functionality                                 that have got in that existing system                                 and replace it with new bits of                                 functionality                                 but driven by the same events from the                                 same                                 database now getting the idea of getting                                 data                                 out of a database into an event                                 streaming platform like kafka                                 may sound a little bit kind of like is                                 this like squares and circles like do                                 these things actually match up                                 because kafka is about data and motion                                 and streams and events                                 and databases are about data at rest                                 like static lumps of data                                 but there's actually a very very tight                                 relationship between the two                                 let's have a little look at that if you                                 think about                                 a database table that holds account                                 balances                                 we say for account id what is the                                 current balance account id one two three                                 four five                                 the current balance is                                                  that balance get there well                                 we deposited some money into that bank                                 account so we deposited                                                                                                         now the balance                                 is                                                                     more money so we deposited                                 another                                                              changed                                 if i query the database table and say                                 what's the current balance it says                                 the current balance is                                                   i spend some money                                 and the balance changes again so what we                                 have here is the idea of a stream                                 of events and you can replay that stream                                 to build                                 the table at any point in that stream                                 you have the actual state                                 that those events roll up to and the                                 table                                 is our state so you can go from a stream                                 to a table                                 but you can also go from a table to a                                 stream every change to that table is an                                 event                                 and if you capture all of those events                                 you could replay them to build the table                                 state                                 so this is called the stream table                                 duality the duality because it goes both                                 ways so we can take tables and existing                                 databases                                 capture the changes from those to give                                 us a stream of events                                 from a stream of events we can then roll                                 through to a state                                 to a table at any time we want to so                                 that gives rise to this great quotation                                 from pat helens                                 the truth is the log the database is a                                 cache of a subset                                 of the log all you actually need like                                 the fundamental pieces that you need                                 are the events with the events you can                                 then build                                 a database table if you want to but you                                 don't have to you can use those events                                 on their own also so these are some of                                 the reasons why we want to get data                                 from a database into kafka and                                 conceptually how we can even think about                                 translating a table                                 into a stream because they're actually                                 intrinsically linked                                 but now let's think about how we're                                 going to go and do that what are the                                 different pieces that we've got                                 at our disposal to actually go and do                                 this well the simple answer is kafka                                 connect                                 and kafka connect is part of apache                                 kafka so if you're using apache kafka                                 you already have                                 kafka connect and it comes with source                                 connectors that you can use to getting                                 data                                 from systems into kafka which is what                                 we're going to talk about here there's                                 also sync connectors                                 so pushing data from kafka down to other                                 places                                 so it gives rise to this great ecosystem                                 of connectors built around                                 kafka connect and the really nice thing                                 about kafka connectors as well as kind                                 of like                                 someone else having invented that wheel                                 and perfected that wheel of doing                                 scalable                                 resilient stream stream integration                                 between kafka and other systems                                 is that you don't have to write any code                                 the people who wrote the connectors                                 wrote the code                                 as users we just write configuration we                                 declare                                 we have data in this place and we would                                 like to put it into that place                                 and get data from this table on this                                 database and put it into this kafka                                 topic                                 or if you're doing a sync from this                                 kafka topic down to this other system                                 but we're all about the source                                 connectors here get data from a database                                 into kafka now many people familiar with                                 databases will have heard of the term                                 cdc                                 change data capture and a lot of people                                 assume                                 there is just one way of doing cdc but                                 it turns out there's two                                 and the purpose of this talk is to                                 really dig into the difference between                                 these two                                 options and help you understand how you                                 can choose                                 which is the best one to use for your                                 requirements                                 so as with most things in it it depends                                 there are two different ways of doing it                                 you'll probably find most people use one                                 of them                                 but that's not to say the other one                                 isn't useful in many different cases                                 also so we have query based change dates                                 capture                                 and we have log based change data                                 capture if you're like familiar with                                 databases and cdc                                 you're probably thinking of log-based                                 change data capture but i'm going to                                 walk through now                                 what the two different types are and how                                 you can choose which one to use                                 so query based change data capture it                                 runs a query                                 against the database so it queries it                                 say what's changed                                 since i last queried so it pulls the                                 database so it says select everything                                 from this particular table                                 where there's a particular column which                                 is going to indicate what's changed                                 so it could be a timestamp it could be                                 an id but something which is going to go                                 up that we can compare against like                                 based on last time                                 has something changed so if we have a                                 database                                 and it looks like this on the left hand                                 side we've got two rows of data denoted                                 by those red                                 rectangles there we can pull the                                 database and say                                 select everything that's changed since                                 our little watermark there our little                                 last previous timestamp that we pulled                                 the database they said well                                 there are two rows they've been created                                 so we get two events                                 in our topic two messages in our plasma                                 topic                                 something changes in the database we                                 insert a new row we poll the database                                 a moment later it says well everything                                 that's changed since we last checked                                 which is now                                 this watermark here this offset here                                 says well what's changed there                                 is this particular row and we get that                                 onto our kafka topic                                 that's conceptually what query based                                 change data capture is doing                                 it's querying the database well it uses                                 jdbc it's literally                                 running a jdbc query against the query                                 using jdbc                                 against the database to work out what's                                 changed                                 log-based change data capture on the                                 other hand looks a lot more scary it's                                 like one of those scenes for like a                                 hacker films i should have done it with                                 green text on the black background i                                 made it look proper like that                                 but what this is doing is using the                                 databases transaction log                                 so the way that relational databases                                 work and implementations differ but                                 broadly speaking                                 they have what's called a transaction                                 log which is what the database                                 writes information to when you make a                                 change and that gives you                                 your ability to recover things and roll                                 back changes and replay changes                                 and like recover from failures and all                                 sorts of stuff like that but the                                 transaction log holds that information                                 so some things in the database we've got                                 two rows of data as before                                 using log-based change data capture                                 we can capture those changes into our                                 kafka topic                                 and something changes in the database                                 and that goes onto the transaction log                                 and we can capture that                                 into our kafka topic so we've got                                 two viable ways of doing the same thing                                 we want to capture changes to the data                                 so most of these ones you can end up                                 with a snapshot first like here's                                 everything in the table and then we're                                 going to capture the changes                                 of everything that happens after that                                 snapshot                                 so now we need to understand how to                                 actually choose                                 which one to use which one's going to be                                 most appropriate                                 for what we're trying to do                                 so query based change data capture we                                 need to understand a little bit more                                 about                                 how does it actually work and what are                                 some of the limitations around it                                 so query based change data capture                                 you've got to have                                 a field in your schema which is going to                                 change                                 when your record does so this could be                                 an id field                                 which goes up like an incrementing id                                 field                                 but usually you'll have a timestamp                                 field which is going to get                                 set when you insert the row and change                                 when you update it                                 so got like a create table statement                                 here some ddl                                 different languages or different                                 databases we'll use different flavors of                                 it                                 but here we're saying the timestamp                                 column is a timestamp surprise surprise                                 i'm saying by default it's got a current                                 timestamp so we insert a row and have                                 the current timestamp                                 on update when it gets updated we're                                 going to also use the current timestamp                                 so when we update that row the timestamp                                 on it will be set so i think this is the                                 mysql implementation                                 you can use triggers or whatever to                                 achieve similar things on other                                 relational databases                                 but you have to have this field in your                                 schema and                                 it'd be fairly uncommon not to have                                 something like that anyway just because                                 it makes life easy when we're building                                 applications                                 but if you don't you can't use query                                 based changes data capture                                 all you can do is like capture the whole                                 table each time                                 which might be useful once but kind of                                 like snapchatting the entire table                                 every however many seconds doesn't                                 always make a great deal of sense                                 so we have to have the ability to have                                 that in the schema already                                 or make those modifications to the                                 schema which if it's your own                                 application maybe fair enough if it's a                                 third party one or a different team in                                 the organization                                 sometimes that can be a sticking point                                 so we have an insert that gets made into                                 the database                                 we can query the database say what's                                 changed since we last checked and we                                 capture                                 that insert we've got other dml                                 operations we make an update                                 so that existing road that we captured                                 into our kafka topic                                 it gets updated and our timestamp column                                 in the database whether a trigger or our                                 application                                 it updates that timestamp column so when                                 we poll the database again and say where                                 the timestamp column                                 value is greater than when we last                                 checked and the polling interval you can                                 customize but                                 whatever the interval set to has the                                 timestamp column got a greater value                                 than when we last checked or if we're                                 using id columns as the id column value                                 gone up since the one that we checked                                 before and we can capture the updates                                 into our kafka topic like that if we                                 delete a message                                 we see okay we do a delete from the                                 table and then we query the database and                                 say tell me about the rows in the table                                 have changed                                 since we last pulled the table that says                                 well okay well                                 that row's deleted obviously we don't                                 know about it because it's not in the                                 table anymore                                 you can't query a table for data that                                 doesn't exist                                 now they kind of like smart alexa works                                 you're probably saying ah well in this                                 particular relational database you can                                 use triggers or                                 uh flashback or all these sorts of                                 different things and conceptually you                                 could                                 but fundamentally query based change                                 data capture cannots                                 unless you go and customize it to                                 whatever capture deletes                                 you can go and fork it and write your                                 own but out of the box query based                                 capture cannot capture deletes because                                 you cannot query a database                                 for data which doesn't exist so that's                                 one of the wrinkles with creator-based                                 jgj's capture                                 the other one is a little bit more                                 subtle but it's potentially the most                                 crucial point in deciding                                 which method are we going to use so                                 let's imagine we're capturing                                 information our orders so we've got an                                 order table and our application rights                                 orders and we're going to replicate that                                 table into a kafka topic so we could                                 feed an analytics system so here we call                                 the database                                 and we say well the previous timestamp                                 was at                                                                 seconds                                 it's a                                                                   the hour                                 that's                                                               seconds we run the query                                 we don't get anything which is fair                                 enough no orders have been placed                                 in that                                                               later                                 we pilot again so this is                                               minutes                                 and                                                                 capture a new row                                 so it says okay all drive                                             shipped to this address                                 this is the time stamp at which it was                                 updated so                                                                                                    before we pulled                                 this record was there that's fine we've                                 captured the current state of the table                                 and we're doing so every                                                if we're capturing that                                 into an analytic system like i say that                                 may be sufficient                                 if all that analytic system wants to                                 know about the state of orders right                                 tell me about all of the orders that                                 we've shipped and                                 whatever that's that's totally fine but                                 if we're starting to build applications                                 around the concept of                                 events or if we're doing analytics based                                 on the progress of an order through the                                 system                                 we want to know about everything that                                 happened to that order not just its                                 current state when we checked the table                                 so if you think about it the order                                 probably got created so maybe                                 it was one second after we last polled                                 so                                                                                                         so just after we polled                                 someone placed the order or like they                                 clicked on a button which sent it into a                                 pending status so the order gets created                                 it's an insert into the table there's no                                 address at this point                                 so then a few seconds later the users                                 obviously created their customer profile                                 and they put an address into it                                 so the record gets updated and then a                                 couple of seconds after that user                                 realizes oh that's the delivery address                                 i wanted like my home address                                 so they update the address on the order                                 again and then                                 it gets shipped okay so it's like super                                 quick processing it's been                                 created we've set the address a couple                                 of times and then we've shipped it                                 all within the space of                                            so when we first checked the table there                                 was no data                                                                                                          captured the state                                 but in between it turns out a whole                                 bunch of stuff                                 has happened so query based change data                                 capture                                 is easier to run because we're just                                 clearing the database                                 all you need are the credentials for the                                 table of select credentials against that                                 table                                 connection details for the database and                                 off we go we can capture the state                                 of the table at the point at which we                                 call it but we cannot capture                                 or guarantee to capture is every single                                 event                                 so what's actually happened during that                                 polling interval is four different                                 events                                 we created an order we changed the                                 address we changed the address again                                 and then we shipped the order those four                                 different events which depending on the                                 system we're building                                 like four completely different things                                 maybe four different microservices want                                 to know about that there's the                                 fraud checking one which needs to know                                 when the address gets changed and the                                 fulfillment one needs to know when it's                                 been shipped these are events and events                                 matter because events model the world                                 around us so capturing the individual                                 events                                 oftentimes is super important so this is                                 where it comes down to like oh we can                                 use query based or log-based change data                                 capture                                 if we're quite happy simply taking a                                 snapshot of the table the state of the                                 table                                 at that point in time that we've pulled                                 it that's fine                                 but if we need the events then we're                                 gonna have to use log based                                 change data capture so query based                                 change data capture                                 it's much easier to set up it needs                                 fewer permissions because we're simply                                 querying the database it's                                 not much different from logging into sql                                 plus and running a query against the                                 database but we're just doing it                                 repeatedly and we are doing it                                 repeatedly so we're actually putting a                                 load                                 on the database because we're going to                                 be calling the database frequently                                 or we call it less frequently because we                                 get a phone call from our friendly dba                                 who says like                                 what's this query that's running every                                 second against the database on a column                                 which i forgot to index                                 it's like oh yeah we just wanted to                                 change data capture against it but well                                 no you're not going to do that against                                 my database                                 says okay we'll dial it down or just                                 like pull the database every minute                                 or every                                                              down to requirements                                 if you don't need the data other than                                 every                                                    that's probably fine if you want to do                                 actual events driven processing                                 when an event happens respond to it                                    minutes isn't going to really cut the                                 mustard                                 you need something which is much more                                 instantaneous                                 we need to have access to the schema to                                 change the schema or                                 certain demands on the schema in terms                                 of having a field there like an                                 incrementing id column                                 and or a timestamp column which we can                                 use to check against for has the row of                                 change                                 and we can't track deletes so tracking                                 deletes all right something being                                 deleted                                 is also an event it's like the absence                                 of data is also something                                 we want to know about so query based                                 change data capture is fine                                 log base change data capture is kinda                                 like it's just like a more refined way                                 of dealing with the data so log base                                 change data capture                                 we have the snapshot of what's in the                                 table currently that mirrors over into                                 our kafka topic                                 we make an update an update goes into                                 the transaction log                                 so that transaction log has information                                 about what row has been updated                                 usually it captures what was the state                                 of the row before the update                                 and after so not only are we capturing                                 that here's                                 what the table currently looks like but                                 we're capturing the fact that                                 something got changed here's what got                                 changed too                                 here's what it was before so the                                 customer changed their address and their                                 address is now                                 this and they've changed it from that so                                 that you actually get really rich                                 events through from the transaction log                                 we capture those into our kafka topic                                 and we can also                                 capture deletes because the delete is an                                 event in the database it's a                                 dml uh statement it gets written to the                                 transaction log                                 so that database needs to capture that                                 into its transaction log because the                                 transaction log is what it replays                                 against the kind of the backup snapshot                                 files we need to rule forward the states                                 of the database so we have the delete in                                 the transaction log we can capture that                                 into our kafka topic also                                 if you take a look at that picture there                                 on the bottom left we've got a                                 transaction log with our kind of                                 our inserts and our updates and our                                 deletes on the right hand side                                 we've got our inserts and our updates                                 and our deletes so                                 what's kind of interesting for me in                                 this is that on the right hand side                                 we've got                                 apache kafka with this idea of like a                                 immutable                                 app end only log of events on the left                                 hand side we've got the concept of a                                 relational database for like                                 decades ago of an immutable series of                                 events you can't go into a relational                                 database and kind of like                                 hack around on the transaction log or if                                 you can you're a braver person than i                                 the databases transaction log is this                                 immutable series of events                                 apache kafka is an immutable series of                                 events it's just that it's                                 distributed and highly scalable and so                                 this gives you this interesting idea                                 that like                                 could kafka be the fundamental basis for                                 like the concept of                                 a database and there's like i know it's                                 kind of it's kind of a clickbaity idea                                 like there's articles written around it                                 is kafka database and so on                                 but conceptually we're doing the same                                 thing we're capturing events                                 in our data and then building stuff on                                 top of it it's just a relational                                 database we kind of like                                 ship the whole package and like with a                                 nice sql layer on top is the api for                                 people to interact with it                                 whereas kafka kind of gives you the                                 framework and the platform but things                                 like key sql db are being built                                 around that to give you a sql interface                                 to those events                                 anyway i kind of like digress i'm going                                 off on a bit of a tangent                                 so log base change here to capture it                                 gives us this the fidelity of the data                                 it captures every single event that                                 happens in the database                                 so we can actually guarantee we've got a                                 snapshot of the table we've quested at a                                 certain point of the snapshot                                 and then we've captured every single                                 change that's happened to that data                                 since that snapshot so we've got a full                                 replica of what's happening to our data                                 in the source system                                 in our kafka topic so we can use that to                                 drive our event-driven applications                                 so there's that great talk from gunner                                 hans peter beforehand about patterns                                 which                                 change data capture and that's the idea                                 of building applications of run on this                                 data                                 you can get out of a database it's much                                 lower latency and lower impact on the                                 source system because it's a lower level                                 api but because it's a lower level api                                 you need much greater access to the                                 system you need to                                 make friends with your dba who you                                 should be friends with anyway because                                 they're lovely people                                 and you need to get the appropriate                                 permissions to install it and                                 access to the database because it is a                                 low-level api that you're working with                                 so that gives us an idea of what query                                 based change data capture is what                                 log-based                                 change data capture is and how you                                 decide which one to use                                 if you're building event driven                                 applications you're going to almost                                 certainly want to be using                                 log-based change data capture if you                                 just want to capture the state                                 of a table and you may be taking like                                 first baby steps with kafka and                                 databases and like                                 what's the easiest like lowest friction                                 way of setting this kind of stuff                                 up then query based change aids capture                                 gives you a nice easy route into it                                 and it may well get you plenty of the                                 way along with like if a proof of                                 concept of like what we can get out what                                 can we do with this data what can we                                 build with it                                 once it's in kafka so having made that                                 decision                                 which one you're going to use let's                                 actually look at some of the tools that                                 are available                                 so query based change data capture is                                 provided by                                 the jdbc connector so you can go to                                 confluence hub and you can see all of                                 these different connectors for kafka                                 connect                                 so the gdbc connector gives you a source                                 connector there's also a sync for                                 pushing data from kafka into a database                                 but that's an entirely different talk                                 so jdbc connector lets you do uh                                 query based change data capture against                                 your source database                                 for log based change data capture                                 there's the oracle connector cdc                                 connector                                 from confluence and for all the other                                 databases and oracle                                 also there's the debesium project which                                 provides some excellent excellent                                 connectors                                 gonna enhance pizza both work on the                                 with division                                 uh lots of guns their project lead on it                                 and this gives you really really good                                 connectors for getting data out of my                                 sequel and postgres and                                 a bunch of other ones as well so those                                 are some really useful tools                                 there are tons of other connectors and                                 like third-party applications                                 for doing getting data out of databases                                 into kafka but those are like the three                                 there which i kind of like particularly                                 choose to highlight                                 hopefully that's been useful i think                                 i've got a few minutes left for q a so                                 i'll turn to that in just a moment but a                                 couple of resources for you                                 um confluent developer is where you can                                 go and learn all about                                 apache kafka there's tutorials there's                                 blogs there's podcasts there's videos                                 there's all sorts of useful stuff there                                 and then you can find                                 me online i'm on twitter i'm at i'm off                                 there's my handle there in the bottom                                 left                                 you can find the slides for this and a                                 bunch of other talks and recordings that                                 i do                                 at uh armoff.dev talks and you can also                                 find me on youtube                                 so like lockdown has been with us for a                                 while now but last year when i kind of                                 got taken off the road and couldn't go                                 to conferences in person                                 i started doing a youtube channel so                                 there's a bunch of talks on there                                 there's a lot of stuff about kafka                                 connect                                 so if the idea of kafka connect is                                 something that you like or you use this                                 already                                 go and check out that channel make sure                                 you subscribe let's get the subscriber                                 number going up and to the right                                 but there's a ton of useful content                                 there also so with that                                 thank you very much for your time and                                 i'll be delighted to take any questions                                 that we have                                 thanks robin that was a really really an                                 awesome talk um                                 um i i like the uh distinguishing                                 between                                 query based and log based cdc                                 um never thought about it that way so                                 we have um a question from the from the                                 audience                                 it's a bit longer so i'll just read it                                 so in a microservice world                                 local databases state is very much the                                 internal                                 representation for service versus the                                 api that this                                 microservice exposes for example there                                 could be some data model changes in the                                 database where the api doesn't change                                 what would be a good pattern for                                 exposing the same api data in kafka                                 so business logic won't have to be                                 replicated between services and                                 event processing oh                                 uh okay i've got the question up here so                                 just give me a second so uh                                 in the microsoft as well okay um                                 a good pattern for exposing the same api                                 data in kafka                                 so business logic won't have replicated                                 i don't actually know i'm afraid i do                                 know who would know and that's gonna                                 enhance better                                 um so that'll be a great question to                                 take to them i know they're on twitter                                 also                                 um so if you are on twitter then tag me                                 on twitter and i'll pass it on to them                                 um or just reach out to them directly um                                 i would need to have a set and a scratch                                 my head about that one which i'll i'll                                 do after this but                                 um i don't have an answer straight away                                 for that i'm sorry                                 you
YouTube URL: https://www.youtube.com/watch?v=4-MeJJt3B2Q


