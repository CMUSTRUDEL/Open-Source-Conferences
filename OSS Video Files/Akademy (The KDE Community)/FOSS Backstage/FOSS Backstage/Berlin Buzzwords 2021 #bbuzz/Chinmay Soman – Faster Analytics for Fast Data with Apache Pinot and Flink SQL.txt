Title: Chinmay Soman – Faster Analytics for Fast Data with Apache Pinot and Flink SQL
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	OLAP data stores like Apache Pinot are emerging to serve low-latency analytical queries at web scale. With its columnar data format and rich indexing strategies, Pinot is a perfect fit for running complex, interactive queries on multi-dimensional data within milliseconds. In some cases, though, streaming data will require non-trivial pre-processing that is not supported in Pinot, like joins and pre-aggregations. What then? 

In this talk, we’ll cover the benefits of combining Pinot and stream processing with Flink SQL to power near real-time OLAP use cases, and build a simple demo to analyze streaming Twitch data (#meta) — from ingestion to visualization!

Speaker:
Chinmay Soman – https://2021.berlinbuzzwords.de/member/chinmay-soman

More: https://2021.berlinbuzzwords.de/session/faster-analytics-fast-data-apache-pinot-and-flink-sql
Captions: 
	                              so                               yeah the welcome to the session where                               i'll be talking about                               how to build complex real-time                               analytical use cases                               using flink and apache pino i guess i                               can just                               skip the intro thanks to fabian for                               introducing                                uh so today i'll begin by discussing the                                use cases of                                real-time analytics and uh                                shed light on why this is fast becoming                                an important need for                                most of the modern businesses today um                                i'll give an overview of apache pinot                                and uh you know explain why it's fit for                                building                                such fast real-time analytical use cases                                i'll discuss the ingestion challenges                                that pino faces today                                which it's not able to overcome on its                                own                                and next i'll talk about apache flink                                and how                                uh it it can be used to overcome uh                                some of these complex ingestion                                challenges in pino                                and finally uh we can conclude with a                                cool demo with with                                twitch streams okay so let's get started                                when we talk about real-time analytics                                there's actually                                many different sub-categories of use                                cases                                and each one has its own unique                                requirements                                we'll go through some of this in the                                next few slides                                one of the most important category is                                user facing analytics                                where you are exposing your analytical                                capabilities directly                                to your customers or or end users so for                                example linkedin                                has this who viewed your profile                                dashboard which it                                provides to all its                                                 members                                where you can get a personalized view of                                profile views sliced across multiple                                dimensions such as time                                industry segment excuse me                                geographical location and so on                                another example is the linkedin feed                                relevance                                where in order to make sure you're not                                seeing the same thing                                again and again we want to know for a                                given                                story your content how many times has                                has a user seen this in the last                                        or so                                and then this can be done with a sql                                query something like this                                now this may seem straightforward but                                you're executing this query                                on a huge database of                                                 users                                and every time you visit linkedin for                                all active members uh this has to be                                executed which translates to                                several tens of thousands of qps on your                                underlying database                                and then each such query must execute                                very quickly                                in the order of milliseconds otherwise                                it's going to be a bad experience for                                the users                                another good example is a restaurant                                manager by ubereats                                this is a dashboard given to restaurant                                owners across the globe                                where they can see different things like                                sales metrics on a week                                week or week manner the inaccurate                                orders uh top selling items and so on                                and and you can imagine to build                                something like this you're also doing a                                lot of concurrent queries                                and again each such query must execute                                very quickly                                another important category of real-time                                analytics is business metrics                                this is where you're tracking the key                                indicators of your business                                in a real-time manner and then doing                                this in real time                                is important uh for day-to-day operation                                 and also                                 things like anomaly detection so for                                 example                                 uh page views is an important business                                 metric for uber                                 or demand and supply ratios is another                                 one for                                 uh sorry page views is an example of                                 linkedin                                 and demand and supply ratios is a                                 business metric for uber                                 and here you see an example where the                                 number of page views suddenly                                 dropped and you want to be able to                                 detect this in real time                                 more importantly you also want to know                                 why that anomaly happened                                 in other words which dimension resulted                                 in in the page views to drop and                                 detecting                                 doing the root cause analysis in real                                 time is also very important                                 and finally we have dashboards which                                 everyone pretty much knows about                                 you know this is one place where you can                                 track all your application                                 and system metrics uh and as you can                                 imagine                                 you know this can also result in a lot                                 of concurrent queries                                 and having a real-time view of this is                                 extremely important for                                 for your operational needs so all such                                 use cases                                 and many more can be built on top of                                 apache pino for those who haven't                                 haven't heard of this apache pino is an                                 open source distributed data store                                 that can ingest data from a wide variety                                 of sources such as kafka                                 s                                                                       for querying in real time                                 at the heart of pinot is is a column                                 store                                 and it features a rich set of indexes                                 and aggregation strategies that make it                                 a great fit for all such use cases                                 and it's it's quite a mature product as                                 of now it's being used in a lot of big                                 data companies around the globe                                 and has a rapidly growing community as                                 well                                 uh some of the largest pinot clusters                                 can do upwards of million plus                                 queries events per second ingestion can                                 easily do                                 hundreds of thousands of queries per                                 second while still maintaining                                 millisecond level latency                                 so this is an exact an overview of how                                 pinot fits in                                 in your overall data ecosystem and we                                 can take                                 the example of linkedin uh so every time                                 people visit linkedin.com                                 all the events generated will be emitted                                 to                                 a streaming system like kafka and all                                 the entity data                                 around users and companies can be stored                                 in something                                 in some oltp store from here                                 data is continuously being archived uh                                 and into a long retention store like                                 htfs                                 for variety of other use cases                                 as i mentioned before pinot can actually                                 ingest data                                 from all these sources so within                                 linkedin                                 we can ingest data from kafka and hdfs                                 and provide a consolidated logical view                                 to the user so we hide the complexity                                 of of the actual data sources and then                                 you can build                                 all these different use cases on top                                 uh if you look under the hood of pinot                                 let's look at                                 what are the different components so the                                 incoming data                                 from the data source is organized in a                                 column format                                 and sprayed out across the what we call                                 as a pinot server                                 and and you know you can have you can                                 add as many window servers as you want                                 and you can configure replication                                 amongst all these servers                                 there's a pinot controller which is                                 responsible for all the cluster                                 coordination                                 functions such as membership replication                                 and partitioning and so on                                 and finally we have the pinot broker                                 which can take a user query or                                 application query                                 and then do a distributed scatter gather                                 across all the servers                                 so what it does is it will identify                                 which servers are responsible                                 for serving this query and then send                                 the query directly to those servers all                                 these servers will then do                                 local processing and then return an                                 intermediate result                                 to the broker the broker will then do a                                 final aggregation and return it back to                                 the user                                 so as i mentioned what makes pino really                                 fast                                 for the real-time analytics is is the                                 all the rich indexing strategies that is                                 available out of the box                                 so for example you can configure                                 inverted sorted or range index for any                                 of the numerical columns                                 in in your schema json index                                 lets you do fast queries on on                                 semi-structured or unstructured data                                 as the name implies geo index will                                 accelerate your geospatial queries                                 and there is a special index called star                                 tree which is also how                                 our company is named which lets you                                 pre-aggregate                                 values across a range of dimensions                                 so this makes complex aggregation                                 queries really really fast and                                 one other feature i want to call out                                 here is the                                 something that we added recently in pino                                 is the ability to                                 observe data so you can actually have                                 real-time data coming through kafka                                 which has mutations                                 and be able to update your pinot table                                 in real time                                 and this is something i'll actually be                                 demoing today                                 okay so now that we know a brief theory                                 of pinot                                 let's see how it uh let's see it in                                 action                                 uh so what i have here is a local uh                                 docker                                 instances for pino kafka and zookeeper                                 oh and i forgot to mention the demo                                 so what in the demo what we'll do is uh                                 we'll                                 we'll consume the twitch stream                                 information                                 uh using using its api and emit all                                 these events into kafka                                 and then subsequently you will ingest it                                 in pinot and                                 query the data in real time                                 okay so i have this nifty                                 python script which all it does is it                                 queries twitch                                 api and then emits the events to kafka                                 so let's let's start that and if you                                 if you look at the kafka topics we                                 should see a                                 something called as twitch streams                                 and just to see how um                                 the events look like they look something                                 like this                                 so we have an id which is the which                                 uniquely identifies a twitch stream you                                 have all the                                 user information you have the game                                 information                                 and also has an event time attribute                                 which defines a point and time at which                                 this event was                                 generated okay so now that the events                                 are                                 in in kafka we can go ahead and                                 and start querying it in pino so when                                 you deploy pino it comes with a                                 convenient                                 ui to do different things like manage                                 your cluster topology                                 and also create tables so let us go                                 ahead and create a table for our twitch                                 stream                                 first things first is to add a schema                                 for our table                                 so we'll add id as as our dimension                                 we can add the game name as another                                 dimension                                 and then these are both strings we can                                 add                                 a viewer count which is a metric                                 and then the event time which is                                 currently                                 in the form of a string so we'll add it                                 as a dimension                                 and one last thing we'll do is add a                                 special column                                 called event time ms which is actually                                 not in in your input kafka stream                                 this will be a derived column and                                 and this will be designated as a time                                 column within pinot so the time column                                 is currently                                 by default how the data is partitioned                                 and in pino                                 so that's our resulting schema let's go                                 ahead and save that                                 so now we can add a real time table                                 with the same name and and within this                                 we can configure                                 how we want to generate the derived                                 column which is event time ms                                 and this is really useful when your                                 input stream does not have                                 does not have the fields in the right                                 format so what i'm going to do here                                 is using a built-in function called from                                 date time                                 and all it does is takes an input column                                 which is event time which is in string                                 and convert that                                 in in milliseconds so let's go ahead and                                 do that                                 so what this is going to do is for every                                 record ingested in p node it's going to                                 apply this transformation and generate a                                 derived column called event time ms                                 and then we will partition data on the                                 new column                                 we also want to specify the kafka topic                                 name                                 and the kafka url                                 there are other things that you can do                                 which i want to go through right now                                 like retention                                 quotas for your query and so on um                                 so let's go ahead and save that okay so                                 now                                 we're ready to uh query the the data                                 coming from twitch                                 and keep in mind this is real uh real                                 live data which is actually happening on                                 twitch right now so if i do pound star                                 you should see the total count                                 increasing                                 as you can see below okay                                 so this is cool but let's do a slightly                                 complicated query                                 which is um                                 we want to do a total count of streams                                 um grouped on the id                                 on the stream id and intuitively you                                 expect                                 this the count per                                 id to be one there should be only one                                 unique stream per id                                 but as you can see we currently have an                                 issue here                                 what you see is there's multiple events                                 happening                                 for a given id and let's take a deeper                                 look                                 why this is happening                                 okay so what you can see here is for the                                 same                                 twitch the stream id you see multiple                                 events                                 with with different event time and also                                 different viewer count                                 and and what's happening is this the                                 twitch stream is                                 constantly being updated and we are                                 injecting these often duplicate or                                 observable events in the kafka stream                                 and in at the moment we haven't                                 configured pino to handle absurds                                 um so this is currently just with pino                                 and the current kafka stream we are                                 unable to                                 to handle upsets so let me go back                                 to my presentation                                 so in order to handle upsets within pino                                 the prerequisite is the input kafka                                 stream must be partitioned on the                                 primary key                                 and in this case that's the id column                                 which was not happening                                 right now now of course you know i could                                 have done that in my in my python script                                 but oftentimes you don't control the                                 input kafka streams right                                 so you need a mechanism to do                                 re-partitioning of your data                                 even more complex scenarios is when your                                 input stream or table does not contain                                 all your data that you want to analyze                                 um and you want to do                                 either a stream stream join or stream                                 table join to to compute this                                 materialization                                 and finally you can have decoration                                 requirement where you have                                 events coming in through your data                                 source and you want to decorate it                                 using an external rpc either with                                 something sitting in an oltp store or                                 behind an api for all such                                 ingestion challenges we rely on apache                                 flink                                 um for again and hopefully you all know                                 apache flink already it's an extremely                                 popular                                 uh stream processing framework which                                 lets you                                 perform computational tasks on bounded                                 and unbounded streams of data                                 it comes with a wide variety of input                                 and output connectors                                 and features a rich api and also                                 includes things like state management                                 which makes it a great fit for                                 building different applications such as                                 event driven applications                                 uh streaming etl and analytics and so on                                 uh of course flink is a quite a mature                                 product and it's used                                 in a lot of companies around the globe                                 especially what i want to focus is the                                 alibaba's numbers from                                      this is quite a while ago the decent                                 numbers are probably much higher                                 but it was able to do                                                    per second                                 at peak which is really really                                 impressive                                 for this particular talk i want to focus                                 on                                 one important aspect of link which is                                 the flink sql                                 and as the name implies it lets you                                 express your computational logic                                 using a declarative way something                                 something like this                                 this is based on the apache calcite                                 grammar which is                                 very similar to ansi sql but also adds                                 some advanced things like window                                 semantics which is required for                                 continuous queries                                 and as you can see here link sql is                                 actually built on top of the existing                                 primitives and in fact given a fling sql                                 query would be                                 translated into the underlying api and                                 executed                                 as a as regular flink job                                 uh and again you know you can execute it                                 on both                                 on unbounded and bounded streams so when                                 you're running it again something like                                 kafka                                 it runs as a continuous query so it                                 keeps generating                                 output continuously and as opposed to                                 something like                                 a standard s                                                          executed as a traditional sql query                                 so this is just a very high level                                 overview of link i                                 i highly recommend the talk from mata                                 pes and                                 other people from vervetica for for                                 flink and flings equal                                 excuse me                                 okay so what we'll do now is to                                 i'll show how we can solve some of the                                 ingestion challenges we saw in pino                                 using flink so we'll go back to our                                 twitch api                                 and we'll continue generating the the                                 real the twitch stream information into                                 kafka                                 but what we're also doing here is to                                 pre-fetch the tags information                                 and store it as a json file and an s                                  at this point we'll be using flink to do                                 a join between                                 this kafka topic and then this and this                                 s                                                                        and and emit the information uh back to                                 kafka                                 the other thing the fling jaw will be                                 doing is repartitioning                                 this this data on the primary key that                                 we need for pnom sorts so it'll be                                 partitioning on the id column and                                 finally                                 i'll show how this can be ingested into                                 pino                                 and we'll do a cool visualization using                                 superset                                 so let me switch back to my demo                                 environment                                 so first thing i'll do is start a fling                                 sql client                                 uh tool which is a very convenient way                                 of                                 submitting your flink queries and                                 starting the actual flink job                                 okay so first thing we want to do is                                 create a table                                 to read from the kafka topic for                                 uh which has the real time to extreme                                 information                                 let's go ahead and do that so it                                 contains all the dimensions                                 from the twitch stream api and also                                 defines where the data is coming from                                 which is our kafka local kafka cluster                                 next we'll create a table to consume                                 data from the                                 um the json file stored in s                                  let's do that it has only two dimensions                                 the tag id and description                                 and as you can imagine we'll be joining                                 on the tag id                                 column and again here we are showing                                 okay                                 the connector is file system will be                                 reading from from s                                  and finally we'll create a table which                                 is a result of the join operation                                 of these two things so it has                                 the dimensions from both the kafka and                                 tags                                 file and we want to emit it back to                                 kafka                                 hence we're using the kafka connector                                 the other thing                                 if you notice here we're defining we're                                 specifying                                 the key field as id so what we want to                                 do is partition the data                                 on the id column in other words                                 all records with the same twitch stream                                 id                                 will end up in the same kafka partition                                 and will enable pino                                 to do upserts okay                                 so now we're ready to actually execute                                 our join query                                 which looks something like this                                 and and again you know it looks pretty                                 identical to a regular                                 ansi sql query we select                                 we project all the dimensions from from                                 the two tables                                 and then define the join criteria which                                 is the tag id                                 and i'm doing a simple inner join here                                 but flink has a lot of advanced ways of                                 doing defining windows for your join                                 function                                 okay so at this point the join was                                 executed                                 we have a job running which is                                 continuously joining data from kafka and                                 s                                  and emitting events to kafka                                 so if i look at                                 my kafka topics i should see a new topic                                 pop                                 up here which is twitch streams with                                 tags                                 and this is the topic which includes the                                 join as well as the                                 repartitioning thanks to flink at this                                 point                                 what i can do is to save time i've                                 already created a pinot schema                                 which has all the dimensions we want and                                 i'm also specifying a primary key here                                 which is the id column similarly i also                                 have                                 an absurd table which looks similar to                                 the table                                 that we created before let's go and add                                 this                                 to pino using the convenient rest api                                 so now we are ready to query our upsell                                 table                                 so as you can see it has the                                 all the dimensions as a result of the                                 join uh                                 from from kafka and s                                                    our group by query and and see what the                                 result looks like now                                 as i mentioned before um                                 this the new pinot table we we have                                 partitioned the date on id column and                                 enabled observed within pino                                 so now the result of the group by                                 oh sorry i'm still using the old table                                 pardon me as you can see now                                 the group by is indeed one and pino is                                 actually successfully                                 uh either doing deduplicating data or                                 handling up search correctly                                 from the input stream uh so in this                                 manner                                 you know we we saw how you know flink                                 can easily do join and repartitioning in                                 a matter of minutes and then this was                                 all uh real                                 data from twitch just to re-emphasize uh                                 at this point what we can do                                 is uh use superset to visualize                                 uh the information so let's go                                 and add the new table that we created in                                 pino which is                                 the stream subsert                                 uh one thing if you haven't used                                 superset before i need to                                 let superset know which is your time                                 column and in our                                 case that's event time milliseconds                                 that's our temporal column                                 and we also want to tell superset the                                 format                                 which is epochs in millisecond                                 okay so now we can start visualizing                                 this data coming from twitch so i'll                                 pick a line chart                                 bucket it by every second and let's say                                 we                                 want to see everything from now to minus                                 seven days                                 okay so you can see the current demo                                 that we ran                                 and something i was testing in the                                 morning                                 and then the queries are returning                                 obviously very fast because what                                 superset is doing is sending it to pino                                 and then querying the twitch stream data                                 in real time                                 we can also do a little bit more complex                                 things like figure out what are the most                                 popular streams                                 happening right now so let's do a group                                 by                                 on on the game name                                 uh and i also you can again see this is                                 really fast                                 because of pino and flink and then this                                 is the current                                 um popular streams happening on twitch                                 as of now                                 so overall uh what we saw let me switch                                 back                                 um just to reiterate what we just saw we                                 have we had a real stream information                                 being emitted                                 into kafka and then tags information                                 going                                 to s                                                                uh repartitioned data also using the                                 same flinksql query uh ingested                                 into pino and pino was able to do handle                                 the absorbs correctly                                 and at this point and you can use                                 anything something like superset to                                 visualize all your data                                 um so i can conclude stop here and then                                 take any questions but                                 overall fling sql is a really powerful                                 construct                                 which lets you do complex things in a in                                 a very very fast manner as you saw right                                 now                                 and is being used at a massive scale and                                 alibaba and                                 other companies apache p is also                                 we saw the distributed and scale out                                 design uh and and the rich indexing                                 support                                 that it features and it's also being                                 used in a lot of companies around the                                 world uh                                 before i stop i do want to acknowledge                                 uh martha pace who was taking a bow here                                 as she should                                 um so she she helped me a lot with the                                 initial demo                                 and answering the fling questions that i                                 had                                 so thank you martha uh at this point i                                 i can stop here and take any questions                                 that you guys have                                 thanks a lot i guess we're waiting for                                 fabian to be on the stage                                 uh while while that is um                                 while fabian is coming back um since i'm                                 unable to see                                 see the questions i can talk a little                                 bit more                                 on uh you know the the pinot                                 architecture and and i mentioned the                                 scale out                                 design um so we i'll quickly                                 talk a little bit more on that while we                                 wait um                                 so as i mentioned uh you know the data                                 is laid out                                 in a column format across across all                                 these servers                                 so this forms this makes it very easy to                                 expand capacity                                 uh on the pinot side anytime you are                                 facing a bottleneck we can just add more                                 servers                                 the controller will automatically                                 get the the new identify the new servers                                 and start putting segments uh pinot                                 segments on onto these new servers                                 similarly you can add brokers uh at any                                 point                                 uh and and this is how we can keep                                 scaling out                                 the pinot cluster at will                                 okay i can again stop here and take any                                 questions                                 uh if there any                                 yeah thanks for this awesome talk uh                                 sorry for the uh for the technical                                 problems                                 no problem um yeah that was really uh                                 really awesome awesome demo i have a                                 question so                                 sure um have you have you thought about                                 uh or do you think it would make sense                                 to um                                 integrate uh flink with uh pinot a                                 little bit                                 tighter yeah similar as you uh as you                                 did with the                                 um like um leveraging uh presto for the                                 uh for for the joint capability would                                 that be an option to like                                 somehow um fuse the systems together                                 yeah great question yeah so this this is                                 a common ask um from from many folks                                 where                                 you want to basically skip an                                 intermediate stage between flink and                                 pino right currently                                 uh as in the demo also i mentioned we                                 have to emit the events to kafka and                                 then                                 in just into pino so currently there is                                 one way                                 that that we are working on right now                                 which is a segment writer api that is                                 available                                 uh for for flink jobs to directly use                                 uh and and produce to pino uh the                                 downside                                 is uh so let me maybe step back into how                                 pinot actually ingests the data                                 um so when we are fetching data from                                 real time                                 the records are ingested one at a time                                 and                                 and and they are being converted into a                                 column format                                 for the corresponding segments um                                 but when you in in the offline world                                 we create the segments outside of pino                                 and then copy it                                 into pinot so that's essentially what we                                 do with the current integration between                                 flink and pino which is                                 use the segment writer api to generate a                                 local segment                                 and then push the local segment to                                 peanut                                 um so the trade-off here is you know the                                 the freshness of your data depends on                                 how big your segment is right so if you                                 keep producing so you can keep                                 appending to your local segment within                                 your fling job                                 uh and let's say you do that for                                    minutes uh                                 so so the data will be available for                                 querying                                                  so it's more like a micro batch more                                 today                                 so so that that's the current mechanism                                 right so you can you can                                 create segments within within flink and                                 push to pino and actually uber                                 is playing around with that um as we                                 speak                                 the other one that we want to get to is                                 a write api                                 in in pino so be able to write one                                 record at a time                                 um directly into pino and this is                                 something that we're still working on                                 and once that's available then fling can                                 directly start writing                                 into pino and make it available in real                                 time                                 yeah awesome thanks um                                 so in the meantime we also got a few                                 questions from the audience um                                 the first one is uh how is apache pino                                 different from google bigquery or                                 aws athena got it um                                 i don't really have any slide for that                                 but i can talk about it um                                 so when you compare uh so pinot                                 um the the emphasis or the                                 pino is really optimized for um                                 accelerating the real-time uh analytics                                 right so                                 the the focus is on reducing the                                 ingestion latency                                 of the data coming in so to basically                                 make the                                 data available to query within                                 milliseconds from when it is generated                                 from the source uh and also the query                                 latency is also                                 uh the focus is to uh keep it on the                                 millisecond                                 range uh and if you look at bigquery                                 the it's optimized for a different set                                 of problems right it's optimized more                                 for more complex                                 sql sql queries where the ingestion                                 latency                                 is may or may not be that important it's                                 okay to have the data coming in minutes                                 later                                 or hours later and the focus is on                                 executing more warehouse style                                 uh complex equal queries uh and again                                 you know the throughput and latency that                                 that bigquery can do uh uh                                 you know to do something like                                         qps can get prohibitively expensive                                 on bigquery whereas pino is designed for                                 handling                                 you know massive qps on for olap                                 cubes cube style queries um with                                 amazon athena i think that's more of um                                 it's it's more like uh i guess you can                                 compare that to                                 flink uh more so than than pino here                                 um it's it's uh pino is a data store so                                 you can put your data in and query it                                 whenever you want so you can have                                 seven months you know one one year in                                 with an uber we have                                 something that has uh one uh almost two                                 years worth of data                                 in pino and some use cases that you can                                 query um                                 and then it's it's a traditional data                                 store right so it's a full semantics                                 whereas amazon athena has more on the                                 push semantic side                                 yeah thanks for the uh yeah so there's                                 one more question                                 um so how rich are the um                                 curing capabilities compared to a                                 regular sql yeah yeah great question so                                 uh we so pino again is optimized for                                 olap                                 queries so it can speed up you know                                 aggregation functions grouped by an                                 order by and all that                                 um but what we don't do effectively                                 is uh anyway joints for example right we                                 the focus is not to support complex                                 joints within pino                                 um so for that we as fabian already                                 mentioned we                                 integrated closely with presto to do                                 all those complex things in in the                                 presto layer                                 and pino can handle the filtering                                 aggregation uh                                 some of the basic window functions uh so                                 that that would be                                 that would be one like joints is one                                 example which is not supported today                                 we do have lookup joints within pino um                                 so what we what we do is                                 let's say you have a small dimension                                 table and you want to decorate                                 your larger fact tables in pino with                                 this small dimension table                                 that is supported today so you can                                 have do a lookup join locally within                                 each pinot server                                 so but having you know large fact fact                                 join i think that that's not supported                                 today                                 okay thank you um one more there's even                                 two more questions um                                 one is a larger one partitioning on the                                 upset key                                 seems like a strong constraint what's                                 the advantage compared to                                 writing every event and then using an                                 analytical function on it                                 like and over partition by order by                                 uh additional key order by event time                                 and then last function on that                                 right yeah so it really comes down to um                                 query latency right uh what what we want                                 uh to do is minimize the work                                 that can that needs to happen at query                                 time um as i mentioned                                 pino is used for a lot of like user                                 facing analytics                                 um and and it's embedded in the core                                 business flow within linkedin and uber                                 so any you know any query latency delays                                 will actually affect                                 the overall site latency for linkedin                                 and uber uh so it's                                 it's imperative that the latency sla is                                 within                                                                                                        of thought we wanted to                                 minimize what we do at query time                                 so we came up with this model where we                                 assume that the data is partitioned on                                 the primary key beforehand                                 um and then within a server we use a                                 simple bitmap                                 uh to keep track of like we are                                 co-locating all the observable events                                 together                                 which enables pino to do handle upsets                                 um you know the on the question of you                                 know                                 pre-partitioning is expensive uh yes                                 yeah                                 there is an additional cost to it but                                 oftentimes                                 you can it's it's a matter of selecting                                 a key within your kafka producer                                 and that's all if you looked at my demo                                 all i did was select a key                                 for my kafka producer and that was it                                 and this can be done with                                 your existing applications or even if                                 you're                                 you know ingesting change log from                                 division you can select a key                                 there and so on                                 so uh how is pinot different from apache                                 droid                                 yeah yeah great question this is pino                                 and druid                                 architecturally very similar both um you                                 know ingest data in the same way                                 they both column store the differences i                                 mentioned in my previous slide                                 uh i can just read it out since we don't                                 have much time                                 uh the one of the main differences is                                 the the different set of indexes                                 that that we already have and we keep                                 adding uh                                 pino's pluggable architecture makes it                                 very easy to add                                 new indexes in a very easy manner so                                 currently you know range index json                                 index                                 geospatial index star tree index this is                                 not available in druid                                  and this is what makes pinot really fast                                  tech search                                  is not available in druid um uh that the                                  leucine index that                                  that we've added in pino being able to                                  observe data                                  that's actually architectural difference                                  that that's there in pino and not into                                  it                                  you
YouTube URL: https://www.youtube.com/watch?v=0byVuWrwOhw


