Title: Frank Conrad – Kubernetes and the dynamic world in the cloud
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Kubernetes get the container running platform to use with really easy add or remove resources. For real 12 factor app this is strait forward and the good way to go.

The challenge comes if you have long running (stateful) apps or big data apps like hadoop, spark, flink,... 

This talk show strategies to meditate this challenges and take advantages out of it. 

For example instead of run small cluster 24h, run a 12 times bigger cluster only for short intervals(~2h). If your jobs are scalable you get you results up to 12 times faster and bigger business value for same resource consumption. Or run a live recording, you need stay until event ends even it is a 24h. Run an AI training which don’t usually work so well with snapshots for recovery.

- use cloud in dynamic way (scale 100x of capacity in minutes is possible)
- per job cluster with the fitting sizing 
- leverage multiple node pools/groups 
- help from k8s operators for deployment 
- how this can work together with workflows like airflow
- k8s cluster auto scaler, how to leverage him and pitfalls
- k8s scheduler, alternatives , options to consider 

Speaker:
Frank Conrad – https://2021.berlinbuzzwords.de/member/frank-conrad

More: https://2021.berlinbuzzwords.de/session/kubernetes-and-dynamic-world-cloud
Captions: 
	                              thank you for                               joining my talk yeah it's                               it's about kubernetes and the dynamic                               world in the cloud                               and i will talk a lot a lot about some                               special things about that me personally                               i co-founded                                                                                                      years                                in the online advertising space and                                science three years i'm working                                in the kubernetes world and run their                                workloads                                at scale in in kubernetes in a dynamic                                way                                my current i'm currently working at                                cisco in the webex ai                                team and my patient has really large                                scale                                low latency processing for worldwide                                distributed systems                                what is the challenge what we typically                                have this is a                                business need dynamic scale even though                                they don't say it because they want to                                have the sla always                                in time and that you achieve it even on                                black friday and special                                events on the other side they want to                                have the operational                                costs under control and                                likely only this pay what they're really                                right now using                                and they want to have a reliable system                                what can deliver new features fast                                and as always time is money to get                                things                                fast out of the door the                                the operational challenges which coming                                to us as a in engineers and operations                                guys to that is basically                                that the the resource requirements of                                jobs are very different from job to drop                                and                                even on the each day or week or month                                or quarterly they can be have very                                different                                resource profiles and also                                if we have problems and needs to catch                                up or do some reprocessing because of                                new features                                or bug fixes                                we need to want to do this                                as fast as possible and with fixed size                                clusters which we have traditionally                                you um yeah you have to schedule the                                uh to schedule optimizations                                a lot the jobs influencing and watching                                each other                                and on uninterrupted jobs you also have                                some problems                                and what we                                ideally wants to have is                                that each job gets the right resources                                what it needs that's independent as                                as possible and have less influence                                against                                each other that you can can change                                existing jobs and you draw                                at new jobs with minimal effects to the                                runtime                                that you don't care so much about idle                                time or of your servers                                because they would go away that you only                                pay what you                                need and                                for that to get this you need a system                                where you can add and remove resources                                in real time                                means scale up and scale down                                and then you only pay what you are                                currently running                                and what effects can can we have such as                                a dynamic scaling system which                                can grow and shrink it's                                it's basically if you say you have a                                fixed budget and you run a job normally                                bisana tpu's a full day                                all the time the class of uh                                then you can also say i run an eight                                times bigger cluster only four hours and                                it cost you the same                                in the cloud and you get your results                                eight eight times earlier and                                if you say okay i i only need uh                                two uh two hours                                                    spend the rest also                                as                                            running some small stuff there or or                                 do some ui responsive things to that                                 and you still under the same operational                                 costs                                 and on the other view to it is you have                                 a static system                                 where you only need basically the full                                 capacity for eight hours                                 and the rest of the day you only need                                 for uh                                    of the cpus and if you do this in a                                 dynamic way you would save                                                                                  and if you then also think about oh i                                 need to add something for peak events                                 like back friday or other things if this                                 is                                    more what you want to need to add you                                 save                                                                                               you even save                                              it's a simple way to save                                 to save costs                                 and what what basically is a cloud give                                 us                                 that you really only pay what you are                                 using                                 you can add and remove resources and                                 this is typically also with a minimum                                 no or minimal lead times we are not                                 talking about minutes normally                                 you which allows you to change cpu                                 memory                                 temp space ssds on on a on a frequent                                 way                                 to do this is normally a problem that                                 you have to do it very provider specific                                 and yeah this                                 is sometimes a challenge                                 so the most real cost savings out of the                                 cloud you get really                                 if you leverage the cloud in a dynamic                                 way means scaling up and down                                 and and in terms if you run starting                                 systems they are most likely more                                 expensive than on-prems over time                                 if you are ignoring capex and opex                                 effects                                 but there's nothing nothing for free if                                 you run a dynamic system you need to                                 monitor your cost because you pay us                                 what you use                                 and if your system                                 uh runs under high load in a traditional                                 system you would get                                 alerts that the system get overloaded                                 your it might                                 even break your sla in a dynamic world                                 you would get the resources you still                                 need your sla                                 but you would get the cost alert because                                 you're spending more                                 than you have planned for                                 and in this way what uh                                 if you use kubernetes in the cloud what                                 it will                                 bring you kubernetes looks like a                                 another cluster but we already had in                                 the past the spark or hadoop clusters                                 of link clusters                                 but it has much better cloud support                                 things                                 it can really do this dynamic up and                                 down scaling via cluster autoscaler                                 it's in this way really dynamic                                 and the other nice thing is you can also                                 run your rest of your workloads which is                                 not big data                                 on on kubernetes clusters and                                 makes them also in a very dynamic way                                 so you get basically one deployment                                 system                                 for for everything and                                 the other nice thing is that basically                                 kubernetes covers you                                 a lot of the provider dependent scale up                                 and down                                 mechanisms because they implemented this                                 one time and you simply only use it                                 and you have on your side only                                 kubernetes dependent deployments                                 which makes you in a way also provide a                                 independent and this                                 with this you go from from the vm world                                 or app                                 per vm world to the container world                                 there are some uh all the cloud                                 providers                                 giving you some hosted kubernetes                                 clusters                                 this is an aws eks in asia aks                                 and google gke which is most                                 sophisticated and                                 what kubernetes also introduce the                                 operator                                 pattern to make it simpler to run                                 complex system                                 like cassandra kafka in a                                 kubernetes environment and this operator                                 pattern it basically takes operations                                 and                                 sae knowledge of running complex systems                                 and put it into code                                 so there are operators available for                                 kafka for databases and also for                                 monitoring stuff                                 and there they get controlled via custom                                 resource definitions which are which are                                 objects in                                 inside the kubernetes system and you                                 find them on                                 operator hub or awesome operator and a                                 couple of other sources google's helps                                 google helps a lot and i get mostly                                 installed via                                 via helm which is the standard packet                                 management for for kubernetes and                                 tools like hand file car customize                                 simplify your the usage                                 of that and making also the                                 configuration more dry                                 what are the big data benefits what we                                 see                                 here is yeah for your scalable                                 jobs you can produce far faster results                                 because                                 you're putting more resources for a                                 short time in it and you get                                 the results earlier you have a chance                                 that this data size or load                                 the compute can grow dynamically you                                 only pay                                 what you really allocate at that time                                 you don't                                 so much need to pre-allocate things for                                 stuff what you only need in a rod uh for                                 a short time                                 like black friday and uh                                 also if you have to recover uh from                                 failed jobs                                 you can add resources to make those                                 faster                                 things to give you some                                 some numbers what it means to run                                 a kubernetes cluster autoscaler                                 how fast you get something so if here an                                 example is a gke cluster                                                                                                 and they are all scale via cluster auto                                 scaler                                 if a part triggers in need of a new node                                 it takes between                                                   to get support pot running on this new                                 node                                 if you try to start                                                  triggers                                 basically thousand new nodes it takes                                 four minutes to get them running                                 if you need to start eighteen thousand                                 pots                                 which large images which also trigger a                                 thousand nodes it takes approximately                                    minutes to start them and so overhead of                                 a node is typically                                 odor                                                                  percent of the memory                                 but you have an as an overhead per node                                 what what is the cluster autoscaler                                 really doing                                 he is responsible to add and remove                                 nodes to a cluster and for scale scale                                 up it basically looks for unscheduleable                                 pots                                 runs the simulation to find the right                                 node pool                                 multiple are groups of nodes which have                                 the same profile                                 and do the right decision                                 by uh via a weight in a similar way as a                                 as the scheduler is doing and on gke it                                 also                                 includes the price of the nodes and                                 the node with the highest weight or node                                 pool with the highest weld                                 will be used to add the new node                                 for scaling down it looks for the                                 underutilized                                 nodes and see if they can                                 be deleted and to always                                 get the resources what you need you you                                 should set the limits high enough for                                 the specific node pools                                 what is always the most more complicated                                 than scale up is scale down                                 you want to do a graceful shutdown so                                 means                                 you don't want to interrupt your stuff                                 if if notes                                 once have to go down because they are                                 underutilized                                 for that the cluster autoscaler is                                 looking                                 to that the node is getting                                 underutilized                                 it's then also checking are there pots                                 with local storage                                 or have no controller which means they                                 would not automatically get restarted                                 have a special annotation uh                                 cluster auto seller kubernetes safe to                                 evict false                                 and and also looking are there resources                                 somewhere else where the pot can run                                 and only if this all fulfills                                 in a will shut down so the                                 safest way to avoid that your pot get                                 uh get evicted from                                 from a node is to set this annotation                                 and even on the                                 scale scale down if this has happened as                                 the pot disruption budget gets respected                                 as well as the                                 graceful termination up to                                            which which are both mechanisms                                 for normal up applications to make sure                                 that you do a really graceful shutdown                                 what this means for for our big data                                 jobs                                 big data jobs are they are slightly                                 different they are mostly really state                                 stateful and the the                                 failure recovery mechanisms are designed                                 for                                 that a few nodes get some problems and                                 get restarted or                                 things things happened but if if                                 basically                                 a bigger amount of uh of the                                 uh pots get affected                                 there's they're really starting                                 struggling on it and especially                                 if they would do a rolling update it                                 would be probable                                 probably for your spark jobs the worst                                 thing worse thing                                 to do and to                                 uh to avoid this happened as i mentioned                                 before                                 set this annotation and then a node will                                 run                                 until your your ports are done                                 the kubernetes scheduler which is a guy                                 who's responsible to run                                 to decide which node a port should run                                 on it                                 and it's basically looking for all nodes                                 and to the heart filtering of the                                 available resources and all the other                                 criterias                                 and if it figure find no node                                 where the port can run on it                                 markets as unschedulable which triggers                                 then the cluster autoscaler to find                                 or generate a new node otherwise                                 it basically calculates                                 a priority on on which node                                 a pod should run and uh                                 by as a default behavior uh                                 it tries to do a very dist well                                 distributed                                 load across your current clusters and                                 um the uh                                 this is this means if you run a lot of                                 low                                 notes which have low low load                                 they have get all evenly load on that                                 this is done pot by pot                                 which can introduce a lot of latency if                                 you                                 start a high number uh of pots                                 and as i mentioned the number before a                                 part of of this high high number on the                                 high number of parts coming out of the                                 scheduler latency                                 there are some some ways where you can                                 influence us by priority classes                                 but at the end um yeah it's in                                 pot-by-pot scheduling but which is a                                 challenge for us for the big data jobs                                 where you                                 basically want mostly wants to have                                 something                                 all or nothing start all my pots of a                                 job                                 or or none of them if                                 if their only party gets started your                                 job will pro likely                                 not good never finish and especially                                 if you run multiple jobs at the same                                 time which get affected by that you can                                 even reach into the                                 deadlock situation the solutions to that                                 is                                 one to use the cluster autoscaler and                                 always the needed resources to that                                 so that you always can run your pots or                                 use another                                 scheduler which addresses this problem                                 which is basically a game schedule                                 this uh to use other schedulers is                                 basically the solution on                                 very large clusters or if you have                                 limited resources                                 and why is very large also because                                 very large means you hit limits and at                                 some point you can't schedule any more                                 so it's it's the same use case as                                 limited resources                                 and for that you need something like a                                 gangster scheduler                                 potentially also this priority cues                                 something what you                                 know from your old yarn clusters for                                 example                                 with with kubernetes you can have                                 multiple schedulers                                 running and there's also some scheduler                                 plugins                                 available for customizations to use that                                 has some pros and cons and                                 the things what you need to care of is                                 each pot must                                 to have have a scheduler assignment and                                 much more important                                 a node pool or a node should be only                                 managed by one scheduler                                 so you have to make sure that the                                 default scheduler or the other scalia                                 are not managing                                 load load on on the same nodes otherwise                                 it's planned for a disaster                                 and there are sometimes some challenges                                 if you use                                 provided a uh based kubernetes clusters                                 on on that um                                 there are there are some schedulers                                 available the                                 most common use is the q patch scheduler                                 there's base based on that much more                                 well integrated things                                 is a vulcan carno one                                 then is also apache unicorn                                 which is also a gang scheduler and                                 target really large                                 large scale and fast scheduling and                                 in the kubernetes scheduling framework                                 there are                                 a couple of scheduler plugins available                                 to do customizations                                 and to get the scheduling                                 influenced in the way you want                                 the alternative way to use the cluster                                 autoscaler is my                                 my my preferred way because you always                                 get                                 at the capacity what you need                                 and with that uh we said you don't have                                 all the challenges                                 with the uh to run                                 a second scheduler in your cluster                                 and this works very well with uh with                                 all the cloud environments                                 because you get the resources                                 you want in in your limits in your quota                                 where you have to look to                                 and you have to set the                                 max nodes per node pool                                 high enough that you always have                                 capacity available and have to monitor                                 on                                 for sure your quota and your cost                                 there's a challenges especially for                                 scale down if you                                 run more than one pot per node                                 why is that that has to do with the cube                                 scheduler                                 behavior of the weld distribution what i                                 come                                 come to it so the preferred way is                                 really that you run                                 one part per node because this makes                                 also scale down and so on efficient and                                 with that                                 you really only                                 i have the running resources what you                                 need at the time                                 the challenge if you are running                                 multiple pods is basically                                 you have to wait until the last port is                                 gone                                 uh on the uh                                 from the node until they can go down                                 means especially if you run multiple                                 jobs                                 at the same time the longest running job                                 is dictating this and also if your                                 pods ending and you are starting on the                                 already running notes                                 you run into the problem that they                                 evenly get distributed by the                                 by the scheduler which also                                 holds a lot no it's longer than it's                                 really needed                                 you can optimize that by getting strong                                 being                                 packing on your kubernetes clusters you                                 can set                                 the auto scaling profile to optimize                                 your utilization in gke for example                                 which solves this problem                                 then then the next thing to optimize the                                 things is really to use                                 dedicated node pools means the node pool                                 which only runs certain workloads                                 what you want to it that is                                 sounds a little bit strange because and                                 in many how to's you                                 you you read you should not do it you                                 should                                 let kubernetes figure out the stuff my                                 experience is                                 it's better better in the dynamic world                                 it's better to use                                 dedicated notebooks for a lot more load                                 to make sure that they are also scaling                                 down in a way                                 you are um you are thinking and you are                                 also much more controlled the scheduling                                 by that                                 and creating node pools basically                                 you add your own labels                                 to it to which are there to bind                                 a pot to this particular node pool                                 and only one label helps you also to do                                 upgrades                                 updates of node pools because you create                                 a new one which has the same labels and                                 um and you can then migrate the workload                                 over that                                 and also you have the names under                                 control so you are not depending on                                 some kubernetes changes or provider                                 changes                                 of the labels and                                 you need also the obtains which are                                 needed to block                                 unwanted ports from from                                 from your nodes you should say set the                                 minimum                                 size of a notebook always to zero that                                 if you don't use a node pool                                 you don't have to pay anything and                                 the max as i said before you should set                                 to                                 to something that you never never reach                                 but don't forget your quota                                 the next optimization piece what we                                 can do is leverage                                 uh to to leverage the default cloud way                                 to                                 separate storage from compute                                 because this allows you basically to                                 restart your port                                 uh with a different resource requests                                 and getting the same data back back from                                 from the storage um and                                 it could be you also with a trick help                                 you to                                 save cross zones network charges                                 if if they are a problem for you                                 so um                                 as i said before big data jobs                                 struggling if if they're getting                                 a huge percentage of ports failing which                                 has happened if a zone is failing                                 and you run it across multiple zones                                 for example three means thirty percent                                 of your pots are gone                                 or or getting problems and                                 um there's basically a trick if you're                                 using storage which is accessible from                                 multiple zones like object stores                                 network file systems or in                                 gke you can use regional persistent                                 disks                                 which means you start your job in one                                 zone and if the zone get a problem you                                 restart it                                 in in another zone which helps you                                 to minimize the network costs as well                                 as reduce the latency in the                                 communication between the nodes                                 this is a stateful set which is                                 manage basically your deployment with                                 dedicated volumes per                                 per pot basically stateful deployments                                 you have also the possibility to change                                 the resource requests                                 or also node affinities tolerations                                 and if you change this stay as a                                 stateful set will then take care                                 to graceful restart all your pots                                 which allows you then to                                 add cpu capacity during the day                                 or at memory on kafka for example                                 you do a rolling restart and double the                                 memory                                 for the high traffic hours for example                                 and because                                 it depends on the workload how how how                                 fast this goes                                 but you can't even do it multiple times                                 a day and                                 basically allows you again to                                 adapt your resource definitions to                                 to your need and the similar stuff can                                 be done                                 with the uh most of the operators which                                 are also triggering then restarts                                 which are yeah operator and                                 uh and so on so so so this is                                 big big data drops like                                 spark flink running at the end in their                                 own sparkling cluster                                 and in kubernetes you can spin up those                                 clusters basically                                 on demand and if you run one one job in                                 the cluster                                 and this allows you to specify the size                                 of the cluster per job                                 you have the cluster per job on demand                                 and this allows you the right sizing                                 and with different node profiles                                 on on on different node pools you can                                 adjust the capacity there and the                                 cluster autoscaler will take care that                                 the resources are available                                 and the operators makes the deployment                                 very very simple                                 and and for spark there are currently                                 two install                                 interesting operators what i mentioned                                 here                                 i have good experience with the google                                 one because it's a very good                                 integration in all the kubernetes stuff                                 and if you use airflow you have even a                                 native                                 integration to easily use it and                                 on the others you have to do a little                                 bit                                 more yeah airflow                                 is a very very integrated                                 now with this kubernetes stuff and                                 allows you                                 to create spark                                 up applications on demand there are                                 some some other tools allow similar                                 stuff                                 yeah flink operators are also available                                 for flink jobs                                 and also as as in in the talk before                                 mentioned                                 there are others as well available                                 so for for storage there are some                                 some some hints especially if you use                                 object stores                                 uh because they're scaling right now all                                 automatically you ha you should always                                 reuse your buckets always use the same                                 buckets for the same jobs                                 use the same pattern even do some time                                 pre-conditioning or warm-up                                 to allow the high parallelisms what                                 big data jobs typically need                                 needing and also avoid writing to local                                 images and use us                                 for temp local ssds or things like that                                 and for the for the images                                 avoid large large images whenever is                                 possible                                 so try to load the data some somewhere                                 else like an nfs server                                 afs or gcp file store or even loaded                                 downloaded                                 from from an from an object store                                 for uninterruptable jobs                                 on on kubernetes run them as a job or as                                 a static port                                 set the limits and the requests equally                                 to make sure that they're not getting                                 get affected much by other pots                                 and the annotation                                 again                                 and if you want to to run such a thing                                 get the kubernetes cluster like a gce                                 have make sure that you have the                                 cluster autoscaler and add a couple of                                 stuff                                 stuff then there is here example hand                                 shots to use                                 yeah and that                                 said bosses thank you for listening                                 and if you have questions let me know                                 you
YouTube URL: https://www.youtube.com/watch?v=iY9H5luFwns


