Title: Jo Kristian Bergum – Search and Sushi; Freshness Counts
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Search and ranking over datasets which are constantly evolving in real time is a challenging problem at scale. Updating the documents in the index with real time signals like inventory status and click through rates can improve the search experience considerably. The fields which needs to be updated at scale can be used as hard filters as part of the retrieval strategy or as another ranking signal. 

In this talk we’ll present an overview of the real time indexing architecture of Vespa.ai which supports true in-place partial updates of searchable fields, including tensor fields. We also compare the real time indexing architecture of Vespa.ai with search engines built on the Apache Lucene library.

Speaker:
Jo Kristian Bergum – https://2021.berlinbuzzwords.de/member/jo-kristian-bergum

More: https://2021.berlinbuzzwords.de/session/search-and-sushi-freshness-counts
Captions: 
	                              welcome to this talk on search and sushi                               freshness counts my name is joe                               christian burgum and i work for                               various media on the vespa team um also                               on twitter                               um yeah so this talk uh i'll give a                               brief                               uh introduction to to vespa and                               searching and ranking over evolving data                                sets                                and i'll do um a little bit of a deep                                dive into                                the vespa real-time indexing                                architecture                                so um what is vespa really um it's the                                open source                                platform for low latency computations                                over large                                evolving data and it's a partially tool                                licensed                                and it has a rich set of features                                you can search filter and rank both                                structured and unstructured data                                it also has vector search capabilities                                to approximate nearest neighbor search                                you can also                                combine approximate nearest neighbors                                search with regular search filters or                                rankings so you can have a hybrid                                retrieval and ranking pipeline it's                                very scalable it's also very fast so                                it's cost effective                                and also has advanced multi-phase                                ranking and retrieval                                and also tensors are first-class                                citizens in the in the vespa document                                model                                and has built-in support for importing                                machine learning models from a wide                                variety of popular                                machine learning frameworks like                                tensorflow pytorch                                on x and also the gbdt family                                executes and light gbm and in this talk                                i'll focus on the real-time indexing                                support investbar and also the true                                partial update uh support um                                but first let's look at the history uh                                of wespa                                so going back actually to                                     the team here in trondheim you used                                around all the web.com which was a a                                large web search engine                                competing with google inktomy altavista                                and other web search players                                at that time in                                     yaoi decided that they also wanted to                                build                                web search technology um they wanted to                                get rid of google                                so they went out and bought inktomy                                overture                                after vista and also this office                                who was powering the web search of                                oldweb.com                                and the team here in trondheim we were                                set up to                                build the next generation vertical                                search platform and that's where the                                name vespa is from                                and west was born in                                     fast forward to                                                      out with                                improved and new real-time indexing                                support                                and finally in                                     uh we open sourced vespa to the world uh                                using an apache                                           and all the development of the vespa                                engine is now in the open                                on github and this year we also                                announced                                the availability or the general                                availability of the vespa cloud which                                allows you to                                run web applications on a hosted cloud                                the scale we operate westport                                at the aortic media is pretty                                significant                                we serve about                                                     queries per                                day and we do about                                             primarily updates                                there's around                                              applications across yahoo and various                                media that are using vespa that includes                                gemini native ads home page local search                                news finance and so on the list is                                pretty long                                 if you go to blog westby ai you can read                                 about some of the interesting use cases                                 that vespa are used for                                 in in yahoo including tensor ranking for                                 home page recommendations                                 here is an overview figure of the vespa                                 architecture of a single deployment                                 of a vespa architecture so on the left                                 side you have the vespa application                                 package                                 this is where you define your vespa                                 application it includes                                 the kind of deployment specification it                                 also includes the the document schemas                                 if you have custom models machine                                 learning models you also import these                                 into the application package and if you                                 have custom code                                 this application package is uploaded to                                 the vespa configuration system                                 which translates this high level                                 configuration                                 into an actual running deployment and                                 running configuration                                 on the top here we have the stateless                                 container cluster                                 which is a java container where you can                                 plug                                 custom query processors working on the                                 queries                                 custom document processors working on                                 the documents and also generally                                 components on top of this there's a set                                 of native apis                                 both http and http to go                                 you can also build your own apis on top                                 of this because it comes with                                 http handlers and so on so you can                                 actually build nice                                 feature-rich apis directly on the                                 serving container                                 then there's the content layer in the                                 content cluster where the magic is                                 happening this is where we store the                                 content and index the content and                                 distribute the content and also has                                 the distributed query execution so that                                 you can                                 fan a carry out and get results from                                 multiple nodes in the cluster                                 westman runs you can run west by using                                 rpms or you can also use docker                                 the docker image name is there you can                                 run it even on your on your laptop                                 so back to the the thing about searching                                 and ranking over evolving data sets                                 so obviously vespa supports the basic                                 crude operations so create                                 read or search update and delete                                 in any given search applications they're                                 usually some hard filters                                 those filters could be explicit by the                                 user so the user get the choice to                                 filter the search results                                 but they could also be hidden for the                                 user built in the application layer                                 for instance spam filtering offensive                                 content and so on are examples of                                 a way of filtering away results that we                                 don't want to show to the end user but                                 these results can also be                                 filtered by the users in some cases when                                 you have a very large document volume                                 and you want to update some parts of the                                 documents for instance the in-stock                                 status or the price or some rating                                 it's very troublesome to actually have                                 if you have a very large content volume                                 if you have to re-index                                 all the data just to change a few fields                                 so that's one important aspect and                                 westbound solves this                                 for you also there there's another thing                                 what i call basically                                 soft filtering it's not really filtered                                 in the explicit way but                                 some properties of the documents will                                 make the document rank                                 much lower so it's actually not surface                                 to the users                                 some singles that could be used in the                                 ranking model could include for instance                                 click feedback from users what are the                                 users clicking for and                                 what are the users not clicking for this                                 information                                 can be fed back to the index so that the                                 ranking is                                 updated based on how the users are                                 interacting                                 now i will go into the real-time                                 indexing architecture in the vespa and                                 i will also give you some history about                                 how the indexing architecture of vespa                                 actually                                 developed over time so when we                                 made real-time indexing architecture or                                 the real-time indexing parts in                                 vespa there were some high-level goals                                 that we wanted to meet we wanted to have                                 very low latency measured in the single                                 digit milliseconds                                 we wanted to have operations visible in                                 the search results immediately                                 when the operation was acknowledged so                                 if you feed the operation to vespa and                                 you get acknowledged back that hey                                 i've taken care of the document then                                 that operation is actually visible in                                 the results                                 we also wanted to have a reasonably high                                 throughput of operations even if we have                                 very low latency                                 and also importantly we want to have a                                 low impact on search serving latency                                 so that in case we do operations and                                 feed and do                                 updates of our index we don't want that                                 to ruin the search latency or the search                                 experience or the                                 service level quality                                 before diving a little bit into this i                                 know that a lot of you                                 are search experts and you know the in                                 and out of search and how search                                 work but let's take a step back and look                                 at the classic inverted index data                                 structure                                 so basically you take a set of documents                                 and to index them to speed up query                                 evaluation                                 you invert the documents so that you                                 first tokenize the documents you figure                                 out which are the vocabulary                                 and then you build a dictionary of the                                 unique set of words or tokens that is                                 occurring in your documents                                 the dictionary contains a pointer to                                 what we call a posting list                                 here in this case there are three words                                 and they're occurring in some documents                                 these structures can help speed up                                 searches and there are some examples                                 here for instance using or                                 and and phrase the posting list might                                 contain different granularity of                                 information                                 for example some posting lists can just                                 say is the dark                                 is the term present in the document or                                 not more                                 information can be added for instance                                 how many times does the term occur                                 and also at which positions i mean                                 posting lists and dictionaries and                                 the classic inverted structures there's                                 been a lot of resources old-fashioned                                 data structure it's been around for a                                 long time                                 one downside of the classic index                                 structure is that it's difficult to                                 update this structure                                 because if a new document is added you                                 both need to update the dictionary and                                 you need to update the posting lists                                 our first take at solving real-time                                 indexing                                 was around                                                               that we had a hierarchy of indexes with                                 the gradually increasing size                                 here in this case we have three active                                 indexes                                 and this is basically a batch immutable                                 index segment                                 once the index is built the index cannot                                 change                                 because then we are taking the documents                                 and we have inverted them                                 and the index is basically frozen and                                 operations against this was operations                                 where you don't you didn't really know                                 when they actually become searchable the                                 search engine says okay i take a new                                 document                                 it might become searchable later and the                                 queries need to fan out                                 all the active indexes and this means                                 that each individual index needs to do a                                 lot of matching                                 looking up in the dictionary reading                                 posting lists and then finally the                                 results emerge                                 in order to not run into a situation                                 where you have hundreds of thousands                                 of indexes you need to merge these in in                                 the background                                 to lower the the the serving cost of the                                 queries                                 and this also puts a cost on the system                                 in terms of io rights and and                                 and and might impact search performance                                 so in our experience this indexing                                 architecture                                 did not really uh it didn't provide a                                 really low indexing legacy and we also                                 saw                                 uh quite significantly impact on the                                 serving latency                                 this illustrates uh the kind of the                                 index fusion                                 and how a new index is built when when                                 you have to                                 merge indexes then the old one can be                                 deleted                                 then you start serving credits on the                                 new one but                                 hardware really evolved so we started in                                 the search business around                                            chart shows                                 uh how much the price of one gigabyte of                                 memory uh over the time since                                                                               as you can see there's a reflect at the                                 roughly cost of one gigabyte                                 in                                                                   dropped to ten dollars per gigabyte in                                                                       so we were operating vespa or the                                 predecessor of west by                                              running on on systems with one                                 less than one gig of memory in                                                                                                                gigs of ram                                                              you know we're getting a lot more ram                                 now what can we do with this                                 we decided that we wanted to move away                                 from this multi index instead we built a                                 mutable memory index structure where we                                 actually can                                 update the dictionary and the posting                                 lists in place                                 in memory and then back this with an                                 immutable index                                 then if the memory index is full then we                                 flush it in the background                                 and then we can merge that with the with                                 the immutable index similar that we did                                 in the previous architecture                                 but this adds a much better buffer so                                 that you can actually do                                 in place updates in the in the memory                                 structures                                 there's also something called attribute                                 data which is really a forward index                                 this is a place where we can store                                 certain                                 fields that we annotate in the document                                 schema that these fields                                 are attributes and the attribute in the                                 field should be in memory                                 so that we can access them for ranking                                 grouping                                 and and also for sorting so that that                                 data is actually in memory                                 here is an example of a vespa document                                 schema                                 there's a document tweet which has a                                 field text it's a type string                                 and we say that we're gonna indexing                                 this it's an index                                 this type of field will be in the                                 mutable memory index                                 will be indexed in the mutable memory                                 index first and then it will be                                 gradually flushed into the immutable                                 index                                 the attribute field here of a type long                                 will be in the attribute data which is                                 the forward index                                 this is the vespa field view inside the                                 content node                                 so when the operation and this does not                                 cover the kind of                                 distribution mechanisms in westbound how                                 we do                                 across multiple nodes this is inside one                                 content node                                 you have a transaction log so you really                                 need to have a transaction log because                                 we are dealing with some memory                                 structures here                                 if we lose power we need to be able to                                 recover the ins the index from the                                 transaction log                                 then there's the immutable index which                                 we already touched on which is basically                                 one large index then there's the                                 document store where we actually store                                 the actual document contents and in the                                 the document contents                                 uh we basically store all of the data                                 for the document because this also                                 allows us to                                 in case we want to redistribute data                                 over multiple nodes                                 uh we have the source data in the                                 document store                                 then the memory index like introduced                                 and attribute data                                 and there's also a mapping into the                                 document store which is in memory                                 and also attribute data where you                                 specify a specific setting in the schema                                 we'll get                                 to that you can add also posting lists                                 on top of the attribute data                                 then there's a feed view so when an                                 operation comes into the system                                 it gets written to the transaction log                                 so that we persist the operation in case                                 is a power failure or something that we                                 need to recover                                 let me write it into the memory index                                 attribute data                                 and in the document summary store                                 and the search view similar we have a                                 queries coming in and the queries have                                 these                                 components to um to research so a query                                 will in parallel search the memory index                                 and the immutable index and the                                 attribute data                                 if they are included in the query or the                                 attributes with the fast search                                 so this is the way the query is set up                                 so that it's fanned out to all these                                 different components                                 now we touched on the kind of basics of                                 around the indexing and the nodes now we                                 look at actually how users are                                 configuring                                 how users are controlling their                                 application                                 in this case we have a schema which has                                 a document tweet                                 and i'm listing a set of name fields                                 here there's id                                 field type long we specify that this is                                 going to be a summary that means that it                                 will be                                 also returned in the search engine                                 result page it's                                 attribute which means it's in the                                 attribute storage so we can have fast                                 access to                                 this field both for grouping searching                                 and ranking                                 there's a text field type string here we                                 specify we're going to index it                                 that means that we will we will tokenize                                 it we will stem it                                 and do the regular indexing to be able                                 to                                 support textile matching                                 then there is a created ad field which                                 is basically the timestamp when the                                 in unix epoch for when the tweet was                                 actually created                                 and in this case we have also attribute                                 but here we add attribute fast search                                 that means that we will add a b                                  indexing structure on top of the                                 attribute so you can have                                 a fast search using posting lists                                 by default attribute field like the                                 id here does not have that structure so                                 if you actually try to just search for                                 the id using this schema that will be a                                 linear scan                                 and there's a key distinction here                                 between index and attribute                                 there's also a likes field here and the                                 topics which is a tensor field where                                 we can store what topics is this tweet                                 about using a tensor or sparse                                 sparse tensor in this case                                 vespa ranking similar you configure                                 vespa ranking in the document schema                                 it's very flexible so you can write                                 handwrite your own expressions or you                                 can use machine learning models                                 but the magic here is that the the                                 machine                                 or the ranking is able to use these                                 fresh signals reading from the attribute                                 store which                                 can be updated in real time                                 here there's an example of a simple text                                 freshness rank profile which                                 does a combination of freshness and bm                                   so bm                                                                 there's a topic ranking here given that                                 the user has                                 declared some interest in the certain                                 topics we compute the sparse dot product                                 between the user interest                                 and the topics in the document so so in                                 order to kind of                                 show the user uh topics that he                                 is interested in and finally there's                                 an example of actually combining a x-key                                 boost                                 model this is a gbt model with                                 on-x deep neural network model                                 so back to the attribute versus index                                 this table                                 kind of summarize uh the the the                                 features                                 that you get when you declare a field as                                 an index or attribute                                 attribute fields are only fast                                 to match over if they have been set with                                 fast search                                 or if there are no other more                                 restrictive terms in the query                                 but they're very fast to update                                 now i get a few api examples so you get                                 the feel of how you interact with                                 the with the apis of vespa here there's                                 a simple                                 curl command using the vespa http api to                                 create a new document so                                 we put the document into we have some                                 text here we have id                                 and we have a created timestamp that's                                 fine                                 then we want to update the document and                                 in this case we update the number of                                 likes and assign the value one                                 this operation is not causing rest but                                 you have to read the original document                                 and then apply the number of likes and                                 then                                 write it back this is applied directly                                 in the attribute                                 right so that is the the crucial part                                 here to                                 have a high throughput of these partial                                 updates                                 here's another one where we can imagine                                 that we have some machine learning                                 process going in the background it will                                 either define which tweets are about                                 which topics                                 and then we can go back and update a                                 large volume of the content pool                                 with this value in this case we assign                                 this tweet                                 it's about search and it's about machine                                 learning and it has some scores                                 in this case we have a pro account of                                 twitter which is actually updating                                 the tweet text this is not uh handle                                 this is handled by reading the document                                 from the document summary store                                 and writing it back to the index so this                                 one is not                                 in place this needs access to the other                                 fields that were originally in the                                 document                                 and here is an example of the query api                                 using the yahoo query language where you                                 specify the application logic                                 in this sql-like syntax                                 where you want to have a filter on on a                                 date                                 which is application specific for                                 instance only search in tweets                                 the last                                                             and in this case the user couries berlin                                 buzzwords and we choose                                 the ranking profile and how many hits we                                 want to return                                 so everybody's wondering about                                 performance obviously                                 so this depends a little bit on the type                                 of hardware you're running on but this                                 is one example where you're doing                                 partial updates of a single                                 uh integer field where you get single                                 digit millisecond latency and we can do                                                                                                    so that's a really high number but note                                 you also have to write to the                                 transaction log and that requires a high                                 i o write capacity                                 and if you're running westbound for                                 example a network attached storage                                 doing a sync operation against that                                 storage cost a lot                                 so but the default sync operation in                                 vespa is that we do actually try to sync                                 sync to storage for for the operation so                                 that they can be durable in case of a                                 power failure                                 you can however tweak this and and set                                 this                                 to false uh the puts against the memory                                 index depends a lot on the size of the                                 input text                                 obviously and also in this case we're                                 going to have multiple threads                                 working on on the memory index so in                                 this case                                 it depends really on the size but                                 usually numbers that we observe on                                 similar type of hardware is from                                                                                           this is not batch oriented indexing this                                 is with low latency with a single digit                                 millisecond latent so once you                                 put the document into the index you get                                 the acknowledge                                 then the actual document is there                                 similar if you look at elasticsearch                                 you will have to pass this refresh                                 setting                                 to true in order to have the same kind                                 of functionality that you have in vespa                                 that was what i had i included a few                                 resources here                                 you can go we read more about vespa you                                 can go we have a slack space                                 we even have a free cloud cloud                                 trial so you can check out our hosted                                 cloud offering                                 we have a twitter account i'm also on                                 twitter and                                 you can also follow us on github and                                 tomorrow there's                                 going to be a search engine debate with                                 me and josh and angel                                 and i'm really looking forward for that                                 so i hope you will take time                                 to join so that was what i had                                 um i'm open to questions                                 thank you joe christian that was great a                                 nice tour of vespa                                 uh and one of you well i think one of                                 your favorite topics to talk about                                 is partial updates um                                 we have one question from andreas uh in                                 the chat and it's a                                 bit more of a general vespa question so                                 he's asking um                                 how can i modify or add analyzers                                 stemming tokenization etc                                 in bespo what does that look like                                 do you have a handy example to show yeah                                 so that's                                 that's a great example so basketball by                                 default we integrate uh                                 with apache open nlp for stemming and                                 tokenization and all of this is                                 happening in java so you can                                 actually take the linguistic class that                                 is there and you can subclass it                                 and make your own uh stammer and                                 tokenize so if you want to kind of                                 extend it so that's definitely                                 definitely possible                                 excellent um so i had a couple of                                 questions                                 you don't mind so you mentioned recovery                                 using transaction log which makes a lot                                 of sense                                 um i'm curious and maybe i missed it uh                                 on                                 the last slide but how often is the                                 immutable                                 memory index flushed and merged into the                                 immutable disk                                 index yeah that's that's that's that's a                                 great question um                                 so uh that will depend on the feed rate                                 because there's a target                                 for the maximum size right so                                 if it's if it becomes too big then it's                                 going to be flushed                                 right so typically the default setting i                                 think is uh we use one gig                                 uh for for the mapper index okay so it                                 definitely will depend on the on the on                                 the feed rate and the update rate right                                 makes sense all right a couple more                                 questions for you yeah                                 great um is there any way to uh                                 encrypt documents being stored                                 right so uh we rely on the file system                                 to do the encryption                                 uh yeah we do that i think that's the                                 normal                                 normal procedure makes sense                                 um so another question how does the                                 indexing performance change                                 as read traffic increases how easy is it                                 to take snapshots                                 of the index okay so those are two great                                 examples so                                 on the read performance versus                                 right performance so basically a lot of                                 these operations are                                 uh cpus the way we throttle it is that                                 there's a parameter called concurrency                                 so it's a high level parameter where you                                 set aside you know how many                                 the sizes of the tread bulls which is                                 depending on                                 the number of cpus you have on the                                 system so typically these are                                 limited uh number of cpus you you don't                                 get to use like                                                                                                       that's                                 how we kind of balance uh search versus                                 uh versus uh indexing                                 the other question well i forgot it i                                 forgot i'm sorry                                 no it's okay um so there's quite a few                                 questions rolling in                                 uh we're at the top of the hour so i'm                                 also gonna suggest we'll ask one more                                 question and then                                 we'll go over to the breakout room                                 yeah that's breakout room france salon                                 and we can i can copy some of those                                 questions over                                 um one more question uh maybe a short                                 one                                 on partial updates are all fields read                                 and                                 indexed or just the one field you are                                 updating                                 it's uh only yeah so if it's attribute                                 field                                 uh we don't uh read the original data                                 from                                 uh the data store so you upload that in                                 place you don't                                 need to read the entire document and                                 re-index it back                                 you
YouTube URL: https://www.youtube.com/watch?v=vFu5g44-VaY


