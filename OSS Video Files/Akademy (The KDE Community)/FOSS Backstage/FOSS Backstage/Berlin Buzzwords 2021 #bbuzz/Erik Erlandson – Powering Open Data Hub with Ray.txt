Title: Erik Erlandson – Powering Open Data Hub with Ray
Publication date: 2021-06-30
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Ray is quickly gaining momentum as a distributed computing platform that combines a powerful parallel compute model with a cloud native serverless-style scaling model. Open Data Hub (ODH) is a flexible and customizable federation of open source data science tools that is a great fit for taking advantage of Ray compute clusters.

In this talk, Erik will explain how to integrate Ray with Open Data Hub, by configuring ODH profiles that deploy on-demand Ray clusters for Jupyter notebooks. He’ll demonstrate Ray in action as a scalable compute resource for ODH, and explore the potential use cases opened up by self-service notebooks backed by Ray distributed computing. Along the way he’ll also discuss the logistics of adapting Ray to OpenShift’s security features.

Attendees will learn how Ray integrates with Open Data Hub’s architecture, and how they can power ODH with Ray to solve distributed computing problems in the popular Jupyter environment.

Speaker: 
Erik Erlandson – https://2021.berlinbuzzwords.de/member/erik-erlandson

More: https://2021.berlinbuzzwords.de/session/powering-open-data-hub-ray
Captions: 
	                              um                               thanks everybody for attending i know a                               lot of you are                               dealing with a time zone issue so i                               appreciate the extra effort                               i'm eric rollinson and i work at red hat                               and i                               work in emerging technologies for uh the                               intersection of the                                machine learning ecosystem with the                                kubernetes                                ecosystem so today                                i want to talk to you about a quote                                unquote powering open data hub                                with ray                                [Music]                                another possible title the talk could be                                using jupiter and ray in the cloud                                so the uh sort of road map for the talk                                is                                i'm going to talk a little about ray at                                                                             and then i'm going to sort of like place                                jupiter and open                                data hub and in context for the talk and                                then talk about sort of the architecture                                of                                what i did to get ray working on odh                                and then follow that with an actual demo                                of the architecture and then i want to                                close with                                some of the collaborations that made                                this work possible                                so um the design goal for ray was to                                occupy                                a sort of niche and ecosystem where                                it's a higher level and better                                abstraction than say                                raw mpi for parallel programming but                                also                                allow a slightly lower level and more                                flexible                                series of computational representations                                than apache spark                                so it wants to sort of live in the                                middle there                                its compute model is consisted of                                tasks and actors and                                there's a nice parallelism with python                                here                                a task is simply analogous to a python                                function                                and actors are direct analogs of                                python classes and the programming                                ergonomics are really very easy you can                                take                                these python definitions and uh decorate                                them with ray                                dot remote and ray will then know how to                                execute them out on its cluster                                um ray operates by                                allowing you to set up uh compute dags                                um like on the left here you can see                                they uh                                sort of highly over engineered way of                                setting up                                summing the numbers one through eight                                you can see that you                                create a bunch of add calls using the                                dot remote                                ray decoration function it's                                automatically provided to you                                if you use ray dot remote                                and you're building up sort of like a                                computation now i want to talk about                                this part at the end here array.get                                ray has something in common with spark                                which is that it has sort of a lazy                                declarative                                compute model and so the first the first                                seven                                lines up there are just setting up a                                computation nothing at all actually                                happens until you say ray.get                                so it's a again allows like spark it                                allows ray to                                sort of decide for you how best to run                                the compute that you ask it to                                i took these diagrams from a great blog                                post by uh robert nishihara um the link                                there is the bottom if you                                recommend it if you want to read more                                about how ray does its job                                um so ray's primary data model                                is um the plasma object store                                um and it is a distributed object store                                it's like most data structures uh in                                python it's just fundamentally                                typeless and schema-less                                 the the store prefers to operate local                                 first and what that means is it'll pull                                 remote data                                 only if it needs to and otherwise it                                 will always prefer to                                 get its data locally on running on ray                                 workers                                 so most read write is local to worker                                 nodes                                 similarly ray's work scheduling model is                                 local first so it will always prefer to                                 run a compute job on the work node that                                 is already running                                 otherwise only if it has to we'll take                                 work elements and push them back to the                                 global scheduler to get rescheduled                                 somewhere else and so                                 anyways local first principle allows                                 rate operate fairly efficiently                                 so to set up the context                                 for you know why rey with jupiter                                 um i just want to talk a little bit                                 about the library ecosystem around ray                                 um out of the box it comes with hyper                                 parameter tuning                                 um a reinforcement learning package                                 a basic stochastic gradient descent                                 um and ray serve for basically doing                                 model serving uh an array cluster                                 and then in addition there is a large                                 ecosystem of community                                 integrations most of the packages you'd                                 be familiar with in ml space are there                                 xgboost desk horvath sklearn                                 um almost everything at this point has                                 got some kind of                                 great integration the link at the bottom                                 you can look at the full list of                                 integrations                                 so if you look at all these things                                 these are of course all packages that                                 data scientists have been using with                                 jupiter                                 for quite a while now and so when you                                 look at it that way                                 it sort of begs for ray to be driven                                 from                                 jupiter notebooks to use these packages                                 accelerated with ray compute                                 and additionally you know it gives you                                 the promise of                                 literary literate programming and                                 interactive programming with ray using                                 jupiter's environment um                                 and more specifically um you know                                 getting this environment as                                 automatically provided to you with a                                 cloud deployment and in my case                                 today i'm talking about the kubernetes                                 container orchestration platform                                 and uh more specifically the flavor of                                 kubernetes i'm using                                 is a open shift                                 so when i first embarked on this uh                                 study                                 the primary way of connecting to a ray                                 cluster was a                                 the ray.init function and this had an                                 interesting property where it only works                                 if you're connecting on the physical                                 node                                 that the ray head node is running on and                                 so you look over on the left                                 to use jupiter you know in something                                 like kubernetes                                 uh with ray you'd actually have to                                 create a single pod that has both                                 jupiter running and the right head node                                 um of course architecturally this is not                                 very appealing it's not good separation                                 of concerns                                 um and the logistics of actually                                 installing and running jupiter and ray                                 are                                 just very awkward to deal with                                 however after i reached out to the array                                 community                                 and it turns out they've been also                                 creating a new way to connect                                 essentially a true client server                                 connection where                                 anything on a network visible to the                                 right head pod can connect to it using                                 the array connect function                                 so this is a total game changer because                                 allows much better                                 cloud native architectures and uh                                 not just jupiter but other you know                                 applications so running you know in the                                 cluster can actually connect hooray and                                 use it                                 as a resource                                 so as i mentioned um you know i chose                                 that                                 that i wanted to try to consume jupiter                                 through the open data hub project                                 and i want to talk a little bit about                                 what that means um so                                 what is open data hub um you know                                 firstly                                 open data hub is an open source                                 downstream of kubeflow                                 with a few modifications uh                                 tuning it for running on openshift and                                 other aspects                                 um it operates as a sort of reference                                 platform for                                 using open source machine learning                                 tooling                                 in the cloud and it's fairly federated                                 which means that these                                 projects um are relatively loosely                                 integrated                                 and um while you get you lose something                                 a little bit you know you lose                                 possibilities of tight tight integration                                 however it makes for a very very                                 flexible                                 environment and in fact it's the                                 flexibility here that allowed me to very                                 easily                                 run you know ray and integrate it with                                 odh                                 um the components you can get um do a                                 pretty good job                                 covering like both the different uh                                 stages of a typical machine learning                                 workflow                                 for cloud native development and they                                 also do a pretty good job of covering                                 all the different persona                                 not just data scientists obviously but                                 business stakeholders                                 you know app developers and i t                                 of course jupiter is at the center of                                 all that                                 um red hat actually uses its own                                 internal deployment of open data hub                                 i've done various                                 projects including you know clustering                                 operational metrics from openshift                                 clusters                                 analyzing customer support data and                                 doing anomaly detection on application                                 logs                                 so my job was made easier by the fact                                 that i had a sort of analogy to work                                 from                                 there's already a spark integration for                                 odh and it works like this if you                                 bring up the open data hub jupiter hub                                 launcher                                 and you pick a spark enabled image                                 um the first thing it will do as always                                 is give you a jupiter environment                                 but it will also go out and look for a                                 uh what's called a single user profile                                 and i'll talk                                 more about that in a second which tells                                 ray                                 how to spin up things like spark                                 clusters to go with your notebooks                                 and it in turn goes and references a                                 service template which basically gives                                 you all the yaml                                 for things running in kubernetes or                                 openshift                                 and using that information it then                                 creates those objects                                 and spins you a little spark cluster and                                 once you have that                                 you're working in jupiter and you can                                 simply connect to that so you have your                                 own                                 self-service personalized data science                                 with spark backing you                                 um so at a high level um                                 you know single user profiles and                                 service templates are nothing but                                 kubernetes config maps there's data                                 and so it obviously you know begs for an                                 attempt                                 to replace these with                                 you know config maps that tell odh how                                 to spin up a ray cluster                                 and connect to that                                 and so spoiler it turned out to be                                 relatively easy to do                                 so here's a single user profile you can                                 see there's nothing real special here                                 it's                                 images and resource specifications so                                 it's all fairly standard you know                                 objects and parameters you'll find                                 working with kubernetes                                 and likewise the service template                                 is just showing you know odh how to                                 initiate a ray cluster                                 custom resource which the ray operator                                 knows how to use to spin clusters                                 these are very very large objects with                                 many parameters so i'm not going to show                                 them all here but                                 you can see a few at the top again this                                 is just                                 standard non-magical kubernetes type                                 ammo                                 and so with that i will now actually                                 run the run an example of this                                 architecture                                 first i've already logged into the                                 cluster                                 and you can see when you log in you get                                 a dashboard it looks like this and                                 you can see over on the left is jupiter                                 hub so if you launch that                                 it takes a little while so i'm not going                                 to do it for you now if you launch that                                 and choose a ray enabled image                                 you can see that it is spun up not just                                 a                                 jupiter hub environment up here but it                                 is spun up a little ray cluster for me                                 here's the head node running in a pod                                 it also automatically produces                                 a little service and route that allows                                 you to view the                                 ray dashboard which is kind of similar                                 to like think the spark dashboard                                 so if we go and look at a notebook let's                                 imagine we want to do some data science                                 with ray um and again see i as i                                 will add comments here uh leveraging the                                 full                                 literate jupiter environment                                 so of course these uh create some images                                 that have the ray dependencies                                 pre-installed so i can simply import                                 them                                 i told the jupiter hub launcher how to                                 give it an environment variable here                                 which has                                 the actual name of the array cluster                                 that it spun up and so i can just use                                 that to connect                                 and so here you can see i'm testing to                                 see if i'm already connected because as                                 we know in jupiter                                 you can run cells more than once and you                                 don't want to like try to reconnect                                 twice so ray                                 doesn't really like that and so here we                                 can see giving me a little information                                 about the                                 head node that i connected to                                 i'm going to be doing an xg boost                                 example today and                                 there's an xg boost ray integration you                                 can see here                                 and it just provides some drop-in                                 replacements                                 that work just like all the normal xg                                 boost objects but                                 are able to leverage ray                                 and so here we're going to load up the                                 sklearn predefined                                 uh breast cancer data set just as a                                 simple example                                 and we're going to create a ray d matrix                                 object so that                                 can actually be easily parallelized                                 now that we have our data you can run a                                 little xg boost training run                                 and here you can see we're using the                                 actual                                 overloaded train function and we're also                                 giving it some right parameters and                                 there are many parameters here you can                                 use but                                 the fundamental one is what kind of                                 parallelism do you want to use i'm just                                 asking it to produce two actors here a                                 very small little                                 parallelism case                                 so we'll let it kick off here                                 ray likes to uh give you lots of log                                 output um                                 it's producing its actors to do a                                 training run                                 and even before i can finish talking                                 about it it's come back and finished                                 [Music]                                 and so i've got a model that was trained                                 in parallel using a self-service                                 ray cluster um but of course that's my                                 part of the story um the reason you like                                 to be in jupiter is you know to                                 use things you can do in jupiter as well                                 as ray so we can do things like examine                                 training results and cells so this thing                                 actually has a very low error rate                                 and better yet visualization um                                 so we can take this and do a little                                 scatter plot of the                                 raw uh logistic regression output versus                                 the truth and                                 of course we'd like all the things that                                 are one to be to the right of our dashed                                 line                                 and things that are                                                      see it's almost always true                                 so there's a little visual                                 representation                                 and it also provides you with                                 a the name of the actual uh url to                                 connect to                                 the ray dashboard which as you can see i                                 already have done                                 so                                 um what's the story here i think that in                                 addition to just simply having easy                                 parallelism backing your compute um                                 i think the bigger story is that it's                                 not just easy it's flexible                                 um ray can support all kinds of you know                                 tooling from the ml space xg boost but                                 also horovod                                 pandas scikit-learn many more                                 and so this architecture of simply                                 connecting to your familiar tooling with                                 jupiter hub                                 but backing all the compute with ray                                 is a very unified platform and                                 not just unified but simplified so in a                                 sense it can actually simplify your                                 cluster deployments for data science                                 you're running                                 a single backend cluster array cluster                                 to do                                 all of your compute instead of bespoke                                 operators for each of the tools                                 so the demo you just saw was running up                                 on the massachusetts open cloud                                 which is run in the partnership between                                 boston university                                 and red hat and also a large                                 collaborative consortium                                 of different universities and some                                 interested businesses and                                 anybody can get up on the massachusetts                                 open cloud i'll show you a link in a                                 minute                                 and if you do this you can run all these                                 examples plus                                 some others that i've created um and                                 you'll get                                 a smallish cluster with a maximum of                                 five workers                                 i recently upgraded the memory to                                   gigabytes and the images have some                                 common tooling                                 installed                                 in addition the particular openshift                                 deployment that i ran on the mass                                 open cloud is being maintained using the                                 operate                                 first project this is a project                                 being run at red hat where we're                                 exploring how to extend the                                 familiar open source principles of                                 developing                                 software in the open to also operating                                 the software and the services                                 in the open and you can read more about                                 that the link at the bottom                                 so as you might guess this has a very                                 strong get                                 ops flavor to it and so here you can                                 actually                                 go view the pull requests that i produce                                 to create this deployment of ray with                                 open data hub                                 the link is at the bottom                                 uh lastly you know there's a lot more to                                 do                                 um it'd be good to get like the ray                                 operator                                 as a community or operator through the                                 operator catalog                                 you know it might be nice to get                                 standardized build pipelines                                 for all the right imagery uh hopefully                                 using red hat's project toff                                 to do the building and i'm hoping for                                 more community use cases                                 i'm hoping more people try to use                                 deployment to do data science with                                 and i definitely want to get a formal                                 integration                                 of ray with kubeflow and open data hub                                 and also there's a lot of potential here                                 for                                 running nodes in ml pipelines like kf                                 pipelines                                 or each node could actually reference a                                 ray cluster                                 and so again thank you for coming to my                                 talk i invite you to                                 play with ray up on the mass open cloud                                 you can see the link at the bottom there                                 to go look at it on the instructions for                                 using the ray imagery is also                                 at the second link and if you have                                 problems or suggestions                                 file issues or pull requests with the                                 operate first                                 environment and please feel free to                                 reach out to me at my email if you have                                 questions or comments                                 thanks everybody                                 you
YouTube URL: https://www.youtube.com/watch?v=mzKtQEU7yxg


