Title: Isabel Zimmerman – Explaining model explainability
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Machine learning doesn’t have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms. 

This session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist’s point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.

Speaker:
Isabel Zimmerman – https://2021.berlinbuzzwords.de/member/isabel-zimmerman

More: https://2021.berlinbuzzwords.de/session/explaining-model-explainability
Captions: 
	                              i am isabel zimmerman                               and i will be teaching about model                               explainability today                               i am a software engineer at red hat's                               artificial intelligence center of                               excellence                               with their ai opso artificial                               intelligence operations team                               i'm also a very recent graduate from                                florida polytechnic university's data                                science program                                and of course the most important                                personal tidbit um                                what was my pandemic hobby i've been                                spending hours as a homemade chemist                                to build the absolute perfect recipe for                                chocolate chip cookies                                so in my every day at work i get to live                                the dream and play around with code and                                break                                code and i get to spend a lot of time                                doing data science work as well                                and i really stumbled upon this topic of                                model explainability                                actually when i was working as an intern                                and it's just a topic that stuck with me                                ever since                                because people often times think that                                machine learning answers                                all of our questions but in reality                                machine learning doesn't even have the                                same objectives as its users                                we'll dive into this mismatch of                                objectives between models and humans                                and we'll talk about how explainably                                explainability helps to close this gap                                but i also want everyone to be able to                                walk away from today                                being able to tell their colleagues                                about what explainability is and why you                                should be implementing it                                and i also want everyone to gain                                knowledge of just a few algorithms                                you can add into your own models and                                where you can add them                                into your machine learning workflow so                                before we go                                any further i think it's time to give                                you kind of the                                                         for explainability                                we know a machine learning model is just                                some sort of algorithm that uses an                                input of data                                such as a cat picture to give an output                                of                                some insight into that data such as                                identifying the photo contents                                the output of the model really only                                answers the question of                                what in this example what is this a                                photo of                                a model will tell you that it's a cat                                the output of the model                                told you the contents of the photo but                                not                                the logic behind making the prediction                                explainability focuses on the why why                                did this model say this was a cat                                was it the eyes the ears the paws the                                color                                so models look to optimize a                                mathematical function using the given                                data                                and humans look to gain insight into                                their problems                                at best these two objectives align                                but at worst machine learning models                                make the front                                page news for unintended but really                                astonishing bias                                explainable machine learning is                                necessary to close this gap                                between the machine goal of output and                                the human goal of                                understanding and we need it when it's                                not enough to just get the prediction                                from your model                                the model must also justify how it came                                to this prediction                                admittedly i'm probably never going to                                convince you that explainability is                                important                                by showing you pictures of cats in fact                                one of the most                                powerful reasons to use explainability                                is that it can help                                uncover bias and improve people's lives                                so let's say a lender uses machine                                learning to determine someone's ability                                 to pay back a loan                                 if the lender is using a black box model                                 someone who is denied a loan                                 may not be given a reason why this                                 unknown reasoning is problematic because                                 one people will never know how they can                                 change their habits to be approved for a                                 loan                                 and two it could be covering up this                                 unintentional                                 but harmful bias so let's think about                                 this entire workflow a little bit more                                 carefully                                 here's your vanilla you know data                                 science workflow                                 and we know that data science starts                                 with having a well-defined problem                                 and then you're going to gather your                                 relevant data um you'll do some feature                                 engineering to make sure you know what                                 you know is all inside of it                                 maybe put it into a workable state                                 you'll do some model tuning                                 and then we'll validate the model and                                 we're going to take a pause at this                                 model validation step because in your                                 validation step                                 you'll be looking at some sort of metric                                 such as                                 root mean squared error or accuracy                                 or maybe precision or recall and these                                 help you know that your model                                 is performing well in our example                                 we'd be making sure that the cat model                                 can robustly tell the difference between                                 cats and dogs                                 or that the people who are getting loans                                 are able to pay them off                                 after tuning you'll deploy your model                                 and continue to look at it to ensure                                 that it continues to perform as expected                                 so there's no data drift or any weird                                 strange things happening in the model                                 once it's out in production                                 so this seems like a pretty you know                                 well-rounded end-to-end machine learning                                 workflow                                 where does explainability fit in                                 and in general explainability happens                                 after a model has been validated you can                                 see there's really arrows all over                                 um this is a process that's repeated                                 many times you may have to go back to                                 feature engineering model training model                                 tuning                                 revalidate the model but you have to                                 start with a model that is                                 tuned and validated in order to have                                 useful explanations                                 i want to be very careful here because i                                 don't want people to think                                 that you can add an explainability                                 algorithm                                 rather than making a thoughtfully                                 engineered model                                 you may have heard that if you put                                 garbage data into a                                 good model you just have a garbage model                                 and the same principles apply here a bad                                 model will never give actionable                                 explanations                                 model explainability does not fix poorly                                 engineered models                                 it simply explains why decisions are                                 being made                                 but let's work it let's look at this                                 workflow step by step                                 and we can see why models might need                                 some more exploration                                 with explanation all right so we're all                                 data scientists here today                                 and this talk we have our data science                                 hat on and we're looking at our data                                 we can kind of do a visual you know                                 exploratory data analysis                                 we see that the data is kind of all                                 trending up and to the right                                 so we've done our that we understand our                                 data we can do some                                 fake feature engineering looking at our                                 data points here                                 and so now we're at our step where we're                                 deciding which model to use                                 and let's start off with the simplest                                 algorithm we can                                 if you can recall your favorite middle                                 school maths teacher                                 hopefully y equals mx plus b rings a                                 bell                                 it's a it's a classic um i'm going to                                 take a moment to give a very important                                 reminder                                 this is a reminder that machine learning                                 is at its core                                 just math sometimes we get so                                 caught up with accuracy or whatever                                 other evaluation metrics                                 we forget what a model is and it's it's                                 just math                                 um we can look at the options for                                 fitting this model and our                                 thoughtfully engineered solution is the                                 middle one                                 so we've trained and tested our                                 algorithm with our data and our                                 algorithm is now our model y equals five                                 x                                 plus one and it's best suited for our                                 data set                                 and we know that it's best suited for                                 our data set because                                 on a very basic level it minimized the                                 average distance between each data point                                 and the line                                 it was the line of best fit it kind of                                 goes in the middle of everything                                 we did a good job and this is math we                                 can interpret                                 we can confidently look at our model and                                 see that this                                 is the best one and we can see why it's                                 the best                                 and we understand what happens to our                                 prediction                                 if we have different inputs we know that                                 if we test a data point that's further                                 to the right                                 we're probably going to be getting a                                 higher prediction and this is intuitive                                 and it makes sense especially when we                                 have the graph right in front of us                                 and it's a great example of a very                                 interpretable model                                 that doesn't need an explainability                                 algorithm for you to understand why it's                                 making decisions                                 we can give ourselves all a pat on the                                 back of being superstar data scientists                                 right now                                 because we                                                               is giving the output                                 um and how to manipulate the inputs as                                 well and this is a really great                                 incredible moment and a win for us all                                 as data scientists                                 until you realize that models really                                 don't look like                                 y equals five plus                                                 they might be part of a larger pipeline                                 or                                 maybe they're just too complex to be                                 understood at a glance                                 as our data sets grow in size and                                 complexity                                 so does the math that makes up our model                                 and when this math is no longer as                                 simple as                                 y equals mx plus b it can become                                 difficult to interpret and explain how                                 our model is working                                 and why it's making decisions and there                                 are many different                                 types of problems and many different                                 types of models                                 that can really benefit from                                 explainability however                                 there is one type of model that                                 especially benefits and that's your                                 black box models                                 in fact this is probably the most                                 important time to use explainability                                 algorithms                                 so black box models are models where you                                 don't explicitly see how the variables                                 of the model are                                 interacting with each other just being                                 used in general                                 they're fairly commonplace in industry                                 since they're quite easy to implement                                 and algorithms exist for nearly every                                 type of machine learning problem that's                                 needed to be solved                                 and they're fairly easy to implement but                                 they can                                 also be very dangerous and i'll repeat                                 that again to make sure it sits in black                                 box models can be                                 very dangerous                                 when you don't know exactly how the                                 model is creating predictions                                 and you're solely relying on evaluation                                 metrics                                 like accuracy to determine if your model                                 is doing well or not                                 it can be easy to have unintended                                 consequences                                 some common household names for black                                 box models are                                 neural networks or gradient boosts or                                 ensembles these models give very high                                 accuracy                                 at the expense of not knowing their                                 inner machinations                                 beyond your naturally occurring black                                 box models                                 even some models that are technically                                 interpretable                                 are treated as black boxes and that's                                 because                                 no data scientist is looking at you know                                 a thousand different                                 inputs into a random forest model even                                 though                                 decision trees are classically labeled                                 as                                 interpretable models our problems and                                 our data are getting more complex                                 and data scientists have to rely heavily                                 on evaluation metrics                                 such as our accuracy for model building                                 and we're once again reminded that a                                 model and a human have different goals                                 models want to optimize their math                                 problem humans want to understand their                                 human problem                                 for some industries and especially                                 highly regulated industries                                 such as healthcare or financials it's                                 not                                 enough to just get the prediction the                                 model must                                 also justify how it came to the                                 prediction                                 explanations can help with this machine                                 learning models can only be                                 audited when they can be interpreted                                 explanations                                 can help with this too                                 so to recap up until now we know that                                 machine learning models have a different                                 goal than humans                                 we know that explainability comes after                                 thoughtfully engineered models                                 and we know that we need explainability                                 because of the                                 increasing complexity of models and the                                 increasing use of black box models                                 so i'm not an explainability sales                                 person even though it might sound like                                 it                                 and i also want to make sure that you                                 guys understand or you all                                 understand that you do not need these                                 every time                                 you create a model                                 first of all not all models need                                 explainability                                 some models are simple we saw this a few                                 slides back in y equals mx plus b                                 we don't need to add a compu complicated                                 computational effort if it's just not                                 needed                                 if we have really robust future                                 engineering or if we have very                                 um in-depth subject matter expertise                                 or if we just have a model that we can                                 fully explain                                 it's not worth our time                                 there's another limitation in use cases                                 not all use cases need explainability um                                 in all honesty sometimes you just                                 don't care um this is most noticeable in                                 some types of forecasting                                 or times where the output just isn't                                 really important enough                                 to justify the added complexity of these                                 algorithms                                 additionally there's some use cases                                 where the problem is very well studied                                 and feature interactions are just                                 overall                                 really well understood and so                                 explainability isn't needed there                                 finally you need to set very healthy                                 boundaries                                 with your explainability algorithms                                 explanations help                                 when you need to justify how a model                                 came to a prediction                                 explanations can expose bias                                 explanations cannot fix biased models                                 they do not make changes to the                                 underlying model or data                                 so you need to really remember what                                 explainability can and cannot do for you                                 all right we've gone through our fine                                 print for the limits of explainability                                 and it's time to look at an application                                 as a disclaimer um i don't work for a                                 healthcare company                                 and i really don't claim to be giving                                 sound medical advice                                 but in my free time i'm really                                 interested in the world of health care                                 and the hospital ecosystem                                 i think they have really interesting                                 problems that can really benefit from                                 machine learning especially                                 so i looked at this openly available                                 sample data from                                 mimic which is de-identified patient                                 data from a real hospital in the united                                 states                                 i was particularly interested in better                                 understanding                                 healthcare-acquired infections and these                                 are all                                 infections that patients are not                                 admitted with                                 but they contract from being in the                                 healthcare environment you know just                                 being in a hospital room and all the                                 bacteria that's moving around in there                                 and i'm focusing in on one healthcare                                 acquired infection called mrsa                                 this data has a lot of inputs on patient                                 demographics such as                                 age gender ethnicity as well as medical                                 information such as symptoms                                 diagnosis length of stay i did a pretty                                 vanilla feature engineering for this and                                 i made my model                                 it's a support vector classifier and                                 even though that the data set is                                 unbalanced it's for                                 it's about                                                                                                             um and we can say that my model is                                 performing                                 fairly well and it could be you know in                                 theory ready to be in production                                 putting in production in quotes because                                 this is a fake in production moment                                 um it's not a perfect example of a very                                 big                                 or highly complex data set or model                                 but this is our pseudo black box                                 the inputs are very wide and i can't see                                 exactly how the model is making these                                 decisions                                 so it's a good contender for my actual                                 absolute favorite use of explainers                                 which is the i'm just curious to see                                 what's happening here usage curiosity                                 can get you quite far                                 my next step in this problem is to look                                 at the contending algorithms                                 for explaining the prediction of these                                 infections                                 i do have to claim that all the examples                                 i use in the next slides are fictitious                                 and completely hypothetical but i want                                 to look at this model at a few different                                 levels                                 levels and the first level is at a                                 global scope                                 this is kind of your                                                    your                                 model understanding how the model itself                                 is making decisions                                 it can answer questions such as which                                 features are important and what's the                                 interaction between these features                                 next we have our local scope so                                 this level of explainability is all                                 about                                 understanding how the model has made a                                 decision                                 for a particular instance there are many                                 more                                 local explainers than global explainers                                 and that's partially because global                                 explainers                                 are very computationally complex and                                 therefore very                                 time consuming and very expensive to run                                 the lack of global explainers also                                 exists                                 because not all instances have the same                                 weights for each feature                                 so we can't generalize individual                                 explanations to the whole model                                 and that's kind of a mouthful to wrap                                 your brain around so maybe a                                 clearer way of saying this is in our                                 healthcare frame of mind                                 not all patients come in with the same                                 symptoms you wouldn't want to generalize                                 the symptoms of one patient to the                                 entire hospital                                 since you would lose your ability to                                 have a unique and accurate diagnosis                                 the same idea applies here you want to                                 diagnose a particular                                 instance and hope that looking at the                                 smaller                                 area will give better results and                                 explanations                                 to other similar data points that you                                 have                                 local explainers can answer questions                                 such as you know which features                                 factored most heavily into this                                 classification                                 and how would the prediction change if                                 certain features changed                                 so i'll first look at um some or a                                 global explainability algorithm                                 as a reminder this is your super high                                 level overview of how your model                                 is giving predictions and the first                                 uh algorithm we're looking at is called                                 shapley and the goal of shapley is to                                 explain the um or i guess contribute the                                 contribution                                 of each feature to the model so if we                                 were looking at this graph here                                 your shapley algorithm would give the                                 output that um                                 globally higher y values are light                                 purple                                 and higher x values are dark purple                                 in the very simplest terms shapley tells                                 me what inputs are important                                 and how important they are                                 so there's some pros and cons to                                 shackley and the first                                 pro that is really important is it's one                                 of the few algorithms that can be used                                 globally                                 um again not many algorithms are able to                                 be global                                 uh there are there is kind of like a                                 hack because                                 tree based models are kind of fast but                                 if you're not wandering through a random                                 forest it's going to take a long time to                                 train this                                 so i could see myself using shapley to                                 ask                                 questions to my model such as what are                                 the most important features my model is                                 using to predict mrsa                                 could it be age location of admission or                                 some sort of specific diagnosis                                 so if i'm looking for a less                                 computationally expensive                                 um explainability algorithm i might be                                 looking into more                                 local algorithms and                                 we'll look at a few different algorithms                                 but we're going to                                 start at and surprise yay                                 um shapley values can also be used                                 locally                                 the benefit of this approach is that                                 local versions of this algorithm are a                                 lot quicker                                 but the negative side hopefully                                 um is that this is the number one                                 favorite explainer for                                 our evil data scientists research has                                 shown                                 that is possible to hide bias                                 in shapley with adversarial attacks                                 this really isn't a big concern for                                 those who                                 are attempting to use this algorithm                                 truthfully                                 but it could cause mistrust with                                 interpreters                                 of shop explanations you'd always have                                 to walk into looking at a shackly                                 explanation                                 with the possibility that it's fake in                                 the back of your head                                 so whereas for the global shapley                                 explainer i would see what features were                                 most important                                 for the model in general or like the                                 hospital in general                                 i can use local shaft to see what                                 features were most important                                 for a specific patient to be flagged as                                 someone who would likely become                                 versa positive and again                                 shapley is still kind of expensive to                                 run it's still computationally                                 robust so for those looking for a less                                 computationally expensive technique                                 uh local interpretable model agnostic                                 explanations and that is such a mouthful                                 you understand why they shorten it to                                 lime                                 um is an algorithm where you assume                                 linearity                                 around a particular instance and create                                 a simpler model                                 the key intuition behind this one is                                 that                                 it's much easier to approximate a black                                 box model                                 by using a simple model locally in the                                 neighborhood of the prediction we want                                 to explain                                 as opposed to trying to approximate the                                 entire model                                 globally so it creates a local version                                 of a linear model                                 because y equals mx plus b is the gift                                 that your middle school teacher                                 just keeps giving in this case it's our                                 middle purple line                                 we might not have any idea of the                                 equation for our red line but we                                 understand and can                                 easily interpret y equals five x plus                                 one                                 um and this is really a great moment for                                 uh local explainability to be able to                                 interpret this                                 instance lime is also model agnostic                                 so it doesn't matter what your                                 underlying model is as long as you have                                 a prediction function                                 lime can be used however                                 there is some danger in using lime                                 because you don't know how far this                                 explanation holds                                 we can see that there's end points in                                 this image                                 but the fit is not always as                                 straightforward as this graph may                                 lead you to believe there is fear that                                 um you can be over confident in your                                 explanations in line                                 and you could have misleading                                 conclusions for                                 unseen but similar instances                                 anchors actually work a lot like lyme um                                 they both proxy local behavior of the                                 model in a linear way                                 but remember that line broke because we                                 didn't know                                 to what extent the explanation held up                                 and anchor kind of steps up to the plate                                 and                                 incorporates coverage i like to think of                                 this coverage as                                 going from a one-dimensional line                                 in lime to a two-dimensional area                                 which we can see since our small purple                                 line                                 has turned into a more robust rectangle                                 that's implementing                                 coverage anchors explain                                 individual predictions of any black box                                 classification model                                 by finding a decision rule or set of                                 features                                 or range of features that anchors                                 the prediction sufficiently so                                 everything that's within                                 this per light purple rectangle                                 would be classified as light purple                                 anchor's main selling point is that the                                 they                                 address the shortcomings of lime but the                                 boundaries                                 are still approximate and occasionally                                 don't even exist                                 in which case inkers become no different                                 than lime                                 um and they still carry the same pros                                 and cons                                 in the context of my mrsa exploration                                 i could see myself using anchors to see                                 what                                 ranges of different features factor into                                 a                                 mrsa positive prediction for example                                 an anchor might be able to tell me that                                 um                                 the output is like ages                                          are at a                                                                so lime shop and anchors all look                                 at what features are important and look                                 at interpretable models                                 for a particular data point but                                 contrastive explanation methods kind of                                 shift gears                                 and focus more on cause and effect so                                 you can test how your model output can                                 be                                 changed for each instance                                 contrast of explanation methods focus on                                 explaining                                 instances in terms of pertinent                                 positives and pertinent negatives                                 pertinent positives refer to features                                 that should be                                 minimally and sufficiently present to                                 predict                                 the same class as the original instance                                 and pertinent negatives identify which                                 features                                 should be minimally and necessarily                                 absent                                 from the instance in order to maintain                                 the original prediction class                                 kind of a mouthful but humans kind they                                 really think in                                 contrastive explanation methods                                 naturally                                 if i was trying to get my partner to                                 locate me at a restaurant                                 i might say that i have a hat on but i'm                                 not wearing red                                 if i wanted to have an explanation for                                 this graph my cem                                 algorithm would tell me that the star is                                 light                                 purple because y is greater than                                    but x is less than                                     well cem is useful for human                                 understanding                                 they get less useful when the classes                                 aren't similar                                 and this is kind of intuitive you can                                 distinguish                                 clear yet subtle differences between a                                 cat and a dog                                 but the differences between a cat and                                 the flu                                 are so numerous that it doesn't even                                 seem logical to list them all out                                 if my cem output is that all people who                                 contract mrsa                                 are admitted through the emergency room                                 but did not show                                 signs of respiratory distress the                                 location would be a pertinent positive                                 and symptoms would be a pertinent                                 negative                                 counter factual explanation is the                                 minimum possible change required                                 to generate the desired output so kind                                 of think if x had not occurred y would                                 not have occurred                                 in our example here if i wanted to                                 change my                                 light purple classification to dark                                 purple                                 the star would have had to move down at                                 least                                 that set amount on the positive side                                 counter factuals don't require access to                                 the data                                 or even the model they only need the                                 predict function to generate                                 outputs but the cons for counter                                 factuals are                                 similar to cems oftentimes there are                                 so many ways to change the output in                                 fact                                 there's so many ways to change the                                 output that it's no longer useful                                 to list all of them out a counter                                 factual that                                 i might uncover with a patient is maybe                                 someone                                 who was admitted through the emergency                                 room would not have acquired an                                 infection                                 if they had entered through urgent care                                 instead                                 and that's just a few of the many                                 possible algorithms                                 but i'm ready to test a few out and i                                 have just a small                                 sample here of the many different open                                 source python libraries that are                                 available                                 not all libraries have the functions we                                 went over                                 so it's good to do some research for                                 what best suits your needs                                 um if you see the link at the bottom                                 there's a really cool                                 uh github repo called ethical ml that                                 has a giant list of all these different                                 open source libraries you can use                                 and today in my demo i'm going to be                                 using alibi explain                                 since it contains a handful of                                 algorithms that i'm interested in                                 implementing                                 so let's hop over to my notebook                                 and just to give you a little bit of                                 background of where i'm at is                                 i'm using something called operate first                                 which                                 is an open source production grade                                 open shift environment so anyone who is                                 wanting to type in                                 odh.operatefirst.cloud                                 you're able to pull up you know your                                 jupiter notebooks and demo everything                                 here                                 you can clone my github repo and play                                 with my code yourself                                 and we can see here that i start out                                 doing my                                 super normal data science stuff i get to                                 import my libraries split into train and                                 test                                 sets i'm loading my model i'm fitting my                                 model                                 um building predict functions and i'm                                 going to be doing                                 shop to start out with and it'll be the                                 local version first                                 i'm going to take a pause here and first                                 of all it's my cause                                 um call to action if you want to see if                                 i'm an evil data scientist or not you                                 can play with my code yourself                                 but also we want to look at this i'm                                 only using                                                  and it's already warning me about having                                 slower run times                                 shap is quite slow but it's fun to use                                 and it's                                 interesting to interpret so once i fit                                 my                                 shop explainer we see it's coming from a                                 black box model                                 it's going to be doing some                                 classification and it'll be at a local                                 and global level there's also some other                                 parameters you're more than welcome to                                 play around with if you                                 desire and i'm not running this live                                 again                                 this is quite long to sit and watch                                 load so we're going to jump right into                                 the visualizations                                 and this is our local visualization and                                 how you would interpret this force plot                                 is we started our base value                                 if the final output is below the base                                 value                                 the patient would be mrsa negative if                                 the final value was above the base value                                 the patient is versa positive and we can                                 see that here                                 and each one of these bars is a feature                                 and                                 you can see there's uh the features are                                 so small on this side it doesn't even                                 give a name                                 but the larger the bar the more                                 important the feature was                                 and we're going to see that for                                 the classification of mrsa positive the                                 most important features                                 are ethnicity and admission location                                 and ethnicity and gender                                 and when i first ran this model i kind                                 of had some                                 data science red flags coming up i was                                 like                                 i had done pretty basic um feature                                 engineering i hadn't done anything                                 weird with my model so it didn't quite                                 make sense why ethnicity and gender were                                 three of the top four features for this                                 model's output                                 so i wanted to make sure that maybe this                                 was a fluke maybe this                                 is just the one patient i wanted to look                                 at a global level                                 and this is my shop output at a global                                 level                                 and we can see here that                                 each one of the points is a particular                                 instance or a particular patient                                 uh these are all the top                                 features the higher up on the list the                                 more important the feature was                                 for creating predictions um overall for                                 the model                                 and we see here that uh once again we                                 have ethnicity gender and ethnicity                                 and this was my accidental and now on                                 purpose                                 call to action to kind of check out your                                 models every now and then                                 because this was not what i had expected                                 if i was in the healthcare world i might                                 pull in a subject matter expert                                 to see if this was right as a non-expert                                 i wouldn't think that ethnicity and                                 gender are the most                                 important factors in receiving an                                 infection                                 but this is what my model's output was                                 so it's a great example for why                                 explainers can expose bias that was                                 completely unintended                                 my accuracy was incredible and it looked                                 like this model could be ready to be put                                 into production but my explainers say                                 slow down there take a pause and look at                                 what's happening with your data                                 so let's recap the last                                                  had together                                 so to recap models are complex                                 explainable machine learning is                                 necessary                                 to close the gap between the machine                                 goal of output                                 and the human goal of understanding and                                 we need explainability when it's not                                 enough to just get the prediction                                 the model must also justify how it came                                 to the prediction explainability helps                                 us understand                                 the why of models                                 to recap black boxes are models where                                 you input data and some magic occurs and                                 you get an                                 output they're really great for speed                                 running a data science workflow                                 and making really high performing models                                 they're less great when you're trying to                                 understand what's happening                                 in the some magic occurs portion of the                                 black box                                 explainability algorithms help crack                                 open                                 the black box and peer in to see how                                 decisions are being made                                 to recap we have algorithms that focus                                 on future importance                                 such as shapley which gets a gold star                                 for being                                 able to aggregate future importance both                                 locally and globally                                 we have lime which creates a simple and                                 easily understood y equals mx plus b                                 to explain a local instance and anchors                                 which does everything line does but                                 better                                 to recap we also learned about                                 algorithms                                 that take a look at the cause and effect                                 view on a model                                 such as contrastive explanation methods                                 and                                 counter factuals to recap                                 explainability algorithms are just                                 they're just really cool                                 and they can be very insightful when you                                 use them carefully                                 i hope you're all curious enough to try                                 an explainability algorithm or two on                                 your models                                 and i'm excited to hear about your new                                 insights and experiences                                 so thank you all for your time                                 yes isabel thank you for this great talk                                 i think that was very interesting a lot                                 of insights a lot to learn                                 um okay let me have a look                                 if there are any questions in our                                 questions tool uh yes there's someone in                                 the chat                                 andrew asks how to evaluate explanation                                 so that is a very good question um                                  there is a incredible book that i've                                  read like three times now that's                                  called interpretable machine learning um                                  and it's all online and i recommend                                  looking into that                                  a lot of it is you can implement                                  something called                                  trust scores which is kind of                                  explainability for explainability                                  and you bring up a really good point                                  that                                  it is kind of an interesting paradox                                  that we're using                                  black box explainability models to look                                  at                                  black box regular models                                  um so there's still a lot into                                  production this is fairly new                                  so i don't have a strong we look at                                  accuracy answer for you                                  but there are kind of additional steps                                  to look at the robustness of your                                  explanation                                  you
YouTube URL: https://www.youtube.com/watch?v=kUJuAuS_ais


