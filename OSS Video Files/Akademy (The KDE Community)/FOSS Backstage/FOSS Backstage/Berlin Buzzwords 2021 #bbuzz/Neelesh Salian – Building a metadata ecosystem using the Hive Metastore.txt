Title: Neelesh Salian – Building a metadata ecosystem using the Hive Metastore
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Metadata has been a key data infrastructure need since the beginning of our team's history at Stitch Fix.

We began this journey in 2015 with the setup of the Hive Metastore to work with Spark, Presto, and the rest of the platform infrastructure. But as our business needs grew, we felt the need to enhance and extend our metadata ecosystem.

In this talk, we want to share our journey of building additional capabilities with metadata to solve data and business challenges. Starting with our base infrastructure - the Hive Metastore, we will highlight each capability that led us to build the extensions into our present day metadata infrastructure. This includes improvements made to the Hive Metastore itself, extending the use of metadata beyond table schemas, and additional microservices we added to make access and use of metadata easier.

Building these capabilities has helped our team use metadata to power internal use cases. We want to share how we went about building this ecosystem and the lessons we learned along the way.

Speaker:
Neelesh Salian – https://2021.berlinbuzzwords.de/member/neelesh-salian

More: https://2021.berlinbuzzwords.de/session/building-metadata-ecosystem-using-hive-metastore
Captions: 
	                              hi i'm nilesh and i want to talk to you                               today about how we built a metadata                               ecosystem                               using the hive metastore at stitchfix                               a little bit about myself i'm a software                               engineer in the stitch fixes data                               platform team where i currently work                               with things like apache spark patchy                               hive                                and solving data problems for data                                scientists i used to work                                cloudera where i worked on spark and                                mapreduce                                and i'm a contributor to the apache                                software foundation for a few projects                                for a long time so here's what i want to                                talk to you about                                today just giving you an overview of                                what stitch fix is                                talk to you about metadata particularly                                how we see it and what's useful to us                                the hype meta store where we began and                                how we started off with                                understanding the problem of metadata                                and then                                talking about building a metadata                                ecosystem around it                                and then finally leaving you off with                                some of the learnings we had                                and the future work that's planned in                                this area                                so what is stitchfix what does the                                company do it is a personalized styling                                service                                we have two avenues of the business you                                create your style profile essentially                                tell                                tell us what your style is and on the                                top you can either                                get five hand-picked items sent to you                                and you can keep what you like                                and send back the rest or                                versus you can have a personalized                                curated store                                where you can check out whatever you                                like                                data science has been the backbone of                                what we do behind the scenes and so                                i'm part of the algorithms organization                                which has about                                                         and platform engineers                                these data scientists are mainly split                                into three verticals merge                                client and styling and the data platform                                sits horizontal to these                                organizations and this is                                this is embodied in the algorithms tour                                so feel free to check out when you get a                                chance to                                understand how data science is used                                behind the scenes                                jumping to the topic and giving you a                                bit more introduction i want to                                talk about metadata and what it means to                                us and                                start off there about the conversation                                about building an ecosystem                                so what does it look like essentially                                barebones definition would be                                data about your data and stored as a                                high level entity that can be consumed                                but in our case it's hive tables do we                                use that                                via the hive metastore to store                                structured data                                and what metadata holds for us is things                                like structure                                schema columns data types and location                                where in the underlying where is the                                underlying data located within the data                                warehouse                                and any additional contextual                                information about the entity                                that you might add                                so how do data scientists actually                                interact with metadata they                                they have two avenues to do that one is                                the rest server                                directly with the rest layer consuming                                metadata like looking at tables or                                interacting with whatever metadata is                                available                                or enter indirectly using it via engines                                so let's say you want to read a table                                right into a table via spark or presto                                so why is this important to us as a                                team in an organization and a company we                                we can read and write data and add                                metadata to it it gives meaning                                 and the path to creating metadata                                 essentially is sort of singular                                 the source of truth is consistent and                                 that's why the whole organization has                                 the same view of                                 all of the metadata and all our engines                                 are distributed frameworks everything                                 else                                 uses the same metastore to read metadata                                 each time                                 and we're seeing the same structure all                                 throughout the organization                                 and finally metadata is helpful to                                 document                                 certain entities help and even auditing                                 and help                                 source information for e-tails like uh                                 understanding where data lives and                                 how to get certain pieces of data                                 so jumping to the high metastore itself                                 like where where we started off and                                 uh i want to talk about some things that                                 it couldn't do for us                                 and uh that sort of gives you uh gives                                 you an idea of where we're going                                 so we chose the hive meta store the                                 metastore metastores the the                                 hypermetastore is the                                 portion within hive that allows you to                                 store and discover metadata                                 but we chose it for its compatibility                                 with things like apache spark and presto                                 it helps provide basic metadata storage                                 and discovery and it was easy to                                 stand up so we set up a standalone                                 hive metastore currently we're on the                                                                              with its own mysql database we use rds                                 behind the scenes                                 and i want to give you an idea of where                                 where it's actually positioned in our                                 infrastructure                                 so going left to right this you'll see                                 spark jobs notebooks                                 we have a pandas interface where data                                 scientists used to read and write data                                 so all of this reads and writes data                                 from the data warehouse                                 which is a combination of amazon s                                      the actual files                                 and the high metastore that we talked                                 about at the center where                                 metadata is stored about the data stored                                 in s                                  presto is used to only read and kafka                                 the event bus                                 is writing data into the data warehouse                                 as well and storing it as high level                                 tables                                 hive tables that are consumable by etls                                 so where does this fall short it's a                                 useful tool but we found                                 being a bit limited and these limited                                 limitations can be categorized as i                                 think of them as absence of abstraction                                 a bit more                                 lack of stability and uh finally                                 difficulty to                                 sort of customize for any additional                                 patterns than non-traditional patterns                                 and                                 the ones that we need so talking about                                 abstraction                                 we since we were only using the meta                                 store from hive we had only thrift                                 as an interface to use so we definitely                                 needed an abstraction to use the                                 metastore                                 we couldn't expect data scientists for                                 our customers to read create and write                                 automated data without a sort of layer                                 of abstraction and so                                 we had to build something on our own                                 rather than depend on something that was                                 already available with stability                                 larger deletions and updates were sort                                 of concerning                                 and uh some of the deletions took um                                 took a concerning look because we we                                 observed database locking that happened                                 into some of those deletions                                 we um we observed polling and repeated                                 reads for certain                                 objects in the meta store and so that                                 was also a concern                                 and some recalls were observably more                                 faster if they go directly to the                                 database and so we observed that the                                 thrift layer itself was a bit slower and                                 less stable for some of these larger                                 method calls                                 and finally the third limitation the                                 support for custom patterns                                 we had specific business cases that                                 couldn't be directly solved by the                                 metastore                                 i'll address them in the upcoming slides                                 but to give you an idea that we didn't                                 want to address these by                                 either forking high or patching it and                                 changing its structure we'd be sort of                                 hurting ourselves with a lot of the                                 patches that we had to do                                 and we didn't want to create something                                 that is specific and out of the box that                                 might be                                 breaking compatibility with spark or                                 presto                                 eventually but despite this                                 with these limitations didn't stop us                                 from using the metastore we thought                                 let's just build something around it to                                 address these limitations uh since the                                 metastore is more hive and be like we                                 ended up naming most of our services                                 somewhat be like and more appearing                                 in nature so coming to the actual                                 building of the                                 the metadata ecosystem i want to give                                 you an overview of what it is                                 and i'll take you to each part and talk                                 about why we built each of them                                 so this is how it stands today this is                                 how it looks going from left to right                                 you have the user facing layer where                                 clients are used by data scientists                                 and those go into the back end layer be                                 it bumblebee and i'll talk about each of                                 those                                 red uh red marked boxes because those                                 are the                                 the key elements uh we have spark emr we                                 have presto clusters                                 and these go into our metastar proxy and                                 then finally towards the extreme right                                 you see the                                 standalone high metastore that i spoke                                 about and that there's a kafka topic                                 emitting                                 events from there i'll talk about each                                 of these things but i wanted to give you                                 an overview at the beginning                                 so let's look at how we tackled                                 specifically abstraction and stability                                 so data scientists needed more                                 expressiveness with metadata like actual                                 crud operations like create                                 read any kind of update delete for                                 metadata and how do we do that so that                                 was sort of a                                 question mark in our ecosystem at the                                 beginning since the metastore stood                                 alone                                 and uh it didn't have sort of this                                 abstraction layer                                 and so what we needed was a layer that                                 wraps the class structure                                 of hive table like table database                                 partitions the basic metadata objects                                 and essentially we needed a readily                                 accessible api                                 and a python client some python is a the                                 default language for our data scientist                                 teams                                 and this wanted to we wanted to allow                                 them more expressiveness with metadata                                 and especially one other thing was                                 making metadata available as a first                                 class field so the hive table would have                                 a sort of generic metadata field to add                                 additional                                 data that would come from data                                 scientists specifically for their needs                                 with regards to stability uh we noticed                                 the hive metastore couldn't handle all                                 the requests like we couldn't                                 bombard it directly with all the                                 department's needs                                 yes it's the source of truth but we                                 needed some sort of layering before it                                 we had to handle                                 requests with spark presto dashboards                                 and things like that so we needed sort                                 of this layer to handle some of those                                 requests                                 a caching mechanism was sought for uh                                 thought to be                                 the easier solution to relieve some of                                 those repeated calls                                 and for some method calls they were                                 observably larger and we needed to                                 bypass thrift to go directly to the                                 database                                 so enter bumblebee like you see the be                                 naming that's coming here                                 it's a rest server with its python                                 client and what we did was                                 first of all abstracted the hive classes                                 and                                 to make it more convenient for usage we                                 returned the python dictionaries to                                 represent them                                 so each of the table database partition                                 would all be represented as python                                 dictionaries                                 and like i said metadata was importantly                                 needed to be a structure within the hive                                 table and so                                 that again became a python dictionary                                 within a table for any auxiliary                                 information                                 that you might need to add with respect                                 to stability like i mentioned some of                                 those calls were                                 things like get partitions get all                                 tables which are more heavier                                 and it made sense to revert them into                                 making it                                 read for mysql and so                                 that was done for those particular calls                                 especially                                 and finally bumblebee added a tiny cache                                 that allowed                                 quicker loads of table objects that were                                 frequently accessed so some of the                                 production tables were heavy hit and so                                 this cache sort of relieved that                                 pressure of reading                                 a repeated object                                 so the this is how it looks the                                 bumblebee server essentially talks to                                 the metastore directly                                 and it makes separate calls to the my                                 sequel database for some operations like                                 i mentioned                                 and to the bottom the bumblebee client                                 had                                 the ability to crud operations on hive                                 artifacts set metadata                                 and i'll talk about the ownership bit in                                 a bit but uh it allowed                                 data scientists to do that as well by                                 interacting with the server                                 so in a python job this was how you you                                 would use bumblebee                                 you would import the client and try to                                 get an object and that's what's returned                                 to you                                 as a python dictionary like i mentioned                                 so to access each of this information                                 you would just do                                 a key search on the the object return                                 and you would                                 get returned the um the need the object                                 that you need like metadata column                                 or name itself so                                 we have empowered the data scientists                                 with metadata but                                 like uncle ben and spider-man said with                                 great metadata comes great                                 responsibilities and so                                 it it made sense to sort of keep these                                 things in                                 in balance and so we had to think a                                 little bit more beyond                                 just expressiveness so we started off                                 with the idea of ownership                                 and making sure that data scientists                                 owned each artifact that they create                                 and so the bumblebee client was sort of                                 augmented to set these ownerships on                                 hive tables                                 and data scientists had to declare this                                 ownership widely create and                                 to indicate ownership of a table but we                                 needed to create maintain a clean                                 metadata ecosystem and not just like                                 having                                 own tables but sort of an accountability                                 of why tables existed                                 so clearly ownership wasn't enough we                                 had we needed to have more context and                                 more purposeful metadata so we asked                                 ourselves                                 questions like why this table exists can                                 we declare its purpose what is its what                                 is it used for                                 what code writes to this table can we                                 clean this table up if it's not                                 referenced or not used                                 and finally we we came up with these                                 questions as a                                 sort of a workflow to add additional                                 metadata to hive tables so that we                                 we come to a point where we know why a                                 table exists                                 in in the meta store and it's not just                                 there for uh for no reason and so                                 this workflow is really helpful to                                 address that                                 and this is how it looks like you create                                 a hive table                                 a metadata service would notify the                                 table owner and they go wrong they go                                 with two rounds of metadata update                                 so in the first round uh the owner would                                 essentially                                 link the table code add comments check                                 schema or any kind of                                 sort of auditing work of the of the                                 table itself                                 and pass it on to another team member to                                 confirm it and to just sort of uh                                 have a second look at whatever the                                 metadata is included                                 and then they could either reject it or                                 notify it                                 and say hey you could change something                                 or they could confirm it and then                                 finally the metadata itself is updated                                 on the table and so now we have this                                 complete picture of why this table                                 exists                                 so this is where increased metadata                                 fit in like the metadata object i                                 mentioned in the hive table                                 is where all these pieces fit together                                 so when it was confirmed                                 let's say the review date and the table                                 code                                 and the owner so there's sort of this                                 accountability of                                 why this table exists and surprise                                 surprise it's a table that stores data                                 about clothes for an apparel company                                 but that's um that's how we sort of gave                                 this notion of uh                                 why uh giving a purpose for a table with                                 like things like comments                                 as simple as that and so we built a                                 service around this                                 increased metadata that we already have                                 now uh to                                 allow us to check if the table has been                                 referenced if there's jobs connected to                                 it if it's been right written to and                                 that's more complicated behind the                                 scenes but                                 the idea is that it helped us clean up a                                 lot of those unused tables                                 so now we talk about uh the third                                 uh and final limitation that we observed                                 essentially building custom patterns                                 and support for them in our ecosystem                                 so solving these um these are                                 non-traditional ones we observe these as                                 we went and grew as a business and so i                                 want to talk about what                                 what these two patterns were they sort                                 of limited the                                 related with the solutions that we came                                 up with and so                                 i sort of clubbed them together in terms                                 of the the need and the                                 the sort of piece of the infrastructure                                 it generated                                 the first one is essentially test data                                 test data was coming in                                 with regular data and was stored in the                                 data warehouse so think of like sample                                 client                                 or any kind of testing you do with any                                 kind of table                                 and that passed on into a production                                 table but we didn't want this                                 test data to essentially create pro                                 training for our algorithms                                 so we needed a way that this test data                                 would be isolated                                 from within a hive table so if you read                                 a table                                 not you would not necessarily get you                                 shouldn't necessarily get something with                                 the test data                                 and so we had to isolate that in a way                                 that it was not                                 hurting some of the algorithms and on                                 the second one                                 we observed and this was a pattern for                                 growing up for a while that                                 data scientists would create table views                                 essentially tables that were mere                                 pointers pointers to a tables partition                                 so let's say a historical table has been                                 storing data every day                                 partitioned by date so a table view                                 is something like a table that just                                 points to whatever the latest partition                                 of that table is                                 and so these tables became abundant and                                 the the cumbersome part                                 was to actually having to update these                                 tables each time a new partition was                                 created                                 so if the historical table wrote data                                 for yesterday                                 you would be updating uh your view table                                 to act accordingly and point to the new                                 partition                                 and so it's sort of this extra piece of                                 workflow that was                                 uh becoming cumbersome and unnecessary                                 so how do we solve this we couldn't                                 essentially modify bumblebee it was it                                 was something that needed to come from                                 the meta store                                 it would take a lot more work if we went                                 through the hive route and patching the                                 binaries                                 and we sort of needed them in our                                 thinking that                                 we needed something like a metastore an                                 interface that was both compatible                                 which was important in spark and presto                                 but it was sort of malleable and                                 changeable                                 in our way that it functions according                                 to our needs                                 and so we thought of this sort of                                 interac indirection layer that                                 worked behind the scenes and we sort of                                 thought about thought of it as a proxy                                 metastore i'll explain it that                                 a bit                                 so enter yellowjacket a bit of a                                 superior insect if you will                                 but we started off with to experiment                                 with basic thrift code                                 to see if we can route traffic uh from                                 the meta store                                 and act as an intermediary between                                 uh our back end and a meta store and so                                 every service now thought of this proxy                                 as the meta store so to begin with we                                 wanted to check if the methods were                                 working and so we                                 checked this with both spark and presto                                 and they were both fine since we                                 supported the method methods available                                 so we call it yellowjacket a thrift                                 server that essentially proxies the load                                 balancer of the hype metastore                                 and uses its database for internal                                 queries                                 and what it did was it supported all the                                 queries                                 uh that are available in all the methods                                 sorry in the                                 in the high metastore thrift layer so                                 anything requests coming from spark or                                 presto or bumblebee or any any other                                 metadata service you would have                                 it would get resolved because it's                                 essentially the similar layer like the                                 um like the meta store itself so now                                 given this flexibility of having these                                 method calls right in front of us we                                 could overload these methods                                 to adjust to our needs and be suitable                                 to some of these patterns                                 so this is where it fit right at the                                 center yellow jacket uh                                 you can see this is the back end layer                                 coming from that earlier                                 diagram i showed you with the bumblebee                                 server the emr cluster                                 the presto cluster all pointing to                                 yellowjacket as if it were the meta                                 store                                 but yellow jacket would talk to a                                 standalone metastore for all its calls                                 and would call the db for certain uh                                 aspects of the                                 test data and the view solution and i'll                                 talk about that in a bit                                 and so this ended up becoming sort of                                 the meta store                                 where every service would hit and the                                 standalone method                                 uh standalone metastore would be left                                 alone                                 what it helped here is uh in addition we                                 could upgrade things like the meta store                                 we came from one to one one two two to                                 two three x                                 fairly recently and so yellow yellow                                 jacket was                                 uh key in that upgrade process as well                                 so isolating um test data is the first                                 solution that we tackled                                 uh let's say we made this part of the                                 workflow that if a high table                                 was expecting or had expected test data                                 the user                                 who was creating the table would have to                                 create a test data partition                                 so just a partition column that was sort                                 of towards the end                                 and specifically we wrote one for the                                 presence of test data in a record and                                 zero for the absence of test data in a                                 record so                                 if a row had zero in a test data column                                 it wouldn't have test data it was more                                 production and one would be                                 classifying it as test data and behind                                 the scenes what we did was                                 uh we hid this partition columns this                                 column particularly so that we                                 know that um we can hide test data when                                 needed                                 and we did this by overloading the get                                 table method and adding a decorator so                                 if the name included a sort of pattern                                 like like i should                                 like it's listed below the double                                 underscore include test                                 data either yellowjacket method would                                 know that you're asking explicitly for                                 test data and it would                                 react as a react the appropriate way                                 i'll give you an example to highlight                                 this but this decorator pattern helped                                 us                                 um save a lot of pain while                                 doing this sort of isolation so let's                                 look at the table                                 test table with a test data partition                                 so if you read this table naturally you                                 would get always the production data so                                 yellowjacket will just think of it as                                 a regular table and would the                                 indirection would just return                                 all test data equals zero because you're                                 not                                 explicitly asking for it and if you're                                 asking for test data you would                                 do the decorator pattern like include                                 test data                                 and or explicitly set test data equals                                   in a sql                                 filter clause and yellowjacket would                                 know behind the scenes given the name                                 and using just by the name that it would                                 surface the test data equals one                                 data so this indirection was really                                 useful to sort of isolate                                 test data from production data and so                                 anybody who doesn't even know the                                 structure would read the table as it is                                 would always get production but let's                                 say somebody explicitly wants test data                                 then they can query it in this way                                 so the view solution was also something                                 we addressed                                 which we thought of let's just automate                                 that and make it make it more easier for                                 data scientists                                 so an example of that is let's look at                                 test table again which has                                 a date partition and i told you this                                 would be a pattern where historical data                                 stored date wise would be                                 made into a pointer table and so that                                 was the the                                 use case that we tried to solve and so                                 let's say you were addressing that today                                 yesterday's data would be latest for                                 today morning and so                                 um looking at that instead of having a                                 separate table to address this and                                 represent this latest data all we did                                 was modify the get table to return a                                 quote-unquote view                                 which is a decorator pattern that we                                 used for                                 exhibiting the view of a table so view                                 would be the latest numerical partition                                 of a table um                                 and it would be auto-generated when                                 required and so you don't need to                                 actually specify                                 or update or do anything this view                                 doesn't exist in the meta store as an                                 entity                                 and it would be generated via yellow                                 jacket when requested and so essentially                                 we would take the                                 the historical table and present the                                 latest partition                                 as um as the table so this indirection                                 was really useful to solve this uh use                                 case                                 and in spark and presto we did it in                                 such a way that with the methods behind                                 the scenes                                 we had to adjust a few things and so                                 test                                 you could do test table underscoring                                 school view and read the                                 latest data available to you so you                                 didn't need any updating happening                                 and the same goes for test data it was                                 uh same one                                 spark and presto as well so you'd ask                                 like how do we track this ecosystem it's                                 a                                 it's a large ecosystem with things like                                 abstraction there's a direction layer                                 now                                 and metadata operations were happening                                 everywhere and so tracking this                                 needed more context just by just by not                                 by simple logging                                 so to understand context like what                                 operation was done                                 maybe a name or type when it was done                                 where did it come from did it come from                                 spark or presto or bumblebee                                 who did it was was it some team or user                                 who had performed it                                 and so tracking this was became a real                                 concern                                 so we called it buzzcom a cheeky name                                 for something like                                 think of a telecom network within a hive                                 and so                                 we observed that the metastore itself                                 came with its event listener                                 and had to support free operations                                 that fired off events so you would                                 you could produce them in a log and get                                 an idea of which events actually                                 happened                                 so but it didn't it didn't have an extra                                 context that was needed in our case for                                 understanding more about the operations                                 so we observed that every                                 create add drop delete method for either                                 table database or partition had                                 something equivalent of a method within                                 environment context                                 it's literally called that and so we                                 noticed that this                                 environment context object had a sort of                                 map structure called properties                                 which was sort of open-ended and we                                 could pass any information we wanted                                 and so we we decided to use those behind                                 the scenes and we patched the ones                                 that were missing we we noticed that                                 altered database was not an event so we                                 added that explicitly                                 and we published this uh these events to                                 a kafka topic so it ends up being more                                 consumable than just writing it                                 into a log and so we parsed that we                                 cleaned up the object screened up the                                 structure and passed it into a topic so                                 it becomes consumable                                 downstream by any sinks written on kafka                                 and so we had to patch spark and hive in                                 our libraries to                                 and bumblebee to use this but uh all in                                 all it ended up                                 using uh this environment context method                                 and                                 passing additional information uh that                                 was useful                                 so finally when you do pass this all the                                 towards the end you would record an                                 event something like this                                 so create database would be the                                 operation source                                 an id to trace it back to where it came                                 from so bumblebee's id would be stored                                 the operation timestamp to indicate the                                 timing of the                                 the operation and finally the source uh                                 which is bumblebee and in other cases                                 would be spark or something else                                 and there's more information in other                                 kinds of events but i wanted to give you                                 an example so things like add partition                                 would store which keys and values were                                 actually added                                 and altered table would store both the                                 new and the old structure so                                 things like that so                                 coming coming finally to the learnings                                 in the future work this was                                 sort of a multi-year effort of building                                 this ecosystem and i wanted to share                                 some of the learnings that                                 came along the way for us and the future                                 work that's planned                                 so what did we learn here though getting                                 essentially the main                                 purpose of getting these things were                                 useful for our business but                                 compatibility with spartan presto or                                 essential and so a lot of testing was                                 done while                                 rolling these out with bumblebee's inter                                 introduction it meant                                 a lot of documentation a lot of helping                                 data scientists to migrate                                 to use this new interface to manage                                 metadata so that was                                 we observed a lot of pull requests                                 everywhere and so                                 um that was a that was a big change in                                 the team and so migration of code was                                 uh took a lot of a lot of effort with                                 yellowjacket it seemed more behind the                                 scenes but                                 specifically challenging for us to                                 implement because we noticed that spark                                 and presto interacted differently with                                 the metastore so i'll give you an                                 example like                                 get partitions is called different                                 methods by                                 different methods are called whether                                 you're coming from spark or whether                                 they're coming from presto so                                 some of those need to be needed to be                                 adjusted and so testing that was also                                 uh quite challenging and finally we                                 wouldn't be anywhere without alarms and                                 alerts but                                 they saved us from disasters and they                                 still continue to do so and so                                 that's something we've always learned                                 and kept behind the uh                                 at the back of our heads to make sure                                 that we set up these things                                 more accurately and finally looking                                 ahead                                 uh this effort with dedicated logging                                 now in place                                 with buzzcom we have eventually a plan                                 to sort of                                 trigger workflows based on metadata                                 changes or any kind of availability of                                 data                                 and so that's something that's being                                 worked on a recovery mechanism for hive                                 is being worked on as well where                                 disaster strikes if something goes wrong                                 using some of this logging that we have                                 now so that we can replace some of the                                 events to understand that we                                 what happened at the time of let's say a                                 crash                                 and i i gave a talk at data ai summit                                 about data quality so feel free to have                                 a look if you                                 if you get a chance but uh i want to                                 mention that we are doing data quality                                 powered by metadata in the tables and so                                 you can declare tests and set rules for                                 tests                                 in metadata and we have a mechanism to                                 read that and perform tests and so                                 that's been expanding                                 as an effort within the company as well                                 so that's uh that's something that                                 metadata has proven to be useful for                                 as well to summarize                                 metadata has been important for us at                                 stitch fix we began with the metastore                                 but we understood some of the                                 limitations                                 that made us expand it into a more                                 full-fledged ecosystem to improve                                 abstraction                                 uh the first one of the limitations we                                 built a rest server and a client                                 that allowed data scientists to be more                                 control of the metadata                                 and give them more expressiveness we                                 added caching additional optimizations                                 to help keep the metastore more stable                                 especially for larger calls or repeated                                 calls specifically                                 and finally we added a proxy meta store                                 that allowed us the ability to                                 provide indirection and support for some                                 business logic that was not                                 while maintaining sort of compatibility                                 with spark and presto and solving the                                 problems that we had                                 so that's all from me thank you happy to                                 take any questions                                 you
YouTube URL: https://www.youtube.com/watch?v=xXwxM6ydDD8


