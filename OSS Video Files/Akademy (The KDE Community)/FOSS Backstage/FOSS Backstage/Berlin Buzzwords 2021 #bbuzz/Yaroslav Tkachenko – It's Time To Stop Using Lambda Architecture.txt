Title: Yaroslav Tkachenko – It's Time To Stop Using Lambda Architecture
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Lambda Architecture has been a common way to build data pipelines for a long time, despite difficulties in maintaining two complex systems. An alternative, Kappa Architecture, was proposed in 2014, but many companies are still reluctant to switch to Kappa. And there is a reason for that: even though Kappa generally provides a simpler design and similar or lower latency, there are a lot of practical challenges in areas like exactly-once delivery, late-arriving data, historical backfill and reprocessing.

In this talk, I want to show how you can solve those challenges by embracing Apache Kafka as a foundation of your data pipeline and leveraging modern stream-processing frameworks like Apache Flink.

Speaker:
Yaroslav Tkachenko – https://2021.berlinbuzzwords.de/session/its-time-stop-using-lambda-architecture

More: https://2021.berlinbuzzwords.de/session/its-time-stop-using-lambda-architecture
Captions: 
	                              uh thank you for coming                               um my name is yaroslav yeah and i'm                               studying internet shopify and i like                               moving things from batch to streaming a                               lot                               and i've been doing this over the last                               few years and today i just wanted to                               uh share uh some of the things i learned                               i hope it's going to be useful                                right but you know before we start just                                a quick                                uh reminder about the lambda                                architecture you know this is one of the                                very classic uh definitions you can find                                um basically about you know                                             when                                you know people came up with this idea                                we didn't have                                a lot of good streaming technologies                                right so batch rolled the world                                um batch technologies were you know very                                reliable                                scalable um and so in this kind of                                architecture                                you implement a batch layer                                with different batch transformations but                                to compensate for the very high latency                                because it can you know                                it can run for a few hours for example                                you know we introduced some kind of                                speed layer                                and then you combine those in various                                different ways                                um and as you can see there's a lot of                                complexity in this kind of                                architecture you know you need to come                                up with two different systems and                                maintain them separately                                and then you need to reconcile so this                                is quite                                complex uh nevertheless you know a lot                                of companies                                used it over the years and it's still                                still in use                                but i think we can do better right and                                i also want to share some of the things                                i saw you know when maybe um this is not                                a                                full land architecture but still people                                use badge in places where you can avoid                                you know for example uh maybe you have a                                streaming job                                and you write a lot of data some kind of                                object store and then occasionally you                                know you detect some bad data                                and you end up using a batch job to go                                and fix it you know maybe go back                                in history another very common use case                                uh maybe you're writing a bunch of files                                in object store                                and uh the small files problem is uh you                                know                                very very common one to have when you                                something like spark or presto                                to query those uh files so you can come                                up with a batch job                                you know to optimize this uh you know                                file size for you to compact those small                                files and this is something that's very                                common as well                                or you know classic example uh                                historical reprocessing like                                something's changing in business logic                                and you need to go back in time and                                basically reprocess                                all data right so i i see all those                                slanted incarnations all those uh                                different                                uh bad use cases all over the place                                and i think again we can do better um so                                cup architecture                                is a very simple idea right you                                essentially take the lambda architecture                                you drop the badge and that you know                                what's left is a gap architect right you                                rely on streaming                                quite a bit and your streaming layer                                basically handles all the new data                                that's coming you have some kind of data                                store                                where you write your results and you                                query the data store right so                                again very similar uh just uh really                                focus on streaming as                                a first-class citizen and rely on                                streaming in all those                                interesting situations right and so                                very very typical concerns that people                                uh                                 you know come up with when you talk                                 about kappa                                 you know and first one is data                                 availability and retention right so                                 maybe use something like apache kafka as                                 your                                 uh streaming platform and maybe you have                                 a default                                 seven days retention for a bunch of                                 kafka topics uh so when the data                                 is gone you know you can't really do                                 much you can't go back to history                                 um what do you do right so that that's a                                 that's a problem                                 data consistency is still sometime uh                                 you know you can hear people say oh this                                 is streaming                                 so you maybe expect some duplicates or                                 even like missing data                                 um and we'll talk more about that today                                 um handling driving data is also very                                 common topic                                 right um and batch and streaming you                                 know can can                                 i can do it differently depending on the                                 variety of different use cases um and                                 finally i think this is                                 also something that's very common you                                 know how do you reprocess how do you                                 backfill data                                 like if you have truly streaming system                                 end-to-end no batch whatsoever                                 how do you go back and like reprocess                                 years of data potentially in a                                 in a reliable and reasonable way right                                 so                                 all those concerns something we want to                                 try to address but before we continue                                 i just want to share you know my                                 thoughts why do i like streaming so much                                 and typically when you compare batch and                                 streaming people think about latency                                 right and it's it's a nice one but it's                                 not the main goal                                 right because you never know how much                                 latency is okay                                 like imagine you have this batch job                                 that's running every                                          and you decide to transition to a                                 streaming system                                 you know uh full kappa what is                                 enough latency or what is a good latency                                 here                                 it's essentially between                                              zero but it can't really say                                 what's fine right so yes decreasing                                 latency is good                                 but i don't think it's the main benefit                                 and uh late driving data is something                                 that                                 actually can be very simple with                                 streaming depending on the use case                                 because with batch usually you're on                                 some kind of incremental batch job maybe                                 every day                                 maybe every few hours and if you have                                 light driving data                                 the typical approach is to just wait                                 longer right so maybe                                 your daily badge starts at uh                                          and you allow this                                                     and it's very common                                 and it's not great because you rarely                                 have that guarantee that all                                 late data will arrive in this four hour                                 window so                                 occasionally it actually arrives later                                 and then you need to go back and                                 reprocess                                 and how much time is it okay like can                                 they go back and we process many days                                 a week more than that all those                                 questions arise and                                 it's just not clear you know with                                 streaming                                 if you for example have stainless                                 transformations and you have                                 your sinks or destinations that support                                 updates that's extremely easy right                                 because                                 no matter how old is the message or the                                 event the                                 piece of data that you receive you know                                 you can always shout it to that                                 historical uh you know partition or date                                 or whatever                                 like that location um and it's gonna be                                 fine                                 right because you can just update that                                 historical um                                 circle um uh record right                                 with stateful transformations it's a bit                                 more tricky right because it depends on                                 how exactly you state how long they keep                                 it how how do you expire it can you                                 recover state somehow                                 and small state is general generally                                 fine but                                 uh the larger the state becomes it can                                 be tricky                                 and if you have uh you know destinations                                 in your data pipeline that don't support                                 updates                                 at all so for example you just store a                                 lot of porky files on s                                  you know this is not something you can                                 easily update from a streaming job you                                 know                                 this this depends so we'll talk more                                 about this as well but i think the main                                 reason why you would like to use                                 streaming over badge nowadays                                 is just operations and observability and                                 mentality around it                                 right with badge it's fine when things                                 fail you know just wait another six                                 hours and hopefully you know the retry                                 drug will fix it                                 if not we might retry one more time or                                 we'll start investigating                                 or maybe someone disabled the wrong job                                 and because you don't really have good                                 monitoring in place                                 or you know good alerts for a bad job                                 you know                                 nobody noticed um and maybe you don't                                 really have good metrics                                 like how much metrics do you instrument                                 for a batch job                                 um and with streaming you know when you                                 use modern frameworks like kafka streams                                 apache link and you deploy it on                                 kubernetes                                 uh you change your mentality right                                 because now you start treating those                                 as real like normal applications                                 web applications perhaps right the                                 server real traffic                                 and you start thinking about uptime                                 expectations and slos and                                 you can fully embrace all the things                                 around say cd and observability                                 all this all these good practices right                                 so i think what's important to keep in                                 mind                                 it's not just latency right it's that                                 mentality that                                 uh allows you to treat your uh data                                 processing applications as                                 you know normal applications or certain                                 traffic and that's quite important                                 if you want to have a reliable data                                 pipeline                                 so now i want to cover a few different                                 building blocks uh that you would need                                 for kappa                                 and i'm going to focus on three key                                 areas here so i'm going to focus on the                                 actual log abstraction or the streaming                                 platform                                 in in my case apache kafka um i'm going                                 to focus                                 on the streaming framework of link and                                 just                                 uh share a little bit about the data                                 syncs or destinations and i'm going to                                 share a few things about iceberg about                                 apache iceberg                                 so uh let's start with kafka and the                                 very basic building block that you might                                 require                                 is kafkatopic compaction you know this                                 is something that's                                 pretty common nowadays people use it                                 quite a bit                                 um and this uh allows you to essentially                                 compact your history in a in a kafka                                 topic and only keep the latest version                                 if you use a message key right so by                                 that message key                                 you can essentially come back to the                                 historical updates and this                                 enables infinite retention right so if                                 you can use compaction                                 um this allows you to keep your data in                                 kafka forever and this is kind of one of                                 the prerequisites for                                 true copper architecture                                 alternative approach would be to use                                 kafka tiered search                                 and this is something that still work in                                 progress there's a kafka improvement                                 proposal for that                                 and you know in theory this feature                                 allows you to                                 keep uh hot data in kafka and call data                                 in an object store like s                                  but from a consumer perspective you                                 still interact with a single kafka topic                                 and all the data movement happens in the                                 background right so if                                 you start consuming from the earliest                                 offset uh kafka                                 will know how to get the data from s                                  visualize it                                 send it your way right and um you know                                 that's still working progress but people                                 have been using this topic archive                                 pattern                                 for years where you simply do that                                 yourself right and then either                                 you consume from kafka ns                                          in parallel have a union of two                                 different sources                                 or you recover data from s                                              to a kafka top you can consume from                                 there                                 but this also enables that instant                                 retention because the object store is                                 very cheap                                 comparing to storing all the data in                                 kafka so this is something you you might                                 need as well                                 um kafka has introduced transactions                                 about four years ago                                 uh so yes this uh syntax on the right is                                 available to you                                 if you want to use um you know with the                                 kafka producer um                                 and yeah it's been introduced quite a                                 bit uh ago                                 uh                                                                 uh you can eliminate duplicates right                                 and this ensures your data consistency                                 uh those transaction transactions um                                 need to work                                 uh with the consumer itself like                                 consumer will need to                                 tweak a few different parameters on                                 their site to understand you know if                                 the message is fully committed or not                                 within the transaction                                 and it also needs some kafka broker                                 tuning but not that much and                                 you know you can just start using it                                 it's been it's been                                 it's been used for years now                                 another very useful building block is                                 some form of data integration                                 that can bring all types of data to                                 kafka right and this solves this                                 but i don't have this data in kafka                                 question                                 um you can use frameworks like kafka                                 connect for example                                 to bring the data in a very                                 uh sort of unified fashion                                 it's important to avoid building one-off                                 integrations                                 because this can get messy and                                 unmanageable very quickly                                 but if you something like kafka connect                                 or some other um                                 similar tools you have essentially a                                 platform for                                 integrating kafka with your sources but                                 as well as syncs right so if you need to                                 send data to different destinations                                 kafka connect can help there as well                                 uh finally here um in order to                                 reproduce um sorry in order to reprocess                                 a lot of data uh with kafka you might                                 need to have a pretty large cluster                                 right that that                                 historical reprocessing is is a tricky                                 uh tricky concept                                 and you know uh if you ever managed                                 kafka you know                                 that scaling kafka is hard like when you                                 increase number of workers in a cluster                                 it needs to do a lot of reshuffling and                                 it can take sour seven days                                 so there is this idea of essentially                                 treating kafka clusters as immutable                                 entities and scaling your capacity by                                 adding more clusters right and                                 this is something that netflix has done                                 a few years ago um                                 this is pretty advanced i would say                                 because you would need                                 to uh you know implement producers and                                 consumers certain way                                 like they would need to discover new                                 clusters and understand how to route                                 data between                                 multiple uh clusters but if you can't do                                 it                                 uh you have this elasticity now where                                 you know for a big historical                                 reprocessing                                 you might wanna you know add a lot the                                 capacity                                 quickly go through the backlog maybe in                                 a few hours                                 and shut down all the clusters you don't                                 need and that happens very quickly                                 so this is pretty aspirational i would                                 say but if you can get it right                                 it's going to be a huge huge benefit                                 so uh now switching to the streaming                                 engine um and one of the mandatory                                 things you need                                 for any kind of complex copper's case is                                 reliable and scalable state                                 as a part of the streaming engine and um                                 you know with a patch of link you have                                 access to a keyed state                                 which is extremely useful concept                                 because uh it allows you to essentially                                 scale your state                                 you know um not forever but to a very                                 large amount                                 uh just because uh all the state is                                 partitioned by definition right                                 you use some some kind of key to                                 partition your state                                 and then you would uh you know have that                                 state kind of                                 distributed among the workers in your                                 cluster                                 and checkpointing guarantees that the                                 state is fault                                 tolerant right it will send the                                 checkpoint data to some kind of                                 persistent object store like s                                         so you can recover from failures                                 and this kind of state is actually not                                 something you                                 you know you maybe see everywhere but                                 it's used as a building block for a lot                                 of                                 higher level concepts so um the next one                                 i want to cover is flank exactly once                                 very similar to kafka one right we want                                 to have end to end exactly once                                 guarantees                                 and the link exactly once uh                                 has this um you know function that's                                 implemented uh                                 for essentially you know having uh                                 two-phase commit semantics                                 and as you can see based on the                                 signature it uses some of the                                 uh state functionality to actually                                 achieve that                                 and it combines that with the kafka                                 transactions                                 right so it now said the leverage kafka                                 transactions it knows how to leverage                                 internal state and it combines those two                                 concepts in order to provide you                                 exactly once delivery all the way right                                 so you                                 you can consume something from kafka                                 transform and produce it and you will                                 pursue that guarantee                                 uh throughout the process and what's                                 useful about this uh it's actually um                                 pretty generic and you can implement                                 your own custom sources and sinks                                 uh to have that exactly once uh delivery                                 guarantees as well                                 um flink state management so again state                                 is important                                 and typically when we talk about                                 stateful streaming you know you                                 might want to use something like                                 windowing aggregations joins                                 uh you know very classic example some                                 kind of window                                 uh for your uh computation that                                 stores uh state in each of the segments                                 um and i think um that that can                                 work really well for simple use cases                                 but with more complex use cases you                                 realize that you need something like                                 you know state variables timers and side                                 outputs like those three building blocks                                 is something that flink provides you to                                 create very complex and advanced                                 workflows                                 like some of the uh scenarios that we                                 built at shopify you know you can                                 use a state as a database essentially uh                                 right because you can manage that state                                 um in a very fine-grained way or you can                                 even come up with workflows where you                                 can repopulate state                                 um by handling layer writing messages                                 again this is something that's pretty                                 low level and you need to                                 embrace you know custom state variables                                 timers maybe side outputs                                 but by doing that you actually have                                 access to more workflows                                 if you just uh use let's say window with                                 aggregations                                 um this is one of my favorite features                                 flink state processor api                                 you know with historical backfill you                                 have this challenge                                 where you know you might need to                                 reproduce a lot of data a lot of typical                                 data                                 when your business logic has changed and                                 you know all the state that you have in                                 your pipeline is essentially                                 you know no longer valid um this api                                 actually allows you to avoid that                                 historical processing right because                                 it gives you programmatic access to the                                 state itself                                 so what you can do instead of going and                                 reprocessing the two years of dating                                 kafka                                 you can write your uh you know little                                 application to                                 read the state update exactly what you                                 need kind of in place                                 and create a new save point and this can                                 combine the power of badge                                 uh because flink provides that api as a                                 batch api at the moment                                 and streaming um so the batch can                                 process the                                 um save point uh this the state                                 file quickly and you can bootstrap a new                                 streaming job                                 right so you avoid that very expensive                                 historical backfill                                 and this is something i think that's                                 unique to flink and                                 very very cool feature that i                                 i just love um                                 let's see so the next i just want to                                 mention briefly about data syncs                                 because i think it's pretty obvious that                                 some data syncs                                 work better for streaming use cases than                                 others and                                 one of the very important things you                                 need to keep in mind is the ability to                                 support                                 updates and absurds right because if                                 your data destination                                 uh you know supports those uh those                                 features                                 uh they can be used for data correction                                 and this is extremely useful                                 uh in case of a um late driving messages                                 and so some of these typical examples                                 right pretty much any relational                                 database                                 can support that uh some nosql databases                                 like hbase or cassandra                                 with all up engines you have pinot that                                 support supports upserts                                 when you have compacted kafka topics you                                 can also support that update                                 per key essentially right and there's                                 also this notion of                                 lake house object stores which i'm going                                 to cover in a second                                 and it can be problematic for you know                                 some all-up engines like druid or click                                 house where updates maybe not first part                                 citizen                                 uh not there by design uh non-conducted                                 topics                                 is also tricky and as i mentioned                                 at the beginning you know if you have a                                 classic object store                                 um something like s                                                 with a lot of parque files which are                                 immutable can be                                 can be very hard to go back and update                                 that historical data                                 right but uh just to                                 you know cover that use case because                                 it's extremely common you know this is                                 how a bunch of data lakes                                 are designed um i really like the idea                                 of using                                 this new uh you know white house                                 technologies                                 maybe heard about apache iceberg delta                                 or hoodie                                 you know those engines they allow you to                                 provide a transactional journal on top                                 of the object store right so you no                                 longer just                                 uh write a bunch of files to s                                       certain location                                 but all those modifications they go                                 through that transactional journal                                 right and this uh allows updates                                 this allows small file uh compaction                                 even time traveling if you want to right                                 and                                 flink has uh iceberg has a flank uh                                 integration                                 uh where it supports uh streaming uh                                 updates                                 uh and yeah this this is something you                                 can just say not all of those                                 technologies                                 support link out of the box but i think                                 this is                                 where where it's going um                                 so as a summary i want to say that you                                 know we                                 try to address all those concerns for                                 data availability and redemption                                 we can use things like data integration                                 and compacted topics and tiered storage                                 to have all the data in kafka and have                                 it there                                 forever or for very long time we can                                 address data consistency                                 uh guarant data consistency concern with                                 exactly once end-to-end deliver                                 semantics                                 we can simplify late driving data                                 handling                                 by using proper state management                                 primitives and proper data syncs that                                 support updates                                 and for data you're processing and                                 backfill you know we can either use                                 something like                                 dynamic kafka clusters for elasticity                                 or we can use save points and state                                 processor api                                 for flink to update that data kind of                                 in place and just to finish                                 i also wanted to show like a couple of                                 use cases and                                 how you can use those building blocks so                                 this is a very common architecture                                 you know you have let's say a stateless                                 set of transformations uh that you want                                 to                                 apply to some data that's coming from                                 like little                                 ingestion api everything is in kafka and                                 in the end you want to route this to                                 multiple data sources                                 like maybe there's a data lake some kind                                 of uh databases search engine like this                                 this is very common                                 right so if you look at the the building                                 blocks we cover                                 you can use tiered storage for keeping                                 the data in kafka                                 you know for very long time you can                                 embrace exactly once delivery                                 uh you can use kafka connect to route                                 and integrate data with different                                 destinations you can use iceberg on top                                 of that                                 s                                                                 guarantees                                 and you can use absurds in other                                 data syncs and finally whenever you need                                 to reprocess                                 maybe you decide to introduce that                                 elasticity                                 at your kafka layer and just quickly                                 bring those large clusters                                 uh and the second use case let's say we                                 need to apply                                 a set of stateful transformations like                                 joints window in aggregations                                 um on top of the same data stream                                 but also another one where we consume                                 data from a relational database maybe                                 something that we have in-house                                 and we perform some kind of analytics                                 and you write it in all of the engine                                 like                                 uh in this case again we use steer                                 storage                                 but for the relational stream we use                                 compacted topics                                 just because it's a very natural fit                                 when you have a relational database as a                                 source                                 your primary keys can serve as message                                 keys                                 and perhaps use something like division                                 for change data capture                                 right so you capture all of that as a                                 data stream                                 you use uh flink for transformation to                                 use exactly once delivery                                 at the flank layer as well as pino you                                 know it needs to                                 configure something for that uh you use                                 save points and state processor api in                                 order to                                 you know backfill and reprocess uh some                                 historical data in place                                 if you if you want to and finally pino                                 supports uh absurds                                 so you can do that for data correction                                 and                                 late traffic data                                 and that's pretty much it i'm happy to                                 answer your questions                                 yeah thanks for this awesome talk i                                 think you've covered a lot of good                                 building blocks uh                                 in in in this in in the last                                                                                                          by the depth and uh like how you put the                                 pieces together                                 um yeah let's see if there's uh if there                                 are questions from the audience                                 not yet um yeah i have a question                                 so um like given all these uh this this                                 nice building blocks to like compose                                 your architecture                                 uh where do you see uh is the biggest                                 gap still                                 or what where do you where where do you                                 think or is still like significant                                 improvement possible                                 good question yeah um so i i think                                 couple areas                                 right so the first one is um you know                                 streaming engines                                 and support for a large state or complex                                 state                                 like i think flink is pretty you know                                 probably the most advanced out there                                 but you definitely have some challenges                                 if you use some some other                                 engines and even with flank you know it                                 doesn't have                                 a lot of support for like let's say you                                 have a windowing                                 uh transformation and the window is                                 closed then we have weight driving data                                 like kind of the state of the art you                                 just just drop that                                 right and i think some of the data                                 pipelines would require you to                                 somehow reprocess that right so maybe                                 like reopen the window                                 i know it sounds tricky but i think that                                 that feature would be really cool to                                 essentially                                 just handle that late driving uh data uh                                 problem you know once once and for all                                 um and the second area of improvement is                                 that                                 uh data lake uh integration right like                                 what iceberg                                 is doing with flink integration i think                                 is very impressive                                 and i think we need to see more of that                                 and uh                                 hopefully more you know engines decide                                 to support that                                 so just like making sure the data can                                 flow to all those                                 data lakes and object stores in a                                 transactional fashion                                 and updates and compaction and time                                 travel is supported                                 yeah thank you um in the meantime there                                 was                                 uh another question came in and that's                                 um                                 are you using the absolute feature of                                 pinot in production                                 so we actually uh decided to use apache                                 druid                                 quite a bit ago and this is kind of how                                 i learned                                 that you know if you don't have absurds                                 it can be                                 uh can be a big problem um but i know                                 about some other companies that use                                 pinot and absurds                                 um like i think stripe has presented                                 something like that recently so you can                                 try to find it online um                                 yeah like with droid unfortunately you                                 don't have absurd's                                 functionality so you need to come up                                 with the system of you know reductions                                 and corrections                                 which is quite tricky um yeah so no not                                 a lot of experience with pinot myself                                 you
YouTube URL: https://www.youtube.com/watch?v=sG8QD8AeWH4


