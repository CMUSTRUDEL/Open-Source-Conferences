Title: Anshum Gupta – Highly available and disaster ready Apache Solr
Publication date: 2021-06-30
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Apache Solr is a critical piece of infrastructure for most companies dealing with data. The systems that Solr often powers are critical, requiring high availability and disaster recovery.
 
Often users tend to undermine the importance of these features, but more importantly there’s a need to better understand the need for these before it’s too late. In the recent past, a lot of effort has been put in by the community to build features that allow for operating a reliable and highly available Solr setup.
 
During this talk, I will talk about the DR and HA options that Solr provides like incremental backups, backup to Blob store, and Cross-DC replication. I will explain the reasons that make them essential, and also explain what these features actually do under the covers.
 
At the end of this talk, attendees would have a better understanding about HA and DR in general, and the options they have to improve the reliability of their Solr clusters. They would also be equipped to enable and use those features to operate a reliable Solr cluster that is disaster ready.

Speaker:
Anshum Gupta – https://2021.berlinbuzzwords.de/member/anshum-gupta

More: https://2021.berlinbuzzwords.de/session/highly-available-and-disaster-ready-apache-solr
Captions: 
	                              hello everyone uh                               welcome uh to this talk um the search                               track of the berlin buzzwords                                    edition i'm going to be talking about                               highly available                               and disaster ready apache solar today                               so uh during the course of this talk i'm                               going to be introducing                               a few concepts talk about dr and then                                talk about ha and the options that solo                                provides out of the box                                uh to uh if you want if you were                                interested in hdr                                what what would your options look like                                so over the years apache solar has                                evolved into something that's more than                                and beyond full text search                                it's come down come over to uh to handle                                use cases                                uh that involve machine learning                                analytics and a lot of other things                                almost like a swiss army knife it's not                                only grown in the number of features                                that it                                offers but that it can help its users                                accomplish but also the scale at which                                these features can be successfully used                                because of that reason uh solar solar's                                been                                used in uh in a wide variety of                                industries                                in in infrastructure that uh i can only                                define as critical                                the ways in which these industries like                                retail banking travel                                research and a bunch of others use solar                                make it essential for                                for it to work reliably both in terms of                                correctness as well as availability                                so we're going to dive directly into                                disaster recovery uh                                disaster recovery was developed in the                                mid late                                                       realization                                uh on the dependence of computer systems                                and that downtime meant a                                loss of revenue uh back in the day                                mainframes would back up on tapes                                and wait until those tapes would reach                                would avoid complete restoration of data                                in case something were to happen over                                the years though                                uh disaster recovery has evolved                                a lot and currently disaster recovery                                involves a set of policies and tools and                                procedure to                                to kind of enable the recovery of um                                of an existing system for its continued                                uh                                availability or continued uh operation                                and that could happen due to either a                                natural or a human disaster                                and disaster recovery generally uses an                                alternate site                                uh to restore that operation uh you know                                for the entire to normalcy for the                                entire site                                as the intention is to have the primary                                location recover it can                                it is generally done one of the two ways                                either uh                                using an offline process something like                                a backup restore                                or more actively or transferring data at                                real time so that if something were to                                happen to your original system                                you could recover from that                                asura has provided uh dr readiness and                                i'm going to                                refer to uh disaster recovery sdr for                                the rest of the slide for the most                                more often than not so solar has                                provided dr readiness options for a                                while                                and while they have abroad over multiple                                releases                                they still continue to be the                                recommended options the same options                                and while uh so backup and restore is                                certainly                                uh the most important dr option                                available but                                cdcr or cross data center replication as                                it's known um                                actively copies data allowing it to be                                also                                to also work as an ha solution um                                but i'd like to highlight that dr                                shouldn't be confused with ha and we're                                 going to get to that                                 in a little bit tr                                 uh an important aspect of dr that is                                 that it is configured                                 with a designated time to recovery um                                 and a recovery point                                 and by by recovery point i really mean                                 that uh                                 checkpoints are generally                                 are generally uh sorry                                 are generally uh maintained and when a                                 restoration has to happen                                 uh data is expected to go back to that                                 restoration point                                 uh essentially translating to not all                                 data that you might have                                 uh you know gotten into the system                                 before the disaster happened is going to                                 be recovered                                 at least that's uh that's by design                                 diving a little deep into backup and                                 restore options for solar                                 to give you a little bit of history of                                 the backup restore features that solar                                 offers it was first introduced in um                                 in version                                                     introduced it was vanilla but um                                 it basically came with some restrictions                                 uh most importantly it only allowed for                                 full backups                                 and was not really cloud friendly in a                                 sense that all it would                                 allow users to do is to back up either                                 on nfs                                 which is local backups or sdfs um                                 and there was no way to only backup                                 data or the configs so it would always                                 back up everything                                 every time and more often than not                                 there were a lot of failed backups and                                 i'm going to go back to                                 why that happened but there was a                                 realization that                                 something needed to change if this                                 feature was to be continued                                 to use to be used by by its users                                 so it needed a new perspective                                 and that new perspective came in the in                                 the form of incremental backups                                 the realization that uh solar tries to                                 back up every bit of data every time you                                 want to back up and people generally                                 want to back up                                 early maybe twice an hour maybe less                                 frequently but                                 uh backing up all of the data every time                                 really meant uh                                 you know uh storing more data also                                 transferring more data across the                                 network uh                                 maybe even to a different data center so                                 with incremental backups uh                                 solar offered something uh that allowed                                 for                                 reducing redundancy of data storage and                                 transfer translating                                 faster and less expensive uh backups and                                 restores um but it did not stop there                                 this entire feature offered two more                                 valuable                                 uh valuable sub features you could say                                 or                                 or things um first one being safety                                 against corruption                                 while the previous backup mechanism did                                 not check uh                                 for a backup index before it backed up                                 that the incremental backups ensured                                 that                                 uh the index files that were that were                                 backed up                                 were check summed uh and to ensure that                                 at the time of backup the index file                                 that you were backing up                                 was good to use if you were to need it                                 um and another more important aspect of                                 this was this entire                                 architecture allowed for backups to be                                 more cloud-friendly so you were no                                 longer restricted to only using                                 uh nfs or hdfs uh file systems                                 so backup and restore evolved into                                 something that                                 that started using incremental backups                                 allowing users to kind of optimize on                                 the resources it requires and                                 also the odds of it succeeding um                                 but it also added extendable interfaces                                 so what that meant was                                 uh you could now have cloud services or                                 cloud service providers                                 um as their backup options well that                                 wasn't something that was released                                 when when this change happened uh                                 but uh over actually the upcoming                                 release was i think uh                                 is going to have support for gcp and                                 there's an open pr                                 for supporting aws or s                                               backup option um for for solar cloud                                 the p are still open so i'm not sure                                 when it's going to be merchant released                                 but uh it's out there and it's looking                                 really good                                 um one one interesting thing that                                 solar allows you to do is to define                                 multiple                                 uh repository implementations and uh                                 with the extendable interfaces that i                                 just spoke about are actually repository                                 implementations                                 um allowing you to have a definition of                                 say htfs                                 gcp aws maybe another you know                                 proprietary backup                                 file store that you that you might have                                 at your workplace                                 and you don't really have to use all of                                 these all the time                                 and you can specify exactly what you                                 want at runtime or making it really easy                                 and convenient for people to                                 uh to switch and use                                 things as they wanted                                 and uh one more interesting aspect of                                 this backup and restore                                 update was um unlike the pre unlike the                                 older backup restore                                 which required users to restore uh                                 an existing index into any into a new                                 collection only                                 the new backup and restore uh yes it did                                 it still continues to work with alias                                 which is also a recommended mechanism                                 if you don't want to change your client                                 code but backup the new version of                                 backup and restore allows users to                                 restore into an existing collection                                 piggybacking on a feature that was                                 released                                 only recently that allows solar indexes                                 or collections to be marked as read-only                                 um a lot at the same time or                                 in the background allowing you to                                 restore into the same collection                                 so how do you how do we use uh backup uh                                 you know backup and restore and for the                                 sake of time just discussing uh                                 backup here you basically have to define                                 uh your implementation that you                                 you might use in your solar setup                                 uh in your config um and                                 while the config that's highlighted here                                 is the local file system repository                                 option the location being an nfs mount                                 point                                 you can you can have a gcp                                 implementation with the upcoming release                                 um and in the future you will be able to                                 have an s                                                            well                                 and solar exposes four apis as part of                                 the backup restore                                 uh umbrella and that's the backup the                                 restore the list backup and the delete                                 backup                                 uh api calls the backup call uh let's                                 see what happens when you sell in a                                 backup call                                 when you send in a backup call to to                                 solar                                 it parses those parameters picks up the                                 right repository implementation                                 based on what you've already configured                                 and what's been provided as part of the                                 request                                 and then sends an internal core admin                                 api call to backup the core                                 well this call is kind of an optional                                 call because it                                 sort of allows you to specify something                                 called the index backup strategy                                 and while right now it supports two uh                                 two strategies out of the box                                 one being copy index files and the other                                 one being none                                 and they do exactly as the name suggests                                 the copy index file                                 um would allow you to take a take an                                 index file and back it up into your                                 implementation of choice at the location                                 that you've specified                                 or none would skip backing up all of                                 your data                                 and move and then it moves on to the                                 next step allow and                                 backs up your configs in this case it                                 would allow you to only backup configs                                 for you to create                                 a collection say that looks exactly like                                 like the current collection but does not                                 hold any of the data that                                 you have uh in your old or the existing                                 collection                                 and towards the end of the request it                                 does some internal housekeeping                                 um where it takes care of things like                                 ensuring                                 you're not uh you don't have too many                                 backups                                 and to clean up the the oldest backup                                 that you might have if you spend if                                 you've specified the number of backups                                 that are allowed                                 for a given repository implementation                                 so moving on to availability                                 so availability is uh is what is what is                                 availability availability is the                                 probability                                 that a system will uh will work as                                 required                                 when required uh during a task or a pro                                 or a mission and the mission is                                 basically a project the time when you                                 really intend to use the system                                 the system which aims to ensure an                                 agreed level of operational performance                                 generally termed as uh as uptime for                                 a higher than normal period and the                                 normal period might vary based on use                                 case to use case                                 um it's called a highly like it's called                                 a highly available system                                 uh when the system is not available it's                                 it's uh                                 it's uh considered to be downtime                                 a highly available system is designed uh                                 with redundancy with redundancy in the                                 system                                 to take care of both micro and macro                                 level failures to overcome competent                                 failures at different levels                                 the systems are designed with no single                                 point in failures so as to ensure                                 uh availability uh even when something                                 were to go down                                 it's generally achieved by uh redundancy                                 monitoring and failover a combination of                                 those so that                                 you should you not only need redundancy                                 but you also need monitoring in order to                                 detect when something goes down                                 and to failover by either taking uh that                                 specific component                                 out of out of action or to die                                 or in addition to direct traffic that                                 was meant for that component                                 or do it onto a healthy component                                 an important aspect here is highly aware                                 high availability for a system does not                                 really translate to                                 a system never going down it's it just                                 means that the system is going to                                 continue to work                                 beyond a certain expected uh                                 availability                                 threshold which is expected out of the                                 system so                                 any highly available system is it is and                                 can                                 can go down it's just whether it still                                 continues to work for the expected                                 duration                                 and while h a and d are uh might sound                                 very similar in terms of what they're                                 trying to achieve                                 they're essentially different uh they                                 have some logical overlap                                 but a highly available system by design                                 handles uh                                 smaller scale issues something you know                                 a smaller                                 component going down whereas dr                                 generally means a bigger problem has                                 happened and you need to now recover                                 after the loss of something that is more                                 than just a small component of the                                 system                                 so solar has provided ha options                                 uh for a while now                                 and replicas are the essential building                                 blocks when it comes to a chain solar                                 replicas provide redundancy um and so ha                                 uh in a sense that if one replica goes                                 down uh                                 another active replica takes over                                 processing the requests                                 uh that were meant for the original                                 replica uh solar does not automatically                                 spin                                 up something but it makes sure that                                 anything that is meant for something                                 that that's not down                                 is redoubted due to an active replica it                                 also takes over                                 other roles and responsibilities like                                 being a leader or                                 when something vertical down ensuring                                 the availability of the system again                                 now however there are times when when                                 due to some error which may be human or                                 not the impact on the system                                 is not limited to one or two components                                 it's not local                                 it's not physically pounded in such a                                 situation you need something                                 bigger that spans across the data                                 centers and that's where cross data                                 center replication                                 allows you to have a highly available                                 system that spans across                                 a single physical data center                                 so cross data center replication um                                 talking about the history of cross data                                 center replication it's it's existed in                                 solar for a while now                                 with the first release of cdcr as it was                                 known                                 uh coming out in in solar                                     and um it was meant to accommodate two                                 or more data centers with limited                                 bandwidth and allow for data to be                                 replicated acro                                 across these but the design uh decisions                                 that were made back back in the time um                                 had some issues                                 uh leading it to uh                                 to failures and problems so                                 for users that tried to use the system                                 and so it was deprecated in atex                                 it's now been removed and the intention                                 is for it to be replaced in                                             approach                                 i've kind of pushed some code up already                                 into my github repository i'm still                                 working on it i have some working model                                 of                                 of this thing but let me talk about what                                 this approach looks like                                 so the new solar with cross dc                                 replication architecture looks something                                 like this                                 um it's it's kind of widely                                 based off of an approach that we've used                                 at apple                                 to achieve trust data center replication                                 uh                                 the reason why why we didn't use what's                                 offered out of the box was not because                                 we didn't believe in it but                                 but because we started using it before                                 solar had cross dc as a solution out of                                 the box                                 um the best part about this entire                                 infrastructure                                 is is that white box in the middle uh                                 all of the queuing mirroring logic                                 something that solar is not designed for                                 it was never designed for                                 uh is is abstracted out in the system as                                 compared to the older system                                 which um which tried to also                                 behave which which also forced solar to                                 behave like a                                 like a queueing service uh eventually                                 leading to problems like                                 unbounded growth in transaction log and                                 out of memory issues                                 in this architecture uh when a client                                 sends a request to solar                                 solar locally indexes that the document                                 um                                 and then puts this document onto                                 under queue and source one and desk two                                 are basically uh topics in this case and                                 the reason why i'm gonna refer to a                                 bunch of things                                 using terminology that's generally                                 common for kafka                                 may not be common for other queuing                                 systems um                                 is it's just because of that uh that the                                 the                                 implementation out of out of the box for                                 this would be                                 would be kafka to begin with um                                 we would use kafka to begin with so                                 solar writes this to a source topic                                 uh let's call it source one because it                                 belongs to data center one on the left                                 hand side                                 uh there's a mirror of some sort that                                 mirrors this                                 into a queue uh that's called test two                                 uh a destination queue for data center                                 two                                 uh and there's a cross data center                                 consumer uh who's                                 who's responsible for consuming                                 everything that's coming into its local                                 destination queue                                 and riding it onto solar uh and as                                 i mentioned uh the isolation of                                 responsibility in this case uh as solar                                 is not expected to be the cue                                 uh takes away a lot of problems that                                 solar might have had with a cdcr                                 solution                                 if you wanted to wear replication                                 it still stays easy again primarily                                 because                                 uh everything inside the white box is is                                 abstracted out of solar and it's not                                 really                                 it doesn't really know what's going on                                 inside that box which is where the                                 complexity got added whatever bit of                                 complexity there might be                                 now in this case when an incoming                                 request comes directly to dc                                  it gets written by solar onto the source                                 topic                                 for that data center gets mirrored to                                 the destination topic                                 of the other data center uh only to be                                 consumed by a                                 dc consumer that's running locally in                                 that uh in in dc one                                 and indexed into uh into this into that                                 uh into the solar cluster running on                                 that data center uh the cross pc                                 consumer has checks and                                 uh has checks in place to ensure and                                 avoid                                 circular mirroring so if if a request                                 were to come                                 uh originally into dc one solar is going                                 to make sure                                 that uh it does not come back                                 through the source top source topic of                                 data center                                                              checks and balances                                 that ensure that that doesn't happen                                 so the requirements for this new                                 architecture are a messaging queue that                                 messaging queue                                 as i mentioned would be kafka to begin                                 with but you could have your own                                 implementation and have your own version                                 of messaging queue                                 it could be rapid mq it could be a file                                 based messaging queue a proprietary                                 internally your company might have                                 created a messaging queue system                                 uh and you could use it uh you'd need a                                 very simple implementation that would                                 allow you                                 to to use that queue you'd need a                                 messaging queue consumer implementation                                 uh producer being optional you'd need a                                 crosstc consumer that would be provided                                 out of the box                                 and nothing would need to really change                                 in there maybe some configurations                                 um and you'd need external versioning                                 for multi-replication                                 the reason why you need it only for                                 multi-way replication is because                                 if you were to send a in the traditional                                 uh one-way replication model                                 it's going to piggyback on solar's                                 versions if provided                                 in the original originating data center                                 to just replicate everything over                                 but if you if you don't uh if you have                                 multiple                                 data centers acting as source then you                                 can't have                                 let that happen because those versions                                 are not synchronized                                 and and you need an external version in                                 that case                                 ingesting data into this entire system                                 um                                 can happen in one of the two ways it can                                 either be sent directly to the queue                                 and consumed by the consumer in which                                 case you would need                                 an external version you would also not                                 get all the benefits                                 of the second option which is using a                                 mirroring update request processor which                                 is a custom                                 request processor that that makes sure                                 that the request is processed                                 the way it's supposed to be processed                                 knowing that uh the trustee                                 is enabled um the biggest one of the                                 biggest benefit that it provides is the                                 abstraction of the underlying queue so                                 if you were to start off with kafka                                 because that's what solar provides out                                 of the box                                 or would provide out of the box um                                 but move on to using a different queuing                                 mechanism because uh that is your                                 you know cue of preference you wouldn't                                 have to change your client code because                                 the client's not writing to this queue                                 and is agnostic of that queue                                 um so that's one of the benefits the                                 second benefit is                                 that it allows for more checks and                                 controls before the submission                                 a request that's coming in might fail on                                 the originating data center in which                                 case it shouldn't be written to the                                 queue                                 but if you if you write directly to the                                 queue all these requests                                 make it into solar it might succeed on                                 one of the data centers                                 might not succeed on the other say                                 because the configs were different                                 something something was off uh and the                                 cross dc consumer in this case would                                 have to be intelligent enough to figure                                 out whether to                                 retry it discard it or what piece what                                 needs to be done with this request                                 and um an important aspect that we used                                 with an apple                                 uh with our cross data center uh                                 replication                                 was to handle deletes better a system                                 like this                                 is designed so that you could have you                                 would avoid any accidents                                 happening um bringing down the system                                 so an accidental delete sent to one data                                 center uh in our case                                 gets mapped into an update that only                                 flags those updates                                 by adding a feel to those documents as                                 as deleted                                 um a parallel process that runs                                 occasionally                                 or frequently uh sorry is responsible to                                 then go ahead and                                 clean up these documents at a later                                 point in time what it will what it                                 allows you to do                                 is say delete came in into data center                                 one um                                 by the time you're you've realized that                                 it was an accidental delete you could                                 still go to data center to                                 undelete those documents because those                                 documents were just marked                                 and then use backup and restore or                                 something else to restore all of that                                 data                                 back you haven't really lost that data                                 all the requests that are processed in                                 this uh                                 in this architecture are mirrored                                 requests uh merit request is nothing but                                 something that                                 encapsulates the original solar request                                 uh                                 while adding uh some mirroring metadata                                 that's used for tracking                                 and metrics purposes uh things like                                 attempt and submit time that allows you                                 to track                                 for example uh the submit time um                                 is the time when the request was                                 originally or first written into the                                 system                                 by one of solar instances in one of the                                 data centers                                 when a receiving data center processes                                 this request                                 it knows how long has it been and what                                 the latency of this                                 looks like so um allowing you to alert                                 if you're off and your slas                                 are not met attempt is another thing                                 that allows you to track an alert if the                                 same request is getting rejected                                 by the cross dc consumer or not being                                 successfully processed by the consumer                                 in which case you could go back and                                 figure out figure out issues like                                 an out of sync config                                 the cross dc consumer uh in this entire                                 framework                                 is a standalone app that has a simple                                 responsibility of reading from the queue                                 and writing to solar                                 but also an important and intelligent                                 ability to figure out                                 what kind of requests to discard and                                 which ones to resubmit                                 into the original queue and i'm again                                 using the                                 term topic because uh kafka is the                                 is the preferred queuing                                 mechanism used here um so                                 after a lot of trials and errors and                                 we've learned it the hard way                                 realized that the only kind of request                                 that is safe to drop                                 are                                                            worse and conflicts in case of                                 optimistic concurrency when a request                                 comes in                                 that the crosstc consumer is trying to                                 send a solar and solar response to the                                                                     would translate to solar already having                                 a more recent                                 version of this document in which case                                 it's safe to discard this                                 this document in all other cases it's                                 safe to                                 retain these documents                                 just like the backup and restore uh                                 story evolved                                 uh into allowing and using more                                 cloud-based or is evolving into                                 using more cloud-based uh                                 backup repositories uh this seems true                                 for this approach here                                 as it's not limited to kafka and even                                 though it might start off with kafka                                 this interface allows for using custom                                 queue implementation                                 and that may or may not be something                                 that you really desire or want or                                 need but it certainly allows you                                 to stay open to the idea of switching                                 over to a totally different queuing                                 infrastructure                                 to extend that interface and this might                                 be                                 a little too much of detail but if you                                 were to be                                 if you were uh to extend the interface                                 and use your own custom messaging queue                                 you basically need to define source                                 config and this is all in flux so                                 uh i wouldn't please don't hold me to                                 this for now                                 um so there's source config that you'd                                 need to implement                                 and across dc consumer that accepts a                                 message processor                                 uh the message processor uh basically                                 has                                 all the logic that deals with uh sending                                 message to solar                                 figuring out what to do in case of a                                 failure                                 and everything else around it so all the                                 intelligent aspect of stuff                                 is in the message processor already                                 pre-programmed the crosstc consumer is                                 what you need to implement                                 the responsibility of which is to get                                 mirrored objects from the queue                                 and put it back to the queue in case of                                 a failure                                 and the message processor is going to                                 let the cross dc consumer know whether                                 an object needs to be put back into the                                 queue or discarded                                 so it really needs to make no decision                                 it just needs to be able to get                                 and put back uh objects from and into                                 the cube                                 and the best part is that this approach                                 works well with event driven systems so                                 which is what i've been working with                                 so um the road ahead on this is to                                 release                                 this kind of more basic                                 trust dc solutions especially because                                                                                                          lot of people                                 i realized kind of won the need for it                                 they're already using                                 uh the old version and would need some                                 form of a crosstv replication to be                                 offered by solar                                 it does not report any metrics to a                                 reporting system as of yet uh                                 or that's not what i'm working on or                                 concentrating on at this point                                 um and that would be great to add it                                 does log                                 all sorts of problems so you can always                                 take your logs and figure out                                 what's going on and if something's off                                 uh or if everything is okay                                 um it does not handle collection api                                 requests which would be a good to have                                 thing uh it's something that we've had                                 with an apple so i'd be happy to add it                                 at a later point in time                                 but for now uh none of that is part of                                 the solution uh                                 this this phase um and it's great to                                 have trustee c replicated clusters                                 but it's really important to have some                                 form of a mechanism to ensure that these                                 clusters are in sync these clusters                                 are working well together um                                 in terms of even if it's not                                 self-healing it doesn't fix any problem                                 it's important for uh for for there to                                 be a way for people to know when                                 when things are out of sync and then                                 support for more queue systems                                 which again is completely subject to                                 what the community might want                                 maybe everyone just uses kafka that's                                 what they love and that's what they use                                 um and they don't really want anything                                 else but uh if people need something                                 else                                 to be supported uh that's that's                                 certainly on the road ahead                                 um and all of this can happen by                                 community participation so if there's                                 some if this is something that interests                                 you                                 please feel free to ping or participate                                 directly whatever                                 uh might be your preferred way uh                                 to get involved in this but yes uh and                                 it doesn't have to be code can be                                 testing it can be trying out                                 pitching in ideas everything is valued                                 so while i spoke a lot about so as a                                 primary data store                                 i spoke about a lot of hadr sort of                                 being safe and stable scalable                                 and you being able to safely use solar                                 and critical systems                                 sorry i'm very sorry i'm sure to                                 interrupt you uh because we are running                                 out of time                                 so maybe find the thought you want to                                 share and then we have to wrap this up                                 in the interest intro yeah this is this                                 is actually the last slide so i'm done                                 yeah                                 so uh the question is whether the                                 you could use solar as a primary data                                 store                                 pldr is known and that is basically                                 because yes it                                 offers h-a-n-d-r but it's just not                                 designed to be a primary data store it's                                 not designed for storing documents that                                 are really large and                                 uh do not follow the                                 the kind of uh format that solar is                                 designed for                                 so yeah um solar offers a lot and it's a                                 great search engine so i would recommend                                 that you use solar for just that                                 um and that's about it thank you so much                                 okay thank you very much armstrom i                                 think this gave us great insights and i                                 think many of us are looking forward                                 to the new cross data center replication                                 i think it's also good to see                                 uh that the stuff you you're developed                                 at apple uh will finally                                 be open sourced so i think that's always                                 good to have something                                 that is practice proven practice proven                                 to                                 to see and to make it into open source                                 you
YouTube URL: https://www.youtube.com/watch?v=Nx5uUVj3QOc


