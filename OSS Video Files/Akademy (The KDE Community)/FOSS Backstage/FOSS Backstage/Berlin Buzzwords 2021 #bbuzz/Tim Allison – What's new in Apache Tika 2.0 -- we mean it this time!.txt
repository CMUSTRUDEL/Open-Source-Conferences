Title: Tim Allison – What's new in Apache Tika 2.0 -- we mean it this time!
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).

Speaker:
Tim Alisson – https://2021.berlinbuzzwords.de/member/tim-allison

More: https://2021.berlinbuzzwords.de/session/whats-new-apache-tika-20-we-mean-it-time
Captions: 
	                              off i go to talk about apache tika                                   the title of the talk was we mean at                               this time and we did                               but we did release at least a                                        and i'll talk about that shortly                               all right so uh                               tca is a overall project that                               deals with file processing                               it's a framework for file type detection                                and then extraction of text and metadata                                you send it bytes you get back uh                                normalized uh                                output across all of those different                                file formats um                                one of the things that got me into tikka                                are several of the things it's easy to                                add new file types for detection it's                                easy to add new parsers works                                recursively with embedded files which is                                really important for a lot of                                complex file types and it has a nice                                integration with tesseract ocr                                this is an example of files that can be                                embedded within                                files and i have seen this in the wild                                people do crazy things                                uh and it's important to have be able to                                have parsers that can understand these                                file formats and then                                be able to process all the different                                files that can be stuck inside of other                                files                                recursively all right so that's kind of                                a quick overview                                of tikka uh for those who aren't                                familiar with it                                uh so the outline of the focus with a                                status of where we are on tikka                                i'll talk about breaking changes in                                    and talk about maven modularization                                a bit of motivation for why we added the                                tikka pipes module and then i'll talk                                go in great detail about tika pipes so                                off we go so                                status we released a                                               january                                uh beta came out towards the end of may                                we were thinking about getting a                                    out by the conference but that didn't                                happen all good progress is being made                                we want to make sure that everything is                                working well                                the other major part of the project is                                we recently added                                nicholas de paza di piazza as a                                committer and pmc member                                uh nicholas and i quite a bit on the                                tika pipes module and i'd like to thank                                him now                                publicly uh for all of this uh                                collaboration on that                                and for his many contributions to apache                                tiga                                all right so tika                                                     at a high level                                uh one is that if you have tesseract                                installed the pdf parser will call                                tesseract                                in auto mode by default so that means                                that if it doesn't find enough                                text on a page or if it finds bad text                                on a page                                it will run tesseract which will be                                surprising from a performance standpoint                                uh all of the parsers have now been                                maven modularized and moved around                                these are not jigsaw modularized yet but                                they are at least maven modularized                                and i'll talk about that in some detail                                in the next slides uh tikka server                                operates in spawn child mode and i'll                                talk about                                that in some slides and the metadata has                                been streamlined with a preference for                                uh standards like dublin core and so on                                okay so may even modularize all the                                things                                so tikka one point x has                                a massive directory with all of the                                different parsers all of the unit tests                                are                                all the test documents are in one                                directory uh it was getting unwieldy                                so we decided to break the parsers into                                three sub modules                                tikka parser standard which are the                                java only uh fairly lightweight                                uh parsers are in the antique pressure                                standard module                                 t-comparisons extended or for those                                 parsers which probably aren't used by a                                 lot of people                                 necessarily but they may require heavy                                 dependencies                                 external network calls to call external                                 resources                                 or they might require native libs like                                 sqloid                                                   module is tikka parser's advanced and                                 that is for super heavy dependencies                                 like dl                                   uh or other heavy-duty processing like                                 text                                 from bytes image recognition nlp that                                 kind of stuff so that's kind of the                                 breakdown of the three sub modules the                                 new three                                 sub modules of tica parsers uh we did                                 integrate tesseract ocr back in                                 up in tk parso's standard because that's                                 um just                                 feels like it's such a fundamental thing                                 to have available                                 and we're not shipping uh tikka with                                 tesseract users have to install it or at                                 least pull it in with tikka server                                 and docker so it's it's not something                                 that um                                 we're shipping uh with with it which is                                 why we're                                 uh incorporating it into tco parser                                 standard                                 all right so all of those so we've then                                 subdivided even further into                                 sub modules for the different file types                                 uh you can see t comparison                                 so that's ticker parser's classic or                                 standard excuse me on the left                                 um t capacitor is extended on the right                                 with the scientific module                                 advanced we have the deep learning                                 we have captioning uh object recognition                                 and a number of other things in advance                                 we also now have thanks to louis                                 mcgivern team a                                 a integration for speech to text                                 or automatic speech recognition uh which                                 reaches out to amazon's asr                                 platform okay so                                 we the the key difference the key                                 surprise for some folks will be that we                                 are now only including t comparison                                 standard in tikka app                                 and also in tikka server so that if you                                 want those extended uh sub modules for                                 scientific format parsing or the                                 sqlite                                                                that makes for a lighter tikka app a                                 lighter tika server fewer dependencies                                 no network calls no                                 http client none of that antica parts                                 are standard                                 um we've modulized tikka server uh in in                                 the same way                                 so see tikka server core is just the                                 server with absolute with no parsers but                                 then we also have the tika server                                 standard which has the standard parsers                                 in it                                 and again users will have to add sub                                 modules for scientific format parsing or                                 sql e                                  we've modularized language detection so                                 you only have to use one language                                 detector                                 detection module you don't have to pull                                 all of them in and then pick                                 only a small portion of that code we've                                 done the same with tika server so we                                 have tk server core                                 you can server classic and we're                                 starting to build out some tika server                                 client                                 we've modularized tk eval and let me                                 take a short break on this                                 so now you can drop the tkeval core jar                                 in your class path for tk server                                 and you will automatically get tikka                                 eval stats run on your files                                 this is useful when you want to do                                 automatic detection of garbled text                                 um the column on the left tig                                     was the text that we pulled out of a                                 file with t                                    language id chinese there were zero                                 common chinese words in that                                 and the top tokens if anybody can reach                                 it for for those who can read chinese is                                 all garbage                                 the ticket                                                             detection we're now pulling out german                                 with much better oov or out of                                 vocabulary rate                                 so these are some statistics they're                                 available in tk eval you can get those                                 now easily in tikka server                                 simply by dropping the eval core jar in                                 your class path                                 and you can use this in action uh or in                                 production um                                 in one particular file the text that was                                 stored in the pdf came out as the top                                 um the out of vocabulary on that is you                                 know                                     probably uh and then based on that                                 statistic you can choose to run ocr in                                 it um you can see that the eos error is                                 not perfect but it's far better than the                                 text that was extracted from that                                 pdf all right so now i'm going to talk                                 about                                 why this matters and talk about                                 why we've added tika pipes so thank you                                 nick birch for                                 crashing jvms at scale so                                 there are the usual you know catch the                                 exceptions kinds of things the parser                                 had                                 wasn't very happy with things but then                                 they're also more catastrophic things                                 like out of memory errors infinite loops                                 memory leaks                                 just code runaway forward processes all                                 of those wonderful things that can                                 happen with parsers                                 and i used to feel awful about this but                                 then i realized that tika really isn't                                 alone and that uh parsers generally                                 uh software generally has                                 vulnerabilities but parsers in specific                                 are extremely uh prone to                                 uh security issues uh whether that's                                 denial of service or                                 remote code execution or other fun                                 things kathleen fisher recently had a                                 great keynote                                 where she was talking about how                                 dangerous parsons can be these stats are                                 from her talk                                 parsers are dangerous especially when                                 you're running untrusted parsers on                                 untrusted                                 uh inputs bad things can happen um font                                 parsers                                 um four of the cves mentioned in this um                                 exploit chain are font parsers uh so i                                 found that rather amusing                                 anyways parsing's dangerous this also                                 came up recently rogue document might                                 bring the process to a halt                                 um that should never happen to you uh if                                 it does                                 um we should talk uh and you know with                                 antiqua we have had these problems we                                 have had                                 infinite loops and other issues which we                                 try to fix as we can                                 but it's it it really is a systemic                                 issue and are infinite                                 loops really that bad yes they are they                                 really really are                                 for your cpus for the environment for                                 everything they really do run for a long                                 long time                                 some might see infinitely all right um                                 so                                 in tikka we until we get verified secure                                 parsers                                 which aren't on the horizon in the near                                 term we're trying to mitigate                                 catastrophes as we can we're doing all                                 sorts of things uh code reviews uh we're                                 having                                 we have a file format or fuzzy module                                 which we just started we have two                                 terabytes of regression corpora                                 we have a mock parser uh which allows                                 you to try out                                 uh what would happen if a miss b                                      parser misbehave for you so here you                                 you add the tikka core test jar to your                                 class path                                 and then you can send an xml file with a                                 mock element in it                                 and you can have the parser do things                                 like throw an oom or do a system exit                                 which can be quite exciting uh so to run                                 tikka safely we have the fork parser                                 which forks uh                                 the parsing into another process tikka                                 batch which does the same thing pretty                                 much in a different way                                 tika server i'll talk about in detail                                 and now in                                                            which both uh use forked processes to do                                 the                                 the hard uh the the risky part of the                                 parsing                                 our overall goal is to be boring we want                                 to be so boring that um i can stop                                 giving talks on tica and it just works                                 there are obviously will always be                                 problems uh there will be out of memory                                 errors there'll be infinite loops                                 but should be able to control those by                                 itself                                 all right so evolution of tikka server                                 is tk server safe                                 well so in the beginning uh you had a                                 tikka you had client calling tikka                                 and when it went out uh with a crash uh                                 it went there's nothing there to restart                                 it uh people around the world had to                                 restart their own                                 uh tikka servers we added spawn child                                 which will become default in tikka                                    which means that                                 um a watcher process starts the server                                 uh when the client sends a file it                                 causes a problem uh                                 t couldn't crash but the watcher will                                 restart                                 the process it will also look for                                 timeouts um uh after memory errors and                                 and crashes so um                                 yeah so that that's where we that will                                 now become default in tikka                                              the spawn child mode in tega                                          while                                 all right so the tika pipes module looks                                 to solve a lot of these uh problems and                                 make                                 make make things much more uh robust                                 scalable and                                 safe the key thing is to isolate parsing                                 into its own process                                 we want to keep the iterator and the                                 command module the client separate from                                 uh the process that's uh doing the                                 parsing um we want to allow                                 for robust timing timeouts and we also                                 want to allow for really long parse                                 times                                 so let's say you want to run ocr on                                     page pdf you're not going to send that                                 to tkka server                                 because currently you have to keep the                                 http connection open                                 uh to get the response back so fetchers                                 and emitters uh the notion here is that                                 you fetch data from someplace do some                                 processing and then emit the output                                 uh fetcher looks like this uh here's a                                 file system fetcher it has a name and it                                 has a path                                 to where it should pull files from                                 uh this is kind of the output uh this is                                 all recursive metadata parser output                                 where you get the metadata and then the                                 content is stuck                                 in the xtiga content key                                 we have metadata filters started in                                    so you can say                                 from what comes out of tikka i want you                                 to map that file that                                 metadata name to something else and we                                 have emitter so it says the first                                 emitter is the solar one emitter and                                 sends something off to solar or you can                                 configure a file system emitter which                                 will send the output                                 file system this is what you send to                                 teka to say here's my fetcher here's my                                 emitter here's the key that i want to                                 use to fetch the file here's my key that                                 i want to admit the file you can                                 inject your own user metadata and you                                 can tell it what to do on a parse                                 exception                                 uh these this all of this stuff works                                 with tka server                                 with pipes and async pipes is you send                                 something and get a response back async                                 is you just send a bunch of responses                                 uh and hope everything works out okay                                 okay so the current state                                 from a scalability standpoint is a                                 client um let's say goes to s                                          bunch of bytes sends those tikka gets                                 the text back and sends those to solar                                 scaling wise this is horrible because                                 the client is                                 sloshing all these bytes all over the                                 place bad for the data center                                 um in the factory emitter idea                                 the client sends a json fetching the                                 topple tatika which sends that json to                                 a separate jbm which does which pulls                                 the bytes out of s                                                       then sends those off to solar                                 so when bad things happen um                                 it will it will restart that jvm and                                 antique will be good to go and also you                                 can have                                 obviously multiple uh worker jvms uh                                 going at the same time so you can set a                                 bunch of uh requests                                 and all of those will work in parallel                                 all right                                 um and the great thing is then you can                                 scale this across a cluster easily                                 because the client's no longer pulling                                 in all the bytes from a data source and                                 then sending all those bytes across the                                 cluster                                 you're just sending json and then tk is                                 pulling the appropriate                                 bytes doing the processing and then                                 forwarding those to                                 uh to an endpoint or emitting those to                                 an endpoint and we also have the notion                                 of a                                 fetch iterator which just kind of                                 automates this so this is a file system                                 pipes iterator which will iterate                                 through files in a file share                                 it will use the fetcher as specified and                                 we'll also use the emitter so here's an                                 example of configuring                                 a little bit of xml to say go to this                                 directory                                 pull all of those files and send them                                 off to solar                                 next step we need more tests and                                 documentation                                 more tests and documentation um                                 especially with uh dockerized                                 mod s                                                                search and hopefully vespa fairly                                 shortly                                 we also need to figure out a way to                                 package jars for tika server all right                                 so                                 in general i think i ran through that at                                 twice the speed                                 sorry about that but off we go so please                                 join the fun uh here are some links                                 and off we go to questions                                 okay thank you very much tim i think                                 this leaves uh                                 quite some time for questions so please                                 don't be shy type in your questions                                 uh in to the chat next to the                                 um to the stage uh my question tim would                                 be                                 let's say i run a enterprise search and                                 i've used tikka                                 for parsing documents uh so far                                 so what would be what would the                                 migration path look like yeah so                                 i i understand that i have to look at                                 all the modules that are available now                                 um would this be the path or how would                                 you approach it                                 sure so it at the at the very least you                                 can                                 pretty much drop in tikka server                                 standard for the old tica server                                 with the one caveat that you will no                                 longer have the um                                 the science parser modules and the                                 sqlite three and with it so you have to                                 add those manually                                 but the tica server standard should                                 should be should act the same there are                                 some differences                                 for those who want to move into the                                 pipes mode though um                                 that's where you'll have to see if you                                 know if we have a fetcher that meets                                 your needs                                 we currently cover s                                                   we want to add some other                                 fetchers and if there's an endpoint an                                 emitter that covers your needs                                 if that's the case then you can start                                 experimenting with                                 scaling that out making sure that it                                 works in your environment                                 and is performant i would encourage                                 everybody                                 to use the mock parser in uh on your dev                                 system                                 uh to see what happens when a parser                                 calls a system exit                                 which most parsers shouldn't we did have                                 one that did at one point                                 but that helps uh imitate what can                                 happen                                 with um when an om killer on the                                 operating system decides that                                 a parser is doing something horrible for                                 the for the                                 survival of the os so those are the                                 that the in in short that's how i would                                 proceed with with upgrading                                 okay uh so now there are a few questions                                 uh in the chat so the first one                                 uh does ticker                                                     the mime type detection or just the                                 extraction and passing                                 [Applause]                                 it does um it's we don't have a cl a                                 clean way of doing it um                                 but yes so if you are using the sub sub                                 modules so if you only want the                                 tca microsoft office parsers for example                                 um or if you let's say you only want the                                 pdf parser but you want to be able to                                 detect whether it's a powerpoint or an                                 xlsx file                                 um you would then need to you would you                                 if you're not using the tika standard                                 stuff which most people should                                 but you're using those little sub                                 modules you would have to include the                                 tikka                                 microsoft parsers module because that's                                 what is now doing the detection                                 for the subtype detection of office                                 files                                 so it is modularized it's not as neat as                                 we would like because we want to have a                                 lot of the stuff                                 available in tikka core for at least the                                 my magic detection which just looks at                                 the first couple of                                 the first thousand bytes of a file um                                 but yes we have                                 uh we have tried to um uh to modularize                                 uh detection as we can                                 again right sorry for people using                                 standard tika standard                                 you you won't notice any differences                                 okay                                 that sounds cool next question is um are                                 you seeing parsers for any of the cloud                                 provider                                 documents being created so uh like                                 google docs on google drive no                                 uh we i haven't um the closest that                                 comes to that is nicholas d piazza who i                                 mentioned earlier i recently opened a                                 ticket for um                                 microsoft exchange                                           onenote files uh because apparently the                                                                     onenotes are different from the regular                                 onenotes but no i haven't seen                                 folks contributing parsers or even heard                                 of                                 the need yet for those file formats                                 but committers are standing by                                 okay all right yeah okay                                 uh i mean if i look at the google drive                                 search maybe there's a place for that                                 okay um so next question is uh will                                 ticker to be a drop-in replacement for                                 use of solar who use the tikka                                 integration                                 yes it should be yeah                                 um for yes it it absolutely should be                                 again with the i'm pretty sure you're                                 not                                 bringing in the sql light dependency so                                 you're not going to lose that um                                 and again this the same thing applies                                 within with the science scientific                                 parsers                                 uh but it should be much cleaner because                                 the tikka standard is not pulling in                                 hdb components and other things that                                 used to clash with solar                                 i would still encourage you not to parse                                 files in the same jvm that solar's                                 running in                                 that's just a recipe for disaster no                                 matter what eric p will tell you                                 yeah that's so muscle okay okay uh                                 next question then maybe somehow related                                 uh for archival data                                 what are the pros and cons of parsing                                 once and storing the past data                                 versus re-parsing when re-indexing                                 i recommend something that i've never                                 seen happen in practice                                 i recommend re-parsing uh and perhaps                                 using the eval out of vocabulary stuff                                 to figure out if you're doing a better                                 job or a worse job                                 as our parsers get better we are pulling                                 out more text we're pulling out                                 more reliable text and again we measure                                 reliability with                                 that outer vocabulary statistic so we                                 know that we are getting better                                 on most files there's always the chance                                 when you move especially with                                 pdfs that as you improve generally that                                 you still might have regressions on a                                 file here or there so from an archival                                 standpoint                                 i would want to be very careful about                                 throwing out old parses where you might                                 have had good                                 good texts that you're not getting now                                 um yeah                                 so look into tk eval and and keep keep                                 refreshing uh those parses                                 as as you deem valuable                                 to your need and if you have the budget                                 to do it                                 you
YouTube URL: https://www.youtube.com/watch?v=OoEHQUlu16w


