Title: Philipp Krenn – Log Management: From grep to Full-Text Search and Back
Publication date: 2021-06-28
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Logs are everywhere. But they have gone through an interesting development over the years:

- grep: This works well as long as you have a single instance to search on. Once you need to SSH into many machines and try to piece together the results of multiple grep commands, things tend not to work that well anymore.
- Splunk: Centralizing those logs and letting users search through them with a piped language in Splunk is the logical step to fix that issue. However, the more data you centralize, the slower this will get.
- ELK: The solution to that idleness is using full-text search. Elasticsearch, in combination with Logstash and Kibana (plus Beats), gave logs a major performance boost. But at what cost?
- Loki: Reducing the scope and going back to a smart data structure combined with grep gives Loki the possibility to reduce costs while still providing good performance.
- Closing the gap: So what are the tradeoffs between the different systems, and are they potentially closing some gaps between performance and cost?

Speaker:
Philipp Krenn – https://2021.berlinbuzzwords.de/member/philipp-krenn

More: https://2021.berlinbuzzwords.de/session/log-management-grep-full-text-search-and-back
Captions: 
	                              hi                               i'm philip let's talk about log                               management or                               actually before we dive into log                               management let's take a look at                               technology and                               moves and counter moves as an                               introduction                               so back in the very old days                                well before my time we had mainframes                                and terminals that were connecting to                                them                                so you wouldn't have the power of a full                                computer on your desk but you would only                                connect to that                                mainframe remotely and                                after some time then we moved over to                                personal computers sometimes those are                                called fat clients heavy rich or                                thick clients and part of that was                                you could work offline you wouldn't need                                that mainframe and all of those                                centralized server capacities and it was                                for example                                better in graphics heavy use cases                                so it was kind of a move away from that                                only centralized resource to a more                                decentralized resource and then                                some time later on we came over to                                thin clients so i remember back when i                                started university for example we had                                citrix all over the the campus and                                everybody could connect to their setup                                or                                load their setup basically on the thin                                clients                                which was fascinating from a technology                                point of view and it gave you a lot of                                mobility                                but it was also quite a pain to actually                                get started because i remember every                                time we wanted to load this up it would                                take a couple of minutes until                                everything was there and actually                                workable                                so while interesting this was still                                kind of like painful and again another                                counter move happened after that                                so after that we kind of all got our own                                laptops and suddenly we had everything                                that we wanted                                offline again could carry it with us and                                we wouldn't need to connect to any                                remote servers again                                the university campus was saving on                                centralized server capacities                                and only needed to provide wi-fi while                                we all carried our own laptops around                                so it was kind of like the counter move                                and now                                today we have kind of like moved in the                                opposite direction again                                where pretty much everything we do is in                                the cloud                                it's maybe i don't know some shared                                google doc it's                                something on github nothing is                                really anywhere um on our end                                devices anymore but it's really just                                spread out all over the place again                                and i think this kind of like move and                                counter move in technology                                is a very nice example of how                                technologies                                often reach some limit or you hit some                                pain point                                and then you need to kind of like change                                concept or switch over to something                                um to make that pain point away and you                                normally trade that pain point for                                something else and then at some later                                point maybe the pendulum is swinging                                back again that you make that counter                                move and i think that centralized versus                                decentralized in computing is a nice                                example of that                                pendulum swinging back and forth and                                maybe at some point we'll move away from                                the cloud again                                and say like for privacy reasons or                                because our devices are so powerful                                we want to have everything locally again                                maybe                                not necessarily but what does all of                                that have to do with logs                                so that general topic of change and                                requirements and trade-off                                is also something that i want to discuss                                now in more detail around logs and                                 how those are also are moving kind of                                 like back and forth between different                                 paradigms                                 and how you can always trade your pain                                 points for                                 new opportunities and potentially also                                 new pain points again                                 so why am i talking about that i work                                 for elastic as a developer advocate so i                                 normally talk about the good                                 stuff that elastic is doing or all the                                 technologies that we have                                 and while i will cover part of that i                                 want to take a bit of a step back today                                 and show you the bigger picture of what                                 is generally happening in the log                                 ecosystem since                                 logging is kind of like pretty clear or                                 dear to our heart we want to keep that                                 and kind of like focus on what is                                 generally happening in the ecosystem                                 what are we doing what are others doing                                 and how can that                                 kind of like move log management forward                                 so this is kind of the                                 the gist of this talk so starting with                                 log management probably the                                 the first thing a lot of people are                                 doing or were using                                 was something like tail f or maybe you                                 were a little more advanced and used                                 less plus capital f so while you could                                 still follow the logs as they were                                 flowing in                                 um you could break out of that follow                                 basically                                 move up and down even search in that                                 file in the error log here                                 and then could jump back into following                                 whereas with tail you would always need                                 to                                 move out of tail switch to another                                 program to search for anything or move                                 up and down and then head back to tail                                 so less is kind of like the slightly                                 more advanced version of                                 tailing or following that file um                                 and that worked until you had this very                                 large log file                                 and you're basically that little                                 submarine trying to find                                 anything in that huge titanic                                 log file that we have here so                                 we had to kind of like switch our tools                                 a little bit and                                 one way to do that probably was or is                                 grep                                 where we can say for example we have an                                 error                                 and we want to find that error in that                                 log file but we just don't want to find                                 the error                                 but we want to find the most common                                 errors or common unique errors in that                                 log file                                 so we are sorting on the the unique                                 lines                                 um sorting recursively and getting them                                 the five                                 top ones of those so we would get the                                 top five                                 unique errors from our log file so                                 this was nice and worked and many people                                 are still using that                                 but it still has some problems so for                                 example if you have any horizontal                                 scaling                                 and you have stuff or not just one                                 node or container or whatever you have                                 but five or ten or more                                 it somehow gets painful to grab the log                                 files there it like                                 well maybe you could try to download all                                 the log wise and grab them or you can                                 have                                                                   servers and then grab there but that's                                 still all                                 not a lot of fun also if you have                                 distributed applications where it's not                                 just                                 one log file but you need to piece                                 together                                                        the wrap alone is probably not going to                                 give you a very clear picture of what is                                 happening there                                 also if you have anything containerized                                 or using kubernetes where                                 containers come and go and the logs are                                 more ephemeral                                 is also not that great in that regard                                 because if                                 the log file for a specific container                                 has been removed um                                 it's gone and you won't find what has                                 been up there anymore                                 so we need a new solution um that is                                 kind of like moving out of like                                 just searching through a centralized log                                 file                                 um and long comes or came                                 splunk for example being relatively                                 early on                                 in that log game there splunk provided                                 the centralization so if you have an                                 application                                 that you need to horizontally scale or                                 that is more distributed                                 you could just have your log forwarders                                 that would forward your logs                                 to the so-called peer nodes that store                                 the data potentially                                 replicate between them so if one of them                                 dies you still have the logs                                 so you will not lose any important log                                 messages even if a node dies and then                                 you have a search head                                 that allows you to search through all of                                 those log files or logs that have been                                 forwarded                                 by your log forwarders so                                 that is both more scalable and also much                                 easier to work in a distributed fashion                                 or                                 horizontally scaled fashion                                 so this was a big step forward for log                                 management                                 and it built in terms of queries on                                 relatively similar                                 concepts so for example i could say the                                 source is this specific                                 error log file here and then i run                                 regexquery and just search for                                 everything that is                                 fatal in there or i say oh i take the                                 ngxs logs                                 i take the one the thousand first                                 entries                                 and then get the top                                                  there                                 so it still looks kind of similar what                                 we were doing grep before and where we                                 were piping one command into the next                                 so it was kind of like a very natural                                 evolution going from that single log                                 file to a more                                 centralized approach and then working                                 with more or less similar                                 queries or concepts at least                                 also splunk generally has two approaches                                 the event index is minimally structured                                 where                                 very few fields are indexed or                                 structured                                 and the rest is just free-flowing and                                 you can                                 grab or just search through it on in                                 any unstructured way you they also have                                 now                                 metrics index which allows you a more                                 highly structured approach                                 which for example if you want to show                                 any metrics um                                 might add a lot of performance gain                                 there                                 but by default events are minimally                                 structured so you don't need to                                 parse your logs for example you don't                                 need to extract all the different                                 pieces to make the most out of that so                                 it was pretty easy to go from there just                                 from a                                 regular log file the problem is                                 speed reached kind of a bump at some                                 point                                 especially if you have a large amount of                                 log files                                 where your searches would just get                                 slower and slower because                                 with minimal structure means you have to                                 search through very large amounts of                                 data                                 um to find what you might be looking for                                 the other thing is                                 it can get pretty expensive since splunk                                 is a commercial offering so you                                 will pay a lot of money if you have a                                 lot of ingestion of data going on there                                 so it was probably time for the pendulum                                 to swing another direction again                                 and the thing that came i don't want to                                 say                                 after it but that kind of like evolved                                 out of that situation was the                                 famous elk stack elasticsearch logstash                                 kibana                                 to get logs parse them store them and                                 then                                 visualize them coming from an eco and                                 open source                                 uh background it provided a much cheaper                                 alternative                                 and also technology wise it just took a                                 very different approach                                 because elasticsearch being the data                                 store storing all the data comes from                                 full text search which leads to one of                                 the very common questions like                                 why would anybody put logs into a full                                 text search engine i                                 get it for if i want to search wikipedia                                 i use                                 full text search engine but do i really                                 do that for my logs as well                                 well kind of like the way you can see it                                 is                                 that storing the logs is kind of the                                 boring part what you want is you want to                                 find                                 your relevant logs quickly and that is                                 exactly what a full text search engine                                 basically is doing so that's why full                                 text search makes sense for logs                                 as well and the way that the data is                                 structured there if you have never                                 seen elasticsearch lucine or any other                                 systems built on them                                 is you take whatever log messages for                                 example you get in                                 you tokenize them which basically means                                 breaking up them up into                                 individual words at least in western                                 languages                                 so you have that dictionary with the                                 individual words                                 um you have the frequency of how many                                 times they are appearing and                                 in which documents so if i'm afterwards                                 searching for                                 coming or fury or whatever i have i                                 don't have to search through                                 all that text body anymore but i can                                 just                                 look up that term in a dictionary and                                 then find the documents where i have                                 those hits and retrieve                                 just those so by structuring the data                                 upfront more and doing some more work                                 there                                 it makes searching much more performant                                 and targeted                                 in the end so you have these full text                                 search capabilities it                                 lucine also allows you to do                                 aggregations filtering                                 sorting on that data so you are not                                 limited you could for example get the                                 statistics of how many                                 errors uh ones and debugs you have                                 in your your logs and just to see how                                 the distribution is and changes over                                 time                                 or you could filter down just on error                                 or fatal                                 messages or you could sort by the most                                 recent                                 log events that you have been collecting                                 so you have all the features that you                                 basically                                 need for log management a query though                                 looks very different than what you had                                 in                                 other systems before so here for example                                 i'm searching                                 logs and i'm using a boolean query using                                 the more structured nature of                                 elasticsearch                                 then you have fields for example in the                                 field log dot                                 level so we have a field level in the                                 sub document in                                 the object log um and that one                                 is limited just to error so we are                                 filtering and making better use of that                                 structured                                 nature of the data that is stored in                                 elasticsearch                                 um you could also in kibana then just                                 for example                                 filter down on a container image name                                 and i'm having a                                 java logging                                                             example in kibana i've retrieved all of                                 those                                 blocks with whatever timestamp and then                                 i could                                 only highlight for example only the                                 arrows since i have nicely broken those                                 out here so all of that structure can be                                 used to make your search                                 faster and potentially also more                                 powerful                                 and get to what you want in a faster way                                 the problem with that is that that                                 parsing is potentially                                 complicated and expensive unless you                                 write in a structured format like json                                 right away which would be highly                                 recommended                                 if you don't have that you might need to                                 write some passing rules                                 and there is of course some overhead for                                 all of that                                 structure and indexing and creating all                                 those index structures                                 to make your searches more performant                                 afterwards so it will take extra                                 overhead in terms of computation it will                                 take extra disk space and it will                                 potentially                                 take some extra memory as well to keep                                 all of those                                 index structures in memory and keep them                                 quickly available                                 so there is a cost to all of that                                 so once again it's kind of time that our                                 pendulum                                 is swinging another way or that people                                 were like okay this is this is good                                 but i still have pain points here so                                 loki from grafana um is                                 trying to take some of these pain points                                 away in a different way                                 so that the structure that loki is                                 trying to do is where                                 while elasticsearch is a general purpose                                 data store                                 loki is exactly focused on logs and is                                 trying to take some clever trade-offs to                                 make that specific problem around logs                                 more or cheaper and keep the performance                                 good while                                 picking some trade-offs there so it has                                 broken up right and read path                                 it can ingest data though it doesn't                                 need all of that high structure                                 that for example elastic surgical lucine                                 would expect and it will store the data                                 then                                 in an index which allows                                 easy access in this kind of like the                                 metadata around it and then chunks                                 which are like the actual message that                                 you have                                 in there and those chunks can for                                 example be stored on very cheap                                 object stores like amazon s                                           with the help of that                                 index finding the right chunks that you                                 want to search                                 um you can query that and by taking                                 these right trade-offs                                 having chunks in the indices and                                 potentially some result caching                                 you can get very good results um while                                 basically cutting some corners or                                 avoiding some things that                                 lucine is doing that are not strictly                                 necessary for the                                 the log use case so that what                                 loki is doing is it has a key value hash                                 that forms the so-called data stream id                                 um so for example i'm saying my                                 component is the printer and this is the                                 specific location                                 and i'm grouping the log level together                                 so i take all of those                                 three attributes i hash them together                                 and that's the stream id and                                 all log messages for this stream then                                 end up                                 in this chunk based on this id                                 and then you take that chunk compress it                                 and                                 store it for example on an object store                                 now if you                                 change any one of these attributes you                                 would get a different                                 stream id and that would go to another                                 chunk                                 so if you know for example that you want                                 to search                                 for printers in one location and that                                 specific error                                 um the index will allow you to go to a                                 chunk                                 like a small subset of all the logs and                                 then                                 search through them you don't have the                                 complicated                                 index structure for all of the messages                                 within that                                 but you can pinpoint to that right                                 subsection                                 and then run a regular expression                                 through that very efficiently                                 so that's kind of a trade-off that loki                                 is doing to keep search                                 fast without having all of those index                                 structures in the background                                 by limiting it through to that stream id                                 and basically grouping relevant sections                                 of course the important part is that                                 your searches are targeted enough that                                 you don't have to search all the chunks                                 but that you can find the right subset                                 of chunks that you want to search                                 to make the most out of them the queries                                 look kind of familiar to what you had in                                 grep so for example for my job                                 i'm searching for all errors or in kafka                                 i'm running a regular expression and                                 search for ts db ops                                 whatever i o colon                                                cassandra                                 i'm just searching for error equals                                 whatever                                 word i find after that so the syntax                                 is rather similar to what you were                                 used from grep already and then trying                                 to take                                 that centralizing the data                                 limiting it down to kind of like the                                 right subset                                 and then running a grep through that                                 subject section to avoid the pain points                                 from what we had in grab earlier                                 to make that work better again now                                 the problem here is that it's optimized                                 for this specific use case                                 so if you have extracted the the wrong                                 key value pairs for the stream id you                                 will need to search a large amount of                                 data                                 which will be slower if you have like                                 high network traffic                                 um it might cost you more on a cloud                                 provider so there are trade-offs for                                 that                                 but it is very optimized for that logs                                 use case                                 so where does that leave us are we                                 at the end of this journey like                                 everything is in the cloud we're done                                 or maybe we're not done not really we're                                 still kind of like on the way to                                 to evolve that system so there are a                                 couple of                                 interesting things going on in parallel                                 right now                                 so one interesting project that has come                                 up recently                                 is lucien grep which is calling itself a                                 greplight utility based on the scene                                 monitor                                 compiled with girl vm native image so                                 you can                                 invoke it very quickly from the shell                                 and you don't have a long jbm starter                                 time                                 also the i the general idea is um or                                 the author's idea here is that maybe you                                 don't                                 always want to write a regular                                 expression because writing regular                                 expressions                                 can be a bit sometimes challenging or                                 painful                                 and there's always this old joke that                                 what is the plural of regex                                 it's regret because nobody can ever read                                 their regular expressions again                                 so maybe you don't want to write                                 everything as a regular expression                                 and you kind of want to marry the the                                 power of lucine that you have                                 tokenization and you maybe have                                 stemming and some other advanced lucian                                 concepts                                 put those on top of the boolean query                                 and then                                 maybe still add the regular expression                                 feature to that                                 and have like that more powerful feature                                 set combined                                 with quick invocation times on the                                 command line                                 and you don't need that entire server                                 infrastructure running to search your                                 logs                                 it's a very early or in a very early                                 stage as a project                                 but it is kind of an interesting                                 approach where these different                                 things try to come together in a new way                                 to kind of avoid                                 or to to avoid pain points that we had                                 in the past                                 another topic that is always swimming                                 back and forth i would say is this more                                 structure and                                 and less structure so splunk also for                                 performance reasons                                 is adding more and more structured                                 fields and i think some of their machine                                 learning capabilities for example                                 require a pretty high structure in their                                 data to actually have                                 a machine learning algorithm make the                                 most out of that data                                 and elasticsearch which comes from that                                 highly structured site has recently                                 added                                 runtime fields where you can have a                                 message and you can                                 extract some parts like a virtual field                                 basically at runtime                                 so you could run a clock pattern or a                                 regular expression                                 to extract a specific part of a message                                 a                                 so you don't have to create all those                                 index structures                                 and b because sometimes you come up with                                 or you figure out that your log pattern                                 needs to extract some information                                 that you didn't think about at first and                                 that you want to extract                                 later on remixing all that data in the                                 right                                 index structure would be very expensive                                 so you might want to do that with a                                 runtime field                                 so it can take some pain to get started                                 away                                 and it fixes up potentially your data                                 while keeping the cost                                 maybe lower whereas splunk is trying to                                 add more structure to take some of their                                 speed                                 and um around not having enough                                 structure away                                 so it's kind of like adding features                                 from the other side                                 to make the most out of that or to avoid                                 the pain points that                                 each system had in the past um                                 another topic is for example features                                 loki has in just in the most recent                                 version in                                     um added multi-line logs which was i                                 think the most highly requested feature                                 in loki because mud line logs are a                                 thing that                                 happened quite frequently and                                 elasticsearch at the same time has                                 a lot of the features and can do i don't                                 want to say everything but can do many                                 things                                 there it's more about making it more log                                 specific and getting away from that                                 general purpose data store                                 and having more optimized data                                 structures for example for logs                                 so for example elasticsearch has                                 recently added a wildcard field which                                 allows more performant wildcards or                                 prepped like or grep queries                                 it has recently added searchable                                 snapshots which basically takes the                                 snapshot the                                 backup which can be stored on an object                                 store and can                                 mount them to let elasticsearch actually                                 search that data                                 so you can also move data to an object                                 store and don't need to keep it                                 on a hot running node anymore but you                                 can search in object store                                 or match on the text field that one is                                 pretty clever                                 so coming from the full text search side                                 um what you want in full text search is                                 you will always want to search be able                                 to search for phrases                                 and you want to have that scored the                                 relevancy like how relevant                                 is what i'm searching for how relevant                                 are these different documents                                 in logs you normally don't have that                                 much relevancy like you don't                                 really care if that log line is so much                                 more relevant than that other log line                                 it's like                                 they contain the error message that i'm                                 looking for i want those i don't care                                 about that score                                 so match only text requires less disk                                 space                                 by not extracting all of that                                 information and storing it in the index                                 structures                                 to make it more performant and cheaper                                 to run                                 for the log specific use case so there                                 it's                                 competition is keeping you sharp to make                                 you                                 move in the right direction that's                                 really what is happening here                                 it's either adding or broadening the                                 feature set or                                 sharpening the specific use case and                                 what is in there                                 so to wrap this up                                 there is this um nice quote from edward                                 stemming                                 um true or not there's some debate about                                 that but i still like it                                 it is not necessary to change survival                                 is not mandatory                                 so as a system if you stop changing and                                 adapting to the pain points and                                 requirements                                 either because ephemeral logs are                                 becoming more and more                                 common or because scale is becoming more                                 and more common                                 if your tooling cannot adapt to that                                 maybe it's not made for the future or                                 for these scenarios                                 so while it's not necessary to adapt to                                 that um                                 if you want to survive as a tool you                                 probably still have to do that                                 the one question that i'm maybe                                 expecting now is                                 what is the benchmark between the                                 different tools and show me                                 how a is much more or much faster than b                                 and i'm afraid there is no good way to                                 benchmark that                                 my favorite comic is here about the two                                 different or                                 two systems that are benchmarked on                                 under similar conditions                                 and one is much better than the other                                 and you can see under similar conditions                                 the house camp has been killed and the                                 squid is                                 thriving so even though this conditions                                 might be the same                                 maybe those are not your conditions so                                 while these vendor benchmarks or                                 so-called bench marketing can always be                                 fun it is not necessarily helpful for                                 you because                                 you potentially need a specific feature                                 set or you have specific requirements in                                 what hardware you have                                 and what your read and write ratio is or                                 what specific types of query you're                                 actually running                                 and if you change any of these                                 parameters it changes how all the                                 systems behave                                 in comparison to each other so while you                                 can always do benchmarks                                 for yourself and your use case and that                                 totally makes sense                                 as a vendor you cannot do that so i will                                 not provide any benchmarks here                                 so of course you should always benchmark                                 your own tools to make sure that they                                 don't get                                 slower or worse over time like the slow                                 boiling frog                                 problem so benchmark yourself strictly                                 um but vendor benchmarking against                                 competitors can be informative to find                                 gaps and improve them                                 but it's not what you should put out                                 because probably nobody has exactly the                                 same scenario that you were benchmarking                                 in the first place                                 to wrap it up let's have a discussion                                 about features                                 speed and cost and what is the right                                 trade-off for you                                 and where should tools be headed thanks                                 so much for joining                                 i hope you learned something if you have                                 any feedback let me know                                 and let's hop over to the discussion                                 thanks a lot for joining                                 you
YouTube URL: https://www.youtube.com/watch?v=1ERZ1tjLVoo


