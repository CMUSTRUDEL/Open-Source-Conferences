Title: Francesco Tisiot – Event-Driven applications: Apache Kafka and Python
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Code and data go together like tomato and basil; not many applications work without moving data in some way. As our applications modernise and evolve to become more event-driven, the requirements for data are changing. In this session we will explore Apache Kafka, a data streaming platform, to enable reliable real-time data integration for your applications.

We will look at the types of problems that Kafka is best at solving, and show how to use it in your own applications. Whether you have a new application or are looking to upgrade an existing one, this session includes advice on adding Kafka using the Python libraries and includes code examples (with bonus discussion of pizza toppings) to use.

With Kafka in place, many things are possible so this session also introduces Kafka Connect, a selection of pre-built connectors that you can use to route events between systems and integrate with other tools. This session is recommended for engineers and architects whose applications are ready for next-level data abilities.

Speaker:
Francesco Tisiot – https://2021.berlinbuzzwords.de/member/francesco-tisiot

More: https://2021.berlinbuzzwords.de/session/event-driven-applications-apache-kafka-and-python
Captions: 
	                              technology                               so let's start by talking about event                               driven application with apache kafka                               python                               as scott said i'm francis concealed                               developer advocate ivan which is a nice                               company building                               open source things in the cloud of your                               choice all                                managed for you so let's not waste any                                more time                                if you're here you are somehow                                interested in kind of event-driven                                application and probably                                you are familiar with python                                on the other side you may wonder                                what is apache kafka why should i                                be interested in apache kafka                                well let me tell you the full story                                if you are a developer you will be                                either working on a kind of new shiny                                application that                                you just created out of nothing and you                                are exposing to the world                                or you are part of a large company and                                you're inherited an old application and                                you have to support it                                to extend it and maybe to take it to the                                new world                                no matter if new or old i've never seen                                an application working in complete                                isolation                                you will have components within the                                application that needs to talk with each                                other                                and if you expose this application into                                the word you will have this application                                talking with other applications around                                so let me share with you a secret apache                                kafka                                is what makes this communication easy                                and reliable but now let's check                                a little bit more what we were doing in                                the past and why kafka is relevant                                nowadays                                if we go back in history few years                                we had our application that were already                                there                                and they were already kind of creating                                events                                and most of them were storing events in                                backend database or other applications                                when we're reading from the database                                and passing those up those events into                                the application itself                                most of the times however the                                communication of the events wasn't done                                for every single message                                the application was piling up a series                                of events and then                                pushing them to the database or if the                                same application or another application                                was reading from the databases                                in batches this meant that we were                                adding a constant delay between when                                an event was created in the real life                                and when we were pushing it to the                                database                                or from when the event was already                                available in the database                                and we were able to read it on the other                                side                                we now live in kind of a fast word and                                we cannot wait batch time                                we cannot wait                                                                                                                           event                                we need to build event driven                                application what are those                                applications that as soon as an event                                happen in real life                                they will immediately need to know about                                it start parsing it                                and probably take the outcome of the                                passing and push it to                                another downstream application which                                will act probably                                as another event-driven application                                creating a chain of those                                but before going in deep into how to                                create an event-driven application                                let's start by defining what is an event                                and you know we are all used now with                                for example mobile phones                                and we are all used to receive                                notification about                                something the notification tell us that                                an event                                happened did someone else send us a                                 message                                 we receive a notification did we made a                                 purchase                                 with our credit card we receive a                                 notification                                 then someone else stole our credit card                                 and made a payment                                 we receive a notification as you can                                 understand we cannot wait                                 six hours five minutes two minutes                                 of batch time before receiving that                                 information we want to receive it                                 immediately and even more in this time                                 of                                 for example pandemic we i have been                                 using                                 a lot our mobile phones in order for                                 example to                                 uh create food orders and to receive                                 food at home and by the time that we                                 switch on our mobile we select the app                                 we select the restaurant and we make the                                 order                                 we are creating a chain or a first event                                 that will create a chain of other events                                 so when we create the order the first                                 thing that                                 it will be done is that the order will                                 be sent for example to your favorite p                                  and that pizzeria will start reacting as                                 an event-driven application and you will                                 start preparing your pizza                                 at a certain point your pizza will be                                 ready                                 and it will be ready at the pizzeria not                                 at your place so it will be another                                 event                                 for the delivery people to come and pick                                 it up and take it                                 to your place now if                                 you start thinking about events you also                                 start realizing                                 that we need to communicate those events                                 past those events as quickly as we can                                 because we as i said before we live in a                                 fast word                                 so the value of the information itself                                 is strictly related to the time that it                                 takes to be delivered                                 going back to the delivery of the pizza                                 the information is useful only from the                                 time that                                 the position of the driver from the time                                 that the position is taken from the                                 driver's cell phone to the time that it                                 lands on our map the delay is minimalist                                                                               if the delay is                                               information about the                                 position of the delivery is not relevant                                 anymore                                 so we need a tool that makes this                                 communication reliable                                 easy and quick and well kafka                                 is exactly the tool but if we have to                                 think about                                 what is kafka well the basic of kafka                                 is really simple is the concept of a log                                 file                                 oh sorry a log a log where we start                                 storing our events as soon as they                                 happen so event number zero happens and                                 we store it in the log                                 event number one happens and we store it                                 two three and four                                 even more kafka is a log which is                                 up and only and immutable this means                                 that we can write only at the end of the                                 log                                 and once we write event zero in the log                                 as message                                 it's not like a record in the database                                 we cannot go there and change                                 the pizza from margarita to diavola                                 event zero will be always like that                                 if something changes the reality of                                 event number zero we will store it as a                                 new event in our log                                 kafka allow us not only to store                                 one type of events allow us to store                                 many type of events because we all know                                 that events happen in                                 different types for example we have the                                 pizza orders                                 and we can have the delivery position so                                 we will store the pizza orders in                                 one log and the delivery position in our                                 log in another log                                 and those logs in kafka terms are called                                 topics                                 so i have topic a and topic b it's                                 orders and delivered position                                 kafka in the back is not meant to                                 run on just you a unique huge server                                 is meant to be a distributed system this                                 means that when you create a kafka                                 instance you are most of the time                                 actually creating a set of nodes                                 a set of nodes that in kafka terms are                                 called brokers                                 and now if we go back to our log                                 information                                 to our topics the topics the topics are                                 stored across the brokers                                 but not only one time they we store                                 the topics across the brokers multiple                                 times                                 so for example we have our sharp pages                                 log                                 that is stored three times following a                                 parameter which is called replication                                 factor                                 of three and then we have the other                                 lock the other topic round edges that                                 could be delivery position                                 which has only two copies so replication                                 factor of two                                 why do we store multiple copies of the                                 same                                 well do we store multiple copies of the                                 same topic                                 of our brokers well because we know that                                 computers are not entirely                                 reliable and we could lose a node but                                 still                                 if you check here we are not going to                                 lose any data both the sharp edges topic                                 and the round edges topic are still                                 there available                                 so we understood how kafka works                                 on the back end now let's try to                                 understand                                 what is an event for kafka                                 it's a platform meant to store and                                 propagate events                                 well kafka is really easy when                                 it defines an event all you need to know                                 about an event if in kafka is that it is                                 composed by                                 a key and a value and kafka really                                 doesn't care about what you put                                 both in the key and the value for kafka                                 is just a series of bytes                                 you can start with really easy messages                                 for example where you store the maximum                                 temperature as key                                 and the value of it                                                    revive the three as value or you could                                 go                                 pretty wild where you store both in json                                 in the key                                 and in value and in the key you store                                 the shop name receiving the pizza order                                 together with the phone line used to                                 make the call                                 and in the value you store all the order                                 details containing the id                                 the name of the person making the order                                 and the list of pizzas                                 you can use any format that you want                                 because for kafka it's just a series of                                 bytes so for example you could use json                                 like in this case                                 which is beautiful because i can read as                                 human the world message                                 but if we have to think about                                 transmitting json over the network                                 is a little bit heavy because for every                                 field it contains both the field name                                 and the fill value if we want to send                                 the same information in a more compacted                                 way we can use other data formats                                 like for example avro that detach the                                 schema from the payload and use a schema                                 registry in order to compact                                 the message and on the other side when                                 we consume the data                                 to re it rotate the message itself                                 so now that we understood what an event                                 what a message for kafka is                                 it's time to understand how we can write                                 to kafka                                 and if we have an application in our                                 case it will be a python application                                 that writes to kafka well then it's                                 called a producer                                 and a producer produces data to a topic                                 in order to produce data to a topic all                                 the producer needs to know                                 is where to find kafka or name and port                                 of the brokers                                 how to authenticate to kafka for example                                 using ssl                                 and how to encode the data from for for                                 example the json format that was using                                 internally to the raw series of bytes                                 that kafka understands                                 old so simple on the other side if we                                 have data in a kafka topic                                 and we want to read with an application                                 well this application will be called a                                 consumer                                 and what the consumer does is it reads                                 event number zero and then communicates                                 back to kafka event number zero done                                 let's move to one reads event number one                                 communicates back to kafka one done                                 let's move to two and so on so forth                                 why communicating back to kafka is                                 really important                                 well because we know again computers are                                 not entirely                                 reliable so we know that for example the                                 consumer could fail                                 but still kafka will know until what                                 point                                 that consumer read that specific log                                 so the next time that the consumer pops                                 up kafka will know                                 which is the next item that the consumer                                 didn't pass previously                                 in order to consume data from kafka all                                 that the                                 consumer needs to know is pretty much                                 the same information as the producer                                 first of all hostname and port                                 second how to authenticate to kafka as                                 before                                 but before when we were producing data                                 we were                                 serializing now it's time to this                                 realize the code                                 from row series of bytes to for example                                 json                                 and the last information is where to                                 find the data so                                 which topic or which list of topics the                                 consumer wants to read from                                 now these were a lot of slides now it's                                 time to                                 see some actual code so it's time for a                                 nice pizza demo                                 what i created so far for this kind of                                 event is a series of notebooks a series                                 of notebooks that we will use in order                                 to interact with kafka                                 how to get kafka it's another discussion                                 you can                                 have it in on premises or you can                                 install yourself in the cloud of your                                 choice or you can use                                 a managed service like the one that ivan                                 offers                                 what they did in the first notebook here                                 is pre-create the environment                                 if we go and check on the ivan side i                                 created                                 two items one kafka instance which is                                 fully managed for you                                 and a postgres instance what we will see                                 now in the rest of the demo is how we                                 can interact with                                 kafka and then also a little secret                                 about having kafka interacting with                                 postgres                                 so let's start                                 now the first item on our list is to                                 create a producer                                 so let's click on the producer and let's                                 check what we have to do                                 we will use the kafka python library                                 which is the basic library allowing you                                 to integrate                                 python and kafka so let's install it                                 now that the library is installed we can                                 create a kafka producer                                 and in order to produce data to kafka                                 all we need to know is                                 where to find kafka hostname and port                                 how to connect using ssl and threat                                 certificates and how to serialize the                                 data                                 from json to a row series of bytes both                                 for the key and the value so we will use                                 key and value which will be json and we                                 will send them                                 as row series of bytes to kafka                                 so let's create the producer now                                 and now that the producer is created                                 let's send our first message to kafka                                 and the first message is a pizza order                                 is francesco myself sending an order for                                 pizza margarita my favorite one                                 so let's send it now                                 the record is in kafka well how can i be                                 sure that the record is in kafka                                 well i need to probably read so let's                                 create a consumer now                                 let's take the consumer and the beauty                                 of                                 jupiter notebooks is i can have multiple                                 notebooks running                                 one against the other so i create the                                 consumer alongside the producer                                 in order to create the consumer there is                                 the kafka consumer and they need to pass                                 kind of the same information                                 the client id is just the name that they                                 give to the consumer                                 let's leave out the group id for now we                                 will come back to this topic later                                 the rest of the information is kind of                                 the same as the producer where to find                                 kafka                                 how to connect using the ssl                                 certificates and now how to deserialize                                 the information that i was serializing                                 before                                 so let's create the consumer now that                                 the consumer                                 is created is ready i can check which                                 topics are available in kafka                                 and i can see that there are some                                 internal topics together with a nice                                 francesco pizza topic that i just                                 created for this purpose                                 now i can subscribe to it and confirm                                 the subscription                                 i subscribe to francesco pizza and now i                                 start reading from it                                 okay we started the thread that reads                                 from kafka and we can immediately                                 see two things the first one being that                                 the thread never ends                                 this is because we are creating now an                                 event-driven application so we                                 will be always there waiting for kafka                                 to have a message ready for us                                 so we will check with kafka hey do you                                 have a message for me                                 no we will wait and then we will check                                 again                                 the second thing that we can notice is                                 that                                 we produced an order for francesco                                 requesting a pizza margarita but we                                 don't see it on the consumer                                 well this is because by default when you                                 attach with a consumer to kafka                                 you start reading from the point in time                                 that you attach to kafka                                 so since we attach with our consumer                                 later than we produce our first                                 message we are not going to read it                                 this is the default behavior and you can                                 change that but just                                 remember that if you're attached to                                 kafka without                                 going back in history you will start                                 reading from the point in time that you                                 attached to it                                 in order to provide you a demonstration                                 that the wall pipeline producer and                                 consumer is working                                 i will now send another couple of events                                 and this now it comes the most kind of                                 problematic part of my talk                                 because i have two new orders one from                                 adele                                 ordering a pizza y and another full mark                                 or during a pizza with chocolate                                 now um i believe you understood that i'm                                 italian                                 so ordering a pizza wire or a pizza with                                 chocolate in italy is not the best thing                                 that you can do                                 however it's your personal choice you                                 can do whatever you want i'm just                                 suggesting                                 if you come to italy try to avoid them                                 so                                 let's now send the orders and let's                                 check                                 if the pipeline is working by checking                                 on the consumer side if we can see them                                 so let's run the producer and                                 immediately                                 we see that we receive both a delay mark                                 order so the wall pipeline producer and                                 consumer                                 is working now that we check the basics                                 let's go back to a little bit more                                 slides                                 and let's talk now                                 about the data in kafka so we said that                                 kafka and the topics in kafka are up and                                 only and immutable                                 this means that we will put data there                                 and once the data is there                                 we cannot change it but of course                                 we maybe don't want to store the data in                                 kafka forever                                 maybe we want only to use kafka storage                                 for like                                 the la the the latest six months of our                                 data set                                 for this we can use what are called                                 topic retention policies                                 so we can tell kafka keep the events in                                 kafka                                 either for for example six months three                                 weeks                                 two hours or if we want to be sure about                                 how much disk is used is used we can say                                 keep the data in kafka until the                                 specific topic                                 reaches for example                                                  then delete the oldest chunk                                 and let it grow again it reaches                                    gigabytes and delete the old chunk                                 you can also use both if you want to                                 have better control over both                                 disk and side and timing so                                 let's now think a little bit more about                                 the size and the size of                                 the log the topic we know and i've                                 just told you that kafka is a platform                                 in order to that allow you to host                                 a lot of events but if you think that                                 we store events in a topic and that                                 topic                                 is stored in a broker this means that                                 either we have to define the amount of                                 events that we want to start based on                                 the size of the disk of the broker                                 or we have to size the disk of the                                 broker depending on how many                                 events we want to store in a precise                                 topic which                                 is a weird trade-off for a platform that                                 aims to store a huge amount of data                                 because in case we have a huge topic we                                 will need huge disk sizes                                 in order to store all the events related                                 to the topic                                 likely for us kafka doesn't really                                 impose this trade-off                                 between number of events or size of the                                 the                                 topic and disk size because it has the                                 concept of topic partitions                                 topic partition is a way of                                 dividing the events of the same type                                 belonging to the same topic                                 into subtopics so if we go back to the                                 analogy of my pizza orders                                 i could divide the events pizza orders                                 into different partitions depending for                                 example from the                                 restaurant receiving the order so it                                 could have mario's order landing the                                 blue partition                                 luigi's order landing in the uh                                 yellow partition in francesco's order                                 landing in the red partition                                 now if we go back to our                                 cluster what is stored in each node                                 is not the wall topic is just a                                 partition                                 so this means that the trade-off is not                                 actually between the                                 wall topic disk size and the                                 sorry the wall topic size and the disk                                 size of a broker but it is between just                                 a partition                                 and the this size of a broker this also                                 means that if we                                 know that we will have a huge topic with                                 a huge amount of data                                 but we have a smaller disk well we just                                 need more partition to fit in                                 and as you can see here also partitions                                 are not stored                                 once but are stored following the                                 replication factor of configuration so                                 this means that even here if we lose a                                 node                                 we are not going to lose any data                                 now it's interesting to understand how                                 you                                 select a partition when you create a                                 topic with multiple partitions                                 well you do that usually in the usu                                 using                                 usually the key part of the message                                 and what kafka does by default is it                                 takes the key                                 attaches it and takes the result of the                                 hash to drive the partition selection                                 ensuring that all messages having the                                 same key will                                 end up in the same partition you can                                 however write your                                 own custom partitioner or you can send                                 messages without the key                                 in that case kafka will select a                                 partition in round robin fashion                                 but you may want to think carefully                                 about                                 which partition you select when you send                                 the message                                 why is that well it's because ordering                                 let me show you a little example we have                                 our producer in python that produces                                 data to a topic with two partitions                                 and we have our consumer now we will                                 have a really                                 simple case where we have only three                                 events                                 the blue event first the yellow event                                 second                                 and the red event third when pushing                                 those events to kafka                                 the event number the blue event will                                 land on partition zero                                 the yellow event will land on partition                                                                                                  partition                                         now when reading from kafka it could                                 happen will not always be the case but                                 it could happen that we will read the                                 events in this sequence                                 blue event first red event second                                 yellow and third why is that well                                 because                                 when we start using partitioning we have                                 to give up on global ordering                                 when we use partition kafka ensures that                                 the correct ordering of events only per                                 partition                                 so when we start using partitions we                                 have to think about for which order we                                 care about the                                 related ordering in the case of pizza                                 orders                                 it's a good example to use the                                 restaurant name because we                                 care about if client a made the order                                 before client b at the same restaurant                                 but we don't care if client a made the                                 order before or after client b on a                                 different restaurant                                 so we understood that partitioning                                 is good because allow us to have a                                 better trade-off between                                 topic size and this space on the other                                 side with partitioning we have to give                                 up on global ordering                                 in order and kafka only ensures the                                 correct ordering per partition                                 but if we think about a topic                                 we can safely assume that he is just                                 kind of a thread                                 writing one event after the other                                 as messages so we can also somehow                                 assume that                                 the throughput of writing to kafka                                 is given by this thread writing one                                 event after the other                                 on the other side if we have more                                 partition we can scale out because                                 now with more partition we have more                                 independent threads                                 that can write in parallel so we can                                 have                                 much many producer                                 write into kafka and also we can have                                 many consumer reading from kafka                                 still when we have many consumers                                 reading from kafka we want to                                 read all the data which are present in                                 the precise topic                                 but probably we don't want to read the                                 same message twice                                 we don't for example want to make the                                 same pizza order twice                                 in order to avoid that situation what                                 kafka does is                                 that it assigns to each consumer                                 a non-overlapping subset of partitions                                 if these words doesn't make much sense                                 well let me show you                                 in our example we have three partitions                                 blue yellow and red and two consumers                                 what kafka will do for example is to                                 assign the blue partition to consumer                                 one                                 and the yellow and red partition to                                 consumer too                                 now again after all this talk it's time                                 to                                 show a little bit of demo so let's go                                 back to our notebook                                 and let's check out now                                 the partitions let's create a new                                 producer                                 there we are we create a new producer                                 is not really needed but let's create                                 for the sake of creating                                 and now we will create a new topic                                 and we will interact with kafka admin                                 client because for this new topic                                 we want to set the number of partitions                                 to choose so we create the kafka admin                                 client                                 and then we create a new topic called                                 the same francesco pizza underscore                                 partition                                 with two partitions                                 let's run this okay we didn't have any                                 errors so the topic is created now                                 before start sending data                                 let me create two consumers                                 let's go back to my set of notebooks                                 i can create the first consumer over                                 here                                 and just to show you i'm using kind of                                 the same settings as before                                 let me start the first consumer and now                                 let me start                                 the second consumer down there                                 there we are let's close this okay                                 so i have two consumer top                                 and bottom reading from kafka                                 i have a topic with two partitions                                 now when i'm sending data                                 since i have two consumers i'm sending                                 two messages with a slightly different                                 key                                 the top one has the key ed zero                                 the bottom one has the key ed                                     everything works as i told you so far                                 i would expect that for example the top                                 message lands in partition zero                                 and the bottom message lands in                                 partition one on the consumer side i                                 expect since i have two consumers                                 that one will only consume data from                                 partition zero                                 and the other one will consume only data                                 from partition one                                 so let's check if all the theory is true                                 let's send the data set and exactly as i                                 told you so far                                 i have the top consumer reading only one                                 of the message                                 and the bottom consumer reading only the                                 other if we check                                 what those two little fields mean this                                 means that the top consumer is reading                                 from partition zero offset                                   first message of partition                                              consumer is reading from partition                                   offset                                   first message of partition                                           on the producer side i send couple more                                 messages                                 reusing the same keys i will expect                                 that the uh order from mac ordering a                                 nice pizza with banana                                 since it's reusing the same key as the                                 frame holder                                 will land on the same partition and the                                 same for                                 jan and adele so expect frank sorry i                                 expect mac                                 to be received by the same consumer that                                 received the order for frank                                 and the same for jan and nadella let's                                 try this out                                 let's run this as expected                                 the top consumer is reading from                                 partition zero offset one second message                                 of partition zero                                 or reading the order for mac and the                                 bottom one                                 same thing jan and adele so all working                                 as expected                                 now let's go to a little bit more slides                                 so far we saw something very simple                                 one or more threads of a producer one or                                 more threads of a consumer                                 however when we read a message from                                 kafka                                 kafka is not going to delete that                                 message making it available for other                                 application                                 to read it again if we go back to our                                 pizza analogy we could have                                 our topic with in this case true                                 consumer that could be our pizza makers                                 they want to consume all the pizza                                 orders                                 from the topic but they don't want to                                 read the same order twice they don't                                 want to make the same pizza twice                                 on the other side since the pizza order                                 is there i could have my billing person                                 that will want to receive a separate                                 copy of all the pizza orders in order to                                 make the bills                                 and this pizza this billing person will                                 want to                                 read the topic at its own pace that has                                 nothing to do with the two pizza makers                                 with kafka it's extremely easy to do                                 that because it has the concept of                                 consumer group                                 so i just need to define the two                                 consumer groups the two pizza makers                                 part of the same consumer group                                 and then i will define my billing person                                 as part of the new consumer group                                 if i do this setting well kafka will                                 send                                 another copy of the same data in the                                 topic                                 also to the other consumer and if you                                 remember                                 at the beginning when i created the                                 first consumer i told you                                 well the group id was something that we                                 will discuss later                                 well that group id is actually how you                                 define different consumer groups                                 so just a string lets you have                                 multiple consumer working one against                                 the other not to consume all messages                                 or if you change the string you will                                 have the consumers                                 defined as different applications                                 the last bit that i want to talk you                                 about is the fact that                                 so far we basically wrote the call to do                                 everything we wrote the code to push                                 data to kafka                                 we wrote the code to read the data from                                 kafka                                 on the other side especially if you are                                 if kafka is not the first tool in your                                 company                                 you will have other tools in your data                                 ecosystem                                 and you will want to integrate kafka                                 with those tools                                 and probably you don't want to reinvent                                 the wheel again you don't want to code                                 all the interactions between any of your                                 data tools and kafka                                 it would be really nice if a pre-built                                 framework could exist                                 in order to make the integration really                                 easy                                 and reliable well let me tell you                                 another secret                                 that framework exists and is called                                 kafka connect                                 with kafka connect i can start                                 integrating kafka with                                 any source or target available                                 for example i have my data in postgres                                 database or in cassandra in google pub                                 sub                                 i can use kafka connect to ingest those                                 that data                                 into topics on the other side i have                                 data                                 installed in kafka topics and i want to                                 send this data set to another technology                                 in my company ecosystem                                 again kafka connect enables you to do                                 that                                 kafka connect also enables you to evolve                                 existing application                                 so if you have your old application in                                 python that was                                 right into a database and you want to                                 take this application into the kind of                                 event-driven world                                 well you want to do that but still you                                 don't                                 want to change the working code you want                                 to leave                                 the application and the database as they                                 are working now                                 what you can do in order to include                                 kafka in the picture is to use                                 some sort of change data capture                                 mechanism that will track any changes                                 happening in a table or a set of tables                                 in the database and propagate                                 those as messages in a kafka topic                                 and you can do that easily with kafka                                 connect                                 also kafka connect allows you to                                 distribute events                                 so once you have your application                                 writing data to a kafka topic                                 and you have for example your team a                                 that wants the data in a particular gdbc                                 database                                 well you can send a copy of that topic                                 data to that database                                 you have another team that wants another                                 copy                                 in bigquery it's just another kafka                                 connect                                 you want to use um                                 s                                                                        your data set                                 well again it's just another kafka                                 connect                                 the beauty of connect is that it makes                                 all this                                 push and pull operation available just                                 as config file and a thread or multiple                                 threads of kafka connect                                 the other beauty that i will show you                                 now is that with ivan you have also                                 kafka connect as a managed service                                 so let me show you quickly                                 if we go back to our notebook let's now                                 get rid of a little bit of things that                                 are not useful                                 anymore and let me create                                 uh new producer again                                 just for the sake of creating another                                 new producer and this time                                 i will not send only francesco is                                 ordering a pizza margarita                                 but okay together with that payload i                                 will                                 also send some information about how the                                 message is structured                                 so i will tell to kafka look that the                                 key is composed by an integer called id                                 and the value is composed by two strings                                 one                                 containing the name of the person                                 ordering the pizza and the other                                 is the pizza itself why i'm doing that                                 well because i want to make the life                                 easier for kafka connect                                 to be able to understand what's the                                 structure of each message                                 and populate guess what a postgres                                 table with the content of this kafka                                 topic                                 so let's define the key and the value                                 and now i'm sending three nice messages                                 to kafka                                 i'm sending both the schema and the                                 payload as i said before and i'm sending                                 three messages one for                                 frank ordering a piece of margarita one                                 for dan                                 ordering a pizza with fries and one for                                 jan ordering a pizza with mushrooms                                 so the data now is in a kafka topic                                 let's check out kafka connect i told you                                 before                                 kafka connect if you ever manage service                                 like the one in ivan                                 you just need a config file that tells                                 which is the topic that you want to                                 take the data from and which is for                                 example your postgres target                                 let me show you the config file in our                                 case                                 if we check the kafka connect setup this                                 is all the information that we have to                                 give to kafka connect so we are saying                                 that we want to take the data from                                 the francesco pizza schema topic and we                                 want to create                                 a kafka connect connector called sync                                 kafka postgres                                 which is a jdbc sync connector we are                                 syncing the data to a jdbc                                 database which is a postgres database we                                  are also telling that the value                                  what we store in the value is a json and                                  this is the url that we are going to use                                  in order to connect                                  to postgres using a very secure new pg                                  user                                  and new password password                                               so let me copy this json configuration                                  file                                  and let me now go to my                                  ivan services in the kafka service                                  now i can go to the connectors                                  and create a new connector                                  this is fetching now the list of                                  available connectors and i'm selecting                                  the jdbc sync                                  now here i can see a list of fields that                                  i could fill                                  manually all things have my json                                  configuration file                                  i can copy and paste                                  in here and click apply and this will                                  parse and fill                                  all the parameters for me so now i can                                  create the connector but before doing                                  that i want to show you that i'm not                                  lying                                  so i want to show you the postgres                                  database which is                                  behind so i have my berlin buzzwords                                  database i'm connecting to it                                  i have my default database                                  the publish public schema which has                                  no tables so now let me go back to                                  my console and create my new connector                                  the new connector is created and now is                                  running                                  all good let's go and check if something                                  happens happen on the database so let me                                  refresh now the list of tables                                  oops not this one                                  just a second the gui is doing tricks on                                  me                                  let's refresh the table                                  no                                  yeah sorry this is the classic demo                                  effect                                  now wait um                                  no and                                  no okay                                  let me try now to refresh the list of                                  tables and i have my francesco pizza                                  schema table                                  if i double click on it i can see that                                  in the data                                  i have my three rows as expected the                                  same three rows that i sent before                                  with my python producer now if i go back                                  to my python producer                                  and now i send another record with                                  giuseppe ordering a nice pizza y                                  this is now produced let me go back to                                  the database                                  and now i now try to refresh and i                                  immediately see                                  also giuseppe holder appearing in the                                  database                                  so what kafka connect did is                                  it took the data from the topic it went                                  to postgres                                  it found out that there was no target                                  table it created the target table and it                                  started populating the target table                                  and now every time there is a new event                                  a new record in the kafka topic                                  it will send it to the postgres table                                  so in order to go back to a little bit                                  understand what we told tonight                                  we understood a lot of things we                                  understood why create                                  event driven application is crucial what                                  is kafka role in this                                  how we can use python in order to                                  interact with kafka to create                                  producers consumers to have multiple                                  consumers                                  working one against the other and using                                  partitions and also to declare                                  multiple applications and we finished                                  with some nice concept about                                  kafka connect that allow us to integrate                                  kafka within the existing data ecosystem                                  if you want to have some more resources                                  well let me give you them                                  first of all it's my twitter handle you                                  can reach out to me if you have any                                  questions regarding                                  kafka python or pizza choices                                  then uh you have the first link which is                                  a github repository containing the                                  notebooks that i've been showing you                                  today                                  so you will be able to create the                                  resources automatically in ivan                                  start playing with kafka and understand                                  the beauties of kafka within the python                                  notebooks                                  then if you want to try kafka                                  but you don't have any streaming data                                  set well i also created for you                                  a nice fake pizza                                  creation fake pizza orders                                  producer that will start producing fake                                  pizza orders                                  in a streaming mode for you the last bit                                  is                                  if you want to try kafka but you don't                                  have kafka                                  well as said before ivan my company                                  offers that as a managed service                                  and we offer not only kafka but also as                                  said kafka connect and                                  mirror maker so just come there we have                                  a nice free trial that lasts one month                                  just check us out and please let me know                                  what you think about it                                  i hope in this like                                                    you an idea of event driven applications                                  python and kafka if you have any                                  questions i'm here to                                  reply to all of them thank you very much                                  thank you francesco uh we have one                                  question the first is um                                  i saw you use jupiter notebooks in your                                  demo are there other tools that you like                                  to use with kafka                                  um yeah so there is a                                  really huge ecosystem around kafka                                  one which i like is kafka cat                                  which is a tool that allows you to                                  basically from the common line                                  to interact with kafka so you can start                                  producing data to kafka you can start                                  reading data from kafka and                                  is especially useful when you start with                                  kafka                                  especially when you start writing your                                  own code against kafka because you could                                  as i said earlier on for example you can                                  start reading from kafka                                  but you miss all the messages that were                                  sent before you attached to kafka with a                                  consumer                                  with kafka with kafka cat allows you                                  with few                                  flags and fields to fix all these items                                  so you can actually                                  use it in order to understand if your                                  code is doing what you're expecting                                  if the data looks as you expect and so                                  on and so forth                                  you
YouTube URL: https://www.youtube.com/watch?v=uXd0CBAjnqw


