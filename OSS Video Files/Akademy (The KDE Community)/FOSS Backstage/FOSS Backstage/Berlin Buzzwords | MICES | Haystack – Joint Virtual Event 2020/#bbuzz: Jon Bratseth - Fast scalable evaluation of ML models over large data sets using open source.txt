Title: #bbuzz: Jon Bratseth - Fast scalable evaluation of ML models over large data sets using open source
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/fast-and-scalable-evaluation-machine-learned-models-over-large-data-sets-using-open-source

Modern solutions to search and recommendation require evaluating machine-learned models over large data sets with low latency. Producing the best results typically require combining fast (approximate) nearest neighbour search in vector spaces to limit candidates, filtering to surface only the appropriate subset of results in each case, and evaluation of more complex ML models such as deep neural nets computing over both vectors and semantic features. Combining these needs into a working and scalable solution is a large challenge as separate components solving for each requirement cannot be composed into a scalable whole for fundamental reasons.

This talk will explain the architectural challenges of this problem, show the advantages of solving it on concrete cases and introduce an open source engine - Vespa.ai - that provides a scalable solution by implementing all the elements in a single distributed execution.
Captions: 
	                              hey I'm unit sets the architect of                               respite a I and I'll be talking about                               fast and scalable regulation on machine                               learning models especially over large                               datasets many of you probably know but                               if not there's an ongoing revolution                               happening in search a kay search to                               total where people are moving from text                               tokens token based retrieval plus                                relevance using typically fairly small                                set of scaler features and gradient                                boosted trees for machine machine                                learning for relevance to embedding both                                the queries and the documents in a                                vector space and doing which we will buy                                nearest neighbor in this vector space                                and then doing relevance typically using                                some variant of deep neural Nets which                                means using large tensors with maybe                                thousands or millions so features in                                addition to this change happening in                                search these technology on the right the                                vector embeddings and so on is also used                                in another set of areas that are using                                typically the same technologies such as                                recommendation personalization ad                                targeting and so on so while there are                                good technologies for each of these                                pieces that you want to be together to                                make a solution on the right it's hard                                to productionize it as I'll talk about                                in a minute so the pieces are typically                                some kind of search engine and library                                for doing approximate nearest neighbor                                search and some kind of mobile server or                                library for evaluating these deep neural                                nets right so each of these pieces are                                good and if you're doing submitting to a                                chi level competition or something like                                that it's quite easy to                                put these together and make it work for                                your submission and that's it right but                                when you want to productionize it you                                run into bernal challenges combining                                these things with good performance is                                challenging in practice if you're doing                                a production solution you typically want                                to combine nearest neighbor search with                                the query features because we solutions                                typically have filters for example if                                you are searching for new news articles                                you want to filter out some publications                                for some customers or some languages or                                comte resource and like that and all                                kinds of solutions have similar business                                needs right and in addition while this                                revolution is ongoing and text embedding                                since ah are becoming better typically                                you get good results only by combining                                these embedding techniques with                                traditional text search beyond                                          on so you need to run a hybrid where you                                do typically do text matching based on                                the both neural nets and the traditional                                text search and Khobar and features of                                both and return that and that gives you                                the best results right so how do you                                combine these things right you need to                                do the text search or the search or                                filters in some way in a search engine                                and then use nearest-neighbor library to                                search for nearest neighbors and then                                you need to combine the results somehow                                if you just do that Maile it will be                                very expensive because you're doing two                                different search risks that may give you                                completely                                this young results and then you need to                                go buy them somehow and also how do you                                know that you're asking for enough to                                actually be able to combine these into                                single results that's another hard                                problem right if you                                the filters filters out                                               documents then you probably won't find                                enough matches in your nearest neighbor                                search to even return a result right so                                this is a hard problem and production                                icing it is also challenging because in                                a production service you need things                                like sustain will to climb out dates                                including removal of documents and so on                                which is libraries typically don't do                                very well but that doesn't matter for                                competitions as all but it matters a lot                                for wheeled production systems and you                                also need reasonably fast restart times                                so the libraries that are only doing                                this stuff in memory and not persisting                                 won't really work well in practice right                                 some of them do and some will not if you                                 are have dis change systems for this and                                 you update them separately then you also                                 need to deal with the case where the                                 update succeeds in one and not the other                                 and they diverge requirements or lastly                                 scaling these solutions is also pretty                                 difficult this means is when you scale                                 to more data or more CPU per query you                                 need to partition your content and                                 spread it or many nodes right now you                                 have the problem that you can't really                                 the model inference that you want to do                                 on a subset of your results or a                                 different mode because that will quickly                                 saturate your network for example if you                                 have a                                                                 do thousand dogs per query if you have                                 actors with                                                           your total capacity will be                                             per second and adding more nodes won't                                 help because you saturate network                                 so model servers won't really help you                                 and more so you need some kind of local                                 level inference library that you need to                                 integrate on all of the content                                 partitions which is a lot more work and                                 more challenging you are the same kind                                 of problem with approximate nearest                                 neighbor integration so obvious all this                                 well one way to do it is to just use                                 vespa a I which is a open source                                 platform that's supports all of these                                 things out of the box it started as a                                 web search engine a long time ago                                 so it has all the traditional text                                 search features text-based relevance                                 weight positions linguistic stemming and                                 so on                                 BM                                                               important for scaling text search or                                 tokens optimized support for gradients                                 decision trees text snippet iing that                                 you want to do in text search and soul                                 but it also has support for nearest                                 neighbor search and approximate nearest                                 neighbor search in vector spaces support                                 for adding tensor they taught your                                 documents and queries and doing tensor                                 mathematics integration with our next                                 intensive flow to import complex machine                                 learning models directly and running                                 them on the content nodes so you get                                 this scaling I just talked about                                 for free and you can combine all these                                 features in a single query and in a                                 single reference model so you can get                                 the best of both worlds and experiments                                 and so on with these different features                                 and lastly it's for high availability                                 production systems so you can change the                                 hardware change the machine there models                                 change the data on low change and so on                                 while you're serving and writing without                                 interruption and these systems are I                                 mean West buys built to scale to                                 hundreds obedience look                                 hundreds of thousands of queries per                                 second and can typically do a couple of                                 tenths of thousands so rice per mole per                                 second sustained and that includes                                 rights that remove documents change                                 fields all of these things so I won't be                                 talking too much about west by itself                                 but mention some of its usages so that                                 you can be assured it's a real                                 production system use it extends layout                                 the company that employs me with this                                 which is Verizon Media you're serving                                 over a billion users red Vespa about                                                                                                        use cases are delivering personalized                                 content to all the users that visit                                 Yahoo pages and so on which means                                 evaluating a burn show while doing all                                 the things I just talked about really                                 very much the user to a vector space and                                 do a vector search to come up with best                                 articles and real machine learning                                 models to fine-tune what you are                                 returning and so on and we do that for                                 every user that is visiting one of these                                 sites in real time when they are loading                                 the page and we are doing the same kind                                 of thing the AB network owned by the                                 company which is the third largest in                                 the work we're doing similar things but                                 even more complex because you take                                 bidding into counts and so and all that                                 friends on RESPA and is serving in real                                 time so just a quick overview oh that's                                 why it's a two-tier system you have a                                 stateless Java container on top that                                 handles the incoming inquiries rights                                 and so on or you can have multiple                                 different container clusters if you like                                 below that you have content clusters                                 that stores                                 actual contents maintain Reverse indices                                 for texts indices for vector                                 nearest-neighbor searching and so on and                                 which is doing all the distributed query                                 execution including finding the matress                                 evaluating machine their models and so                                 on because these systems can contain no                                 many nodes many processing zone we also                                 have an administration and config                                 cluster that sets up and manages these                                 nodes for you and what the user is                                 seeing is a more high-level abstraction                                 which we call an application package                                 which I'll show you an example later the                                 application package basically describes                                 the system that you want to run and                                 complain stand a Java components that                                 you want to run the machine learn models                                 and so when you work with the                                 application you just change the                                 application package and deploy it and                                 the system will safely carry out the                                 change from the currently running system                                 to the system described by the new                                 version of the application package so we                                 typically do this in a CD fashion where                                 you just we have a process that turns                                 from github or whatever you using                                 building your application package and                                 just submitting it to this and it will                                 roll it out safely in production for                                 others approximate nearest neighbor                                 searches work on vespa for user it's                                 just another query item that you can                                 combine with any others in the query                                 tree so you can combine text search and                                 nearest neighbor in the same query and                                 even have multiple nearest neighbor                                 operators or different fields or                                 whatever in the same query the                                 approximate nearest neighbor                                 implementation we use is based on the h                                 sv algorithm which is a network                                 algorithm which is fastest algorithms                                 generally we have our own implementation                                 that the live worst                                 on the needs I talked about earlier like                                 supporting removal of modes from the                                 graph and so on and it also works                                 efficiently with other query turn so you                                 can combine into the filters and so on                                 and still do an efficient approximate                                 nearest neighbor search now this model                                 inference work in Vespa                                 so Vespa has tensor data model where you                                 can add tensors to both documents and                                 queries and the application package so a                                 tensor is just multi-dimensional                                 collection of numbers each of the                                 dimensions can be sparse or dance and                                 you can combine for sometimes dimensions                                 in the same tensor as I showed an                                 example over here where you have a two                                 dimensional tensor with a sparse key and                                 a dense vector so it's really a map or                                 vectors right then you can do tensor                                 math to express machined or models or                                 business logic over these tensors                                 there's a small set of core operations                                 which we use in our cancer engine for                                 optimization and that may have a larger                                 set or higher level functions which are                                 the ones you will typically use in your                                 models but which maps to those primitive                                 functions that we have joined and map                                 and so on which is quite neat but not of                                 interesting for users so yet you just                                 use the high level methods or if you                                 don't want to write your expressions by                                 hand you can just deploy tensorflow                                 or omx or extra boost or like GBM models                                 directly in Vespa and let's Pavel do the                                 translation automatically when you                                 deploy the model so we have our own                                 tensor execution engine inside Vespa                                 that                                 optimized for repeated execution or the                                 models or many data which is what we                                 typically want to do in this kind of                                 systems right                                 you're not just evaluating over a single                                 data point per query but you're                                 evaluating our many data points articles                                 or movies or authorities and just to                                 show a quick example or the hybrid model                                 thing I talked about earlier what we see                                 really almost every time when we look at                                 the performance we get out of these                                 various models is that you don't get the                                 best models by using either some                                 traditional texts features or by using a                                 neural net model but you get the very                                 best performance by combining both and                                 here's a very simple example here where                                 we have some traditional text features                                 here in Boehm Google rank profile and                                 another right profile which is just the                                 distance in a vector space or this                                 embedding and then we have a hybrid                                 model which is just some or ball things                                 and that all performs children so it's a                                 very simple example because it's from                                 one of our sample applications for the                                 daily stress this point so I'm going to                                 go to another example application a bit                                 more in depth and I've chosen to                                 application that we call core                                        vespa both AI when the pondan pandemic                                 broke out the Allen Institute released                                 data set or initially for Kittleson and                                 now about on here in turkey Towson                                 papers about the corona virus at least                                 related to the corona virus somehow and                                 my team turned around to take a week or                                 two out to build the tool to help                                 exploring these data sets so that                                 researchers secured                                 more quickly do science to learn both                                 the new disease which seemed like a                                 important thing to do at the time                                 so these combined traditional text                                 search features with article similarity                                 search and also grouping and filtering                                 which is something it typical to do when                                 you do exploration and here everything                                 is open both the datasets and also the                                 best by itself but also the respiration                                 that incorporates that implements the                                 core                                                                    that we built on top so that's the                                 advantage of a stitch open data set                                 everything is open sourced this is what                                 JJ said the data set is very small just                                 two hundred thousand hundred and thirty                                 thousand articles but vespa scales to                                 about a million times as much content                                 without really changing anything other                                 than adding more notes videos you need                                 more resources for that obviously so let                                 me see the presentation and show you the                                 core                                                                     is the front page here you can write a                                 query as you would expect but I'll just                                 click on one of these now this one for                                 example so this is a rather complex                                 query and you get results as you would                                 expect and here you see all the matches                                 that you get in various sources journals                                 and so on so this is grouping feature in                                 vespa and then you can also do a search                                 for similar articles here and what we're                                 doing them is adding this related to                                 term to the query which is just picked                                 up by a custom java component in this                                 application that fetches that                                 to go from Vespa fetches the embedding                                 vector of data article and then adds                                 that embedding vector to the query that                                 is then sent down to get combination of                                 the text features that you added any                                 query here to a nearest neighbor search                                 over that article so you get the                                 combination of both and that's very                                 useful when you are exploring right                                 because you have an article that                                 represents somehow the topic you are                                 interested in and then you combine that                                 with text search features that more                                 precisely expresses conditions on what                                 you're interested in right you can also                                 enter the article itself which is just                                 served from Vespa as well and here you                                 can also do a similar article search or                                 different embedding vectors that are                                 provided things like that okay so how is                                 this implemented let's go into it a bit                                 more in detail so this is github repo                                 for the front end and we have a separate                                 repo for the back end which is no sorry                                 one link for the for the vespa                                 application which is the example of an                                 application package which I mentioned                                 before so this is the repo for the vespa                                 application I'll go to what it contains                                 but first I have it checked out here so                                 I checked out this ribbon go to source                                 and here you can see the slice of the                                 whole thing so it's it contains like GBM                                 model that we have been experimenting                                 with so there's that's a lot of lines of                                 codes but of course also generated by                                 that machine learning but the work on                                 that is just about                                     lines are cold implementing this entire                                 core                                                                    any size you want and you can combine                                 vector similarity and text search snip a                                 team grouping and aggregation and all                                 these things so let's look at what it                                 actually contains so you have the                                 application itself which basically                                 contains these two files a services file                                 which case describes the clusters that                                 you want to run in this case we run wall                                 mode a stateless Java or container                                 clusters and one content cluster that                                 holds the contents and the container                                 cluster we have some custom Java                                 components which we'll take a quick look                                 at later then we just specifies the                                 resources that is cluster should run on                                 these trends on the public respite lab                                 and then we can just specify the                                 resources we will have and deploy it and                                 the system will get those resources on a                                 BS and run in this case we just                                 specified the real the sources of each                                 node and then we say we want from four                                 to two four nodes depending on the load                                 we are seen for the content cluster we                                 there so a little bit attuning year and                                 also tuning all the snippets and apart                                 from that we just referenced a single                                 schema that we use for the documents and                                 again I'm specifying the resources and                                 that's it and then we have a deployment                                 text map which specifies where this                                 route drum and this just runs in a                                 single ABS region if you're self hosting                                 Vespa is really the same thing but                                 instead of saying this you just list the                                 actual hosts that you want to run for                                 this cluster here that's the only                                 difference                                 and then you don't need the Department                                 XML flow so the else is here there's the                                 machine learned light GBM mobile and                                 some certificates and a specification of                                 the stuff you can send in a query which                                 is these embedding vectors and then                                 there's the single schema were to use                                 that describes the data we have air                                 which is a single type representing the                                 scholarly article itself so it has a                                 bunch of fields as you would expect with                                 the title the content itself citations                                 and whatnot and in addition some                                 embedding vectors so we have an                                 embedding vector for the abstract for a                                 title and then we have another embedding                                 vector which is supplied by the alum                                 Institute team which is called a                                 spectrum building and those are all                                 single dimension dance tensors the                                 scheme also describes how we can rank or                                 alternatively evaluate machine Mobile's                                 or this kind of data which is what we                                 call the run                                 profile so there's a bunch of those here                                 I won't go into them in detail but                                 there's one that just to normal text                                 features on that is BM                                                  the normal text features and then we                                 have one that references that uses light                                 GBM model and this could also be                                 combined with other features and                                 expressions and tensors and whatnot                                 because all of this is just math as you                                 can see here you can say plus like GBM                                 model here                                 or whatever we also have some models                                 that are used for the related searchers                                 where we just access what we call a goal                                 score all this embedded vector nearest                                 neighbor search which will return a                                 distance and that's really all you need                                 to create an application in addition we                                 have some custom Java code here to                                 implement the stuff I mentioned around                                 searching for related articles so we                                 call these components that can intercept                                 the query and all the results a searcher                                 they just implement a single method                                 which is the search method which gets                                 the query and returns two results in                                 this case it's just looking to see if                                 there is one of these related to items                                 in a query if not it just returns which                                 means it just nothing and you have a                                 normal search otherwise it's translating                                 that related to item to the approximate                                 nearest neighbor operator which is in a                                 sub list let's take a quick look at that                                 as well here we can see that as you can                                 see it's just two new a nearest neighbor                                 item here we don't approach allow                                 approximate nearest neighbor because the                                 data set is so small so that's the only                                 thing you would change other than adding                                 resources if you want to do scale to a                                 billion documents you will definitely                                 set allow approximate true but other                                 than that everything will be the same                                 the adds the great and I'm item method                                 this used up here where we combine it                                 with the other items in a query and here                                 you can also see an example where                                 we create                                                                combine them with or to search near four                                 nearest neighbors both in the abstract                                 and the title so things like that you                                 can do freely and it just works okay                                 that's all I really wanted to cover and                                 that's really all the waste in this                                 application you can easily check this                                 out                                 yourself if you go to github.com that's                                 behind chain sample applications you can                                 find it here or just go to core mine                                 team and click the open source leak on                                 top ok so to wrap up vectorbase                                 retrieval and tensor based relevance                                 which is one way to look at these deep                                 neural networks at least one you want to                                 just do inference is emerging as an                                 alternative to Dyson traditional search                                 and it's also already the state of art                                 for recommendation personalization and                                 targeting and so on but production using                                 these methods on your own even though                                 there are good tools for each of the                                 pieces it's hard to combine them to a                                 production quality system that this good                                 performance in all cases and sustain                                 good performance as you make changes and                                 that you can combine with filtering and                                 traditional search and so on which also                                 is operable and scalable if you don't                                 want to do all that work you can just                                 try out                                 let's photodiode which provides all the                                 weight in a single integrated solution                                 with better performance than you would                                 get                                 for sure by combining these pieces on                                 your own and you can find Vespa at                                 baseball DoDEA so that's all then we can                                 switch to live and take questions                                 so thanks John for the great                                 presentation the chord                                                  super useful so all the best with that                                 guys we still have a couple of minutes                                 if you have any questions please ask                                 them on the slack Channel I guess shown                                 you already provided a link to the                                 github site to you maybe well be                                 awaiting do you have any further                                 feedback on how you plant we have all                                 this power what's the roadmap so where                                 we're spending most of our efforts right                                 now really is on the cloud service for                                 all the applications started using it in                                 my company we provide a cloud service                                 and we just were recently started                                 providing that cloud service to external                                 customers as well so they are mostly                                 focusing on making that more broadly                                 available and adding more features for                                 you know making it cheaper to run on                                 things like that so we do see                                 question the question is from Edward                                 that's basically oh sorry okay so                                 there's one more before from Maya she                                 would like to understand how can we                                 build vector embeddings for articles I                                 guess it's more like how can you add                                 them yeah yeah I think maybe the                                 question is how to come up with the                                 vectors so that's the machine learning                                 part really and that's somebody else's                                 problem as far as we concerned we just                                 make it fast to between them and compute                                 with them once you have created the                                 vectors but how do you create the                                 embeddings that's the machine learning                                 part which typically happens outside                                 list we have another question so it's                                 basically from Edward and he's asking if                                 he so how so does the wisp architecture                                 allow plugging new artificial neural                                 network algorithms so basically how                                 extensible is the architecture so the so                                 the tensor language we have allows you                                 to express pretty much all the models                                 I've seen recently as we had to when                                 people came up with Berk type models                                 transformer models with lots of matrix                                 and so on we had to extend the tensor                                 math language a bit but apart from that                                 it should handle all kinds of models you                                 would come up with with what we have                                 there already because the core                                 operations that I mentioned like                                 MapReduce and so on are very general in                                 general so you can pretty much increment                                 all kinds of computations over tensors                                 on table and I think there was one more                                 question as well yes so actually I think                                 Edward has a follow-up question                                 I'm not sure I understand oh but you see                                 it as well yeah if we can plug in other                                 mail approximate nearest neighbor search                                 algorithms into Vespa no you cannot not                                 without lots and lots of work                                 it's basically what you have been doing                                 for about six months now it's to plug in                                 one algorithm for this in the West bar                                 which means implementing it in C++ so                                 that it works for with the rest of the                                 engine and supports all the operations                                 that we need to support with high TripIt                                 including removal of documents and so on                                 most of these algorithms don't handle                                 this very well so I don't think it is                                 worthwhile for production to plug                                 something in unity implemented from                                 scratch with all these requirements                                 taken into account if it's more than for                                 experimenting but I think we haven't                                 chosen the right waiting for this now so                                 I don't think there's a great need to                                 plug in something else to be honest                                 right                                 and there's another question but Maya                                 also related to embedding some text                                 retrieval features I think part of                                 answered that maybe if you just want to                                 comment a bit more yeah yeah do you                                 include them in the single model sheet                                 mask yeah so combining embeddings with                                 text retrieval features there's two                                 parts to it one right one is the                                 retrieval you want to retrieve both the                                 nearest neighbor to some vector but you                                 also want to retrieve the documents that                                 are not their neighbors but are matching                                 the same tokens so we want to retrieve a                                 mix of both and that's sort of logically                                 easy but difficult to do efficiently                                 because when you want to do it                                 efficiently you want to evaluate both                                 things in parallel region taking filters                                 into account and so that's not a reason                                 why you need to integrate this deep into                                 engine to make it really efficient but                                 we have done that so when you're using                                 it is just                                 you create a nearest neighbor item or                                 several items in the query tree and you                                 can just combine it with handle or and                                 so on with texts items and the other                                 part is irrelevance and as you saw there                                 in one model that we actually get got                                 pretty good result from you just added                                 together the closeness in vector space                                 with some simple text features so                                 features like being                                                and just add them together probably a                                 weighted sum it's fine                                 so do you have this benchmarking results                                 on where does it yabba-doo yeah it's                                 part of a sample application that we                                 provide so you can run the whole thing                                 yourself actually if you look in the                                 parent directory of the thing I shared                                 earlier you'll find all the sample                                 applications with benchmark sensible so                                 great I don't think it I don't see any                                 other question so thanks again John and                                 so everyone we can of course continue                                 the discussion in the breakout Channel                                 so basically the B bus - and you know                                 thanks again for the presentation and                                 have a nice evening                                 [Music]                                 you
YouTube URL: https://www.youtube.com/watch?v=JqZSNJIRfW4


