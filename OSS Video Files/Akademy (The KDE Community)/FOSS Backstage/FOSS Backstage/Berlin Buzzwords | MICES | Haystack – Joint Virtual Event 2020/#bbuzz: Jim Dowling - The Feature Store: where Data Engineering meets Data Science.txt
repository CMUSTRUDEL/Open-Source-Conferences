Title: #bbuzz: Jim Dowling - The Feature Store: where Data Engineering meets Data Science
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/feature-store-where-data-engineering-meets-data-science

Engineering features for machine learning is hard. Before you start, you need to know: are you developing the features for training the model (Python?) or for serving the model (Java/javascript/etc), and if both - how do you ensure consistency of your features between training and inferencing? Could anybody else in your organization find the feature useful in their model(s)? If you are using a traditional data warehouse, how do you retrieve the value of a feature from last year (that has now been overwritten with more recent data) to test my model on data from last year?  How do you efficiently join features originating from different backend systems.

In this talk, we will answer these questions in the context of the Feature Store. We will show how a Feature Store can provide a natural interface between Data Engineers, who create reusable features from diverse data sources, and Data Scientists, who experiment with predictive models, built from the same features. We will dive into the only fully open-source Feature Store for machine learning, Hopsworks, to better understand the potential of Feature Stores.
Captions: 
	                              hello everyone it's my pleasure to                               introduce Jim darling with the CEO                               clocks and also an associate professor                               at kth and he will be talking to us                               about the feature store where data                               engineering meets data science so over                               to you Jim ty can you hear me can you                               see me yes what am I wearing this is my                               third future so I'm not there in person                                but I'm there in spirits I'm in                                Stockholm actually so I'm calling it                                from Socko so yeah my name is Jim                                darling I'm I have the jewel position                                I'm a CEO of a start-up collage of                                clocks and then I'm also associate                                professor and this project I'm going to                                talk about that feature stores in                                general and what is the feature store                                but I'm also going to be specific we                                have an open source feature store that                                we've developed it's the first up own                                it's actually the only fully open source                                features store in the world right now                                it's called helps works and we've been                                developing it for a number of years or                                the platform called hops works for a                                number of years we worked on it at the                                University kth a research institute in                                stockholm rice and now the company are                                primarily driving this platform                                logical thoughts so it's been the result                                of a number of years of work but I get                                to concentrate on the feature store                                aspect of the platform because that is a                                modular platform and we're finding a lot                                of interest in the platform so the                                company comes from a research background                                and our chief scientist is a professor                                kth but we have a diverse bunch of                                people from different companies like                                Spotify team Taylor and others so let's                                get started and talk about what is a                                feature store this is a data engineering                                conference I'm going to give it data                                engineering perspective on the future                                store because like anything there's                                multiple perspectives on the same thing                                a data scientist might think a feature                                story is an easy way for them to get                                features for training models which is                                kind of true but the data scientist                                doesn't have to do all the hard work                                that's the data engineers job you as a                                data engineer need to get the data                                that's over here I'm going to use a pen                                if that works and we want to get it into                                a numerical representation                                data scientists work with so shock                                horror data scientists tend to work with                                arrays of numbers they don't they work                                with numerical data primarily so they                                don't work with varchars they don't                                really care too much about you know                                database backends de lakes and they                                don't know too much and they don't                                necessarily want to know too much so                                part of the challenge in in building                                end-to-end machine learning pipelines is                                that and you probably have heard this if                                you're interested in this talk you'll                                probably assume that a machine learning                                pipeline is an end-to-end pipeline where                                we take our raw data that's or and then                                we're going to transform us we're going                                to transform it and we're gonna have                                some features and the features will be                                used to train a model with this big                                enmity on that model and it's an                                end-to-end pipeline and there are some                                frameworks that still believe this but                                we don't so what we see is that we have                                this API in the middle this API here is                                mind the gap this is the feature store                                so I'm going to use FS to indicators and                                this is the API between data engineering                                and data science so the data engineers                                over here will work with data lakes                                databases online databases data                                warehouses event streams Kafka and what                                they're going to do is they're going to                                take the raw data and they're going to                                transform it into features and we're                                going to call those feature pipelines                                so a feature pipeline is a day it's                                effectively a data data processing                                pipeline that you may be familiar from                                platforms like flink and and SPARC and                                the feature pipelines will take the raw                                data at the backend and transform it                                 into features and I'll explain what a                                 feature isn't in a minute because not                                 all data engineers know exactly what a                                 feature is so that's going to be one                                 pipeline and the monolithic end-to-end                                 machine learning pipeline going from the                                 raw data to the model we're breaking up                                 into a feature pipeline and then we'll                                 have a training pipeline over here I'm                                 going to talk about both of them but                                 will briefly introduce both but                                 basically you'll have two pipelines one                                 running at one cadence so the data will                                 come in at a certain cadence and then                                 the training pipelines tend to be either                                 operationalized and they'll run at a                                 picker cadence as well or                                 we'll be done on demand so data                                 scientists will run pipelines to create                                 training data train models and so on so                                 let's get cracking what is a feature so                                 here's an example of the feature and it                                 comes from a data engineering                                 perspective we have a table on the                                 left-hand side we're gonna call it hotel                                 bookings and every time a booking is                                 made we enter a row in this table and                                 the table will have for example the room                                 number maybe customer ID for who who                                 booked it at the room booking dates and                                 so on so I'm only showing two the                                 columns for simplicity and it might you                                 know you might have a row per days even                                 depending on how you've implemented this                                 so you might have a start date and an                                 end date for your booking so the                                 features what a feature is is think of                                 it being a column in a table in the                                 database that will help you predict                                 something so it's it's a piece of                                 information that's useful for for making                                 a prediction because ultimately that's                                 what our models are that we get in in                                 data science we have a model and it's                                 used to basically put in some data and                                 make predictions so if we can take the                                 raw data on the left and convert it into                                 a representation transform it into a                                 representation that makes it easier for                                 machine learning algorithms to work with                                 that data to make predictions from that                                 data then we we do feature                                 transformations so an example here would                                 be to compute this thing called the load                                 factor and in this case the load factor                                 is a load factor for a given room and                                 this is a feature and that feature isn't                                 present in the original hotel bookings                                 database we actually have to write some                                 code to transform the raw data into the                                 load factor and load factor is quite a                                 useful feature because it can be used                                 for anything to predict whether room                                 will be free it could be so we might                                 have one pipeline that's trying to train                                 a model to compute whether room will be                                 available at a given base we could have                                 another pipeline there another problem                                 or we trying to predict the amount of                                 revenue that a hotel room will generate                                 over a period of time so there's many                                 different uses for this feature and                                 that's another motivation for the future                                 store so yeah do you find a feature                                 store is that                                 if we can reuse a feature many times in                                 many different models                                 well maybe we should reuse it maybe you                                 shouldn't rewrite many different                                 pipelines to compute recompute that                                 feature in potentially many different                                 ways which are not consistent so you're                                 now at the point of saying well maybe I                                 can have a library for that we're gonna                                 see that yes a library will get any part                                 of the way but typically we actually                                 cache the feature data rather than just                                 using a library so let's let's move on                                 and give a more concrete example this is                                 more data science like here we have a                                 table with an ID and we have an array of                                 features and you may look at this data                                 and say I've done a machine learning                                 course and I can see that this is                                 numerical data and we know that when we                                 train models we have numerical data and                                 everything's great and maybe the Wizkid                                 davis scientist looks at this and says                                 well you know I see quite you know quite                                 a spread between these numbers so we                                 might have some miracle instability here                                 I'm going to normalize these numbers so                                 these numbers are floating point numbers                                 normalizing it means it means squashing                                 those numbers into a range typically                                 between either                                                                                                                               we're taking the original column the                                 features column and we're converting it                                 into another column called l                                            norm which the l                                                      over those values and you write some                                 code to do that so in in a typical you                                 know data engineering company you'll                                 have a little bit of code it might be                                 written in pandas it might be written in                                 PI SPARC this example is showing you the                                 code anti-spark and it's going to use an                                 l                                                                       life is already available in ice bar so                                 our particular features or the hop                                 search feature store it builds on three                                 main concepts and I'm jumping straight                                 into that rather than talking                                 historically about other features stores                                 so companies like uber and airbnb if                                 doctor at their features there's a                                 website called feature store org that                                 has a very good summary of available                                 production feature stores and pretty                                 much every hyper scale company as a                                 feature store but our feature store is a                                 general-purpose feature store we didn't                                 build it for a specific domain we didn't                                 have a domain-specific language to do                                 feature end                                 earing so the example I showed you was                                 code in PI spark so the abstraction we                                 build our future store on is a data                                 frame so what we the abstractions we                                 provide to users we're gonna ingest the                                 data into our feature store as data                                 frames so as a result of that we what we                                 have our features as a kind of a flat                                 namespace of what features are available                                 in the feature store will see in our                                 latest version we actually scoped them                                 by the Future group name and but today                                 I'm just gonna show you the flat                                 namespace version but the features that                                 come into the Future store they come in                                 as data frames or groups of features so                                 maybe we have a weakness this is an                                 example from data science it's quite a                                 well-known example you're trying to                                 predict if a passenger and the Titanic                                 survived or not and this data set exists                                 and maybe it comes from a database okay                                 and you know in a different type of use                                 case this data will be updated                                 frequently in our platform we support                                 hoodie as a packing table for the for                                 these feature groups so they're going to                                 be stored on one hoodie in hive so as an                                 external table in hive but we're going                                 to use the hoodie file format that means                                 we can do up certs we can update these                                 feature groups as they come in we don't                                 need to drop them and recreate them but                                 the the data comes in and we store it                                 the set of features as a feature groups                                 here here we have four features inside                                 the titanic past peer list data but data                                 will come in from many different sources                                 not just from a one database it may come                                 in from a data Lake and may come in from                                 a Kafka it may come in from wherever so                                 let's assume that another source of data                                 came in and in this case the data was a                                 historian who looked up the passengers                                 of the titanic and looked up how much                                 money they had in their bank account and                                 this came in from this museum over here                                 so we now have a set of features in our                                 feature store they're grouped by future                                 groups and data scientists can come in                                 and they can look at the features and                                 they can basically say well I I would                                 like to join together a bunch of these                                 creatures that are in the feature store                                 to create some training test data that                                 I'll use to train my model with so what                                 they would do typically in this case if                                 you saw this data is you might have I                                 offices that if I joined the bike                                 balanced and here with the existing                                 features in our Titanic passenger list                                 well then I might be better able to                                 predict if the passenger survives or not                                 okay so the data scientist is gonna join                                 these together I'll show you the code                                 right doing do this it's much simpler                                 than this diagram but they'll join those                                 features together                                 get a training test data sets actually                                 data frame they're gonna get back and                                 then they can decide on file format to                                 materialize that as if they're gonna you                                 can work with the data frame directly                                 but often if you're doing deep learning                                 you might say well gonna store this TF                                 record file format and I'm gonna store                                 it where GCSE s                                                        house if S which is a next-generation                                 HDFS so these are the concepts that we                                 have you have features they're grouped                                 together you can combine them or join                                 them together to create training test                                 datasets and you store those wherever                                 you want to use them now everything here                                 is going to be version because we want                                 to be able to reproduce this process of                                 creating training data sets and with the                                 help of hoodie we can actually even even                                 version based on the current state of                                 the future group the current state of it                                 as of this moment in time okay so let's                                 let's take a little step back that's the                                 abstractions that we have in our                                 platform and talked a bit about where                                 these features come from so the features                                 will come in and be ingested into the                                 feature store I said already it's going                                 to be a store and some of them will have                                 very low latency requirements and some                                 of them will maybe be ingested                                 periodically every day or every week or                                 every month from a dead lake they may                                 come in from operational databases data                                 warehouses Kafka other event sources and                                 here a couple of examples of them so we                                 have one company we're working with who                                 need the features to be transformed in                                 less than two seconds so someone's                                 entering something on a form then we                                 need that data being entered by the user                                 to be transformed into features so they                                 can be used by the application but                                 that's a very low latency use case but                                 for all of the other use cases where we                                 have at you know                                                     minutes or hours or days well then we                                 can we can run different types of                                 applications so we can maybe use a batch                                 spark application to take the data from                                 the                                 back in platform and push it into the                                 feature store and in this case we also                                 do streaming applications for the lower                                 latency data sources so all of these                                 will come in these features will come in                                 active and the different Cadence's                                 they're going to update the feature                                 store so who's going to use the future                                 store well we have it here an online                                 application and the online application                                 for example if it's trying to predict                                 something like fraud it might say well I                                 have a bunch of features that I got                                 which were for example what is being                                 purchased                                 what's the ID of the item being                                 purchased what is my session look like                                 right now my shopping cart do you have                                 it I have an identity already in the                                 system that we can use so these IDs we                                 can use to look up features historical                                 features about the user have they bought                                 off a lot from here have their shopping                                 cart has it been filled up and emptied                                 recently laughs you know anything any                                 events or features that can help predict                                 if with this transaction is going to be                                 fraudulent or not that we can go to the                                 feature store other users of the feature                                 store our data scientists will create                                 training data and we saw that already in                                 the previous example but also you're                                 gonna have batch applications can just                                 pull features directly and use them to                                 make predictions so it's not going to                                 pull out the actual label and it's just                                 going to say I need I can use these                                 features I have a model and now it can                                 actually just run those features against                                 the model to make predictions or score                                 that model now the thing that's                                 interesting here is that if the feature                                 store were a database you would expect                                 that it would be able to provide the                                 features of the features store unless                                 it's the online application in less than                                                                                                          it will scale to many terabytes in size                                 we have a customer swear bank you have                                 over                                                                 just one particular use case those who                                 talk at the sparks last part somewhat                                 about that so the problem here is that                                 we want to store many terabytes of data                                 we want to scale our database and it                                 also needs to have extremely low latency                                 so there aren't any existing databases                                 that we're aware of anybody else who's                                 developing a future store is aware of                                 and that has these properties so what we                                 do is we actually split up the feature                                 store into an online layer that's going                                 to serve the feature as a serving store                                 an offline feature store so this is a                                 scalable sequel database typically so                                 what what this does is it now                                 complicates the process quite                                 significantly as you can see here                                 whenever we want to store these features                                 we need to actually now know how am i                                 storing to online or I started an                                 offline I'm restoring to both and you                                 can imagine the complexity of writing                                 your spark streaming app or your spark                                 batch app and it's got to like to your                                 online database and it's also going to                                 write your offline database so our                                 online database is my sequel cluster or                                 NDB and are offline database is Apache                                 hive and they share the same metadata                                 layer in our platform now what we did to                                 make this very complex architecture                                 simpler it's kind of lambda architecture                                 like but we have introduced a data frame                                 API so what the data frame API gives us                                 is the ability for our applications so                                 we have spark streaming apps here for                                 example and they're gonna just save                                 their data frames to the feature store                                 and our API gives them that simplicity                                 they don't need to worry about whether                                 it's going online offline they just set                                 a flag when they create a feature group                                 and say hey this is going to both online                                 and offline or this is just going to                                 offline and the data frame API doesn't                                 just work for spark it also works for                                 pandas Python data friends it will                                 convert them to power to park' files and                                 ultimately offline data will be stored                                 as park' and external tables and hive                                 with OD if you decide to use woody and                                 then the the online feature store will                                 have tables in my sequel cluster for use                                 cases where we can't we don't have                                 enough time to push the data at the                                 feature store and pull it out at the                                 online application and we count on our                                 platform support link and I think can                                 read data from an account of the topic                                 that's input and then write the output                                 to another Kafka topic and the other                                 line lock can pull it and we can do                                 we're doing that for a two second use                                 case so so what we introduced here was                                 this this notion of the data frame API                                 to simplify the process of ingesting                                 data so let's have a look at the code                                 and there's a very simple example we                                 were going to have a data frame that's                                 going to                                 data from a back-end system and then                                 we're going to do some feature end                                 engineering on that data frame we saw an                                 example earlier and then we create a                                 feature group if we want to make it                                 online we just set online equals true                                 here and it will synchronize that                                 updates to that data frame to both the                                 online and offline feature stores so                                 let's go ahead so yeah we've covered                                 this part here this is the ingestion of                                 data into the PQ store that's a quick                                 look at online applications and then how                                 we create training path data sets oh                                 creating training cuts data sets we'll                                 start with that one so our platform uses                                 at spark to to join the feature groups                                 together so if you have features that                                 that come from several different feature                                 groups and your code might look                                 something like this FS get features and                                 in the flat namespace case we just put                                 in the names of the features and it will                                 return a data frame so what it's                                 actually doing is it's running a spark                                 application to read up these data frames                                 join them together and using the common                                 join key so if our creep on it will look                                 for the largest overlapping set of                                 primary keys between the two data frames                                 involved and if you've multiple data                                 frames then it's going to look between                                 the joints it will again examine the                                 largest overlapping set of join keys or                                 primary keys now this spark application                                 can run on our platform mouse works or                                 it can run on data breaks for example                                 next any external spark cluster will                                 work and it will that spark application                                 will then materialize the training test                                 data to wherever you want it to be                                 stored and this is what it looks like in                                 code this is a slightly more complete                                 example and in this example we get back                                 this data frame as the join of all of                                 the features and we just say hey I'd                                 like this data to be stored as TF                                 records and we have another parameter                                 here called connector and the connector                                 can point to an s                                                        system and hop so fast and everything is                                 version so you can specify versions if                                 you want you can get latest versions if                                 you want to increment the version of a                                 training test test dataset so it's                                 basically yes now we have updated this                                 API                                 somewhat and to hardscope so I'm going                                 to show you how                                 we do this so in the newer version that                                 will be released this month it's not not                                 that yes and we get the feature groups                                 so we need to know them where where the                                 features come from what feature groups                                 so they come from so we get the Future                                 groups and it's good practice to specify                                 the version to make product like proper                                 production code now what you can do it                                 to try and use our query planner again                                 as you can just say in a very pythonic                                 way select all the features from the                                 first feature group and join them with                                 all the features from the second feature                                 and this data frame that you get return                                 join features we we can then save it so                                 in this case when we now have an extra                                 line because the training data object is                                 a metadata object it's a lazy object                                 that we use to create the training data                                 set so at this point there's no training                                 data has been materialized it's only                                 after we make the last line called TD                                 that save on the data frame that we got                                 back from the feature groups that we                                 joined together it's only at this point                                 then here where we're actually going to                                 save the date train data to a file                                 system and in this case we're saving it                                 to s                                                                   here so some other improvements we have                                 apart from the things we have before us                                 which is that the file format things                                 like numpy you know peda storm are                                 popular CSV or popular file formats but                                 it's nice and easy now to just create                                 the split so your training day will be                                                                                                         tested and                                                             it just makes it a little bit easier I'm                                 using this new API now we support                                 time-travel queries for creating these                                 training data sets you can say hey I'd                                 like to have the training data for these                                 feature groups as the Train data looked                                 like at this moment in time or you can                                 say for example give me the training                                 data that or that the data the features                                 that arrived in a feature group between                                 a time interval between a start point                                 and an end point in time so the way it                                 works is it's using Apache hoodie                                 underneath the covers to basically get                                 the changes in a feature group in a                                 particular interval and this is the                                 interval case where we're going to get                                 the bank data as it look at the bank                                 data that arrived between the                                        January                                      than the fourth                                                          first case we're going to get this                                 feature group as it looked like at this                                 particular moment in time to this point                                 of times we can secretly have this                                 keyword house up and that's what we have                                 effectively here                                 is very similar to Delta link by                                 barracks and it provides atomic and                                 incremental updates of feature groups                                 which is really nice and there's a                                 hoodie talk I think out Thursday which                                 you might want to look into so how does                                 an online feature story use this or                                 online applications use this online                                 feature store well if you have an online                                 application that we have here what it                                 wants to do firstly that says well I'm                                 missing a bunch of features so I need to                                 go to the feature store to guess the                                 features that I'm missing things like                                 historical information about what the                                 user has done maybe their credit history                                 anything that's been computed offline                                 and materialized to the online feature                                 store we can do so what the online                                 application can do and this can be done                                 within the module as well but basically                                 they make a query to the feature store                                 and say hey I have the I have the keys                                 the primary keys for this particular                                 training data set so I have the primary                                 keys for the feature groups that were                                 joined together to make up this training                                 dataset can you please give me back a                                 feature vector so that basically does a                                 a bunch of primary key lookups and joins                                 them together in the online feature                                 storm icicle cluster when you start up                                 the app you typically will will get that                                 query back as a cached prepared                                 statement and then you just make the                                 it'll make the query for you directly on                                 the database you know there's not a two                                 two round-trips just a single round trip                                 to get that feature vector so what yeah                                 app once you have the feature vector you                                 go to your model and typically we don't                                 serve models embedded inside the                                 application if it's tensorflow serving                                 server we would deploy the model on that                                 test was our server typically in                                 kubernetes                                 and then it would to be replicated                                 across different availability zones for                                 example in the cloud and our database                                 our online database is a che it's                                 transactional and it supports high                                 availability cross availability zones in                                 the Clio                                 so                                 a little bit more detail on how that                                 works internally our online application                                 has it's received a number of ids entity                                 IDs if it's a after all example you                                 might have the idea of the order the                                 customer has involved the amount of                                 money the product ID and things like                                 that with those ID IDs and our keys as                                 we're calling them here and what we can                                 do is we can go to the feature store and                                 say hey I want to prepared statements to                                 look up the feature vector with these                                 keys so once it's got that prepared                                 statement it can cache it locally the                                 other application and then it can make                                 this query on the other line PQ store                                 with that set of keys and then once it's                                 got back the feature vector it can then                                 make the predictions on the model so                                 this is part of hops works like I said                                 it's a modular platform and the feature                                 store itself it's a it's here and it                                 needs the filesystem                                 it has a hive and my seagull Buster you                                 can run the whole platform so you can do                                 your feature engineering here and your                                 model training here on a platform you                                 can even do a model serving and                                 monitoring if you want and optionally                                 katka is included in the platform as                                 well so it's a it's a pretty complete                                 platform for doing not just model                                 training but also of data engineering we                                 can see here the weave in of air flow                                 and I'll show a quick demo on that as                                 well in a minute so here's what Manto                                 and pipeline looks like and it's                                 remember it's not an end-to-end pipeline                                 because the pipeline will will start                                 here at the back end and end the first                                 pipeline will start there and the second                                 pipeline will be training a little                                 appear after so that's a quick look we                                 have feature engineering that happens                                 first and once the features are in the                                 features store then the data scientist                                 can can basically do all of that we can                                 see here these stats that they can train                                 models and and it's an experiment it's                                 an haider the process you get features                                 you get training data you train models                                 you analyze your models validate them                                 deploy them to a model repository and                                 then applications patch application and                                 use them directly if you have online or                                 operational models you then deploy that                                 model to a model sparing server and                                 online applications can use them you                                 don't mind applications or the monster                                 itself can get the feature actors                                 as it needs them to build the whole                                 feature vector that he used to serve the                                 models we do model monitoring by                                 actually taking the predictions that are                                 made and logging into Kafka and then we                                 can use spark streaming typically to to                                 monitor those predictions to make sure                                 that the incoming features don't                                 diverged significantly from the features                                 that we trained the model on and the way                                 we do that is we actually go to the                                 online feature store and say hey give me                                 the statistics and descriptive                                 statistics for this training data set                                 and then we can compare those with the                                 ones that are being computed on live                                 data so we use Windows to compute that                                 you know the mean the standard deviation                                 max min forgiven features and then we                                 can automatically pair those with the                                 training statistics and then notify if                                 there's a problem so a platform is a bit                                 more than this we do multi worker                                 training I'm going to maybe show Maggie                                 which is awaiting her parameter                                 optimization but it's just part of the                                 example and then we also do                                 project-based multi-tenancy so you can                                 have sensitive data in a shared cluster                                 and there is also support for provenance                                 and similar to T effects and M up flow                                 and what we do it implicitly and I won't                                 have time to go through that in this                                 talk I'm going to do a demo in a second                                 but I just tell you how you can try out                                 the platform it is open source there's a                                 community version there's an installer                                 for it if you go to our documentation                                 just google help search documentation                                 you'll find us if you're interested in                                 trying it the enterprise version you can                                 go to the manage version we have a SAS                                 version on AWS cooperate study I and                                 then we also have an Enterprise version                                 that can be installed on premise or in                                 the cloud so an azure and GCP as well so                                 I'm gonna do a demo and I give this go                                 through a demo where we have a churn                                 model we're computing a turn model for                                 telecom users this data set came from a                                 Kaggle competition but we're gonna                                 create a feature group from a this                                 original raw data we're gonna create                                 some training data from the features and                                 that we've added the feature store                                 trainer model using it and that's kind                                 of as far as we'll get in this example                                 you can extend this then to do online                                 future servants and it's going to skip                                 to the my browser I'm not going to show                                 you how to get started and hubscher it's                                 AI but this is what I'm using and you                                 can create an account er you need to                                 link your AWS account to to enable hop                                 source AI to launch a cluster in your                                 account so you just create click once                                 you've done I can click create cluster                                 I've run this one already                                 now that's this one here I'll just show                                 you so when when it comes up you'll see                                 basically this and you can log in now                                 I've created a project already but I                                 have with a couple of minutes so I'll                                 just show you how to create a new                                 project I'll call this one Telecom                                 a project is I mentioned already it's a                                 it's a kind of a sandbox we have this                                 project based multi-tenancy model so all                                 data and users and programs inside this                                 project kind of like the gift of                                 repository and they're going to be                                 private so if I give someone access to                                 the project as a in a role called data                                 scientist then they can't copy data out                                 they can't even read data from other                                 projects that are a member of so it's                                 it's really you're really restricting                                 them inside here so I created this                                 project and the example I'm going to go                                 through is here at Jim darling slash                                 turn on github and well I can just do to                                 get started and get installed a bunch of                                 Python libraries so in this platform the                                 way install python libraries is you just                                 use calendar or pip                                 so off you go and it'll take a couple of                                 seconds at the example actually it's                                 going to be it's some raw telecom data                                 but I'm going to use XG boost to train                                 the model we'd support next you boostin                                 scikit-learn cantaloupe i torch there                                 the pretty common ones the common                                 frameworks that people use take a couple                                 of seconds you can actually also                                 important to calm the file here instead                                 we're going slightly faster but i                                 thought i'll do it this way so we have                                 hip here as well and i can just search                                 you down so what makes it slightly                                 different than that will actually build                                 a docker file in the background from                                 your kondeh environment file here now                                 that's kind of nice because data                                 scientist then don't need to actually                                 write docker files                                 [Music]                                 the last layer here at physical                                 imbalance learn there we go                                 okay so they're gonna start installing                                 them this is what we get in our base                                 environment what I'm gonna do is there's                                 no feature store here currently we have                                 feature group strain data sets feature                                 search on food store details I'm just                                 gonna open a notebook and in the                                 notebook I think this should be okay I'm                                 going to put in my kit with repo which                                 was here copied us and I added an API                                 key already so we enabled it that's                                 pretty good idea and this is a kid have                                 repo I have an API key upload it to hops                                 right so you can see in our                                 documentation I had to do that today                                 upload turn and be ok it doesn't look                                 like one of those okay so a quick look                                 secrets okay let's go back to Nate look                                 it again there we go and I'm gonna just                                 deploy this from master so it takes a                                 second to go get up and find us okay so                                 in this case it's actually going to run                                 a PI spark application I'm gonna give it                                 two gigs to the driver two gigs to the                                 executor and it's just a single CPU it's                                 not a huge amount of resources and this                                 is something called and hit lab and we                                 can see here there's a number of                                 notebooks we have this this is our data                                 set here and this is what you'll find a                                 geared up so what I'm going to do is a                                 couple of things I'm going to copy this                                 data set into hops works we could run                                 with it locally one I'm gonna make a job                                 out of this later on so I'm just going                                 to go through this so let's not blow                                 let's create a folder first                                 I gotta call the full return and I'm                                 just gonna we're gonna yeah I'm gonna                                 upload my I some of the code from from                                 github - I got about everything in here                                 actually - two hops work so this is                                 uploading data to the file system so I                                 got this copy from local star - the path                                 in HDFS it just copies this data and it                                 maintains the same username so if we go                                 back here we'll see that the date has                                 been copied oh now you cannot probably                                 you know attach the community ID if you                                 want to get to notice which version of                                 the repo it is so I'm going to go back                                 and I'm going to actually then run the                                 first notebook which is to create a                                 feature group let's actually before we                                 do that let's check to make sure that                                 everything has been installed yes it has                                 and then the actually before we do that                                 I'm going to close this notebook and I'm                                 going to go back for one second and show                                 you the feature store because I notice                                 I'm running out of time so we can this                                 is a feature store that I've already run                                 and this one is what you'll get when you                                 run the tour and you can see we have a                                 bunch of feature groups here and we have                                 the ability to look up statistics on the                                 future groups so when you ingest a                                 feature group it will automatically by                                 default it will compute these statistics                                 so things like clustering analysis we                                 have correlations and distributions of                                 individual features so this is nice for                                 exploratory data analysis you there's a                                 UI here for creating training data sets                                 by selecting features I'm gonna skip                                 that and we have individual features                                 here so the overview the feature story                                 here you can see there's                                                 feet two groups one training dataset and                                 [Music]                                 yeah so that's a basic overview the                                 feature you can search for features here                                 and we can see here that we can search                                 of features across the different feature                                 stores in fact I think I might have a                                 minute to just run this one example I                                 just skipped through it and                                 the there's it there's a bunch of                                 different it's building back in Louis                                 okay so I'm just gonna run this here run                                 ourselves so this is a spark application                                 and what it's doing is it's reading up                                 some data from from our HDFS so I copied                                 this data in I can copy it here I copied                                 it into churn so we need to change that                                 resources churn okay that's the correct                                 path and it's going to do some feature                                 engineering so this is the feature                                 engineering you can look at it and then                                 it's going to create the feature group                                 so once it's created a feature group                                 what it's running the spark application                                 here now we can look at that when it                                 starts we can see it just as a single                                 executor and if we go back we can see                                 what it look like when it finished so I                                 run this already and when it finishes                                 you'll get this feature group appearing                                 in here now it computes statistics on it                                 we saw that already we can get a preview                                 of the data as well which is kind of                                 nice                                 that's this one so like as a data                                 scientist you often want to preview data                                 very quickly what I what I also did in                                 this case is I you can also take those                                 notebooks and turn them into jobs so                                 that's originally where I created them                                 as I copied them to HDFS because we can                                 create a job from those notebooks and                                 then from those notebooks we have a                                 every notebook can be turned individual                                 job and we can actually chain them                                 together in air flow and I'm just going                                 to show that it's the last thing I do                                 before I finish so you can have for                                 example your training pipeline look like                                 this we can add a two notebooks for                                 example we could say the first one is                                 we're going to compute the hyper                                 parameters and save that and then in the                                 second notebook we are going to wait for                                 the hyper parameters to be computed and                                 they're going to train so we computed                                 good height parameters once we get good                                 how clear parameters you can pass them                                 on to training so this this particular                                 notebook that I just created which was                                 one of these here this is a a a                                 it's not a notebook it's a it's a dag                                 it's a die in an air flow we can                                 actually run that them and you can show                                 you how to run as often as you want and                                 it was this one I created I think you                                 can change it to run like air early or                                 on an event and we can see that what                                 will happen is that it'll actually start                                 the job running when it kicks off we can                                 see it started kicked off here let's see                                 yeah so it's going to call those two                                 phases those two different notebooks and                                 if we go back to our jobs UI we can see                                 that it's kicked off the first one it's                                 just accepted and then this will go into                                 the running State so now it's in the                                 running State it's just like any sparked                                 out you can kind of monitor it look at                                 the metrics and so on I'm at a time so                                 I'm going to go back and state thank you                                 that's kind of the brief demo of the                                 platform we have a lot more videos on                                 the website you can look up its logic                                 voxcom we're on twitter and we're on                                 kiddo                                 and i have a lot of people to thank                                 other company and you can see their                                  names there okay so that's it from me                                  are we back to stream era that's kind of                                  cool infinite view so thanks about                                  German that was a very interesting                                  presentation I hope you still have time                                  for a couple of questions maybe we yeah                                  I'll drop over to their to their to the                                  room I you know I know I have another                                  meeting coming up but I'll be there for                                  compliments okay maybe we can just take                                  a couple of questions here and then I                                  will maybe because the day just one just                                  one and then okay                                  all right actually I think the most                                  common one that we have is related to                                  the time travel API so a couple of                                  questions so is it possible with hops                                  words to go back in time with feature                                  values to consistently back test your ml                                  model and there's the related question                                  so about the time travel API yeah one of                                  the main use cases is to kind of avoid                                  future poisoning of label data so I                                  don't want get me all the records at                                  time t                                                                   all the records as they were at the time                                  of an event let's say click off the                                  record click off records could                                  makes comment on that yeah so I mean                                  like we you can add a you know a column                                  to your future Gertz called the time of                                  which the event was created as well and                                  typically you might roll them up to a                                  day and if you want to or to an hour or                                  so in some interval of time and so so                                  when you join your feature groups if you                                  join on that column then it will match                                  them up according to the time so that                                  the features will be correct at the same                                  time so you can explicitly explicitly                                  join on the on the time column your                                  feature groups then that will work fine                                  you
YouTube URL: https://www.youtube.com/watch?v=TPuqyNJX7T4


