Title: #bbuzz: Niels Basjes - When your stream doesn’t stream very well
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/when-your-stream-doesnt-stream-very-well

So you got the job of creating this great new streaming analysis on top of an existing data stream. To avoid overloading your laptop you start the project by listening to the data from the test environment which gives you a manageable volume. You get the data into Apache Flink or Beam and you see raw data coming in. Yet your first attempts at doing a very simple analysis on this data results in … nothing coming out. Then you simply take the raw data and use the advised way to write it to something like HBase ... and it takes 30 minutes for the records to appear in the database.

What is going wrong?

The reality of streaming analytics is that the analytics works great on a continuous and big enough stream. Testing streams are very often “too dry” causing all kinds of basic systems in stream processing frameworks to behave differently from what you want. But not only the streams in the test environment are a problem, also streams that are used to process incoming files (i.e. batches on a stream) can be quite a problem to handle correctly.

In this talk I will go into some of the practical problems we ran into while building streaming applications with Apache Kafka, Apache Flink and similar tools over the last years. And I will show the solutions we use that allow people to successfully build analytics/processing solutions on these "not so streaming" streams.
Captions: 
	                              everyone let's welcome our next speaker                               Niels he works at Bolcom and his focus                               areas are around scale and reliability                               he's Niels is an active open source                               developer a PMC member of the Apache                               Avro project he contributes to other                               projects like Hadoop HBase flink beam                               and storm this year at Berlin buzzwords                               needs is gonna speak about when your                                stream doesn't stream very well so let's                                have it for me                                hi so welcome                                it's time I am going to talk about                                experiences from the wild when building                                actual streaming applications and things                                move differently than you expect                                so benefi gender I'm gonna briefly talk                                about our context in which I am creating                                these systems we're talking about                                streaming solutions and specifically                                about broken and problematic streams I'm                                also going to explain how these systems                                works at least the basics and then also                                include solutions directions that we are                                working on and some final conclusions                                first a bit of background from about me                                for those who don't know me yet I have a                                background in both computer science and                                business I have been doing software                                development algorithm research and                                architecture for a long time nowadays I                                put corporate inventor on my business                                card because that best explains what I                                really do and I contribute to a lot of                                open source projects and I speak at                                quite a few conferences so the context                                where work is Bolcom it's an online                                retailer we sell stuff we sell lots of                                stuff in fact we have over                                              items actually available right now                                it varies per day so this is a from a                                from a while ago and one of the things                                you need to do in order to make sure                                that customers are able to find their                                products is personalization                                so because I'm recognized we do all                                kinds of changes to the pages to make                                sure that the person is able to find                                what they're looking for so we have a                                new service in my region I get to see                                that we have some general advertising                                and these are based on the fact that I                                have kids that like movies like Harry                                Potter and transformers but also I like                                to play games and in addition                                I looked at something hey maybe other                                things are interesting as well and                                products items I looked at before you                                can understand that in order to make all                                of this as real-time as possible we do                                quite a bit of stream processing                                nowadays we use both of our chief                                Lincoln Apache beam in a lot of our                                production scenarios and for this talk I                                have constructed a very simple example                                use case that is actually something I                                tried to build and failed and ran into a                                lot of problems and I'm going to explain                                those problems see in this case this                                it's a person behind a laptop who visits                                our website and the interaction events                                go into Kafka I spoke out on this                                project last year at this conference so                                look back at that if you're more                                interested about that in this example                                I'm attaching a very simple flink SQL                                statement because I want to get a graph                                on the number of pageviews per hour so                                this link SQL is then stored in HBase in                                this example so you can attach something                                that shows you graph now let's pull this                                very simple application apart into the                                elements that that I have problems with                                and the first thing to realize is that                                time is essential in our use case I am                                saying interval one hour so the main                                question is then what kind of time are                                we talking about are we talking about                                even time are we interested in reporting                                on the time that the event originally                                occurred if we build a processing around                                this concept that is nice because in                                case of a processing disruption                                eventually consistent the end result                                will remain the same because the end                                result solely depends on the data                                systems like flink also support in just                                in time and processing time which is                                essentially the time the data arrived in                                 the processing system or in the in the                                 source and where it was processed                                 it's important to realize that if you                                 build something on top of this that in                                 case of a processing disruption the                                 result will be different because the                                 processing time will be later than                                 expected so the end result really                                 depends on both the data and the                                 operations of the processing system now                                 the most common example to explain all                                 this as the Star Wars where we have the                                 story time event time that goes from                                 episode                                                                 of the movies was that we first got the                                 mill                                                                     last three and the side stories were                                 dabbled in between in a wrong order also                                 and then later on they created the nice                                 series called the Mandalorian which is                                 also somewhere halfway the processing                                 time is a fundamentally different thing                                 than event time and looking at the                                 things around me I see that for example                                 in operations a lot of times you need                                 processing time because you need to know                                 when something actually happened how the                                 operations of a system actually behaved                                 what you see here is a disturbance we                                 had in one of her flink processing tasks                                 in our in our production environment it                                 went down for an hour and then had an                                 enormous bubble of higher processing to                                 catch up what it still had to do in this                                 example we are doing web stats we want                                 to know the reporting on the event time                                 we want reporting on when something                                 really happened the second part of the                                 application that needs addressing is the                                 group by event time because that's what                                 we're doing we're doing a group by hour                                 now if you're doing a group by on a                                 fixed data set that is nice because you                                 have an overview of all events in there                                 and you can simply sort them group them                                 and then have the grew by of whatever                                 fields your feel like but this is a                                 forever changing data set it's a                                 never-ending stream it is                                 impossible to have an overview of all                                 events so if you want to do a group by                                 type of operation you have to do it                                 slightly different and streaming systems                                 like link use the concept of a window                                 which is essentially a group by buffer                                 you incrementally fill it with events as                                 they arrive and then have some kind of                                 assessment of completeness and when that                                 assessment arrives you take all of these                                 events and ship them out for further                                 processing so when is a window complete                                 the construct they use is called                                 watermarks these are special records in                                 the data stream that are hidden from the                                 application but are there for the                                 framework to react on so a window sees                                 them but the application code doesn't                                 and they simply indicates that there                                 will no be no more older events so if                                 you see them you can respond by shipping                                 out the window and it's common to base                                 them on the data which gives you an                                 estimate or a guess of completeness                                 there are situations where you guess                                 wrong I'm not going to in going into                                 those situations in this talk so a very                                 simple application we create records                                 throw them in Kafka and then into a                                 flinger thing so the first thing to                                 realize is that you when you started the                                 window has a sense of last seen                                 watermark which is very long time ago a                                 to the dinosaurs well actually it's min                                 long which is                                                           the first dinosaurs only appeared                                    million years later so it's even older                                 than the dinosaurs but you get the idea                                 so my application creates an event that                                 goes through Kafka into the application                                 and goes into a bucket that is grouped                                 for the                                                            another event arrives and then some part                                 of the code says hey I saw something                                 occur and I choose now to insert a                                 watermark so it inserts a watermark for                                                                                                   and then the application says the                                    o'clock window is complete I can ship it                                 out and use it for the reporting the                                 first thing to notice is that Kafka is                                 not a simple pipe a Kafka consists of                                 Kafka topic persists consists of                                 partitions and partitions are                                 essentially smaller pipes that each give                                 ordering guarantees and you can have a                                 lot of them to have a large scalability                                 but on a per PI basis there are ordering                                 guarantees so our application really                                 looks a bit like this per incoming pipe                                 you can have a source component you can                                 also have source components doing                                 multiple but let's keep it at a                                 one-on-one example for this talk so                                 again our application starts firing                                 these events same as before but now you                                 see that it has for each incoming source                                 it has a separate rememory of what is                                 the latest watermark so then at the                                 other one a message arrives and only                                 then the assessment is wait is made that                                 the                                                                     part in the application that we need to                                 think about is the writing to the                                 database that part if you use HBase and                                 I'm using this as an example I truly see                                 that in a lot of other database systems                                 you see similar effects writing through                                 a database you have a method that puts                                 in a single record now in case of HBase                                 there is a remote call to the database                                 server                                 that puts that single record in and you                                 have to wait for the network latency for                                 it to return which can be quite slow if                                 you have a lot of them so that is why in                                 HBase they have something called a                                 buffered mutator which actually buffers                                 two megabytes of mutations and when the                                 buffer is full it does a single call to                                 the back-end writing in a way that is if                                 you have a lot of measurements a lot                                 faster                                 so I have this application I run it on                                 my laptop and I get no output at all                                 and I kinda panic well it's important to                                 realize that some streams don't stream                                 very well and over the years I have come                                 to recognize for situations for classes                                 of streams that downstream very well and                                 the first is slow streams streams with a                                 very low event rate the second is slim                                 streams if own streams that only have                                 events in in one or a few Kafka                                 partitions there are streams that are                                 called bursty or batchi where you have                                 bursts of events and then long gaps in                                 between those this is by design usually                                 and there are broken streams also with                                 large gaps but now caused by the                                 disruption or by maintenance so let's                                 look at some of those some examples of                                 this I have encountered a combination of                                 slow and slim streams on our test                                 environment so let's get back to our                                 example application you know I'm on the                                 test environment and I'm clicking on our                                 website so I'm producing some events                                 that are stored in Windows and the                                 watermarks drive and then if you look                                 back also to my other talk what you see                                 is that we have said we are doing a                                 partition by session ID that way every                                 event for a single session goes into a                                 single Kafka partition which means that                                 the ordering is maintained and then also                                 at the end of flink                                 depending on the application of course                                 we have the option of maintaining that                                 ordering by doing a key by session ID                                 and that is very nice but in this                                 situation I'm alone I'm alone on this                                 website and that means that I have to                                 have wait a really really long time                                 before somebody else creates an event                                 that takes that dinosaur away because                                 when that dinosaur is away can my window                                 be shipped out and you see that only the                                                                                                          the                                                                   guarantee from all inputs that it is                                 finished now this drawing only shows you                                 two Kafka partitions but in our                                 production environment we have over                                     that has to do with a number of hard                                 drives in our cluster but this is not                                 all why I don't get any data the other                                 reason is that we're writing to HBase                                 and over there is the buffered mutator                                 so the windows are going or by far not                                 enough to flush the buffer another very                                 common example I've seen is slim streams                                 and something we ran into a couple of                                 years ago when we first started with our                                 project is that on acceptance we run                                 loads tests so we have a load to                                 narrator some kind of robot that                                 produces traffic to test the website and                                 in this processing it is important to                                 realize that the windows while there are                                 incomplete they occupy memory memory in                                 the state system of something like flink                                 while this can be persisted to some kind                                 of a stories layer it is memory that you                                 are using so if you are done having a                                 low test generator that is not that                                 clever and produces events with                                 everything the same session ID over or                                 only a few session IDs you will blow it                                 up because none of the windows will ever                                 get flushed all of the events will stay                                 in the application because that dinosaur                                 is not taken out and if you have this                                 situation you thought islets the                                 terabyte of memory should be plenty                                 that's not if you run the load as long                                 enough anything breaks another exam                                 that I have seen in production as                                 something we call bursty streams data                                 that arrives in bursts and a good                                 example is something we were designing a                                 while ago is when we have partners that                                 want to offer upload data to a landscape                                 for example a seller that wants to                                 update their prices so we want to have                                 two kinds of interfaces for these people                                 arrest interface where they can have a                                 REST API with a single change and a file                                 upload so the REST API is you know API                                 some transport like pops up or Kafka and                                 then some processing and then a storage                                 in some database system and in this                                 example I'm using HBase or BigTable and                                 as a side thing the file upload and then                                 every time the file arrives you pull                                 that apart into single lines and throw                                 it into the same processing line                                 pipeline now if somebody uploads for                                 example a five megabyte file that is cut                                 into all the individual records and the                                 first two megabytes of those records                                 just simply pass on into the database                                 the second two megabytes pass on into                                 the database and the last                                            will remain stuck in the buffer then                                 somebody inserts the records Friday as                                 REST API and another one and they will                                 simply be remain stuck in the buffer and                                 then there are broken streams an in                                 production we all go down for                                 maintenance sometimes even the largest                                 systems can go down for maintenance and                                 one of those very large systems I see is                                 the Niagara Falls                                 a very very large streaming system that                                 went down for maintenance in                                           put up a dam and shut it down for                                 maintenance and in our very simple                                 application having a disruption in the                                 data stream would mean that you know                                 data stays in the buffer and it's not                                 written to HBase so what                                 are the kinds of solutions that we've                                 come up with how do we try to handle                                 these things well first if I the first                                 of all the group by event time it's                                 slightly more complex than our simple                                 example because we do multistage                                 processing it's not just the website the                                 Kafka and the flink we have an ad rich                                 clean filter and step in between                                 with another Kafka topic this step                                 determines visit if a session is idle                                 for more than                                                       visit starts it slaps on GOP idea                                 information okay based on the IP address                                 what country it disassembles the user                                 agent string to figure out is it a phone                                 or a tablet and based on those                                 assessments we do our first assessment                                 or if this is this a robot or not and                                 last but not least we apply some GDP our                                 processing for example the IP address is                                 cleaned is nullified so how do we do                                 then change how do we then fix this well                                 there are two primary solution                                 directions that I've seen either you                                 change the data for example you inject                                 non data records that are effectively                                 just a timestamp I'll go into that a bit                                 more later or II change the processing                                 and in flink they recently committed a                                 feature that allows detecting an idle                                 timeout so if a Kafka topic or partition                                 does not have data for a too long time                                 it automatically marks it as idle and                                 effectively says your we're not looking                                 at you anymore in terms of watermarks I                                 find it very important to stress that if                                 you use this feature you are mixing                                 processing time and event time you have                                 an event time system and suddenly the                                 processing time matters it may be useful                                 in some cases I consider it a risk so                                 way we tried to handle this is that we                                 now have a very simple single threaded                                 dummy component that simply creates                                 every                                                                   event that is thrown into all cough                                 competitions any overhead is negligible                                 so how does that look                                 well the code is really simple you have                                 a Kafka producer you just ask it for all                                 the partitions and then keep running                                 forever do the sleep or something and                                 then get the time create a single                                 measurement based on that timestamp and                                 then for every partition send out that                                 message it is important to note that the                                 message that goes into all of the                                 traffic or partitions is identical they                                 all have the same key the same value at                                 the same times then so how does that                                 look in our streams well a normal stream                                 now looks something like this where the                                 green balls represent the data and the                                 red clocks the the fake records in a                                 slow stream you now have this in a slim                                 stream you know I have this and if you                                 have a bursty or broken stream you get                                 something like this so you always have                                 the timestamp events in all partitions                                 that trigger watermarks that make sure                                 that processing continues so if I look                                 back at this drawing what we have now is                                 next to our website in ready to run in                                 to feed the data into Kafka at timed                                 event generator which are essentially                                 guaranteed Fantine events it's a very                                 simple thing that runs from Cuban                                 Eddie's and if it's not there it's not                                 too much of a problem because it's                                 automatically restarted within seconds                                 so now because of those events                                 if this intermediate component goes down                                 for maintenance the component at the end                                 can detect the difference between the                                 website is down and the stream is down                                 and the                                 detection alternative that is new ink in                                 Flint                                                                because it will start changing the way                                 the data is handled so how do you see                                 the difference between side down and                                 stream down it's actually quite simple                                 if the site is down you have the                                 watermarks but no data if the stream is                                 down there is nothing at all and I think                                 you should simply wait for the data to                                 arrive there is however a catch the                                 enriched step must handle it all                                 processing steps must be able to to work                                 with this so what you see is that in our                                 application because we are maintaining                                 ordering per session                                 we first do the flink operation key by                                 session ID which routes everything with                                 the same session ID to a single                                 component and then in the one component                                 that we treat receives all the timestamp                                 events some kind of deduplication has to                                 occur and then at some point it has to                                 forward all of these to the outgoing                                 partitions of the rest of the stream and                                 it's important to realize that you must                                 D duplicate and forward because the                                 number of partitions in the incoming                                 stream and in the outcoming stream are                                 in general different so if you have ten                                 partitions coming in in                                                  fifteen outgoing must have a timestamp                                 event I do realize that the ordering and                                 the synchronization between the various                                 streams is hard so that's still                                 something that we need to figure out how                                 to handle properly any final implication                                 must also handle it because now it                                 receives the data and the watermarks or                                 the the timestamp events create                                 watermarks from those and then drop the                                 timestamp events after that normal                                 processing can't continue because the                                 data will still be only the data so this                                 seems to work for us                                 in our situation but I am thinking can                                 we make this something more generic can                                 we make this a feature that can go into                                 flink or some system like that so I was                                 thinking what if we just redefine what                                 we call our topology so in the talk in                                 in the example so far we had two because                                 we had to fling jobs and at the start of                                 the pipeline it is important to realize                                 that wall clock time processing time                                 event time are all the same so the                                 events are created in the producing                                 system whether it's a sensor or website                                 or whatever with the time and of the                                 machine at that moment and then when it                                 goes through the pipeline we will create                                 each time a watermark that goes with it                                 through the flow within the flink                                 topology and then it's thrown away then                                 the data goes into Kafka again and we                                 repeat that in the in the final step so                                 what if we look at it slightly                                 differently what what if we have the                                 same set of applications but now we see                                 this as the topology and we call our                                 event generator a watermark producer                                 then we can say that the watermarks are                                 created at the same time stamp as the                                 wall clock time processing time event                                 time as the data and they are thrown                                 into Kafka as well and if then they go                                 through the day data stream it's more                                 like this they are simply demultiplexed                                 and multiplex deserialized and                                 serialized every time they go in and out                                 of a fling component the hard part like                                 I mentioned before will be things like                                 the filtering in the deduplication                                 things I also realized is that this                                 generation of the watermarks has to be                                 done at a place that is consistent with                                 all event producers                                 if you have an event producer that does                                 a lot of buffering for example an IOT                                 device or an a mobile app that caches                                 the data for a quote/unquote long time                                 then this system will also not work I                                 spoken last year at fling forward with                                 somebody from Netflix and they explained                                 to me that for example their mobile app                                 is a problem because all the events of                                 somebody watching a movie in the in the                                 airplane with delay the data a very very                                 long time                                 also the deduplication is very important                                 because if you have a few operations                                 within your topology are the number of                                 duplicates of all the watermarks will be                                 very very great so that is something                                 that I still have to look into and on                                 how that can be solved and also ordering                                 I suspect that this will only work if                                 you have a transport type in-between                                 that guarantees ordering so the final                                 step to think about that we will talk                                 about is the writing to the database                                 HBase the perfect mutator with a two                                 mega byte buffer the default setting but                                 you can change that that was essentially                                 written in the time of MapReduce                                 everything was a batch there were no                                 stream processing systems so the                                 implementation simply flushes the buffer                                 when it's full or when you close it down                                 when you do a shutdown it does a flush                                 and with if you have problematic streams                                 the data stuck and the buffer as I've                                 shown now the solution in these kinds of                                 situations in this in this situation was                                 actually quite simple                                 so in                                                                    which is now in the clonic libraries                                 that allows you to periodically check is                                 there data in the buffer and if it's                                 there for too long or you can set that                                 in a number of milliseconds flush it and                                 this has solved for our streaming                                 solutions the problems that we were                                 running into so                                 in general if you look at applications                                 and you're trying to develop something                                 and no data comes out don't panic                                 have a look at all the things I've shown                                 you and it may be possible that in your                                 case it is a normal effect of the way                                 these systems work and I hope the the                                 scenarios I've given you will help you                                 in in solving that concluding this what                                 I have seen over the years is that the                                 primary calls for all of these problems                                 is buffering buffering in combination                                 with problematic streams and then the                                 question when is my buffer flushed and                                 if the buffering is done because of an                                 algorithm like a grew by or a window I                                 recommend you solve it from the data or                                 from the algorithm and you can do that                                 if you truly understand both your data                                 and your algorithm and in any case I                                 recommend sticking to even time avoid                                 missed mixing in the processing time if                                 you can but if your use case allows for                                 it and it's not a problem you can use                                 this and if your buffering for                                 performance like a writing the buffer                                 for a database or something like that                                 just use a timeout Kafka has the Kafka                                 producer has something like that there                                 is a max linker time over there nowadays                                 the HBase has it and it's quite simple                                 to implement in most systems that are                                 out there thank you for listening                                 this was my talk if you're curious about                                 the other talk I mentioned last year                                 there is a link at the bottom of the                                 screen towards a playlist that I've                                 created with the talks I've given over                                 the last few years so you can look it up                                 there and I assume there are questions                                 now so let me see how that works                                 it means thanks to the dog I don't like                                 I saw a few people starting to die                                 but then I don't see your question there                                 yet so let's give it another minute                                 otherwise yes people might have more                                 informal questions and we can use the                                 breakout rooms for that I'm really                                 curious to hear what people think of my                                 opinion around the way to handle water                                 marks in idle situations and what they                                 are they think of my opinion regarding                                 the the idle time at system all right                                 there's a question yes max asks                                 concerning the slim and slow streams                                 wouldn't it suffice to use one                                 processing time triggers to trigger a                                 window computation from time to time or                                 and to add a time-based flushing to the                                 HBase output well this is essentially                                 what the new feature in flink does the                                 first point processing time triggers the                                 trigger window computation occasionally                                 and my statement is that if you use this                                 in a stream that is primarily focused                                 around processing then this will work                                 fine if you have a stream where the                                 computation is based on the event time                                 you are mixing the two and and and and                                 that is the reason for me to think about                                 that it will cause problems because now                                 the data is no longer purely based on                                 the outcome of the processing is no                                 longer purely based on the data but also                                 based on the processing of the pipeline                                 and depending on the use case that is a                                 very important sight point to make                                 depending on the use case this may be                                 fine and in some cases it won't                                 you
YouTube URL: https://www.youtube.com/watch?v=bQmz7JOmE_4


