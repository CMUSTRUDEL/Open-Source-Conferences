Title: #bbuzz: Caito Scherr - Streaming, Fast and Slow
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/streaming-fast-and-slow

What if you were given 2 weeks to prepare for running your first marathon, and had to be able to keep up with the fastest runner?
This is the story of a small team that jumped head first into building a top tier stream processing (Apache Flink) service on a tight timeline, and how they prepared for being able to keep up with the fastest components. 

This talk will be of interest to those who want to build larger, more complex stream processing applications (and quickly), but will also benefit anyone who is looking to adopt a new streaming technology (or use it in a way that is new to their company), at any scale. Other takeaways include pragmatic steps to leverage unique Flink features to more effectively solve common, real world data streaming problems.
Captions: 
	                              hello everyone                               and welcome to streaming fast and slow                               how to successfully be an early adopter                               of stream processing at your company                               with a particular focus on integration                               operations and analytics                               my name is kato sher and i'm a software                               engineer and i live in portland oregon                               in the u.s i have a variety of hobbies                                including woodworking                                dance and running and i first started                                working with stream processing in                                     with apache flank                                i'm here because of an experience i had                                as an early adopter of stream processing                                at my old company                                for a top tier project and i want to                                talk about the things i learned                                about successfully integrating new                                streaming application into a complex                                non-streaming ecosystem on a short                                timeline                                so i'm going to start with some                                background what the challenges were our                                use case                                next i'll go over integrating stream                                processing into a non-streaming                                ecosystem                                which is the fun part where the people                                problems and tech problems really tend                                to go hand in hand                                next i'll go over what we learned about                                operations and lastly i'll cover                                analytics focusing on monitoring and                                alerting                                before we go into background this is                                probably a good time to set some                                definitions                                i'm referring to my team here as early                                adopters but that can mean a lot of                                different things to different people                                so in this context i'm mainly referring                                to being the first one or first team at                                your company                                that's building a stream processing app                                particularly for production use                                or even more specifically the first one                                to use a particular framework at your                                company                                this is also a good time to mention that                                unfortunately the project i'm talking                                about here is not open source                                so please still feel free to reach out                                to me about it but there are some                                proprietary things that i may not be                                able to talk about                                okay so on a high level this project was                                to create a new data pipeline                                to organize track and store customer                                usage information                                in a way that's highly secure and can be                                queried for in real time by various                                sources                                on the logistical side this is a top                                tier system that would need to interact                                with every product                                and a variety of other apps and services                                particularly those involving                                accounts or subscriptions in terms of                                people in process                                there's a familiar story of a tight                                timeline and a small team                                but this had the additional                                complications of also being at a company                                that was completely new to the framework                                we selected and was also relatively new                                to stream processing                                this is an example of what customers                                would see which represents usage for                                their account                                with subscriptions to various products                                and this shows three of usually about                                seven                                so this needs to be accurate because it                                will be closely tied with billing                                and it would need to be accessible in                                near real time because customers                                sales etc may want to check their usage                                at any time on a really granular level                                how many minutes they were connected to                                the product how many cpu did they use                                in the last hour for another project                                product etc                                and i feel like we already have a nice                                list of buzzwords going here so i'm                                going to throw in                                some high throughput this shows just one                                customer                                here in the in the slide who could have                                 data for seven plus project                                 products some having multiple units of                                 measurement                                 data retention specifications and                                 subscription tiers                                 add in the fact that there's a lot of                                 metadata about                                 a lot of this and then of course                                 multiplied by a huge number of customers                                 so we needed a system that could handle                                 this growth and sustainably handle it                                 for the foreseeable future                                 luckily there's some frameworks out                                 there that can solve for all of these                                 challenges which is awesome                                 and they can do so in really a powerful                                 and impressive ways                                 so problem solved right                                 in the end yeah um but in order to use                                 the features that would end up solving                                 all of our problems                                 we would need to integrate this new                                 application to a large complex                                 pre-existing system                                 that may not be prepared at all                                 for the impact of a powerful fast stream                                 processing framework                                 um that you're adding into it so we knew                                 this                                 that we would need to compensate for                                 certain operational costs like storage                                 capacity for storing state since we                                 picked apache flink                                 storing save points uh since those don't                                 normally get cleaned up by the framework                                 however we may not have been prepared                                 for how much we needed to factor in                                 the use cases of our dependent upstream                                 and downstream apps into our program                                 design and configuration                                 so why streaming fast and slow                                 first i want to talk about speed one of                                 the reasons people love some of the most                                 popular stream processing frameworks is                                 because they optimize for low latency                                 and can consume data obscenely fast                                 both apache flank and apache spark for                                 instance can process tens of millions of                                 records per second which                                 of course is awesome so these frameworks                                 can enable you to stream data through a                                 top speed                                 cool but this means that the first                                 problem that we encountered was                                 as the engineer or engineering team how                                 do you keep up with something that                                 that's that fast to me it was kind of                                 like training for a big sporting event                                 like a marathon for your first time i                                 actually had a great example of this                                 happen to me last august                                 in the before times when you could go                                 outside with people in large groups                                 um as i mentioned one of my hobbies is                                 running and i signed up for a multiple                                 day relay race with thousands of other                                 runners                                 across half the state of oregon i also                                 may have signed up late and had about                                 two weeks to train                                 so it ended up being a very appropriate                                 analogy uh the race was like nothing i'd                                 ever done before                                 i would need to run on multiple                                 different types of terrain and different                                 temperatures and times a day                                 i would need to navigate areas i'd never                                 been to and coordinate with lots of                                 other people running before and after me                                 and i would need to be able to integrate                                 well with the                                 streams of people running alongside me                                 and on other routes at different paces                                 intersecting with mine                                 i had to come to terms with the fact                                 that just because i'm a runner                                 does not mean i'm prepared for this                                 special kind of race                                 just like how just because my team was                                 full of talented engineers and had                                 healthy applications                                 didn't mean that we were automatically                                 prepared for jumping in and keeping up                                 with a new                                 super fast stream processing framework                                 at that scale                                 but it also didn't mean that we couldn't                                 do it or even that we couldn't do it on                                 that tight timeline                                 it just meant that we had to be extra                                 cognizant of a few critical things                                 before we started                                 in both scenarios it wasn't just about                                 understanding stream processing or being                                 a skilled engineer or athlete                                 it was about how much of the teams                                 around you                                 understood the process and how well you                                 understood your impact on the others                                 in order to really integrate that                                 successfully                                 for the relay race i had to completely                                 reevaluate how i trained                                 i had to prepare to integrate the                                 aspects of running as familiar with                                 into a large complicated event that                                 involved lots of other teams running in                                 parallel to us                                 i would need to run in a way that could                                 be made compatible with their processes                                 and meet the standards of the event                                 system as a whole                                 my team had to be aware of the running                                 patterns and understand how to address                                 issues that came up                                 with the teams that we were running                                 upstream of us on the route                                 in both situations we had to strengthen                                 some communication processes within the                                 team                                 but more importantly we had to be able                                 to communicate well with other teams                                 that we wouldn't normally be interacting                                 with on a normal daily                                 jog just because by nature of there                                 being such an ongoing stream of runners                                 and that stream contained so many                                 runners we had to recognize that our                                 presence would now be impacting them in                                 ways that we had not before                                 i also had to invest in completely new                                 running gear my old gear and other                                 resources that i'd used in the past                                 past had been fine when i was running                                 only in small batches                                 but may not be able to hold up to this                                 new situation                                 when i'm running in small batches with                                 time to stop                                 check my progress refuel or readjust                                 there's certain resources i can just                                 skip if they're inconvenient like                                 wearing the right shoes                                 even getting the right fuel or                                 rehydration but of course if you're                                 running                                 anything continuously you have to figure                                 out ways to check your progress and                                 adjust your gear                                 while you're still running and of course                                 to be able to do that often if needed                                 and sustainably                                 so even if my old gear did miraculously                                 hold up for such a seemingly endless                                 dream                                 of running is it worth it for it to just                                 merely hold up                                 or is it worth investing in resources                                 that you feel confident will be able to                                 scale with you                                 ones that can really keep up to your                                 full potential for power and speed                                 and of course having to adequately test                                 the resources making sure they're                                 selecting the best ones and the best                                 configurations                                 additionally in this situation we'd be                                 running through uncharted territory with                                 no cell phone reception                                 as in no existing way to track our                                 progress for that area                                 so we need to re-evaluate how we                                 monitored our runners health and speed                                 and how we could alert on any potential                                 stops in progress or other related                                 issues                                 so in this analogy maybe you're a really                                 experienced engineer                                 maybe you've even worked with some other                                 really powerful fast really athletic                                 streaming frameworks before                                 maybe your applications are really                                 healthy and they've been in good enough                                 shape to accommodate other really                                 impressive changes and new technologies                                 the engineering team that i was on was                                 also comprised of experienced engineers                                 who had dealt with a lot of                                 really new and different things and most                                 of our applications were also in really                                 good shape                                 however just like in the relay race we                                 found that in order to keep up with the                                 speed of stream processing particularly                                 when as powerful as flank                                 and especially when integrating our new                                 service service into something so large                                 and complex                                 we still had to really slow down and                                 make some serious adjustments to how we                                 normally went about that process                                 we had to be particularly careful in                                 this case about how we impacted the tech                                 ecosystem around us                                 could the non-streaming sources upstream                                 of us and downstream apps and our sync                                 where the data ends up in our case                                 internal data stores                                 be able to handle this new continuous                                 stream and could the ecosystem of people                                 and teams around us                                 be able to troubleshoot any issues that                                 come up if they're unfamiliar with                                 stream processing or flink concepts                                 we need to drastically adjust our gear                                 our resources like our container                                 configurations memory allocation etc                                 and this one seems obvious because of                                 course you'd want to accommodate a                                 higher amount of data that you'd be                                 expecting to receive                                 you want to set yourself up for scaling                                 upwards                                 and you know but in this case we had to                                 be extra clear with stakeholders and                                 others that might be in charge                                 of say approving the resources we've                                 requested                                 especially if they're unfamiliar with                                 stream processing that we may actually                                 need more resources than just that                                 but also we may likely need more                                 ownership of those resources than before                                 so you know or whoever was in charge of                                 those resources that person would also                                 need to be familiar with stream                                 processing                                 i'll get into that later so we would                                 need to be able to allocate extra                                 resources                                 and have more autonomy over them to be                                 able to handle potential data spikes                                 and if the scene stream stops and                                 restarts unexpectedly                                 and any other issues that we hadn't                                 really had to deal with when we were                                 processing data in batches                                 which we as the engineers who have                                 researched stream processing would                                 understand                                 but again it's really about                                 understanding who else in your ecosystem                                 is aware of that                                 finally we were in uncharted territory                                 since we couldn't just stop and examine                                 a well-defined batch of data once it'd                                 already been processed                                 or wait until the records are                                 successfully written out to our data                                 store at the very end of the pipeline                                 we would need to find better ways of                                 looking inside the pipeline to monitor                                 our throughput latency and any other                                 potential unexpected stops or spikes                                 before they become an issue and again                                 this seems really obvious                                 but i want to really emphasize the scale                                 that we're dealing with here and just                                 how much more that complicates it                                 so this was a particularly important uh                                 issue since many of the teams                                 we have so many different teams that                                 interacted with us and that we're                                 writing out to our pipeline as well                                 so starting with integration um asks for                                 integrating into the technical ecosystem                                 this is a sad one uh some of the best                                 features of stream processing for your                                 team                                 can also be the biggest drawbacks for                                 teams that interact with you                                 just ouch so firstly                                 if you're unifying streams of data owned                                 by different teams you may have                                 you may find that some of those teams                                 who thought they all treated their data                                 the same                                 maybe don't um so we found that it's                                 best to assume                                 that all of these teams upstream of you                                 may be treating their data differently                                 and and of course to make sure to have                                 those conversations early in the process                                 so in our case we had really specific                                 needs for aggregating data                                 and exactly what that needed to look                                 like so                                 not all teams were able to implement                                 logic fast enough on their ends                                 to sufficiently adhere to our schema                                 before launch again this is for a                                 project that's on a tight timeline so                                 how do you                                 be successful in that shorter amount of                                 time so                                 we had to factor that into our time and                                 design                                 we actually ended up creating a second                                 flink app that provided all the                                 aggregation calculations on the data                                 that still needed it                                 but this wasn't just an issue of extra                                 engineering hours spent on coding                                 this meant we now had a complicated                                 ownership problem                                 we had to get really creative about how                                 we split up who owned pre-aggregated                                 data                                 for which teams and subsequently who was                                 responsible for alerting and monitoring                                 on it                                 this in turn meant that we had to become                                 a lot more familiar with another team's                                 domain in order to have                                 context for even the small slice of                                 their system that we were monitoring for                                 and i want to emphasize that for some of                                 the biggest hurdles we had                                 the best resolutions came from really                                 slowing down and talking with those                                 upstream and downstream teams                                 really optimizing for our understanding                                 of                                 our impact on each other's domains and                                 letting that data                                 inform our architecture and design                                 process                                 otherwise you could end up at the last                                 minute with data in a form that's                                 unusable for you                                 or you could end up with no one wanting                                 to take ownership of monitoring and                                 consequently                                 insufficient insight into your data and                                 worse insufficient                                 alerting and this leads to some other                                 related issues                                 oops yeah with uh did not skip ahead                                 with being an early adopter and part of                                 that is i want to talk about                                 creating and automating community within                                 our within your company                                 for the stream processing framework that                                 you've selected                                 and basically how this was really one of                                 the best ways we found to mitigate some                                 of those roadblocks                                 this was helpful because as i mentioned                                 we were impacting teams in ways that we                                 had not impacted them before                                 we were also using shared resources like                                 aws buckets and internal container and                                 deployment tools                                 in a way that they had never been used                                 before either this meant we needed to                                 understand                                 other teams applications much better                                 than we'd normally have to                                 but it also meant that other teams and                                 leaders architects                                 etc suddenly needed to be familiar with                                 our monitoring                                 as well as be familiar with some basic                                 stream processing and flink concepts                                 so they could understand our monitoring                                 so what do i mean by automating an                                 internal community                                 okay so if we address each inter-team                                 related issue as it comes up                                 this would be a long and painful process                                 and it would keep us away from our                                 engineering                                 work so we found that the heart of most                                 these issues were the same                                 engineers having to repeatedly advocate                                 for or explain our system or flink                                 concepts to others                                 this meant though that we could easily                                 automate resolution for these issues                                 and make a virtuous cycle of information                                 sharing through creating things like a                                 safe space for others to experiment with                                 stream processing and in our case flink                                 as well as providing interactive                                 documentation and of course                                 a good detailed map so here's where the                                 running analogy gets a little too                                 literal                                 so my team for the relay race were also                                 my co-workers in a different org though                                 so my running team and my engineering                                 team both use the same internal                                 platform to both create blog posts that                                 both cover the same two topics                                 including we both had how-to manuals                                 for anyone who was interested in either                                 training like my running team                                 or creating their own flink cluster and                                 how to integrate that with the internal                                 tools at that company                                 we each also had an overview for the                                 project with a glossary of terms                                 which was really helpful uh the blog                                 format of course was chosen specifically                                 so that people from other teams                                 leadership etc could ask questions or                                 even give advice in the comments                                 super simple super effective um sharing                                 these posts saved us                                 so much time and having to explain what                                 the heck we're working on in both cases                                 so as for supporting experimentation                                 both for the launch of the relay race                                 and for our new streaming application                                 my team in both situations created                                 accessible chat rooms to support people                                 who are experimenting with                                 and wanting to get help for something                                 that was new for them                                 in both cases people would ask each                                 other questions about what resources                                 progress monitoring tools and other                                 things that people were used that were                                 related                                 people would ask if others wanted to                                 collaborate on training sessions                                 together                                 they would encourage others for how well                                 their sessions went and commiserated                                 when experiment failed                                 this sounds basic because it is super                                 basic                                 but it was an incredibly important                                 baseline for us                                 that enabled us to cultivate a group of                                 people who felt comfortable                                 doing their own work to become more                                 knowledgeable and became increasingly                                 more enthusiastic about participating                                 and helping each other out                                 in the end my team was able to lean on                                 them                                 a lot uh and lean on this whole                                 community that we had built                                 and they were able to help us out and                                 actually get us unstuck on a lot of real                                 problems really big problems um later on                                 in the project                                 so it was huge and i couldn't share the                                 internal work slack but here's a                                 screenshot from the slack space for the                                 running team                                 um so just replace elevation gain with                                 records per second and miles with kafka                                 topics and you can                                 you could basically swap those two out                                 thirdly                                 always take a map in both scenarios this                                 wasn't just about having a math                                 map of the pathway from start to finish                                 but it was essential that we clarified                                 the integration points                                 along the way so other teams could                                 quickly assess where they were and who                                 they impacted in case of an emergency                                 again it's all about making this                                 very efficient and easy to read so with                                 each more detailed iteration                                 of our map uh other teams that                                 interacted us with us were able to                                 become                                 more and more autonomous giving us more                                 time to work on our engineering work                                 another essential map that we used in                                 both cases was a flow chart for incident                                 response and triage                                 if you get lost in the woods and end up                                 running in a circle which did happen to                                 someone                                 or your flink application stops and                                 instead of restarting successfully it                                 begins to cycle for minutes on end scary                                 stuff                                 in both scenarios the incident response                                 flow charts were ridiculously simple                                 because that's exactly what you need if                                 you get lost in three in the morning                                 either way which brings us to operations                                 most people here probably have a good                                 idea of the main operational drawbacks                                 drawbacks and risks of stream processing                                 like storage                                 particularly with frameworks like flink                                 where you might be storing things like                                 save points                                 in a different way that you're storing                                 elements like your state's state                                 snapshots most well architected stream                                 processing frameworks                                 have some features or workarounds that                                 make this pretty easily remedy though                                 we even planned ahead for this project                                 um for how well                                 that flinks specifically would integrate                                 technically with the tools that we'd be                                 using and had support for                                 a lot of our choices had to do with how                                 well it could integrate with things                                 like mesos and zookeeper however                                 because this was a large company with                                 lots of operations separated into                                 specialized teams                                 we still had issues with the fact that                                 just because these tools are technically                                 compatible                                 didn't mean that they were configured in                                 a way that was going to be really                                 helpful to us                                 it also meant that we had no control                                 over reconfiguring them                                 and they might be a shared resource in                                 which case configuration would be slow                                 and involve a lot of completely                                 unrelated teams as well                                 internal deployment tools are probably                                 the most common example for this                                 and something that we did have to find a                                 workaround for                                 since we didn't own them and deployment                                 is used by just about every team                                 reconfiguring them for just our use case                                 was                                 was and would be a very arduous process                                 we worked around this by creating and                                 owning a deploy script in the beginning                                 so we could we could get started and                                 still be able to deploy and and be able                                 to iterate quickly                                 and we did that while we were working on                                 a more automated long-term solution                                 which we were eventually able to do and                                 at first we manually uploaded the script                                 and that was something we were able to                                 hook into the tools using a hook that we                                 wrote as well so there was a lot that                                 went in there for                                 creating this uh on our team from                                 scratch                                 so lastly analytics with the running                                 analogy                                 it was important to make sure that                                 monitoring was well coordinated                                 between teams and our stakeholders                                 in this case in the running case                                 stakeholders could be the rest of your                                 team who's ahead or behind you or it                                 could be your family                                 who's following your progress on a                                 running app that's streaming your gps                                 signal to them in real time which was                                 pretty cool                                 during the relay race my stakeholders                                 which was my family were remotely                                 following my progress on the app                                 and when my progress stopped and                                 disappeared suddenly at three in the                                 morning                                 with no way of alerting them that was                                 some pretty unhappy stakeholders                                 and none of us wants unhappy                                 stakeholders                                 so although my team learned that we                                 needed better monitoring for                                 our running routes kind of like a                                 pipeline                                 it helped that the the team at least                                 knew what the normal pace was for each                                 runner                                 which gave them a reasonable threshold                                 to know                                 when to alert of a certain duration of                                 time it passed without seeing a runner                                 exit the                                 pipeline luckily the engineering team                                 made several features to compensate for                                 this including creating unique fields to                                 key data on just for                                 within the pipeline in the stream                                 processing scenario                                 stakeholders often ended up being teams                                 that own the applications                                 upstream and downstream of us as i said                                 earlier                                 we found that we had to be much more                                 familiar with particularly our upstream                                 applications                                 than we'd ever needed to be for our old                                 batch applications                                 and this is particularly true for                                 applications that may have really                                 sensitive alerting around                                 increases or pauses and data flow                                 especially                                 if their way of alerting and what they                                 want to alert on                                 is not totally aligned with what you're                                 looking for um                                 and again that might sound obvious but                                 it was something that we really had to                                 pause and take a                                 pretty solid look at so if you're                                 relying on their alerting to signify any                                 issues                                 that might legitimately impact your                                 pipeline definitely a great opportunity                                 for                                 collaborating on that and re-evaluating                                 that                                 so for ensuring that these teams                                 understood our monitoring                                 most of that was about                                               relabeling our dashboards and posting                                 them in our stakeholder slack                                 so ended up being pretty efficient                                 actually so for instance                                 we may need to monitor spikes and data                                 from a particular source                                 like one of our incoming kafka topics                                 but the data coming in from that source                                 only affects one of our downstream teams                                 so                                 if we wanted to be nice and keep things                                 really efficient                                 we wanted to make sure that it was                                 really clear in the labeling so that                                 only that team would know that it                                 affected them                                 and also that they would absolutely know                                 that it that they should be paying                                 attention to it as well so they wouldn't                                 miss it                                 and uh earlier going back to the the                                 running analogy                                 so in that several day relay race those                                 issues like unexpected stops in progress                                 could signify a really serious issue so                                 it was imperative to have a way to                                 monitor an alert on this                                 it was also important to understand as i                                 said earlier the typical pace of a                                 runner                                 or in the stream processing case it's                                 important to know what normal                                 looks like for your application and i                                 don't just                                 mean kind of a good enough gut feeling                                 but really making sure the whole team                                 understands what normal is on a very                                 granular level                                 we also found that for all of our older                                 projects                                 and all the other um non-streaming                                 applications that we started from                                 scratch                                 really only the team needed to know what                                 normal looked like and for this                                 situation we found that a lot more                                 stakeholders architects etc                                 also needed to be made aware of what                                 normal looks like                                 because it's just gonna have that much                                 wider impact so                                 um it doesn't really have to be that                                 complicated is what we found                                 so what i have here in these slides is a                                 bit of an oversimplified example                                 but we've got three products that were                                 upstream of us represented here                                 in this situation                                                   spike                                 was normal but we only knew that was                                 normal by talking to those upstream                                 teams                                 and we also found that just knowing what                                 normal is                                 and just saying okay i've checked that                                 box i know what it is i'm going to put                                 it in documentation or whatever                                 isn't always enough because what if they                                 change their schedule                                 or what if one team's products event                                 time processing is based in utc                                 and the other in pst uh most of these                                 things were not a problem                                 until we started combining those data                                 streams into one pipeline                                 uh also what if one of those time zones                                 does daylight savings and one doesn't                                 so finding those things out ahead of                                 time was incredibly helpful                                 um and it's really about again not just                                 finding what normal is now                                 but what normal is going to look like in                                 the future what normals look like in the                                 past                                 and keeping an eye on if normal changes                                 and how long of a period of time means                                 normal and what is good and bad in that                                 situation                                 so again that normal spike might move in                                 the future                                 so it's really about understanding what                                 their as in the upstream teams                                 threshold for alerting is and how that's                                 going to impact you                                 and it's again it's about ensuring if                                 you're monitoring the right metrics                                 and what you're doing to ensure you know                                 what those metrics                                 not being correct means so in this one                                 we actually had some debates                                 um again this is just an example but                                 it's an example of basically a real life                                 thing                                 of you know we had the threshold a                                 little bit higher so it was actually                                 above                                 that the spike that's in red um                                 and the upstream teams let us know like                                 no this actually really matters that                                 spike is bad we want to make sure that                                 we are alerting on that the instant                                 it comes up there even though that same                                 spike is perfectly fine if it happens                                 within                                 the                                                                 understanding about where that threshold                                 is for those teams and not just about                                 for                                 for your pipeline as well um                                 so yeah like are there daily spikes that                                 are okay                                 and uh and again this example i think                                 one of these was that the app                                 did that team's app did right like on a                                 once a day batch                                 so it's all very good stuff to know and                                 in both scenarios                                 seeing a slowdown in progress might                                 actually be okay but it's all about                                 the fact that you want to calculate a                                 threshold to alert on based on that                                 normal and based on what abnormally slow                                 is for that particular application or                                 data stream or that runner                                 and so you know when you really need to                                 start getting concerned                                 so in the end i love working with stream                                 processing because it can be so                                 incredibly fast frameworks like apache                                 flink spark and kafka streams                                 have amazing features that can and and                                 honestly genuinely for us                                 did solve a lot of really big                                 complicated problems                                 however keeping up with something that                                 fast and powerful can definitely take                                 some creative readjustments to integrate                                 if you're putting that into a                                 pre-existing non-streaming system                                 uh take it from me though when you speed                                 through that finish line totally worth                                 it so with that                                 thank you so much everyone and i'm                                 really looking forward to meeting you                                 all                                 in the social and in the slack space so                                 come say hi                                 and a special thank you to berlin                                 buzzwords haystack mices                                 and all the event staff and um yeah                                 big thanks                                 so thank you everyone to join the                                 session with kaido                                 hi uh so feel free to post any questions                                 on the channel                                 bebas                                                             and uh yeah                                 cool thank you so much thank you uh very                                 much for moderating this this was                                 awesome                                 uh very surreal to watch myself talk                                 oh that was super fun and yeah feel free                                 to post up on the channel um                                 i as i as martina mentioned i won't be                                 able to join the breakout session um but                                 i'm here for the conference and i'll be                                 at all the socials and                                 like seriously please feel free to reach                                 out to me over slack or the                                 umbrella meetings i promise and friendly                                 you
YouTube URL: https://www.youtube.com/watch?v=6AHDYEwLi6Y


