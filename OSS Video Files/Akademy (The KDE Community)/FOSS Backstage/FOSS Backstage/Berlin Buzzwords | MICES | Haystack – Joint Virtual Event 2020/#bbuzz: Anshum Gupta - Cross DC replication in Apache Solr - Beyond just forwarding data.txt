Title: #bbuzz: Anshum Gupta - Cross DC replication in Apache Solr - Beyond just forwarding data
Publication date: 2020-07-05
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/cross-dc-replication-apache-solr-beyond-just-forwarding-data

The increase is data and traffic accompanied with the feature set offered by Solr has led to an increased usage of Solr in recent times. As a lot of these applications are critical, a reliable disaster recovery mechanism has become essential.

While Solr already offers a way to accomplish disaster recovery using cross-dc replication out of the box, there are other ways to accomplish the same. Copying data is the obvious part of this system but a lot of supporting features or systems like monitoring, self-defense mechanism etc. are needed for a complete cross-dc solution.

A simple solution, like the one that Solr offers out of the box works 90% of the times, but the lack of supporting features, monitoring tools, and also self-defense mechanism make it a difficult choice in a lot of places. Using such a system might catch you off-guard and leave you with a false sense of DR availability.

During this talk, I would like to talk about a few approaches of achieving cross-dc availability in Solr, something that I have tried and used over the years and highlight the pros and cons of each of them. I would also talk about all the satellite features and applications that are needed to ensure consistency of these clusters in addition to the self-healing, and self-defense mechanisms that are needed to reliably run and use the DR clusters.

At the end of this talk, attendees would have a much better understanding of achieving cross-dc for Solr and their reliability levels. They would also have learnt about the supporting systems that are needed to have a solid cross-dc story in Solr, one that scales and works reliably.
Captions: 
	                              you know so hi everyone welcome to my                               education in a passive solar beyond just                               forwarding data and thank you for the                               introduction                               so these enough for the day and thank                               you for bearing with us the agenda for                               the day is I'm going to start off with                               talking about the basic architecture of                               solar cloud then move on to talking                                about the need for disaster recovery and                                then I'm gonna talk about different                                approaches to achieving cross DC and                                solar and going to spend most of the                                time for this talk on that one specific                                thing and then wrap it up by talking                                about what I believe could be the future                                of of how achieved rustici in solar in a                                manner that he serve and works for most                                people so a very simplified version of a                                solar architecture diagram is something                                that looks like this                                you've got shards that have leaders                                followers and they talk to each other                                and they can they pretty much play the                                same role other than the fact that every                                incoming update is drawn through the                                leader who's also responsible for                                versioning and doing a bunch of other                                you know managerial things as far as                                handling documents and data is concerned                                so like any distributed system it seems                                fairly easy to run and manage and the                                shard replicas there they provide                                scalability and availability so if one                                of them goes away the other one comes in                                assumes leadership also if years if you                                need to support more traffic you can                                always add more more replicas there and                                so that kind of solves most of the                                problem for standard use cases but when                                it comes to critical systems it's kind                                of a no-go to just rely on those basic                                basic things that a basic slow cloud                                setup provides because any critical                                system for need to account for things                                like hardware failures natural                                calamities and more most importantly and                                the most common of the reasons as to why                                things go down human errors inevitable                                solutions to solve these                                the most common one being backup and                                restore well it doesn't really do                                completely I mean off the job                                also when you're backing up and                                restoring unique you should be backing                                it up to a data center oh that's that's                                outside of your primary data center only                                because if something were to happen to                                your data center you need somewhere to                                restore your data from so that comes                                with a lot of caveats in addition to the                                fact that it it doesn't provide                                everything that you might need from from                                our solution for that supports a                                critical system so there comes across TC                                replication which not only provides                                guarantees around availability but also                                provides things like scalability and                                reduced latency so for example you know                                if you were running a service that was                                supposed to only clear to North America                                and you had your data sitting in data                                centers across the u.s. all of a sudden                                you had to provide some sort of support                                to order some you were launching the                                same service making this data accessible                                to people and in Europe you could set up                                across DC replicated solar cluster in a                                data center in Europe and that would                                allow for you to have lower latencies                                for users who are in Europe so what what                                is cross this your application in the                                most simplest simplest of terms it's                                nothing but a means to achieve the Sun                                of mirroring effect across data centers                                for solar clusters so anything that you                                video ingest in data center one should                                show up in data center two for it to be                                searched on retrieved or whatever so                                that's the most that's a basic                                understanding basic need of why you                                might want the CDC across data center                                application so now we're going to talk                                about the different approaches to                                achieving cross data center replication                                the first one the client base replicate                                is something that we started off with                                with an apple and it can't predates all                                other solutions because it doesn't                                 really require too much to be built on                                 it it does rely on your users being wise                                 it does rely on your users taking                                 responsibility so this this solution                                 kind of predates everything else that                                 was built to work out of the box or                                 things that for support something like                                 this if you look at what happens in a in                                 a client based data central application                                 as obvious it's the responsibility for                                 everything relies on the client in such                                 case and the client is basically                                 responsible for managing external                                 versions so it does provide some form of                                 not some form of but a pretty good                                 optimistic concurrency and so as long as                                 you you're worsening your documents                                 correctly so there's gonna take care of                                 things for you but but the versioning                                 has to come from the client in a setup                                 that relies on the client to provide                                 cross data central replication it and                                 not only that in this case client would                                 also have to plan also ends of managing                                 request failures in three tries so the                                 challenges in such a system though are                                 if you had if you had a client sending                                 data and say it succeeds in the local                                 data center that's activator so if you                                 had a client sending data to your data                                 center and it succeeds and one of the                                 data centers but does not succeed on the                                 other data center it's the clients                                 responsibility to make sure that that                                 that failures taken care of and the data                                 is synchronized and consistency is                                 checked and users are alerted when when                                 the data doesn't make it through to                                 you know across all the data centers not                                 only that it's all the process has to be                                 synchronous some extent the reason being                                 if the client sends data to data center                                 one hears back it's positive acts since                                 data to data center two doesn't hear                                 back it's its responsibility to close                                 that loop and either delete that data                                 from data center one or wait until data                                 center                                                         positively acknowledge that request so                                 it kind of makes it rather difficult for                                 the client to operate in such a mode                                 also when it when a data center goes                                 down so for example if data center to go                                 to go away due to network outage weather                                 issues whatever it might be the client                                 is going great stuck and that might                                 require changes in the configuration for                                 the client to make sure that you can go                                 ahead and you can go ahead and forget                                 those problems and ignore those problems                                 for the client to understand that that                                 it's okay to ignore those failures might                                 require a config change in in such a                                 case so pilot while the system looks                                 very simple to begin with in my opinion                                 the cost to operate this is pretty high                                 especially if you are going to end up                                 using it at any given point in time and                                 that that comes from our experience                                 having asked our users to start off with                                 this vicar's of lack of any other                                 solution a long long time ago and it's                                 kind of still okay if you're doing this                                 by yourself for a sort of cluster that                                 you're the user of but kind of gets                                 really difficult when you're just a                                 provider of the platform than others use                                 because as I said in one of my talks I                                 think last year sorry I've said in one                                 of my talks last year                                 is that when you're finding a platform                                 your hue servers are not your best                                 friends so there they want the crates                                 table of your running system they want                                 their best the best performance to come                                 out of it but they're not they're                                 willing to take you down if that means                                 you know a marginal improvement for                                 theory use case so they're not they're                                 not bothered but they will bother you                                 when it comes to resolving a problem and                                 so you know in such a system where                                 you're relying on your client to send in                                 these this data and make sure that it's                                 consistently sent across multiple data                                 centers and cleaned up when there's need                                 to do so that problem gets amplified                                 when you're running a platform so I                                 really feel that in in platforms this                                 value kind of goes not it not to zero                                 but a time stood to negative because                                 you're firefighting and trying to figure                                 out how to now heal and fix these                                 problems that were created by users who                                 did not understand the implication of                                 what they were doing so that brings me                                 to the to the next approach so while                                 that was something that I know                                 well we haven't were doing and a few                                 other companies were also doing things                                 that way a bunch of people in the                                 community realized there was a need for                                 a genuine cross data center replication                                 solution within solar and and so a few                                 folks in the community Erik Erikson I                                 guess primarily Oh fanned out and                                 started working on across DC replication                                 mechanism that would be supported by                                 solar out out of the box and the way it                                 works is any request that comes in and                                 remember most of these requests that I'm                                 looking at or either admin requests that                                 make changes to the state of the system                                 or their requests that are there update                                 requests anything outside of that does                                 not really need                                 [Music]                                 replication for example select ways have                                 no need to be replicated to go anywhere                                 other than the data center of their                                 they're sent to and so to collection                                 admin requests that cost no state                                 changes to the cluster itself so when a                                 request comes in the data centers are                                 working in isolation part they to have                                 some knowledge about each other so the                                 shard in this example the shard one                                 leader is going to not only index things                                 locally and send them to the follower                                 but is also going to somehow manage to                                 send this data across the pipe so that                                 across DC replication happens to the                                 leader of the corresponding shard in                                 another tanker now if there are more                                 than one of these kinds this leader is                                 going to ensure that it reaches every                                 other leader that should have this data                                 this document so in terms of the code                                 path the way it really works is the                                 request comes in and for our few who are                                 of the era of how sort of works there's                                 something called a update request                                 processor chain which is nothing but a                                 but a pipeline of sort that works to                                 process if every incoming update and so                                 on the leader it comes in gets processed                                 through the update request processor                                 chain and then gets dumped into the                                 transaction log transaction log and                                 solar is is basically it's a log it's a                                 log of all the updates that are coming                                 in and there is a thread that's running                                 on side of the leader that called the                                 CDCR replicator thread that tracks what                                 are the replicated data centers data                                 center clusters or CDCR clusters and                                 reads data of this transaction log and                                 based on some basic configuration around                                 bandwidth utilization and other other                                 things reach data from the transaction                                 log and                                 is that out to the leader of the target                                 DC leader on the target DC the                                 difference on the target DC being it's                                 smart enough to make sure that it                                 doesn't this data does not does not come                                 back to the primary DC that originally                                 sent this data so in that in our diagram                                 what we sees that the transaction log                                 basically is acting like a cue because                                 everything that's indexed locally is                                 going into the transaction log kind of                                 getting buffered there and then being                                 read by by the thread that's responsible                                 to send out all of these updates to                                 everyone who spoke was supposed to be                                 replicating and we do get and eventually                                 consistently our cluster that supports                                 both active passive and active active                                 set ups but with a bunch of caveat                                 especially because the versioning in in                                 solar CDC our solution relies on a                                 clocks and so if you have data centers                                 who have clocks let us think that might                                 be that might be kind of complicated or                                 or scary like the best part in my                                 opinion here is that its standalone                                 which basically means it translates to                                 no external dependency you don't really                                 need anything else running on the side                                 you don't need to rely on a third party                                 system also because it's part of solar                                 you do get community support from others                                 we're using the exact same solution                                 instead of having their own custom                                 solutions trying to replicate data from                                 one place to the other                                 so the rip but obviously this comes with                                 limitations one of them being the                                 transaction log considering its use of                                 the queue if your data if your target                                 data center for some reason goes away                                 your primary data center is going to                                 accumulate all all the incoming updates                                 in its transaction log without purging                                 them only because it can't                                 that's the only place from fair to                                 through free play and replicate stuff to                                 target data centers and that that                                 certainly Mike Ranney are at risk on                                 your privateer                                 Center the configuration on this one is                                 not easy for some reason there's                                 collection level configs and cluster                                 level configs and a bunch of other                                 configs that kind of make it harder to                                 use in my opinion and then the                                 transaction log approach doesn't really                                 catch up because you're looking at a                                 file based transaction log that's                                 sitting on the system                                 OAH which is okay for small use cases                                 that don't have a ton of data coming in                                 every second but if the target cluster                                 is gone and you have a ton of data                                 coming in the transaction log will grow                                 to a size that those CDCR thread is not                                 going to be able to catch up with so                                 it's kind of a no-go for any                                 high-throughput situation and then the                                 the support for implicit wrapping is                                 missing which what that basically means                                 in solar is if you are not using solar                                 scoffs that idea routers or hash-based                                 traveler robbers if you're trying to                                 send data explicitly to a specific shard                                 that will not get replicated by the CDCR                                 that is that comes out of the box with                                 solar because that's not supported the                                 metrics that CDCR supports also get                                 reset on restart a lot of those metrics                                 are okay to be reset but there are times                                 when you might need historical data                                 which will go away as soon as your solar                                 instances bounced and obviously there's                                 an extra burden on chart leaders because                                 they're responsible for replicating all                                 of this data across to the other data                                 center so why spoke about all of that                                 one interesting thing to know is that we                                 never really used at Apple CCR that was                                 offered by solar not because we had                                 concerns I mean there were certainly                                 things that we were concerned about but                                 because while the fever runs on the                                 first approach to solving CDC are by                                 using client-side replication we had                                 already started working on our own                                 internal solution that was closely                                 coupled with                                 with what what we needed and while that                                 happened the community-based CDC are                                 also was introduced but we already had                                 our own version of CDC are that we                                 thought was doing the job pretty well                                 and was also interesting a lot of these                                 problems that I just spoke about so this                                 third and the last part is pretty much                                 that covering and touching upon those                                 things so we've been using this for                                 think about for five years now and what                                 it provides is it writes an easy way to                                 replicate you know n ways across n data                                 centers in an active-active manner it                                 also allows for provides retries and                                 error handling error handling for failed                                 requests across data centers and                                 consistency isn't it wasn't a sorry                                 that's wrong I mean consistency in this                                 system is checked it's not guaranteed                                 obviously because if something happens                                 and you get out of sync the least that                                 would happen is that he would get no                                 award n't a consistency we have systems                                 in place therefore that would allow us                                 to fix it but it and the very least                                 informs us of consistencies when they                                 happen and insights were missing from                                 the previous rep trustee see replication                                 things like latency consistency errors                                 and retry counts but in this case we                                 build a system where we get all of those                                 numbers as well so how is this this set                                 up look so the clock this is the basic                                 architecture diagram basic architecture                                 diagram for what we use and the Hughes                                 Kafka people trustees your application                                 using proprietary cues but also Kafka                                 and when a request from the client comes                                 in it comes into the primary or whatever                                 is this                                 targeted solar DC that it comes into                                 that is its own local zookeeper and it                                 has its own local cue so if you look at                                 the closely coupled orange yellow and                                 the green boxes they're together in one                                 one data center and then there's a                                 mirror in the middle that mirrors this                                 onto another cue and sends this data to                                 be consumed by something called the                                 cross DC consumer which is a standalone                                 app and we're going to get to that in a                                 bit and that runs in isolation again and                                 has no idea about the existence of off                                 the data center that this request                                 originally came in came into so what                                 this gives us is the ability to isolate                                 the Q and a mirror from from all of the                                 stuff that is best solar so see we're                                 running short on time now so data flow                                 in in this trusty C plugin kind of is                                 implemented as an update request process                                 or when a client sends in a request it                                 comes in where the doc version n an ID                                 and the receiving data center accepts it                                 only if the received doc version is more                                 than the already existing version for                                 that document if that document exists in                                 the index already and then as and then                                 solar obviously assigns its own version                                 value which is used internally where's                                 my solar but what this this update                                 request processor does is it strips off                                 this version field and and inserts this                                 into the Q into the source Q on on the                                 source or the originating data center                                 which is then replicated and copied over                                 to the other other data center by the                                 mirror in the middle so the its then                                 received by the cross DC consumer and                                 the job the cross DC consumer is pretty                                 straightforward it's a very                                 straightforward simple app what it does                                 is it reads from                                 the destination queue which is isolated                                 from the source here because the mirror                                 just mirrors stuff based on config and                                 then it reads that data and it tries to                                 send these updates to solar now this                                 does request could in isolation                                 succeed or fail fee over time learnt by                                 large error by making a lot of errors as                                 to what makes sense to be retried and                                 what makes sense to be resubmitted or                                 discarded so one of the one of the these                                 things happen when when a request is                                 processed by the trustee see consumer                                 they either are successful requests or                                 their failed request that should not be                                 retried anymore or their temporary                                 failures that could be fixed and so                                 these are then resubmitted onto a                                 separate topic and we submit these with                                 with a time stamp on when this was lost                                 retried how many times it has been                                 retried so far so that or when it comes                                 in again to be free retried win no how                                 many times has it been three tried to                                 limit                                 at or alert if if request has been                                 retried too many times                                 without any success and we put some form                                 of an exponential back-off algorithm in                                 there to make sure that the same request                                 doesn't get get retried too often I know                                 and the two kinds of requests that                                 primarily if we finally ended up                                 rejecting outrightly are the four nines                                 which is basically something is a case                                 where sort of would go out and say that                                 based on optimistic concurrency the you                                 know the version that you're sending for                                 this document is older than what I                                 already have which means there's no need                                 to send me this update I already have                                 newer version of this document or in                                 case of a collection creation come on                                 that that fails because the collection                                 already exists because if the way it's                                 set up right now is the primary DC's                                 submits these requests into the queue if                                 for some reason it failed or passed and                                 two of these requests were submitted to                                 the primary DC and made it into the                                 queue they would be received by the                                 receiving DC's they should just be                                 rejected because they they're kind of                                 important they wouldn't cause any damage                                 but there's no point retrying them and                                 the other good thing here is the envy of                                 replication which kind of makes it easy                                 and isolates everything so if you look                                 at the solar cluster at the top the                                 yellow and the red block the yellow red                                 and green boxes that are around each                                 other or kind of they work in isolation                                 so the solar cluster writes to a load to                                 the source topic solar also works a long                                 way the cross DC consumer which reads                                 from a destination topic and then                                 there's a mirror sitting in the middle                                 that's configured to just trade off of                                 all sort of a source topic and replica                                 and copy this data or into multiple                                 destination topics so if you wanted to                                 add another another data center yes                                 there's some some effort that would be                                 required to bootstrap these but once you                                 have your indexes book strapped all you                                 need to do is                                 set up or add a convict with a mirror                                 set up a destination topic start a                                 crossed easy consumer and you'd have a                                 sort of cluster that would now be                                 receiving updates from from other sort                                 of clusters and also mirror is something                                 that's lightweight and is kind of                                 provided by most most queue systems                                 handling of elite requests is a little                                 different because they're not versioned                                 relate by queries are not handled                                 they're not supported deeper IDs are                                 however supported but it works as a                                 tombstone and this to zoning is very                                 different from listening to stones in a                                 sense that the document is left there as                                 per if you were to compare to a leucine                                 document the document is left there                                 active and alive further in terms of the                                 scene but in terms of the the business                                 use case its marked as deleted so that                                 none of the requests come back with                                 these documents the reason why this was                                 done this way was to ensure that any                                 accidental delete request that was sent                                 to a primary DC does not blindly delete                                 stuff and replicate that stuff because                                 that would defeat the purpose of having                                 a TR strategy because it wouldn't                                 address that that human error so you                                 could go back and undo leave these but                                 if we generally never need this which                                 allows us to run path jobs to clean up                                 all of these tombstone documents on a                                 regular basis inconsistency detection                                 which is kind of super critical because                                 if you have a crosstie C set up up and                                 running but you don't know if the data                                 across your data centers is actually                                 replicated whether or not you you don't                                 really know whether you can use it and                                 in case of complicated systems that                                 might lead to to a situation where                                 you're investing a ton of money into                                 large D are clusters that are sitting                                 but data                                 is inconsistent and unusable so you                                 didn't need an inconsistency detection                                 mechanism at the very least to make sure                                 that all the money that you have                                 invested in setting up a dr cluster is                                 is actually ready to be used or ready to                                 or it's kind of making sense so as I                                 said detection is the more essential                                 part of it healing not so much because                                 there are multiple ways to heal you can                                 replicate again also if you're having                                 multiple having this problem over and                                 over again that's a bigger problem to                                 look at as to why you're cropping data                                 or why do you even have this is this                                 inconsistency but otherwise all you need                                 is a detection mechanism at the very                                 least and this detection has to happen                                 not on just merely the basis of number                                 of documents which a lot of people - as                                 a very basic rudimentary approach to                                 figuring out if the data across two                                 different disease is the same or not but                                 what we do is that we we check on every                                 document a la document ID and version                                 trouble to make sure that that that pair                                 exists across all disease and the                                 interesting thing in designing a system                                 like this is to a is to account for all                                 the data that is still in flow so when                                 you look at the state of of a primary                                 data center or a tailor or potato Center                                 X when you look at another data center                                 that's supposed to be a mirror of this                                 one there would be data that would have                                 gone from into between or between these                                 two data centers cause a for a different                                 version of your data's not sure that                                 you'll be looking at so when you're                                 designing such a system you need to be                                 aware of of this and I think we                                 certainly don't have enough time to look                                 at this but the way we solve this                                 problem is by using Merkle tree based                                 implementation and the way it works is                                 every document so we have a bunch of                                 seeds we define how many seats do we                                 have and we define the number of buckets                                 or seeds                                 just randomized and when we define seeds                                 and pockets is basically a                                 two-dimensional array the top every                                 document in a data center is then hashed                                 using each of those seeds and then                                 marked in each of those buckets so you                                 see one blue cross in each of those rows                                 there that represents they talk one or                                 the bucket that document belong to when                                 they were hashed using seed                                                                                                                   your document your seed and your number                                 of pockets and you do the same exercise                                 in your secondary data center and you                                 compare these two to figure out the                                 difference or the missing data between                                 both of these now the way we look go                                 back to circa to look at whether they're                                 consistent or not is not to just read                                 run this after say say two minutes but                                 what we do is we we have a predefined                                 time limit and say okay we know the                                 documents that were not visible in the                                 last in the last run so what we wanted                                 concentrate on now is did those                                 documents ever show up if they did show                                 up or they are there with the version                                 that we saw them originally in the                                 primary data center with or with a newer                                 version both of these cases are good                                 cases but if they either did not show up                                 or showed up but had a version that was                                 lower then then what we saw the primary                                 data center that's a red flag that's                                 when we kind of alert for like hey there                                 seems to be an inconsistency it's                                 something something needs to be done                                 about it and the cool part about that                                 brooch also is of using such a system is                                 you have a queue based system right so                                 you can always replay things if you                                 think you missed out and stop on                                 something or you're dropped data it's                                 always possible to go back and replay if                                 all of this data is sitting on a system                                 that you put to work with your solar                                 cluster rather than completely relying                                 on just your solar cluster                                 so to summarize their approach or                                 approach their segregation of                                 responsibility is something that we                                 really wanted to do so that Solar is not                                 left with taking care of stuff outside                                 of search and queuing and mirroring or                                 managed by third party cues so what that                                 means is you don't have to really even                                 run it yourself you could be using and                                 consuming a service that is either                                 provided by someone else it could be it                                 could be publicly provided queuing the                                 chasm or it could be another team that                                 provides the service to you in your                                 organization and there's retrying of                                 failed requests in addition to the                                 possibility of just retrying everything                                 but rewinding the head on your cue and                                 add another TC is easy because all of                                 these all of these data centers are kind                                 of agnostic of each other the only                                 person who really knows about the                                 existence of each of those pcs is is the                                 consistency checker which runs in                                 isolation or the mirror config and the                                 crusty C consumer as I said doesn't use                                 solar resources you're not going to run                                 out of the trunk disk space because of                                 transaction log exploding or the leader                                 being bombarded with a ton of updates                                 will not translate into okay now I'm                                 kind of contending for resources where I                                 want to send this data across a city                                 using CDCR but at the same time also                                 processed them locally and make sure                                 that this data is up and running stable                                 on the primary PC to begin with and then                                 you could have poor visibility into                                 telemetry metrics if if you use a cue                                 that is managed by a third-party system                                 so to just wrap it up the feature the we                                 did have a discussion in the community                                 and talk about converging the solutions                                 so that we have fun end-to-end solution                                 we didn't get the bat we didn't have the                                 bandwidth to do that so far but the                                 plans to do that in the near future if                                 that's something that other folks in the                                 community are interested in so watching                                 for the zeros and if you're interested                                 in Crusty's DC replication and solar                                 please reach out and participate in the                                 community and yeah any self any form of                                 participation it doesn't have to be just                                 a code level participation but design                                 use is sharing your use cases challenges                                 that you face is as valuable as anything                                 else as the code or providing tests so                                 yeah participate that's about it thank                                 you so much yeah um thanks for the talk                                 there like a few questions on Channel                                 but then she take one question out of                                 those and then have the rest have you                                 answered the rest in the Gigi breakout                                 room up to the top so I'm gonna you like                                 answer it in the short version because                                 it seems like the question evolved                                 around the gia pointing to remove CDCR                                 from solar so if you have like answer to                                 why there's a Giro and like that and                                 like people have questions around that                                 we can answer that now and then you can                                 probably I can I control the I cannot                                 try and answer that in as few words as                                 possible for now and then we can and                                 elaborate in an offline discussion yeah                                 because of the challenges that I                                 mentioned that we realized that the                                 current CDCR                                 has it kind of seems like it needs to be                                 either removed or redone in either case                                 removed from its from the way it                                 currently stands I to to have something                                 that is usable in a practical and set up                                 in a practical environment so yeah                                 that's that's the reason why I said the                                 feature of this is for the community to                                 discuss and come up with a solution that                                 converges and actually solves the                                 problem rather than have a feature that                                 exists but comes with so many drawbacks                                 that it makes it impossible to use in in                                 the real world                                 you
YouTube URL: https://www.youtube.com/watch?v=tJLVN0pT0uc


