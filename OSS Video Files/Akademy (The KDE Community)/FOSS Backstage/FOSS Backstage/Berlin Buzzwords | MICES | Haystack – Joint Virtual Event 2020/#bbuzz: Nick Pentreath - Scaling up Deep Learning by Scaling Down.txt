Title: #bbuzz: Nick Pentreath - Scaling up Deep Learning by Scaling Down
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/scaling-deep-learning-scaling-down


In the last few years, deep learning has achieved dramatic success in a wide range of domains, including computer vision, artificial intelligence, speech recognition, natural language processing and reinforcement learning.

However, good performance comes at a significant computational cost. This makes scaling training expensive, but an even more pertinent issue is inference, in particular for real-time applications (where runtime latency is critical) and edge devices (where computational and storage resources may be limited).

This talk will explore common techniques and emerging advances for dealing with these challenges, including best practices for batching; quantization and other methods for trading off computational cost at training vs inference performance; architecture optimization and graph manipulation approaches.
Captions: 
	                              hello everyone and welcome to this                               berlin buzzwords                                                       deep learning by scaling down                               i'm nick pentrith i'm ml nick on twitter                               github linkedin                               i'm a principal engineer working at ibm                               center for open source data and ai                               technologies code                               where i work on machine learning and ai                                open source software                                i'm an apache spark committee and pmc                                member and author of machine learning                                with spark                                before we begin a little bit about code                                or the center for open source data and                                ai technologies                                we're a team of over                                               developers within ibm                                and we work on contributing to an                                advocating for                                open source projects that are                                foundational to ibm's data and ai                                product offerings                                this includes the python data science                                stack apache spark is a core component                                of this stack                                open exchanges for data and deep                                learning models                                deep learning frameworks including                                tensorflow and pytorch                                kubeflow ai fairness and ethics                                as well as open standards for model                                deployment                                today we'll start with a deep learning                                overview and discuss the computational                                challenges involved                                with training and deploying deep                                learning models we'll then look at three                                broad classes of approach                                for dealing with these challenges                                including model architectures model                                compression techniques                                and model distillation and then we'll                                wrap up with the conclusion                                we'll start with the basic machine                                learning workflow we start with data                                we analyze that data and we typically                                want to train a machine learning model                                using that data now                                our data typically doesn't arrive in a                                nicely packaged format ready for machine                                learning                                it arrives in a raw format and we need                                to convert it                                pre-process it and do feature                                transformation and extraction to get it                                into a form amenable to machine learning                                typically feature vectors and                                tensors we then train a machine learning                                model                                or deploy it to a live environment where                                it predicts                                on new data coming in and new data comes                                into the process                                really turning this workflow into a loop                                now as part of this workflow the three                                main areas that are                                compute intensive are processing data in                                particular training models                                and model deployment                                deep learning is a branch of machine                                learning that has been around for quite                                a number of years the original theory                                dates back to the                                      and some of the computer models                                originated around the middle of the                                                                                                      an old perceptron machine a neural                                network from the                                      neural networks fell out of favor during                                the                                            in part because of a lack of world                                success in applications                                and partly because of the inability to                                actually compute and train these models                                they've seen a recent resurgence gt                                 factors the first being bigger and                                better and more open data sets                                as mobile phones edge devices and                                internet scale data collection have                                really proliferated                                as well as standardized data sets for                                competition such as imagenet                                combined with this we've seen better                                hardware gpus and now                                custom hardware focused on deep learning                                applications such as tensor processing                                units tpus                                and the third leg is improvements to the                                 algorithms architectures and                                 optimization techniques and the software                                 side                                 and these combined have led to new                                 state-of-the-art results across                                 computer vision speech recognition                                 natural language processing language                                 translation                                 and many more modern neural networks                                 are really called deep learning because                                 they are                                 neural networks made up multiple layers                                 and in computer vision convolutional                                 neural networks are a core building                                 block for state-of-the-art models                                 and have been used in image                                 classification object detection                                 segmentation                                 and many other applications for                                 sequences and time                                 series applications such as machine                                 translation text generation                                 recurrent neural nets have been                                 extremely successful                                 and in other natural language processing                                 applications word embeddings                                 transformers and attention mechanisms                                 have seen success                                 and finally modern deep learning                                 frameworks provide computation graph                                 abstraction                                 automatic differentiation hardware                                 acceleration support                                 and high levels of flexibility allowing                                 practitioners and researchers                                 to create state-of-the-art models in a                                 much easier fashion than previously                                 instead of handcrafting solutions                                 in the previous era of ai and deep                                 learning applications                                 the compute required roughly followed a                                 two-year doubling time of moore's law                                 but in the modern era as we've seen an                                 explosion in                                 very complex and large solutions that                                 doubling time                                 is something like three to four months                                 and this really highlights that we need                                 software-based solutions we cannot just                                 simply rely                                 on improved hardware to really solve                                 these problems                                 so we'll use an example today throughout                                 and that is image classification which                                 is a very common                                 deep learning computer vision task                                 we start with an input image we send                                 that through our large neural network                                 for inference                                 and we get a prediction for the class                                 out                                 a very common and highly performance                                 modern deep learning neural network for                                 image classification is called inception                                 v                                  and we can see here that the core model                                 is made up of these                                 convolutional blocks typically a                                 convolution operator followed by                                 normalization                                 and then an activation function often                                 rectified linear unit or something like                                 that                                 and we can actually boil this                                 computation down to effectively a set of                                 matrix multiplier                                 and addition operations and if we look                                 at the entire network we can see we have                                 many many of these                                 blocks that are sitting in the network                                 and that                                 if we count up all the parameters this                                 inception model has                                            parameters and achieves a                                              accuracy on imagenet                                 if we look at the accuracy versus                                 computational complexity                                 of various networks the early models                                 tended to have                                 quite a lot of parameters not so much                                 operations                                 and achieve moderate performance we then                                 moved into                                 a new phase which was                                 increasing the number of parameters and                                 operational                                 computational complexity of these models                                 in order to move up the accuracy curve                                 and then the next phase was starting to                                 have more and more efficient                                 network architectures so trying to                                 achieve                                 high accuracy with relatively fewer                                 computations                                 another way of looking at this is to                                 look at the information density                                 or level of computational efficiency in                                 each of these networks                                 and we can see that the models that                                 achieve                                 a relatively high accuracy for                                 relatively low number of operations are                                 clearly the most efficient they have a                                 relatively high                                 density or accuracy percentage per                                 parameter effectively                                 but in absolute terms these are still                                 quite low numbers                                 maxing out just below                                                   telling us that these large                                 networks are in fact over parameterized                                 and not very efficient in terms of their                                 representation                                 when you think about deep learning                                 deployments typically on the model                                 training side                                 we use substantial hardware typically                                 gpu or multi-gpu or clusters of gpu                                 or other customer hardware or specific                                 hardware like tpus                                 and we can throw a lot of computes                                 obviously at a cost                                 at this training and when it comes to                                 cloud-based deployment scenarios or                                 on-premise                                 deployment scenarios we can use similar                                 hardware to deploy models                                 and we can then trade off cost versus                                 performance                                 if we want to use more gpus we can just                                 throw some more                                 potentially some more money at the                                 problem but when it comes to                                 deploying into edge devices we cannot                                 just simply do that                                 edge devices are very diverse and have                                 far more limited resources in general                                 than                                 available on cloud or on-premise                                 hardware                                 they have limited memory and that memory                                 is not all available to our application                                 we typically have to compete with other                                 applications on the device similarly                                 with compute there may be                                 limited computes there may be some                                 fairly powerful                                 mobile gpus or edge gpus but we still                                 are needing to                                 compete with other applications in terms                                 of a                                 computational resource and finally                                 network bandwidth can be extremely                                 limited                                 and variable so we cannot just deploy                                 large powerful models in order to get                                 accuracy in these                                 scenarios we need to be cognizant of                                 memory footprint we need to be cognizant                                 of                                 the compute efficiency and the latency                                 involved                                 and the latency requirements from our                                 users                                 and in fact getting the model onto the                                 device can be a problem due to bandwidth                                 so many of these considerations also                                 apply to low latency applications such                                 as                                 high frequency trading financial                                 applications                                 programmatic trading and advertising                                 where we have to make decisions                                 in very much real time                                 and that could be low single-digit                                 milliseconds even microsecond latency                                 requirements                                 so again we can't just have a huge model                                 that takes a long time to compute even                                 if it's highly accurate                                 so how do we improve the performance                                 efficiency in order to meet these                                 requirements                                 we'll discuss four approaches today the                                 first is just improving architectures                                 second is and third compression                                 techniques for models                                 model pruning and quantization and the                                 final one is model distillation                                 the first thing we can do is potentially                                 try to make networks more efficient in                                 the way that they are designed and                                 indeed this has been                                 a key focus of research recently so if                                 we look at the basic inception model                                 on the left we can see that as we saw                                 the standard convolutional building                                 blocks is what makes up this model                                 and on the right we have mobile net                                 version one which is                                 one of the more famous recent                                 specialized architectures targeting                                 edge devices and other low resource                                 environments                                 and the key difference here is that the                                 building block is no longer                                 a standard convolutional block but                                 instead a depth wise convolutional                                 building block                                 so this is effectively splitting the                                 convolutional operator into a depth-wise                                 convolution followed by                                 another convolution and this is leads to                                 about                                 eight times less computation taking                                 place with                                 giving up a little bit of accuracy so if                                 we saw that the image                                 inception model had                                            parameters with a                                      accuracy then the mobilenet model has                                 over                                                                    give up about eight percent                                 accuracy                                 another key advance in in this area of                                 research                                 is to set up the core backbone of the                                 network                                 in such a way that it can be scaled so                                 this allows us to scale                                 mobilenet for example to be thinner or                                 wider                                 so we can scale the number of parameters                                 at each layer and we can also scale the                                 resolution of the input image                                 representation and this allows us to                                 target                                 the environment that we want to so if we                                 have a an environment with plenty of                                 compute available                                 we can scale that up and get more                                 accuracy if we have a much more resource                                 constrained environment                                 we can scale it down and give up                                 accuracy but still be able to run                                 in that environment so this idea was                                 taken through to mobilenetv                                              improvement                                 it uses the same depth wise                                 convolutional backbone but add some                                 some further algorithmic and network                                 tricks linear bottlenecks and shortcut                                 connections                                 and effectively we're just trying to                                 move up the curve here more more to the                                 upper left                                 part of this this chart and again we can                                 scale to                                 to higher latency and higher accuracy or                                 lower latency and low accuracy                                 depending on the environment that we're                                 in and the requirements of the                                 application                                 so mobile mid v                                                       same number of parameters                                 for an extra couple of percent accuracy                                 of course measured on                                 the standard benchmark dataset of                                 imagenet                                 so you can see here that these model                                 classes                                 are trying to achieve a much higher                                 efficiency                                 so they may not be the best performers                                 but they are very strong performers with                                 very low parameters uh number of                                 parameters and computational overhead                                 and indeed we can see that the class of                                 mobile nets                                 as well as other efficient backbone                                 based architectures such as shuffle and                                 squeeze net                                 are much more efficient in terms of the                                 information density within the                                 parameters                                 still these numbers are around the                                                                                                          still work that can be done                                 another recent advance which has become                                 more and more popular is the use of                                 neural architecture research                                 to find these backbone models so the                                 idea here is to effectively let                                 deep learning do some of the work and to                                 search the space of available                                 network architectures in order to find                                 the the best backbone architecture that                                 can then be scaled up and down                                 and this is done by optimizing both for                                 accuracy as well as efficiency in the                                 term of                                 in the form of floating point operations                                 efficientnet is one                                 recent example of this and we can                                 effectively                                 outperform previous models and scale it                                 from the b                                        which has a few more parameters than a                                 mobile net for example but achieves                                 about five percent more accuracy                                 and we can scale that up all the way                                 through to the b                                  which is effectively still the same                                 architecture just much bigger                                 and deeper and that goes up to                                    million parameters and gives us a boost                                 of                                 up to                                               the same idea has been applied to the                                 mobile net architecture for example so                                 again applying a neural architecture                                 search paradigm                                 that is hardware aware so targeting both                                 the                                 performance and the efficiency                                 and here we can get a network with about                                 five million parameters                                 and sort of three roughly three percent                                 increase                                 in accuracy versus the old mobile nets                                 now a key challenge with neural                                 architecture search is that it requires                                 a huge amount of computational resources                                 in order to find the best architecture                                 and in fact to train                                 each different architectural sub                                 architecture                                 now this is also the case for                                 manual design if you're trying to                                 manually design these networks and these                                 backbone networks                                 there's a lot of effort involved a lot                                 of testing a lot of training                                 and a lot of experimentation so                                 the question asked by some researchers                                 at mit in the                                 ibm mit data lab is can                                 we train one network to do all of this                                 for us and this is the idea behind once                                 for all train one network and specialize                                 it for efficient deployment                                 so the idea is to train one large                                 network once                                 and then be able to effectively cherry                                 pick out the sub-network                                 targeting a specific environment or each                                 device                                 or operating system or hardware                                 accelerator for example                                 and in this way we have a much more                                 efficient mechanism for                                 achieving the same result and still                                 being able to                                 target each one of these environments                                 and trading off                                 the efficiency versus and the                                 computational                                 considerations versus the accuracy but                                 still achieve state of the art results                                 so the second approach general approach                                 that we'll look at                                 is that of trying to compress the model                                 we've seen effectively that                                 many of these models and especially the                                 large networks are                                 effectively over parameterized a lot of                                 the                                 weights in there are maybe not that                                 important so                                 we have to ask the question can we                                 perhaps take some of these weights out                                 of the model                                 but still have the model perform pretty                                 well and this is the idea behind                                 model pruning the idea is to reduce the                                 number of model parameters by removing                                 the ones that are not important                                 in fact in other words the ones that are                                 have a small impact on prediction                                 this is very similar to the idea of                                 regularization with the l                                       we want to shrink down small weights                                 that have little impact on the                                 prediction                                 and set them to zero if we can set                                 weights to zero                                 then we can effectively ignore them we                                 can ignore them when we save the model                                 so that gives us a much smaller size on                                 disk and across the wire for                                 sending the model back and forth so we                                 compress it in that form                                 and it can also give us lower latency if                                 we                                 can do the compute in a sparsity aware                                 manner                                 so there are two broad types of pruning                                 the first is one shot pruning which is                                 effectively                                 doing this process post training so you                                 run through the network once and you                                 prune the weights                                 and hopefully at the end you get a more                                 efficient model that can still perform                                 reasonably well and the second is                                 iterative pruning                                 and the idea here is to prune and then                                 retrain and this follows a schedule                                 an iterative approach so step one is to                                 do the pruning drop the least important                                 weights                                 and then retrain and                                 then go through and drop the the least                                 important weights and retrain and so on                                 and so on                                 and the idea would be to target a                                 specific sparsity that you want to                                 achieve                                 or a computational budget that you want                                 to be able to fit into                                 and stop the pruning when either of                                 those or when one of those                                 conditions is met                                 now what's quite interesting is that we                                 can actually                                 achieve quite a high level of sparsity                                 via model pruning                                 without giving up much so this is for                                 imagenet based image classification                                 models                                 from the tensorflow model optimization                                 library docs                                 and as you can see here in an inception                                 model                                 in fact both for inception and mobilenet                                 we can achieve a                                    sparsity by uh while giving up a very                                 very small amount of accuracy                                 so that really is showing us that these                                 models are over parameterized and                                 there's a lot of effectively superfluous                                 information in their superfluous weights                                 now if we want to to further trade off                                 accuracy and get a much smaller model                                 then we can make models more and more as                                 fast do more and more pruning                                 and move down this curve to the right                                 getting a smaller model but at that                                 point starting to give up                                 accuracy and this doesn't just work for                                 image classification                                 here's an example from language                                 translation as you can see here we can                                 in fact achieve a very high level of                                 sparsity while in fact                                 getting a very small gain in our model                                 performance                                 and thereafter again we're trading off                                 performance for                                 for model size so this is indicating                                 that                                 model pruning is playing almost a                                 regularization role                                 and again these large models for                                 natural language processing tasks are                                 also very much over parameterized                                 the next model compression technique                                 we'll discuss is called quantization                                 the idea behind quantization is that                                 most deep learning computation uses                                                                                                      numbers                                 and quantization reduces the numerical                                 precision of the weights and the                                 operator                                 operations on that weight by binning                                 values                                 so if we start with a                                                 point                                 representation we want to effectively                                 turn that into a much sparser                                 representation                                 takes up a lot less size and if you can                                 go down from                                         to say                                                        effectively                                 two times a saving on the size                                 so the idea of behind binning is that if                                 we look at the distribution of the                                 weight values we can effectively try and                                 approximate those weight values by a                                 much smaller number of pins                                 now the main complexity here                                 is in dealing with overflow so as we                                 turn the computation and the                                 weight representation to smaller                                 precision representations in particular                                 when accumulating                                 things like gradients during training                                 these can be very very small numbers and                                 small updates                                 so we risk overflowing the                                 representation so the                                 the the key solution here is to use some                                 sort of intermediate                                 representation for storage of things                                 such as gradient updates                                 and accumulations so we can use a larger                                 size for example                                                                                                       during that process                                 and then re-quantize it at the end to                                 eight bits or throw it away if we don't                                 need it                                 for example after training so popular                                 targets for quantization are                                        floating point                                 and eight-bit integer coding                                 there are two types of quantization you                                 can do this post training                                 very much similar to model pruning where                                 you take a pre-trained model                                 and you just apply a quantization for                                 effectively for influence only                                 so this is useful if you can't retrain                                 the model for whatever reason                                 or it's not computationally feasible                                 perhaps to retrain the model                                 seeing as it's a very large model                                 model and very complex and expensive to                                 train                                 so in this case you typically would give                                 up accuracy                                 and most of the targets are typically                                 going to be float                                                                                                          quantization or                                               the second approach is to do it in a                                 training aware manner                                 so this is much more complex and for                                 example as we mentioned you have to deal                                 with                                 overflows potential overflows but this                                 can                                 really provide a huge efficiency gain                                 with effectively                                 minimal or close to zero loss in                                 accuracy so here on the right we see                                 an example of this for image                                 classification models again from the                                 tensorflow                                 model optimization documentation                                 and for the inception model we can see                                 that in fact both                                 post training and training aware                                 quantization give a pretty similar                                 accuracy degradation whereas for                                 mobilenet the post training quantization                                 results in a much                                 larger degradation and accuracy but if                                 we do training aware quantization we                                 give up very little so this again is                                 showing us that very large models very                                 complex models like conception                                 are effectively somewhat over                                 parametrized they've got a lot of                                 superfluous information in there                                 because if we are reducing the precision                                 and effectively                                 increasing the level of approximation in                                 the weights and the operations                                 it's actually not having much impact for                                 a much more efficient and sort of leaner                                 model like a mobile net                                 they are much more informationally dense                                 so adding                                 further levels of approximation                                 effectively                                 into the weights and operations is going                                 to have more impact                                 we can see here the impact on latency                                 and model size                                 so in some cases post-training                                 quantization might even might in fact                                 increase latency as we see here for                                 mobilenet                                 but in in both cases we get a                                 significant                                    reduction in the model size and for very                                 large models typically                                 you'll get a decrease in latency for                                 training aware again                                 we get very large efficiency gains and                                 forward for giving up very little in                                 terms of accuracy                                 so quantization used to be a very tricky                                 thing to do                                 involving very large amounts of custom                                 code                                 but now it's much much easier it's baked                                 into tensorflow and pi torch itself                                 with and as well as third-party toolkits                                 such as distiller for pytorch                                 and it's much much easier to use and you                                 can just run the optimization code                                 on your model or train using training                                 aware quantization                                 the final technique we'll discuss                                 briefly is called model distillation                                 and as we've seen large models may be                                 quite over parametrized and relatively                                 inefficient in their representations                                 so the idea here is can we take such a                                 large complex model and use it as a                                 teacher                                 to to effectively teach a smaller                                 simpler student model                                 so we want to distill down the core                                 knowledge of that large model                                 or potentially large set of models in an                                 ensemble                                 and see if we can represent that                                 knowledge by a much                                 simpler student model so we see here                                 a representation of this where we have a                                 teacher model typically with a lot                                 more layers or more complexity or depth                                 than a student model                                 and we want to transfer that knowledge                                 to the student model so we're training                                 the student model on                                 the teacher model's predictions there                                 are a number of tricks                                 involved here but effectively                                 what we see at the bottom right is                                 results from the original jeff hinton                                 paper on model distillation                                 and we can see that a distilled single                                 model can actually outperform the                                 baseline                                 model and architecture are used                                 and gets very close to the                                          ensemble which is actually used as the                                 teacher model                                 model distillation is lesser used in                                 image classification                                 this day i mean certainly around but but                                 probably less                                 popular but it's been quite successful                                 in                                 natural language natural language                                 processing in particular                                 for recent transformer based models such                                 as birds                                 some distal models such as distilberts                                 and tiny bird for example                                 have achieved very very small model                                 sizes and                                 much more efficient models for                                 either giving up very little in terms of                                 accuracy or in some cases even                                 outperforming                                 some of the some of the previous                                 state-of-the-art larger bird models                                 so again this is really indicating that                                 there's a lot of                                 inefficiency in the model architectures                                 and these large                                 image classification models as well as                                 large language models                                 are really over parameterizing                                 and there's a lot more that can be done                                 to improve these architectures and                                 that'll be                                 definitely something of a focus for                                 research going forward                                 so we've seen that this space is rapidly                                 evolving                                 it's an area of very very very rapid                                 development in research                                 and new efficient model architectures                                 are always being released                                 especially for targeting edge devices                                 and                                 no resource deployment targets so if one                                 fits your need if you are doing image                                 based models computer vision and                                 is one available that's pre-trained                                 great you can use that                                 in many cases your particular use case                                 may not actually fit                                 well with something that is out there                                 and been made                                 public and released so in that case you                                 can look at these compression techniques                                 pruning and quantization and as you've                                 seen they can yield very large                                 efficiency gains                                 and without giving up necessarily too                                 much in terms of accuracy but                                 you typically can you have a lot of                                 tweaks and tools                                 and knobs that you can turn to trade off                                 how much accuracy you're willing to give                                 up versus your computational budget                                 and your computational resources of your                                 target                                 it's very easy to do this with a very                                 strong built-in support in the deep                                 learning frameworks these days                                 one could even look at combining pruning                                 and quantization that is obviously a lot                                 trickier                                 which you do first probably you prune                                 and then quantize and so on                                 um this is it's not clear exactly                                 the best way to go about doing this so                                 that'll be an                                 area of much more kind of                                 experimentation to figure out what works                                 well                                 model distillation is generally less                                 popular it's a lot more complex to do                                 and it does require training and                                 training effectively                                 more models so again more                                 computationally intensive at training                                 time                                 but it can be very you know potentially                                 very compelling for natural language                                 processing tasks especially where                                 there's an existing model that's been                                 released                                 like a distal bit                                 thank you very very much for joining me                                 today i encourage you to check out                                 code.org which lists all the open source                                 projects within the data and ai space                                 that we work on                                 at code you can find me on twitter and                                 github at ml nick                                 and i also encourage you to check out                                 the model asset exchange which is a free                                 and open                                 exchange for deep learning models                                 state-of-the-art models where we have a                                 number of                                 different image classification models as                                 examples and you can try out                                 both the large architectures and the                                 small                                 mobile and edge device focused                                 architectures for example for image                                 classification                                 and segmentation i've also                                 left a set of references in the slides                                 for some of the                                 topics that we've discussed today thank                                 you very much                                 uh thank you again for the talk um that                                 was a bit kind of technical difficulties                                 so there is one question that the person                                 asking on slack                                 why do you think it's uh pruning and                                 quantization is tricky                                 uh yeah yeah so firstly thanks to                                 everyone for                                 for listening and your questions                                 pruning and quantization is a bit tricky                                 to do um                                 together just because it's it's kind of                                 not clear                                 what is the principled approach to take                                 um so typically                                 you know so for example if you um                                 do you do pruning first and then                                 quantize or quantizing and then pruning                                 so if you quantize the the all the                                 weights                                 um the pruning mechanism may not work                                 quite as you expect and you you may get                                 you know an end result which is                                 degrading too much                                 um accuracy um and                                 and but but i think that if you do it                                 the other way around                                 it's going to be probably a little bit                                 better in other words specify the model                                 first                                 and then once you're happy with that you                                 can quantize and                                 and and get get the uh                                 the slightly you know compressed model                                 without losing too much performance                                 but either way it's not a um                                 there's no kind of                                 good formula for saying exactly how you                                 should do it                                 both of them would probably work there's                                 a couple of papers                                 out there where they actually use all of                                 these techniques together so                                 quantization pruning                                 but coming up with a kind of scheme for                                 doing it                                 is is not                                 there's no kind of accepted one one way                                 to do it i guess                                 cool awesome um there was another                                 question from nicola                                 uh is there like any good implementation                                 of nas and aas                                 or capital uh sure neural architecture                                 search is                                 generally um                                 kind of auto auto ml or auto ai for deep                                 learning                                 there's there's a lot of stuff out there                                 um                                 i haven't i've put i haven't put any any                                 direct links to that in the presentation                                 but                                 um in most of the                                 the main uh kind of research labs                                 i mean ibm research included uh google                                 you know facebook and everyone have it's                                 got quite a lot of stuff around                                 the way that they do neural architecture                                 search                                 google has published some of the details                                 about how they                                 generate things like the efficient nets                                 and the mobile nets v                                            so those are all actually published                                 papers and and if you go if you follow                                 the links                                 um in in the                                 presentation which which i think will be                                 posted for for the google blog post for                                 example for efficient mobile net                                 they'll they have links to the papers                                 there that have um internal                                 details about how they do the                                 architecture search                                 um similarly i actually posted the                                 once for all model in the slack channel                                 and in that github repo at the bottom                                 actually                                 you'll find a bunch of related work                                 which includes                                 automl for architecting efficient and                                 specialized neural networks                                 and also ml for model compression and                                 acceleration on mobile devices                                 architecture search on target tasks and                                 hardware so there's a few                                 there's a bit of detail there about                                 some of the specifically hardware when                                 your architecture search and then                                 finally the the cloud                                 providers again including ibm google                                 aws and everybody have their own                                 versions of auto                                 ai and kind of neural architecture                                 search                                 as part of their cloud machine learning                                 platforms                                 cool awesome and i guess more in general                                 also narrow architecture search is also                                 like slightly different direction right                                 because i mean the distillation would be                                 like very similar                                 once you already have modeling trying to                                 scale it up otherwise it's just like                                 trying to solve problems in general                                 right so slightly                                 different direction yeah yeah yeah                                 correct                                 yeah awesome um i guess uh people are                                 typing so let's see if they're gonna be                                  fast enough to type the question do you                                  like use it also like in production for                                  your i don't know your own project so                                  it's like mostly like research and uh                                  you know investigation like what is                                  possible what is the usual like life                                  cycle                                  of you uh starting with a model and                                  after bringing it to                                  like distilled version and more                                  inference how long it takes for you                                  usually                                  uh i mean most of the the work that we                                  do                                  uh in our group is is really related to                                  open source projects                                  so one of the one of the                                  projects that we work on that i                                  mentioned is the model asset exchange                                  within our group which is a free and                                  open source resource for deep learning                                  models                                  and for some of the image models                                  we are working with this                                  so all those models are completely free                                  and we typically                                  take take the state-of-the-art model for                                  object detection or                                  you know image segmentation or name                                  density recognition whatever the case                                  may be                                  and we package it up in a docker                                  container exposing a standardized rest                                  api and that's all kind of completely                                  free and open                                  we work with a few internal groups                                  we sometimes approach us looking for                                  open source solutions for their                                  customers for example um                                  and one such group has been the uh                                  working on edge device deployment so                                  like arm architectures and                                  uh and similar so there you know you                                  have this                                  this problem where you your default                                  model might be the large                                  you know very accurate model for um or                                  image classification but                                  but they want a you know highly targeted                                  mobile network so                                  we allow them to kind of just plug                                  different models into the docker                                  container when they build it for                                  different architectures                                  as of yet we haven't directly applied                                  the quantization side                                  but if they you know if for example in                                  this particular use case it turns out                                  that they need a lot more efficiency                                  they may come back to us and say can we                                  actually                                  train a customer model with quantization                                  and real either kind of advisory                                  capacity so                                  most of this is um is you know                                  some of it's applied mostly in in                                  selecting the different                                  architectures and the the best kind of                                  uh                                  target model for each target platform uh                                  for quantization and model pruning it's                                  it's mostly what's available in the                                  in the deep learning libraries and that                                  but we we're not using it directly in                                  our projects at the moment                                  okay awesome so another maybe one of the                                  last questions from peter                                  uh what kind of metrics you're looking                                  during the optimization                                  while scaling down besides size accuracy                                  and required operations right                                  because uh i guess if i extend this                                  question um you can do pruning right but                                  it only works like if you're                                  um you know like edge device or whatever                                  like a cpu                                  supporting like sparse operations right                                  so like do you also sometimes like                                  looking more                                  what is a specific uh you know executor                                  are going to be running that or is like                                  any technique actually to get it closer                                  and what other metrics are helpful for                                  that                                  yeah i mean for the most part you do                                  need to be aware of what your                                  what the capabilities of your execution                                  environment on the software side so yeah                                  you're right                                  there's no point in pruning if you can't                                  take advantage of the sparsity                                  um so most of the time you're going to                                  be targeting uh                                  you know something like tf light or                                  you know or something so it depends on                                  if you're going to an edge device or                                  mobile device                                  and you only work in tensorflow                                  obviously it depends um                                  and suddenly you know javascript if                                  you're looking up that maybe maybe the                                  browser or mobile browser                                  onyx.js or whatever you want to be using                                  um                                  so that's a part of it but i mean                                  generally you'd                                  look at not just size on disk for memory                                  but also                                  memory you know the the memory footprint                                  of the running program                                  um and that's often quite different and                                  and sometimes a bit unexpected so a lot                                  of the time                                  um you might find that the                                  network on disk is a certain size but                                  when but it kind of explodes memory when                                  you when you're loading it                                  typically that's that's going to be some                                  inefficiency or bug somewhere                                  and yeah we actually come across an                                  issue like that and a project we're                                  working on right now it's you've got                                  something                                                                                                       exposed to                                  whatever gigabytes in memory so you know                                  things like that obviously you need to                                  be checked it's not enough to just say                                  oh i've got a small                                  serialized model it must be good but                                  apart from that pretty much yeah                                  size accuracy and you know                                  or some other metric of uh compute                                  resources                                  is pretty much what you look at                                  um federating learning setup                                  uh yeah i mean that's a good point i                                  mean i                                  i haven't worked directly on federated                                  learning at this point                                  [Music]                                  and that's mostly something where you                                  know                                  companies or organizations that are that                                  are kind of have access to                                  a network of of edge devices or you know                                  user devices where there's there's                                  user data and there's a cape on the                                  device and this                                  capacity for federated learning we're                                  not really working on stuff like                                  that um i think they you know                                  effectively                                  the considerations are similar                                  you you have to then take into account                                  not just training but inference                                  because typically you want to do a                                  gradient update                                  on the on the mobile device and then                                  send it back for federated learning                                  the reality is the the compute for                                  for computing a gradient step versus                                  uh versus just inference is not that                                  much extra event so                                  but you you would have to definitely                                  take it into account                                  um as well as the fact that you don't                                  have access necessarily to all of the                                  cpu                                  or gpu you know for doing that so that's                                  more of a                                  challenge of i don't think it's so much                                  an architectural                                  and network architecture challenges as                                  just the software challenge by saying                                  schedule the update for a dance you know                                  a time where the                                  the device is not busy so yeah                                  completely makes sense maybe last                                  question for me is you mentioned like                                  memory footprint right and how things                                  are kind of like exploding in memory                                  um and i've seen also some groups also                                  working on the                                  having some ml specific like compiler                                  let's say google with                                  mlar right and you know when you confuse                                  operations like while                                  actually compiling that um how do you                                  feel about that right do you think it's                                  something that like in i don't know a                                  couple years is gonna be                                  replacing all of those uh model                                  converters and like pruning and                                  everything else or this kind of uh                                  yet like a bit far-fetched and we're not                                  gonna see first result pretty soon                                  uh yeah interesting question i mean i i                                  think i think we're gonna see a lot                                  happening there um                                  yeah the the with compilers it's always                                  there's always going to be edge cases um                                  so i don't think it's ever going to be                                  perfect                                  but i think you can achieve a lot and                                  it's certainly very clear that there's                                  already and if you just just training a                                  model in you know tensorflow and keras                                  and all the kind of                                  there's a lot of magic happening with                                  graph construction and you end up i                                  think with a lot of time                                  something quite inefficient so being                                  able to com you know                                  to run a kind of compiler type of                                  process over that um or at least even                                  just                                  doing pruning not from a from the weight                                  perspective but just from a graph                                  operator                                  perspective and doing post optimization                                  is important so yeah i mean uh                                  i don't think it's going to be a silver                                  bullet like it's going to                                  solve every problem but certainly i                                  think that's definitely going to be                                  there's going to be a lot of progress                                  there and i mean if you look at                                  the the new the new stuff coming out                                  like uh uh                                  uh what is it the area have you                                  obviously got the xla stuff                                  and um                                  new frameworks which are effectively                                  trying to more and more move to                                  intermediate representation                                  forms yeah um yeah so i mean i think we                                  will                                  see that yeah awesome thank you again                                  for your talk and uh for                                  awesome uh answers for the questions um                                  i guess we're gonna continue like in                                  breakout room                                  and uh for the live stream we are done                                  thank you again see ya                                  thank you everyone                                  you
YouTube URL: https://www.youtube.com/watch?v=irClz_qZOL4


