Title: #Haystack: Tim Allison - Evolving Relevance
Publication date: 2020-07-02
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/evolving-relevance

This talk builds on work by Simon Hughes and others to apply genetic algorithms (GA) and random search for finding optimal parameters for relevance ranking. While manual tuning can be useful, the parameter space is too vast to be confident that one has found optimal parameters without overfitting.We'll present Quaerite (https://github.com/tballison/quaerite), an open source toolkit that allows users to specify experiment parameters and then run a random search and/or a GA to identify the best settings given ground truth.We'll offer an overview of mapping parameter space to a GA problem in both Solr and Elasticsearch, the importance of the baked-in n-fold cross-validation, and the surprises and successes found with deployed search systems.
Captions: 
	                              and welcome back to our haystack TV                               stream today                               so our second talk is from Tim Allison                               tim has been working in natural language                               processing since                                                     years his focus has shifted to around                               content extraction Advanced Search and                               relevance tuning Tim Tim is currently                               working at the Jet Propulsion Laboratory                                at the California Institute of                                Technology and he is also a member of                                the Apache Software Foundation the chair                                of hacci tika commuter and apache open                                LLP Apache Solr Apache PDF box and                                Apache Pio I and in the former life he                                was a professor of Latin and Greek so I                                can think we can we can safely say he he                                knows a lot about stuff and without any                                further ado I'm going to introduce Tim                                Tims gonna be talking today about                                genetic algorithms and random search for                                finding optimal parameters for relevance                                ranking Tim Creighton thank you so much                                Shirley and thank you to the organizers                                of Berlin buzzwords and haystack and my                                seas for bringing off a fantastic set of                                talks and colloquiums                                I've had a lot of fun so thank you                                alright so I'm going to talk a little                                bit first a little bit about me                                Charlie's gone over some of the details                                some of you may have seen some of my                                work on tika so the other important                                point about this talk before I really                                get going is that some of this work                                initially as with all evolving things                                evolved from earlier work at my former                                employer than later corporation so some                                of these slides are under copyright with                                them so I will be talking today about                                teka teka and more teeka I'm joking but                                seriously that's not why we're here                                today we are here today instead to talk                                about genetic algorithms and evolving                                ways of improving search so I'll start                                off with the introduction and motivation                                for what I from my evolution I'll talk                                about some of the methods that I                                developed a handle and with some                                temperance and some findings                                of recent findings alright so first I                                had tipped to Simon Hughes who spoke                                about evolving relevancy back in                                     before him others had tried had also                                experimented with this it's a fun area                                to work I will say that I did not get                                involved with this particular set of                                algorithms because I thought it was                                necessarily the best tool to use but                                rather it's something that I kind of                                evolved into and it just seems like an                                easy next step to take so I'll share                                some of some of the work on that today                                and off we go alright so we now know                                there is a a chorus of open source                                relevance tools as was announced                                yesterday two years ago so that's the                                combination of Cupid re quirky and Samui                                that is all a suite of really useful                                tools I'm I was really gratified to see                                that they're placed into a single                                framework to some degree so that you can                                have a nice toolkit for a number of                                different areas the reason that I                                started working with quiet air building                                it initially was before those had all                                been put together and also I felt that                                there wasn't quite something out there                                that allowed me to do what I wanted to                                and I'll talk about some of the                                differences between what my little tool                                can do and what some of the other tools                                can do and hopefully once my stuff                                matures more there could be room for it                                maybe of course we will see but I do                                look forward to collaborating whether                                that's simply transferring our ideas                                knowledge findings and so on whether                                it's actual code or not not not a big                                concern for me all right so why even                                bother with this first off of course as                                we all know search is easy it's solved                                so we shouldn't be here so that after I                                hear from around the world even though                                everybody nobody's nobody can speak in                                this channel of course search is not                                easy and we all know that Google makes                                it look easy so we have the uphill                                battle of trying to explain to people                                 why it's not especially on site search                                 and intranet search Thank You charlie                                 for this                                 beautiful diagram for folks in this                                 audience you already know that the                                 components of a search system search is                                 not easy there are lots of different                                 areas for tuning and lots of various                                 areas for things to go wrong so trying                                 to figure out where to focus ones energy                                 trying to explore the parameter space to                                 understand how your components are                                 working together to yield high-quality                                 results can be challenging especially                                 when the parameter space is so big                                 speaking of which as of this is somewhat                                 dated but I don't think it's changed too                                 much something this is these are some                                 available parameters just within a kind                                 of conventional solar I say conventional                                 to differentiate from Trey's talk and                                 all of the topics he was talking about                                 which are fantastic on the thought                                 factors in the dense vectors and all of                                 that fun stuff and the knowledge graphs                                 before you even get to that with Ian's                                 solar and within elasticsearch you have                                 all the different tokenizer you can use                                 you have all the different token filters                                 and as we all know you can stack them on                                 top of each other order matters you have                                 different query parsers you can boost                                 all sorts of different things you can do                                 phrasal boosting and shingling you can                                 do minimum should match you can do token                                 or field based scoring best fields most                                 fields you can synonym list taxonomy z'                                 and thanks shout out to Erica for an                                 awesome talk on that and some of the                                 complexities with those obviously you                                 can also now manipulate the scoring                                 parameters and be m                                             ecosystem elevate file for solar                                 alternate methods to do that in                                 elasticsearch external signal enrichment                                 so lots of different things to do that                                 including analysis of Cori logs all                                 sorts of other things there and then of                                 course it finally at the bottom I get                                 around to rewriting via machine learning                                 whether that's learning to rank or other                                 things and of course as tre pointed out                                 in this talk just earlier there are all                                 sorts of other ways of measuring                                 similarity including with inspectors so                                 it's the the number of parameters we                                 have to play with is rather astounding                                 and let's not forget I love this one                                 that each token filter can have its own                                 a whole bunch of parameters so the the                                 the parameter space is just enormous as                                 we all know so what to do what to do                                 for this conference I don't need to                                 mention go out and buy that book if you                                 haven't already you'll get two copies                                 it's a really really nice way of                                 thinking about how to craft the relevant                                 signal from with various techniques and                                 how to layer different types of                                 relevance to achieve what you're trying                                 to do one of my takeaways in that book                                 and it was absolutely foundational to my                                 work with relevance and relevance tuning                                 but one thing I found didn't quite work                                 for me in the context I was working in                                 is that the one-off queries or the                                 optimizing for individual queries here                                 and there didn't have the systemic                                 approach that I wanted for some of the                                 clients I was working with at the time                                 and now so that book has really not in                                 my opinion has really nice set of basics                                 and wit tools and ways of training and                                 in tuning systems but I wanted to go a                                 little bit beyond that into seeing how                                 we can make changes that that have an                                 effect across a broader swath of queries                                 so the other thing yeah and also thank                                 you Doug and John for permission to use                                 the search engineer image in this talk                                 all right so what I'll be focusing on                                 now obviously there are in my mind there                                 are about three it was three kind of                                 high level categories for evaluating how                                 well search system is doing there's a                                 user feedback and hug flavors of that                                 then there are how people are actually                                 using your system I'm doing                                 click-through analysis and conversion                                 all of that stuff and then the third is                                 when you have ground truth based                                 relevance tuning for this audience I                                 don't have to talk about how important                                 it is to have good ground truth you all                                 know that people are given talks on it                                 it's very hard to do good ground truth                                 and also I very much appreciated jet                                 Rome and Byron's talk on the importance                                 of measurements and                                 walking into organizations that had no                                 measurement and complained that the                                 search is awful and then you know you do                                 have to start building up the sense that                                 you have to be able to measure against                                 something but I sing to the quote to the                                 choir here so I'll stop with that if you                                 do want a good laugh do check out the                                 tanks link and problems with overfitting                                 especially when ground truth is                                 expensive overfitting is a very serious                                 risk in that you can appear to get                                 better performance numbers but then in                                 practice performance I get worse or                                 certainly won't see the benefits that                                 you would expect to see based on your                                 experiments if you don't have a careful                                 separation of train and test all right                                 so ground truth we all know what this                                 looks like to some degree and so you                                 know you have some kind of relevance                                 ranking a doc ID and query for choir                                 attack and I should step back acquire                                 'te is Latin it means search as in seek                                 and you shall find the truth it seemed a                                 thorough imperative so it's the notion                                 is search find those but not actual                                 search search and search in the                                 parameter space for what sets of                                 parameters what combinations of                                 technologies can help lead to improved                                 results so the workflow at this point is                                 hacky it starts off with a command line                                 and a ground truth file which I already                                 explained experiments which I'll talk                                 about shortly                                 pumps those into a little h                                           and then launches those queries against                                 those queries in those experiments                                 against solar or elasticsearch then                                 outputs a directory of reports so yes                                 that is my user interface just a series                                 CSV close I'm sorry that's where I am if                                 anybody wants to contribute or if                                 anybody wants to help you to create this                                 into another project do let me know                                 front ends are not my thing all right so                                 some of the components the the main part                                 of of this is in core obviously CLI is                                 where the experiments in the command                                 line code is the thing that people might                                 find useful if we've all done this                                 before is have a unified interface to                                 talk to elastic or solar for at least                                 those areas where they agree which is a                                 whole bunch                                 so that I can say you know give me a                                 connector to solar or elastic the                                 connector figures out what version it is                                 applies the right connector so that you                                 can send documents for indexing or you                                 can do querying and other things across                                 those different systems without having                                 to do specific stuff and change the                                 dependencies so that's been one                                 component that might be useful for                                 others that I think is kind of nice                                 because you can copy stuff from                                 elasticsearch to index to an elastic                                 search seven index without using the                                 actual elastic client which is based on                                 one of those versions or you can go from                                 solar to elastic easily so that's us                                 kind of side benefit of this this                                 project I would also encourage people to                                 check out the examples directory and the                                 examples read me on github page that                                 walks you through how to run experiments                                 and so on so that's kind of structurally                                 where it is there I put in a placeholder                                 for logs I don't have any log analysis                                 now but I'm hoping to build that out                                 hopefully with others all right so what                                 I found myself doing and why I came to                                 this is that I had some ground truth                                 which was nice to have for this client I                                 forget about                                                    judgments for documents and I had all of                                 solar available to me so I started you                                 know playing with I'm changing the                                 various features and seeing if I could                                 bump the scores from their baseline but                                 what I found is I was twirling with                                 things is that I wasn't keeping track of                                 what I was doing I wasn't doing things                                 in an assistant systematic way and the                                 output wasn't standardized I was focused                                 on one on evaluation metric and I wanted                                 more diversity in those so this is                                 basically boredom and a need for                                 reproducibility and consistency is what                                 got me to where I am here so the key                                 components for running experiments in                                 clarity are scores I don't so you have                                 an array of different types of scoring                                 that you can do and then a map of                                 experiments I'll go into each of those                                 in detail now so the scores of                                 implemented and end ECG a number of                                 other things that various clients found                                 useful one of which I call at least                                 wanted and how                                 means that you got at least one of the                                 target talking that's at                                                 or at                                                                 the other traditional precision recall                                 and so on and then there are also you                                 can specify experiments where you have                                 which server you're hitting and what                                 kind of query how you want to shape that                                 query so this is kind of a high-level                                 view of what that experiment JSON file                                 looks like yeah so you can have at least                                 one a                                                                   you'll get all of those results I so I                                 have some really rudimentary things like                                 number zero results total Docs returned                                 and so on so you've got a pretty broad                                 sense of what how you're performing and                                 you can see all of these scores in for                                 each experiment with only one run all                                 right                                 so in slightly more detail experiments                                 this is as you can tell a solar query so                                 that this experiment says boost title                                 ten boost has to give me a tie of zero                                 point eight and so on                                 I also want to do some phrasal boosting                                 so this you cannot you what's nice is                                 that you can specify this you have the                                 chase on around so that you can tie                                 specifics Paris to specific outputs and                                 in the forward actually make progress                                 the output is pretty straight forward                                 those excel files this is one of them                                 it's just the per query per experiment                                 score so here we have a query of contact                                 the experiment is called text and and we                                 have how many documents had at least one                                 at least one it one normalized this kind                                 of King would have gained for that query                                 in that experiment total Doc's returned                                 and so on the other thing I do is then                                 do roll-ups of those statistics so for                                 the experiment text minimum match to how                                 many had at least one it at one what was                                 the mean normalize discounted chemical                                 game what was the median was a standard                                 deviation and I one of the things I'd                                 like to share is that what I don't often                                 see in talks and I know people are doing                                 this on the inside but please let's talk                                 about more as a community it is is                                 looking at the standard deviations                                 looking at the beans because sometimes                                 those are kind of crazy and if you don't                                 apply basic                                 significance tests and other things it's                                 not clear that you're actually seeing an                                 improvement and I recognize the                                 difference between significance and                                 great statistical significance and                                 operational significance and I also                                 recognize that statistical significance                                 is maybe not as useful as some want it                                 to be or as we all used to think it was                                 but still having some metric to                                 understand how big those differences are                                 is is really critical and I'd like to                                 try to share that and get that out to                                 the degree that it's not to them to the                                 degree that it needs to be lots of folks                                 as I said are already well ahead of me                                 on these topics but then the third type                                 of output is any pair wise p-value                                 differences between the experiments so                                 this says that you know the experiment                                 text minimum should match to the p-value                                 difference between that and text stem is                                                                                                         significance difference the testicle a                                 significant difference between those and                                 you can see that as you move down the                                 statistical significance goes up but                                 it's still nothing in this little ground                                 truths at this little place at that                                 worked with alright so that was                                 generation zero where I wanted                                 replicability I wanted automaticity for                                 those experiments I just wanted to be                                 able to run stuff get the results back                                 in a standardized format but then I                                 thought why am I sitting here generating                                 those experiments when we have all of                                 those features I'm basically kind of                                 lazy as we all as all coders are and why                                 can't we just get the computer to                                 generate those experiments so that was                                 that was kind of my second to the step                                 in in this evolution towards                                 evolutionary algorithms was getting                                 specifying parameter sets and then                                 allowing a computer to generate the the                                 full set of experiments that one can                                 launch and the other thing of course is                                 that you know you can spoke as a hit                                 point here with a number copy and this                                 relevance engineer there are there's a                                 large parameter space so you can have                                 different analyzer chains you cannot                                 feel boost and field ranges you know you                                 can have billion min should match tied                                 PF p f                                                                   of those things you can do boost by the                                 time you add in all of those                                 things and generate all of the                                 experiments with all of the combinations                                 you are definitely in the area and at                                 risk of overfitting it's possible and                                 it's useful to do that sometimes but you                                 have to be really careful that you're to                                 avoid overfitting that's what led me                                 into some other things so all right yeah                                 so this is what can happen with with                                 this is what the feature factories look                                 like so instead of the experiments we                                 have feature factories so you can have                                 different URLs so you can send queries                                 to different servers which may have                                 different configurations on them                                 and you specify ranges so what fields do                                 I want to use for any diznex query what                                 kind of weight what weight ranges do I                                 want to have how many fields do I want                                 to have if I only want to try one field                                 or two fields or three fields different                                 tiles values and query operator values                                 and so on so you just this combination                                 when you do all of the permutations                                 generates                                                               thing I added those parameter izybelle                                 strings which means that you can take                                 this boost which is a recency boost and                                 you can see in the as you see tell for                                 me pointing my screen modem pointing to                                 the you can take the                                                     those boost together at the same way so                                 your range is saying I want to range at                                 that point in the formula of                                             and then I want the next value to be                                 exactly the same as the first you can do                                 some pretty complex parameter as                                 parameterizing to specify different                                 types of experiments in different                                 different ranges of experiments but as                                 you can tell if you do all the                                 permutations you get permutations                                 explosion yeah if you have seven fields                                 you get up to two thousand experiments                                 when you have two weights so that's why                                 I moved into as a natural evolution into                                 some other options so one option is                                 random search and that is where you just                                 say now give me a hundred random                                 experiments and let's see where I live                                 where I land up where I land and that's                                 certainly an option the risk of                                 overfitting is is lower with that and                                 doing full beam search but it's it's it                                 can be quite useful and it's certainly                                 more efficient                                 that's what led me to a genetic                                 algorithm which is how can I improve on                                 random search and that is you know                                 pretty straightforward from a genetic                                 algorithm perspective where with each                                 generation you only let the top                                 experiments move on to the next                                 generation and I'll talk about this in                                 greater detail but we're moving from                                 we're moving from specifying experiments                                 to generating experiments and then                                 taking the output of those generating                                 generated experiments yet my generation                                 two of my personal growth in this two to                                 twenty-two actual machine learning on                                 figuring out what parameter sets work                                 well together so the genetic algorithm                                 basics it's so simple even I can figure                                 it out remember Charlie mentioned that                                 whole classics thing even I can do this                                 in an athis straightforward so the                                 notion is you generation zero you have                                 five experiments with different settings                                 you run those against specific against                                 the algorithm of your choice here I'm                                 running against notionally and ECG of                                 ten for those different experiments you                                 take the you score them and then you                                 create generation one so one operation                                 in genetic algorithms is crossover where                                 you take features of one with features                                 of another and you create two two                                 crossed over cross over death                                 experiments and then you can mutate them                                 so you take features of one so if you                                 had title and content in one in                                 generation                                                               you might change the boost on one of                                 those fields or some other way to mutate                                 the features of that query you couldn't                                 you can transfer those directly over                                 probably into the next generation if you                                 want and the other thing is that you can                                 prevent poor performers from moving into                                 the next generation and just create a                                 new random experiment and by random of                                 course it's a random of those those                                 feature sets are those that I talked                                 about earlier then you score those and                                 then rinse and repeat where you can                                 randomly select some for crossover                                 mutation carry through and then we're                                 just going to create a new random random                                 experiments                                 until convergence or until the heat                                 death of the universe which can happen                                 with some of these sometimes alright so                                 how does this differ from learning to                                 rank well as Peter Moses pointed out to                                 me after last time I gave this this is a                                 learning to rank and some of my thinking                                 about this was inspired by an early                                 learning to rank paper that talked about                                 any machine learning that you use to                                 figure out how to rank results this is                                 quite different though from the modern                                 day solar elastic use of learning to                                 rank in that well first the the                                 similarities                                 you still need sound engineering                                 decisions your analysis chain has to                                 match your data it has to be a good fit                                 for your data for the queries you have                                 to ground truthing you have to have good                                 ground truth the main difference between                                 this and what we now use as the phrase                                 line crank and we've seen land elastic                                 and solar land is that this is meant for                                 settings for that overall general                                 initial search it's not a rear ankle                                 function on on a subset of those those                                 features that said it can certainly be                                 adapted to be used in systems that have                                 learning to rank and can be used in                                 coordination with learning to rank                                 remember earlier you could specify which                                 server you can send those queries to you                                 can also specify which handlers you want                                 to use if you have a custom handler or                                 other things that you can customize you                                 can specify those as features in running                                 your experiments so at this point                                 generally it's basically it's how well                                 can we configure things for our general                                 general baseline search in the system                                 given the huge number of parameters we                                 have to play with all right the other                                 thing that I did in generation                                           was build in cross-validation so that                                 it's not something that researchers have                                 to reinvent it's and it's not something                                 that you can get away from mostly but                                 it's built into it you cannot run the                                 genetic algorithm across well validation                                 unless you intentionally say I only want                                 one fold don't do that                                 so that's built into it which is                                 so that makes the risk of overfitting                                 much smaller the other thing is it                                 allows you to see that the variance                                 across those different fold so you get a                                 sense of how fragile or how what the                                 variance is from those different                                 policies so you know that when you it                                 gives you a better sense of the                                 reliability of those results on new sets                                 of data so if you could say oh it'll                                 work really well in some it'll probably                                 not work well on others on average we're                                 probably doing better so                                 cross-validation is really important all                                 right so as we all know in this problem                                 this slide probably isn't necessary for                                 the folks in the audience but for for                                 some if it's new here you go with                                 cross-validation the notion is you take                                 a set let's say here we have four you                                 have a single data set at each iteration                                 you set three of them as training and                                 then you test on the fourth and in the                                 next iteration yeah so it holds zero you                                 train on the first three tests on the                                 fourth so but say we get DCG of ten you                                 get                                                                 three I'm sorry train on those three                                 tests on that one and so on so that                                 you're you're making the most you're                                 making very efficient use of your data                                 and you're getting a sense of what the                                 variances across the different                                 experiments so after you do that you                                 take the average also of course the                                 standard deviation and or these parents                                 or just eyeballing it is an important                                 thing to do if you're only running one                                 experiment and you get a number you                                 think oh that's that's great but you                                 don't get a sense of the breadth that                                 you think you can get certainly Monte                                 Carlo methods and other placer and other                                 methods are a good way of getting that                                 sense of variance but it's it's really                                 critical to understanding how well                                 you're doing in and if you are actually                                 improving in some meaningful way the                                 other key thing here is to report and                                 talk about the average I will admit at                                 some place I worked there I did have a                                 colleague once I wanted to report only                                 the best fold and I still shriek from                                 that and I think we can all agree please                                 don't do that all right so what does                                 this look like in practice as I                                 mentioned this is all command lies when                                 you're running it you get fold zero what                                 the training is so this said that we ran                                 a bunch of experiments on on training                                 those were the results we got we applied                                 full zero generation four                                 number two to the test data and that is                                 the value we got at the bottom of point                                 five five and it's pretty common to see                                 some variation this this data says that                                 I'm using is the data set that open                                 source connections and talking John put                                 out so it's a smallish data set so you                                 expect to see kind of these broad it's a                                 huge variance from training to testing                                 in reality you also see some things                                 depending on how big your data set is                                 and how uniform your data is and then                                 for each you can also you'll get the                                 roll-up across that so the experiment                                 the test for for the first four fold two                                 with the highest that was point seven                                 nine the test for fold zero was point                                 five five one five four and then there's                                 the median of point six three but you                                 can see that the standard deviation or                                 even just eyeballing how the variance is                                 is quite high for this little test set                                 alright so initial findings for one                                 client I was able to boost their NDC g                                   from point two five to three                                 given the amount of ground truth they                                 had I think that that was actually a                                 meaningful bump so I was really happy                                 with that                                 the bad is that when you set when you                                 have when you open up the entire                                 parameter set you don't run it long                                 enough you can often do worse than                                 baseline or where people are starting                                 from the great thing in the critical                                 thing for me is whether or not I'm                                 actually doing better and I'll talk                                 about this again in the next couple of                                 slides the key thing for me is that I'm                                 no longer specifying experiments those                                 are being generated automatically I have                                 a traceability for all of the                                 experiments I did and all of the results                                 I had I can reproduce all of those                                 experiments and I can now and I don't                                 have to twiddle with those things I                                 don't have to get a bed at nine o'clock                                 maybe ps                                                               that tomorrow I don't care anymore                                 because I can try all of those things                                 are the computer can try all of those                                 things I can focus on more fun things                                 and more you know more advanced you know                                 feature engineering single signal                                 enrichment external signals I'm                                 hopefully getting some kind of spend                                 with the dense vectors and so on so                                 that's the key thing is that I'm not                                 saying that this is the next cognitive                                 blockchain that will revolutionize                                 everything but this does K take care of                                 a lot of the                                 plumbing kinds of tasks the boring kinds                                 of tasks that we can do as relevance                                 engineers right the other thing is that                                 I would now instead of I'd like to move                                 from P values to L value which is if                                 Jimmy Lin said that a difference was                                 worth reporting in Twitter then I will                                 also report that and call that the L                                 value so just like to introduce that one                                 again if you don't wanna use P values                                 use L values alright for this talk I                                 worked with Nate day and open source                                 connections and thank you for the                                 inspiration for this little hopping in                                 puddles that I did and also for your                                 collaboration with the data and design                                 this is you know publicly available data                                 when you succinate sign an agreement                                 with them what I am presenting here is                                 not final it's not comparable with other                                 teams I'm not trying to sell anything                                 it's just I we had the data around and I                                 ran with it to see what we could have so                                 this one is the background linking task                                 where there are six hundred and seventy                                 thousand documents fifty-seven topics                                 and the notion is if we've given one                                 document find the find documents that                                 would be useful for background for that                                 so it's effectively more like this query                                 so I added that to acquire two fairly                                 recently so that I could play a round                                 with these with this document so so the                                 baseline if you just use a default                                 elasticsearch more like this query on a                                 Content field you get normalize discount                                 just kind of keep me looking at five of                                 one three eight now with the genetic                                 algorithm you bump that all the way up                                 to point four one six which isn't a huge                                 gain it certainly wasn't as large as the                                 gain reported by some folks at track one                                 set of authors but from baseline of                                 point three five two point five five                                 when they started adding more exciting                                 features an LP on and the extraction                                 they did machine learning to measure the                                 distance the semantic distance between                                 article categories and then they also of                                 course applied learning to rank on these                                 various things so they and they saw a                                 much bigger boost but my results are not                                 verbal but the key thing again for me is                                 even if I didn't do a great job at track                                 I didn't have to spend time on those on                                 all of the built-in parameters that you                                 can play with that's taken care of for                                 me if I happen to do well or happen to                                 get a boost in end ECG or something                                 that's even meaningful that's that's a                                 side benefit and fantastic the key thing                                 is that I've explored the parameter                                 space uncomfortable with it I can now be                                 the more fun things so Renee asked me                                 how well it scales this is a screenshot                                 of my poor server when I was launching                                 twenty threads of queries against the                                 elastic cluster with Washington Post                                                                                                         is my little laptop as you can see                                 Firefox is taking out more CPUs than my                                 look Java little Java pirated that's                                 sending off all of those queries getting                                 back to results and doing all of the                                 calculation so it scales fairly well it                                 does it you can hammer elastic or solar                                 and I did find out that elastic will not                                 allow you to query more than a thousand                                 times concurrently I think maybe                                        so there are limits but those limits are                                 are on the solar and on the elastic                                 search side and as we all know the                                 systems are crazily little bust so don't                                 hammer your production server but you                                 can do a fair amount of queries fairly                                 quickly with this all right so I'll be                                 wrapping up a little bit early he'll be                                 leaving more time for discussion and for                                 lunch or dinner I guess very quickly                                 depending on where you are in the world                                 next steps obviously documentation I                                 have some decent read knees but more                                 documentation would be good I'd like to                                 add some ground truth free measures so                                 overlapping rank correlation just so                                 that if you don't have ground truth and                                 you'll want to see how different a new                                 configuration would be from your                                 baseline configuration you can do that                                 one thing that really is I would why I                                 want to have a explain ability or                                 interpret ability so let's say the                                 genetic algorithm finds that this                                 cluster of experiments does does a                                 better job what features of those                                 experiments mark them as different from                                 everything else is it the fact that they                                 relied mostly on the content field                                 versus the title field or something like                                 that                                 I could actually get around integrating                                 a grown-up Bayesian optimization package                                 at some point I'm not sure that I want                                 to because I think there's a lot more to                                 be gained from learning to rank and some                                 of the other methods but I might                                 outreach certainly talking to the chorus                                 folks and there's some where they have                                 connections that he open-source I can't                                 remember name of the company quite                                 anyways yeah in working with others                                 whether it's to transfer the actual code                                 or at least some of the ideas that                                 people find useful and then made the                                 integration to other projects as                                 possible my takeaways from this automate                                 automate automate even automate the                                 generation of experiments so that you                                 don't have to sit there and twiddle with                                 the different parameters that one can                                 play with replicability traceability                                 statistical variance significant                                 statistical significance all of these                                 things are really important and let's                                 continue to do what we're doing with                                 those across the field so that's where                                 we have questions if you want to see                                 this live I can try a demo we do have                                 some time although I'll defer to Charlie                                 if we want to go straight to questions                                 and off we go shall I go for a demo I                                 think we bear in mind the time to try                                 some questions said great let's see how                                 we get through the questions maybe have                                 a government's for a demo I think it                                 thank you Tim fantastic stuff so our                                 first question you may recognize Rena                                 Kriegler no we tried to reduce the                                 number of configurations and parameter                                 configurations to try out have you any                                 thoughts about reducing the number of                                 queries                                 some of them might correlate with                                 optimal RAM RAM values and metrics yes                                 so that's another area that I would like                                 to build out is is clustering on query                                 types to see if there are families that                                 do well with different with different                                 configurations obviously the the head                                 queries are probably going to be                                 different than the tail queries and as                                 we oh we know you know people searching                                 for shoes or                                 the people searching for shoes or and                                 the people searching for                                 for other kinds of things are going to                                 be looking differently so the degree to                                 which we can cluster those results I                                 think think would help with that in my                                 experience I haven't really had to                                 decrease the number of queries because                                 elastic and solar are so powerful that I                                 can launch thousands of queries and it                                 works in a reasonable amount of time but                                 that certainly certainly leads to a good                                 point about about understanding the                                 nature of your queries and potentially                                 if you get a good signal from the                                 different types of parties you have as                                 you know and if you can do machine                                 learning to do the categorization then                                 that can be a really useful thing for                                 you know korean do machine learning look                                 at what settings work best with that                                 kind of query and shoot that off but                                 thank you I hadn't thought about that so                                 though that's just my little bit of                                 rambling on on how one might condense                                 the queries I'm sorry before I go go on                                 what is mildly amusing to me is this is                                 this overall process it's very similar                                 to what I'm doing on another project                                 which I mentioned briefly at the                                 beginning where we're doing fuzzing on                                 files and the notion there is also can                                 you decrease the corpus size and get the                                 same results so that you don't have to                                 do all of that processing so it's fun to                                 see randomization in another field being                                 extraordinarily useful fantastic thank                                 you so Mateo asks actually yes would you                                 recommend your approach to optimize                                 recall instead of nd CG okay leaving the                                 latter for re-ranking with learn to rank                                 and other techniques so yes absolutely                                 and I I didn't go into this but there's                                 a notion of different categories of                                 queries and you could certainly use this                                 to work on how you can improve recall                                 across those categories so you get the                                 good baseline and then leaving the rest                                 for learning of to ranked it to pick up                                 with a precision what I what I proposed                                 though is that you can also use learning                                 to rank in this whole framework                                 instead of specifying what fields you                                 can do you can say I want to external                                 with these fields but also have send it                                 to this handler which is which uses                                 learning to rank so you could put those                                 together and see am I in the same                                 experimental framework to see if you are                                 doing better                                 so yes recall baseline recall is where I                                 was headed with this and then yes a                                 precision as as Trey was pointing out                                 with the more advanced methods may may                                 may help out quite a bit with the                                 knowledge crops and so on so yes                                 completely yeah a good point thank you                                 okay so then our next question is kind                                 of the perfect question for an                                 open-source maintainer we have Edward                                 asks how can we contribute to this work                                 sure I mean the so the easiest thing is                                 to use it or look at it and see what                                 things you like about it and if there                                 are things you really like about it go                                 get the course people to put it into                                 their system                                 but seriously yes so use it break it it                                 is still hot off the press it's still an                                 alpha version I had to make changes even                                 as I was putting this talk together but                                 do you use it and let me know how it's                                 working if there ways that I can prove                                 it I am now actively developing it for                                 current work so I had gotten back into                                 the codebase and it's easy for me to fix                                 things it took me a couple of hours to                                 add the more like this query I there are                                 a number of other query types that I                                 would like to add so feedback on looking                                 on on github or on Twitter or on any of                                 the other media so feedback feedback and                                 obviously pull requests of course                                 fantastic thank you and Cedric asks                                 thinking about enterprise search and the                                 fact that types of users may have                                 different objectives with the same query                                 would you recommend a great one ground                                 truth per user type or would you see a                                 way to do it all at once and he pens                                 dreaming here                                 right I mean so has everybody said but                                 seriously if you have worked with people                                 with different very different budgets                                 or what they can pay for effectively                                 what they can afford in the complexity                                 of the evaluation framework I have not                                 worked in the e Rita and in the retail                                 space where search actually is but I                                 want you to know I work more in                                 government and other places where it's                                 important but not they just don't have                                 the resources that a bunch of people in                                 this meeting have available to them so                                 within with inquiry Ted there is a                                 notion of a query set and I put that in                                  there so that you could have different                                  personas so that you could optimize and                                  look at at those different personas in                                  my little example from the tmdb data                                  from open source connections Cogan John                                  there are clearly queries that are title                                  works and there's clearly titles that                                  are actor queries and having having a                                  division and having those in two                                  different sets again as I mentioned                                  earlier so hopefully when the queries                                  come in you can do automatic                                  classification on them and then figure                                  out about how to boost them and other                                  resources to support that from an                                  evaluation perspective and an                                  implementation perspective that would be                                  the ideal world and yes we can all dream                                  fantastic that that's the end of our                                  questions so thank you Tim we'll be                                  taking a short break and we'll be moving                                  on to our last talk today but thank you                                  Tim Tim Tim who's possibly the only                                  classically trained rocket data                                  scientist thanks Tim thank you with our                                  last talk thank you so much                                  you
YouTube URL: https://www.youtube.com/watch?v=rhNmplUwhf8


