Title: #bbuzz: Alexey Grigorev - Deep learning in production: Serving image models at scale
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/deep-learning-production-serving-image-models-scale

Deep learning achieves great performance in many areas, and it’s especially useful for computer vision tasks. However, using deep learning in production is challenging: it requires a lot of effort for developing and running the infrastructure to serve deep learning models at scale.

In this talk, we present a system for classifying images on one of the largest online classified advertising platforms. The main requirement for this system is to classify tens of millions of images daily and be able to operate reliably even during peak hours.

It took a year and lots of trial and error to arrive at the system we currently use. We present the details of this journey and tell our story: how we approached it initially, what worked and what didn’t, how it evolved and how it’s working right now.

Of course, we also walk you through the technical details and show how to implement a similar system using Python, AWS, Kubernetes, MXNet, and TensorFlow.
Captions: 
	                              hello                               sexy and today I'm going to talk                               introduction first a few words about me                               I have I've been working as a software                               engineer professionally for more than                               ten years and six of them I spent                               working with machine learning systems                               right now I work as a lead at a                               scientist at electro and this is you can                                see the logo forex group in this picture                                is a company it is a group of online                                classifieds companies so maybe you've                                heard of some of them it looks that go                                and a couple of more brands and the main                                idea of online classifieds is a place                                where you can share where you can solve                                something where you can buy something so                                this is Alex India it is the place where                                people can come and so things they don't                                need anymore                                or people where people can come and buy                                things we use things for cheaper prices                                um Alex India is one of the biggest                                websites so a couple of hours                                gee brain Alex India then we also have                                presence Africa Asia South America so                                this on this slide you see it's Ukraine                                and typically for online classifieds                                pictures are very important so when you                                want to buy something when you browse                                through the catalogue you see many many                                pictures of things and having good                                pictures is very important for for                                deciding whether you want to learn more                                about something what do you want to                                contact the seller and ask to to arrange                                anything to buy something olives we have                                a lot of images                                                      uploaded by our customers daily and the                                challenge we are going to talk about in                                this presentation is how to apply deep                                learning models to ten million images                                per day so in this talk will first                                discuss motivation why are we doing this                                why do we need it then we'll spend some                                time talking how we train models and                                then after training is done there comes                                next step how to actually serve the                                model and we'll talk about the evolution                                of model serving that we had two weeks                                so how we started initially how it                                evolved what was the original                                architecture what was what were some                                drawbacks of this lecture how we changed                                it and provide so imagine you're you                                have a car and you want to solve so what                                you do is you go to Alex you create                                listing view in some details and then                                you take a picture and what happens next                                this feature of this picture is uploaded                                to our image hosting our image hosting                                and is based on s/                                                  that in AWS for storing files so this is                                basically I think where you can put                                files and then you can get buy out back                                and every day                                ten million images are uploaded to these                                image postings is this him image hosting                                that means that we have billions and                                billions of picture in all three buckets                                what we want to do what we want to know                                about these images is wanted to know                                some information about them how good are                                these images because as mentioned                                earlier for people who want to buy                                things it's very important to have a                                good picture to get a good impression or                                how the item might look like and decide                                whether they want to contact the seller                                and if a picture is good then it                                maximizes the chances that they will                                decide to actually contact the seller                                and buy the item of Julia so here we                                have two pictures one picture is better                                quality the other picture is worse                                so we want to know which pictures are                                good which pictures about and in case a                                picture is not the best quality we want                                to contact the seller and suggest some                                ways to to improve the image and the                                overall listing then we are also                                interested in what what is it on these                                images what are the objects on these                                images as you see there are many many                                things that people can post an upload                                bikes cars sometimes people can upload                                something they want to they can try to                                sell something that they aren't supposed                                to sell like happen we also want to know                                 that in this case we have somebody's                                 trying to sell my chatter of course this                                 is we should prevent this from happening                                 so we from images who want to know what                                 is on the image so for each image we                                 want to know whether it's an image of a                                 truck whether it's an image of a fish or                                 it's an image of some web so the idea is                                 simple so how can we extract this                                 information of course using machine                                 learning and so we get all this images                                 that we have send them to the machine                                 learning model and then store the output                                 somewhere in a in a database so this can                                 be emitted at the database well and all                                 these labels all these categories are                                 the fields in this database basically                                 the information about each image to the                                 pan is clear we want to use machine                                 learning how do we actually train models                                 so                                 Ramos is quite simple these days                                 there are any services cloud services                                 that make the job a lot easier than the                                 few years ago we use Amazon sage maker                                 with sage maker it's quite simple so all                                 we need to do is upload the data to s                                  and trance h make a job the sage maker                                 job and gets the data from us we train                                 the model with the parameters we specify                                 and saves ourselves                                 so it looks like that so we have a way                                 to collect the data so it can be our own                                 service for labeling or you can also use                                 Amazon Mechanical Turk and then for each                                 image we know for example image quality                                 how good the images we get all this data                                 and save to s                                                        have take so you may just pass the                                 labels then we can run a sage maker job                                 so you make a job pay just a you know                                 just wrong s                                                            saves the results today game to s                                       modifier quite simple and we can have a                                 model in more time so let's say we spent                                 some time a week to train in the model                                 we have it the model is quite good                                 what's next what we can do next is for                                 example since we use sage maker for                                 training we can also use sage maker for                                 model serving so what we want to do is                                 just take this model and put it inside a                                 sage maker endpoint but then actually                                 not we don't just want to put this model                                 in to an endpoint we want to get all the                                 images that have in SV run them through                                 the model and save the results in                                 metadata database such that the users                                 can benefit from this so we need a way                                 to to get images put them to model                                 safety results make                                 so now we'll talk about how to actually                                 do this now to do it we create a special                                 service called metadata service that                                 clients of this service that is                                 typically others the other teams that                                 meet the predictions of our models they                                 communicate to this this may be that the                                 service                                 talks to a model the rapper this is a                                 simple very simple service that simply                                 fetches images from s                                                   images and then sends them to search me                                 say you make her processes the images                                 returns the predictions and this model                                 our original predictions again to                                 metadata service metadata services the                                 results to database and response to the                                 user now the user can use can use these                                 predictions to do what whatever they                                 want and the first initial version was                                 quite good so we could already analyze                                 the quality of images that we had and                                 somehow educate our users that in cases                                 that image is not good we can say what                                 is wrong with the image and suggest                                 waste enjoyed until about some problems                                 with our initial rejection                                 first of all stage maker turned out to                                 be quite expensive if we when we simply                                 deploy it to our own kubernetes cluster                                 instead of using sage bacon on point we                                 could reduce costs like four times then                                 the next step we noticed that it's quite                                 difficult to to deal with spites of                                 traffic when we have sudden spice spikes                                 of traffic and when during during some                                 days suddenly a lot of users try to                                 upload images then to gracefully scale                                 up and scale down we added a bit of we                                 made our services synchronous so good                                 iq between the metadata service and                                 modular rapper and with this Q we can it                                 was a lot easier to actually scale our                                 models because we didn't need to process                                 everything immediately we could just                                 wait a bit and simply scale our models                                 up and then process through pics of                                 traffic we have two models so basically                                 for each of the model we had a special                                 thing that we previously called model                                 wrapper for each separate one which                                 could fetch images from s                                                to tender for serving or MX MX net                                 server so serving and process models so                                 let's walk through the process so first                                 a client's and submits a request so it                                 can be for this files I want to know the                                 category of the objects on these images                                 so it means we want to run a                                 classification model then the metadata                                 service responds immediately saying that                                 your request is include wait will tell                                 you when it's finished now metadata                                 service checks the database to see if we                                 already have some results for some of                                 the files if we don't we submit this                                 request to a queue then the image                                 category model wrapper listens to this                                 queue both messages from there typically                                 does this in batches of                                                 images get the images from s                                           does some pre-processing because we need                                 to get the images besides them convert                                 to numpy array do some pre-processing                                 like normalize the arrays then                                 eventually takes the race and converts                                 them to protocol once we have heard                                 above we use your PC to talk don't                                 answer for serving                                 we sent requests on the fossil in the                                 place with results a game over jealousy                                 and then the model rapper puts the                                 results to the response kill metadata                                 service distance to this queue yes the                                 results save them to the database and                                 then finally responds to to the client                                 and this is using a call back saying hey                                 for these IDs these are the categories                                 and this worked quite well so we could                                 use this in this models for for many use                                 cases like one I already mentioned                                 detecting the images with not great                                 quality and then suggesting this else                                 ways to improve it and the other case                                 was moderation when somebody is trying                                 to solve something they are not supposed                                 to so we could catch these images and                                 not that can go live to platform                                 terrible unfortunately some drawbacks in                                 this architecture so this was pretty                                 inconvenient for the clients because                                 when a system is an asynchronous it                                 makes it quite difficult to to to work                                 with this instead of just simply saying                                 as sending the request engage me                                 response they need to also keep track of                                 photos so what request and then also                                 provide a callback so it means the thing                                 to make the service publicly available                                 to to make this endpoint it's a bit                                 difficult for the clients then it's also                                 not really the real time because in some                                 cases when we have pics of traffic it's                                 difficult to let's say at the same time                                                                                                        course there will be some delay in                                 working through this backlog of images                                 and it means that somebody is trying to                                 sell again it                                 this point we might need to wait fire                                 for sometimes ten minute we come                                    minutes before we can actually catch                                 this case and remove the ad in these                                 cases we of course want to react real in                                 real time and although you know about                                 guns at the moment they are uploaded to                                 the platform and it's also too much                                 synchronous like we need to do have a                                 lot of queues and just following through                                 the system to see how requests are                                 propagating sometimes quite difficult                                 it's also expensive we use SKS public                                 use and it works through polling so the                                 the clients could services who listen on                                 the queue and they simply ask the queue                                 hey are there new messages other new                                 messages and they doing this of course                                 there is some delay but eventually for                                 each request we have to pay a certain                                 amount of money and then at some point                                 half of the costs for the infrastructure                                 was just simply SKS polling that was too                                 expensive then we also had some                                 duplicated project because this model                                 wrappers they both need to talk to s                                     some pre-processing so when creating                                 these services we need to somehow put                                 this logic in the library it was and be                                 difficult mundane then database that we                                 have was not good for analytics we can                                 of course store the results there and                                 use them for responses but when people                                 wanted to analyze the results of the                                 models we couldn't simply use that for                                 that and we also also used my sequel and                                 with our traffic we found out pretty                                 fast that just grows too much and it's a                                 bit of a burden to maintain to make sure                                 that the database is not overloaded                                 it has sufficiently large in sauce and                                 things like that and finally when we                                 need to add a new model turns out that                                 there are too many places where we need                                 to let me need to modify so when we want                                 to add a third model we need to change                                 something in the metadata service we                                 need to add two more queues we need to                                 create a new service for the rapper we                                 need to put some logic tail for getting                                 files from us three then put these two                                 tons of for serving or MMS so it's a lot                                 of work to just add a new model that's                                 why we try to make it a bit simpler this                                 is something I will talk about now so                                 this is what we have and we try to see                                 how we can improve it so the first step                                 that we did was to to get all this model                                 wrappers and put them into one service                                 so now we don't have multiple services                                 is just one thing and we call it in                                 image model service because this theme                                 contained the logic for getting the data                                 from s                                                                 talking to tons of observing to the                                 actual models with this setup we no                                 longer needed the metadata service and                                 all these queues and the client simply                                 could send HTTP requests you do IMS                                 itself now and then we weren't happy                                 about MMS MMS is a max net model service                                 so that's why we also they moved it and                                 with this our architecture becomes quite                                 simple so in retrospect like right now                                 it seems quite easy yeah so this is in                                 retrospect what we should have started                                 with just a simple thing that accepts                                 requests and then forward sound to                                 actual models of course we need to have                                 a caching layer                                 on top of that to make sure that we                                 don't we don't score our images multiple                                 times and the way we do it we use                                 dynamic viq by the store and for the key                                 instead of using the file name we use                                 md                                                                    few duplicates on our platform                                 people often upload the same image                                 multiple times so in this case we don't                                 want to send it to score it again                                 and send it to the model we can see that                                 for this md                                                       results and we can simply return to the                                 user in the same we don't need to                                 calculate md                                                         attack from s                                                          most cases is the same as md                                            needed to add a few more models so the                                 first model was detecting if there is a                                 artificially embedded text on the                                 pictures in some cases it's prohibited                                 on our platforms - to add this text we                                 want to have a model that detects that                                 it's the case and help us remove this                                 from the platform so there was another                                 model that simply checks if there is                                 text so not adding this model was quite                                 simple so we do you need to touch many                                 places so we just needed you to adjust                                 the code of Emma's                                 to add the new pre-processing class for                                 this and add another instance of tons of                                 observing with this model then we had                                 another problem that we also wanted to                                 solve with deep learning it was we                                 wanted to detect nudity so we trained                                 another model for classifying if an                                 image is safe for work or not that was                                 another model we used max net for that                                 again trained using sage maker and for                                 that we wrote our own MX net serving                                 because we didn't like MMS                                 we wrote a simple thing simple wrapper                                 around him on Mike's Amex net that can                                 get a compressed numpy array in Pocket                                 apply the model and return back the                                 response something pretty similar to                                 tender for serving but instead of using                                 browser buff it used HTTP that was the                                 our final architecture so it was quite                                 simple one caveat here was that tuning                                 it was more difficult than in                                 asynchronous case because when we have a                                 synchronous we can gracefully react to                                 Vixen traffic when there is a sudden                                 sudden peak                                                            we can simply put them to the queue and                                 scale it out and then process through                                 the backlog and then scale it down and                                 we don't need to worry that something                                 gets lost here we need to be more                                 careful with this because if we are                                 synchronous and need to respond                                 immediately it means that to react to                                 these peaks of traffic we need to                                 sometimes over-provision instances so                                 some to have some instances that are                                 idle and one once they started once they                                 start they ship traffic we add more                                 instances in this way we can react to                                 peaks of traffic with with less problems                                 but it's again it took her quite a while                                 to actually tune it to be able to do                                 process through a lot of requests at the                                 same time with asynchronous case it was                                 easier not just one last thing is we                                 have analysts and analysts are quite                                 interested in analyzing the results of                                 these predictions and of course they                                 often want to do to see how many images                                 for example                                 contain text or images for pornographic                                 or how images contained cancer things                                 like this how many cars were there among                                 images with the previous setup it was                                 difficult and let's briefly talk about                                 how we made it easier for analysts so                                 the moment a user uploads an image to s                                  as we can generate s                                                     saying hey there was a file in the                                 bucket do something with this and it's                                 possible to put these notifications to                                 take you and then what we can do is we                                 can simply listen to this q                                          events - all the new newly uploaded                                 images and then simply send them to IMS                                 to our image model service then model                                 service responds with results we can put                                 it to these artists we can a system in                                 our case and then eventually save it to                                 s                                                                     simply put them to look at all can have                                 the data in Latino tables athena is is                                 basically a managed Chico engine and                                 presto and you can use it to query all                                 the results so analysts could use sequel                                 to way they know and love to query the                                 results of our models and they were very                                 happy about this that was it                                 so we talked about motivation why we                                 wanted to to build our model server why                                 we needed to serve different models when                                 we briefly talked about how we trained                                 models and then discussed our retexture                                 for this just want to summarize all the                                 talk into the main takeaway points so we                                 use deep learning to extract metadata                                 from images                                 there's a couple of models and then we                                 run them on safe results in metadata                                 database we you say the message maker                                 and it makes it very easy to to train                                 deep learning models we simply need to                                 specify the location with the data the                                 parameters of the model press a button                                 and trains a model and saves the results                                 to still serving with each maker is not                                 as nice as training so it's you need it                                 requires a bit of code to actually get                                 the images from s                                                        we need and then it gets a bit expensive                                 at scale when we need to process a lot                                 of images at the same time working                                 through a lot of images through peaks of                                 traffic is easier when the system is in                                 Cygnus but there is a downside of that                                 the ter more complex they're more                                 convenient for the users and then they                                 are not always real time because when                                 there is a backlog of items in the                                 queues that we need to process it may                                 take some time to to process all the                                 items we use the three event                                 notifications mechanism a non-intrusive                                 way to connect our systems to connect                                 all the images our image hosting from                                 the english posting and then process                                 them and put the results to to the Avena                                 table and that made it a lot easier for                                 analysts to analyze all the results and                                 then finally athena is quite an easy to                                 maintain solution for analytics it's                                 scalable you pay only for the data use                                 can you simply can put all the data in                                 s                                                                      that is almost all from my side so this                                 talk is based on two blog posts in our                                 block you can go there and read for for                                 more details for more information there                                 is also in the part two a bit there are                                 some details about the way we serve                                 comics net models so if you're                                 interested go check it and then finally                                 I am working on a book called machine                                 during boot camp boot boot camp the idea                                 is to teach machine learning through                                 projects if you're interested go check                                 the link and you can get                                             discount with the code here I will                                 appreciate if you give me any feedback                                 on the talk you find it interesting or                                 maybe it was too slow or too fast so if                                 you want to do it you can check the                                 squirrel code or this thing and give me                                 some feedback also there you'll find the                                 link to the slides and you can also if                                 you're interested in a free copy of                                 machine learning bootcamp boot camp                                 leave your others email address and you                                 get a chance to to win a free copy this                                 is all from me thank you for attention                                 please let me know if you have any                                 questions I'll be very happy to answer                                 them thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=JJnmGqypE2k


