Title: #Haystack: Jacopo Tagliabue - Few-shot and zero-shot personalization using deep architectures.
Publication date: 2020-07-02
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	Full Title: "Not all those who browse are lost": few-shot and zero-shot personalization for digital commerce using deep architectures. 

More: https://berlinbuzzwords.de/session/not-all-those-who-browse-are-lost-few-shot-and-zero-shot-personalization-digital-commerce

Personalization in IR is one of the hottest topics in the AI-takes-all economy: we should not aim to be "just" semantically relevant, but also tailor results to users' preferences and intent. However, personalization in digital commerce is easier said than done: most shoppers visit a given store no more than twice a year, and bounce rates across verticals show that it is important to personalize as early as possible. In this talk we share effective strategies to tackle the challenge of "in-session personalization" in NLP and IR tasks, starting from the straightforward case of "one-shop" personalization and then generalizing to more. On the business side, we argue using industry benchmarks and data from our network that in-session personalization is a fundamental part of any relevance journey in the hyper-competitive e-commerce market. On the tech side, we build on the latest deep learning trends to show how increasingly sophisticated representations of real-time intent can power personalization in product search. Once dense architectures are in place, we are ready to tackle the challenge of "transfer learning": can shopper's intent be transferred from one shop to another without annotated data? Using insights from lexical learning, sessions from different shops can be projected in the same space to power "zero-shot" personalization for downstream NLP services; finally, we prove with quantitative and qualitative benchmarks that zero-shot predictions are a significant improvement over industry baselines.
Captions: 
	                              I used to say when I'm the last                               presenter that conference I used to say                               that I hate to be the one before you and                               a beer or wine but I guess the cool                               thing is about virtual conferences is                               that you can actually add wine while you                               listen to my part and it's probably                               gonna make it actually more interesting                               so thanks so much for the introduction                                charlie like a very quick overview of                                who I am and what we do a caballo                                since I guess you all of Google so you                                can actually you know search that                                yourself with more time since we have a                                lot of interesting things to say today                                so I used to be the founder of a                                start-up in Silicon Valley - so these                                are the three founders and obviously the                                more and someone and the funny one - xur                                was recently acquired last year by North                                American unicorn caballo - basically                                further enhance the AI and NLP                                capabilities of the company for those of                                you who do know caballo caballo is a                                like AI PowerSearch recommendation                                engine with five and employs                                approximately a thousand customer                                deployments Cabrera is a total of more                                than                                                                couple of years in particular last year                                cobia raised two hundred million dollars                                to achieve the price status of unicorn                                and again like six months it's like no                                more than six months ago nine months ago                                now they acquired my own startup and                                I've been the leader a scientist of                                caballo that's part before diving deep                                into the material today let's let's get                                into into credits this is one of my                                favorite quote so if you steal from                                    plugins you steal from many it's                                research it's a bunch of people we                                really want to thank so Christine look                                at zero and Federico Federico is a                                postdoc fellow at Bocconi University in                                Italy Christine Lucca and Shiro are my                                colleagues at creo and this work is a                                joint work with them so nothing that you                                will see today would be impossible                                without their help and of course it's                                you know it's it's customary to change                                this is occasion if there's any mistake                                or any errors in this talk I'm sure it's                                that fault and not mine so today we're                                gonna discuss of a very very very                                important use cases in commerce this is                                to to user to shoppers that goes into                                into into any commercial one is bob                                injuries in hand bob is a basketball fan                                so it browse for some basketball product                                and then you start typing something in                                the search bar N and then we want                                ideally the e-commerce shop to actually                                recognize his intent and provide very                                relevant things like Nike brown shoes or                                NBA jersey or something like that and                                he's a tennis fan                                she's a tennis fan so and really like                                Stanley she's browsing for tennis                                rackets and then a tennis skirts and so                                when she types em in the same sport                                apparel shop we wanted the shop to                                recognize that and propose something                                like Nidal rocket or Knight women's                                shoes and so on and so forth                                BOM Anand need not to be necessarily                                logged in and above Ana needs you know                                they may be the first time they actually                                go into this website so what we're gonna                                discuss today is how we're gonna build                                this type of experience using the latest                                tools from deep learning and our                                research club some fact about e-commerce                                for those of you that I mean what you                                know a lot about search but maybe not                                about the industry space so real website                                by real website I mean you know not                                Amazon not eBay not Salon like you know                                the vast majority website you are                                actually gonna browse on I have two big                                problems one is high bounce rate which                                means that as when user goes on a                                website                                after a few interaction they typically                                get out of it so it's very hard to keep                                using on your website the second one is                                that the user base is very small which                                means that most of the people actually                                come back to the website two or three                                 times in an entire year                                 these two facts combined for two                                 important generalization that we need to                                 keep in mind when we discuss                                 personalization commerce                                 first one is personalization need to                                 happen as early as possible in the                                 journey and with as little user data as                                 possible                                 as early as possible because if you                                 don't provide a personalized experience                                 people will leave little user did as                                 possible because as we discussed very                                 few users will actually come back two or                                 three more times                                 so the actual amount of data you have                                 about a single user it's very very small                                 which you know all these things together                                 imply that every personalization                                 solution out there that relies on user                                 three are basically not covering the                                 vast majority of user so they are useful                                 for you know this small portion of user                                 that comes back but a vast majority user                                 will still not be basically benefit from                                 anything like that so the other fact                                 that I want to convey about e-commerce                                 is that session information is rich so                                 when people go on our website even if                                 they're not really logged in what they                                 do typically follow an intent or a trend                                 so this is like a sample from a real                                 session of one of our customer this is                                 an again she's just browsing for a guest                                 t-shirt she's actually on searching for                                 a guest t-shirt she click on another                                 t-shirts and now if you ask what is                                 she's gonna buy what is the most likely                                 item she's gonna buy she's gonna buy one                                 she's gonna buy two or she's gonna buy                                 three the answer that most people give                                 to this question is obviously one which                                 is actually what happened in practice in                                 the session that actually took place on                                 the shop how do we know that as humans                                 but we kind of clearly recognize the                                 intent which is buying a packet like a                                 t-shirt for women and we also recognize                                 in this case brand awareness like you                                 know she only browsed things for guests                                 and she explicitly used language yes                                 t-shirt to actually single out those                                 product in fact if you take so this is                                 the top searches that are issued in a                                 real sport apparel shop from different                                 section the website and as you see the                                 linguistic behavior of the users changes                                 drastically depending on the session of                                 the website you're in again in somehow                                 reinforcing the idea that what happens                                 within a session or what a business near                                 real-time is very very relevant to                                 provide personalized NLP capabilities so                                 now that the problem is clear we need to                                 answer two questions so how we can to                                 answer the questions like how can we                                 teach sessions to machine so how we can                                 give machine the same kind of                                 understanding of what the intent is in a                                 session and then once you do that how we                                 can use these information so the way in                                 which we understand intent to                                 personalize language behavior we're                                 gonna do you know we're gonna tackle                                 these two questions in turn the first                                 thing we're gonna do is that we're gonna                                 build a vector space using deep learning                                 we're gonna exploit technique noses work                                 to back                                 which many of you will be familiar with                                 that Trey Granger discussed briefly                                 about that like two stalks ago so we're                                 not going to spend so much time on it                                 but the general idea is that you can                                 build a dense space of words by                                 exploiting the fact that similar words                                 appeared together in similar context                                 right we're going to do the same but                                 we're going to say with products because                                 that's what people interacted with when                                 they actually browse your web site the                                 intuition is the same like similar                                 products have been in similar browsing                                 context it's kind of the machine                                 learning counterpart to the saying of                                 you can you know you know a lot of a man                                 by the company keeps right this is the                                 kind of same things like you know a lot                                 about a product you can learn a lot                                 about products by the company keeps in                                 people session so if you think of                                 typical word to vac training you will                                 have something like you know talking in                                 a session the cat is on the mat and this                                 token are passed to desire government                                 that's basically is going to be a dense                                 representation of all these tokens like                                 the cat is on what we're doing is proud                                 to back is we're going to fit the same                                 algÃºn it's called Steve Graham I'm gonna                                 fill this in my groin we're gonna fit                                 products in a session so for example                                 this is a ski team session and we're                                 gonna fit this product to the argument                                 what happens and this is a real vector                                 space what happens when you do this in                                 in in with real session data from you                                 know hundreds of thousands of users is                                 that you come up with a space where                                 products are located depending on                                 basically their latent properties so                                 similar properties will be closer in the                                 space if we understand product as a                                 space now we have a way of representing                                 user session as basically walk or path                                 into that space so similar the sneakers                                 would be in similar part of the of the                                 of the space and if you imagine like                                 representing a session of of a shopper                                 that's looking for different pair of                                 sneakers what what you're actually                                 seeing it's some money in being in one                                 part of the space is opposed to the                                 other so what as humans we understand is                                 your activity team or your interests or                                 what people in commerce see because same                                 intent is represented for machine                                 as portion of this space again there's a                                 clear analogy here you can tell a lot                                 about people preferences if they say to                                 you that they go on vacation on a Lulu                                 or if they're going on vacation in Maine                                 in the same way you can understand a lot                                 about a person if you know where the                                 person is in this in this abstract space                                 so the way in which represent session is                                 basically we every time the user is                                 interacting with a product on your                                 website we're going to capture that                                 interaction and we're gonna place these                                 users in the part of the space when that                                 product is the more product you see the                                 more basically remove this centroid                                 around and so that is going to be our                                 representation of your session the cool                                 thing about this method is that it can                                 be built in a completely unsupervised                                 way right so as long as you have a way                                 to track user interaction you can fit                                 this to a proper back model and get                                 automatically the space with no human                                 intervention so all of these can be done                                 at scale across many clients now the                                 second question is how do we personalize                                 the language remember that we want to                                 provide Bob and Ann with different                                 language suggestion how do personalized                                 language based on this based on this                                 information the answer to this is what                                 it's called conditional language model                                 so a language model generally you may                                 you may you may you may probably know                                 this it's just a probabilistic model                                 they would have signed to any e-commerce                                 a probability for any query for example                                 if your if your shop is selling sport                                 apparel the language model for your shop                                 who tends to say that the probability of                                 the word shoes in a query is much higher                                 than the probability of the word - right                                 because your query your shop is all                                 about sport on the other hand if you're                                 if you're dealing with an electronic                                 shop your language model would probably                                 tell you that the probability of iPhone                                 is way bigger than the probability of                                 shoes just because you know that that's                                 what basically people are searching and                                 buying on your on your shoes on your                                 shop a conditional language model goes                                 one step further so now we don't have                                 just a model there are science                                 probability to query we have a model                                 that ascends provi to query taking into                                 account the context so take into account                                 the fact that for example you were                                 browsing basketballs                                 or you were browsing the family section                                 we're gonna do that with the general                                 architecture in deep learning which is                                 called the encoder/decoder architecture                                 the idea here is that you have basically                                 two neural network one your network                                 which takes which has the task of                                 encoding in a in a single Aten space the                                 information about the session as we saw                                 information about the session is                                 captured for us using products or using                                 the product space that we generated and                                 then the encoded session is fed to a                                 decoder network which is another network                                 that has the the job of starting from                                 that lattin's state basically generate                                 the language we want in our case again                                 the first part is gonna is gonna is                                 going to be provided by prof. track so                                 our skipper model and the second part is                                 provided by a type on your a recurrent                                 neural network which is known as LS TM                                 which is the peripheral network in case                                 you want to deal with with language and                                 you know generating sequence of things                                 in our case generating sequence of                                 characters so the answer to our second                                 question which is how can you session to                                 personalized languages well we take the                                 encoder decoder model from the deep                                 learning literature and we use it as our                                 conditional language model so in this                                 case language probabilities so the                                 probability of nicely brown shoes versus                                 natal racket will depend both on                                 linguistic information that is how much                                 natal and an IKE our query direct                                 popular in the e-commerce and session                                 data which is you know based on the fact                                 that you know annum Bob this very                                 different part of the product space so                                 probably they would expect the behavior                                 of the search part to reflect those                                 real-time intent so does it work well                                 the too long did the transfer is yes and                                 we extensively test this these our                                 search idea against you know some                                 industry and research baseline not gonna                                 spend like an awful amount of time on on                                 these numbers as there's like the                                 presentation as all the links to the                                 actual research papers and the things we                                 publish an open source code                                 as I think you know it's it's it's more                                 interesting if we discuss the general                                 vibe instead of the numbers but what you                                 can see here so we basically test our                                 method which is called vector sequence                                 with different lengths on the on the                                 suggestion bar so zero is when nothing                                 has been typed so is when the user just                                 click on the search bar and you have to                                 basically infer everything SL equals one                                 is that when the user type one character                                 SL equals two is when the user actually                                 type two characters we bench my design                                 in popularity which is what most people                                 are doing in the industry I think it's a                                 fair assessment                                 so basically ranking queries suggestion                                 based on you know how popular they are                                 and we try different way in which we can                                 use dense vectors to personalize                                 experience one is called image to                                 sequence which instead of using the deep                                 rod of space that we we explained is                                 using the images of the product cut out                                 so the idea here is that when the user                                 interact with some product in a session                                 yet struct without convolutional neural                                 network deep features from that image                                 from the image of the product and use                                 that in your encoder but the cool thing                                 about encoder decoder architectures that                                 they're very general so you can swap in                                 and swap out different encoding and                                 basically with basically no changes to                                 the underlying code you can train many                                 different models search to protract is a                                 is a is a different alternative to the                                 full encoder decoder ranking still based                                 on on deep factors panel just that we                                 play slightly different flavor and                                 vector sequence is our model which as                                 you see outperform all the other models                                 by a significant margin in particular                                 with the empty query is                                                  times better in MRI than the than the                                 typical frequency based model any still                                 more you know more than twice as much                                 accurate with one character queries if                                 you want to see like real example of                                 what would happen and with the model so                                 these are the products these are like                                 first session the product is an image of                                 the product that has been interacted                                 with with the user the seed is the                                 lecture that has been typed and then we                                 have this distinguishing between                                 popularity and the encoder decoder                                 model that we propose this query                                 untranslated so there are region from                                 different language and that's why                                 sometimes you know the see that and be                                 in the                                 and the end this thing doesn't really                                 match but it gives you like a good                                 flavor of how well the model is                                 capturing their lying intention like if                                 you take for example the second row you                                 have a tennis racket the CDs are and                                 obviously the popularity model you know                                 tennis is popular in in Italy but not so                                 much so the things are way more popular                                 and so the model will suggest you know                                 Reebok CrossFit but the model that we                                 built actually is able to capture the                                 intent the family's intent and actually                                 propose tennis racket as the best as the                                 best suggestion and you see the other                                 the other it can pose well I think it's                                 it's very it's very easy to get why the                                 model is qualitatively way better than a                                 pure frequency based model so now that                                 we now that we know we capture session                                 intent so we don't assume anything about                                 the user except the interaction within                                 the current session and now that we know                                 how to generate language model                                 probability based on that we're going to                                 take this one step further so since it's                                 very very very very hard to get user                                 coming back to your shop the question is                                 what if what if context is from a                                 different shop so now we discussed you                                 know we discussed that it's it's very                                 important to to be able to personalize                                 the experience of the user with as                                 little data as possible from one website                                 now we're going to take these two                                 extreme and we're gonna we're going to                                 try and personalize pub experience when                                 Bob is browsing a basketball product on                                 on shop number one then you leave shop                                 number one it goes on shop number two                                 and the first thing he does is press M                                 on shop number two so the question is                                 shop number two never so Bob before shop                                 number two may or may not be affiliated                                 with shop number one we're gonna discuss                                 that later and the question is can we                                 actually personalize Bob experience with                                 zero literally zero data point on shop                                 number two to see how that is even                                 theoretically possible it may be good to                                 like you know explore a sample of the                                 product base for two shops in the                                 Sport apparel business as you can see                                 here the shops obviously are different                                 this space is gonna be different for the                                 proper you know they have different                                 catalogs and you know different                                 different direction may have very                                 different traffic but there are some                                 analogy now things are actually in the                                 space in the in the case in the in the                                 simple case you can see that snowboards                                 and and a soccer ball a can of in the                                 same position in shop                                                   you flip the space with the                                 transformation this thing should remind                                 you of NLP literature where people                                 actually add the same observation about                                 word to vac remember the word vector we                                 discussed well let's imagine that                                 instead of having shops you have                                 languages so now you have English and                                 you have Spanish and the idea is that                                 you will train your vectors and then you                                 project your vectors into into a space                                 what'd you find out I think this is this                                 is pretty cool is that English and                                 Spanish you can immediately draw like                                 some sort of analogy of what four is in                                 one side and four is in the other so the                                 idea is that if these things are not                                 exactly the same but somehow they're                                 topologically similar maybe there's a                                 way to go from one into the other you                                 can actually exploit these you know                                 analogy even further if you know how you                                 know neural translation model actually                                 work into these days so if you go and                                 google translate and you and your side                                 may be actually kinda means I like talks                                 in Italian and you want that to be                                 translated in French what happens                                 simplifying a bit is again another sort                                 of encoding and decoding so yang the                                 scene there are two neural network one                                 which is taking the English or the                                 Italian or whatever whatever language                                 you want is encoding into Latin space                                 and then there's a neural network that                                 decodes it's in the target space so what                                 we did again following the analogy with                                 the NLP word is that we're treating                                 shops as languages so what we train a                                 deeply neuron model to do we train a                                 model to basically translate from                                 products in one space to product into                                 another space in the same sense you can                                 train a language model that you can                                 actually that you can actually translate                                 between French                                 English or English and Germany so the                                 results are very good again we call it                                 zero shot in France because in shop -                                 you have no information about Bob so the                                 only things you can do is that you can                                 move the intent then you learn a shot                                 one using Proctor back you translate it                                 using an Iran translation model and then                                 you use that to condition your your                                 probability as you can see like three                                 different we benchmark three four and                                 model one is the non-personalized which                                 is what basically everybody's doing at                                 the moment so if the shopper is new to                                 this website we're going to count it as                                 view so there's no personalization                                 involved weights there is an                                 unsupervised version in which you it's a                                 bit complex but in which you use                                 basically the images into catalogue to                                 build some sort of guide for the                                 alignment of these two factor space and                                 the supervised translation that we just                                 discussed a bit more as you can see                                 obviously the proposed model vastly                                 outperforms the non-personalized                                 baseline and even the unsupervised                                 version so the version in which you                                 don't have any data when a user goes                                 from one shop to the other so the                                 version is completely unsupervised it's                                 still better than the non                                 personalization so as for the conclusion                                 and next step and then I think we have a                                 bit of time to discuss the details and                                 you know like the use cases well this                                 whole idea of personalizing things with                                 intent it's very scalable as it can                                 build it can be built in a completely                                 unsupervised fashion and it can build in                                 a very scalable fashion either for very                                 big for very big ecommerce and can be                                 used to inject personalization in all                                 deep architecture that you want let's                                 take another case of type-ahead                                 now we're not trying to predict the word                                 itself like an Nike but we're trying to                                 suggest the category to narrow down your                                 result I mean if you go on most                                 ecommerce but you know Amazon eBay and                                 so on once you start typing sometime the                                 e-commerce will nod you in selecting a                                 specific category or specific facet in                                 which we execute the search right would                                 it be nice if that category will all                                 would also be complete                                 personalized depending on what you're                                 doing and you can basically use the same                                 idea of you know plot to vac encoder                                 decoding and so on and support you can                                 basic applied the same idea to provide                                 people with personalized category                                 suggestions not just suggestion itself                                 if you want                                 we recently share like open source code                                 in a paper to actually get you started                                 with this problem if you want to learn                                 more as I understand we cover a lot of                                 things and obviously we have to skip                                 over a lot of technical details you can                                 obviously the load our papers and and                                 and research work we presented you know                                 this work with at the web conference in                                 Taipei we're going to present part of                                 this work at ACL in July I mean                                 virtually in July and the work is public                                 and often we also share code and                                 implementation if you want to take part                                 in our experiments or work with us on                                 specific things please get in touch we                                 already have like live collaboration                                 with world-class institution like Max of                                 research Bocconi University and                                 researcher from all around the world                                 join us to improve the state of the art                                 of NLP nai in commerce and search in                                 general if you prefer something a bit                                 lighter than full research paper we also                                 try to blog and popularize and                                 evangelize the field in a more with more                                 a more gentle touch so please go on our                                 website and you will find like                                 high-level description about these use                                 cases with a more of a let's say                                 industry angle than just a pure                                 scientific angle obviously we touch only                                 like a like a tiny portion of what is                                 the what is the possibilities of                                 personalization in commerce but                                 hopefully with we we got the impression                                 of the amount of information that it's                                 hidden in a session and how many cool                                 things we can do if you find a way to                                 unlock them I always like to finish my                                 thoughts with the we record from Alan                                 Turing to week to whom we base gave her                                 everything and we can always see a short                                 distance ahead but we can see plenty                                 there the needs to be done as always say                                 let's not forget                                 it's up to us to get it done thanks so                                 much for for listening to this fantastic                                 thank you very cocoa                                 we've got certainly got some questions                                 in the channel for you so I'll start off                                 with a question from Matteo what's the                                 strategy to apply this approach to                                 e-commerce sites that are new or that                                 haven't properly collected user                                 interactions in the past you have very                                 very good questions so for the within                                 shop case I would suggest to start with                                 the image version that we that we                                 briefly discuss and that is discussed at                                 length in the paper the idea is that you                                 can jump start instead of waiting to                                 utak for you to collect enough                                 interaction to train a proper route to                                 back model which may take a couple of                                 months or even more depending on the                                 traffic if you can just collect like you                                 know interactions and use the images in                                 the catalog which never changes and                                 they're available at day one you can try                                 and use that to basically build a                                 personalization strategy as for the same                                 question cross shop that really depends                                 like the best thing is that like some of                                 the biggest retailers in the world a                                 multi group brand gap West Nike and so                                 on so ideally they probably have                                 historical data on what people do when                                 they move from one brand to the other                                 and in that case you can use the                                 supervised translation method that I                                 explained but there's another method                                 which what we call unsupervised                                 which doesn't rely on any cross tracking                                 so as long as we have data from shop one                                 and data from shop B let's say banana                                 republic and gap as long as you have                                 those data separately you can use the                                 method we propose to align this space                                 and then you can do basically zero shot                                 inference okay thank you so our next                                 question is from Jim Walker you said you                                 had a slide referencing an industry                                 baseline you were able to exceed by                                    times can you clarify what that                                 benchmark is yes surely I mean by it by                                 industry baseline a means a noisy                                 channel models when like when dipandi                                 when the when the language model is                                 estimated as a pure language model so                                 not                                 our model and is estimated purely from                                 empirical frequencies yeah so that that                                 would be will be I mean I would say as a                                 reason approximation of like non                                 sophisticated approach in the industry                                 but also the one that's clearly more                                 common as all the other baseline are                                 deep learning based which are way more                                 sophisticated in fact are more accurate                                 than than this simple heuristic but then                                 if you actually go and see what                                 retailers are doing nowadays the number                                 of retailers they will actually have                                 already deep learning in place would be                                 like a like a like incredible minority                                 so when I say industry I understand is a                                 generalization but I think I mean this                                 is landed in my experience in the                                 individual business okay thank you okay                                 while that answers your question                                 andreas factor how do you handle the                                 cold start problem for new words phrases                                 new brands new product names D so there                                 are two things you can do so what one so                                 everything we do in the in the NL in the                                 type I'd space is based on what it's                                 normally known as the retain memory rank                                 kind of idea so you have a very fast                                 model that approves first a bunch of                                 suggestion and then use deep learning or                                 very sophisticated stuff to Ray rank                                 things so since you cannot really                                 suggest things that didn't appear before                                 if there's not a probability there that                                 doesn't really that doesn't really                                 happen on the linguistic side so the                                 cold start problem for the linguist if I                                 is not really a problem for products is                                 more interesting we have a public web                                 and other like publication in in machine                                 learning conference coming up and the                                 idea is that you do two things first you                                 train a proctor back model based on all                                 the interaction of your website then you                                 discard the vectors that has been                                 generated for rare sk use or of course                                 new SKU won't even be there but then use                                 another model to basically fake or infer                                 the position of these new or rail                                 vectors into the space so for that                                 without weather long it was a longer                                 another talk another paper to to present                                 about be super happy to share you I mean                                 if you reach out to me like privately I                                 can share the draft of the work that's                                 are saying thank you                                 diva asks if you conducted online                                 evaluation in addition to                                 of an evaluation for this one not not                                 now so we did online evaluation for the                                 I don't know if you still see my screen                                 do you see McLean no no okay okay but                                 yeah so at some example screens visible                                 now okay yeah okay so that's a no point                                 like I mentioned the fact that you can                                 use the same kind of architecture with                                 some twisties obviously but you can use                                 that to personalize the category in                                 type-ahead for that we did some online                                 testing and and improve like                                 statistically significant uplift of                                 course the magnitude of the of leap will                                 heavily depend on how good the system                                 was in the first place so if we're                                 always testing against ourselves which                                 we are already using deploy which were                                 ready using good models the uplift is                                 marginal but if you think of applying                                 that to a standard numbers MySpace line                                 the uplift would be of course much                                 bigger okay okay thank you not in US how                                 do you detect or handle the change of                                 user intent I are you searching for two                                 related items in the same session like a                                 shirt and a pair of trousers that's a                                 very good question so what happens when                                 you look I mean at least when you look                                 at at the typical customer like it sites                                 customers say people making I don't know                                 you know between between twenty-five and                                 a hundred million dollars online                                 revenues so Alex are ranking between ten                                 thousand a hundred thousand just to give                                 you like a frame of reference so bid to                                 make ecommerce what we find out at                                 leasing the verticals that we know very                                 well is that change of intent for people                                 is not as frequent as you may as you may                                 imagine so when people go on a web site                                 like a lot of time they tends to be in                                 the sport opera busy for games they tend                                 to be mono activity that's not always                                 true though so and what what happened                                 there you can do a bunch of things you                                 can either take instead of taking                                 imagine imagine following this person in                                 the product space right now basically                                 what you're saying changing of intent                                 meaning that you go from one part of the                                 product space and you suddenly go to                                 another part so what you can do when you                                 build this representation you can either                                 discard all the information so you can                                 for example weight the vectors you know                                 by recency like you know like                                 exponential discounting you know older                                 older product you interacted with so                                 this session will very much react to the                                 latest thing you're doing so that's one                                 option the other option again because                                 encoder/decoder is so flexible is that                                 you can replace the encoder we propose                                 which is the centroid of all the                                 products you interacted with and you can                                 actually use the net HTM that as well                                 you can basically do sequence scenes and                                 sequence out so now the model will be                                 able to condition is give you basically                                 the entire sequence of product that you                                 that you fit in we add benchmarks on                                 that but the truth is is that it's                                 marginally in our cases it was                                 marginally better than the average                                 account without waybig like we don't                                 women were more sophistication and                                 complexity in training and the point so                                 we drop that but in theory if you have                                 enough data and your use case supported                                 it's a simple change in the encoder part                                 okay okay there's one final question                                 here another one from who says do you                                 think the encoder decoder network we                                 work for other inputs like documents for                                 inputs as in we can encode documents you                                 interacted weeds and and then you                                 generate language yes                                 the problem with documents is that the                                 representation is Oracle is less                                 straightforward then with either images                                 which is you know CNN and that's                                 basically done or brought - wette prod                                 vectors which is again like prod two                                 vectors now I would say it's it's a                                 fairly stable                                 it's a FedEx stable technology used by                                 yes if you have short documents you know                                 collection of sentence in source one and                                 you want to use like the latest encoded                                 metal X a sentence bird just to make up                                 like a practical example you could                                 probably do the same like if you have                                 user interact with documents you encode                                 the documents that user interact with                                 with sentence bird and then you feed                                 that as your encoder and then you should                                 be able to get realistic type I'd                                 completion based on the on the on the                                 quality on the semantics of these                                 documents but the overall quality of                                 what we did is heavily dependent on the                                 quality of the representation of the                                 intent which in our case brought back                                 work exceedingly well and we had                                 extensive tests before going down that                                 road so we we made sure first that                                 Proctor vac really capture what's what                                 you know the latent dimensions of                                 products in commerce and then we went                                 down the road if you can get the same                                 level                                 assurance in document space for your use                                 cases there's no reason to think that it                                 didn't work okay fantastic                                 thank you so much a couple I think we're                                 done with our questions thanks to you                                 I'm going to bring this to a close thank                                 you very much for your talk thanks much                                 you
YouTube URL: https://www.youtube.com/watch?v=PFfSiE4CGPY


