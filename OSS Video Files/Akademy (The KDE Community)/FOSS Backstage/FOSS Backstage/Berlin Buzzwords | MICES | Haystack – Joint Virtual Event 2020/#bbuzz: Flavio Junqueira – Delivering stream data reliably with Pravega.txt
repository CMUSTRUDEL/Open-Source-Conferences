Title: #bbuzz: Flavio Junqueira – Delivering stream data reliably with Pravega
Publication date: 2020-07-08
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/delivering-stream-data-reliably-pravega

Pravega is storage for streams. Pravega exposes stream as a core storage primitive, which enables applications continuously generating data (e.g., from sensors, end users, servers, or cameras) to ingest and store such data permanently. Applications that consume stream data from Pravega are able to access the data through the same API, independent of whether it is tailing the stream or reprocessing historical data. Pravega has some unique features such as the ability of storing an unbounded amount data per stream, while guaranteeing strong delivery semantics with transactions and scaling according to changes to the workload.

Pravega streams build on segments: append-only sequences of bytes that can be open or sealed. Segments are indivisible units that form streams and that enable important features. Segments enable Pravega to spread the load of streams across servers, to scale streams, to implement transactions efficiently, and to implement state replication.

In this presentation, we overview the architecture of Pravega and its core features. To enable reliable ingestion of events, we discuss how to use transactions along with state synchronization to guarantee that events from memoryful sources are ingested exactly once. Such a property is important to guarantee correctness when processing the data.

Pravega is an open-source project hosted on GitHub, and aims to be a community-driven project. It is under active development and encourages interested contributors to check it out and interact with the developers.
Captions: 
	                              um hello everybody thanks for joining                               the station today i hope everyone is                               safe and sound                               um so my name is flavio i am a senior                               director and senior distinguished                               engineer now                               i have been working on this project                               called provega for the past few years                               with my colleagues that i'm                                in in the team and today i will be                                giving an introduction                                of provega i'll tell you a bit about                                what it is and                                and and its architecture its core                                features                                and then i'll focus on one specific                                problem which is the one of ingesting                                data                                in with exactly one semantics using                                the some of the features and mechanisms                                that we expose in provega                                so that's the the core of the                                presentation today                                first let me give you a few key points                                about provega                                i want i wanted to remember some of                                these terms because i'll be talking                                about them                                in more detail across the top and these                                are some of the things i'd like you to                                remember                                after this presentation so pervega is a                                state a stream                                store is a storage system that exposes                                a stream as as its main primitive                                the architectural project is based on                                the concept of or the abstraction of                                segments                                and the reason why segments are                                interesting is because they enable                                us to flexibly compose streams                                and to implement some very important                                features                                when it comes to uh storing streams                                and embedding such storage in in data                                pipelines                                like scaling streams transactions                                and even state replication proviga is an                                open source project                                um i haven't slide two                                important urls one is our website                                provega.io                                and the other one is our vega repository                                on github                                so if you're interested uh go visit and                                and check it out                                in this talk there will be um in broad                                terms four                                sections one the first one i will                                cover data streams i will you know i                                know perhaps a good chunk of the people                                if not everyone                                in the audience knows what or has some                                notion of what a data stream is                                but i want us to be on the same page so                                i'll talk a bit about data streams from                                from my perspective then i'll do an                                introduction of                                chopra vega including talking a bit                                about its architecture                                then i'll focus on the specific problem                                that i mentioned which is the one                                of ingesting data in an exactly once                                managed to upper vega stream                                and then uh hopefully if time allows                                i'll give you some i give you a quick                                demo and show                                you some some code all right so let's                                talk a bit about data streams                                so when i think about data streams what                                i'm referring to                                is um data that is coming from sources                                that are continuously generating it                                and it could be end users in traditional                                applications                                uh that you can think of like social                                networks or users that                                are doing you know shopping online and                                they are uh purchasing and their online                                transactions all those                                can generate a continuous flow of                                of events which constitutes a                                an event stream um or one or more                                advanced streams                                but it's not only about end users it can                                have machine generated data                                i can have servers that are continuously                                emitting telemetry                                i can have tools and services that we                                 use on a daily basis                                 jira gets jenkins all those                                 all those outputting messages                                 continuously that we want to ingest and                                 process and perhaps visualize                                 and then we have another set which is uh                                 a hot topic it has been a hot topic for                                 a few years it remains a hot topic and                                 will continue to be given                                 the the expansion of the of the the                                 space of applications                                 that care about it which is iot uh and                                 which is related to edge and in those                                 you have                                 maybe sensors devices that are                                 continuously emitting                                 um uh samples uh                                 events it could be yeah so it sounds                                 simple events continuously meeting data                                 that                                 you might want to broad you might want                                 to ingest and not in processing various                                 ways                                 you might wanna you might wanna have a                                 digital twins application in which you                                 have                                 a um a digital mirror of devices and                                 group them                                 and visualizing uh in different ways so                                 all those are                                 examples of sources of continuously                                 generated data                                 and how those mapped to uh to the notion                                 of streams that i've been talking about                                 so the landscape that uh that we put                                 prevegan to is this one in which we have                                 uh on the left hand side a                                 distinct sources of of continuously                                 generated data                                 as i mentioned could be end users but be                                 machine generated                                 uh like sensors                                 connected cars servers                                 any of these things we ingest that data                                 and we process it now processing it                                 might require my requirements for steps                                 it's not necessarily ingesting the data                                 processing can be done                                 you can have multiple stages of of                                 storing the data and processing the data                                 and outputting and storing some more and                                 processing it again                                 so you could have multiple stages of                                 that                                 but the ultimate goal could be a number                                 of things again it could be                                 visualizing the the data that is being                                 that has been generated uh it could be                                 its raw form it could be um different                                 ways in which makes it more intuitive to                                 understand                                 what um what what's happening in                                 in in a fleet of servers and set of                                 devices                                 um it could be alerts about bad things                                 that are happening in the system it                                 could be                                 um it could be insights                                 about about users again about devices                                 about the environment                                 uh recommendations uh actionable                                 insights all those are valid outputs for                                 uh                                 for such data pipelines                                 now when it comes to use cases that we                                 have                                 uh that we have for prevega one                                 one type of use case that we have been                                 seeing quite frequently is the one of                                 drones                                 which are emitting telemetry and uh and                                 video streams                                 so there are from drones you're seeing                                 at                                 at least two streams again one of which                                 is video the other day is telemetry                                 you want to ingest that data you want to                                 process it                                 and and often you want to process the                                 data                                 as it's been generated so you're                                 essentially taking the stream                                 but also you want to process                                 in retrospect so perform historical                                 processing                                 over the data data that has been gested                                 uh some time back                                 and you want to go back and either                                 reprocess it or perform some kind of                                 historical processing from a                                 from scratch and the reason why various                                 companies                                 want to do this are they go from up                                 inspecting the health of of your cattle                                 to                                 uh airplanes between in between flights                                 looking for structural damage on a                                 on such airplanes so if there is                                 substantially                                 another type of use case is the one of                                 industrial iot                                 where you have um where you have cameras                                 you have sensors                                 they owe emitting data continuously and                                 again the the principle                                 of ingesting the data processing the the                                 fresh data                                 and perhaps going back to the to the                                 other data and performing historical                                 processing                                 is very appealing to a lot of these use                                 cases so inducer iot we have seen                                 a number of um a number of use cases                                 that also have                                 such requirements                                 now stepping back and thinking a bit                                 more abstractly about                                 about data streams let's um let's talk                                 about what are the what are some                                 of the core properties that we would                                 like or we observe when talking about                                 streams                                 so the most intuitive way of thinking                                 about a stream is uh                                 one sequence of uh of events as                                 new events come in you append those                                 events to the                                 to to that stream so it has it has a                                 head at the beginning of the stream it                                 has a tail                                 which is where you are appending the the                                 new events                                 but it's not as simple as that because                                 we can have                                 a fleet of servers a fleet of a a group                                 of                                 of of sensors and all emitting samples                                 at the same time so a stream looks                                 more like this where we have a certain                                 degree of of parallelism                                 but it doesn't stop there either because                                 the                                 traffic can fluctuate traffic can                                 fluctuate because of uh                                 of uh daily cycles weekly cycles of                                 regular cycles                                 the in the in the generation of data                                 uh it could be also uh                                 occasional spikes because introduced                                 more devices introduced more servers                                 uh or maybe a chunk of them have been                                 decommissioned or crashed                                 so all those could induce fluctuations                                 in that uh in that                                 in that traffic and so it's not                                 just a constant uh a constant rate                                 for the stream all the time it could                                 observe the fluctuation of dropping                                 or increasing                                 these streams are also unbounded right                                 so you can you can have stream running                                 for                                 for as long as you like it's uh we were                                 talking about data that is being                                 generated continuously so                                 it's going to be generated for as long                                 as the sources are there                                 and ideally we do not make an                                 artificial split between the old data                                 and and the fresh data which i'm going                                 to call                                 the lambda way in in reference to the to                                 the architecture                                 so ideally we make such a distinguished                                 distinction between fresh and historical                                 data                                 and when we expose the abstraction of                                 the stream                                 uh the stream encompasses all the data                                 you need from it                                 the historical part of it and uh and the                                 tail part of the                                 of the of the stream                                 but all of that is about the right side                                 of the stream so appending to the stream                                 creating the stream but also we need                                 when we think about a system that                                 provides                                 the ability to users to                                 um to store such a stream we also need                                 to give the degree                                 of reading such a stream in a way that                                 is uh                                 that uh that is uh not only convenient                                 but it meets the requirements of uh                                 of applications and so if                                 if we are to fluctuate the traffic and                                 scale a stream then we need on the right                                 side we also need to provide                                 that same capability on the on the read                                 side                                 so in the end if if i group all those                                 characteristics that i just                                 mentioned uh go back to the notion of a                                 stream and and                                 and prevega the key thing that we're                                 trying to do with provega is to provide                                 a storage system that gives up                                 properties that are not only are                                 desirable from the perspective of an                                 application                                 performing stream analytics or                                 processing streams in a                                 in general but also that uh that those                                 properties they meet                                 um the characteristics of a cloud native                                 application so they provide                                 the they provide the the notion of                                 unbounded data for a stream so stream                                 can                                 accommodate uh as much data as its                                 storage systems allow it uh                                 it's elastic so it can fluctuate it's                                 its capacity according to them                                 according to the incoming traffic it's                                 consistent                                 it provides semantics that allow allow                                 it to be correct when you process the                                 data and                                 it enables the the applications                                 both to tail the stream or process the                                 fresh data                                 and perform historical processing the                                 all the data                                 of of data analytics of um of the stream                                 that's what i'm referring to when i say                                 cloud native in the context of                                 of storage for data streams                                 let's now talk a bit about about vega                                 remember i mentioned right in the                                 beginning that one key concept in                                 provega is the one of a stream segment                                 a stream segment is a stream of bytes                                 it's an appendage data structure weapon                                 bites to it                                 notes that i mentioned events many times                                 and now i'm talking about bytes                                 so in vega the segments they do not                                 store                                 segment events directly they store uh                                 the bytes of the events                                 and to get to the bytes we expect the                                 serializer to be given so that                                 we can we can perform that                                 transformation                                 now we with segments we                                 we can provide parallelism so we can                                 have a number                                 of segments in parallel and                                 from a writer perspective we the writer                                 uses routing keys                                 to map an append to a given segment                                 and that's important because we provide                                 per key order                                 now one thing that is interesting in                                 provega                                 is that that parallelism that degree of                                 parallelism is not static                                 a streaming pro vega can scale up and                                 down and so this                                 i'm showing a representation of a stream                                 that starts                                 with uh with two segments remember that                                 i'm starting my stream from uh                                 from from the right side from the head                                 it starts with two segments                                 then scales up and and goes it becomes                                 five                                 five segments and then it scales down it                                 becomes three so this kind of dynamics                                 um is a feature that uh that we provide                                 in provega and                                 uh streams can scale that way according                                 to a policy that um                                 that is described when you create a                                 stream                                 another feature that we can provide with                                 segments is transactions                                 so when an application comes and creates                                 a transaction                                 what provega does internally is to                                 create temporary transaction segments so                                 the green                                 segments on a on the slide correspond to                                 the temporary transaction segments                                 it's temporary in the sense that uh it                                 don't exist until the                                 the transaction commits or aborts so all                                 appends                                 done to the transaction uh while the                                 transaction is open                                 are going to go to those temporary                                 transaction segments                                 and when the application decides to                                 commit then those transactions are                                 merged to the main                                 segments of the stream in which case the                                 data becomes                                 becomes available from that point                                 onwards or the application can                                 decide to abort and the transaction                                 segments are simply                                 discarded and in that case the data does                                 not become visible at all                                 and while this transaction is ongoing it                                 does not block                                 the data in the in the the mainstream                                 segments                                 finally another interesting um                                 another interesting feature that i'm                                 going to use for the exactly one                                 suggestion case                                 is the one of uh of state synchronizers                                 persisting estate in in prevega and in a                                 way that i can be shared and synchronize                                 across                                 a set of processes uh we do that by                                 having a special feature of um                                 of streams which is performing                                 conditional pens so we have this                                 abstraction of a revision stream                                 and the revision stream is what the the                                 state synchronizer                                 which is again this abstraction that                                 gives you the ability of processing                                 states                                 across prostities uses so state                                 synchronizer builds on                                 on these revision streams and enables us                                 to um                                 to uh to update state in a consistent                                 manner                                 and share across processes if if needed                                 let's now talk a bit about the the                                 private architecture um                                 the improv vega if you are                                 ingesting data you can in and you're                                 ingesting using the event api                                 you use an event writer so the                                 application is an event writer one or                                 more                                 to append events to um to the stream                                 the proviga checks the writer position                                 in the case that uh that we have                                 disconnections and we need to reconnect                                 so that uh the writer knows where to                                 resume from                                 then the application reads from uh from                                 that stream using event readers                                 we group the event readers into reader                                 groups and                                 those readers split the load of of                                 segments                                 among them and they use the state                                 synchronizer to perform that uh that                                 kind of coordination                                 so we can add in and remove readers                                 as as needed so that gives us that bit                                 of growing and shrinking                                 uh the capacity of uh of of a reader                                 group                                 and the readers were given the presence                                 of stream scaling remember that i                                 mentioned the stream scaling feature                                 where                                 the degree of of parallelism in the                                 stream                                 can change over time so prevega takes                                 care of of preserving the order                                 of of segments that are that are                                 generated                                 and and that's um that assignment of                                 of segments as they are created and they                                 are sealed that's performed                                 internally by uh by the logic of uh well                                 by providing by the logic of                                 of the reader group so the reader groups                                 work even in the presence of uh                                 in preserve order even in the presence                                 of uh of autoscape                                 we have um so there is the we prevent                                 the control plane in the data plane                                 um with the controller and the segment                                 store                                 so the controller is the component that                                 is responsible for the lifecycle of                                 streams                                 it takes requests to create a stream to                                 delete a stream                                 and also takes care of transactions so                                 if if an application creates a                                 transaction appends to it and then                                 commits the control                                 will be responsible for performing the                                 merges                                 and the segment store is unaware of                                 streams the segment store all the deals                                 with the life cycle of                                 segments stores the the segment data it                                 performs                                 merges create segments delete segments                                 as needed                                 now both the controller and the segment                                 store are stateless                                 they pres they keep states in uh                                 in a storage tier which has uh three                                 main components                                 uh we use zookeeper for some uh                                 coordination                                 tasks we do not use the keeper for                                 metadata                                 so the the we do not rely on zookeeper                                 for metadata data that can grow without                                 bound so                                 as we add new streams uh the state that                                 is kept in zookeeper does not                                 does not grow accordingly now in the                                 segment store                                 we rely on on both                                 bookkeeper uh for what we for for our                                 journals                                 so as data come in we store data in uh                                 in journal for uh for for the sorry                                 bookkeeper ledgers                                 uh for durability and we                                 asynchronously flush the data to                                 long-term storage we can be                                 implemented with fire or object we give                                 different options for their long-term                                 storage                                 inside the segment store we have the                                 concept of of segment containers the                                 segment container                                 is it is not to be confused with the                                 linux containers                                 is a grouping of segments is the units                                 we use uh of work assignment across                                 segment store instances so given the set                                 of uh                                 of uh segment containers in the system                                 uh the controller will be responsible                                 for                                 assigning all those segment control                                 containers across the the segment store                                 instances                                 and this is the next point that i want                                 to quickly touch upon so                                 the controller uh knows the segment the                                 second containers that                                 that are in the system and and it                                 assigns                                 the existing segment containers to the                                 to the existing segment store instances                                 and in the case say that we add a new                                 segment store instance the controller                                 will be responsible for performing the                                 the reassignment                                 so that it balances the load across the                                 the existing                                 segment stores                                 now looking at the right and the read                                 path let's look more closely and see                                 what's going on so                                 on the right path an event stream writer                                 if he wants to append                                 events then he first will talk to the                                 controller                                 and see which segment store is supposed                                 to connect to                                 once it does it it will connect to the                                 segment store and start appending bytes                                 the segment store won't acknowledge to                                 the event writer                                 until it has persisted in apache boot                                 keeper                                 and once it does it can acknowledge um                                 but the data is                                 immediately written to long-term storage                                 that data is                                 synchronously flushed to uh to that                                 tiered storage so apache bookkeeper is                                 there                                 to guarantee durability we we use it                                 because we want to guarantee                                 low latency in the presence of a of                                 small rights                                 uh but then we rely on long-term storage                                 to keep data                                 for uh for for a longer period of                                 periods of time remember that one of our                                 goals                                 is to store an unbounded amount of data                                 per                                 per string and for that we have again                                 different options                                 it can be based on file or objects and                                 this is configurable                                 at the deployment time                                 for the read path uh we it's similar the                                 event stream reader needs to contact the                                 controller to know which segment                                 store to talk to um the segment store                                 has a cache where it keeps the data                                 that uh that is supposed to flush to                                 long-term storage                                 and that's considered to be the detail                                 of uh                                 of the streams or of the segments that                                 that is managing for the segment                                 containers it has assigned to it                                 so when an event stream reader submits a                                 request to read data                                 from                                 okay something have                                 something happened to my to my um                                 to my headset i don't know if uh if i'm                                 audible                                 um okay so sorry let me go back                                 all right so let's talk about the read                                 path so on the read path again you                                 the event stream reader contacts the                                 controller then it talks to the to the                                 segment store                                 request data from the segment store if                                 it has in it                                 in the cache it's going to return from                                 the cache and this is suppose                                 this is expected to happen in the case                                 that uh the reader is stealing                                 the the stream if not if it's a cache                                 missed then we'll                                 you read data from from long-term                                 storage and uh                                 and populate the cache and we'll serve                                 from there                                 note that um we never serve reads from                                 apache bookkeeper today                                 so that data in bookkeeper is used only                                 upon                                 recovery so for example when we move a                                 segment container to a different segment                                 store                                 uh or when the segment store uh give us                                 given segments for instance                                 um restarts uh a segment container                                 it can repopulate the cache or with uh                                 with uh or in its metadata from the                                 apache boot keeper lighters                                 okay so that was an overview of uh                                 of prevegan so remember that i talked a                                 bit about data streams and uh and some                                 core properties uh then i covered                                 provega i talked about some of the core                                 features                                 and then i i talked a bit about the                                 architecture of prevega                                 now i'm going to switch gears and talk                                 about one concrete problem which is the                                 one of uh                                 ingesting data in a in an exactly once                                 manner to a pro vegas stream                                 so the setup looks like this i have a                                 data source                                 which can be a number of things it can                                 be an end user it can be again                                 a server it can be a sensor it can be uh                                 even perfect                                 itself uh or files uh                                 in a file system i have an application                                 that is receiving this data is reading                                 this data                                 and it's using an eventwriter to append                                 data to                                 a vegas stream so that's that's the                                 the setup now                                 there are a couple of uh in broad terms                                 two source two two data source types                                 that we care about                                 and i'm going to call them memory less                                 and memory full                                 so memoryless are the sources that are                                 not capable of reference meeting events                                 so they produce events they emit it and                                 they forget about it there's no                                 buffering there is no                                 no no caching there is uh no persistent                                 store                                 right so we cannot guarantee exactly one                                 semantics for such sources                                 in the process of process crashes so                                 we'll see what we can do                                 and one example would be stateless                                 sensors that again emits samples and do                                 not                                 um uh and do not keep them                                 and then there's the memory foam version                                 of it which are data sources that are                                 capable of retransmitting from any given                                 offset                                 so from a position that you can refer to                                 uh of course you might not be able to                                 arbitrarily go back in time so that's                                 probably                                 that's possibly bounded but from uh for                                 all practical purposes                                 it's uh you should be able to resume                                 from any point that uh that you need to                                 so that's the assumption we're making                                 for such data sources                                 many examples would be uh would be files                                 or uh or                                 vegas streams all right so memoryless                                 sources                                 so from memoryless sources um let's say                                 a sample                                 a sensor so it emits three samples the                                 application                                 starts receiving them and appending them                                 to a chopra vegas stream                                 so the application uh takes                                 blue and reds and uh and makes requests                                 you append them                                 now let's say that uh the connection                                 breaks at that point                                 right so what happened to the events                                 what has gone through and what has not                                 gone through                                 so to deal with that uh we                                 we do the following and here are four                                 steps that uh that i i want to show                                 in this slide so the first step is uh                                 the writer                                 appending the blue events there is                                 internally there is a notion of a writer                                 id                                 and the segment store keeps a mapping of                                 the writer id to the event that has been                                 successfully appended                                 so when blue is successfully appended uh                                 it sets the event number to one and                                 acknowledges                                 back to the reader now in uh                                 in in step two the writer                                 submits the red events but the                                 connection breaks                                 at that point and he doesn't know if red                                 event has gone through or not                                 uh it happens that he has gone through                                 the segment store uh has a record of it                                 but the writer doesn't know so so                                 in that case uh what the writer is going                                 to do is it's going to                                 start a new connection as part of that                                 you have you do a handshake with the                                 segment store                                 and you tell its writer id the segment                                 store will send back                                 the last event that has been written now                                 the writer knows that uh the red event                                 has been written so he moves to the                                 black events                                 and uh and uh wrist and submits the                                 black event to uh                                 to be appended and now it's acknowledged                                 the                                 the the application is good to go right                                 so the application                                 has appended everything that uh that he                                 needed                                 now what happens if instead of the                                 connection breaking the application                                 crashes so in that case                                 um the application can                                 can restart but it doesn't have                                 a way of of getting the the it doesn't                                 have a way of determining what has been                                 written it doesn't have a way of                                 of getting from the sense of what's                                 missing so the sensor can be transmitted                                 by by design so let's look at what we                                 can do                                 with memory memory food sources now uh                                 because we can do                                 some interesting things there so with                                 memory food sources let's assume that                                 such a source is capable of rewinding                                 so it's based on some notion of of                                 offset                                 a simple example of such a source would                                 be a set of files that we want to ingest                                 in in order and the offset can be                                 a file name and a file name and enough                                 an offset                                 for that file there could be more                                 complex                                 examples of such a source it could be                                 say a flink job where                                 an offset could be a a job snapshot                                 for the sake of example in this                                 presentation i'll focus on                                 files as a data source okay                                 files as a data source the application                                 starts reading                                 it it reads the first two events blue                                 and reds it depends then                                 and then the application crashes right                                 so the question is which events have                                 been successfully                                 appended to the to the stream so the                                 application doesn't know                                 so what we can do here is to introduce                                 transactions                                 so in the way we introduce transactions                                 the application creates a transaction                                 before it appends anything                                 inside provider when it creates a                                 transaction it will proviga will create                                 the transaction segments as i have                                 described before                                 then it will append events to the                                 transaction                                 um the and once it has done it                                 it commits the transaction and now all                                 events become available as part of                                 of the mainstream                                 then for the remaining events it creates                                 a new transaction                                 it appends the remaining events                                 it commits the transaction and now we're                                 done                                 so we have covered the the the whole                                 file right and append all the                                 the the corresponding events but now                                 what happens if the application crashes                                 in the middle                                 of a transaction so before an                                 application has the chance                                 of uh of uh of committing it                                 so let's say that we're in the situation                                 that we committed the first transaction                                 but we have we haven't committed the                                 second transaction                                 and then the application crashes so how                                 does it know uh up to what offset has                                 been successfully                                 committed um it doesn't right so it                                 needs                                 some state to determine uh where where                                 the application was                                 and even if there is an outstanding                                 transaction so that's where we                                 introduced the state synchronizer                                 so the state synchronizer is an                                 abstraction available as part of the                                 client api                                 it enables the coordination of state                                 across process uh when i say process um                                 you know it can be application instances                                 we do use it internally as part of our                                 reader groups to coordinate the actions                                 of                                 of the readers in a in a reader group                                 but again it's also exposed through the                                 api and applications can use it                                 the processes that that use the state                                 synchronizer they can update states                                 conditionally and they can read state                                 updates to update the                                 their um their local state in our case                                 in in our application in this example i                                 will persist states                                 using the state synchronizer interface                                 and the state is a simple pair                                 of starting file set offset and                                 transaction id                                 so now the application will create a                                 transaction before it does anything                                 it will update the state in the                                 synchronizer saying you know i met                                 offset                                                                                                transaction id with                                 with this id and then you will perform                                 the                                 appends and then we'll commit the                                 transaction                                 and for the remaining events it will                                 create                                 a new transaction updates the state of                                 the synchronizer                                 update events and then commit the                                 transaction                                 and now what happens if the application                                 crashes before                                 completing the transaction so if the                                 application crashes                                 before completing the transaction so now                                 we have a transaction open                                 the events have been appended but the                                 transaction is open uh the application                                 will reads                                 they will recover and as part of their                                 recovery will read the state of the                                 state synchronizer                                 it will determine that uh it would check                                 the status of transaction with the                                 because it has the i the transaction id                                 it will check the the status of the                                 transaction against provega                                 it would tell provigo to abort the                                 transaction                                 and it will start over so create a                                 transaction                                 append the the state of the synchronizer                                 append                                 the the the remaining events and then                                 commit the transaction                                 and now our job is uh is done for for                                 that file                                 okay so now i wanted to give a quick                                 talk a bit about codename and give a                                 quick demo                                 um so                                 the simple code that i have to show so                                 you have the url                                 in the at the bottom of uh at the bottom                                 right of the slide                                 and what this code does is the following                                 uh in in broad terms it iterates over a                                 set of                                 of sample files uh and we perform one                                 transaction per file to simplify it                                 uh there's no bound in the amount of                                 data in a transaction so                                 it doesn't matter the size of the file                                 even though this can be calibrated                                 uh if it's if it's uh it's small enough                                 then uh though doing one five                                 transaction is uh                                 is is perfectly reasonable and we commit                                 zones once                                 all the events have been written then                                 it's simple code also implements a state                                 synchronizer                                 upon creating a new transaction it                                 updates the synchronized states                                 the state is a single value an instance                                 of status where status is                                 uh is a file id transaction id                                 upon recovery it fetches the latest                                 status abort any outstanding transaction                                 and and sets the the start file                                 this is how the streaming initialization                                 look like there are two streams                                 one stream for the state synchronizer uh                                 and one for the data stream                                 now note that we have we are setting the                                 scaling policy for both streams                                 uh in this case we don't care about the                                 auto scaling                                 and so we are just setting a fixed                                 number of segments so                                 one for the safe synchronization because                                 we don't need more and                                 uh actually you can't use more and uh                                 and for the data stream we're setting                                 a fixed number of ten segments                                 then we iterate over the files so given                                 the list of files                                 uh for each file uh for each file we                                 begin a transaction we update the state                                 of the state synchronizer with the file                                 id                                 and um and the transaction id note that                                 uh                                 if we're recovering you know if this is                                 a restart and we're recovering then uh                                 then this is going to skip some files                                 and and now once we start a file we                                 write the events of a file to the                                  transaction and then we commit it                                  implementing the synchronized the                                  synchronizer is is also simple we need                                  to define what the status                                  uh the state i'm calling it up data both                                  status here                                  then we need to define what is the                                  initial states and and how                                  it performs state updates and all those                                  are                                  uh defined in those in those three                                  classes                                  and then we have access methods that uh                                  that that's what is                                  exposed to the applications how the                                  application interacts                                  with with the synchronizer class                                  for recovery once we once the                                  application starts                                  it gets the state of the synchronizer it                                  would check the status of                                  of the outstanding transaction if it's                                  too open then the application aborts it                                  if it's supporting then uh then uh it                                  let it be it's uh                                  it um it starts from uh from the given                                  file id                                  because it assumes that it hasn't been                                  done yet now if it's committed then it                                  moves                                  to the to the to the next file                                  aborting the transaction as in the open                                  case is not strictly necessary because                                  provega would eventually time out and                                  and uh and                                  remove it but it's good practice to                                  remove any data data that is unnecessary                                  so now i want to give a demo of the                                  approach that i have described and i                                  want to show the flow                                  of the of the sample code that i i've                                  just talked about                                  so let me talk a bit about the demo                                  setup first                                  i will i'll have a bunch of pre-created                                  input files                                  uh specifically i have                                                files                                  each file has a thousand samples                                  each sample is another object                                  representing a simple event                                  that has three fields then i will ingest                                  the content of those files into                                  a vega stream using the technique                                  i have just i have described using                                  transactions in a state synchronizer                                  and then i use a flink job just to check                                  the state of uh                                  of the stream and and visualize it                                  through uh through the dashboard                                  so let's go let's go watch the demo                                  let's first check that out all the files                                  are there                                  that we can ingest them into a vegas                                  stream                                  so all the files are there as i                                  mentioned they have been pre-generated                                  now the next step is to start pro vegas                                  standalone                                  i will start by vegas send them on and                                  leave it running in the                                  in the background and then                                  the next step is to run the process that                                  is going to read from the files                                  and ingest it because it's going to be                                  using transactions we'll see                                  messages in the output making references                                  to the transactions it's creating                                  uh and committing now this is going to                                  be                                  using one transaction file so we'll read                                  the content of a file                                  right in as part of a transaction and                                  then commit it                                  so again we'll see one transaction per                                  file                                  so let's get it to run uh we should                                  shortly start saying it uh                                  it's starting the transactions all right                                  so it's starting the first transaction                                  um i have slowed down the the the the                                  transactions so that we can see the                                  output otherwise it would go too fast                                  uh so it's going as i mentioned one file                                  at a time                                  now let's let's break this let's break                                  the flow                                  uh so that we see what happens so if you                                  break the flow what should happen is                                  uh because i i'm trying to ingest it in                                  exactly once manner                                  i don't want to repeat data i don't want                                  to have duplicates of of data that i i                                  have already ingested                                  so it should from the states                                  um of the application determine what is                                  the next transaction                                  or the next file to start from so                                  because it committed                                  um it committed up to                                  nine we would expect it to start at uh                                  at                                                                     start at                                                            depending on the on the current state so                                  let's see how                                  yes it starts from                                                    keep going it keeps going                                  so let's do a few more and uh and repeat                                  it                                  so let's control c again let's resume                                  again                                  see where it starts from                                  they should again skip the files that it                                  has already ingested                                  and uh and resume from the one that he                                  has                                  hasn't really committed so it correctly                                  it starts from                                        now let's see the state of the stream uh                                  as i mentioned we'll do that using                                  a think job so let's start a local flink                                  cluster                                  and start a reader                                  to read the content of the stream                                  let's see that's in the                                  in the dashboard                                  okay so the job has started                                  uh we should eventually see                                  we should eventually see the job um                                  reading the                                  the whole                                                that are supposed to be in the stream                                  yes so                                  it updated up to                                                       the the exact number that we expect to                                  see                                  because again we have                                                     file                                  uh that adds up to a total of of                                                                                                            miss anything so                                  this is what we we wanted to show                                  now let's go back to the to the                                  presentation                                  one aspect i haven't i haven't covered                                  in the demo                                  but it's very very important if we're                                  talking about running this in production                                  is concurrency so let's say                                  that the process data is supposed to                                  ingest                                  crashes and we want to finish the job so                                  we would start a new a new process                                  but in distributed systems you never                                  really know                                  things are just slow or they crashed so                                  it could                                  it could happen that uh we have we start                                  a new writer but there's the old one is                                  still                                  still is still there and in such cases                                  we could end up having                                  duplicates to have two processes reading                                  from the same file starting a                                  transaction and                                  and committing it so we would need to                                  change                                  the logic in the in the in the in our                                  program to do that                                  and that's possible because again of the                                  condition updates that are                                  able to do through the the state                                  synchronizer that would avoid                                  duplicates uh                                  with respect to making sure that we read                                  all the content we just saw the content                                  there are two parts to it one is                                  the logic is such that uh no writer                                  skips                                  files so it has if i was to sort it or                                  there are files                                  so guarantees that it goes through                                  everything if                                  if it finishes um but but also we need                                  to guarantee that it finishes                                  in which case we need to use some                                  mechanism for                                  electing a leader let's say so some form                                  of leader selection                                  they will guarantee that uh we ingest                                  all the                                  other the content we're interested in                                  one last topic to to to close the                                  presentation                                  is uh exactly one entrance so i have                                  discussed in this presentation uh                                  exactly one semantics for ingestion but                                  clearly                                  an application is interested in exactly                                  in exactly applications are interesting                                  exactly one semantics end to end                                  not only the ingestion i have                                  covered informal presentations the                                  mechanisms to do it but let's recap here                                  just so that we have                                  we have the full picture so if we if we                                  use the scheme here to                                  ingest in an exactly one smarter using                                  transactions and synchronizers                                  um now we want to have a pipeline where                                  we read                                  data from proviga and uh and we end up                                  outputting to prevega again                                  we can use checkpoints at the source and                                  the transactions at the sink                                  to implement a mechanism that guarantees                                  exactly one sanctuary                                  so applications can do it or they can                                  use                                  say extreme processors which implement                                  that for you so if you write a job and                                  say                                  apache flink that uh and it's using                                  vega as a source and as a sync you would                                  be able to                                  use such such features from pravega with                                  such mechanisms                                  so in flink you have this implementation                                  of a two-phase                                  commit protocol and the way this works                                  with uh with provega is by                                  is by having the master initiating a                                  checkpoint and requesting a checkpoint                                  from provega                                  then running the the distributed                                  snapshotting algorithm def link                                  implements                                  and when the the snapshot completes                                  the transactions that at the sync are                                  able to commit                                  and make the output uh available so                                  that's                                  that's roughly how this um how this                                  works                                  i'm not ready to wrap up so in                                  conclusion                                  what i have covered today is i have                                  started describing                                  provega prevega is a storage system                                  for streams it's one of its main                                  abilities is to                                  provide um applications with uh with an                                  a with                                  apis to read fresh data with low latency                                  and also go back in time and uh and                                  process                                  historically so process historical data                                  of a stream                                  it builds on the concept of segments                                  segments enable a number of interesting                                  features                                  uh we have discussed transactions uh                                  there is auto scaling                                  um um even safe synchronizers                                  so all those come from the fact that we                                  build on this                                  this foundation of segments and                                  provega is an open source project we are                                  currently hosted on uh                                  hosting the source code on github                                  one specific problem that i have covered                                  which is very important for applications                                  that require strong                                  strong consistency guarantees is the one                                  of ingest                                  ingesting data in an exactly once manner                                  so i have described how to use                                  transactions and the state synchronizer                                  in provega                                  to achieve that goal but of course                                  just ingesting is not sufficient so i                                  have also uh briefly touched upon                                  how to uh to obtain exactly one's                                  entrance describing                                  some some earlier mechanisms that i i                                  have talked about                                  in other um in other presentations using                                  checkpoint transactions to uh                                  for exactly one's end to end this is                                  actually implementing apache flink so if                                  you're apache link user you can                                  uh have exactly one sanctuary using                                  provega                                  and flink                                  this uh this is a list of uh i want to                                  leave with uh with a                                  list of resources links to various                                  things that i have covered throughout                                  the talk the provega website                                  the github uh flink connector and                                  the sample code for the demo i have                                  shown today                                  so thank you                                  you
YouTube URL: https://www.youtube.com/watch?v=2KK-KNArcPo


