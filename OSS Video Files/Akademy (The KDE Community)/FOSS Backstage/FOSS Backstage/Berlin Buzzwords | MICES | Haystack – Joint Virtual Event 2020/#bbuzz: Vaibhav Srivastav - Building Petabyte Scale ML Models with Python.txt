Title: #bbuzz: Vaibhav Srivastav - Building Petabyte Scale ML Models with Python
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/building-petabyte-scale-ml-models-python

Although building ML models on small/ toy data-set is easy, most production-grade problems involve massive datasets which current ML practices don’t scale to. In this talk, we cover how you can drastically increase the amount of data that your models can learn from using distributed data/ml pipes.

Detailed Description:

It can be difficult to figure out how to work with large data-sets (which do not fit in your RAM), even if you’re already comfortable with ML libraries/ APIs within python. Many questions immediately come up: Which library should I use, and why? What’s the difference between a “map-reduce” and a “task-graph”? What’s a partial fit function, and what format does it expect the data in? Is it okay for my training data to have more features than observations? What’s the appropriate machine learning model to use? And so on…

In this talk, we’ll answer all those questions, and more!

We’ll start by walking through the current distributed analytics (out-of-core learning) landscape in order to understand the pain-points and some solutions to this problem.

Here is a sketch of a system designed to achieve this goal (of building scalable ML models):

    1. a way to stream instances
    2. a way to extract features from instances
    3. an incremental algorithm

Then we’ll read a large dataset into Dask, Tensorflow (tf.data) & sklearn streaming, and immediately apply what we’ve learned about in last section. We’ll move on to the model building process, including a discussion of which model is most appropriate for the task. We’ll evaluate our model a few different ways, and then examine the model for greater insight into how the data is influencing its predictions. Finally, we’ll practice this entire workflow on a new dataset, and end with a discussion of which parts of the process are worth tuning for improved performance.

Detailed Outline:

    1. Intro to out-of-core learning
    2. Representing large datasets as instances
    3. Transforming data (in batches) – live code [3-5]
    4. Feature Engineering & Scaling
    5. Building and evaluating a model (on entire datasets)
    6. Practicing this workflow on another dataset
    7. Benchmark other libraries/ for OOC learning
    8. Questions and Answers

Key takeaway

By the end of the talk participants would know how to build petabyte scale ML models, beyond the shackles of conventional python libraries.

Participants would have a benchmarks and best case practices for building such ML models at scale.
Captions: 
	                              everyone good evening the                               today to welcome by Bakshi                               so what                               data scientists with Deloitte and he                               will be talking to us about building                               petabyte scale machine learning models                               so on to you alright thanks a lot now                               good evening everyone today I'm gonna be                               talking to you about machine learning                                models with Python all right let me just                                quickly switch to where my screen is                                yeah before I get started first of all                                thank you so much to the boolean                                buzzwords organizing committee I know                                that the times are not that great and                                for you know pulling through this online                                conference it's amazing thank you so                                much hope I am able to justify my                                presence here now let me just give a                                quick walkthrough of Who I am I'm a data                                scientist I work with Deloitte                                Consulting here in India which means                                that right now it's it's it's                                         p.m. my time and I architect machine                                learning workflows and products for                                Fortune Technology to end clients on                                Google cloud I've been I've been working                                in the realm of data or data science                                machine learning for about five to six                                years now and have been building large                                scale machine learning workflows for all                                these friends right now enough about me                                let's talk about the agenda right so                                what I'm going to be talking about today                                with you all is first of all why do we                                need distribute distributed machine                                learning right I mean all the code that                                you put out on your laptops works works                                fine right so what's the specific need                                of distributed machine learning within                                that realm what is out of coal machine                                learning right then we're gonna do a bit                                of deep dive in terms of how do we build                                a scalable machine learning worth you                                and then as as they always say that no                                talk is good without code so we're gonna                                do a bit of code walk through about how                                you can build scalable machine learning                                walktroughs workflows with with dusk and                                with tensorflow and give you ready to                                use code snippets which which which you                                can leverage in your you know                                experiments at your work or for your                                projects and so on right                                then we're gonna try and answer the the                                question of which one should you use                                since we're gonna be talking about two                                approaches and then we're gonna head for                                questions and answers all right without                                further ado now before we we we get into                                why do we need distributed machine                                learning                                let's then do a quick walkthrough of                                what machine learning is right so in a                                typical machine learning example you                                would have some data right for this                                particular use case let's let's take the                                example of predicting whether a person                                has their mask on or not right so my so                                my data would would basically be images                                of people who would have their mask on                                or who would not have their mask on                                right and so I basically have two                                glasses and given a person going through                                my lab through my camera I want to bring                                whether they have their mask on so mine                                so my task is a classification task I                                want to pacify whether they have their                                masks fall or not right and I have some                                images for that now what I'll do is I'll                                like in a typical machine learning task                                I built a machine learning model for it                                and I'll build a model for it it can be                                I can be a linear model it can be a                                Bayesian model can be a you know can be                                a neural network can be a nonlinear                                model and can pretty much be anything                                pick your favorite model for this right                                then the next part off of that machine                                learning workflow would would be                                defining a loss function right what this                                loss function typically would do is it                                would define the quality or like how                                accurate your your your model is at a                                given point of time right and and that                                can be through when it's searching into                                the into into the space of all of your                                data to find the exact equation which                                which defines your your data to its                                 labels right and last but not least                                 comes the optimization comes the                                 optimization procedure right so in this                                 particular case what we'll do is we                                 optimize on minimizing the loss so the                                 minimum the loss the the better by                                 modular P now we haven't touched                                 distributed machine learning up until                                 now but this is how your basic machine                                 learning workflow looks like right so                                 you have some data you have a task it                                 can be classification regression you'll                                 have a modeling you know trying to do                                 that task you have some sort of a loss                                 function which would help your model                                 converge to                                 the best possible or the most accurate                                 model that it can be given the hyper                                 parameters that you define for it and                                 they know then an optimization procedure                                 to get to that to get to that global                                 minima in the in the fastest possibility                                 right now this was all this was all fine                                 right we had we have my machine learning                                 workflow and this works fine on my                                 laptop this works fine on my washing                                 machine but why do I need distributed                                 machine learning right that's where                                 these three components come in right                                 typically you would need distribute                                 machine learning when you have massive                                 data scale right so building a machine                                 learning model on say an iris dataset                                 which has about                                                       easy on your laptop or your workstation                                 right because it can easily fit in your                                 realm right and you can easily build                                 your model on your ram itself right but                                 what if you're dealing with a different                                 problem so for example if we if we take                                 our example further what if you have                                 images of people from from airports from                                 metro stations from there on the streets                                 and you're trying to predict whether                                 they have their mask on or not right                                 that can be terabytes of data but that                                 data would not fit on your particular                                 laptop or on your particular machine                                 reading cluster right because you've got                                 huge feeder scale so typically when you                                 have large data scale that's when that's                                 when you need distributed machine                                 learning right a second s massive model                                 scales right so ever since the vgg                                    model in in image or the GPT models in                                 NLP there's like almost every other week                                 or almost every other month you would                                 see that that the scale of them of the                                 or rather the size of the machine                                 learning model has been increasing                                 drastically right in fact couple of                                 weeks back open air released their GPT                                 three language model which has                                     billion                                 parameters right I mean that model would                                 not fit in in lekha                                 in your memory itself just for inference                                 let alone be able to train or like you                                 know                                 fine given it further so typically when                                 you're dealing with large model                                 architectures or large neural networks                                 for you to be able to train those you                                 need a distributed machine learning                                 workflow right                                 third is is there something which is                                 applicable across the board right so                                 this is something that you would need                                 for for efficient computation of your                                 algorithms right so say for example if                                 you're building a neural network right                                 so we all know that gradient descent for                                 you know we can use gradient descent for                                 finding the best possible parameters for                                 your model right but that but that's a                                 brute-force approach right so that's                                 gonna take a lot of time but similarly                                 you can also use two plastic region in                                 the center right which is which is                                 insanely first and manifold times better                                 than gradient descent in terms of                                 getting the output fine gradient descent                                 would be accurate but it's faster right                                 so how can we make these algorithms                                 efficient for their computation rate or                                 rather how how can we how do we be able                                 to distribute the task itself for these                                 algorithms but them to run as fast as                                 possible right so this is why we need                                 distributed machine learning right now                                 now that we know why we need distributed                                 machine learning let's talk about out of                                 Coromant right so out of core ml as the                                 as the as the name suggests it's it's                                 basically machine learning which machine                                 running which runs at a scale where in                                 your data is is is of higher order as                                 compared to your core itself or meaning                                 your now so it's basically an algorithm                                 which exploits your external storage                                 that can be a hard disk or your buffer                                 buffer memory in order to support large                                 data volumes that cannot be supported by                                 primary memory right so now let's let's                                 spend a minute on this wait                                            ml in like a typical machine learning in                                 like a typical machine learning world                                 would would Venus that you take your                                 data set in our case let's take the                                 images of people whether they have their                                 masks on or off and you batch those                                 images rate you batch those images to to                                 say                                 which is just enough to fit in your ramp                                 it in your in your primary memory itself                                 and then you then you take that data                                 right and then you basically do a                                 partial fit on your model right and then                                 you you give your model more data and                                 let it update its its its weights                                 continuity is the time you've exhausted                                 on the data right that's what out of                                 poor ml is and why it is novel whatever                                 why it's useful is say for example if                                 you have a laptop which which has eight                                 gigs of RAM                                 or if you have a virtual machine which                                 has eight gigs of RAM and your data size                                 is                                                                       can chunk your data in such a way and                                 you can create your feature engineering                                 pipeline in such a way that you would                                 not require to fit all the data in one                                 go                                 that's why out of                                                     we're going to be doing a deep type of                                 this we do not right now going back to                                 our going back to our example right how                                 would this out of core ml fit into the                                 example that we that we spoke about                                 right in the in in the use case that we                                 were talking about out of poor ml would                                 would would look like something wherein                                 I I split my data into two partitions                                 right so there is one data set partition                                 one then there is a data set partition                                 to write my tasks still remains the same                                 which is to classify except now my data                                 set has reduced and this is being                                 operated on two different nodes right                                 which can communicate with each other                                 again my model still remains the same                                 this can be say for example for our                                 image classification tasks we can assume                                 it sir it's a CNN type architecture                                 photo module right now there's a slight                                 update in my architecture here what's                                 changed is the loss function so if you                                 see on the left side the loss function                                 now runs from I equal to                                               by                                                                      similarly on the other side it runs from                                 I equal to M by                                                        like half of the data set is this side                                 and half the data set on the other side                                 right and also what has happened is my                                 optimization role has also updated right                                 so what what's happened to my                                 optimization doing now is that it                                 basically optimizes on                                 loss of tasks one which has the aricept                                 partition one and also for the loss of                                 data set partition two right and then it                                 sounds it up and and and in that way                                 because all of these are communicating                                 with each other you can distribute the                                 tasks across you know two nodes or three                                 nodes or four nodes and still be able to                                 get to the best possible solution cool                                 now that was that was a lot of curium                                 then in in in past ten minutes right                                 let's talk about how this would look                                 like in real life right so there are                                 there are two ways you can you can build                                 out of poor ml or basically scale a                                 scalable machine learning more closely                                 the first is by using incremental models                                 right incremental models or something                                 which would support chunking of data                                 right in a typical scikit-learn lingo                                 you you would basically do a partial fit                                 on your model so this is basically your                                 stochastic gradient descent progresses                                 your random forests your the the neural                                 net within the second library and and                                 couple moves there's an exhaustive list                                 if you search incremental models on                                 cyclone you would find that in fact I'll                                 link it in the presentation as well so                                 as soon as you find it you can get a                                 list of all the second compliant                                 incremental models so what you'll have                                 to do with with model or partial fit is                                 you have to create your custom logic                                 which chunks your data into small small                                 sizes and then feeds it into the into                                 the model right and then with each first                                 the model will start learning about your                                 behaviors of the data right so that's                                 that's the end of the most recommended                                 way if you already have a machine                                 learning workflow built in like a legacy                                 fashion and you want to update it to                                 work on more data you know like on                                 larger data set size the second thing is                                 is something we'll generally recommend                                 to people who are building new workflows                                 now right or starting a project from                                 scratch is to build and define a draft                                 base training flow right so if you                                 typically use template ends of logo or                                 PI torch or MX net or you know tasks ml                                 or or like any new deep learning library                                 that that keeps from popping up every                                 other week what they would generally do                                 is they would build a directed acyclic                                 graph                                 of your entire data your pre-processing                                 jobs your you know your your business                                 rules and pretty much any functions any                                 classes that you give it'll create a                                 graph out of it                                 now what this graph does is it does not                                 load everything into memory first right                                 what it would do is it would save the                                 references of your functions your                                 classes your data itself your patches                                 your dictionaries whatever you gift into                                 that graphic it would save the reference                                 of it right and what it would do is it                                 would take that graph and when it is                                 executing and when it's at a particular                                 node of execution then it would load                                 those references into memory right so in                                 in that sense it is very easy to use                                 these these graph based training jobs to                                 use these to use these graph based                                 training jobs and you know build your                                 machine learning workflow and it's very                                 easy to replicate the same flow on on                                 multiple nodes so what we were talking                                 about before you can have two nodes                                 learning the same graph object and then                                 converge their results together right so                                 in just about a site or two we're going                                 to be doing a bit of code walkthrough of                                 both of these approaches now again I                                 spoke about these two scenarios again                                 I'm going to rush pass through them                                 scenario one is something where you                                 already have a machine learning workflow                                 probably written in scikit-learn or a                                 starts modules you know job and scenario                                 two is where you were building a new                                 experiment flow from scratch like how do                                 you tackle these two scenarios and you                                 build a scalable machine learning                                 workflow which can take you and build a                                 flow which can which is very much                                 scalable and be able to take as much                                 weight as it throughout it so for the                                 for the for the older for the older or                                 old legacy machine learning workflows                                 my recommendation would be to use tasks                                 tasks is is again like a distributed                                 analytics pipeline natively in Python                                 which is written natively in Python and                                 and helps you create batches of data you                                 can think of it as analogous to pandas                                 except this is scalable pandas and it                                 can automatically help you in defining                                 your optimization strategies your data                                 distribution strategies and                                 make it worthwhile and actually make it                                 make it faster for you to build models                                 which are out of code right and ask has                                 ties with pandas task has very very nice                                 compatibility with scikit-learn and                                 starts models and all of these packages                                 so you can mix the two you can use tusk                                 pre-processing your your you know your                                 data and then you scikit-learn do well                                 your models itself right now comes now                                 towards the deep learning module side                                 you can use like my personal favorite is                                 the sense of law and we're going to be                                 doing like a quick walkthrough of that                                 to spin in a bit and but I wouldn't                                 restrict you to just tensorflow you can                                 use tensor flow you can use MX net you                                 can use PI torch all three of those have                                 a very robust ecosystem so feel free to                                 use that right all right now enough talk                                 let's let's let's talk about some code                                 right let me just so just just a quick                                 heads up all the code that I'm going to                                 be working through would are being                                 uploaded as a collab notebook and you                                 can access those notebooks through the                                 links given in the presentation I try                                 and share those on the slack chart as                                 well so that you can get access to it as                                 early as possible but yeah so you don't                                 have to worry about taking screenshots                                 and stuff like that you can easily get                                 access to those so I actually have this                                 open let's first talk about building                                 distributed training workflows with                                 tensorflow                                 so this is the this is actually a collab                                 notebook and they uploaded it into into                                 cable goodness so that you can actually                                 get like the best sense of it I would                                 recommend you doing the same thing as                                 well and you know try and run this so                                 cool so when we talk about distributed                                 training with tens of you right so                                 tensorflow                                 natively provides you couple of                                 distribution strategies right so these                                 are these are neatly packed in tf                                     distributed strategy api with intensive                                 flow rate in this particular example                                 what we're going to be using is we're                                 going to be using TF for distributed                                 mirrored strategy right now let's let's                                 talk about this mirrored strategy for                                 for half a minute here right so what                                 mirrored strategy does is whatever tens                                 of your code that you write write it                                 replicated across all the GPUs or all                                 the nodes that you have connected with                                 your with a node on which you're running                                 right so it would replicate the same                                 code that you're running on multiple                                 machines right and then it would run                                 those right on different chunks of data                                 and then eventually the the context                                 manager would take all the outputs from                                 these nodes and then give you the final                                 output that's how the distribution                                 strategy works if you are able to see my                                 screen you will I have the distributed                                 training tensorflow guide open right                                 there are about six to ten different                                 training strategies you can see here                                 there is multi worker mirrored strategy                                 there's just me leechers PPO                                 clustered resolver strategy and so on                                 there are there are a lot of it and you                                 can also define your own strategy as                                 well so again the link to this is is in                                 the notebook so you can easily check                                 that out as well                                 all right I see that I have very less                                 time left so I'm just gonna zoom pass                                 this again so what we're going to be                                 doing here is using the middle TF dot                                 distribute mirrored strategy we're going                                 to be training a very simple Kiera's or                                 more dense flow                                                     model right and we're going to be using                                 fashion MLS theta self weight for all                                 those of you who do not know fashion MLS                                 dataset this is something which soul                                 and/or research came up with and this is                                 basically set basically a data set of                                                                                                      categories right these categories can be                                 shirts can be socks can be bells can be                                 you know skirts or trousers and so on                                 and so forth it you can look look it up                                 like what all categories are there and                                 this is basically one step ahead the                                 empliciti knows and everyone uses MS                                 data sets but the clothing apparel one                                 has has quite a lot of challenges                                 associated with it right so that's the                                 data set that we're trying to do where                                 we're going to build like a                                 classification image reciprocation model                                 hopefully a very simple model like that                                 and see how that how that would look                                 like you know typical                                 sorry in a in a typical distributed                                 fashion so here are my you know normal                                 imports so I have my dense probate                                 assets I have my tens of                                                 see that I'm using tf                                                    then this this MLS data set is something                                 which is already in the which is already                                 in the TF data sets itself so we load                                 that and I define my distribution                                 strategy here which is TF run                                 distributed minute strategy way so again                                 like note that whatever is being added                                 into like your typical intensive low                                 flow rate so up until now from whatever                                 way you would write a simple tense of                                 the pipeline the only thing that is                                 added is ta protein distributed minute                                 strategy so I then you set up your your                                 input pipeline this is again simple you                                 you have your trained examples you have                                 your test examples right and then you                                 have a couple of like like a very simple                                 pre-processing pre-processing step where                                 in your you essentially scaling your                                 images back to the scale of                                            you can this is being done by the scale                                 function then over here you're                                 essentially just applying that scale                                 function using a map function and we                                 create a very simple model another thing                                 to note there is this new line adder                                 which is which strategy dot scope so                                 this tells my my my tensor flow directed                                 acyclic graph that all the model                                 everything that's written within the                                 model everything that's defined as model                                 is something which comes in the scope of                                 my distributed strategy right so that's                                 that that's something that's changed                                 right and my model is quite simple I                                 have a convolutional there I have a                                 maximum layer I have a fattening layer I                                 have a dense layer and then I have a                                 dense then layer because of any losses                                 right you can see that analogous to our                                 example my loss function is a sparse                                 categorical cross entropy                                 Louis and I'm using an atom optimizer so                                 pretty much boilerplate code                                 nothing nothing nothing fancy here                                 everything's saying the only thing                                 that's been added is the strategy voted                                 right and again I'm just defining these                                 callbacks this is the tensor board is                                 something that I                                 works with tidal but when you run it on                                 your own laptops this would run quite                                 fine so you would be able to see all the                                 losses the accuracy and the and and all                                 the other metrics at unifying onto the                                 drain support itself then we define                                 model check point this is to save the                                 model after every epoch and we define a                                 learning rate scaleable this is                                 basically to be able to expedite the way                                 I train my model so I will have you know                                 a higher learning rate at the start and                                 at like the first epoch so that I can                                 jump to to do my prove the minimum as                                 fast as possible and then I can reduce                                 the learning rate right so again pretty                                 much simple boilerplate code you can you                                 can see it here and then we do train and                                 evaluate so this I've done this                                 benchmark when you run it on your laptop                                 right on one like a very simple laptop                                 for EPOC takes about a minute or two                                 right but when I'm running on a                                 distributed platform using the mirrored                                 strategy you can you can see that it's                                 taking                                                                   let's not go into the accuracy and so on                                 it's giving like                                                 accuracy but that's fine that's that's                                 more of like how the architecture is                                 defined and and so on right let's step                                 ahead and then you can see that I have                                 all my models kind of put into this                                 checkpoint folder so I have my                                 checkpoint                                                               right and again since my model is                                 already there I can quickly pick it up                                 and load the model and also evaluate it                                 right so up until now I've already saved                                 my model am able to load it so you don't                                 have to come up with some sort of fancy                                 you know code because you you had                                 distributed training environment you                                 don't have to change anything in your                                 instance but your influence still                                 remains the same right and it works out                                 fine however if I go here if you if you                                 want to have like a like a distributed                                 evaluation as well right you should you                                 you you can do the same thing with the                                 same strategy right you define the same                                 strategy school and you would be able                                 together you would be able to get a                                 a distributed influenced environment as                                 well right so I know that I rush through                                 a lot of code and again I'm going to                                 give the I'm going to send the the link                                 to this notebook out to you but yeah                                 this was just to showcase like how a                                 very simple distributed strategy can be                                 applied to your you know normal tensor                                 flow code which is towards three lines                                 of code you should be able to build a                                 highly scalable workflow without a lot                                 of effort right now I know that I'm kind                                 of pressed by time but I'm going to do                                 like a quick walkthrough of how the how                                 the task process works like right again                                 just a just a quick check quick                                 checkpoint here what we're going to be                                 doing in the second let me take a step                                 back so in the tensor for example what                                 my main aim of talking over there was                                 that how do you build machine learning                                 workflows if you want to do if you were                                 to solve say a deep learning problem                                 right but what if you you have like a                                 normal statistical problem right how                                 would you build a workflow for that and                                 earlier on we decided that you know we                                 can open through how tasks and cycle                                 don't kind of go with each other so in                                 this example we're going to be talking                                 about how tasks and XJ boost and kind of                                 work together right so again dusk just a                                 quick refresher on tasks is nothing but                                 a large panel dieter frame you know                                 library and it sort of helps you you                                 know build larger than memory you know                                 larger than memory data frames and kind                                 of help you in in process it goes right                                 these are all on all the same inputs                                 just a quick thing again so dusk                                 gifts like when you run into dust coat                                 it will give you like a nice dashboard                                 so you can see how your workers and                                 cores and your memory is being utilized                                 when you're running your dashboard as                                 well                                 okay again for this exercise but using                                 the New York City taxi fare prediction                                 so essentially it's Excel regression                                 problem and we're trying to predict the                                 fare of a New York City Taxi based on                                 their baby so multiple features like                                 what is their source what is their                                 target and so on now just a quick check                                 if I was to talk about if I was to talk                                 about using tasks how would you be able                                 to take a mean of first                                                  ask that would be something like this                                 you do you create a numpy array of n P                                 dot range                                                            basically put it into tasks using d dot                                 from array and you do why not mean not                                 compute right and this isn't technically                                 fast you were able to get the get the                                 results and you know less than a second                                 right similarly you can see that that                                 there were about there what about                                        think about                                                            able to you know get those into into                                 into a directory in about twenty five                                 point seven seven seconds let's try and                                 load this into into my trailer frame for                                 for about fifty five million rows pandas                                 would just not work you can see that I                                 have the pandas line here this just did                                 not work right my turn will crashed my                                 computer crashed and so on so this would                                 just not work pandas would just not be                                 able to take this much memory in but                                 task was able to devote it right in just                                                                                                          the references of data and it was able                                 to get this data right now again over                                 here we're basically defining through                                 the data frame itself for defining what                                 should be my train types my desk types                                 defining all of those I'm defining my                                 training columns my you know I'm                                 defining a bounding box in terms of you                                 know where my where my city is and where                                 my sources where my destination is and                                 so on right and then I have couple of                                 functions in terms of how do I load my                                 data so what I'll call them so I want to                                 take what what fraction of those rows do                                 I do I would do I know just to millions                                 and so on so we you can configure all of                                 this through and then we note that eat                                 away and then you can see that the -                                 data frame was able to get everything                                 you                                 doneita frame and although fair amounts                                 what we have to predict the date time                                 the longitude the latitude drop-off pick                                 up one of these these things are a kind                                 of ended a skater frame itself right and                                 again I can't stress on this enough like                                 being able to do this on a                                              data set in like just couple of seconds                                 is magical because if you were to do it                                 in pandas you would just not be able to                                 do right and then you basically create                                 your extra boost model and over here                                 we're using extra boost and and ask                                 actually post and this is the same way                                 that you build your model and I see that                                 there's an error                                 all right I'm not going to get into the                                 error right now but I fix it before I                                 shared this long with you but                                 essentially using the same code and                                 using just the extreme post model you                                 would be able to create a scalable                                 feature processing pipeline and be able                                 to you know push it into XT post and and                                 get results out of it                                 now I have just one slide to to cover so                                 I'm just going to quickly head over back                                 to my slides and again put both the pod                                 snippets are are on the are on the links                                 in the slides and we can go through it                                 now you may ask the question that which                                 one should I use should I use the task                                 scikit-learn combo should I use the                                 tensorflow and and like the PI torch one                                 the answer to that that is that there                                 have actually no solutions they are just                                 play toss like so it depends on it                                 depends on what your use cases what your                                 problem is I I personally use a lot of                                 tasks like a flow and combo I also use                                 ten flow for my tea planning book as                                 well but the way I defined my                                 distinction is that I use whenever I                                 have to build a simple circuit or model                                 I would use tasks along with                                 scikit-learn to build O's but if I have                                 to build something which is probably                                 with uses image processing or nasha and                                 processing I would build it again                                 tensorflow using K after theta with that                                 thank you so much I know I rushed                                 through a lot of content but I hope this                                 was helpful to you you can you can tweet                                 out your questions to me at or eat                                 underscore VB on Twitter and you can                                 find more about my talks on their blog                                 as well                                 and if you have any questions we can                                 take those now great thanks for both                                 very interesting pointers and insights                                 guys if you have any questions please                                 ask in the slack channel maybe we still                                 have a couple of minutes actually I have                                 a question myself so when you talk about                                 trade-offs so when we talk about you                                 know distributing that's a machine                                 learning workload so something we have                                 been using is to you spark                                 do you have some thoughts on how sparkle                                 pairs with the desk or is it also it                                 depends now so actually actually spark                                 is a very good option                                 right spark has very nice CENTAC                                 integrations with with you know spark ml                                 and and and also with tensorflow                                 in in certain silos right but but the                                 reason where I digress from spark is                                 generally when you write the spark code                                 right                                 maybe me in my spark or in Scala for                                 your pre-processing and so on it becomes                                 quite clunky because whenever you you're                                 building your pipelines you can find you                                 can put it into production quite easily                                 right but it's native interactions with                                 quite a lot of you know well built up                                 libraries within Python like you know pi                                 torch or with MX net and so on becomes                                 quite quite difficult right so you would                                 you would have to introduce some                                 middleware in between your spark P                                 processing pipeline and your module                                 itself so again I a lot of phones you                                 spark for their machine learning models                                 and they put it as well they also will                                 spark streaming jobs as well to put                                 their models in streaming pipelines as                                 well but my my my only problem with that                                 is that it becomes too clunky for you                                 know one person to build something or                                 just like a small team to build                                 something out of and plus obviously it                                 adds like a lot of you know language                                 issues as well that you write in Scala                                 or you write in Java or you write in you                                 know Python or using my spot whatever                                 yeah so you would say dusk is more of a                                 lightweight native kind of yes                                 alternator yeah so actually dusk is                                 being used by by by by a lot of                                 organizations so DARPA uses it which is                                 the defense in in in in u.s. a lot of                                 you know aeronautical companies like                                 NASA also use it and also a lot of HPC                                 is use it as well thankful for                                 high-performance computing so it's                                 picked up quite fast and it's built from                                 ground up in Python so it's quite fast                                 as well great thanks for well I don't I                                 guess we had a bit out of time and I                                 don't see any more question so guys                                 please feel free to reach out to web                                 hobo was lack or Briella                                 and thanks again web of yeah I guess                                 it's a bit late in your part but have a                                 great evening and keep in touch                                 thanks thanks thanks Cheers                                 you
YouTube URL: https://www.youtube.com/watch?v=mWTlDV2Kb10


