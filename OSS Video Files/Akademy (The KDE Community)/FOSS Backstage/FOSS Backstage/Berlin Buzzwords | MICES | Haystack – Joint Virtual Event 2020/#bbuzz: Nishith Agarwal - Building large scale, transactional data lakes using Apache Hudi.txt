Title: #bbuzz: Nishith Agarwal - Building large scale, transactional data lakes using Apache Hudi
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/building-large-scale-transactional-data-lakes-using-apache-hudi

With the proliferation of data in the past years, most business critical decisions are heavily influenced by deep data analysis. As companies rely more on data for their functioning; storing, managing and accessing data intelligently and efficiently is more important than ever before. 

As more business decisions are driven by data in real time, we require strong guarantees such as acceptable latencies, high data quality and system reliability. Moving from a full-reload to a delta model of ingesting quickly became the primary way to ingest large amounts of data at scale. A number of such ingest patterns showcased how a transaction support on such datasets could benefit use-cases immensely. 

Hudi, an apache project is attempting to introduce uniform data lake standards. Hudi is a storage abstraction library that uses Spark as an execution framework. In this talk, we will discuss how Hudi can provide ACID semantics to a data lake. We will discuss some of the basic primitives such as upsert & delete required to achieve acceptable latencies in ingestion while at the same time providing high quality data by enforcing schematization on datasets. Additionally, we will also discuss more advanced primitives such as restore, delta-pull, compaction & file sizing required for reliability, efficient storage management and to build incremental ETL pipelines. We will dig deeper into Hudi’s metadata model that allows for O(1) query planning as well as how it helps support Time-Travel queries to facilitate building feature stores for machine learning use-cases. Apache Hudi builds on open-source file formats; we will discuss how to easily onboard your existing dataset to Hudi format while keeping the same open-source formats so you can start utilizing all the features provided by Hudi without needing to make any drastic changes to your data lake. We will talk about the challenges faced in productionizing large Spark based Hudi jobs @scale at Uber and discuss how we addressed them. 

Finally, we will make the case for the future, discussing various other primitives that will facilitate in building rich and portable data applications.
Captions: 
	                              hello again                               a session by Agarwal nish it needs                               the hoodie project at Huber and is a                               main contributor on Apache hoodie                               project                               he's also blogger and he will tell us                               about building large-scale transactional                               data leaks using Apache hoodie this                               it's your turn alright thank you I mean                                so I'll just hard over again yeah                                welcome folks my name is Nisha Agarwal                                and today I'm going to talk to you about                                building a large-scale transactional                                database with the passivity so a little                                bit about me I work as a particle EPMC                                and have some happy news to share with                                you who do you recently graduated to an                                apache a top-level project I also work                                as an engineering manager uber will                                largely working on problems in the data                                world such as standardized suggestion                                estimate ization data latency and sort                                of defining primitives or materially I                                have a couple of links here for other                                relevant blogs that maybe you know I'm                                useful in the context of this                                presentation all right so let's dive                                into it so what is the data rate so                                Deary Lake is a personalized deposit                                chain that allows you to store your                                structure as well as unstructured data                                at skew so the idea is to store data as                                is without having to first you know run                                structuring on that either so that you                                can turn on different types of you know                                workloads on it                                analytics dashboards visualizations                                another dictator processing to make                                better decisions so let's look at some                                of the requirements from Adelaide so the                                first requirement I want to talk about                                is incremental database ingestion so you                                know think about you know some you know                                upstream yield is that you might have                                which you are using for business                                critical functions and you're performing                                a bunch of inserts updates and deletes                                you know to that table to you know to                                maintain your business use cases and                                then you want this represented in your                                briefly so this this the data that's                                that's contained in the in these kinds                                of tables are usually high value deal                                maybe user information may be other                                transactional information and you know                                one of the ways you stood like                                have this data represented on to the                                data late is by bulk loading you know                                these tables on to the data rate some of                                the problems that plague this approach                                are in a bulk loadstone scale they add                                more load to the database and at the                                same time they involve a lot of base for                                rewriting of data depending on how you                                how much of data you are called floating                                the second requirement I want to talk                                about is you know D duping log events so                                now think about a bunch of you know                                impression events that may have been                                produced by your web application or your                                mobile application and you want to                                replicate this on to let you donate and                                at the end of the day you don't want any                                duplicates on the deal so it could be a                                message something like which has an                                event ID some it's a date and time you                                know values and a bunch of information                                about the interaction so this kind of                                data is generally like really high scale                                time series zero you know order of                                several billions or even a trillion                                begins with messages per day nowadays                                and you know off the order of few                                millions per second and some of the                                causes for these duplicates could be you                                know resize on the client you know                                failures on the network as well as data                                pipelines that may be ingesting this                                which are at least once only so the                                problems that happen with you know                                duplicates are you know problems such as                                over counting so say you're you know                                evaluating these metrics to drive some                                business metric those these more                                impressions leads to low fidelity data                                third requirement is around storage                                management you know when we developed                                early it was initially developed for                                HDFS and you know one of the previous                                 things about HDFS is it does not like a                                 lot of small files given some of the                                 limitations of how the metadata is                                 handled but the same time you know who                                 he runs on other route stores as well                                 but like you know small files in general                                 are a problem because they end up                                 controlling you know how fast or slow                                 your queries can run or you know you're                                 stressing some of the file system                                 metadata anyways and so the other                                 solution could be to write big files but                                 the problem is writing big files                                 involves waiting for these files to be                                 written so for example if you're writing                                 a                                                                                                                                        say your schema so the solution could be                                 okay we write small files but we want to                                 convert them into big files so can we do                                 file stitching you know the problem is                                 file stitching is okay it's                                 non-standardized you know how do we get                                 consistency of all applications to do                                 file stitching and even if you do pants                                 which in your queries the small files                                 that anything is exposed to be exposed                                 to the small to the cooler to the to the                                 query engines and so you anyway see some                                 sort of next query deputation the fourth                                 in the fourth an important requirement                                 is transactional rights the idea is to                                 build bring acid semantics to the tea                                 delay so as in when you know we ingest                                 more and more change data capture and                                 late-arriving events and at the same                                 time you want to have content readers                                 and writers how we support acid                                 semantics on the table so in the context                                 of the dynamic like let me just define                                 asset so a stance for a promise City                                 which is essentially the publishing of                                 data so say you're ingesting a data and                                 the data fields Midway we should                                 basically either so things that have                                 been atomic be published or things that                                 are have not and then not so things that                                 have not been consistency is about you                                 know only the valid data is saved so                                 anything that's invalid is rolled back                                 and not exposed to the users snapchat                                 eyes so isolation is essentially like                                 okay we have read committed data and at                                 the same time we have confidence writers                                 and readers how do we provide isolation                                 of you know read committed either from                                 from these content operations and                                 finally Bute ability we do not want any                                 data loss or idiotic the fifth                                 requirement is around faster Drive ETL                                 leader so now so now think about you                                 know you have these unstructured data                                 that you've ingested on your reader lake                                 and you essentially wants to perform                                 some derived analysis on these on this                                 data for example you have a Rob demon                                 stable you know well and you want to                                 standardize these payments across let's                                 say multiple series a multiple countries                                 and you want to develop a device table                                 on top of that so these these kinds of                                 pipelines generally involve like                                 multiple stage ptl's you know very                                 prominent back                                 and with a fairly large amount of Peter                                 and one of the you know problems with                                 you know working with the right video                                 sets is how do you keep them fresh as                                 your upstream data is changing and you                                 know how do you scale them as you want                                 to do V computations and window joins                                 and then finally I think you know one of                                 the very upcoming requirements for                                 Madeira Lake is how do we handle data                                 deletions and you know compliance you                                 know in the recent days there are a lot                                 of flight requirements from like                                 following strict rules on data at                                 retention or having to delete records                                 reg data and you want to do this across                                 all your data sets on on the develop so                                 you essentially want an efficient way to                                 be able to delete this data which                                 involves maybe like a point is look up                                 on index but I want on the data but at                                 the same time you still want app                                 optimized scans on this data because                                 that's the bulk of the use cases that                                 are running on these deer ticks and                                 finally we want to propagate you monkeys                                 deleted records downstream to other you                                 know derive tables so recapping all                                 these requirements you know we need                                 incrementally to be suggestion you know                                 for you know fresh data avoiding the                                 rewrites of large amounts of data we                                 want to be able to like we do blog                                 events and at the same time boots for is                                 management to make sure that as our log                                 events scale we also scale our you know                                 the file the distribute file system that                                 we use and then we wants transactional                                 writes along with faster Drive ETL for                                 you know improved and faster                                 you know warehousing and then we want                                 compliance and some unique key                                 constraints to handle late for having                                 data for example that set deletes and                                 director actions so at this point let me                                 introduce Apache query so for e stands                                 for Hadoop absurd cell in terminals so                                 think of a bunch of cough cough stream                                 Philippines change logs that you can                                 ingest onto a holy table and at the same                                 time you can also ingest                                 you know on chain foodie upstream tables                                 into downstream another other three                                 tables and all of this data is available                                 in different forms you could use query                                 engines such as - spark for in                                 leader by plans or you can use art and                                 pesto for interactive queries and fully                                 exposes different types of queries to be                                 able to you know meet different use                                 cases and all of this data is can be                                 stored in like any cloud store or HDFS                                 basically any data distributed file                                 system compatible storage all right so                                 so who really supports like two                                 different people types and I quickly                                 wanted to get into the details of this                                 to give you some sort of an idea you                                 know where things are going                                 so the first table type that really                                 supports is coffee on right so think of                                 a bunch of data a batch of data that                                 you're actually wanting to ingest into                                 equity managed table and you know he                                 will say okay you know I want to write                                 these data into two different files                                 based on some sort of file sizing depth                                 you may have defined what he manages all                                 of these operations on a timeline and so                                 he starts a intense and shows an intent                                 of this operation by opening a commit                                 and saying that this summit is implied                                 finally when the data is actually                                 written this summit is atomically                                 published and these versions of files                                 are available to be queried and I say                                 this as a read of Timaeus query because                                 you know these are like just pure party                                 files so this this so copy-on-write                                 could be looked at as a chopped and                                 replacement for your you know canonical                                 park' tables and hive and you get                                 basically any columnar read performance                                 that you would get from part yet now                                 let's say there's another batch of data                                 that comes in that essentially wants to                                 provide a absurd sum of the entries in                                 the already existing data Hoodie                                 maintains an index with which it can                                 route the data into you know where                                 should it be written to and then in                                 copy-on-write where we basically that                                 the contents of like a version at c                                     new version which is attempting to be                                 written at c                                                      updates are possible and at this point                                 since the data is not atomically                                 committed the only contents of version                                 at c                                                                     is atomic weight emitted you now are                                 able to see the version at c                                          worked well for many use cases until it                                 did it forward and you know some of the                                 cases that it didn't this is what i'm                                 going to talk about next so so what                                 problems will be faced so let's say we                                 have a bunch                                 of you know new and updated trips very                                 common use case for uber where you have                                 a bunch of incremental updates and you                                 know these updates end up spanning over                                 a bunch of you know these updates and                                 inserts spanning over a bunch of                                 partitions on on the data so now the                                 copy online you'd end up you know                                 rewriting these kinds of files whichever                                 are affected due to updates and writing                                 files as in search now the average                                 interest time could be about                                            where a lot of time is spent on writing                                 you know equal equitable sized particles                                 for example now think of another                                 scenario where there are a lot of                                 historical ships being updated for some                                 certain reasons and then you have a                                 bunch of incremental updates that now                                 spans over like you know all sorts of                                 partitions and like thousands and                                 thousands of files now that's a ton of                                 i/o with copy-on-write                                 because you end up be writing all these                                 files multiple times as these updates                                 are happening and at the same time you                                 know the job has to do a lot more work                                 so you really need to scale your job so                                 depending on you know how you have what                                 kind of resources are at your disposal                                 this could cause a serious ingestion                                 Regency so at this point if you look up                                 if you think of the typical day level in                                 the dirac as I talked about that it                                 device data sets that are built on top                                 of your raw data sets now all of these                                 you know meter sets experience the same                                 problem and this is the sense of being a                                 very fascinating effect so this so that                                 in summary the problems that we see are                                 in a write amplification due to you know                                 files beginning be written higher                                 ingestion latency depending on how you                                 how what resources are at your disposal                                 and you know given the fact that we                                 write party files and reread party files                                 if you end up writing larger and larger                                 files your ingestion latency becomes                                 more and more because a lot of time time                                 is spent in writing those files so let's                                 find a solution so if you think beyond                                 what we have so one of the key problems                                 that we noticed is okay how are we                                 handling updates so instead of you know                                 updating in you know inline updating                                 those files in mind how about we append                                 these updates to get out the file and                                 you know that                                 depending these updates lowers injection                                 latency directly because we are not                                 spending time rewriting those files at                                 the same time since we're not rewriting                                 those files and if we can you know keep                                 cross-match data into you know these                                 delta files we can actually reduce write                                 amplification and at the same time maybe                                 writing larger files will be feasible                                 depending on how we route these updates                                 and how we route inserts so at this                                 point what we implemented is more                                 gyeongree and a high level this is hard                                 words so you have the same bunch of data                                 batch of data that's incoming into a                                 fully managed table the timeline says                                 okay there is a new data new data that                                 statement that needs to be written it                                 writes them into let's say your choices                                 that salmon or I can't park a files so                                 it dies irae into party files it is                                 committed and now a bunch of updates                                 come along and at this point in                                 copy-on-write we would have reversion                                 file at c                                                              here we essentially write this data to                                 an unmatched                                 you know delta file so and once the                                 commit has done these this data is                                 available but now you can choose between                                 you know obviously this this helps us                                 achieve you know the lesser light                                 amplification by injection agency so now                                 there's a trade-off because between okay                                 you want fresh data and if you do then                                 you use the real-time queries on putting                                 to actually merge the base data which is                                 the party data which the Delta read bits                                 the Delta data to serve on the fly parts                                 one at the same time if you are very                                 sensitive to you know very fast you                                 continue to have you know use read                                 optimized queries but that comes at a                                 cost of latency but what if you know you                                 wanted both you wanted you know to have                                 read optimize data bit fresh data so so                                 this is where we want to bound the query                                 side cost of being able to merge on the                                 fly so so as so as we ingest we wanna                                 keep ingesting fast                                 we don't wanna like you know let that go                                 but can we actually bound this cost and                                 that's where we introduced asynchronous                                 compactions so the goal is basically to                                 for merge so generate is speed up                                 ingestion                                 because in line compaction slows it down                                 while at the same time bound the right                                 side my side cost so what he supports                                 lock-free MVCC based at asynchronous                                 compaction to redouble ingestion and                                 compaction while keeping injection                                 latency are affected so yeah so this                                 these are all you know details about                                 buddy you know what what we've                                 implemented work works so let me talk                                 about finally what are the guarantees                                 that woody provides across all these                                 phenotypes so how do you provide the                                 Adamic multirow commits it is basically                                 supported by a monotonically increasing                                 timestamp to atomic we publish new file                                 versions we only expose valid data to                                 the place any failed or infrared data is                                 rolled back and not exposed it provides                                 not for isolation using MVCC so you can                                 have confidence leaders compactors and                                 you can have the writer and then you                                 know the isolation will be maintained                                 across all of these and finally we use                                 the commit protocol that we have along                                 with the guarantees of a distributed                                 file system to guarantee guarantee                                 durability all right so so these are                                 things that you know what he supports                                 right now and you know in terms of like                                 what set of table types you can have for                                 your ingestion use cases and we have a                                 bunch of you know use CSS ad over which                                 use other you know out-of-the-box                                 solutions which internally use the end                                 of these table types so the fact so let                                 me talk about some facts and figures                                 about three a to burn so i do where we                                 run about                                                    transactional tyranny                                 ingesting into more than                                             people's and writing about                                             plus records per day so we have a body                                 based data leak so it's a unified                                 analytical storage so at uber it looks                                 something like this we have a bunch of                                 upstream database change logs and rocks                                 after events which are pushed both into                                 top and then you know our batch                                 ingestion can range anywhere between                                   minutes to                                                            data is written into HDFS and then                                 because of the you know because of                                 features provided by everybody provides                                 you know three different types of                                 queries for you to be able to serve                                 different use cases so column there will                                 be performance queries to be used by you                                 know data science ml or                                 you know other like you know quite                                 sensitive you know use cases real-time                                 queries are you know like provide low                                 latency DS data in dashboards and maybe                                 ad hoc queries and incremental queries                                 could be used to build like you know the                                 RAI BTL datasets and like warehouse                                 so so apart from like building out the                                 disk and were really get a duper there                                 are other use cases are over you know                                 which is supported by out-of-the-box                                 solutions a Bible so I'll talk about a                                 couple of them one of them is the hoodie                                 Delta streamer so every Delta streamer                                 is basically a mini ingestion framework                                 implementing inquiry to be able to you                                 know ingest from different sources to to                                 let's say and so you know the ubers                                 global network stream is basically                                 powered in near real-time using for                                 iterative streamer the the high-level                                 architecture looks something like this                                 so we have a bunch of rowdy the raw                                 event tables free Delta streamer you                                 know instrumentally poles you know                                 changes from this raw table and then you                                 know for the ubers network analytics use                                 cases they transform the entities into                                 you know Delta somebody's again using                                 transformation functions provided by the                                 Delta streamer and then they essentially                                 up straight into a final summary table                                 where they merge you know their deltas                                 into a in other Delta on the summary                                 table and this you know this allows the                                 global networks analytics team to                                 actually maintain a good history of                                 what's happening in the in the network I                                 suppose storing many raw events another                                 use case is essentially building et ELLs                                 you know downstream derived data sets                                 and spot data source integration is                                 highly popular there so with a lot of                                 the folks using you know ETL you know                                 they're more proficient in like five                                 boys and Python programming so in that                                 world you know hoody is used with PI                                 spark on the bounty resource and look                                 something like this so there is a bunch                                 of you know high queries that implement                                 people from rajid a table they transform                                 this data and rights to let's say you                                 know intermediate staging tables at this                                 point they may want to join this                                 intermediate data with other you know                                 into the other tables to produce a model                                 and finally all the information the                                 model table can be absurd using three so                                 these are some of the use cases that you                                 know are popular at uber we built the                                 holy data Lake and then you know                                 different kinds of out-of-the-box                                 solutions used to support use cases and                                 so we also use a bunch of advanced                                 primitives that when he provides and                                 I'll talk about a few of them so one of                                 the most primitive is how do you recover                                 from data corruption so some common                                 questions and production systems are you                                 know if what if a bug resulted in                                 incorrect data being pushed to my                                 injection system or what is an obscene                                 system incorrectly marked some common                                 values is not so how do you prove how do                                 you be so this is something that could                                 happen and how do you basically adjust                                 easy what he does that for you who do                                 you have has an ability to restore a                                 table to a last known correct time and                                 it provides two of the two you know two                                 ways to do this one is using safe points                                 which is basically cheating checkpoints                                 and different instants of time or you                                 know you could basically use the file                                 versions retained where you could go                                 back as far as in time as needed so the                                 the positive benefits of using safe                                 points is that it optimizes the number                                 of file versions you need to keep so                                 reduces the disk space while this you                                 know the downside is doesn't work right                                 now fortunately the file versions works                                 for both copy-on-write in more general                                 but the downside is it requires extra                                 storage capacity the other you know                                 advanced primitive is incremental Poe                                 which is you know very popular for                                 reducing the amount of data that you                                 scan as well as reducing the amount of                                 data that you write in many direct data                                 sets so there are two ways to you know                                 use the increment lakeil one of them is                                 using spark so you use your spark data                                 source you initialize some of the you                                 know some of the parameters that we need                                 to do and you know the one of the                                 important things to note is basically                                 you can give a beginning instant line                                 which is basically hey like from what                                 time do I want to instrument three pull                                 all of this data and then you can have a                                 data set register that data set as a                                 table if you want to and then use the                                 sparks equal to pirate that you we also                                 have a port that using hive and you have                                 to set a bunch of                                 you know a high connection parameters                                 and you know you you can hire you can                                 also provide a bunch of batches that you                                 want to consume from this begin instant                                 time and then you can go and select you                                 know and then you can run a query on                                 that on top of that instrumental tea                                 so the other cool feature that you know                                 buddy provides is the ability to time                                 travel and basically quite different                                 snapshots in time and you know this                                 lecture works in congestion conjunction                                 with incremental poll so now that you                                 know using spired what you can do is you                                 know redefine you already define the                                 begin instant time what you can do in                                 addition to that is a is a sign an end                                 instant time and essentially in that                                 time range is whatever snapshot you                                 wanna play you can go and query that                                 snapshot you can do a similar thing                                 using hive but there is a slight caveat                                 to that there is no such an instant and                                 support in the hive                                 so you have to convert the else instant                                 time to like num commits to read and                                 then use that but we are looking at                                 adding a consume and timestamp so that                                 you know you have like sort of the same                                 feature parity between spark invites and                                 this is very this is a very popular                                 thing used for like you know machine                                 learning feature source which which is                                 becoming more and more popular                                 right so now that you know running all                                 of these features at scale as especially                                 at were you know we ran into a lot of                                 fly you know we did a lot of performance                                 tuning in spark and spark applications                                 and I want to share some of the                                 learnings with you so you know one of                                 the things we realized is you know the                                 the type of sterilizer that you use                                 matters a lot and so we have ruber we we                                 use class utiliser for all our                                 civilzation work especially in smart                                 applications it has a lesser memory                                 footprint than java serializer as well                                 as much possible                                 the second thing I want to touch upon is                                 shuffle service so you know social                                 service is something that's used by                                 spark for example to you know move                                 around data when you are performing                                 shuffles and so if you have the                                 opportunity we use your resource                                 scheduler or business manager like yarn                                 you know consider using of external                                 shuffle service                                 especially for large memory requirements                                 it's the reliability of shuttle service                                 is challenging and it contributes                                 directly to job stability and you know                                 doing this helped us in that initially                                 and you know how do I manage you know                                 how spot applications are reading large                                 amounts of data the third thing I want                                 to touch upon is a sparse memory model                                 so understanding how to effectively use                                 heat versus non heat is extremely                                 important and as well as you know how do                                 you - you know the heat memory that you                                 use which basically could be as naive as                                 the executed numpy that you have how do                                 you tune that because we noticed that a                                 lot of times you know people run smart                                 applications where there is lack of                                 understanding of whether then the the                                 throughput of the job is high given the                                 resources that the job has been provided                                 so so Google did implement an open                                 source profiler to be able to profile                                 your forms and to see if you're actually                                 using the number that you know you you                                 given to your spark shop and lastly your                                 back configurations can make or break                                 the day so investing in contact tunings                                 constantly is is extremely important and                                 if you have an automated feedback                                 mechanism to tune that parts of                                 something that we have an uber is                                 extremely valuable and as you run like                                 larger jobs you choose between different                                 table types are tuning boutique contigs                                 is also you know like extremely                                 important is as your based on use cases                                 different configs can give you different                                 set of performances right so that's                                 that's what we have you know up until                                 now I'll talk to you about what's coming                                 soon and the and what's in the roadmap                                 for you next so here we have a release                                                                                                         next couple of months and there are two                                 super cool features that are that are                                 being worked on so one is you know we've                                 had a lot of you know how do you convert                                 an existing canonical five table to a                                 party table essentially the people want                                 to migrate the canonical tables to                                 overeat Abel's to you know support up                                 search the mental pose and we want an                                 efficient way to do that so I'll                                 encourage you to read the full RFC which                                 also has your examples and                                 you know scale numbers on how how fast                                 and how efficient this is but I'd like                                 to give you a very quick you know tidbit                                 of how it works essentially you know you                                 use a data frame to trigger an action on                                 the table and all you tell the action is                                 that the operation type is a bootstrap                                 operation and Anthony will basically go                                 ahead in the background and do that for                                 you and the second very cool feature is                                 you know hoodie has invested a lot in                                 you know how do you optimize rights how                                 do you ingest data how do you manage                                 your latency ride amplification we have                                 so much of meta data that we can                                 actually improve queries drastically so                                 there is a RFC that's like that's an                                 implementation right now which is to                                 provide order of one very planning and                                 what I mean by that is you know as your                                 table cells drawers instead of letting                                 pipes and like you know to do this query                                 planning for you by scanning like                                 different partitions or you can do this                                 in constant time with the metadata                                 that's already available we also want to                                 eliminate file listing something that                                 you know many cloud stores and HDFS                                 format like and this is the the PR is                                 already out the RFC has more details                                 around it finally you know given that we                                 have all of this information in this                                 metadata you know we want to expose some                                 column column indexes on holy datasets                                 which can allow you to you know perform                                 where clauses so as to say on you know                                 non partition columns anti-foreign                                 custom or inspect so that's that those                                 are the upcoming features and you know                                 some of the notable items that we have                                 in the roadmap are as follows so you                                 know who he provides different types of                                 pluggable indexing mechanisms and we're                                 working working on one which will                                 basically provide blazing-fast absurds                                 for you know all large workloads where a                                 large number of data might be be mine                                 might have been sorted                                 we're working on a couple of other                                 things around you know data clustering                                 on you know we we already do storage                                 management of you know how do you size                                 files how do you make sure you know you                                 write equitable like size files but                                 same time we also want to see okay how                                 if based on like applications how does                                 this should this deal be clustered sort                                 of like the couple you know how do you                                 ingest data from how do you actually lay                                 out this data you know based on like                                 application requirements there is a PR                                 out for real-time queries and custo and                                 in the longer term there is a a lot of                                 ask in the community to have integrate                                 in twittering and there is some initial                                 work that stuff around that with this                                 that's all I had                                 thank you you know watch as I mentioned                                 a partially recently recently graduated                                 to a top-level project we have a very                                 exciting an ambitious roadmap and a very                                 budding and community so if you're                                 interested please you don't connect with                                 us at develop a a put it out of pocket                                 or or follow us on Twitter or if you're                                 interested in trying out buddy please go                                 to her eat or the path she toured there                                 is a very cool docker you know                                 QuickStart that you can actually you                                 know use to star start like using and                                 realizing your use case in a matter of                                 minutes all right at this time I'm happy                                 to answer any questions and we have one                                 question from Nicola who's telling us                                 that he's already tried hive with hoodie                                 and can't wait for time travel there to                                 his question his first question is about                                 spark can you make an example of                                 external shuttle service can I make an                                 example of external chapel service so I                                 so I'll tell you what I mean by external                                 shuttle service so you know whenever you                                 shuffle between you know when it's foggy                                 up starts and you want to shuffle data                                 you know there will be some sort of the                                 server that froggies which reads data                                 from the local disk that you flex fill                                 to and then provides it from one mapper                                 to reducer so as to say and so you know                                 you could either use the stuff that's                                 provided by spark standalone or you                                 could have a resource manager do that                                 for you and you know depending on you                                 know what kind of memory requirements                                 you have and                                 what reliability you want you you may                                 want to choose one with one of the other                                 and for us you know we externally                                 shuffle service on yarn work we have                                 some more work which we did on top of                                 that which is not part of the stock but                                 that's something that would for us okay                                 then how do you plan to get rid of                                 finest thing yeah so I think you know                                 that if you go and read the RFC the RFC                                 number is RFC fifteen essentially you                                 know we already have all of this                                 metadata since hoodie is the writer to                                 be stables we already have all of the                                 metadata so if you look at the dory                                 folder all these command files already                                 have them already does so the idea is                                 basically consolidate that metadata and                                 be able to read entries from that                                 metadata as opposed to you know asking                                 hive matter store and then going to HDFS                                 to figure out which files the peasant on                                 this okay have you evaluated other file                                 formats like OLC or carbon data yeah I                                 think there is it PR on RC I think there                                 is some back and forth on that so so                                 I'll talk about it in to two aspects one                                 is well how will fully support these                                 file formats so there is a push to                                 support of RC at uber we had you know RC                                 and party initially we had both and then                                 we moved to party for you know some of                                 the community work that's been done some                                 sort of like in coatings that were not                                 available I think at this point it may                                 be worthwhile to like reevaluate oversee                                 part carries a lot of work has been done                                 and put years will eventually support                                 both okay cool that's so for the                                 questions                                 I think the breakout room will stay open                                 for for all the night if you want to                                 continue discussion with Nicola who is                                 following the talk actively I think he's                                 typing maybe he said he's got another                                 question no it was a thanks from him                                 okay so I think you can yeah you can                                 continue discussion freely on the                                 breakout room and this was the last                                 session of the day I think I'm deepest                                 to at least so thank you a lot for this                                 this this                                 so I didn't really know about the                                 subject but learned a lot and it was                                 clear for me so thank you                                 I guess it was for the others - thank                                 you initiate and have a great end of day                                 thank you bye thank you I am for                                 moderating thank you for all your                                 patience                                 you
YouTube URL: https://www.youtube.com/watch?v=_XCu-zpKG6A


