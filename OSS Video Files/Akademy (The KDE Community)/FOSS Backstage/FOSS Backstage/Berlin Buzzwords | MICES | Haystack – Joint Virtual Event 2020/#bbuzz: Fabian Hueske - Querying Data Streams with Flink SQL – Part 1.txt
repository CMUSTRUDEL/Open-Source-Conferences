Title: #bbuzz: Fabian Hueske - Querying Data Streams with Flink SQL – Part 1
Publication date: 2020-09-10
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/querying-data-streams-flink-sql-part-i-eu

Apache Flink supports SQL as a unified API for stream and batch processing. SQL is easier to use than Flink’s lower-level APIs and covers a wide variety of use cases. In this hands-on tutorial you will learn how to run SQL queries on data streams with Apache Flink. We will look at the concepts behind continuous queries and dynamic tables and will show you how to solve different use cases with streaming SQL, including enriching and joining streaming data, computing windowed aggregations, and maintaining materialized views in external storage systems.

Prerequisites:
 - No prior knowledge of Apache Flink is required. We assume basic knowledge of SQL
 - You will need a computer with at least 8 GB RAM and Docker installed. To save time during the event, we would also like to ask you to set up the tutorial environment beforehand by following the instructions at https://github.com/ververica/sql-training/wiki/Setting-up-the-Training-Environment
Captions: 
	                              this tutorial is                               about flink sql it's actually                               designed to be a full day training where                               i'm gonna                               try to crunch it down to two times uh                               two hours now                               we're probably not we can't cover                               everything but                               all the whole material and everything                                and slides and exercise                                everything is available online so even                                if                                we don't we're not able to                                cover the whole thing you can still                                have a look at the slides yourself and                                try to solve the exercises                                all right so i hope you all                                also were able to follow the                                setup instructions                                let me see                                so the setup instructions are                                in this repository i have a verica                                github                                i'll get up a very secret training and                                if you click                                on the                                wiki link then                                [Music]                                there's here on the menu on the right                                hand side                                is the setting of the training                                environment instructions                                it's actually quite simple it's uh it's                                a docker compose setup so if you have                                docker installed and                                have assigned enough resources so you                                should                                assign something around three to four                                gigs                                then everything should pretty much work                                out of the box if you                                bring up the docker compose environment                                all right but now let's get get                                get started so i'm not sure how much                                you've                                uh how much you've heard about apache                                flink so i'll                                give a very brief introduction into                                apache flink and then                                move into the secret parts of it                                [Music]                                i'm not let me see if i can                                see the questions here yeah so from time                                to time i'm simply                                we'll come over to this uh to this                                window and see if there's if there                                are any questions and um                                yeah i think it's also possible that you                                uh enabling microphone and                                maybe uh talk to me directly let's let's                                see if that works later                                okay so i'm getting confused with all                                these windows                                so all right so apache flink is a                                distributed data processing system                                it's um a system for                                yeah processing large amounts of data in                                a parallel fashion                                you can process all kinds of data with                                apache flink                                it's good for processing real-time                                events so streams of data                                for instance coming from                                log systems like apache kafka                                or sensor data streams but you can also                                process uh                                static data like data like like files                                reading from a                                distributed file system or reading data                                from a database                                flink is used for mainly three different                                types of applications                                one of them being streaming so-called                                streaming or what we call streaming                                pipelines                                it's basically um yeah a type of                                application that moves data from one                                system to the other                                uh maybe doing some some uh                                transformations aggregations                                enriching the data on and so on so                                basically something that you can imagine                                as being a                                low latency etl style of job                                then there is also analytics both for                                streaming and batch data so just the                                traditional uh yeah but what you would                                usually do with the sql queries or                                machine learning                                 applications analyzing the data and                                 crunching it down                                 and then finally there's another type of                                 application                                 what we call event driven applications                                 uh which are                                 applications that are basically um                                 yeah get their their their data or the                                 interaction                                 through events and then process all                                 these events you can                                 you can think of like a event driven                                 application uh processing order events                                 for instance                                 um flink recently um added a new api                                 for for these uh event-driven                                 applications                                 uh these so-called state-funded state                                 fun                                 api my colleague stefan even just gave a                                 talk about this                                 so if you're interested in that i can                                 really recommend going back and                                 checking out his talk as well                                 all right so that's this is like apache                                 flink on a very high level from the use                                 case point of view you can run flink in                                 very different kinds of environments                                 like uh kubernetes uh yarn messes                                 uh also uh on bare metal clusters if you                                 really want to do that you can                                 integrate it integrates with many many                                 other systems in the                                 in the ecosystem you can store your data                                 on hdfs                                 s                                                             like a full-fledged uh distributed data                                 processor                                 uh flink comes with uh very flexible and                                 expressive apis                                 it guarantees the correctness so there                                 is                                 exactly one state consistency so even if                                 one of your distributed processes goes                                 down                                 flink guarantees that it will                                 continue processing once or it will                                 recover this                                 task and continue processing as is that                                 basically as if this error never                                 happened um                                 it has event time semantics we'll                                 actually have have a look at that                                 uh tomorrow a little bit and it runs at                                 a really large scale                                 so we have users                                 that run flink jobs on ten thousand of                                 course                                 managing several terabytes of data                                 this is a list of some of the companies                                 using apache flink either using flink                                 for the internal use cases or                                 providing services or                                 yeah computing services based on apache                                 link                                 for instance amazon or alibaba offering                                 flink in the cloud cloud environments um                                 other users like                                 lyft or                                 or researchgate or uber use                                 flink for their internal data processing                                 needs this is just a small                                 well not that small but uh but a sample                                 of                                 all the flink users um we are uh running                                 a conference                                 uh called apache uh sorry fling forward                                 um have run it for uh like several times                                 now and there is a bunch of talks that                                 we recorded and put                                 on youtube so                                 why would you like to use sql for stream                                 processing                                 well first of all stream processing                                 is implementing stream processing                                 requires a certain skill set it's not                                 that it's not that easy to find so first                                 of all you need                                 at least for apache flink you need a                                 good                                 java or scala developers so flink is                                 implemented in java so and                                 integrate into the jbm ecosystem so you                                 need                                 somebody who can code in java or scada                                 you need                                 a good knowledge of stream processing                                 concepts like how to handle                                 uh how to work with uh time and state                                 and also since it's a distributed system                                 have some                                 some some knowledge on uh experience in                                 working with                                 distributed data processing systems so                                 that's actually a skill set that is                                 well not that                                 that available yet at the same time                                 see everybody knows sql everybody uses                                 sql or has used sql at some point in                                 their                                 career everybody who's working with data                                 sql queries                                 can be optimized and are usually                                 efficiently executed by the by the                                 system that                                 runs the sql queries and if you                                 do it right you can have a unified                                 syntax and semantics for both batch and                                 streaming data and this is basically                                 what                                 flink is trying to build so                                 flink flink aims to provide a                                 standard compliant sql service to query                                 both static and streaming data alike                                 and of course running that on apache                                 flink using                                 the correctness guarantees scalability                                 and performance of                                 flink so um                                 let me quickly check if there's any                                 questions not yet                                 so what is what makes streaming sql                                 different from traditional sql                                 well um if you think about it                                 basically or or most tables that you                                 query with                                 uh a secret query are uh are are                                 changing right you have a have it                                 have a table that is modified by some                                 application with transactions                                 uh some kind of order table where                                 constant continuously new orders are in                                 inserted or tables about                                 your customers the customers                                 change their addresses or whatever so                                 basically                                 usually database tables are are changing                                 right and all these changes are                                 basically maintained by the database                                 and whenever you run a query on such a                                 table                                 the database system will conceptually                                 take a snapshot of the table                                 and run the query on this static                                 snapshot and then                                 once the query is done it gives you the                                 result of the                                 of the data of the of the query and this                                 basically means                                 that the input of the query is is finite                                 it's like some some finite amount of                                 of rows and um                                 since the input is fine it also the                                 query result is finite                                 and it also never needs to be updated                                 because                                 you run a query of a snapshot of the                                 data and                                 you get a get a result for that if you                                 run the query later                                 on the next day you might get a                                 different result because                                 the data was changed in the meantime um                                 stream sql processors work a little bit                                 different                                 um instead of taking a snapshot snapshot                                 of the table                                 these uh clear processors uh basically                                 connect to the table process the data                                 and they also process all the upcoming                                 updates which means the queries                                 continuously running                                 and it receives all the updates that are                                 applied                                 on the on the original on the source                                 table and then                                 uh apply the changes on the input                                 um compute basically the delta                                 uh on the on the previous result and                                 then update the result so you have a                                 continuously running query                                 that automatically mirrors all the                                 changes on the input                                 to the uh to the output                                 this basically means that the input of                                 of a streaming query is unbounded                                 because you're continuously waiting for                                 new update updates on these on the                                 source table                                 but it also means that the result of                                 such a query is never final                                 because it might change at any point in                                 time they might at                                 any point in time there might come some                                 some row in this in the source table                                 that changes just some fields or some                                 some rows of the output so the output                                 needs to be continuously updated                                 however you can you can basically                                 also show that the semantics of such a                                 query of a query that is run on the                                 static                                 um snapshot of a table or contingency on                                 the table                                 um are actually the same and we'll have                                 a                                 small small example for that so                                 let's say we're running a one-time query                                 on a changing table                                 we have this simple table here three                                 fields user                                 some change time and a url click time                                 and some url so you can imagine this is                                 some kind of                                 click stream table where every click                                 that a user does                                 ends up being a row in this table so we                                 have this                                 table user mary clicks on at                                            at noon                                 on some link then comes user bob user                                 mary again                                 and at that point in time somebody                                 wants to run a query and runs this query                                 i'm saying okay i'd like to know how                                 often every user has clicked                                 on on a link so what the                                 what a traditional data system would do                                 it would basically take a snapshot of                                 this                                 query would run the query on it                                 and if while the queue is running or                                 after the query is running a new                                 row would be added well this row would                                 not be considered right                                 so basically                                 the query would only run over the first                                 three rows                                 and when the query finishes result is                                 produced                                 and that's it the query terminates at                                 some point                                 the result is final and the input is                                 is uh continued to change however the                                 query was only run on a snapshot of                                 it if we run this query now on uh                                 on a in a continuous way well then let's                                 say we start the query at this point in                                 time when the table is empty                                 now the first row arrives the query                                 ingests                                 the first row and updates the results so                                 at this point in time there's only one                                 user who clicked                                 some link which is user mary and they                                 clicked exactly once                                 all right now the next row comes and the                                 query updates the result the next row                                 comes and now here you see                                 that we're not only uh inserting a new                                 um we're not only inserting a new row                                 but we're also actually updating this                                 row here uh basically changing the count                                 from                                 two from one to two and at this point in                                 time                                 the result of the query is exactly the                                 same as the                                 uh as the query that we run on the on                                 the same snapshot that on the snapshot                                 that                                 concert had this exactly the same data                                 as                                 the table right now and if now we will                                 we're adding                                 more one more row then this update will                                 always also be                                 being reflected in the in the table                                 so by this uh                                 small small demo i wanted to basically                                 to show that                                 you can have the same the same semantics                                 for sql queries regardless whether you                                 evaluate them                                 uh once on a snapshot of data or                                 continue continuously                                 on new arriving data                                 all right any questions so far                                 all right then                                 let's continue um all right so these                                 queries were actually                                 pretty simple uh because i just used                                 them for uh for a simple example                                 um but um flink                                 actually supports a fairly uh fairly                                 large set of sql                                 both on the streaming and the batch side                                 on the batch side actually uh flink                                 supports a full tpc ds support                                 which is a tpcds it's a fairly                                 well-known                                 benchmark for analytical queries ds                                 stands for decision support                                 and the whole benchmark consists of                                                                                flink can run all of them for the                                 streaming uh                                 look at looking at streaming we'll                                 support the simple                                 simple things like selection and                                 projection from where clauses                                 select from where uh also supporting                                 different types of grouper aggregations                                 uh                                 several types of joins uh user defined                                 functions                                 uh we also support um so-called over                                 windows we'll have a look at                                 what that is um if you're not familiar                                 with these uh                                 this is standard sql syntax so all of                                 this is on standard sql syntax                                 if you're not familiar with these over                                 windows we'll discuss them tomorrow                                 um yeah and there's also something that                                 is pretty exciting                                 which is the so-called match recognize                                 clause it's a fairly recent addition to                                 the sql standard                                 it came with a sql                                               it's an it's a clause that allows you to                                 evaluate patterns on on tables                                 and you can imagine that pattern                                 matching and stream processing or                                 and continuously evaluating newly                                 arriving data                                 is can lead to very interesting or can                                 means that you can solve very                                 interesting use cases with it                                 um so what are the common use cases for                                 flink seeker well                                 i told you a little bit about data                                 pipelines before so this is something                                 that is uh                                 obviously a common use case for apache                                 link                                 because you can use it to easily move                                 data around so you can                                 read data from for instance from a kafka                                 topic and then                                 perform some some aggregations and write                                 it to a file or to a database                                 basically moving data from one system to                                 the other                                 and applying arbitrary transformations                                 or joining data streams together                                 with sql is much easier than                                 implementing a                                 full-fledged stream processing job in                                 java or scala for that                                 the other use case is analytics                                 here meaning that you're not not really                                 moving data around                                 but but mostly using flink to                                 compute aggregates or                                 or some kind of some other metrics that                                 you're interested in                                 and then you can store the result of                                 this query for instance                                 uh as something that is kind of like                                 similar to a materialist view                                 in a database and then use um                                 a security database to always get the                                 latest result of the                                 of the query                                 all right so much for a rough                                 introduction into what                                 uh flink sql is                                 let's now have a look at the uh at the                                 at the training environment                                 um all right what we're doing in the                                 training                                 let me quickly check all right uh so                                 what will we do                                 in this uh in this tutorial well first                                 of all we'll uh                                 run a bunch of queries on streaming data                                 um so you get a get a feeling                                 for how this works and what you can                                 actually do with it uh                                 we'll uh also uh express like common                                 stream processing operations like                                 aggregating data streams on time                                 hopefully also getting to the point that                                 we can                                 join streams together or write some of                                 these                                 next recognized clauses if we don't                                 come to that as i said all the material                                 is online available in the github                                 repository and you can                                 check out the slides and the exercises                                 uh also after the after the tutorial                                 we'll also                                 write some of the use of link sql to                                 write data to external systems so just                                 running sql queries on streams is fun                                 but when you actually want to do                                 something with it you kind of like have                                 to                                 materialize your results somewhere and                                 we'll do that with apache kafka so                                 writing                                 the result of a streaming query back                                 into kafka or into my sql                                 and then we'll use flink's cli client                                 sql cli client for all of the                                 exercises in the tutorial                                 we're using um a scenario here which is                                 based on                                 uh taxi ride data so this this is a                                 public data set that basically                                 uh provides data about taxi rights in                                 new york city                                 we're using this as a yeah basically as                                 a scenario to run off some of the                                 queries to make it a little bit                                 more meaningful interesting we splitted                                 the data                                 up into three different tables there's a                                 rights table that                                 contains one start and one end event for                                 every                                 taxi ride that happened so when the taxi                                 rate starts there's an event                                 going into the rights table that says                                 this this is the time this is                                 the coordinates longitude and latitude                                 this is how many passengers we                                 we are we are                                 moving with this right and then there's                                 also an ant event that says okay the                                 right ended                                 at this location at this time so every                                 right                                 has two is represented as two                                 uh different events then there is a                                 ferrous table                                 which has one payment event for each                                 ride which happens                                 um shortly before the end of the                                 the ride and then there's another table                                 driver changes                                 which has one event uh for every time                                 when a taxi is                                 used by another by a different driver so                                 basically we're capturing here                                 whenever yeah taxi is                                 used by a different driver so all of                                 these tables are already                                 registered and available for                                 for immediate use in our training                                 environment so there is no need for you                                 to define these tables                                 and each of these tables is backed by a                                 kafka topic                                 so you can imagine that there is some                                 process pushing data into these tables                                 and what you see in the                                 cli client is basically all the data                                 that was pushed                                 into these kafka topics here's                                 some some sample data so if you run a                                 simple                                 select stack query from rights you'll                                 get a result that looks like this so you                                 see there's an                                 id for a right there's a taxi id                                 basically                                 the car that did the did the write                                 is start here is a boolean variable that                                 says true if it's at a start event                                 longitude latitude time and passenger                                 account                                 for the fares table it looks quite                                 similar we have the right id                                 basically you want to know for which                                 right somebody paid                                 there's the time when the payment                                 happened there's the payment                                 method here yeah this csh means cash i                                 guess                                 amount of tip talls and fare                                 and then there is these uh this driver                                 change table where we have a                                 taxi id the drive id and when                                 the timestamp when the user when the                                 driver started using the                                 the taxi                                 the training environment that                                 we will use based on docker compose so                                 each of the                                 colored boxes here basically corresponds                                 to a docker container                                 we have a docker container for the cli                                 client                                 this is the container that we basically                                 use to run our queries                                 when you submit a query it basically                                 gets submitted                                 to job manager job manager                                 is a component of apache flink which is                                 the uh                                 yeah the master node of                                 uh of apache flink                                 there is a web front end that you can                                 access when everything's running here                                 it's on localhost                                      um and then there's the task manager                                 component flink cluster can have one or                                 more task managers task managers are                                 other workers that are executing the                                 tasks and                                 um basically when you submit a cover to                                 the job manager                                 it uh breaks the job done into sub                                 into multiple tasks and distributes them                                 to the task ventures for execution                                 um there's a container for apache kafka                                 because we use                                 apache kafka as a as a data source here                                 the sequel click line                                 has actually two rows it's not only                                 there for running in the cli                                 but it also continuously pushes data                                 into apache kafka at                                                basically means that it pushes data                                 with                                              timestamp timestamp speed basically if                                 you see here                                 two timestamps okay they have all the                                 time all the same time stamps but if you                                 have                                 see here two timestamps here then you                                 basically um                                 um this is at                                                           one minute and one                                 there's like roughly                                               between                                 uh these two events but uh to make the                                 whole                                 environment a little bit more                                 interesting we actually push them only                                 two seconds uh three seconds apart from                                 each other so to just get a little bit                                 more data                                 because this is a real data set and just                                 looking at it                                 at regular speed would be a little bit                                 boring                                 yeah then there's a small zookeeper node                                 which is                                 at this point still needed by kafka they                                 are currently working on getting                                 getting rid of the zookeeper dependency                                 and then there is also a small                                 mysql container that will use to push                                 data into                                 okay so um i said we're gonna we're                                 going to run the queries                                 using the flink seal icline this is a                                 component that comes with apache flink                                 and here two screenshots so you can get                                 a                                 rough rough idea how it looks it's just                                 a regular sql cli                                 and the flinxieli client has                                 two modes of executing queries                                 the first one is the so-called                                 interactive query submission mode                                 here you basically the user enters the                                 query                                 submits the query to the cli                                 the secret client has                                 has a catalog embedded has an optimizer                                 also embedded and the optimizer                                 optimizes the query and submits the                                 resulting                                 job to the flink cluster and then the                                 flink cluster will start accessing the                                 the data or reading the data from the                                 from the tables that were referenced in                                 the query                                 the data goes into the query                                 and the data goes into the queue that's                                 running in the flink cluster                                 and then flink feeds the results back                                 into                                 the sql click line and the cli displays                                 the uh the results so this is the                                 interactive mode so you basically run                                 the queries and you directly see the                                 results interactive in the cli client                                 and there's another mode when you're                                 submitting a query as a using the insert                                 into                                 syntax this basically means that you                                 want to write the result into some other                                 table                                 at that uh if you do that the cure is                                 again optimized by the sql iclient                                 but and submit to the flink cluster but                                 here the result of the query is                                 basically fed back into the                                 system that holds the sync table and                                 the sqlc like line doesn't get any of                                 the results back it only gets                                 back a message that the query was                                 submitted and this is the                                 id of the query then you can look up the                                 query in the                                 end link web ui                                 all right so much for the                                 introduction let's now uh try to get our                                 hands a little bit dirty                                 and work with the actual thing                                 um i have to admit i've never done a                                 virtual tutorial                                 before so usually this would be the time                                 when                                 you guys start coding and                                 working and whenever you have a question                                 you raise your hand and i uh                                 come here come to your uh                                 to your desk and then you ask me a                                 question                                 so i'm not quite sure how we should do                                 it i would say                                 maybe i'll walk you through the                                 exercises and whenever something is                                 unclear                                 you'll ask a question and                                 i'm going slow so you have a chance to                                 also                                 do some things on your own let's just                                 start the                                 the environment so i'm let me increase                                 the font size a little bit                                 um                                 so i'm bringing up the docker compose                                 environment                                 now all the containers are being started                                 and now i basically                                 enter the sql cli                                 client                                 by running this command it basically                                 runs this                                 shell script in the sql c like container                                 and here you see that we have this nice                                 squirrel here                                 on the c li secrecy light line all right                                 let's first                                 check uh which which which tables are                                 available                                 so we'll just say show tables                                 and you see those are the three tables                                 that we actually um                                 already talked about you can                                 uh describe a table let's say the rights                                 table                                 and uh get some some information about                                 the schema here                                 you see right at the tax cd and so on                                 uh right time here is a kind of like a                                 special column it's a so-called row time                                 column                                 which we will use tomorrow when we talk                                 about                                 queries and time this is basically                                 somewhat corresponding to a                                 to the event time attribute that you get                                 when you use uh                                 the flink data stream api so if you have                                 have done that before if not                                 don't worry about it yet uh we'll come                                 to that later                                 so we can now let's say                                 run a simple query on this table                                 and yeah when i start basically started                                 the command line client                                 or the docker compost environment um                                 the uh sql cli container started pushing                                 data into                                 on the apache kafka topic and this is                                 still happening                                 uh now so we're basically still reading                                 the data                                 that is being pushed to apache kafka                                 so you see it's continuously adding more                                 data it's not like reading a lot of data                                 from this kafka topic and still being uh                                 displaying it                                 but actually the data is fed into the                                 kafka topic                                 right at this point in time                                 so usually this interactive session has                                 meant                                 that you like become familiar with the                                 environment here                                 so we've done that select star from                                 rights                                 there's also a bunch of sql functions                                 already available                                 we say show functions                                 this is basically the list of all                                 functions that are                                 built into apache flink                                 um                                 yeah this is the list of uh let's build                                 in functions                                 and um in addition to that there's also                                 a few                                 um user defined functions that we                                 implemented here                                 that make it easier to uh to work with                                 some of the data                                 like for instance a time diff function                                 that takes to timestamps and then                                 basically returns the difference uh                                 of the timestamps in milliseconds there                                 is this                                 um is in uh nyc                                 function where you pass the longitudinal                                 the latitude                                 and the function will basically tell you                                 whether this                                 location is roughly in the near new york                                 city area                                 to functions to area id                                 where longitude and latitude coordinates                                 are mapped into a grid of cells                                 each cell being roughly                                            meters                                 we're going to use that to later                                 aggregate on this                                 on this area id and also the inverse                                 function the two coordinates function                                 where you                                 basically pass the array id and then it                                 will uh                                 return the center of the of of the cell                                 um yeah and then there's also this                                 driver function so i'm not going to                                 talk about this now it's uh mostly                                 important for some of the                                 joining exercises later flink also                                 support or the                                 cli client also supports uh creating                                 views so you can                                 pretty easily um                                 implement views like this this now                                 creates a view                                 right starts where we only have the                                 rights that                                 the right events that are right start                                 events so we can now                                 select from                                 right starts                                 and just as you would expect now we get                                 all the starting events you see                                 the start flag is always true                                 not that exciting to be honest                                 um all right i think                                 we can                                 skip the exercises here that's mostly                                 for                                 for defining views so here's                                 an exercise this task is to ex the task                                 of this exercise to cleanse the tables                                 of right events                                 by removing events that do not start or                                 end in new york city                                 for this we would basically use the um                                 the is an nyc user defined function                                 and then call the function with the                                 coordinates um with the start or                                 end coordinates of                                 the of the event and then filter on                                 filter on the result and only letting                                 those through that are actually in                                 in new york city so it's very simple                                 select statement here                                 all right so um yeah what we've done so                                 far was yeah                                 fairly fairly simple simple very basic                                 sql stuff nothing exciting yet                                 but yeah you just got in basically an                                 idea of                                 how these how this training environment                                 looks like and what it means to or                                 how it feels to run simple queries on                                 data streams                                 so in flink or the                                 like the underlying concept and flink                                 that that flink uses for for running                                 continuous secret queries                                 is a so-called dynamic table and um                                 a dynamic table is basically what the                                 name suggests it's a table that is uh                                 changing over time that is evolving um                                 as i said before basically every table                                 is is changing but                                 usually this is not exposed to the to                                 the query right because when you run a                                 query                                 uh the databases uh database system                                 takes a snapshot                                 um using techniques like uh yeah                                 this this isolation and transactions to                                 actually make sure that                                 you only get a consistent view of the                                 data                                 but                                 the security itself only sees a static                                 data set                                 in contrast when you're running a                                 continuous query in flink                                 you're running the query on a dynamic                                 table and                                 the query itself internally needs to use                                 needs needs to have some some state to                                 hold intermediate results                                 and then the output of such a                                 query is again a dynamic table                                 a dynamic table can get its data                                 through different kinds of connectors                                 flink is a                                 has a bunch of connectors it has a                                 kafka connector for apache kafka you can                                 also read data                                 via a jdbc uh driver so there's actually                                 many more databases that you can access                                 than just post or send my sql every                                 database that has uh has a jdbc                                 connector                                 it also works with different file                                 systems like hdfs or s                                  and then again can materialize the                                 result of a query which is represented                                 internally again as a dynamic table                                 also to all these different kinds of                                 systems                                 something that is important to uh to                                 understand is                                 that the dynamic table is not                                 necessarily something that is                                 materialized within apache flink                                 so it's uh first of all it's a more or                                 less a                                 conceptual conceptual thing here                                 it does not mean that flink fully                                 materializes or a                                 copy of the of the input data                                 there is sometimes depending on the on                                 the                                 data and depending on the on the query                                 [Music]                                 some or all of the table data needs to                                 be materialized                                 but this is not true for all queries                                 there is                                 also many queries that materialize only                                 a small                                 small portion of the dynamic table                                 in state and uh can be therefore being                                 executed quite                                 uh quite efficiently um                                 it's also important to know that the                                 flink community continuously                                 uh works on improving the connectors and                                 also adding new connectors to different                                 systems so                                 this is something that is uh has become                                 uh yeah a little bit of a focus now                                 also to make it easier to connect uh                                 different systems                                 uh to this to this infrastructure                                 so if you look at this basically there's                                 you can you can think of                                 the query processing like in three                                  different steps                                  the first step is basically getting data                                  into uh into apache flink you can think                                  of like it like basically converting the                                  data that comes from kafka                                  to a concept conceptual to a dynamic                                  table i said before it's not necessarily                                  materialized but conceptually                                  there's a conversion step                                  from from these source systems to                                  dynamic table then there is the query                                  execution                                  that gets the dynamic the                                  table from the dynamic table and                                  computes it                                  to a new dynamic table then there's the                                  output step                                  when the dynamic table now needs to be                                  written to a sync system so there's the                                  ingestion step there's the processing                                  step and then there's the                                  uh output step                                  um right now for this uh stream or for                                  this uh                                  input to dynamic table conversion flink                                  in the current version flink                                                                                                     or or repent mode which basically means                                  that                                  all records that are pushed to flink                                  are interpreted as insert                                  as records that are inserted into a                                  table so for instance if                                  this stream of events comes from apache                                  kafka                                  then each of these events would                                  correspond to a new row                                  in this table this means of course that                                  this table is ever growing and the more                                  data that is added to this                                  table on the conceptual dynamic tablets                                  uh is is growing uh                                  more and more and uh if you think about                                  that and have a high volume stream                                  um then this is also a clear indication                                  that flink                                  is would not even be able to materialize                                  such a                                  such a table internally instead because                                  it would over time                                  just grow too large                                  however as i said so this is currently                                  the only only way to                                  get data into or convert                                  external data into a dynamic table                                  interpreting all of the rows as insert                                  statements                                  for the upcoming release flink one                                  whoops flink                                      [Music]                                  the community worked on                                  work on a mode that can also ingest                                  full change locks and for instance                                  i can convert the basium change locks                                  into dynamic tablets so                                  you can basically connect                                  for instance if you're extracting                                  postgres locks or the                                  postgres converting a post table into a                                  change log                                  using using dybasium you could                                  basically connect the dybasium change                                  log to apache flink                                  and then have a dynamic table that                                  exactly mirrors                                  um the uh table in in postgres and then                                  run queries on                                  uh on this table and when something is                                  changed in                                  uh in the postgres table the uh                                  changelook would forward this                                  change into apache flink apache flink                                  would pick up this change apply it                                  on the on the on the query and emit the                                  result                                  um so now you basi we basically have                                  this                                  conceptual this dynamic table in apache                                  flank and                                  we can run some queries on on it and                                  these tablets can be cured just with                                  regular sql so there's no                                  not really special syntax required there                                  is                                  uh there are a few like special                                  uh how to say that uh                                  patterns i would say it's still it is                                  standard sql but it's a                                  it's a certain certain patterns that                                  patterns that you use                                  later if we're handling with uh temporal                                  conditions so we're gonna talk about                                  that tomorrow                                  uh but uh the the syntax                                  is uh all a standard compliance sql                                  syntax                                  and when such a query is continuously                                  started                                  then the results are as i said                                  incrementally computed and updated                                  so let's have a look at what that means                                  for uh with a simple example                                  so again we have uh have a have this                                  clicks table here on the left hand side                                  um with a user click tab url and we are                                  running a simple filter query on                                  uh on this on this table then                                  we're just checking for urls that                                  [Music]                                  access this home home url                                  then the first record access exactly                                  that ul so that's why it passes                                  the next one doesn't and so on                                  and you see that this is a very simple                                  query that                                  basically also means that this query                                  that the result of this                                  query will result in a table that is                                  also append only so at any point in time                                  we can make for every record we can make                                  a decision                                  whether this record is part of the                                  result or not                                  and once we made the decision there's no                                  reason to really                                  change the result of this because the                                  query doesn't change and the row doesn't                                  change                                  so all                                  all updates here in the result table are                                  always                                  append only this is also a good example                                  for instance for a query                                  that would not need to materialize any                                  of the input data right you can                                  imagine that this clicks tablets                                  conceptually super large                                  but flink does not doesn't need to                                  materialize any of the rows                                  so the dynamic table would basically not                                  be materialized for this query                                  we can just apply a simple                                  the the simple filter on the fly and uh                                  not need to materialize any of the any                                  of the data                                  if we run a different query so this is                                  the query that we had in our other                                  example before                                  or a similar query                                  we basically have an aggregation query                                  now and if we run the query                                  we basically the result table gets                                  updated                                  and here you basically see                                  that the rows of the result table need                                  to be retracted so we need to                                  uh                                  to update some of the rows that we                                  already emitted                                  and change them if whenever we                                  when we see new data so this the                                  behavior of this query                                  is is different in this in this regard                                  to the other one                                  because we need to change                                  some of the results that we already                                  emitted                                  so now coming back to the uh to the last                                  let me first check if there's any                                  questions no                                  so if we know uh now the last                                  part of the qr execution is then                                  basically converting the                                  dynamic table back to the output system                                  and there's also different modes how                                  how this can be done and it pretty much                                  depends on whether the                                  result table needs to be updated or not                                  because if they if the result table has                                  updates they kind like                                  need to be somehow encoded into the                                  outgoing stream and there's different                                  ways how fling can do that                                  [Music]                                  the first case is the uh is the simple                                  case                                  where the uh result of it of a query                                  uh like this simple filter query is an                                  insert only stream in this case it's the                                  conversion is very                                  is just exactly the same as uh                                  as before as for for the append only or                                  insert only                                  ingestion in this case basically each                                  row that passes                                  the that is produced for by the query                                  is just passed on as an insertion                                  record and the downstream system can                                  just write out                                  all of these rows that it receives                                  because                                  it knows none of these rows ever needs                                  to be updated again                                  so this is something that we can use to                                  write data to an apache kafka topic                                  into some kind of file but also to other                                  more                                  other data stores that allow more data                                  more                                  update modes like elasticsearch key                                  value store or                                  a regular database                                  so this is the kind of like the easiest                                  output mode because we can just                                  continuously write data out and never                                  need to update anything                                  if however the query looks like this it                                  has                                  some kind of aggregation theory that                                  also needs to update some of the results                                  then all these changes can be converted                                  into                                  insertion and deletion                                  statements so for instance if you look                                  at the at the third record here                                  where we get the second value for mary                                  and this time when we when the queuer is                                  processing this record                                  we need to basically uh remove the                                  previous or                                  the previous result for mary which means                                  we                                  here the minus indicates that we delete                                  this record                                  so before we for the first record we                                  inserted mary                                  with the count of one then we got the                                  next record for bob                                  added this one with the count of one and                                  then we get the next record for mary                                  so we need to update the count and we do                                  this by first deleting this                                  on our previous result and then                                  inserting a new one                                  so this basically means that                                  this kind of conversion mode needs                                  some external system that is able to                                  delete arbitrary records                                  this is not very easy if you if you want                                  to do that on a file system it's not                                  usually not                                  not efficient because if you're writing                                  data out for instance in some kind of                                  format like or orc                                  these formats are good for bulk                                  ingestion but                                  not very efficient for deleting                                  individual records                                  so you usually wouldn't do that                                  or wouldn't write such a data that you                                  kind of need to                                  update on a per record level                                  to a two file system however if you want                                  to materialize such a                                  such result in a relational database or                                  in a key value store like elasticsearch                                  then this is much easier because                                  you can um delete individual records and                                  also insert them                                  and finally there's another upset                                  another mode                                  the so-called absurd mode upset and                                  delete in this case                                  this is basically all the update                                  operations are happening                                  on a on a key on a unique key so each                                  row of the output needs to have a unique                                  key                                  that can be used to address this record                                  in case of our query here we and this is                                  the user field                                  because we group on users so for every                                  user we get a                                  unique every user has exactly one one                                  result record                                  and now we can basically                                  update                                  we can update the result using a key                                  which is                                  more efficient because we don't                                  depending on the system we don't need to                                  first delete the record and                                  add it again but instead we can say                                  please update this record                                  and this is the new value and for this                                  for this kind of output mode you can                                  think of                                  writing this to something like a                                  compacted kafka topic                                  kafka is these are compacted topics                                  where you can                                  define a key message and then the topic                                  will always only store the                                  or provide the latest value for a key                                  or again a key value store or a                                  relational database                                  let's continue with operators in state                                  um                                  so there is this goes nowhere a little                                  bit into the                                  internals of of the system                                  so there's basically three different                                  types of operators that flink                                  flink users to process such queries                                  there is stateless operators like filter                                  and projection                                  these are operators that don't need to                                  persist any intermediate results when a                                  filter gets a wreck it gets                                  gets a record it can directly make the                                  decision                                  whether this record should pass or                                  should not pass when it                                  when a projection operator gets a row it                                  can immediately apply the                                  transformation that is defined in the                                  projection and pass the row on                                  and once the row is forwarded                                  it doesn't need to remember this row                                  anymore because it's a                                  it's a simple operation and the operator                                  doesn't need to have any state for that                                  there's other operators like operators                                  like aggregations or joins                                  that need to memorize                                  records or intermediate results because                                  these these operators um combine                                  multiple records with each other right                                  so an aggregation operator um if you                                  have for instance a                                  group by sum operator it sums the values                                  of multiple rows so it kind of needs to                                  have some intermediate state to                                  remember what what the current                                  intermediate                                  value for the for for the sum is                                  a join operator joins rows of different                                  tables together                                  so it first needs it kind of like needs                                  to remember the rows of the one side                                  and when a row comes from the other side                                  it kind of needs to look up                                  if there are any matching partners for                                  for this                                  new row on the other side and vice versa                                  this is roughly how uh how streaming                                  joins work                                  so a join join and also aggregation                                  operators need to                                  need to materialize some state                                  internally right                                  and then there is a and another type of                                  uh                                  of operators the so-called temple                                  operators which we'll discuss                                  uh tomorrow and these operators have                                  some kind of temporal condition                                  that bound the                                  uh how to say that                                  the bond the range of the computation so                                  for instance you can have a                                  have a group i operation that groups                                  records that arrived within                                             if you do that you know that after                                     minutes you don't                                  need these results anymore because you                                  can perform the computation                                  completed provide a immediate result                                  and then the next                                                       whatever happens you never have to look                                  back                                  at the at the previous                                                these operators                                  also hold some state but some                                  intermediate state but they're able to                                  clean up this state automatically                                  because                                  there's some temporal condition that                                  just bounce                                  how long an operation needs to needs to                                  hold the state                                  but as i said we'll talk about that a                                  little bit more in detail tomorrow                                  um so if you look at this query again                                  this is the                                  this this this group back theory um then                                  we see that                                  um the query here internally basically                                  needs to hold                                  uh one count for every user that is                                  being processed right                                  when we uh when when we think of how                                  they revolve the result                                  um before this record he arrived uh the                                  count for mary was two                                  so internally the query had to remember                                  for mary                                  the for the user mary the current card                                  is two and when                                  this the last record here arrived uh                                  it said okay i need to count this so                                  counting means i have to inc                                  i've seen another record for um for mary                                  i have to increment                                  now the count for this record                                  um and then it said okay the new count                                  here                                  uh is three                                  and um internally also start this                                  this intermediate result uh as a state                                  but also pass it on to the to the output                                  of the query                                  so that the query could could update the                                  result in the external system                                  so this query here this simple query                                  here basically has                                  three different kinds of key value pairs                                  internally                                  one for a merry one for bob and one for                                  this                                  yeah so that's basically yeah and                                  something that that i didn't mention yet                                  is                                  that the query here basically needs to                                  hold this state                                  forever so there's uh no point in time                                  when this con query runs continuously uh                                  if if you run this query for one year                                  for instance                                  then the query would accumulate all                                  unique users                                  that that were                                  that were seen by the query                                  because at any point in time there could                                  be a new record coming                                  that updated our account for for any                                  user                                  so um this also shows that                                  for for some of the queries you have to                                  be not necessarily                                  uh careful but you can't like have                                  should uh                                  think a little bit about what the query                                  would internally do                                  and then do some uh back on the napkin                                  calculation whether this is something                                  that the system can handle or not                                  if you're just computing counts for                                  users then this is something that                                  is probably not too bad um                                  yeah um it kind of like depends it not                                  not                                  for some of the aggregations it actually                                  does not only depend on the                                  on the number of user keys                                  but it can also depend on the type of                                  aggregation function that you're using                                  for instance if you're using a min                                  function or max function                                  then it can happen that                                  the state for this function is not only                                  the                                  current minimum or maximum but also                                  all of the intermediate values because                                  if you have if you have a                                  source table that might also remove some                                  of the values it might happen that the                                  smallest value of the                                  or that if you have a min function that                                  exactly the minimum                                  value is removed and then you would                                  basically need to update it                                  to the second smallest value and if that                                  is again removed you would need to                                  update it to the third                                  to the previously third smallest value                                  so for instance for a min or a max                                  function it can happen                                  that the state of the aggregation                                  function is basically                                  one small data point for every                                  record that was for every record of the                                  input                                  dynamic table                                  so um yeah so this is                                  just a just a summary of what i said                                  before                                  some of these materializing operators                                  like aggregations and joints                                  need to hold state forever because there                                  is no                                  temporal boundary for this for this                                  computation                                  and                                  data can change at any point in time so                                  and in order to account for that you                                  just need to                                  have a pro possibly                                  a large amount of intermediate state                                  i also said that these tempera operators                                  can be a little bit more clever about it                                  because                                  um if you if you have a temporary                                  boundary around your                                  uh your computation then you also know                                  how long you need to hold the state                                  and once you know that you don't need                                  the state anymore once you know that                                  you you passed the temporal boundary you                                  know that you've seen                                  all of the data you can produce a result                                  and then also discard                                  all of the intermediate data                                  so um but what can you do to                                  or is is there a way that you can how                                  can you handle this                                  uh possibly growing state so there's                                  different types of                                  or different solutions how you how you                                  can do that if you have                                  some data that is growing rather slowly                                  like                                  the number of users that you have then                                  you can                                  solve this problem possibly by just                                  scaling up the query so if                                  you run the query on two nodes you can                                  just scale the query possibly to                                  three or four nodes um if you have a                                  growing number of users then                                  this is probably a first of all a good                                  sign                                  and you                                  maybe you can afford to run the curie                                  just on a larger setup                                  if this is not the case                                  then you can also use something that                                  flink calls                                  idle state cleanup                                  and this is useful for situations like                                  this if your data first has some kind of                                  session id                                  and you would like to group by the                                  session id then uh                                  it is pretty clear from the context or                                  from from what we know                                  that this session is only valid for a                                  certain amount of time right                                  um if the session hasn't is                                  has become basically become in inactive                                  uh we know that it will not be updated                                  at any point in time uh in the future                                  again                                  however this is something that the                                  system that flink doesn't really know                                  and therefore can really account for so                                  it doesn't know                                  that this session will not be used                                  session id will not be used again                                  and hence it will not automatically                                  clean up the state                                  uh in a situation like this you can                                  configure something like this idle state                                  cleanup                                  and if you do that flink                                  will automatically remove state that has                                  not been accessed for                                  a certain amount of time and this is                                  basically something that you can                                  configure                                  if you know that a session uh will not                                  be used again if it hasn't been updated                                  for                                  for an hour for instance you could                                  set this idle state cleanup time for one                                  or                                                                      to be on the safe side                                  and then if it has been updated for for                                  two hours                                  flink would go ahead and remove this                                  date                                  for this session id                                  this works well if the data that was                                  removed                                  won't be used will not be used again                                  um so in that case uh all the                                  computations are remain valid                                  and everything is fine however if you                                  remove some state too early                                  then flink will just see the new                                  record as something completely new and                                  uh will not remember that it has had                                  done any computations for this                                  before so at that point in time you then                                  would get                                  inconsistent results and basically what                                  you're doing here is                                  you would possibly trading the accuracy                                  of the                                  of the result for the size of this data                                  um yeah so to summarize um                                  streams are or the input is basically                                  interpreted as some kind of changelog                                  for table                                  in the current version flink                                             support                                  insert into changelogs basically change                                  logs that represent                                  only insertions in the upcoming version                                  where that is currently being finalized                                  there will be support for full change                                  locks                                  secret curious that we run on a dynamic                                  table                                  produce a new dynamic table and the                                  query                                  kind of determines whether the result                                  dynamic table is an append only table or                                  a table that is that is being updated                                  and when you convert it back                                  there's also different modes how you can                                  convert a dynamic table back                                  if the dynamic table is also insert only                                  it's a fairly straightforward con                                  conversion because every                                  record that is being inserted                                  conceptually inserted into the dynamic                                  table                                  is being emitted as a neuro                                  if there is uh update also update and                                  delete changes                                  then there is uh different update modes                                  like insert and delete or upset and                                  delete                                  um that can be used to write the data                                  out                                  to uh to an external system usually you                                  don't really                                  um you don't have to do that manually                                  when you can select a connector for a                                  sync table                                  then the connector will basically know                                  which kind of uh conversions it                                  it supports and then flink will                                  automatically use the                                  right conversion for this connector                                  to write the data out and if you tried                                  for instance to write                                  some some updating data to a kafka topic                                  then flink will simply say                                  no sorry i can't do that there's                                  no way the the connector does not                                  support updating                                  results or updating rows in in kafka                                  um yeah you can run basically                                  uh just regular secret furious on these                                  dynamic tables                                  um right actually writing and executing                                  these                                  curiouses is rather easy because it's                                  just standard sql however                                  you should pay some attention to the                                  state requirements of your query and                                  depending on the query and the input                                  data the state might just grow                                  very large                                  okay then let's maybe have a look at the                                  sorry the results so the                                  first exercise was fairly simple query                                  we just want to basically get a                                  histogram of how many rights                                  happened with how many passengers so if                                  we                                  remember how the table writes table                                  looked like there's this passenger                                  account field                                  and so we can do a simple query                                  select                                  oops passenger account                                  can't star from                                  rights                                  oh i think we only wanted to have rights                                  that started right so                                  yes start                                  where is start                                  group by passenger account and if you                                  run this query                                  you'll basically see oh there's even a                                  right with no passenger                                  or a couple of rides with no passengers                                  um                                  so you basically see how the result is                                  continuously refined                                  based on the data that is uh that is                                  that is arriving                                  you can actually if this is uh going too                                  fast you can actually also                                  slow down the update rate                                  so a bit well no this way wrong way                                  let's say now we have an update every                                  one minute                                  um so the uh see like line doesn't up                                  out update at the at the rate at which                                  the query updates the result but                                  only update up refreshes the page every                                  every one minute so here you can see                                  that clearly                                  uh rights with a single single passenger                                  are                                  clearly uh the most                                  followed by rights with two passengers                                  and so on                                  all right so that's a fairly simple                                  query and i think the only                                  interesting part here is that it's like                                  continuously being                                  being updated and and refined                                  the other query was a little bit more                                  uh like the from the                                  structure point of view exactly the same                                  in this case we want to group the data                                  based on the                                  hour of time and on the area id so                                  basically                                  um it's kind of like the                                  kind like very similar here                                  so here we group on a couple of more                                  fields we group on this to area id                                  function start hour                                  basically converts the time stem into                                  only extracts the hour of day from the                                  timestamp                                  we again filter on is in new york city                                  and so on and adding a having clause                                  and then this query                                  sorry                                  if we run this                                  very                                  it does exactly what we're                                  expected to do so you see here we have                                  now we're now here at                                  hour of day eight                                  simply because due to this                                            we're already eight hours into the                                  taxi ride events                                  all right so much for for this um                                  then let's                                  yes so the crap yeah so so andre asked                                  the question um                                  uh except for table and field names uh                                  fling sql is case                                  insensitive that is uh that is true yeah                                  so uh table field names need to be                                  are case sensitive and for everything                                  else for the keywords                                  yeah you can do it however you want                                  okay then let's maybe continue how much                                  time is lefty i've got                                  something like i think                                             left if i'm right he started a bit                                  late or                                                                 this one                                  um                                  [Music]                                  this is about                                  how you can create tables create tables                                  in the sense of a ddl statement                                  and connect the table to an external                                  system and then                                  use this table basically also to write                                  out the                                  query result to an external system                                  so um i did not explicitly mention this                                  before                                  um but apache flink is not a data store                                  so flink does not                                  store any data except for                                  in-flight data that is needed to for for                                  query processing or processing of                                  streaming applications                                  so you cannot use flink as a as                                  a database to just store                                  all your data it's                                  focusing on processing data                                  it stores as i said intermediate data                                  that is needed for for                                  processing but it's not a database so                                  whenever                                  you're                                  interacting with data you probably read                                  it from some extent system and                                  system and when you're done usually you                                  also write it out to some external                                  system                                  and flink provides connectors for many                                  different storage systems                                  and formats for                                  sql this is apache kafka because it's                                  the most                                  uh most widely used um                                  [Music]                                  stream store or system to uh to                                  distribute data streams                                  it has connectors for uh jdbc i                                  mentioned that before                                  elasticsearch apache hbase                                  it also has a good integration with                                  apache hive which is uh                                  mostly valuable for uh for a                                  batch running batch queries so you can                                  read                                  five tables with flink sql and then                                  use flink's flink sequence batch engine                                  to                                  process these these hive tables                                  you can write to different file systems                                  and you can also write in different                                  formats uh such as                                  arbor json csv um parquet and or c                                  there's it's a bit of                                  not that easy to say which combination                                  works how                                  for instance packet or a ceo of                                  obviously file systems                                  formats that are only relevant in the                                  context of file systems                                  whereas avro for instance averages and                                  csv                                  are also relevant for                                  instance for apache kafka which stores                                  binary data that you can encode                                  in whatever you where you want so for                                  instance for kafka                                  flink sql supports uh records that are                                  serialized in in avro json or cc                                  and then other systems like jdbc                                  obviously have their own storage system                                  so they don't really                                  need to need a um                                  apache ever                                  there's also um yeah here there's this                                  uh link in the documentation that                                  basically tells you like what                                  uh combination and which formats and um                                  connectors are currently available for                                  flink sql                                  um in the next version in flink uh                                      there'll be                                  many more connectors and uh yeah better                                  support for                                  for for some of the external systems                                  so if you want to create a table now you                                  basically have to                                  provide different types of information                                  here                                  first of all um the create                                  table step and the dll syntax looks in                                  the first part                                  should look familiar to you so you say                                  create                                  table you give the table a name for                                  instance order here                                  and then you specify the schema of the                                  of of the table and this is just a                                  regular schema definition so                                  you can have an order id and give it a                                  type a                                  big end you have an older amount you can                                  give it a type decimal                                  flink supports all of the common                                  commonly used                                  sql types and then there is something                                  like order time                                  that is of type timestamp                                  and then comes something that you                                  probably haven't seen before which is                                  this watermark field                                  watermark for order time you see that                                  here again for the timestamp                                  and then some kind of expression all the                                  time minus interval of                                             this is a watermark definition                                  watermarks are flink's mechanism for                                  tracking progress and                                  time progress as i said we'll                                  talk about queries and time tomorrow so                                  i won't go into the details here                                  but the important thing here is that you                                  can                                  specify the watermark expression                                  directly                                  within the ddl syntax                                  it's optional so if you don't need                                  watermarks                                  or time handling then you don't don't                                  have to                                  provide this watermark                                  this watermark clause and then the next                                  part                                  is also something that not sure                                  you might have seen before in other                                  systems that don't that are not data                                  stores                                  but this is basically a definition that                                  gives all of the connected format                                  properties that                                  flink needs to create an appropriate                                  connector                                  to read and write the data so here for                                  instance we're connecting to kafka                                  so connected type is kafka it's a simple                                  key value format                                  the connected version is universal that                                  is                                  a flink internal thing that uses the                                  yeah kafka's universal                                  api for for accessing kafka                                  we obviously also need to tell the                                  connector from which kafka topic we want                                  to read this is called auras here                                  there's some other properties like                                  bootstrap servers here                                  this is like running on my local machine                                  and we also want to specify format                                  we want to read json data here and                                  flink internally basically from the                                  schema definition of the table i can                                  infer that                                  there's json schema that it needs to                                  read                                  so um this is now the basically the                                  properties that you                                  would need to provide to read from a                                  json encoded kafka topic obviously if                                  you're using                                  avro or using some other                                  storage system all these properties uh                                  look uh look a bit different there's                                  also more properties here for                                  kafka you could also specify whether you                                  want to read from the beginning of the                                  topic or from the end of the topic                                  uh you can provide a group id and so on                                  so there's lots more lots of more                                  configuration                                  or configuration options that i didn't                                  didn't show here                                  great table is not the only syntax that                                  flink supports it also supports                                  altering dropping of tables or functions                                  so you can                                  you use that to register your own user                                  defined functions                                  you can create databases and so on and                                  the ddl support is also extended in                                  future versions                                  again flink                                                 we'll add a couple of nice features here                                  as well                                  internally flink's default catalog is                                  not durable                                  so for instance the catalog that is used                                  in the demo environment is based on the                                  is a simple in-memory catalog                                  that stores all the catalog data in                                  memory but that does not persist                                  the the information uh anywhere on the                                  secrecy like line                                  has a has a yaml file                                  where you can uh specify all the all the                                  connectors                                  properties and when the                                  cli client is started this year profile                                  is parsed and uh                                  tables are created for all the entries                                  in the yaml file and this is basically                                  how                                  these three tables the rights the fares                                  and the driver changes                                  tables were initialized in the cli                                  client that we use for the demo                                  environment                                  but if you would now create a new table                                  in the docker container then would                                  simply                                  stop the client and restart the client                                  this table would be done so                                  by default                                  none of the catalog data is really                                  persisted in in flink fling is not is                                  again not a data store it doesn't store                                  data like in                                  at some external locations however                                  flink supports hives metastore                                  so you can simply connect                                  flink with a hive metastore and then use                                  that as a                                  persistent external catalog service um                                  when you then                                  basically connect flink to uh to                                  metastore you                                  create a new catalog i could call it                                  hcat or whatever                                  and then in the flink ui in the flink                                  cli you can say use catalog hcat and                                  when you create a new table there                                  then this table information will be                                  persisted in hive better store                                  and even when you then stop the client                                  restart it again                                  if you again connect to hcat                                  sorry um then the table that you created                                  in the previous session                                  will still be there                                  okay so um writing table to                                  writing query results to external tables                                  as a                                  something that is very very easy um just                                  regular                                  sql syntax uh you just use the insert                                  into statement for that                                  so here's a very super simple example                                  let's say we have                                  some table people                                  [Music]                                  that has just information about persons                                  and then we could just filter or filter                                  on the x range between                                                                                                            into a table called teenagers                                  if you do that then the schema of the                                  the result schema of the query must                                  match with the                                  table schema so here for instance                                  teenager                                  must have those two fields name and age                                  and if one of these fields is not in is                                  not                                  available in teenager the query will                                  fail and flink will say i don't know how                                  to write this data to teenager                                  and also the tables                                  the connector that was configured for                                  teenage for the                                  teenagers table needs to be able to                                  update                                  the table but                                  basically needs to be able to write the                                  result table of this query um to the                                  external system so for instance if this                                  is this is a simple filter query so                                  there should not be any issues with it                                  but if you if you would um                                  have a query here that produces an                                  updating result                                  then we cannot simply write this                                  updating result to a file system or to a                                  kafka topic                                  because the connector would not know how                                  to                                  forward a delete or abstract change                                  into the into the sync system that holds                                  the                                  teenager data                                  let's do one more hands-on exercise this                                  is now                                  uh not the next one it's actually the                                  last one                                  so if we go to the wiki page again it's                                  this                                  number six creating tables and writing                                  query results to external tables                                  let's only do the                                  let's only do the this one here                                  maintaining a continuous                                  the updated materialist view in my sql                                  the other one needs                                  uh the other exercise above that needs                                  um a temporal operator that we're just                                  gonna discuss                                  tomorrow um so                                  i would um                                  say let maybe um let's give it                                  five minutes or so um and you                                  try to run this query i prepared the ddl                                  statement here                                  for you already so you don't have to                                  look up all of these                                  connector properties you can simply copy                                  paste this for                                  for this exercise and then there is a                                  small                                  exercise here for for query that you                                  could specify                                  and while the query is running you can                                  then                                  also look into my sequel and see how the                                  table in my sql is changing                                  yeah i would say let's you can you can                                  try that for                                  a few minutes and then we can                                  go through that together uh one more                                  time and                                  i yeah point out some hopefully                                  interesting                                  uh interesting things about this                                  exercise                                  yeah yeah so the the job keeps running                                  when you exit the cli this is the                                  idea of this detached mode exactly                                  so if you look at the uh and in the                                  flink                                  ui and the flag flink                                  web front end you see that the job is                                  still running actually you cannot even                                  maybe that's amazing i guess that's a                                  missing feature but you cannot even                                  cancel the job from the                                  fleeing c like fling cli anymore you                                  get the uh drop id back but um there's                                  currently no feature to cancel the job                                  you have to go into the                                  flink ui and then stop it there                                  all right i think we're                                  somewhat close towards the end i would                                  no go through this exercise um                                  and um show a little bit how it works                                  and then                                  we can have maybe a few more questions                                  towards the end if there's if there are                                  any questions                                  and um yeah all right so the um                                  the exercise here is basically to um                                  sorry to write the result of a of a                                  query to my secret table right                                  my secret table is defined by this                                  with this ddl statements it's a simple                                  table called area counts                                  has two fields area id and account                                  it's backed by the jdbc connector and                                  this is the                                  the database they connect to its mysql                                  database table area accounts                                  user and password and so on                                  this is all runs in the dock container                                  in my secret docker container so                                  now if you say                                  describe                                  area counts                                  we get just the schema information back                                  if you now want to write into this table                                  let me maybe create                                  we can just go to the                                  to another terminal and if we now                                  connect to my sql                                  the table has been or the the my secret                                  table has been created before so                                  uh usually you would have also go to my                                  sql and then                                  um create a tablet there but uh we                                  already did that                                  so there's no need to create an extra                                  table in my sequence if you know                                  say                                  oops show tables                                  and run this you see there's exactly                                  this error accounts table                                  and if we say describe                                  area counts whoops                                  uh it has exactly it's um first we just                                  hit end and the other one is a big end                                  and this is how the table looks in my                                  sql and what we did                                  with this connector here is we basically                                  create a table in the flink catalog that                                  is backed by the                                  table in mysql if you know right to this                                  let's maybe check what's in the table                                  select                                  from every accounts                                  then this table is empty                                  and if you now start a static variant                                  flink that writes to this table                                  insert into                                  area counts                                  um select                                  what is it air                                  to area id i hope                                  long lut s                                  area id and                                  card star from                                  [Music]                                  rights group by                                  to area id long                                  blood oops                                  right and if you know start this query i                                  hope it's working oh yeah                                  um so now we see job id this                                  and um there seems to be something                                  running                                  if we select now here we see that there                                  are a couple of rows inserted into this                                  table                                  and it also seems that this data is                                  changing and it's changing because flink                                  automatically                                  writes the result of the of this group                                  by query                                  that we started into uh into my sql                                  and we can now also look how the query                                  looks like if we connect                                  to um                                  to the flink web ui and we see that this                                  security is running and it's exactly the                                  query that                                  i just wrote insert into area accounts                                  select                                  to area and so on we can look at the                                  query get the query plan here                                  it's a simple query runs with a paradism                                  of one                                  we have here this is flink tries to get                                  as much                                  computation as possible into a into a                                  processing node so this is actually the                                  source node                                  it's a projection down to only the                                  fields that we read                                  and if we would have added a filter also                                  the filter would have been in here                                  and then it does a hash partitioning                                  here okay hash partitioning with the                                  parallelism of                                  one doesn't really make sense but                                  imagine it would be like                                           um and then there's a group aggregate                                  here that                                  does the group by on the area id the                                  count                                  and uh the writing to the flink                                  to writing the data to the to the to the                                  sync                                  you can see here how many records have                                  been received and produced and                                  let me see if the queue is running with                                  checkpoints it's not running with the                                  checkpoint so                                  um if we would have configured the see                                  like line maybe we should do that in the                                  future                                  to have check pointing enabled then the                                  query would also be checkpointed as a                                  reg                                  just like a regular flink drop so um if                                  you have some experience with flink                                  before checkpoints of link's mechanism                                  to                                  um for our fault tolerance so you can                                  think of it                                  as basically that flink periodically                                  takes a                                  snapshot of all state in the in the of                                  the job                                  writes the state into a distributed                                  persistent durable storage like                                  s                                                    if some failure happens the whole                                  job gets                                  the the the checkpoint is uh taken                                  to re-initialize a new job and                                  since we did a copy of everything the                                  job                                  then looks exactly the same when it                                  continues it looks exactly the same                                  as as at the point in time when the                                  checkpoint was taken                                  and if you use transactional syncs then                                  actually                                  the whole behavior is as if nothing ever                                  happened                                  so even even if you had a failure                                  you will not see anything in your output                                  or in your state everything will be                                  um absolutely consistent all right so                                  this is basically now the query that we                                  are                                  running um i said before we cannot even                                  change it and                                  cancel the query here from the flink cli                                  so we have to do it from the dashboard                                  we just go here can click on cancel                                  cancel the job yes                                  come on                                  now the job is being cancelled                                  and also                                  yeah okay it's a bit hard now to see but                                  there are no more updates here on the                                  on the table in my sequel and obviously                                  you could now also                                  cure this my secret table from some some                                  other um                                  from from some other application or with                                  the dashboarding                                  application and since the table is                                  updated                                  pretty much in real well not exactly                                  real-time but with very low latency                                  you can you can get then                                  the application that reads from the my                                  secret table always gets the                                  freshest results                                  you
YouTube URL: https://www.youtube.com/watch?v=9QcuJ82m6Q4


