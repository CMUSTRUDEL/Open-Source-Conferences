Title: #bbuzz: Bruno Roustant – A Journey to Write a New Lucene PostingsFormat
Publication date: 2020-07-07
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/journey-write-new-lucene-postingsformat

Some hard technical challenges are better solved by changing the foundations. We had a use case of searching many fields with strong constraints on memory and performance. We needed a massive number of fields to support field level security at scale and open the path to machine learned ranking models.

A custom PostingsFormat allowed for a solution with greater efficiencies than our prior solution. We developed a new Lucene PostingsFormat called UniformSplit, we deployed it at a very large scale, and we open-sourced it. We learned a lot during the journey, especially about micro-benchmarking, java memory consumption, compact data representation and high performance Lucene indices.

This presentation is a good medium to share what we learned with step backwards, the learnings on the Lucene mechanisms, the tips and the pitfalls we encountered. And as we continued the development, we will share the latest works and production measures.
Captions: 
	                              hello I am no stone search engineer has                               set socks and listen Scirocco meter                               today I will talk about listen posting                               format and the journey we had to write a                               new one                               what is the posting format it's the                               structure and format of the listen                               inverted index it's a low-level code a                               foundation why did we need a new posting                                poet it's all about new challenges of a                                massive number of field at scale so                                let's have an overview of our journey to                                solve them and share our experience here                                is the agenda on the left the agenda                                sections and on the right the journey                                map each agenda section corresponds to a                                line in the map the white bullets on the                                map represent challenges and solutions                                while the orange triangles represent                                issues and obstacles that's a big                                journey filled with challenges issues                                and solutions let's start by putting                                some contexts and by describing the                                challenges Salesforce search is a search                                as a platform in a highly multi-tenant                                context at scale on the right we can                                have an idea of the scale on top of that                                we want to search a massive number of                                fields up to                                                          queries why because the documents are                                highly structured and diverse we enforce                                dynamic field level security one user                                may not be authorized to search all                                fields we also improve the relevancy by                                learning models which apply boosts per                                individual field                                so we have challenging constraints mm                                online charts pass or a node while                                searching more than                                                     approach was obviously to use the                                standard listen index we benchmarked it                                with our production data shape and scale                                and as anticipated it failed the node                                went quickly out of memory with terrible                                research query throughput before                                explaining the details                                let's look directly at the journey                                outcome given the constraints we                                developed a new recent posting format                                called uniform split it is designed to                                be simple and extensible the base                                version follows the posting format API                                and a regular field index approach and                                an extension called Shelton's makes it                                possible to store and access efficiently                                a massive number of fields we did a                                performance campaign we fixed issues and                                eventually the performance and memory                                are good except for spell check fuzzy                                queries we will explain that in a moment                                I would like to thank boss                                         Villegas do one and David smiley who                                contribute to this project now let's                                come back to the journey map to get more                                details so we have seen that we had very                                challenging constraint this is the first                                line of the map then we started to                                investigate solutions we needed a                                solution to reduce memory usage and                                increase query throughput our main                                challenge was to reduce memory usage not                                a small issue a serious out of memory                                break                                here is the structure of the Rossini                                inductive index on the left you can see                                the index layers and the order from top                                to bottom on the right the index                                structures corresponding to each index                                layer let's say we search the term big                                in the title field the recent index will                                get the title the term dictionary for                                the title field then it will seek the                                term big in the dictionary then                                depending on the query it will gather                                more data from the postings and payload                                such as the ideas of documents                                containing the term and the positions of                                the term in each document in our context                                we had a big memory issue because we                                have                                                                   in memory actually with loosing                                      dictionary is now kept on disk and not                                in memory this is called FST of it even                                if this may not be a memory issue                                anymore it becomes a disk access issue                                because there are so many blocks to load                                for a single query with so many fields                                so to solve the memory and performance                                issues the idea is to swap the index                                 layers between fields and terms and we                                 are we are going to do that now here we                                 swap on the Left the terms becomes the                                 first layer the fields becomes the                                 second layer the field information is                                 now stored per term in the index when we                                 seek the term big we now look at the                                 field information in the dictionary this                                 has                                 two advantages first we share the terms                                 of all fields in the single prefix tree                                 this is very compact compared to the                                     prefix trees we had before second our                                 use case is to search the same query in                                 all                                                             disjunction max query in our example we                                 searched the term B in three fields                                 title auto and body so now it becomes                                 possible to seek only ones the term                                 perfect tree we search once but we can                                 cache the datums data for all fields and                                 serve it for all of our field doing only                                 one index Ruka so we have the solution                                 to swap the fields and terms layers to                                 share terms between field but actually                                 there are various ways to implement that                                 we explore them we implemented initially                                 a virtual fields layer wrapping the rest                                 in standard block tree it works but it's                                 a trick with limitations we skipped                                 other options we even missed one                                 eventually we decided to design a new                                 posting format not only a specific to us                                 key to our use case but that could be                                 shared to the community so we were going                                 to design a new posting format this is a                                 complex work we had to have clear goals                                 and focus on them and we knew we had to                                 prepare to compromises because the                                 existing standard posting format is                                 highly optimized we took the opportunity                                 of an internship to explore                                 possibilities we brainstormed and                                 coming back to the Johnny map we saw the                                 challenges and the issues on memory and                                 performances we saw the investigations                                 and the Chatham's principle now we are                                 going to explore the recent posting                                 format API before looking at the detail                                 of the new posting format first what is                                 the posting for that it's the way we                                 store the investing index in listen we                                 get the posting format from the codec                                 the codec is composed of multiple parts                                 of the listen index such as posting                                 format but also dog values format stored                                 field format and others we will focus on                                 the posting format API only the posting                                 format API is layer                                                    fields consumer to write the English                                 index and a field producer to read it                                 and search it for a given field the                                 fields producer provides terms instances                                 which gives access to the terms in a                                 given field field dictionary let's take                                 an example if we search the term big in                                 the field title we access the fields                                 producer and we get the terms instance                                 colors corresponding to the field title                                 then each time we need to seek or                                 iterate through the terms of the fields                                 we create a stateful damson you tamson                                 'm allows us to browse the index and see                                 the term big we access the term prefix                                 corresponding to the field then we go                                 through the tree and we get the                                 corresponding term block to                                 look for the search term from there we                                 can further access the posting permit we                                 access the document list position                                 payloads depending on the query needs                                 coming back to our new posting format we                                 want to swap the field inside the terms                                 dictionary                                 this will be the shared terms uniform                                 speed extension actually we have to                                 design a new posting format up to terms                                 in the purple part here we can still                                 reuse a big building block the postings                                 annum which manages the postings and                                 payloads this is the green part here                                 okay we knew which API we had to                                 implement next step was to think about                                 the design of this new posting format so                                 what is uniform split it's a technique                                 to write the inverted index inductive                                 index is composable of blocks of terms                                 to split the search efficiently standard                                 blocked rebuilds the blocks based on the                                 common prefixes of the terms uniform                                 split builds the blocks by targeting a                                 uniform block size the number of terms                                 in a block to access those blocks we map                                 the blocks with an efficient and very                                 compact data structure the listen FST we                                 call block key the first term of each                                 each block and we store the block keys                                 in the FST                                 as we put uniformly the blocks                                 get a minimal number of blocks so the                                 FST size is minimized actually we don't                                 need the full term to distinguish a term                                 from its previous we can take the                                 minimal disk we distinguish in prefix                                 MDP instead for the same functionality                                 we can store block keys MDP instead of                                 the full term this reduces the FST                                 footprint however sometimes we can still                                 have long block keys MDP so there is a                                 good chance to find a smaller NDP within                                 the neighborhoods here in the example                                 instead of taking five as the key of the                                 third block we can select the term field                                 just before because it's MDP is smaller                                 f so we further optimize by selecting                                 the local optimal block keys with the                                 shortest MDP by looking at the FST on                                 the left we can see it holds less and                                 less data when we access a block we scan                                 sequentially the terms in the block this                                 means we can encode incrementally the                                 terms in the block on the right we see                                 the incremental encoding of the terms                                 this reduces the disk footprint and                                 speeds up both the block loading and                                 terms matching and last but not least we                                 start by first comparing with the middle                                 term in the block and we jump directly                                 there if the term we look for is after                                 this is like a first step of a binary                                 search inside the block dividing by two                                 the number of terms we have to scan                                 sequential                                 so this is all the optimization for the                                 uniform split principle at that point of                                 the journey we had our goals and design                                 defined we started to implement our new                                 posting format we implemented a base                                 version of uniform split with the                                 regular index layers ordering fills                                 first and terms                                 second this uniform split base was                                 extensible so we could continue with an                                 extension for the shuttle principal with                                 the index layer swapped terms first and                                 field second the extensibility allows                                 other extensions not necessarily                                 specific to our use case eventually we                                 implemented to posting formats one base                                 for general purpose and low memory                                 footprint and an extension to support                                 efficiently a massive number of fields                                 still extensible itself a world test                                 what about test implementing to new                                 posting format must require a huge                                 testing effort actually no we didn't                                 have to do much on this side hopefully                                 we could leverage the awesome listen                                 randomized test framework those tests                                 are testing a lot of Corner K is an                                 extreme usage and you can configure them                                 to run your code good we had the two                                 implementations well tested then of                                 course we need we needed to benchmark                                 the performance as we can see the                                 performance was going to be a journey in                                 we found multiple challenges to address                                 let's see the difference between being                                 functionally complete and                                 production-ready                                 the first version worked already seen                                 test passed and the shared extension                                 also worked we started to benchmark with                                 a first micro benchmark in dude we                                 didn't want to invest too much at this                                 stage this benchmarking has limitation                                 and it only guided us on some choices we                                 confirmed them in the next stages we                                 focused our goals on some specific                                 queries at this stage we didn't have the                                 right data volume but the performance                                 results were encouraging for the next                                 stage we leveraged the listen until                                 benchmarking commonly used in the scene                                 to compare codecs it when it benchmarks                                 a wide variety of queries for values                                 index Isis and actually we immediately                                 discovered that the short-term extension                                 had serious performance issues when                                 building index when merging the index                                 Iran segment so we customized the merge                                 method and the performance when good                                 running multiple benchmark we found also                                 problems with the memory allocation and                                 garbage garbage collection time we had                                 to review the object allocations and                                 reuse we changed some objects to become                                 mutable we also reviewed the lambda and                                 Java stream code that can lead to                                 unanticipated object allocations and                                 finally GC went good                                 finally we could compare uniform split                                 with the recent standard block tree post                                 in fact the speed for many term or                                 phrase queries was good for uniform                                 split                                 notice how the index size impacts the                                 relative importance of the posting                                 format in the lookup speed but with this                                 benchmark we discovered we add a serious                                 performance issues with the spell check                                 or further queries we investigate we                                 investigated way actually                                 the investigation and fixes spanned over                                 several Maltese we customized the term                                 intersect method which is called by                                 queries such as prefix wildcard spell                                 check fuzzy queries after two                                 unsatisfying attempts the third one gave                                 nice improvement available now since                                 recent                                                                                                                       this is a compromise we had to make                                 resonated benchmarks a wide variety of                                 queries but it doesn't measure memory                                 GC or disk the last step was to go to                                 shadow experiments and real productions                                 the final version of uniform split in                                 projection includes an additional cash                                 at posting format level driven by query                                 hints it is adapted to our use case                                 queries and data shape when we compared                                 the performance in production we could                                 see impressive gains compared to the                                 standard individual field approach and                                 we could see significant gain compared                                 to the virtual field layer                                 approach including a very important                                 saving on garbage collection activity as                                 a conclusion what did we learn from this                                 journey what was the outcome about                                 posting formats we learned a lot first                                 they are also a posting format not only                                 the standard block tree they are all                                 with dedicated use case there is a                                 perfect posting format to combine them                                 second you don't have to write                                 everything from scratch there are many                                 existing components to reuse the other                                 the recent test routes are extensive the                                 test if what is actually light and there                                 is also the reason until benchmarking                                 finally it's good to discuss early with                                 the community to get feedback so there                                 is a new posting format called uniform                                 split it's a good candidate for                                 extension and customization it has a                                 short-term extension to support massive                                 number of fields efficiently uniform                                 split is efficient for most queries less                                 for                                 we combined boss uniform split and                                 Brooktree with a per field botany                                 posting format - under fuzzy queries                                 that's it thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=av0yQY3pklA


