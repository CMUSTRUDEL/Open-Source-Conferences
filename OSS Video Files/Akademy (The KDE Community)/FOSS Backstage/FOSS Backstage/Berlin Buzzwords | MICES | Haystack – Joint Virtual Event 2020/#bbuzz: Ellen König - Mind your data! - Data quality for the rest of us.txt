Title: #bbuzz: Ellen König - Mind your data! - Data quality for the rest of us
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/mind-your-data-data-quality-rest-us

As software engineers, we take pride in our code quality. As data scientists, in the quality of our models and analyses. As a data engineer, I take pride in the quality of the datasets I provide access to.

Everyone in IT works with data one way or another, be it producing, managing or using it. Yet like water for fish, we often fail to notice data because it is all around us. And, just like fish in the water suffer bad water quality, we suffer if our data quality decreases. Unlike the fish in the water though, we can actually all contribute to addressing data quality issues.

In my talk, I want to encourage you to become more aware of data quality concerns. I will discuss what data quality is, how we can identify data quality issues and some strategies for addressing them. As a practical example, I will share our experiences with monitoring data quality using Amazon Research's deequ framework.
Captions: 
	                              hello and uh                               welcome to the myself live stream um                               we have um ellen kernish up to give the                               next talk um ellen                               is the senior data engineer at                               thoughtworks                               um and we'll be giving a talk on                               data quality the title is mind your data                               data quality                                for the rest of us let me just introduce                                ellen                                thank you very much cool so                                whenever i talk about data and data                                quality with the engineers                                i sometimes feel a bit like the fish in                                this this kind of story                                which you may be familiar with so                                there's an older fish meeting a younger                                fish in the water                                and the older fish is asking the younger                                fish what is water                                and the younger fish is very confused by                                this question he's asking                                no sorry the older fish is asking the                                younger fish how is the water today                                and the younger fish is very confused by                                this this question is                                asking like what is water and sometimes                                especially when we                                for us that work a lot of data i                                sometimes have the same feeling                                that we also so immersed in our data and                                our technology our software                                that we're not really always very                                precisely clear about what we're exactly                                dealing with and i think for especially                                when we talk about data quality it's                                really useful to have a                                very precise understanding of what data                                is that's why                                i would like to zoom out in the                                beginning of my talk a little bit and                                just                                clarify for a second what can how can we                                understand data quality                                and so if we look at data up in the                                dictionary the interesting thing is                                there are two definitions                                of data one is information is specially                                 on numbers                                collected to be examined and considered                                and used to help with making decisions                                for example the data shows that more                                than                                                               workforce is hispanic                                on the other hand there's also a                                technical definition                                and that's information in an electronic                                form that can be stored and processed by                                a computer for example                                the student's task was to prepare all                                the posters and electronic data                                for the publicity campaign and what i                                found really interesting when i looked                                this up was that these things are really                                two sides of the same code it's not one                                is more correct or one only applies in                                one circumstance                                but whenever we talk about data it's                                really both things that are relevant and                                while we as engineers we tend to look at                                data more in the sense of we can restore                                it and we process and that's all                                the whole team of this conflict is like                                throwing and scaling and streaming                                so it's all about this technical                                definition of data but when we talk                                about data quality it's really                                i find it really useful to keep in mind                                this other more business oriented                                definition of data which is                                data is about making decisions and it's                                helping us making these                                decisions and it's really both at the                                same time                                and so just like when about data there's                                different ways to look at it                                also if you look at data quality there's                                different ways to look at it                                so and so really what data quality is                                and what                                high quality data means is depends on                                who you ask for instances you ask                                the data consumers they usually take a                                more usage-oriented perspective                                they will ask things like does our data                                made our consumer's expectations                                does our data satisfy the requirements                                of its usage                                 on the other hand if we look at                                 it from the perspective of a business of                                 the business as a whole                                 then we usually might take a more value                                 oriented perspective                                 and we might ask ourselves things like                                 how much value are we getting out of our                                 data                                 how much are we willing to invest into                                 our data                                 and finally but certainly not last there                                 is an engineering perspective which is                                 probably the one                                 perspective on data quality that we are                                 most familiar with which is                                 oriented around standards and there we                                 might ask things like to which degree                                 does our data fulfill its specifications                                 and in particular how accurate complete                                 and timely is our data                                 and again it's not an either or thing                                 and all of these perspectives on data                                 have the                                 data quality have their own validity and                                 it's really about                                 from which perspective are we looking at                                 it and how can we be                                 empathetic if we if we are working                                 together on data quality                                 challenges how can we make use of all                                 these different perspectives                                 and in in the rest of my talk i would                                 like to                                 introduce you to a kind of running                                 example that i use                                 and it's even though this talk is based                                 a lot on the experience i have as a                                 data engineering consultant the example                                 i picked is                                 completely unrelated to my client                                 because it's unconfidential and i can                                 easily talk about it and kind of simple                                 and easy to illustrate things with it                                 and the example is about this point you                                 might                                 be wondering what's so interesting about                                 this front and nothing particularly                                 interesting about it it's just that's                                 the point in my mom's garden                                 and it's a pretty old pond it's about                                 two or three decades old my grandfather                                 built it originally                                 and as you can see it's full of algae                                 that's why my mom isn't really fond of                                 this pond                                 and she wants to get rid of it but i'm                                 personally as you might have seen from                                 the example with the fish in the                                 beginning                                 i'm really fond of ponds and i really                                 like this pond                                 and so i tried to convince my mom to                                 keep it and she told me                                 we can keep it if i've managed to get                                 rid of the algae so i cleaned up the                                 pond                                 and i i took out some algae but of                                 course the algae keep growing back and                                 so the question is now how can i stop                                 the algae growth                                 and i'm not really big on on pond                                 knowledge so i thought i could do an                                 experiment                                 to figure out why do these algae grow                                 and what what can i do to stop the algae                                 growth or at least reduce the algae                                 growth                                 and so i thought what i could do is do                                 some kind                                 turn this into some kind of data science                                 or analytics challenge                                 where we look at the different factors                                 that contribute to algae growth like the                                 water chemistry and the weather                                 conditions                                 and correlated or related to the                                 algae coverage to find figure out which                                 vectors are particularly contributing to                                 the algorithm                                 and then i might look monitor things                                 like the ph level the nitrate phosphate                                 and the hardness of the water                                 and also the sun hours per day and the                                 temperature each day                                 and the rainfall and then of course                                 what part of the pond is covered in                                 algae                                 and of course                                 as with any data collection challenge                                 there will be                                 data processing thing there will be data                                 quality issues that might come up                                 so the first challenge is that i don't                                 live next to the point so i would                                 have to ask my mom to get me the data                                 every day                                 and that might introduce a lot of issues                                 into the into the this whole into this                                 whole process                                 so for instance we don't really have                                 expensive equipment to measure                                 the water quality we have to rely on                                 these kind of hobbyist data called                                 um water water chemical kits                                 and they're not super reliable so the                                 measurements might be really noisy                                 another thing is that the algae coverage                                 is really                                 difficult to estimate consistently it's                                 easy to get say                                 one day estimate the same i'll recover                                 as                                    and the next day at                                                  to us wouldn't be a big difference but                                 if you show this and say a                                 logistic regression it might become                                 really sensitive to such inconsistently                                 estimated numbers                                 and also my mom might just forget to do                                 the measurements some days or maybe                                 she's having a bad day and she doesn't                                 want to leave the house or whatever                                 happens or maybe it's raining all day                                 there might be reasons why she doesn't                                 want to collect the data                                 and also as as she sends me the                                 measurements every day via email                                 they might get mixed up i might save                                 them in the wrong file i might                                 confuse them in my excel spreadsheet all                                 sorts of things might happen that                                 might get us to the wrong data and                                 finally the data might even get lost                                 some days maybe it's stuck in the email                                 ends up in my spam folder                                 or maybe she forgets to take a                                 measurement or maybe accidentally delete                                 the number or think kinds of things                                 might happen and even more things of                                 course might happen                                 and all of this would make the                                 prediction that i'm trying to run all                                 the correlations that i'm trying to                                 compute                                 really really unreliable and might not                                 help me much with my                                 undertaking and so that for that i was                                 asking myself                                 how can we invite that data quality                                 avoid data quality issues                                 and again i would like to                                 first take a bit into the look into the                                 theory to clarify some                                 some answer define some roles here and                                 so when we talk about data quality                                 management                                 in in the literature there's you usually                                 find three different kind of roles which                                 is the data producers and the data                                 consumers as the business world                                 and they're kind of what what the name                                 implies so the data producers are the                                 people that create collect and                                 maintain the data so in the example that                                 might be my mom gathering the                                 measurements in the spreadsheet                                 and the data consumers are the people                                 that use the data in their work                                 activities                                 so in our example that might be me                                 trying to understand the correlations                                 on the other hand there's also a                                 technical role and those are called data                                 custodians                                 and those people are what we commonly                                 also understand nowadays as data                                 engineers                                 they design develop and operate the data                                 infrastructure                                 and that might be me building a python                                 script for the analysis based on the                                 email spreadsheet                                 and the interesting thing is each of                                 these rules can contribute to data                                 quality                                 for example as a data producer it's                                 important to enter the right data                                 or to validate the label data correctly                                 correct eight                                 errors in entries and labels                                 and                                 and sorry enter the data on time                                 and as a data consumer on the other hand                                 it's important that i check the                                 plausibility of the data if there's                                 nothing really weird in it                                 that i interpret the data carefully and                                 data report data quality issues so that                                 they can be fixed                                 and last but not least as a custodian                                 the person that's kind of in between the                                 producers and the consumers                                 um we we can we need to make sure that                                 we've done                                 validate our data transformations that                                 we monitor data quality that we report                                 data quality issues                                 and that will deliver data on time                                 and of course there are other things so                                 we can contribute these are just a few                                 examples                                 and now i would like to dig a bit deeper                                 because                                 of the audience of this conference                                 because into the data custodian world                                 and i would like to dig deeper in how we                                 can measure data quality                                 and so for the first the tool we usually                                 use to                                 measure data quality there's a concept                                 of the data quality dimension                                 and the data quality dimension is a set                                 of data quality attributes                                 that represents a single aspect or                                 construct of data quality again that's a                                 definition that's commonly used in the                                 literature on this topic                                 and the interesting thing about the data                                 quality dimension is that it connects a                                 lot of different concepts in data                                 quality management                                 so for example the data quality                                 dimension is tied to a data quality                                 perspective you remember that we had the                                 usage perspective the value                                 perspective and the standards                                 perspective and for each of them we can                                 define different dimensions that help us                                 understand these perspectives so for                                 instance for the user's perspective                                 it might be relevance for the value                                 perspective it might be value added                                 and for the standard perspective it                                 might be uniqueness                                 and then based on these dimensions we                                 can find specific metrics that let us                                 measure the quality for each dimension                                 so we can get an                                 estimate about how good the data is in                                 each of these dimensions                                 and then based on these findings we can                                 finally design a data quality                                 improvement strategy that's specific to                                 the dimension                                 and of course now the interesting                                 question in                                 a tech conference is of course we don't                                 want to measure all of this by hand                                 we could in theory but it would be                                 really really tedious especially if we                                 have large data sets it might be                                 even really painful and really expensive                                 so the question is what dimension can we                                 measure automatically and for me it was                                 a really interesting realization of                                 finding that                                 a lot of dimensions can really not be                                 measured automatically so in                                 if we want to measure them we need some                                 kind of human judgment involved                                 that might be things common dimensions                                 that are mentioned in                                 as desirable are things like                                 interpretability of data                                 appropriate volume ease of understanding                                 ease of access security relevance value                                 added believability                                 and so on and so forth but there are a                                 few things that we can measure and we                                 can measure them at different levels                                 so we can for instance measure them at a                                 data point level things like accuracy                                 completeness of field values and other                                 things are on the data set level things                                 like completeness of the data set                                 uniqueness of data sets and timeliness                                 and again other things                                 but i found it really interesting that a                                 lot of what is considered desirable in                                 terms of data quality really cannot be                                 measured                                 technically but there are a few things                                 nevertheless that we can measure                                 automatically and so we                                 while we can get a full picture with our                                 automated delta quality reports                                 we can at least get some kind of picture                                 from our data quality reports                                 and so as the last kind of theoretical                                 input that i'll give today there's the                                 two strategies for data quality                                 validation                                 it's really there's a lot of different a                                 lot of different ways and a lot of                                 different tooling out there to measure                                 data quality                                 but really they all boil down to two                                 different strategies i found one is                                 rule based which work well whenever we                                 can                                 define absolute references for data                                 quality                                 so for example if we had                                 if we look at my metric of algae                                 coverage we can say the aggregate                                 coverage must never be empty                                 because as soon as it's empty we cannot                                 use the whole day daily data point                                 because we can correlate our other                                 fields with the                                 what we want to predict or call it and                                 another thing that's very a kind of                                 obvious fixed rule is sun hours must be                                 between                                                               pretty unlikely but definitely                                                                                                     error so we use rule-based validation                                 strategies                                 for conditions this must be met in any                                 case for the data to be valid                                 and there's other things we can also                                 define on the data set level                                 for example there must be exactly seven                                 entries per week                                 and all dates must be unique these are                                 all routes we can define and if they're                                 broken there's definitely something                                 wrong now interestingly there's also a                                 more fuzzy way to                                 measure define data quality or validate                                 data                                 and that's with anonymous detection                                 which is a conflict that's                                 or at least used to be i'm not sure if                                 it's still as widely talked about but                                 for a while it wasn't a really hype                                 topic                                 and but really what it boils down to is                                 whenever                                 it it's data anonymously based detection                                 works well whenever we can define data                                 quality                                 relative to other data points so that's                                 often used                                 when we have spikes or drops in time                                 serious data                                 so then we the relative means we look at                                 relative to the past                                 through the historic data                                 and another way to cite it according to                                 wikipedia is                                 anonymously detection is the event                                 identification of rare                                 items events or observations which rise                                 suspicions                                 so again that points to the fact the                                 interesting thing is                                 with the fixed rules we definitely if                                 they are violated we definitely know                                 something is wrong                                 whereas with anonymous detection we just                                 know there is something off                                 but it's not necessarily wrong it might                                 actually be a valid measure                                 just something might have drastically                                 changed in the data and collecting                                 environment that made the                                 that made the data change for example                                 if we you remember the ph value of the                                 water quality                                 that we could say the ph value should                                 not change dramatically                                 saying not more than                                                    could change more for instance if you                                 pull a bunch of                                 acid or some other chemical into the                                 water or                                 if it just rains a lot and the whole                                 water gets flooded out                                 then the ph value might change                                 dramatically and it would be a perfectly                                 very                                 valid measure to say that a test chained                                 more than                                    but unless something really dramatic                                 happened to the point                                 probably if it changes more than                                       might be off                                 and similarly the number of measures                                 should only be increasing over time if                                 you want to have an example on the data                                 set level                                 and now i would like to share some                                 experiences with you to make it a bit                                 more practical                                 about how that i had with monitoring or                                 my team had with monitoring                                 data quality and data patterns i'll                                 stick to the example but the                                 the evaluation and the tooling                                 experiences are really based on a client                                 project                                 i had i just anonymized the tools to                                 conform with the                                 with the pond example and so let's                                 imagine                                 right now my mom sends me an email with                                 the data                                 and then i have around a small titan                                 script and it put outputs some graphs                                 and some                                 some nice correlation or regression                                 coefficients or whatever i want to do                                 and that tells me the water quality but                                 the thing is                                 this thing needs to run a lot of times                                 because you can't based on just a few                                 data points you can't really analyze                                 correlations that well so we need to run                                 this say over three months or something                                 somewhere                                 some longer period and that means a lot                                 of data and it's getting really serious                                 and tesios also means error prone and                                 that's all things we can avoid by just                                 automating things                                 and so let's imagine i build a kind of                                 small data pipeline                                 with a and i build this kind of cdci                                 pipeline to deploy my code                                 automatically in all the things we like                                 to do in software engineering                                 and then what we can introduce is the                                 idea of a data quality gate                                 so data quality gates are an idea that                                 comes from continuous improvement                                 continuous deployment                                 and it's something that can be used to                                 increase the confidence towards the                                 deployed service                                 and most precisely a quality gate is an                                 automated checkpoint which the deployed                                 artifact needs to pass to girl life                                 for code these are things we know very                                 well so this might be codes at different                                 levels so integration tests unit tests                                 acceptance tests                                 service level tests usability tests all                                 sorts of level of tests that i think                                 something has to pass                                 but it might also be things like study                                 code analysis                                 so different kind of things that our                                 code has to pass in order to                                 to be considered production ready and                                 for data we can define something very                                 similar actually                                 which is we can use those different                                 validation strategies                                 and define based on those our automatic                                 checks and our nominal                                 base detect validation to figure out                                 is this data set ready to go put into                                 production or is there something that we                                 should check                                 or fix and so                                 i could extend my little pipeline by not                                 only having a script that reads the                                 the spreadsheets from the                                 email account but also have my add my                                 validation checks                                 and anonymous detection steps and only                                 then                                 use the data to compute my analysis                                 and then i was wondering what tool                                 should i use and i came up with this                                 kind of little checklist which tells me                                 um what are the requirements for data                                 quality monitoring tools                                 and there are few things i came up with                                 and i think i have to speed up a little                                 bit                                 things like it can compute the needed                                 metrics can perform static checks                                 can alert on validation failures can                                 visualize the current and historic state                                 of the metrics                                 for and can integrate into our existing                                 tool chain                                 and there were a few there's actually                                 quite a few data quality companies                                 coming up right now                                 but some examples i looked at include                                 spark for scalabase dq which is the one                                 i'll                                 talk about later in the rest of my talk                                 and there's also a patent-based                                 framework called great expectations from                                 superconductive                                 and the queue which we used in our                                 project                                 is as i mentioned it's it's produced by                                 aws labs                                 and it's a scala library for data                                 quality validation on large data sets                                 using spark                                 so you wouldn't actually use it on a                                 small use case like my point example                                 but usually when we when we don't                                 usually build data pipelines for such                                 small examples so when we have a                                 reasonably large data set it makes sense                                 to you but it actually makes sense to                                 deploy spark it also                                 could be a good idea to use something                                 like the queue                                 and the nice thing about the queue is it                                 provides both rule                                 based validations and anomaly based data                                 validation so it provides a lot of                                 features that we can use                                 and it's also fairly easy to use                                 actually which was plus                                 was a was a pleasant experience so                                 for instance if we want to do a rule                                 based validation all we have to do is                                 the whole example fits onto one slide so                                 first we instantiate the verification                                 suit which is the                                 kind of wrapper that we use to institute                                 call                                 start validation then we define the data                                 set                                 we add a check with the daisy chain that                                 and for instance here we define three                                 checks                                 one that the data set has seven rows so                                 one for each day                                 um that the algae coverage field                                 should never be empty and that the data                                 is unique which you might remember were                                 all                                 things we defined earlier as examples                                 then we run the whole thing                                 and then we just check the status of the                                 verification result                                 and if it's not a success we can do                                 something here we're just printing out                                 that we found an error                                 but we could also throw an exception or                                 we could                                 notify our logging system or whatever                                 you want to do with it                                 and very similarly an anonymously based                                 validation                                 uses a similar schema it's not much                                 longer it's a little bit                                 more complicated but not by much again                                 we start our verification                                 suit then we define uh call our own data                                 method to define on the dataset we want                                 to use                                 and then we do something slightly                                 different which is we use in the                                 repository                                 and the idea of a repository is that we                                 need some place to keep track of the                                 historic data                                 that's what we do in this repository                                 it's it's usually a kind of json file                                 where you just append                                 the data it could be residing say on s                                  or someplace                                 and then you'd with the next call you                                 append save or append the results of the                                 validation that you just run so your                                 match computed metrics                                 and then then you do the actual                                 anonymity detection                                 and the queue here uses a very simple                                 strategy it just compares to                                 the last day data point with the current                                 data point and says                                 and looks at what the decrease is and as                                 you might remember we wanted to make                                 sure that our data set size never                                 decreases so we just define it should                                 never decrease                                 and we run this on the size of the data                                 set                                 and finally again                                 we check whether our status was                                 successful or not and                                 if it wasn't successful we just say okay                                 there was an anonymity detected                                 or we do something whatever else we want                                 to do with the status                                 so that's so that is pretty cool                                 actually we found and it's very easy to                                 add this to any kind of spark                                 based data pipeline and also the other                                 really cool thing about the queues                                 there's a lot of these different kind of                                 metrics that you can use                                 and they can operate both on columns of                                 the data set and on the entire data sets                                 and they can they can be fairly                                 consistently used for both rule and                                 anonymously based validation                                 and they provide a lot of example                                 metrics                                 so for data point level dimensions on                                 data set level dimensions                                 and that is also pretty that that means                                 you can in theory run some very powerful                                 data validation if you want to but of                                 course like everything everything has                                 drawbacks as well                                 and so what we found is                                 that the queue is a really promising                                 framework but                                 it has some quirks still where it might                                 not be completely ready for production                                 or at least for complex                                 use cases so on the positive side                                 the queue is really fast because it's                                 directly implemented in spark and                                 it computes its rules checks and both                                 and even the anonymous detection really                                 fast                                 as you can have seen the validation can                                 be implemented with very little code                                 there are lots of metrics to choose from                                 as you've also seen                                 and the library code is fairly easy to                                 understand for digging deeper than the                                 examples                                 and it's under it's on the very active                                 development which is also nice so                                 even some of the things i'm saying right                                 now might be already outdated because we                                 we did a project couple of months ago                                 and the library has already changed                                 but the limitations are that right still                                 at this very point                                 the documentation is very limited so                                 there's very few documentation of                                 concepts                                 and most of the usage is just basic                                 examples so if you if you want to use                                 all the power that the queue seems to                                 provide                                 you really need to dig into the code to                                 understand what it's doing and figure it                                 out by yourself what it's doing and                                 there were also a few                                 bugs or at least one book where we found                                 that we had a false positive due to a                                 sampling issue                                 and now i would like to wrap up my talk                                 so                                 as you might remember from the beginning                                 data has to cite                                 informat information for decision making                                 and information in electronic forms and                                 both of these really are two sides of                                 the same coin                                 and when we talk about data quality                                 management both of these definitions are                                 useful and relevant and likewise                                 data quality also has different three                                 different perspectives that we can take                                 on it                                 and it we can look at it from a user's                                 perspective from a business value                                 perspective and from an                                 engineering standard perspective and all                                 of us contribute to high data quality                                 whether we're data consumers data                                 producers or data processors                                 and those engineers our main role is to                                 automate the validation and monitoring                                 that was my talk and now i'm happy to                                 answer questions if there were any                                 hi there ellen uh thanks for that that                                 was a really interesting talk                                 um i'm a conference organizer and                                 not a data scientist but i still got                                 quite a lot out of that                                 um we have some time for some questions                                 now so                                 um if you want to ask a question you                                 can ask it in the slack we have one from                                 matthias um atheist asks um                                 do you use any machine learning models                                 to validate your                                 data quality i personally do not i'm                                 sure                                 i mean especially if you we talk about                                 complex anomaly detection there are                                 definitely machine learning                                 models you could use but i don't have                                 experience with that so i                                 i don't i would also try whenever i can                                 to keep the                                 data validation methods as simple as                                 possible and there's a lot of things you                                 can do with this basic checks and basic                                 strategies for anonymous detections                                 because you don't really want to add                                 extra complexity and something where you                                 already aren't totally sure whether it's                                 an issue or not                                 and the simpler the strategy the easier                                 it is to figure out what happened                                 and why the issue occurred                                 okay um i can see that um some people                                 that                                 are typing some questions in the at the                                 moment so hopefully they will come                                 through                                 soon um while we wait i wonder                                 if you can give us an update on the pond                                 how is the pond doing now                                 the pond is still kind of full of algae                                 and i really need to clean it again so                                 we're still collecting data okay                                 in process good um                                 so um                                 we have a more a comment from lars                                 lazadson says finally a talk that's fast                                 enough to keep me awake                                 i have the habit of watching recorded                                 presentations at                                            um so do we have                                 any more questions write them down we                                 have someone                                 writing a question                                 uh yep we have a question from alex oh                                 it says um are there methods                                 uh to assigning data quality at the                                 point of use                                 based on usage                                 i'm not sure i totally got the question                                 could you repeat the question perhaps                                 yeah are there methods                                 of assessing data quality at the point                                 of use                                 or based on usage                                 i don't know to be honest there's                                 i mean in the sense of what you                                 what i could imagine yourself doing is                                 you can definitely if you have a                                 say in an application that uses data                                 you can track where your data is used                                 and where people get confused and say if                                 you have                                 if you have a dashboard in your                                 application you can see where people get                                 stuck                                 and that's something we've actually                                 tried out at the                                 at the client where i was previously                                 working                                 but i'm not sure if they're systematic                                 methods so it's it's a very                                 custom field way of figuring and it's a                                 very high level                                 it's a very abstract because you might                                 find out so for example that at the                                 point of usage                                 people always get stuck at a certain                                 metrics that they look at                                 but then you still don't know why                                 they're they've they get stuck at this                                 matter so                                 it's def there's definitely some                                 indicators you can get safe from                                 tracking data                                 but it's it's further removed than if                                 you say analyze the data                                 somewhere in your pipeline or the cl                                 generally the closer the                                 to your to production you ano you figure                                 out the data quality issue                                 the easier it is also to fix and the                                 easier it is to detect                                 does that answer a question or did i get                                 in a totally different direction than                                 what you were asking for                                 we will see um                                 while we wait uh we have another                                 question um is there any relation                                 between how you code test your pipelines                                 and the data quality tests                                 um yes there is actually so                                 if you when we especially when we talk                                 about data quality                                 sorry data transformations that we do in                                 pipeline say we aggregate some data                                 we combine some data sets and all these                                 kind of things                                 those testing those transformations                                 definitely contributes to higher quality                                 of data so                                 as soon as you as you have any bugs in                                 in your basic data transformations                                 then of course those propagate into your                                 data quality                                 so so data so the testing the pipeline                                 the code tests are definitely a part of                                 ensuring data quality                                 okay and we have one more um                                 so from a higher level who needs to take                                 care of data quality in a company                                 well i would say anyone who's involved                                 in the data                                 that's why i have this point on my                                 summary slide um                                 i mean there's becau                                 there's different layers to this at this                                 question so the first level is of course                                 everybody who's involved somewhere in                                 either producing data or consuming data                                 or then using the data say by                                 making sure that the data is plausible                                 and all these kind of things                                 that they also that deal directly with                                 the data they're definitely all                                 responsible for data quality                                 but my experience is that data quality                                 initiatives because they are kind of                                 across different departments and that                                 they cannot just be                                 located in inside the data team or i.t                                 department                                 traditionally or wherever them whoever                                 might own the databases and maybe the                                 applications                                 so that definitely needs to be if the                                 data is considered valuable enough to                                 actually invest in data quality there                                 also needs to be some                                 senior level commitment in into the data                                 quality management                                 okay cool thank you um                                 uh we can uh continue this chat in the                                 breakout room afterwards i will                                 um post a link to the jitsi room so if                                 anyone would like to continue the                                 discussion                                 that's where it will take place um yeah                                 check the slack for that and                                 just wanted to say thank you again ellen                                 for a great talk                                 thanks for being with us thanks for                                 having me                                 you
YouTube URL: https://www.youtube.com/watch?v=98BkfAF4zHY


