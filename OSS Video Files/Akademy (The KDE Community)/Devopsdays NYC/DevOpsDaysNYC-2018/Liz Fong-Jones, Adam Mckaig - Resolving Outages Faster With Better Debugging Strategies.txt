Title: Liz Fong-Jones, Adam Mckaig - Resolving Outages Faster With Better Debugging Strategies
Publication date: 2018-02-01
Playlist: DevOpsDaysNYC-2018
Description: 
	Engineers spend a lot of time building dashboards to improve monitoring but still spend a lot of time trying to figure out what’s going on and how to fix it when they get paged. Building more dashboards isn’t the solution, using dynamic query evaluation and integrating tracing is.

https://www.devopsdays.org/events/2018-new-york-city/program/fong-jones-mckaig/
Captions: 
	00:00:00,000 --> 00:00:18,210
[Music]

00:00:15,170 --> 00:00:20,039
so my name is bliss function I'm joined

00:00:18,210 --> 00:00:23,910
by Adam McKay and we're both from the

00:00:20,039 --> 00:00:26,880
site reliability engineering school so I

00:00:23,910 --> 00:00:29,429
wanted to give a brief idea of creative

00:00:26,880 --> 00:00:31,920
the expertise that we never bring of how

00:00:29,429 --> 00:00:36,930
we think about the mugging system so

00:00:31,920 --> 00:00:38,520
I've spent a while at Google it's been

00:00:36,930 --> 00:00:40,290
on eight different teams so I've seen a

00:00:38,520 --> 00:00:42,840
whole wide range of different ways of

00:00:40,290 --> 00:00:47,190
teams cope with using tooling and

00:00:42,840 --> 00:00:49,920
process and techniques and right now my

00:00:47,190 --> 00:00:51,539
day job is I work with the customer

00:00:49,920 --> 00:00:53,160
reliability engineering team cheaper

00:00:51,539 --> 00:00:54,390
customers how do you give up

00:00:53,160 --> 00:00:56,190
how do you do x-ray because at the end

00:00:54,390 --> 00:00:58,399
of the day every is a way of doing a

00:00:56,190 --> 00:00:58,399
loss

00:00:58,559 --> 00:01:04,500
my name is Ida McCabe and I joined yes I

00:01:01,949 --> 00:01:06,360
were equally fat one year ago I'm still

00:01:04,500 --> 00:01:10,740
relatively new the reason I'm talking

00:01:06,360 --> 00:01:12,780
today is when I joined Google I was kind

00:01:10,740 --> 00:01:14,369
of blown away that they are operating

00:01:12,780 --> 00:01:17,400
systems much much bigger than anything

00:01:14,369 --> 00:01:19,320
I've ever touched before but things go

00:01:17,400 --> 00:01:21,330
wrong like like at every company we had

00:01:19,320 --> 00:01:23,340
production altitude but somehow they

00:01:21,330 --> 00:01:26,460
were able to solve those outages using

00:01:23,340 --> 00:01:28,740
tools and techniques which one magical

00:01:26,460 --> 00:01:29,880
but they were so so sophisticated

00:01:28,740 --> 00:01:31,049
compared to what I've been using before

00:01:29,880 --> 00:01:33,329
they sort of blew me away

00:01:31,049 --> 00:01:37,280
so this Punk is really aimed somewhere

00:01:33,329 --> 00:01:37,280
at myself but 18 months ago

00:01:39,780 --> 00:01:45,510
after just to say you can adopt in your

00:01:41,909 --> 00:01:47,670
own organization maybe help them so

00:01:45,510 --> 00:01:49,229
let's talk about some grabbing details

00:01:47,670 --> 00:01:51,570
like what do we assume that you're

00:01:49,229 --> 00:01:53,520
already doing in order to kind of get to

00:01:51,570 --> 00:01:55,890
most of these techniques so we're

00:01:53,520 --> 00:01:57,360
assuming that you have a service or

00:01:55,890 --> 00:01:59,280
collection of services they have some

00:01:57,360 --> 00:02:01,229
micro services and we're assuming that

00:01:59,280 --> 00:02:03,119
you have maybe like a kubernetes cluster

00:02:01,229 --> 00:02:04,799
or that you're running across the suite

00:02:03,119 --> 00:02:06,360
of maybe dozens of them straight back

00:02:04,799 --> 00:02:08,489
hundreds not thousands but at least

00:02:06,360 --> 00:02:10,259
dozens of those since then you don't

00:02:08,489 --> 00:02:12,150
want them to have pet names anymore

00:02:10,259 --> 00:02:13,440
and we're also should mean that you're

00:02:12,150 --> 00:02:14,760
exploring some kind of measurements

00:02:13,440 --> 00:02:15,840
because you want to be able to debug

00:02:14,760 --> 00:02:17,790
your system you want to be able to

00:02:15,840 --> 00:02:20,580
figure out what's going on inside of

00:02:17,790 --> 00:02:22,290
your system but the problem is when you

00:02:20,580 --> 00:02:23,970
multiply all this out the number of

00:02:22,290 --> 00:02:25,350
hosts by the number of distinct metrics

00:02:23,970 --> 00:02:26,670
you're exploring we're assuming that

00:02:25,350 --> 00:02:28,739
you're struggling the struggle of

00:02:26,670 --> 00:02:32,040
complexity that you're starting to

00:02:28,739 --> 00:02:34,080
struggle with how do I understand what's

00:02:32,040 --> 00:02:36,300
going on each post from the inmate

00:02:34,080 --> 00:02:38,430
system and also understand what's going

00:02:36,300 --> 00:02:40,950
on at the top level of my entire

00:02:38,430 --> 00:02:42,000
intermediate system if I'm like we

00:02:40,950 --> 00:02:43,380
assume as you know some degree of

00:02:42,000 --> 00:02:44,850
redundancy you have some degree of

00:02:43,380 --> 00:02:46,410
partitioning on your service whether

00:02:44,850 --> 00:02:49,799
it's across availability zones or

00:02:46,410 --> 00:02:51,329
regions and lastly for the purpose of

00:02:49,799 --> 00:02:52,799
imagination we're assuming that you're

00:02:51,329 --> 00:02:54,390
serving and using requests because

00:02:52,799 --> 00:02:56,010
that's workloads are a completely

00:02:54,390 --> 00:02:58,549
different problem that requires

00:02:56,010 --> 00:03:01,320
different sense and she needs to solve

00:02:58,549 --> 00:03:03,269
so let's assume that your system is

00:03:01,320 --> 00:03:06,299
merrily right along and then something

00:03:03,269 --> 00:03:08,459
breaks how do we find out well some

00:03:06,299 --> 00:03:09,780
people wind up saying we want to just

00:03:08,459 --> 00:03:11,190
mother everything in the instant

00:03:09,780 --> 00:03:12,420
anything looks anywhere that's going to

00:03:11,190 --> 00:03:13,769
hate us that's going to cause problems

00:03:12,420 --> 00:03:16,410
because you're going to be doing all the

00:03:13,769 --> 00:03:18,390
time so we're assuming that you realized

00:03:16,410 --> 00:03:20,730
that things get really goes really fast

00:03:18,390 --> 00:03:22,650
and said you're focusing on user paint

00:03:20,730 --> 00:03:24,570
they have a service level indicator that

00:03:22,650 --> 00:03:26,310
says you just receive errors at your

00:03:24,570 --> 00:03:28,200
hyper rate and they have some kind of

00:03:26,310 --> 00:03:30,510
sort of service level objective or air

00:03:28,200 --> 00:03:31,920
above it that says here's how long were

00:03:30,510 --> 00:03:33,150
allowed to be down and if we start

00:03:31,920 --> 00:03:35,250
looking like we're going to be down

00:03:33,150 --> 00:03:38,760
we're running out too much then we need

00:03:35,250 --> 00:03:41,910
to go and pay someone so we get Paige

00:03:38,760 --> 00:03:45,079
Noah we need to prioritize mitigating

00:03:41,910 --> 00:03:45,079
and root causing this

00:03:45,530 --> 00:03:50,400
so I want to quickly go through this

00:03:48,150 --> 00:03:52,349
kind of way that we model people's

00:03:50,400 --> 00:03:55,110
response some of you may have seen it in

00:03:52,349 --> 00:03:57,599
the department who do loop Orion sorry

00:03:55,110 --> 00:03:58,950
observe orient decide act but this is

00:03:57,599 --> 00:04:01,220
kind of market oriented so you're

00:03:58,950 --> 00:04:04,170
getting your SLO something breaks and

00:04:01,220 --> 00:04:05,790
then you get page so what do you do well

00:04:04,170 --> 00:04:08,099
first we have to do the blast radius

00:04:05,790 --> 00:04:09,329
great are we having the bullet do I need

00:04:08,099 --> 00:04:11,129
to bring more people into getting

00:04:09,329 --> 00:04:12,720
response right now ways and Raider

00:04:11,129 --> 00:04:15,090
region right now in order to stop the

00:04:12,720 --> 00:04:17,729
heat but after that there's this

00:04:15,090 --> 00:04:19,500
critical loop that happens of we need to

00:04:17,729 --> 00:04:20,910
formulate a hypothesis I want to feel

00:04:19,500 --> 00:04:22,530
get bogged down here and say I wouldn't

00:04:20,910 --> 00:04:23,880
look at my list with 50 dashboards and

00:04:22,530 --> 00:04:25,350
CEO there I can't figure out what broke

00:04:23,880 --> 00:04:26,880
which would these one squiggly lines

00:04:25,350 --> 00:04:29,400
changes at the same house another

00:04:26,880 --> 00:04:30,510
squiggly line so we formulate a

00:04:29,400 --> 00:04:32,220
hypothesis and then we do some

00:04:30,510 --> 00:04:33,889
experiments we cast as we grow is this

00:04:32,220 --> 00:04:37,110
actually the thing that grow this and

00:04:33,889 --> 00:04:38,970
then we develop a fix and then we test

00:04:37,110 --> 00:04:40,680
it or fix actually working production

00:04:38,970 --> 00:04:42,590
and if you're lucky everything does not

00:04:40,680 --> 00:04:46,110
work against our leader within a flow

00:04:42,590 --> 00:04:49,020
but the problem is sometimes that you're

00:04:46,110 --> 00:04:51,120
wrong sometimes your fix doesn't work or

00:04:49,020 --> 00:04:52,849
sometimes your hypothesis is completely

00:04:51,120 --> 00:04:54,990
wrong and the things thought was wrong

00:04:52,849 --> 00:04:56,669
actually isn't the problem in your

00:04:54,990 --> 00:05:01,229
system and then you're chasing a ghost

00:04:56,669 --> 00:05:03,570
rather than the real problem so we think

00:05:01,229 --> 00:05:05,490
that it's important to focus on how do

00:05:03,570 --> 00:05:07,440
you reduce that time of hypothesis

00:05:05,490 --> 00:05:09,240
formulation and testing to get your

00:05:07,440 --> 00:05:11,760
outages resolved faster and root cause

00:05:09,240 --> 00:05:13,889
faster so we're going to work a real

00:05:11,760 --> 00:05:16,229
example that has three key techniques

00:05:13,889 --> 00:05:17,400
that are looking at it for you first of

00:05:16,229 --> 00:05:20,130
all is layered healing

00:05:17,400 --> 00:05:23,039
second of all is doing arbitrary data

00:05:20,130 --> 00:05:25,169
joints and finally is the concept of

00:05:23,039 --> 00:05:27,090
mashing together your distributed traces

00:05:25,169 --> 00:05:30,240
or high cardinality tags

00:05:27,090 --> 00:05:31,620
together with your distribution data so

00:05:30,240 --> 00:05:33,840
those are the three techniques that we

00:05:31,620 --> 00:05:35,400
aim to demonstrate for you and one

00:05:33,840 --> 00:05:37,500
important note about technologies here

00:05:35,400 --> 00:05:39,930
yes we're showing you for the first time

00:05:37,500 --> 00:05:41,400
a set of UI tooling and a set of

00:05:39,930 --> 00:05:44,010
back-end stuff that Google does

00:05:41,400 --> 00:05:47,789
internally but a lot of this stuff is

00:05:44,010 --> 00:05:49,860
perfectly possible to do with any with

00:05:47,789 --> 00:05:51,240
virtually any successful

00:05:49,860 --> 00:05:54,330
whether it's for fun out of weather

00:05:51,240 --> 00:05:56,700
whether it's whether it's Zipkin you can

00:05:54,330 --> 00:05:58,620
do this with let's technology that's out

00:05:56,700 --> 00:06:00,750
there right now so we'll make public

00:05:58,620 --> 00:06:02,430
invitations were possible and if there

00:06:00,750 --> 00:06:03,990
are some guests will highlight what the

00:06:02,430 --> 00:06:05,400
gaps are so maybe you'll feel on inspire

00:06:03,990 --> 00:06:08,100
to work on kind of closing we can

00:06:05,400 --> 00:06:11,550
already see what's possible and finally

00:06:08,100 --> 00:06:13,110
because we do work on UCP so the staff

00:06:11,550 --> 00:06:14,550
river does not completely influenced

00:06:13,110 --> 00:06:16,980
some of these things we have these guys

00:06:14,550 --> 00:06:18,780
too and we're hoping that by kind of

00:06:16,980 --> 00:06:20,850
driving people to think about how you

00:06:18,780 --> 00:06:22,290
can do debugging better that one of

00:06:20,850 --> 00:06:28,980
those features eventually wins they do

00:06:22,290 --> 00:06:35,990
we prepare to use them you're Peter this

00:06:28,980 --> 00:06:35,990
is so so I get paged about these things

00:06:38,570 --> 00:06:42,300
there are too many fluid queries and it

00:06:40,890 --> 00:06:44,580
looks like that's happening to us West

00:06:42,300 --> 00:06:46,050
one but that's really all I can tell at

00:06:44,580 --> 00:06:48,120
this point I don't really have very much

00:06:46,050 --> 00:06:50,520
we could we could try and stuff a little

00:06:48,120 --> 00:06:52,380
bit of context into our desire to know

00:06:50,520 --> 00:06:53,160
the patient's but let's mention there

00:06:52,380 --> 00:06:54,570
were so many things that could

00:06:53,160 --> 00:06:56,130
potentially be going wrong with so many

00:06:54,570 --> 00:06:58,350
posts that could be going wrong it would

00:06:56,130 --> 00:07:11,960
be completely overwhelming so the first

00:06:58,350 --> 00:07:14,250
thing that we do a lot of reasons but

00:07:11,960 --> 00:07:16,800
actually we have so many replicas

00:07:14,250 --> 00:07:18,900
running all over the world some of them

00:07:16,800 --> 00:07:20,520
could be broken we could have say two or

00:07:18,900 --> 00:07:22,290
three replicas in some zones some we're

00:07:20,520 --> 00:07:26,010
able to just gotten slowed for whatever

00:07:22,290 --> 00:07:27,720
reason so their task is to find those

00:07:26,010 --> 00:07:29,850
records to hunt them down and to fix

00:07:27,720 --> 00:07:31,110
them but alternatively once we drill

00:07:29,850 --> 00:07:32,940
into this we might find that the problem

00:07:31,110 --> 00:07:34,950
is just distributed everywhere and it

00:07:32,940 --> 00:07:36,780
isn't necessarily one of our machines

00:07:34,950 --> 00:07:38,520
are one of my relatives at the pond it's

00:07:36,780 --> 00:07:40,020
a one of our users which is a problem or

00:07:38,520 --> 00:07:41,640
one of our configurations which is the

00:07:40,020 --> 00:07:42,900
problem or shard which is the problem

00:07:41,640 --> 00:07:45,630
all in which case we're gonna have to

00:07:42,900 --> 00:07:46,920
diagnose what that problem is and get to

00:07:45,630 --> 00:07:48,150
the bottom area social we can resolve

00:07:46,920 --> 00:07:51,420
it's not going to be as simple as just

00:07:48,150 --> 00:07:52,530
tracking down a broken machine so the

00:07:51,420 --> 00:07:56,240
technique which I'm going to use to

00:07:52,530 --> 00:07:56,240
solve the first problem we call layer

00:07:59,539 --> 00:08:17,909
Tom's Tom's invention is you saw we will

00:08:16,379 --> 00:08:19,979
page because one of our escalators was

00:08:17,909 --> 00:08:22,949
in danger and I whistle is for this

00:08:19,979 --> 00:08:25,409
imaginary service of regional so for

00:08:22,949 --> 00:08:39,599
some reason us worst ones we saw he's

00:08:25,409 --> 00:08:42,510
throwing too many slip queries like a

00:08:39,599 --> 00:08:43,890
regional level which is a bird movie

00:08:42,510 --> 00:08:46,440
this data centers we have zones or

00:08:43,890 --> 00:08:48,000
availability zones and within those

00:08:46,440 --> 00:08:49,140
zones of course we have to choose and

00:08:48,000 --> 00:08:56,070
the ends and containers with blah blah

00:08:49,140 --> 00:08:58,649
blah now we don't have so we don't

00:08:56,070 --> 00:09:00,390
because we have redundancy in those

00:08:58,649 --> 00:09:02,370
regions so ideally if one of those zones

00:09:00,390 --> 00:09:04,620
is broken all advances will shift the

00:09:02,370 --> 00:09:07,019
traffic so I didn't receive a page save

00:09:04,620 --> 00:09:08,519
us West one see if perfect idea for all

00:09:07,019 --> 00:09:10,920
I know is that you its whisp you want

00:09:08,519 --> 00:09:13,769
defer so first row down into the zone

00:09:10,920 --> 00:09:18,360
second buddy once we found zone we're

00:09:13,769 --> 00:09:19,560
gonna hope we find some broken machines

00:09:18,360 --> 00:09:21,510
in there certain but I think we're gonna

00:09:19,560 --> 00:09:24,149
need to drill down to individual machine

00:09:21,510 --> 00:09:25,920
level inside of that so now the reason

00:09:24,149 --> 00:09:27,810
we can't just say let's look at very

00:09:25,920 --> 00:09:29,430
simple machine releasable vm in the

00:09:27,810 --> 00:09:31,050
world right now is that that would just

00:09:29,430 --> 00:09:32,760
be a huge jumble of lines the

00:09:31,050 --> 00:09:34,290
cardinality is far too high or couldn't

00:09:32,760 --> 00:09:36,810
possibly pick out one birthing machine

00:09:34,290 --> 00:09:39,199
in a cluster of five thousand dolphin so

00:09:36,810 --> 00:09:42,360
this drilling ban has been a beauty so

00:09:39,199 --> 00:09:50,579
to start if I were to take that link I

00:09:42,360 --> 00:09:54,019
received in my page OB picking here this

00:09:50,579 --> 00:09:54,019
is system

00:09:56,450 --> 00:10:03,840
so there's nothing magical about the

00:10:02,280 --> 00:10:05,760
gone it's just it happens to be the tool

00:10:03,840 --> 00:10:08,130
that we use a lot of these techniques is

00:10:05,760 --> 00:10:09,840
where say could really be implemented

00:10:08,130 --> 00:10:19,860
I'm essentially any might not be quite

00:10:09,840 --> 00:10:22,020
as elegant but also I did have to use

00:10:19,860 --> 00:10:24,900
this workflow so people is really an

00:10:22,020 --> 00:10:27,180
example here so we consume this chart

00:10:24,900 --> 00:10:29,600
that whatever metric which happens to be

00:10:27,180 --> 00:10:31,830
the latency I so life at 98th percentile

00:10:29,600 --> 00:10:33,360
is being pushed up in the cost of fresh

00:10:31,830 --> 00:10:36,420
or at some point and we can assume that

00:10:33,360 --> 00:10:37,740
this is probably when I got page so the

00:10:36,420 --> 00:10:39,480
first thing I'm going to do to solve

00:10:37,740 --> 00:10:41,700
this problem is pop open the query panel

00:10:39,480 --> 00:10:43,230
now this metric it doesn't exist in

00:10:41,700 --> 00:10:44,760
isolation it's not just any number which

00:10:43,230 --> 00:10:46,740
is output by some of our paper system

00:10:44,760 --> 00:10:49,500
which is being measured it's actually

00:10:46,740 --> 00:11:00,930
it's evaluating behind the scenes he's

00:10:49,500 --> 00:11:03,030
in this query interface which says is

00:11:00,930 --> 00:11:04,860
uswest 1 that's why I'm seeing one of

00:11:03,030 --> 00:11:07,650
these regions rather than all of these

00:11:04,860 --> 00:11:10,440
regions and it's important to note at

00:11:07,650 --> 00:11:12,270
this point my vibration but this isn't

00:11:10,440 --> 00:11:14,070
what I'm doing here I'm clicking

00:11:12,270 --> 00:11:16,500
because I'm debugging in real time but

00:11:14,070 --> 00:11:18,150
all the stuff is provisioned using extra

00:11:16,500 --> 00:11:19,530
query language we use configures code

00:11:18,150 --> 00:11:21,420
just like everybody else so when we're

00:11:19,530 --> 00:11:23,070
making changes to are already going to

00:11:21,420 --> 00:11:24,870
click around of course we make some

00:11:23,070 --> 00:11:28,470
changes in text we text them and we push

00:11:24,870 --> 00:11:30,030
them out of production again this is

00:11:28,470 --> 00:11:32,550
this is where I'm gonna be clicking

00:11:30,030 --> 00:11:34,620
around so first let's just remove the

00:11:32,550 --> 00:11:36,030
filter so I can see the same metrics for

00:11:34,620 --> 00:11:38,370
all of our regions

00:11:36,030 --> 00:11:39,780
the reason I would do that is as Liz

00:11:38,370 --> 00:11:41,600
mentioned to establish the blast radius

00:11:39,780 --> 00:11:44,250
so that's the first thing that we do

00:11:41,600 --> 00:11:46,650
because it's possible I got paged route

00:11:44,250 --> 00:11:48,540
uswest 1 but actually our regions are

00:11:46,650 --> 00:11:49,980
working if this has just happens to be

00:11:48,540 --> 00:11:51,660
the first one and I can look forward to

00:11:49,980 --> 00:11:53,070
more pages about this so just to

00:11:51,660 --> 00:11:55,380
establish this is not a full advantage

00:11:53,070 --> 00:11:57,360
I've modified the query

00:11:55,380 --> 00:11:59,370
and now looking at the latency SLI for

00:11:57,360 --> 00:12:00,930
our region's thankfully everything else

00:11:59,370 --> 00:12:02,760
seems to be pretty funny so you've

00:12:00,930 --> 00:12:05,340
established blast radius is one region

00:12:02,760 --> 00:12:06,750
now this will be a great time to drain

00:12:05,340 --> 00:12:08,610
that region if we have a leave it the

00:12:06,750 --> 00:12:15,950
pole to do that shift will attract it to

00:12:08,610 --> 00:12:15,950
us East one while we work it out so

00:12:16,250 --> 00:12:21,450
we're going back to your search phone

00:12:18,930 --> 00:12:23,160
we're gonna so we're sort of we were

00:12:21,450 --> 00:12:24,990
already disregarding a lot of it with

00:12:23,160 --> 00:12:26,490
disregarding two-thirds of the machines

00:12:24,990 --> 00:12:30,330
in the world already because we don't

00:12:26,490 --> 00:12:32,670
care R Us East in US central I mean what

00:12:30,330 --> 00:12:34,110
we're gonna do is drill down into this

00:12:32,670 --> 00:12:36,360
query and they said this is what we call

00:12:34,110 --> 00:12:42,680
it lefty where is this in the outside

00:12:36,360 --> 00:12:46,110
way just to regional actively replace

00:12:42,680 --> 00:12:48,420
this query in place with the query which

00:12:46,110 --> 00:12:50,820
was used to generate the data which I'm

00:12:48,420 --> 00:12:54,780
looking at summarized now we could see

00:12:50,820 --> 00:12:56,550
you Tony in focus when I'm debugging

00:12:54,780 --> 00:12:58,500
takes a second or so to pull all this

00:12:56,550 --> 00:13:00,330
data people the world we would see this

00:12:58,500 --> 00:13:02,190
is a little bit slower because this is

00:13:00,330 --> 00:13:04,770
I'm doing this I'm pulling all of this

00:13:02,190 --> 00:13:06,300
data in real time a query time while I'm

00:13:04,770 --> 00:13:08,190
investigating if we were to say how

00:13:06,300 --> 00:13:09,510
alerting on all of this so every single

00:13:08,190 --> 00:13:11,010
time we wanted to check everything work

00:13:09,510 --> 00:13:11,700
we have to talk to every single machine

00:13:11,010 --> 00:13:13,500
in the world

00:13:11,700 --> 00:13:15,120
and of course we put here typically slow

00:13:13,500 --> 00:13:16,550
for alerting so we don't do that we

00:13:15,120 --> 00:13:19,320
build up these progressive players

00:13:16,550 --> 00:13:20,880
that's genetics right the first benefit

00:13:19,320 --> 00:13:23,190
is that it runs faster the second

00:13:20,880 --> 00:13:24,420
benefit is that abstraction she says she

00:13:23,190 --> 00:13:25,830
received out of it which is gonna stop

00:13:24,420 --> 00:13:27,390
thinking about the query and just look

00:13:25,830 --> 00:13:34,490
at what's gone stream of it and then if

00:13:27,390 --> 00:13:41,070
you're interested in being in deeper so

00:13:34,490 --> 00:13:43,650
is complex I've gone from the regional

00:13:41,070 --> 00:13:46,350
view taking this query which essentially

00:13:43,650 --> 00:13:48,300
few you don't need to read every single

00:13:46,350 --> 00:13:50,730
step listen to what it does this fetches

00:13:48,300 --> 00:13:51,930
the latency as a life or uses own as a

00:13:50,730 --> 00:13:53,339
distribution and

00:13:51,930 --> 00:13:56,610
kind of had some together it mashes them

00:13:53,339 --> 00:13:59,310
together to make a regional sli so

00:13:56,610 --> 00:14:00,990
rather than doing that I can actually

00:13:59,310 --> 00:14:02,760
drill into the query and I can group it

00:14:00,990 --> 00:14:04,860
by assuming they said of grouping it by

00:14:02,760 --> 00:14:06,210
region I can say no I want to look at

00:14:04,860 --> 00:14:07,980
the distributions of each of these

00:14:06,210 --> 00:14:10,589
different zones so that's what I've done

00:14:07,980 --> 00:14:12,810
here and you can see if I pop it in the

00:14:10,589 --> 00:14:14,550
sidebar which even the most of this

00:14:12,810 --> 00:14:15,899
presentation because this chart there's

00:14:14,550 --> 00:14:18,149
already small enough in the middle of

00:14:15,899 --> 00:14:21,089
the screen there you can see one of our

00:14:18,149 --> 00:14:23,130
zones is actually have much higher

00:14:21,089 --> 00:14:29,640
latency than the other zones so we've

00:14:23,130 --> 00:14:32,330
gone so we can have another filter for

00:14:29,640 --> 00:14:35,850
this query interactively we're now

00:14:32,330 --> 00:14:45,450
looking at me for one's own which is us

00:14:35,850 --> 00:14:47,610
West one similarly this would be a great

00:14:45,450 --> 00:14:50,250
time to train because we now know that

00:14:47,610 --> 00:14:54,240
our 5,000 shoes around the world we

00:14:50,250 --> 00:14:57,529
assume is somewhere it's isolated to us

00:14:54,240 --> 00:15:01,200
West one so we're gonna move forward

00:14:57,529 --> 00:15:04,950
let's see what else they find out about

00:15:01,200 --> 00:15:07,860
us West wanna be the very last operation

00:15:04,950 --> 00:15:10,380
here is is a transformation if you see

00:15:07,860 --> 00:15:11,910
the type the type of data we're just

00:15:10,380 --> 00:15:13,470
going into this last stage is a

00:15:11,910 --> 00:15:17,670
distribution that's a distribution of

00:15:13,470 --> 00:15:24,000
the latency but all of the replicas and

00:15:17,670 --> 00:15:25,440
then pile into one number to say most of

00:15:24,000 --> 00:15:27,660
this distribution but I want to measure

00:15:25,440 --> 00:15:29,610
the 98th percentile ladies because we

00:15:27,660 --> 00:15:31,170
found it very well alert making a lot of

00:15:29,610 --> 00:15:33,839
assists if the distribution looks kinda

00:15:31,170 --> 00:15:35,579
like that then st. being away but we can

00:15:33,839 --> 00:15:38,070
say is you can see at 98 or something

00:15:35,579 --> 00:15:39,990
like this latency 98 there's about 100

00:15:38,070 --> 00:15:41,790
milliseconds then send me an alert

00:15:39,990 --> 00:15:44,100
that's what I wonder city for when I'm

00:15:41,790 --> 00:15:44,459
debugging a little more context than

00:15:44,100 --> 00:15:47,100
that

00:15:44,459 --> 00:15:49,230
cuz it's so hard to tell what's wrong

00:15:47,100 --> 00:15:51,810
from this so I can interactively remove

00:15:49,230 --> 00:15:53,459
this last step and now the query is

00:15:51,810 --> 00:15:54,990
going to be the entire distribution

00:15:53,459 --> 00:15:57,750
because of course we're not limited to

00:15:54,990 --> 00:15:59,250
just point line is we also we can plot

00:15:57,750 --> 00:16:02,100
distributions as heat maps on here as

00:15:59,250 --> 00:16:03,510
well and I've done this enough client it

00:16:02,100 --> 00:16:05,940
looked a little foreign to me

00:16:03,510 --> 00:16:08,040
it's almost like reading the matrix by

00:16:05,940 --> 00:16:09,269
looking at this distribution you can get

00:16:08,040 --> 00:16:13,949
a pretty good idea of what might be

00:16:09,269 --> 00:16:17,970
wrong you can see these lines 15 1999

00:16:13,949 --> 00:16:20,730
and 99.9 liar hasn't really moved at all

00:16:17,970 --> 00:16:21,690
so I already know it's not that the

00:16:20,730 --> 00:16:23,190
whole zones broken

00:16:21,690 --> 00:16:25,500
it's not that every single query that

00:16:23,190 --> 00:16:26,730
goes into us which one be just is taking

00:16:25,500 --> 00:16:28,769
forever because most of them actually

00:16:26,730 --> 00:16:30,899
are taken as long as they have which

00:16:28,769 --> 00:16:32,670
looks to be about 70 milliseconds but at

00:16:30,899 --> 00:16:35,610
some point the higher percentiles

00:16:32,670 --> 00:16:36,959
started rising so I know that there's

00:16:35,610 --> 00:16:38,940
some problem which is affecting some

00:16:36,959 --> 00:16:40,769
subset with queries which are going in

00:16:38,940 --> 00:16:42,389
and out us West Point B honestly it

00:16:40,769 --> 00:16:50,430
would be great if all of the queries

00:16:42,389 --> 00:16:52,170
will slow because because that would

00:16:50,430 --> 00:16:52,920
mean you expert on B's break and I can

00:16:52,170 --> 00:16:55,019
pull the lever

00:16:52,920 --> 00:16:58,829
I know I can get out of there but that's

00:16:55,019 --> 00:17:03,930
not the case so we're gonna place this

00:16:58,829 --> 00:17:05,429
definition with the raw data which we're

00:17:03,930 --> 00:17:08,429
finally getting down and dissected it

00:17:05,429 --> 00:17:10,199
successfully enough that we can now look

00:17:08,429 --> 00:17:12,419
at the distribution of all the

00:17:10,199 --> 00:17:14,429
percentiles of the replicas in u.s. West

00:17:12,419 --> 00:17:16,199
Point B and we can see clearly one of

00:17:14,429 --> 00:17:17,429
them is broken we found that task we've

00:17:16,199 --> 00:17:19,829
sort of solved the murder mystery at

00:17:17,429 --> 00:17:22,230
least we found the body at this point so

00:17:19,829 --> 00:17:24,900
we're not having another filter to say

00:17:22,230 --> 00:17:27,179
filter only this one replicas in the

00:17:24,900 --> 00:17:29,070
u.s. West quantity I'm gonna do the same

00:17:27,179 --> 00:17:31,080
as I did before and remove that 98

00:17:29,070 --> 00:17:32,520
percentile filter and now I'm looking at

00:17:31,080 --> 00:17:35,790
this tribution of requests on this one

00:17:32,520 --> 00:17:37,440
right okay so now we know that the

00:17:35,790 --> 00:17:39,299
requests because unlike the previous

00:17:37,440 --> 00:17:46,980
distribution of the actual baseline is

00:17:39,299 --> 00:17:48,809
risen much slower than the so the

00:17:46,980 --> 00:17:50,250
medication here is just killed that

00:17:48,809 --> 00:17:51,330
right there straight away we're gonna go

00:17:50,250 --> 00:17:55,919
any further what's gonna kill it and

00:17:51,330 --> 00:17:58,260
hope everything goes so this that's the

00:17:55,919 --> 00:18:00,510
end of the arm that first wreckage layer

00:17:58,260 --> 00:18:02,549
and this is a very simple antigen but

00:18:00,510 --> 00:18:03,960
still because they're at the cardinality

00:18:02,549 --> 00:18:05,460
so hundra's there are so many replicas

00:18:03,960 --> 00:18:07,020
involved it's just there's no way that

00:18:05,460 --> 00:18:09,240
we could possibly exciting the latency

00:18:07,020 --> 00:18:11,040
for all and I'll find the dead one

00:18:09,240 --> 00:18:13,680
automatic one because it's just too many

00:18:11,040 --> 00:18:14,790
orphans are you gonna - for each of

00:18:13,680 --> 00:18:16,470
these there's no way you could have done

00:18:14,790 --> 00:18:17,040
that we had to interactively drill down

00:18:16,470 --> 00:18:19,260
to find it

00:18:17,040 --> 00:18:21,660
yeah it's impossible so what we had to

00:18:19,260 --> 00:18:24,990
do this way chop chop chop until we

00:18:21,660 --> 00:18:27,780
found the problem but unfortunately not

00:18:24,990 --> 00:18:30,200
not all problems can be solved that way

00:18:27,780 --> 00:18:35,370
I'm gonna go a little quicker on this

00:18:30,200 --> 00:18:37,080
because we include a certain amount of

00:18:35,370 --> 00:18:41,190
data with a single metric we call them

00:18:37,080 --> 00:18:43,710
tanks so we can find delay we can find a

00:18:41,190 --> 00:18:45,570
winter zone or which region it's a North

00:18:43,710 --> 00:18:48,450
query is coming from but we can for

00:18:45,570 --> 00:18:51,320
example find out which kernel version is

00:18:48,450 --> 00:18:54,030
is this machine which serve this query

00:18:51,320 --> 00:18:56,010
we can't exactly find out what what is

00:18:54,030 --> 00:18:57,360
the time or is the patch version we just

00:18:56,010 --> 00:18:59,790
we couldn't include every single metric

00:18:57,360 --> 00:19:01,680
so all we need to do is pull in other

00:18:59,790 --> 00:19:04,260
metrics which we can correlate with

00:19:01,680 --> 00:19:07,850
those so I'm gonna start with a brand

00:19:04,260 --> 00:19:10,020
new query yeah if you know a new tab

00:19:07,850 --> 00:19:13,530
this is this is sort of the simplest

00:19:10,020 --> 00:19:16,080
pace of a query where we're essentially

00:19:13,530 --> 00:19:18,510
pulling we're pulling a metric or kernel

00:19:16,080 --> 00:19:20,490
version which as I mentioned it pulls

00:19:18,510 --> 00:19:22,050
out the version of the kernel Rahmani

00:19:20,490 --> 00:19:23,700
trauma shoes there's nothing to do the

00:19:22,050 --> 00:19:25,890
latency is just reporting this every

00:19:23,700 --> 00:19:29,640
five seconds or so so we can keep track

00:19:25,890 --> 00:19:30,870
of it now you can still know there are

00:19:29,640 --> 00:19:32,700
there's a lot of information about each

00:19:30,870 --> 00:19:34,260
metric there's the hostname there's the

00:19:32,700 --> 00:19:38,130
location there's a username and all of

00:19:34,260 --> 00:19:40,170
that and we can use these to join all of

00:19:38,130 --> 00:19:43,470
the data together so what I've done here

00:19:40,170 --> 00:19:45,300
is I grouped the metrics which are

00:19:43,470 --> 00:19:48,180
coming out by the version and this is

00:19:45,300 --> 00:19:50,550
the magical thing about having all of

00:19:48,180 --> 00:19:53,210
this data now standard we can now see

00:19:50,550 --> 00:19:55,710
that the majority of tasks are running

00:19:53,210 --> 00:19:57,510
running one kernel version almost 50

00:19:55,710 --> 00:19:58,860
orphan and there are a few outliers in

00:19:57,510 --> 00:20:01,410
this and my hypothesis is there

00:19:58,860 --> 00:20:02,970
something about this which is causing a

00:20:01,410 --> 00:20:05,460
problem one of these conversions might

00:20:02,970 --> 00:20:07,920
be causing this lady's giving so I'm

00:20:05,460 --> 00:20:09,670
gonna open up another tab and this is

00:20:07,920 --> 00:20:11,650
the query which I mentioned

00:20:09,670 --> 00:20:13,420
to do ministry this is just delinquency

00:20:11,650 --> 00:20:16,360
for all of our records all over the

00:20:13,420 --> 00:20:17,770
world it's just a huge temple of paper

00:20:16,360 --> 00:20:21,400
there's not a tremendous amount we can

00:20:17,770 --> 00:20:23,080
do with this but if I were to join these

00:20:21,400 --> 00:20:23,830
queries together and the most important

00:20:23,080 --> 00:20:25,510
part with a comma should have

00:20:23,830 --> 00:20:26,650
highlighted on the left is we've taken

00:20:25,510 --> 00:20:28,060
those two queries and you see there's

00:20:26,650 --> 00:20:30,430
one box at the bottom which is join

00:20:28,060 --> 00:20:31,570
these streams together now okay the

00:20:30,430 --> 00:20:33,280
details of how this happens but

00:20:31,570 --> 00:20:35,200
essentially we're able to call one

00:20:33,280 --> 00:20:37,030
metric with another metric using the

00:20:35,200 --> 00:20:38,500
username the job name data location the

00:20:37,030 --> 00:20:40,570
replica and all of them I hope it's

00:20:38,500 --> 00:20:42,280
clear how we've now highlighted on the

00:20:40,570 --> 00:20:46,690
right that we've augmented that linking

00:20:42,280 --> 00:20:49,390
some data with kernel version data so if

00:20:46,690 --> 00:20:53,320
I stare straight to the conclusion we

00:20:49,390 --> 00:20:55,000
know the latency by that version or tag

00:20:53,320 --> 00:20:56,890
that we just added we can see something

00:20:55,000 --> 00:20:58,840
really interesting which is that one of

00:20:56,890 --> 00:21:01,030
these kernels is causing much higher

00:20:58,840 --> 00:21:02,260
latency than any of the other cars on

00:21:01,030 --> 00:21:04,720
the others are pretty much where they

00:21:02,260 --> 00:21:07,210
were if I pop up in the sidebar I can

00:21:04,720 --> 00:21:08,500
see and we seem to have one old opal

00:21:07,210 --> 00:21:10,450
which is causing a tremendous amount

00:21:08,500 --> 00:21:13,390
more lately than anyone else so at this

00:21:10,450 --> 00:21:15,130
point I mean I've already during the

00:21:13,390 --> 00:21:17,350
zone of course and I've sold of them

00:21:15,130 --> 00:21:19,810
evenly but at this point I can follow

00:21:17,350 --> 00:21:22,090
further incentives and say hey some of

00:21:19,810 --> 00:21:23,920
these are replicas Arang an old colonel

00:21:22,090 --> 00:21:25,690
firstly I don't know why secondly it's

00:21:23,920 --> 00:21:30,010
causing a lot of latency so you can take

00:21:25,690 --> 00:21:31,450
a look at that and fix it now I'm gonna

00:21:30,010 --> 00:21:32,890
skip over this legs I just said old

00:21:31,450 --> 00:21:34,720
thing running out of time little but we

00:21:32,890 --> 00:21:39,520
have one more strategy which makes these

00:21:34,720 --> 00:21:41,200
look like so I don't just spent maybe 15

00:21:39,520 --> 00:21:43,930
minutes trying to figure out what was

00:21:41,200 --> 00:21:45,100
going on by onion peeling and he

00:21:43,930 --> 00:21:47,080
remembers you to do that for each

00:21:45,100 --> 00:21:48,460
hypothesis he was investigating for

00:21:47,080 --> 00:21:50,680
whether it's Colonel Bergen or whether

00:21:48,460 --> 00:21:51,880
it's which rack you're running on or any

00:21:50,680 --> 00:21:53,440
number of different things you have to

00:21:51,880 --> 00:21:55,870
spend a bunch of time so here metadata

00:21:53,440 --> 00:21:58,030
so I would introduce you a technique

00:21:55,870 --> 00:21:59,680
that really makes your distributed

00:21:58,030 --> 00:22:02,620
tracing useful and makes your high

00:21:59,680 --> 00:22:04,030
cardinality fields actually useful in a

00:22:02,620 --> 00:22:07,030
way that you can pinpoint problems

00:22:04,030 --> 00:22:11,440
precisely so the way that we do this is

00:22:07,030 --> 00:22:13,860
by dynamically sampling so we sample for

00:22:11,440 --> 00:22:16,380
a given distribution bucket kind of the

00:22:13,860 --> 00:22:19,830
time to mention in one access

00:22:16,380 --> 00:22:22,610
the value sentencing in another axis we

00:22:19,830 --> 00:22:26,100
may choose to keep for each bucket a

00:22:22,610 --> 00:22:27,960
specific subset of Tad's associated with

00:22:26,100 --> 00:22:30,780
one query that happened to fall into

00:22:27,960 --> 00:22:33,270
that bucket or in specifics for instance

00:22:30,780 --> 00:22:35,640
Zipkin worst after another trace trace

00:22:33,270 --> 00:22:37,920
an idea associated with that but when we

00:22:35,640 --> 00:22:40,980
go looking later we can go and find in

00:22:37,920 --> 00:22:43,320
our graphing UI and go and say show me

00:22:40,980 --> 00:22:45,420
an example of a query that had this

00:22:43,320 --> 00:22:47,550
lengthy at this time and give me the

00:22:45,420 --> 00:22:50,130
exact machine it originated from kernel

00:22:47,550 --> 00:22:52,230
version you name it it can find it and

00:22:50,130 --> 00:22:54,920
that's really the key inside of

00:22:52,230 --> 00:22:58,890
exemplars that makes debugging a snare

00:22:54,920 --> 00:23:02,160
so it should hear what happens when you

00:22:58,890 --> 00:23:05,460
ask it to plot by a task ID and to show

00:23:02,160 --> 00:23:07,140
me examples for the task IDs and it will

00:23:05,460 --> 00:23:08,820
show you a whole bunch of examples of

00:23:07,140 --> 00:23:11,640
glacis kind of in the middle lady that

00:23:08,820 --> 00:23:13,920
are increasing and I can go in and I can

00:23:11,640 --> 00:23:16,200
hover my mouse over one of those little

00:23:13,920 --> 00:23:18,090
dots that's an individual sample that's

00:23:16,200 --> 00:23:20,070
been attached to a bucket and then it

00:23:18,090 --> 00:23:22,770
will show me everything sharing that

00:23:20,070 --> 00:23:24,840
same collection of tags so in this case

00:23:22,770 --> 00:23:26,940
it's saying hey all of these things that

00:23:24,840 --> 00:23:29,790
have high latency they're all coming

00:23:26,940 --> 00:23:31,920
from the same place and not only that I

00:23:29,790 --> 00:23:32,220
can go and click and find out hey wait a

00:23:31,920 --> 00:23:34,770
minute

00:23:32,220 --> 00:23:37,620
these all are coming from task ID number

00:23:34,770 --> 00:23:40,890
four right so I've reduced all of that

00:23:37,620 --> 00:23:42,750
kind of onion peeling down into 20

00:23:40,890 --> 00:23:44,940
seconds as querying and clicking to

00:23:42,750 --> 00:23:46,620
figure out what is my hypothesis it's

00:23:44,940 --> 00:23:48,210
probably a bad machine ok what's bad

00:23:46,620 --> 00:23:52,410
machine is it now I know I need to go

00:23:48,210 --> 00:23:54,180
and look at or restart tasks for here's

00:23:52,410 --> 00:23:55,530
another one this is actually real data

00:23:54,180 --> 00:23:57,810
this is not something that we made up

00:23:55,530 --> 00:23:59,850
then we have to be careful speed with

00:23:57,810 --> 00:24:02,730
numbers and eyesight you'll notice

00:23:59,850 --> 00:24:04,290
carefully clip the axes so here's a real

00:24:02,730 --> 00:24:06,950
distribution and one thing that you'll

00:24:04,290 --> 00:24:09,270
note is I want to highlight that the

00:24:06,950 --> 00:24:11,280
99th percentile agency is like slowly

00:24:09,270 --> 00:24:13,650
creeping up but if you didn't have the

00:24:11,280 --> 00:24:15,960
heat map you wouldn't be able to see

00:24:13,650 --> 00:24:17,970
that there's a band and queries that are

00:24:15,960 --> 00:24:19,920
slow and there are more of them as time

00:24:17,970 --> 00:24:20,659
is progressing right that's the thing

00:24:19,920 --> 00:24:22,789
that becomes

00:24:20,659 --> 00:24:24,950
invisible if you only have access to

00:24:22,789 --> 00:24:26,659
your percentile lines it causes you to

00:24:24,950 --> 00:24:29,389
miss patterns that are an especially

00:24:26,659 --> 00:24:32,419
critical so not only that but we can

00:24:29,389 --> 00:24:37,039
actually say I want to see one of these

00:24:32,419 --> 00:24:39,710
slow queries that is at n units so I

00:24:37,039 --> 00:24:42,649
click there and it was saying here's we

00:24:39,710 --> 00:24:44,269
trace here is a distributed trace like a

00:24:42,649 --> 00:24:46,429
zip can trace to our trail version is

00:24:44,269 --> 00:24:48,259
called upper and it will give me a link

00:24:46,429 --> 00:24:50,659
right there and say here's the Pearson

00:24:48,259 --> 00:24:52,609
latency and here's a way to go and dig

00:24:50,659 --> 00:24:55,190
in and find out what actually happened

00:24:52,609 --> 00:24:56,690
for that one Kari so if I click there

00:24:55,190 --> 00:24:58,369
what happened well I get a basic view

00:24:56,690 --> 00:25:00,799
that says ok you talk to this server and

00:24:58,369 --> 00:25:02,869
this is long but I can actually expand

00:25:00,799 --> 00:25:05,659
this all the way down and say hey wait a

00:25:02,869 --> 00:25:08,389
minute it's trying to make a bunch of

00:25:05,659 --> 00:25:10,009
calls to write data and it waits for the

00:25:08,389 --> 00:25:12,440
batch to Lin and then it writes the

00:25:10,009 --> 00:25:14,929
entire batch at once and for some reason

00:25:12,440 --> 00:25:18,289
more queries are having to wait the peso

00:25:14,929 --> 00:25:21,049
on amount of time and units before their

00:25:18,289 --> 00:25:22,999
question disk and not only that we can

00:25:21,049 --> 00:25:24,859
also see in certain situations we can

00:25:22,999 --> 00:25:26,749
even see like log annotations like what

00:25:24,859 --> 00:25:28,070
log lines were written in the middle

00:25:26,749 --> 00:25:29,570
destitution of the query so this is

00:25:28,070 --> 00:25:31,909
tremendously powerful except let's me

00:25:29,570 --> 00:25:34,820
dig from can you know I've had a bunch

00:25:31,909 --> 00:25:36,320
of stuff that's slow - what specific

00:25:34,820 --> 00:25:38,629
example can I look at and formulate a

00:25:36,320 --> 00:25:40,009
hypothesis as to what's going wrong so

00:25:38,629 --> 00:25:41,629
now that I said okay you know it's

00:25:40,009 --> 00:25:43,039
probably an issue with what flushing now

00:25:41,629 --> 00:25:45,169
I can go and look at my love plus

00:25:43,039 --> 00:25:46,789
address if I didn't cabinets I mean

00:25:45,169 --> 00:25:48,440
poking around that 50 different desk or

00:25:46,789 --> 00:25:51,259
screen as they grow is that love washing

00:25:48,440 --> 00:25:52,789
isn't that the isn't that the disk layer

00:25:51,259 --> 00:25:56,090
itself is being slow what's going on

00:25:52,789 --> 00:25:57,470
there why is it being slow so what I

00:25:56,090 --> 00:25:59,869
want to tell you here is basically that

00:25:57,470 --> 00:26:01,609
if you keep your high cardinality tags

00:25:59,869 --> 00:26:03,409
and sample them or if you keep traces

00:26:01,609 --> 00:26:05,539
and sample them there's right now have a

00:26:03,409 --> 00:26:07,519
much better time because it'll but you

00:26:05,539 --> 00:26:09,349
pinpoint exactly some time stamps and

00:26:07,519 --> 00:26:11,179
even distributed trace executions that

00:26:09,349 --> 00:26:13,849
way you really debug and figure out

00:26:11,179 --> 00:26:15,259
what's going on you still need to be

00:26:13,849 --> 00:26:16,879
able to onion field and you still need

00:26:15,259 --> 00:26:19,159
to be over there we construct queries in

00:26:16,879 --> 00:26:20,039
order to value these hypotheses but I've

00:26:19,159 --> 00:26:21,299
typically spend

00:26:20,039 --> 00:26:22,859
ten or fifteen minutes if they don't

00:26:21,299 --> 00:26:23,519
have a trace trying to figure out what

00:26:22,859 --> 00:26:26,609
went wrong

00:26:23,519 --> 00:26:29,159
so hypothesis test gets faster and

00:26:26,609 --> 00:26:30,840
around just get results faster so as

00:26:29,159 --> 00:26:32,789
milk both of these things independently

00:26:30,840 --> 00:26:35,389
exist kind of in the public market in

00:26:32,789 --> 00:26:37,950
terms of distributed tracing exists

00:26:35,389 --> 00:26:39,359
distribution graphing exists the people

00:26:37,950 --> 00:26:40,679
that are closest to publicly I'm

00:26:39,359 --> 00:26:43,559
pointing this I would say or honeycomb

00:26:40,679 --> 00:26:44,729
but we're hoping that soon other people

00:26:43,559 --> 00:26:46,919
will be doing this as well and this will

00:26:44,729 --> 00:26:48,960
be a common paradigm that all of us use

00:26:46,919 --> 00:26:51,059
in order to be classmates faster and

00:26:48,960 --> 00:26:52,470
then you know we can spend that errbody

00:26:51,059 --> 00:26:53,970
it oughta much nicer things we can spend

00:26:52,470 --> 00:26:55,859
that error but another investor we're

00:26:53,970 --> 00:26:59,940
making our customers happier or maybe

00:26:55,859 --> 00:27:02,729
maybe increasing RS close so it's

00:26:59,940 --> 00:27:04,139
important to have once you're done it's

00:27:02,729 --> 00:27:05,729
important to actually break down what

00:27:04,139 --> 00:27:07,229
can you do how to do solve the problem

00:27:05,729 --> 00:27:09,119
in case the same as you props up later

00:27:07,229 --> 00:27:11,279
you have to figure out what's going on

00:27:09,119 --> 00:27:12,749
they can reuse your work right but for

00:27:11,279 --> 00:27:15,479
committing these things to durable

00:27:12,749 --> 00:27:17,159
long-term dashboards we don't really

00:27:15,479 --> 00:27:18,599
recommend doing that unless something is

00:27:17,159 --> 00:27:20,759
happening over and over but something's

00:27:18,599 --> 00:27:22,169
happening over and over maybe you should

00:27:20,759 --> 00:27:25,349
be doing the engineer and half of your

00:27:22,169 --> 00:27:27,179
job and fixing it so don't proliferate -

00:27:25,349 --> 00:27:29,700
works every task for you add finally

00:27:27,179 --> 00:27:31,379
cost in storage has a cost and compute

00:27:29,700 --> 00:27:33,570
and hasn't cost in your cognitive

00:27:31,379 --> 00:27:35,249
overhead because no - rather you just

00:27:33,570 --> 00:27:37,019
wind up being exactly identical so

00:27:35,249 --> 00:27:38,729
chasing yesterday's problems if we're

00:27:37,019 --> 00:27:42,149
really going to help you so feel better

00:27:38,729 --> 00:27:44,489
cooling fixed tomorrow's problems so in

00:27:42,149 --> 00:27:46,590
summary if you think about the whole

00:27:44,489 --> 00:27:48,539
lacunae loop focusing on time to resolve

00:27:46,590 --> 00:27:50,909
is most important focusing on triangle

00:27:48,539 --> 00:27:52,379
hypothesize and testaments work and if

00:27:50,909 --> 00:27:54,539
you have bad cop wearing and if you have

00:27:52,379 --> 00:27:56,340
exemplars you can do that a lot faster

00:27:54,539 --> 00:27:58,229
through and if you can't do these things

00:27:56,340 --> 00:28:03,080
you're spending a lot of time spinning

00:27:58,229 --> 00:28:03,080
rather than actually fixing problems

00:28:04,410 --> 00:28:12,609

YouTube URL: https://www.youtube.com/watch?v=U72b4Nl0Ftw


