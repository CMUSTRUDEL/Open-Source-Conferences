Title: Enterprise Kafka: Kafka as a Service - Todd Palino, Clark Haskins
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,000 --> 00:00:04,740
all right I'm Clark Haskins this is Todd

00:00:02,939 --> 00:00:06,750
Polina we're from LinkedIn we're gonna

00:00:04,740 --> 00:00:10,260
be talking to you today about how we use

00:00:06,750 --> 00:00:11,580
Kafka so why are you here

00:00:10,260 --> 00:00:14,639
you want to figure out what Kafka is all

00:00:11,580 --> 00:00:16,590
about you're running Kafka you're

00:00:14,639 --> 00:00:19,470
interested to see how you can use it

00:00:16,590 --> 00:00:20,939
more you're looking for some you know

00:00:19,470 --> 00:00:24,449
cool things that we've done so we'll

00:00:20,939 --> 00:00:28,380
share them with you today as I mentioned

00:00:24,449 --> 00:00:30,779
this is who we are so we are Kafka site

00:00:28,380 --> 00:00:34,170
reliability engineers at LinkedIn so a

00:00:30,779 --> 00:00:36,510
site reliability engineer it's kind of a

00:00:34,170 --> 00:00:39,450
bay area term we're the application

00:00:36,510 --> 00:00:42,780
administrators as well as the architects

00:00:39,450 --> 00:00:45,350
for how we deploy Kafka as well as

00:00:42,780 --> 00:00:48,710
developers we write scripts and tools to

00:00:45,350 --> 00:00:51,600
make different applications run better

00:00:48,710 --> 00:00:55,890
and our number one priority is to keep

00:00:51,600 --> 00:00:58,670
the set running all the time so uh now

00:00:55,890 --> 00:01:00,840
here's Todd with the overview of Kafka

00:00:58,670 --> 00:01:03,059
all right so we'll start with some of

00:01:00,840 --> 00:01:04,460
the basics of what Kafka is so we have

00:01:03,059 --> 00:01:09,869
everyone on the same page

00:01:04,460 --> 00:01:11,670
basically Kafka now not this guy Kafka

00:01:09,869 --> 00:01:14,400
is a published subscribed messaging

00:01:11,670 --> 00:01:16,080
system so in our service we have a

00:01:14,400 --> 00:01:19,200
broker that's what we call a server in

00:01:16,080 --> 00:01:21,030
Kafka which is talking to zookeeper

00:01:19,200 --> 00:01:22,710
because it stores a lot of its metadata

00:01:21,030 --> 00:01:24,570
and zookeeper and we have a producer

00:01:22,710 --> 00:01:26,460
that generates messages that are going

00:01:24,570 --> 00:01:29,430
into our system and a consumer that is

00:01:26,460 --> 00:01:31,470
reading messages out now all of our data

00:01:29,430 --> 00:01:33,180
is organized into topics and partitions

00:01:31,470 --> 00:01:35,939
so in this case we have a topic named a

00:01:33,180 --> 00:01:37,320
and partition 0 and the producer is

00:01:35,939 --> 00:01:38,790
producing messages into it and the

00:01:37,320 --> 00:01:40,860
consumers reading message that's out of

00:01:38,790 --> 00:01:42,570
it but we don't want just one we want to

00:01:40,860 --> 00:01:44,000
because now we can handle more data we

00:01:42,570 --> 00:01:47,880
can balance it out across more

00:01:44,000 --> 00:01:49,500
partitions and our producer is gonna

00:01:47,880 --> 00:01:51,060
send messages they're gonna get stored

00:01:49,500 --> 00:01:54,030
in Kafka they're going to get consumed

00:01:51,060 --> 00:01:55,920
on the other end but we don't want just

00:01:54,030 --> 00:01:57,000
one brick or one broker is not good if

00:01:55,920 --> 00:01:58,590
that goes over we're going to have

00:01:57,000 --> 00:02:00,119
problems so now we have to and we've

00:01:58,590 --> 00:02:01,469
moved our partitions out so they're on

00:02:00,119 --> 00:02:03,869
two different servers we get more

00:02:01,469 --> 00:02:06,840
throughput this way we can also have

00:02:03,869 --> 00:02:08,640
resiliency by backing up our partitions

00:02:06,840 --> 00:02:10,979
we have in sync replicas on either side

00:02:08,640 --> 00:02:13,710
so now what happens is when one broker

00:02:10,979 --> 00:02:15,720
drops the other broker takes over as

00:02:13,710 --> 00:02:17,730
the leader now at any given time only

00:02:15,720 --> 00:02:19,560
one broker can be the leader for any

00:02:17,730 --> 00:02:20,970
given partition so at this time the

00:02:19,560 --> 00:02:24,180
second broker is the leader for both

00:02:20,970 --> 00:02:26,790
partitions when our server comes back up

00:02:24,180 --> 00:02:28,380
it's not the leader for anything nothing

00:02:26,790 --> 00:02:30,630
happens to it no data goes to it

00:02:28,380 --> 00:02:34,650
whatsoever until something else happens

00:02:30,630 --> 00:02:37,470
to the cluster so what is our Kafka

00:02:34,650 --> 00:02:39,120
cluster its disk base this is important

00:02:37,470 --> 00:02:41,040
because we're not keeping everything in

00:02:39,120 --> 00:02:43,650
memory we're storing our data long-term

00:02:41,040 --> 00:02:45,750
it's durable because we have multiple

00:02:43,650 --> 00:02:47,730
brokers we can replicate our data across

00:02:45,750 --> 00:02:49,260
multiple brokers within the cluster we

00:02:47,730 --> 00:02:51,180
know that if anything go we drop one

00:02:49,260 --> 00:02:54,090
broker everything's going to come back

00:02:51,180 --> 00:02:56,040
ok it's also scalable if we need to

00:02:54,090 --> 00:02:57,720
support more data within our cluster

00:02:56,040 --> 00:02:59,670
more messages per second all we need to

00:02:57,720 --> 00:03:01,650
do is add more partitions add more

00:02:59,670 --> 00:03:04,500
brokers spread out the load we can go as

00:03:01,650 --> 00:03:08,640
horizontal as we want pretty much like

00:03:04,500 --> 00:03:09,810
this room it's also low latency messages

00:03:08,640 --> 00:03:11,580
tend to get published within

00:03:09,810 --> 00:03:13,250
milliseconds they're available for the

00:03:11,580 --> 00:03:16,230
consumers to read whenever they want

00:03:13,250 --> 00:03:17,910
we're not talking second latency we're

00:03:16,230 --> 00:03:19,680
talking millisecond latency we're

00:03:17,910 --> 00:03:21,900
working to reduce that even more we're

00:03:19,680 --> 00:03:24,900
looking at ways to go real time on Kafka

00:03:21,900 --> 00:03:27,840
now and it has finite retention so you

00:03:24,900 --> 00:03:29,790
can set the retention of your data to be

00:03:27,840 --> 00:03:31,770
in terms of the amount of data you store

00:03:29,790 --> 00:03:33,270
on the disk you can set it to be in

00:03:31,770 --> 00:03:35,490
terms of the number of days that you

00:03:33,270 --> 00:03:38,130
store data for example most of our data

00:03:35,490 --> 00:03:39,870
in our clusters is stored for a week so

00:03:38,130 --> 00:03:42,000
that means once the producer produces

00:03:39,870 --> 00:03:43,380
the message into Kafka it sits around in

00:03:42,000 --> 00:03:45,600
the brokers for a week and can be

00:03:43,380 --> 00:03:47,700
consumed by as many consumers as want to

00:03:45,600 --> 00:03:49,920
see it it doesn't need to be reproduced

00:03:47,700 --> 00:03:54,000
by cop by the producer over and over and

00:03:49,920 --> 00:03:56,850
over for each consumer and it's not

00:03:54,000 --> 00:03:58,620
idempotent at this time you can kind of

00:03:56,850 --> 00:04:00,690
get there a little bit with using keys

00:03:58,620 --> 00:04:02,970
and log compression but it's not quite

00:04:00,690 --> 00:04:05,130
there our developers assure us that this

00:04:02,970 --> 00:04:08,760
is being worked on and will we saw very

00:04:05,130 --> 00:04:11,340
shortly so how do we use Kafka at

00:04:08,760 --> 00:04:13,350
LinkedIn we have multiple data centers

00:04:11,340 --> 00:04:14,460
we have multiple clusters in each data

00:04:13,350 --> 00:04:17,220
center we're handling a lot of

00:04:14,460 --> 00:04:18,810
information so we're mirroring between

00:04:17,220 --> 00:04:20,220
all of those clusters we're taking data

00:04:18,810 --> 00:04:22,410
from our production tiers we're

00:04:20,220 --> 00:04:25,410
aggregating it so that it can be used

00:04:22,410 --> 00:04:27,419
we are flowing it to other systems what

00:04:25,410 --> 00:04:29,189
kind of data are we talking about

00:04:27,419 --> 00:04:31,110
we're talking about metrics on all of

00:04:29,189 --> 00:04:32,759
our applications all of our servers so

00:04:31,110 --> 00:04:35,189
if we want to know how much disk is

00:04:32,759 --> 00:04:36,749
being used on a particular server that

00:04:35,189 --> 00:04:38,340
informations in Kafka of the metrics

00:04:36,749 --> 00:04:40,560
flow into Kafka and then they flow out

00:04:38,340 --> 00:04:42,360
to a graphing system and an alert system

00:04:40,560 --> 00:04:44,669
that we use to get all that information

00:04:42,360 --> 00:04:47,009
if we want to know what an application

00:04:44,669 --> 00:04:49,319
response time is that's in Kafka as well

00:04:47,009 --> 00:04:51,810
every single metric goes into Kafka

00:04:49,319 --> 00:04:53,639
we're also doing tracking information so

00:04:51,810 --> 00:04:56,129
every time the user takes an action on

00:04:53,639 --> 00:04:57,779
the website every single call that

00:04:56,129 --> 00:04:59,699
happens to build the home page all of

00:04:57,779 --> 00:05:01,889
that information gets flowed into Kafka

00:04:59,699 --> 00:05:03,810
as well and we're also doing queuing

00:05:01,889 --> 00:05:05,909
which is coordination between

00:05:03,810 --> 00:05:08,159
applications within the data center so

00:05:05,909 --> 00:05:10,050
sending out emails for example they'll

00:05:08,159 --> 00:05:11,669
get queued in a Kafka cluster and then

00:05:10,050 --> 00:05:15,509
pulled off by another application and

00:05:11,669 --> 00:05:17,849
sent out so a lot of the tracking

00:05:15,509 --> 00:05:19,770
information in particular is how we get

00:05:17,849 --> 00:05:21,599
our data to Hadoop it comes in from the

00:05:19,770 --> 00:05:23,339
front end websites it goes into Kafka

00:05:21,599 --> 00:05:25,050
the tracking information all gets flowed

00:05:23,339 --> 00:05:26,939
to Hadoop and then it goes back out

00:05:25,050 --> 00:05:28,379
again via another set of clusters that

00:05:26,939 --> 00:05:29,909
push it back out to the front end

00:05:28,379 --> 00:05:32,069
applications that want to know about it

00:05:29,909 --> 00:05:33,479
this is how we feed changes into search

00:05:32,069 --> 00:05:35,759
this is how we feed changes into

00:05:33,479 --> 00:05:38,460
recommendation systems it's used for

00:05:35,759 --> 00:05:42,240
almost everything within LinkedIn

00:05:38,460 --> 00:05:44,189
touches Kafka in one way or another so

00:05:42,240 --> 00:05:46,649
really what we're talking about is

00:05:44,189 --> 00:05:50,009
running Kafka as a service we don't

00:05:46,649 --> 00:05:51,750
control the producers that put data into

00:05:50,009 --> 00:05:54,029
Kafka and we don't control the consumers

00:05:51,750 --> 00:05:56,610
that pull data out as the sres we just

00:05:54,029 --> 00:05:59,610
run the platform of Kafka this is

00:05:56,610 --> 00:06:01,110
opposed to a lot of other installations

00:05:59,610 --> 00:06:02,939
of Kafka where the people who are

00:06:01,110 --> 00:06:04,949
running Kafka also control all the data

00:06:02,939 --> 00:06:07,080
going in and all the data going out so

00:06:04,949 --> 00:06:09,990
we've got a lot of unique concerns

00:06:07,080 --> 00:06:11,310
because of this how much are we how much

00:06:09,990 --> 00:06:14,310
data are we talking about

00:06:11,310 --> 00:06:17,849
we have over 300 Kafka brokers at

00:06:14,310 --> 00:06:19,770
LinkedIn and that's actually low because

00:06:17,849 --> 00:06:24,120
we just deployed four new clusters last

00:06:19,770 --> 00:06:26,789
week we have over 18,000 topics in those

00:06:24,120 --> 00:06:29,129
brokers and we have over a hundred and

00:06:26,789 --> 00:06:31,110
forty thousand partitions worth of data

00:06:29,129 --> 00:06:34,469
that we're separating everything out

00:06:31,110 --> 00:06:36,419
into on an average day we're doing two

00:06:34,469 --> 00:06:39,269
hundred and twenty billion messages into

00:06:36,419 --> 00:06:40,720
produced into Kafka and we're doing

00:06:39,269 --> 00:06:43,450
about 40 terabytes

00:06:40,720 --> 00:06:45,490
of data coming in we have a hundred and

00:06:43,450 --> 00:06:47,740
sixty terabytes of data going out so

00:06:45,490 --> 00:06:49,480
here's where the log based retention

00:06:47,740 --> 00:06:52,930
really comes into play we don't have to

00:06:49,480 --> 00:06:55,600
produce as much data as we consume at

00:06:52,930 --> 00:06:58,060
peak we're doing three point two five

00:06:55,600 --> 00:07:00,670
million messages per second give or take

00:06:58,060 --> 00:07:03,010
and we're talking about about five and a

00:07:00,670 --> 00:07:05,140
half gigabytes per second or gigabits

00:07:03,010 --> 00:07:07,030
per second of traffic inbound to all the

00:07:05,140 --> 00:07:09,100
clusters and about eighteen gigabits per

00:07:07,030 --> 00:07:11,320
second of traffic outbound from all the

00:07:09,100 --> 00:07:13,540
clusters so as you can tell we've got a

00:07:11,320 --> 00:07:15,370
lot of data flowing through a lot of

00:07:13,540 --> 00:07:17,860
different systems and it gives us some

00:07:15,370 --> 00:07:21,550
fairly unique challenges on a daily

00:07:17,860 --> 00:07:26,290
basis alright so here the some of the

00:07:21,550 --> 00:07:28,180
challenges that we've ran into and the

00:07:26,290 --> 00:07:30,070
solutions so one of the big things is

00:07:28,180 --> 00:07:32,980
Kafka's still young it's still being

00:07:30,070 --> 00:07:34,540
actively developed we sit with the core

00:07:32,980 --> 00:07:35,980
team of developers every day and so one

00:07:34,540 --> 00:07:38,620
of the things we've been able to do is

00:07:35,980 --> 00:07:40,330
influence their development to fix our

00:07:38,620 --> 00:07:42,520
operational problems the other thing

00:07:40,330 --> 00:07:44,800
we've done is some operations wizardry

00:07:42,520 --> 00:07:46,570
to get around the problems until the

00:07:44,800 --> 00:07:49,480
developers can implement better

00:07:46,570 --> 00:07:51,640
solutions so hyper growth so there's

00:07:49,480 --> 00:07:53,500
more and more data every single day it's

00:07:51,640 --> 00:07:56,740
constantly growing it's never going to

00:07:53,500 --> 00:07:59,080
get smaller so you have to be able to

00:07:56,740 --> 00:08:01,300
expand the clusters to uh to keep up and

00:07:59,080 --> 00:08:05,110
then more importantly you have to be

00:08:01,300 --> 00:08:07,870
able to balance them so previously with

00:08:05,110 --> 00:08:11,350
Kafka before Oh 8:1 when you add a new

00:08:07,870 --> 00:08:13,390
broker to your existing cluster that new

00:08:11,350 --> 00:08:15,729
broker would just sit there and do

00:08:13,390 --> 00:08:19,240
nothing until you added a new topic or

00:08:15,729 --> 00:08:21,700
added new partitions so with the o8 one

00:08:19,240 --> 00:08:23,729
release one of the cool things that was

00:08:21,700 --> 00:08:25,960
added was this new feature called meta

00:08:23,729 --> 00:08:27,520
partition reassignment so what that

00:08:25,960 --> 00:08:30,190
allows us to do is move active

00:08:27,520 --> 00:08:32,469
partitions onto other brokers move

00:08:30,190 --> 00:08:35,830
things around and so we created a script

00:08:32,469 --> 00:08:37,450
that allows us to balance the load of

00:08:35,830 --> 00:08:40,599
the entire cluster so after we add in

00:08:37,450 --> 00:08:42,280
new brokers we can then take and average

00:08:40,599 --> 00:08:43,510
out you know the amount of data that

00:08:42,280 --> 00:08:46,660
should be on every single cluster and

00:08:43,510 --> 00:08:49,900
then spread it out with an optimal

00:08:46,660 --> 00:08:51,700
number of moves to you know not have

00:08:49,900 --> 00:08:53,800
drastic impacts on the cluster while

00:08:51,700 --> 00:08:54,640
that's happening so one of the other

00:08:53,800 --> 00:08:57,850
issues that we ran

00:08:54,640 --> 00:08:59,020
- is as Todd mentioned every single

00:08:57,850 --> 00:09:01,090
application at LinkedIn sends their

00:08:59,020 --> 00:09:02,710
metrics into Kafka well it turns out

00:09:01,090 --> 00:09:05,860
they also send their blogging data to

00:09:02,710 --> 00:09:07,750
that same cluster so logging killed the

00:09:05,860 --> 00:09:11,320
cluster one day someone who deployed

00:09:07,750 --> 00:09:14,080
some some application and it had tons of

00:09:11,320 --> 00:09:16,360
debug logs it all went into Kafka and it

00:09:14,080 --> 00:09:17,950
literally killed it so one of the cool

00:09:16,360 --> 00:09:19,720
things we were able to do is build

00:09:17,950 --> 00:09:21,970
quality of service within Kafka thanks

00:09:19,720 --> 00:09:23,920
to that partition reassignment tool so

00:09:21,970 --> 00:09:25,380
what this is is we're able to build

00:09:23,920 --> 00:09:29,640
essentially a cluster within a cluster

00:09:25,380 --> 00:09:32,200
so rather than building a new cluster

00:09:29,640 --> 00:09:34,270
we're able to segregate certain brokers

00:09:32,200 --> 00:09:36,850
to handle certain types of data so we

00:09:34,270 --> 00:09:38,680
did here is we separated the metrics

00:09:36,850 --> 00:09:40,570
data which is our most important data on

00:09:38,680 --> 00:09:45,420
this cluster onto its own dedicated

00:09:40,570 --> 00:09:47,590
brokers and then we're able to then

00:09:45,420 --> 00:09:50,170
separate things out and essentially do

00:09:47,590 --> 00:09:53,440
quality of service the big advantage

00:09:50,170 --> 00:09:55,480
over creating a new cluster is at

00:09:53,440 --> 00:09:57,100
LinkedIn every single application sends

00:09:55,480 --> 00:09:59,290
their logging and metric stated Akaka so

00:09:57,100 --> 00:10:01,330
we have to change every single instance

00:09:59,290 --> 00:10:04,030
of every single application to get this

00:10:01,330 --> 00:10:05,860
to work so the beauty of this solution

00:10:04,030 --> 00:10:07,780
that we came up with is it's transparent

00:10:05,860 --> 00:10:09,910
to the users they have no idea that this

00:10:07,780 --> 00:10:14,260
actually happened but our metrics data

00:10:09,910 --> 00:10:16,240
is now got better service we also had

00:10:14,260 --> 00:10:19,180
some deployment nightmares so parallel

00:10:16,240 --> 00:10:20,380
deployments wasn't possible so we wound

00:10:19,180 --> 00:10:22,810
up spending a whole lot of time

00:10:20,380 --> 00:10:24,160
babysitting sequential deployments so if

00:10:22,810 --> 00:10:26,050
you have more than one broker down

00:10:24,160 --> 00:10:27,760
within your cluster you're losing data

00:10:26,050 --> 00:10:30,850
because you're not going to have enough

00:10:27,760 --> 00:10:33,430
active leaders that's not acceptable

00:10:30,850 --> 00:10:37,630
so what we're able to do is influence

00:10:33,430 --> 00:10:41,080
the developers to create shutdown hooks

00:10:37,630 --> 00:10:42,310
so before any broker shuts down it

00:10:41,080 --> 00:10:43,870
checks the health of the rest of the

00:10:42,310 --> 00:10:45,460
cluster to ensure that everything is up

00:10:43,870 --> 00:10:46,930
and running as it should

00:10:45,460 --> 00:10:50,890
before that broker enters the shutdown

00:10:46,930 --> 00:10:52,060
sequence so if any brokers have under

00:10:50,890 --> 00:10:54,160
replicated partitions for whatever

00:10:52,060 --> 00:10:58,960
reason the cluster will the broker will

00:10:54,160 --> 00:11:00,400
refuse to shut down C so one of the

00:10:58,960 --> 00:11:03,970
other issues around in C was zookeeper

00:11:00,400 --> 00:11:05,380
so with Kafka ate all the consumer

00:11:03,970 --> 00:11:08,860
offset information is stored in the

00:11:05,380 --> 00:11:11,410
zookeeper so we have 140,000 partitions

00:11:08,860 --> 00:11:13,090
think and every single one of those has

00:11:11,410 --> 00:11:16,810
at least one thing that reads from it

00:11:13,090 --> 00:11:18,210
and if it periodically updates its

00:11:16,810 --> 00:11:20,890
offset informations uzuki but that's

00:11:18,210 --> 00:11:23,680
140,000 writes the zookeeper couldn't

00:11:20,890 --> 00:11:26,140
scale so what we were able to do is we

00:11:23,680 --> 00:11:29,230
put zookeeper on SSDs and there's lots

00:11:26,140 --> 00:11:30,640
of different arguments on the internet

00:11:29,230 --> 00:11:32,980
about whether or not that's a good idea

00:11:30,640 --> 00:11:36,280
or not we tried it it worked really well

00:11:32,980 --> 00:11:38,590
for us we dropped our average latency of

00:11:36,280 --> 00:11:41,110
zookeeper down to zero milliseconds we

00:11:38,590 --> 00:11:42,640
thought we broke it turns out it was

00:11:41,110 --> 00:11:47,250
just zero that's how it was supposed to

00:11:42,640 --> 00:11:49,930
be so now total talk about monitoring

00:11:47,250 --> 00:11:51,820
all right so one of the big things that

00:11:49,930 --> 00:11:53,890
we have to do because everyone depends

00:11:51,820 --> 00:11:56,200
on Kafka is we need to make sure it's up

00:11:53,890 --> 00:11:59,320
and running all the time because no

00:11:56,200 --> 00:12:01,390
matter what happens any application that

00:11:59,320 --> 00:12:04,080
breaks anything goes wrong within

00:12:01,390 --> 00:12:07,990
LinkedIn what do I hear

00:12:04,080 --> 00:12:09,730
Kafka's broken how many of you have

00:12:07,990 --> 00:12:11,680
little girls I have little girls

00:12:09,730 --> 00:12:14,500
anyone who has little girls knows that

00:12:11,680 --> 00:12:15,130
this is Pinkie Pie Pinkie Pie is always

00:12:14,500 --> 00:12:17,530
happy

00:12:15,130 --> 00:12:19,750
she loves Kafka Kafka moves all her data

00:12:17,530 --> 00:12:22,570
around so why is she so angry all of a

00:12:19,750 --> 00:12:24,790
sudden well it's because everything's

00:12:22,570 --> 00:12:27,340
Kafka's fault first if you have an alert

00:12:24,790 --> 00:12:29,680
that has Kafka in the title it must be

00:12:27,340 --> 00:12:34,030
Kafka spall just because you're reading

00:12:29,680 --> 00:12:36,190
data from Kafka so most of the time when

00:12:34,030 --> 00:12:37,930
an application that's using Kafka has a

00:12:36,190 --> 00:12:40,720
problem that especially one that's

00:12:37,930 --> 00:12:43,480
consuming data from Kafka it's something

00:12:40,720 --> 00:12:45,280
called lag so what's lag lag is the

00:12:43,480 --> 00:12:47,320
difference between where your consumer

00:12:45,280 --> 00:12:49,330
is in the stream of message data and

00:12:47,320 --> 00:12:50,680
where the broker is the difference

00:12:49,330 --> 00:12:52,690
between the two is the amount of lag

00:12:50,680 --> 00:12:56,650
that you have it's how far behind you

00:12:52,690 --> 00:12:58,660
are reading messages so this is a

00:12:56,650 --> 00:13:00,760
consumer problem most of the problems

00:12:58,660 --> 00:13:03,130
are consumer problems unless something

00:13:00,760 --> 00:13:05,110
about Kafka actually is broken which is

00:13:03,130 --> 00:13:06,490
fairly often you know we fix things we

00:13:05,110 --> 00:13:11,920
break things we fix things that's what

00:13:06,490 --> 00:13:13,720
we do so why do consumers lag sometimes

00:13:11,920 --> 00:13:16,180
it's an application problem sometimes

00:13:13,720 --> 00:13:18,790
it's because they have cheese garbage

00:13:16,180 --> 00:13:20,350
collection issues and they're they slow

00:13:18,790 --> 00:13:22,720
down or they lose their connection to

00:13:20,350 --> 00:13:24,610
zookeeper and they've lost their place

00:13:22,720 --> 00:13:27,310
or their processing takes a long time

00:13:24,610 --> 00:13:28,839
and they're just taking too long to

00:13:27,310 --> 00:13:31,779
process their data and data is stacking

00:13:28,839 --> 00:13:34,259
up because it's coming in too quickly it

00:13:31,779 --> 00:13:37,779
could be a Coptic client problem so as

00:13:34,259 --> 00:13:39,339
the Kafka team so we are the Kafka sres

00:13:37,779 --> 00:13:41,920
we also have the Kafka developers the

00:13:39,339 --> 00:13:43,660
Kafka developers are responsible for the

00:13:41,920 --> 00:13:46,149
client and the produce to the consumer

00:13:43,660 --> 00:13:48,279
client and the consumer or the producer

00:13:46,149 --> 00:13:49,899
client at LinkedIn so it could be a

00:13:48,279 --> 00:13:51,670
problem in the Kafka client there could

00:13:49,899 --> 00:13:53,740
be a bug that they're running into it

00:13:51,670 --> 00:13:55,360
could be a performance problem in that

00:13:53,740 --> 00:13:56,860
case it kind of does become a Kafka

00:13:55,360 --> 00:14:00,250
problem and you know we need to address

00:13:56,860 --> 00:14:02,199
that bug and get it fixed so if

00:14:00,250 --> 00:14:04,180
everyone's blaming Kafka all the time

00:14:02,199 --> 00:14:05,529
how do we get any sleep at night because

00:14:04,180 --> 00:14:08,259
we're the ones that get called at 3:00

00:14:05,529 --> 00:14:10,629
a.m. when something's broken the biggest

00:14:08,259 --> 00:14:13,269
thing is educating users and when we

00:14:10,629 --> 00:14:14,649
talk about educating users we're talking

00:14:13,269 --> 00:14:17,050
about teaching them why lag is their

00:14:14,649 --> 00:14:18,459
fault so we go in and we look at their

00:14:17,050 --> 00:14:20,439
application we'll take a look at their

00:14:18,459 --> 00:14:21,279
GC logs we'll take a look at the errors

00:14:20,439 --> 00:14:22,990
that they're getting out of their

00:14:21,279 --> 00:14:25,180
application and try to determine what's

00:14:22,990 --> 00:14:27,279
going on we'll look at it we'll start

00:14:25,180 --> 00:14:29,740
with looking at our clusters and say are

00:14:27,279 --> 00:14:31,269
we healthy if Kafka is healthy then

00:14:29,740 --> 00:14:32,920
let's look at your application is your

00:14:31,269 --> 00:14:34,809
application healthy are you running into

00:14:32,920 --> 00:14:36,519
a bug is there something that we can

00:14:34,809 --> 00:14:39,550
tweak within your configuration to make

00:14:36,519 --> 00:14:41,559
things run a little better for you but

00:14:39,550 --> 00:14:43,779
as part of that as I said we have to

00:14:41,559 --> 00:14:45,250
make sure that we're okay on our side

00:14:43,779 --> 00:14:47,920
because the first thing we need to be

00:14:45,250 --> 00:14:51,100
able to say is no the Kafka cluster is

00:14:47,920 --> 00:14:53,350
healthy or if it's not there's a problem

00:14:51,100 --> 00:14:54,819
and we need to fix it right now and

00:14:53,350 --> 00:14:57,939
generally we want to know about our

00:14:54,819 --> 00:15:00,009
problems before our users do so we

00:14:57,939 --> 00:15:01,930
monitor a lot of things about Kafka we

00:15:00,009 --> 00:15:04,029
monitor the Kafka brokers themselves

00:15:01,930 --> 00:15:05,829
we monitor zookeeper as the cough gasps

00:15:04,029 --> 00:15:08,769
Aires we're also responsible for all the

00:15:05,829 --> 00:15:10,750
zookeeper clusters we monitor our mirror

00:15:08,769 --> 00:15:11,980
makers and I'll get into exactly what

00:15:10,750 --> 00:15:14,319
those are in just a moment

00:15:11,980 --> 00:15:16,089
we monitor our audit infrastructure

00:15:14,319 --> 00:15:18,069
which I'll also get into in just a

00:15:16,089 --> 00:15:21,339
moment and then we have restful

00:15:18,069 --> 00:15:22,569
interfaces - Kafka as well that are for

00:15:21,339 --> 00:15:24,339
anyone who doesn't want to use the Java

00:15:22,569 --> 00:15:25,660
client for some reason or they can't use

00:15:24,339 --> 00:15:28,899
the Java client or they're coming in

00:15:25,660 --> 00:15:30,639
externally and then we also monitor our

00:15:28,899 --> 00:15:32,380
week over week trending so we look at

00:15:30,639 --> 00:15:34,569
what was going on last week and what's

00:15:32,380 --> 00:15:36,339
going on this week if we have the same

00:15:34,569 --> 00:15:38,439
spike in use

00:15:36,339 --> 00:15:40,990
every week at the same time it's

00:15:38,439 --> 00:15:42,759
probably normal if we see our that we're

00:15:40,990 --> 00:15:44,800
trending up a little bit this week over

00:15:42,759 --> 00:15:46,959
last week we have a growth curve and we

00:15:44,800 --> 00:15:49,660
have to be able to address that and

00:15:46,959 --> 00:15:53,019
expand things as we get closer to our

00:15:49,660 --> 00:15:56,439
capacity so when we're monitoring the

00:15:53,019 --> 00:15:59,439
kafka brokers what are we looking at if

00:15:56,439 --> 00:16:01,959
you monitor nothing else monitor under

00:15:59,439 --> 00:16:03,939
replicated partitions what an under

00:16:01,959 --> 00:16:05,889
replicated partition is is it means that

00:16:03,939 --> 00:16:08,139
one of the brokers is reporting that

00:16:05,889 --> 00:16:10,779
it's the leader for a partition and

00:16:08,139 --> 00:16:12,670
there aren't enough insync replicas so

00:16:10,779 --> 00:16:15,040
you said you wanted two replicas of any

00:16:12,670 --> 00:16:17,019
partition and I only see one the one

00:16:15,040 --> 00:16:19,899
that I'm running so there must be a

00:16:17,019 --> 00:16:21,220
problem somewhere under replicated

00:16:19,899 --> 00:16:23,230
partitions tells you a lot of

00:16:21,220 --> 00:16:25,269
information about your cluster if you

00:16:23,230 --> 00:16:26,860
have high counts from a lot of your

00:16:25,269 --> 00:16:28,899
brokers in the cluster at the same time

00:16:26,860 --> 00:16:30,579
for a long period of time then you

00:16:28,899 --> 00:16:33,040
probably have a broker that's offline

00:16:30,579 --> 00:16:36,339
and you need to address that if you have

00:16:33,040 --> 00:16:38,439
a quick spike of under applications on

00:16:36,339 --> 00:16:41,139
one or two brokers and then it goes away

00:16:38,439 --> 00:16:43,240
well then you probably just have a spike

00:16:41,139 --> 00:16:45,399
in your incoming traffic and you might

00:16:43,240 --> 00:16:47,920
need to adjust how your clusters laid

00:16:45,399 --> 00:16:49,509
out how many partitions you have you you

00:16:47,920 --> 00:16:52,269
probably have some sort of performance

00:16:49,509 --> 00:16:54,610
issue that you have to address another

00:16:52,269 --> 00:16:57,009
thing we monitor is offline partitions

00:16:54,610 --> 00:16:59,079
but as Clark noted if a partitions

00:16:57,009 --> 00:17:01,240
offline that means we're losing data and

00:16:59,079 --> 00:17:03,550
that's not an acceptable situation so if

00:17:01,240 --> 00:17:05,169
this number is ever anything other than

00:17:03,550 --> 00:17:07,059
zero then we have a serious problem

00:17:05,169 --> 00:17:09,490
because that's why we have replication

00:17:07,059 --> 00:17:11,409
is so that we don't have if we do have a

00:17:09,490 --> 00:17:12,880
problem we have a little bit of time to

00:17:11,409 --> 00:17:16,240
take care of it before it actually

00:17:12,880 --> 00:17:18,640
becomes an offline situation we monitor

00:17:16,240 --> 00:17:20,949
the broker partition count this is the

00:17:18,640 --> 00:17:23,230
number of partitions that each broker is

00:17:20,949 --> 00:17:25,980
managing whether it's as a replica or as

00:17:23,230 --> 00:17:28,419
a leader and we use the scripts like the

00:17:25,980 --> 00:17:30,309
partition balance tools that Clark

00:17:28,419 --> 00:17:32,470
talked talked about to make sure that

00:17:30,309 --> 00:17:34,899
this is always fairly even because that

00:17:32,470 --> 00:17:37,030
keeps our load across the entire cluster

00:17:34,899 --> 00:17:38,620
to something that is the same on every

00:17:37,030 --> 00:17:41,380
broker we can assure that we're fully

00:17:38,620 --> 00:17:43,539
utilizing our cluster we also looked at

00:17:41,380 --> 00:17:45,010
the data size on disk because not only

00:17:43,539 --> 00:17:46,840
do we want to know that we have the same

00:17:45,010 --> 00:17:49,120
number of partitions we want to know

00:17:46,840 --> 00:17:51,040
that they're about equal in size because

00:17:49,120 --> 00:17:52,720
that determines how much incoming and

00:17:51,040 --> 00:17:56,200
outgoing network traffic there is on

00:17:52,720 --> 00:17:57,940
those on that particular broker so we

00:17:56,200 --> 00:18:02,710
look at our leader partition count as

00:17:57,940 --> 00:18:05,050
well as I said a partition can only have

00:18:02,710 --> 00:18:06,640
one leader so this means that for any

00:18:05,050 --> 00:18:08,590
given partition only one broker is

00:18:06,640 --> 00:18:10,059
getting the traffic for that partition

00:18:08,590 --> 00:18:11,890
so we want to make sure that each of our

00:18:10,059 --> 00:18:15,370
brokers has approximately the same

00:18:11,890 --> 00:18:17,170
number of leaders this makes sure again

00:18:15,370 --> 00:18:18,040
that all of our traffic is balanced

00:18:17,170 --> 00:18:20,559
across the board

00:18:18,040 --> 00:18:23,080
and we're also looking at our network

00:18:20,559 --> 00:18:25,720
utilization because we can scale things

00:18:23,080 --> 00:18:28,270
horizontally if we start getting up to

00:18:25,720 --> 00:18:30,040
too much of a percentage of the network

00:18:28,270 --> 00:18:32,350
interfaces in use then we can just add

00:18:30,040 --> 00:18:33,970
another broker spread out the partitions

00:18:32,350 --> 00:18:35,710
get a little bit more leeway before we

00:18:33,970 --> 00:18:37,510
have to figure out how to use ten gig

00:18:35,710 --> 00:18:39,850
partitions on all of our brokers and

00:18:37,510 --> 00:18:42,090
make the network guys really unhappy

00:18:39,850 --> 00:18:43,900
when we do that

00:18:42,090 --> 00:18:46,450
zookeeper is the next thing we're

00:18:43,900 --> 00:18:48,340
monitoring primarily we're monitoring

00:18:46,450 --> 00:18:50,679
ensemble availability zookeepers a

00:18:48,340 --> 00:18:52,030
fairly simple beast you want to make

00:18:50,679 --> 00:18:53,650
sure your servers are up you want to

00:18:52,030 --> 00:18:55,809
make sure your latency is low and that's

00:18:53,650 --> 00:18:57,160
about it so we want to make sure that

00:18:55,809 --> 00:18:57,700
all of our servers are online all the

00:18:57,160 --> 00:19:00,520
time

00:18:57,700 --> 00:19:02,290
we generally run five server ensembles

00:19:00,520 --> 00:19:04,150
in production which means we can lose

00:19:02,290 --> 00:19:07,570
two of them before we're gonna start to

00:19:04,150 --> 00:19:10,090
have a serious problem but like anyone

00:19:07,570 --> 00:19:11,800
else if we lose one then we're all up

00:19:10,090 --> 00:19:14,530
and taking a look at that and making

00:19:11,800 --> 00:19:17,500
sure everything's squared away so the

00:19:14,530 --> 00:19:19,420
other thing is latency this doesn't be

00:19:17,500 --> 00:19:21,429
it hasn't been as much of a problem for

00:19:19,420 --> 00:19:23,650
us since we went to SSDs because it's

00:19:21,429 --> 00:19:26,760
really hard to monitor a stream of zeros

00:19:23,650 --> 00:19:28,840
all the time but we'll take that bullet

00:19:26,760 --> 00:19:31,360
we look at the UH number of outstanding

00:19:28,840 --> 00:19:33,850
requests this is another metric that

00:19:31,360 --> 00:19:37,179
went to zero when we went to solid state

00:19:33,850 --> 00:19:39,400
disks because of the way zookeeper

00:19:37,179 --> 00:19:41,770
processes data it processes all the

00:19:39,400 --> 00:19:44,230
requests serially so one request comes

00:19:41,770 --> 00:19:45,820
in that request gets processed the

00:19:44,230 --> 00:19:47,440
response gets sent to the client and

00:19:45,820 --> 00:19:49,809
then the next requesting queue gets

00:19:47,440 --> 00:19:51,910
processed if there's a something that's

00:19:49,809 --> 00:19:54,970
slowing down request getting processed

00:19:51,910 --> 00:19:57,370
for example a disk problem that will

00:19:54,970 --> 00:19:59,140
slow down every request in the queue and

00:19:57,370 --> 00:20:01,420
we'll start to see that number of

00:19:59,140 --> 00:20:02,560
outstanding requests build up that's

00:20:01,420 --> 00:20:04,930
when you know we

00:20:02,560 --> 00:20:06,970
we start to have an issue so mirror

00:20:04,930 --> 00:20:09,160
maker and audit these are these are what

00:20:06,970 --> 00:20:12,190
put our clusters together this is what

00:20:09,160 --> 00:20:14,830
makes our our Kafka infrastructure

00:20:12,190 --> 00:20:16,630
actually be an infrastructure so in our

00:20:14,830 --> 00:20:18,960
normal cluster we have a producer whose

00:20:16,630 --> 00:20:21,370
producing messages into a single cluster

00:20:18,960 --> 00:20:23,230
then we add a mirror maker because we've

00:20:21,370 --> 00:20:25,330
got another cluster that we want to move

00:20:23,230 --> 00:20:27,040
that data to so the mirror maker sits in

00:20:25,330 --> 00:20:28,840
the middle it consumes all of the data

00:20:27,040 --> 00:20:31,300
from the first cluster and produces it

00:20:28,840 --> 00:20:33,430
into the second cluster we use this for

00:20:31,300 --> 00:20:35,200
things like aggregating all of our data

00:20:33,430 --> 00:20:37,420
from production so we have multiple data

00:20:35,200 --> 00:20:40,240
centers we aggregate all the data set

00:20:37,420 --> 00:20:43,780
all the metrics into one cluster so that

00:20:40,240 --> 00:20:44,950
it can be consumed more easily so what

00:20:43,780 --> 00:20:47,680
do we monitor with mirror maker

00:20:44,950 --> 00:20:49,660
we monitor lag why do we monitor lag

00:20:47,680 --> 00:20:51,370
when that's a consumer problem because

00:20:49,660 --> 00:20:53,110
our mirror maker is a consumer and we

00:20:51,370 --> 00:20:54,400
have to be good consumers and we have to

00:20:53,110 --> 00:20:56,680
do what we're supposed to do and keep an

00:20:54,400 --> 00:20:58,570
eye on our lag lag tells us how far

00:20:56,680 --> 00:21:00,220
behind the mirror maker is copying

00:20:58,570 --> 00:21:02,740
messages over to the new cluster so we

00:21:00,220 --> 00:21:05,290
want that to be low because the larger

00:21:02,740 --> 00:21:07,090
that number is the longer it's taking

00:21:05,290 --> 00:21:09,250
for messages to get from one cluster to

00:21:07,090 --> 00:21:11,980
the next the slower our metrics are the

00:21:09,250 --> 00:21:14,140
slower our tracking data is we're also

00:21:11,980 --> 00:21:15,940
looking at dropped messages this is

00:21:14,140 --> 00:21:18,340
another metric where anything other than

00:21:15,940 --> 00:21:21,040
zero is completely unacceptable to us if

00:21:18,340 --> 00:21:21,880
we drop a message it means we're losing

00:21:21,040 --> 00:21:24,130
tracking data

00:21:21,880 --> 00:21:25,780
it means we're losing metrics we're

00:21:24,130 --> 00:21:28,720
losing tracking data we're losing money

00:21:25,780 --> 00:21:30,970
because those are ad clicks those are

00:21:28,720 --> 00:21:32,620
impressions this is all the data that's

00:21:30,970 --> 00:21:34,960
getting processed in Hadoop and push

00:21:32,620 --> 00:21:36,580
back out and if that's not perfect all

00:21:34,960 --> 00:21:40,840
the time then we have a serious problem

00:21:36,580 --> 00:21:42,610
going on so what else do we have to

00:21:40,840 --> 00:21:43,870
assure that these pipelines are healthy

00:21:42,610 --> 00:21:45,240
all the time we have something called

00:21:43,870 --> 00:21:47,740
the audit consumer and this is

00:21:45,240 --> 00:21:50,500
particularly in this isn't part of open

00:21:47,740 --> 00:21:52,870
source Kafka what the audit consumer

00:21:50,500 --> 00:21:55,330
does is every single producer that is

00:21:52,870 --> 00:21:57,310
producing messages into Kafka is also

00:21:55,330 --> 00:22:00,220
producing information about how many

00:21:57,310 --> 00:22:02,800
messages is it produced so over a 10

00:22:00,220 --> 00:22:04,060
minute period or over 1 minute period

00:22:02,800 --> 00:22:05,710
however it's configured for that

00:22:04,060 --> 00:22:07,540
particular cluster the producer will

00:22:05,710 --> 00:22:11,380
produce another message to a custom

00:22:07,540 --> 00:22:14,260
topic that says into topic a I produced

00:22:11,380 --> 00:22:15,640
100 messages and that flows into the

00:22:14,260 --> 00:22:15,950
cluster and it flows through the mirror

00:22:15,640 --> 00:22:19,159
make

00:22:15,950 --> 00:22:20,960
as well we have a special consumer

00:22:19,159 --> 00:22:23,029
called an audit consumer that reads all

00:22:20,960 --> 00:22:25,639
of the messages out of the cluster

00:22:23,029 --> 00:22:27,289
everything except that special those

00:22:25,639 --> 00:22:28,880
special messages that the producer was

00:22:27,289 --> 00:22:30,470
putting in it doesn't care about those

00:22:28,880 --> 00:22:32,990
but it cares about every other message

00:22:30,470 --> 00:22:35,720
in the cluster and it writes information

00:22:32,990 --> 00:22:39,080
out into that same special topic that

00:22:35,720 --> 00:22:41,419
says over this period of time I saw a

00:22:39,080 --> 00:22:44,450
hundred messages produced by this

00:22:41,419 --> 00:22:46,190
producer into this topic and that all

00:22:44,450 --> 00:22:48,769
goes into the cluster and it all flows

00:22:46,190 --> 00:22:50,389
through the mirror maker as well so what

00:22:48,769 --> 00:22:53,299
do we monitor about our audit consumers

00:22:50,389 --> 00:22:54,980
we monitor lag they're a consumer we

00:22:53,299 --> 00:22:56,659
monitor lag there should be a recurring

00:22:54,980 --> 00:22:58,159
topic everyone who's a consumer of

00:22:56,659 --> 00:23:00,889
Kafka's should be monitoring their lag

00:22:58,159 --> 00:23:02,809
to know how far behind they are and then

00:23:00,889 --> 00:23:04,250
we have a completeness check as well

00:23:02,809 --> 00:23:06,049
which doesn't come from the audit

00:23:04,250 --> 00:23:09,019
consumer but it comes from another piece

00:23:06,049 --> 00:23:11,419
called the audit UI so the audit UI sits

00:23:09,019 --> 00:23:14,330
at the lowest layer of our clusters and

00:23:11,419 --> 00:23:16,429
it consumes all of that audit state and

00:23:14,330 --> 00:23:18,590
all of those message counts out of Kafka

00:23:16,429 --> 00:23:21,529
and it puts all the information together

00:23:18,590 --> 00:23:23,570
for each tier and it presents a web

00:23:21,529 --> 00:23:27,080
interface for us and it generates email

00:23:23,570 --> 00:23:29,299
alerts which say over this period of

00:23:27,080 --> 00:23:32,539
time there were X number of messages

00:23:29,299 --> 00:23:35,720
produced into Kafka at Tier one

00:23:32,539 --> 00:23:37,730
I saw X number of messages at tier two

00:23:35,720 --> 00:23:39,350
I saw X number of messages that means

00:23:37,730 --> 00:23:41,779
our completeness is a hundred percent

00:23:39,350 --> 00:23:44,269
and we're very happy if it's ever not a

00:23:41,779 --> 00:23:46,669
hundred percent all of a sudden we start

00:23:44,269 --> 00:23:48,559
getting emails that say which tier is

00:23:46,669 --> 00:23:51,649
not a hundred percent how much it's off

00:23:48,559 --> 00:23:53,389
by so we monitor this for our producers

00:23:51,649 --> 00:23:56,360
we monitor this for each of our cluster

00:23:53,389 --> 00:23:58,370
tears and the Hadoop tier also emits

00:23:56,360 --> 00:23:59,990
information about audit and we find out

00:23:58,370 --> 00:24:03,110
exactly how much data is getting to

00:23:59,990 --> 00:24:05,269
Hadoop because if anything is behind a

00:24:03,110 --> 00:24:07,820
hundred percent then we have the dupe

00:24:05,269 --> 00:24:09,559
jobs that need to stop running and wait

00:24:07,820 --> 00:24:12,019
for the data to catch up before we can

00:24:09,559 --> 00:24:13,340
generate reports for our executives and

00:24:12,019 --> 00:24:15,830
if our executives don't get their

00:24:13,340 --> 00:24:17,149
reports they get very cranky and we have

00:24:15,830 --> 00:24:18,710
to then deal with that we have to answer

00:24:17,149 --> 00:24:22,549
why they're not getting their reports

00:24:18,710 --> 00:24:24,590
every hour so this is one of the graphs

00:24:22,549 --> 00:24:27,409
from our audit UI and you can see over

00:24:24,590 --> 00:24:28,480
time it shows how many messages were

00:24:27,409 --> 00:24:30,280
received

00:24:28,480 --> 00:24:32,830
what you can't see is that each of the

00:24:30,280 --> 00:24:34,990
that line that's there is actually five

00:24:32,830 --> 00:24:38,559
different lines that are all exactly the

00:24:34,990 --> 00:24:40,540
same right up to the end when one of the

00:24:38,559 --> 00:24:43,000
clusters starts reporting that it's

00:24:40,540 --> 00:24:45,190
getting fewer messages than the rest and

00:24:43,000 --> 00:24:47,080
this was actually a problem with in this

00:24:45,190 --> 00:24:48,910
case it was the audit consumer itself

00:24:47,080 --> 00:24:51,510
was not able to read messages fast

00:24:48,910 --> 00:24:54,340
enough so the audit data was not clean

00:24:51,510 --> 00:24:57,610
the other UI also produces another graph

00:24:54,340 --> 00:25:00,730
which is how long it takes data to get

00:24:57,610 --> 00:25:02,790
to each tier so again all of the day all

00:25:00,730 --> 00:25:05,440
the numbers are very low right around

00:25:02,790 --> 00:25:07,960
ten or eleven minutes because there's a

00:25:05,440 --> 00:25:09,970
ten minute batch time in there and then

00:25:07,960 --> 00:25:12,130
all of a sudden the number gets very

00:25:09,970 --> 00:25:13,660
high for one tier and this again shows

00:25:12,130 --> 00:25:15,309
us that we have a problem with how long

00:25:13,660 --> 00:25:17,919
it's taking data to flow through our

00:25:15,309 --> 00:25:18,790
system to that particular tier and we

00:25:17,919 --> 00:25:21,370
need to address that

00:25:18,790 --> 00:25:23,080
so between Mirror maker and audit UI we

00:25:21,370 --> 00:25:24,940
have these pipelines that flow through

00:25:23,080 --> 00:25:26,230
our infrastructure and make sure that

00:25:24,940 --> 00:25:29,020
all of our data gets where it's supposed

00:25:26,230 --> 00:25:32,650
to go and we know that all of it got

00:25:29,020 --> 00:25:34,450
there every single time so the other

00:25:32,650 --> 00:25:36,429
side of monitoring is now that we know

00:25:34,450 --> 00:25:37,120
what's going on we can make it work a

00:25:36,429 --> 00:25:39,910
little bit better

00:25:37,120 --> 00:25:41,290
so what are we tuning primarily we're

00:25:39,910 --> 00:25:43,330
tuning the hardware and we're tuning the

00:25:41,290 --> 00:25:45,280
operating system Kafka itself is a

00:25:43,330 --> 00:25:48,250
fairly efficient beast so when we're

00:25:45,280 --> 00:25:50,530
talking about Kafka the hardware in the

00:25:48,250 --> 00:25:53,380
operating system we start with Colonel

00:25:50,530 --> 00:25:56,260
tuning there's a lot of kernel tuning

00:25:53,380 --> 00:25:58,150
you can do there's it's kind of a little

00:25:56,260 --> 00:26:00,700
bit of magic in there as to exactly what

00:25:58,150 --> 00:26:03,370
you want to touch for us the first thing

00:26:00,700 --> 00:26:05,530
is that we never want to swap if Kafka

00:26:03,370 --> 00:26:07,720
swaps then our Layton sees go up so high

00:26:05,530 --> 00:26:10,360
that nobody's getting anything done so

00:26:07,720 --> 00:26:13,360
swap eNOS is set to zero on all of our

00:26:10,360 --> 00:26:14,830
boxes now obviously that's a request to

00:26:13,360 --> 00:26:17,470
the kernel that it not swap it's not a

00:26:14,830 --> 00:26:20,799
guarantee that it won't swap we also run

00:26:17,470 --> 00:26:23,169
with very very large service so we give

00:26:20,799 --> 00:26:26,260
Kafka for example four gigs of memory to

00:26:23,169 --> 00:26:27,970
work with it's running on a 64 gig box

00:26:26,260 --> 00:26:29,890
and there's nothing else running on that

00:26:27,970 --> 00:26:32,770
system with it primarily because it

00:26:29,890 --> 00:26:34,179
needs all the page cache for i/o so

00:26:32,770 --> 00:26:37,720
we're also looking at our page cache

00:26:34,179 --> 00:26:39,940
settings we allow more dirty pages in

00:26:37,720 --> 00:26:41,450
memory at any given time but we allow

00:26:39,940 --> 00:26:43,250
less dirty cash because

00:26:41,450 --> 00:26:45,680
the cash is more important to us than

00:26:43,250 --> 00:26:49,190
the pages in memory are those can those

00:26:45,680 --> 00:26:50,570
can sit a little bit longer the other

00:26:49,190 --> 00:26:53,390
big thing that we have to monitor is

00:26:50,570 --> 00:26:55,220
disk throughput as I noted earlier Kafka

00:26:53,390 --> 00:26:57,230
is disk based all of the data is getting

00:26:55,220 --> 00:26:58,910
spooled out to disk very little is

00:26:57,230 --> 00:27:01,610
actually sitting in memory most of it

00:26:58,910 --> 00:27:03,830
sits on the disk so how do we make sure

00:27:01,610 --> 00:27:06,380
our disk operates properly all the time

00:27:03,830 --> 00:27:08,390
well the answer as with most things as

00:27:06,380 --> 00:27:10,700
you throw more money at it we add more

00:27:08,390 --> 00:27:13,940
spindles so on all of our servers we

00:27:10,700 --> 00:27:16,700
have 14 disks dedicated to Kafka's data

00:27:13,940 --> 00:27:18,140
running in a raid 10 configuration so we

00:27:16,700 --> 00:27:21,140
get as much throughput out of those as

00:27:18,140 --> 00:27:23,540
possible we also use a larger longer

00:27:21,140 --> 00:27:27,200
commit interval on that particular mount

00:27:23,540 --> 00:27:29,870
point because because we're replicating

00:27:27,200 --> 00:27:32,660
our data from broker to broker we can

00:27:29,870 --> 00:27:35,840
suffer a data loss on any one broker if

00:27:32,660 --> 00:27:37,610
it crashes so whereas the normal commit

00:27:35,840 --> 00:27:39,650
interval I believe is 30 seconds we're

00:27:37,610 --> 00:27:41,930
actually running with 120 seconds on

00:27:39,650 --> 00:27:43,910
that mount point because that means we

00:27:41,930 --> 00:27:47,150
have to go out to disk less often

00:27:43,910 --> 00:27:49,790
provides more efficient operation again

00:27:47,150 --> 00:27:51,710
if that disk crashes for some reason the

00:27:49,790 --> 00:27:53,060
broker goes away another broker will

00:27:51,710 --> 00:27:56,570
pick it up we know that the data is

00:27:53,060 --> 00:27:59,480
still safe even if that happens the

00:27:56,570 --> 00:28:01,460
other side of tuning is the Java Virtual

00:27:59,480 --> 00:28:03,950
Machine so when we're talking about

00:28:01,460 --> 00:28:05,450
tuning the Java we're really talking

00:28:03,950 --> 00:28:06,920
about tuning garbage collection that's

00:28:05,450 --> 00:28:09,620
the the big thing that you have to touch

00:28:06,920 --> 00:28:12,680
now for most of the most people for most

00:28:09,620 --> 00:28:14,630
of the time tuning Java is this dark art

00:28:12,680 --> 00:28:16,400
you have to know all the little magic

00:28:14,630 --> 00:28:18,230
and do all the special little things and

00:28:16,400 --> 00:28:19,940
you have to spend weeks and weeks

00:28:18,230 --> 00:28:22,700
looking at your application to see

00:28:19,940 --> 00:28:25,010
exactly what it's doing yeah we don't do

00:28:22,700 --> 00:28:27,560
that anymore so the first thing we did

00:28:25,010 --> 00:28:28,430
was we moved to Java 7 update 51 as soon

00:28:27,560 --> 00:28:31,970
as it was available

00:28:28,430 --> 00:28:34,070
we tried update 21 for a while update 21

00:28:31,970 --> 00:28:36,350
is ok but it has some serious bugs in

00:28:34,070 --> 00:28:38,750
the garbage collection why do we go to

00:28:36,350 --> 00:28:40,280
Java 7 update 51 as I'm sure a lot of

00:28:38,750 --> 00:28:42,860
you know you get the garbage first

00:28:40,280 --> 00:28:44,930
collector we've actually very recently

00:28:42,860 --> 00:28:47,000
turned this on on all of our Kafka

00:28:44,930 --> 00:28:49,730
instances and it has performed

00:28:47,000 --> 00:28:52,700
wonderfully for us now the way garbage

00:28:49,730 --> 00:28:54,200
first works is it's designed to take the

00:28:52,700 --> 00:28:56,600
magic out of tuning

00:28:54,200 --> 00:28:59,659
it's designed to give you fewer dials

00:28:56,600 --> 00:29:01,940
fewer knobs to touch and provide more

00:28:59,659 --> 00:29:03,619
automation behind figuring out what the

00:29:01,940 --> 00:29:07,820
garbage collector has to do to hit your

00:29:03,619 --> 00:29:10,639
targets it works very well so the first

00:29:07,820 --> 00:29:13,070
thing we do is we set the heap size we

00:29:10,639 --> 00:29:14,629
specify a target pause time for our GC

00:29:13,070 --> 00:29:17,419
now this is the amount of time we'd like

00:29:14,629 --> 00:29:20,059
most of our GCS to take in our case we

00:29:17,419 --> 00:29:22,789
use 20 milliseconds for Kafka and it

00:29:20,059 --> 00:29:24,679
seems to work out pretty well you don't

00:29:22,789 --> 00:29:27,080
set the new size anymore so you don't

00:29:24,679 --> 00:29:29,119
lock in the sizes of any of your

00:29:27,080 --> 00:29:31,220
generations because you want the garbage

00:29:29,119 --> 00:29:33,950
collector itself to manage those for you

00:29:31,220 --> 00:29:35,929
if you do set the new size you override

00:29:33,950 --> 00:29:39,739
the GC pause time and you might as well

00:29:35,929 --> 00:29:41,539
be back tuning Java yourself again so

00:29:39,739 --> 00:29:43,759
what did this do for us it dropped all

00:29:41,539 --> 00:29:46,820
of our GCS on McAfee brokers to less

00:29:43,759 --> 00:29:51,350
than 15 milliseconds per full second

00:29:46,820 --> 00:29:53,480
spent in GC we have steady 22 22 second

00:29:51,350 --> 00:29:55,159
GC intervals all the time but the most

00:29:53,480 --> 00:29:58,580
important thing to us is we no longer

00:29:55,159 --> 00:30:01,399
have any full GC cycles within Kafka

00:29:58,580 --> 00:30:03,950
brokers we get them once in a while when

00:30:01,399 --> 00:30:06,230
we have a load spike and traffic goes up

00:30:03,950 --> 00:30:10,399
very quickly and then drops off we'll

00:30:06,230 --> 00:30:12,710
see one or two GC cycles on one or two

00:30:10,399 --> 00:30:14,779
brokers and then they'll go away those

00:30:12,710 --> 00:30:16,850
full GCS are only 200 to 400

00:30:14,779 --> 00:30:21,259
milliseconds though so they're actually

00:30:16,850 --> 00:30:22,909
not a problem for us so that's a pretty

00:30:21,259 --> 00:30:26,539
much what we do with tuning to make this

00:30:22,909 --> 00:30:30,470
all work properly alright so in closing

00:30:26,539 --> 00:30:31,700
Oh 8 2 is coming out very soon and

00:30:30,470 --> 00:30:34,789
there's some cool new features that are

00:30:31,700 --> 00:30:36,139
coming with that one of the big things

00:30:34,789 --> 00:30:37,330
is the consumer offsets aren't a lot

00:30:36,139 --> 00:30:39,769
going to be stored in the zookeeper

00:30:37,330 --> 00:30:42,799
Kafka is gonna store them itself in a

00:30:39,769 --> 00:30:44,989
specialized topic for offsets delete

00:30:42,799 --> 00:30:47,480
topic this was kind of there in oh wait

00:30:44,989 --> 00:30:50,690
one didn't work if you ran it it kills

00:30:47,480 --> 00:30:53,570
your cluster it's gonna be fixed in a

00:30:50,690 --> 00:30:55,539
way to further down the road new

00:30:53,570 --> 00:30:58,789
producers coming out with much higher

00:30:55,539 --> 00:31:02,359
throughput capability as well as an

00:30:58,789 --> 00:31:04,700
improved API upcoming operational work

00:31:02,359 --> 00:31:06,109
so we're going to learn to share so we

00:31:04,700 --> 00:31:07,670
talked about all these scripts that

00:31:06,109 --> 00:31:09,770
we've written in things like that and

00:31:07,670 --> 00:31:12,280
we're gonna deal Inc to defy them so

00:31:09,770 --> 00:31:14,450
what I mean by that is currently they're

00:31:12,280 --> 00:31:16,550
written within the LinkedIn Python stack

00:31:14,450 --> 00:31:18,950
we're gonna take the LinkedIn part out

00:31:16,550 --> 00:31:21,260
and open source them we're gonna write a

00:31:18,950 --> 00:31:23,960
strip for shrinking a cluster why would

00:31:21,260 --> 00:31:26,330
you do that we're not quite sure yet

00:31:23,960 --> 00:31:29,030
but we know how to do it so we're able

00:31:26,330 --> 00:31:30,170
to do it then we're also writing a

00:31:29,030 --> 00:31:32,390
cluster comparison tool

00:31:30,170 --> 00:31:33,890
so as Todd showed you you know we have

00:31:32,390 --> 00:31:35,240
lots of clusters all talking to each

00:31:33,890 --> 00:31:37,010
other and if you don't have the

00:31:35,240 --> 00:31:39,110
partition counts for those topics set

00:31:37,010 --> 00:31:40,790
the same on every single cluster you'll

00:31:39,110 --> 00:31:43,070
run into bottlenecks you know that if

00:31:40,790 --> 00:31:44,990
the first cluster has 64 partitions next

00:31:43,070 --> 00:31:46,730
one is 8 well it just sometimes you

00:31:44,990 --> 00:31:49,010
can't keep up so we're gonna write a

00:31:46,730 --> 00:31:51,020
tool to compare the clusters to ensure

00:31:49,010 --> 00:31:53,870
that everything's the same across your

00:31:51,020 --> 00:31:56,090
configuration we're also going to work

00:31:53,870 --> 00:31:57,950
with advanced monitoring so we want to

00:31:56,090 --> 00:32:00,050
be able to check catch these load spikes

00:31:57,950 --> 00:32:02,960
as they happen so if a particular topic

00:32:00,050 --> 00:32:04,700
or logging event happens we want to be

00:32:02,960 --> 00:32:06,830
able to see who's sending us all this

00:32:04,700 --> 00:32:09,590
data all at once and then we can take

00:32:06,830 --> 00:32:12,350
appropriate actions to prevent it from

00:32:09,590 --> 00:32:16,460
killing the cluster so how can you get

00:32:12,350 --> 00:32:18,820
involved cough got out of patchy org you

00:32:16,460 --> 00:32:22,610
can join the mailing list the users list

00:32:18,820 --> 00:32:24,350
join the IRC channels on freenode you

00:32:22,610 --> 00:32:27,770
can contribute tools if you feel so

00:32:24,350 --> 00:32:33,910
inclined you can talk to us our

00:32:27,770 --> 00:32:33,910
information is right here and questions

00:32:38,980 --> 00:32:42,400
how are you managing that amount of

00:32:41,140 --> 00:32:44,799
lobsters are you guys using chef or

00:32:42,400 --> 00:32:46,990
puppet to deploy Kafka or what easy put

00:32:44,799 --> 00:32:48,940
the appointment in the configuration so

00:32:46,990 --> 00:32:52,270
LinkedIn has its own internal stack for

00:32:48,940 --> 00:32:54,100
that it's what we use to just that's one

00:32:52,270 --> 00:32:55,960
more question security have you guys had

00:32:54,100 --> 00:32:58,750
to worry about the it's coming in at

00:32:55,960 --> 00:33:03,970
wrist and in transmission it's coming

00:32:58,750 --> 00:33:06,100
we're officially we know that work as at

00:33:03,970 --> 00:33:09,580
LinkedIn we're gonna be forced to use it

00:33:06,100 --> 00:33:11,620
very soon so it will be coming yeah it's

00:33:09,580 --> 00:33:13,750
something that the developers are

00:33:11,620 --> 00:33:14,919
actively talking about and tried to

00:33:13,750 --> 00:33:16,330
figure out exactly when it's gonna

00:33:14,919 --> 00:33:17,380
happen they were talking about doing it

00:33:16,330 --> 00:33:19,330
sometime this year

00:33:17,380 --> 00:33:20,679
it looks like it may take a little

00:33:19,330 --> 00:33:23,350
longer than that just because of the

00:33:20,679 --> 00:33:25,270
complexity we've been asking we you know

00:33:23,350 --> 00:33:28,000
our pushes for both as you said

00:33:25,270 --> 00:33:30,730
encryption of the data in transit you

00:33:28,000 --> 00:33:32,559
can already have a producer encrypt the

00:33:30,730 --> 00:33:34,960
data and have the consumer decrypt it

00:33:32,559 --> 00:33:37,750
but we actually want full TLS on the

00:33:34,960 --> 00:33:39,820
entire transmission so that we can have

00:33:37,750 --> 00:33:41,950
right now if we have someone who's

00:33:39,820 --> 00:33:45,250
outside of LinkedIn producing data in

00:33:41,950 --> 00:33:47,950
they need to produce into one of our

00:33:45,250 --> 00:33:49,390
rest interfaces and they have to we have

00:33:47,950 --> 00:33:51,490
to encrypt it with a VPN tunnel or

00:33:49,390 --> 00:33:54,190
something else and then the other thing

00:33:51,490 --> 00:33:55,570
obviously is ACLs on the data itself so

00:33:54,190 --> 00:33:57,850
that we can assure who's talking to

00:33:55,570 --> 00:33:59,140
particular things it's pretty complex

00:33:57,850 --> 00:34:03,030
it's definitely something on their

00:33:59,140 --> 00:34:03,030
roadmap but we can't say exactly when

00:34:03,750 --> 00:34:09,220
you mentioned data segregation for the

00:34:06,640 --> 00:34:11,350
login metrics yeah what sort of knobs or

00:34:09,220 --> 00:34:13,090
tweaks or conditions can you do to

00:34:11,350 --> 00:34:14,919
choose when to segregate or when not to

00:34:13,090 --> 00:34:17,290
segregate so essentially it's all done

00:34:14,919 --> 00:34:18,850
by the the topic name so we ensure that

00:34:17,290 --> 00:34:21,190
topic all the partitions for a

00:34:18,850 --> 00:34:23,440
particular topic exist on those

00:34:21,190 --> 00:34:25,600
particular brokers we store that in a

00:34:23,440 --> 00:34:27,550
key value store and then we were able to

00:34:25,600 --> 00:34:30,070
write scripts that periodically check to

00:34:27,550 --> 00:34:31,810
make sure no new topics were created on

00:34:30,070 --> 00:34:37,629
this part a promise brokers and then we

00:34:31,810 --> 00:34:39,879
move them away if they were any other

00:34:37,629 --> 00:34:43,909
questions

00:34:39,879 --> 00:34:45,470
go for it so you've got 18,000 topics

00:34:43,909 --> 00:34:47,539
all right at the moment we're kind of

00:34:45,470 --> 00:34:49,460
looking at how to figure out how to do

00:34:47,539 --> 00:34:51,230
topic segregation so just in terms of

00:34:49,460 --> 00:34:52,609
your experience and I know this isn't a

00:34:51,230 --> 00:34:55,730
generic answer you're gonna give me but

00:34:52,609 --> 00:34:56,899
in the LinkedIn experience how did you

00:34:55,730 --> 00:34:58,549
guys decide how we were going to

00:34:56,899 --> 00:34:59,960
segregate things like topics and metrics

00:34:58,549 --> 00:35:01,460
was it purely by scale or was there

00:34:59,960 --> 00:35:02,799
functional reasons as well are you

00:35:01,460 --> 00:35:05,329
saying by dividing it between clusters

00:35:02,799 --> 00:35:06,920
I'm actually talking about okay we need

00:35:05,329 --> 00:35:09,349
a new topic for this and there was some

00:35:06,920 --> 00:35:11,180
reason behind the needing of that new

00:35:09,349 --> 00:35:12,500
topic and generally speaking what are

00:35:11,180 --> 00:35:14,690
the reasons behind okay we need a new

00:35:12,500 --> 00:35:16,880
topic for this right so that I mean it's

00:35:14,690 --> 00:35:17,990
pretty much outside of our realm for

00:35:16,880 --> 00:35:19,849
that stuff because we're essentially

00:35:17,990 --> 00:35:22,849
running Kafka as a service so anyone

00:35:19,849 --> 00:35:24,529
that wants to create a topic can they

00:35:22,849 --> 00:35:26,390
just start sending us data and the topic

00:35:24,529 --> 00:35:28,010
gets created there's other you know

00:35:26,390 --> 00:35:29,510
things that they have to need to get the

00:35:28,010 --> 00:35:32,809
their schema approved and things like

00:35:29,510 --> 00:35:34,690
that that we have internals LinkedIn but

00:35:32,809 --> 00:35:38,539
we're not really part of that process

00:35:34,690 --> 00:35:40,339
yeah we do actually for the most part

00:35:38,539 --> 00:35:42,380
people break out their topics by the

00:35:40,339 --> 00:35:43,880
type of data it is so there's one topic

00:35:42,380 --> 00:35:46,009
for a pageview

00:35:43,880 --> 00:35:48,170
there's one topic for an ad click

00:35:46,009 --> 00:35:50,839
there's a topic for somebody clicked on

00:35:48,170 --> 00:35:52,250
a job there's so there's topics for all

00:35:50,839 --> 00:35:54,319
of these things and they get aggregated

00:35:52,250 --> 00:35:56,269
at the other end as Clark mentioned we

00:35:54,319 --> 00:35:57,920
have you know a review committee that

00:35:56,269 --> 00:35:59,690
reviews the data models and make sure

00:35:57,920 --> 00:36:01,849
that the consistent they all contain the

00:35:59,690 --> 00:36:05,869
same types of data in the same types of

00:36:01,849 --> 00:36:07,670
fields but our focus really is running

00:36:05,869 --> 00:36:09,349
it as a service we don't want to have to

00:36:07,670 --> 00:36:10,849
care about the data as you know as the

00:36:09,349 --> 00:36:12,109
guys running Kafka we don't want to have

00:36:10,849 --> 00:36:13,160
to care about the data that's going

00:36:12,109 --> 00:36:16,250
through it we just want to know the

00:36:13,160 --> 00:36:18,200
sizes of what it is just won't finding

00:36:16,250 --> 00:36:20,359
the question sorry on the prior to using

00:36:18,200 --> 00:36:22,519
g1g see where you guys even CMS or what

00:36:20,359 --> 00:36:24,589
we're using prior after we were you

00:36:22,519 --> 00:36:33,609
saying pärnu and CMS okay all right

00:36:24,589 --> 00:36:33,609

YouTube URL: https://www.youtube.com/watch?v=7dkSze52i-o


