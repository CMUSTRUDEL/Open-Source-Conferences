Title: Mesos Elastically Scalable Operations, Simplified - Niklas Nielsen and Adam Bordelon
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,179 --> 00:00:05,730
good morning everyone and thanks for

00:00:02,730 --> 00:00:07,379
showing up I am Nicholas and this is

00:00:05,730 --> 00:00:10,590
Adam and we are super stoked to be here

00:00:07,379 --> 00:00:13,410
today to talk about missus elastically

00:00:10,590 --> 00:00:15,120
scalable operations simplified I hope

00:00:13,410 --> 00:00:19,020
you've been Agron's not but I can see

00:00:15,120 --> 00:00:22,050
that this is a recursive acronym yeah we

00:00:19,020 --> 00:00:24,269
had fun doing it so let's start doing

00:00:22,050 --> 00:00:29,099
the usual poll how many of you guys are

00:00:24,269 --> 00:00:31,920
somehow involved with operations it's

00:00:29,099 --> 00:00:34,340
about half that's great how many of you

00:00:31,920 --> 00:00:37,410
guys have heard about missus before

00:00:34,340 --> 00:00:38,969
almost everybody that's great how many

00:00:37,410 --> 00:00:44,270
are using missiles as a part of their

00:00:38,969 --> 00:00:46,950
infrastructure one okay - great

00:00:44,270 --> 00:00:50,820
hopefully by the end of a petty con that

00:00:46,950 --> 00:00:52,559
number would increase so before we get

00:00:50,820 --> 00:00:54,210
started you can actually launch a

00:00:52,559 --> 00:00:55,980
message cluster right now for those of

00:00:54,210 --> 00:00:58,410
you who have laptops or on your laps

00:00:55,980 --> 00:00:59,760
and if you do so by the end of this talk

00:00:58,410 --> 00:01:02,760
your likes they have a message cluster

00:00:59,760 --> 00:01:04,799
that you could try out we created a tool

00:01:02,760 --> 00:01:08,880
that launch is small but complete

00:01:04,799 --> 00:01:11,189
message clusters on Amazon ec2 so if you

00:01:08,880 --> 00:01:14,070
go to elastic Doug mesosphere at i/o and

00:01:11,189 --> 00:01:15,960
that's the message with large clusters

00:01:14,070 --> 00:01:18,869
in three steps like you choose the

00:01:15,960 --> 00:01:20,580
cluster size you enter your easy -

00:01:18,869 --> 00:01:23,880
credentials and your email address and

00:01:20,580 --> 00:01:25,500
you push launch and the only thing that

00:01:23,880 --> 00:01:27,829
we store is your email address that we

00:01:25,500 --> 00:01:30,479
can tell you when the cluster is running

00:01:27,829 --> 00:01:33,869
but now why is it Adam and I are talking

00:01:30,479 --> 00:01:36,060
here about missiles today we are both

00:01:33,869 --> 00:01:38,280
working at a start-up that's called

00:01:36,060 --> 00:01:42,180
mesosphere and we work out of this great

00:01:38,280 --> 00:01:43,979
place in San Francisco we are a young

00:01:42,180 --> 00:01:46,560
and small company that works with in and

00:01:43,979 --> 00:01:48,840
around Apache missiles and we hope

00:01:46,560 --> 00:01:51,119
getting companies on board using

00:01:48,840 --> 00:01:53,130
missiles and their data centers that's

00:01:51,119 --> 00:01:55,200
why we here today I worked closely

00:01:53,130 --> 00:01:58,680
together with Adam who's a excellent

00:01:55,200 --> 00:02:00,659
distributed systems engineer he has map

00:01:58,680 --> 00:02:02,850
our and Amazon on his resume and worked

00:02:00,659 --> 00:02:05,450
on everything from HPC clusters to

00:02:02,850 --> 00:02:05,450
compilers

00:02:08,080 --> 00:02:12,820
Nicholas here is a hard-working Dane

00:02:10,119 --> 00:02:15,250
with an endless supply of gifts and he's

00:02:12,820 --> 00:02:17,200
done DevOps and Copenhagen hacked on

00:02:15,250 --> 00:02:19,630
supercomputers at Lawrence Livermore and

00:02:17,200 --> 00:02:22,170
built flash beams at Adobe now he's a

00:02:19,630 --> 00:02:26,320
top committer on the Apache Mesa project

00:02:22,170 --> 00:02:28,120
Thanks so we have three things we want

00:02:26,320 --> 00:02:29,860
to share today we want to share our

00:02:28,120 --> 00:02:32,860
master story and how we came to learn

00:02:29,860 --> 00:02:34,090
about mesos we want to share the

00:02:32,860 --> 00:02:35,890
Mesmer's approach and how mrs.

00:02:34,090 --> 00:02:37,690
accomplishes cluster scheduling and

00:02:35,890 --> 00:02:39,430
lastly we're going to talk about

00:02:37,690 --> 00:02:41,560
missiles in production and how some of

00:02:39,430 --> 00:02:44,560
the companies are using missiles as a

00:02:41,560 --> 00:02:46,150
vital part of their infrastructures so

00:02:44,560 --> 00:02:49,660
jumping right into the our first section

00:02:46,150 --> 00:02:51,820
or story like many of you guys we are

00:02:49,660 --> 00:02:53,320
skeptical by nature and we weren't born

00:02:51,820 --> 00:02:57,400
believing that Messer's is a great

00:02:53,320 --> 00:02:59,020
technology so I think the hypothesis

00:02:57,400 --> 00:03:00,220
when we started looking at missiles

00:02:59,020 --> 00:03:01,380
could be captured in these three

00:03:00,220 --> 00:03:04,090
sentences

00:03:01,380 --> 00:03:07,030
Messer's is distributed systems to build

00:03:04,090 --> 00:03:08,680
and run distributed systems missiles

00:03:07,030 --> 00:03:10,959
provides fine grain resource sharing in

00:03:08,680 --> 00:03:14,489
isolation and enables high availability

00:03:10,959 --> 00:03:18,220
and fault tolerance for your cluster

00:03:14,489 --> 00:03:19,300
right like many of you guys we don't

00:03:18,220 --> 00:03:23,110
believe everything but that we're being

00:03:19,300 --> 00:03:24,550
told but something started to make sense

00:03:23,110 --> 00:03:26,489
when we thought about it and when real

00:03:24,550 --> 00:03:30,459
related it to our previous experiences

00:03:26,489 --> 00:03:32,190
because we had seen this before both

00:03:30,459 --> 00:03:34,959
Adam and I have worked with

00:03:32,190 --> 00:03:36,720
supercomputers in grad school and HPC

00:03:34,959 --> 00:03:38,830
clusters like boa and sperm are

00:03:36,720 --> 00:03:41,800
unavoidable parts of those kind of

00:03:38,830 --> 00:03:44,770
workflows researchers submit their job

00:03:41,800 --> 00:03:47,230
requests to the queue along with runtime

00:03:44,770 --> 00:03:48,730
estimates and the scheduler decides the

00:03:47,230 --> 00:03:51,100
best way to pack and prioritize those

00:03:48,730 --> 00:03:54,670
jobs and within a couple of hours your

00:03:51,100 --> 00:03:56,260
job has run on thousands of course but

00:03:54,670 --> 00:03:58,540
when a supercomputers scheduler gives

00:03:56,260 --> 00:04:00,880
you thousands of nodes to run your

00:03:58,540 --> 00:04:02,950
physics simulations it's basically

00:04:00,880 --> 00:04:05,019
setting up a temporary static cluster

00:04:02,950 --> 00:04:07,810
and you're expected to use those

00:04:05,019 --> 00:04:09,880
resources in their entirety and this is

00:04:07,810 --> 00:04:12,239
a different class of workload that you

00:04:09,880 --> 00:04:15,280
would typically see in a data center

00:04:12,239 --> 00:04:16,930
where you will think about boxes that is

00:04:15,280 --> 00:04:18,829
running web servers and memcache a--'s

00:04:16,930 --> 00:04:21,229
and databases

00:04:18,829 --> 00:04:23,870
but why our services and responsibility

00:04:21,229 --> 00:04:26,240
is tied to VMs or machines for that sake

00:04:23,870 --> 00:04:31,849
a box for mail server a box for our

00:04:26,240 --> 00:04:33,830
database a box for you name it so when

00:04:31,849 --> 00:04:35,870
you are at the frontier of data big data

00:04:33,830 --> 00:04:39,440
and big computer you're running to

00:04:35,870 --> 00:04:42,560
problems with scale and reliability

00:04:39,440 --> 00:04:45,080
before anyone else so just like Google

00:04:42,560 --> 00:04:49,340
released MapReduce at BigTable not as

00:04:45,080 --> 00:04:51,050
products but as a ideas Google ork the

00:04:49,340 --> 00:04:53,000
Google board scheduler showed the

00:04:51,050 --> 00:04:55,219
necessity to operate at very different

00:04:53,000 --> 00:04:56,719
abstractions to break loose of the

00:04:55,219 --> 00:05:00,289
useful way of thinking about plus the

00:04:56,719 --> 00:05:01,940
notes with Borg services run on top of a

00:05:00,289 --> 00:05:04,069
cluster without having to know

00:05:01,940 --> 00:05:06,289
necessarily where it will actually run

00:05:04,069 --> 00:05:08,060
and this seems to be a very powerful way

00:05:06,289 --> 00:05:10,669
of thinking about operation and running

00:05:08,060 --> 00:05:13,190
services because app developers can

00:05:10,669 --> 00:05:18,169
think and code against resources instead

00:05:13,190 --> 00:05:19,969
of against specific machines and when

00:05:18,169 --> 00:05:21,379
machines have been abstracted away you

00:05:19,969 --> 00:05:24,409
can make provisioning and launch times

00:05:21,379 --> 00:05:26,449
much faster I remember like my old boss

00:05:24,409 --> 00:05:30,380
looking over my shoulder when we were

00:05:26,449 --> 00:05:31,909
starting up new ec2 instances waiting to

00:05:30,380 --> 00:05:34,610
start provisioning and meet our demand

00:05:31,909 --> 00:05:36,349
for incoming traffic and to combat this

00:05:34,610 --> 00:05:39,770
problem mesos uses lightweight

00:05:36,349 --> 00:05:43,130
containers for resource isolation so

00:05:39,770 --> 00:05:45,380
that jobs can start within seconds here

00:05:43,130 --> 00:05:47,900
this figure of typical Parisian times

00:05:45,380 --> 00:05:49,849
and there's actually a large-scale bare

00:05:47,900 --> 00:05:52,550
metal machines would take hours to start

00:05:49,849 --> 00:05:56,360
VMs would take minutes and containers

00:05:52,550 --> 00:06:00,830
takes up seconds but what happens when

00:05:56,360 --> 00:06:03,139
things break meso seems to be a tool and

00:06:00,830 --> 00:06:05,479
platform to East life of system

00:06:03,139 --> 00:06:07,300
operators making life easy to run poll

00:06:05,479 --> 00:06:11,870
tolerant and highly available services

00:06:07,300 --> 00:06:13,550
so let's do another poll who have you

00:06:11,870 --> 00:06:19,250
guys had to step out of important events

00:06:13,550 --> 00:06:22,009
to deal with program services like half

00:06:19,250 --> 00:06:26,180
all or more how many of you guys have

00:06:22,009 --> 00:06:27,370
been working by pagers at night yep same

00:06:26,180 --> 00:06:29,689
crowd

00:06:27,370 --> 00:06:32,029
one of my sidekicks during grad school

00:06:29,689 --> 00:06:34,159
was as a DevOps in the small startup in

00:06:32,029 --> 00:06:36,439
Copenhagen and it was my responsibility

00:06:34,159 --> 00:06:38,930
it's make sure that all services was

00:06:36,439 --> 00:06:40,819
running on Amazon ec2 but even with

00:06:38,930 --> 00:06:45,740
redundant services things eventually

00:06:40,819 --> 00:06:48,610
went wrong so one time while I was at

00:06:45,740 --> 00:06:50,509
Roskilde festival which is pictured here

00:06:48,610 --> 00:06:52,490
our back-end nodes

00:06:50,509 --> 00:06:54,469
shut down and I had to assess it it's

00:06:52,490 --> 00:06:57,169
just a chin from my phone and restart

00:06:54,469 --> 00:06:59,060
our license service so when I heard

00:06:57,169 --> 00:07:01,639
about the automatic fault tolerance that

00:06:59,060 --> 00:07:04,249
is possible with mesos I realized that

00:07:01,639 --> 00:07:05,960
our manual systems were much too brittle

00:07:04,249 --> 00:07:08,900
back then we were doing operations the

00:07:05,960 --> 00:07:10,400
wrong way and abstracting the notion of

00:07:08,900 --> 00:07:14,389
machines could have made it much easier

00:07:10,400 --> 00:07:16,009
to bring those services back up so when

00:07:14,389 --> 00:07:17,870
we compared the best approach with our

00:07:16,009 --> 00:07:20,779
own experiences it became clear that

00:07:17,870 --> 00:07:24,939
this we needed to be thinking the meses

00:07:20,779 --> 00:07:27,289
way about data center operations right

00:07:24,939 --> 00:07:29,419
so now if we want to start thinking of

00:07:27,289 --> 00:07:31,669
the data center as a single computer

00:07:29,419 --> 00:07:33,409
entity then we need to have a good data

00:07:31,669 --> 00:07:34,969
center operating system you know the

00:07:33,409 --> 00:07:36,529
thing that allows you to write and run

00:07:34,969 --> 00:07:38,479
applications on your data center and

00:07:36,529 --> 00:07:40,939
make sure that all your apps play nicely

00:07:38,479 --> 00:07:43,129
with each other so we at Mesa sphere

00:07:40,939 --> 00:07:45,319
want to build a data center OS with meso

00:07:43,129 --> 00:07:47,360
s-- as the kernel and if you're building

00:07:45,319 --> 00:07:49,219
an OS of any kind you need to have a

00:07:47,360 --> 00:07:51,229
powerful set of tools for administrators

00:07:49,219 --> 00:07:54,740
and a good API for application

00:07:51,229 --> 00:07:56,659
developers mesas provides an SDK for

00:07:54,740 --> 00:07:58,399
distributed systems programming that

00:07:56,659 --> 00:08:00,379
makes it easy to build new distributed

00:07:58,399 --> 00:08:03,379
applications or migrate existing apps

00:08:00,379 --> 00:08:05,539
and services on DeMaio's then Hindman

00:08:03,379 --> 00:08:08,060
sitting over here is one of the original

00:08:05,539 --> 00:08:09,919
authors of mesas and like he said we

00:08:08,060 --> 00:08:11,539
want people to be able to program for

00:08:09,919 --> 00:08:13,969
the data center just like they program

00:08:11,539 --> 00:08:15,979
for their laptop but bin will cover the

00:08:13,969 --> 00:08:18,139
maysa API for developers in more detail

00:08:15,979 --> 00:08:20,149
after lunch so we'll ignore the app

00:08:18,139 --> 00:08:24,379
developers for now and focus on the SIS

00:08:20,149 --> 00:08:26,360
admits who really matters so sis admin's

00:08:24,379 --> 00:08:28,759
your life is hard enough without the

00:08:26,360 --> 00:08:30,740
users complaining that their service

00:08:28,759 --> 00:08:33,079
died or they suddenly need to scale up

00:08:30,740 --> 00:08:35,390
20 more machines or somebody else is

00:08:33,079 --> 00:08:36,800
using up all their ram and there's

00:08:35,390 --> 00:08:38,180
always that one service that doesn't

00:08:36,800 --> 00:08:39,919
play nicely with others so you have to

00:08:38,180 --> 00:08:40,490
fence it off in its own separate cluster

00:08:39,919 --> 00:08:42,320
and

00:08:40,490 --> 00:08:44,450
your manager is asking why he's wasting

00:08:42,320 --> 00:08:45,290
money on machines that are only using 8%

00:08:44,450 --> 00:08:47,300
of their CPU

00:08:45,290 --> 00:08:51,350
it's a memcache fleet man it's using

00:08:47,300 --> 00:08:53,930
tons of memory so maysa wants to let you

00:08:51,350 --> 00:08:56,149
run more applications and services on

00:08:53,930 --> 00:08:57,260
less hardware without having to wake up

00:08:56,149 --> 00:09:00,860
in the middle of the night to deal with

00:08:57,260 --> 00:09:02,870
somebody else's problems let's walk

00:09:00,860 --> 00:09:04,100
through an example say this is your data

00:09:02,870 --> 00:09:06,110
center there's some nice-looking

00:09:04,100 --> 00:09:09,230
machines you've got there and let's run

00:09:06,110 --> 00:09:11,300
some apps on them you run Hadoop for a

00:09:09,230 --> 00:09:13,540
big nightly batch job like ETL

00:09:11,300 --> 00:09:16,520
processing of yesterday's stock results

00:09:13,540 --> 00:09:18,200
you run SPARC for interactive queries so

00:09:16,520 --> 00:09:20,720
your quants can do their analysis on the

00:09:18,200 --> 00:09:22,040
latest data and you run a rails app for

00:09:20,720 --> 00:09:24,620
the website that displays stock

00:09:22,040 --> 00:09:27,470
recommendations and lets investors buy

00:09:24,620 --> 00:09:29,750
and sell so you put Hadoop on nodes one

00:09:27,470 --> 00:09:32,060
through three and spark on nodes four

00:09:29,750 --> 00:09:35,420
through six and rails on nodes seven

00:09:32,060 --> 00:09:36,709
through nine but operating systems

00:09:35,420 --> 00:09:38,390
aren't supposed to make you specify

00:09:36,709 --> 00:09:40,730
exactly which resources to run your

00:09:38,390 --> 00:09:42,860
applications on what is your laptop ran

00:09:40,730 --> 00:09:44,930
like that asking you which CPU to use

00:09:42,860 --> 00:09:49,339
for each application you launch it's

00:09:44,930 --> 00:09:51,740
absurd furthermore static partitioning

00:09:49,339 --> 00:09:54,230
like this creates resource silos and

00:09:51,740 --> 00:09:55,610
idle resources in each silo are cut off

00:09:54,230 --> 00:09:58,640
from other applications that could make

00:09:55,610 --> 00:10:00,410
use of them so what if your hadoop your

00:09:58,640 --> 00:10:02,899
recurring hadoop job finishes early and

00:10:00,410 --> 00:10:04,610
has extra resources available what if

00:10:02,899 --> 00:10:05,750
nobody wants to run sharp queries in the

00:10:04,610 --> 00:10:08,240
middle of the night does the spark

00:10:05,750 --> 00:10:09,589
cluster just sit idle and why provision

00:10:08,240 --> 00:10:11,630
your web cluster large enough to handle

00:10:09,589 --> 00:10:14,209
a slash peak when you have hardly a

00:10:11,630 --> 00:10:15,589
tenth of that traffic daily didn't we

00:10:14,209 --> 00:10:19,190
learn as children we're supposed to

00:10:15,589 --> 00:10:21,410
share so maysa allows resource sharing

00:10:19,190 --> 00:10:24,680
between applications which increases

00:10:21,410 --> 00:10:26,839
both throughput and utilization maysa

00:10:24,680 --> 00:10:29,180
gives you better throughput because with

00:10:26,839 --> 00:10:30,890
all those previously idle resources you

00:10:29,180 --> 00:10:32,600
can scale out your Hadoop job to run

00:10:30,890 --> 00:10:35,029
more simultaneous mappers and reducers

00:10:32,600 --> 00:10:36,860
and complete the job faster you can even

00:10:35,029 --> 00:10:40,060
scale up your SPARC and rails clusters

00:10:36,860 --> 00:10:42,680
on demand you get better utilization to

00:10:40,060 --> 00:10:44,270
using multi-tenancy on maysa has more

00:10:42,680 --> 00:10:45,620
than doubled the utilization at

00:10:44,270 --> 00:10:47,209
companies with thousands of machines

00:10:45,620 --> 00:10:49,820
where percentages were previously

00:10:47,209 --> 00:10:51,380
running in the teens google claims to

00:10:49,820 --> 00:10:53,620
have saved an entire data center by

00:10:51,380 --> 00:10:55,930
using multi-tenancy with borg

00:10:53,620 --> 00:10:58,180
sister technology to May sews but this

00:10:55,930 --> 00:11:02,200
is still a lot of hand waving Nikolas

00:10:58,180 --> 00:11:04,660
can you give us some details your bet so

00:11:02,200 --> 00:11:07,000
this is the missus architecture and

00:11:04,660 --> 00:11:09,310
business consists of master and slave

00:11:07,000 --> 00:11:13,000
notes which assist applications in

00:11:09,310 --> 00:11:15,700
running tasks in a cluster so in missus

00:11:13,000 --> 00:11:19,660
we refer to applications as Meadows

00:11:15,700 --> 00:11:24,000
frameworks so in this example we have

00:11:19,660 --> 00:11:26,890
two frameworks running on this cluster

00:11:24,000 --> 00:11:30,250
Hadoop and marathon and they interact

00:11:26,890 --> 00:11:32,260
with the missus master we can have

00:11:30,250 --> 00:11:34,150
multiple masters in place for high

00:11:32,260 --> 00:11:38,020
availability and they coordinate leader

00:11:34,150 --> 00:11:40,060
election with zookeeper as you might

00:11:38,020 --> 00:11:42,640
guess tasks of the unit of execution

00:11:40,060 --> 00:11:44,830
within missus and the master schedules

00:11:42,640 --> 00:11:48,220
those tasks on the slaves available

00:11:44,830 --> 00:11:50,339
resources slaves use executors to

00:11:48,220 --> 00:11:52,660
coordinate the execution of tasks and

00:11:50,339 --> 00:11:55,000
each framework could define its own

00:11:52,660 --> 00:11:58,930
executor to specify how to handle those

00:11:55,000 --> 00:12:00,640
tasks well I can go in-depth on in every

00:11:58,930 --> 00:12:06,040
component here but I'll start with

00:12:00,640 --> 00:12:07,720
resource isolation on the slaves the

00:12:06,040 --> 00:12:09,880
business slave isolate executors and

00:12:07,720 --> 00:12:11,920
they're running tasks in lightweight

00:12:09,880 --> 00:12:15,089
containers using the next C groups is

00:12:11,920 --> 00:12:18,220
here marked with dot a lot dotted lines

00:12:15,089 --> 00:12:20,650
and it's in this figure we have an a

00:12:18,220 --> 00:12:23,670
hadoop test tracker executor and the

00:12:20,650 --> 00:12:26,470
message default executor running

00:12:23,670 --> 00:12:31,570
MapReduce tasks and a ruby script

00:12:26,470 --> 00:12:34,690
respectively containers can also grow

00:12:31,570 --> 00:12:37,209
and shrink as tasks run and they then

00:12:34,690 --> 00:12:40,600
complete so if we envision or imagine

00:12:37,209 --> 00:12:42,430
that the best executor completes and a

00:12:40,600 --> 00:12:44,830
third task come in to run on the Hadoop

00:12:42,430 --> 00:12:47,860
executor the container would be resized

00:12:44,830 --> 00:12:49,029
to consent to contain and to isolate the

00:12:47,860 --> 00:12:52,480
resources that were granted by the

00:12:49,029 --> 00:12:54,279
master getting a bit point to details

00:12:52,480 --> 00:12:56,860
about the resource isolation and how it

00:12:54,279 --> 00:13:00,580
works in vessels when you start a slave

00:12:56,860 --> 00:13:02,650
you specify a container riser and that

00:13:00,580 --> 00:13:05,050
includes a launcher which which defines

00:13:02,650 --> 00:13:07,000
how to launch a container and a set of

00:13:05,050 --> 00:13:07,720
Isolators that enforce resource

00:13:07,000 --> 00:13:10,990
constraints

00:13:07,720 --> 00:13:13,180
like CPU and memory here we are

00:13:10,990 --> 00:13:16,990
enforcing CPU shares and memory

00:13:13,180 --> 00:13:18,610
constraints with cgroups Isolators but

00:13:16,990 --> 00:13:21,070
missiles can actually track and isolate

00:13:18,610 --> 00:13:23,950
more resource types that are being

00:13:21,070 --> 00:13:26,920
enforced and that allows us to manage

00:13:23,950 --> 00:13:30,700
resources like IP addresses and ports in

00:13:26,920 --> 00:13:32,740
disk space GPUs if you want it and when

00:13:30,700 --> 00:13:34,810
isolate isolation mechanisms for these

00:13:32,740 --> 00:13:38,410
kind of resources becomes available we

00:13:34,810 --> 00:13:41,880
can easily plug them into my selves but

00:13:38,410 --> 00:13:44,080
you might be wondering what about docker

00:13:41,880 --> 00:13:47,440
container ization bottarga is already

00:13:44,080 --> 00:13:51,100
possible using the marathon framework

00:13:47,440 --> 00:13:52,810
for missiles but we're working on a way

00:13:51,100 --> 00:13:55,060
to make containerization much more

00:13:52,810 --> 00:13:57,390
flexible just like the darker or

00:13:55,060 --> 00:13:59,770
transparent to all missiles frameworks

00:13:57,390 --> 00:14:02,170
and that's something we call the

00:13:59,770 --> 00:14:04,360
external container Iser which provides

00:14:02,170 --> 00:14:08,020
like a plot plugging in interface for

00:14:04,360 --> 00:14:09,910
containerization so on a slave startup

00:14:08,020 --> 00:14:12,130
you specify an external container as a

00:14:09,910 --> 00:14:14,050
program or script that missiles will

00:14:12,130 --> 00:14:15,690
interact with as was an internal

00:14:14,050 --> 00:14:18,220
container Iser

00:14:15,690 --> 00:14:20,950
on top of the default images that you

00:14:18,220 --> 00:14:22,540
can specify on startup this book also

00:14:20,950 --> 00:14:25,780
includes extensions to how you define

00:14:22,540 --> 00:14:27,670
tasks so when you run a test you can

00:14:25,780 --> 00:14:30,940
actually pick and choose for example

00:14:27,670 --> 00:14:34,330
dogger images or KBM ISOs that is that

00:14:30,940 --> 00:14:35,800
that particular test should run in this

00:14:34,330 --> 00:14:37,180
is the Laporte in progress that's the

00:14:35,800 --> 00:14:40,120
caveat here but you're gonna actually

00:14:37,180 --> 00:14:41,230
already try it out we release the first

00:14:40,120 --> 00:14:43,150
external container Iser

00:14:41,230 --> 00:14:46,600
program out there for darker

00:14:43,150 --> 00:14:49,990
containerization and we call it demos we

00:14:46,600 --> 00:14:53,650
just named after the smallest moon

00:14:49,990 --> 00:14:55,180
that's around mars and you can find it

00:14:53,650 --> 00:14:56,230
on our github page and there should be

00:14:55,180 --> 00:15:00,430
instructions into how to get that

00:14:56,230 --> 00:15:02,860
running great so we've got a system with

00:15:00,430 --> 00:15:05,500
multitasking resource sharing and

00:15:02,860 --> 00:15:08,640
isolation and that's all well and good

00:15:05,500 --> 00:15:11,230
until something catches on fire

00:15:08,640 --> 00:15:14,830
so beaker knows that everything fails

00:15:11,230 --> 00:15:16,930
and so did my boss at Amazon one of our

00:15:14,830 --> 00:15:19,780
mantras at Amazon was everything fails

00:15:16,930 --> 00:15:21,070
all the time machines die or become

00:15:19,780 --> 00:15:23,470
unreachable due to hard

00:15:21,070 --> 00:15:25,180
where OS or network failures the

00:15:23,470 --> 00:15:27,220
different processes within maysa could

00:15:25,180 --> 00:15:29,470
die due to bugs or other unpredictable

00:15:27,220 --> 00:15:30,790
events or maybe the machine or a process

00:15:29,470 --> 00:15:32,860
just needs to be restarted for an

00:15:30,790 --> 00:15:35,370
upgrade you always have to have a

00:15:32,860 --> 00:15:38,290
recovery plan

00:15:35,370 --> 00:15:40,660
fortunately maysa was designed with no

00:15:38,290 --> 00:15:42,670
single point of failure this diagram

00:15:40,660 --> 00:15:44,230
highlights framework recovery the

00:15:42,670 --> 00:15:46,120
framework client at the top right went

00:15:44,230 --> 00:15:47,650
down but the master doesn't need to

00:15:46,120 --> 00:15:48,400
alert the tasks running on the slaves

00:15:47,650 --> 00:15:50,620
just yet

00:15:48,400 --> 00:15:52,630
the master keeps monitoring the tasks

00:15:50,620 --> 00:15:54,460
progress and waits for another framework

00:15:52,630 --> 00:15:56,440
to reconnect with the same framework ID

00:15:54,460 --> 00:15:58,210
this could be the same node that

00:15:56,440 --> 00:15:59,590
restarted its framework process or a

00:15:58,210 --> 00:16:02,350
completely different node that wants to

00:15:59,590 --> 00:16:04,360
take over for the failed node in either

00:16:02,350 --> 00:16:06,220
case the framework will rear edge astir

00:16:04,360 --> 00:16:07,930
and then the master will update the

00:16:06,220 --> 00:16:09,970
framework with any tasks that completed

00:16:07,930 --> 00:16:11,860
while it was gone and the framework can

00:16:09,970 --> 00:16:15,490
continue launching new tasks as if

00:16:11,860 --> 00:16:17,530
nothing happened even the master can

00:16:15,490 --> 00:16:20,050
fail over and the zookeeper quorum will

00:16:17,530 --> 00:16:22,000
just elect a new leader the newly

00:16:20,050 --> 00:16:24,130
elected master will review a replicated

00:16:22,000 --> 00:16:25,720
log called the registry to recover its

00:16:24,130 --> 00:16:27,520
state about what frameworks and slaves

00:16:25,720 --> 00:16:30,640
were connected and what tasks were

00:16:27,520 --> 00:16:32,830
running where since failover to a new

00:16:30,640 --> 00:16:34,480
master is almost instantaneous slaves

00:16:32,830 --> 00:16:36,340
keep running their tasks and both

00:16:34,480 --> 00:16:38,200
frameworks and slaves will detect the

00:16:36,340 --> 00:16:41,980
new master in zookeeper and re register

00:16:38,200 --> 00:16:44,620
with it and if the slave process has to

00:16:41,980 --> 00:16:46,720
be restarted for example for an upgrade

00:16:44,620 --> 00:16:48,790
we'd still want the tasks to continue

00:16:46,720 --> 00:16:51,970
running especially for stateful services

00:16:48,790 --> 00:16:53,260
like Redis and memcache and Mace those

00:16:51,970 --> 00:16:55,780
executives will do just that

00:16:53,260 --> 00:16:57,430
when the slave process restarts it loads

00:16:55,780 --> 00:17:00,430
its check pointed State to learn what

00:16:57,430 --> 00:17:02,200
pids to reconnect to for each task then

00:17:00,430 --> 00:17:05,170
it updates the tasks statuses and re

00:17:02,200 --> 00:17:08,410
registers with the master tasks continue

00:17:05,170 --> 00:17:10,870
running uninterrupted of course if the

00:17:08,410 --> 00:17:12,490
entire slave machine dies you'll lose

00:17:10,870 --> 00:17:15,430
any tasks that are running on the slave

00:17:12,490 --> 00:17:16,839
because the whole machines gone so we

00:17:15,430 --> 00:17:19,240
leave it up to the framework to respond

00:17:16,839 --> 00:17:20,920
to lost or failed tasks stateful

00:17:19,240 --> 00:17:22,630
frameworks like memcache or Redis would

00:17:20,920 --> 00:17:25,240
need to replicate their state in order

00:17:22,630 --> 00:17:26,949
to be tolerant of task failures but

00:17:25,240 --> 00:17:30,150
stateless frameworks like Hadoop can

00:17:26,949 --> 00:17:32,770
just relaunch the tasks on other slaves

00:17:30,150 --> 00:17:34,450
with all these benefits it's no wonder

00:17:32,770 --> 00:17:34,750
so many frameworks are being run and

00:17:34,450 --> 00:17:37,030
even

00:17:34,750 --> 00:17:39,850
built on top of maces you can run your

00:17:37,030 --> 00:17:42,400
Hadoop or MPD up MPI jobs do stream

00:17:39,850 --> 00:17:44,470
processing with storm run data services

00:17:42,400 --> 00:17:46,420
like Cassandra and elasticsearch use

00:17:44,470 --> 00:17:49,030
Jenkins for continuous integration or

00:17:46,420 --> 00:17:51,520
run standalone executables like go and

00:17:49,030 --> 00:17:55,000
rails apps docker containers or your own

00:17:51,520 --> 00:17:56,890
custom service and we built the Chronos

00:17:55,000 --> 00:18:00,880
and marathon frameworks on top of maces

00:17:56,890 --> 00:18:03,280
to make life with maces even easier you

00:18:00,880 --> 00:18:05,470
can think of Chronos as distributed cron

00:18:03,280 --> 00:18:08,050
with dependencies when you add a new

00:18:05,470 --> 00:18:10,510
Chronos job you specify a name command

00:18:08,050 --> 00:18:12,490
schedule the repetition frequency and

00:18:10,510 --> 00:18:15,910
any parent jobs so you can build complex

00:18:12,490 --> 00:18:18,640
workflows of scheduled tasks the Chronos

00:18:15,910 --> 00:18:20,380
UI shows the status of recent jobs at a

00:18:18,640 --> 00:18:22,390
glance so you can quickly spot failures

00:18:20,380 --> 00:18:26,890
and get warm fuzzies when everything's

00:18:22,390 --> 00:18:30,190
green you can use marathon for long

00:18:26,890 --> 00:18:32,560
running services get it marathon long

00:18:30,190 --> 00:18:34,900
running think of it like an it D for

00:18:32,560 --> 00:18:37,660
your data center or upstart or system D

00:18:34,900 --> 00:18:39,940
if you prefer just add your application

00:18:37,660 --> 00:18:42,700
with a name command the CPU and memory

00:18:39,940 --> 00:18:44,710
resources for a task for each task the

00:18:42,700 --> 00:18:46,810
number of instances and neur eyes you

00:18:44,710 --> 00:18:48,400
need for dependencies once it's running

00:18:46,810 --> 00:18:50,620
you can use marathon to suspend or

00:18:48,400 --> 00:18:53,680
destroy your service or scale it up or

00:18:50,620 --> 00:18:55,330
down so combined with Chronos you can

00:18:53,680 --> 00:18:57,460
schedule all your recurring batch jobs

00:18:55,330 --> 00:19:00,370
and keep your long-running services up

00:18:57,460 --> 00:19:02,850
and running all on Apache mesas for

00:19:00,370 --> 00:19:04,750
everything else build your own service

00:19:02,850 --> 00:19:07,330
build your own framework we have

00:19:04,750 --> 00:19:08,230
framework api's in C++ Java Python

00:19:07,330 --> 00:19:11,590
skaila

00:19:08,230 --> 00:19:14,800
Erlang closure go and anything else that

00:19:11,590 --> 00:19:16,390
can be taught to speak protobufs and you

00:19:14,800 --> 00:19:18,160
can see the number of maysa related

00:19:16,390 --> 00:19:22,510
projects on github is steadily growing

00:19:18,160 --> 00:19:24,580
and the next one could be yours but you

00:19:22,510 --> 00:19:28,390
probably want to know who's actually

00:19:24,580 --> 00:19:30,610
using my sauce in production all your

00:19:28,390 --> 00:19:33,280
favorite companies and more and don't

00:19:30,610 --> 00:19:35,920
ignore that logo at the bottom we use

00:19:33,280 --> 00:19:38,050
meso s-- marathon and Chronos internally

00:19:35,920 --> 00:19:40,030
at Mesa sphere for elastic meso s--

00:19:38,050 --> 00:19:42,550
which we showed at the beginning of the

00:19:40,030 --> 00:19:45,460
talk our spark analytics and other

00:19:42,550 --> 00:19:47,170
internal services so now we'll dive

00:19:45,460 --> 00:19:48,130
deeper into a few of the biggest users

00:19:47,170 --> 00:19:51,850
of meso s

00:19:48,130 --> 00:19:53,860
I'll start with HubSpot HubSpot develops

00:19:51,850 --> 00:19:56,289
a software-as-a-service

00:19:53,860 --> 00:19:58,690
product for inbound marketing with

00:19:56,289 --> 00:20:00,669
features for social media email content

00:19:58,690 --> 00:20:03,490
management web analytics and search

00:20:00,669 --> 00:20:05,200
engine optimization I'm not sure exactly

00:20:03,490 --> 00:20:08,890
what all that means but I do know that

00:20:05,200 --> 00:20:10,690
marketing means lots of data so HubSpot

00:20:08,890 --> 00:20:12,760
runs a mixture of web services long

00:20:10,690 --> 00:20:15,610
running processes one-off tasks and

00:20:12,760 --> 00:20:17,230
scheduled jobs all on top of mesas they

00:20:15,610 --> 00:20:19,000
have over a hundred and fifty services

00:20:17,230 --> 00:20:22,450
running inside May sews on hundreds of

00:20:19,000 --> 00:20:24,520
servers inside Amazon ec2 according to

00:20:22,450 --> 00:20:27,820
the VP of engineering Elias Torres

00:20:24,520 --> 00:20:29,650
HubSpot deploys 300 times a day on a

00:20:27,820 --> 00:20:33,070
minimal number of servers by using

00:20:29,650 --> 00:20:35,530
Apache mesas they've reported numerous

00:20:33,070 --> 00:20:37,090
benefits from using mesas they get

00:20:35,530 --> 00:20:38,770
reduced time to deploy and reduce

00:20:37,090 --> 00:20:40,299
developer friction because developers

00:20:38,770 --> 00:20:41,919
get immediate access to cluster

00:20:40,299 --> 00:20:44,679
resources whether for scaling or

00:20:41,919 --> 00:20:46,270
introducing new services and developers

00:20:44,679 --> 00:20:48,070
no longer need to understand the process

00:20:46,270 --> 00:20:51,370
for requisitioning requisitioning

00:20:48,070 --> 00:20:53,950
hardware or servers they get increased

00:20:51,370 --> 00:20:56,320
reliability because hardware failures

00:20:53,950 --> 00:20:58,690
are more transparent to developers as

00:20:56,320 --> 00:21:01,000
services are automatically replaced when

00:20:58,690 --> 00:21:02,500
tasks are lost in other words developers

00:21:01,000 --> 00:21:05,559
are no longer paged because of a simple

00:21:02,500 --> 00:21:07,570
hardware failure and scheduled tasks are

00:21:05,559 --> 00:21:09,159
not tied to a single service server

00:21:07,570 --> 00:21:11,919
which could fail at any time taking the

00:21:09,159 --> 00:21:13,750
cron job with it and resource

00:21:11,919 --> 00:21:16,600
utilization is improved which directly

00:21:13,750 --> 00:21:18,010
corresponds to reduce costs services

00:21:16,600 --> 00:21:20,350
which were previously running on over

00:21:18,010 --> 00:21:23,710
provisioned hardware now use the exact

00:21:20,350 --> 00:21:27,340
amount of resources required Niklas you

00:21:23,710 --> 00:21:28,570
take us through the next company yep so

00:21:27,340 --> 00:21:31,179
most of you guys are probably familiar

00:21:28,570 --> 00:21:33,940
with a B&B but for those of you who

00:21:31,179 --> 00:21:35,740
aren't it's a very successful startup in

00:21:33,940 --> 00:21:38,799
San Francisco that makes it super easy

00:21:35,740 --> 00:21:42,039
to share your apartment or to be a guest

00:21:38,799 --> 00:21:43,720
in another person's home but what you

00:21:42,039 --> 00:21:45,520
might not realize is that Airbnb is a

00:21:43,720 --> 00:21:47,440
very data-driven company and they

00:21:45,520 --> 00:21:49,900
actually pass process petabytes of data

00:21:47,440 --> 00:21:53,830
and they do all of that on top of

00:21:49,900 --> 00:21:56,710
missiles so according to the vice

00:21:53,830 --> 00:21:58,179
president of engineering Mike Curtis the

00:21:56,710 --> 00:21:59,710
idea is to make it so that smaller

00:21:58,179 --> 00:22:01,309
number of engineers can have a higher

00:21:59,710 --> 00:22:04,909
impact through automation

00:22:01,309 --> 00:22:08,289
mesos to capture some of the high-level

00:22:04,909 --> 00:22:10,249
benefits that they've had with Miso's

00:22:08,289 --> 00:22:12,379
they're now running multi tenant

00:22:10,249 --> 00:22:15,889
clusters with Hadoop coexisting with

00:22:12,379 --> 00:22:18,950
Cronus part and storm now note no longer

00:22:15,889 --> 00:22:20,299
needing independent Hadoop clusters it

00:22:18,950 --> 00:22:22,730
enables all our teams to build

00:22:20,299 --> 00:22:24,499
distributed systems much faster and

00:22:22,730 --> 00:22:27,950
they're running all of this on top of

00:22:24,499 --> 00:22:30,259
Amazon ec2 instances we want to thank

00:22:27,950 --> 00:22:32,299
Brendan Brendan Matthews from from a B&B

00:22:30,259 --> 00:22:34,940
from sharing the data about their

00:22:32,299 --> 00:22:37,850
message clusters but he asked us not to

00:22:34,940 --> 00:22:40,039
share any figures so that's the caveat

00:22:37,850 --> 00:22:42,259
with this infrastructure stack this is a

00:22:40,039 --> 00:22:45,830
stack that we put together from the data

00:22:42,259 --> 00:22:48,139
that we got from from from Brendan so

00:22:45,830 --> 00:22:50,869
most notably is that we actually have

00:22:48,139 --> 00:22:55,279
multiple Hadoop clusters running hello

00:22:50,869 --> 00:22:57,799
alongside SPARC and presto and Redis and

00:22:55,279 --> 00:23:00,499
rails and much more on top of marathon

00:22:57,799 --> 00:23:03,409
and crona's and they used those clusters

00:23:00,499 --> 00:23:06,740
to build search indices to reorder

00:23:03,409 --> 00:23:08,990
search rankings for their rental pricing

00:23:06,740 --> 00:23:13,190
suggestion systems trust and safety if

00:23:08,990 --> 00:23:15,830
fraud detection systems and now to the

00:23:13,190 --> 00:23:18,440
Polly or not only probably about the

00:23:15,830 --> 00:23:22,070
biggest user and contributor to mesos

00:23:18,440 --> 00:23:25,039
which is Twitter you probably all you

00:23:22,070 --> 00:23:29,029
know what Twitter is but don't realize

00:23:25,039 --> 00:23:32,019
the scale of their operation they have

00:23:29,029 --> 00:23:34,789
200 240 million monthly active users

00:23:32,019 --> 00:23:37,970
processes puzzling 500 million tweets

00:23:34,789 --> 00:23:40,279
per day up to 150 thousand tweets per

00:23:37,970 --> 00:23:43,820
second which results in hundreds of

00:23:40,279 --> 00:23:47,480
terabytes per day of compressed data but

00:23:43,820 --> 00:23:48,740
how does missives tie in to this as the

00:23:47,480 --> 00:23:50,990
senior vice president of engineering

00:23:48,740 --> 00:23:53,360
explain like missus it's a credit it's

00:23:50,990 --> 00:23:55,309
critical for Twitter's continued success

00:23:53,360 --> 00:23:59,600
at scale and that's how they build new

00:23:55,309 --> 00:24:01,580
services it's allowed them to scale to

00:23:59,600 --> 00:24:04,909
thousands of bare-metal machines and

00:24:01,580 --> 00:24:07,789
leverage a share pool of servers across

00:24:04,909 --> 00:24:09,860
datacenters and Twitter is in fact that

00:24:07,789 --> 00:24:12,669
running the largest gnome production

00:24:09,860 --> 00:24:12,669
business cluster

00:24:12,680 --> 00:24:17,090
and as Chris said mezzos has transformed

00:24:15,830 --> 00:24:19,730
the way that developers think about

00:24:17,090 --> 00:24:20,930
launching new services at Twitter so

00:24:19,730 --> 00:24:22,340
instead of thinking about static

00:24:20,930 --> 00:24:25,720
machines engineers are now thinking

00:24:22,340 --> 00:24:29,120
about resources like CPU memory and disk

00:24:25,720 --> 00:24:31,310
and just like every amis infrastructure

00:24:29,120 --> 00:24:32,840
stack this is with a big caveat this is

00:24:31,310 --> 00:24:35,030
what we've been put together from the

00:24:32,840 --> 00:24:39,290
publicly available data that's out there

00:24:35,030 --> 00:24:41,480
but as you can see servo key Twitter

00:24:39,290 --> 00:24:44,660
services is running on Rizzo's which is

00:24:41,480 --> 00:24:48,530
analytics time ahead and ads and they

00:24:44,660 --> 00:24:50,540
run on premise but what you can also see

00:24:48,530 --> 00:24:52,310
is that not all services are running on

00:24:50,540 --> 00:24:54,980
missiles and so some are still running

00:24:52,310 --> 00:24:56,990
on dedicated hardware but the services

00:24:54,980 --> 00:25:00,470
that do run on missiles run on their

00:24:56,990 --> 00:25:01,910
Aurora framework which was recently open

00:25:00,470 --> 00:25:06,680
sourced and I believe there is a talk

00:25:01,910 --> 00:25:07,820
going on in the room next door so that

00:25:06,680 --> 00:25:09,860
was just a few of the companies that

00:25:07,820 --> 00:25:13,040
relies on missiles as a part of their

00:25:09,860 --> 00:25:17,180
infrastructure and that Browns our final

00:25:13,040 --> 00:25:19,580
section of the talk so we told you about

00:25:17,180 --> 00:25:21,860
our story weeks explaining the missiles

00:25:19,580 --> 00:25:24,200
approach and we gave some example of big

00:25:21,860 --> 00:25:27,110
companies that are running and reaping

00:25:24,200 --> 00:25:28,370
big awards by using reserves like

00:25:27,110 --> 00:25:31,850
increased cluster utilization and

00:25:28,370 --> 00:25:34,940
reliability so this is the final

00:25:31,850 --> 00:25:38,090
outreach missis's of results as many

00:25:34,940 --> 00:25:39,800
other open source projects as a result

00:25:38,090 --> 00:25:42,170
of its community and we welcome new

00:25:39,800 --> 00:25:45,470
contributors so if you're interested

00:25:42,170 --> 00:25:49,820
come and talk to us or been during the

00:25:45,470 --> 00:25:51,620
conference join our mailing list if

00:25:49,820 --> 00:25:56,000
you're interested in the development of

00:25:51,620 --> 00:25:57,680
missiles or if you want to contribute go

00:25:56,000 --> 00:26:01,130
to the missiles Apache misses website

00:25:57,680 --> 00:26:02,480
and find us on github and if if you're

00:26:01,130 --> 00:26:05,000
impatient are getting missiles up and

00:26:02,480 --> 00:26:07,160
running we had message for your actually

00:26:05,000 --> 00:26:09,410
hosts prepackaged message builds for

00:26:07,160 --> 00:26:15,320
both Linux and Mac atmosphere at i/o

00:26:09,410 --> 00:26:17,540
/downloads so I don't know if anybody of

00:26:15,320 --> 00:26:19,940
you guys started a elastic message

00:26:17,540 --> 00:26:24,740
cluster when you when you started but

00:26:19,940 --> 00:26:26,779
anyhow if you intend to do it later you

00:26:24,740 --> 00:26:29,389
will get an email

00:26:26,779 --> 00:26:32,959
when because has been provisioned with

00:26:29,389 --> 00:26:35,119
tons of personalized tutorials I will

00:26:32,959 --> 00:26:37,820
show you how to get up to speed running

00:26:35,119 --> 00:26:40,459
spark running storm Hadoop Aurora and

00:26:37,820 --> 00:26:45,139
much more and that's on mesosphere io

00:26:40,459 --> 00:26:52,690
lifeline so uh yeah with that being said

00:26:45,139 --> 00:26:52,690
I think it's time for our Q&A questions

00:27:10,990 --> 00:27:17,419
that there is a yeah I'll repeat it is

00:27:16,070 --> 00:27:21,860
there a single point where you can

00:27:17,419 --> 00:27:23,840
manage your cluster there is business

00:27:21,860 --> 00:27:28,250
has a built-in web maybe you are web UI

00:27:23,840 --> 00:27:33,230
so if you go to the to the master IP

00:27:28,250 --> 00:27:34,850
import they'll be in the the email the

00:27:33,230 --> 00:27:36,200
web your I would would be accessible for

00:27:34,850 --> 00:27:37,490
you so you can see all the running

00:27:36,200 --> 00:27:40,519
frameworks you can see all your slaves

00:27:37,490 --> 00:27:42,620
and tasks and that's a great tool to

00:27:40,519 --> 00:27:45,110
anticipate when things go wrong because

00:27:42,620 --> 00:27:47,870
then you can go to the sandbox on in at

00:27:45,110 --> 00:27:49,039
any slave for any job and see like the

00:27:47,870 --> 00:27:52,940
standard out and standard error for that

00:27:49,039 --> 00:27:54,559
particular task right and I mean you can

00:27:52,940 --> 00:27:57,590
also use the marathon and Chronos see

00:27:54,559 --> 00:28:00,139
you eyes for your long running services

00:27:57,590 --> 00:28:03,620
and batch jobs find that a lot of our

00:28:00,139 --> 00:28:05,120
customers actually the users stay in

00:28:03,620 --> 00:28:08,899
those you eyes and only the sysadmin

00:28:05,120 --> 00:28:11,570
digs into the actual meso cui there's

00:28:08,899 --> 00:28:14,029
also command-line restful interfaces as

00:28:11,570 --> 00:28:17,299
well so if you want to build your own UI

00:28:14,029 --> 00:28:20,440
you can just grab the statistics and

00:28:17,299 --> 00:28:23,740
information from our restful api and and

00:28:20,440 --> 00:28:23,740
go from there

00:28:33,890 --> 00:28:40,130
no the so we use the question is when

00:28:37,850 --> 00:28:41,390
you provision a mesas cluster do you

00:28:40,130 --> 00:28:45,410
expect these services are already

00:28:41,390 --> 00:28:47,809
running the answer is no they could be

00:28:45,410 --> 00:28:51,020
but then they're not necessarily being

00:28:47,809 --> 00:28:52,640
managed by meso s-- ideally you're just

00:28:51,020 --> 00:28:54,679
running a meso slave on each of these

00:28:52,640 --> 00:28:58,970
nodes and a couple of my sauce masters

00:28:54,679 --> 00:29:03,710
and you'll host the executor Zin HDFS or

00:28:58,970 --> 00:29:06,410
somewhere could be FTP HTTP wherever and

00:29:03,710 --> 00:29:09,380
when a framework connects to the master

00:29:06,410 --> 00:29:11,750
it tells the master to you know pull

00:29:09,380 --> 00:29:13,370
down the the master will then tell the

00:29:11,750 --> 00:29:16,160
slaves to pull down the executor and

00:29:13,370 --> 00:29:18,860
launch the Hadoop task tracker or your

00:29:16,160 --> 00:29:20,120
rails app or whatever on those slaves so

00:29:18,860 --> 00:29:22,309
you don't have to like manage

00:29:20,120 --> 00:29:24,140
distributing and deploying all of these

00:29:22,309 --> 00:29:25,549
applications yourself and with the

00:29:24,140 --> 00:29:28,330
docker integration you can just have a

00:29:25,549 --> 00:29:31,309
docker registry that's ready to run your

00:29:28,330 --> 00:29:33,230
services right there yeah it's actually

00:29:31,309 --> 00:29:35,059
a part of the tasks task description

00:29:33,230 --> 00:29:37,669
then you can give like a set of you are

00:29:35,059 --> 00:29:44,500
eyes that contains tarballs of how to

00:29:37,669 --> 00:29:44,500
run your job other questions yes

00:29:58,580 --> 00:30:04,100
yeah I think that you could get a lot of

00:30:01,910 --> 00:30:07,760
let's let me repeat the question

00:30:04,100 --> 00:30:12,950
yeah how do we accomplish fine-grained

00:30:07,760 --> 00:30:14,180
resource isolation scheduling okay great

00:30:12,950 --> 00:30:15,920
that's that's great question because

00:30:14,180 --> 00:30:17,870
that's actually something that we did

00:30:15,920 --> 00:30:20,230
not talk about at all is how does it

00:30:17,870 --> 00:30:22,580
look how our allocations being done and

00:30:20,230 --> 00:30:24,380
then you can get a lot more context but

00:30:22,580 --> 00:30:29,450
also going to to Ben's talk later on

00:30:24,380 --> 00:30:31,990
today but we use an alpha model and see

00:30:29,450 --> 00:30:35,360
if I can find my supporting slide here

00:30:31,990 --> 00:30:38,840
so if your imagine that this is the

00:30:35,360 --> 00:30:41,770
master this is your framework you the

00:30:38,840 --> 00:30:43,640
master would offer the framework

00:30:41,770 --> 00:30:46,850
available resources at a particular

00:30:43,640 --> 00:30:48,920
slave the framework can then either

00:30:46,850 --> 00:30:51,770
decline it it looks at it and saying

00:30:48,920 --> 00:30:53,510
that's not enough or choose to saying

00:30:51,770 --> 00:30:56,120
that I want to run something on those

00:30:53,510 --> 00:30:59,150
resources but you don't need to use the

00:30:56,120 --> 00:31:00,980
entire offer let's say you got four CPUs

00:30:59,150 --> 00:31:03,860
and eight gigs of memory you can use a

00:31:00,980 --> 00:31:07,280
fraction of it and the other resources

00:31:03,860 --> 00:31:08,810
would be reoffer to another slave to

00:31:07,280 --> 00:31:10,880
another to another framework to another

00:31:08,810 --> 00:31:13,580
framework that's right so in that sense

00:31:10,880 --> 00:31:16,160
you can pick and choose and the

00:31:13,580 --> 00:31:18,170
resources that you chose would get go

00:31:16,160 --> 00:31:20,060
all the way down to the container Iser

00:31:18,170 --> 00:31:23,060
which would enforce it so it could be

00:31:20,060 --> 00:31:29,180
like a fraction of a CPU 100 makes 32

00:31:23,060 --> 00:31:31,730
mix or the entire the entire offer I

00:31:29,180 --> 00:31:33,880
don't know who was first and carrots

00:31:31,730 --> 00:31:33,880
first

00:31:55,310 --> 00:32:00,110
oh okay

00:31:58,340 --> 00:32:03,950
so there's a question whether or not

00:32:00,110 --> 00:32:08,030
mezzos itself would scale according to

00:32:03,950 --> 00:32:10,160
to higher load the answer is no at the

00:32:08,030 --> 00:32:11,750
moment auto scaling is something that I

00:32:10,160 --> 00:32:13,760
think would fit better in a framework

00:32:11,750 --> 00:32:18,140
that's running on top that can provision

00:32:13,760 --> 00:32:20,390
new slaves that attach to a master and

00:32:18,140 --> 00:32:21,740
as soon as a new slave registers with

00:32:20,390 --> 00:32:23,390
the master it's offering up its

00:32:21,740 --> 00:32:25,280
resources and then the master will offer

00:32:23,390 --> 00:32:26,990
it to whatever frameworks are available

00:32:25,280 --> 00:32:29,660
so all you have to do is start new

00:32:26,990 --> 00:32:32,230
slaves and you know you'll automatically

00:32:29,660 --> 00:32:32,230
scale out

00:32:40,920 --> 00:32:48,100
good question so the question was like

00:32:44,110 --> 00:32:50,980
how does Hadoop and/or how does Mesa and

00:32:48,100 --> 00:32:54,610
Hadoop interact with respect to yarn we

00:32:50,980 --> 00:33:00,100
don't have the Mesa arn integration yet

00:32:54,610 --> 00:33:02,800
but so like Airbnb is using Hadoop 1 so

00:33:00,100 --> 00:33:05,590
you can think of the job tracker as the

00:33:02,800 --> 00:33:08,950
framework and then the task tracker is

00:33:05,590 --> 00:33:11,800
the executor and then we actually wrap

00:33:08,950 --> 00:33:13,600
the you know whichever scheduler you

00:33:11,800 --> 00:33:17,200
choose fair scheduler capacity schedule

00:33:13,600 --> 00:33:22,090
we wrap that with a meso scheduler and

00:33:17,200 --> 00:33:24,910
that way mesas will be providing these

00:33:22,090 --> 00:33:26,830
resource offers and then Hadoop can

00:33:24,910 --> 00:33:29,460
figure out how to distribute that among

00:33:26,830 --> 00:33:29,460
the different jobs

00:33:47,650 --> 00:33:53,210
so the question was about dynamically

00:33:51,110 --> 00:34:02,420
resizing containers specifically with

00:33:53,210 --> 00:34:03,770
Hadoop okay I haven't dug into that too

00:34:02,420 --> 00:34:06,680
much in detail but from what I

00:34:03,770 --> 00:34:08,900
understand I mean the resizing is really

00:34:06,680 --> 00:34:12,140
just about what resources are available

00:34:08,900 --> 00:34:16,090
to a particular executor to launch its

00:34:12,140 --> 00:34:18,860
tasks so in this case a task tracker I

00:34:16,090 --> 00:34:23,230
mean I suppose you can I think you can

00:34:18,860 --> 00:34:23,230
update the number of slots dynamically

00:34:29,770 --> 00:34:39,410
you could sure that they're different

00:34:36,680 --> 00:34:41,480
resource types is only a matter it's not

00:34:39,410 --> 00:34:43,370
only a matter of let's say an integer be

00:34:41,480 --> 00:34:45,500
like a number of CPUs it'll also be a

00:34:43,370 --> 00:34:46,730
set right where you know you would have

00:34:45,500 --> 00:34:50,810
every particular slot so you would know

00:34:46,730 --> 00:34:54,890
which slot or you could have ranges as

00:34:50,810 --> 00:34:56,360
you do when you do or ports these pits

00:34:54,890 --> 00:34:59,420
so they're different resource types

00:34:56,360 --> 00:35:01,490
where one of them could fit that

00:34:59,420 --> 00:35:04,370
particular use case when when you start

00:35:01,490 --> 00:35:06,440
a slave you you can you can add those

00:35:04,370 --> 00:35:08,510
kind of extra attributes for example to

00:35:06,440 --> 00:35:12,740
say well this this machine in this box

00:35:08,510 --> 00:35:14,060
has four extra GB GPUs or it's a box

00:35:12,740 --> 00:35:17,510
that has this particular version of the

00:35:14,060 --> 00:35:18,440
kernel or if you only wanted to make if

00:35:17,510 --> 00:35:21,320
you wanted to make sure that you were

00:35:18,440 --> 00:35:23,060
only running one Cassandra service on

00:35:21,320 --> 00:35:26,030
each node and never more than that you

00:35:23,060 --> 00:35:28,220
can make make up some dummy resource

00:35:26,030 --> 00:35:30,350
that's called a Cassandra slot there's

00:35:28,220 --> 00:35:32,840
only one of those on each node and once

00:35:30,350 --> 00:35:34,340
you know we've given you you know the

00:35:32,840 --> 00:35:36,170
master will offer a framework the

00:35:34,340 --> 00:35:37,850
Cassandra framework hey here's a node

00:35:36,170 --> 00:35:40,070
with one Cassandra slot and then the

00:35:37,850 --> 00:35:42,170
Cassandra framework will launch one

00:35:40,070 --> 00:35:43,910
there and then we offer you another set

00:35:42,170 --> 00:35:45,410
of resources on that node and we say we

00:35:43,910 --> 00:35:46,970
don't have any Cassandra slots though

00:35:45,410 --> 00:35:50,120
and then the Cassandra framework can

00:35:46,970 --> 00:35:52,580
just ignore it so maysa is a two level

00:35:50,120 --> 00:35:54,980
scheduler so we try to keep the core

00:35:52,580 --> 00:35:57,170
master scheduler pretty thin and simple

00:35:54,980 --> 00:36:00,070
and rely on the frameworks to handle

00:35:57,170 --> 00:36:00,070
their own custom behavior

00:36:10,680 --> 00:36:16,270
good question so the question was if in

00:36:13,870 --> 00:36:17,980
order to keep my long-running process is

00:36:16,270 --> 00:36:22,270
running do I need to keep the scheduler

00:36:17,980 --> 00:36:22,780
itself running the answer is no not

00:36:22,270 --> 00:36:24,940
really

00:36:22,780 --> 00:36:27,280
the frameworks once the framework

00:36:24,940 --> 00:36:28,900
launches each of these tasks they'll

00:36:27,280 --> 00:36:30,760
just keep running even if the framework

00:36:28,900 --> 00:36:32,950
disconnects you could launch the

00:36:30,760 --> 00:36:35,080
framework inside the mesas cluster if

00:36:32,950 --> 00:36:37,840
you're using marathon for example then

00:36:35,080 --> 00:36:41,440
the marathon framework is running and it

00:36:37,840 --> 00:36:43,930
will launch these frameworks as you know

00:36:41,440 --> 00:36:45,340
containers within maces and those

00:36:43,930 --> 00:36:47,320
containers can manage the long-running

00:36:45,340 --> 00:36:49,930
services and marathon will make sure if

00:36:47,320 --> 00:36:51,250
the framework if your framework dies it

00:36:49,930 --> 00:36:53,490
will make sure that it gets relaunched

00:36:51,250 --> 00:36:56,110
somewhere else I also want to clarify

00:36:53,490 --> 00:36:57,970
let's say if you are running you're

00:36:56,110 --> 00:36:59,320
building your own framework then you

00:36:57,970 --> 00:37:01,960
will need to think about high

00:36:59,320 --> 00:37:03,640
availability of that framework and a

00:37:01,960 --> 00:37:06,370
framework like Mountain does that it

00:37:03,640 --> 00:37:08,650
would use the same zookeeper cluster to

00:37:06,370 --> 00:37:12,970
do coordination of multiple marathon

00:37:08,650 --> 00:37:15,370
instances being running but a BMV uses

00:37:12,970 --> 00:37:17,350
the the use case that Adam just said

00:37:15,370 --> 00:37:19,090
where the framework processes are

00:37:17,350 --> 00:37:23,590
actually being run by marathon that then

00:37:19,090 --> 00:37:25,770
connects to the same master question in

00:37:23,590 --> 00:37:25,770
the back

00:37:37,880 --> 00:37:46,350
okay good question two questions really

00:37:40,740 --> 00:37:47,520
and so the first one wait sorry I

00:37:46,350 --> 00:37:53,550
blinked thinking about the second

00:37:47,520 --> 00:37:54,840
question okay right the minimum number

00:37:53,550 --> 00:37:59,400
of compute nodes were and start to make

00:37:54,840 --> 00:38:02,090
sense um well it's been a a frequently

00:37:59,400 --> 00:38:05,430
asked question yeah and the the smallest

00:38:02,090 --> 00:38:07,890
cluster that we run is six notes because

00:38:05,430 --> 00:38:09,690
we want to have three notes for for our

00:38:07,890 --> 00:38:11,760
masters to have highly available masters

00:38:09,690 --> 00:38:13,770
and then from that point on you can have

00:38:11,760 --> 00:38:15,870
an arbitrary number of slaves you can

00:38:13,770 --> 00:38:18,450
start from oh say it makes sense for

00:38:15,870 --> 00:38:21,210
like the fourth full slave to the 5th to

00:38:18,450 --> 00:38:24,150
the 6th not only for for increased

00:38:21,210 --> 00:38:26,100
utilization but for reliability yeah and

00:38:24,150 --> 00:38:27,150
and some of it just depends on what

00:38:26,100 --> 00:38:29,400
you're trying to do with your mesas

00:38:27,150 --> 00:38:30,060
cluster because you know if you don't

00:38:29,400 --> 00:38:33,600
mind

00:38:30,060 --> 00:38:36,300
manually SS aging into 4 compute nodes

00:38:33,600 --> 00:38:38,130
and starting your services and then you

00:38:36,300 --> 00:38:40,620
don't really need a Mesa AZ cluster but

00:38:38,130 --> 00:38:41,880
I mean it does provide isolate

00:38:40,620 --> 00:38:43,800
especially if you're only running one

00:38:41,880 --> 00:38:44,730
service but if you want the isolation

00:38:43,800 --> 00:38:47,640
because you're running two different

00:38:44,730 --> 00:38:50,940
frameworks mesas helps with that if you

00:38:47,640 --> 00:38:53,640
want the EZ scaling you know mesas helps

00:38:50,940 --> 00:38:58,770
you go from three nodes to a thousand

00:38:53,640 --> 00:39:03,780
nodes the second question was the barrel

00:38:58,770 --> 00:39:06,060
up they weren't the only thing that is

00:39:03,780 --> 00:39:10,290
running that's important to run it's a

00:39:06,060 --> 00:39:12,000
mess of slave process and when a job is

00:39:10,290 --> 00:39:13,760
being started or an executive even

00:39:12,000 --> 00:39:16,620
started that is starting underneath

00:39:13,760 --> 00:39:19,920
underneath that slave process and that's

00:39:16,620 --> 00:39:20,910
being isolated with seat groups does

00:39:19,920 --> 00:39:22,650
that make sense the only thing you need

00:39:20,910 --> 00:39:24,480
to have running on a computer note it's

00:39:22,650 --> 00:39:28,230
a state process and and you can run that

00:39:24,480 --> 00:39:30,590
on bare metal or or on VMs it doesn't

00:39:28,230 --> 00:39:30,590
really matter

00:39:33,050 --> 00:39:39,630
yes it uses all it uses cgroups to

00:39:36,270 --> 00:39:42,600
handle all that which is built into

00:39:39,630 --> 00:39:45,210
Linux you can run all this on Mac OS 2

00:39:42,600 --> 00:39:54,240
but you don't get all the cgroups

00:39:45,210 --> 00:39:56,340
isolation no yes well it's not a part of

00:39:54,240 --> 00:40:02,550
miss or a part of Linux is running on

00:39:56,340 --> 00:40:05,430
top right oh yeah but yes we're not the

00:40:02,550 --> 00:40:09,750
on node operating system right you would

00:40:05,430 --> 00:40:12,810
run Linux like a bun to Red Hat whatever

00:40:09,750 --> 00:40:14,760
you want as long as it's modern enough

00:40:12,810 --> 00:40:17,400
to support cgroups so you're running

00:40:14,760 --> 00:40:20,100
some linux on each of your servers or

00:40:17,400 --> 00:40:22,590
each of your VMs and then you start a

00:40:20,100 --> 00:40:24,870
meso slave process within that and then

00:40:22,590 --> 00:40:27,090
that manages talking to the master nodes

00:40:24,870 --> 00:40:30,690
which are also running on some Linux box

00:40:27,090 --> 00:40:33,560
somewhere and starting processes and

00:40:30,690 --> 00:40:33,560
running all the tasks

00:40:40,410 --> 00:40:46,660
true you can run this on cloud stack you

00:40:43,630 --> 00:40:49,500
can run it on ec2 you can run it on any

00:40:46,660 --> 00:40:52,540
any virtualized system or on-premise

00:40:49,500 --> 00:40:56,530
hard bare metal hardware so this quiz

00:40:52,540 --> 00:41:07,480
sir one last question okay we'll see if

00:40:56,530 --> 00:41:09,940
we can do - I mean Twitter is running it

00:41:07,480 --> 00:41:13,380
on thousands of nodes

00:41:09,940 --> 00:41:16,570
I can't specify how many thousands but I

00:41:13,380 --> 00:41:20,070
we have not run into scalability issues

00:41:16,570 --> 00:41:20,070
at that scale what

00:41:32,990 --> 00:41:37,970
absolutely the question is whether it's

00:41:36,200 --> 00:41:40,730
good for just bringing up long-running

00:41:37,970 --> 00:41:43,550
services like Cassandra marathon is

00:41:40,730 --> 00:41:45,890
built specifically for that you you know

00:41:43,550 --> 00:41:47,720
you only need to have let's say you only

00:41:45,890 --> 00:41:50,330
need to have one Cassandra service

00:41:47,720 --> 00:41:51,560
running you can you know use marathon to

00:41:50,330 --> 00:41:54,080
make sure that there's always one

00:41:51,560 --> 00:41:56,869
running and Mase's will just

00:41:54,080 --> 00:41:58,910
automatically launch it on one of the

00:41:56,869 --> 00:42:02,180
slave nodes you don't even have to care

00:41:58,910 --> 00:42:08,530
which slave node and we'll just manage

00:42:02,180 --> 00:42:08,530
that for you all right one more

00:42:19,049 --> 00:42:23,470
it does and it's it's a high-frequency

00:42:21,930 --> 00:42:26,739
API between

00:42:23,470 --> 00:42:32,710
mezzos master and the framework so for

00:42:26,739 --> 00:42:35,650
every task so for interactive requests

00:42:32,710 --> 00:42:39,430
so if you if you have these services

00:42:35,650 --> 00:42:42,249
already running then the framework can

00:42:39,430 --> 00:42:44,739
talk directly to the task so you don't

00:42:42,249 --> 00:42:48,999
the resource offers is just to get the

00:42:44,739 --> 00:42:51,160
initial services up and running or for

00:42:48,999 --> 00:42:55,319
batch jobs it's to run the actual batch

00:42:51,160 --> 00:42:57,940
jobs but for interactive services it's

00:42:55,319 --> 00:42:59,200
you usually have the tasks already

00:42:57,940 --> 00:43:01,779
running and you would have the framework

00:42:59,200 --> 00:43:04,869
as a client that just connects to the

00:43:01,779 --> 00:43:08,170
task for some other things like spark

00:43:04,869 --> 00:43:10,569
queries it may be that you do just want

00:43:08,170 --> 00:43:13,059
to run these really fast interactive

00:43:10,569 --> 00:43:15,279
queries that where you'll be launching

00:43:13,059 --> 00:43:17,559
tasks on the fly and may so this is very

00:43:15,279 --> 00:43:21,009
lightweight it's a very fast

00:43:17,559 --> 00:43:23,559
offer mechanism so we can go about as

00:43:21,009 --> 00:43:26,039
fast as you know a few networked

00:43:23,559 --> 00:43:26,039
messages

00:43:28,300 --> 00:43:40,590
ah very good question was about security

00:43:37,360 --> 00:43:44,100
in May so's so right now we have a

00:43:40,590 --> 00:43:46,540
framework and soon slave authentication

00:43:44,100 --> 00:43:49,530
so that you can make sure that the

00:43:46,540 --> 00:43:51,820
framework know the that only

00:43:49,530 --> 00:43:53,950
authenticated frameworks only approved

00:43:51,820 --> 00:43:56,860
frameworks register so nobody's gonna be

00:43:53,950 --> 00:43:59,380
running Bitcoin mining on your mesas

00:43:56,860 --> 00:44:01,900
cluster unless you want to let them or

00:43:59,380 --> 00:44:03,790
that nobody's going to you know have

00:44:01,900 --> 00:44:05,770
their laptop attached to your mesas

00:44:03,790 --> 00:44:08,680
cluster as a slave and start stealing

00:44:05,770 --> 00:44:11,640
tasks so we've got that right now we're

00:44:08,680 --> 00:44:16,230
looking into we're actively working on

00:44:11,640 --> 00:44:20,050
authorization and Kerberos and

00:44:16,230 --> 00:44:21,760
encryption yeah SSL encryption over all

00:44:20,050 --> 00:44:27,210
the channels so that's a work in

00:44:21,760 --> 00:44:27,210
progress but actively being worked on

00:44:30,300 --> 00:44:37,690
right now you put trust your users or

00:44:35,320 --> 00:44:39,910
firewall it yeah I mean if you could

00:44:37,690 --> 00:44:41,080
have just some edge nodes that are you

00:44:39,910 --> 00:44:43,180
know the frameworks that actually

00:44:41,080 --> 00:44:45,670
interact with it a kind of isolated

00:44:43,180 --> 00:44:49,180
cluster but a lot of these companies

00:44:45,670 --> 00:44:51,730
they you know have some inherent trust

00:44:49,180 --> 00:44:54,730
in their users but not all companies can

00:44:51,730 --> 00:44:56,350
trust it all their customers so yeah I

00:44:54,730 --> 00:44:58,240
think we're about out of time

00:44:56,350 --> 00:45:00,430
if you have any further questions you

00:44:58,240 --> 00:45:03,610
can come and talk to us after the talk

00:45:00,430 --> 00:45:05,820
outside we'll be around thank you very

00:45:03,610 --> 00:45:05,820

YouTube URL: https://www.youtube.com/watch?v=EI0ROkf0vks


