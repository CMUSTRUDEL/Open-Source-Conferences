Title: Building a One Million QPS Cassandra DBaaS Solution on Kubernetes Randy Abernethy, Yekasa Kosuru
Publication date: 2019-06-19
Playlist: Apache Roadshow Chicago, 2019
Description: 
	In 2018, State Street began using Kubernetes to increase velocity for their software pipeline and to improve developer experience and productivity. However, a production microservices platform is not complete without a highly scalable state management solution. To this end, State Street adopted Apache Cassandra as one of its key platform database offerings, enabling developers to avail themselves of the low latency and extreme scalability offered by Cassandra. In this talk, Yekasa and Randy will discuss the architecture of the Kubernetes-based Cassandra solution deployed at State Street. They will share their experiences, challenges and plans for the future, while also detailing the benefits the solution has delivered to the various teams working on the platform at one of the worldâ€™s largest financial institutions.
Captions: 
	00:00:01,460 --> 00:00:09,480
all right little better buddy forget to

00:00:04,790 --> 00:00:12,080
show this is the FinTech track so if you

00:00:09,480 --> 00:00:14,490
like money you're in the right place so

00:00:12,080 --> 00:00:17,340
we're getting these off way with the

00:00:14,490 --> 00:00:38,550
presentation Sandra and rocks TV

00:00:17,340 --> 00:00:52,440
I am so pumped my mind was here and

00:00:38,550 --> 00:00:55,890
she's like my name is 60 swishy bank is

00:00:52,440 --> 00:00:57,600
a bank is the custodian yet so for those

00:00:55,890 --> 00:01:00,059
of you in the next attraction are you

00:00:57,600 --> 00:01:03,690
get except this time where these people

00:01:00,059 --> 00:01:07,710
would 10% off for transactions so that's

00:01:03,690 --> 00:01:12,360
about 36 pretty accurately as a pocket

00:01:07,710 --> 00:01:14,850
gun down nothing like you're like the

00:01:12,360 --> 00:01:17,270
difference when we this way another

00:01:14,850 --> 00:01:27,810
three cents it that way who gets raped

00:01:17,270 --> 00:01:30,000
domain okay so my mom is not that's not

00:01:27,810 --> 00:01:32,250
a flexpays you convenient and there's

00:01:30,000 --> 00:01:35,880
not normality that they did you know we

00:01:32,250 --> 00:01:37,860
did molecular alright so let's talk

00:01:35,880 --> 00:01:43,950
about the active channels that we

00:01:37,860 --> 00:01:47,130
apprentice operate so Stacy like most of

00:01:43,950 --> 00:01:49,649
the other companies that did micro

00:01:47,130 --> 00:01:53,790
services architecture for major

00:01:49,649 --> 00:01:57,360
developer ready make sex and as we look

00:01:53,790 --> 00:02:00,060
at this lap state by the way let me

00:01:57,360 --> 00:02:05,700
confuse myself rent increase ready

00:02:00,060 --> 00:02:08,660
i bypassed a sec I mean yourself and I'm

00:02:05,700 --> 00:02:11,300
joined by Randy Abernathy from Erickson

00:02:08,660 --> 00:02:15,140
Randy and his team

00:02:11,300 --> 00:02:17,570
personal humanities and so we believe

00:02:15,140 --> 00:02:19,790
not that quite a bit to implement some

00:02:17,570 --> 00:02:23,930
of the solutions like paper dr. Frank

00:02:19,790 --> 00:02:26,330
so so when you get across this micro

00:02:23,930 --> 00:02:29,090
circuits architecture the one thing that

00:02:26,330 --> 00:02:31,070
stood out was this given your story so

00:02:29,090 --> 00:02:34,760
this is not nothing new Richards to be a

00:02:31,070 --> 00:02:37,520
nun for 10 plus years whatever so we

00:02:34,760 --> 00:02:41,090
looked at you know I was tasked to

00:02:37,520 --> 00:02:42,430
develop and deploy the key managed or as

00:02:41,090 --> 00:02:46,310
part of the divestment

00:02:42,430 --> 00:02:49,070
so d-backs I will always look at me

00:02:46,310 --> 00:02:51,620
important things for the boy immediately

00:02:49,070 --> 00:02:53,900
under the best umbrella what's what is

00:02:51,620 --> 00:02:55,850
always sucks so why should it be over

00:02:53,900 --> 00:02:59,090
some of it because the financial tracks

00:02:55,850 --> 00:03:01,250
right you can come up the compliance

00:02:59,090 --> 00:03:04,010
audit controls all of those things good

00:03:01,250 --> 00:03:06,290
things that you need to do to access the

00:03:04,010 --> 00:03:08,570
financial industry deep into the stand

00:03:06,290 --> 00:03:10,880
and all my presence can be inferred from

00:03:08,570 --> 00:03:12,560
because open-source makes it a lot

00:03:10,880 --> 00:03:15,110
easier for us we can go right and center

00:03:12,560 --> 00:03:18,980
stack and fill it mustn't deploy where

00:03:15,110 --> 00:03:21,800
everywhere high performance so this is

00:03:18,980 --> 00:03:23,480
my favorite which is high performance

00:03:21,800 --> 00:03:25,700
system is any system that delivers a lot

00:03:23,480 --> 00:03:29,000
of you look like a million QPS the

00:03:25,700 --> 00:03:34,340
models foster and then Linda she is on a

00:03:29,000 --> 00:03:36,410
really low sub-millisecond X's none of

00:03:34,340 --> 00:03:40,100
the times you know people look at the

00:03:36,410 --> 00:03:41,600
median and average is just okay if it's

00:03:40,100 --> 00:03:44,720
important you gotta look at that that's

00:03:41,600 --> 00:03:46,700
that is necessary but it's not

00:03:44,720 --> 00:03:48,800
sufficient if you were to deploy the set

00:03:46,700 --> 00:03:51,590
scale you have to really focus on

00:03:48,800 --> 00:03:53,930
between energy lines the tail agencies

00:03:51,590 --> 00:03:55,400
that get you those are the ones that

00:03:53,930 --> 00:03:57,650
pick you up with me because of the men

00:03:55,400 --> 00:04:00,830
that the system is at my mom's well you

00:03:57,650 --> 00:04:03,590
have any of the phone that's what if you

00:04:00,830 --> 00:04:05,800
understand will you be ninja nature and

00:04:03,590 --> 00:04:14,269
we go though Randy's going to pursue

00:04:05,800 --> 00:04:18,400
next and next one is the cognitive right

00:04:14,269 --> 00:04:22,360
so this is the Amazon Google everyone is

00:04:18,400 --> 00:04:24,520
teaching us these bases being power and

00:04:22,360 --> 00:04:31,840
you know the very students

00:04:24,520 --> 00:04:33,490
consistent definition of space what it

00:04:31,840 --> 00:04:35,409
means that you can race in your

00:04:33,490 --> 00:04:37,990
departments and then you automate

00:04:35,409 --> 00:04:42,099
athleticism right that's the simplest

00:04:37,990 --> 00:04:44,169
way I could so what does pretty much say

00:04:42,099 --> 00:04:46,530
we think that you can write managing

00:04:44,169 --> 00:04:48,970
still be available to managing

00:04:46,530 --> 00:04:51,159
accountability back on their social I

00:04:48,970 --> 00:04:53,740
think that we do manage like it should

00:04:51,159 --> 00:04:55,930
be system should be doing these things

00:04:53,740 --> 00:04:58,870
automatically and if you haven't

00:04:55,930 --> 00:05:01,389
necesito backup guess what that backup

00:04:58,870 --> 00:05:03,580
is not going right you run into so many

00:05:01,389 --> 00:05:07,800
instances where you go to pull the back

00:05:03,580 --> 00:05:14,710
of the rest or something whatever

00:05:07,800 --> 00:05:16,719
so you know Thank You mr. Dexter right

00:05:14,710 --> 00:05:18,849
so you take a backup you test it and

00:05:16,719 --> 00:05:22,479
then you know when it is good you put in

00:05:18,849 --> 00:05:26,169
the artifacts otherwise who knows this

00:05:22,479 --> 00:05:28,500
well work it what what so these are the

00:05:26,169 --> 00:05:32,409
three pillars that we'll be looking at

00:05:28,500 --> 00:05:39,370
before we took a solution important okay

00:05:32,409 --> 00:05:43,029
so so Roxanne drop this we look at

00:05:39,370 --> 00:05:45,219
different key values bad source of the

00:05:43,029 --> 00:05:47,319
course on Thomas what's the topic

00:05:45,219 --> 00:05:48,539
Stephen its maturity it stability is

00:05:47,319 --> 00:05:51,370
reliability

00:05:48,539 --> 00:05:54,909
but what really make it compelling for

00:05:51,370 --> 00:05:56,830
Oxford's the the work that we gots to be

00:05:54,909 --> 00:06:00,310
injured or or the instagram guys have

00:05:56,830 --> 00:06:05,020
done which is they replaced the lowest

00:06:00,310 --> 00:06:08,289
layers in the summer staff trustees from

00:06:05,020 --> 00:06:10,810
Facebook's it's a keeper in store

00:06:08,289 --> 00:06:14,560
it's just a library sir hook it up and

00:06:10,810 --> 00:06:17,020
call it use it our you why and Cassandra

00:06:14,560 --> 00:06:18,460
way said of the Instagram guys have

00:06:17,020 --> 00:06:21,370
taken Sondra

00:06:18,460 --> 00:06:23,620
deserve it atop can and leave takes the

00:06:21,370 --> 00:06:26,169
bottom half but because the rocks and

00:06:23,620 --> 00:06:28,900
rather the gossipy is diplomatic see for

00:06:26,169 --> 00:06:30,880
this and it performs lot more efficient

00:06:28,900 --> 00:06:36,139
and you'll see the difference between

00:06:30,880 --> 00:06:38,990
nginx the one that I will stop so

00:06:36,139 --> 00:06:40,849
by the way become from our Facebook a

00:06:38,990 --> 00:06:43,699
spear and a wonderful talk last year at

00:06:40,849 --> 00:06:46,060
Cortana like we haven't seen miss fames

00:06:43,699 --> 00:06:48,529
ever encourage you to you know birthday

00:06:46,060 --> 00:06:51,409
and then of course enough that you have

00:06:48,529 --> 00:06:52,969
the Ross Andru you the points right so

00:06:51,409 --> 00:06:57,050
that's the next big question so we

00:06:52,969 --> 00:06:59,240
looked at humanities you know as you all

00:06:57,050 --> 00:07:03,529
know this is another episode from Google

00:06:59,240 --> 00:07:06,050
and the when it leaves you you can play

00:07:03,529 --> 00:07:07,400
the orchestra orchestration and fully

00:07:06,050 --> 00:07:08,779
loaded with all the features that you

00:07:07,400 --> 00:07:11,389
would need for production scale

00:07:08,779 --> 00:07:13,789
deployments like statehood Saxon replica

00:07:11,389 --> 00:07:16,150
sets and persistent volumes you know

00:07:13,789 --> 00:07:21,590
release that's good enough for us to go

00:07:16,150 --> 00:07:25,129
- so that's all silly - right so sorry

00:07:21,590 --> 00:07:28,370
Roxy V Lane and our given AIDS that's

00:07:25,129 --> 00:07:31,099
the solution okay but doesn't

00:07:28,370 --> 00:07:34,279
destination does have gas for example if

00:07:31,099 --> 00:07:37,719
you download from the Facebook people it

00:07:34,279 --> 00:07:40,069
doesn't mean if you have snapshots watch

00:07:37,719 --> 00:07:41,750
so the listen apps are said know that

00:07:40,069 --> 00:07:47,210
when the time comes we either take a

00:07:41,750 --> 00:07:48,529
backup or recover you need to destroy

00:07:47,210 --> 00:07:51,319
mean is it working properly

00:07:48,529 --> 00:07:54,469
there's many gaps here so what we've

00:07:51,319 --> 00:07:57,680
done is we've actually the back of our

00:07:54,469 --> 00:08:00,469
solution is it's a pretty elegant

00:07:57,680 --> 00:08:02,449
solution and we're taking it to the next

00:08:00,469 --> 00:08:08,379
round but this is phase one certainly

00:08:02,449 --> 00:08:08,379
it's not an open source yet let me know

00:08:10,539 --> 00:08:16,759
whether they are going to bilities and

00:08:14,240 --> 00:08:19,370
all open source code you know you see a

00:08:16,759 --> 00:08:22,610
lot of buzz from from stage going

00:08:19,370 --> 00:08:24,710
forward we have my team as contributors

00:08:22,610 --> 00:08:26,240
to rock sleepy so be accent work very

00:08:24,710 --> 00:08:28,849
closely with Facebook and making those

00:08:26,240 --> 00:08:31,539
contributions as well understand what

00:08:28,849 --> 00:08:34,519
the orchestration so now that you have

00:08:31,539 --> 00:08:36,620
some of the backups the rest or the

00:08:34,519 --> 00:08:38,209
expect that take care of you still have

00:08:36,620 --> 00:08:42,260
to package up your application and

00:08:38,209 --> 00:08:43,690
deploy it in a way that you don't want

00:08:42,260 --> 00:08:47,700
all the because we

00:08:43,690 --> 00:08:50,500
that appears egos then you larger your

00:08:47,700 --> 00:08:53,350
ability but that may be atleast its own

00:08:50,500 --> 00:08:54,550
want to be terrible so there is a little

00:08:53,350 --> 00:08:56,770
bit of work there in terms of

00:08:54,550 --> 00:09:02,500
orchestration right so and then I'm

00:08:56,770 --> 00:09:04,860
personally of Cuban music and then

00:09:02,500 --> 00:09:07,660
Dominic comes to the containerization

00:09:04,860 --> 00:09:10,330
Sandra the developers that were using

00:09:07,660 --> 00:09:11,860
Cassandra they fell in love with the

00:09:10,330 --> 00:09:15,160
environment variables you can actually

00:09:11,860 --> 00:09:19,960
add some Wilson somewhere pick it up at

00:09:15,160 --> 00:09:24,040
runtime we lost all those things so what

00:09:19,960 --> 00:09:27,490
we did was we have at the second so to

00:09:24,040 --> 00:09:31,810
make it useful for our developers that

00:09:27,490 --> 00:09:35,020
at stage people so there's no gastric

00:09:31,810 --> 00:09:38,020
whale but what we've done so far with

00:09:35,020 --> 00:09:39,820
the monitoring a backup and restore now

00:09:38,020 --> 00:09:43,390
that now you could believe that this is

00:09:39,820 --> 00:09:45,480
good enough to deploy a success and what

00:09:43,390 --> 00:09:48,850
you're going to see now them to the

00:09:45,480 --> 00:09:51,460
sandbox group you will see the number

00:09:48,850 --> 00:09:53,890
with the exact context that has been

00:09:51,460 --> 00:09:55,810
implemented monitoring good stuff we

00:09:53,890 --> 00:09:57,730
make it production ready but also you

00:09:55,810 --> 00:10:08,350
will see some performance numbers from a

00:09:57,730 --> 00:10:17,410
production electric bus trip that's all

00:10:08,350 --> 00:10:19,810
right the implementation is really kind

00:10:17,410 --> 00:10:21,430
of driven by the things that the case

00:10:19,810 --> 00:10:24,610
was highlighted in fact that we wanted

00:10:21,430 --> 00:10:27,490
to move to cloud made it so that means

00:10:24,610 --> 00:10:29,980
you know microservices and container

00:10:27,490 --> 00:10:32,410
acting and dynamic orchestration but one

00:10:29,980 --> 00:10:34,480
of the things that cloud native at least

00:10:32,410 --> 00:10:37,060
at the very early stages didn't really

00:10:34,480 --> 00:10:38,770
address was platform services anything

00:10:37,060 --> 00:10:41,150
that's stateful

00:10:38,770 --> 00:10:42,740
sort of is anathema in a

00:10:41,150 --> 00:10:44,240
servus sort of world where you supposed

00:10:42,740 --> 00:10:46,220
to be building these little stateless

00:10:44,240 --> 00:10:48,530
guys that you can scale to the nines

00:10:46,220 --> 00:10:50,450
right and there it blasted you startup

00:10:48,530 --> 00:10:52,160
ten you shut down five you know that's

00:10:50,450 --> 00:10:53,840
not the kind of slapping around I want

00:10:52,160 --> 00:10:55,850
to do is something that's storing your

00:10:53,840 --> 00:10:58,430
data right moving and terrified around

00:10:55,850 --> 00:10:59,840
you know every time you scale is

00:10:58,430 --> 00:11:01,340
probably not a good idea

00:10:59,840 --> 00:11:03,770
this is a completely different approach

00:11:01,340 --> 00:11:05,690
that you take for stateful types of

00:11:03,770 --> 00:11:08,030
systems in a cloud native environment

00:11:05,690 --> 00:11:11,060
and usually those solutions are part of

00:11:08,030 --> 00:11:13,970
a platform they can consume as a service

00:11:11,060 --> 00:11:16,940
by the the application developer so the

00:11:13,970 --> 00:11:19,870
devs that are in the cloud you know

00:11:16,940 --> 00:11:23,030
we're going to use dynamodb or you know

00:11:19,870 --> 00:11:24,470
the cloud providers offering and they

00:11:23,030 --> 00:11:26,540
just do no consume it as a service

00:11:24,470 --> 00:11:28,370
poking your employees in financial

00:11:26,540 --> 00:11:30,500
services institution you know there's

00:11:28,370 --> 00:11:33,080
still a lot of regulations and things

00:11:30,500 --> 00:11:37,850
like that that keep you from potentially

00:11:33,080 --> 00:11:39,830
doing something and so a lot of these

00:11:37,850 --> 00:11:41,840
firms want to have all the staple stuff

00:11:39,830 --> 00:11:44,360
running in their own environment so the

00:11:41,840 --> 00:11:45,980
Trinities along the road came up with

00:11:44,360 --> 00:11:47,630
this thing called the staple set and

00:11:45,980 --> 00:11:50,390
then more services and features started

00:11:47,630 --> 00:11:52,460
to show up and so in cases teams

00:11:50,390 --> 00:11:55,250
basically charged with creating this

00:11:52,460 --> 00:11:58,850
platform from nothing and you know again

00:11:55,250 --> 00:12:00,680
the tool the job seems to be Cassandra

00:11:58,850 --> 00:12:02,930
her part of this they actually have

00:12:00,680 --> 00:12:04,490
several different media based platforms

00:12:02,930 --> 00:12:06,560
and developers get to choose right most

00:12:04,490 --> 00:12:07,970
deaths want to be able to say hey this

00:12:06,560 --> 00:12:09,980
is a relational task I'm doing

00:12:07,970 --> 00:12:12,230
transactions and stuff give me you know

00:12:09,980 --> 00:12:13,940
our EP MS craving about that if they

00:12:12,230 --> 00:12:15,770
offer that as a service and then a I

00:12:13,940 --> 00:12:17,480
mean something is crazy scalable and

00:12:15,770 --> 00:12:20,780
ridiculously fast that's what that's

00:12:17,480 --> 00:12:24,110
pieces for and so on the back end you

00:12:20,780 --> 00:12:27,230
have this well-known tried and true code

00:12:24,110 --> 00:12:29,900
path that's really rich and powerful and

00:12:27,230 --> 00:12:31,820
has lots of features and it is really

00:12:29,900 --> 00:12:35,210
really fast and literally scalable

00:12:31,820 --> 00:12:37,430
largely but this this is this whole

00:12:35,210 --> 00:12:39,320
thing is his job right and everybody

00:12:37,430 --> 00:12:40,280
knows that the Achilles heel in job is

00:12:39,320 --> 00:12:42,950
harder to collect

00:12:40,280 --> 00:12:44,480
maybe the GC kicks in it stopped the

00:12:42,950 --> 00:12:47,330
world and if you're in the middle of a

00:12:44,480 --> 00:12:49,910
financial transaction that's not cool so

00:12:47,330 --> 00:12:50,600
we use switch over to rocks you're now

00:12:49,910 --> 00:12:52,010
dealing with

00:12:50,600 --> 00:12:55,190
Plus Denman there's no garbage

00:12:52,010 --> 00:12:57,590
collection much more deterministic kind

00:12:55,190 --> 00:13:00,200
of lassie all the way out there at p99

00:12:57,590 --> 00:13:03,230
and so the beauty of this model is they

00:13:00,200 --> 00:13:04,880
can get both you can do applications

00:13:03,230 --> 00:13:06,890
stuff that doesn't have this may be

00:13:04,880 --> 00:13:08,450
time-sensitive criticality and can

00:13:06,890 --> 00:13:11,120
handle a little blip every once in a

00:13:08,450 --> 00:13:13,460
while over here and you can get the rich

00:13:11,120 --> 00:13:16,280
functionality of being able to you know

00:13:13,460 --> 00:13:18,470
do queries that produce lots of Records

00:13:16,280 --> 00:13:19,280
and things like that this is to keep out

00:13:18,470 --> 00:13:20,990
of the store

00:13:19,280 --> 00:13:24,350
you've given a key it'll give a little

00:13:20,990 --> 00:13:26,090
rally if you say give me you know all of

00:13:24,350 --> 00:13:27,770
the things that are like this it's gonna

00:13:26,090 --> 00:13:30,800
barf right it's not going to help you so

00:13:27,770 --> 00:13:33,080
not every application can use this not

00:13:30,800 --> 00:13:35,150
every on occasion can use this but they

00:13:33,080 --> 00:13:37,400
needed to code them you get this massive

00:13:35,150 --> 00:13:40,010
scale and you get to choose kind of your

00:13:37,400 --> 00:13:42,110
last profile up based on feature set and

00:13:40,010 --> 00:13:44,450
it really kind of works out icing so the

00:13:42,110 --> 00:13:46,310
idea then is to package this up it with

00:13:44,450 --> 00:13:49,510
container and just feel to have any of

00:13:46,310 --> 00:13:52,700
these discs eat and to get to that you

00:13:49,510 --> 00:13:54,920
give to that 1 million 3 per second you

00:13:52,700 --> 00:13:57,110
need about you know 10 and pretty beefy

00:13:54,920 --> 00:13:59,390
box it's 10 pretty beefy violence

00:13:57,110 --> 00:14:02,090
offenses would respect that but the

00:13:59,390 --> 00:14:05,270
platform basically looks like this in in

00:14:02,090 --> 00:14:08,440
kubernetes environment we basically have

00:14:05,270 --> 00:14:11,270
a bunch of computers and those guys

00:14:08,440 --> 00:14:13,490
provision with nbme so these are really

00:14:11,270 --> 00:14:15,170
fast SSDs and the cloud instances and

00:14:13,490 --> 00:14:16,970
they're locally attacks try to write one

00:14:15,170 --> 00:14:20,150
over a network to talk to a storage

00:14:16,970 --> 00:14:21,590
array and the reason that people like to

00:14:20,150 --> 00:14:22,940
go over networks to talk to storage

00:14:21,590 --> 00:14:25,010
arrays is because they can click a

00:14:22,940 --> 00:14:28,280
button and snapshot right if you're in

00:14:25,010 --> 00:14:30,770
AWS or what happens they can do things

00:14:28,280 --> 00:14:32,690
like I put the button back them up they

00:14:30,770 --> 00:14:34,370
can do things like detach them from one

00:14:32,690 --> 00:14:36,680
system and attach them to another system

00:14:34,370 --> 00:14:39,380
you can't do that at least they're on

00:14:36,680 --> 00:14:41,390
the box but the usually Cassandra is

00:14:39,380 --> 00:14:43,640
Cassandra lets you create e spaces with

00:14:41,390 --> 00:14:44,810
x replication if you want you know

00:14:43,640 --> 00:14:46,250
assume copies of your data

00:14:44,810 --> 00:14:47,630
cuz you're a little paranoid or sweet

00:14:46,250 --> 00:14:49,880
Papa company it because you're really

00:14:47,630 --> 00:14:52,190
paranoid you do it and when a box goes

00:14:49,880 --> 00:14:53,630
down you just don't care this time is

00:14:52,190 --> 00:14:55,760
going to take care of it is just you can

00:14:53,630 --> 00:14:58,100
background and unique

00:14:55,760 --> 00:15:00,139
the developers don't care it cases teams

00:14:58,100 --> 00:15:01,820
tears because they gotta make sure

00:15:00,139 --> 00:15:03,320
there's enough Headroom for that data

00:15:01,820 --> 00:15:05,269
replication to take place when those

00:15:03,320 --> 00:15:06,980
kinds of failures happen but beyond that

00:15:05,269 --> 00:15:10,190
you're not going to lose anything right

00:15:06,980 --> 00:15:11,810
at this fella so this is in today's

00:15:10,190 --> 00:15:14,029
world this would be calling for system

00:15:11,810 --> 00:15:16,519
volume because a little volume manager

00:15:14,029 --> 00:15:19,010
that we've got set up that provisions

00:15:16,519 --> 00:15:20,990
all of the SSDs that computer on an

00:15:19,010 --> 00:15:22,839
instance automatically not set up as a

00:15:20,990 --> 00:15:25,970
game in seven committees a gaming set

00:15:22,839 --> 00:15:27,740
cherries a service it runs once on every

00:15:25,970 --> 00:15:30,440
single node so if you want to expand

00:15:27,740 --> 00:15:37,910
your cluster just add four nodes and

00:15:30,440 --> 00:15:39,070
then instantly the TVs probably the

00:15:37,910 --> 00:15:40,850
disks that are there and then

00:15:39,070 --> 00:15:43,670
automatically you're going to be able to

00:15:40,850 --> 00:15:46,670
use them the next piece is up at this

00:15:43,670 --> 00:15:49,639
level there's a scheduler in kubernetes

00:15:46,670 --> 00:15:51,980
that knows how to find a home for a

00:15:49,639 --> 00:15:53,959
staple set the case to mention this AZ's

00:15:51,980 --> 00:15:56,750
are really important right to sign your

00:15:53,959 --> 00:15:58,430
own reputation has to know about your

00:15:56,750 --> 00:16:00,260
real looking he smells right it's got to

00:15:58,430 --> 00:16:02,750
be wired into your fault domains because

00:16:00,260 --> 00:16:05,449
if it doesn't know that these three

00:16:02,750 --> 00:16:07,310
machines are in the same AC it could put

00:16:05,449 --> 00:16:09,800
all of the replicas of your data in that

00:16:07,310 --> 00:16:12,800
same AC when you as he goes down by my

00:16:09,800 --> 00:16:14,569
data so we've got all of this kind of

00:16:12,800 --> 00:16:16,639
configure so that's Steckler

00:16:14,569 --> 00:16:20,569
you know this this guy is learning from

00:16:16,639 --> 00:16:22,819
Amazon using they call a stitch where

00:16:20,569 --> 00:16:24,709
the daisies are and so it's good to make

00:16:22,819 --> 00:16:26,540
sure that replicas of your data are

00:16:24,709 --> 00:16:28,459
distributed across Daisy so anybody

00:16:26,540 --> 00:16:30,110
who's got the two replicas or more it's

00:16:28,459 --> 00:16:32,510
gonna have at least two daisies involved

00:16:30,110 --> 00:16:34,940
you have 3zz a triple replication

00:16:32,510 --> 00:16:36,440
they'll have one and each a Z so this is

00:16:34,940 --> 00:16:38,779
capable sense are going to be scaled

00:16:36,440 --> 00:16:40,819
across daisies and the data is going to

00:16:38,779 --> 00:16:43,279
be scaled across daisies because the

00:16:40,819 --> 00:16:44,839
awareness is not only a kinetic level in

00:16:43,279 --> 00:16:47,360
the staple set and the kubernetes

00:16:44,839 --> 00:16:52,279
scheduler but also up America Cassandra

00:16:47,360 --> 00:16:53,720
law so when the observability front

00:16:52,279 --> 00:16:55,790
observability in the cloud native

00:16:53,720 --> 00:16:57,740
systems really critical because you take

00:16:55,790 --> 00:16:59,870
these monolithic things that you used to

00:16:57,740 --> 00:17:01,580
have before here's my app and you would

00:16:59,870 --> 00:17:03,560
slow them into

00:17:01,580 --> 00:17:05,120
hundreds maybe tens hundreds of little

00:17:03,560 --> 00:17:07,160
services all talking to each other

00:17:05,120 --> 00:17:09,080
and when something goes wrong what the

00:17:07,160 --> 00:17:11,270
heck happened right is it that service

00:17:09,080 --> 00:17:12,980
this one was it a network issue didn't

00:17:11,270 --> 00:17:15,410
know about it was just transient right

00:17:12,980 --> 00:17:16,520
how do you know well observability has

00:17:15,410 --> 00:17:19,700
got three legs

00:17:16,520 --> 00:17:22,010
there's blogging which are events right

00:17:19,700 --> 00:17:24,890
a customer connection that type of thing

00:17:22,010 --> 00:17:26,450
there's a metrics CPU memory that's a

00:17:24,890 --> 00:17:28,610
time series right every five minutes

00:17:26,450 --> 00:17:30,830
show me CPU memory and then there's

00:17:28,610 --> 00:17:33,350
Tracy which is contact sensitive

00:17:30,830 --> 00:17:35,060
information right now just user may need

00:17:33,350 --> 00:17:36,680
this call which causing that service to

00:17:35,060 --> 00:17:38,570
talk to that service to talk to this

00:17:36,680 --> 00:17:39,380
service to hit this data set to come

00:17:38,570 --> 00:17:41,870
back right

00:17:39,380 --> 00:17:44,180
that's context sensitive stuff Tracy so

00:17:41,870 --> 00:17:47,390
we need to have all that stuff baked

00:17:44,180 --> 00:17:50,000
into the solution and so the first piece

00:17:47,390 --> 00:17:52,220
of the puzzle was together metrics so we

00:17:50,000 --> 00:17:53,690
actually monitor the state of the

00:17:52,220 --> 00:17:55,550
systems and things like that

00:17:53,690 --> 00:17:58,460
fortunately casandra's got all sorts of

00:17:55,550 --> 00:18:00,320
great DMX output that you can use to you

00:17:58,460 --> 00:18:02,420
know see transactions and lasty and

00:18:00,320 --> 00:18:04,730
things like that and so all of the cyber

00:18:02,420 --> 00:18:07,940
quadrants ferment it for this is G of X

00:18:04,730 --> 00:18:10,010
data but what we want is for this to be

00:18:07,940 --> 00:18:11,930
databases of service right is Nick

00:18:10,010 --> 00:18:13,820
developers say I needed to sign or

00:18:11,930 --> 00:18:15,500
cluster it's going to need this capacity

00:18:13,820 --> 00:18:19,030
skin of the boxes won't let me use it

00:18:15,500 --> 00:18:21,140
and I want a monitoring right so the

00:18:19,030 --> 00:18:27,860
probably the most popular tool interface

00:18:21,140 --> 00:18:29,900
for monitoring is Prometheus so lets you

00:18:27,860 --> 00:18:31,670
watch your own customer for your seven

00:18:29,900 --> 00:18:34,660
clusters that you created or the one and

00:18:31,670 --> 00:18:37,910
the truth is that you've got to read any

00:18:34,660 --> 00:18:39,590
cluster that has 20 different developers

00:18:37,910 --> 00:18:41,990
operating on almost separate

00:18:39,590 --> 00:18:45,680
applications of different namespaces you

00:18:41,990 --> 00:18:47,960
can't have 20 different for medias

00:18:45,680 --> 00:18:50,660
instances scraping all this data you

00:18:47,960 --> 00:18:52,820
have found a single instance that's part

00:18:50,660 --> 00:18:55,490
of the platform that collects all of the

00:18:52,820 --> 00:18:57,710
data and a federated wave actually

00:18:55,490 --> 00:18:58,820
provides the data out there there are

00:18:57,710 --> 00:19:00,590
some ways that you can do things

00:18:58,820 --> 00:19:02,450
differently but this is pretty much the

00:19:00,590 --> 00:19:04,880
best model of the lowest utilization

00:19:02,450 --> 00:19:06,800
of the the actual underlying system so

00:19:04,880 --> 00:19:08,330
we've got a master at instance which is

00:19:06,800 --> 00:19:10,100
really just the time series database

00:19:08,330 --> 00:19:13,310
with a scraper and then we've got

00:19:10,100 --> 00:19:15,710
instances they're created 30 bats and so

00:19:13,310 --> 00:19:18,620
we need to pull a lot of each designer

00:19:15,710 --> 00:19:21,350
instances you'd your own dashboard we

00:19:18,620 --> 00:19:23,200
send up practices counters and meters of

00:19:21,350 --> 00:19:27,350
course customizer

00:19:23,200 --> 00:19:29,360
so the solution deploys the students at

00:19:27,350 --> 00:19:31,280
Berkeley benches configurable size to

00:19:29,360 --> 00:19:33,260
make it however big you want and it

00:19:31,280 --> 00:19:35,780
actually employs the monitoring solution

00:19:33,260 --> 00:19:38,030
just like that also an HPA so a

00:19:35,780 --> 00:19:39,770
horizontal pod autoscaler and cloud

00:19:38,030 --> 00:19:43,820
native systems you get faster by adding

00:19:39,770 --> 00:19:45,710
more and the downside is that a lot of

00:19:43,820 --> 00:19:47,690
databases don't work that back but

00:19:45,710 --> 00:19:49,850
Cassandra does so it's a perfect than

00:19:47,690 --> 00:19:51,440
the solution and you can said this where

00:19:49,850 --> 00:19:53,690
was a lot of scalar and it will watch

00:19:51,440 --> 00:19:55,820
storage so we started running out of

00:19:53,690 --> 00:19:57,110
store it'll provision new boxes because

00:19:55,820 --> 00:19:58,700
you're using local storage there's

00:19:57,110 --> 00:19:59,360
nobody to get more right except atom

00:19:58,700 --> 00:20:01,970
boxes

00:19:59,360 --> 00:20:03,800
it will also watch CPU memory and it'll

00:20:01,970 --> 00:20:05,270
scale a lot of custom events if you want

00:20:03,800 --> 00:20:07,580
to that come out of deep just like

00:20:05,270 --> 00:20:09,530
transactions per second or something so

00:20:07,580 --> 00:20:11,630
that can be turned on while I wrap the

00:20:09,530 --> 00:20:13,730
scales we've also set it up to be one

00:20:11,630 --> 00:20:15,470
way normally horizontal pot I was

00:20:13,730 --> 00:20:17,420
dealers to go both ways they're elastic

00:20:15,470 --> 00:20:19,940
but you don't want to scale down your

00:20:17,420 --> 00:20:22,400
database without human supervision right

00:20:19,940 --> 00:20:24,740
that's a really bad idea because all of

00:20:22,400 --> 00:20:26,780
a sudden not a funny did you just take

00:20:24,740 --> 00:20:28,640
away a machine which means everybody

00:20:26,780 --> 00:20:31,100
else gets the load of that box shared

00:20:28,640 --> 00:20:33,590
across them but also that data just

00:20:31,100 --> 00:20:35,330
disappeared which means every one of

00:20:33,590 --> 00:20:36,920
those replicas those partition

00:20:35,330 --> 00:20:38,840
traffickers have to show up somewhere

00:20:36,920 --> 00:20:40,250
else and now the sudden data is going to

00:20:38,840 --> 00:20:41,870
be copied and when you shut that guy

00:20:40,250 --> 00:20:44,210
down it's gonna try to force all of its

00:20:41,870 --> 00:20:46,430
data from the networking and so not only

00:20:44,210 --> 00:20:47,960
is the load gonna come up because that

00:20:46,430 --> 00:20:49,580
guy disappeared that the load is going

00:20:47,960 --> 00:20:51,500
to come up even more because there's all

00:20:49,580 --> 00:20:53,570
this operational overhead that has to

00:20:51,500 --> 00:20:55,670
happen you can collapse your entire

00:20:53,570 --> 00:20:57,560
environment the production set so we

00:20:55,670 --> 00:21:00,980
need that so that it only goes up there

00:20:57,560 --> 00:21:03,080
never scaling you down and only scale up

00:21:00,980 --> 00:21:04,910
so humans can choose to scale down a lot

00:21:03,080 --> 00:21:07,580
too but that's up to them in their

00:21:04,910 --> 00:21:09,830
monitoring you know sad there's a backup

00:21:07,580 --> 00:21:12,490
scheduler also that you can deploy which

00:21:09,830 --> 00:21:15,190
Mohana the snapshot the entire cluster

00:21:12,490 --> 00:21:16,990
and then push it out to an s3 compatible

00:21:15,190 --> 00:21:18,940
object store which could be Swift on

00:21:16,990 --> 00:21:21,370
OpenStack or could be video or something

00:21:18,940 --> 00:21:25,030
like that and all of this stuff by the

00:21:21,370 --> 00:21:28,630
way is using column which is a way to

00:21:25,030 --> 00:21:30,670
create sort of a camel template of an

00:21:28,630 --> 00:21:32,230
application and then you just the users

00:21:30,670 --> 00:21:34,510
just fill in the little things that they

00:21:32,230 --> 00:21:37,300
want to change like turn this on turn

00:21:34,510 --> 00:21:38,920
that on and give me any notes right they

00:21:37,300 --> 00:21:40,630
fill in some variables to get the button

00:21:38,920 --> 00:21:47,080
off the ghost and you get the whole

00:21:40,630 --> 00:21:49,570
solution so resilient AZ diversity is

00:21:47,080 --> 00:21:53,080
kind of interesting because not only we

00:21:49,570 --> 00:21:56,320
need to have nodes just distributed

00:21:53,080 --> 00:21:59,020
across multiple availability zones but

00:21:56,320 --> 00:22:00,880
you also need to have your cause for the

00:21:59,020 --> 00:22:02,890
staple set that actually is housing the

00:22:00,880 --> 00:22:05,500
Cassandra solution to distribute it

00:22:02,890 --> 00:22:07,750
across the availability zones and the

00:22:05,500 --> 00:22:09,760
Cassandra engines running inside those

00:22:07,750 --> 00:22:11,320
pods have to know what availability

00:22:09,760 --> 00:22:13,750
zones there is so they can distribute

00:22:11,320 --> 00:22:16,120
their data so it's really like every

00:22:13,750 --> 00:22:19,200
tier of the solution needs to needs to

00:22:16,120 --> 00:22:22,390
know what the fault domains are and so

00:22:19,200 --> 00:22:26,140
it's pretty straightforward from a

00:22:22,390 --> 00:22:29,110
kubernetes standpoint in class a sure

00:22:26,140 --> 00:22:30,190
you know AWS Google Cloud they're all

00:22:29,110 --> 00:22:32,860
going to make it very easy for

00:22:30,190 --> 00:22:35,500
kubernetes region to figure out hey this

00:22:32,860 --> 00:22:39,070
nervous in this AZ and spin up nodes and

00:22:35,500 --> 00:22:41,020
the appropriate agencies we are saying

00:22:39,070 --> 00:22:42,700
we use a solution public ops which is

00:22:41,020 --> 00:22:45,580
kubernetes operations and it's pretty

00:22:42,700 --> 00:22:46,960
easy to use on Amazon probably easiest

00:22:45,580 --> 00:22:48,340
but there's things like who spray out

00:22:46,960 --> 00:22:50,980
there and other other tools that you can

00:22:48,340 --> 00:22:53,050
use the clouds break break there's also

00:22:50,980 --> 00:22:54,970
a lot of kubernetes is service out there

00:22:53,050 --> 00:22:57,880
all the big clouds have an IBM Google

00:22:54,970 --> 00:23:00,870
Amazon you know Patrick look I might get

00:22:57,880 --> 00:23:03,760
a 12 node kubernetes cluster or a 1200

00:23:00,870 --> 00:23:05,680
for every level and so that's that's

00:23:03,760 --> 00:23:08,560
kind of at the lowest level 3 3 x

00:23:05,680 --> 00:23:10,660
squared weight when it comes to the pods

00:23:08,560 --> 00:23:11,890
that's not automatic the scheduler is

00:23:10,660 --> 00:23:14,410
going to say hey if you want another

00:23:11,890 --> 00:23:16,210
storage pot let me find a know that has

00:23:14,410 --> 00:23:17,950
the requirements that you asked for and

00:23:16,210 --> 00:23:22,180
if you're saying I made this much CPU

00:23:17,950 --> 00:23:23,210
this much memory and I need half no it

00:23:22,180 --> 00:23:25,100
has got the store

00:23:23,210 --> 00:23:27,440
label because it's a special note that

00:23:25,100 --> 00:23:29,390
has pregnant via B's on it the special

00:23:27,440 --> 00:23:31,039
that we'll just find one of those it

00:23:29,390 --> 00:23:33,860
doesn't particularly care about what AC

00:23:31,039 --> 00:23:37,039
that that particular plot lands in

00:23:33,860 --> 00:23:38,299
unless you tell it to and so you know

00:23:37,039 --> 00:23:40,100
there's a little bit of a kubernetes

00:23:38,299 --> 00:23:42,409
artistry in the solution where you have

00:23:40,100 --> 00:23:44,899
to in your specifications for how to set

00:23:42,409 --> 00:23:47,480
up a stable set say I want any affinity

00:23:44,899 --> 00:23:49,610
by availability zone and so what happens

00:23:47,480 --> 00:23:51,770
is the first pot goes in some random a

00:23:49,610 --> 00:23:54,020
seat the next point goes in some

00:23:51,770 --> 00:23:56,330
different random AZ because it's got an

00:23:54,020 --> 00:23:58,159
TF in and let's say when we have to a

00:23:56,330 --> 00:23:59,809
Z's the third pod now it doesn't really

00:23:58,159 --> 00:24:00,620
matter which one it picks right they're

00:23:59,809 --> 00:24:03,710
both equally

00:24:00,620 --> 00:24:05,779
you know unhappy right if you're over

00:24:03,710 --> 00:24:06,860
here you have your allies with you here

00:24:05,779 --> 00:24:08,450
over here you have another pod with you

00:24:06,860 --> 00:24:10,370
so it just picks one but once that

00:24:08,450 --> 00:24:11,899
happens now it's clear hey there's

00:24:10,370 --> 00:24:13,520
there's much more gravity over here I'd

00:24:11,899 --> 00:24:15,529
like to stay away and it's maybe from

00:24:13,520 --> 00:24:17,179
the two no AC and I'm going to go over

00:24:15,529 --> 00:24:18,919
to OneNote and so it just kind of keeps

00:24:17,179 --> 00:24:20,570
paying far more depth three or four aces

00:24:18,919 --> 00:24:24,140
will do the same okay

00:24:20,570 --> 00:24:26,990
and so that's in any level and then in

00:24:24,140 --> 00:24:29,779
the cosigner level there's a piece of

00:24:26,990 --> 00:24:31,220
Java code called a snitch and you can

00:24:29,779 --> 00:24:33,110
configure different stitches some of

00:24:31,220 --> 00:24:34,850
them you can say here's a file that

00:24:33,110 --> 00:24:36,080
tells you where I am so in the old days

00:24:34,850 --> 00:24:36,860
when everybody was writing their stuff

00:24:36,080 --> 00:24:39,740
in their own d-desires

00:24:36,860 --> 00:24:41,240
people still do of course the rack was

00:24:39,740 --> 00:24:43,820
the familiar do name right because the

00:24:41,240 --> 00:24:45,620
rack has a top iraq switch and power and

00:24:43,820 --> 00:24:47,630
so that's switch with that power goes

00:24:45,620 --> 00:24:49,039
down the whole rack goes out so you can

00:24:47,630 --> 00:24:51,020
rather have their designer plus your

00:24:49,039 --> 00:24:53,029
distributed across three four five racks

00:24:51,020 --> 00:24:54,980
so in cassandra feel you're doing to

00:24:53,029 --> 00:24:56,539
wrap so in that some way of figuring out

00:24:54,980 --> 00:24:59,480
in a cloud native environment cut-up

00:24:56,539 --> 00:25:01,970
figure out from OpenStack or from Amazon

00:24:59,480 --> 00:25:04,700
or from you know the people cloud or

00:25:01,970 --> 00:25:07,039
whatever it is what the AZ is and

00:25:04,700 --> 00:25:08,929
translate that into casandra's rack you

00:25:07,039 --> 00:25:11,210
know nomenclature and so that's what a

00:25:08,929 --> 00:25:13,130
snitch duct you can do it from a

00:25:11,210 --> 00:25:15,399
property flowery to set up you know

00:25:13,130 --> 00:25:19,130
manually but that's not you know really

00:25:15,399 --> 00:25:20,809
automate everything if you use pre-built

00:25:19,130 --> 00:25:22,909
so we're actually using the east

00:25:20,809 --> 00:25:27,200
consistent which conveniently works on

00:25:22,909 --> 00:25:29,720
OpenStack and it also works on ec2 on

00:25:27,200 --> 00:25:32,690
Amazon beyond that's what it was both

00:25:29,720 --> 00:25:34,549
were course but the downside is if you

00:25:32,690 --> 00:25:36,419
ever wanted to deploy the solution on

00:25:34,549 --> 00:25:38,630
bare metal Cooper

00:25:36,419 --> 00:25:41,070
but there's no clap but there's still

00:25:38,630 --> 00:25:44,340
fault domains right Congrats as a

00:25:41,070 --> 00:25:47,100
special label that you put on nose beta

00:25:44,340 --> 00:25:49,799
not kubernetes not io / bit of a splash

00:25:47,100 --> 00:25:53,010
zone and then you've set it to something

00:25:49,799 --> 00:25:57,779
that it can provide anybody who asks and

00:25:53,010 --> 00:25:59,400
so future work the one the mothers but

00:25:57,779 --> 00:26:00,720
one of the several things that we would

00:25:59,400 --> 00:26:02,100
like to do going forward to make the

00:26:00,720 --> 00:26:04,350
solution even better is building

00:26:02,100 --> 00:26:06,750
kubernetes specific snitch so at the

00:26:04,350 --> 00:26:08,130
moment you have to manually snitch or

00:26:06,750 --> 00:26:10,740
you have to do something at ec2

00:26:08,130 --> 00:26:13,799
compatible like the ones that that's how

00:26:10,740 --> 00:26:15,510
we get the assigned reside to know you

00:26:13,799 --> 00:26:18,360
know we're good the data is that failure

00:26:15,510 --> 00:26:20,190
main connection between the cloud to the

00:26:18,360 --> 00:26:22,500
kubernetes layer all the way up in the

00:26:20,190 --> 00:26:26,429
facade is that's where it wrapped all

00:26:22,500 --> 00:26:28,080
right so yeah auto-scaling

00:26:26,429 --> 00:26:31,110
so the others again I mentioned this is

00:26:28,080 --> 00:26:34,020
kind of a one-way solution so this is a

00:26:31,110 --> 00:26:36,779
combination of something building i'll

00:26:34,020 --> 00:26:38,520
see advisor see advisors the container

00:26:36,779 --> 00:26:40,380
advisor you can buy them on docker

00:26:38,520 --> 00:26:42,179
just stand alone and buildings spit out

00:26:40,380 --> 00:26:44,250
all sorts of great metrics on what's

00:26:42,179 --> 00:26:45,659
happening on that docker instance but

00:26:44,250 --> 00:26:48,090
it's built into the kubernetes node

00:26:45,659 --> 00:26:49,830
manager to complete these days so that I

00:26:48,090 --> 00:26:51,630
think spitting out metrics like how many

00:26:49,830 --> 00:26:53,789
pods are here with the total committed

00:26:51,630 --> 00:26:56,090
memory and CPU is and things like that

00:26:53,789 --> 00:26:58,649
and then you know there's stuff like I

00:26:56,090 --> 00:27:01,380
know turkey advanced stuff from the

00:26:58,649 --> 00:27:03,960
previous scrapers and all of these

00:27:01,380 --> 00:27:05,940
metrics go into three heavies through

00:27:03,960 --> 00:27:07,529
this aggregation avionics you know the

00:27:05,940 --> 00:27:09,990
pressing metrics jumpin that server

00:27:07,529 --> 00:27:12,179
metrics and give me indeed the

00:27:09,990 --> 00:27:14,279
horizontal mascara has access to all

00:27:12,179 --> 00:27:16,049
that so anything that could grab this

00:27:14,279 --> 00:27:18,240
posture you can be measured you can use

00:27:16,049 --> 00:27:20,250
to trigger scaling event and so

00:27:18,240 --> 00:27:22,260
historically you know memory and CPU

00:27:20,250 --> 00:27:23,940
within the things but for us that's not

00:27:22,260 --> 00:27:25,740
the most important thing if you're about

00:27:23,940 --> 00:27:27,510
to run out of disk space that's really

00:27:25,740 --> 00:27:29,100
important it doesn't matter if you use

00:27:27,510 --> 00:27:31,140
quiescent you know you're about to

00:27:29,100 --> 00:27:33,870
happen to space you've gotta scale right

00:27:31,140 --> 00:27:36,390
so so we have our HP n wired up to a

00:27:33,870 --> 00:27:39,330
bunch of really important you know kind

00:27:36,390 --> 00:27:41,970
of metrics and it will then scale up now

00:27:39,330 --> 00:27:42,759
the way that HD is are set up went

00:27:41,970 --> 00:27:44,949
through tremendous

00:27:42,759 --> 00:27:47,409
a minute max right you can never scale

00:27:44,949 --> 00:27:49,719
me on the max you could never scale for

00:27:47,409 --> 00:27:51,759
the min and so we create a little bit of

00:27:49,719 --> 00:27:54,099
glue if you're one of these pods so that

00:27:51,759 --> 00:27:56,229
every time as a scaling event they move

00:27:54,099 --> 00:27:58,359
up the minimum to the number of pods

00:27:56,229 --> 00:28:00,879
there are in that staple set so it will

00:27:58,359 --> 00:28:04,239
never ever be able to scale down and so

00:28:00,879 --> 00:28:06,489
you can you can respect the developers

00:28:04,239 --> 00:28:07,929
request or never charge be more than

00:28:06,489 --> 00:28:10,869
next by saying I've got two end nodes

00:28:07,929 --> 00:28:12,879
it'll have more but you can also make

00:28:10,869 --> 00:28:17,409
sure that we are enough scaling down and

00:28:12,879 --> 00:28:20,769
causing your after class introduction so

00:28:17,409 --> 00:28:24,339
back understorms is a tricky one backing

00:28:20,769 --> 00:28:27,129
up a machine is one thing backing up

00:28:24,339 --> 00:28:29,409
data and eventually consistent cluster

00:28:27,129 --> 00:28:33,489
is another thing right there's no way

00:28:29,409 --> 00:28:35,319
it's physics impossibility to you know

00:28:33,489 --> 00:28:36,819
know the distributed state of a

00:28:35,319 --> 00:28:38,440
distributed system at any moment in time

00:28:36,819 --> 00:28:40,389
because you don't know which things are

00:28:38,440 --> 00:28:42,549
in flight which things are committed and

00:28:40,389 --> 00:28:44,499
you know the two generals problem right

00:28:42,549 --> 00:28:46,119
hey is two generals was out of field

00:28:44,499 --> 00:28:49,869
here and the bad guys here the bad guy

00:28:46,119 --> 00:28:52,149
has five armies you've got three and

00:28:49,869 --> 00:28:53,949
this other guys got three right if you

00:28:52,149 --> 00:28:56,559
both attack at the same time you win it

00:28:53,949 --> 00:28:58,989
six against five so send a message to

00:28:56,559 --> 00:29:01,839
this guy say hey let's attack this guy

00:28:58,989 --> 00:29:04,509
says he sent me the message let's attack

00:29:01,839 --> 00:29:05,889
at midnight but if I attack him he

00:29:04,509 --> 00:29:07,690
doesn't know I got the message and he

00:29:05,889 --> 00:29:09,309
doesn't attack I get wiped out five hits

00:29:07,690 --> 00:29:11,289
three so I'm gonna send a confirmation

00:29:09,309 --> 00:29:13,089
hey I got your message

00:29:11,289 --> 00:29:15,339
this guy's like ok cool they got our

00:29:13,089 --> 00:29:17,799
message but they don't know that we got

00:29:15,339 --> 00:29:19,269
the confirmation right what if they're

00:29:17,799 --> 00:29:20,769
gonna wait so we tell them that we

00:29:19,269 --> 00:29:22,419
actually got this so we send them the

00:29:20,769 --> 00:29:24,969
confirmation right and this is a

00:29:22,419 --> 00:29:26,319
battlefield the courier might get shot

00:29:24,969 --> 00:29:28,329
right it's a network right you don't

00:29:26,319 --> 00:29:30,459
trust and so you just can never know

00:29:28,329 --> 00:29:32,949
right and so how do we deal with the you

00:29:30,459 --> 00:29:34,539
know a situation like that well the

00:29:32,949 --> 00:29:37,509
story was really good at handling this

00:29:34,539 --> 00:29:39,549
as the ability to know you know which

00:29:37,509 --> 00:29:41,440
commits happened after which other ones

00:29:39,549 --> 00:29:43,389
you know there's a newer and the process

00:29:41,440 --> 00:29:46,599
and things like that so all they really

00:29:43,389 --> 00:29:48,879
need to do is you both us snapshot of

00:29:46,599 --> 00:29:51,459
freeze and at a given point in time

00:29:48,879 --> 00:29:54,069
across every node it's fairly close

00:29:51,459 --> 00:29:55,599
timestamp wise right now we need to push

00:29:54,069 --> 00:29:56,590
all that data out to some sort of

00:29:55,599 --> 00:29:59,470
storage system

00:29:56,590 --> 00:30:02,889
so what we've done is we created a

00:29:59,470 --> 00:30:05,350
little backup script that is essentially

00:30:02,889 --> 00:30:07,149
going to be invoked by a kubernetes cron

00:30:05,350 --> 00:30:08,379
job so you can set up a cron job and set

00:30:07,149 --> 00:30:10,600
your schedule how often you want to

00:30:08,379 --> 00:30:12,519
snapshot every 5 min incidents may be

00:30:10,600 --> 00:30:15,999
really critical data every half an hour

00:30:12,519 --> 00:30:19,210
once a day whatever you'd like and that

00:30:15,999 --> 00:30:21,070
script in parallel is when you hit every

00:30:19,210 --> 00:30:23,799
single node at the exact same moment say

00:30:21,070 --> 00:30:25,360
snapshot now the problem is if you

00:30:23,799 --> 00:30:26,889
snatched out all the notes at once and

00:30:25,360 --> 00:30:29,379
have them all start loading into a

00:30:26,889 --> 00:30:32,619
objects store you're going to crush your

00:30:29,379 --> 00:30:34,899
network and so part of the puzzle is

00:30:32,619 --> 00:30:36,669
this delay here which is going to be

00:30:34,899 --> 00:30:39,039
applied incrementally based on the

00:30:36,669 --> 00:30:41,110
staples that's horrible so if your node

00:30:39,039 --> 00:30:42,639
0 and the staple set you go right away

00:30:41,110 --> 00:30:44,499
if you're known why I multiply that

00:30:42,639 --> 00:30:47,110
number by 1 that's going to seconds wait

00:30:44,499 --> 00:30:48,490
before you start out of them again that

00:30:47,110 --> 00:30:51,190
can be more sophisticated it can

00:30:48,490 --> 00:30:53,379
actually monitor our activity on the you

00:30:51,190 --> 00:30:54,999
know s3 store or whatever and say hey

00:30:53,379 --> 00:30:56,619
these wait to be skies are actually done

00:30:54,999 --> 00:30:58,480
before you start offloading is there's

00:30:56,619 --> 00:30:59,919
other things that you can do but this is

00:30:58,480 --> 00:31:02,350
kind of a nice happy medium it's a

00:30:59,919 --> 00:31:04,179
simple solution easy to monitor and so

00:31:02,350 --> 00:31:05,950
when the cluster backs up really this is

00:31:04,179 --> 00:31:07,990
deep-ass so how many of these clusters

00:31:05,950 --> 00:31:08,350
there are it can be 50 of them or

00:31:07,990 --> 00:31:11,230
something

00:31:08,350 --> 00:31:14,350
and so you specify your objects or write

00:31:11,230 --> 00:31:16,029
this case s3 Amazon that you specify the

00:31:14,350 --> 00:31:18,730
bucket that you want to go to so rocks

00:31:16,029 --> 00:31:20,590
path you specify the name of the cluster

00:31:18,730 --> 00:31:21,970
you actually don't that's going to come

00:31:20,590 --> 00:31:23,679
automatically from the plus when you

00:31:21,970 --> 00:31:24,909
prayed my keys in the chart you give it

00:31:23,679 --> 00:31:27,279
a name where it's going to generate one

00:31:24,909 --> 00:31:29,350
for you so you know this is dishonor a

00:31:27,279 --> 00:31:31,960
zero one that's the specific cluster to

00:31:29,350 --> 00:31:36,369
be fit then you know their abilities of

00:31:31,960 --> 00:31:38,710
ah if you back up and node and there's

00:31:36,369 --> 00:31:40,929
two copies of this data and that's the

00:31:38,710 --> 00:31:42,429
guy from availability zone beat you

00:31:40,929 --> 00:31:45,369
don't want to restore it to an

00:31:42,429 --> 00:31:46,899
availability zone eight you're back in

00:31:45,369 --> 00:31:48,879
this masking boat where they goes down

00:31:46,899 --> 00:31:52,749
you lose all your data right you have to

00:31:48,879 --> 00:31:54,279
know what the AC is and then finally

00:31:52,749 --> 00:31:56,019
you've got to know that you can sign her

00:31:54,279 --> 00:31:58,059
that's a good bid right and then we just

00:31:56,019 --> 00:32:00,610
put the data in there now the tricky

00:31:58,059 --> 00:32:03,220
thing is you can copy the whole data set

00:32:00,610 --> 00:32:05,289
of a you know multi terabyte storage

00:32:03,220 --> 00:32:07,270
on a regular basis is going to be too

00:32:05,289 --> 00:32:10,659
expensive so what you really want to do

00:32:07,270 --> 00:32:12,580
is just copy the dip and the beautiful

00:32:10,659 --> 00:32:15,429
thing about Cassandra and rocks is that

00:32:12,580 --> 00:32:18,669
they both are capable when you snapshot

00:32:15,429 --> 00:32:20,169
them they're going to create SST type

00:32:18,669 --> 00:32:22,450
files right they're going to create

00:32:20,169 --> 00:32:24,549
these static files that never change

00:32:22,450 --> 00:32:26,320
now these files can go out of date right

00:32:24,549 --> 00:32:28,270
if you become whole there's lots of

00:32:26,320 --> 00:32:30,190
tombstones in where the deletions have

00:32:28,270 --> 00:32:32,380
been flagged but the data is still there

00:32:30,190 --> 00:32:33,909
and you compact that and once in a while

00:32:32,380 --> 00:32:35,440
right so compaction is another big

00:32:33,909 --> 00:32:38,049
expense you gotta watch out for news

00:32:35,440 --> 00:32:40,179
databases but the beauty is that those

00:32:38,049 --> 00:32:41,830
SS T's once created will never change

00:32:40,179 --> 00:32:44,350
so when you get a snapshot with a memory

00:32:41,830 --> 00:32:46,270
splash you got your SSD files you can

00:32:44,350 --> 00:32:47,950
know that those things can be safely

00:32:46,270 --> 00:32:52,049
backed up as they can't be affected it

00:32:47,950 --> 00:32:55,630
can't be you know changed and so the

00:32:52,049 --> 00:32:58,120
solution is it knows which SS T's it's

00:32:55,630 --> 00:33:00,250
already uploaded and it only unloads the

00:32:58,120 --> 00:33:02,140
new ones and so each snapshot has a

00:33:00,250 --> 00:33:04,360
bunch of part of links so the actual

00:33:02,140 --> 00:33:06,520
real files in the table directory but

00:33:04,360 --> 00:33:07,900
you can tell from the table directory

00:33:06,520 --> 00:33:09,730
which ones you've already got you just

00:33:07,900 --> 00:33:12,669
create the new ones so it's essentially

00:33:09,730 --> 00:33:17,169
an incremental backup solution that is

00:33:12,669 --> 00:33:20,409
pretty efficient so a recovery side this

00:33:17,169 --> 00:33:24,970
is an interesting one so imagine no one

00:33:20,409 --> 00:33:27,000
goes down and it's not coming back what

00:33:24,970 --> 00:33:30,159
you need to do is you need to be able to

00:33:27,000 --> 00:33:34,419
need to be able to say okay I want to

00:33:30,159 --> 00:33:37,840
bring this guy back and I want it to you

00:33:34,419 --> 00:33:39,370
know recover in the proper way so all

00:33:37,840 --> 00:33:41,260
you actually need to do in this solution

00:33:39,370 --> 00:33:43,690
is you need to create recovery files

00:33:41,260 --> 00:33:45,159
you're going to get three storage system

00:33:43,690 --> 00:33:48,549
you create a recovery file that's

00:33:45,159 --> 00:33:51,490
specifies which no you want to restore

00:33:48,549 --> 00:33:53,260
and that's it and then when you create a

00:33:51,490 --> 00:33:54,909
new node and you do this by scaling the

00:33:53,260 --> 00:33:57,730
staple salary and you despite the

00:33:54,909 --> 00:33:59,200
leading the PV or the guy that failed so

00:33:57,730 --> 00:34:01,960
the key asking if we created somewhere

00:33:59,200 --> 00:34:04,000
else in cluster either way when the new

00:34:01,960 --> 00:34:05,470
guy starts up as a special container in

00:34:04,000 --> 00:34:07,000
our kubernetes pot called the unit

00:34:05,470 --> 00:34:09,099
container that's going to go in the book

00:34:07,000 --> 00:34:10,359
to see if there's a restore job so the

00:34:09,099 --> 00:34:11,950
first thing he doesn't he looks like in

00:34:10,359 --> 00:34:14,210
his disk consents another negative

00:34:11,950 --> 00:34:16,520
manner no okay

00:34:14,210 --> 00:34:17,960
Brandon no just adding more capacity or

00:34:16,520 --> 00:34:19,700
I'm supposed to be recovering for

00:34:17,960 --> 00:34:21,859
somebody so then he goes here he says

00:34:19,700 --> 00:34:23,839
her recovery job yes there is great

00:34:21,859 --> 00:34:26,570
downloads all that data and then when

00:34:23,839 --> 00:34:28,129
the data is local it kicks off Cassandra

00:34:26,570 --> 00:34:30,560
like nothing ever happened he rejoins

00:34:28,129 --> 00:34:32,389
the plus verb has got a new IP but that

00:34:30,560 --> 00:34:33,589
just takes a you know 30 seconds or so

00:34:32,389 --> 00:34:35,570
didn't go she ate with everybody else

00:34:33,589 --> 00:34:38,119
hey I changed my ID sorry but I'm back

00:34:35,570 --> 00:34:42,409
and everything stick together so that's

00:34:38,119 --> 00:34:43,760
kind of the recovery side the tweaks you

00:34:42,409 --> 00:34:45,109
mentioned misses an inch that's Peter

00:34:43,760 --> 00:34:46,849
where also there's a custom decision

00:34:45,109 --> 00:34:48,800
provider when you Cassandra cluster

00:34:46,849 --> 00:34:50,990
starts out like if you added a new node

00:34:48,800 --> 00:34:52,399
how does it know which plus grid

00:34:50,990 --> 00:34:53,990
supposed to join there could be 40 of

00:34:52,399 --> 00:34:55,790
them out there so there's got to be some

00:34:53,990 --> 00:34:57,800
way of telling that guy hey these guys

00:34:55,790 --> 00:34:59,810
are like Keystone Miller's these are the

00:34:57,800 --> 00:35:01,400
special nodes that are never going to

00:34:59,810 --> 00:35:03,980
change and in the staple set when you

00:35:01,400 --> 00:35:05,780
scale it up nodes have an ordinal that

00:35:03,980 --> 00:35:07,280
never change the IP could change when

00:35:05,780 --> 00:35:09,830
they move around but the ordinal never

00:35:07,280 --> 00:35:12,470
changes because cyber 0 1 - 0 is always

00:35:09,830 --> 00:35:14,660
the sunderland Esther that's its

00:35:12,470 --> 00:35:19,760
identity that this data is associated

00:35:14,660 --> 00:35:22,190
with that thing right and so 0 1 2 3 4 5

00:35:19,760 --> 00:35:24,440
6 scaling up when you scale down it goes

00:35:22,190 --> 00:35:27,020
the opposite way 6 5 4

00:35:24,440 --> 00:35:29,270
so the most stable elements of a staple

00:35:27,020 --> 00:35:31,430
setting who raises your 1 2 & 3

00:35:29,270 --> 00:35:33,290
and so we got accustomed seat provider

00:35:31,430 --> 00:35:35,839
that talks to you know the kubernetes

00:35:33,290 --> 00:35:37,940
talk figures out exactly which of these

00:35:35,839 --> 00:35:40,070
guys are you know those people set sure

00:35:37,940 --> 00:35:41,839
once you've read the current set and it

00:35:40,070 --> 00:35:43,339
provides that the consumer and that's

00:35:41,839 --> 00:35:45,230
how the know this is going to cluster so

00:35:43,339 --> 00:35:47,480
again it's this subtle but very

00:35:45,230 --> 00:35:49,460
lightweight blue between the kubernetes

00:35:47,480 --> 00:35:52,490
piece of the platform and the designer

00:35:49,460 --> 00:35:56,210
piece of the Bible so a little bit of

00:35:52,490 --> 00:35:59,300
performance in testing the solution with

00:35:56,210 --> 00:36:02,720
with the Roxanne we're back in 10

00:35:59,300 --> 00:36:06,500
only 10 kind of surprisingly 10 notes 10

00:36:02,720 --> 00:36:09,050
instances I create extra larges which

00:36:06,500 --> 00:36:12,020
have 32 cores 24 dudes are very

00:36:09,050 --> 00:36:14,720
important be giving disks will give you

00:36:12,020 --> 00:36:18,020
a million reads per second that's pretty

00:36:14,720 --> 00:36:20,119
good for 10 nodes right 100,000 bees per

00:36:18,020 --> 00:36:23,030
second scaling linearly up to a million

00:36:20,119 --> 00:36:24,740
with 10 notes and you keep talking at

00:36:23,030 --> 00:36:26,100
lunch actually one of the experiments we

00:36:24,740 --> 00:36:28,800
still have all the plates

00:36:26,100 --> 00:36:30,090
these are big nodes and they're bang for

00:36:28,800 --> 00:36:33,300
the buck there they're more expensive

00:36:30,090 --> 00:36:35,280
than like a 16 for box and Cassandra

00:36:33,300 --> 00:36:37,590
actually really likes to bomb smaller

00:36:35,280 --> 00:36:39,750
boxes because every box you've created

00:36:37,590 --> 00:36:42,030
has more i/o right there's more part

00:36:39,750 --> 00:36:44,310
concurrency more parallelism less

00:36:42,030 --> 00:36:46,290
bottlenecks and Plus you know contention

00:36:44,310 --> 00:36:49,590
and things like that and since it's such

00:36:46,290 --> 00:36:52,350
a great you know scale out solution let

00:36:49,590 --> 00:36:54,660
us get we took this and turned it into

00:36:52,350 --> 00:36:56,580
eight or 16 core boxes which would be

00:36:54,660 --> 00:36:59,370
cheaper Bank for electricity you eyes on

00:36:56,580 --> 00:37:01,920
Amazon and then we double or quadruple

00:36:59,370 --> 00:37:03,510
the number of boxes would we get the

00:37:01,920 --> 00:37:06,090
same performance or would we get more

00:37:03,510 --> 00:37:07,500
right so my supposition is that only a

00:37:06,090 --> 00:37:09,510
hypothesis and then trying to get what

00:37:07,500 --> 00:37:11,640
we're going to work on this new 20% less

00:37:09,510 --> 00:37:14,310
money or 10% less money you know at

00:37:11,640 --> 00:37:16,070
Amazon and maybe 10 or 15% better

00:37:14,310 --> 00:37:18,330
performance because we've got more i/o

00:37:16,070 --> 00:37:19,890
and the scale out and that's you know

00:37:18,330 --> 00:37:20,370
one of the big bottom max memory of your

00:37:19,890 --> 00:37:22,020
heads

00:37:20,370 --> 00:37:23,670
so we'll see but at the end of the day

00:37:22,020 --> 00:37:26,010
you heard in case you're talking about

00:37:23,670 --> 00:37:29,310
how important that key 99x right you've

00:37:26,010 --> 00:37:34,440
got a million clients one percent of

00:37:29,310 --> 00:37:37,260
them is 10,000 people 10,000 people are

00:37:34,440 --> 00:37:39,720
going to experience this right p99 lakh

00:37:37,260 --> 00:37:41,880
see see your ears oh yeah my average

00:37:39,720 --> 00:37:43,710
lack sees you know two milliseconds in

00:37:41,880 --> 00:37:45,330
this you know terabyte and a half

00:37:43,710 --> 00:37:49,140
environment that's great

00:37:45,330 --> 00:37:50,760
yeah it's not because a full 10,000 of

00:37:49,140 --> 00:37:52,860
those million people really million

00:37:50,760 --> 00:37:55,680
transactions are going to get with 12

00:37:52,860 --> 00:37:57,960
milliseconds of last and in the p99

00:37:55,680 --> 00:37:59,940
mounting case where they smack into a

00:37:57,960 --> 00:38:02,880
garbage collection no you're in you're

00:37:59,940 --> 00:38:04,620
in a really bad way right 263

00:38:02,880 --> 00:38:05,820
milliseconds you know that's a quarter

00:38:04,620 --> 00:38:08,850
of a second that's a pretty serious

00:38:05,820 --> 00:38:11,370
experience hit over two and then you

00:38:08,850 --> 00:38:13,530
will get B switch to rock Sandra right

00:38:11,370 --> 00:38:14,880
using a key value store sure you don't

00:38:13,530 --> 00:38:16,740
have all the features but if you just

00:38:14,880 --> 00:38:19,170
need that functionality white paper all

00:38:16,740 --> 00:38:21,930
the extra and you're in this mode where

00:38:19,170 --> 00:38:22,610
you have a much more predictable kind of

00:38:21,930 --> 00:38:25,800
p99

00:38:22,610 --> 00:38:27,450
so that's uh that's that so the solution

00:38:25,800 --> 00:38:29,640
there's a bunch of artifacts there's the

00:38:27,450 --> 00:38:31,800
cops employment there's the metric

00:38:29,640 --> 00:38:33,570
solution there's chart and employees all

00:38:31,800 --> 00:38:35,130
the stuff there's the C provide our

00:38:33,570 --> 00:38:36,780
students know me a stitch there's the

00:38:35,130 --> 00:38:38,710
container bills there's the backup and

00:38:36,780 --> 00:38:56,910
restore strips

00:38:38,710 --> 00:38:56,910
and yeah okay

00:40:36,790 --> 00:40:39,599
Asher

00:42:02,569 --> 00:42:58,840
with the weight beware yes the audacity

00:42:57,089 --> 00:43:05,410
and we're faster

00:42:58,840 --> 00:43:13,120
we managed our access pattern but we're

00:43:05,410 --> 00:43:14,740
gonna feet should be actually full table

00:43:13,120 --> 00:43:17,200
scans not good in the database like that

00:43:14,740 --> 00:43:18,250
but but as far as dealing it's kind of

00:43:17,200 --> 00:43:20,470
an interesting thing is that the

00:43:18,250 --> 00:43:23,110
Cassandra layer in this is the cluster

00:43:20,470 --> 00:43:24,940
in spots right so when you do a query

00:43:23,110 --> 00:43:26,470
and it kids one of the Cassandra nodes

00:43:24,940 --> 00:43:29,710
any of the records there's no master

00:43:26,470 --> 00:43:32,590
they can they're gonna say okay which

00:43:29,710 --> 00:43:35,020
partition is that in and where is that

00:43:32,590 --> 00:43:37,090
partition and then they send a query to

00:43:35,020 --> 00:43:38,830
that guy or if it's all of them and they

00:43:37,090 --> 00:43:40,960
all get a query they produce results the

00:43:38,830 --> 00:43:43,000
coordinator then organizes the data so

00:43:40,960 --> 00:43:44,830
these are the simplest of queries right

00:43:43,000 --> 00:43:46,360
it's like here's a key okay the

00:43:44,830 --> 00:43:48,340
coordinator says it's in that partition

00:43:46,360 --> 00:43:50,110
give me the data that guy goes and grabs

00:43:48,340 --> 00:43:51,670
that you know uses the polly filter or

00:43:50,110 --> 00:43:54,040
whatever down in the lowest layer to get

00:43:51,670 --> 00:43:55,660
that data out of the the disparate in

00:43:54,040 --> 00:43:57,580
memory and it returns it back on the

00:43:55,660 --> 00:43:59,740
coordinator sends it out so you'd expect

00:43:57,580 --> 00:44:01,060
these operations to be fast but that's

00:43:59,740 --> 00:44:02,230
why you're getting wet you know this is

00:44:01,060 --> 00:44:04,780
what this solution you're getting crazy

00:44:02,230 --> 00:44:06,900
scaling million queries a second pretty

00:44:04,780 --> 00:44:10,450
good

00:44:06,900 --> 00:44:12,520
can you briefly explain and how the

00:44:10,450 --> 00:44:15,060
switching between rocks and consign girl

00:44:12,520 --> 00:44:25,630
work so I did the reason it's there

00:44:15,060 --> 00:44:28,510
collection oh so it's still there on all

00:44:25,630 --> 00:44:32,200
the Cassandra sign up defenses but

00:44:28,510 --> 00:44:34,900
because Lukas Sagra stuff is it as a lot

00:44:32,200 --> 00:44:36,760
a lot of it Street provisions and so

00:44:34,900 --> 00:44:37,930
once all the caches and everything be

00:44:36,760 --> 00:44:41,740
warmed up there's not a lot of activity

00:44:37,930 --> 00:44:43,990
there sir but basically you know Jane I

00:44:41,740 --> 00:44:45,670
request from the Cassandra mean to down

00:44:43,990 --> 00:44:48,310
to the storage engine and so the

00:44:45,670 --> 00:44:50,230
Instagram guys went in Cassandra Witcher

00:44:48,310 --> 00:44:53,320
reticulum wasn't really designed to be

00:44:50,230 --> 00:44:55,420
pluggable storage wise with but it's not

00:44:53,320 --> 00:44:57,130
unprecedented right my sequel allows I

00:44:55,420 --> 00:44:59,350
know key and ice and all these different

00:44:57,130 --> 00:45:01,150
plugins and so they just took that same

00:44:59,350 --> 00:45:03,250
methodology and came to Simon said

00:45:01,150 --> 00:45:05,350
what's the stuff that's cluster

00:45:03,250 --> 00:45:07,599
smarts about figuring out me to talk to

00:45:05,350 --> 00:45:09,700
you at what's the stuff it's disks

00:45:07,599 --> 00:45:12,279
months about how to be a smart disk

00:45:09,700 --> 00:45:15,999
cache let's draw the line there and then

00:45:12,279 --> 00:45:17,920
they basically created an API and then

00:45:15,999 --> 00:45:20,049
they took that same thing because

00:45:17,920 --> 00:45:21,430
Windows with you when you call Cassandra

00:45:20,049 --> 00:45:23,829
it's getting ready to get the storage

00:45:21,430 --> 00:45:25,359
engine the key space is what matters

00:45:23,829 --> 00:45:27,670
that's not the discriminator and if the

00:45:25,359 --> 00:45:29,259
key space is rocks TV are you gonna find

00:45:27,670 --> 00:45:32,799
me something else that hurts rocks TB

00:45:29,259 --> 00:45:34,809
then the decision logic they added into

00:45:32,799 --> 00:45:36,579
Cassandra will call the rocks engine if

00:45:34,809 --> 00:45:38,739
it's some other key space it'll call the

00:45:36,579 --> 00:45:40,749
Cassandra engine so it just Jameson

00:45:38,739 --> 00:45:42,989
rocks tbo and after that automatically

00:45:40,749 --> 00:45:45,519
equal zero so there's the sunrise you're

00:45:42,989 --> 00:45:48,479
basically taking a lot of Tullio

00:45:45,519 --> 00:46:03,759
day two videos side by leaving this

00:45:48,479 --> 00:46:07,210
connection and changing say you're doing

00:46:03,759 --> 00:46:10,559
a lot of you guys have used its base

00:46:07,210 --> 00:46:14,109
it's beautiful tutorials for iOS phones

00:46:10,559 --> 00:46:17,859
we're going to say call and then guess

00:46:14,109 --> 00:46:20,229
for the matter users that's popular the

00:46:17,859 --> 00:46:23,349
i us cost and actually going on in

00:46:20,229 --> 00:46:28,269
safety case basis Rossi because you know

00:46:23,349 --> 00:46:31,539
Pam forget they look at all that max

00:46:28,269 --> 00:46:35,829
work in broccoli be the Seaport it's not

00:46:31,539 --> 00:46:39,670
like a fish so here's a way what what is

00:46:35,829 --> 00:46:44,200
the down yep good way but it is using

00:46:39,670 --> 00:46:52,950
Java is B minus B that is most of the

00:46:44,200 --> 00:46:52,950
heavy work you see make sense thanks

00:47:02,530 --> 00:47:08,050
every job is very dark

00:47:12,470 --> 00:47:17,110
so just everybody knows we have about 10

00:47:14,960 --> 00:47:21,280
minutes to the transit to the next hop

00:47:17,110 --> 00:47:21,280

YouTube URL: https://www.youtube.com/watch?v=MTz36FUfv_4


