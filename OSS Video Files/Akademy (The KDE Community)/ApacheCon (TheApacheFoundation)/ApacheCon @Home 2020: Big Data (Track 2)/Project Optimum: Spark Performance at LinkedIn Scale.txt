Title: Project Optimum: Spark Performance at LinkedIn Scale
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 2)
Description: 
	Project Optimum: Spark Performance at LinkedIn Scale
Yuval Degani

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

A typical day for Spark at LinkedIn means running 35 million RAM-GB-hours, on top of 7 million CPU-core-hours of 20,000 unique applications. Maintaining predictable performance and SLAs, as-well-as optimizing performance across a complex infrastructure stack and a massive application base, all while keeping up with a 4x YoY workload growth is an immense challenge. Project Optimum addresses those needs by providing a set of performance analysis, reporting, profiling, and regression detection tools designed for a massive-scale production environment. It allows platform developers as-well-as application developers to detect even subtle regressions or improvements in application performance with a limited sample set, using a statistically-rigorous approach that can be automated and integrated into production monitoring systems. In this talk, we will cover how Project Optimum is used at LinkedIn to scale our Spark infrastructure while providing a reliable production environment. We will demonstrate its role as an ad-hoc performance debugging and profiling tool, an automatic regression detection tracking pipeline, and an elaborate reporting system geared towards providing users with insights about their jobs.

Yuval is a Staff Software Engineer at Linkedin, where he is focused on scaling and developing new features for Hadoop and Spark. Before that, Yuval was a Sr. Engineering Manager at Mellanox Technologies, leading a team working on introducing network acceleration technologies to Big Data and Machine Learning frameworks. Prior to his work in the Big Data and AI fields, Yuval was a developer, an architect, and later a team leader in the areas of low-level kernel development for cutting-edge high-performance network devices. Yuval holds a BSc in Computer Science from the Technion Institute of Technology, Israel.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,480 --> 00:00:28,720
hello everyone

00:00:25,840 --> 00:00:30,720
thanks for joining um hope you're having

00:00:28,720 --> 00:00:32,719
a good morning good day

00:00:30,720 --> 00:00:33,920
good night uh from wherever you're

00:00:32,719 --> 00:00:36,960
joining

00:00:33,920 --> 00:00:40,320
uh so my name is yuval degani and uh

00:00:36,960 --> 00:00:41,520
today i'm gonna talk about how we do

00:00:40,320 --> 00:00:44,239
spark performance

00:00:41,520 --> 00:00:46,960
at linkedin scale in our offline

00:00:44,239 --> 00:00:49,840
infrastructure

00:00:46,960 --> 00:00:51,520
so i'll try to be quick because i have a

00:00:49,840 --> 00:00:54,160
lot of things that i want to

00:00:51,520 --> 00:00:56,160
share with you today i'll leave a few

00:00:54,160 --> 00:00:58,719
minutes at the end for q a

00:00:56,160 --> 00:01:01,359
so please uh hold on uh if you're

00:00:58,719 --> 00:01:01,359
waiting for that

00:01:03,840 --> 00:01:09,200
all right um can

00:01:07,119 --> 00:01:13,840
uh everyone hear me okay can someone

00:01:09,200 --> 00:01:13,840
post in the chat just to be sure

00:01:14,000 --> 00:01:20,080
awesome all right so let me get started

00:01:17,200 --> 00:01:21,600
uh so my name is yuval i'm part of the

00:01:20,080 --> 00:01:22,640
linkedin's data infrastructure

00:01:21,600 --> 00:01:26,479
organization

00:01:22,640 --> 00:01:27,119
uh focused only mostly on scaling spark

00:01:26,479 --> 00:01:30,720
and hadoop

00:01:27,119 --> 00:01:32,400
in our offline infra um i have a pretty

00:01:30,720 --> 00:01:33,520
long history of performance work in the

00:01:32,400 --> 00:01:36,159
big data

00:01:33,520 --> 00:01:36,880
ecosystem in general i used to work on

00:01:36,159 --> 00:01:39,280
uh

00:01:36,880 --> 00:01:40,079
hardware accelerations using rdma and

00:01:39,280 --> 00:01:42,399
different

00:01:40,079 --> 00:01:43,600
networking technologies in the cloud and

00:01:42,399 --> 00:01:47,200
on-prem

00:01:43,600 --> 00:01:51,040
to accelerate spark and hdfs

00:01:47,200 --> 00:01:51,040
in my earlier

00:01:51,119 --> 00:01:54,880
experience i used to work on kernel

00:01:52,880 --> 00:01:58,399
level and hardware accelerations

00:01:54,880 --> 00:01:58,880
uh on kind of a lower level side of

00:01:58,399 --> 00:02:02,000
things

00:01:58,880 --> 00:02:05,040
in drivers and so on

00:02:02,000 --> 00:02:06,320
um so a little bit about linkedin i

00:02:05,040 --> 00:02:08,640
suppose

00:02:06,320 --> 00:02:10,640
a lot of you are familiar but uh the

00:02:08,640 --> 00:02:13,280
economic graph is basically our

00:02:10,640 --> 00:02:14,480
biggest product uh you can think of it

00:02:13,280 --> 00:02:16,400
as what we call a

00:02:14,480 --> 00:02:17,760
digital representation of the global

00:02:16,400 --> 00:02:20,640
economy so

00:02:17,760 --> 00:02:21,280
our offline infra as well as our online

00:02:20,640 --> 00:02:24,879
infra

00:02:21,280 --> 00:02:25,840
is uh basically uh allocated with the

00:02:24,879 --> 00:02:29,280
job of

00:02:25,840 --> 00:02:31,680
serving uh 700 million members

00:02:29,280 --> 00:02:33,519
uh along with all the jobs companies

00:02:31,680 --> 00:02:34,879
skills and schools that are tracked

00:02:33,519 --> 00:02:37,200
within linkedin

00:02:34,879 --> 00:02:39,120
uh so it's can you as you can imagine

00:02:37,200 --> 00:02:40,080
that there's a massive amount of data

00:02:39,120 --> 00:02:42,640
that we need to

00:02:40,080 --> 00:02:43,680
deal with and also keep resource

00:02:42,640 --> 00:02:47,599
utilization

00:02:43,680 --> 00:02:48,319
uh efficient a little bit about spark at

00:02:47,599 --> 00:02:51,760
linkedin

00:02:48,319 --> 00:02:52,879
so this is changing daily uh i can say

00:02:51,760 --> 00:02:55,360
that the first time that i

00:02:52,879 --> 00:02:56,480
wrote this slide a couple of months back

00:02:55,360 --> 00:02:58,879
it was a little different

00:02:56,480 --> 00:02:59,760
but these are the latest numbers uh so

00:02:58,879 --> 00:03:02,720
we have

00:02:59,760 --> 00:03:04,080
north of 10 000 nodes uh in our offline

00:03:02,720 --> 00:03:07,120
infra about 30

00:03:04,080 --> 00:03:09,840
000 daily spark applications running um

00:03:07,120 --> 00:03:12,159
and uh shuffle data is pretty impressive

00:03:09,840 --> 00:03:15,280
are we talking about 10 petabytes a day

00:03:12,159 --> 00:03:18,319
and then uh this is following growth of

00:03:15,280 --> 00:03:20,000
about 3x year-over-year in terms of uh

00:03:18,319 --> 00:03:20,879
the daily spark applications that we

00:03:20,000 --> 00:03:25,120
have

00:03:20,879 --> 00:03:27,440
so uh it's not optional for us to

00:03:25,120 --> 00:03:28,799
figure out how to scale performance uh

00:03:27,440 --> 00:03:30,799
for spark at linkedin

00:03:28,799 --> 00:03:32,319
it is crucial for us to be able to keep

00:03:30,799 --> 00:03:34,879
up with the growth that we're

00:03:32,319 --> 00:03:37,920
experiencing

00:03:34,879 --> 00:03:40,400
um a little bit more about spark at

00:03:37,920 --> 00:03:42,720
linkedin so we have about 3 000

00:03:40,400 --> 00:03:43,760
internal spark users it's a pretty

00:03:42,720 --> 00:03:45,920
massive

00:03:43,760 --> 00:03:47,120
amount of people that we need to support

00:03:45,920 --> 00:03:49,519
and uh

00:03:47,120 --> 00:03:50,640
this is where it comes in where we would

00:03:49,519 --> 00:03:54,319
want to provide

00:03:50,640 --> 00:03:55,200
self-serving uh tools for users to be

00:03:54,319 --> 00:03:58,000
able to

00:03:55,200 --> 00:03:58,879
you know debug and improve performance

00:03:58,000 --> 00:04:02,080
on their own

00:03:58,879 --> 00:04:04,720
and address inefficiencies in their jobs

00:04:02,080 --> 00:04:06,159
we have a very diversified ecosystem for

00:04:04,720 --> 00:04:08,720
those spark users

00:04:06,159 --> 00:04:10,560
some of them are dealing with metrics ai

00:04:08,720 --> 00:04:12,080
machine learning models

00:04:10,560 --> 00:04:14,159
and the traditional data science and

00:04:12,080 --> 00:04:17,199
data warehouse

00:04:14,159 --> 00:04:21,040
business intelligence and so on

00:04:17,199 --> 00:04:23,840
um so what is project optimum um

00:04:21,040 --> 00:04:26,560
first of all as most of you already know

00:04:23,840 --> 00:04:28,960
scaling spark is very hard

00:04:26,560 --> 00:04:31,120
and this is where we come in and try to

00:04:28,960 --> 00:04:32,000
uh alleviate those problems and kind of

00:04:31,120 --> 00:04:34,479
make it easier

00:04:32,000 --> 00:04:36,000
for users to uh you know make their jobs

00:04:34,479 --> 00:04:39,280
more efficient

00:04:36,000 --> 00:04:41,199
so the motivation is uh uh

00:04:39,280 --> 00:04:43,280
three things that i can you know kind of

00:04:41,199 --> 00:04:45,280
think about is like a global idea that

00:04:43,280 --> 00:04:47,440
we need to consider so first of all

00:04:45,280 --> 00:04:49,040
uh stability is a very important thing

00:04:47,440 --> 00:04:51,840
that we want to introduce into the

00:04:49,040 --> 00:04:53,600
system uh we want to have a gatekeeper

00:04:51,840 --> 00:04:55,520
where we could catch performance

00:04:53,600 --> 00:04:56,080
degradation and in-front applications

00:04:55,520 --> 00:04:57,440
before

00:04:56,080 --> 00:04:59,199
they are actually deployed into

00:04:57,440 --> 00:05:00,960
production

00:04:59,199 --> 00:05:03,440
and also we would like to have early

00:05:00,960 --> 00:05:03,840
warning for cluster performance issues

00:05:03,440 --> 00:05:05,919
and not

00:05:03,840 --> 00:05:07,840
have to wait until you know a massive

00:05:05,919 --> 00:05:10,160
amount of jobs needs to run before we

00:05:07,840 --> 00:05:11,680
see problems popping up

00:05:10,160 --> 00:05:13,680
for productivity we would like to

00:05:11,680 --> 00:05:15,600
provide a quick and iterative

00:05:13,680 --> 00:05:17,600
uh experimentation platform for

00:05:15,600 --> 00:05:18,800
validating code or config changes in

00:05:17,600 --> 00:05:21,919
applications

00:05:18,800 --> 00:05:23,280
uh when users are developing or

00:05:21,919 --> 00:05:25,440
trying to improve the performance of

00:05:23,280 --> 00:05:27,440
their existing applications

00:05:25,440 --> 00:05:29,039
um so we would like to provide tooling

00:05:27,440 --> 00:05:30,800
to accelerate development of new

00:05:29,039 --> 00:05:32,479
infra performance features so not only

00:05:30,800 --> 00:05:33,520
workflows but also from the infra

00:05:32,479 --> 00:05:36,000
perspective

00:05:33,520 --> 00:05:37,440
how do we improve things and make sure

00:05:36,000 --> 00:05:38,880
that we are actually improving and not

00:05:37,440 --> 00:05:41,680
degrading

00:05:38,880 --> 00:05:43,520
uh predictability uh we would want to

00:05:41,680 --> 00:05:44,800
surface the currently observed metrics

00:05:43,520 --> 00:05:45,840
and trends that are going on in the

00:05:44,800 --> 00:05:48,000
cluster right now

00:05:45,840 --> 00:05:50,160
so that application owners can then

00:05:48,000 --> 00:05:54,639
validate against that and better plan

00:05:50,160 --> 00:05:54,639
uh to meet their slas of their workflows

00:05:55,199 --> 00:05:59,360
so in terms of the ecosystem that we're

00:05:57,199 --> 00:06:02,000
dealing here uh dealing with here

00:05:59,360 --> 00:06:03,280
um so we have the application side of

00:06:02,000 --> 00:06:06,160
things which could be

00:06:03,280 --> 00:06:07,840
any kind of schedule jobs or ad hoc jobs

00:06:06,160 --> 00:06:09,600
experimentation and so on and what we

00:06:07,840 --> 00:06:13,360
call the golden flows which we'll

00:06:09,600 --> 00:06:15,840
uh talk about later uh all those go

00:06:13,360 --> 00:06:17,520
into our yarn clusters uh so mostly

00:06:15,840 --> 00:06:19,120
running hadoop and spark jobs a little

00:06:17,520 --> 00:06:22,639
bit of hive and so on

00:06:19,120 --> 00:06:23,440
and then those are basically emitting

00:06:22,639 --> 00:06:26,080
metrics

00:06:23,440 --> 00:06:28,400
different types of metrics that we later

00:06:26,080 --> 00:06:32,880
use in order to analyze performance

00:06:28,400 --> 00:06:35,199
so think of it as yarn resource manager

00:06:32,880 --> 00:06:36,560
metrics we have hdfs client side metrics

00:06:35,199 --> 00:06:39,280
spark metrics

00:06:36,560 --> 00:06:41,440
and also cpu profiling all those are

00:06:39,280 --> 00:06:42,240
persisted and provided to the analysis

00:06:41,440 --> 00:06:46,479
data

00:06:42,240 --> 00:06:48,080
layer where grid bench data set comes in

00:06:46,479 --> 00:06:50,479
and we'll talk about grid bench later

00:06:48,080 --> 00:06:54,720
this is kind of the main topic of this

00:06:50,479 --> 00:06:56,160
uh session um and those analysis tools

00:06:54,720 --> 00:06:58,639
enable us to create

00:06:56,160 --> 00:06:59,680
reports so those could be ad hoc reports

00:06:58,639 --> 00:07:02,960
that we can

00:06:59,680 --> 00:07:06,560
make protocol and then share and persist

00:07:02,960 --> 00:07:08,560
and kind of document regulate dashboards

00:07:06,560 --> 00:07:10,000
in real time so we can

00:07:08,560 --> 00:07:11,919
figure out what's going on in the

00:07:10,000 --> 00:07:13,360
cluster across different types of jobs

00:07:11,919 --> 00:07:14,800
and different types of metrics that we

00:07:13,360 --> 00:07:17,759
care about

00:07:14,800 --> 00:07:20,000
and also reporting the variance in jobs

00:07:17,759 --> 00:07:20,800
across time so we can see if something

00:07:20,000 --> 00:07:23,840
is going

00:07:20,800 --> 00:07:24,560
uh in a uh not a very good direction in

00:07:23,840 --> 00:07:26,479
terms of

00:07:24,560 --> 00:07:28,800
uh the let's say the delay that we're

00:07:26,479 --> 00:07:32,479
seeing or uh you know any kind of other

00:07:28,800 --> 00:07:34,479
interference in metrics um the other

00:07:32,479 --> 00:07:36,080
the last thing is automation so once we

00:07:34,479 --> 00:07:38,400
have all those reports and allow

00:07:36,080 --> 00:07:38,800
analysis that enable us to come up with

00:07:38,400 --> 00:07:41,199
a

00:07:38,800 --> 00:07:42,400
a b testing uh platform so that you can

00:07:41,199 --> 00:07:44,479
experiment on

00:07:42,400 --> 00:07:47,199
different configs or different types of

00:07:44,479 --> 00:07:49,840
uh different coding options and so on

00:07:47,199 --> 00:07:51,840
uh and also alerting uh with the

00:07:49,840 --> 00:07:53,039
dashboards and all those uh metrics

00:07:51,840 --> 00:07:54,879
being populated

00:07:53,039 --> 00:07:56,720
uh we can set up alerts to catch these

00:07:54,879 --> 00:08:00,160
kind of issues uh in a pretty

00:07:56,720 --> 00:08:01,919
early state so there are three

00:08:00,160 --> 00:08:03,759
pillars on how we address all those

00:08:01,919 --> 00:08:05,120
things uh first of all there's the

00:08:03,759 --> 00:08:06,879
metric pipelines

00:08:05,120 --> 00:08:09,360
uh then we'll talk about grid bench like

00:08:06,879 --> 00:08:12,960
i mentioned and the golden flows

00:08:09,360 --> 00:08:15,120
uh let's start with the metric pipelines

00:08:12,960 --> 00:08:16,400
so where all those metrics are actually

00:08:15,120 --> 00:08:19,680
coming from right now

00:08:16,400 --> 00:08:21,520
so we have four main producers of

00:08:19,680 --> 00:08:22,560
metrics that we consume in order to

00:08:21,520 --> 00:08:25,520
provide

00:08:22,560 --> 00:08:27,520
performance analysis so the first one is

00:08:25,520 --> 00:08:28,479
the spark history server which i suppose

00:08:27,520 --> 00:08:31,440
most of you are

00:08:28,479 --> 00:08:32,800
uh very familiar with uh so the

00:08:31,440 --> 00:08:34,560
application metrics

00:08:32,800 --> 00:08:35,839
through spark h3 server are exposed to

00:08:34,560 --> 00:08:37,760
rest api

00:08:35,839 --> 00:08:38,959
and then those are basically available

00:08:37,760 --> 00:08:41,760
in real time or

00:08:38,959 --> 00:08:43,760
near real time depend on how long it

00:08:41,760 --> 00:08:45,120
takes to actually process the logs for a

00:08:43,760 --> 00:08:48,240
particular job

00:08:45,120 --> 00:08:51,600
and it gives us a pair job per stage

00:08:48,240 --> 00:08:54,000
peer task and per executor metrics

00:08:51,600 --> 00:08:55,600
the resource manager these are yarn

00:08:54,000 --> 00:08:56,480
metrics that are emitted to kafka

00:08:55,600 --> 00:08:58,720
through flume

00:08:56,480 --> 00:09:00,640
and then provide us with an idea of the

00:08:58,720 --> 00:09:02,480
resource consumption per job and then

00:09:00,640 --> 00:09:06,160
the scheduling delays

00:09:02,480 --> 00:09:07,920
for hdfs we have dfs client

00:09:06,160 --> 00:09:09,920
metrics that are emitted to kafka as

00:09:07,920 --> 00:09:11,920
well directly from the executors

00:09:09,920 --> 00:09:14,320
and those kind of expose the blocking

00:09:11,920 --> 00:09:16,480
times for i o and metadata ops

00:09:14,320 --> 00:09:18,240
kind of completing the picture also into

00:09:16,480 --> 00:09:21,200
the storage domain

00:09:18,240 --> 00:09:22,560
and then for profiling we have cpu flame

00:09:21,200 --> 00:09:23,440
graphs that are generated by each

00:09:22,560 --> 00:09:25,279
executor

00:09:23,440 --> 00:09:26,880
uh this is kind of optional it's up to

00:09:25,279 --> 00:09:27,680
the user if they want to configure their

00:09:26,880 --> 00:09:30,160
job to pre

00:09:27,680 --> 00:09:31,680
to produce those uh those are persisted

00:09:30,160 --> 00:09:34,000
to hdfs and then

00:09:31,680 --> 00:09:37,839
uh it could be accessed right after the

00:09:34,000 --> 00:09:40,640
job has finished running

00:09:37,839 --> 00:09:41,279
so the first idea about the metrics uh

00:09:40,640 --> 00:09:42,959
pipelines

00:09:41,279 --> 00:09:45,519
is for us to be able to liberate the

00:09:42,959 --> 00:09:47,279
data so when i say liberate the data we

00:09:45,519 --> 00:09:49,040
would want to make all those metrics

00:09:47,279 --> 00:09:51,200
available for different types of

00:09:49,040 --> 00:09:52,640
analysis so some of them could be the

00:09:51,200 --> 00:09:53,279
tooling that we're going to present

00:09:52,640 --> 00:09:55,519
today

00:09:53,279 --> 00:09:56,399
but not only that we have a pretty

00:09:55,519 --> 00:09:58,800
massive

00:09:56,399 --> 00:09:59,519
uh amount of people that are doing data

00:09:58,800 --> 00:10:01,519
science

00:09:59,519 --> 00:10:02,560
uh that are developing platforms in kind

00:10:01,519 --> 00:10:04,800
of higher level

00:10:02,560 --> 00:10:05,760
about above spark and hadoop and all

00:10:04,800 --> 00:10:08,480
those uh

00:10:05,760 --> 00:10:10,160
kind of lower level platforms uh that

00:10:08,480 --> 00:10:12,079
they would want to

00:10:10,160 --> 00:10:13,760
also consume those metrics and produce

00:10:12,079 --> 00:10:15,040
their own kind of performance analysis

00:10:13,760 --> 00:10:17,440
reports

00:10:15,040 --> 00:10:18,480
so the first thing that we've done is to

00:10:17,440 --> 00:10:21,360
basically break down

00:10:18,480 --> 00:10:21,920
all those sources and then make the data

00:10:21,360 --> 00:10:25,519
available

00:10:21,920 --> 00:10:26,320
uh through two uh phases so the first

00:10:25,519 --> 00:10:28,640
would be

00:10:26,320 --> 00:10:29,519
uh basically all the metrics get emitted

00:10:28,640 --> 00:10:31,680
to kafka

00:10:29,519 --> 00:10:32,880
and then they are available in real time

00:10:31,680 --> 00:10:34,399
and

00:10:32,880 --> 00:10:36,320
this way real time and streaming

00:10:34,399 --> 00:10:38,160
consumers can then just

00:10:36,320 --> 00:10:39,440
subscribe to those topics and process

00:10:38,160 --> 00:10:41,839
the data and

00:10:39,440 --> 00:10:43,839
let's say populate dashboards and so on

00:10:41,839 --> 00:10:46,560
in real time

00:10:43,839 --> 00:10:47,519
um and then everything gets persisted to

00:10:46,560 --> 00:10:50,160
hdfs

00:10:47,519 --> 00:10:50,880
so that batch and query uh consumers can

00:10:50,160 --> 00:10:53,040
also

00:10:50,880 --> 00:10:54,079
start processing these jobs and we can

00:10:53,040 --> 00:10:56,800
talk about that uh

00:10:54,079 --> 00:10:56,800
in a few minutes

00:10:57,200 --> 00:11:02,079
so uh we have three pillars here uh that

00:11:00,560 --> 00:11:03,279
we're creating so basically we have

00:11:02,079 --> 00:11:05,839
kafka for real time

00:11:03,279 --> 00:11:07,600
like i said metrics are emitted in uh to

00:11:05,839 --> 00:11:09,839
kafka and minimal latency

00:11:07,600 --> 00:11:11,279
and then we have direct emissions from

00:11:09,839 --> 00:11:13,519
clients uh we

00:11:11,279 --> 00:11:15,760
uh stream the log files and so on and

00:11:13,519 --> 00:11:17,839
that enables real-time dashboarding

00:11:15,760 --> 00:11:18,880
but the caveat is that it has limited

00:11:17,839 --> 00:11:22,240
retention

00:11:18,880 --> 00:11:23,920
usually a few days back on hdfs this is

00:11:22,240 --> 00:11:26,720
where we have etl pipelines to

00:11:23,920 --> 00:11:29,279
materialize the kafka topics into hdfs

00:11:26,720 --> 00:11:30,160
and then that kind of brings the latency

00:11:29,279 --> 00:11:32,720
from uh

00:11:30,160 --> 00:11:34,079
uh into minutes uh two hours we're

00:11:32,720 --> 00:11:36,880
working to minimize that

00:11:34,079 --> 00:11:37,120
but it is part of the system that you

00:11:36,880 --> 00:11:40,480
know

00:11:37,120 --> 00:11:42,480
etl pipelines usually takes uh uh a few

00:11:40,480 --> 00:11:44,880
minutes at least to populate

00:11:42,480 --> 00:11:46,880
um it registers all the data for will

00:11:44,880 --> 00:11:48,880
high for sql access and that enables

00:11:46,880 --> 00:11:50,639
batch query through spark hive

00:11:48,880 --> 00:11:51,920
um and basically on-demand queries for

00:11:50,639 --> 00:11:54,480
presto as well

00:11:51,920 --> 00:11:55,680
uh the pro for this is that we have

00:11:54,480 --> 00:11:58,079
unlimited

00:11:55,680 --> 00:11:59,760
retention practically we keep about

00:11:58,079 --> 00:12:02,560
three years of history

00:11:59,760 --> 00:12:03,279
for all of those metrics another thing

00:12:02,560 --> 00:12:06,320
that this

00:12:03,279 --> 00:12:09,519
uh enables is for users to derive

00:12:06,320 --> 00:12:13,200
topics so basically users can derive

00:12:09,519 --> 00:12:15,200
either in real time or in batches and

00:12:13,200 --> 00:12:16,959
derivation in real time could be

00:12:15,200 --> 00:12:18,000
something that runs over a samsung job

00:12:16,959 --> 00:12:21,120
that transformed

00:12:18,000 --> 00:12:23,200
one topic to another

00:12:21,120 --> 00:12:24,320
um this is an example of how we do

00:12:23,200 --> 00:12:27,680
derivation of

00:12:24,320 --> 00:12:30,399
kafka topic even in this context so

00:12:27,680 --> 00:12:31,200
um the way that we populate the spark

00:12:30,399 --> 00:12:34,800
metrics

00:12:31,200 --> 00:12:37,680
uh in this in project optimum

00:12:34,800 --> 00:12:39,360
is uh we have the yarn topic which

00:12:37,680 --> 00:12:41,519
basically contains all of the resource

00:12:39,360 --> 00:12:44,079
manager completed application records

00:12:41,519 --> 00:12:45,440
and those are emitted into kafka now we

00:12:44,079 --> 00:12:47,200
have a samsa job

00:12:45,440 --> 00:12:49,760
that is basically the spark tracking

00:12:47,200 --> 00:12:51,519
service and what it does it subscribes

00:12:49,760 --> 00:12:52,079
to the yarn topic and then every

00:12:51,519 --> 00:12:55,120
application

00:12:52,079 --> 00:12:56,800
that gets completed it will then go into

00:12:55,120 --> 00:12:59,839
the spark history server and pull

00:12:56,800 --> 00:13:02,480
all of the metrics through a rest api

00:12:59,839 --> 00:13:03,600
and then construct a record pair spark

00:13:02,480 --> 00:13:06,240
application run

00:13:03,600 --> 00:13:07,600
and then emit that into kafka again so

00:13:06,240 --> 00:13:09,600
we basically have a kind of a

00:13:07,600 --> 00:13:10,480
transformation using samsa where we

00:13:09,600 --> 00:13:13,600
transform

00:13:10,480 --> 00:13:14,639
the yarn topic onto the spark metrics

00:13:13,600 --> 00:13:16,160
topic

00:13:14,639 --> 00:13:20,160
so this is a nice example of how

00:13:16,160 --> 00:13:22,079
derivation could work

00:13:20,160 --> 00:13:23,200
so the next pillar that we want to talk

00:13:22,079 --> 00:13:25,200
about is

00:13:23,200 --> 00:13:28,480
uh grid bench this is kind of the main

00:13:25,200 --> 00:13:31,760
tool that we're going to discuss today

00:13:28,480 --> 00:13:34,959
so grid bench is about self-servicing

00:13:31,760 --> 00:13:36,959
users and enabling them to provide to

00:13:34,959 --> 00:13:39,279
basically see different insights about

00:13:36,959 --> 00:13:41,279
their jobs and enable them to

00:13:39,279 --> 00:13:43,199
uh improve and iterate and detect

00:13:41,279 --> 00:13:45,680
regressions and so on

00:13:43,199 --> 00:13:46,959
so before we dive into grid bench let's

00:13:45,680 --> 00:13:49,279
talk a little bit about

00:13:46,959 --> 00:13:50,560
what are the existing tooling that exist

00:13:49,279 --> 00:13:52,800
out there

00:13:50,560 --> 00:13:54,399
so dr elephant is one example dr

00:13:52,800 --> 00:13:55,360
elephant was actually started at

00:13:54,399 --> 00:13:58,800
linkedin and we

00:13:55,360 --> 00:14:00,399
are using it extensively but it has

00:13:58,800 --> 00:14:03,360
limited support for spark

00:14:00,399 --> 00:14:04,480
i only analyzes basic metrics to detect

00:14:03,360 --> 00:14:06,480
skewness

00:14:04,480 --> 00:14:08,240
memory and cpu utilization and

00:14:06,480 --> 00:14:10,639
inefficiencies and so on

00:14:08,240 --> 00:14:12,639
uh it does not have a full access to the

00:14:10,639 --> 00:14:13,760
spark metrics and this is due to storage

00:14:12,639 --> 00:14:16,720
limitation

00:14:13,760 --> 00:14:18,480
uh in how dr elephant is designed

00:14:16,720 --> 00:14:20,560
another thing is that dr elephant only

00:14:18,480 --> 00:14:22,079
supports a single application view where

00:14:20,560 --> 00:14:22,639
we would want to get to a point where we

00:14:22,079 --> 00:14:25,760
can

00:14:22,639 --> 00:14:27,519
uh actually uh see a history of

00:14:25,760 --> 00:14:28,240
different runs of the same workflow and

00:14:27,519 --> 00:14:31,360
kind of

00:14:28,240 --> 00:14:32,800
see how it behaves over time

00:14:31,360 --> 00:14:35,680
and another thing is a spark history

00:14:32,800 --> 00:14:39,120
server that we've uh already mentioned

00:14:35,680 --> 00:14:40,240
um the basic the most significant caveat

00:14:39,120 --> 00:14:41,920
as we see for it

00:14:40,240 --> 00:14:43,519
is that it only supports a single

00:14:41,920 --> 00:14:46,480
application view

00:14:43,519 --> 00:14:48,560
not only that it has limited access to

00:14:46,480 --> 00:14:51,040
historical runs due to some scalability

00:14:48,560 --> 00:14:54,959
issues in level bd

00:14:51,040 --> 00:14:57,760
and then there's another

00:14:54,959 --> 00:14:59,920
entire aspect that is missing uh so the

00:14:57,760 --> 00:15:02,160
spark history server does not have

00:14:59,920 --> 00:15:03,440
a perspective on the resource

00:15:02,160 --> 00:15:06,399
consumption from

00:15:03,440 --> 00:15:08,000
uh from yacht uh this is why we're

00:15:06,399 --> 00:15:09,040
missing some metrics from yarn that we

00:15:08,000 --> 00:15:10,880
need to include here

00:15:09,040 --> 00:15:12,320
so spark history server is just not

00:15:10,880 --> 00:15:13,760
enough

00:15:12,320 --> 00:15:16,000
another thing is that spark history

00:15:13,760 --> 00:15:17,760
server does not aggregate metrics across

00:15:16,000 --> 00:15:18,560
stages and that makes it a little bit

00:15:17,760 --> 00:15:20,880
difficult to

00:15:18,560 --> 00:15:22,480
kind of figure out what is the overall

00:15:20,880 --> 00:15:25,120
metrics that we see for a particular

00:15:22,480 --> 00:15:25,120
application

00:15:25,519 --> 00:15:30,160
so grid bench is a holistic approach for

00:15:28,399 --> 00:15:32,560
spark application performance

00:15:30,160 --> 00:15:34,560
uh we would want to do a few things here

00:15:32,560 --> 00:15:36,880
uh we would want to provide

00:15:34,560 --> 00:15:38,959
ad hoc analysis for spark applications

00:15:36,880 --> 00:15:41,199
basically produce reports that user can

00:15:38,959 --> 00:15:42,560
dive into and figure out what's going on

00:15:41,199 --> 00:15:43,920
in their job

00:15:42,560 --> 00:15:45,839
through different stages through

00:15:43,920 --> 00:15:48,480
different metrics and so on

00:15:45,839 --> 00:15:49,519
and then also allow aggregate analysis

00:15:48,480 --> 00:15:51,680
of multiple runs

00:15:49,519 --> 00:15:54,240
so users can analyze their application

00:15:51,680 --> 00:15:56,399
behavior over time

00:15:54,240 --> 00:15:58,959
you know across different times of the

00:15:56,399 --> 00:16:00,560
day different types of input sizes and

00:15:58,959 --> 00:16:02,720
so on

00:16:00,560 --> 00:16:04,160
we would want to allow comparisons of

00:16:02,720 --> 00:16:06,240
different sets of runs

00:16:04,160 --> 00:16:08,800
so like a user can check what happened

00:16:06,240 --> 00:16:12,079
before and after a particular change

00:16:08,800 --> 00:16:14,720
uh allow regression detection and then

00:16:12,079 --> 00:16:15,600
uh we would also want to uh the reports

00:16:14,720 --> 00:16:18,720
to be portable

00:16:15,600 --> 00:16:20,639
and persistent uh so that uh just to

00:16:18,720 --> 00:16:23,040
make it more flexible for users to

00:16:20,639 --> 00:16:25,120
you know to make this work for their

00:16:23,040 --> 00:16:27,199
particular use case

00:16:25,120 --> 00:16:28,560
and also provide access to historical

00:16:27,199 --> 00:16:30,880
data

00:16:28,560 --> 00:16:33,199
in our context usually this is three

00:16:30,880 --> 00:16:33,199
years

00:16:34,399 --> 00:16:38,399
so uh grid bench allows two running

00:16:37,360 --> 00:16:42,160
modes basically

00:16:38,399 --> 00:16:43,519
and uh this is uh where flexibility is

00:16:42,160 --> 00:16:45,519
the main idea here

00:16:43,519 --> 00:16:46,959
uh so good bench analysis logic is

00:16:45,519 --> 00:16:47,680
designed to be consumed from different

00:16:46,959 --> 00:16:50,560
environments

00:16:47,680 --> 00:16:51,519
uh one option is the ad hoc calls

00:16:50,560 --> 00:16:54,079
through the cli

00:16:51,519 --> 00:16:55,920
so we have a user facing reporting tool

00:16:54,079 --> 00:16:57,440
and it's deployed automatically to

00:16:55,920 --> 00:17:00,000
pretty much every machine at linkedin

00:16:57,440 --> 00:17:02,720
making it available for every spark user

00:17:00,000 --> 00:17:04,000
and then it generates portable html

00:17:02,720 --> 00:17:06,079
reports with interactive

00:17:04,000 --> 00:17:08,240
controls so that user can actually dive

00:17:06,079 --> 00:17:10,640
in into specific specific metrics

00:17:08,240 --> 00:17:11,919
uh we'll see a demo for that uh very

00:17:10,640 --> 00:17:14,240
soon

00:17:11,919 --> 00:17:15,679
there's also a text output option for

00:17:14,240 --> 00:17:17,679
whoever is trying to

00:17:15,679 --> 00:17:18,880
uh you know treat this output

00:17:17,679 --> 00:17:20,720
programmatically

00:17:18,880 --> 00:17:22,880
and kind of build alerts or any kind of

00:17:20,720 --> 00:17:26,319
logic on top of that

00:17:22,880 --> 00:17:29,039
um so uh batch

00:17:26,319 --> 00:17:31,200
and streaming uh this is the other uh

00:17:29,039 --> 00:17:33,520
type of running mode that we allow

00:17:31,200 --> 00:17:34,960
so basically the logic for grid bench is

00:17:33,520 --> 00:17:37,840
consumable through a java

00:17:34,960 --> 00:17:40,160
artifact uh you don't only have to run

00:17:37,840 --> 00:17:41,919
the cli itself you can

00:17:40,160 --> 00:17:43,600
basically depend on some of the

00:17:41,919 --> 00:17:44,480
artifacts that we built some of the

00:17:43,600 --> 00:17:47,280
utilities

00:17:44,480 --> 00:17:49,280
and produce the report uh in place for

00:17:47,280 --> 00:17:51,679
example if you build a spark job

00:17:49,280 --> 00:17:52,960
that analyzes a lot of application

00:17:51,679 --> 00:17:54,640
across time

00:17:52,960 --> 00:17:56,000
then it's not something that you can do

00:17:54,640 --> 00:17:59,600
in a cli because it's

00:17:56,000 --> 00:18:00,240
just too much data um so for that we can

00:17:59,600 --> 00:18:02,080
uh

00:18:00,240 --> 00:18:03,360
users can consume the java art effect

00:18:02,080 --> 00:18:06,320
and uh make those

00:18:03,360 --> 00:18:07,760
uh transformations and analysis uh same

00:18:06,320 --> 00:18:10,799
goes for streaming

00:18:07,760 --> 00:18:12,400
and uh that is used for populating

00:18:10,799 --> 00:18:16,640
dashboards creating reports

00:18:12,400 --> 00:18:16,640
and uh also triggering alerts

00:18:17,440 --> 00:18:20,960
so where would we where do we get the

00:18:19,200 --> 00:18:24,400
metrics for grid bench

00:18:20,960 --> 00:18:26,480
um so there are several options here

00:18:24,400 --> 00:18:28,720
the first one is from spark history

00:18:26,480 --> 00:18:31,679
server and this was the original

00:18:28,720 --> 00:18:32,480
uh metric source that was only supported

00:18:31,679 --> 00:18:35,440
at grid edge

00:18:32,480 --> 00:18:37,360
uh in the early version uh so like we've

00:18:35,440 --> 00:18:40,799
discussed there's a rest api that is

00:18:37,360 --> 00:18:42,160
exposed and kind of uh exposes all the

00:18:40,799 --> 00:18:43,120
metrics that spark history server

00:18:42,160 --> 00:18:46,720
collects

00:18:43,120 --> 00:18:48,240
and then it lacks metrics from yarn hdfs

00:18:46,720 --> 00:18:52,480
and so on like we've discussed

00:18:48,240 --> 00:18:52,480
so it's a kind of incomplete source

00:18:52,799 --> 00:18:56,000
presto allows us a little bit more

00:18:55,440 --> 00:18:58,000
complex

00:18:56,000 --> 00:18:59,760
filtering through sql queries so spark

00:18:58,000 --> 00:19:01,600
history server you basically have to

00:18:59,760 --> 00:19:03,039
provide the application id in order to

00:19:01,600 --> 00:19:04,720
search for applications

00:19:03,039 --> 00:19:06,559
and that kind of makes it a little bit

00:19:04,720 --> 00:19:08,320
awkward if you're trying to analyze a

00:19:06,559 --> 00:19:10,000
particular job across time

00:19:08,320 --> 00:19:12,080
you need to find another method on how

00:19:10,000 --> 00:19:14,240
to achieve on how to basically retrieve

00:19:12,080 --> 00:19:17,760
those application ids and that

00:19:14,240 --> 00:19:20,559
kind of makes it more complex um

00:19:17,760 --> 00:19:21,200
so presto gives us the more complex

00:19:20,559 --> 00:19:24,400
filtering

00:19:21,200 --> 00:19:26,480
with sql queries it

00:19:24,400 --> 00:19:28,080
can read records that combine all the

00:19:26,480 --> 00:19:28,640
metrics that we have some from spark

00:19:28,080 --> 00:19:31,760
history

00:19:28,640 --> 00:19:32,160
server from yarn and from hdfs and also

00:19:31,760 --> 00:19:34,320
allow

00:19:32,160 --> 00:19:36,720
low latency ad hoc querying which is

00:19:34,320 --> 00:19:38,559
what presto is all about

00:19:36,720 --> 00:19:40,240
for batch and streaming uh we have

00:19:38,559 --> 00:19:42,320
built-in support for spark data frame

00:19:40,240 --> 00:19:44,080
api for feeding metrics into

00:19:42,320 --> 00:19:45,520
a grid bench and basically generating

00:19:44,080 --> 00:19:48,400
reports out of those

00:19:45,520 --> 00:19:48,880
uh uh data frames uh this is used for

00:19:48,400 --> 00:19:50,320
some

00:19:48,880 --> 00:19:52,000
uh batch and streaming jobs that are

00:19:50,320 --> 00:19:54,960
generating more uh

00:19:52,000 --> 00:19:56,880
kind of data sets that are derived from

00:19:54,960 --> 00:19:58,880
the grid bench metrics

00:19:56,880 --> 00:20:00,799
uh the same could be to consume metrics

00:19:58,880 --> 00:20:03,120
from kafka topics and then generate

00:20:00,799 --> 00:20:04,640
a new topic with the grid bench output

00:20:03,120 --> 00:20:09,280
like we've discussed

00:20:04,640 --> 00:20:09,280
for the spark history server case

00:20:09,360 --> 00:20:15,360
so um not to get you guys too bored

00:20:13,039 --> 00:20:16,960
let's see a little bit of the reports

00:20:15,360 --> 00:20:20,720
that we have right now

00:20:16,960 --> 00:20:21,440
so uh grid bench gui is pretty new so

00:20:20,720 --> 00:20:23,600
this is

00:20:21,440 --> 00:20:24,960
a very rough at this point we're still

00:20:23,600 --> 00:20:26,400
working on it

00:20:24,960 --> 00:20:28,799
but i want to show you guys around a

00:20:26,400 --> 00:20:30,480
little bit i want to show

00:20:28,799 --> 00:20:32,159
some reports that we have and how we

00:20:30,480 --> 00:20:34,559
enable users to kind of

00:20:32,159 --> 00:20:36,400
self-serve and diagnose whatever is

00:20:34,559 --> 00:20:38,960
going on with their jobs

00:20:36,400 --> 00:20:40,080
so this is an example of a single

00:20:38,960 --> 00:20:41,520
application report

00:20:40,080 --> 00:20:43,760
so let's say you have an application

00:20:41,520 --> 00:20:45,919
that just finished and then grid bench

00:20:43,760 --> 00:20:48,880
can produce this report for it

00:20:45,919 --> 00:20:49,360
uh the most basic thing it provides is

00:20:48,880 --> 00:20:51,760
uh

00:20:49,360 --> 00:20:53,760
kind of an overview acro across all of

00:20:51,760 --> 00:20:56,320
the stages and all of the jobs

00:20:53,760 --> 00:20:56,960
of all the metrics uh that we care about

00:20:56,320 --> 00:20:59,520
these are

00:20:56,960 --> 00:21:01,440
kind of a minimized list of uh important

00:20:59,520 --> 00:21:04,640
uh metrics

00:21:01,440 --> 00:21:07,360
now you can then

00:21:04,640 --> 00:21:08,480
filter by a particular job or further by

00:21:07,360 --> 00:21:11,919
a particular

00:21:08,480 --> 00:21:12,480
stage and then see how the metrics kind

00:21:11,919 --> 00:21:16,159
of

00:21:12,480 --> 00:21:17,200
uh where time is spent within that

00:21:16,159 --> 00:21:20,240
particular

00:21:17,200 --> 00:21:21,919
stage we can see so that for this stage

00:21:20,240 --> 00:21:23,360
most of the time is spent in executor

00:21:21,919 --> 00:21:26,559
cpu time which

00:21:23,360 --> 00:21:30,400
makes sense this is like the main

00:21:26,559 --> 00:21:32,640
job thread another type of report that

00:21:30,400 --> 00:21:36,240
we have is what we call the diff report

00:21:32,640 --> 00:21:37,520
think of it as two reports presented

00:21:36,240 --> 00:21:39,360
side by side

00:21:37,520 --> 00:21:41,840
where you can compare two application

00:21:39,360 --> 00:21:42,880
runs uh in this case we can compare the

00:21:41,840 --> 00:21:45,280
global

00:21:42,880 --> 00:21:46,640
metrics and see that uh where are like

00:21:45,280 --> 00:21:49,440
the major differences

00:21:46,640 --> 00:21:51,200
uh in the time that was spent uh we see

00:21:49,440 --> 00:21:52,799
that there is major difference in gc

00:21:51,200 --> 00:21:55,440
time here for example

00:21:52,799 --> 00:21:56,080
and at i o time so that kind of helps

00:21:55,440 --> 00:21:58,799
users

00:21:56,080 --> 00:21:59,760
understand you know where most of the

00:21:58,799 --> 00:22:01,919
time or mis

00:21:59,760 --> 00:22:04,320
where most of the inefficiencies are

00:22:01,919 --> 00:22:08,240
within their applications

00:22:04,320 --> 00:22:10,960
um another option here is to display

00:22:08,240 --> 00:22:11,440
biometric uh for example what we have

00:22:10,960 --> 00:22:14,880
here

00:22:11,440 --> 00:22:19,200
it basically uh lets the user see

00:22:14,880 --> 00:22:21,840
uh what uh type of metric and what stage

00:22:19,200 --> 00:22:23,840
uh is spent uh like where the metrics

00:22:21,840 --> 00:22:26,240
are spending time for each particular

00:22:23,840 --> 00:22:28,320
uh metric in this case for the executor

00:22:26,240 --> 00:22:31,679
cpu time we can see that

00:22:28,320 --> 00:22:33,679
for the matching stages 15 and 17

00:22:31,679 --> 00:22:35,679
there's a bit of a difference between

00:22:33,679 --> 00:22:38,000
the two application runs

00:22:35,679 --> 00:22:40,000
so the way that we do it is uh we take

00:22:38,000 --> 00:22:42,320
the two applications and we analyze the

00:22:40,000 --> 00:22:43,600
dags so that we can match jobs and

00:22:42,320 --> 00:22:45,280
stages

00:22:43,600 --> 00:22:47,440
so we can actually compare on a

00:22:45,280 --> 00:22:49,840
per-stage basis

00:22:47,440 --> 00:22:50,720
this kind of enables us to dive in

00:22:49,840 --> 00:22:52,480
deeper

00:22:50,720 --> 00:22:54,799
across different applications if they

00:22:52,480 --> 00:22:57,520
match in their dax if they do not match

00:22:54,799 --> 00:22:58,559
uh it will not allow you to see uh the

00:22:57,520 --> 00:23:01,679
pair job or per

00:22:58,559 --> 00:23:04,720
stage analysis only kind of the global

00:23:01,679 --> 00:23:07,360
so let's browse through uh to

00:23:04,720 --> 00:23:08,880
the third type of report that we have so

00:23:07,360 --> 00:23:12,320
it's a bit

00:23:08,880 --> 00:23:14,080
heavy um okay there it is

00:23:12,320 --> 00:23:16,320
so this is the aggregate report the

00:23:14,080 --> 00:23:17,039
aggregate report it usually it basically

00:23:16,320 --> 00:23:19,919
takes a

00:23:17,039 --> 00:23:20,640
set of application ids instead of just

00:23:19,919 --> 00:23:22,000
one

00:23:20,640 --> 00:23:23,679
let's say if someone is trying to

00:23:22,000 --> 00:23:24,640
analyze their application behavior

00:23:23,679 --> 00:23:27,679
across time

00:23:24,640 --> 00:23:30,400
this gives you kind of an overview of

00:23:27,679 --> 00:23:31,600
where where are you at in terms of

00:23:30,400 --> 00:23:34,960
variance across

00:23:31,600 --> 00:23:37,200
a particular metrics for for example for

00:23:34,960 --> 00:23:38,080
executor cpu time we can see the mean is

00:23:37,200 --> 00:23:39,679
right here

00:23:38,080 --> 00:23:42,320
and then this gives you kind of an

00:23:39,679 --> 00:23:44,240
overview on the quantiles

00:23:42,320 --> 00:23:46,000
the last type of report and probably the

00:23:44,240 --> 00:23:49,360
most useful one and the most

00:23:46,000 --> 00:23:51,600
uh uh i guess uh

00:23:49,360 --> 00:23:53,200
the most important one of all of those

00:23:51,600 --> 00:23:54,960
uh is what we call the performance

00:23:53,200 --> 00:23:57,520
regression report

00:23:54,960 --> 00:23:58,240
so this type of report basically gives

00:23:57,520 --> 00:24:00,320
you uh

00:23:58,240 --> 00:24:02,240
it takes two sets of application runs

00:24:00,320 --> 00:24:05,760
consider it as before and after

00:24:02,240 --> 00:24:08,000
let's say that you have a job that

00:24:05,760 --> 00:24:08,799
started performing really bad at some

00:24:08,000 --> 00:24:10,799
point

00:24:08,799 --> 00:24:13,039
and then you would want to understand

00:24:10,799 --> 00:24:15,520
why so you can choose two application

00:24:13,039 --> 00:24:19,120
sets before and after a particular date

00:24:15,520 --> 00:24:22,480
and then compare their statistical

00:24:19,120 --> 00:24:23,679
values and try to understand what are

00:24:22,480 --> 00:24:26,880
the differences

00:24:23,679 --> 00:24:29,440
so we can see that for application set 1

00:24:26,880 --> 00:24:31,279
executed cpu time we can see that the

00:24:29,440 --> 00:24:34,080
median is very far

00:24:31,279 --> 00:24:36,240
and then uh there's a a very big

00:24:34,080 --> 00:24:37,840
difference in how the quantiles are

00:24:36,240 --> 00:24:39,600
being distributed across the two

00:24:37,840 --> 00:24:40,640
application sets so there's definitely

00:24:39,600 --> 00:24:43,360
something that's

00:24:40,640 --> 00:24:44,720
uh different here between the two sets

00:24:43,360 --> 00:24:48,480
now another thing that this

00:24:44,720 --> 00:24:49,360
uh provides is it does a regression

00:24:48,480 --> 00:24:51,760
analysis

00:24:49,360 --> 00:24:52,559
to tell you whether performance has

00:24:51,760 --> 00:24:55,600
regressed

00:24:52,559 --> 00:24:56,880
or had no change or it was improved

00:24:55,600 --> 00:24:59,120
and then it also gives you the

00:24:56,880 --> 00:25:00,799
confidence level so

00:24:59,120 --> 00:25:02,559
you might see regression but with very

00:25:00,799 --> 00:25:03,520
low confidence but if it's high

00:25:02,559 --> 00:25:05,919
confidence

00:25:03,520 --> 00:25:07,600
uh you know it usually means that uh

00:25:05,919 --> 00:25:09,679
there is actual regression here

00:25:07,600 --> 00:25:11,600
if there's low confidence uh it means

00:25:09,679 --> 00:25:14,320
that uh the user should be

00:25:11,600 --> 00:25:15,039
uh going back and running uh the

00:25:14,320 --> 00:25:18,080
application

00:25:15,039 --> 00:25:18,960
a few more times so that the data points

00:25:18,080 --> 00:25:22,240
will be more

00:25:18,960 --> 00:25:25,840
uh kind of uh uh meaningful

00:25:22,240 --> 00:25:25,840
and have less outliers in them

00:25:26,080 --> 00:25:32,320
so uh let me go back to the slides

00:25:29,679 --> 00:25:34,960
and explain a little bit on how we do

00:25:32,320 --> 00:25:38,000
regression analysis

00:25:34,960 --> 00:25:40,640
so regression detection um is done here

00:25:38,000 --> 00:25:43,679
through statistical analysis tools

00:25:40,640 --> 00:25:45,679
so we have uh we basically

00:25:43,679 --> 00:25:48,240
went through kind of an aggregate

00:25:45,679 --> 00:25:50,400
analysis research phase that we've done

00:25:48,240 --> 00:25:52,080
on various metrics that uh basically

00:25:50,400 --> 00:25:53,520
revealed that all of them are normally

00:25:52,080 --> 00:25:56,559
distributed

00:25:53,520 --> 00:25:58,880
that kind of drove us uh into

00:25:56,559 --> 00:25:59,600
uh some statistical methods that are

00:25:58,880 --> 00:26:03,520
designed

00:25:59,600 --> 00:26:07,440
in this uh for this particular uh uh

00:26:03,520 --> 00:26:09,679
case so we collected test data to

00:26:07,440 --> 00:26:11,600
measure the effectiveness of several

00:26:09,679 --> 00:26:13,520
statistical solutions that we've came up

00:26:11,600 --> 00:26:16,880
with and then

00:26:13,520 --> 00:26:16,880
we were able to see that

00:26:17,120 --> 00:26:20,559
there was real degradation in some of

00:26:18,799 --> 00:26:21,360
those that we were able to pick up with

00:26:20,559 --> 00:26:25,440
some of those

00:26:21,360 --> 00:26:27,919
options and then we found that t-test

00:26:25,440 --> 00:26:29,120
was uh basically the most reliable way

00:26:27,919 --> 00:26:32,320
in the simpler

00:26:29,120 --> 00:26:34,000
way of detecting regression um and

00:26:32,320 --> 00:26:36,240
basically detecting significant

00:26:34,000 --> 00:26:39,039
uh statistical differences between uh

00:26:36,240 --> 00:26:41,200
two samples

00:26:39,039 --> 00:26:42,880
so how do we detect statistical

00:26:41,200 --> 00:26:45,679
differences uh we use

00:26:42,880 --> 00:26:48,159
uh welch's t-test which is a version of

00:26:45,679 --> 00:26:50,240
the student these tests that

00:26:48,159 --> 00:26:52,159
is designed for independent samples

00:26:50,240 --> 00:26:56,000
basically unequal variances

00:26:52,159 --> 00:26:59,279
or unequal sample sizes so in this case

00:26:56,000 --> 00:27:01,520
uh given two sets of samples uh

00:26:59,279 --> 00:27:03,840
or i mean two sets of data points or you

00:27:01,520 --> 00:27:07,120
can call them two samples

00:27:03,840 --> 00:27:12,480
and let's say if we set alpha to be

00:27:07,120 --> 00:27:15,360
uh 0.05 which means the 95 percentile

00:27:12,480 --> 00:27:16,559
then if the welch's t test produces a

00:27:15,360 --> 00:27:18,960
value that is

00:27:16,559 --> 00:27:20,240
smaller than this alpha it means that we

00:27:18,960 --> 00:27:23,279
have a statistically

00:27:20,240 --> 00:27:26,799
significant difference so this is one

00:27:23,279 --> 00:27:30,080
uh part of this and then the next thing

00:27:26,799 --> 00:27:30,799
is how do we produce the confidence

00:27:30,080 --> 00:27:34,000
level

00:27:30,799 --> 00:27:36,960
um this is driven from the power

00:27:34,000 --> 00:27:39,279
of the hypothesis test so basically we

00:27:36,960 --> 00:27:42,320
have the effect size the effect size

00:27:39,279 --> 00:27:43,360
is a very common uh coefficient that is

00:27:42,320 --> 00:27:46,559
built on top of

00:27:43,360 --> 00:27:48,559
cohen's d and then those basically

00:27:46,559 --> 00:27:50,880
produce those uh

00:27:48,559 --> 00:27:53,200
categorizations that you see here so

00:27:50,880 --> 00:27:53,679
basically if the coefficient value is

00:27:53,200 --> 00:27:56,640
this

00:27:53,679 --> 00:27:58,399
then the sample size is tiny if the

00:27:56,640 --> 00:28:00,880
sample size is tiny this

00:27:58,399 --> 00:28:02,240
translates directly into very low

00:28:00,880 --> 00:28:04,960
confidence

00:28:02,240 --> 00:28:07,440
low medium high and so on and then if a

00:28:04,960 --> 00:28:09,760
user sees the confidence level

00:28:07,440 --> 00:28:10,720
as not high it means that they need to

00:28:09,760 --> 00:28:12,559
go back

00:28:10,720 --> 00:28:15,120
run the application a few more times

00:28:12,559 --> 00:28:17,840
collect more data points so that the

00:28:15,120 --> 00:28:19,360
statistical analysis model will be uh

00:28:17,840 --> 00:28:21,919
more

00:28:19,360 --> 00:28:21,919
reliable

00:28:24,640 --> 00:28:30,880
so how we compute cohen's d

00:28:28,000 --> 00:28:32,399
so cohen's d is a pretty popular formula

00:28:30,880 --> 00:28:33,279
uh it's not something that we invented

00:28:32,399 --> 00:28:36,880
here but

00:28:33,279 --> 00:28:39,520
the idea here is to kind of understand

00:28:36,880 --> 00:28:41,360
uh the differences in the standard

00:28:39,520 --> 00:28:42,880
deviations what's called the pooled

00:28:41,360 --> 00:28:45,039
standard deviation

00:28:42,880 --> 00:28:46,080
and then that kind of gives you an idea

00:28:45,039 --> 00:28:48,320
whether this

00:28:46,080 --> 00:28:49,279
sample set or those two sample sets are

00:28:48,320 --> 00:28:51,200
too far apart

00:28:49,279 --> 00:28:52,720
if they're uh you know what is their

00:28:51,200 --> 00:28:58,240
strength uh

00:28:52,720 --> 00:28:59,840
or power in this case

00:28:58,240 --> 00:29:01,840
all right so the next thing we're going

00:28:59,840 --> 00:29:03,520
to talk about is what we call the golden

00:29:01,840 --> 00:29:04,080
flows so we've just mentioned that

00:29:03,520 --> 00:29:06,799
earlier

00:29:04,080 --> 00:29:08,320
uh but we haven't touched uh through

00:29:06,799 --> 00:29:11,679
what this thing is actually

00:29:08,320 --> 00:29:12,720
uh is so the golden flows are a

00:29:11,679 --> 00:29:15,600
carefully

00:29:12,720 --> 00:29:17,200
curated set of jobs that represent uh

00:29:15,600 --> 00:29:18,480
typical cluster workloads that we have

00:29:17,200 --> 00:29:20,559
at linkedin

00:29:18,480 --> 00:29:21,760
uh there are uh most of them are cloned

00:29:20,559 --> 00:29:23,919
from real flows

00:29:21,760 --> 00:29:25,440
uh uh some of them are very high

00:29:23,919 --> 00:29:28,960
priority flows that we

00:29:25,440 --> 00:29:31,520
in particular care about their slas

00:29:28,960 --> 00:29:34,960
um some are artificially made to cover

00:29:31,520 --> 00:29:37,279
specific problematic areas that we

00:29:34,960 --> 00:29:38,720
have witnessed that are not covered as

00:29:37,279 --> 00:29:42,480
we wanted to be

00:29:38,720 --> 00:29:44,559
in the real flows that we've created

00:29:42,480 --> 00:29:46,880
and all those basically create a

00:29:44,559 --> 00:29:48,960
continuous series of metrics that allow

00:29:46,880 --> 00:29:51,360
real-time view of cluster health

00:29:48,960 --> 00:29:52,640
so we have a small set of flows that

00:29:51,360 --> 00:29:55,200
represent

00:29:52,640 --> 00:29:57,200
a vast majority of the population of

00:29:55,200 --> 00:29:59,840
flows that we have in the cluster

00:29:57,200 --> 00:30:01,679
and that gives us a perspective on how

00:29:59,840 --> 00:30:04,799
things are going in the cluster

00:30:01,679 --> 00:30:07,840
uh it gives us an early indication if

00:30:04,799 --> 00:30:10,559
uh some platform is failing if there's

00:30:07,840 --> 00:30:13,600
like inavailability of some data

00:30:10,559 --> 00:30:16,000
and it really helps in kind of

00:30:13,600 --> 00:30:16,720
early detection of problems before they

00:30:16,000 --> 00:30:18,320
become

00:30:16,720 --> 00:30:21,440
something that affect you know the

00:30:18,320 --> 00:30:21,440
majority of the flows

00:30:21,679 --> 00:30:26,559
so what is the golden flows uh uh and so

00:30:25,039 --> 00:30:29,520
this is basically a suite

00:30:26,559 --> 00:30:30,880
the golden flow suite uh is packaged as

00:30:29,520 --> 00:30:33,600
a deployable zip

00:30:30,880 --> 00:30:34,240
on top of azkaban so this zip basically

00:30:33,600 --> 00:30:37,039
contains

00:30:34,240 --> 00:30:38,240
all the flow definitions the jars

00:30:37,039 --> 00:30:40,480
artifacts

00:30:38,240 --> 00:30:42,240
uh configs and basically everything

00:30:40,480 --> 00:30:42,960
that's needed in order to emit those

00:30:42,240 --> 00:30:45,279
metrics

00:30:42,960 --> 00:30:47,200
uh uh into the common pipelines so that

00:30:45,279 --> 00:30:48,240
we can set up alerting dashboards and so

00:30:47,200 --> 00:30:50,159
on

00:30:48,240 --> 00:30:52,480
uh the input data set path is

00:30:50,159 --> 00:30:53,919
configurable so that user can pin to a

00:30:52,480 --> 00:30:56,399
particular input

00:30:53,919 --> 00:30:58,240
uh when they run those golden flows so

00:30:56,399 --> 00:31:00,240
those golden flows actually depend on

00:30:58,240 --> 00:31:04,000
real data so they have to have

00:31:00,240 --> 00:31:06,159
a real input that they can consume

00:31:04,000 --> 00:31:07,840
there is validation steps along the way

00:31:06,159 --> 00:31:09,760
um just to incorp

00:31:07,840 --> 00:31:11,039
that just to be able to detect if

00:31:09,760 --> 00:31:13,039
something is wrong let's say if the

00:31:11,039 --> 00:31:14,799
input data is not there

00:31:13,039 --> 00:31:16,880
or the input size is different than what

00:31:14,799 --> 00:31:18,399
we expect this is something that we want

00:31:16,880 --> 00:31:21,360
to

00:31:18,399 --> 00:31:23,039
be able to surface before we report

00:31:21,360 --> 00:31:26,000
results

00:31:23,039 --> 00:31:28,000
so the golden flow suite is it can run

00:31:26,000 --> 00:31:28,320
on any of our clusters and then deployed

00:31:28,000 --> 00:31:30,320
over

00:31:28,320 --> 00:31:32,559
azkaban and it runs over our yarn

00:31:30,320 --> 00:31:34,399
clusters and consuming input data from

00:31:32,559 --> 00:31:35,919
hdfs and producing the same kind of

00:31:34,399 --> 00:31:38,399
metrics that we have for traditional

00:31:35,919 --> 00:31:40,159
spark jobs

00:31:38,399 --> 00:31:41,519
so we have several deployment options

00:31:40,159 --> 00:31:43,519
for golden flows

00:31:41,519 --> 00:31:45,279
the permanent option is where we have a

00:31:43,519 --> 00:31:47,120
per cluster deployment

00:31:45,279 --> 00:31:49,519
and this is where the input data is

00:31:47,120 --> 00:31:52,880
static we do that in order to

00:31:49,519 --> 00:31:53,919
reduce as much of inconsistencies across

00:31:52,880 --> 00:31:56,960
different runs

00:31:53,919 --> 00:31:58,159
if you run with different input data and

00:31:56,960 --> 00:31:59,519
compare the two runs

00:31:58,159 --> 00:32:01,840
you might see something completely

00:31:59,519 --> 00:32:04,320
different just because of how the data

00:32:01,840 --> 00:32:04,960
just because of the data size or the

00:32:04,320 --> 00:32:08,320
content

00:32:04,960 --> 00:32:11,120
that might be different so

00:32:08,320 --> 00:32:11,600
this kind of serves as a gatekeeper and

00:32:11,120 --> 00:32:13,840
uh

00:32:11,600 --> 00:32:16,559
let's say when we deploy a new set of

00:32:13,840 --> 00:32:18,480
configs or new versions of our platforms

00:32:16,559 --> 00:32:20,000
then we promote those through the ranks

00:32:18,480 --> 00:32:22,720
through our different clusters

00:32:20,000 --> 00:32:24,320
and that enable us to see what happens

00:32:22,720 --> 00:32:26,159
before we get into the production

00:32:24,320 --> 00:32:28,399
clusters

00:32:26,159 --> 00:32:30,240
all those feed operational dashboards

00:32:28,399 --> 00:32:32,640
alerts and recurrent reports like the

00:32:30,240 --> 00:32:36,240
stuff that we've discussed earlier

00:32:32,640 --> 00:32:39,519
um so another option for deployment

00:32:36,240 --> 00:32:40,559
is for experimentation so users are able

00:32:39,519 --> 00:32:43,919
to deploy this

00:32:40,559 --> 00:32:44,799
zip basically to any of our existing

00:32:43,919 --> 00:32:47,120
clusters

00:32:44,799 --> 00:32:47,840
and then customize to run on their

00:32:47,120 --> 00:32:49,919
particular

00:32:47,840 --> 00:32:51,919
code or in their particular version or

00:32:49,919 --> 00:32:54,960
configs that they're trying to test

00:32:51,919 --> 00:32:56,559
and kind of figure out um what is what

00:32:54,960 --> 00:32:57,760
are the changes that are doing and how

00:32:56,559 --> 00:33:00,559
they affect

00:32:57,760 --> 00:33:02,159
uh runtime of golden flows for example

00:33:00,559 --> 00:33:02,640
if we're trying to change something in

00:33:02,159 --> 00:33:04,640
spark

00:33:02,640 --> 00:33:06,480
maybe a default config or maybe update

00:33:04,640 --> 00:33:08,320
the version that gives us an

00:33:06,480 --> 00:33:09,600
idea and what will happen when we deploy

00:33:08,320 --> 00:33:12,159
this into production

00:33:09,600 --> 00:33:13,760
and how real jobs will behave and that

00:33:12,159 --> 00:33:18,320
kind of enables us to

00:33:13,760 --> 00:33:21,120
iterate without causing any side effects

00:33:18,320 --> 00:33:21,679
um and another one is the thermal option

00:33:21,120 --> 00:33:24,320
so

00:33:21,679 --> 00:33:26,240
these this is basically uh uh designed

00:33:24,320 --> 00:33:27,840
for temporary deployments

00:33:26,240 --> 00:33:29,679
let's say if you're trying to evaluate a

00:33:27,840 --> 00:33:31,600
new cluster um

00:33:29,679 --> 00:33:32,960
could be in the cloud let's say if

00:33:31,600 --> 00:33:35,519
you're trying to uh

00:33:32,960 --> 00:33:36,240
evaluate different types of instances

00:33:35,519 --> 00:33:38,000
and see

00:33:36,240 --> 00:33:40,320
what is like the compute efficiency that

00:33:38,000 --> 00:33:42,559
you get from them so you can deploy this

00:33:40,320 --> 00:33:44,799
uh independent zip as long as you have

00:33:42,559 --> 00:33:47,200
the surrounding infrastructure like

00:33:44,799 --> 00:33:48,080
azkaban yarn and all those stuff and

00:33:47,200 --> 00:33:49,760
then uh

00:33:48,080 --> 00:33:51,120
as long as you provide the input and

00:33:49,760 --> 00:33:54,399
make it available

00:33:51,120 --> 00:33:57,519
then you can basically evaluate

00:33:54,399 --> 00:34:00,640
new clusters and not have to abide to

00:33:57,519 --> 00:34:00,640
our existing clusters

00:34:01,600 --> 00:34:08,399
so for the design choices that we've

00:34:04,880 --> 00:34:10,879
decided to go with um so

00:34:08,399 --> 00:34:11,760
in terms of curation how how do we get

00:34:10,879 --> 00:34:14,560
those flows

00:34:11,760 --> 00:34:16,399
uh so the main idea here is to provide

00:34:14,560 --> 00:34:17,359
as much coverage as possible we want to

00:34:16,399 --> 00:34:19,760
cover

00:34:17,359 --> 00:34:22,079
as many platforms as possible configs

00:34:19,760 --> 00:34:24,720
different times of the day or the week

00:34:22,079 --> 00:34:26,639
and then critical and high priority jobs

00:34:24,720 --> 00:34:29,040
uh are very important for us to get

00:34:26,639 --> 00:34:31,760
you know very early indication of what's

00:34:29,040 --> 00:34:33,679
uh going on with them

00:34:31,760 --> 00:34:35,599
the idea is to have floor or platform

00:34:33,679 --> 00:34:36,879
owners provide candidates and then

00:34:35,599 --> 00:34:38,720
monitor their health

00:34:36,879 --> 00:34:40,079
and maintain their versioning otherwise

00:34:38,720 --> 00:34:42,800
it will not be

00:34:40,079 --> 00:34:43,440
scalable for us as the performance team

00:34:42,800 --> 00:34:46,399
to kind of

00:34:43,440 --> 00:34:46,399
maintain all of this

00:34:46,720 --> 00:34:50,399
so it depends on realflow executing a

00:34:48,800 --> 00:34:52,399
real data and then

00:34:50,399 --> 00:34:54,320
some of them are custom designed like

00:34:52,399 --> 00:34:56,000
we've discussed

00:34:54,320 --> 00:34:58,000
uh so the principles that we keep in

00:34:56,000 --> 00:34:58,640
mind is to keep customizations to a

00:34:58,000 --> 00:35:00,640
minimum

00:34:58,640 --> 00:35:02,800
uh this complicates maintenance and kind

00:35:00,640 --> 00:35:03,839
of drift or you know from wherever the

00:35:02,800 --> 00:35:07,119
original flow

00:35:03,839 --> 00:35:07,520
is we only put emphasis on consistency

00:35:07,119 --> 00:35:09,839
so

00:35:07,520 --> 00:35:10,720
input data is pinned like we discussed

00:35:09,839 --> 00:35:12,880
earlier

00:35:10,720 --> 00:35:14,720
uh we disable dynamic allocation and

00:35:12,880 --> 00:35:17,119
disable uh speculative

00:35:14,720 --> 00:35:18,160
execution because they create very much

00:35:17,119 --> 00:35:21,280
very uh

00:35:18,160 --> 00:35:23,839
difficult inconsistencies in the metrics

00:35:21,280 --> 00:35:25,520
for managing side effects we require

00:35:23,839 --> 00:35:27,599
that the output is steered away from

00:35:25,520 --> 00:35:29,440
production paths so we won't affect any

00:35:27,599 --> 00:35:32,240
kind of production data sets

00:35:29,440 --> 00:35:33,119
and that requires reviews by the flow

00:35:32,240 --> 00:35:37,839
owner

00:35:33,119 --> 00:35:39,839
for us to onboard new golden flows

00:35:37,839 --> 00:35:42,160
so the insights that golden flows

00:35:39,839 --> 00:35:44,240
provide is first of all predictability

00:35:42,160 --> 00:35:46,640
we can have a clear idea on what we

00:35:44,240 --> 00:35:48,880
should expect compared to the last day

00:35:46,640 --> 00:35:49,760
or the last two weeks and then that

00:35:48,880 --> 00:35:52,880
enables

00:35:49,760 --> 00:35:54,240
us to kind of uh uh see trends that are

00:35:52,880 --> 00:35:55,839
going on in metrics

00:35:54,240 --> 00:35:57,359
uh across different times across

00:35:55,839 --> 00:35:58,560
different platforms for different types

00:35:57,359 --> 00:36:00,880
of flows

00:35:58,560 --> 00:36:01,920
and then provide early detection of

00:36:00,880 --> 00:36:04,560
problems that are

00:36:01,920 --> 00:36:06,720
uh you know starting to show up uh even

00:36:04,560 --> 00:36:09,440
before we get into production

00:36:06,720 --> 00:36:10,240
and the last thing is it allows us to

00:36:09,440 --> 00:36:13,359
compare

00:36:10,240 --> 00:36:15,599
the golden flow behavior on how it

00:36:13,359 --> 00:36:16,720
uh aligns with the general population of

00:36:15,599 --> 00:36:19,440
flows

00:36:16,720 --> 00:36:21,440
and if we see a diversion at some point

00:36:19,440 --> 00:36:23,440
that kind of drives us into

00:36:21,440 --> 00:36:24,880
reconfiguring the golden flow to better

00:36:23,440 --> 00:36:26,079
match what we see in the general

00:36:24,880 --> 00:36:29,280
population

00:36:26,079 --> 00:36:29,280
and vice versa

00:36:31,520 --> 00:36:36,800
all right a quick case study um

00:36:34,880 --> 00:36:38,560
so i'm going to go over this quickly

00:36:36,800 --> 00:36:40,640
because we're i want to leave enough

00:36:38,560 --> 00:36:42,560
time for a q a

00:36:40,640 --> 00:36:44,560
so something interesting that we've seen

00:36:42,560 --> 00:36:46,320
in our cluster regarding shared resource

00:36:44,560 --> 00:36:48,480
interference

00:36:46,320 --> 00:36:49,520
at one point we saw a high priority

00:36:48,480 --> 00:36:52,000
production flow

00:36:49,520 --> 00:36:52,960
that showed the highly varied end-to-end

00:36:52,000 --> 00:36:56,240
run times

00:36:52,960 --> 00:36:58,000
and kind of missing slas the grid bench

00:36:56,240 --> 00:36:58,960
metrics didn't show any significant

00:36:58,000 --> 00:37:01,440
differences

00:36:58,960 --> 00:37:03,760
uh that we were able to kind of

00:37:01,440 --> 00:37:06,000
highlight and figure out what's wrong

00:37:03,760 --> 00:37:08,160
and then digging deeper into each

00:37:06,000 --> 00:37:12,320
particular node we were able to see

00:37:08,160 --> 00:37:14,000
that there was a misconfigure c group

00:37:12,320 --> 00:37:15,520
that created a resource leak between

00:37:14,000 --> 00:37:18,800
cpus and kind of

00:37:15,520 --> 00:37:20,240
caused another job to consume most of

00:37:18,800 --> 00:37:23,359
the compute resources for

00:37:20,240 --> 00:37:24,240
our job that we're interested in um so

00:37:23,359 --> 00:37:26,400
we fixed the c

00:37:24,240 --> 00:37:28,320
group configurations and then that also

00:37:26,400 --> 00:37:30,560
prompted us uh

00:37:28,320 --> 00:37:32,240
to create an initiative to increase

00:37:30,560 --> 00:37:34,079
cluster predictability

00:37:32,240 --> 00:37:36,079
uh by addressing the shared resource

00:37:34,079 --> 00:37:38,079
interference that goes for disk io

00:37:36,079 --> 00:37:41,520
shuffle system resources and this is a

00:37:38,079 --> 00:37:45,200
new initiative that we're working on

00:37:41,520 --> 00:37:47,359
so what's coming up next um so we're

00:37:45,200 --> 00:37:48,839
uh planning on open sourcing grid bench

00:37:47,359 --> 00:37:50,320
and making it available for the

00:37:48,839 --> 00:37:52,480
community

00:37:50,320 --> 00:37:54,160
we're working on contributing our shs

00:37:52,480 --> 00:37:55,599
changes back to spark upstream so

00:37:54,160 --> 00:37:57,440
everyone can enjoy

00:37:55,599 --> 00:37:58,880
the necessary changes that we've made in

00:37:57,440 --> 00:38:00,560
order to

00:37:58,880 --> 00:38:02,400
collect the metrics that we're seeing in

00:38:00,560 --> 00:38:04,560
grid bench

00:38:02,400 --> 00:38:06,400
we're going to add straggler detection

00:38:04,560 --> 00:38:08,560
highlighting bottlenecks

00:38:06,400 --> 00:38:11,119
skewness detection heuristics and

00:38:08,560 --> 00:38:14,240
integration with cpu profiling

00:38:11,119 --> 00:38:15,440
um so yeah hold tight this is all coming

00:38:14,240 --> 00:38:18,880
up

00:38:15,440 --> 00:38:21,200
so we have a few more minutes um

00:38:18,880 --> 00:38:22,960
sorry if i was going too quick at the

00:38:21,200 --> 00:38:26,480
end but i wanted to

00:38:22,960 --> 00:38:33,839
make sure i leave enough time for q a so

00:38:26,480 --> 00:38:33,839
uh yeah let me read through the chat

00:38:34,720 --> 00:38:41,040
okay so for kafka nodes uh i'm

00:38:38,800 --> 00:38:42,960
starting from the first question let me

00:38:41,040 --> 00:38:44,560
repeat that is

00:38:42,960 --> 00:38:46,079
could you give a rough estimate of how

00:38:44,560 --> 00:38:49,359
many kafka nodes you need

00:38:46,079 --> 00:38:51,839
for every other service so

00:38:49,359 --> 00:38:54,320
i it's difficult for me to say but i can

00:38:51,839 --> 00:38:54,320
say that

00:38:54,400 --> 00:38:59,359
the for example the spark tracking

00:38:56,720 --> 00:39:02,240
service that we have

00:38:59,359 --> 00:39:04,400
it runs within a single kafka node with

00:39:02,240 --> 00:39:07,520
a lot of other jobs running on it

00:39:04,400 --> 00:39:09,040
um i'll try to follow up with a link to

00:39:07,520 --> 00:39:10,800
engineering blog that we have

00:39:09,040 --> 00:39:12,560
uh maybe there is some more information

00:39:10,800 --> 00:39:15,839
about our kafka infra which i'm

00:39:12,560 --> 00:39:15,839
less familiar with

00:39:16,160 --> 00:39:20,960
um okay kafka and spark have their own

00:39:19,680 --> 00:39:24,240
data streaming api

00:39:20,960 --> 00:39:27,359
based on your experience i would ask

00:39:24,240 --> 00:39:32,000
which one is currently used at linkedin

00:39:27,359 --> 00:39:35,040
for i suppose this is uh streaming data

00:39:32,000 --> 00:39:38,480
um so uh

00:39:35,040 --> 00:39:40,720
we are uh extensive users of kafka

00:39:38,480 --> 00:39:42,240
pretty much uh every type of metric

00:39:40,720 --> 00:39:45,119
every type of

00:39:42,240 --> 00:39:45,520
record that starts in our online world

00:39:45,119 --> 00:39:48,400
or

00:39:45,520 --> 00:39:49,040
our website and ends up in our offline

00:39:48,400 --> 00:39:52,400
world

00:39:49,040 --> 00:39:55,040
passes through kafka and then it depends

00:39:52,400 --> 00:39:56,000
what happens in the way like if it ends

00:39:55,040 --> 00:39:58,560
up directly into

00:39:56,000 --> 00:40:00,560
hdfs using etl pipelines that's usually

00:39:58,560 --> 00:40:05,040
what happens for kafka topics

00:40:00,560 --> 00:40:08,640
and then this is where kafka topics can

00:40:05,040 --> 00:40:10,880
be transformed into data sets

00:40:08,640 --> 00:40:13,040
but we also use samsa in order to

00:40:10,880 --> 00:40:14,000
transform kafka topics into other kafka

00:40:13,040 --> 00:40:18,240
topics

00:40:14,000 --> 00:40:21,280
um spark streaming is less of a use

00:40:18,240 --> 00:40:23,200
for us at linkedin as far as i uh

00:40:21,280 --> 00:40:24,400
know at least so it's definitely not a

00:40:23,200 --> 00:40:27,280
mainstream

00:40:24,400 --> 00:40:27,280
method that we use

00:40:28,400 --> 00:40:35,440
okay for compute efficiency from the

00:40:32,400 --> 00:40:38,000
platform's perspective

00:40:35,440 --> 00:40:39,359
do you come up with a mechanism to drive

00:40:38,000 --> 00:40:40,400
users to develop more efficient

00:40:39,359 --> 00:40:42,400
workloads

00:40:40,400 --> 00:40:44,480
for example team b has low average

00:40:42,400 --> 00:40:46,480
performance score hence can't deploy new

00:40:44,480 --> 00:40:49,680
jobs on production

00:40:46,480 --> 00:40:50,319
okay this is a very good question so uh

00:40:49,680 --> 00:40:53,920
actually

00:40:50,319 --> 00:40:56,560
we have something like this in place

00:40:53,920 --> 00:40:57,839
as part of dr elephant so dr elephant

00:40:56,560 --> 00:41:00,720
creates reports

00:40:57,839 --> 00:41:02,160
uh following each job and if you want to

00:41:00,720 --> 00:41:04,960
nominate your job

00:41:02,160 --> 00:41:07,040
into the production clusters it has to

00:41:04,960 --> 00:41:07,839
uh comply with some of the requirements

00:41:07,040 --> 00:41:10,880
that we've said

00:41:07,839 --> 00:41:13,680
that we've set in dr elephant um

00:41:10,880 --> 00:41:14,240
right now there's no integration between

00:41:13,680 --> 00:41:17,520
uh

00:41:14,240 --> 00:41:19,680
dr elephant uh performance analysis

00:41:17,520 --> 00:41:21,760
uh and grid bench but this is definitely

00:41:19,680 --> 00:41:24,960
something that we're planning to do

00:41:21,760 --> 00:41:27,280
and i expect that you know down the path

00:41:24,960 --> 00:41:28,240
uh we will enable something like this so

00:41:27,280 --> 00:41:30,640
basically

00:41:28,240 --> 00:41:31,599
kind of uh surface to the users that

00:41:30,640 --> 00:41:33,200
their job

00:41:31,599 --> 00:41:34,640
performance score is this and that and

00:41:33,200 --> 00:41:37,680
they should address it

00:41:34,640 --> 00:41:41,200
and so on but this is uh currently just

00:41:37,680 --> 00:41:41,200
you know a thought at this point

00:41:42,560 --> 00:41:47,040
all right um another question do you

00:41:44,880 --> 00:41:48,880
have any visibility on the performance

00:41:47,040 --> 00:41:50,079
on the ongoing spark jobs to catch

00:41:48,880 --> 00:41:53,119
issues before

00:41:50,079 --> 00:41:56,880
sla breaches okay very very good

00:41:53,119 --> 00:41:58,720
uh question um so yeah we do we have the

00:41:56,880 --> 00:42:01,839
golden flows which are running

00:41:58,720 --> 00:42:03,920
uh pretty much uh it depends on the type

00:42:01,839 --> 00:42:05,680
of the golden flow but uh

00:42:03,920 --> 00:42:06,960
most of them are running every hour or

00:42:05,680 --> 00:42:08,960
every two hours

00:42:06,960 --> 00:42:10,640
so that gives us income some kind of an

00:42:08,960 --> 00:42:11,760
early indication if something is about

00:42:10,640 --> 00:42:14,560
to go wrong

00:42:11,760 --> 00:42:16,079
um i suppose you were asking like if we

00:42:14,560 --> 00:42:17,280
have any kind of indication for a

00:42:16,079 --> 00:42:19,280
particular job

00:42:17,280 --> 00:42:20,480
if it's going to miss the sla while it's

00:42:19,280 --> 00:42:22,960
already running

00:42:20,480 --> 00:42:23,599
uh this isn't something that we have

00:42:22,960 --> 00:42:26,079
right now

00:42:23,599 --> 00:42:32,240
but this is a very interesting thought i

00:42:26,079 --> 00:42:32,240
must admit um

00:42:32,400 --> 00:42:36,640
okay what are you looking for forward to

00:42:34,800 --> 00:42:36,960
the most in the future of streaming is

00:42:36,640 --> 00:42:40,560
it

00:42:36,960 --> 00:42:43,920
related to your work at linkedin um

00:42:40,560 --> 00:42:45,680
okay uh very cool question i suppose you

00:42:43,920 --> 00:42:49,119
know what we've discussed so far

00:42:45,680 --> 00:42:49,520
is not designed for streaming jobs at

00:42:49,119 --> 00:42:52,720
all

00:42:49,520 --> 00:42:54,560
this is meant for batch jobs and then uh

00:42:52,720 --> 00:42:56,400
streaming jobs it's a whole different

00:42:54,560 --> 00:42:58,800
story on how you can analyze them

00:42:56,400 --> 00:42:59,839
because metrics you know come on kind of

00:42:58,800 --> 00:43:02,000
a

00:42:59,839 --> 00:43:04,160
continuous basis so you always have to

00:43:02,000 --> 00:43:06,000
keep let's say a window looking back

00:43:04,160 --> 00:43:07,920
and trying to understand what happened

00:43:06,000 --> 00:43:08,960
and where this is going in terms of

00:43:07,920 --> 00:43:11,200
trends

00:43:08,960 --> 00:43:13,839
so it's definitely going to be very

00:43:11,200 --> 00:43:17,680
interesting to see how we can adapt

00:43:13,839 --> 00:43:18,480
those analysis to uh streaming workloads

00:43:17,680 --> 00:43:20,480
which are

00:43:18,480 --> 00:43:22,079
uh you know very different from batch

00:43:20,480 --> 00:43:23,040
workloads that we can just compare

00:43:22,079 --> 00:43:26,240
complete runs

00:43:23,040 --> 00:43:29,760
instead of just windows of runs so

00:43:26,240 --> 00:43:32,240
uh yeah this is not part of the uh

00:43:29,760 --> 00:43:34,160
scope that we're dealing with right now

00:43:32,240 --> 00:43:35,119
like i said spark streaming is not a

00:43:34,160 --> 00:43:38,240
major

00:43:35,119 --> 00:43:41,440
uh uh case uh at the linkedin

00:43:38,240 --> 00:43:44,480
offline infra but this is definitely a

00:43:41,440 --> 00:43:44,480
very interesting topic

00:43:48,400 --> 00:43:53,599
all right so i see that there aren't any

00:43:51,839 --> 00:43:55,599
other questions

00:43:53,599 --> 00:43:57,599
i'd like to thank everyone for attending

00:43:55,599 --> 00:44:00,240
i hope you enjoyed and that

00:43:57,599 --> 00:44:03,119
you learned a few things and there were

00:44:00,240 --> 00:44:06,720
at least a few takeaways

00:44:03,119 --> 00:44:08,240
i'm very happy to have been given this

00:44:06,720 --> 00:44:12,880
opportunity to talk to you guys

00:44:08,240 --> 00:44:12,880
and yeah stay safe and thanks for

00:44:20,839 --> 00:44:23,839
joining

00:44:57,200 --> 00:44:59,280

YouTube URL: https://www.youtube.com/watch?v=D47CSeGpBd0


