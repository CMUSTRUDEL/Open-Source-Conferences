Title: Heating Up Analytical Workloads with Apache Druid
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 2)
Description: 
	Heating Up Analytical Workloads with Apache Druid
Gian Merlino

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Druid is a modern analytical database that implements a memory-mappable storage format, indexes, compression, late tuple materialization, and a vectorized query engine that can operate directly on compressed data. This talk goes into detail on how Druid's query processing layer works, and how each component contributes to achieving top performance for analytical queries. We'll also share war stories and lessons learned from optimizing Druid for different kinds of use cases.

Gian Merlino is CTO and a co-founder of Imply, a San Francisco based technology company, and a committer on Apache Druid. Previously, Gian led the data ingestion team at Metamarkets (now a part of Snapchat) and held senior engineering positions at Yahoo. He holds a BS in Computer Science from Caltech.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:28,240 --> 00:00:33,840
all right great let's get started um

00:00:30,560 --> 00:00:36,239
looks like the chat is uh great here um

00:00:33,840 --> 00:00:37,040
for all those for all you that are in

00:00:36,239 --> 00:00:38,960
the chat please

00:00:37,040 --> 00:00:40,160
feel free to ask questions in the chat

00:00:38,960 --> 00:00:42,239
uh if anything comes up

00:00:40,160 --> 00:00:43,360
and i don't i won't interrupt and i'm

00:00:42,239 --> 00:00:46,399
saying to answer them

00:00:43,360 --> 00:00:50,160
but i will take them at the end um

00:00:46,399 --> 00:00:53,280
and uh all right let's get started

00:00:50,160 --> 00:00:53,680
so um hi uh so today i'm here to talk

00:00:53,280 --> 00:00:55,360
about

00:00:53,680 --> 00:00:56,840
heating up analytical workloads with

00:00:55,360 --> 00:01:00,399
apache druid

00:00:56,840 --> 00:01:03,760
um who am i

00:01:00,399 --> 00:01:06,400
i am jam rolino i'm a committer and pmc

00:01:03,760 --> 00:01:08,880
chair at apache druid and i'm also

00:01:06,400 --> 00:01:11,360
co-founder at imply which is a company

00:01:08,880 --> 00:01:12,640
that is built upon apache jira and where

00:01:11,360 --> 00:01:14,720
we're spending

00:01:12,640 --> 00:01:16,400
a lot of uh we're doing a lot of

00:01:14,720 --> 00:01:17,840
investment into apache gear itself and

00:01:16,400 --> 00:01:20,240
doing a lot of contribution to

00:01:17,840 --> 00:01:21,439
upstream and we're really excited about

00:01:20,240 --> 00:01:22,799
open source and applied so if you're

00:01:21,439 --> 00:01:24,000
interested in working on cool open

00:01:22,799 --> 00:01:26,640
source stuff

00:01:24,000 --> 00:01:30,159
uh like apache cured then we're hiring

00:01:26,640 --> 00:01:30,159
and come talk to us afterwards

00:01:30,479 --> 00:01:34,400
um so today what are we going to talk

00:01:32,320 --> 00:01:36,560
about i'll talk about the

00:01:34,400 --> 00:01:38,079
data temperature spectrum so this title

00:01:36,560 --> 00:01:39,680
of this talk is heating up on local

00:01:38,079 --> 00:01:40,000
workloads talk about what do i mean by

00:01:39,680 --> 00:01:41,920
heating

00:01:40,000 --> 00:01:43,360
things up and we're going to meet the

00:01:41,920 --> 00:01:45,520
pastor druid and we're going to

00:01:43,360 --> 00:01:47,759
then see if we can apply a little bit of

00:01:45,520 --> 00:01:51,680
what we learned about how it works

00:01:47,759 --> 00:01:51,680
to solving some problems

00:01:52,079 --> 00:01:56,159
okay so first off data temperature uh

00:01:54,880 --> 00:01:58,479
what do i mean by that

00:01:56,159 --> 00:01:59,360
so we sort of think of in in the apache

00:01:58,479 --> 00:02:02,320
jury world

00:01:59,360 --> 00:02:03,200
um we sort of think of data ranging on

00:02:02,320 --> 00:02:06,560
the spectrum

00:02:03,200 --> 00:02:09,520
from cold to hot and uh

00:02:06,560 --> 00:02:11,200
that's not saying bad about thing about

00:02:09,520 --> 00:02:12,720
about cold data cold's not bad that's

00:02:11,200 --> 00:02:15,440
not good it's just different

00:02:12,720 --> 00:02:15,840
and how are they different um so the top

00:02:15,440 --> 00:02:19,440
here

00:02:15,840 --> 00:02:22,480
we have uh our hot data representation

00:02:19,440 --> 00:02:23,760
a beautiful bowl of hot cheetos and

00:02:22,480 --> 00:02:25,920
takis and some nice

00:02:23,760 --> 00:02:27,599
cilantro and lime on that very hot and

00:02:25,920 --> 00:02:29,520
fresh

00:02:27,599 --> 00:02:30,879
and hot and fresh is really what what

00:02:29,520 --> 00:02:32,560
hot date is all about so

00:02:30,879 --> 00:02:34,319
on the right hand side these are some

00:02:32,560 --> 00:02:35,280
typical requirements for hot data

00:02:34,319 --> 00:02:37,920
workloads

00:02:35,280 --> 00:02:39,360
you typically want queries to be done

00:02:37,920 --> 00:02:41,519
just a couple seconds

00:02:39,360 --> 00:02:42,640
you typically want to be running on

00:02:41,519 --> 00:02:44,800
fresh data

00:02:42,640 --> 00:02:46,319
you may want to be able to be run on

00:02:44,800 --> 00:02:48,000
real-time data as it comes in for

00:02:46,319 --> 00:02:49,360
example purposes

00:02:48,000 --> 00:02:51,440
you're really interested in high

00:02:49,360 --> 00:02:53,280
concurrency because you

00:02:51,440 --> 00:02:55,120
are going to have a lot of users on the

00:02:53,280 --> 00:02:59,040
platform

00:02:55,120 --> 00:03:01,120
and you are doing highly interactive

00:02:59,040 --> 00:03:03,280
queries that we might call olaf queries

00:03:01,120 --> 00:03:04,640
online analytical queries so slicing

00:03:03,280 --> 00:03:05,760
dicing looking at things from different

00:03:04,640 --> 00:03:10,000
angles

00:03:05,760 --> 00:03:11,599
and the one one really good example of a

00:03:10,000 --> 00:03:12,159
hot analytics application out in the

00:03:11,599 --> 00:03:14,640
wild

00:03:12,159 --> 00:03:16,000
is google analytics so if you use google

00:03:14,640 --> 00:03:17,280
analytics you log into it

00:03:16,000 --> 00:03:19,040
and you get to slice and dice through

00:03:17,280 --> 00:03:21,120
your data you don't just see reports you

00:03:19,040 --> 00:03:24,080
also can sort of explore things

00:03:21,120 --> 00:03:25,760
and i think that the 10 second elevator

00:03:24,080 --> 00:03:27,200
pitch for apache druid is

00:03:25,760 --> 00:03:28,959
if you were building google analytics

00:03:27,200 --> 00:03:30,480
and you wanted to to

00:03:28,959 --> 00:03:32,799
find a good back end for it i bet you

00:03:30,480 --> 00:03:35,440
would be a good back end for that

00:03:32,799 --> 00:03:37,440
um on the other side of the spectrum we

00:03:35,440 --> 00:03:39,280
have colder data

00:03:37,440 --> 00:03:42,000
colder data the requirements there slow

00:03:39,280 --> 00:03:43,120
queries are okay less fresh data is okay

00:03:42,000 --> 00:03:45,200
we're generally aiming for low

00:03:43,120 --> 00:03:47,120
concurrency and our use cases are going

00:03:45,200 --> 00:03:49,200
to be less interactive and more

00:03:47,120 --> 00:03:50,799
oriented around reporting and planning

00:03:49,200 --> 00:03:52,080
and um

00:03:50,799 --> 00:03:54,159
you know we might wonder what's the

00:03:52,080 --> 00:03:55,360
point of this spectrum why

00:03:54,159 --> 00:03:56,799
have this whole thing why not make

00:03:55,360 --> 00:03:57,519
everything hot because hot just sounds

00:03:56,799 --> 00:04:00,000
better

00:03:57,519 --> 00:04:01,680
um and the reason for that is generally

00:04:00,000 --> 00:04:04,879
going to be cost related

00:04:01,680 --> 00:04:06,080
so these uh colder workloads can be

00:04:04,879 --> 00:04:08,080
handled very cheaply

00:04:06,080 --> 00:04:09,519
by having compute that's sort of spun up

00:04:08,080 --> 00:04:10,400
when you need it and spun down you don't

00:04:09,519 --> 00:04:12,159
need it

00:04:10,400 --> 00:04:14,640
it can also be handled cheaply by

00:04:12,159 --> 00:04:16,079
reading things off of cold storage

00:04:14,640 --> 00:04:17,519
instead of having things

00:04:16,079 --> 00:04:19,199
instead of using caches instead of using

00:04:17,519 --> 00:04:21,199
membranes to use a local disk

00:04:19,199 --> 00:04:22,720
and on the hot side we're going to focus

00:04:21,199 --> 00:04:25,360
more on situations where

00:04:22,720 --> 00:04:26,479
the coordinating system is always online

00:04:25,360 --> 00:04:28,000
we're going to be making

00:04:26,479 --> 00:04:29,919
heavy use of we're going to be eating

00:04:28,000 --> 00:04:32,479
the entire buffalo of the hardware

00:04:29,919 --> 00:04:34,320
so we're going to be leveraging memory

00:04:32,479 --> 00:04:37,440
leveraging disk leveraging

00:04:34,320 --> 00:04:38,639
remote storage all to their maximum um

00:04:37,440 --> 00:04:40,000
and and that's kind of where the

00:04:38,639 --> 00:04:43,840
difference is between these two kinds of

00:04:40,000 --> 00:04:43,840
ways of thinking about data come from

00:04:44,400 --> 00:04:48,160
uh so it's here the visual a visual

00:04:46,720 --> 00:04:50,320
representation of what

00:04:48,160 --> 00:04:52,320
a hot analytics application is all about

00:04:50,320 --> 00:04:54,000
uh so this is implied pivot this is the

00:04:52,320 --> 00:04:56,080
application that we built and apply to

00:04:54,000 --> 00:04:58,080
run on top of druid

00:04:56,080 --> 00:04:59,520
and it's really applications like these

00:04:58,080 --> 00:04:59,919
that are widespread exist in the first

00:04:59,520 --> 00:05:01,840
place

00:04:59,919 --> 00:05:03,280
and the reason that i've used videos in

00:05:01,840 --> 00:05:04,720
this slide is

00:05:03,280 --> 00:05:06,800
because i think these kinds of

00:05:04,720 --> 00:05:08,720
applications are really

00:05:06,800 --> 00:05:10,240
shown at their best when they're moving

00:05:08,720 --> 00:05:10,800
they're meant to be interactive you're

00:05:10,240 --> 00:05:13,039
meant to

00:05:10,800 --> 00:05:14,639
sort of drag and drop click on things

00:05:13,039 --> 00:05:16,720
that have cross-filtering

00:05:14,639 --> 00:05:18,160
um and these kinds of of quick

00:05:16,720 --> 00:05:20,560
drag-and-drop interactions

00:05:18,160 --> 00:05:21,440
are possible even on really huge amounts

00:05:20,560 --> 00:05:26,080
of data

00:05:21,440 --> 00:05:28,560
um if you have a great system behind it

00:05:26,080 --> 00:05:29,360
um the use cases for that for why would

00:05:28,560 --> 00:05:31,680
you want this

00:05:29,360 --> 00:05:33,280
i think the two the two interesting or

00:05:31,680 --> 00:05:35,120
two most interesting use cases for why

00:05:33,280 --> 00:05:37,919
you want this are exploration and

00:05:35,120 --> 00:05:39,440
monitoring um exploration meaning you

00:05:37,919 --> 00:05:41,039
know i don't necessarily have

00:05:39,440 --> 00:05:42,880
a specific report that i want to

00:05:41,039 --> 00:05:44,240
generate i'm just trying to understand

00:05:42,880 --> 00:05:46,400
what's going on with my data

00:05:44,240 --> 00:05:47,680
uh maybe i saw some sort of anomaly and

00:05:46,400 --> 00:05:48,560
i want to understand the root cause of

00:05:47,680 --> 00:05:50,320
that anomaly and

00:05:48,560 --> 00:05:52,800
this understanding of root causes of

00:05:50,320 --> 00:05:56,240
things is an exploratory process

00:05:52,800 --> 00:05:58,000
um being able to drag and drop through

00:05:56,240 --> 00:05:59,840
things being able to slice and dice

00:05:58,000 --> 00:06:01,759
through things and having that sub

00:05:59,840 --> 00:06:04,720
second to a couple second response time

00:06:01,759 --> 00:06:05,199
um means the exploratory process is much

00:06:04,720 --> 00:06:08,479
more

00:06:05,199 --> 00:06:09,840
powerful um and then monitoring yeah

00:06:08,479 --> 00:06:11,039
that's the other side of it monitoring

00:06:09,840 --> 00:06:12,400
is all about observability

00:06:11,039 --> 00:06:14,800
it's about seeing what's happening right

00:06:12,400 --> 00:06:16,160
now and on that

00:06:14,800 --> 00:06:19,120
to enable monitoring it's important to

00:06:16,160 --> 00:06:19,120
have real-time congestion

00:06:19,600 --> 00:06:24,560
okay so uh let's talk a little bit about

00:06:22,160 --> 00:06:28,720
architectures and where this all fits in

00:06:24,560 --> 00:06:30,560
uh so um in a typical data architecture

00:06:28,720 --> 00:06:32,400
in a big data or streaming architecture

00:06:30,560 --> 00:06:34,240
you have some data sources

00:06:32,400 --> 00:06:35,600
um you're gonna have a data lake that's

00:06:34,240 --> 00:06:38,560
that's pure storage

00:06:35,600 --> 00:06:39,759
um you're gonna have uh some kind of

00:06:38,560 --> 00:06:41,759
data mover

00:06:39,759 --> 00:06:44,000
that gets data from your data sources

00:06:41,759 --> 00:06:46,960
data lake um the data lake might be

00:06:44,000 --> 00:06:49,120
something like uh hdfs or it might be

00:06:46,960 --> 00:06:52,319
one of the cloud services

00:06:49,120 --> 00:06:52,880
like um s3 and aws or other cloud

00:06:52,319 --> 00:06:56,000
vendors

00:06:52,880 --> 00:06:57,680
have similar offerings um

00:06:56,000 --> 00:06:59,120
but at any rate you'll have a data mover

00:06:57,680 --> 00:07:01,199
that gets data there and some of those

00:06:59,120 --> 00:07:04,080
data movers like apache kafka

00:07:01,199 --> 00:07:05,840
um are actually much they go much much

00:07:04,080 --> 00:07:07,199
beyond any movers i think if the apache

00:07:05,840 --> 00:07:09,120
copper folks heard me call them david

00:07:07,199 --> 00:07:11,440
mover they didn't probably slap me

00:07:09,120 --> 00:07:12,960
um because uh that's not just a david

00:07:11,440 --> 00:07:14,800
mover it's also something that

00:07:12,960 --> 00:07:16,319
is able to be the the center of the

00:07:14,800 --> 00:07:19,280
stream ecosystem

00:07:16,319 --> 00:07:21,440
um but at any rate uh from my

00:07:19,280 --> 00:07:23,520
perspective as a database person i think

00:07:21,440 --> 00:07:26,319
it's i think of that ecosystem as data

00:07:23,520 --> 00:07:28,880
moving um

00:07:26,319 --> 00:07:29,680
where you want to move data to is in the

00:07:28,880 --> 00:07:31,360
lake

00:07:29,680 --> 00:07:33,120
and you can quarry data directly from

00:07:31,360 --> 00:07:35,599
the lake with the corey engine

00:07:33,120 --> 00:07:37,120
a lot of query engines these days work

00:07:35,599 --> 00:07:39,039
like that

00:07:37,120 --> 00:07:40,639
a good example is hive another good

00:07:39,039 --> 00:07:41,840
example is presto

00:07:40,639 --> 00:07:44,319
a lot of the modern cloud data

00:07:41,840 --> 00:07:45,680
warehouses the way that they work under

00:07:44,319 --> 00:07:46,560
the hood is they're going to be pouring

00:07:45,680 --> 00:07:48,160
data out of

00:07:46,560 --> 00:07:49,919
a cloud object store so they are

00:07:48,160 --> 00:07:53,759
essentially acting as corey engines top

00:07:49,919 --> 00:07:55,199
data lakes and

00:07:53,759 --> 00:07:56,879
you can also have a concept of serving

00:07:55,199 --> 00:07:58,240
database and serving database is

00:07:56,879 --> 00:08:01,199
something that would store an

00:07:58,240 --> 00:08:03,199
optimized copy of your data uh for

00:08:01,199 --> 00:08:05,599
serving purposes for serving application

00:08:03,199 --> 00:08:06,240
and for in this spot people would use

00:08:05,599 --> 00:08:09,440
things like

00:08:06,240 --> 00:08:12,800
hbase people these things like cassandra

00:08:09,440 --> 00:08:14,960
and today druid uh

00:08:12,800 --> 00:08:15,840
so druid is is we built drug to be a

00:08:14,960 --> 00:08:18,960
serving layer

00:08:15,840 --> 00:08:20,479
for analytical applications um and

00:08:18,960 --> 00:08:22,960
the history of druid is really the

00:08:20,479 --> 00:08:24,560
history of people migrating to it for

00:08:22,960 --> 00:08:27,599
these kinds of applications

00:08:24,560 --> 00:08:29,599
uh and are generally away from things

00:08:27,599 --> 00:08:31,440
that are either other things that were

00:08:29,599 --> 00:08:33,200
seen in serving layers so like migrating

00:08:31,440 --> 00:08:33,839
an application from hbase to german

00:08:33,200 --> 00:08:37,440
something that

00:08:33,839 --> 00:08:38,719
is pretty common um and

00:08:37,440 --> 00:08:40,800
also migrating from things that you

00:08:38,719 --> 00:08:43,039
would consider from as query engines so

00:08:40,800 --> 00:08:45,120
migrating from like high repressor to it

00:08:43,039 --> 00:08:48,399
for application server

00:08:45,120 --> 00:08:49,600
um and that's because it's it's it's

00:08:48,399 --> 00:08:50,560
something that's like i was saying

00:08:49,600 --> 00:08:53,920
earlier designed

00:08:50,560 --> 00:08:56,240
to occupy this area that's the sort of

00:08:53,920 --> 00:08:58,959
the hottest end of the spectrum

00:08:56,240 --> 00:09:00,080
um but okay how does it what is druid

00:08:58,959 --> 00:09:02,080
how does it work

00:09:00,080 --> 00:09:03,200
um i'm gonna do i'm gonna go through a

00:09:02,080 --> 00:09:05,760
little whirlwind tour

00:09:03,200 --> 00:09:07,279
of what your it is uh and then spend

00:09:05,760 --> 00:09:08,959
some more time talking about how it

00:09:07,279 --> 00:09:10,320
works under the hood

00:09:08,959 --> 00:09:12,480
and i hope that you'll go away from this

00:09:10,320 --> 00:09:14,959
talk with an understanding of

00:09:12,480 --> 00:09:16,000
what architectural properties exactly

00:09:14,959 --> 00:09:17,600
make druid

00:09:16,000 --> 00:09:20,000
something that occupies as part of the

00:09:17,600 --> 00:09:20,000
spectrum

00:09:20,399 --> 00:09:24,320
okay so first off what is droid druid

00:09:22,320 --> 00:09:26,080
power is analytical applications

00:09:24,320 --> 00:09:27,519
um like i mentioned there's there's

00:09:26,080 --> 00:09:29,120
implied pivot that's the app that we

00:09:27,519 --> 00:09:29,519
built for durant there's a bunch of

00:09:29,120 --> 00:09:31,519
other

00:09:29,519 --> 00:09:33,200
analytical applications like booker

00:09:31,519 --> 00:09:35,839
superset metabase

00:09:33,200 --> 00:09:37,360
um custom apps people build with sql uh

00:09:35,839 --> 00:09:40,959
jira does speak sql

00:09:37,360 --> 00:09:41,519
um these uh are third party applications

00:09:40,959 --> 00:09:44,640
that

00:09:41,519 --> 00:09:46,240
uh um well these are all third party

00:09:44,640 --> 00:09:48,560
applications that are built from top of

00:09:46,240 --> 00:09:48,560
jury

00:09:48,839 --> 00:09:54,320
um druid tends to be uh it's gotten some

00:09:52,560 --> 00:09:55,680
some i think pretty exciting and pretty

00:09:54,320 --> 00:09:57,600
good adoption in

00:09:55,680 --> 00:09:59,200
some of the big tech companies so it's

00:09:57,600 --> 00:10:01,120
used by netflix it's used by

00:09:59,200 --> 00:10:03,360
mo pub which is a twitter business unit

00:10:01,120 --> 00:10:05,040
it's used by airbnb

00:10:03,360 --> 00:10:06,720
and what these use cases i'll have in

00:10:05,040 --> 00:10:09,120
common is

00:10:06,720 --> 00:10:10,800
um need for high performance at really

00:10:09,120 --> 00:10:12,480
big scale

00:10:10,800 --> 00:10:14,399
so we're talking hundred plus billion

00:10:12,480 --> 00:10:16,399
rows per day of ingestion we're talking

00:10:14,399 --> 00:10:17,920
trillions of rows retained

00:10:16,399 --> 00:10:20,320
we're talking clusters of hundreds of

00:10:17,920 --> 00:10:22,160
servers and doing all this stuff while

00:10:20,320 --> 00:10:24,079
doing both streaming and batch ingestion

00:10:22,160 --> 00:10:26,959
and still achieving that sub second a

00:10:24,079 --> 00:10:30,240
few seconds query latency

00:10:26,959 --> 00:10:32,640
um the

00:10:30,240 --> 00:10:34,320
benchmarks are always good um one

00:10:32,640 --> 00:10:35,200
benchmark done somewhat recently last

00:10:34,320 --> 00:10:37,680
year was a

00:10:35,200 --> 00:10:38,640
a third party benchmark done by some

00:10:37,680 --> 00:10:41,839
academic folks

00:10:38,640 --> 00:10:44,399
about drew versus presto and hive

00:10:41,839 --> 00:10:45,600
um and the numbers come out looking

00:10:44,399 --> 00:10:46,079
pretty good i would say i would

00:10:45,600 --> 00:10:47,519
categorize

00:10:46,079 --> 00:10:49,120
presto and hive it's this core engine

00:10:47,519 --> 00:10:50,560
kind of system and drew it as a serving

00:10:49,120 --> 00:10:54,000
layer kind of system

00:10:50,560 --> 00:10:55,760
um and uh

00:10:54,000 --> 00:10:57,200
numbers these double-digit triple digit

00:10:55,760 --> 00:10:59,040
numbers are um

00:10:57,200 --> 00:11:00,399
exciting and this is this is why people

00:10:59,040 --> 00:11:02,079
start using druid for these sorts of hot

00:11:00,399 --> 00:11:04,959
workloads

00:11:02,079 --> 00:11:06,640
um at implied we also did some

00:11:04,959 --> 00:11:08,480
benchmarks with jury versus

00:11:06,640 --> 00:11:10,240
a couple of the leading cloud-based data

00:11:08,480 --> 00:11:11,200
warehouses i don't necessarily want to

00:11:10,240 --> 00:11:14,480
call them out by name

00:11:11,200 --> 00:11:16,160
but um suffice to say that

00:11:14,480 --> 00:11:17,519
those things internally are also sort of

00:11:16,160 --> 00:11:19,360
query engine like

00:11:17,519 --> 00:11:20,880
uh they're also querying data lakes and

00:11:19,360 --> 00:11:22,640
we see similar

00:11:20,880 --> 00:11:24,240
advantages in price performance we're

00:11:22,640 --> 00:11:25,200
using apache during versus using one of

00:11:24,240 --> 00:11:27,120
those things for these

00:11:25,200 --> 00:11:29,839
these hot analytic workloads and power

00:11:27,120 --> 00:11:29,839
and data applications

00:11:30,000 --> 00:11:34,160
so okay hopefully that uh

00:11:33,200 --> 00:11:35,440
hopefully that was a little bit of

00:11:34,160 --> 00:11:36,880
motivation to care about the next

00:11:35,440 --> 00:11:37,680
section which is going to get technical

00:11:36,880 --> 00:11:39,279
pretty fast

00:11:37,680 --> 00:11:40,959
um how does this work how are we doing

00:11:39,279 --> 00:11:42,720
this stuff how does apache do it work

00:11:40,959 --> 00:11:44,640
under the hood um

00:11:42,720 --> 00:11:45,920
and you know like i said hopefully this

00:11:44,640 --> 00:11:47,519
will inspire you to check out the

00:11:45,920 --> 00:11:47,920
technology a little bit more and may

00:11:47,519 --> 00:11:50,079
even

00:11:47,920 --> 00:11:51,760
inspire you to uh contribute a little

00:11:50,079 --> 00:11:52,160
bit because um you know we are at apache

00:11:51,760 --> 00:11:56,480
pod

00:11:52,160 --> 00:11:59,519
and uh we're all about that here um

00:11:56,480 --> 00:12:02,560
okay so when we get to how it works uh

00:11:59,519 --> 00:12:02,959
i always like to start with this quote

00:12:02,560 --> 00:12:07,200
from

00:12:02,959 --> 00:12:09,680
linustorboltz uh one of the um

00:12:07,200 --> 00:12:10,800
well and definitely a personality in our

00:12:09,680 --> 00:12:13,360
field

00:12:10,800 --> 00:12:14,800
bad programmers worry about the code

00:12:13,360 --> 00:12:16,560
good programmers worry about data

00:12:14,800 --> 00:12:19,040
structures and their relationships

00:12:16,560 --> 00:12:20,399
and i think this is a very interesting

00:12:19,040 --> 00:12:22,240
quote

00:12:20,399 --> 00:12:23,839
i don't necessarily love the value

00:12:22,240 --> 00:12:24,399
judgments about bad and good but i think

00:12:23,839 --> 00:12:25,760
that the

00:12:24,399 --> 00:12:27,040
the point that he's trying to make is

00:12:25,760 --> 00:12:29,279
the data structures and the way they

00:12:27,040 --> 00:12:31,360
relate to each other is very is critical

00:12:29,279 --> 00:12:32,959
um and it's critical to understanding

00:12:31,360 --> 00:12:33,760
what what sets one system apart from

00:12:32,959 --> 00:12:36,000
another system

00:12:33,760 --> 00:12:38,320
rather than specifics about how the code

00:12:36,000 --> 00:12:41,519
is written or how the code is organized

00:12:38,320 --> 00:12:42,800
so um i'm going to take this to heart

00:12:41,519 --> 00:12:44,560
and we're going to talk about

00:12:42,800 --> 00:12:46,800
how apache thinks about things from a

00:12:44,560 --> 00:12:50,000
data structure perspective and from a

00:12:46,800 --> 00:12:50,000
data layout perspective

00:12:51,360 --> 00:12:54,399
so the first thing to understand about

00:12:52,959 --> 00:12:56,720
apache jared is

00:12:54,399 --> 00:12:58,480
that when you are making queries through

00:12:56,720 --> 00:12:59,200
it it's a sql system like i was saying

00:12:58,480 --> 00:13:00,800
earlier

00:12:59,200 --> 00:13:02,639
um when you make a sql query through

00:13:00,800 --> 00:13:05,040
apache druid you

00:13:02,639 --> 00:13:06,160
uh you send the query to the broker

00:13:05,040 --> 00:13:08,639
which is

00:13:06,160 --> 00:13:10,639
a single server that's going to be the

00:13:08,639 --> 00:13:11,600
responsible for getting that query done

00:13:10,639 --> 00:13:13,200
it's not going to do most of the

00:13:11,600 --> 00:13:14,800
computation most of the computation will

00:13:13,200 --> 00:13:16,959
be done by data servers

00:13:14,800 --> 00:13:18,399
and the data servers have these two

00:13:16,959 --> 00:13:21,519
components that run

00:13:18,399 --> 00:13:24,000
within them historicals and indexers

00:13:21,519 --> 00:13:24,560
indexers are responsible for loading new

00:13:24,000 --> 00:13:26,639
data

00:13:24,560 --> 00:13:28,240
historicals are responsible for serving

00:13:26,639 --> 00:13:29,200
queries on top of data that's already

00:13:28,240 --> 00:13:31,360
been loaded

00:13:29,200 --> 00:13:32,959
um you can actually you see there's an

00:13:31,360 --> 00:13:34,399
arrow here from the broker to the index

00:13:32,959 --> 00:13:35,120
or the broker can actually query data

00:13:34,399 --> 00:13:37,600
that's in the middle

00:13:35,120 --> 00:13:38,720
of being loaded by an indexer and that's

00:13:37,600 --> 00:13:41,199
how real time

00:13:38,720 --> 00:13:42,079
coordinating works in druid so typically

00:13:41,199 --> 00:13:43,440
the most recent

00:13:42,079 --> 00:13:44,880
hour or so data if you're doing a

00:13:43,440 --> 00:13:45,680
streaming ingestion will be served from

00:13:44,880 --> 00:13:46,880
the indexer

00:13:45,680 --> 00:13:49,760
then everything older than that from the

00:13:46,880 --> 00:13:52,959
historical um

00:13:49,760 --> 00:13:54,399
the in the historicals uh i'll focus

00:13:52,959 --> 00:13:56,880
mostly on the historicals for this next

00:13:54,399 --> 00:14:00,079
bit because

00:13:56,880 --> 00:14:00,639
for most workloads what we've seen is

00:14:00,079 --> 00:14:03,920
that

00:14:00,639 --> 00:14:05,040
the limiting factor for performance

00:14:03,920 --> 00:14:06,880
and so what you need to care about the

00:14:05,040 --> 00:14:08,320
most is historical performance and

00:14:06,880 --> 00:14:09,440
that's because there's a pretty small

00:14:08,320 --> 00:14:11,600
slice of data

00:14:09,440 --> 00:14:12,959
that's uh coming in in real time it's

00:14:11,600 --> 00:14:13,440
sort of on the indexers and it's going

00:14:12,959 --> 00:14:16,320
to

00:14:13,440 --> 00:14:18,079
move to the historical soon but the vast

00:14:16,320 --> 00:14:19,839
majority of data all your retention past

00:14:18,079 --> 00:14:21,199
about an hour or so is on historicals

00:14:19,839 --> 00:14:22,320
and so that that's uh

00:14:21,199 --> 00:14:24,720
that's where most of your data is going

00:14:22,320 --> 00:14:27,360
to live um

00:14:24,720 --> 00:14:30,240
in the historicals there are data is

00:14:27,360 --> 00:14:31,920
broken down what we call segments

00:14:30,240 --> 00:14:33,600
pictorially i've represented them as

00:14:31,920 --> 00:14:35,519
these little yellow boxes with columns

00:14:33,600 --> 00:14:37,519
inside them and that's

00:14:35,519 --> 00:14:39,920
not a coincidence that it's a column

00:14:37,519 --> 00:14:42,800
architecture under the hood

00:14:39,920 --> 00:14:44,320
each of these segments has usually about

00:14:42,800 --> 00:14:46,880
a few million rows in it

00:14:44,320 --> 00:14:47,360
um and you can think of each segment as

00:14:46,880 --> 00:14:50,320
like a

00:14:47,360 --> 00:14:50,720
miniature database um this is a concept

00:14:50,320 --> 00:14:52,800
that

00:14:50,720 --> 00:14:54,160
is used by a lot of other analytical

00:14:52,800 --> 00:14:55,839
data systems

00:14:54,160 --> 00:14:57,360
to have these sort of chunked up

00:14:55,839 --> 00:15:00,320
columnar

00:14:57,360 --> 00:15:01,279
segment files data files you can think

00:15:00,320 --> 00:15:03,040
of individual

00:15:01,279 --> 00:15:04,639
work and parquet files and the data lake

00:15:03,040 --> 00:15:07,040
as something like this

00:15:04,639 --> 00:15:08,720
and i think in bigquery there's a tablet

00:15:07,040 --> 00:15:12,320
concept that's like this

00:15:08,720 --> 00:15:15,440
it's a pretty universal concept

00:15:12,320 --> 00:15:17,040
um in druid uh each of these segments is

00:15:15,440 --> 00:15:20,320
partitioned by time

00:15:17,040 --> 00:15:24,560
um and in this example here i've got

00:15:20,320 --> 00:15:26,480
a a a sort of a pre-code example of

00:15:24,560 --> 00:15:28,320
ticket sales being made for various

00:15:26,480 --> 00:15:31,920
artists in various cities

00:15:28,320 --> 00:15:34,560
uh and um what we're gonna do first

00:15:31,920 --> 00:15:35,440
is we're gonna split up data by time um

00:15:34,560 --> 00:15:37,120
we might also

00:15:35,440 --> 00:15:38,720
do a secondary partition if your data is

00:15:37,120 --> 00:15:40,560
really big by some other dimension but

00:15:38,720 --> 00:15:41,920
we'll always pretend my time first

00:15:40,560 --> 00:15:43,680
and the reason we're doing that is to

00:15:41,920 --> 00:15:46,240
enable global time index

00:15:43,680 --> 00:15:47,600
so um this is leveraging the fact that

00:15:46,240 --> 00:15:49,040
for the kinds of applications people

00:15:47,600 --> 00:15:50,720
typically build on druid

00:15:49,040 --> 00:15:53,040
it's really common to have a time filter

00:15:50,720 --> 00:15:55,759
on most of your queries

00:15:53,040 --> 00:15:57,120
and having a global time index uh means

00:15:55,759 --> 00:15:57,920
that we can really quickly narrow down

00:15:57,120 --> 00:15:59,759
to the

00:15:57,920 --> 00:16:01,120
time ranges that relate to a particular

00:15:59,759 --> 00:16:02,720
query um

00:16:01,120 --> 00:16:04,320
so that means each segment in this

00:16:02,720 --> 00:16:07,360
particular example is going to cover

00:16:04,320 --> 00:16:08,000
an hour of data and again if you have

00:16:07,360 --> 00:16:09,759
more data

00:16:08,000 --> 00:16:11,120
in that hour than one segment can hold

00:16:09,759 --> 00:16:11,759
if you have more than a few million rows

00:16:11,120 --> 00:16:12,959
per hour

00:16:11,759 --> 00:16:14,399
then we're just gonna do a secondary

00:16:12,959 --> 00:16:17,839
positioning and split it into multiple

00:16:14,399 --> 00:16:17,839
segments each without a few million rows

00:16:18,639 --> 00:16:21,759
um okay so let's talk a little bit about

00:16:21,120 --> 00:16:24,720
what happens

00:16:21,759 --> 00:16:25,600
inside a segment so inside the segment

00:16:24,720 --> 00:16:27,360
the

00:16:25,600 --> 00:16:28,880
query engine data format are really

00:16:27,360 --> 00:16:30,720
tightly integrated and this is where the

00:16:28,880 --> 00:16:32,320
data structure stuff comes in

00:16:30,720 --> 00:16:34,880
so there's four things i'm going to talk

00:16:32,320 --> 00:16:36,639
about and on the next few slides i'll go

00:16:34,880 --> 00:16:39,199
through an example query

00:16:36,639 --> 00:16:41,199
and i'll trace through how that query

00:16:39,199 --> 00:16:43,279
runs on a specific segment

00:16:41,199 --> 00:16:44,720
um and and hopefully that will help

00:16:43,279 --> 00:16:47,680
illustrate uh

00:16:44,720 --> 00:16:49,040
both how we store data in jira and how

00:16:47,680 --> 00:16:52,480
computations on the data

00:16:49,040 --> 00:16:54,079
actually work so

00:16:52,480 --> 00:16:55,360
the first thing i'll be talking about or

00:16:54,079 --> 00:16:56,399
one thing i'll be talking about is

00:16:55,360 --> 00:16:59,600
compression

00:16:56,399 --> 00:17:00,240
um we compress every uh we compress all

00:16:59,600 --> 00:17:02,240
your data

00:17:00,240 --> 00:17:03,519
by first plate into columns and then

00:17:02,240 --> 00:17:04,000
compressing each column with something

00:17:03,519 --> 00:17:05,199
specific

00:17:04,000 --> 00:17:07,199
to the type of data that's in that

00:17:05,199 --> 00:17:09,600
column this

00:17:07,199 --> 00:17:10,480
is a pretty common column or storage

00:17:09,600 --> 00:17:12,400
technique

00:17:10,480 --> 00:17:14,240
uh i'll talk about our secondary indexes

00:17:12,400 --> 00:17:16,799
so we have bitmap indexes for

00:17:14,240 --> 00:17:17,600
all of your string columns unless you

00:17:16,799 --> 00:17:20,880
have turned them

00:17:17,600 --> 00:17:22,480
off for that particular column um

00:17:20,880 --> 00:17:24,640
i'll talk a bit about how we operate on

00:17:22,480 --> 00:17:24,959
compressed data so one example of that

00:17:24,640 --> 00:17:27,360
is

00:17:24,959 --> 00:17:28,559
well we dictionary code uh all the

00:17:27,360 --> 00:17:30,240
string columns

00:17:28,559 --> 00:17:31,840
and we'll operate on these dictionary

00:17:30,240 --> 00:17:32,720
codes instead of the strings whenever we

00:17:31,840 --> 00:17:34,880
can

00:17:32,720 --> 00:17:36,320
that's one example there's a few others

00:17:34,880 --> 00:17:38,320
and then i'll we'll talk about late

00:17:36,320 --> 00:17:39,760
materialization which is just a fancy

00:17:38,320 --> 00:17:41,760
database personal term

00:17:39,760 --> 00:17:42,799
for not reading things that you don't

00:17:41,760 --> 00:17:46,080
need to read until

00:17:42,799 --> 00:17:48,000
you have to okay

00:17:46,080 --> 00:17:49,840
so let's let's first let's look at a

00:17:48,000 --> 00:17:51,760
segment um

00:17:49,840 --> 00:17:53,120
this is what a segment looks like uh

00:17:51,760 --> 00:17:54,400
this is that same

00:17:53,120 --> 00:17:55,919
state of scheme we were talking about

00:17:54,400 --> 00:17:57,120
earlier so there's a time stamp there's

00:17:55,919 --> 00:17:58,640
an artist there's city

00:17:57,120 --> 00:18:00,400
price and counts this is a ticket sales

00:17:58,640 --> 00:18:02,960
data set

00:18:00,400 --> 00:18:03,679
this particular segment has eight rows

00:18:02,960 --> 00:18:05,520
in it

00:18:03,679 --> 00:18:06,799
uh i know i said they usually have a few

00:18:05,520 --> 00:18:09,120
million um

00:18:06,799 --> 00:18:10,480
but the font size would be too small if

00:18:09,120 --> 00:18:12,799
i tried to put them on here so this one

00:18:10,480 --> 00:18:16,400
has eight

00:18:12,799 --> 00:18:18,640
the order of the columns here uh

00:18:16,400 --> 00:18:19,919
in a sense doesn't matter because uh

00:18:18,640 --> 00:18:21,520
they're all stored separately in the

00:18:19,919 --> 00:18:22,799
segment but in a sense it does matter

00:18:21,520 --> 00:18:24,799
because we store the columns in the same

00:18:22,799 --> 00:18:26,160
order they're sorted in

00:18:24,799 --> 00:18:28,160
and the sorting matters reasons we'll

00:18:26,160 --> 00:18:28,960
talk about later uh maybe a bit later

00:18:28,160 --> 00:18:30,160
but for now

00:18:28,960 --> 00:18:32,400
uh let's not worry too much about the

00:18:30,160 --> 00:18:34,559
orderly columns are stored in um

00:18:32,400 --> 00:18:36,720
the order of the rows is very important

00:18:34,559 --> 00:18:38,720
because it's all correlated so the first

00:18:36,720 --> 00:18:40,400
value in the time column matches the

00:18:38,720 --> 00:18:41,840
first value in the price column which

00:18:40,400 --> 00:18:43,280
matches the first value in the top

00:18:41,840 --> 00:18:46,880
column you take the first value of every

00:18:43,280 --> 00:18:50,320
column and that that equals a row

00:18:46,880 --> 00:18:52,000
um okay so in terms of

00:18:50,320 --> 00:18:53,039
compression we're gonna like i mentioned

00:18:52,000 --> 00:18:53,919
we're gonna compress each one

00:18:53,039 --> 00:18:56,480
differently

00:18:53,919 --> 00:18:57,200
the first column the time column um

00:18:56,480 --> 00:18:59,200
because

00:18:57,200 --> 00:19:01,440
all in this example the timestamps are

00:18:59,200 --> 00:19:03,840
all the same in a real example

00:19:01,440 --> 00:19:04,880
um they wouldn't be necessarily the same

00:19:03,840 --> 00:19:07,919
but they would be

00:19:04,880 --> 00:19:09,600
uh all in a pretty small range because

00:19:07,919 --> 00:19:11,039
this segment remember is storing about

00:19:09,600 --> 00:19:12,559
an hour's worth of data so they'd all be

00:19:11,039 --> 00:19:14,640
within an hour of each other

00:19:12,559 --> 00:19:17,120
so what we'll do there is we'll do some

00:19:14,640 --> 00:19:17,840
sort of encoding where we store the base

00:19:17,120 --> 00:19:19,520
value

00:19:17,840 --> 00:19:21,440
and then every row we don't actually

00:19:19,520 --> 00:19:24,400
store the full time stamp we store

00:19:21,440 --> 00:19:25,440
a bit packed representation of the

00:19:24,400 --> 00:19:28,640
difference between

00:19:25,440 --> 00:19:30,080
that row and the base value this reduces

00:19:28,640 --> 00:19:31,840
the amount of biscuits to sort by row

00:19:30,080 --> 00:19:32,160
and is a is a form of how we can press

00:19:31,840 --> 00:19:33,520
the

00:19:32,160 --> 00:19:36,160
long columns and then on top of that

00:19:33,520 --> 00:19:37,679
we'll do lz4

00:19:36,160 --> 00:19:39,520
we'll do similar things for the data

00:19:37,679 --> 00:19:43,039
section of the string columns

00:19:39,520 --> 00:19:44,799
um the index uh

00:19:43,039 --> 00:19:46,480
we compress that using one of two

00:19:44,799 --> 00:19:48,640
algorithms either concise

00:19:46,480 --> 00:19:49,679
uh bitmap compression or lowering bitmap

00:19:48,640 --> 00:19:52,000
compression

00:19:49,679 --> 00:19:53,440
um roaring is the default now as a

00:19:52,000 --> 00:19:56,640
couple releases go

00:19:53,440 --> 00:19:58,000
uh we do support both um and the

00:19:56,640 --> 00:19:59,280
dictionary is itself a form of

00:19:58,000 --> 00:20:00,400
compression like i was mentioning we

00:19:59,280 --> 00:20:02,640
don't have to actually store

00:20:00,400 --> 00:20:04,080
each individual uh artist's name we can

00:20:02,640 --> 00:20:06,400
just store in the data section we can

00:20:04,080 --> 00:20:07,600
just store each name one time

00:20:06,400 --> 00:20:10,240
and then store the dictionary code in

00:20:07,600 --> 00:20:13,919
the data section

00:20:10,240 --> 00:20:16,559
um and uh okay so this is

00:20:13,919 --> 00:20:18,960
this is how we store our segment let's

00:20:16,559 --> 00:20:22,480
look at the sample query

00:20:18,960 --> 00:20:24,640
so this quarry uh

00:20:22,480 --> 00:20:26,159
is selecting the city and the total

00:20:24,640 --> 00:20:28,159
price um

00:20:26,159 --> 00:20:29,679
for a particular artist grouping by city

00:20:28,159 --> 00:20:31,039
so i was just saying for this particular

00:20:29,679 --> 00:20:33,919
artist justin

00:20:31,039 --> 00:20:35,520
um what are all the cities that

00:20:33,919 --> 00:20:37,280
experience ticket sales for this artist

00:20:35,520 --> 00:20:39,520
and what was the total price of tickets

00:20:37,280 --> 00:20:42,640
in that city so a pretty simple query

00:20:39,520 --> 00:20:45,280
um and let's go through how we are going

00:20:42,640 --> 00:20:45,280
to compute this

00:20:46,559 --> 00:20:49,919
so the first thing that we're going to

00:20:48,559 --> 00:20:50,559
do is we're going to resolve the wear

00:20:49,919 --> 00:20:53,919
filter

00:20:50,559 --> 00:20:56,080
um and um

00:20:53,919 --> 00:20:57,919
we are going to do that by first looking

00:20:56,080 --> 00:20:59,360
the dictionary

00:20:57,919 --> 00:21:00,960
and one important thing with the

00:20:59,360 --> 00:21:01,440
dictionary is that the story is sorted

00:21:00,960 --> 00:21:03,840
in

00:21:01,440 --> 00:21:05,200
lexicographic order and that's important

00:21:03,840 --> 00:21:06,720
because it means that we can do

00:21:05,200 --> 00:21:08,000
binary searches through dictionary to

00:21:06,720 --> 00:21:09,520
find things we don't actually have to do

00:21:08,000 --> 00:21:10,960
it within your scan

00:21:09,520 --> 00:21:12,559
so we don't have to read every valuable

00:21:10,960 --> 00:21:14,720
dictionary just to find a single value

00:21:12,559 --> 00:21:15,600
so here we will probably read two of

00:21:14,720 --> 00:21:17,120
them

00:21:15,600 --> 00:21:18,400
we'll probably be catching first and see

00:21:17,120 --> 00:21:19,039
that's not it and then notice that we

00:21:18,400 --> 00:21:20,960
have to go up

00:21:19,039 --> 00:21:22,080
the binary search will readjust and

00:21:20,960 --> 00:21:25,120
we'll get justin as

00:21:22,080 --> 00:21:29,360
zero um so

00:21:25,120 --> 00:21:31,360
now we know that dictionary code zero

00:21:29,360 --> 00:21:33,919
dictionary id 0 for artist is the one

00:21:31,360 --> 00:21:37,039
that we're looking for

00:21:33,919 --> 00:21:38,799
um next we're doing the index so

00:21:37,039 --> 00:21:40,320
we have the index is stored in the same

00:21:38,799 --> 00:21:42,720
order as the dictionary

00:21:40,320 --> 00:21:44,240
so we just do a random access into the

00:21:42,720 --> 00:21:47,919
index to pick out

00:21:44,240 --> 00:21:51,440
the index values for the first

00:21:47,919 --> 00:21:52,480
entry uh and um this is one of those

00:21:51,440 --> 00:21:55,520
areas where

00:21:52,480 --> 00:21:56,960
it's useful to us that we are going to

00:21:55,520 --> 00:21:57,600
be able to sort of you know like i

00:21:56,960 --> 00:22:01,039
saying

00:21:57,600 --> 00:22:02,880
um use the whole the uh the whole server

00:22:01,039 --> 00:22:04,480
um so these indexes are doing a new sort

00:22:02,880 --> 00:22:05,760
of memory um

00:22:04,480 --> 00:22:06,880
as opposed to being read off disk or

00:22:05,760 --> 00:22:08,400
right off the network or something like

00:22:06,880 --> 00:22:12,960
that

00:22:08,400 --> 00:22:14,640
um so we uh

00:22:12,960 --> 00:22:16,240
uh but nothing the entire segment by the

00:22:14,640 --> 00:22:17,280
way is not sort of memory uh the data

00:22:16,240 --> 00:22:19,440
sections are

00:22:17,280 --> 00:22:20,799
um the data sections may be paged in and

00:22:19,440 --> 00:22:22,720
out

00:22:20,799 --> 00:22:24,159
okay so we're gonna read the index we're

00:22:22,720 --> 00:22:26,880
gonna see that the index uh

00:22:24,159 --> 00:22:27,440
is a bitmap um with this zero one and

00:22:26,880 --> 00:22:31,200
two

00:22:27,440 --> 00:22:33,280
set um and that means that the

00:22:31,200 --> 00:22:34,640
first three rows row zero one and two

00:22:33,280 --> 00:22:38,320
are the rows that have

00:22:34,640 --> 00:22:38,720
uh the value adjusting so okay so now we

00:22:38,320 --> 00:22:40,720
know

00:22:38,720 --> 00:22:42,720
which rows have that value and so the

00:22:40,720 --> 00:22:45,039
where clause has been

00:22:42,720 --> 00:22:46,400
the where clause has been resolved and

00:22:45,039 --> 00:22:47,600
one interesting thing or one important

00:22:46,400 --> 00:22:49,039
thing is that we didn't actually have to

00:22:47,600 --> 00:22:49,840
look at the data section at all for the

00:22:49,039 --> 00:22:51,120
artist column

00:22:49,840 --> 00:22:53,520
even though the filter is on artists we

00:22:51,120 --> 00:22:55,600
only looked at the index and dictionary

00:22:53,520 --> 00:22:59,200
and this is that that concept of being

00:22:55,600 --> 00:22:59,200
really economical about what you read

00:22:59,440 --> 00:23:02,480
um okay so the next thing we're gonna do

00:23:01,440 --> 00:23:05,280
is we

00:23:02,480 --> 00:23:06,400
are going to go to the city column the

00:23:05,280 --> 00:23:08,320
price column we're going to look at the

00:23:06,400 --> 00:23:09,200
first three rows the ones that match our

00:23:08,320 --> 00:23:11,200
filter

00:23:09,200 --> 00:23:12,559
um and we're going to aggregate them

00:23:11,200 --> 00:23:13,200
we're going to do the group by in the

00:23:12,559 --> 00:23:16,320
summing

00:23:13,200 --> 00:23:18,480
in the next stage because

00:23:16,320 --> 00:23:20,480
we know there's only three possible

00:23:18,480 --> 00:23:22,080
cities and three is a small number

00:23:20,480 --> 00:23:24,159
we're going to allocate an array for

00:23:22,080 --> 00:23:25,919
this um if

00:23:24,159 --> 00:23:27,760
there was a larger number of cities or

00:23:25,919 --> 00:23:30,960
we didn't know the number of cities

00:23:27,760 --> 00:23:32,559
um then instead of an array we'd use a

00:23:30,960 --> 00:23:35,200
hash table

00:23:32,559 --> 00:23:36,320
so we use either an array or hash table

00:23:35,200 --> 00:23:37,360
believe it or not

00:23:36,320 --> 00:23:39,520
this is actually a really important

00:23:37,360 --> 00:23:42,240
optimization because when you get

00:23:39,520 --> 00:23:44,320
into the weeds of what databases like

00:23:42,240 --> 00:23:46,080
you would spend their time on

00:23:44,320 --> 00:23:48,240
computing hash codes and doing hashtag

00:23:46,080 --> 00:23:48,799
regulations is actually a pretty big

00:23:48,240 --> 00:23:51,760
chunk of it

00:23:48,799 --> 00:23:52,799
so using an array is a big win uh when

00:23:51,760 --> 00:23:55,120
possible

00:23:52,799 --> 00:23:56,000
um so in this case we are going to use

00:23:55,120 --> 00:23:57,440
an array

00:23:56,000 --> 00:23:59,360
uh we're going to read the first three

00:23:57,440 --> 00:24:01,840
columns where one and or

00:23:59,360 --> 00:24:02,480
we're going to see that that one has one

00:24:01,840 --> 00:24:06,240
value

00:24:02,480 --> 00:24:08,720
29 12. 2 has two values 1800

00:24:06,240 --> 00:24:11,039
and 1953 and they add them together and

00:24:08,720 --> 00:24:14,480
get 37.53

00:24:11,039 --> 00:24:15,840
0 didn't have any values so it's null

00:24:14,480 --> 00:24:18,159
and then we're done we've read the first

00:24:15,840 --> 00:24:20,000
three entries in the city we read the

00:24:18,159 --> 00:24:25,120
first three answers from the price

00:24:20,000 --> 00:24:26,880
um and we have computed a uh aggregation

00:24:25,120 --> 00:24:28,960
next we go to the dictionary and we

00:24:26,880 --> 00:24:29,600
replace those two dictionary ids one and

00:24:28,960 --> 00:24:32,720
two

00:24:29,600 --> 00:24:34,240
um with la sf we don't replace zero

00:24:32,720 --> 00:24:36,240
because there's no reason to because

00:24:34,240 --> 00:24:37,840
the nothing's allocated for zero so we

00:24:36,240 --> 00:24:40,960
ignore that one

00:24:37,840 --> 00:24:41,440
um at this point we have the result we

00:24:40,960 --> 00:24:43,279
have

00:24:41,440 --> 00:24:47,360
uh the result for this segment is

00:24:43,279 --> 00:24:49,440
there's there's 2912 for la 3753 for sf

00:24:47,360 --> 00:24:50,559
and to get the result we did not have to

00:24:49,440 --> 00:24:53,279
read that much we

00:24:50,559 --> 00:24:54,240
had to read um a couple entries from

00:24:53,279 --> 00:24:55,360
dictionary because we were doing a

00:24:54,240 --> 00:24:57,919
binary search

00:24:55,360 --> 00:24:59,360
we had to read one entry from the index

00:24:57,919 --> 00:25:01,520
for one column

00:24:59,360 --> 00:25:02,880
um we had to read two entries in the

00:25:01,520 --> 00:25:04,960
dictionary for another column

00:25:02,880 --> 00:25:05,919
and then we had to read three rows in

00:25:04,960 --> 00:25:09,120
the data section

00:25:05,919 --> 00:25:10,159
of city and price um and so this is this

00:25:09,120 --> 00:25:12,720
is pretty

00:25:10,159 --> 00:25:14,159
minimal in terms of uh what you could

00:25:12,720 --> 00:25:15,679
the lowest you could possibly read for

00:25:14,159 --> 00:25:16,720
this query without pre-computing some of

00:25:15,679 --> 00:25:18,240
the results

00:25:16,720 --> 00:25:20,640
um and that's that's really the key to

00:25:18,240 --> 00:25:23,039
performance the key to performance is

00:25:20,640 --> 00:25:26,480
not doing things and organizing your

00:25:23,039 --> 00:25:26,480
data in a way that enables you to not do

00:25:26,840 --> 00:25:30,720
things

00:25:28,000 --> 00:25:31,360
um okay so the final thing i want to

00:25:30,720 --> 00:25:34,159
talk about

00:25:31,360 --> 00:25:35,520
is sort of practical applications of uh

00:25:34,159 --> 00:25:37,360
some of what we've learned

00:25:35,520 --> 00:25:38,880
um or implications of it and how to

00:25:37,360 --> 00:25:40,960
think like um

00:25:38,880 --> 00:25:42,960
how to think with a database person when

00:25:40,960 --> 00:25:45,840
you are modeling your data and when

00:25:42,960 --> 00:25:45,840
you're using the database

00:25:46,000 --> 00:25:49,200
um so i want to talk about the power of

00:25:48,320 --> 00:25:51,760
data layout

00:25:49,200 --> 00:25:53,760
so uh in the previous slides we had this

00:25:51,760 --> 00:25:56,320
sort of schematic

00:25:53,760 --> 00:25:57,760
this a schematic version of what a

00:25:56,320 --> 00:25:58,640
column looks like this data section

00:25:57,760 --> 00:26:02,640
dictionary section

00:25:58,640 --> 00:26:04,320
they have section let's consider a

00:26:02,640 --> 00:26:07,760
a column that's a bit longer instead of

00:26:04,320 --> 00:26:09,840
eight rows let's consider only 16 rows

00:26:07,760 --> 00:26:10,880
and one thing that i mentioned in

00:26:09,840 --> 00:26:14,159
passing in the

00:26:10,880 --> 00:26:16,000
previous slides uh is that we're also

00:26:14,159 --> 00:26:16,480
going to lz4 compress these data

00:26:16,000 --> 00:26:18,480
sections

00:26:16,480 --> 00:26:20,000
on top of our regular big packing and

00:26:18,480 --> 00:26:22,720
that's because it turns out that when

00:26:20,000 --> 00:26:23,919
you apply those two together

00:26:22,720 --> 00:26:26,640
they tend to stack top of each other

00:26:23,919 --> 00:26:27,919
pretty well so we're going to lz4

00:26:26,640 --> 00:26:31,760
compress each of these chunks

00:26:27,919 --> 00:26:33,840
um lc4 is a compression algorithm that

00:26:31,760 --> 00:26:34,799
requires decompressing an entire chunk

00:26:33,840 --> 00:26:38,000
at a time

00:26:34,799 --> 00:26:40,559
so if we split this row or the segment

00:26:38,000 --> 00:26:42,400
up into four row chunks and compressed

00:26:40,559 --> 00:26:45,440
each four row chunks separately

00:26:42,400 --> 00:26:47,360
um what that means is that

00:26:45,440 --> 00:26:48,480
in order to read a single row out of the

00:26:47,360 --> 00:26:50,240
chunk we have to actually read the

00:26:48,480 --> 00:26:51,440
entire chunk decompress the entire chunk

00:26:50,240 --> 00:26:53,520
of lc4

00:26:51,440 --> 00:26:55,039
and then we can do a random access into

00:26:53,520 --> 00:26:56,480
that chunk to get the bits and then we

00:26:55,039 --> 00:26:59,919
can decode the bits

00:26:56,480 --> 00:27:01,919
um and this is uh

00:26:59,919 --> 00:27:03,679
this we only need to read the chunks

00:27:01,919 --> 00:27:06,320
that have

00:27:03,679 --> 00:27:07,919
data that we need to read but we want to

00:27:06,320 --> 00:27:09,600
avoid situations where we're reading

00:27:07,919 --> 00:27:12,000
very small amounts out of a large number

00:27:09,600 --> 00:27:12,000
of chunks

00:27:12,400 --> 00:27:19,440
so one way to do that is that if you

00:27:16,000 --> 00:27:20,320
are often if you're often filtering by

00:27:19,440 --> 00:27:23,360
something

00:27:20,320 --> 00:27:25,200
that is very selective

00:27:23,360 --> 00:27:26,880
um so let's say there's a there's a

00:27:25,200 --> 00:27:29,120
column that you often filter on

00:27:26,880 --> 00:27:30,640
and that column is is very selective in

00:27:29,120 --> 00:27:32,480
the sense that filters on our column

00:27:30,640 --> 00:27:34,080
tend to narrow down your data set to a

00:27:32,480 --> 00:27:35,760
very small percentage down to

00:27:34,080 --> 00:27:37,679
one percent or half percent or two

00:27:35,760 --> 00:27:40,799
percent of the overall data

00:27:37,679 --> 00:27:41,760
um it really is a good idea to also sort

00:27:40,799 --> 00:27:43,760
by that column

00:27:41,760 --> 00:27:45,279
um partitioning store by that column and

00:27:43,760 --> 00:27:47,039
sorting by that column means that we

00:27:45,279 --> 00:27:49,120
minimize the number of blocks to read

00:27:47,039 --> 00:27:50,480
when filtering out that column um

00:27:49,120 --> 00:27:51,279
because all the blocks for the same

00:27:50,480 --> 00:27:52,799
filter

00:27:51,279 --> 00:27:54,240
all the rows with the same filter value

00:27:52,799 --> 00:27:55,279
are likely to be together in the same

00:27:54,240 --> 00:27:56,799
few blocks

00:27:55,279 --> 00:27:58,159
and so here let's say that's artist

00:27:56,799 --> 00:28:00,080
let's say that we actually have three

00:27:58,159 --> 00:28:01,279
artists maybe we have

00:28:00,080 --> 00:28:03,520
thousands of artists we're always

00:28:01,279 --> 00:28:05,200
filtering by artists and each one has a

00:28:03,520 --> 00:28:07,440
small slice the overall data set so we

00:28:05,200 --> 00:28:10,559
might sort on that

00:28:07,440 --> 00:28:12,000
and here instead of each one being all

00:28:10,559 --> 00:28:14,880
spread out

00:28:12,000 --> 00:28:16,480
no artist is in more than two chunks and

00:28:14,880 --> 00:28:17,760
because each artist is no more than two

00:28:16,480 --> 00:28:19,039
chunks if we're filtering out a single

00:28:17,760 --> 00:28:20,799
artist we never have to read more than

00:28:19,039 --> 00:28:22,240
two chunks to satisfy a query as opposed

00:28:20,799 --> 00:28:25,200
to potentially having to read

00:28:22,240 --> 00:28:25,919
all four chunks uh in the for example in

00:28:25,200 --> 00:28:27,200
the

00:28:25,919 --> 00:28:29,440
in the middle thing you can see that

00:28:27,200 --> 00:28:31,039
that kesha number one um

00:28:29,440 --> 00:28:32,880
there's a one in every chunk and so we

00:28:31,039 --> 00:28:34,960
would have to decompress every chunk in

00:28:32,880 --> 00:28:37,760
order to read those rows

00:28:34,960 --> 00:28:38,480
so this is good this is uh this is good

00:28:37,760 --> 00:28:42,320
um

00:28:38,480 --> 00:28:45,120
and uh how do we actually do it um

00:28:42,320 --> 00:28:45,679
well the way that you do it in druid is

00:28:45,120 --> 00:28:47,279
you

00:28:45,679 --> 00:28:48,880
put the thing you want to sort by first

00:28:47,279 --> 00:28:51,279
in the dimensions list

00:28:48,880 --> 00:28:52,640
um rows are sorted first by time and

00:28:51,279 --> 00:28:53,679
then are sorted by dimensions in the

00:28:52,640 --> 00:28:55,120
order that you provide them with the

00:28:53,679 --> 00:28:56,320
dimension spec if you haven't used root

00:28:55,120 --> 00:28:56,559
before and this doesn't look familiar to

00:28:56,320 --> 00:28:59,520
you

00:28:56,559 --> 00:29:01,200
don't worry about it it's part of the

00:28:59,520 --> 00:29:02,320
definition for how to load data integer

00:29:01,200 --> 00:29:06,240
which is written in its

00:29:02,320 --> 00:29:08,880
json specification for a data loader

00:29:06,240 --> 00:29:10,840
um so whatever is first here is going to

00:29:08,880 --> 00:29:13,840
be immediately after time in the sort

00:29:10,840 --> 00:29:13,840
order

00:29:14,080 --> 00:29:18,000
there's also a little trick that i

00:29:16,240 --> 00:29:19,600
always love these little tricks

00:29:18,000 --> 00:29:21,279
there's a little trick that at some

00:29:19,600 --> 00:29:21,600
point i think will be unnecessary

00:29:21,279 --> 00:29:23,279
because

00:29:21,600 --> 00:29:24,960
i think it's likely we'll want to make

00:29:23,279 --> 00:29:27,039
this a first class feature

00:29:24,960 --> 00:29:28,960
but right now it's a trick uh as of the

00:29:27,039 --> 00:29:32,240
latest version 19.

00:29:28,960 --> 00:29:32,720
um and this is what if you want to put

00:29:32,240 --> 00:29:34,159
time

00:29:32,720 --> 00:29:35,919
after artist what if you want to sort by

00:29:34,159 --> 00:29:37,279
artist first and then time as opposed to

00:29:35,919 --> 00:29:38,559
sorting by time first you know what i

00:29:37,279 --> 00:29:41,760
was saying earlier is that

00:29:38,559 --> 00:29:44,960
we implicitly always sort by time first

00:29:41,760 --> 00:29:48,320
well the way to do that is you can

00:29:44,960 --> 00:29:50,880
make a secondary timestamp column that

00:29:48,320 --> 00:29:52,320
has a timestamp that comes your original

00:29:50,880 --> 00:29:54,159
data

00:29:52,320 --> 00:29:55,440
and then make the primary timestamp

00:29:54,159 --> 00:29:57,919
column um

00:29:55,440 --> 00:29:59,360
the same granularity as your petitioning

00:29:57,919 --> 00:30:01,039
so let's say you're positioning by day

00:29:59,360 --> 00:30:03,440
that means every segment has one day of

00:30:01,039 --> 00:30:05,120
data uh petition by day and make the

00:30:03,440 --> 00:30:06,799
primary time stamp just a day

00:30:05,120 --> 00:30:08,480
all those values are the same because

00:30:06,799 --> 00:30:10,080
they're all the same even though

00:30:08,480 --> 00:30:11,600
technically we're sorting by it first

00:30:10,080 --> 00:30:13,279
it's not going to affect the sort order

00:30:11,600 --> 00:30:15,279
because they're all the same anyway

00:30:13,279 --> 00:30:16,880
um and then the secondary timestamp

00:30:15,279 --> 00:30:18,240
column will have

00:30:16,880 --> 00:30:20,720
what you would call the real-time stamp

00:30:18,240 --> 00:30:22,880
in it and that can be put anywhere you

00:30:20,720 --> 00:30:24,000
want in sort order so this is sort of a

00:30:22,880 --> 00:30:27,120
fun little trick

00:30:24,000 --> 00:30:28,559
um that uh you know it

00:30:27,120 --> 00:30:29,840
like i was saying i prefer to not be a

00:30:28,559 --> 00:30:31,760
trick i prefer it to be a first class

00:30:29,840 --> 00:30:34,960
feature but for now it's a trick

00:30:31,760 --> 00:30:36,399
um i just wanted to show the impact of

00:30:34,960 --> 00:30:39,279
doing stuff like this

00:30:36,399 --> 00:30:40,640
on a potential impact of doing stuff

00:30:39,279 --> 00:30:43,440
like this on

00:30:40,640 --> 00:30:45,760
data layout and on performance so here's

00:30:43,440 --> 00:30:48,559
a real world example of a flame graph

00:30:45,760 --> 00:30:50,960
which is an awesome tool for measuring

00:30:48,559 --> 00:30:53,600
performance and understand performance

00:30:50,960 --> 00:30:54,240
uh here's a real world flame graph um

00:30:53,600 --> 00:30:56,159
showing

00:30:54,240 --> 00:30:57,519
uh what performance is like on a query a

00:30:56,159 --> 00:31:00,080
particular data set that had these

00:30:57,519 --> 00:31:01,679
features i mentioned it did have a

00:31:00,080 --> 00:31:03,519
these queries were all filtering on a

00:31:01,679 --> 00:31:06,159
certain column

00:31:03,519 --> 00:31:07,440
that column uh was such that every

00:31:06,159 --> 00:31:08,559
filter would hit about one or two

00:31:07,440 --> 00:31:10,880
percent of the data

00:31:08,559 --> 00:31:12,320
and it was all jumbled up um it was all

00:31:10,880 --> 00:31:14,000
jumbled up such that a lot of queries

00:31:12,320 --> 00:31:16,880
had to read a lot of chunks and then

00:31:14,000 --> 00:31:18,960
decode small numbers of rows from those

00:31:16,880 --> 00:31:20,480
chunks um

00:31:18,960 --> 00:31:22,960
and here's what you get what you get is

00:31:20,480 --> 00:31:26,240
look at all this lc4 decompression

00:31:22,960 --> 00:31:26,880
all these arrows point to um time being

00:31:26,240 --> 00:31:29,519
spent

00:31:26,880 --> 00:31:31,440
just decompressing lz4 we're spending so

00:31:29,519 --> 00:31:34,320
much time doing this

00:31:31,440 --> 00:31:36,720
uh in this case in this world real world

00:31:34,320 --> 00:31:38,399
example what we did was we

00:31:36,720 --> 00:31:39,840
did both of those things we changed the

00:31:38,399 --> 00:31:42,720
sort order to put that column first and

00:31:39,840 --> 00:31:46,640
we did the secondary timestamp trick

00:31:42,720 --> 00:31:48,880
and same queries

00:31:46,640 --> 00:31:50,159
um eight times faster uh eight times

00:31:48,880 --> 00:31:53,120
lower cpu time

00:31:50,159 --> 00:31:54,880
um and the overall profile looks a lot

00:31:53,120 --> 00:31:55,360
nicer it looks a lot more well behaved

00:31:54,880 --> 00:31:57,840
instead of

00:31:55,360 --> 00:31:58,640
lz4 decompression being 90 plus percent

00:31:57,840 --> 00:32:00,799
of the time

00:31:58,640 --> 00:32:02,080
it's now you know it's it's there's

00:32:00,799 --> 00:32:04,159
still we're still doing a lot of it but

00:32:02,080 --> 00:32:05,919
that makes sense because

00:32:04,159 --> 00:32:08,080
we need to do it to deal with worry it's

00:32:05,919 --> 00:32:09,519
it's um but it's not

00:32:08,080 --> 00:32:10,960
it's not everything we're doing we're

00:32:09,519 --> 00:32:12,080
seeing we're seeing actual visible

00:32:10,960 --> 00:32:13,440
amounts of time being spent in other

00:32:12,080 --> 00:32:15,519
parts of the quarry stack

00:32:13,440 --> 00:32:16,720
and the overall cpu time per quarry is

00:32:15,519 --> 00:32:20,399
eight times less

00:32:16,720 --> 00:32:22,480
um so

00:32:20,399 --> 00:32:23,600
i hope that this i hope in this talk you

00:32:22,480 --> 00:32:27,200
got a little bit of a flavor

00:32:23,600 --> 00:32:28,880
of what is the purpose of

00:32:27,200 --> 00:32:30,559
uh thinking about data on a temperature

00:32:28,880 --> 00:32:33,519
spectrum um

00:32:30,559 --> 00:32:33,840
why we in the apache georgia world think

00:32:33,519 --> 00:32:35,600
that

00:32:33,840 --> 00:32:37,440
that druid is a good solution on the hot

00:32:35,600 --> 00:32:38,880
side of that spectrum when you really do

00:32:37,440 --> 00:32:39,360
want to use everything you want to use

00:32:38,880 --> 00:32:41,840
memory

00:32:39,360 --> 00:32:43,039
disk and deep storage and all the stuff

00:32:41,840 --> 00:32:45,120
to the fullest

00:32:43,039 --> 00:32:46,480
um and how thinking about the way that

00:32:45,120 --> 00:32:49,120
databases lay out

00:32:46,480 --> 00:32:49,760
data and how they process it thinking

00:32:49,120 --> 00:32:52,640
about that

00:32:49,760 --> 00:32:54,240
and then using those thoughts to control

00:32:52,640 --> 00:32:55,919
how they do it can actually make really

00:32:54,240 --> 00:32:57,840
huge differences in the performance you

00:32:55,919 --> 00:32:58,320
get out of databases like druid and

00:32:57,840 --> 00:32:59,840
others

00:32:58,320 --> 00:33:02,080
these sorts of tricks work on any sort

00:32:59,840 --> 00:33:04,960
of problem database

00:33:02,080 --> 00:33:07,039
um okay i think uh that's that's the

00:33:04,960 --> 00:33:09,519
last slide i had so

00:33:07,039 --> 00:33:10,880
thank you all for coming stay in touch

00:33:09,519 --> 00:33:12,159
follow us on twitter we have a twitter

00:33:10,880 --> 00:33:15,679
if you're in iowa

00:33:12,159 --> 00:33:16,320
um the uh please join the community if

00:33:15,679 --> 00:33:18,200
you're interested

00:33:16,320 --> 00:33:19,760
in chatting more about druid it's at

00:33:18,200 --> 00:33:22,480
juror.apache.org

00:33:19,760 --> 00:33:24,080
we're on asf slack we do meet ups which

00:33:22,480 --> 00:33:25,519
are all virtual now one day they'll stop

00:33:24,080 --> 00:33:27,120
being virtual again

00:33:25,519 --> 00:33:28,559
um and of course like i mentioned

00:33:27,120 --> 00:33:30,640
earlier we're hiring for

00:33:28,559 --> 00:33:31,600
uh working under it that implies you're

00:33:30,640 --> 00:33:34,720
interested in

00:33:31,600 --> 00:33:39,440
that kind of thing uh

00:33:34,720 --> 00:33:44,240
thank you let's see q a um

00:33:39,440 --> 00:33:44,240
do we have anything in the chat

00:33:47,440 --> 00:33:51,919
i don't see anything but um

00:33:50,559 --> 00:33:53,760
now it's more minutes so hang out for

00:33:51,919 --> 00:33:56,960
two hours and then uh and then i'm gonna

00:33:53,760 --> 00:33:56,960
go check out some of the other sessions

00:34:01,120 --> 00:34:05,120
uh do you have a link to the benchmark

00:34:02,640 --> 00:34:07,840
from druid and presto and hive

00:34:05,120 --> 00:34:09,359
um i think if you search for it you'll

00:34:07,840 --> 00:34:12,240
find it why don't i search for it and

00:34:09,359 --> 00:34:12,240
see if i can find it

00:34:18,839 --> 00:34:21,839
um

00:34:22,399 --> 00:34:25,040
yeah here it is

00:34:27,599 --> 00:34:31,919
uh what was the tool used for profiling

00:34:29,839 --> 00:34:35,040
time spent decompression yeah great

00:34:31,919 --> 00:34:39,440
question um

00:34:35,040 --> 00:34:42,800
uh that was swiss java knife uh

00:34:39,440 --> 00:34:45,359
and um here's a link that both has

00:34:42,800 --> 00:34:47,119
a analogies article at our site that has

00:34:45,359 --> 00:34:49,200
a link on how to download it and also

00:34:47,119 --> 00:34:50,399
some instructions on how to use it it's

00:34:49,200 --> 00:35:03,839
a really cool tool

00:34:50,399 --> 00:35:03,839
one of my favorite tools

00:35:05,119 --> 00:35:08,720
all right well thank you all for coming

00:35:06,640 --> 00:35:10,320
um and i'm gonna go take a break and

00:35:08,720 --> 00:35:14,079
then head check out some other sessions

00:35:10,320 --> 00:35:14,079
and uh have a good time at apache town

00:35:14,839 --> 00:35:17,839
everybody

00:35:38,240 --> 00:35:40,320

YouTube URL: https://www.youtube.com/watch?v=2o8P2UTKa4I


