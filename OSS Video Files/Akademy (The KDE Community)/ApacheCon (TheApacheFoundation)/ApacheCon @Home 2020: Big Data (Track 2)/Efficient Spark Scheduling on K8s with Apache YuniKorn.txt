Title: Efficient Spark Scheduling on K8s with Apache YuniKorn
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 2)
Description: 
	Efficient Spark Scheduling on K8s with Apache YuniKorn
Weiwei Yang, Gao Li

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Yunicorn (Incubating) is a new open-source project, which is a standalone resource scheduler of container orchestration platforms. Currently, it provides a fully functional resource scheduler alternative for K8s that manages and schedules Big Data workloads. We embrace Apache Spark for data engineering and machine learning, and by running Spark on K8s, we are able to exploit compute power promisingly under such highly elastic, scalable, and multi-paradigm architecture. We made a lot of effort on enhancing the core resource scheduling, in order to bring high performance, efficient-sharing, and multi-tenancy oriented capabilities to Spark jobs. In this talk, we will focus on revealing the architecture of the cloud-native infrastructure; How we leverage YuniKorn scheduler to redefine the resource scheduling on Cloud. We will introduce how YuniKorn manages quotas, resource sharing, and auto-scaling, and ultimately how to schedule large scale Spark jobs efficiently on Kubernetes in the cloud.

Weiwei Yang:
Weiwei Yang is a Staff Software Engineer from Cloudera, an Apache Hadoop committer and PMC member. He is focused on technology around large scale, hybrid computation systems. Before Cloudera, he worked in Alibaba’s realtime computation infrastructure team that serves large scale big data workloads. Currently, Weiwei is leading the efforts for resource scheduling and management on K8s. Weiwei holds a master’s degree from Peking University.
Gao Li:
Li Gao is an engineering lead and infrastructure software engineer from Databricks, a leading open source and cloud vendor focusing on unified analytics cloud. Li’s focuses are mainly on large scale distributed infrastructure across public cloud vendors and bringing kubernetes to the AI and big data compute workloads. Prior to Databricks, Li has led major data infrastructure and data platform efforts in companies such as Lyft, Fitbit, Salesforce, etc.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,000 --> 00:00:26,880
great

00:00:24,400 --> 00:00:28,560
okay uh i guess we can we can get

00:00:26,880 --> 00:00:31,119
started

00:00:28,560 --> 00:00:33,760
so let me share my screen and hand it

00:00:31,119 --> 00:00:33,760
over to you

00:00:36,800 --> 00:00:40,480
okay i think you can get started all

00:00:39,040 --> 00:00:43,680
right thanks everyone

00:00:40,480 --> 00:00:45,760
so oh thank you for coming and today's

00:00:43,680 --> 00:00:48,000
topic we are talking about uh

00:00:45,760 --> 00:00:50,879
having efficient spark scheduling with

00:00:48,000 --> 00:00:53,360
attached unicorn

00:00:50,879 --> 00:00:53,360
next time

00:00:54,079 --> 00:00:58,079
so have a brief introduction uh my name

00:00:57,360 --> 00:01:00,079
is liga

00:00:58,079 --> 00:01:02,239
i'm currently i'm the tech lead on

00:01:00,079 --> 00:01:04,159
databricks a computer fabric team

00:01:02,239 --> 00:01:05,360
previously i was leading the data

00:01:04,159 --> 00:01:07,280
infrastructure

00:01:05,360 --> 00:01:09,040
especially the compute infrastructure at

00:01:07,280 --> 00:01:12,080
lyft and uh

00:01:09,040 --> 00:01:13,840
wayway young right now he is detected at

00:01:12,080 --> 00:01:16,880
cloudera compute platform

00:01:13,840 --> 00:01:18,159
and he is a apache group committer and

00:01:16,880 --> 00:01:20,400
the pmc member

00:01:18,159 --> 00:01:21,680
previously technically that real-time

00:01:20,400 --> 00:01:27,840
computing fraud at

00:01:21,680 --> 00:01:27,840
alibaba next

00:01:31,119 --> 00:01:38,479
all right so before we introduce the

00:01:35,119 --> 00:01:40,880
the apache unicorn 97 stage

00:01:38,479 --> 00:01:43,280
right now for apache spark there are

00:01:40,880 --> 00:01:46,560
different ways to run spark at a

00:01:43,280 --> 00:01:49,280
scale or in production so

00:01:46,560 --> 00:01:51,360
this one gives a highlight of what a

00:01:49,280 --> 00:01:54,640
potential ways to run spark

00:01:51,360 --> 00:01:57,840
currently there are different

00:01:54,640 --> 00:01:59,040
uh front uh flows of uh follow this

00:01:57,840 --> 00:02:00,880
upgrade you can see there's

00:01:59,040 --> 00:02:02,479
typically there are two different types

00:02:00,880 --> 00:02:04,799
of uh

00:02:02,479 --> 00:02:07,360
persona that can bookspark there are

00:02:04,799 --> 00:02:10,319
users interactive users

00:02:07,360 --> 00:02:12,000
data engineers scientists and deep

00:02:10,319 --> 00:02:14,000
learning scientists and so on

00:02:12,000 --> 00:02:15,120
and there's also different kinds of job

00:02:14,000 --> 00:02:18,319
spots and the

00:02:15,120 --> 00:02:21,200
script that can talk to you through

00:02:18,319 --> 00:02:22,080
different sets of api horsepower program

00:02:21,200 --> 00:02:25,599
api

00:02:22,080 --> 00:02:29,120
there are also job service apis that

00:02:25,599 --> 00:02:31,599
eventually come down to a core component

00:02:29,120 --> 00:02:33,040
in spark which is sparkle and scheduler

00:02:31,599 --> 00:02:34,480
that can launch a different compute

00:02:33,040 --> 00:02:36,720
resources

00:02:34,480 --> 00:02:37,760
in the beginning spark has a uh uh the

00:02:36,720 --> 00:02:40,720
majority of the

00:02:37,760 --> 00:02:41,120
uh production worker running apache uh

00:02:40,720 --> 00:02:44,080
uh

00:02:41,120 --> 00:02:44,959
hadoop and young specifically in hadoop

00:02:44,080 --> 00:02:47,920
200 and

00:02:44,959 --> 00:02:50,000
later there is also laser os and the

00:02:47,920 --> 00:02:51,120
recent trend is on kubernetes and

00:02:50,000 --> 00:02:53,440
obviously there's also

00:02:51,120 --> 00:02:54,319
a long cluster mode which is standalone

00:02:53,440 --> 00:02:57,440
mode

00:02:54,319 --> 00:03:01,120
those one can run on user public cloud

00:02:57,440 --> 00:03:01,599
or on permanent premise and or hybrid of

00:03:01,120 --> 00:03:03,200
both

00:03:01,599 --> 00:03:05,840
and the storage sticker talks to

00:03:03,200 --> 00:03:11,360
different blob storage and

00:03:05,840 --> 00:03:11,360
or block storage next site

00:03:16,840 --> 00:03:22,560
so the

00:03:19,440 --> 00:03:25,040
one of the reason we want to use uh

00:03:22,560 --> 00:03:25,920
kubernetes to run smart uh uh in the

00:03:25,040 --> 00:03:29,280
recent trend

00:03:25,920 --> 00:03:32,879
is uh kubernetes is one of the

00:03:29,280 --> 00:03:35,680
uh right now is the most popular

00:03:32,879 --> 00:03:36,319
container orchestration engine uh not

00:03:35,680 --> 00:03:38,720
just for

00:03:36,319 --> 00:03:40,159
spark but for many other micro services

00:03:38,720 --> 00:03:43,120
and so on so

00:03:40,159 --> 00:03:44,640
the advantage of uh choosing kubernetes

00:03:43,120 --> 00:03:46,959
to run this part these days

00:03:44,640 --> 00:03:49,519
is first a corrupt it provides a shared

00:03:46,959 --> 00:03:51,840
resource capability that can

00:03:49,519 --> 00:03:54,239
that can share the computer resource

00:03:51,840 --> 00:03:56,480
between different types of jobs

00:03:54,239 --> 00:03:58,640
and also because of the nature of

00:03:56,480 --> 00:04:01,120
containerization it can support multiple

00:03:58,640 --> 00:04:02,799
smart versions python versions

00:04:01,120 --> 00:04:05,120
and even conversion controllers

00:04:02,799 --> 00:04:05,519
different containers in a shared cluster

00:04:05,120 --> 00:04:08,000
so

00:04:05,519 --> 00:04:10,239
that's much more efficient and

00:04:08,000 --> 00:04:10,959
fine-grained control of those resource

00:04:10,239 --> 00:04:14,080
usage

00:04:10,959 --> 00:04:15,200
on cluster for both a faster duration on

00:04:14,080 --> 00:04:18,880
the development side

00:04:15,200 --> 00:04:22,000
and a stable uh production rollout

00:04:18,880 --> 00:04:22,320
in the production site uh the third one

00:04:22,000 --> 00:04:24,639
uh

00:04:22,320 --> 00:04:25,680
that's important to choose kubernetes to

00:04:24,639 --> 00:04:29,199
run spark

00:04:25,680 --> 00:04:29,919
is with the this one uh kubernetes

00:04:29,199 --> 00:04:33,040
provides

00:04:29,919 --> 00:04:34,000
uh uh a forward compatible unified

00:04:33,040 --> 00:04:36,240
infrastructure

00:04:34,000 --> 00:04:38,960
not just for data compute but for many

00:04:36,240 --> 00:04:42,880
of microservice ecosystems that

00:04:38,960 --> 00:04:44,880
can have a single pane of observability

00:04:42,880 --> 00:04:49,600
and the resource isolation support

00:04:44,880 --> 00:04:53,120
or in a single uh thing of glass

00:04:49,600 --> 00:04:56,400
the last one is this one's critical for

00:04:53,120 --> 00:04:58,720
enterprise support is in container

00:04:56,400 --> 00:04:59,759
world in the containerized kubernetes

00:04:58,720 --> 00:05:01,840
the

00:04:59,759 --> 00:05:03,199
access control can be very fine-grained

00:05:01,840 --> 00:05:03,759
at a different part level and a

00:05:03,199 --> 00:05:07,360
different

00:05:03,759 --> 00:05:09,199
uh and at a different uh

00:05:07,360 --> 00:05:10,960
at a different job level and which is

00:05:09,199 --> 00:05:13,360
the much harder to do

00:05:10,960 --> 00:05:14,639
in the past in a non-containerized

00:05:13,360 --> 00:05:17,360
resource compute

00:05:14,639 --> 00:05:17,360
next slide please

00:05:18,639 --> 00:05:24,320
so beyond this kubernetes there's also a

00:05:22,160 --> 00:05:25,840
pattern emerging in the last

00:05:24,320 --> 00:05:27,759
couple years which is called

00:05:25,840 --> 00:05:29,680
multi-cluster compute

00:05:27,759 --> 00:05:32,320
using kubernetes uh because the

00:05:29,680 --> 00:05:34,960
capability of kubernetes can be

00:05:32,320 --> 00:05:35,520
federated across many different clusters

00:05:34,960 --> 00:05:38,960
so

00:05:35,520 --> 00:05:41,840
imagine is even larger compute world

00:05:38,960 --> 00:05:42,320
you can have this food clusters using

00:05:41,840 --> 00:05:45,600
different

00:05:42,320 --> 00:05:48,880
uh i call here the data uh gateway api

00:05:45,600 --> 00:05:51,600
and that can share persistence or

00:05:48,880 --> 00:05:52,800
or have isolated resources and with

00:05:51,600 --> 00:05:55,039
shared metadata

00:05:52,800 --> 00:05:57,280
that multi-cluster compute can even

00:05:55,039 --> 00:05:59,360
still cluster compute

00:05:57,280 --> 00:06:01,280
on kubernetes even more beyond a single

00:05:59,360 --> 00:06:02,639
cluster which is a pattern emerging that

00:06:01,280 --> 00:06:05,600
the unicorn

00:06:02,639 --> 00:06:06,000
apache required later will introduce the

00:06:05,600 --> 00:06:08,080
future

00:06:06,000 --> 00:06:10,000
roadmap that can start supporting

00:06:08,080 --> 00:06:14,800
multi-cluster compute for

00:06:10,000 --> 00:06:14,800
spark and kubernetes next slide please

00:06:15,280 --> 00:06:20,560
um before i dive into the uh the

00:06:18,479 --> 00:06:22,080
deeper on that bone let's go back to a

00:06:20,560 --> 00:06:25,280
single in a single

00:06:22,080 --> 00:06:28,319
spark job in a single uh spark uh

00:06:25,280 --> 00:06:30,160
uh kubernetes cluster this is a

00:06:28,319 --> 00:06:31,680
anatomy of the running a sparkling

00:06:30,160 --> 00:06:33,520
kubernetes uh

00:06:31,680 --> 00:06:34,800
the reason i introduced this one is this

00:06:33,520 --> 00:06:38,000
one is key to

00:06:34,800 --> 00:06:40,080
for us to understand how the existing

00:06:38,000 --> 00:06:42,080
uh sparkling kubernetes works through

00:06:40,080 --> 00:06:46,800
the cluster manager that is part of the

00:06:42,080 --> 00:06:48,479
apache spark and in the kubernetes world

00:06:46,800 --> 00:06:51,199
this cluster manager is talking to the

00:06:48,479 --> 00:06:54,160
group api to allocate individual

00:06:51,199 --> 00:06:55,360
uh parts in the driver part executor

00:06:54,160 --> 00:06:58,479
part the driver

00:06:55,360 --> 00:07:00,560
uh also sending to the coupe api to

00:06:58,479 --> 00:07:04,000
allocate those different parts

00:07:00,560 --> 00:07:06,479
but from the scheduler perspective uh uh

00:07:04,000 --> 00:07:07,280
the coupe scheduler by default uh in the

00:07:06,479 --> 00:07:09,280
default

00:07:07,280 --> 00:07:11,840
in the default scheduler you can only

00:07:09,280 --> 00:07:12,319
allocate at the level of the load and

00:07:11,840 --> 00:07:14,720
part

00:07:12,319 --> 00:07:16,560
uh to do the binding right now the

00:07:14,720 --> 00:07:19,680
missing piece is the

00:07:16,560 --> 00:07:23,039
advocacy on the spark app level so

00:07:19,680 --> 00:07:25,199
that creates a few issues as we scale

00:07:23,039 --> 00:07:27,520
the spark on kubernetes next slide

00:07:25,199 --> 00:07:27,520
please

00:07:29,360 --> 00:07:35,440
so uh one of the issues uh uh

00:07:32,479 --> 00:07:35,759
as i illustrated in the previous slide

00:07:35,440 --> 00:07:38,160
is

00:07:35,759 --> 00:07:39,039
uh default scheduler is a part level

00:07:38,160 --> 00:07:42,160
scheduling

00:07:39,039 --> 00:07:45,280
and uh in the uh

00:07:42,160 --> 00:07:47,840
uh in uh uh

00:07:45,280 --> 00:07:49,039
scheduling uh scenario you can imagine

00:07:47,840 --> 00:07:50,960
on kubernetes

00:07:49,039 --> 00:07:52,960
when there's many different uh spark

00:07:50,960 --> 00:07:55,280
clusters uh requests coming in

00:07:52,960 --> 00:07:56,960
to allocate these different parts

00:07:55,280 --> 00:07:59,840
because the group scheduler

00:07:56,960 --> 00:08:01,120
is focusing on the power level there

00:07:59,840 --> 00:08:03,440
there are

00:08:01,120 --> 00:08:05,280
situations there can be resource racing

00:08:03,440 --> 00:08:08,560
between different parts from different

00:08:05,280 --> 00:08:12,800
uh smart clusters which can

00:08:08,560 --> 00:08:13,759
make the jobs become less efficient for

00:08:12,800 --> 00:08:16,960
uh for both

00:08:13,759 --> 00:08:18,800
resource utilization and uh

00:08:16,960 --> 00:08:21,039
the latency perspective for different

00:08:18,800 --> 00:08:23,360
jobs that's one

00:08:21,039 --> 00:08:25,440
issues we've seen in the past for the

00:08:23,360 --> 00:08:25,919
default scheduler of kubernetes to first

00:08:25,440 --> 00:08:29,440
part

00:08:25,919 --> 00:08:29,440
another one is uh

00:08:29,759 --> 00:08:34,080
we tried using the kubernetes part

00:08:32,560 --> 00:08:35,519
priority which is uh

00:08:34,080 --> 00:08:38,479
one of the features building with

00:08:35,519 --> 00:08:40,479
kubernetes the the power of priority can

00:08:38,479 --> 00:08:42,640
set the scheduling preference or the

00:08:40,479 --> 00:08:44,720
priority between different parts you can

00:08:42,640 --> 00:08:47,279
potentially attack those different

00:08:44,720 --> 00:08:49,279
spark parts to you say there's a

00:08:47,279 --> 00:08:50,720
priority schedule between those one but

00:08:49,279 --> 00:08:54,000
the problem with that

00:08:50,720 --> 00:08:55,920
part uh priority is aesthetic so

00:08:54,000 --> 00:08:58,240
it becomes harder as you have many

00:08:55,920 --> 00:08:59,200
different types of spark jobs or spark

00:08:58,240 --> 00:09:02,320
clusters coming

00:08:59,200 --> 00:09:04,800
on on this given kubernetes cluster it's

00:09:02,320 --> 00:09:07,600
become even harder to dynamically adjust

00:09:04,800 --> 00:09:09,120
the priority parameter or relative

00:09:07,600 --> 00:09:12,240
priorities between different

00:09:09,120 --> 00:09:14,399
spark jobs and also there's no guarantee

00:09:12,240 --> 00:09:16,480
the higher priorities will actually get

00:09:14,399 --> 00:09:19,600
scheduled across a different point

00:09:16,480 --> 00:09:23,360
uh different nodes uh and the last one

00:09:19,600 --> 00:09:24,320
is the common issue we're seeing is

00:09:23,360 --> 00:09:27,040
there's

00:09:24,320 --> 00:09:27,440
uh for certain jobs especially like uh

00:09:27,040 --> 00:09:30,320
uh

00:09:27,440 --> 00:09:30,959
high priority streaming jobs there are

00:09:30,320 --> 00:09:33,519
uh

00:09:30,959 --> 00:09:34,560
need for uh uh first thing first our

00:09:33,519 --> 00:09:38,880
scheduling

00:09:34,560 --> 00:09:41,040
uh for the for the job as a whole

00:09:38,880 --> 00:09:42,480
instead of individual parts of the job

00:09:41,040 --> 00:09:45,519
for those cases the

00:09:42,480 --> 00:09:48,160
the coup scheduler is lacking a

00:09:45,519 --> 00:09:48,640
guarantee of the uh fiber scheduling for

00:09:48,160 --> 00:09:52,160
us

00:09:48,640 --> 00:09:54,959
during the resource racing scenario

00:09:52,160 --> 00:09:54,959
next slide please

00:09:57,200 --> 00:10:00,640
so let's continue there are a few other

00:09:59,600 --> 00:10:04,240
issues

00:10:00,640 --> 00:10:06,959
and even a larger kubernetes

00:10:04,240 --> 00:10:07,839
we've seen in the past one of the is a

00:10:06,959 --> 00:10:10,160
higher latency

00:10:07,839 --> 00:10:11,120
for scheduler when you have a lot of

00:10:10,160 --> 00:10:14,720
different

00:10:11,120 --> 00:10:18,000
uh part allocations on

00:10:14,720 --> 00:10:19,680
a given large

00:10:18,000 --> 00:10:21,839
kubernetes cluster for example you have

00:10:19,680 --> 00:10:24,320
a a

00:10:21,839 --> 00:10:25,680
large number of hundreds of loads

00:10:24,320 --> 00:10:27,760
approaching a thousand nodes

00:10:25,680 --> 00:10:28,959
there are scheduling if you have a lot

00:10:27,760 --> 00:10:31,200
of parts coming in

00:10:28,959 --> 00:10:33,519
everything can be as high as about 100

00:10:31,200 --> 00:10:36,560
seconds which can introduce

00:10:33,519 --> 00:10:37,920
unacceptable latency to your jobs and

00:10:36,560 --> 00:10:41,360
the

00:10:37,920 --> 00:10:43,519
the one is the fair

00:10:41,360 --> 00:10:44,560
sharing of the resources become very

00:10:43,519 --> 00:10:47,760
unpredictable

00:10:44,560 --> 00:10:51,120
in those clusters large clusters

00:10:47,760 --> 00:10:54,320
for the default scheduler even without

00:10:51,120 --> 00:10:58,399
fibo that the fair sharing is not there

00:10:54,320 --> 00:11:00,240
or not reliably working and

00:10:58,399 --> 00:11:02,480
in certain workloads large clusters

00:11:00,240 --> 00:11:04,640
especially for those job oriented

00:11:02,480 --> 00:11:05,680
clusters there's always a lead to

00:11:04,640 --> 00:11:07,920
balance between

00:11:05,680 --> 00:11:09,360
the different fiber and fair

00:11:07,920 --> 00:11:11,120
requirements to share those with the

00:11:09,360 --> 00:11:14,320
compute resources but it's

00:11:11,120 --> 00:11:16,959
uh not there to achieve with

00:11:14,320 --> 00:11:18,560
using the default scheduler uh there are

00:11:16,959 --> 00:11:21,440
also a few things that is

00:11:18,560 --> 00:11:22,640
uh from usability side to have a

00:11:21,440 --> 00:11:25,920
visibility from

00:11:22,640 --> 00:11:28,640
operational and the production uh

00:11:25,920 --> 00:11:29,120
production usage of those large clusters

00:11:28,640 --> 00:11:31,200
is

00:11:29,120 --> 00:11:32,800
uh we need a way to manage dynamically

00:11:31,200 --> 00:11:35,120
manage the

00:11:32,800 --> 00:11:36,240
hierarchy of the priorities or hierarchy

00:11:35,120 --> 00:11:40,560
of the different

00:11:36,240 --> 00:11:43,680
uh fibro parameters and also the

00:11:40,560 --> 00:11:46,800
we from the operation site there's a

00:11:43,680 --> 00:11:49,680
lead for a richer and

00:11:46,800 --> 00:11:53,200
online uh user visibility into what's

00:11:49,680 --> 00:11:56,320
going on with scheduling and see

00:11:53,200 --> 00:11:59,440
how the scheduled latency is so always

00:11:56,320 --> 00:12:02,399
is not available at current stage with

00:11:59,440 --> 00:12:02,399
the default scheduler

00:12:04,240 --> 00:12:09,040
so next slide so

00:12:09,200 --> 00:12:12,480
wherever you are continue so we are now

00:12:11,360 --> 00:12:15,120
moving on to

00:12:12,480 --> 00:12:16,800
introduce how yurikon the new scheduler

00:12:15,120 --> 00:12:20,160
can help to scaling

00:12:16,800 --> 00:12:24,000
spark on kubernetes thank you lee

00:12:20,160 --> 00:12:26,560
um so lee just introduced

00:12:24,000 --> 00:12:28,320
the the pro the motivation why we move

00:12:26,560 --> 00:12:32,320
spiral concubine that is

00:12:28,320 --> 00:12:33,360
and the problems we saw uh where we won

00:12:32,320 --> 00:12:36,959
large scale

00:12:33,360 --> 00:12:38,720
spark and kubernetes so eventually

00:12:36,959 --> 00:12:40,800
so sparky is a unified data process

00:12:38,720 --> 00:12:44,240
engine and it's really powerful

00:12:40,800 --> 00:12:45,839
but it's also very complex right so um

00:12:44,240 --> 00:12:47,360
right now we have kubernetes so we can

00:12:45,839 --> 00:12:50,160
build a common

00:12:47,360 --> 00:12:50,880
infrastructure to to run our spark

00:12:50,160 --> 00:12:54,639
reloads

00:12:50,880 --> 00:12:56,800
workloads on on on prime and cloud

00:12:54,639 --> 00:12:58,959
public cloud private cloud ali cloud

00:12:56,800 --> 00:13:02,079
right this is very flexible

00:12:58,959 --> 00:13:05,360
but when user starts lands on kubernetes

00:13:02,079 --> 00:13:09,279
um their business logic is complex so

00:13:05,360 --> 00:13:11,360
they have a lot of a lot of types of

00:13:09,279 --> 00:13:12,560
jobs they have ad hoc queries they have

00:13:11,360 --> 00:13:15,279
bad jobs

00:13:12,560 --> 00:13:16,240
they have workflows and they have stream

00:13:15,279 --> 00:13:19,279
jobs

00:13:16,240 --> 00:13:21,760
what does this mean so potentially

00:13:19,279 --> 00:13:22,480
this could be very large skill and also

00:13:21,760 --> 00:13:25,519
it can

00:13:22,480 --> 00:13:27,120
have both batch and long running jobs uh

00:13:25,519 --> 00:13:29,040
when you run long-running jobs on

00:13:27,120 --> 00:13:29,760
kubernetes that's easy but you when you

00:13:29,040 --> 00:13:32,639
run

00:13:29,760 --> 00:13:34,480
large-scale bad jobs on kubernetes it's

00:13:32,639 --> 00:13:36,800
become problematic

00:13:34,480 --> 00:13:38,959
and also uh in spark some of the deep

00:13:36,800 --> 00:13:39,920
link pipelines will have very strict

00:13:38,959 --> 00:13:42,240
constraints

00:13:39,920 --> 00:13:44,399
in this case you usually you need the

00:13:42,240 --> 00:13:47,199
schedule to help you to guarantee

00:13:44,399 --> 00:13:49,519
for example the gun scheduling and also

00:13:47,199 --> 00:13:51,279
um we are facing the issues that we have

00:13:49,519 --> 00:13:52,000
multiple users or teams to share the

00:13:51,279 --> 00:13:54,240
environment

00:13:52,000 --> 00:13:55,519
how we can plan the capacity in the

00:13:54,240 --> 00:13:58,320
cluster how we can

00:13:55,519 --> 00:14:00,079
make sure these teams or users can can

00:13:58,320 --> 00:14:01,120
work with each other peacefully so

00:14:00,079 --> 00:14:04,399
that's a

00:14:01,120 --> 00:14:07,120
that's also a very big challenge um

00:14:04,399 --> 00:14:07,519
and also when user starts to consume the

00:14:07,120 --> 00:14:10,800
pro

00:14:07,519 --> 00:14:11,440
platform you we will need to make sure

00:14:10,800 --> 00:14:15,519
their

00:14:11,440 --> 00:14:19,760
job priorities and sras can be satisfied

00:14:15,519 --> 00:14:22,880
so all these are very complex problems

00:14:19,760 --> 00:14:24,720
we do not have a solution today so

00:14:22,880 --> 00:14:26,000
the challenges can fall into three

00:14:24,720 --> 00:14:29,680
categories

00:14:26,000 --> 00:14:31,600
um the first one is resource management

00:14:29,680 --> 00:14:34,480
we needed uh fine green resource

00:14:31,600 --> 00:14:36,800
management comparing today on kubernetes

00:14:34,480 --> 00:14:39,120
we need to pursue the balance between

00:14:36,800 --> 00:14:41,440
the research sharing and the efficiency

00:14:39,120 --> 00:14:43,680
for multi-tenancy development this is

00:14:41,440 --> 00:14:45,680
something missing today in kubernetes

00:14:43,680 --> 00:14:46,880
the second challenge is the job

00:14:45,680 --> 00:14:50,000
scheduling uh

00:14:46,880 --> 00:14:52,000
likely just mentioned so on the

00:14:50,000 --> 00:14:54,320
default kubernetes scheduler doesn't do

00:14:52,000 --> 00:14:56,480
any of the job scheduling so it only

00:14:54,320 --> 00:14:58,480
schedules the parts

00:14:56,480 --> 00:15:01,199
it doesn't have any job level scheduling

00:14:58,480 --> 00:15:05,279
capability which is not very good for

00:15:01,199 --> 00:15:07,600
for for our complex scenarios uh

00:15:05,279 --> 00:15:09,199
uh essentially kubernetes service

00:15:07,600 --> 00:15:11,920
oriented resource scheduler

00:15:09,199 --> 00:15:12,639
that doesn't that cannot satisfy the

00:15:11,920 --> 00:15:15,440
needs

00:15:12,639 --> 00:15:17,040
to run complex big data scenarios and

00:15:15,440 --> 00:15:20,399
also the performance

00:15:17,040 --> 00:15:22,399
this is also very important when you run

00:15:20,399 --> 00:15:24,000
a lot of jobs on your cluster the

00:15:22,399 --> 00:15:25,680
performance means

00:15:24,000 --> 00:15:28,000
the higher performance means you can

00:15:25,680 --> 00:15:30,399
save more cost

00:15:28,000 --> 00:15:31,519
this is a critical factor to reduce the

00:15:30,399 --> 00:15:34,000
to control the cost

00:15:31,519 --> 00:15:36,320
and also can improve the sales of the

00:15:34,000 --> 00:15:36,320
jobs

00:15:36,959 --> 00:15:40,480
then um that's why we we started a

00:15:39,920 --> 00:15:42,639
project

00:15:40,480 --> 00:15:43,920
um apache unicorn is an incubating

00:15:42,639 --> 00:15:46,800
project right now

00:15:43,920 --> 00:15:48,480
and um we've been starting we've been

00:15:46,800 --> 00:15:52,000
working on this project since

00:15:48,480 --> 00:15:53,120
early 2019. uh it is a standalone

00:15:52,000 --> 00:15:56,160
resource manager

00:15:53,120 --> 00:15:58,800
for for kubernetes we built

00:15:56,160 --> 00:16:00,399
we have built all the essential

00:15:58,800 --> 00:16:01,040
scheduling capabilities into this

00:16:00,399 --> 00:16:04,079
scheduler

00:16:01,040 --> 00:16:07,199
to make sure we can address the

00:16:04,079 --> 00:16:10,720
the needs to run big data on kubernetes

00:16:07,199 --> 00:16:13,360
and we have used the decoupled design

00:16:10,720 --> 00:16:14,480
so we have a abstraction of the schedule

00:16:13,360 --> 00:16:16,639
interface

00:16:14,480 --> 00:16:17,759
so unicorn is not just can run on

00:16:16,639 --> 00:16:21,600
kubernetes it can run

00:16:17,759 --> 00:16:22,399
for it can um port it to other system as

00:16:21,600 --> 00:16:24,959
well

00:16:22,399 --> 00:16:24,959
very easily

00:16:25,920 --> 00:16:30,240
the major features we we have included

00:16:28,160 --> 00:16:33,040
in unicorn includes

00:16:30,240 --> 00:16:33,519
the first one is the hierarchy of queues

00:16:33,040 --> 00:16:35,600
so it

00:16:33,519 --> 00:16:37,279
provides the fine grain control over the

00:16:35,600 --> 00:16:39,680
resources for different tenants

00:16:37,279 --> 00:16:41,040
right that means can can set up the

00:16:39,680 --> 00:16:44,079
hierarchy cues

00:16:41,040 --> 00:16:47,199
possibly that can can map to the

00:16:44,079 --> 00:16:49,199
organizing organization architecture the

00:16:47,199 --> 00:16:52,480
structure of the organization

00:16:49,199 --> 00:16:53,759
and set up some min and max q capacity

00:16:52,480 --> 00:16:56,320
to make sure

00:16:53,759 --> 00:16:56,959
each of the queue can get its own fair

00:16:56,320 --> 00:16:59,360
sharing

00:16:56,959 --> 00:17:00,000
the minimal resource and also have a

00:16:59,360 --> 00:17:03,600
control

00:17:00,000 --> 00:17:07,039
on the max quota so this kind of

00:17:03,600 --> 00:17:07,360
a mojo um this kind of mode can define

00:17:07,039 --> 00:17:09,520
how

00:17:07,360 --> 00:17:10,559
elastic can it can be for each of the

00:17:09,520 --> 00:17:13,679
resource

00:17:10,559 --> 00:17:15,839
cues and the second

00:17:13,679 --> 00:17:17,679
feature is the job scheduling which is

00:17:15,839 --> 00:17:20,559
very very important for

00:17:17,679 --> 00:17:21,039
for for big data workloads so unicorn

00:17:20,559 --> 00:17:23,439
will

00:17:21,039 --> 00:17:24,640
cue the jobs in resource queues and

00:17:23,439 --> 00:17:27,839
schedule them

00:17:24,640 --> 00:17:30,880
with respect to the certain ordering

00:17:27,839 --> 00:17:32,720
on policy the ordering policy is is the

00:17:30,880 --> 00:17:34,400
key to ensure that the jobs can be

00:17:32,720 --> 00:17:38,720
scheduled

00:17:34,400 --> 00:17:39,760
as expected and also the policy can be

00:17:38,720 --> 00:17:42,480
customized

00:17:39,760 --> 00:17:43,360
right now we have fifo we have fair uh

00:17:42,480 --> 00:17:46,240
depends on

00:17:43,360 --> 00:17:48,880
the different needs and also we are

00:17:46,240 --> 00:17:50,400
working on the priority support so

00:17:48,880 --> 00:17:52,480
from the job level can do a lot of

00:17:50,400 --> 00:17:55,200
things

00:17:52,480 --> 00:17:57,360
and the third is the essential

00:17:55,200 --> 00:17:59,520
capabilities so this means

00:17:57,360 --> 00:18:01,280
we will need this essential scheduling

00:17:59,520 --> 00:18:05,039
capabilities to ensure that

00:18:01,280 --> 00:18:09,280
high performance of the scheduling and

00:18:05,039 --> 00:18:10,880
and also we have the needs to support

00:18:09,280 --> 00:18:12,320
support some of the scheduling features

00:18:10,880 --> 00:18:14,640
like gun scheduling

00:18:12,320 --> 00:18:16,559
resource reservation preemption all

00:18:14,640 --> 00:18:19,520
these features are essential

00:18:16,559 --> 00:18:21,200
for running big data or the ai workloads

00:18:19,520 --> 00:18:24,320
and kubernetes

00:18:21,200 --> 00:18:26,480
the last one is cloud native so unicom

00:18:24,320 --> 00:18:28,240
from the first day we are designing

00:18:26,480 --> 00:18:32,640
designing it to be a lightweight

00:18:28,240 --> 00:18:35,120
and very very easy to extend

00:18:32,640 --> 00:18:37,360
this is a scheduler that can be easily

00:18:35,120 --> 00:18:39,360
installed on kubernetes

00:18:37,360 --> 00:18:41,039
and it can work with the cluster auto

00:18:39,360 --> 00:18:45,120
scaler very easily

00:18:41,039 --> 00:18:46,960
to be running on on cloud and

00:18:45,120 --> 00:18:49,760
also you can work with the auto scaler

00:18:46,960 --> 00:18:52,400
to scale up and down the compute push

00:18:49,760 --> 00:18:54,840
it is a stately service and can be very

00:18:52,400 --> 00:18:57,760
very easily

00:18:54,840 --> 00:19:00,880
deployed

00:18:57,760 --> 00:19:02,080
um let's go over look into a little more

00:19:00,880 --> 00:19:05,120
detail about

00:19:02,080 --> 00:19:08,480
how spark is running with unicorn

00:19:05,120 --> 00:19:10,720
this is a complete example

00:19:08,480 --> 00:19:12,400
this is the user's view and this is what

00:19:10,720 --> 00:19:15,840
happens in kubernetes and

00:19:12,400 --> 00:19:17,840
this is um what happens in unicorn so

00:19:15,840 --> 00:19:20,480
at the beginning user submits a spark

00:19:17,840 --> 00:19:22,160
job uh it can be down by spar submits

00:19:20,480 --> 00:19:24,640
where it can be done by

00:19:22,160 --> 00:19:26,640
by creating a spark application crd

00:19:24,640 --> 00:19:28,000
which is uh which leverage the spark

00:19:26,640 --> 00:19:30,160
operator

00:19:28,000 --> 00:19:31,919
so um from kubernetes we can see the

00:19:30,160 --> 00:19:34,640
spark driver will be

00:19:31,919 --> 00:19:36,000
will be pending so this is how spark

00:19:34,640 --> 00:19:38,880
works right the

00:19:36,000 --> 00:19:39,760
the first part will be the driver and

00:19:38,880 --> 00:19:43,120
the driver will

00:19:39,760 --> 00:19:46,799
apply for the executor parts then

00:19:43,120 --> 00:19:47,360
in unicorn um we are putting the driver

00:19:46,799 --> 00:19:50,400
pod

00:19:47,360 --> 00:19:52,160
to we know that this is from one of the

00:19:50,400 --> 00:19:55,120
one of the application and i'll put the

00:19:52,160 --> 00:19:58,240
application into one of the leave queue

00:19:55,120 --> 00:19:59,600
in the hierarchy and then inside of the

00:19:58,240 --> 00:20:01,919
unicorn we start to

00:19:59,600 --> 00:20:02,799
allocate the resource for this for this

00:20:01,919 --> 00:20:06,400
part

00:20:02,799 --> 00:20:08,159
what do we do um a big difference

00:20:06,400 --> 00:20:10,159
comparing to the default scheduler we

00:20:08,159 --> 00:20:12,640
will do the all the

00:20:10,159 --> 00:20:13,919
sort of the cues we'll find which queue

00:20:12,640 --> 00:20:17,200
needs the resource most

00:20:13,919 --> 00:20:19,440
then we go to that queue and then do our

00:20:17,200 --> 00:20:20,799
sorting on the applications within that

00:20:19,440 --> 00:20:24,720
queue

00:20:20,799 --> 00:20:26,559
then we pick up application then

00:20:24,720 --> 00:20:29,039
do another starting on the request to

00:20:26,559 --> 00:20:32,559
find the report to allocate resource

00:20:29,039 --> 00:20:34,080
so this gives us some of the we can have

00:20:32,559 --> 00:20:37,039
some policies on the queue level

00:20:34,080 --> 00:20:37,760
and on the app level and the request

00:20:37,039 --> 00:20:41,120
level

00:20:37,760 --> 00:20:44,720
uh to determine how to sort them

00:20:41,120 --> 00:20:46,799
and once we make a location um so

00:20:44,720 --> 00:20:47,760
and here now the driver puzzle depending

00:20:46,799 --> 00:20:50,080
on the system

00:20:47,760 --> 00:20:52,080
the java is still in the starting phase

00:20:50,080 --> 00:20:55,360
once we have made the application

00:20:52,080 --> 00:20:58,320
the unicorn will ask the aps

00:20:55,360 --> 00:20:59,520
api server to bind to the pod to a node

00:20:58,320 --> 00:21:01,120
and

00:20:59,520 --> 00:21:02,880
so we received from kubernetes versus

00:21:01,120 --> 00:21:03,520
the job driver part is found to a

00:21:02,880 --> 00:21:05,679
specific

00:21:03,520 --> 00:21:06,799
node and the driver user will see the

00:21:05,679 --> 00:21:08,799
driver is running

00:21:06,799 --> 00:21:11,440
the driver pod is running once the

00:21:08,799 --> 00:21:15,039
driver started it will initialize

00:21:11,440 --> 00:21:16,080
um once this initialization is finished

00:21:15,039 --> 00:21:18,480
it will start to

00:21:16,080 --> 00:21:19,360
apply for for the executors for its

00:21:18,480 --> 00:21:22,480
actually

00:21:19,360 --> 00:21:24,720
task right so we'll see a bunch of

00:21:22,480 --> 00:21:25,760
exclusives get created in the on

00:21:24,720 --> 00:21:28,400
kubernetes

00:21:25,760 --> 00:21:30,240
and again in this hierarchy we are

00:21:28,400 --> 00:21:33,679
putting all these executors

00:21:30,240 --> 00:21:35,600
into a certain lift queue and into a

00:21:33,679 --> 00:21:38,080
certain application

00:21:35,600 --> 00:21:39,600
so we'll see those pending points for

00:21:38,080 --> 00:21:43,360
this application

00:21:39,600 --> 00:21:45,200
um again we will do those sorting and

00:21:43,360 --> 00:21:47,280
look at from the queues to the app to

00:21:45,200 --> 00:21:50,640
the pod and the no and

00:21:47,280 --> 00:21:53,440
find the best note for this part

00:21:50,640 --> 00:21:55,039
do this again this is a scheduling cycle

00:21:53,440 --> 00:21:57,440
then we can find

00:21:55,039 --> 00:21:58,480
one allocation at a time for for the

00:21:57,440 --> 00:22:00,720
executors

00:21:58,480 --> 00:22:01,919
then we'll see all the executors have

00:22:00,720 --> 00:22:04,000
been bound to the

00:22:01,919 --> 00:22:06,320
to on the system and the spark job is

00:22:04,000 --> 00:22:06,320
running

00:22:07,360 --> 00:22:11,840
then i will go over go through some of

00:22:09,919 --> 00:22:14,559
the examples to show

00:22:11,840 --> 00:22:15,679
why this can make queries different

00:22:14,559 --> 00:22:18,480
comparing to the

00:22:15,679 --> 00:22:20,240
to the default scheduler in youtube to

00:22:18,480 --> 00:22:23,280
charge i have a leftover handset

00:22:20,240 --> 00:22:24,720
we have this is the case where we don't

00:22:23,280 --> 00:22:27,200
do not have the unicorn

00:22:24,720 --> 00:22:27,919
and the right hand set is the case where

00:22:27,200 --> 00:22:30,960
we have the

00:22:27,919 --> 00:22:31,760
unicorn installed on the cluster so uh

00:22:30,960 --> 00:22:34,400
very simple

00:22:31,760 --> 00:22:36,720
the first first example is we run a

00:22:34,400 --> 00:22:37,760
large number of concurrent jobs in the

00:22:36,720 --> 00:22:40,559
same name space

00:22:37,760 --> 00:22:42,640
this is common so if you want to run

00:22:40,559 --> 00:22:45,120
something on kubernetes you always

00:22:42,640 --> 00:22:47,679
run them in the name space right so uh

00:22:45,120 --> 00:22:50,559
it is very common that a lot of people

00:22:47,679 --> 00:22:51,200
run their concurrent jobs in the spark

00:22:50,559 --> 00:22:54,400
jobs

00:22:51,200 --> 00:22:56,000
in the same name space on the on this

00:22:54,400 --> 00:22:58,960
side we can see there's um

00:22:56,000 --> 00:22:59,919
job one two drop n and each job will

00:22:58,960 --> 00:23:03,120
have one driver

00:22:59,919 --> 00:23:05,280
and multiple executors we do not we do

00:23:03,120 --> 00:23:07,200
not know how many jobs are there

00:23:05,280 --> 00:23:09,919
but we just go ahead to submit them to

00:23:07,200 --> 00:23:12,080
the namespace so what happens next

00:23:09,919 --> 00:23:14,000
we'll see in the namespace of course

00:23:12,080 --> 00:23:16,080
there will be a resource code

00:23:14,000 --> 00:23:17,600
defined without unicorn there will be a

00:23:16,080 --> 00:23:18,480
resource code defined for each of the

00:23:17,600 --> 00:23:21,440
namespace

00:23:18,480 --> 00:23:22,640
to make sure we have the quota

00:23:21,440 --> 00:23:25,360
management

00:23:22,640 --> 00:23:26,240
and what we'll see is um only the driver

00:23:25,360 --> 00:23:28,880
pods

00:23:26,240 --> 00:23:29,840
so the this part highlighted with this

00:23:28,880 --> 00:23:32,480
counter is the

00:23:29,840 --> 00:23:33,520
driver pod only the driver pods gets

00:23:32,480 --> 00:23:35,679
allocated

00:23:33,520 --> 00:23:37,120
and all these funds were used all the

00:23:35,679 --> 00:23:39,600
resources in the queue

00:23:37,120 --> 00:23:41,200
are in the in the resource quota which

00:23:39,600 --> 00:23:43,679
means

00:23:41,200 --> 00:23:44,320
uh we what we will see is the job 1 and

00:23:43,679 --> 00:23:46,320
job 8

00:23:44,320 --> 00:23:47,360
are started but only the driver parts

00:23:46,320 --> 00:23:50,320
are running

00:23:47,360 --> 00:23:51,120
and the job line to the rest of the jobs

00:23:50,320 --> 00:23:53,840
are

00:23:51,120 --> 00:23:54,480
all filled because um they are rejected

00:23:53,840 --> 00:23:57,120
by the

00:23:54,480 --> 00:23:57,600
by the animation controller because they

00:23:57,120 --> 00:24:00,960
are

00:23:57,600 --> 00:24:02,080
exceeding the resource quota so this is

00:24:00,960 --> 00:24:04,720
bad right so

00:24:02,080 --> 00:24:06,720
user will have to resubmit their jobs

00:24:04,720 --> 00:24:09,279
from job line to job end

00:24:06,720 --> 00:24:11,919
and they might need to do that multiple

00:24:09,279 --> 00:24:13,360
times until all the jobs can run

00:24:11,919 --> 00:24:15,520
however with unicorn it will be

00:24:13,360 --> 00:24:17,760
different so unicorn we have um

00:24:15,520 --> 00:24:19,120
we still run the jobs in the namespace

00:24:17,760 --> 00:24:21,520
but we'll have a queue

00:24:19,120 --> 00:24:23,679
mapped to that namespace automatically

00:24:21,520 --> 00:24:26,480
and we can enforce the faithful ordering

00:24:23,679 --> 00:24:28,320
and also if we need we can select

00:24:26,480 --> 00:24:31,840
another policy but we are using

00:24:28,320 --> 00:24:32,799
fifa ordering versus the as the example

00:24:31,840 --> 00:24:35,679
um

00:24:32,799 --> 00:24:37,360
what unicomplete do differently is uh

00:24:35,679 --> 00:24:39,840
each will allocate the

00:24:37,360 --> 00:24:40,400
resource for the first job and the

00:24:39,840 --> 00:24:43,200
weights you

00:24:40,400 --> 00:24:43,679
enter um the driver parts applied for

00:24:43,200 --> 00:24:47,760
its

00:24:43,679 --> 00:24:49,760
executors so we will see on this cluster

00:24:47,760 --> 00:24:51,520
in this namespace we will see the job1

00:24:49,760 --> 00:24:53,200
and job2

00:24:51,520 --> 00:24:56,159
get started and the rest of the driver

00:24:53,200 --> 00:24:56,159
part will be pending

00:24:56,559 --> 00:25:02,159
so job job3 to drop an appending and

00:24:59,840 --> 00:25:03,520
after some time because this is a batch

00:25:02,159 --> 00:25:05,360
you know we're running some of the bad

00:25:03,520 --> 00:25:07,520
jobs some of the parts were

00:25:05,360 --> 00:25:09,840
will finish and release the resource

00:25:07,520 --> 00:25:11,760
then we all see the job three

00:25:09,840 --> 00:25:13,120
the driver pod gets allocated for drop

00:25:11,760 --> 00:25:15,760
three

00:25:13,120 --> 00:25:16,480
uh along with some other products get

00:25:15,760 --> 00:25:18,799
free

00:25:16,480 --> 00:25:20,640
resource we can see the job and started

00:25:18,799 --> 00:25:22,880
so eventually all the res where all the

00:25:20,640 --> 00:25:26,080
jobs will be finished without further

00:25:22,880 --> 00:25:29,679
user interaction uh it will simplify the

00:25:26,080 --> 00:25:33,360
the client side operations

00:25:29,679 --> 00:25:36,640
um another thing is the job queue

00:25:33,360 --> 00:25:38,559
um when you submit the cases when you

00:25:36,640 --> 00:25:39,760
submit a spark job to a namespace where

00:25:38,559 --> 00:25:42,720
the wilder resource

00:25:39,760 --> 00:25:43,600
has been used has already been used by

00:25:42,720 --> 00:25:47,279
other parts

00:25:43,600 --> 00:25:50,159
this is also very common because

00:25:47,279 --> 00:25:51,200
our clusters are busy so you don't know

00:25:50,159 --> 00:25:52,480
then

00:25:51,200 --> 00:25:54,240
there might be possible that the name

00:25:52,480 --> 00:25:55,360
service had do not have enough research

00:25:54,240 --> 00:25:58,559
for your job

00:25:55,360 --> 00:26:00,799
so let's say someone starts the job and

00:25:58,559 --> 00:26:01,600
submit to this name space again we have

00:26:00,799 --> 00:26:04,880
the resource code

00:26:01,600 --> 00:26:07,039
to control the quota you'll see the part

00:26:04,880 --> 00:26:10,000
will be directly rejected

00:26:07,039 --> 00:26:11,600
right and the jumbo fill this is again

00:26:10,000 --> 00:26:12,080
uh this is not something user really

00:26:11,600 --> 00:26:13,840
want

00:26:12,080 --> 00:26:15,919
uh like i mentioned here is this some

00:26:13,840 --> 00:26:18,240
lazy user i i would like you

00:26:15,919 --> 00:26:20,240
the system to handle the finger for me

00:26:18,240 --> 00:26:22,000
so with unicorn is different

00:26:20,240 --> 00:26:23,840
so the powder will be pending here

00:26:22,000 --> 00:26:25,679
waiting for resources

00:26:23,840 --> 00:26:28,159
once some of the resources being

00:26:25,679 --> 00:26:30,960
released in the in the namespace

00:26:28,159 --> 00:26:31,440
it will be allocated to the namespace so

00:26:30,960 --> 00:26:33,600
there's

00:26:31,440 --> 00:26:38,080
nothing else the user needs to do

00:26:33,600 --> 00:26:40,320
eventually the job will be finished

00:26:38,080 --> 00:26:41,760
also another example is the job priority

00:26:40,320 --> 00:26:44,559
so right now let's see

00:26:41,760 --> 00:26:45,760
um the example is we submit a high

00:26:44,559 --> 00:26:48,559
priority job

00:26:45,760 --> 00:26:49,520
when the classes are fully utilized this

00:26:48,559 --> 00:26:52,240
is something

00:26:49,520 --> 00:26:53,120
very urgent i want to get results really

00:26:52,240 --> 00:26:55,279
soon

00:26:53,120 --> 00:26:56,720
and without unicorn when we submitted

00:26:55,279 --> 00:26:58,559
the job to the system

00:26:56,720 --> 00:27:01,279
as you can see the both the name spaces

00:26:58,559 --> 00:27:03,279
are being fully utilized

00:27:01,279 --> 00:27:04,559
what will happen it will trigger the

00:27:03,279 --> 00:27:06,720
printing letting let's

00:27:04,559 --> 00:27:08,880
let's um assume that we have enabled the

00:27:06,720 --> 00:27:10,559
printing so you will try to start to

00:27:08,880 --> 00:27:13,760
print other parts

00:27:10,559 --> 00:27:14,720
uh based on the private class so this

00:27:13,760 --> 00:27:16,640
will happen

00:27:14,720 --> 00:27:19,200
very randomly right you don't know you

00:27:16,640 --> 00:27:23,039
don't have control which part it will go

00:27:19,200 --> 00:27:25,120
it will go and the prints uh it could be

00:27:23,039 --> 00:27:26,720
so it could be print multiple jobs like

00:27:25,120 --> 00:27:29,919
the example here it prints

00:27:26,720 --> 00:27:31,200
two eight three parts from two jobs two

00:27:29,919 --> 00:27:34,240
different jobs

00:27:31,200 --> 00:27:37,760
it could be print printing other

00:27:34,240 --> 00:27:40,399
parts belong to other name spaces were

00:27:37,760 --> 00:27:42,080
printed apart from higher prior jobs

00:27:40,399 --> 00:27:44,080
which is also bad but

00:27:42,080 --> 00:27:45,600
default scatter doesn't have a job

00:27:44,080 --> 00:27:48,720
priority so

00:27:45,600 --> 00:27:51,360
but either way it is bad

00:27:48,720 --> 00:27:52,320
and with unicorn what we can do is we

00:27:51,360 --> 00:27:55,039
can

00:27:52,320 --> 00:27:56,960
enforce the job permission only happen

00:27:55,039 --> 00:27:59,360
happens in the job level

00:27:56,960 --> 00:28:00,880
and if you have a higher level priority

00:27:59,360 --> 00:28:03,520
job

00:28:00,880 --> 00:28:04,880
we can make sure the job only prints the

00:28:03,520 --> 00:28:07,279
lower priority job part

00:28:04,880 --> 00:28:07,919
within the same name space so in this

00:28:07,279 --> 00:28:10,159
case

00:28:07,919 --> 00:28:10,960
we can ensure only minimal side of the

00:28:10,159 --> 00:28:13,760
job gets

00:28:10,960 --> 00:28:14,720
affected by the permission and the rest

00:28:13,760 --> 00:28:19,440
of job should be

00:28:14,720 --> 00:28:19,440
still running as uh as they need to be

00:28:22,880 --> 00:28:26,080
another important thing is about the

00:28:24,320 --> 00:28:29,200
research fairness because we are

00:28:26,080 --> 00:28:31,360
we're we kept talking about how to

00:28:29,200 --> 00:28:34,720
sharing resources between tenants

00:28:31,360 --> 00:28:36,480
so resource fairness is a key um in this

00:28:34,720 --> 00:28:40,480
example

00:28:36,480 --> 00:28:43,360
we have two users here kevin and jessica

00:28:40,480 --> 00:28:44,640
so these submit a lot of jobs in their

00:28:43,360 --> 00:28:46,000
own name spaces and

00:28:44,640 --> 00:28:47,840
as you can see we have two different

00:28:46,000 --> 00:28:49,840
namespaces for the users

00:28:47,840 --> 00:28:51,440
this is also very common because we want

00:28:49,840 --> 00:28:52,240
to each of tenants to have their own

00:28:51,440 --> 00:28:54,320
link space

00:28:52,240 --> 00:28:55,840
and we have a dedicated caller for each

00:28:54,320 --> 00:28:58,000
of the namespace

00:28:55,840 --> 00:29:00,880
so what we'll say if they submit a lot

00:28:58,000 --> 00:29:04,080
of jobs and uh what will say

00:29:00,880 --> 00:29:05,760
it's a possibly that will uh so

00:29:04,080 --> 00:29:07,679
the cabinet's namespace get fully

00:29:05,760 --> 00:29:10,559
allocated all the resources

00:29:07,679 --> 00:29:11,440
all the jobs gets their resources to run

00:29:10,559 --> 00:29:13,520
but

00:29:11,440 --> 00:29:17,200
left the cluster doesn't have enough

00:29:13,520 --> 00:29:20,399
resource for to run the jessica's um

00:29:17,200 --> 00:29:24,320
job which what turns out to be like this

00:29:20,399 --> 00:29:27,200
um kevin gets more resources but

00:29:24,320 --> 00:29:28,640
uh jessica were not happy this is very

00:29:27,200 --> 00:29:32,799
common in the

00:29:28,640 --> 00:29:35,600
in the in kubernetes without a fairness

00:29:32,799 --> 00:29:35,600
guarantee right

00:29:35,840 --> 00:29:42,080
here jessica won't be happy because

00:29:39,600 --> 00:29:43,279
her job will get affected by some other

00:29:42,080 --> 00:29:46,799
one's job

00:29:43,279 --> 00:29:49,440
and the excess cannot be satisfied and

00:29:46,799 --> 00:29:50,000
also the job execution time will become

00:29:49,440 --> 00:29:53,120
very

00:29:50,000 --> 00:29:55,600
unpredictable so what happens with

00:29:53,120 --> 00:29:57,440
unicorn with unicorn we are enforcing

00:29:55,600 --> 00:30:00,799
the resource fairness allocation

00:29:57,440 --> 00:30:04,159
between uh between these namespaces

00:30:00,799 --> 00:30:06,320
so we will be able to

00:30:04,159 --> 00:30:07,600
ensure that the users get similar amount

00:30:06,320 --> 00:30:09,360
of resources

00:30:07,600 --> 00:30:11,200
and we're not going to starve any of the

00:30:09,360 --> 00:30:14,240
user because of this

00:30:11,200 --> 00:30:17,360
and if the cluster has some

00:30:14,240 --> 00:30:19,520
free resource freed up the user runs

00:30:17,360 --> 00:30:20,480
below his fair share will get resource

00:30:19,520 --> 00:30:23,120
first

00:30:20,480 --> 00:30:25,279
so they have their fairness we guarantee

00:30:23,120 --> 00:30:27,200
the fairness for each of the tenants

00:30:25,279 --> 00:30:30,320
which will be good for for the

00:30:27,200 --> 00:30:30,320
multi-tenancy environment

00:30:31,039 --> 00:30:35,039
um another thing very important the

00:30:34,240 --> 00:30:37,440
fine-grain

00:30:35,039 --> 00:30:38,720
resource content management without

00:30:37,440 --> 00:30:40,960
unicorn what you can do

00:30:38,720 --> 00:30:41,919
is to set up a bunch of kubernetes name

00:30:40,960 --> 00:30:44,240
spaces

00:30:41,919 --> 00:30:46,000
each of namespace can set up setup with

00:30:44,240 --> 00:30:48,240
a certain quota

00:30:46,000 --> 00:30:50,240
but namespaces are flight so you can

00:30:48,240 --> 00:30:53,840
only set up something like this

00:30:50,240 --> 00:30:55,919
and also when you when you use the coda

00:30:53,840 --> 00:31:00,559
to match this cluster resource

00:30:55,919 --> 00:31:03,120
the quota is always overbooked so

00:31:00,559 --> 00:31:04,000
sometimes you were even you get your

00:31:03,120 --> 00:31:05,679
quota

00:31:04,000 --> 00:31:07,039
but you are not able to use the resource

00:31:05,679 --> 00:31:09,039
because some other ones

00:31:07,039 --> 00:31:11,039
will be using the resources taking the

00:31:09,039 --> 00:31:12,880
resources from you

00:31:11,039 --> 00:31:14,799
and the wiz unicorn things will be

00:31:12,880 --> 00:31:17,840
different so we can set up

00:31:14,799 --> 00:31:19,279
hierarchical cues like this so this is

00:31:17,840 --> 00:31:20,720
something we can set in the

00:31:19,279 --> 00:31:23,760
configuration

00:31:20,720 --> 00:31:24,640
these are called the pretty um static

00:31:23,760 --> 00:31:28,559
cues

00:31:24,640 --> 00:31:30,799
um they are the parents and

00:31:28,559 --> 00:31:33,279
these are still the name spaces and we

00:31:30,799 --> 00:31:34,080
can easily attach namespace to a certain

00:31:33,279 --> 00:31:37,519
parent

00:31:34,080 --> 00:31:40,720
so in this way we can we can control the

00:31:37,519 --> 00:31:42,960
the overall capacity plan um

00:31:40,720 --> 00:31:44,320
this can map to the organization

00:31:42,960 --> 00:31:46,880
structure

00:31:44,320 --> 00:31:48,880
based on the amount of tendency it needs

00:31:46,880 --> 00:31:50,559
so this is pretty dynamic and where we

00:31:48,880 --> 00:31:51,039
are leveraging the placement rules to do

00:31:50,559 --> 00:31:52,880
this

00:31:51,039 --> 00:31:55,120
so all those things are happen

00:31:52,880 --> 00:31:57,039
automatically you do not

00:31:55,120 --> 00:32:00,480
you do not need to do a lot of changes

00:31:57,039 --> 00:32:03,440
or configurations

00:32:00,480 --> 00:32:04,880
um and at the last we have a very good

00:32:03,440 --> 00:32:07,919
central management ui

00:32:04,880 --> 00:32:09,039
where you can keep track of um keep

00:32:07,919 --> 00:32:10,240
tracking of what happens

00:32:09,039 --> 00:32:12,480
on your cluster you can see the

00:32:10,240 --> 00:32:15,760
applications you can see

00:32:12,480 --> 00:32:17,919
about the pods and

00:32:15,760 --> 00:32:19,039
we have a list applications you can you

00:32:17,919 --> 00:32:20,799
can you can

00:32:19,039 --> 00:32:23,600
track the information of the application

00:32:20,799 --> 00:32:27,279
and also the the queues

00:32:23,600 --> 00:32:29,200
so the major difference um

00:32:27,279 --> 00:32:30,799
we have we have a lot of features

00:32:29,200 --> 00:32:33,760
comparing to the default scheduler

00:32:30,799 --> 00:32:35,519
such as scheduling the app job ordering

00:32:33,760 --> 00:32:38,080
fine green resource management

00:32:35,519 --> 00:32:39,440
resource fairness those are the things i

00:32:38,080 --> 00:32:41,519
discovered from those

00:32:39,440 --> 00:32:43,600
examples very simple but they are very

00:32:41,519 --> 00:32:44,399
important and also native supports the

00:32:43,600 --> 00:32:46,159
big data

00:32:44,399 --> 00:32:47,440
workloads currently we are running

00:32:46,159 --> 00:32:50,480
unicorn result

00:32:47,440 --> 00:32:53,039
with with spark flink tensorflow

00:32:50,480 --> 00:32:53,600
very heavily on some of the environment

00:32:53,039 --> 00:32:55,600
and

00:32:53,600 --> 00:32:57,600
we have a better we have a pretty good

00:32:55,600 --> 00:33:00,480
skill and the performance we have some

00:32:57,600 --> 00:33:01,679
perform performance evaluation on

00:33:00,480 --> 00:33:04,720
thousands of notes

00:33:01,679 --> 00:33:07,039
and we can roughly achieve

00:33:04,720 --> 00:33:09,279
two times better than the default

00:33:07,039 --> 00:33:11,600
scheduler

00:33:09,279 --> 00:33:13,039
and i will go just very quickly talking

00:33:11,600 --> 00:33:15,200
about the recent work

00:33:13,039 --> 00:33:16,880
um the very recent one is the gun

00:33:15,200 --> 00:33:18,159
scheduling so we are introducing gun

00:33:16,880 --> 00:33:21,360
scheduling in the next

00:33:18,159 --> 00:33:24,399
release uh this is all lasting um

00:33:21,360 --> 00:33:25,200
thematics so user can can specify the

00:33:24,399 --> 00:33:27,440
task group

00:33:25,200 --> 00:33:28,640
like what we have defined here as an

00:33:27,440 --> 00:33:32,559
example

00:33:28,640 --> 00:33:36,080
and uh which specified the gun members

00:33:32,559 --> 00:33:38,159
and also um we we have introduced that

00:33:36,080 --> 00:33:39,519
general application saturday so user can

00:33:38,159 --> 00:33:43,200
very easily to integrate

00:33:39,519 --> 00:33:44,640
with with the app 30 and uh to to

00:33:43,200 --> 00:33:47,120
to leverage these scheduling

00:33:44,640 --> 00:33:49,279
capabilities more easily

00:33:47,120 --> 00:33:51,679
and also um there's a configurable

00:33:49,279 --> 00:33:53,200
scheduling policy which determine the

00:33:51,679 --> 00:33:56,559
behavior what you want to

00:33:53,200 --> 00:33:58,960
what user wants to to

00:33:56,559 --> 00:33:59,840
wants the schedule to to schedule the

00:33:58,960 --> 00:34:04,159
application

00:33:59,840 --> 00:34:06,880
uh when when in different occasions

00:34:04,159 --> 00:34:07,679
and also another recent work is the

00:34:06,880 --> 00:34:10,800
federation

00:34:07,679 --> 00:34:13,280
and uh um golly just mentioned

00:34:10,800 --> 00:34:14,159
uh when we moved to when we moved to

00:34:13,280 --> 00:34:16,000
kubernetes and

00:34:14,159 --> 00:34:18,079
one of the major issues about

00:34:16,000 --> 00:34:21,280
scalability

00:34:18,079 --> 00:34:22,399
and i just joined hadoos meetup last

00:34:21,280 --> 00:34:24,399
week and

00:34:22,399 --> 00:34:27,040
some of the hadoop users actually are

00:34:24,399 --> 00:34:29,200
running very large scale

00:34:27,040 --> 00:34:31,919
hadoop clusters thousands of nodes or

00:34:29,200 --> 00:34:33,520
even tens of thousands of nodes

00:34:31,919 --> 00:34:35,359
when we move those large large-scale

00:34:33,520 --> 00:34:37,280
workloads to kubernetes

00:34:35,359 --> 00:34:39,760
we cannot do that in a single cluster

00:34:37,280 --> 00:34:42,320
and we will need to expand our clusters

00:34:39,760 --> 00:34:43,839
uh to get them as a compute boost and in

00:34:42,320 --> 00:34:46,399
this architecture we can

00:34:43,839 --> 00:34:48,639
deploy multiple compute pools and each

00:34:46,399 --> 00:34:50,320
of them will be managed by

00:34:48,639 --> 00:34:51,839
managing the schedule and the scheduling

00:34:50,320 --> 00:34:55,119
will be done by unicorn

00:34:51,839 --> 00:34:57,760
as a member and we can scale out

00:34:55,119 --> 00:34:58,560
we can scale out as many as um classes

00:34:57,760 --> 00:35:00,720
we want

00:34:58,560 --> 00:35:02,240
and all those those class computer

00:35:00,720 --> 00:35:04,800
proofs will report

00:35:02,240 --> 00:35:06,480
their resource usage now all those

00:35:04,800 --> 00:35:08,560
status to the

00:35:06,480 --> 00:35:09,599
unicorn group manager whereas the

00:35:08,560 --> 00:35:12,000
federation like

00:35:09,599 --> 00:35:12,880
architecture so admin can manage the

00:35:12,000 --> 00:35:14,400
resource

00:35:12,880 --> 00:35:16,960
from the central place on the control

00:35:14,400 --> 00:35:18,240
plane and the users only need to work

00:35:16,960 --> 00:35:20,720
with the gateway and the gateway will

00:35:18,240 --> 00:35:23,040
dispatch their job to a certain

00:35:20,720 --> 00:35:24,640
certain computer pool to to do to run

00:35:23,040 --> 00:35:28,000
the jobs

00:35:24,640 --> 00:35:31,440
so this is very flexible to scale

00:35:28,000 --> 00:35:34,160
the entire platform

00:35:31,440 --> 00:35:36,160
currently uh we have open sourced uh

00:35:34,160 --> 00:35:40,640
since july 17

00:35:36,160 --> 00:35:44,160
and um the the unicorn project is now a

00:35:40,640 --> 00:35:47,200
approaching computer since january 2020.

00:35:44,160 --> 00:35:50,640
now latest database version is 0.9

00:35:47,200 --> 00:35:52,480
we just released that months ago and

00:35:50,640 --> 00:35:54,880
we have a very good community members

00:35:52,480 --> 00:35:58,079
from alibaba cardero microsoft

00:35:54,880 --> 00:36:00,800
a lot of companies are joining this com

00:35:58,079 --> 00:36:01,200
community help us to build this project

00:36:00,800 --> 00:36:04,320
i'm

00:36:01,200 --> 00:36:08,560
i'm quite thankful for that

00:36:04,320 --> 00:36:10,400
and uh last um i want to call out to

00:36:08,560 --> 00:36:13,040
to join us in the community if you like

00:36:10,400 --> 00:36:15,200
if you're interested to interested about

00:36:13,040 --> 00:36:16,400
how to run how to run big data on

00:36:15,200 --> 00:36:18,160
kubernetes

00:36:16,400 --> 00:36:19,920
this is definitely a project you want to

00:36:18,160 --> 00:36:22,000
look at so we have the

00:36:19,920 --> 00:36:23,520
our all our information public on the

00:36:22,000 --> 00:36:26,000
website we have the

00:36:23,520 --> 00:36:26,960
github repo you would like to to read

00:36:26,000 --> 00:36:29,760
some code

00:36:26,960 --> 00:36:30,480
and also we have a main list the slack

00:36:29,760 --> 00:36:33,680
channel

00:36:30,480 --> 00:36:35,520
used for for communication um and also

00:36:33,680 --> 00:36:37,520
we have the bi-weekly and monthly sync

00:36:35,520 --> 00:36:39,599
up meetings for different time zones

00:36:37,520 --> 00:36:41,119
so you will find a spot if you like to

00:36:39,599 --> 00:36:46,079
join us

00:36:41,119 --> 00:36:46,079
thank you that's all for today um

00:36:46,640 --> 00:36:52,160
so not sure do we have any

00:36:52,839 --> 00:36:55,839
questions

00:36:59,040 --> 00:37:02,800
okay i've got a question there's a

00:37:01,839 --> 00:37:06,240
similar project

00:37:02,800 --> 00:37:09,520
palantir palantir forks called

00:37:06,240 --> 00:37:11,680
the robix and collection we do not have

00:37:09,520 --> 00:37:14,000
any connection with palantir so

00:37:11,680 --> 00:37:15,359
i what i know is palantir has a

00:37:14,000 --> 00:37:18,720
kubernetes

00:37:15,359 --> 00:37:21,599
extender schedule extender it's not a

00:37:18,720 --> 00:37:22,400
schedule from beauty from scratch so it

00:37:21,599 --> 00:37:25,440
adds some

00:37:22,400 --> 00:37:30,960
some plugging to do

00:37:25,440 --> 00:37:32,960
to specify some improvements for spark

00:37:30,960 --> 00:37:36,320
that's something i am aware of i'm not

00:37:32,960 --> 00:37:39,440
sure if that is the rubik's

00:37:36,320 --> 00:37:41,760
but that is still follow the same

00:37:39,440 --> 00:37:43,040
same workflow as the default scheduler

00:37:41,760 --> 00:37:45,280
um i don't think

00:37:43,040 --> 00:37:46,240
they can easily support the application

00:37:45,280 --> 00:37:49,839
skills like

00:37:46,240 --> 00:37:51,280
we're just talking about it's a kind of

00:37:49,839 --> 00:37:52,640
a different so we don't we do not have

00:37:51,280 --> 00:37:55,200
any connection with them

00:37:52,640 --> 00:37:57,680
and we are so unicorn is a standalone

00:37:55,200 --> 00:38:00,160
scheduler we work that from the scratch

00:37:57,680 --> 00:38:01,760
so it can be replaced default scheduler

00:38:00,160 --> 00:38:05,839
instead of a plugin

00:38:01,760 --> 00:38:05,839
i think that's the major difference

00:38:10,560 --> 00:38:13,839
oh got a second question would you be

00:38:12,880 --> 00:38:15,599
able to expand

00:38:13,839 --> 00:38:17,200
one of the points where you mention

00:38:15,599 --> 00:38:19,920
unicorn is optimized for

00:38:17,200 --> 00:38:21,040
performance uh very interesting so for

00:38:19,920 --> 00:38:25,280
performance

00:38:21,040 --> 00:38:28,000
um we actually so one of the major

00:38:25,280 --> 00:38:30,079
improvement we have done is um unicorn

00:38:28,000 --> 00:38:32,160
itself is a standard service so we run

00:38:30,079 --> 00:38:34,800
every compute in memory

00:38:32,160 --> 00:38:36,000
we are we are reducing the most of

00:38:34,800 --> 00:38:39,520
interactions with

00:38:36,000 --> 00:38:43,520
api server and we do the

00:38:39,520 --> 00:38:46,240
scheduling cycle um in the in memory and

00:38:43,520 --> 00:38:48,240
we do the nodes and we do the

00:38:46,240 --> 00:38:51,280
optimizations for the

00:38:48,240 --> 00:38:52,960
job and the queue sorting policies and

00:38:51,280 --> 00:38:55,520
and more importantly for the node

00:38:52,960 --> 00:38:59,040
sorting we do some of the

00:38:55,520 --> 00:39:00,800
we have some cache mechanism in the in

00:38:59,040 --> 00:39:02,480
in the code to make sure that we when we

00:39:00,800 --> 00:39:04,640
do the node sorting we do

00:39:02,480 --> 00:39:06,160
it with high performance so those are

00:39:04,640 --> 00:39:08,160
the things we have done for

00:39:06,160 --> 00:39:10,079
to improve the performance and uh

00:39:08,160 --> 00:39:13,119
roughly can get about

00:39:10,079 --> 00:39:15,440
um twice better than the than the

00:39:13,119 --> 00:39:18,400
default scheduler we have a report

00:39:15,440 --> 00:39:19,920
um in the in our website so if you like

00:39:18,400 --> 00:39:22,320
you can take a look

00:39:19,920 --> 00:39:23,280
uh how we evaluate the performance with

00:39:22,320 --> 00:39:26,400
with cookmark

00:39:23,280 --> 00:39:28,880
with the simulations yeah

00:39:26,400 --> 00:39:31,599
performance actually have multiple uh

00:39:28,880 --> 00:39:34,800
angles to this one is this uh

00:39:31,599 --> 00:39:35,520
scheduling and latency and throughput

00:39:34,800 --> 00:39:38,960
another one

00:39:35,520 --> 00:39:41,599
is the actual efficiency the the

00:39:38,960 --> 00:39:43,280
resource utilization efficiency of the

00:39:41,599 --> 00:39:45,200
computer resource from the application

00:39:43,280 --> 00:39:47,200
perspective so in the in the case of the

00:39:45,200 --> 00:39:50,320
spark jobs best jobs

00:39:47,200 --> 00:39:52,640
the efficiency of the smart job uh

00:39:50,320 --> 00:39:54,720
resource utilization has improved a lot

00:39:52,640 --> 00:39:56,400
with the unicorn scheduler compared to

00:39:54,720 --> 00:39:59,920
the default scheduler in a

00:39:56,400 --> 00:39:59,920
busy kubernetes cluster

00:40:01,200 --> 00:40:05,200
any concern to have related to

00:40:03,680 --> 00:40:07,119
kubernetes upgrade is there any risk

00:40:05,200 --> 00:40:10,079
conversion conflict between the

00:40:07,119 --> 00:40:12,000
main branch okay so uh this is a good

00:40:10,079 --> 00:40:14,079
question so right now we are

00:40:12,000 --> 00:40:15,359
uh we are supporting uh kubernetes

00:40:14,079 --> 00:40:19,599
version from 1

00:40:15,359 --> 00:40:20,400
14 to 17 116. 117 something we haven't

00:40:19,599 --> 00:40:23,200
done

00:40:20,400 --> 00:40:24,319
a lot of testing but i personally i

00:40:23,200 --> 00:40:25,920
don't think we are

00:40:24,319 --> 00:40:27,760
we have a big issue to support that

00:40:25,920 --> 00:40:30,000
version because basically

00:40:27,760 --> 00:40:32,400
this is um we implement the schedule

00:40:30,000 --> 00:40:34,319
from scratch we have some dependency

00:40:32,400 --> 00:40:36,079
uh from the kubernetes but we are using

00:40:34,319 --> 00:40:39,280
the measuring the

00:40:36,079 --> 00:40:40,480
the public apis and those apis won't be

00:40:39,280 --> 00:40:45,520
changing a lot

00:40:40,480 --> 00:40:47,760
across releases and also

00:40:45,520 --> 00:40:48,720
the major dependency for the kubernetes

00:40:47,760 --> 00:40:51,920
version

00:40:48,720 --> 00:40:53,359
might be might be the predicts so we

00:40:51,920 --> 00:40:55,520
leveraged some of the

00:40:53,359 --> 00:40:56,400
kubernetes apis to run the predict for

00:40:55,520 --> 00:40:59,040
us

00:40:56,400 --> 00:40:59,920
for that part if we have some issue with

00:40:59,040 --> 00:41:02,960
00:40:59,920 --> 00:41:04,960
that might be it but most likely i do

00:41:02,960 --> 00:41:06,240
not expect there be a lot of issues

00:41:04,960 --> 00:41:08,880
because the things we're using are

00:41:06,240 --> 00:41:08,880
pretty stable

00:41:12,880 --> 00:41:16,560
thanks for questions great questions

00:41:19,119 --> 00:41:24,000
maybe you can stay for another few

00:41:20,640 --> 00:41:24,000
minutes you can get more

00:41:32,839 --> 00:41:37,280
questions

00:41:34,240 --> 00:41:38,640
where slides be available somewhere i

00:41:37,280 --> 00:41:41,599
think so so i think

00:41:38,640 --> 00:41:42,160
listed uh we were submitting our slides

00:41:41,599 --> 00:41:45,440
to

00:41:42,160 --> 00:41:48,240
to our perch uh com for glazer um

00:41:45,440 --> 00:41:49,040
they should be putting this somewhere i

00:41:48,240 --> 00:41:50,480
believe so and

00:41:49,040 --> 00:42:03,839
the video will be available on the

00:41:50,480 --> 00:42:17,839
youtube channel

00:42:03,839 --> 00:42:17,839
okay thank you

00:42:20,400 --> 00:42:23,680
questions more questions

00:42:24,640 --> 00:42:28,480
maybe you can stay for another one or

00:42:26,800 --> 00:42:31,839
two minutes

00:42:28,480 --> 00:42:31,839
cause we have some more questions

00:42:34,880 --> 00:42:39,680
well uh we always welcome to you know we

00:42:38,160 --> 00:42:41,280
have the website maybe you can just

00:42:39,680 --> 00:42:44,160
google

00:42:41,280 --> 00:42:44,880
you can see our website and there has

00:42:44,160 --> 00:42:48,000
some we

00:42:44,880 --> 00:42:48,880
we publish some of the contact

00:42:48,000 --> 00:42:51,119
information

00:42:48,880 --> 00:42:52,400
slack channel where the community

00:42:51,119 --> 00:42:54,079
meetings there

00:42:52,400 --> 00:42:56,000
so you can you can join us if you have

00:42:54,079 --> 00:42:57,200
any more questions or you want to know

00:42:56,000 --> 00:42:59,440
more about unicorn

00:42:57,200 --> 00:43:01,280
feel free to reach out to us we're very

00:42:59,440 --> 00:43:03,119
happy to get more people

00:43:01,280 --> 00:43:06,960
involved in the in the community so we

00:43:03,119 --> 00:43:06,960
can possibly to build this project

00:43:10,839 --> 00:43:13,839
better

00:43:24,079 --> 00:43:29,599
okay cool uh i think that's it for today

00:43:27,599 --> 00:43:31,280
thank you guys thank you for joining us

00:43:29,599 --> 00:43:34,319
wisdom meeting thank you gali

00:43:31,280 --> 00:43:41,839
thank you today yeah thank you thank you

00:43:34,319 --> 00:43:41,839
bye bye

00:43:48,800 --> 00:43:50,880

YouTube URL: https://www.youtube.com/watch?v=xEKB56Sg0CQ


