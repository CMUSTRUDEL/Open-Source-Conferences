Title: Flink SQL in 2020: Time to show off!
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 2)
Description: 
	Flink SQL in 2020: Time to show off!
Timo Walther

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Four years ago, the Apache Flink community started adding SQL support to ease and unify the processing of static and streaming data. Today, Flink runs business critical batch and streaming SQL queries at Alibaba, Huawei, Lyft, Uber, Yelp, and many others. Although the community made significant progress in the past years, there are still many things on the roadmap and the development is still speeding up. In the past months, several significant improvements and extensions were added including support for DDL statements, refactorings of the type system and the catalog interface, as well as Apache Hive integration. Since it is difficult to follow all development efforts that happen around Flink SQL and its ecosystem, it is time for an update. This session will focus on a comprehensive demo of what is possible with Flink SQL in 2020. Based on a realistic use case scenario, we'll show how to define tables which are backed by various storage systems and how to solve common tasks with streaming SQL queries. We will demonstrate Flink's Hive integration and show how to define and use user-defined functions. We'll close the session with an outlook of upcoming features.

Timo Walther is a committer and PMC member of the Apache Flink project. He studied Computer Science at TU Berlin. Alongside his studies, he participated in the Database Systems and Information Management Group there and worked at IBM Germany. Timo joined the project before it became part of the Apache Software Foundation. Today he works as a senior software engineer at Ververica. In Flink, he is mainly working on the Table & SQL ecosystem.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:36,719 --> 00:00:44,879
okay

00:00:37,440 --> 00:00:48,800
then let's get started i guess

00:00:44,879 --> 00:00:50,559
so so welcome to my talk um fling sequel

00:00:48,800 --> 00:00:53,840
in 2012.

00:00:50,559 --> 00:00:56,879
um first i want to introduce myself so

00:00:53,840 --> 00:00:57,520
i'm team walther i'm an apache fling

00:00:56,879 --> 00:00:59,840
committer

00:00:57,520 --> 00:01:00,800
and part of the project management

00:00:59,840 --> 00:01:03,680
committee of

00:01:00,800 --> 00:01:05,840
apache flink i have been working on

00:01:03,680 --> 00:01:08,960
flink for a very long time

00:01:05,840 --> 00:01:10,640
now so i'm part of the flink team before

00:01:08,960 --> 00:01:13,760
it actually became part of the

00:01:10,640 --> 00:01:16,080
patchy software foundation right now i'm

00:01:13,760 --> 00:01:19,680
working as a software engineer

00:01:16,080 --> 00:01:22,080
at america so the verica

00:01:19,680 --> 00:01:24,799
was called data artisans before and got

00:01:22,080 --> 00:01:28,560
recently acquired by alibaba

00:01:24,799 --> 00:01:32,560
at viverica i am part of the sdk

00:01:28,560 --> 00:01:33,200
team and i'm focusing mostly on the

00:01:32,560 --> 00:01:37,680
table

00:01:33,200 --> 00:01:37,680
and sql api and its ecosystem

00:01:38,479 --> 00:01:43,119
so for those of you that don't know much

00:01:41,360 --> 00:01:45,680
about fling i will also give you

00:01:43,119 --> 00:01:47,520
like a very very short introduction to

00:01:45,680 --> 00:01:49,600
what is actually a patrick link

00:01:47,520 --> 00:01:52,159
so apache flink is a distributed data

00:01:49,600 --> 00:01:55,280
processing system

00:01:52,159 --> 00:01:57,759
as you can see we support a variety of

00:01:55,280 --> 00:01:59,119
use cases you can build event-driven

00:01:57,759 --> 00:02:03,040
applications

00:01:59,119 --> 00:02:06,840
you can build streaming pipelines

00:02:03,040 --> 00:02:08,160
um you can do combined batch and stream

00:02:06,840 --> 00:02:10,800
analytical

00:02:08,160 --> 00:02:12,000
pipelines all of that on different

00:02:10,800 --> 00:02:16,400
storage systems

00:02:12,000 --> 00:02:18,480
on different deployments like kubernetes

00:02:16,400 --> 00:02:20,959
yarn mises and so on

00:02:18,480 --> 00:02:23,200
you can connect to a variety of

00:02:20,959 --> 00:02:27,440
different

00:02:23,200 --> 00:02:27,440
external systems for storage like

00:02:27,520 --> 00:02:32,000
real-time event queues like kafka you

00:02:29,840 --> 00:02:32,959
can also connect to databases file

00:02:32,000 --> 00:02:36,400
systems

00:02:32,959 --> 00:02:38,959
other key value stores so

00:02:36,400 --> 00:02:40,720
everything like most most of the data

00:02:38,959 --> 00:02:43,280
can be processed with fling from logs to

00:02:40,720 --> 00:02:46,400
transactions to clicks iot

00:02:43,280 --> 00:02:48,560
um different different things um

00:02:46,400 --> 00:02:50,560
if you want to summarize flink in one

00:02:48,560 --> 00:02:53,280
sentence like typical

00:02:50,560 --> 00:02:55,360
marketing sentences you will find all

00:02:53,280 --> 00:02:58,480
the keywords in this sentence

00:02:55,360 --> 00:03:01,200
so flink allows you to to do stateful

00:02:58,480 --> 00:03:02,239
computations over streams and it does

00:03:01,200 --> 00:03:04,480
that

00:03:02,239 --> 00:03:06,480
in a real time fashion but you can also

00:03:04,480 --> 00:03:10,000
process historical data

00:03:06,480 --> 00:03:12,720
fast scalable fault tolerant

00:03:10,000 --> 00:03:13,840
with event time semantics you can

00:03:12,720 --> 00:03:16,640
process very large

00:03:13,840 --> 00:03:17,200
state in the orders of terabytes and all

00:03:16,640 --> 00:03:20,159
of that

00:03:17,200 --> 00:03:23,440
can be happening in exactly once with

00:03:20,159 --> 00:03:26,560
exactly one's guarantees

00:03:23,440 --> 00:03:30,080
um so in this talk i will mostly focus

00:03:26,560 --> 00:03:32,720
on on sql but there are a lot of other

00:03:30,080 --> 00:03:34,640
flexible expressive apis for different

00:03:32,720 --> 00:03:36,159
use cases we have a complex event

00:03:34,640 --> 00:03:38,799
processing api

00:03:36,159 --> 00:03:40,400
we have a stateful function api we have

00:03:38,799 --> 00:03:43,040
a data stream api

00:03:40,400 --> 00:03:43,920
um it is worth to look into the other

00:03:43,040 --> 00:03:46,959
apis

00:03:43,920 --> 00:03:48,640
as well as i said before from

00:03:46,959 --> 00:03:51,280
the correctnesses or guarantees we can

00:03:48,640 --> 00:03:54,080
give exactly one state consistency

00:03:51,280 --> 00:03:55,200
and event semantics are one of the most

00:03:54,080 --> 00:03:58,239
important features of

00:03:55,200 --> 00:04:01,120
link and it also has

00:03:58,239 --> 00:04:02,000
like flink has already proven at scale

00:04:01,120 --> 00:04:05,040
um

00:04:02,000 --> 00:04:08,000
it runs on tens of uh

00:04:05,040 --> 00:04:09,040
of thousands of cores and manages

00:04:08,000 --> 00:04:12,400
already

00:04:09,040 --> 00:04:15,920
terabytes tens of terabytes of state

00:04:12,400 --> 00:04:18,720
either in memory or on disk

00:04:15,920 --> 00:04:19,440
here you can see some of the users of

00:04:18,720 --> 00:04:22,639
flink

00:04:19,440 --> 00:04:25,520
alibaba is a very big user of link

00:04:22,639 --> 00:04:27,440
but we also have telecom companies we

00:04:25,520 --> 00:04:30,560
have banks

00:04:27,440 --> 00:04:33,600
we have e-commerce companies

00:04:30,560 --> 00:04:37,280
like zlando or other group yelp

00:04:33,600 --> 00:04:43,759
uber all the big tech companies

00:04:37,280 --> 00:04:47,680
are are in this chart as you can see

00:04:43,759 --> 00:04:50,320
so but now let's focus on on sql

00:04:47,680 --> 00:04:52,320
what is flink sql in a nutshell so

00:04:50,320 --> 00:04:55,199
basically what we aim is to have a

00:04:52,320 --> 00:04:58,479
standard compliant sql service

00:04:55,199 --> 00:05:03,759
that allows you to query both static

00:04:58,479 --> 00:05:06,960
and also non-static streaming data like

00:05:03,759 --> 00:05:09,039
data while

00:05:06,960 --> 00:05:11,199
building on top of link which means we

00:05:09,039 --> 00:05:12,800
can leverage all the performance of link

00:05:11,199 --> 00:05:17,520
the scalability

00:05:12,800 --> 00:05:17,520
the exactly wants consistency and so on

00:05:18,720 --> 00:05:25,120
so maybe quickly explain again what

00:05:22,080 --> 00:05:27,360
what do we understand under streaming

00:05:25,120 --> 00:05:30,639
sql semantics

00:05:27,360 --> 00:05:31,919
so first of all um usually tables are

00:05:30,639 --> 00:05:34,960
not static

00:05:31,919 --> 00:05:38,800
tables they they change over time

00:05:34,960 --> 00:05:42,320
um think about like transactions

00:05:38,800 --> 00:05:44,880
from applications or we have inserts

00:05:42,320 --> 00:05:47,600
continuous inserts from some etl

00:05:44,880 --> 00:05:49,919
processors if you think about

00:05:47,600 --> 00:05:51,440
traditional database systems traditional

00:05:49,919 --> 00:05:54,479
processors

00:05:51,440 --> 00:05:56,800
of sql those are usually working on

00:05:54,479 --> 00:05:59,600
static snapshots of a table

00:05:56,800 --> 00:06:00,400
or static files or something so that

00:05:59,600 --> 00:06:04,000
means the

00:06:00,400 --> 00:06:06,400
the the query input is basically finite

00:06:04,000 --> 00:06:07,199
but also the result is finite and once

00:06:06,400 --> 00:06:10,960
the query

00:06:07,199 --> 00:06:14,800
has been executed the result is also

00:06:10,960 --> 00:06:17,919
definitive um streaming sql means

00:06:14,800 --> 00:06:20,319
that you actually have more

00:06:17,919 --> 00:06:22,080
of like you basically you you um you

00:06:20,319 --> 00:06:24,000
create standing queries like you're

00:06:22,080 --> 00:06:26,560
running continuous querying

00:06:24,000 --> 00:06:27,600
you just you sped specify these queries

00:06:26,560 --> 00:06:31,120
once

00:06:27,600 --> 00:06:33,360
on changing or how we call it dynamic

00:06:31,120 --> 00:06:35,440
tables so that means that the query

00:06:33,360 --> 00:06:39,280
input is unbounded

00:06:35,440 --> 00:06:42,319
and then the result obviously is also

00:06:39,280 --> 00:06:45,840
unbounded and is continuously

00:06:42,319 --> 00:06:49,199
updated and a very very important

00:06:45,840 --> 00:06:51,840
thing to note here is even though

00:06:49,199 --> 00:06:54,800
we have it we have stream sql what we

00:06:51,840 --> 00:06:56,639
aim for is having the same semantics

00:06:54,800 --> 00:06:58,479
of the query so it doesn't matter if you

00:06:56,639 --> 00:07:01,280
run the same query

00:06:58,479 --> 00:07:02,240
on a bounded static snapshot or snapshot

00:07:01,280 --> 00:07:06,560
of a table

00:07:02,240 --> 00:07:10,400
or on a continuously changing table

00:07:06,560 --> 00:07:12,560
so how do we actually achieve that

00:07:10,400 --> 00:07:14,840
so first of all let's look at how

00:07:12,560 --> 00:07:16,160
traditional databases process a static

00:07:14,840 --> 00:07:19,199
table

00:07:16,160 --> 00:07:21,759
so data comes in but we don't process

00:07:19,199 --> 00:07:24,800
the data yet

00:07:21,759 --> 00:07:26,960
we have to trigger some execution we we

00:07:24,800 --> 00:07:29,360
specify our query

00:07:26,960 --> 00:07:33,039
so we take the entire data as input we

00:07:29,360 --> 00:07:36,319
are processing the query and then we are

00:07:33,039 --> 00:07:37,840
outputting the result at the end so in

00:07:36,319 --> 00:07:39,759
this case we have

00:07:37,840 --> 00:07:41,840
some some clicks some url clicks at

00:07:39,759 --> 00:07:43,520
different point in time

00:07:41,840 --> 00:07:45,280
in the end we are computing a count so

00:07:43,520 --> 00:07:48,960
mary has a count of two

00:07:45,280 --> 00:07:48,960
and bob has a count of one

00:07:51,440 --> 00:07:56,800
so let's look how would the continuous

00:07:54,080 --> 00:07:59,120
query on a changing table look like

00:07:56,800 --> 00:08:00,639
so we have the same data coming in step

00:07:59,120 --> 00:08:04,080
by step

00:08:00,639 --> 00:08:06,080
we have our query defined

00:08:04,080 --> 00:08:08,080
and what happens now is that whenever a

00:08:06,080 --> 00:08:08,960
row comes in we are also already

00:08:08,080 --> 00:08:11,520
producing an

00:08:08,960 --> 00:08:13,919
output row immediately so in this case

00:08:11,520 --> 00:08:16,639
mary has a count of one

00:08:13,919 --> 00:08:17,680
then bob comes in bob has also account

00:08:16,639 --> 00:08:19,680
of one

00:08:17,680 --> 00:08:21,120
then another mary comes in and as you

00:08:19,680 --> 00:08:24,160
can see we just

00:08:21,120 --> 00:08:28,160
updated the count from one to two

00:08:24,160 --> 00:08:31,199
in the results table but as you can see

00:08:28,160 --> 00:08:32,399
like in the end if we have processed all

00:08:31,199 --> 00:08:35,279
records

00:08:32,399 --> 00:08:37,200
um the result is identical to the to the

00:08:35,279 --> 00:08:41,839
one-time query

00:08:37,200 --> 00:08:41,839
at least at this point in time

00:08:45,360 --> 00:08:49,360
why is this pattern stream unification

00:08:47,440 --> 00:08:52,880
so important

00:08:49,360 --> 00:08:56,080
so first of all usability we don't want

00:08:52,880 --> 00:08:58,480
to have like a custom stream sql syntax

00:08:56,080 --> 00:09:01,080
no we want to be standard compliant so

00:08:58,480 --> 00:09:04,240
you just have a declarative

00:09:01,080 --> 00:09:06,080
industry-wide language

00:09:04,240 --> 00:09:07,680
and we are trying not to have any

00:09:06,080 --> 00:09:10,560
streaming specific

00:09:07,680 --> 00:09:12,320
results or semantics so also from a

00:09:10,560 --> 00:09:14,959
portability point of view

00:09:12,320 --> 00:09:17,040
you can copy as uh i don't know like a

00:09:14,959 --> 00:09:19,760
sql query from an oracle database over

00:09:17,040 --> 00:09:22,880
to flink sql or back

00:09:19,760 --> 00:09:24,560
in the in the best case that should work

00:09:22,880 --> 00:09:27,120
and also from a portability point of

00:09:24,560 --> 00:09:29,760
view you can run the same query

00:09:27,120 --> 00:09:31,360
on bounded and on unbounded data so you

00:09:29,760 --> 00:09:35,279
can also decide

00:09:31,360 --> 00:09:38,399
when you want to start in the long

00:09:35,279 --> 00:09:40,880
timeline of your data i mean you have

00:09:38,399 --> 00:09:42,000
data from the past you will have data in

00:09:40,880 --> 00:09:44,640
the future

00:09:42,000 --> 00:09:45,200
and you have some data until now and you

00:09:44,640 --> 00:09:47,120
with this

00:09:45,200 --> 00:09:48,240
with this flexible semantics you can

00:09:47,120 --> 00:09:51,519
basically

00:09:48,240 --> 00:09:54,080
decide instead of instead of

00:09:51,519 --> 00:09:56,320
like cutting the timeline into pieces

00:09:54,080 --> 00:09:58,640
you can define where you want to

00:09:56,320 --> 00:09:59,440
where you want to start the processing

00:09:58,640 --> 00:10:01,519
do you want to start

00:09:59,440 --> 00:10:03,200
now for the future do you want to start

00:10:01,519 --> 00:10:05,519
it somewhere in the past

00:10:03,200 --> 00:10:07,839
until now or even from somewhere in the

00:10:05,519 --> 00:10:10,160
past and bootstrapping the state

00:10:07,839 --> 00:10:12,399
backfilling the results from historical

00:10:10,160 --> 00:10:16,720
data and then your processing

00:10:12,399 --> 00:10:16,720
into the future so this is up to you

00:10:17,120 --> 00:10:21,440
um next topic we have to talk about is

00:10:20,800 --> 00:10:23,440
what about

00:10:21,440 --> 00:10:25,120
time because actually we are not in the

00:10:23,440 --> 00:10:26,959
streaming space

00:10:25,120 --> 00:10:28,720
where we are in the steep we are in the

00:10:26,959 --> 00:10:30,640
streaming space so what does that mean

00:10:28,720 --> 00:10:32,079
time is a very important

00:10:30,640 --> 00:10:33,920
building block when it comes to

00:10:32,079 --> 00:10:36,800
streaming

00:10:33,920 --> 00:10:36,800
especially because

00:10:37,279 --> 00:10:44,720
some of the operations might need some

00:10:40,560 --> 00:10:48,000
some semantics of some temporal

00:10:44,720 --> 00:10:50,000
temporal um yeah operations

00:10:48,000 --> 00:10:52,320
um so i don't want to get into the

00:10:50,000 --> 00:10:55,120
details here because i want to start

00:10:52,320 --> 00:10:56,079
uh with a demo but what i can say is

00:10:55,120 --> 00:10:58,560
that flink

00:10:56,079 --> 00:10:59,519
sql supports sophisticated event time

00:10:58,560 --> 00:11:01,920
handling

00:10:59,519 --> 00:11:04,720
under the hood we are using the concept

00:11:01,920 --> 00:11:10,240
of watermarks that is also

00:11:04,720 --> 00:11:12,959
used um for other or at other frameworks

00:11:10,240 --> 00:11:14,800
but usually this is not necessary for

00:11:12,959 --> 00:11:17,120
the users so

00:11:14,800 --> 00:11:19,440
internally we are performing some

00:11:17,120 --> 00:11:21,680
streaming optimizations

00:11:19,440 --> 00:11:23,440
but those don't affect the sql queries

00:11:21,680 --> 00:11:24,480
itself so it's just an internal

00:11:23,440 --> 00:11:27,200
optimization

00:11:24,480 --> 00:11:28,240
so what we are trying to do is to keep

00:11:27,200 --> 00:11:31,600
the state size

00:11:28,240 --> 00:11:34,240
low we try to clean up state um

00:11:31,600 --> 00:11:36,720
whenever this is possible we are

00:11:34,240 --> 00:11:37,920
triggering computations um based on

00:11:36,720 --> 00:11:41,040
watermarks

00:11:37,920 --> 00:11:43,680
um if this is if this is possible

00:11:41,040 --> 00:11:45,680
but we will see that in the demo so how

00:11:43,680 --> 00:11:47,920
does our demo look like

00:11:45,680 --> 00:11:49,440
we have different storage systems so the

00:11:47,920 --> 00:11:52,480
demo is pretty huge

00:11:49,440 --> 00:11:55,519
we have apache kafka we have my sequel

00:11:52,480 --> 00:11:56,959
we have an s3 like dummy storage with

00:11:55,519 --> 00:11:59,920
mineio

00:11:56,959 --> 00:12:00,720
we are using hive as a meta meta store

00:11:59,920 --> 00:12:05,360
to store

00:12:00,720 --> 00:12:07,600
some catalog metadata um we use flink sd

00:12:05,360 --> 00:12:10,240
processing unit that unifies pattern

00:12:07,600 --> 00:12:13,600
stream processing for all these systems

00:12:10,240 --> 00:12:16,079
um and we will also show how we

00:12:13,600 --> 00:12:17,920
process continuous queries in kafka and

00:12:16,079 --> 00:12:20,480
mysql

00:12:17,920 --> 00:12:22,399
so our demo environment looks like this

00:12:20,480 --> 00:12:22,959
so that we are using the sql client

00:12:22,399 --> 00:12:26,639
that's a

00:12:22,959 --> 00:12:30,399
the cli client part of

00:12:26,639 --> 00:12:33,200
apache flink standards distribution

00:12:30,399 --> 00:12:35,120
um yeah we have hive media store here we

00:12:33,200 --> 00:12:37,040
will submit queries to flink's job

00:12:35,120 --> 00:12:39,680
manager the job manager will schedule

00:12:37,040 --> 00:12:42,079
jobs and execute the queries on the

00:12:39,680 --> 00:12:43,760
the worker nodes the task managers those

00:12:42,079 --> 00:12:47,920
are executing the tasks

00:12:43,760 --> 00:12:49,680
and communicate to all the systems

00:12:47,920 --> 00:12:51,040
we have like a little script in the

00:12:49,680 --> 00:12:53,040
docker image

00:12:51,040 --> 00:12:55,680
that serves as a data provider and it

00:12:53,040 --> 00:12:58,399
will ingest data a little bit slower

00:12:55,680 --> 00:12:59,680
into kafka so that not all the data is

00:12:58,399 --> 00:13:02,399
immediately available

00:12:59,680 --> 00:13:04,880
but slowly is dropping into into our

00:13:02,399 --> 00:13:04,880
systems

00:13:05,440 --> 00:13:11,839
um for our demo we are using

00:13:09,440 --> 00:13:15,360
um a schema that is very similar to the

00:13:11,839 --> 00:13:17,600
very famous tpch benchmark

00:13:15,360 --> 00:13:19,040
um but we have modified that a little

00:13:17,600 --> 00:13:22,320
bit so i mean

00:13:19,040 --> 00:13:24,720
typical is also typical in other um

00:13:22,320 --> 00:13:25,360
streaming or like in other companies

00:13:24,720 --> 00:13:27,760
that you don't

00:13:25,360 --> 00:13:30,320
have just one system so in this case

00:13:27,760 --> 00:13:32,079
frequently updating tables are stored in

00:13:30,320 --> 00:13:35,360
kafka so we have orders

00:13:32,079 --> 00:13:36,160
line items and light items have a

00:13:35,360 --> 00:13:39,040
currency

00:13:36,160 --> 00:13:40,079
so we also have a rates history so that

00:13:39,040 --> 00:13:43,120
we can convert

00:13:40,079 --> 00:13:45,600
the currency into euros um

00:13:43,120 --> 00:13:47,760
and then we have like seldomly updated

00:13:45,600 --> 00:13:51,220
tables that are stored in my sequel

00:13:47,760 --> 00:13:53,120
such as regionation customer

00:13:51,220 --> 00:13:56,079
[Music]

00:13:53,120 --> 00:13:57,839
and also rates for a different use case

00:13:56,079 --> 00:14:00,240
so let's start with the demo

00:13:57,839 --> 00:14:00,959
um yeah like 40 minutes for this talk

00:14:00,240 --> 00:14:03,760
are not

00:14:00,959 --> 00:14:04,240
optimal we will see how far we can get

00:14:03,760 --> 00:14:08,320
in this

00:14:04,240 --> 00:14:10,320
in this 40 minutes the demo is available

00:14:08,320 --> 00:14:14,000
so you can download the docker image

00:14:10,320 --> 00:14:19,440
the the demo data all here

00:14:14,000 --> 00:14:19,440
on github so let me start

00:14:20,560 --> 00:14:25,839
so first i have to start my docker

00:14:23,120 --> 00:14:25,839
container

00:14:29,360 --> 00:14:34,639
so all the systems are starting up um we

00:14:32,560 --> 00:14:35,920
can also quickly show that we have

00:14:34,639 --> 00:14:41,120
actually stored something

00:14:35,920 --> 00:14:45,760
in kafka so let's quickly show

00:14:41,120 --> 00:14:50,560
something here

00:14:45,760 --> 00:14:50,560
so i'm querying kafka for the orders

00:14:51,920 --> 00:14:58,000
i hope it works yeah you can see the

00:14:53,680 --> 00:15:03,040
orders are coming in slowly but steadily

00:14:58,000 --> 00:15:03,040
we can also quickly look into mysql

00:15:05,920 --> 00:15:13,839
so this is the mysql client we can show

00:15:08,399 --> 00:15:13,839
the tables in

00:15:14,160 --> 00:15:20,079
we can show the tables in my sql

00:15:17,920 --> 00:15:22,160
so all the the customer production

00:15:20,079 --> 00:15:24,000
tables are here we can also describe

00:15:22,160 --> 00:15:26,560
them real quick

00:15:24,000 --> 00:15:29,120
this is our like the customer table

00:15:26,560 --> 00:15:29,120
looks like

00:15:29,199 --> 00:15:32,639
um yeah we have

00:15:32,800 --> 00:15:38,000
some mini oh running

00:15:36,240 --> 00:15:40,480
as you can see right now like this is

00:15:38,000 --> 00:15:42,880
this replaces our s3 storage

00:15:40,480 --> 00:15:43,600
um as you can see right now it is empty

00:15:42,880 --> 00:15:47,199
but we will

00:15:43,600 --> 00:15:50,720
fill this uh this storage soon we have

00:15:47,199 --> 00:15:54,720
flink running and flink is running

00:15:50,720 --> 00:16:09,839
it's just waiting for jobs

00:15:54,720 --> 00:16:09,839
so let's get started with a sql client

00:16:10,959 --> 00:16:15,279
so first of all we have already

00:16:12,240 --> 00:16:16,320
registered some dynamic tables those are

00:16:15,279 --> 00:16:19,519
kafka based

00:16:16,320 --> 00:16:21,759
we put the we have put them in in

00:16:19,519 --> 00:16:24,160
the default catalog so with the show

00:16:21,759 --> 00:16:26,720
tables command we can simply

00:16:24,160 --> 00:16:28,560
list our tables that are backed by by

00:16:26,720 --> 00:16:31,120
kafka topics

00:16:28,560 --> 00:16:33,360
we can also describe our production

00:16:31,120 --> 00:16:36,880
orders here

00:16:33,360 --> 00:16:38,959
as you can see those this is the schema

00:16:36,880 --> 00:16:41,199
that is stored in the kafka topic

00:16:38,959 --> 00:16:44,480
we can also read from the kafka topic

00:16:41,199 --> 00:16:46,880
from within the sql client

00:16:44,480 --> 00:16:49,920
and here you can see how the how the

00:16:46,880 --> 00:16:53,519
events are coming in in real time

00:16:49,920 --> 00:16:56,320
into the into the client

00:16:53,519 --> 00:16:57,199
takes some time yeah and as you can see

00:16:56,320 --> 00:17:00,000
slowly

00:16:57,199 --> 00:17:02,320
the data is coming in we can also pause

00:17:00,000 --> 00:17:04,559
this whole thing

00:17:02,320 --> 00:17:05,439
and maybe look into some of the row some

00:17:04,559 --> 00:17:08,400
of the data

00:17:05,439 --> 00:17:08,400
if we're interested

00:17:10,880 --> 00:17:16,000
for the mysql tables we have stored the

00:17:13,760 --> 00:17:17,039
meta information about the mysql tables

00:17:16,000 --> 00:17:20,720
in hive

00:17:17,039 --> 00:17:24,880
so we are using the hive catalog here

00:17:20,720 --> 00:17:24,880
and then we are showing the tables there

00:17:25,520 --> 00:17:28,960
here's the customer the nation the race

00:17:27,439 --> 00:17:32,240
in the region

00:17:28,960 --> 00:17:35,919
same as before we can also read from

00:17:32,240 --> 00:17:38,799
mysql and print the bounded

00:17:35,919 --> 00:17:38,799
result here

00:17:39,440 --> 00:17:42,960
that's it the program finished because

00:17:41,840 --> 00:17:44,720
yeah it's not a

00:17:42,960 --> 00:17:47,840
it's not kafka it's just my sequel and

00:17:44,720 --> 00:17:47,840
we just list the table

00:17:49,679 --> 00:17:55,039
so let's get started with some with some

00:17:52,960 --> 00:17:57,520
real queries

00:17:55,039 --> 00:17:58,880
we start with static with static data

00:17:57,520 --> 00:18:01,919
let's assume you want to

00:17:58,880 --> 00:18:04,240
query some data so we basically want to

00:18:01,919 --> 00:18:06,880
fork our production data from kafka

00:18:04,240 --> 00:18:08,000
and maybe we want to store some of some

00:18:06,880 --> 00:18:10,880
some

00:18:08,000 --> 00:18:11,520
some snapshot of the production data in

00:18:10,880 --> 00:18:14,720
some

00:18:11,520 --> 00:18:17,120
development um

00:18:14,720 --> 00:18:18,320
file so what we are doing here we are

00:18:17,120 --> 00:18:20,640
creating a table

00:18:18,320 --> 00:18:22,640
we are calling this table def orders we

00:18:20,640 --> 00:18:24,960
are defining the schema

00:18:22,640 --> 00:18:26,480
we say we want to back this table by

00:18:24,960 --> 00:18:29,600
file system

00:18:26,480 --> 00:18:33,440
with an s3 storage and we want to

00:18:29,600 --> 00:18:36,080
have that as a csv format

00:18:33,440 --> 00:18:37,440
so i'm registering the table and now we

00:18:36,080 --> 00:18:40,840
can simply

00:18:37,440 --> 00:18:43,840
fill the the table with some data from

00:18:40,840 --> 00:18:43,840
kafka

00:18:45,280 --> 00:18:48,640
so now we're reading from kafka and

00:18:47,600 --> 00:18:52,240
inserting

00:18:48,640 --> 00:18:54,799
into this csv file we can also see that

00:18:52,240 --> 00:18:55,600
in the ui so the fling job is running

00:18:54,799 --> 00:18:58,880
here

00:18:55,600 --> 00:19:03,760
it's a very simple job and the min io

00:18:58,880 --> 00:19:08,559
storage should show a file now hopefully

00:19:03,760 --> 00:19:08,559
okay it doesn't but it should

00:19:09,120 --> 00:19:16,080
okay let's hope that this is not some

00:19:11,760 --> 00:19:16,080
problem so let me cancel the job again

00:19:22,080 --> 00:19:26,160
okay there there is it's just a very

00:19:24,080 --> 00:19:27,600
tiny csv file that's why it wasn't

00:19:26,160 --> 00:19:30,640
showing

00:19:27,600 --> 00:19:32,559
um cool so

00:19:30,640 --> 00:19:33,919
that means now we have forked a little

00:19:32,559 --> 00:19:37,520
bit of the production data

00:19:33,919 --> 00:19:40,720
into our into our csv file

00:19:37,520 --> 00:19:44,080
we can also query our csv file now

00:19:40,720 --> 00:19:47,120
as before select star from death orders

00:19:44,080 --> 00:19:47,919
and here's the content of the of the csv

00:19:47,120 --> 00:19:50,960
file

00:19:47,919 --> 00:19:53,200
we can perform like regular

00:19:50,960 --> 00:19:54,720
sql so for example let's count the

00:19:53,200 --> 00:19:58,559
number of rows that is

00:19:54,720 --> 00:19:58,559
that are in these csv files

00:19:59,520 --> 00:20:10,000
yeah we have 1000 rows in there

00:20:07,440 --> 00:20:12,240
and an important thing to know is right

00:20:10,000 --> 00:20:14,240
now we were actually executing this into

00:20:12,240 --> 00:20:16,400
in the streaming mode because batch is

00:20:14,240 --> 00:20:18,480
just a special case of streaming

00:20:16,400 --> 00:20:21,120
if you want to do this a little bit more

00:20:18,480 --> 00:20:22,159
performant you can also enable the batch

00:20:21,120 --> 00:20:26,159
mode so that

00:20:22,159 --> 00:20:26,159
speeds up queries a little bit

00:20:26,320 --> 00:20:30,080
so now let's look at a little bit more

00:20:28,640 --> 00:20:33,200
complex query

00:20:30,080 --> 00:20:36,320
let's assume we want to compute

00:20:33,200 --> 00:20:40,960
the revenue per

00:20:36,320 --> 00:20:43,440
currency and minute in our orders

00:20:40,960 --> 00:20:45,679
this would be like a typical sql queries

00:20:43,440 --> 00:20:48,559
sql query so we are doing a group by

00:20:45,679 --> 00:20:50,640
we are sealing the order time to minute

00:20:48,559 --> 00:20:53,520
then we are doing some counting

00:20:50,640 --> 00:20:55,919
and some summing we can execute that of

00:20:53,520 --> 00:20:55,919
course

00:20:58,799 --> 00:21:05,679
so here would be the result yeah that is

00:21:02,559 --> 00:21:06,640
the batch way of doing it we can also go

00:21:05,679 --> 00:21:10,320
back into

00:21:06,640 --> 00:21:10,320
uh into the streaming mode

00:21:10,880 --> 00:21:15,679
and have more like real-time results

00:21:13,600 --> 00:21:16,640
instead of just having one final result

00:21:15,679 --> 00:21:20,559
at the end

00:21:16,640 --> 00:21:20,559
so let's execute the same query again

00:21:23,039 --> 00:21:26,159
as you can see the results are exactly

00:21:24,880 --> 00:21:29,520
the same

00:21:26,159 --> 00:21:31,440
but now i want to show you a little bit

00:21:29,520 --> 00:21:33,919
some more information about what is

00:21:31,440 --> 00:21:36,320
going on under the hood

00:21:33,919 --> 00:21:37,840
for that i'm changing the result mode so

00:21:36,320 --> 00:21:40,640
what i want to show you is that

00:21:37,840 --> 00:21:42,240
flink is actually under the hood is

00:21:40,640 --> 00:21:45,440
actually a changelog

00:21:42,240 --> 00:21:47,360
processor and if we are executing the

00:21:45,440 --> 00:21:50,799
query like this one

00:21:47,360 --> 00:21:52,880
um again with this other view you can

00:21:50,799 --> 00:21:54,240
see that actually what comes out of the

00:21:52,880 --> 00:21:57,520
runtime

00:21:54,240 --> 00:21:59,679
is our changing rows so like basically

00:21:57,520 --> 00:22:01,520
deletions and insertions so whenever a

00:21:59,679 --> 00:22:05,120
count changes

00:22:01,520 --> 00:22:08,640
we are updating um the old row

00:22:05,120 --> 00:22:09,919
and inserting a new row and that is not

00:22:08,640 --> 00:22:12,159
always what we want

00:22:09,919 --> 00:22:13,360
in a streaming fashion what we want

00:22:12,159 --> 00:22:15,679
sometimes

00:22:13,360 --> 00:22:16,720
is a little bit more efficient state

00:22:15,679 --> 00:22:19,600
handling

00:22:16,720 --> 00:22:21,919
um and event time semantics so that you

00:22:19,600 --> 00:22:23,520
always have insert only changes coming

00:22:21,919 --> 00:22:27,600
out of your pipeline

00:22:23,520 --> 00:22:30,400
and for that we have actually prepared

00:22:27,600 --> 00:22:32,559
functions that you can use for example

00:22:30,400 --> 00:22:35,120
in this case

00:22:32,559 --> 00:22:35,760
we are using a tumbling function so this

00:22:35,120 --> 00:22:38,240
tumble

00:22:35,760 --> 00:22:40,720
end and tumble here grouped by tumble

00:22:38,240 --> 00:22:41,520
does exactly the same as the previous

00:22:40,720 --> 00:22:43,679
query

00:22:41,520 --> 00:22:45,440
but this this time the query is a little

00:22:43,679 --> 00:22:48,480
bit more streamified

00:22:45,440 --> 00:22:51,120
that means if we execute this query

00:22:48,480 --> 00:22:52,320
um you can actually see that only

00:22:51,120 --> 00:22:56,400
insertions

00:22:52,320 --> 00:22:59,120
come out um come out of the run time

00:22:56,400 --> 00:22:59,840
because the the runtime waits until a

00:22:59,120 --> 00:23:02,640
watermark

00:22:59,840 --> 00:23:04,400
comes in and based on the watermarks um

00:23:02,640 --> 00:23:06,840
the system knows okay

00:23:04,400 --> 00:23:08,960
for until here i can do the processing

00:23:06,840 --> 00:23:12,720
without having to emit

00:23:08,960 --> 00:23:16,640
another updating row so only

00:23:12,720 --> 00:23:16,640
inserts can be emitted at the end

00:23:17,039 --> 00:23:21,679
so let's go back to the original

00:23:19,360 --> 00:23:26,320
execution mode

00:23:21,679 --> 00:23:26,320
and also the result mode

00:23:28,480 --> 00:23:32,000
let's talk a little bit more

00:23:30,070 --> 00:23:36,720
[Music]

00:23:32,000 --> 00:23:38,640
about about streaming um especially when

00:23:36,720 --> 00:23:42,559
we talk about streaming

00:23:38,640 --> 00:23:45,039
um joining data is i guess one of the

00:23:42,559 --> 00:23:48,640
most important topics

00:23:45,039 --> 00:23:51,120
in data processing

00:23:48,640 --> 00:23:52,240
so i want to start with a regular join

00:23:51,120 --> 00:23:54,799
and then i want to show you

00:23:52,240 --> 00:23:56,320
five different ways of how can we join

00:23:54,799 --> 00:24:00,480
data in flink

00:23:56,320 --> 00:24:03,120
so the first regular join um

00:24:00,480 --> 00:24:06,880
would just happen on static tables so we

00:24:03,120 --> 00:24:06,880
can just use our hive catalog

00:24:08,320 --> 00:24:12,720
and yeah perform like regular sql i

00:24:10,960 --> 00:24:14,080
think i don't have to explain this query

00:24:12,720 --> 00:24:17,279
it looks complicated

00:24:14,080 --> 00:24:19,279
but yeah it just joins some of the dev

00:24:17,279 --> 00:24:20,159
orders with the production customers in

00:24:19,279 --> 00:24:23,919
my sequel

00:24:20,159 --> 00:24:26,320
production nation production region like

00:24:23,919 --> 00:24:27,520
complex join operation and then in the

00:24:26,320 --> 00:24:31,039
end it does some

00:24:27,520 --> 00:24:35,600
some counting yeah nothing fancy

00:24:31,039 --> 00:24:37,120
every database system can also do that

00:24:35,600 --> 00:24:40,240
the difference is that we're querying

00:24:37,120 --> 00:24:40,240
different systems here

00:24:40,400 --> 00:24:45,520
yeah this would be the result um

00:24:43,440 --> 00:24:47,240
so how does it actually look like if we

00:24:45,520 --> 00:24:50,359
go back to streaming

00:24:47,240 --> 00:24:50,359
[Music]

00:24:53,600 --> 00:25:00,080
so again we have the same query

00:24:57,120 --> 00:25:00,720
we are in the streaming mode now and as

00:25:00,080 --> 00:25:04,559
you can see

00:25:00,720 --> 00:25:07,679
also this query works so here

00:25:04,559 --> 00:25:09,200
we are not joining with the with the csv

00:25:07,679 --> 00:25:11,039
file anymore but here we are really

00:25:09,200 --> 00:25:15,840
joining kafka

00:25:11,039 --> 00:25:15,840
with my sql tables

00:25:21,600 --> 00:25:25,520
as you can see this is real time so we

00:25:23,840 --> 00:25:29,840
are always joining the newest kafka

00:25:25,520 --> 00:25:29,840
record with the tables in my sql

00:25:31,200 --> 00:25:34,799
and yeah it's a never-ending query

00:25:35,840 --> 00:25:40,720
the properties like some notable

00:25:37,360 --> 00:25:43,679
properties of this join is

00:25:40,720 --> 00:25:45,360
the problem of streaming is or like the

00:25:43,679 --> 00:25:46,480
of the semantics of the join in my

00:25:45,360 --> 00:25:49,919
sequel is

00:25:46,480 --> 00:25:52,400
that in theory all tables can change at

00:25:49,919 --> 00:25:55,360
any time so the system needs to keep

00:25:52,400 --> 00:25:56,080
all the tables in flink state which can

00:25:55,360 --> 00:25:59,440
be very

00:25:56,080 --> 00:26:00,000
very expensive so ideally what you want

00:25:59,440 --> 00:26:03,760
to do

00:26:00,000 --> 00:26:06,320
is time bound the join a little bit

00:26:03,760 --> 00:26:07,120
because maybe you know that within a

00:26:06,320 --> 00:26:11,200
certain time

00:26:07,120 --> 00:26:13,520
period all the rows will arrive

00:26:11,200 --> 00:26:14,960
so like i don't know like in a window of

00:26:13,520 --> 00:26:16,880
10 minutes

00:26:14,960 --> 00:26:18,880
the matching row should have arrived and

00:26:16,880 --> 00:26:19,520
we can perform the join and after that

00:26:18,880 --> 00:26:23,120
we occur

00:26:19,520 --> 00:26:24,159
we can also discard rows that we don't

00:26:23,120 --> 00:26:25,679
need anymore

00:26:24,159 --> 00:26:27,200
and that's also what i'm showing in the

00:26:25,679 --> 00:26:29,760
next um

00:26:27,200 --> 00:26:31,200
in the next example so this example

00:26:29,760 --> 00:26:34,720
shows

00:26:31,200 --> 00:26:38,159
an interval join in flink

00:26:34,720 --> 00:26:40,799
let me show the query first

00:26:38,159 --> 00:26:42,960
so what does this join do we are joining

00:26:40,799 --> 00:26:45,279
the production line item table

00:26:42,960 --> 00:26:46,400
and the production orders so again

00:26:45,279 --> 00:26:50,159
orders are kafka

00:26:46,400 --> 00:26:52,880
line items are are

00:26:50,159 --> 00:26:54,960
also kafka in this case so we are

00:26:52,880 --> 00:26:57,919
joining two kafka topics

00:26:54,960 --> 00:26:58,720
and here you can see we are defining

00:26:57,919 --> 00:27:02,080
some

00:26:58,720 --> 00:27:05,120
um some time-based

00:27:02,080 --> 00:27:07,520
constraint where we say okay the line

00:27:05,120 --> 00:27:09,600
item order time must be between the

00:27:07,520 --> 00:27:12,640
order order time

00:27:09,600 --> 00:27:17,520
plus a minus

00:27:12,640 --> 00:27:21,200
five minutes so in this case flink can

00:27:17,520 --> 00:27:24,240
efficiently execute this join and we'll

00:27:21,200 --> 00:27:26,399
also just keep the last five minutes

00:27:24,240 --> 00:27:28,960
in state for performing and computing

00:27:26,399 --> 00:27:28,960
this join

00:27:31,039 --> 00:27:36,320
the next topic is that sometimes maybe

00:27:34,159 --> 00:27:37,600
you also want to do like a stream

00:27:36,320 --> 00:27:41,200
enrichment

00:27:37,600 --> 00:27:41,760
so the stream comes in um like an event

00:27:41,200 --> 00:27:45,840
comes in

00:27:41,760 --> 00:27:49,760
in kafka and you actually want to enrich

00:27:45,840 --> 00:27:52,880
this event with some data from mysql

00:27:49,760 --> 00:27:53,760
we are using the for system we are using

00:27:52,880 --> 00:27:56,960
the for system

00:27:53,760 --> 00:28:01,360
time as of semantics of sql

00:27:56,960 --> 00:28:04,399
that means in this case

00:28:01,360 --> 00:28:06,240
whenever a line item comes in we are

00:28:04,399 --> 00:28:09,919
looking up

00:28:06,240 --> 00:28:12,720
um the row based on on the key here

00:28:09,919 --> 00:28:13,919
in my sql so we are performing a request

00:28:12,720 --> 00:28:17,200
to my sql

00:28:13,919 --> 00:28:20,799
give me the information about this

00:28:17,200 --> 00:28:22,640
currency and this just at the current

00:28:20,799 --> 00:28:25,440
time so we are just looking up

00:28:22,640 --> 00:28:26,240
the current value of the or like the

00:28:25,440 --> 00:28:30,080
currency

00:28:26,240 --> 00:28:32,960
the current currency conversion rate

00:28:30,080 --> 00:28:33,760
at the current wall clock time this can

00:28:32,960 --> 00:28:37,279
be done

00:28:33,760 --> 00:28:37,279
in this in this query

00:28:41,120 --> 00:28:44,399
but sometimes what you actually want to

00:28:43,520 --> 00:28:48,240
do

00:28:44,399 --> 00:28:52,000
is you want to compute results

00:28:48,240 --> 00:28:54,640
um or you want to perform

00:28:52,000 --> 00:28:56,240
a join with the conversion rate at that

00:28:54,640 --> 00:28:58,640
point in time so you don't want to look

00:28:56,240 --> 00:29:00,960
up in my sql at the current time

00:28:58,640 --> 00:29:02,559
but you want to look up let's say let's

00:29:00,960 --> 00:29:05,360
assume we have an event

00:29:02,559 --> 00:29:07,120
coming in at 12 we want to know the

00:29:05,360 --> 00:29:10,399
currency conversion

00:29:07,120 --> 00:29:12,399
at this point in time at 12 so in event

00:29:10,399 --> 00:29:14,880
time basically and we want to also

00:29:12,399 --> 00:29:15,760
quickly process maybe historical data

00:29:14,880 --> 00:29:18,159
and we want to

00:29:15,760 --> 00:29:19,039
at every point in time we always want to

00:29:18,159 --> 00:29:21,440
use

00:29:19,039 --> 00:29:22,640
the value the conversion rate that was

00:29:21,440 --> 00:29:26,320
valid for this

00:29:22,640 --> 00:29:29,440
point in time and this is also possible

00:29:26,320 --> 00:29:32,240
um in flink um

00:29:29,440 --> 00:29:33,440
we are calling this like temporal table

00:29:32,240 --> 00:29:36,320
join

00:29:33,440 --> 00:29:38,480
um this is how the query looks like so

00:29:36,320 --> 00:29:41,760
we have registered

00:29:38,480 --> 00:29:44,240
a special temporal table before

00:29:41,760 --> 00:29:45,039
and this temporal table is basically a

00:29:44,240 --> 00:29:48,080
temp

00:29:45,039 --> 00:29:52,000
a parameterized view that means

00:29:48,080 --> 00:29:53,600
you are giving in the order time of the

00:29:52,000 --> 00:29:57,600
line items

00:29:53,600 --> 00:29:59,360
and the temporal table will give you

00:29:57,600 --> 00:30:00,799
the conversion or the currency

00:29:59,360 --> 00:30:04,000
conversion rate

00:30:00,799 --> 00:30:04,000
at that point in time

00:30:06,960 --> 00:30:10,240
yeah i can also show it in how it is

00:30:08,960 --> 00:30:13,440
executed

00:30:10,240 --> 00:30:16,399
um but i think it's not

00:30:13,440 --> 00:30:17,120
super fancy to see that but like just so

00:30:16,399 --> 00:30:18,960
that you know

00:30:17,120 --> 00:30:22,159
there are different ways of joining in

00:30:18,960 --> 00:30:25,200
fling for different use cases

00:30:22,159 --> 00:30:25,679
and in in in case of the temporal table

00:30:25,200 --> 00:30:28,720
join

00:30:25,679 --> 00:30:32,159
you can even put the rates

00:30:28,720 --> 00:30:34,880
the updating rates um into a kafka topic

00:30:32,159 --> 00:30:38,240
so everything is backed by kafka

00:30:34,880 --> 00:30:39,520
and and can be computed not based on

00:30:38,240 --> 00:30:43,039
processing time

00:30:39,520 --> 00:30:45,279
but really on the time when the event

00:30:43,039 --> 00:30:47,120
happened

00:30:45,279 --> 00:30:51,039
so one last feature that i want to show

00:30:47,120 --> 00:30:52,640
you is pattern matching in flink

00:30:51,039 --> 00:30:54,320
maybe you have heard of the match

00:30:52,640 --> 00:30:57,600
recognize clause

00:30:54,320 --> 00:30:58,880
so match recognize is pretty new in the

00:30:57,600 --> 00:31:01,120
sql standard

00:30:58,880 --> 00:31:03,200
and it allows you to really do like

00:31:01,120 --> 00:31:04,559
complex event processing and finding

00:31:03,200 --> 00:31:09,279
patterns

00:31:04,559 --> 00:31:09,279
in your tables for that

00:31:10,000 --> 00:31:14,159
i need a couple of of lines so i will

00:31:12,880 --> 00:31:16,840
quickly

00:31:14,159 --> 00:31:18,399
explain what i want to do in this demo

00:31:16,840 --> 00:31:21,200
um

00:31:18,399 --> 00:31:22,960
the the the goal of this query is i want

00:31:21,200 --> 00:31:24,720
to find customers

00:31:22,960 --> 00:31:28,159
that have changed their delivery

00:31:24,720 --> 00:31:31,440
behavior that means i want to search

00:31:28,159 --> 00:31:32,320
for a pattern where the last x line

00:31:31,440 --> 00:31:35,279
items

00:31:32,320 --> 00:31:35,600
had a regular shipping and then from now

00:31:35,279 --> 00:31:38,880
to

00:31:35,600 --> 00:31:41,760
from then to now um the customer

00:31:38,880 --> 00:31:42,799
doesn't want like a pay on pay on

00:31:41,760 --> 00:31:46,080
delivery

00:31:42,799 --> 00:31:48,640
um like the the

00:31:46,080 --> 00:31:50,320
like he was always paying in advance and

00:31:48,640 --> 00:31:54,080
now he wants to pay

00:31:50,320 --> 00:31:56,720
uh on delivery um so first we need to

00:31:54,080 --> 00:31:58,080
create a helper for you for that um this

00:31:56,720 --> 00:32:02,000
helper view

00:31:58,080 --> 00:32:04,480
um just joins the production line items

00:32:02,000 --> 00:32:06,399
with the production orders it's the same

00:32:04,480 --> 00:32:09,679
as a similar query that i showed

00:32:06,399 --> 00:32:14,159
before so it's a it's an interval join

00:32:09,679 --> 00:32:17,120
around five minutes

00:32:14,159 --> 00:32:18,240
and now comes the important part based

00:32:17,120 --> 00:32:21,440
on this view

00:32:18,240 --> 00:32:22,799
so we have line items and orders joined

00:32:21,440 --> 00:32:25,919
together

00:32:22,799 --> 00:32:27,760
um now i want to execute this query so

00:32:25,919 --> 00:32:30,159
here you can see the match recognized

00:32:27,760 --> 00:32:30,159
clause

00:32:30,960 --> 00:32:35,279
we are doing positioning order by and

00:32:33,679 --> 00:32:36,880
then we are doing some measurements and

00:32:35,279 --> 00:32:38,480
here you can see the patterns that we

00:32:36,880 --> 00:32:39,840
are looking for so we are defining the

00:32:38,480 --> 00:32:43,360
patterns other and

00:32:39,840 --> 00:32:45,200
cod collect on delivery um

00:32:43,360 --> 00:32:47,200
and the patterns are defined as

00:32:45,200 --> 00:32:47,679
following so this one is collect on

00:32:47,200 --> 00:32:49,600
there

00:32:47,679 --> 00:32:51,279
is not collect on delivery and then we

00:32:49,600 --> 00:32:54,799
have a collector delivery

00:32:51,279 --> 00:32:56,080
and yeah if five of those not collecting

00:32:54,799 --> 00:32:58,399
delivery are followed

00:32:56,080 --> 00:32:59,200
by the collect and delivery then we are

00:32:58,399 --> 00:33:01,600
finding some

00:32:59,200 --> 00:33:02,559
some um pattern here it's just an

00:33:01,600 --> 00:33:05,519
example

00:33:02,559 --> 00:33:06,880
but it shows the the power of this match

00:33:05,519 --> 00:33:11,120
recognized clause

00:33:06,880 --> 00:33:11,760
quite good so because we're running out

00:33:11,120 --> 00:33:15,440
of time

00:33:11,760 --> 00:33:16,080
i will stop the demo here um but we have

00:33:15,440 --> 00:33:19,600
a lot of

00:33:16,080 --> 00:33:20,720
other um a lot of other queries that you

00:33:19,600 --> 00:33:24,000
can run

00:33:20,720 --> 00:33:28,720
in the github repository we

00:33:24,000 --> 00:33:31,840
have example queries how you can

00:33:28,720 --> 00:33:35,120
i can show you real quick

00:33:31,840 --> 00:33:37,760
how you can maintain materialized views

00:33:35,120 --> 00:33:40,159
with a little bit more sophisticated

00:33:37,760 --> 00:33:42,159
examples again

00:33:40,159 --> 00:33:44,670
we will also show how you can write

00:33:42,159 --> 00:33:46,080
results into mysql

00:33:44,670 --> 00:33:48,000
[Music]

00:33:46,080 --> 00:33:50,399
by simply creating another table and

00:33:48,000 --> 00:33:52,799
this with the jdbc

00:33:50,399 --> 00:33:53,919
connector we can write into into my sql

00:33:52,799 --> 00:33:56,799
again

00:33:53,919 --> 00:33:58,960
um we can also visualize that with

00:33:56,799 --> 00:34:00,720
grafana if we want that's also part of

00:33:58,960 --> 00:34:02,880
this docker image

00:34:00,720 --> 00:34:03,760
um if you're interested you can have a

00:34:02,880 --> 00:34:07,360
look

00:34:03,760 --> 00:34:10,320
at that again i will also share the

00:34:07,360 --> 00:34:12,560
slides afterwards

00:34:10,320 --> 00:34:14,480
so i want to give you some outlook or

00:34:12,560 --> 00:34:15,359
like also some some status of the

00:34:14,480 --> 00:34:18,399
current

00:34:15,359 --> 00:34:21,679
flink 111 features

00:34:18,399 --> 00:34:24,800
so for for for

00:34:21,679 --> 00:34:26,800
for the operations um we support regular

00:34:24,800 --> 00:34:30,240
select from where queries

00:34:26,800 --> 00:34:32,639
um regular group by and having clauses

00:34:30,240 --> 00:34:35,520
group bias can happen non-windowed or

00:34:32,639 --> 00:34:38,240
you can have the streaming optimized

00:34:35,520 --> 00:34:40,079
tumblehop and session windows as i

00:34:38,240 --> 00:34:41,520
presented we have different join

00:34:40,079 --> 00:34:44,480
semantics time window

00:34:41,520 --> 00:34:47,040
joints inner outer non-windowed joints

00:34:44,480 --> 00:34:48,800
so regular sql joins inner outer

00:34:47,040 --> 00:34:50,800
you can define your own user defined

00:34:48,800 --> 00:34:51,839
functions for scalar aggregation or

00:34:50,800 --> 00:34:54,800
table valued

00:34:51,839 --> 00:34:56,159
processing then there are some operators

00:34:54,800 --> 00:34:57,920
that only

00:34:56,159 --> 00:34:59,200
support that are only supported in

00:34:57,920 --> 00:35:02,240
streaming right now

00:34:59,200 --> 00:35:04,960
like over windows with bounded and

00:35:02,240 --> 00:35:07,440
unbounded preceding semantics

00:35:04,960 --> 00:35:09,760
then this inner join with this time

00:35:07,440 --> 00:35:11,599
version or temporal tables external

00:35:09,760 --> 00:35:15,119
lookup joins

00:35:11,599 --> 00:35:16,160
mesh recognize and for the batch side we

00:35:15,119 --> 00:35:19,280
fully support

00:35:16,160 --> 00:35:21,839
the tpcs tpcds

00:35:19,280 --> 00:35:21,839
benchmark

00:35:22,320 --> 00:35:25,440
a very important feature that i just

00:35:23,920 --> 00:35:28,720
want to highlight again

00:35:25,440 --> 00:35:30,480
is the recently added change log

00:35:28,720 --> 00:35:33,280
processing support

00:35:30,480 --> 00:35:34,240
so we support the bc in the debiasium

00:35:33,280 --> 00:35:37,359
format now

00:35:34,240 --> 00:35:40,640
so you can connect to the cdc

00:35:37,359 --> 00:35:44,079
change data capture logs from oracle

00:35:40,640 --> 00:35:45,200
phosphorus and so on by by using um this

00:35:44,079 --> 00:35:47,040
connector

00:35:45,200 --> 00:35:48,320
and you can read that for example from

00:35:47,040 --> 00:35:51,200
kafka and

00:35:48,320 --> 00:35:52,400
then perform queries on top of that

00:35:51,200 --> 00:35:55,200
there was also

00:35:52,400 --> 00:35:56,560
a talk from my colleague marta also here

00:35:55,200 --> 00:35:59,599
on an apache con

00:35:56,560 --> 00:36:02,400
maybe you're also interested in that

00:35:59,599 --> 00:36:04,320
in general think sql is evolving very

00:36:02,400 --> 00:36:08,480
fast we're adding a lot of features

00:36:04,320 --> 00:36:10,480
every month or every every release

00:36:08,480 --> 00:36:12,800
you have the possibility to connect many

00:36:10,480 --> 00:36:13,119
systems in the data ecosystem you can

00:36:12,800 --> 00:36:15,359
run

00:36:13,119 --> 00:36:16,960
continuous queries at scale both on

00:36:15,359 --> 00:36:18,400
static and dynamic data

00:36:16,960 --> 00:36:20,480
and you can do a lot more as i said

00:36:18,400 --> 00:36:24,640
before there a lot of other

00:36:20,480 --> 00:36:28,079
apis notebook support python and so on

00:36:24,640 --> 00:36:41,839
as well so if there are any questions

00:36:28,079 --> 00:36:41,839
i'm happy to answer them now

00:36:46,480 --> 00:36:52,160
so the first question was

00:36:49,839 --> 00:36:54,000
how does flink sql performance in

00:36:52,160 --> 00:36:57,599
bounded case reading pakistan

00:36:54,000 --> 00:36:59,920
for instance comparing to spark so

00:36:57,599 --> 00:37:01,119
we are we are trying to be competitive

00:36:59,920 --> 00:37:04,320
with spark

00:37:01,119 --> 00:37:07,599
i saw benchmarks that even outperformed

00:37:04,320 --> 00:37:10,240
spark because

00:37:07,599 --> 00:37:11,200
of the way how the flink execution

00:37:10,240 --> 00:37:14,720
engine looks like

00:37:11,200 --> 00:37:17,520
because we have a pipeline or streaming

00:37:14,720 --> 00:37:19,040
execution engine that means instead of

00:37:17,520 --> 00:37:21,760
like

00:37:19,040 --> 00:37:22,560
instead of cutting the the pipeline into

00:37:21,760 --> 00:37:25,119
chunks

00:37:22,560 --> 00:37:25,680
we can also directly forward results

00:37:25,119 --> 00:37:27,119
that are

00:37:25,680 --> 00:37:29,119
available immediately to the next

00:37:27,119 --> 00:37:31,599
operator so that sometimes

00:37:29,119 --> 00:37:32,880
also improves the performance compared

00:37:31,599 --> 00:37:34,880
to spark

00:37:32,880 --> 00:37:36,720
maybe you can maybe you can search

00:37:34,880 --> 00:37:38,640
online i think there was a blog post

00:37:36,720 --> 00:37:40,160
also recently

00:37:38,640 --> 00:37:42,079
about the blink planner and the

00:37:40,160 --> 00:37:44,800
performance to other systems but i'm not

00:37:42,079 --> 00:37:44,800
sure about that

00:37:45,440 --> 00:37:51,839
next question is sql generating a

00:37:49,119 --> 00:37:52,880
streaming pipeline with operators behind

00:37:51,839 --> 00:37:56,160
the scene

00:37:52,880 --> 00:37:59,599
yes exactly so you're basically

00:37:56,160 --> 00:38:03,040
you're just defining a sql query um

00:37:59,599 --> 00:38:06,640
this sql query is translated

00:38:03,040 --> 00:38:07,359
into one level so so flings things api

00:38:06,640 --> 00:38:09,280
is actually like

00:38:07,359 --> 00:38:10,880
things main api is actually the data

00:38:09,280 --> 00:38:14,880
stream api

00:38:10,880 --> 00:38:17,760
and the the sql engine um

00:38:14,880 --> 00:38:19,280
generates a plan that uses some some

00:38:17,760 --> 00:38:22,320
operator stack behind

00:38:19,280 --> 00:38:23,760
the data stream api but you can also

00:38:22,320 --> 00:38:26,880
switch back and forth so

00:38:23,760 --> 00:38:29,920
you can also define a pipeline in sql

00:38:26,880 --> 00:38:34,079
and then for example you can transform

00:38:29,920 --> 00:38:36,160
that to a data stream api um

00:38:34,079 --> 00:38:37,280
object and then you can work on data

00:38:36,160 --> 00:38:40,160
stream api so

00:38:37,280 --> 00:38:42,720
you can mix and match all the apis

00:38:40,160 --> 00:38:42,720
together

00:38:47,040 --> 00:38:50,240
are there other questions

00:38:55,920 --> 00:39:00,960
so is it possible to yes lincoln

00:38:58,240 --> 00:39:03,280
kubernetes is supported yes

00:39:00,960 --> 00:39:05,760
and it's getting more and more attention

00:39:03,280 --> 00:39:15,839
i would say

00:39:05,760 --> 00:39:15,839
and better integration

00:39:16,320 --> 00:39:20,720
other questions we still have two

00:39:18,079 --> 00:39:20,720
minutes left

00:39:26,839 --> 00:39:29,839
okay

00:39:30,400 --> 00:39:34,079
so just to mention it again if you're

00:39:33,280 --> 00:39:37,359
interested

00:39:34,079 --> 00:39:40,960
in flink or in sql there is also our

00:39:37,359 --> 00:39:44,320
own um conference very soon like

00:39:40,960 --> 00:39:48,720
in october 19th to 22nd

00:39:44,320 --> 00:39:50,640
um it is again a virtual conference

00:39:48,720 --> 00:39:52,000
and it's also for free so you can

00:39:50,640 --> 00:39:54,640
register there for free

00:39:52,000 --> 00:39:56,480
and we are also offering training so

00:39:54,640 --> 00:40:00,079
there is a dedicated

00:39:56,480 --> 00:40:02,480
i think half day or one day sql training

00:40:00,079 --> 00:40:05,359
with hands-on exercises

00:40:02,480 --> 00:40:07,119
if you're interested to learn to learn

00:40:05,359 --> 00:40:07,839
more i would really recommend this

00:40:07,119 --> 00:40:10,880
conference

00:40:07,839 --> 00:40:13,920
and we have amazing keynote speakers i

00:40:10,880 --> 00:40:18,079
think from epic games and from linkedin

00:40:13,920 --> 00:40:18,079
so if you have time feel free to join

00:40:20,960 --> 00:40:25,760
i see one more question um is flink sql

00:40:23,520 --> 00:40:29,440
nz super compliant yes we are trying

00:40:25,760 --> 00:40:32,319
to do that um but even

00:40:29,440 --> 00:40:32,720
nc sql sometimes mean that every vendor

00:40:32,319 --> 00:40:35,920
has

00:40:32,720 --> 00:40:36,880
slightly different semantics but yes we

00:40:35,920 --> 00:40:39,200
are really trying

00:40:36,880 --> 00:40:40,560
to be super compliant we are reading the

00:40:39,200 --> 00:40:43,760
sql standard

00:40:40,560 --> 00:40:46,720
all the time um to be sure

00:40:43,760 --> 00:40:47,839
that we are not creating something that

00:40:46,720 --> 00:40:50,960
you cannot use in

00:40:47,839 --> 00:40:51,680
a different system so the the ddl might

00:40:50,960 --> 00:40:53,599
not be

00:40:51,680 --> 00:40:56,960
sql compliant but the queries are

00:40:53,599 --> 00:40:56,960
definitely super confined

00:41:00,160 --> 00:41:04,560
okay cool so thank you very much i think

00:41:02,640 --> 00:41:07,280
it's time to wrap this up

00:41:04,560 --> 00:41:08,560
was nice to present here and have a good

00:41:07,280 --> 00:41:21,839
day

00:41:08,560 --> 00:41:21,839
thank you bye bye

00:41:36,720 --> 00:41:38,800

YouTube URL: https://www.youtube.com/watch?v=Pii0jmc5pPU


