Title: GeoSpark: Manage Big Geospatial Data in Apache Spark
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Geospatial
Description: 
	GeoSpark: Manage Big Geospatial Data in Apache Spark
Jia Yu, Mohamed Sarwat

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

The volume of spatial data increases at a staggering rate. This talk comprehensively studies how GeoSpark extends Apache Spark to uphold massive-scale spatial data. During this talk, we first provide a background introduction of the characteristics of spatial data and the history of distributed spatial data management systems. A follow-up section presents the vital components in GeoSpark, such as spatial data partitioning, index, and query algorithms. The third section then introduces the latest updates in GeoSpark including geospatial visualization, integration with Apache Zeppelin, Python and R wrapper. The fourth part finally concludes this talk to help the audience better grasp the overall content and points out future research directions.

Jia Yu:
Jia Yu is an Assistant Professor at Washington State University School of EECS. He obtained his Ph.D. in Computer Science from Arizona State University in Summer 2020. Jiaâ€™s research focuses on database systems and geospatial data management. In particular, he worked on distributed data management systems, database indexing, and data visualization. He is the main contributor of several open-sourced research projects such as Apache Sedona (incubating), a cluster computing framework for processing big spatial data.
Mohamed Sarwat :
Mohamed is an assistant professor of computer science at Arizona State University. Dr. Sarwat is a recipient of the 2019 National Science Foundation CAREER award. His general research interest lies in developing robust and scalable data systems for spatial and spatiotemporal applications. The outcome of his research has been recognized by two best research paper awards in the IEEE International Conference on Mobile Data Management (MDM 2015) and the International Symposium on Spatial and Temporal Databases (SSTD 2011), a best of conference citation in the IEEE International Conference on Data Engineering (ICDE 2012) as well as a best vision paper award (3rd place) in SSTD 2017. Besides impact through scientific publications, Mohamed is also the co-architect of several software artifacts, which include GeoSpark (a scalable system for processing big geospatial data) that is being used by major tech companies. He is an associate editor for the GeoInformatica journal and has served as an organizer / reviewer / program committee member for major data management and spatial computing venues. In June 2019, Dr. Sarwat has been named an Early Career Distinguished Lecturer by the IEEE Mobile Data Management community.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,480 --> 00:00:26,960
giving a

00:00:25,440 --> 00:00:30,160
[Music]

00:00:26,960 --> 00:00:33,920
my talk uh about

00:00:30,160 --> 00:00:34,559
geospark so i'm really excited about

00:00:33,920 --> 00:00:36,880
here

00:00:34,559 --> 00:00:38,399
about being here in apache khan so a

00:00:36,880 --> 00:00:39,040
little bit of introduction about myself

00:00:38,399 --> 00:00:42,160
my name is

00:00:39,040 --> 00:00:44,719
mohammad sarwat i'm

00:00:42,160 --> 00:00:46,239
a professor of computer science

00:00:44,719 --> 00:00:46,640
assistant professor of computer science

00:00:46,239 --> 00:00:50,640
at

00:00:46,640 --> 00:00:52,640
arizona state university um i'm

00:00:50,640 --> 00:00:55,520
a little bit about my background so i

00:00:52,640 --> 00:00:59,120
got my phd from university of minnesota

00:00:55,520 --> 00:01:02,160
and my is basically in the

00:00:59,120 --> 00:01:05,680
area of uh data systems

00:01:02,160 --> 00:01:07,760
uh so my work is basically being the gap

00:01:05,680 --> 00:01:11,040
between the geospatial

00:01:07,760 --> 00:01:12,159
and the data systems community so i've

00:01:11,040 --> 00:01:14,560
uh

00:01:12,159 --> 00:01:15,280
i built systems and i publish work in

00:01:14,560 --> 00:01:18,000
both

00:01:15,280 --> 00:01:19,520
communities these are example of just a

00:01:18,000 --> 00:01:21,360
computer science technical community

00:01:19,520 --> 00:01:24,159
that i'm a member of

00:01:21,360 --> 00:01:25,280
and uh that's the reason i chose

00:01:24,159 --> 00:01:28,320
actually this kind of

00:01:25,280 --> 00:01:30,799
area is that it

00:01:28,320 --> 00:01:32,560
it leads to the fact that my research is

00:01:30,799 --> 00:01:36,240
quite interdisciplinary

00:01:32,560 --> 00:01:39,200
and this is a great thing um

00:01:36,240 --> 00:01:40,400
so that's uh that's that's about myself

00:01:39,200 --> 00:01:44,399
so

00:01:40,400 --> 00:01:46,640
before i go ahead and talk about the

00:01:44,399 --> 00:01:47,759
g spark i want and some of you actually

00:01:46,640 --> 00:01:50,000
have pointed this out

00:01:47,759 --> 00:01:52,840
but i want to talk about the concept of

00:01:50,000 --> 00:01:54,880
the spatial data

00:01:52,840 --> 00:01:57,759
dilute so

00:01:54,880 --> 00:01:59,280
uh spatial the the spatial analysis or

00:01:57,759 --> 00:02:00,799
the spatial data community has been

00:01:59,280 --> 00:02:02,399
there for decades

00:02:00,799 --> 00:02:04,159
we have a lot of really really

00:02:02,399 --> 00:02:06,479
interesting systems a lot of really

00:02:04,159 --> 00:02:09,200
smart people working in this field

00:02:06,479 --> 00:02:11,599
uh however in the last i would say

00:02:09,200 --> 00:02:15,760
decade or last 15 years

00:02:11,599 --> 00:02:15,760
we have shifted

00:02:15,920 --> 00:02:19,599
due to a lot of events so we there have

00:02:18,879 --> 00:02:22,720
been a lot

00:02:19,599 --> 00:02:24,560
more interest in the geospatial work

00:02:22,720 --> 00:02:26,400
and there are reasons behind that and

00:02:24,560 --> 00:02:29,280
this is what i want to talk about when i

00:02:26,400 --> 00:02:32,319
talk about the spatial dated luge

00:02:29,280 --> 00:02:36,400
so uh recently we've been

00:02:32,319 --> 00:02:36,959
witnessing a lot of events that showed

00:02:36,400 --> 00:02:40,239
how

00:02:36,959 --> 00:02:42,239
our community is actually really really

00:02:40,239 --> 00:02:46,560
important

00:02:42,239 --> 00:02:49,040
and uh it has shown also

00:02:46,560 --> 00:02:51,040
that spatial data analysis is is really

00:02:49,040 --> 00:02:54,319
crucial to a lot of 3d

00:02:51,040 --> 00:02:54,319
uh not just uh

00:02:54,800 --> 00:02:58,720
it's not like a log it's not a luxury to

00:02:57,360 --> 00:03:00,400
have

00:02:58,720 --> 00:03:02,959
to do spatial data analysis is actually

00:03:00,400 --> 00:03:05,360
quite necessary and

00:03:02,959 --> 00:03:07,200
see here in the in the slides i'm just

00:03:05,360 --> 00:03:09,840
taking a screenshot of some of the

00:03:07,200 --> 00:03:10,480
news recently about review to covet 19

00:03:09,840 --> 00:03:12,720
and

00:03:10,480 --> 00:03:14,080
uh the music us economy shut down we've

00:03:12,720 --> 00:03:16,400
seen some talks about

00:03:14,080 --> 00:03:18,159
how people can analyze satellite imagery

00:03:16,400 --> 00:03:21,040
to understand how the shutdown

00:03:18,159 --> 00:03:23,200
affected economy and traffic in general

00:03:21,040 --> 00:03:25,840
so this is actually

00:03:23,200 --> 00:03:27,280
a quite interesting time we're living in

00:03:25,840 --> 00:03:29,920
for

00:03:27,280 --> 00:03:31,040
spatial data for the use of spatial data

00:03:29,920 --> 00:03:34,239
analysis

00:03:31,040 --> 00:03:36,159
but i would say uh this creates again

00:03:34,239 --> 00:03:37,840
these applications create data

00:03:36,159 --> 00:03:39,280
but as also mentioned i believe

00:03:37,840 --> 00:03:42,400
yesterday in one of the talks

00:03:39,280 --> 00:03:42,400
the data may not be

00:03:43,120 --> 00:03:48,080
as big it's big data but it's not too

00:03:46,879 --> 00:03:51,760
big in a sense

00:03:48,080 --> 00:03:54,560
what created the spatial data deluge

00:03:51,760 --> 00:03:56,640
deluge is actually more about it and it

00:03:54,560 --> 00:03:59,439
was a turning point for our community

00:03:56,640 --> 00:04:00,480
is basically the invention of uh the

00:03:59,439 --> 00:04:05,120
apps

00:04:00,480 --> 00:04:08,080
so that was basically uh back in 2008

00:04:05,120 --> 00:04:09,840
when uh when like again the iphone

00:04:08,080 --> 00:04:11,599
started to have apps and this actually

00:04:09,840 --> 00:04:14,159
announcement from apple

00:04:11,599 --> 00:04:16,560
it was a great thing because almost

00:04:14,159 --> 00:04:18,639
every single app we have

00:04:16,560 --> 00:04:21,440
this phone you move around with it has

00:04:18,639 --> 00:04:24,560
gps device it has a location device

00:04:21,440 --> 00:04:26,080
and every s every app sort of provide a

00:04:24,560 --> 00:04:27,120
location-based service so it sends the

00:04:26,080 --> 00:04:29,120
location

00:04:27,120 --> 00:04:30,800
and start to do things based on it based

00:04:29,120 --> 00:04:34,400
upon this location

00:04:30,800 --> 00:04:36,320
and that's the idea the very idea of

00:04:34,400 --> 00:04:38,000
this

00:04:36,320 --> 00:04:39,919
of the app and again the location

00:04:38,000 --> 00:04:41,040
technology and phones

00:04:39,919 --> 00:04:42,960
is that what created especially spatial

00:04:41,040 --> 00:04:45,840
deluge now

00:04:42,960 --> 00:04:46,160
apps generates tons of geospatial data

00:04:45,840 --> 00:04:49,919
so

00:04:46,160 --> 00:04:52,720
and these are just examples right so uh

00:04:49,919 --> 00:04:53,360
like uber used to generate a lot of data

00:04:52,720 --> 00:04:56,160
and the

00:04:53,360 --> 00:04:58,320
mobile cases by sharing in china

00:04:56,160 --> 00:05:01,840
generates tons of data per day

00:04:58,320 --> 00:05:04,320
and this data is obviously a spatial

00:05:01,840 --> 00:05:04,880
it's a spatial temporal it's mobility so

00:05:04,320 --> 00:05:08,000
it's

00:05:04,880 --> 00:05:10,720
generally geospatial data and it's a lot

00:05:08,000 --> 00:05:14,560
of data that needs to be

00:05:10,720 --> 00:05:16,320
analyzed and to extract some value from

00:05:14,560 --> 00:05:18,880
some of these companies or even

00:05:16,320 --> 00:05:20,479
cities and governments can can use this

00:05:18,880 --> 00:05:21,759
data to extract value from

00:05:20,479 --> 00:05:23,600
so this is this is what i mean by

00:05:21,759 --> 00:05:24,960
spatial data deluge and this is what

00:05:23,600 --> 00:05:27,759
created even

00:05:24,960 --> 00:05:28,960
created our field all of us actually

00:05:27,759 --> 00:05:32,479
attending here now

00:05:28,960 --> 00:05:36,400
in our field it made it more interesting

00:05:32,479 --> 00:05:38,880
and more needed in the last decade or so

00:05:36,400 --> 00:05:40,320
and it's still and it will keep growing

00:05:38,880 --> 00:05:43,440
even

00:05:40,320 --> 00:05:46,400
so the problem with this uh like

00:05:43,440 --> 00:05:48,240
with this kind of data is that yes uh

00:05:46,400 --> 00:05:51,199
people back in the time when we

00:05:48,240 --> 00:05:53,360
like 30 years ago or 20 years ago when

00:05:51,199 --> 00:05:55,840
we had geospatial software or geospatial

00:05:53,360 --> 00:05:55,840
systems

00:05:56,400 --> 00:05:59,600
these were good tools because we have a

00:05:58,560 --> 00:06:01,360
very small data

00:05:59,600 --> 00:06:06,160
so people working for example with shape

00:06:01,360 --> 00:06:08,720
files working with

00:06:06,160 --> 00:06:10,720
generally very very small c and uh

00:06:08,720 --> 00:06:14,240
country data or something like this

00:06:10,720 --> 00:06:16,400
so the problem that

00:06:14,240 --> 00:06:18,319
we have tons of and a lot of people are

00:06:16,400 --> 00:06:19,919
talking about data is the new oil

00:06:18,319 --> 00:06:23,440
and we can extract a lot of value from

00:06:19,919 --> 00:06:26,639
data but the systems that existed

00:06:23,440 --> 00:06:29,759
the classic system traditional systems

00:06:26,639 --> 00:06:34,080
were not up to speed

00:06:29,759 --> 00:06:36,319
with uh the spaceship data to which

00:06:34,080 --> 00:06:37,520
so the question was how to proc such

00:06:36,319 --> 00:06:40,960
data

00:06:37,520 --> 00:06:42,800
so to process such data um like back

00:06:40,960 --> 00:06:45,199
then like

00:06:42,800 --> 00:06:49,280
we asked this question i was like let's

00:06:45,199 --> 00:06:51,919
say in 2013 2014

00:06:49,280 --> 00:06:53,520
listed a lot of people were big data was

00:06:51,919 --> 00:06:55,840
a big deal and still is

00:06:53,520 --> 00:06:57,360
but a lot of people are interested in

00:06:55,840 --> 00:07:00,319
apache hadoop

00:06:57,360 --> 00:07:02,720
which is a great tool however it came

00:07:00,319 --> 00:07:06,160
with a lot of uh

00:07:02,720 --> 00:07:06,800
shortcomings uh and what happened is

00:07:06,160 --> 00:07:10,080
that

00:07:06,800 --> 00:07:11,120
apache park started was in its early

00:07:10,080 --> 00:07:14,880
stage

00:07:11,120 --> 00:07:17,919
but apache spark was based on our

00:07:14,880 --> 00:07:19,120
assessment in our group we we thought

00:07:17,919 --> 00:07:21,759
that

00:07:19,120 --> 00:07:22,400
from a standpoint of scalability from a

00:07:21,759 --> 00:07:24,400
state

00:07:22,400 --> 00:07:25,840
of more interactivity and faster

00:07:24,400 --> 00:07:28,000
processing of the data

00:07:25,840 --> 00:07:29,440
still in a very large cluster we thought

00:07:28,000 --> 00:07:31,759
that would be

00:07:29,440 --> 00:07:33,199
the choice so we took we took apache

00:07:31,759 --> 00:07:34,800
spark it was not

00:07:33,199 --> 00:07:36,800
back then it was still in early stages

00:07:34,800 --> 00:07:39,280
then we looked at it and we said okay

00:07:36,800 --> 00:07:42,080
can we process geospatial data

00:07:39,280 --> 00:07:43,599
at scale that kind of huge spatial data

00:07:42,080 --> 00:07:46,639
with apache spark

00:07:43,599 --> 00:07:48,319
so about we looked at apache spark we

00:07:46,639 --> 00:07:50,479
asked this question about can we proc

00:07:48,319 --> 00:07:51,759
your spatial data with apache spark we

00:07:50,479 --> 00:07:54,879
realized

00:07:51,759 --> 00:07:58,240
that apache spark did not provide

00:07:54,879 --> 00:08:00,560
a geospatial data processing api

00:07:58,240 --> 00:08:02,240
so out of the box you download apache

00:08:00,560 --> 00:08:04,000
spark you try to

00:08:02,240 --> 00:08:06,160
set up a cluster we did not provided

00:08:04,000 --> 00:08:10,000
used to data processing api

00:08:06,160 --> 00:08:11,840
so the problem is that geospatial data

00:08:10,000 --> 00:08:13,360
was treated so when you talk to you like

00:08:11,840 --> 00:08:15,520
if you talk to people that

00:08:13,360 --> 00:08:16,400
are not in the geospatial domain and ask

00:08:15,520 --> 00:08:18,639
them okay

00:08:16,400 --> 00:08:19,520
there is no api to produce spatial data

00:08:18,639 --> 00:08:21,440
a lot of people say

00:08:19,520 --> 00:08:23,440
that's that's okay you can just treat

00:08:21,440 --> 00:08:24,479
the geospatial attribute as yet another

00:08:23,440 --> 00:08:26,960
attribute

00:08:24,479 --> 00:08:28,400
of the data and things would be fine it

00:08:26,960 --> 00:08:32,240
will work

00:08:28,400 --> 00:08:35,680
yes it will theoretically work

00:08:32,240 --> 00:08:36,959
but the idea is that to do that you need

00:08:35,680 --> 00:08:39,039
to write

00:08:36,959 --> 00:08:41,200
thousands of lines of code to implement

00:08:39,039 --> 00:08:42,800
just simple spatial operations

00:08:41,200 --> 00:08:44,399
something like spatial join or even

00:08:42,800 --> 00:08:46,240
define uh

00:08:44,399 --> 00:08:47,440
different spatial object types and all

00:08:46,240 --> 00:08:50,399
that kind of stuff

00:08:47,440 --> 00:08:52,320
so that that is a problem uh another

00:08:50,399 --> 00:08:54,560
thing if trading geospatial data is yet

00:08:52,320 --> 00:08:57,440
another attribute let's assume that

00:08:54,560 --> 00:08:57,920
uh and again a lot of companies and a

00:08:57,440 --> 00:09:00,160
lot of

00:08:57,920 --> 00:09:01,519
uh scientists they're very smart people

00:09:00,160 --> 00:09:03,200
they'll say no i don't want

00:09:01,519 --> 00:09:04,560
yes i can write these thousand lines of

00:09:03,200 --> 00:09:07,680
code i don't like

00:09:04,560 --> 00:09:09,760
i don't mind doing that that's okay but

00:09:07,680 --> 00:09:11,200
who would guarantee that all the code

00:09:09,760 --> 00:09:14,880
that you wrote is

00:09:11,200 --> 00:09:16,720
optimized so that's that's another

00:09:14,880 --> 00:09:17,839
problem is that yes you can write all

00:09:16,720 --> 00:09:19,760
that sort of code

00:09:17,839 --> 00:09:22,160
to process geospatial data and on top of

00:09:19,760 --> 00:09:24,080
spark but

00:09:22,160 --> 00:09:25,839
is this code optimized and what i mean

00:09:24,080 --> 00:09:29,760
by optimized here

00:09:25,839 --> 00:09:32,560
that um

00:09:29,760 --> 00:09:33,760
like for example is it fast enough is it

00:09:32,560 --> 00:09:37,519
efficient enough for in

00:09:33,760 --> 00:09:38,959
for instance do i use the resources of

00:09:37,519 --> 00:09:40,000
the cluster let's assume i'm going to

00:09:38,959 --> 00:09:43,200
run this

00:09:40,000 --> 00:09:45,360
in the cloud in amazon ews or in

00:09:43,200 --> 00:09:46,480
microsoft azure whatever it is do i use

00:09:45,360 --> 00:09:50,160
the resources

00:09:46,480 --> 00:09:50,880
of like the resources i have wisely or i

00:09:50,160 --> 00:09:52,959
just like

00:09:50,880 --> 00:09:54,560
instead of something that can be done

00:09:52,959 --> 00:09:58,080
with five machines

00:09:54,560 --> 00:09:59,279
i put in 50 machines yes

00:09:58,080 --> 00:10:01,440
you're going to get the same results

00:09:59,279 --> 00:10:03,279
eventually but the cost is going to be

00:10:01,440 --> 00:10:05,760
huge the cost of operation are going to

00:10:03,279 --> 00:10:06,560
be huge so cost of operation constant

00:10:05,760 --> 00:10:09,760
maintenance

00:10:06,560 --> 00:10:10,640
optimization the speed of the data

00:10:09,760 --> 00:10:13,279
science pipeline

00:10:10,640 --> 00:10:14,480
if you're running data science or tasks

00:10:13,279 --> 00:10:17,279
you want to make sure that

00:10:14,480 --> 00:10:18,720
you do things fast so that was that was

00:10:17,279 --> 00:10:22,399
a touch

00:10:18,720 --> 00:10:23,360
so that's we asked uh ourselves these

00:10:22,399 --> 00:10:24,959
questions and again

00:10:23,360 --> 00:10:26,399
we could not efficiently optimize these

00:10:24,959 --> 00:10:28,320
spatial queries

00:10:26,399 --> 00:10:30,240
out of the box again with apache spark

00:10:28,320 --> 00:10:31,920
with the bare metal apache spark

00:10:30,240 --> 00:10:34,079
so no spatial proximity aware low

00:10:31,920 --> 00:10:36,000
gaussian noise patient and c's no data

00:10:34,079 --> 00:10:38,240
spatial data compression techniques

00:10:36,000 --> 00:10:39,279
so what we did is that we looked at

00:10:38,240 --> 00:10:41,680
police and

00:10:39,279 --> 00:10:43,600
we took it as a challenge and we said

00:10:41,680 --> 00:10:45,200
okay we're going to take apache spark

00:10:43,600 --> 00:10:48,160
and we're going to kind of shake the

00:10:45,200 --> 00:10:51,279
system and

00:10:48,160 --> 00:10:52,240
build with it a geospatial data

00:10:51,279 --> 00:10:56,160
processing

00:10:52,240 --> 00:11:00,320
system that is again scalable

00:10:56,160 --> 00:11:01,120
fast and cost efficient so that was uh

00:11:00,320 --> 00:11:04,880
the motivation

00:11:01,120 --> 00:11:08,480
and hence game geospark

00:11:04,880 --> 00:11:12,640
so the esparc again is a

00:11:08,480 --> 00:11:16,560
data processing system it's geospatial

00:11:12,640 --> 00:11:21,040
geospatial data um

00:11:16,560 --> 00:11:24,800
what we did here the surface geospark

00:11:21,040 --> 00:11:24,800
reads input

00:11:25,600 --> 00:11:30,800
input uh geospatial data formats

00:11:29,279 --> 00:11:32,560
so you can eat like for example shape

00:11:30,800 --> 00:11:36,000
file g json documents

00:11:32,560 --> 00:11:37,600
you can even read uh csv you can use so

00:11:36,000 --> 00:11:38,720
many of the data formats or even like

00:11:37,600 --> 00:11:40,399
pose gis

00:11:38,720 --> 00:11:42,480
from the database or something like this

00:11:40,399 --> 00:11:45,440
or oracle spatial so you can read

00:11:42,480 --> 00:11:45,920
any of this patient data formats loaded

00:11:45,440 --> 00:11:49,040
into

00:11:45,920 --> 00:11:50,560
geospark and on top

00:11:49,040 --> 00:11:52,000
the data scientist or the user of the

00:11:50,560 --> 00:11:52,959
system can write you spatial data

00:11:52,000 --> 00:11:57,519
processing task

00:11:52,959 --> 00:11:59,839
on this date and

00:11:57,519 --> 00:12:01,200
after you do this dhpark will

00:11:59,839 --> 00:12:04,240
automatically

00:12:01,200 --> 00:12:05,360
again do the load balancing will

00:12:04,240 --> 00:12:08,959
automatically

00:12:05,360 --> 00:12:10,639
uh take care of the how the cluster is

00:12:08,959 --> 00:12:14,240
set up

00:12:10,639 --> 00:12:18,720
to process the data processing area

00:12:14,240 --> 00:12:22,800
so this is this is basically the

00:12:18,720 --> 00:12:24,639
general idea behind geosparc and again

00:12:22,800 --> 00:12:26,560
these data formats that you support this

00:12:24,639 --> 00:12:29,600
is these are just examples there are

00:12:26,560 --> 00:12:31,680
so many other data formats and also

00:12:29,600 --> 00:12:32,240
geospark is designed so that you can

00:12:31,680 --> 00:12:35,920
even

00:12:32,240 --> 00:12:37,920
extend the data

00:12:35,920 --> 00:12:39,440
kind of module of it to load data

00:12:37,920 --> 00:12:40,959
different formats as well off

00:12:39,440 --> 00:12:42,720
if you come up tomorrow with a new

00:12:40,959 --> 00:12:45,040
format or a new standard for geospatial

00:12:42,720 --> 00:12:45,040
data

00:12:45,279 --> 00:12:48,800
so there are several apis when it comes

00:12:47,839 --> 00:12:52,000
to

00:12:48,800 --> 00:12:55,839
geospark one of the most

00:12:52,000 --> 00:12:58,959
popular api is the the sql api

00:12:55,839 --> 00:12:59,839
so sql is a like as a language is very

00:12:58,959 --> 00:13:01,680
popular

00:12:59,839 --> 00:13:03,360
most data scientists and the engineers

00:13:01,680 --> 00:13:06,079
are familiar with sql

00:13:03,360 --> 00:13:06,639
uh when it comes to processing the data

00:13:06,079 --> 00:13:12,079
so

00:13:06,639 --> 00:13:12,079
in geospark we also provided support for

00:13:12,320 --> 00:13:20,560
sql and we implemented most of the

00:13:16,399 --> 00:13:23,839
uh the sql functions in the either

00:13:20,560 --> 00:13:24,160
the ogc standard or the uh mm3 stranded

00:13:23,839 --> 00:13:26,000
like

00:13:24,160 --> 00:13:27,760
several standards it comes to geospatial

00:13:26,000 --> 00:13:30,480
data example of these standards is the

00:13:27,760 --> 00:13:31,360
ogc standard so the idea is that the

00:13:30,480 --> 00:13:35,839
same way

00:13:31,360 --> 00:13:35,839
you used to run sql

00:13:36,160 --> 00:13:39,920
the same way you used to run sql in for

00:13:39,440 --> 00:13:43,279
example

00:13:39,920 --> 00:13:45,600
postgen or oracle spatial or any of the

00:13:43,279 --> 00:13:46,800
existing spatial databases you can run

00:13:45,600 --> 00:13:49,839
also sql like you can

00:13:46,800 --> 00:13:50,480
write the same sql in g as spark which

00:13:49,839 --> 00:13:51,920
makes it

00:13:50,480 --> 00:13:53,920
easy to use as well because we don't

00:13:51,920 --> 00:13:55,279
want to reinvent the wheel here we want

00:13:53,920 --> 00:13:58,399
to use the same api

00:13:55,279 --> 00:14:00,079
so that the data can entertain this

00:13:58,399 --> 00:14:02,079
capability again and all the

00:14:00,079 --> 00:14:03,120
features that we talked about and still

00:14:02,079 --> 00:14:06,320
use the same

00:14:03,120 --> 00:14:06,320
language that they're used to

00:14:06,639 --> 00:14:10,399
so the architecture and there are other

00:14:08,480 --> 00:14:11,120
apis right so i'm going to talk about

00:14:10,399 --> 00:14:14,959
later but

00:14:11,120 --> 00:14:18,560
let's focus now on the sql api

00:14:14,959 --> 00:14:21,920
so this basically uh

00:14:18,560 --> 00:14:26,240
it looks like that so it extends the

00:14:21,920 --> 00:14:29,120
uh extends the rdd

00:14:26,240 --> 00:14:30,800
uh face off apache spark with something

00:14:29,120 --> 00:14:32,399
called and build new things on top of it

00:14:30,800 --> 00:14:34,240
called the spatial brazilian distributed

00:14:32,399 --> 00:14:37,440
data set a spatial rdd

00:14:34,240 --> 00:14:37,920
so spatial rdd is also an immutable

00:14:37,440 --> 00:14:40,160
object

00:14:37,920 --> 00:14:41,040
and it's distributed across the cluster

00:14:40,160 --> 00:14:43,519
partitioned

00:14:41,040 --> 00:14:44,959
and all these objects are also load

00:14:43,519 --> 00:14:47,360
balanced so you can look

00:14:44,959 --> 00:14:48,480
points you can load polygons rectangle

00:14:47,360 --> 00:14:50,079
rdds

00:14:48,480 --> 00:14:52,240
from the data that is loaded from the

00:14:50,079 --> 00:14:55,279
files or from the database right

00:14:52,240 --> 00:14:56,399
different types of spatial and

00:14:55,279 --> 00:14:58,800
geometrical

00:14:56,399 --> 00:14:59,920
objects you can represent in the spatial

00:14:58,800 --> 00:15:01,440
rdd

00:14:59,920 --> 00:15:03,600
the good thing about this is that it can

00:15:01,440 --> 00:15:05,360
be automatically partitioned based on

00:15:03,600 --> 00:15:06,240
the spatial processivity the geospatial

00:15:05,360 --> 00:15:09,360
proximity

00:15:06,240 --> 00:15:10,480
it can be also indexed using patient

00:15:09,360 --> 00:15:12,240
indices

00:15:10,480 --> 00:15:13,760
and also it supports geometrical

00:15:12,240 --> 00:15:17,760
transformation actions

00:15:13,760 --> 00:15:17,760
on these kind of spatial objects

00:15:20,000 --> 00:15:22,240
so

00:15:23,519 --> 00:15:26,720
so this is basically the idea of the

00:15:26,079 --> 00:15:29,120
spatial

00:15:26,720 --> 00:15:30,959
key on top of the spatial rdd there are

00:15:29,120 --> 00:15:31,600
also like a spatial query processing

00:15:30,959 --> 00:15:33,600
layer

00:15:31,600 --> 00:15:34,639
so this is basically the layer when you

00:15:33,600 --> 00:15:36,560
run uh

00:15:34,639 --> 00:15:38,160
traditional spatial queries again if you

00:15:36,560 --> 00:15:40,240
have a spatial range operator if you

00:15:38,160 --> 00:15:42,079
have a spatial k n uh

00:15:40,240 --> 00:15:44,000
operation and so on and so forth and

00:15:42,079 --> 00:15:46,880
we're spatial join

00:15:44,000 --> 00:15:48,720
it these operations even though they're

00:15:46,880 --> 00:15:50,399
similar to database operations were used

00:15:48,720 --> 00:15:52,000
for data processing operations were used

00:15:50,399 --> 00:15:54,320
to do it like for example supposed just

00:15:52,000 --> 00:15:56,399
or oracle spatial or any existing

00:15:54,320 --> 00:15:58,320
database systems but the way they're

00:15:56,399 --> 00:16:00,560
implemented they're implemented using

00:15:58,320 --> 00:16:03,759
the are the spatial rdd api which

00:16:00,560 --> 00:16:06,959
written in scala and java

00:16:03,759 --> 00:16:09,360
and then on top of it even there is your

00:16:06,959 --> 00:16:10,880
there is the spatial query optimizer so

00:16:09,360 --> 00:16:12,800
the query optimizer will basically

00:16:10,880 --> 00:16:16,160
decide which way

00:16:12,800 --> 00:16:19,600
is the best way to process uh

00:16:16,160 --> 00:16:21,120
the spatial uh the query that you write

00:16:19,600 --> 00:16:22,880
or the data processing operation that

00:16:21,120 --> 00:16:26,079
you write in

00:16:22,880 --> 00:16:27,519
the spark cluster so there are there is

00:16:26,079 --> 00:16:29,440
an optimization

00:16:27,519 --> 00:16:30,880
module inside that will decide that

00:16:29,440 --> 00:16:31,920
figure out what what is the best way

00:16:30,880 --> 00:16:34,079
best way means

00:16:31,920 --> 00:16:36,000
again the way that will minimize the

00:16:34,079 --> 00:16:39,120
cost to running things in the cluster

00:16:36,000 --> 00:16:39,680
while at the same time uh achieving the

00:16:39,120 --> 00:16:42,480
highest

00:16:39,680 --> 00:16:44,240
parallelism possible and also reducing

00:16:42,480 --> 00:16:47,759
the latency when it comes

00:16:44,240 --> 00:16:47,759
to the application running on top

00:16:48,480 --> 00:16:51,759
so uh spatial art uh the the good thing

00:16:51,040 --> 00:16:53,600
about like

00:16:51,759 --> 00:16:55,680
in the spatial rdd is very optimized for

00:16:53,600 --> 00:16:58,160
geospatial data in a sense that

00:16:55,680 --> 00:17:00,079
it it has a spatial compression

00:16:58,160 --> 00:17:03,680
technique that compress data

00:17:00,079 --> 00:17:05,120
uh so that when you have an object

00:17:03,680 --> 00:17:06,720
either when you store it in memory in

00:17:05,120 --> 00:17:07,679
one of the machines in the cluster or

00:17:06,720 --> 00:17:10,880
you move it around

00:17:07,679 --> 00:17:12,880
during data transfer because of

00:17:10,880 --> 00:17:14,720
the data processing you have it's very

00:17:12,880 --> 00:17:16,799
optimized it's very compact

00:17:14,720 --> 00:17:18,000
so that reduces the cost the network

00:17:16,799 --> 00:17:20,000
cost and even the memory

00:17:18,000 --> 00:17:21,360
cost of the cluster as well so this is a

00:17:20,000 --> 00:17:25,039
one one

00:17:21,360 --> 00:17:27,120
one good thing about spatial rdds

00:17:25,039 --> 00:17:29,600
also the automatic partitioning so the

00:17:27,120 --> 00:17:33,679
automatic partitioning is another

00:17:29,600 --> 00:17:36,880
great thing about special rdds um

00:17:33,679 --> 00:17:40,720
so the idea here is that we

00:17:36,880 --> 00:17:42,960
automatic like we implemented

00:17:40,720 --> 00:17:44,240
state of the art or de facto spatial

00:17:42,960 --> 00:17:46,960
partitioning technique

00:17:44,240 --> 00:17:47,840
and the user the data scientists can

00:17:46,960 --> 00:17:51,039
entertain

00:17:47,840 --> 00:17:54,320
any of these so implemented uniform grid

00:17:51,039 --> 00:17:57,440
for example kd trees qualtries arteries

00:17:54,320 --> 00:17:59,200
partitioning methods and the idea is to

00:17:57,440 --> 00:18:00,960
achieve load balancing when you run

00:17:59,200 --> 00:18:02,160
whatever kind of data processing task on

00:18:00,960 --> 00:18:06,000
top of this

00:18:02,160 --> 00:18:07,919
um we have another

00:18:06,000 --> 00:18:09,679
like we have like some kind of an

00:18:07,919 --> 00:18:12,799
autopilot kind of

00:18:09,679 --> 00:18:15,520
feature also inside uh geospark

00:18:12,799 --> 00:18:16,480
in in a sense that the insta instead of

00:18:15,520 --> 00:18:20,320
the data scientist

00:18:16,480 --> 00:18:23,919
using the choosing the partitioning

00:18:20,320 --> 00:18:27,360
technique it can automatically be chosen

00:18:23,919 --> 00:18:27,360
based on the distribution of the data

00:18:27,520 --> 00:18:31,600
um also there are indexing we have the

00:18:29,679 --> 00:18:34,240
global index we have the local index

00:18:31,600 --> 00:18:34,960
uh the global index is basically decided

00:18:34,240 --> 00:18:37,120
to use

00:18:34,960 --> 00:18:38,240
is designed to do partition elimination

00:18:37,120 --> 00:18:42,799
to not

00:18:38,240 --> 00:18:45,840
using things within uh not using things

00:18:42,799 --> 00:18:47,919
uh not using closed like machines that

00:18:45,840 --> 00:18:51,679
are not involved in that processing

00:18:47,919 --> 00:18:53,440
we have also uh local uh indexing and

00:18:51,679 --> 00:18:57,280
the local industry basically is to kind

00:18:53,440 --> 00:18:59,360
of minimize the reduce the modification

00:18:57,280 --> 00:19:01,120
on each machine and all of this will

00:18:59,360 --> 00:19:02,880
speed up the computation because if you

00:19:01,120 --> 00:19:04,400
reduce the amount of computation on each

00:19:02,880 --> 00:19:05,440
machine to do that spatial data

00:19:04,400 --> 00:19:08,000
processing

00:19:05,440 --> 00:19:08,799
that will be a very that will also

00:19:08,000 --> 00:19:12,720
reduce

00:19:08,799 --> 00:19:14,640
the time to find to finish the query

00:19:12,720 --> 00:19:17,120
no so back on this figure this is just

00:19:14,640 --> 00:19:19,919
the view of the architecture

00:19:17,120 --> 00:19:20,400
on top of the rdd there are different

00:19:19,919 --> 00:19:22,840
join

00:19:20,400 --> 00:19:24,720
we implemented various reprocessing

00:19:22,840 --> 00:19:27,440
operations

00:19:24,720 --> 00:19:28,080
so the query processing operations i'm

00:19:27,440 --> 00:19:30,480
not going to

00:19:28,080 --> 00:19:32,320
obviously explain how each one of them

00:19:30,480 --> 00:19:33,600
is implemented an example of them is the

00:19:32,320 --> 00:19:35,200
spatial join operator

00:19:33,600 --> 00:19:37,039
we have different implementation of the

00:19:35,200 --> 00:19:38,960
spatial join

00:19:37,039 --> 00:19:40,799
this is just in this slide showing just

00:19:38,960 --> 00:19:41,679
one of the implementation the dips the

00:19:40,799 --> 00:19:45,200
partitions

00:19:41,679 --> 00:19:48,240
do point and then remove duplicates

00:19:45,200 --> 00:19:49,520
uh the idea is if using two data sets

00:19:48,240 --> 00:19:52,080
and join them together

00:19:49,520 --> 00:19:54,320
and they're co-partitioned based on the

00:19:52,080 --> 00:19:55,760
spatial proximity of objects

00:19:54,320 --> 00:19:58,559
there are various implementations

00:19:55,760 --> 00:20:01,760
however the query optimizer

00:19:58,559 --> 00:20:04,720
so the query optimizer on top

00:20:01,760 --> 00:20:07,039
decide which spatial join algorithm to

00:20:04,720 --> 00:20:09,200
use or which

00:20:07,039 --> 00:20:12,080
plan in general to use in order to

00:20:09,200 --> 00:20:13,679
process the various tasks

00:20:12,080 --> 00:20:16,559
you've got about 10 minutes left and

00:20:13,679 --> 00:20:16,559
we've got so right

00:20:16,799 --> 00:20:20,559
there are like we extend the apache

00:20:19,200 --> 00:20:22,640
spark catalyst to support

00:20:20,559 --> 00:20:25,600
facial expressions heuristic rules and

00:20:22,640 --> 00:20:28,720
rdd statistics

00:20:25,600 --> 00:20:30,400
and then uh there is also cost-based

00:20:28,720 --> 00:20:31,280
facial optimizations that are also

00:20:30,400 --> 00:20:34,559
embedded in the

00:20:31,280 --> 00:20:37,120
apache sport catalyst and

00:20:34,559 --> 00:20:38,240
using this kind of cost based optimizer

00:20:37,120 --> 00:20:41,440
we decide which

00:20:38,240 --> 00:20:43,840
way to choose which plan to choose

00:20:41,440 --> 00:20:44,799
again as you you see here there are two

00:20:43,840 --> 00:20:48,640
different plans

00:20:44,799 --> 00:20:50,960
down to run the

00:20:48,640 --> 00:20:52,559
this spatial drawing operation let me

00:20:50,960 --> 00:20:53,840
use the range join another one the

00:20:52,559 --> 00:20:55,280
broadcast join

00:20:53,840 --> 00:20:57,440
some of them may be better than the

00:20:55,280 --> 00:20:58,320
other in certain situations so the idea

00:20:57,440 --> 00:20:59,760
here is to

00:20:58,320 --> 00:21:02,000
for the quicker optimizers to choose the

00:20:59,760 --> 00:21:04,240
best and again the ultimate goal is

00:21:02,000 --> 00:21:05,360
essentially whatever you have a data

00:21:04,240 --> 00:21:08,080
processing task

00:21:05,360 --> 00:21:08,640
spatial join is just one line that you

00:21:08,080 --> 00:21:10,880
write

00:21:08,640 --> 00:21:13,679
in your entire spatial data science test

00:21:10,880 --> 00:21:15,600
so you want this

00:21:13,679 --> 00:21:17,200
this query to be as optimized as

00:21:15,600 --> 00:21:20,240
possible so that you don't solve the

00:21:17,200 --> 00:21:20,240
data science pipeline

00:21:20,720 --> 00:21:23,679
these are just numbers based on

00:21:22,159 --> 00:21:24,640
commodity machines we have in the lab

00:21:23,679 --> 00:21:27,919
again we're

00:21:24,640 --> 00:21:27,919
an academic group so

00:21:28,000 --> 00:21:33,440
you can definitely uh process

00:21:31,200 --> 00:21:34,480
like if you have more powerful machines

00:21:33,440 --> 00:21:36,400
you can even do

00:21:34,480 --> 00:21:37,679
the spatial join in in a way more

00:21:36,400 --> 00:21:41,039
efficient uh way

00:21:37,679 --> 00:21:43,120
faster so um

00:21:41,039 --> 00:21:45,039
this this was just an introduction about

00:21:43,120 --> 00:21:46,720
you spark

00:21:45,039 --> 00:21:48,799
just an announcement today some of you

00:21:46,720 --> 00:21:50,720
may already know so

00:21:48,799 --> 00:21:53,360
g spark has started as an academic

00:21:50,720 --> 00:21:56,960
project at arizona state university

00:21:53,360 --> 00:22:00,880
uh so the team recently

00:21:56,960 --> 00:22:03,600
geospark has been accepted as an apache

00:22:00,880 --> 00:22:05,120
project in the integrator program and it

00:22:03,600 --> 00:22:08,320
has been renamed to

00:22:05,120 --> 00:22:10,400
apache sedona so uh

00:22:08,320 --> 00:22:11,679
so when i say geospark and apache sedona

00:22:10,400 --> 00:22:13,919
is the same exact thing

00:22:11,679 --> 00:22:15,039
however remove from now on we're going

00:22:13,919 --> 00:22:18,640
to say

00:22:15,039 --> 00:22:21,360
just the patch to doughnut so

00:22:18,640 --> 00:22:24,320
uh so this is just these are the

00:22:21,360 --> 00:22:26,080
information about apache sedona that is

00:22:24,320 --> 00:22:28,080
if you want to learn more about the

00:22:26,080 --> 00:22:30,400
features this is the website

00:22:28,080 --> 00:22:31,840
also there is the github link there's a

00:22:30,400 --> 00:22:33,600
lot of activity a lot of people are

00:22:31,840 --> 00:22:35,120
using the system which

00:22:33,600 --> 00:22:37,120
obviously is something that we're really

00:22:35,120 --> 00:22:38,640
excited about and also there is if you

00:22:37,120 --> 00:22:40,640
want to follow the news there is the

00:22:38,640 --> 00:22:42,000
twitter handle for the apache donut

00:22:40,640 --> 00:22:47,120
system

00:22:42,000 --> 00:22:47,120
um so um

00:22:49,200 --> 00:22:58,000
so can you see or so what we have here

00:22:53,280 --> 00:22:58,000
is you can see here

00:22:59,360 --> 00:23:03,600
so we have apache sedona you can again

00:23:02,480 --> 00:23:05,919
read all these

00:23:03,600 --> 00:23:07,919
different formats and on top of it there

00:23:05,919 --> 00:23:09,120
are a lot of sub projects that we've

00:23:07,919 --> 00:23:12,159
been working on

00:23:09,120 --> 00:23:13,760
so there is uh there is a python api

00:23:12,159 --> 00:23:14,880
because realize not everybody wants to

00:23:13,760 --> 00:23:17,840
use sql

00:23:14,880 --> 00:23:18,559
or so we have a wrapper a python wrapper

00:23:17,840 --> 00:23:21,760
so if you

00:23:18,559 --> 00:23:25,120
if you want to use for example uh

00:23:21,760 --> 00:23:26,400
apache sedona with geopandas or

00:23:25,120 --> 00:23:28,000
something like this you can use the

00:23:26,400 --> 00:23:30,640
python wrapper that we have

00:23:28,000 --> 00:23:31,200
there's also an r wrapper that is that

00:23:30,640 --> 00:23:33,600
there is

00:23:31,200 --> 00:23:35,440
it's released but it's been also still

00:23:33,600 --> 00:23:38,880
being tested

00:23:35,440 --> 00:23:41,279
for to integrate your r

00:23:38,880 --> 00:23:42,159
applications with apache sedona we're

00:23:41,279 --> 00:23:44,559
trying to implement

00:23:42,159 --> 00:23:46,240
also temporal capability beside the

00:23:44,559 --> 00:23:48,000
geospatial capability

00:23:46,240 --> 00:23:49,600
you can implement that now but it's not

00:23:48,000 --> 00:23:50,400
fully optimized at the portal aspect

00:23:49,600 --> 00:23:53,120
within

00:23:50,400 --> 00:23:54,559
trajectories and mobility also in

00:23:53,120 --> 00:23:56,000
spatial temporal data this is also

00:23:54,559 --> 00:23:59,520
something that we

00:23:56,000 --> 00:24:01,840
we want to do and we want to also focus

00:23:59,520 --> 00:24:04,080
on spatial streaming in the future so

00:24:01,840 --> 00:24:07,360
some of these things the python are

00:24:04,080 --> 00:24:09,039
accumulation they exist some of them

00:24:07,360 --> 00:24:11,039
their future projects that we will be

00:24:09,039 --> 00:24:13,679
working on to extend apache sedona for

00:24:11,039 --> 00:24:16,960
the community to use more stuff

00:24:13,679 --> 00:24:20,000
we're also planning to event

00:24:16,960 --> 00:24:22,080
for the apache sedona summit

00:24:20,000 --> 00:24:24,799
uh event so it's coming soon uh please

00:24:22,080 --> 00:24:27,440
state uh yeah stay tuned

00:24:24,799 --> 00:24:27,440
uh for that

00:24:28,240 --> 00:24:32,720
uh so next steps so i like before i end

00:24:31,600 --> 00:24:35,760
i want to i want to

00:24:32,720 --> 00:24:39,360
open the floor for discussion but uh

00:24:35,760 --> 00:24:41,039
and the next steps that we want to

00:24:39,360 --> 00:24:42,799
follow is basically the bigger

00:24:41,039 --> 00:24:45,120
goal is to enable spatial data science

00:24:42,799 --> 00:24:47,679
at scale so what do i mean by this what

00:24:45,120 --> 00:24:51,840
do you mean by spatial data science so

00:24:47,679 --> 00:24:53,440
i'm quoting uh quoting luke insulin

00:24:51,840 --> 00:24:55,200
from the university of chicago defines

00:24:53,440 --> 00:24:57,279
spatial data science

00:24:55,200 --> 00:24:58,400
it treats location distance and spatial

00:24:57,279 --> 00:25:00,880
interaction as core

00:24:58,400 --> 00:25:02,799
aspects of the data and employ

00:25:00,880 --> 00:25:05,360
specialized method and software

00:25:02,799 --> 00:25:06,880
to store retrieve explore analyze

00:25:05,360 --> 00:25:09,919
visualize and learn from such

00:25:06,880 --> 00:25:12,400
data so the idea here is like

00:25:09,919 --> 00:25:14,000
how can you do that with the spatial

00:25:12,400 --> 00:25:17,440
data deluge that we're talking

00:25:14,000 --> 00:25:18,000
about so there have been already so many

00:25:17,440 --> 00:25:19,919
tools

00:25:18,000 --> 00:25:21,919
to support geospatial data they're

00:25:19,919 --> 00:25:24,559
either specialized use facial tools or

00:25:21,919 --> 00:25:26,720
some of them are

00:25:24,559 --> 00:25:29,600
just diff general purpose data

00:25:26,720 --> 00:25:33,279
processing tools

00:25:29,600 --> 00:25:34,320
from this slide is just giving a general

00:25:33,279 --> 00:25:36,960
idea of how

00:25:34,320 --> 00:25:38,159
this data science application or

00:25:36,960 --> 00:25:39,919
pipeline works

00:25:38,159 --> 00:25:41,760
so you have a data system in the back

00:25:39,919 --> 00:25:44,320
end that can be posgs

00:25:41,760 --> 00:25:45,279
oracle spatial and you have a data

00:25:44,320 --> 00:25:48,320
science tool and

00:25:45,279 --> 00:25:51,840
data science tool can be r tableau as

00:25:48,320 --> 00:25:53,520
request that you can use to perform some

00:25:51,840 --> 00:25:55,520
data science tasks

00:25:53,520 --> 00:25:57,440
or even a machine learning library that

00:25:55,520 --> 00:25:59,200
works here like

00:25:57,440 --> 00:26:01,279
so you have the data scientists issue

00:25:59,200 --> 00:26:04,000
the analysis and then

00:26:01,279 --> 00:26:05,440
from the analysis use like this analysis

00:26:04,000 --> 00:26:11,840
sent to the data science tool

00:26:05,440 --> 00:26:11,840
it is science tool user query

00:26:13,440 --> 00:26:16,960
yeah james i cannot hear you but i can

00:26:15,360 --> 00:26:19,600
see the the chat

00:26:16,960 --> 00:26:21,039
i will just finish in a minute here

00:26:19,600 --> 00:26:22,799
there is a query in the query go to the

00:26:21,039 --> 00:26:25,440
data system get the data

00:26:22,799 --> 00:26:27,279
and then this is the data processing and

00:26:25,440 --> 00:26:28,960
preparation task

00:26:27,279 --> 00:26:31,279
and then you send the data data science

00:26:28,960 --> 00:26:33,360
tool and the results are again an

00:26:31,279 --> 00:26:34,480
answer which is presented to the user

00:26:33,360 --> 00:26:36,799
and this

00:26:34,480 --> 00:26:38,640
is actually goes in there's a lot of

00:26:36,799 --> 00:26:40,960
iterations that you go it's like okay

00:26:38,640 --> 00:26:42,880
data scientist access data science tool

00:26:40,960 --> 00:26:44,880
get the data

00:26:42,880 --> 00:26:46,000
and then run the analytics task the data

00:26:44,880 --> 00:26:48,400
science task and then

00:26:46,000 --> 00:26:49,679
send the result to the user at the end

00:26:48,400 --> 00:26:51,360
and again

00:26:49,679 --> 00:26:54,159
process you can run so many iterations

00:26:51,360 --> 00:26:57,240
to come up with the insights you need

00:26:54,159 --> 00:26:58,480
the problem is that the data retrieval

00:26:57,240 --> 00:27:00,320
represents

00:26:58,480 --> 00:27:02,159
slow of the data is too large and the

00:27:00,320 --> 00:27:05,039
data is large

00:27:02,159 --> 00:27:06,720
uh so how we solve this we like again

00:27:05,039 --> 00:27:07,679
apache sedona can help you solve this

00:27:06,720 --> 00:27:10,799
problem here

00:27:07,679 --> 00:27:14,640
a little bit so apache sedona

00:27:10,799 --> 00:27:17,919
helped you with this however even if you

00:27:14,640 --> 00:27:21,440
scale the query processing right fast

00:27:17,919 --> 00:27:23,760
the result of the process when the data

00:27:21,440 --> 00:27:26,799
is prepared the result's still so big

00:27:23,760 --> 00:27:30,320
so when you try to for example

00:27:26,799 --> 00:27:32,880
show things in esri or for example

00:27:30,320 --> 00:27:34,080
build a map in tableau on this huge

00:27:32,880 --> 00:27:37,039
result

00:27:34,080 --> 00:27:38,880
it will these kind of tools like for

00:27:37,039 --> 00:27:41,039
example tableau when you run this

00:27:38,880 --> 00:27:45,039
it will crash if you want to visualize a

00:27:41,039 --> 00:27:45,039
map on massive scale data it will crash

00:27:45,840 --> 00:27:48,880
so how can we solve this interactivity

00:27:47,919 --> 00:27:50,159
problem here

00:27:48,880 --> 00:27:52,159
so that's another challenge and an

00:27:50,159 --> 00:27:54,960
example of this is here with tableau

00:27:52,159 --> 00:27:56,240
is that you need to make sure that this

00:27:54,960 --> 00:27:58,399
whole kind of

00:27:56,240 --> 00:27:59,520
visualization is very interactive even

00:27:58,399 --> 00:28:02,320
with large scale

00:27:59,520 --> 00:28:04,559
query resulted in the data system so

00:28:02,320 --> 00:28:07,679
this is one thing that

00:28:04,559 --> 00:28:09,679
that we are also working on how can we

00:28:07,679 --> 00:28:12,399
do the science to make this work in a

00:28:09,679 --> 00:28:17,840
more interactive way

00:28:12,399 --> 00:28:17,840
so um

00:28:18,240 --> 00:28:22,240
so again uh i'll leave you with this

00:28:23,120 --> 00:28:31,840
and i'm ready to take more questions

00:28:32,399 --> 00:28:36,720
so shall i take the questions in the uh

00:28:40,399 --> 00:28:45,840
mo can you hear me it's george

00:28:49,279 --> 00:28:52,399
now he looks muted

00:28:57,039 --> 00:29:13,840
it's best to do as much as you can in

00:28:58,960 --> 00:29:13,840
chat for the next few minutes jim

00:29:26,159 --> 00:29:30,320
so is spark support three point as spark

00:29:28,480 --> 00:29:34,720
three point xp supported soon

00:29:30,320 --> 00:29:36,720
yes um how does it compare to magellan

00:29:34,720 --> 00:29:37,919
uh there are there are a few comparisons

00:29:36,720 --> 00:29:40,399
with several papers not

00:29:37,919 --> 00:29:41,840
by us by all the third party that shows

00:29:40,399 --> 00:29:44,880
that for example

00:29:41,840 --> 00:29:46,080
we compared with magellan we can achieve

00:29:44,880 --> 00:29:48,640
we can be

00:29:46,080 --> 00:29:50,320
fast we can also have there's more

00:29:48,640 --> 00:29:51,919
comprehensive

00:29:50,320 --> 00:29:53,600
geospatial data operations that we can

00:29:51,919 --> 00:29:55,600
support

00:29:53,600 --> 00:29:57,039
this compared to magellan and on the

00:29:55,600 --> 00:29:59,679
sedona website you will see some

00:29:57,039 --> 00:30:02,880
comparisons with other systems as well

00:29:59,679 --> 00:30:05,200
uh do you plan to support 3d yes we plan

00:30:02,880 --> 00:30:08,399
to support 3d we're talking to some

00:30:05,200 --> 00:30:11,360
uh people now to we're trying to load

00:30:08,399 --> 00:30:12,720
point clouds also data into the system

00:30:11,360 --> 00:30:13,279
currently in the current version we

00:30:12,720 --> 00:30:16,399
don't

00:30:13,279 --> 00:30:18,159
but we're exploring all of this uh so

00:30:16,399 --> 00:30:19,760
geomesa also provides a spatial

00:30:18,159 --> 00:30:24,240
extension to

00:30:19,760 --> 00:30:28,000
uh spatial extension to spark

00:30:24,240 --> 00:30:30,399
uh yes we're familiar with geo mesa

00:30:28,000 --> 00:30:32,080
and there are some spatial extensions to

00:30:30,399 --> 00:30:33,679
spark and gmasa

00:30:32,080 --> 00:30:36,080
but there are some special extensions to

00:30:33,679 --> 00:30:39,440
a lot of things in gmas as well

00:30:36,080 --> 00:30:41,760
we are we are

00:30:39,440 --> 00:30:43,679
like again we focused on sparking we

00:30:41,760 --> 00:30:44,880
actually kind of squeeze the performance

00:30:43,679 --> 00:30:46,559
as much as we can

00:30:44,880 --> 00:30:48,240
so that we can achieve all these kind of

00:30:46,559 --> 00:30:49,679
things can there

00:30:48,240 --> 00:30:51,360
be collaboration we would love to

00:30:49,679 --> 00:30:54,559
collaborate with uh

00:30:51,360 --> 00:30:55,840
gmasa developers for sure and i believe

00:30:54,559 --> 00:30:59,120
there are some kind of

00:30:55,840 --> 00:31:01,360
talks especially when it comes to the

00:30:59,120 --> 00:31:04,559
common things we use

00:31:01,360 --> 00:31:06,799
um so i'm looking at other

00:31:04,559 --> 00:31:08,720
questions rio so the planning for sedona

00:31:06,799 --> 00:31:11,600
to handle rapture data yes

00:31:08,720 --> 00:31:14,000
that's also another thing uh in the

00:31:11,600 --> 00:31:17,440
planning

00:31:14,000 --> 00:31:19,840
okay so all right can you read it

00:31:17,440 --> 00:31:19,840
okay

00:31:23,440 --> 00:31:26,880
all right so their interoperability

00:31:25,679 --> 00:31:30,559
interac

00:31:26,880 --> 00:31:33,519
operability between sedona and

00:31:30,559 --> 00:31:36,399
between sedona and things like

00:31:33,519 --> 00:31:39,760
geotrellis stratoframes and geomesa

00:31:36,399 --> 00:31:42,399
uh there are things that we do similar

00:31:39,760 --> 00:31:42,880
similarly uh there are things that we

00:31:42,399 --> 00:31:45,120
also

00:31:42,880 --> 00:31:48,000
all like like we follow standards when

00:31:45,120 --> 00:31:51,200
it comes to geospatial data for maps

00:31:48,000 --> 00:31:52,080
uh so if these systems follow the same

00:31:51,200 --> 00:31:55,200
standards that

00:31:52,080 --> 00:31:56,320
they can be interoperable uh there are

00:31:55,200 --> 00:31:57,679
things that are

00:31:56,320 --> 00:31:59,279
that we implement like that are

00:31:57,679 --> 00:32:00,159
implemented in several systems that are

00:31:59,279 --> 00:32:03,279
actually

00:32:00,159 --> 00:32:06,080
uh i would say redundant uh we

00:32:03,279 --> 00:32:07,600
realize that right because either and

00:32:06,080 --> 00:32:10,640
i'll say it was done by

00:32:07,600 --> 00:32:12,399
um with a good intent like people like

00:32:10,640 --> 00:32:14,240
some people didn't know about you spark

00:32:12,399 --> 00:32:15,840
when it started long time ago

00:32:14,240 --> 00:32:17,519
maybe g spark didn't get a lot of

00:32:15,840 --> 00:32:19,600
traction earlier

00:32:17,519 --> 00:32:20,880
maybe we didn't know initially about geo

00:32:19,600 --> 00:32:23,360
mesa when we started just

00:32:20,880 --> 00:32:24,880
so we started this actually back in time

00:32:23,360 --> 00:32:28,000
like years years ago

00:32:24,880 --> 00:32:29,200
so but i believe uh interoperability

00:32:28,000 --> 00:32:32,159
would be a great thing to achieve

00:32:29,200 --> 00:32:32,159
between these systems

00:32:32,799 --> 00:32:36,960
all right so i think i've

00:32:37,440 --> 00:32:45,200
yes i think i've covered everything

00:32:41,360 --> 00:32:45,200
and we go over time where we're good

00:32:46,559 --> 00:32:49,600
awesome so it didn't go over time thank

00:32:48,559 --> 00:32:53,840
you so much guys

00:32:49,600 --> 00:32:56,320
please please uh use the patches donut

00:32:53,840 --> 00:32:57,919
and tell us if there are any issues and

00:32:56,320 --> 00:33:01,440
if we're gonna be we're gonna release

00:32:57,919 --> 00:33:04,159
uh we're gonna announce the sedona uh

00:33:01,440 --> 00:33:04,559
summit event soon so please do that as

00:33:04,159 --> 00:33:08,000
well

00:33:04,559 --> 00:33:10,320
and also uh at the event you can even

00:33:08,000 --> 00:33:12,240
like kind of uh

00:33:10,320 --> 00:33:13,679
criticize the system and say what you

00:33:12,240 --> 00:33:16,399
don't like about it we like that

00:33:13,679 --> 00:33:25,840
that will make us better thank you so

00:33:16,399 --> 00:33:25,840
much guys

00:33:50,720 --> 00:33:52,799

YouTube URL: https://www.youtube.com/watch?v=SW7ok75YghE


