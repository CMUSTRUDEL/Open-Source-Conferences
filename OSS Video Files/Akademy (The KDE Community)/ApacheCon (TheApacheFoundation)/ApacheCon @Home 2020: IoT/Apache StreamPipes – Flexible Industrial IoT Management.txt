Title: Apache StreamPipes – Flexible Industrial IoT Management
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020: IoT
Description: 
	Apache StreamPipes – Flexible Industrial IoT Management
Patrick Wiener

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Emerging data-driven use cases in the manufacturing business often require continuous integration and analysis of sensor data to identify time-critical situations. Apache StreamPipes is a new project in the Apache Incubator which aims at providing a self-service industrial IoT toolbox to enable non-technical users to connect, analyze and explore IoT data streams. It provides many connectors for industrial communication protocols and a library of reusable algorithms to analyze sensor measurements or camera images based on simple rules up to machine learning methods. A variety of data sinks allow for easy exchange with third party systems, including many Apache IoT and Big Data projects (including Apache PLC4X, Apache Kafka, Apache IoTDB). In this talk, we give an overview of Apache StreamPipes (incubating) and interactively show how to extend the IoT toolbox and create a custom data processor using the integrated Software Development Kit.

Patrick Wiener currently works at the FZI Research Center for Information Technology in Karlsruhe. His research interests include Distributed Computing (Cloud, Edge/Fog Computing), IoT, and Stream Processing. Patrick is an expert for infrastructure management such as containers and container orchestration frameworks. He has worked in several public-funded research projects related to Big Data Management and Stream Processing in domains such as manufacturing, logistics and geographical information systems.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,880 --> 00:00:27,359
so

00:00:25,359 --> 00:00:29,039
first of all thank you very much for

00:00:27,359 --> 00:00:31,359
joining i got the chat

00:00:29,039 --> 00:00:33,120
right open next to me so hopefully at

00:00:31,359 --> 00:00:34,800
the end of this talk we're gonna have

00:00:33,120 --> 00:00:37,680
a couple of more minutes to go through

00:00:34,800 --> 00:00:41,200
your questions and

00:00:37,680 --> 00:00:43,680
with that um yeah warm welcome

00:00:41,200 --> 00:00:44,239
to the rest of the world my name is

00:00:43,680 --> 00:00:45,920
patrick

00:00:44,239 --> 00:00:47,520
and i'm going to talk about apache

00:00:45,920 --> 00:00:51,840
stream pipes

00:00:47,520 --> 00:00:51,840
flexible industrial iot management

00:00:54,640 --> 00:00:59,280
so first of all who am i and how did i

00:00:57,600 --> 00:01:01,680
get here

00:00:59,280 --> 00:01:04,080
i'm basically a researcher by day and

00:01:01,680 --> 00:01:06,880
work at a german

00:01:04,080 --> 00:01:07,520
company it's called fzi and i love open

00:01:06,880 --> 00:01:10,240
source

00:01:07,520 --> 00:01:12,960
and that's what i do most of the time on

00:01:10,240 --> 00:01:16,560
the other time um

00:01:12,960 --> 00:01:20,560
last year at the apache con in las vegas

00:01:16,560 --> 00:01:23,119
we had a talk by dominic which was here

00:01:20,560 --> 00:01:25,920
and it was a talk about stream pipes

00:01:23,119 --> 00:01:28,960
itself but we weren't at the

00:01:25,920 --> 00:01:31,439
one of the apache projects at that time

00:01:28,960 --> 00:01:33,360
and then we decided okay the community

00:01:31,439 --> 00:01:37,119
is really cool and we really

00:01:33,360 --> 00:01:39,439
enjoyed the new the new booth that we

00:01:37,119 --> 00:01:41,360
experienced through through other

00:01:39,439 --> 00:01:44,720
projects such as plc for x

00:01:41,360 --> 00:01:46,640
and iot db and so we we said okay

00:01:44,720 --> 00:01:48,479
hey let's let's try it and join the

00:01:46,640 --> 00:01:49,360
incubator and that's what we basically

00:01:48,479 --> 00:01:52,479
did last year

00:01:49,360 --> 00:01:54,399
in november um we joined the incubator

00:01:52,479 --> 00:01:57,119
with the stream pipes

00:01:54,399 --> 00:01:59,200
and besides being a ppmc member of

00:01:57,119 --> 00:02:02,960
apache stream pipes i really love gifs

00:01:59,200 --> 00:02:06,840
and for those of you who want to

00:02:02,960 --> 00:02:09,200
tag me in their tweets those are the

00:02:06,840 --> 00:02:11,440
handles so

00:02:09,200 --> 00:02:13,440
taking a look at the agenda for today um

00:02:11,440 --> 00:02:15,520
first of all i want to give a brief

00:02:13,440 --> 00:02:17,920
overview of the landscape of some

00:02:15,520 --> 00:02:20,840
motivations and backgrounds

00:02:17,920 --> 00:02:22,080
related to industrial iot and factory

00:02:20,840 --> 00:02:24,800
4.0

00:02:22,080 --> 00:02:26,319
then some basics about stream pipes like

00:02:24,800 --> 00:02:29,120
stream pipes 101

00:02:26,319 --> 00:02:29,920
including a little demo to just get you

00:02:29,120 --> 00:02:32,800
sorted

00:02:29,920 --> 00:02:34,480
what it's like and then diving a little

00:02:32,800 --> 00:02:37,440
bit deeper

00:02:34,480 --> 00:02:38,400
technically what's what's going on

00:02:37,440 --> 00:02:40,400
behind the ui

00:02:38,400 --> 00:02:42,080
under the hood how is everything working

00:02:40,400 --> 00:02:45,440
together and

00:02:42,080 --> 00:02:46,640
then i want to focus on the the way of

00:02:45,440 --> 00:02:49,360
how

00:02:46,640 --> 00:02:51,519
new components how new algorithms can be

00:02:49,360 --> 00:02:54,879
introduced to stream by display

00:02:51,519 --> 00:02:57,040
using our software development kit and

00:02:54,879 --> 00:02:58,080
finally i want to conclude by showing

00:02:57,040 --> 00:03:00,560
our roadmap

00:02:58,080 --> 00:03:05,840
and how every one of you could

00:03:00,560 --> 00:03:05,840
contribute in the future

00:03:06,239 --> 00:03:13,760
so looking at a typical industrial iot

00:03:10,159 --> 00:03:14,560
factory 4.0 scenario we we see that

00:03:13,760 --> 00:03:16,480
there are

00:03:14,560 --> 00:03:18,000
a lot of things going on there so

00:03:16,480 --> 00:03:20,879
basically there are data streams

00:03:18,000 --> 00:03:23,280
everywhere when when looking at this uh

00:03:20,879 --> 00:03:26,159
small picture we see

00:03:23,280 --> 00:03:26,480
robots that are doing a pick and place

00:03:26,159 --> 00:03:29,599
um

00:03:26,480 --> 00:03:31,840
operation we see machines that are

00:03:29,599 --> 00:03:33,920
maybe welding some parts together or

00:03:31,840 --> 00:03:37,120
cameras that are analyzing

00:03:33,920 --> 00:03:38,560
pictures or they're they're light

00:03:37,120 --> 00:03:42,239
sensors

00:03:38,560 --> 00:03:43,599
that produce certain boolean values and

00:03:42,239 --> 00:03:45,840
of course there might be some

00:03:43,599 --> 00:03:47,599
environmental data such as temperatures

00:03:45,840 --> 00:03:50,560
humidities etc

00:03:47,599 --> 00:03:51,440
that is going on on a shop floor level

00:03:50,560 --> 00:03:56,000
and

00:03:51,440 --> 00:03:58,000
from this kind of heterogeneous data

00:03:56,000 --> 00:04:00,720
on the shop floor level more

00:03:58,000 --> 00:04:04,879
heterogeneity is basically added

00:04:00,720 --> 00:04:04,879
when it comes to the used protocols

00:04:05,519 --> 00:04:08,720
when it comes to the use protocols such

00:04:08,480 --> 00:04:15,840
as

00:04:08,720 --> 00:04:15,840
ross mqtt modbus or plc for x

00:04:18,880 --> 00:04:23,919
and plc for x i hope everything's okay

00:04:21,600 --> 00:04:27,120
with the camera i got some notes that

00:04:23,919 --> 00:04:31,440
maybe the camera is kind of shaky

00:04:27,120 --> 00:04:32,400
um maybe someone can can write in the

00:04:31,440 --> 00:04:34,479
chat

00:04:32,400 --> 00:04:43,199
if not i just continue and hopefully

00:04:34,479 --> 00:04:46,400
everything's going to sort out

00:04:43,199 --> 00:04:48,320
and um with that with that kind of

00:04:46,400 --> 00:04:49,040
heterogeneity on the on the shop floor

00:04:48,320 --> 00:04:52,080
level

00:04:49,040 --> 00:04:55,840
um there are luckily um

00:04:52,080 --> 00:04:58,240
some some great universal protocols

00:04:55,840 --> 00:04:59,759
such as plc for x that makes life a

00:04:58,240 --> 00:05:01,840
little bit more easier

00:04:59,759 --> 00:05:04,080
by by having this kind of universal

00:05:01,840 --> 00:05:07,120
control plane in order to access certain

00:05:04,080 --> 00:05:10,560
protocols such as modbus or plc

00:05:07,120 --> 00:05:12,639
however collecting like all of them

00:05:10,560 --> 00:05:14,320
is not quite intuitive and it's still

00:05:12,639 --> 00:05:17,199
like a major challenge

00:05:14,320 --> 00:05:17,919
to to bridge like the gap from the

00:05:17,199 --> 00:05:21,039
initial

00:05:17,919 --> 00:05:23,520
um from the initial machine

00:05:21,039 --> 00:05:26,720
um to a harmonized format to to

00:05:23,520 --> 00:05:26,720
analytics framework

00:05:27,440 --> 00:05:30,960
and that's where stream pipes basically

00:05:30,000 --> 00:05:33,919
sits on top

00:05:30,960 --> 00:05:34,880
as an industrial iot toolbox for domain

00:05:33,919 --> 00:05:36,800
experts

00:05:34,880 --> 00:05:38,479
that allows to connect to these

00:05:36,800 --> 00:05:42,080
individual protocols

00:05:38,479 --> 00:05:46,080
really easy easily in an intuitive way

00:05:42,080 --> 00:05:50,000
reusing exemplified here certain

00:05:46,080 --> 00:05:52,240
other universal protocols such as plc4x

00:05:50,000 --> 00:05:53,280
in order to get the data out of the

00:05:52,240 --> 00:05:56,000
machines

00:05:53,280 --> 00:05:56,400
in order to bridge that first mile and

00:05:56,000 --> 00:05:59,440
then

00:05:56,400 --> 00:06:03,120
um like do some fancy stuff analytics uh

00:05:59,440 --> 00:06:03,120
stuff with it with the data itself

00:06:03,759 --> 00:06:07,759
so now that you've got a brief

00:06:05,840 --> 00:06:11,039
introduction about the motivation

00:06:07,759 --> 00:06:14,160
um let's talk about stream pipes 101

00:06:11,039 --> 00:06:16,080
so what is stream pipes actually

00:06:14,160 --> 00:06:17,840
stream pipes we have this kind of

00:06:16,080 --> 00:06:19,440
mission statement

00:06:17,840 --> 00:06:21,039
and we say it's an open source

00:06:19,440 --> 00:06:24,080
industrial toolbox

00:06:21,039 --> 00:06:27,199
that enables the non-technical users

00:06:24,080 --> 00:06:30,880
and without the it skills to connect

00:06:27,199 --> 00:06:34,000
analyze and exploit the iot data streams

00:06:30,880 --> 00:06:36,960
and when i say and talk about toolbox i

00:06:34,000 --> 00:06:37,840
mean various parts of it so i'm going to

00:06:36,960 --> 00:06:41,680
go through

00:06:37,840 --> 00:06:45,360
like each of the individual parts here

00:06:41,680 --> 00:06:47,520
starting by connect which is our

00:06:45,360 --> 00:06:48,560
adapter marketplace in order to easily

00:06:47,520 --> 00:06:51,360
connect the

00:06:48,560 --> 00:06:54,560
um heterogeneous data sources then we

00:06:51,360 --> 00:06:56,880
once the data is in the system

00:06:54,560 --> 00:06:58,000
we got the pipeline editor with the

00:06:56,880 --> 00:07:01,360
extensible set

00:06:58,000 --> 00:07:04,000
of algorithms and and basically

00:07:01,360 --> 00:07:05,360
ranging from simple filters to more

00:07:04,000 --> 00:07:07,360
advanced algorithms

00:07:05,360 --> 00:07:10,080
even wrapping machine learning models

00:07:07,360 --> 00:07:11,599
itself then we got a live dashboard in

00:07:10,080 --> 00:07:14,800
order to quickly

00:07:11,599 --> 00:07:18,160
get an overview of what's going on

00:07:14,800 --> 00:07:19,919
and recently um with we've just been

00:07:18,160 --> 00:07:22,000
working on the on the next release we

00:07:19,919 --> 00:07:23,199
got a better version coming up of a data

00:07:22,000 --> 00:07:25,919
explorer which is

00:07:23,199 --> 00:07:26,720
more steered towards historical data

00:07:25,919 --> 00:07:30,080
analysis

00:07:26,720 --> 00:07:33,120
analysis um which is in in our

00:07:30,080 --> 00:07:35,360
experience quite um

00:07:33,120 --> 00:07:36,400
a good way to to explore what was going

00:07:35,360 --> 00:07:39,520
on like the last

00:07:36,400 --> 00:07:42,560
couple of um of minutes ago

00:07:39,520 --> 00:07:45,919
and then we got the notifications which

00:07:42,560 --> 00:07:48,960
is an internal thing where we can

00:07:45,919 --> 00:07:50,560
notify um base or throw notifications

00:07:48,960 --> 00:07:53,199
based on on certain

00:07:50,560 --> 00:07:56,240
events that are modeled with an

00:07:53,199 --> 00:07:59,440
analytical pipeline

00:07:56,240 --> 00:08:00,720
so how does it work um looking at the

00:07:59,440 --> 00:08:03,520
really high level

00:08:00,720 --> 00:08:04,800
um it's more or less four steps so first

00:08:03,520 --> 00:08:07,840
of all

00:08:04,800 --> 00:08:09,840
you connect your iot data sources

00:08:07,840 --> 00:08:11,440
therefore we we got the the mentioned

00:08:09,840 --> 00:08:14,240
iot

00:08:11,440 --> 00:08:16,240
adapter marketplace currently with more

00:08:14,240 --> 00:08:19,520
than 30 adapters

00:08:16,240 --> 00:08:21,680
once the data is connected to the system

00:08:19,520 --> 00:08:23,360
we can switch over to the pipeline

00:08:21,680 --> 00:08:24,160
editor which you see here in that

00:08:23,360 --> 00:08:27,360
picture

00:08:24,160 --> 00:08:28,639
and leverage our reusable data processes

00:08:27,360 --> 00:08:30,639
and sync

00:08:28,639 --> 00:08:33,279
things um currently there are more than

00:08:30,639 --> 00:08:34,240
100 so-called pipeline elements ready to

00:08:33,279 --> 00:08:37,360
use

00:08:34,240 --> 00:08:38,479
and deploy and execute it more or less

00:08:37,360 --> 00:08:41,839
wherever you want

00:08:38,479 --> 00:08:45,120
um so we got docker images for

00:08:41,839 --> 00:08:46,240
x 86 architectures as well as arm

00:08:45,120 --> 00:08:49,519
architectures

00:08:46,240 --> 00:08:52,640
you can run it on linux and windows on

00:08:49,519 --> 00:08:54,720
on mac os and we got a helm chart for

00:08:52,640 --> 00:08:58,880
kubernetes deployment as well

00:08:54,720 --> 00:09:02,160
and more important it's it's 0.4

00:08:58,880 --> 00:09:03,519
where we see where stream pipes shines

00:09:02,160 --> 00:09:06,720
at this point

00:09:03,519 --> 00:09:08,080
is realizing use cases and when talking

00:09:06,720 --> 00:09:09,839
about use cases we've

00:09:08,080 --> 00:09:11,760
experienced in the community and

00:09:09,839 --> 00:09:14,240
throughout our work um

00:09:11,760 --> 00:09:15,600
we've experienced more or less these

00:09:14,240 --> 00:09:18,959
kind of four categories

00:09:15,600 --> 00:09:21,600
and use cases so first of all like

00:09:18,959 --> 00:09:22,240
there's the continuous asset monitoring

00:09:21,600 --> 00:09:24,880
so

00:09:22,240 --> 00:09:26,000
we only want to to get a brief overview

00:09:24,880 --> 00:09:29,120
of what's going on

00:09:26,000 --> 00:09:31,760
by looking at this at the dashboard and

00:09:29,120 --> 00:09:32,240
and see the current state of a machine

00:09:31,760 --> 00:09:35,440
so

00:09:32,240 --> 00:09:37,360
this must not really be a deep

00:09:35,440 --> 00:09:38,720
analytical use case as more or less to

00:09:37,360 --> 00:09:41,920
get this

00:09:38,720 --> 00:09:43,760
sort of real-time experience what's the

00:09:41,920 --> 00:09:47,040
current state of the machine

00:09:43,760 --> 00:09:50,240
then um comes the kpi analytics

00:09:47,040 --> 00:09:51,839
side so we don't really are interested

00:09:50,240 --> 00:09:55,120
in the raw

00:09:51,839 --> 00:09:58,320
raw metrics itself but we want to get

00:09:55,120 --> 00:10:01,360
the relevant production or asset kpis

00:09:58,320 --> 00:10:04,000
out of the system by using um

00:10:01,360 --> 00:10:05,200
by using one of the pipeline elements in

00:10:04,000 --> 00:10:08,320
order to calculate

00:10:05,200 --> 00:10:11,839
what's really relevant for you to to get

00:10:08,320 --> 00:10:14,800
a good overview of how healthy your

00:10:11,839 --> 00:10:17,279
current production is running

00:10:14,800 --> 00:10:19,279
then i'm coming more to the advanced

00:10:17,279 --> 00:10:22,399
analytic sides

00:10:19,279 --> 00:10:24,640
where we see stream pipes used quite

00:10:22,399 --> 00:10:27,519
heavily in the community

00:10:24,640 --> 00:10:28,640
as the training data collection so for

00:10:27,519 --> 00:10:31,120
that

00:10:28,640 --> 00:10:33,600
like pre-processing on the raw data

00:10:31,120 --> 00:10:36,720
coming from the machines is always

00:10:33,600 --> 00:10:38,959
a quite intense task to do and with

00:10:36,720 --> 00:10:44,399
stream pipes and the newly introduced

00:10:38,959 --> 00:10:47,279
um or upcoming and the 0.67

00:10:44,399 --> 00:10:47,920
release data explorer you can really

00:10:47,279 --> 00:10:51,279
easily

00:10:47,920 --> 00:10:53,120
connect to your machine reprocess your

00:10:51,279 --> 00:10:54,959
data and store it in the data lake and

00:10:53,120 --> 00:10:56,880
export it as a csv

00:10:54,959 --> 00:11:00,560
um to to build up this kind of training

00:10:56,880 --> 00:11:03,040
data repository quite easily and

00:11:00,560 --> 00:11:04,000
last but not least you can use stream

00:11:03,040 --> 00:11:06,240
pipes for

00:11:04,000 --> 00:11:08,160
process and product quality inspection

00:11:06,240 --> 00:11:11,760
either by

00:11:08,160 --> 00:11:13,760
using let's let's say complex event

00:11:11,760 --> 00:11:16,240
processing rules like

00:11:13,760 --> 00:11:18,079
a certain threshold is higher is

00:11:16,240 --> 00:11:19,040
exceeded within the last couple of

00:11:18,079 --> 00:11:21,920
minutes

00:11:19,040 --> 00:11:22,880
and on the one hand and on the other

00:11:21,920 --> 00:11:24,880
hand you can

00:11:22,880 --> 00:11:26,000
quite easily wrap the machine learning

00:11:24,880 --> 00:11:28,800
model

00:11:26,000 --> 00:11:30,000
in a data processor and apply it on on

00:11:28,800 --> 00:11:32,800
real and live

00:11:30,000 --> 00:11:32,800
data coming

00:11:34,320 --> 00:11:39,600
so let's look at the pipeline editor

00:11:37,920 --> 00:11:41,360
and the certain types of pipeline

00:11:39,600 --> 00:11:44,000
elements that we got

00:11:41,360 --> 00:11:45,839
so it all starts once your data is

00:11:44,000 --> 00:11:48,480
connected with a data stream

00:11:45,839 --> 00:11:49,120
and a data stream is an audit sequence

00:11:48,480 --> 00:11:51,440
of event

00:11:49,120 --> 00:11:54,480
provided by stream pipes connect the

00:11:51,440 --> 00:11:56,800
adapter marketplace i was talking about

00:11:54,480 --> 00:11:58,800
in order to to access the data from the

00:11:56,800 --> 00:12:02,000
industrial event source which could be

00:11:58,800 --> 00:12:04,240
a universal robot via the ros operating

00:12:02,000 --> 00:12:06,959
system

00:12:04,240 --> 00:12:09,200
then there are data processors they

00:12:06,959 --> 00:12:12,240
apply a configurable function

00:12:09,200 --> 00:12:15,519
on one or more input streams like

00:12:12,240 --> 00:12:17,839
transforming enriching filtering etc

00:12:15,519 --> 00:12:19,600
and produces an output event stream

00:12:17,839 --> 00:12:22,880
again

00:12:19,600 --> 00:12:25,680
and lastly we end the um the graph

00:12:22,880 --> 00:12:27,120
with the data things um which marked the

00:12:25,680 --> 00:12:30,240
end of a pipeline

00:12:27,120 --> 00:12:32,240
and they either forward an event input

00:12:30,240 --> 00:12:32,639
stream to an internal sync i was talking

00:12:32,240 --> 00:12:34,480
about

00:12:32,639 --> 00:12:36,959
for example live dashboard or the

00:12:34,480 --> 00:12:39,839
historic data explorer

00:12:36,959 --> 00:12:40,480
or any third-party things as well so you

00:12:39,839 --> 00:12:46,079
could

00:12:40,480 --> 00:12:46,079
forward it to kafka to iotdb to ross etc

00:12:47,440 --> 00:12:52,880
so here you see a couple of more

00:12:50,560 --> 00:12:55,760
screenshots about the

00:12:52,880 --> 00:12:56,880
iot toolbox complementing our pipeline

00:12:55,760 --> 00:13:00,720
and detour

00:12:56,880 --> 00:13:02,240
and with that i would switch over to

00:13:00,720 --> 00:13:06,240
give you a brief overview

00:13:02,240 --> 00:13:11,120
of how stream pipes work

00:13:06,240 --> 00:13:14,160
and here hopefully everyone sees it

00:13:11,120 --> 00:13:21,120
stream pipes is running currently on my

00:13:14,160 --> 00:13:23,120
macbook let's log in first

00:13:21,120 --> 00:13:25,200
and uh on the home screen you're

00:13:23,120 --> 00:13:26,959
provided with a brief overview of what's

00:13:25,200 --> 00:13:27,839
currently happening within stream five

00:13:26,959 --> 00:13:29,680
so we see

00:13:27,839 --> 00:13:31,200
that there are actually two pipelines

00:13:29,680 --> 00:13:34,800
already running

00:13:31,200 --> 00:13:38,399
we got 81 pipeline elements installed

00:13:34,800 --> 00:13:40,959
and we got a brief description about the

00:13:38,399 --> 00:13:43,760
the various components that we're using

00:13:40,959 --> 00:13:45,600
so to get started let's switch over to

00:13:43,760 --> 00:13:47,839
connect

00:13:45,600 --> 00:13:49,839
which is the adapter marketplace i was

00:13:47,839 --> 00:13:50,399
mentioning and here you see it we've got

00:13:49,839 --> 00:13:53,519
a

00:13:50,399 --> 00:13:58,000
huge variety of different protocols and

00:13:53,519 --> 00:14:00,480
and um ways of how we could connect to

00:13:58,000 --> 00:14:01,199
to certain data endpoints so there's

00:14:00,480 --> 00:14:04,240
kafka

00:14:01,199 --> 00:14:07,519
pulsar there's also

00:14:04,240 --> 00:14:11,600
http streams um

00:14:07,519 --> 00:14:15,760
there's mqtt some specific protocols

00:14:11,600 --> 00:14:16,959
such as netview or a plc for x of course

00:14:15,760 --> 00:14:18,720
and for the demo

00:14:16,959 --> 00:14:20,320
i'm going to switch over to a data

00:14:18,720 --> 00:14:20,959
simulator because i don't have the

00:14:20,320 --> 00:14:23,920
machine

00:14:20,959 --> 00:14:24,560
running right now but the steps of how

00:14:23,920 --> 00:14:27,279
to connect

00:14:24,560 --> 00:14:28,560
to to a certain asset is more or less

00:14:27,279 --> 00:14:31,680
the same

00:14:28,560 --> 00:14:34,560
so first of all you would

00:14:31,680 --> 00:14:35,839
be provided with a certain dialogue to

00:14:34,560 --> 00:14:39,120
to set up um

00:14:35,839 --> 00:14:42,880
for example the connections to your um

00:14:39,120 --> 00:14:46,480
mqtt endpoint or to your plc

00:14:42,880 --> 00:14:49,680
here we just specify the event rate so

00:14:46,480 --> 00:14:52,880
every second we want to produce a

00:14:49,680 --> 00:14:54,800
flow rate sensor event

00:14:52,880 --> 00:14:57,600
then what you see here is that the

00:14:54,800 --> 00:14:58,000
system guesses the schema so it picks up

00:14:57,600 --> 00:15:00,000
certain

00:14:58,000 --> 00:15:02,079
events from the event stream and

00:15:00,000 --> 00:15:03,360
provides a user with a with a short

00:15:02,079 --> 00:15:06,240
overview of

00:15:03,360 --> 00:15:07,279
the potential schema and it lets the

00:15:06,240 --> 00:15:10,240
user refine

00:15:07,279 --> 00:15:12,560
this kind of schema so here we detected

00:15:10,240 --> 00:15:14,320
these various event fields

00:15:12,560 --> 00:15:16,000
it also detected that we've got a

00:15:14,320 --> 00:15:19,040
timestamp in the event

00:15:16,000 --> 00:15:21,040
as well and for example the temperature

00:15:19,040 --> 00:15:24,240
event here

00:15:21,040 --> 00:15:26,880
we know that it's decreased celsius

00:15:24,240 --> 00:15:27,839
and we could right now configure it to

00:15:26,880 --> 00:15:30,880
switch from

00:15:27,839 --> 00:15:34,000
um decrease

00:15:30,880 --> 00:15:35,839
decrease celsius um to

00:15:34,000 --> 00:15:37,360
another unit for example decree

00:15:35,839 --> 00:15:40,480
fahrenheit right in the

00:15:37,360 --> 00:15:42,240
in the connect instance so without any

00:15:40,480 --> 00:15:46,079
kind of programming involved we could

00:15:42,240 --> 00:15:46,079
easily transform these kind of units

00:15:47,920 --> 00:15:54,639
then we give the adapter a name

00:15:51,120 --> 00:15:57,279
flow rate apache con and start it

00:15:54,639 --> 00:15:58,399
so this kind of configuration is sent to

00:15:57,279 --> 00:16:01,360
the connect

00:15:58,399 --> 00:16:02,639
worker instance which is configured in

00:16:01,360 --> 00:16:05,519
the background

00:16:02,639 --> 00:16:06,959
and it connects to this kind of

00:16:05,519 --> 00:16:08,000
industrial asset here it's only

00:16:06,959 --> 00:16:09,839
simulated

00:16:08,000 --> 00:16:11,199
and presents the user with a live

00:16:09,839 --> 00:16:15,120
information about

00:16:11,199 --> 00:16:15,120
the data currently flowing in the system

00:16:15,199 --> 00:16:20,639
and with that we could easily switch

00:16:17,040 --> 00:16:20,639
over to the pipeliner detour

00:16:20,880 --> 00:16:27,440
where we've got the

00:16:24,399 --> 00:16:31,759
overview of

00:16:27,440 --> 00:16:35,199
various already existing adapters

00:16:31,759 --> 00:16:37,279
and we can use ours just drag and drop

00:16:35,199 --> 00:16:40,880
it into the canvas

00:16:37,279 --> 00:16:42,480
and use any kind of um data processor

00:16:40,880 --> 00:16:45,600
that is already available

00:16:42,480 --> 00:16:47,839
to us so let's say we're interested in

00:16:45,600 --> 00:16:49,360
just basically filtering towards a

00:16:47,839 --> 00:16:51,680
certain threshold so we can use the

00:16:49,360 --> 00:16:54,959
numerical filter

00:16:51,680 --> 00:16:57,279
connect these elements and configure the

00:16:54,959 --> 00:16:59,519
numerical filter processor

00:16:57,279 --> 00:17:00,800
so we want to filter for example the

00:16:59,519 --> 00:17:03,759
mass flow

00:17:00,800 --> 00:17:05,120
which should be higher than a certain

00:17:03,759 --> 00:17:08,400
threshold let's say

00:17:05,120 --> 00:17:11,760
three and save it

00:17:08,400 --> 00:17:13,600
so now this element is configured and

00:17:11,760 --> 00:17:16,000
now we're not interested in anything

00:17:13,600 --> 00:17:19,199
further so we just want to see the live

00:17:16,000 --> 00:17:24,079
output so we use the dashboard sync

00:17:19,199 --> 00:17:24,079
connect it again give it a name

00:17:29,120 --> 00:17:34,559
and save the pipeline also give the

00:17:31,679 --> 00:17:34,559
pipeline a name

00:17:36,000 --> 00:17:38,720
and start it

00:17:40,480 --> 00:17:44,080
so now the description of the pipeline

00:17:43,200 --> 00:17:46,559
itself

00:17:44,080 --> 00:17:48,000
is distributed to the various components

00:17:46,559 --> 00:17:50,080
that were involved

00:17:48,000 --> 00:17:51,120
um so for example here the numerical

00:17:50,080 --> 00:17:54,640
filter

00:17:51,120 --> 00:17:55,120
and the dashboard sync so we could

00:17:54,640 --> 00:17:57,840
switch

00:17:55,120 --> 00:17:59,919
over to the live dashboard you see here

00:17:57,840 --> 00:18:00,559
an overview of already running pipelines

00:17:59,919 --> 00:18:04,559
that are

00:18:00,559 --> 00:18:08,080
pre-configured by me beforehand

00:18:04,559 --> 00:18:10,559
so we can use that dashboard here

00:18:08,080 --> 00:18:11,679
let's say edit and add a new

00:18:10,559 --> 00:18:16,000
visualization

00:18:11,679 --> 00:18:19,760
on the apache pipeline demo one

00:18:16,000 --> 00:18:23,120
say table let's just say

00:18:19,760 --> 00:18:29,840
here we wanna see the mass flow

00:18:23,120 --> 00:18:33,280
and the timestamp create

00:18:29,840 --> 00:18:35,280
so now we should only see the

00:18:33,280 --> 00:18:36,799
mass flow values that are above our

00:18:35,280 --> 00:18:40,400
threshold

00:18:36,799 --> 00:18:42,160
and save it i was talking about the the

00:18:40,400 --> 00:18:45,440
new better feature that is upcoming

00:18:42,160 --> 00:18:47,600
so we got the data explorer um so

00:18:45,440 --> 00:18:49,120
therefore we can browse back in in

00:18:47,600 --> 00:18:52,160
history and see what was

00:18:49,120 --> 00:18:56,400
um going on like a couple of minutes ago

00:18:52,160 --> 00:18:59,679
so therefore i set up a pipeline also

00:18:56,400 --> 00:18:59,679
with the same values

00:19:00,240 --> 00:19:06,400
and as you can see here um like the last

00:19:03,120 --> 00:19:10,480
15 minutes we get this overview we can

00:19:06,400 --> 00:19:11,039
like zoom in to detect certain patterns

00:19:10,480 --> 00:19:13,919
maybe

00:19:11,039 --> 00:19:14,480
we can hover over the individual data

00:19:13,919 --> 00:19:15,919
points

00:19:14,480 --> 00:19:17,840
and just get it get a better

00:19:15,919 --> 00:19:22,080
understanding of what's what's

00:19:17,840 --> 00:19:25,360
really going on in the system okay

00:19:22,080 --> 00:19:28,559
that i would like to switch back

00:19:25,360 --> 00:19:31,520
and come to the technical deep dive

00:19:28,559 --> 00:19:33,679
so stream pipes architecture is more

00:19:31,520 --> 00:19:35,919
than just a web application so

00:19:33,679 --> 00:19:36,960
um we always describe it as this kind of

00:19:35,919 --> 00:19:40,160
micro service

00:19:36,960 --> 00:19:41,760
architectural approach so starting from

00:19:40,160 --> 00:19:44,320
the data source itself

00:19:41,760 --> 00:19:47,120
we saw that we could easily connect

00:19:44,320 --> 00:19:49,440
various heterogeneous protocols via

00:19:47,120 --> 00:19:52,240
streampipes connect adapters so

00:19:49,440 --> 00:19:54,960
basically every connector itself

00:19:52,240 --> 00:19:56,240
they can be bundled together and run in

00:19:54,960 --> 00:19:59,360
in docker containers

00:19:56,240 --> 00:20:00,400
and they can be geo-distributed to to

00:19:59,360 --> 00:20:02,080
places

00:20:00,400 --> 00:20:04,159
right at the edge close to the data

00:20:02,080 --> 00:20:07,840
source itself where we can harmonize the

00:20:04,159 --> 00:20:10,960
data pre-process it et cetera

00:20:07,840 --> 00:20:13,440
on the next layer we got our

00:20:10,960 --> 00:20:14,240
universal transport layer our message

00:20:13,440 --> 00:20:17,360
broker

00:20:14,240 --> 00:20:18,320
which is exchangeable so for that we

00:20:17,360 --> 00:20:21,520
mostly

00:20:18,320 --> 00:20:21,760
rely on apache kafka but we could switch

00:20:21,520 --> 00:20:25,200
it

00:20:21,760 --> 00:20:28,720
to jms or mqtt within

00:20:25,200 --> 00:20:29,200
settings in in the ui the next kind of

00:20:28,720 --> 00:20:32,720
layer

00:20:29,200 --> 00:20:36,640
is the pipeline element microservice

00:20:32,720 --> 00:20:39,760
layer here we see the individual

00:20:36,640 --> 00:20:42,640
algorithms or or things here

00:20:39,760 --> 00:20:43,360
that could run on a single standalone

00:20:42,640 --> 00:20:46,559
instance

00:20:43,360 --> 00:20:49,280
but we also got wrappers for

00:20:46,559 --> 00:20:50,880
like frameworks such as apache flink

00:20:49,280 --> 00:20:54,400
where you can really scale out

00:20:50,880 --> 00:20:57,679
this kind of data processing and um

00:20:54,400 --> 00:20:59,440
lastly coming more towards the ui we got

00:20:57,679 --> 00:21:00,880
the pipeline management which is the

00:20:59,440 --> 00:21:03,679
core of stream pipes

00:21:00,880 --> 00:21:04,880
where everything runs together service

00:21:03,679 --> 00:21:09,039
registration

00:21:04,880 --> 00:21:09,039
service discovery etc

00:21:09,760 --> 00:21:16,000
so let's look at a basic pipeline

00:21:13,360 --> 00:21:17,600
so here we got an industrial event

00:21:16,000 --> 00:21:20,640
source

00:21:17,600 --> 00:21:22,159
it's just a machine here producing a

00:21:20,640 --> 00:21:28,480
temperature event

00:21:22,159 --> 00:21:32,000
it says 30 73.5

00:21:28,480 --> 00:21:35,280
double value so what we want to do is um

00:21:32,000 --> 00:21:36,720
access that kind of the kind of events

00:21:35,280 --> 00:21:38,480
via mqtt

00:21:36,720 --> 00:21:39,919
i'm setting up a streamcast connect

00:21:38,480 --> 00:21:44,080
instance we

00:21:39,919 --> 00:21:46,320
um connect them then hook it up to a

00:21:44,080 --> 00:21:50,320
filter as we did before

00:21:46,320 --> 00:21:52,640
setting a threshold and pipeline it

00:21:50,320 --> 00:21:55,200
pipeline it to a dashboard or a data

00:21:52,640 --> 00:21:55,200
lake thing

00:21:55,360 --> 00:22:00,880
so what's really going on here in the

00:21:58,799 --> 00:22:01,919
in the connect worker which is a docker

00:22:00,880 --> 00:22:04,960
container

00:22:01,919 --> 00:22:06,000
is that for for every instance we get a

00:22:04,960 --> 00:22:08,480
description

00:22:06,000 --> 00:22:10,640
so for example here the temperature

00:22:08,480 --> 00:22:12,720
value itself

00:22:10,640 --> 00:22:14,960
has a as a unit here in decreased

00:22:12,720 --> 00:22:15,840
celsius um we get information about the

00:22:14,960 --> 00:22:18,799
event schema

00:22:15,840 --> 00:22:21,039
about data types such as double or um

00:22:18,799 --> 00:22:25,200
user-provided configurations such

00:22:21,039 --> 00:22:28,960
as the ip report or a certain topic

00:22:25,200 --> 00:22:31,440
to connect to and this kind of

00:22:28,960 --> 00:22:33,120
this kind of information this kind of

00:22:31,440 --> 00:22:36,799
meter information

00:22:33,120 --> 00:22:39,600
is then sent via a post request

00:22:36,799 --> 00:22:40,080
to the certain instance where the

00:22:39,600 --> 00:22:43,440
connect

00:22:40,080 --> 00:22:45,840
worker is running and then a new

00:22:43,440 --> 00:22:47,200
adapter thread is instantiated here for

00:22:45,840 --> 00:22:50,480
mqtt

00:22:47,200 --> 00:22:53,039
and within the payload of this

00:22:50,480 --> 00:22:54,360
post request we have this kind of

00:22:53,039 --> 00:22:55,679
information about the user

00:22:54,360 --> 00:22:59,440
configurations

00:22:55,679 --> 00:23:01,919
in order to configure the mpdt adapter

00:22:59,440 --> 00:23:03,440
and what we call the event grounding

00:23:01,919 --> 00:23:05,200
which are information about the

00:23:03,440 --> 00:23:07,280
transport layer such as what kind of

00:23:05,200 --> 00:23:08,799
protocol we are using internally here

00:23:07,280 --> 00:23:11,120
kafka

00:23:08,799 --> 00:23:11,919
what kind of format are we using for the

00:23:11,120 --> 00:23:14,320
events

00:23:11,919 --> 00:23:16,799
where kafka can be reached and what kind

00:23:14,320 --> 00:23:20,240
of output topic should be used

00:23:16,799 --> 00:23:22,559
because then in the next step when we

00:23:20,240 --> 00:23:23,280
connect it to let's say the numerical

00:23:22,559 --> 00:23:25,039
filter

00:23:23,280 --> 00:23:26,640
we also get this kind of description

00:23:25,039 --> 00:23:29,960
layer around this

00:23:26,640 --> 00:23:32,400
processing function so here we got user

00:23:29,960 --> 00:23:34,640
configurations

00:23:32,400 --> 00:23:37,200
such as the threshold what kind of

00:23:34,640 --> 00:23:40,400
operator so

00:23:37,200 --> 00:23:42,480
crater then or a property such as the

00:23:40,400 --> 00:23:45,760
event field we want to

00:23:42,480 --> 00:23:48,320
filter selected by a user

00:23:45,760 --> 00:23:49,120
we model the certain output strategies

00:23:48,320 --> 00:23:51,039
what is kind

00:23:49,120 --> 00:23:52,960
what it produced by this numerical

00:23:51,039 --> 00:23:54,880
filter instance afterwards

00:23:52,960 --> 00:23:56,080
so here let's say we want to keep the

00:23:54,880 --> 00:24:00,320
input schema

00:23:56,080 --> 00:24:03,039
and don't append it or just customize it

00:24:00,320 --> 00:24:04,960
then we got stream requirements so each

00:24:03,039 --> 00:24:06,880
of the processing functions do have

00:24:04,960 --> 00:24:08,720
stream requirements so here

00:24:06,880 --> 00:24:10,320
in order for the numerical filter to

00:24:08,720 --> 00:24:13,520
work we need at least

00:24:10,320 --> 00:24:14,799
one field that has a numeric numeric

00:24:13,520 --> 00:24:17,679
value in it

00:24:14,799 --> 00:24:18,240
and of course we get information about

00:24:17,679 --> 00:24:22,320
our

00:24:18,240 --> 00:24:24,640
transport layer such as what kind of

00:24:22,320 --> 00:24:25,440
formats and protocols are supported here

00:24:24,640 --> 00:24:28,000
for example

00:24:25,440 --> 00:24:30,480
json or priv to support it as well as

00:24:28,000 --> 00:24:32,720
kafka or mpdg supported by that instance

00:24:30,480 --> 00:24:36,080
as well

00:24:32,720 --> 00:24:38,960
and here same again we get

00:24:36,080 --> 00:24:41,279
a post request with a certain payload to

00:24:38,960 --> 00:24:43,520
that kind of

00:24:41,279 --> 00:24:45,120
data processor that is hosting the

00:24:43,520 --> 00:24:48,640
numerical filter

00:24:45,120 --> 00:24:50,640
function and we send

00:24:48,640 --> 00:24:51,919
the provided user conflicts as well as

00:24:50,640 --> 00:24:53,039
the information about the event

00:24:51,919 --> 00:24:57,200
grounding

00:24:53,039 --> 00:24:57,200
and the input and output topics

00:24:57,440 --> 00:25:01,840
so lastly for the syncs here the

00:24:59,919 --> 00:25:05,200
dashboard and data lake sync

00:25:01,840 --> 00:25:06,559
it happens exactly the same so we send

00:25:05,200 --> 00:25:09,840
the post request

00:25:06,559 --> 00:25:13,279
request with a payload and here we see

00:25:09,840 --> 00:25:16,880
that for the runtime we're using

00:25:13,279 --> 00:25:22,000
we start just two individual

00:25:16,880 --> 00:25:22,000
threads for each of the provided things

00:25:22,960 --> 00:25:27,679
so now one can ask why do we need such a

00:25:25,840 --> 00:25:30,480
description layer

00:25:27,679 --> 00:25:31,520
we use it for for user guidance because

00:25:30,480 --> 00:25:34,240
we're dealing with the

00:25:31,520 --> 00:25:35,919
domain experts and and non-technical

00:25:34,240 --> 00:25:38,400
people and we want to

00:25:35,919 --> 00:25:39,840
make it as easy as convenience as as

00:25:38,400 --> 00:25:42,240
possible

00:25:39,840 --> 00:25:44,000
so we use that kind of information to

00:25:42,240 --> 00:25:45,919
support the user throughout the pipeline

00:25:44,000 --> 00:25:49,200
modeling process

00:25:45,919 --> 00:25:52,159
we want to prevent the user from

00:25:49,200 --> 00:25:53,600
not connecting non-compatible elements

00:25:52,159 --> 00:25:55,520
so when talking about the stream

00:25:53,600 --> 00:25:57,840
requirements for example

00:25:55,520 --> 00:26:01,200
our numerical filter has a requirement

00:25:57,840 --> 00:26:04,320
that it needs at least one numerical

00:26:01,200 --> 00:26:08,159
field in in the input event stream

00:26:04,320 --> 00:26:10,720
and so if we would not have this kind of

00:26:08,159 --> 00:26:13,200
compatibility check we would run in

00:26:10,720 --> 00:26:16,080
certain situations that

00:26:13,200 --> 00:26:17,360
would lead to invalid connections that

00:26:16,080 --> 00:26:20,000
we want to fetch

00:26:17,360 --> 00:26:21,600
eagerly and provide the user with direct

00:26:20,000 --> 00:26:23,039
feedback so we can see here in the

00:26:21,600 --> 00:26:26,240
screenshot

00:26:23,039 --> 00:26:29,840
a flow rate input data stream and

00:26:26,240 --> 00:26:31,919
a boolean processor that

00:26:29,840 --> 00:26:34,000
that counts every change from true to

00:26:31,919 --> 00:26:36,159
false and and now the flow rate data

00:26:34,000 --> 00:26:38,480
stream does not have does not provide

00:26:36,159 --> 00:26:39,919
a boolean event field so only numeric

00:26:38,480 --> 00:26:42,159
fields are provided

00:26:39,919 --> 00:26:43,600
and thus when the user wants to connect

00:26:42,159 --> 00:26:46,480
these kind of elements

00:26:43,600 --> 00:26:47,600
um he or she is provided with direct

00:26:46,480 --> 00:26:50,799
feedback

00:26:47,600 --> 00:26:54,640
and that's that's why we use that kind

00:26:50,799 --> 00:26:54,640
of description layer in the first place

00:26:54,799 --> 00:27:02,159
now coming into extending stream pipes

00:26:59,360 --> 00:27:02,880
so as mentioned we got various runtime

00:27:02,159 --> 00:27:05,279
options so

00:27:02,880 --> 00:27:08,000
you can run standalone we basically

00:27:05,279 --> 00:27:11,279
started out from using plain java

00:27:08,000 --> 00:27:12,159
processors we also got run times for

00:27:11,279 --> 00:27:14,720
city

00:27:12,159 --> 00:27:16,559
and now you can you can follow the

00:27:14,720 --> 00:27:18,240
development of our new

00:27:16,559 --> 00:27:19,679
runtime option that we're currently

00:27:18,240 --> 00:27:22,320
working on with the

00:27:19,679 --> 00:27:24,080
heightened wrapper to to focus more on

00:27:22,320 --> 00:27:26,799
the data science community

00:27:24,080 --> 00:27:28,559
but we also got wrapper options for

00:27:26,799 --> 00:27:32,840
distributed

00:27:28,559 --> 00:27:34,960
runtimes such as flingspark or kafka

00:27:32,840 --> 00:27:38,399
streams

00:27:34,960 --> 00:27:38,880
now coming to the demo itself um so we

00:27:38,399 --> 00:27:41,360
got a

00:27:38,880 --> 00:27:43,919
my creator data processor which is

00:27:41,360 --> 00:27:46,240
really simplified data processor

00:27:43,919 --> 00:27:49,039
that once connected it does not have

00:27:46,240 --> 00:27:51,120
really um

00:27:49,039 --> 00:27:52,399
requirement on the input data stream so

00:27:51,120 --> 00:27:54,399
it does

00:27:52,399 --> 00:27:56,720
the requirements is any property which

00:27:54,399 --> 00:27:59,760
means it does not have any

00:27:56,720 --> 00:28:03,440
it supports json format and

00:27:59,760 --> 00:28:06,960
kafka as the transport layer and

00:28:03,440 --> 00:28:10,080
the user is required to put in

00:28:06,960 --> 00:28:11,360
a creating text such that it is rendered

00:28:10,080 --> 00:28:14,799
in the ui

00:28:11,360 --> 00:28:17,360
on this side you can see it and

00:28:14,799 --> 00:28:18,960
as an output strategy we want to append

00:28:17,360 --> 00:28:22,559
this kind of reading message

00:28:18,960 --> 00:28:25,279
to the um already existing

00:28:22,559 --> 00:28:25,279
event stream

00:28:26,000 --> 00:28:30,000
and the application logic is just

00:28:29,200 --> 00:28:33,120
written in a few

00:28:30,000 --> 00:28:33,760
lines of code so we see here that the my

00:28:33,120 --> 00:28:36,559
creature

00:28:33,760 --> 00:28:38,159
implements uh implements a certain

00:28:36,559 --> 00:28:41,039
interface

00:28:38,159 --> 00:28:42,559
which is consisting of three methods the

00:28:41,039 --> 00:28:44,240
on invocation which

00:28:42,559 --> 00:28:47,360
is used to extract these kind of

00:28:44,240 --> 00:28:49,600
parameters such as secreting itself

00:28:47,360 --> 00:28:51,520
then we got the on event which is

00:28:49,600 --> 00:28:53,679
triggered basically on every

00:28:51,520 --> 00:28:54,559
event that is received by the transport

00:28:53,679 --> 00:28:57,600
layer

00:28:54,559 --> 00:28:59,840
and here we just add

00:28:57,600 --> 00:29:01,520
the the new field with the greeting that

00:28:59,840 --> 00:29:04,559
is received by

00:29:01,520 --> 00:29:07,360
the user and we got the on detach

00:29:04,559 --> 00:29:08,480
which we don't really use right here so

00:29:07,360 --> 00:29:10,720
this could

00:29:08,480 --> 00:29:11,600
be used in cases where for example you

00:29:10,720 --> 00:29:13,440
use

00:29:11,600 --> 00:29:15,120
a database in the background and you

00:29:13,440 --> 00:29:16,320
establish a connection on the on

00:29:15,120 --> 00:29:18,880
invocation

00:29:16,320 --> 00:29:20,480
and you can stop this kind of connection

00:29:18,880 --> 00:29:24,480
and clean everything up here

00:29:20,480 --> 00:29:26,960
on detach so everything's

00:29:24,480 --> 00:29:28,480
on github the video is going to be on

00:29:26,960 --> 00:29:32,159
youtube afterwards so

00:29:28,480 --> 00:29:35,910
no worries about the links and

00:29:32,159 --> 00:29:37,600
with that i'm going to show you the sdk

00:29:35,910 --> 00:29:40,080
[Music]

00:29:37,600 --> 00:29:42,159
so i'm not going to go through the code

00:29:40,080 --> 00:29:43,760
it's documented it's basically just the

00:29:42,159 --> 00:29:48,799
cloned repository here

00:29:43,760 --> 00:29:52,320
already got my creator processor running

00:29:48,799 --> 00:29:54,559
so it's running here in intellij and

00:29:52,320 --> 00:29:56,159
my streampipes instance locally is

00:29:54,559 --> 00:29:58,159
running in docker

00:29:56,159 --> 00:29:59,360
um so i'm going to switch over to the ui

00:29:58,159 --> 00:30:02,399
in order to

00:29:59,360 --> 00:30:04,960
to tell you um how

00:30:02,399 --> 00:30:06,960
to install and extend running a

00:30:04,960 --> 00:30:10,000
streampipes cluster

00:30:06,960 --> 00:30:10,799
so let's go back to the home screen and

00:30:10,000 --> 00:30:12,480
let's say

00:30:10,799 --> 00:30:14,240
we're just currently working on this

00:30:12,480 --> 00:30:16,720
kind of extension

00:30:14,240 --> 00:30:17,600
the the my creature extension and for

00:30:16,720 --> 00:30:21,440
that um

00:30:17,600 --> 00:30:23,840
we started a first instance and

00:30:21,440 --> 00:30:25,600
we want to install it in into the into

00:30:23,840 --> 00:30:27,440
the system so therefore we can switch

00:30:25,600 --> 00:30:28,799
over to the install pipeline elements

00:30:27,440 --> 00:30:30,799
here

00:30:28,799 --> 00:30:32,799
and what we're doing here is fetching

00:30:30,799 --> 00:30:33,679
all the available pipeline elements that

00:30:32,799 --> 00:30:37,520
are currently

00:30:33,679 --> 00:30:40,720
running or not yet install

00:30:37,520 --> 00:30:44,159
so we we can

00:30:40,720 --> 00:30:51,600
go here and switch over to

00:30:44,159 --> 00:30:54,799
the available ones and reload the items

00:30:51,600 --> 00:30:56,320
and we're querying right now the core of

00:30:54,799 --> 00:30:59,039
stream pipes

00:30:56,320 --> 00:31:01,679
which is reaching out to to all the the

00:30:59,039 --> 00:31:04,960
endpoints

00:31:01,679 --> 00:31:06,240
so we see here the my creator with its

00:31:04,960 --> 00:31:10,399
description

00:31:06,240 --> 00:31:10,399
and we can simply install it

00:31:11,440 --> 00:31:19,840
takes a couple of seconds for the

00:31:14,159 --> 00:31:19,840
description to load

00:31:25,360 --> 00:31:31,679
so okay so five more minutes i got the

00:31:29,360 --> 00:31:34,399
note

00:31:31,679 --> 00:31:34,399
gonna be okay

00:31:41,600 --> 00:31:47,840
my macbook is really running hot right

00:31:44,840 --> 00:31:47,840
now

00:31:56,880 --> 00:32:15,840
maybe we need to reload it first

00:32:00,159 --> 00:32:15,840
let's try it again

00:32:20,399 --> 00:32:23,519
so seems like the demo gods are not with

00:32:23,200 --> 00:32:26,559
me

00:32:23,519 --> 00:32:29,679
this time so um

00:32:26,559 --> 00:32:33,440
i gonna switch over to

00:32:29,679 --> 00:32:36,320
the um github repository

00:32:33,440 --> 00:32:37,600
that i was mentioning um where i got the

00:32:36,320 --> 00:32:40,720
description and a

00:32:37,600 --> 00:32:41,279
nice gif um that you can follow in order

00:32:40,720 --> 00:32:44,320
to

00:32:41,279 --> 00:32:48,159
get this um setup working

00:32:44,320 --> 00:32:51,039
and um just stay here for a little

00:32:48,159 --> 00:32:51,760
uh because here we see um like the last

00:32:51,039 --> 00:32:55,279
couple of

00:32:51,760 --> 00:32:58,960
um of points of this kind of demo where

00:32:55,279 --> 00:33:00,000
attach this or add a new visualization

00:32:58,960 --> 00:33:02,320
and see here

00:33:00,000 --> 00:33:04,000
the creating message i'm going to switch

00:33:02,320 --> 00:33:07,440
back timewise

00:33:04,000 --> 00:33:08,640
to the presentation talking about the

00:33:07,440 --> 00:33:10,880
roadmap

00:33:08,640 --> 00:33:13,120
we got bunch of new features coming up

00:33:10,880 --> 00:33:16,080
so quickly going over it

00:33:13,120 --> 00:33:16,960
um python runtime wrapper was already

00:33:16,080 --> 00:33:19,600
mentioned

00:33:16,960 --> 00:33:20,399
we're working on edge deployments like

00:33:19,600 --> 00:33:22,799
bring in

00:33:20,399 --> 00:33:25,440
more advanced deployment options options

00:33:22,799 --> 00:33:28,640
for various pipeline elements to

00:33:25,440 --> 00:33:29,760
the users we're working on not only

00:33:28,640 --> 00:33:32,080
being able to define

00:33:29,760 --> 00:33:33,679
pipelines by the by the means of the

00:33:32,080 --> 00:33:36,960
pipeline editor but creating

00:33:33,679 --> 00:33:39,279
them from code itself and

00:33:36,960 --> 00:33:40,640
some other features about failure

00:33:39,279 --> 00:33:44,399
handling

00:33:40,640 --> 00:33:44,399
resiliency and monitoring

00:33:44,559 --> 00:33:49,039
if you want to get involved check out

00:33:46,799 --> 00:33:52,080
our issues on gera

00:33:49,039 --> 00:33:54,880
see our sips or stream pipes improvement

00:33:52,080 --> 00:33:56,399
proposals subscribe to the mailing list

00:33:54,880 --> 00:33:59,120
and slack channel

00:33:56,399 --> 00:34:03,200
to get involved into the discussions and

00:33:59,120 --> 00:34:03,200
just help us grow the community further

00:34:04,159 --> 00:34:07,600
just a small note don't miss the next

00:34:06,080 --> 00:34:11,040
talk because it's going to be a

00:34:07,600 --> 00:34:13,919
really awesome it's it's a talk by chris

00:34:11,040 --> 00:34:16,320
and toddy and philip i'm talking about

00:34:13,919 --> 00:34:18,480
analyzing the industrial iot data with

00:34:16,320 --> 00:34:22,480
plc for x and stream pipes

00:34:18,480 --> 00:34:24,320
so just um stay there in the iot track

00:34:22,480 --> 00:34:28,079
and don't miss out on it

00:34:24,320 --> 00:34:31,679
and with that um thank you very much

00:34:28,079 --> 00:34:35,359
and i'm gonna be in slack and

00:34:31,679 --> 00:34:38,480
um yeah just reach out to me um

00:34:35,359 --> 00:34:40,240
when whenever you feel like i think we

00:34:38,480 --> 00:34:43,520
don't have time for

00:34:40,240 --> 00:34:45,839
too many questions because

00:34:43,520 --> 00:34:47,440
maybe it took me so long but i'm gonna

00:34:45,839 --> 00:34:50,320
go and stay here in the channel

00:34:47,440 --> 00:34:51,119
gonna try to go through the questions

00:34:50,320 --> 00:34:54,800
here

00:34:51,119 --> 00:34:57,119
and if not just um just

00:34:54,800 --> 00:34:59,040
go out and reach out to me and on slack

00:34:57,119 --> 00:34:59,920
and we can we can chat about stream

00:34:59,040 --> 00:35:03,119
pipes

00:34:59,920 --> 00:35:03,920
and if you liked it please leave a star

00:35:03,119 --> 00:35:17,839
in github

00:35:03,920 --> 00:35:17,839

YouTube URL: https://www.youtube.com/watch?v=pfLfVv5EyLs


