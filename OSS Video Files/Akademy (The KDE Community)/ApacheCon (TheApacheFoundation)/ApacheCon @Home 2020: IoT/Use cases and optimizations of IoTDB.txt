Title: Use cases and optimizations of IoTDB
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020: IoT
Description: 
	Use cases and optimizations of IoTDB
Jialin Qiao

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache IoTDB is a high performance database for time-series data management on the edge and cloud for Internet of Things. This talk will introduce some use cases of IoTDB, including Meteorological station data management, Subway data management and power plants monitoring applications. The read/write performance optimization and database tunning are also involved.

Ph.D student of school of software, Tsinghua University. Expert in IoTDB's storage engine, query engine and application implementation on IoTDB.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:26,320 --> 00:00:34,239
okay let's start

00:00:31,199 --> 00:00:36,880
hello everyone my name is jalin chao

00:00:34,239 --> 00:00:38,079
today i will introduce the use cases and

00:00:36,880 --> 00:00:41,760
optimizations of

00:00:38,079 --> 00:00:44,879
ltdb i will give this talk

00:00:41,760 --> 00:00:47,360
in five parts first let me introduce

00:00:44,879 --> 00:00:47,360
myself

00:00:47,520 --> 00:00:51,440
i'm a phd student in chiang hai

00:00:50,000 --> 00:00:54,800
university

00:00:51,440 --> 00:00:58,480
and i take part in ltdb project since

00:00:54,800 --> 00:01:01,359
2016 and became an initial commuter and

00:00:58,480 --> 00:01:04,239
pmc of apache ltdb

00:01:01,359 --> 00:01:04,879
in the community i participated in the

00:01:04,239 --> 00:01:08,159
function

00:01:04,879 --> 00:01:09,920
designing review prs and when users

00:01:08,159 --> 00:01:12,960
report some problem

00:01:09,920 --> 00:01:15,840
i will locate and fix box besides

00:01:12,960 --> 00:01:19,520
currently i investigate the improvements

00:01:15,840 --> 00:01:22,000
of the car engine of iot db

00:01:19,520 --> 00:01:23,439
before the main content i will give a

00:01:22,000 --> 00:01:26,799
brief introduction about

00:01:23,439 --> 00:01:29,759
iot db iot db

00:01:26,799 --> 00:01:30,640
is an iot native database with high

00:01:29,759 --> 00:01:33,680
performance

00:01:30,640 --> 00:01:34,079
for data management and analysis on the

00:01:33,680 --> 00:01:37,200
age

00:01:34,079 --> 00:01:40,400
under the cloud ltdb was born in

00:01:37,200 --> 00:01:44,479
2015 in qinghai university and

00:01:40,400 --> 00:01:47,520
entered the apache incubator in 2018.

00:01:44,479 --> 00:01:50,799
recently a big event of iot db is

00:01:47,520 --> 00:01:52,159
graduation and we become a top-level

00:01:50,799 --> 00:01:54,880
project

00:01:52,159 --> 00:01:56,159
with our mentor's help and effort of

00:01:54,880 --> 00:02:00,119
contributors

00:01:56,159 --> 00:02:04,240
we released nine versions and attracted

00:02:00,119 --> 00:02:06,399
78 contributors and we integrate well

00:02:04,240 --> 00:02:10,080
with other apache projects

00:02:06,399 --> 00:02:12,479
such as prc4x flink hadoop and

00:02:10,080 --> 00:02:12,479
spark

00:02:14,080 --> 00:02:22,000
ltdb is for iot data management where

00:02:17,520 --> 00:02:24,400
iot data is mainly the time series data

00:02:22,000 --> 00:02:27,200
the lifecycle of iot data mainly

00:02:24,400 --> 00:02:30,400
contains file parts

00:02:27,200 --> 00:02:34,560
data collection pre-processing storage

00:02:30,400 --> 00:02:34,560
data analysis and applications

00:02:35,280 --> 00:02:41,519
first data generated from eight

00:02:38,400 --> 00:02:42,800
devices could be collected through plc

00:02:41,519 --> 00:02:45,519
forex

00:02:42,800 --> 00:02:46,080
then it usually be sent to a message

00:02:45,519 --> 00:02:48,480
queue

00:02:46,080 --> 00:02:51,599
or streaming processing framework such

00:02:48,480 --> 00:02:54,640
as browser kafka and flink

00:02:51,599 --> 00:02:57,920
after the pre-processing the data flows

00:02:54,640 --> 00:03:01,200
into the storage layer where ltdb

00:02:57,920 --> 00:03:03,519
locates ltdb could support

00:03:01,200 --> 00:03:05,760
efficient data storage and real-time

00:03:03,519 --> 00:03:08,800
query in low latency

00:03:05,760 --> 00:03:11,680
usually inside a system architecture

00:03:08,800 --> 00:03:12,319
the time series data is stored in a time

00:03:11,680 --> 00:03:15,840
series

00:03:12,319 --> 00:03:16,640
database for real-time query at the same

00:03:15,840 --> 00:03:19,680
time

00:03:16,640 --> 00:03:20,560
data is usually stored in another data

00:03:19,680 --> 00:03:24,080
warehouse

00:03:20,560 --> 00:03:27,120
for batch processing and analysis

00:03:24,080 --> 00:03:28,959
in the analytics layer by integrating

00:03:27,120 --> 00:03:31,519
with spark and how

00:03:28,959 --> 00:03:32,159
users could analyze the data stored in

00:03:31,519 --> 00:03:35,680
ltb

00:03:32,159 --> 00:03:38,239
directly in this way iot db

00:03:35,680 --> 00:03:40,080
could be seen as a database and also a

00:03:38,239 --> 00:03:42,400
data warehouse

00:03:40,080 --> 00:03:44,000
you do not need to store data twice in

00:03:42,400 --> 00:03:48,239
two systems

00:03:44,000 --> 00:03:48,239
which reducing the system complexity

00:03:49,040 --> 00:03:55,840
the architecture of ltdb is as follows

00:03:52,959 --> 00:03:56,239
first time serious data is generated

00:03:55,840 --> 00:04:00,000
from

00:03:56,239 --> 00:04:02,319
edge devices or other data sources

00:04:00,000 --> 00:04:03,519
to support a lightweight time series

00:04:02,319 --> 00:04:06,879
data management

00:04:03,519 --> 00:04:08,400
in the age we first designed a columnar

00:04:06,879 --> 00:04:11,840
file format

00:04:08,400 --> 00:04:13,280
called ts file ts file is to manage time

00:04:11,840 --> 00:04:16,400
series data

00:04:13,280 --> 00:04:19,840
and could be used independently at

00:04:16,400 --> 00:04:22,639
age just like pakistan

00:04:19,840 --> 00:04:23,040
based on the ts file we built the ltdb

00:04:22,639 --> 00:04:25,600
engine

00:04:23,040 --> 00:04:26,639
for flexible data management and the

00:04:25,600 --> 00:04:30,240
rich quality

00:04:26,639 --> 00:04:30,960
of time series users could access the

00:04:30,240 --> 00:04:34,479
iotdb

00:04:30,960 --> 00:04:35,280
engine through two apis the jdbc and the

00:04:34,479 --> 00:04:39,680
section

00:04:35,280 --> 00:04:42,800
apis on top of the iot db engine

00:04:39,680 --> 00:04:46,400
command line interface graphana adapter

00:04:42,800 --> 00:04:50,080
and some system tools are developed

00:04:46,400 --> 00:04:53,440
suppose we store the data of each device

00:04:50,080 --> 00:04:54,639
in each ltdb instance and we want to

00:04:53,440 --> 00:04:57,360
collect the

00:04:54,639 --> 00:04:59,199
data of all of them into one ltdb in the

00:04:57,360 --> 00:05:01,440
cloud

00:04:59,199 --> 00:05:04,720
then we could use the sync tool to

00:05:01,440 --> 00:05:07,759
transfer the data files

00:05:04,720 --> 00:05:11,840
finally both the ts file and iot db

00:05:07,759 --> 00:05:11,840
could be accessed by spark

00:05:13,520 --> 00:05:18,080
ltdb adopts a three-structured schema

00:05:16,639 --> 00:05:21,199
management

00:05:18,080 --> 00:05:21,840
the path from root to temperature is an

00:05:21,199 --> 00:05:25,520
example

00:05:21,840 --> 00:05:29,120
of time series each node in this path

00:05:25,520 --> 00:05:30,320
is separated by the dot in this schema

00:05:29,120 --> 00:05:33,759
tree

00:05:30,320 --> 00:05:34,400
all time series start from root storage

00:05:33,759 --> 00:05:37,120
group

00:05:34,400 --> 00:05:38,160
is a concept that is a group of time

00:05:37,120 --> 00:05:40,000
series

00:05:38,160 --> 00:05:42,000
which lacks the database in the

00:05:40,000 --> 00:05:44,960
relational databases

00:05:42,000 --> 00:05:49,280
in each story group how is time series

00:05:44,960 --> 00:05:52,320
will start in the same data files

00:05:49,280 --> 00:05:54,560
the last level but one is device

00:05:52,320 --> 00:05:56,400
and the last level is the time series

00:05:54,560 --> 00:05:59,199
level

00:05:56,400 --> 00:06:00,319
the devices is inferred automatically

00:05:59,199 --> 00:06:03,919
when you create

00:06:00,319 --> 00:06:06,960
a time series although this tree has

00:06:03,919 --> 00:06:08,080
five layers actually each time series

00:06:06,960 --> 00:06:12,000
could have different

00:06:08,080 --> 00:06:15,039
layers which is adaptive for the complex

00:06:12,000 --> 00:06:15,039
iot scenario

00:06:15,759 --> 00:06:23,680
the storage engine of rtdb is the tlrsm

00:06:19,440 --> 00:06:26,880
engine which is similar to the lsm3

00:06:23,680 --> 00:06:27,680
we leverage the concept of time to

00:06:26,880 --> 00:06:30,720
accelerate

00:06:27,680 --> 00:06:33,360
the insertion and query first

00:06:30,720 --> 00:06:34,960
we store the out of order data and other

00:06:33,360 --> 00:06:37,840
data separately

00:06:34,960 --> 00:06:39,440
to accelerate the queries in the time

00:06:37,840 --> 00:06:42,800
series database

00:06:39,440 --> 00:06:45,759
time the partition is an important task

00:06:42,800 --> 00:06:46,400
in ltdb we partition and index the data

00:06:45,759 --> 00:06:49,520
by the time

00:06:46,400 --> 00:06:51,440
interval we also use time as a main

00:06:49,520 --> 00:06:54,960
index

00:06:51,440 --> 00:06:58,560
in the tls engine

00:06:54,960 --> 00:06:58,800
data is appended into an out of order or

00:06:58,560 --> 00:07:01,039
an

00:06:58,800 --> 00:07:03,199
ordered map table according to its

00:07:01,039 --> 00:07:05,919
timestamp

00:07:03,199 --> 00:07:06,240
when the mem table reaches a threshold

00:07:05,919 --> 00:07:09,680
it

00:07:06,240 --> 00:07:12,479
is flashed to the disk there are three

00:07:09,680 --> 00:07:15,520
compaction procedures in ltdb

00:07:12,479 --> 00:07:16,400
the first is compacting the most recent

00:07:15,520 --> 00:07:19,440
data into

00:07:16,400 --> 00:07:23,360
larger blocks to accelerate the recent

00:07:19,440 --> 00:07:25,520
data queries because of hot compaction

00:07:23,360 --> 00:07:28,479
and the second compaction is to

00:07:25,520 --> 00:07:30,560
eliminate the out of order data

00:07:28,479 --> 00:07:34,080
the third compaction is enlarging the

00:07:30,560 --> 00:07:34,080
historical data blocks

00:07:35,919 --> 00:07:44,720
the following is some typical

00:07:37,599 --> 00:07:47,280
applications that are using ltdb

00:07:44,720 --> 00:07:48,160
in shanghai china the subway monitoring

00:07:47,280 --> 00:07:51,840
application

00:07:48,160 --> 00:07:54,879
uses ltdb to store the sensor data

00:07:51,840 --> 00:07:55,360
the character of this application is is

00:07:54,879 --> 00:07:59,360
high

00:07:55,360 --> 00:08:00,319
red throughput there are 1 million time

00:07:59,360 --> 00:08:03,840
series

00:08:00,319 --> 00:08:09,199
which come from 300 subway trains

00:08:03,840 --> 00:08:13,520
and each phone has 3 and 200 sensors

00:08:09,199 --> 00:08:16,800
the data collection frequency is 5 hertz

00:08:13,520 --> 00:08:20,479
each record contains all sensors of one

00:08:16,800 --> 00:08:24,319
device therefore total

00:08:20,479 --> 00:08:27,520
400 billion points are generated per day

00:08:24,319 --> 00:08:29,759
which occupies one terabyte on this per

00:08:27,520 --> 00:08:29,759
month

00:08:31,440 --> 00:08:36,159
the next application is the power plant

00:08:34,080 --> 00:08:38,800
monitoring

00:08:36,159 --> 00:08:39,200
this is another interesting use case in

00:08:38,800 --> 00:08:42,320
this

00:08:39,200 --> 00:08:43,039
application each power plant deploys an

00:08:42,320 --> 00:08:46,399
iot db

00:08:43,039 --> 00:08:50,000
instance the whole power plant is seen

00:08:46,399 --> 00:08:53,040
as a device and this device contains

00:08:50,000 --> 00:08:57,120
three hundred thousand time series

00:08:53,040 --> 00:08:59,440
if we treat each device as a table

00:08:57,120 --> 00:09:00,160
in the relational database then this

00:08:59,440 --> 00:09:04,080
table has

00:09:00,160 --> 00:09:05,440
300 000 columns which is really a valid

00:09:04,080 --> 00:09:08,959
table

00:09:05,440 --> 00:09:12,720
the data is collected every five

00:09:08,959 --> 00:09:15,760
seconds there are many query types

00:09:12,720 --> 00:09:18,720
the first query type is the latest

00:09:15,760 --> 00:09:19,200
point of the given time series the

00:09:18,720 --> 00:09:22,240
second

00:09:19,200 --> 00:09:25,920
query type is getting the raw data of

00:09:22,240 --> 00:09:29,200
our series in the last day

00:09:25,920 --> 00:09:30,160
and the third query samples data of one

00:09:29,200 --> 00:09:34,240
series

00:09:30,160 --> 00:09:34,240
every five minutes during our day

00:09:35,279 --> 00:09:42,480
a tobacco company in china is building

00:09:38,640 --> 00:09:45,120
the intelligent cigarette factory

00:09:42,480 --> 00:09:46,240
the whole systems architecture has three

00:09:45,120 --> 00:09:49,680
layers

00:09:46,240 --> 00:09:53,120
the top level is a cloud platform

00:09:49,680 --> 00:09:55,839
of the company which summarizes the data

00:09:53,120 --> 00:09:58,720
of all its factories

00:09:55,839 --> 00:10:00,080
the factory is a middle layer each

00:09:58,720 --> 00:10:04,079
factory has multiple

00:10:00,080 --> 00:10:06,720
shops for example the silk making shop

00:10:04,079 --> 00:10:08,480
the wrap around shop and the material

00:10:06,720 --> 00:10:11,519
flow shop

00:10:08,480 --> 00:10:14,480
each factory and shop deploys an iot db

00:10:11,519 --> 00:10:16,160
instance and the data is generated from

00:10:14,480 --> 00:10:18,480
the shop

00:10:16,160 --> 00:10:19,760
so the question is how to synchronize

00:10:18,480 --> 00:10:23,360
data between different

00:10:19,760 --> 00:10:25,279
layers actually the solution depends on

00:10:23,360 --> 00:10:28,240
the business requirements

00:10:25,279 --> 00:10:29,760
in this case both the shop player and

00:10:28,240 --> 00:10:32,880
the factory layer

00:10:29,760 --> 00:10:33,920
want to have real-time data which is to

00:10:32,880 --> 00:10:36,560
say that the

00:10:33,920 --> 00:10:37,839
investment latency should be within a

00:10:36,560 --> 00:10:41,120
second

00:10:37,839 --> 00:10:44,399
so double rat is needed that is

00:10:41,120 --> 00:10:45,440
once the data is collected it is written

00:10:44,399 --> 00:10:48,320
to the ltdb

00:10:45,440 --> 00:10:51,440
instance in the shop and the factory

00:10:48,320 --> 00:10:55,279
layer at the same time

00:10:51,440 --> 00:10:58,880
the iotdb instance in the company level

00:10:55,279 --> 00:11:00,320
is used for big data analysis so query

00:10:58,880 --> 00:11:03,120
is mainly oriented

00:11:00,320 --> 00:11:05,440
to the historical data so the latency

00:11:03,120 --> 00:11:08,560
could be ours

00:11:05,440 --> 00:11:12,160
therefore the sync2 of iotdb

00:11:08,560 --> 00:11:14,800
could be used once a data file is closed

00:11:12,160 --> 00:11:15,760
in the factory it could be transferred

00:11:14,800 --> 00:11:18,959
to the ltdb

00:11:15,760 --> 00:11:23,279
in the company level currently data

00:11:18,959 --> 00:11:23,279
files are generated every 4 hours

00:11:24,079 --> 00:11:27,200
then i will introduce many interesting

00:11:26,399 --> 00:11:31,440
requirements

00:11:27,200 --> 00:11:33,680
and optimizations we did in these cases

00:11:31,440 --> 00:11:34,800
the optimizations include the schema

00:11:33,680 --> 00:11:37,600
management

00:11:34,800 --> 00:11:38,320
query types and performance lattice

00:11:37,600 --> 00:11:41,360
throughput

00:11:38,320 --> 00:11:41,360
and memory control

00:11:41,600 --> 00:11:44,880
when using ltdb in an application the

00:11:44,480 --> 00:11:48,240
first

00:11:44,880 --> 00:11:49,440
important thing is to set a proper style

00:11:48,240 --> 00:11:52,240
groups

00:11:49,440 --> 00:11:52,720
usually the number of survey group is

00:11:52,240 --> 00:11:57,360
about

00:11:52,720 --> 00:11:58,480
10 to 15 50 which is the number of cpu

00:11:57,360 --> 00:12:01,680
cards

00:11:58,480 --> 00:12:03,839
to get high parallelism

00:12:01,680 --> 00:12:05,279
because each stereo group is an

00:12:03,839 --> 00:12:09,360
independent storage

00:12:05,279 --> 00:12:11,760
engine that has one thread to serve rats

00:12:09,360 --> 00:12:13,200
there are usually two ways to choose the

00:12:11,760 --> 00:12:16,079
study group

00:12:13,200 --> 00:12:17,440
one is to choose a proper attributes of

00:12:16,079 --> 00:12:20,800
your time series

00:12:17,440 --> 00:12:24,000
that is that has a suitable cardinality

00:12:20,800 --> 00:12:27,120
for example suppose we have 30

00:12:24,000 --> 00:12:29,440
factories then we could set the factory

00:12:27,120 --> 00:12:31,839
as the story group level

00:12:29,440 --> 00:12:33,200
the other way is to manually partition

00:12:31,839 --> 00:12:36,800
your time series

00:12:33,200 --> 00:12:38,959
suppose you have 100 devices

00:12:36,800 --> 00:12:40,000
and you could partition them into 10

00:12:38,959 --> 00:12:44,240
serial groups

00:12:40,000 --> 00:12:47,040
by a hash function the number of device

00:12:44,240 --> 00:12:50,160
should be designed properly because a

00:12:47,040 --> 00:12:53,519
large number of devices will bring like

00:12:50,160 --> 00:12:57,279
memory overhead to ltdb currently

00:12:53,519 --> 00:13:00,320
the device number of one ltdb instance

00:12:57,279 --> 00:13:03,680
should be should not exist 100

00:13:00,320 --> 00:13:05,680
000 or so therefore

00:13:03,680 --> 00:13:08,160
please ensure that you know how many

00:13:05,680 --> 00:13:10,800
devices are in your system

00:13:08,160 --> 00:13:12,880
because device is auto set when you

00:13:10,800 --> 00:13:15,519
create a time series

00:13:12,880 --> 00:13:16,399
if you do not have if you do not name

00:13:15,519 --> 00:13:19,760
the series

00:13:16,399 --> 00:13:22,240
properly the real number of devices

00:13:19,760 --> 00:13:23,120
may much larger than your thought for

00:13:22,240 --> 00:13:26,079
example

00:13:23,120 --> 00:13:26,720
when you create a time series from root

00:13:26,079 --> 00:13:29,440
dot

00:13:26,720 --> 00:13:30,880
sg to the device dot environment then

00:13:29,440 --> 00:13:34,399
the device will be set

00:13:30,880 --> 00:13:37,600
as root from to device

00:13:34,399 --> 00:13:38,160
if you open a minionless suffix behind

00:13:37,600 --> 00:13:41,440
the time

00:13:38,160 --> 00:13:44,320
series for example a suffix that named

00:13:41,440 --> 00:13:49,360
value then the measurement will be set

00:13:44,320 --> 00:13:53,040
as a device

00:13:49,360 --> 00:13:55,120
in most databases especially the

00:13:53,040 --> 00:13:55,839
relational databases the schema should

00:13:55,120 --> 00:13:59,360
be set

00:13:55,839 --> 00:14:00,480
before the data is ingested but in many

00:13:59,360 --> 00:14:03,680
iot cases

00:14:00,480 --> 00:14:06,160
the schema is defined inside the devices

00:14:03,680 --> 00:14:08,079
and we do not know the precise schema

00:14:06,160 --> 00:14:09,120
such as the data type of each time

00:14:08,079 --> 00:14:12,000
series

00:14:09,120 --> 00:14:13,279
until the data is collected to solve

00:14:12,000 --> 00:14:16,399
this problem

00:14:13,279 --> 00:14:18,639
we introduce we introduce the reducer

00:14:16,399 --> 00:14:21,199
schema automatically

00:14:18,639 --> 00:14:24,079
when do undo insertion and inverse

00:14:21,199 --> 00:14:26,639
schema from the inserted values

00:14:24,079 --> 00:14:29,600
then we got one more question what

00:14:26,639 --> 00:14:32,240
schema is expected from users

00:14:29,600 --> 00:14:32,959
we first classified the values by its

00:14:32,240 --> 00:14:35,360
formatting

00:14:32,959 --> 00:14:38,160
such as the boolean stream integral

00:14:35,360 --> 00:14:40,720
screen flow stream and the text stream

00:14:38,160 --> 00:14:43,040
then we allow users to configure how to

00:14:40,720 --> 00:14:45,680
infer data types of each format

00:14:43,040 --> 00:14:47,040
for example we could create the boolean

00:14:45,680 --> 00:14:50,480
stream

00:14:47,040 --> 00:14:53,199
as bully or text and create the

00:14:50,480 --> 00:14:56,959
integral stream as integer or float our

00:14:53,199 --> 00:14:56,959
double data type in ltdb

00:14:57,760 --> 00:15:04,000
in the previous version when we read one

00:15:01,360 --> 00:15:04,959
time series in ts file we need to read

00:15:04,000 --> 00:15:08,800
the metadata

00:15:04,959 --> 00:15:11,360
of our time series of our device

00:15:08,800 --> 00:15:12,399
recall that in the powerplant monitoring

00:15:11,360 --> 00:15:17,040
application

00:15:12,399 --> 00:15:19,120
each device contains 300 000 time series

00:15:17,040 --> 00:15:20,880
this will bring much challenge to the

00:15:19,120 --> 00:15:23,680
query performance

00:15:20,880 --> 00:15:25,600
to solve this problem we build a tree

00:15:23,680 --> 00:15:28,480
structure of metadata to

00:15:25,600 --> 00:15:31,120
manage the mass time series meditating

00:15:28,480 --> 00:15:31,120
each file

00:15:31,519 --> 00:15:34,880
also in the powerplant monitoring

00:15:33,519 --> 00:15:39,040
application

00:15:34,880 --> 00:15:42,240
users want to get the latest point of

00:15:39,040 --> 00:15:44,720
50 000 time series

00:15:42,240 --> 00:15:45,440
here the latest point contains our

00:15:44,720 --> 00:15:48,720
timestamp

00:15:45,440 --> 00:15:51,920
and our value to support this scenario

00:15:48,720 --> 00:15:55,759
efficiently we define a special query

00:15:51,920 --> 00:15:58,720
called the last query for this scenario

00:15:55,759 --> 00:15:59,120
the result has three columns the first

00:15:58,720 --> 00:16:02,160
is

00:15:59,120 --> 00:16:05,920
time the second is time series path

00:16:02,160 --> 00:16:09,199
and the value to answer this

00:16:05,920 --> 00:16:09,519
query quickly we catch the latest value

00:16:09,199 --> 00:16:12,720
of

00:16:09,519 --> 00:16:15,759
each series when insertion

00:16:12,720 --> 00:16:19,120
so we could return the last point of our

00:16:15,759 --> 00:16:21,120
shift without within one second

00:16:19,120 --> 00:16:23,839
this is because we don't need to read

00:16:21,120 --> 00:16:23,839
the disk

00:16:24,320 --> 00:16:28,880
daily report is an important requirement

00:16:27,040 --> 00:16:31,839
in the manufacture

00:16:28,880 --> 00:16:34,800
to support this the group group by in

00:16:31,839 --> 00:16:38,399
time interval query is often used

00:16:34,800 --> 00:16:40,800
users first choose a query time range

00:16:38,399 --> 00:16:41,759
then split the whole range by an

00:16:40,800 --> 00:16:44,880
interval

00:16:41,759 --> 00:16:48,480
for example sublease the timeline from

00:16:44,880 --> 00:16:52,000
1 to 10 by an interval 2 to get

00:16:48,480 --> 00:16:54,959
5 intervals then users apply

00:16:52,000 --> 00:16:57,680
aggregation function average on each

00:16:54,959 --> 00:17:00,399
interval to get the final results

00:16:57,680 --> 00:17:01,440
however there is one scenario that is

00:17:00,399 --> 00:17:05,760
above semantic

00:17:01,440 --> 00:17:08,000
could not support for example

00:17:05,760 --> 00:17:08,799
users want to get the average

00:17:08,000 --> 00:17:11,919
temperature

00:17:08,799 --> 00:17:15,600
from 8 00 am to 8 pm of

00:17:11,919 --> 00:17:17,520
each day in the last month so biosecure

00:17:15,600 --> 00:17:21,360
could not support this

00:17:17,520 --> 00:17:24,400
because all intervals are contiguous

00:17:21,360 --> 00:17:25,760
to support this we extend the group by

00:17:24,400 --> 00:17:28,640
time interval

00:17:25,760 --> 00:17:30,160
to the group by sliding window with

00:17:28,640 --> 00:17:32,799
three parameters

00:17:30,160 --> 00:17:33,360
the first is the whole query range the

00:17:32,799 --> 00:17:36,160
second

00:17:33,360 --> 00:17:38,000
is the aggregation interval the third is

00:17:36,160 --> 00:17:40,880
a sliding step

00:17:38,000 --> 00:17:46,000
by this way each aggregation interval do

00:17:40,880 --> 00:17:49,200
not need to be adjacent

00:17:46,000 --> 00:17:51,679
in the process flow diagram

00:17:49,200 --> 00:17:53,280
each monitoring point is collected

00:17:51,679 --> 00:17:56,000
independently

00:17:53,280 --> 00:17:56,480
even they are intended to collect a

00:17:56,000 --> 00:17:59,600
fixed

00:17:56,480 --> 00:18:02,000
frequency the real time stamp

00:17:59,600 --> 00:18:02,880
library slightly and the values of

00:18:02,000 --> 00:18:08,320
different

00:18:02,880 --> 00:18:08,320
series cannot align by time perfectly

00:18:08,559 --> 00:18:14,880
this causes trouble to data analysis

00:18:11,600 --> 00:18:18,240
to solve this problem users

00:18:14,880 --> 00:18:21,440
usually use the time sample inquiry

00:18:18,240 --> 00:18:24,960
the query process is sample

00:18:21,440 --> 00:18:27,440
opponent at a fixed interval if the

00:18:24,960 --> 00:18:29,760
if there doesn't exist a point at as a

00:18:27,440 --> 00:18:33,679
sample timestamp

00:18:29,760 --> 00:18:37,039
then use the previous value to fill it

00:18:33,679 --> 00:18:38,960
but down sampling for multiple series

00:18:37,039 --> 00:18:40,400
the values could be aligned by time

00:18:38,960 --> 00:18:45,600
perfectly

00:18:40,400 --> 00:18:47,840
so how we support this function in ltdb

00:18:45,600 --> 00:18:49,679
the downside volume could be implemented

00:18:47,840 --> 00:18:54,000
by the group by time interval

00:18:49,679 --> 00:18:54,559
in ltdb after splitting the query time

00:18:54,000 --> 00:18:57,600
range

00:18:54,559 --> 00:18:58,880
into multiple interval we could apply an

00:18:57,600 --> 00:19:02,480
aggregation function

00:18:58,880 --> 00:19:04,240
called last value on each interval

00:19:02,480 --> 00:19:06,240
by this way we support the downside

00:19:04,240 --> 00:19:09,600
volume easily

00:19:06,240 --> 00:19:10,000
however in some cases there is no data

00:19:09,600 --> 00:19:12,640
point

00:19:10,000 --> 00:19:13,120
at the whole interval then we get a null

00:19:12,640 --> 00:19:16,240
value

00:19:13,120 --> 00:19:18,480
at this interval which cause trouble for

00:19:16,240 --> 00:19:20,480
the data analysis of the data

00:19:18,480 --> 00:19:23,600
realization

00:19:20,480 --> 00:19:27,120
to fill this null value

00:19:23,600 --> 00:19:29,919
the field class which uses the result

00:19:27,120 --> 00:19:31,600
of the nearby intervals to fill this

00:19:29,919 --> 00:19:33,679
interval

00:19:31,600 --> 00:19:38,799
currently we support the previous field

00:19:33,679 --> 00:19:41,120
for the down sampling

00:19:38,799 --> 00:19:42,720
the query throughput is affected by the

00:19:41,120 --> 00:19:46,960
memory allocated

00:19:42,720 --> 00:19:50,080
for each series suppose the size of

00:19:46,960 --> 00:19:52,880
the memory buffer workout memory table

00:19:50,080 --> 00:19:53,679
is one gigabyte and we need to manage

00:19:52,880 --> 00:19:57,440
ten thousand

00:19:53,679 --> 00:20:01,440
time series then each series

00:19:57,440 --> 00:20:04,559
will only have 600 points

00:20:01,440 --> 00:20:07,600
if we read the raw data of the series

00:20:04,559 --> 00:20:11,039
with markus during a day

00:20:07,600 --> 00:20:14,080
then we need to read 100 data blocks

00:20:11,039 --> 00:20:14,880
which may be time consuming as we can

00:20:14,080 --> 00:20:17,600
see

00:20:14,880 --> 00:20:19,520
the data blocks many depends on the size

00:20:17,600 --> 00:20:22,960
of the main table

00:20:19,520 --> 00:20:25,600
currently ltdb has a dynamic parameter

00:20:22,960 --> 00:20:28,640
adapter model

00:20:25,600 --> 00:20:30,240
this model will dynamically adjust the

00:20:28,640 --> 00:20:32,960
system parameters

00:20:30,240 --> 00:20:33,280
such as the size of the main table and

00:20:32,960 --> 00:20:36,640
the

00:20:33,280 --> 00:20:40,000
data file for memory control

00:20:36,640 --> 00:20:42,880
when the load is changed this is to

00:20:40,000 --> 00:20:46,960
avoid the out of memory

00:20:42,880 --> 00:20:51,200
however this model works very cautious

00:20:46,960 --> 00:20:51,200
which leads to small data blocks

00:20:53,120 --> 00:20:56,640
to increase the query throughput we

00:20:55,360 --> 00:20:59,840
could manually configure

00:20:56,640 --> 00:21:03,200
ltdb to get larger data blocks

00:20:59,840 --> 00:21:07,520
first the total memory of ltdb could be

00:21:03,200 --> 00:21:10,400
set by the max heap size in ltdb-nv

00:21:07,520 --> 00:21:12,960
shell then you could disable the

00:21:10,400 --> 00:21:15,200
parameter adapter

00:21:12,960 --> 00:21:17,280
and set the multiple size and the ts

00:21:15,200 --> 00:21:18,960
value set according to the following

00:21:17,280 --> 00:21:22,240
equation

00:21:18,960 --> 00:21:24,960
to liberate productivity we are

00:21:22,240 --> 00:21:25,919
improving our memory control strategy in

00:21:24,960 --> 00:21:28,000
the next

00:21:25,919 --> 00:21:31,840
big version you will no longer need to

00:21:28,000 --> 00:21:31,840
worry about this

00:21:32,080 --> 00:21:36,080
to further increase query throughput we

00:21:35,120 --> 00:21:38,480
introduced the

00:21:36,080 --> 00:21:39,200
level compaction compaction is a

00:21:38,480 --> 00:21:42,960
necessary

00:21:39,200 --> 00:21:45,360
model in the allison based system

00:21:42,960 --> 00:21:48,159
this feature will also be included in

00:21:45,360 --> 00:21:48,159
next version

00:21:51,120 --> 00:21:55,280
out of other data is a common case in

00:21:53,840 --> 00:21:59,440
iot scenario

00:21:55,280 --> 00:22:02,559
we save the time series of inserted data

00:21:59,440 --> 00:22:04,240
is not in chronological order is out of

00:22:02,559 --> 00:22:06,960
other data

00:22:04,240 --> 00:22:08,320
according to our statistics in a wind

00:22:06,960 --> 00:22:12,159
turbine plant

00:22:08,320 --> 00:22:14,799
in 2018 there are one half of out of

00:22:12,159 --> 00:22:16,480
other data and the most out of other

00:22:14,799 --> 00:22:20,000
data is in the recent

00:22:16,480 --> 00:22:23,280
time interval out of other data

00:22:20,000 --> 00:22:25,600
is not variated by database because

00:22:23,280 --> 00:22:27,520
it will impact the system performance in

00:22:25,600 --> 00:22:31,120
many cases

00:22:27,520 --> 00:22:31,760
first in the raw data query if two data

00:22:31,120 --> 00:22:34,480
blocks

00:22:31,760 --> 00:22:35,039
are overlapped we need to merge the two

00:22:34,480 --> 00:22:39,520
blocks

00:22:35,039 --> 00:22:41,600
by a microsoft for the aggregation query

00:22:39,520 --> 00:22:42,640
we generate we will generate the

00:22:41,600 --> 00:22:46,320
synopsis

00:22:42,640 --> 00:22:47,919
for each data block for example the mean

00:22:46,320 --> 00:22:49,679
value the max value

00:22:47,919 --> 00:22:51,039
the first time time and the last

00:22:49,679 --> 00:22:53,840
timestamp

00:22:51,039 --> 00:22:55,760
then we do not need to read the row into

00:22:53,840 --> 00:22:58,000
aggregations

00:22:55,760 --> 00:22:59,840
but out of other data will describe the

00:22:58,000 --> 00:23:02,799
synopsis information

00:22:59,840 --> 00:23:09,360
and we need to read the road data to

00:23:02,799 --> 00:23:12,640
answer the aggregation query

00:23:09,360 --> 00:23:14,400
to get rid of out of other data we could

00:23:12,640 --> 00:23:16,559
do some effort

00:23:14,400 --> 00:23:18,559
both in the client set and in the server

00:23:16,559 --> 00:23:21,679
set

00:23:18,559 --> 00:23:24,799
first in the server set ltdb

00:23:21,679 --> 00:23:28,480
tolerates limited out of other data

00:23:24,799 --> 00:23:30,400
in the memory buffer which lacks a

00:23:28,480 --> 00:23:33,600
sliding window

00:23:30,400 --> 00:23:34,400
in the server we distinguish out of

00:23:33,600 --> 00:23:37,840
other data

00:23:34,400 --> 00:23:38,320
in the device level for example if the

00:23:37,840 --> 00:23:41,440
max

00:23:38,320 --> 00:23:45,760
flash the time of a device at is

00:23:41,440 --> 00:23:49,120
t the newly inserted data of this device

00:23:45,760 --> 00:23:52,240
whose time less than or equal to t is

00:23:49,120 --> 00:23:54,640
out of other data and the data that

00:23:52,240 --> 00:23:56,960
larger than t could be seen as other

00:23:54,640 --> 00:23:56,960
data

00:23:57,120 --> 00:24:02,240
further the compaction could be done to

00:24:00,080 --> 00:24:04,720
eliminate out of other data

00:24:02,240 --> 00:24:06,559
you could enable the merge the

00:24:04,720 --> 00:24:09,600
compaction function by set

00:24:06,559 --> 00:24:10,559
the merge interval second to a non-zero

00:24:09,600 --> 00:24:12,960
value

00:24:10,559 --> 00:24:14,080
which means a merge thread will be

00:24:12,960 --> 00:24:18,720
started

00:24:14,080 --> 00:24:21,679
every merge interval

00:24:18,720 --> 00:24:23,679
in the client side users could try to

00:24:21,679 --> 00:24:28,480
write the data of each device

00:24:23,679 --> 00:24:30,799
in the sending order the left insertions

00:24:28,480 --> 00:24:32,080
doesn't contain out of other data

00:24:30,799 --> 00:24:35,520
because the timestamp

00:24:32,080 --> 00:24:36,880
of each record is increasing from one to

00:24:35,520 --> 00:24:38,960
three

00:24:36,880 --> 00:24:40,080
the red two figures may have out of

00:24:38,960 --> 00:24:43,360
order data

00:24:40,080 --> 00:24:45,919
because the time is in descending order

00:24:43,360 --> 00:24:48,559
or where the same type of different

00:24:45,919 --> 00:24:50,799
series in a device

00:24:48,559 --> 00:24:52,240
why this scenario will generate out of

00:24:50,799 --> 00:24:55,440
other data

00:24:52,240 --> 00:24:58,720
for example when you insert a point

00:24:55,440 --> 00:25:01,600
host timestamp is one the memory useful

00:24:58,720 --> 00:25:04,640
and the data is flashed to disk

00:25:01,600 --> 00:25:06,799
then another point of this series is the

00:25:04,640 --> 00:25:09,279
same time stamp

00:25:06,799 --> 00:25:10,480
samsung bank should be treated as out of

00:25:09,279 --> 00:25:13,360
order data

00:25:10,480 --> 00:25:14,400
because we do not modify the disk data

00:25:13,360 --> 00:25:17,600
we just

00:25:14,400 --> 00:25:17,600
appended the data blocks

00:25:20,559 --> 00:25:24,400
to increase the red throughput we could

00:25:23,200 --> 00:25:27,679
use multiple

00:25:24,400 --> 00:25:30,960
data directories then each disk could

00:25:27,679 --> 00:25:33,840
share some io overhead

00:25:30,960 --> 00:25:34,320
we use the red hat log to ensure the

00:25:33,840 --> 00:25:37,600
data

00:25:34,320 --> 00:25:38,720
safety but this will impact the red

00:25:37,600 --> 00:25:41,200
performance

00:25:38,720 --> 00:25:42,159
so it's better to start the data

00:25:41,200 --> 00:25:45,679
headlock or

00:25:42,159 --> 00:25:50,320
separate the disk if you have a ssd

00:25:45,679 --> 00:25:50,320
then your ssd for the data log is better

00:25:54,960 --> 00:26:01,360
to support low latency in the insertion

00:25:58,400 --> 00:26:03,600
when writing data points we just append

00:26:01,360 --> 00:26:05,440
them directly to the tail of the main

00:26:03,600 --> 00:26:09,279
table

00:26:05,440 --> 00:26:11,840
the insertion time complexity is one

00:26:09,279 --> 00:26:12,400
and then query data we copy the data in

00:26:11,840 --> 00:26:15,760
memory

00:26:12,400 --> 00:26:17,279
and then do a start the query time

00:26:15,760 --> 00:26:24,480
complexity for memory

00:26:17,279 --> 00:26:27,520
data is o unlock n

00:26:24,480 --> 00:26:29,840
last we optimize the right performance

00:26:27,520 --> 00:26:33,120
in the right interface

00:26:29,840 --> 00:26:36,320
for java developers ltdb provides

00:26:33,120 --> 00:26:39,840
two interfaces the gdbc and the native

00:26:36,320 --> 00:26:43,760
interface session which is a nosql

00:26:39,840 --> 00:26:46,960
interface the section is more efficient

00:26:43,760 --> 00:26:49,200
than gdbc because it avoids the sql

00:26:46,960 --> 00:26:51,520
parsing process

00:26:49,200 --> 00:26:53,760
there are two main data structures in

00:26:51,520 --> 00:26:57,520
the session

00:26:53,760 --> 00:26:59,520
one is record and the other is tablet

00:26:57,520 --> 00:27:01,200
the main difference between these two

00:26:59,520 --> 00:27:05,120
structures

00:27:01,200 --> 00:27:08,480
is if we take the data as a table

00:27:05,120 --> 00:27:11,200
if there is no values

00:27:08,480 --> 00:27:12,880
then there are records otherwise it is a

00:27:11,200 --> 00:27:16,080
tablet

00:27:12,880 --> 00:27:18,840
in most cases right data by tablet

00:27:16,080 --> 00:27:21,840
is more efficient than your data by

00:27:18,840 --> 00:27:21,840
workers

00:27:23,520 --> 00:27:28,559
the memory control is important for our

00:27:25,679 --> 00:27:30,960
database in ltdb

00:27:28,559 --> 00:27:31,840
each story group has an independent

00:27:30,960 --> 00:27:34,399
engine

00:27:31,840 --> 00:27:34,960
which contains working mam table to

00:27:34,399 --> 00:27:38,240
receive

00:27:34,960 --> 00:27:41,440
the address when the total size

00:27:38,240 --> 00:27:44,559
of man table reaches a threshold

00:27:41,440 --> 00:27:46,799
we will flash the biggest one then it

00:27:44,559 --> 00:27:49,919
becomes a flexible table

00:27:46,799 --> 00:27:49,919
that is read-only

00:27:51,600 --> 00:27:55,120
to start the data in mam table more

00:27:53,760 --> 00:27:58,000
efficiently

00:27:55,120 --> 00:27:59,520
and the same memory we use array of

00:27:58,000 --> 00:28:02,559
primitive data type

00:27:59,520 --> 00:28:04,799
to store the data and use a pool to

00:28:02,559 --> 00:28:07,760
catch the address to reduce the gc

00:28:04,799 --> 00:28:07,760
in the gbm

00:28:08,559 --> 00:28:12,960
the last part is the frequently asked

00:28:10,799 --> 00:28:16,000
questions

00:28:12,960 --> 00:28:18,559
you could treat this as a lookup table

00:28:16,000 --> 00:28:19,120
most of these problems will be solved in

00:28:18,559 --> 00:28:22,640
the

00:28:19,120 --> 00:28:22,640
future future version

00:28:23,039 --> 00:28:26,480
and you can contact us through our mail

00:28:25,520 --> 00:28:29,120
list

00:28:26,480 --> 00:28:31,200
before sending emails you need to

00:28:29,120 --> 00:28:33,600
subscribe the email

00:28:31,200 --> 00:28:34,880
another way to get feedback is to create

00:28:33,600 --> 00:28:38,000
the issues in the

00:28:34,880 --> 00:28:39,200
our github or in the jira looking

00:28:38,000 --> 00:28:42,240
forward to see you

00:28:39,200 --> 00:28:45,840
in your in our mail list

00:28:42,240 --> 00:28:45,840

YouTube URL: https://www.youtube.com/watch?v=NnW8X92iVEI


