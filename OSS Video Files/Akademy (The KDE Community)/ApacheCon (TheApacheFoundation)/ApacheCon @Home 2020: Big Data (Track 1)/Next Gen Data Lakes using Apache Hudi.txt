Title: Next Gen Data Lakes using Apache Hudi
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Next Gen Data Lakes using Apache Hudi
Balaji Varadarajan, Sivabalan Narayanan

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Data Lakes are one of the fastest growing trends in managing big data across various industries. Data Lakes offer massively scalable data processing over vast amounts of data. One of the main challenges that companies face in building a data lake is designing the right primitives for organizing their data. Apache Hudi helps implement uniform, best-of-breed data lake standards and primitives. With such primitives in place, next generation data lake would be about efficiency and intelligence. Businesses expect their data lake installations to cater to their ever changing needs while being cost efficient. In this talk, we will discuss new features in Apache Hudi that is catered towards building next-gen data-lake. We will start with basic Apache Hudi primitives such as upsert & delete required to achieve acceptable latencies in ingestion while at the same time providing high quality data by enforcing schematization on datasets. We will look into the novel “record level index” supported by Apache Hudi and how it supports efficient upserts. We will then dive into how Apache Hudi supports query optimization by leveraging its rich metadata. Efficient storage management is a key requirement for large data lake installation. We will look at how Apache Hudi supports intelligent and dynamic re-clustering of data for better storage management and faster query times. Finally, we will discuss how to easily onboard your existing dataset to Apache Hudi format, so you can leverage Apache Hudi efficiency without making any drastic changes to your existing data lake.

Balaji Varadarajan:
Balaji Varadarajan is currently a Staff Engineer in Robinhood's data platform team. Previously he was a tech lead in Uber data platform working on Apache Hudi and Hadoop platform at large. Previously, he was one of the lead engineers in LinkedIn’s databus change capture system. Balaji’s interests lie in large-scale distributed data systems.
Sivabalan Narayanan:
Sivabalan Narayanan is a senior software engineer at Uber overseeing data engineering broadly across the network performance monitoring domain. He is an active contributor to Apache Hudi and also big data enthusiasist whose interest lies in building data lake technologies. Previously, he was one of the core engineers responsible for builiding Linkedin's blob store.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,160 --> 00:00:29,679
thanks

00:00:26,320 --> 00:00:32,399
okay so i'll just kick start the talk

00:00:29,679 --> 00:00:34,239
welcome everyone for the talk uh i'm

00:00:32,399 --> 00:00:36,160
going to talk about apache hoodie and

00:00:34,239 --> 00:00:38,559
how to use it to build next-generation

00:00:36,160 --> 00:00:41,440
data lakes

00:00:38,559 --> 00:00:41,440
let me go to the next slide

00:00:42,960 --> 00:00:48,239
yeah my name is balaji vadarajan i am

00:00:45,120 --> 00:00:50,160
currently a pmc member in apache hoodie

00:00:48,239 --> 00:00:51,360
i am working as a staff engineer in

00:00:50,160 --> 00:00:53,520
robin hood

00:00:51,360 --> 00:00:55,039
my background lies in building

00:00:53,520 --> 00:00:56,399
large-scale distributed systems and i

00:00:55,039 --> 00:01:00,399
have worked in

00:00:56,399 --> 00:01:03,359
linkedin database uh over to you

00:01:00,399 --> 00:01:05,119
okay uh this is silverburn uh i'm also

00:01:03,359 --> 00:01:07,840
one of the active apache

00:01:05,119 --> 00:01:09,439
food and community uh i work in uber and

00:01:07,840 --> 00:01:11,680
mobile networking space

00:01:09,439 --> 00:01:13,439
uh but my interest lies in distributed

00:01:11,680 --> 00:01:14,960
systems so i worked in linkedin in a

00:01:13,439 --> 00:01:17,280
distributed lobster

00:01:14,960 --> 00:01:20,240
and now in my non-official arts i

00:01:17,280 --> 00:01:20,240
contribute to hurry

00:01:21,360 --> 00:01:27,280
so let's take a look at today's agenda

00:01:24,640 --> 00:01:27,920
so you start off with uh uh taking a

00:01:27,280 --> 00:01:30,320
look at

00:01:27,920 --> 00:01:31,680
the general requirements for a data lake

00:01:30,320 --> 00:01:33,360
and then in the next section we can talk

00:01:31,680 --> 00:01:34,560
about how a next generation data lake

00:01:33,360 --> 00:01:36,000
might look like and what are its

00:01:34,560 --> 00:01:38,079
requirements

00:01:36,000 --> 00:01:40,240
uh and then following which we can take

00:01:38,079 --> 00:01:42,320
a look at uh what is apache hoodie what

00:01:40,240 --> 00:01:45,280
are its code principles

00:01:42,320 --> 00:01:47,280
and design in the next section we can

00:01:45,280 --> 00:01:49,840
talk about how you can actually leverage

00:01:47,280 --> 00:01:51,840
hoodie to build an efficient data lake

00:01:49,840 --> 00:01:53,520
we can go through some of the tools and

00:01:51,840 --> 00:01:54,479
some how to achieve some of the use

00:01:53,520 --> 00:01:57,520
cases

00:01:54,479 --> 00:01:58,799
uh in streaming analytical world and

00:01:57,520 --> 00:02:00,399
then in the next section we can talk

00:01:58,799 --> 00:02:01,200
about some of the upcoming features in

00:02:00,399 --> 00:02:03,200
hoodie

00:02:01,200 --> 00:02:05,360
and then we can end the talk with some q

00:02:03,200 --> 00:02:05,360
a

00:02:05,439 --> 00:02:10,319
so before we dive into uh take a look at

00:02:08,560 --> 00:02:11,920
requirements of a data lake let's see

00:02:10,319 --> 00:02:13,680
what a data lake is

00:02:11,920 --> 00:02:15,760
uh so it's a central edge repository to

00:02:13,680 --> 00:02:18,080
store a data scale uh irrespective

00:02:15,760 --> 00:02:20,879
whether it's structured or unstructured

00:02:18,080 --> 00:02:22,400
these are typically used for a large

00:02:20,879 --> 00:02:24,239
analytical workloads

00:02:22,400 --> 00:02:25,680
uh built on top of hadoop compatible

00:02:24,239 --> 00:02:27,440
cloud stores uh

00:02:25,680 --> 00:02:29,440
you could typically see a different year

00:02:27,440 --> 00:02:31,599
of tables like raw tables direct tables

00:02:29,440 --> 00:02:35,200
or aggregated tables and so on for

00:02:31,599 --> 00:02:36,000
serving different needs in general these

00:02:35,200 --> 00:02:37,840
are used for

00:02:36,000 --> 00:02:39,280
generating dashboards reports or

00:02:37,840 --> 00:02:40,400
visualizations or even information

00:02:39,280 --> 00:02:42,400
learning models

00:02:40,400 --> 00:02:44,480
uh the end goal will be mainly for

00:02:42,400 --> 00:02:45,440
monitoring purpose or even at some times

00:02:44,480 --> 00:02:47,519
it's it's

00:02:45,440 --> 00:02:49,200
uh it's critical to take some business

00:02:47,519 --> 00:02:50,000
decisions based on the reports generated

00:02:49,200 --> 00:02:53,440
from these

00:02:50,000 --> 00:02:55,599
analytical workloads so now let's

00:02:53,440 --> 00:02:56,879
dive into uh requirements to build a

00:02:55,599 --> 00:02:58,640
data lake

00:02:56,879 --> 00:03:00,159
now let's take a case of a database

00:02:58,640 --> 00:03:02,080
change capture use case

00:03:00,159 --> 00:03:03,760
so your your architecture might look

00:03:02,080 --> 00:03:05,360
something like this but your inserts

00:03:03,760 --> 00:03:06,800
updates and deletes are maintained in an

00:03:05,360 --> 00:03:09,040
oltp database

00:03:06,800 --> 00:03:11,120
so from there you take a periodic

00:03:09,040 --> 00:03:12,319
snapshot and regenerate the tables in

00:03:11,120 --> 00:03:15,120
your data lake

00:03:12,319 --> 00:03:17,200
as you could see even though updates

00:03:15,120 --> 00:03:18,640
span only a smaller percentage of the

00:03:17,200 --> 00:03:21,200
entire data set size

00:03:18,640 --> 00:03:22,959
uh you have you might have to take an

00:03:21,200 --> 00:03:23,680
entire snapshot every time and then

00:03:22,959 --> 00:03:26,400
regenerate

00:03:23,680 --> 00:03:28,319
entire entire summary table for instance

00:03:26,400 --> 00:03:29,680
uh as you could see you can't do a bulk

00:03:28,319 --> 00:03:32,080
load every time

00:03:29,680 --> 00:03:33,840
uh because this leads to unnecessary

00:03:32,080 --> 00:03:35,360
read and write amplification

00:03:33,840 --> 00:03:37,280
and you also end up wasting a lot of

00:03:35,360 --> 00:03:40,319
computer sources along the way

00:03:37,280 --> 00:03:42,080
uh this also means that uh it results in

00:03:40,319 --> 00:03:44,000
unavailability of fresh data in your

00:03:42,080 --> 00:03:47,840
downstream tables because you can't run

00:03:44,000 --> 00:03:47,840
a period snapshot every few minutes

00:03:47,920 --> 00:03:51,040
the next one is an incremental pool

00:03:49,519 --> 00:03:53,680
which goes hand in hand you can

00:03:51,040 --> 00:03:55,680
incremental injection

00:03:53,680 --> 00:03:57,200
so here you could see a typical

00:03:55,680 --> 00:03:58,319
organization of different tables in a

00:03:57,200 --> 00:04:00,720
data lake

00:03:58,319 --> 00:04:01,599
so from different real-time data sources

00:04:00,720 --> 00:04:03,680
uh

00:04:01,599 --> 00:04:05,280
typically you ingest something into

00:04:03,680 --> 00:04:05,840
something called raw tables in your data

00:04:05,280 --> 00:04:08,640
lake

00:04:05,840 --> 00:04:09,760
uh from draw tables uh you generate some

00:04:08,640 --> 00:04:11,920
derived tables

00:04:09,760 --> 00:04:13,439
uh you do some filtering cleansing or

00:04:11,920 --> 00:04:14,480
pruning and then you generate the right

00:04:13,439 --> 00:04:16,720
tables

00:04:14,480 --> 00:04:18,799
from derived tables you you you could

00:04:16,720 --> 00:04:20,000
actually uh have some aggregated tables

00:04:18,799 --> 00:04:22,240
like summary tables

00:04:20,000 --> 00:04:23,759
which is actually fed into your your

00:04:22,240 --> 00:04:26,880
dashboards reports

00:04:23,759 --> 00:04:28,479
or any of these visualizations as you

00:04:26,880 --> 00:04:29,040
could see here let's say if you want to

00:04:28,479 --> 00:04:30,960
keep

00:04:29,040 --> 00:04:32,479
uh if you want to keep data fresh for

00:04:30,960 --> 00:04:35,040
the aggregated table c

00:04:32,479 --> 00:04:36,720
uh you can't actually uh read the entire

00:04:35,040 --> 00:04:37,120
derived table from time and again and

00:04:36,720 --> 00:04:39,120
then

00:04:37,120 --> 00:04:40,479
uh repopulate the entire summary table

00:04:39,120 --> 00:04:41,840
or aggregated table

00:04:40,479 --> 00:04:43,520
so your data lake should support

00:04:41,840 --> 00:04:46,639
incremental pull uh

00:04:43,520 --> 00:04:48,400
which means that it just takes uh

00:04:46,639 --> 00:04:50,000
the updates from the last checkpoint at

00:04:48,400 --> 00:04:50,880
time and then apply it to your

00:04:50,000 --> 00:04:53,759
aggregated

00:04:50,880 --> 00:04:53,759
aggregated table

00:04:54,560 --> 00:04:57,759
de-doping log events uh it's pretty much

00:04:57,360 --> 00:05:00,320
uh

00:04:57,759 --> 00:05:02,240
a necessity when you're dealing with uh

00:05:00,320 --> 00:05:04,720
uh kafka kind of pipelines

00:05:02,240 --> 00:05:06,400
uh let's say it's a scenario of uh

00:05:04,720 --> 00:05:07,840
aggregating some stats on impression

00:05:06,400 --> 00:05:10,000
events so from kafka

00:05:07,840 --> 00:05:11,600
you ingest into your data lake as you

00:05:10,000 --> 00:05:13,360
could see this is a high scale time

00:05:11,600 --> 00:05:14,880
series data

00:05:13,360 --> 00:05:16,880
to the order of millions or even

00:05:14,880 --> 00:05:18,320
billions per day

00:05:16,880 --> 00:05:21,280
there are chances of duplicates in this

00:05:18,320 --> 00:05:23,600
pipeline due to various reasons

00:05:21,280 --> 00:05:25,600
for example the clients could retry due

00:05:23,600 --> 00:05:27,440
to various networking errors

00:05:25,600 --> 00:05:30,080
or the data pipeline which you're using

00:05:27,440 --> 00:05:33,199
to push to kafka

00:05:30,080 --> 00:05:34,400
could gather at least one semantics so

00:05:33,199 --> 00:05:36,160
there are there are a lot of problems

00:05:34,400 --> 00:05:38,400
with duplic duplications it could result

00:05:36,160 --> 00:05:39,919
in low fidelity data so the reports

00:05:38,400 --> 00:05:41,199
which you generate are not very much

00:05:39,919 --> 00:05:43,360
reliable

00:05:41,199 --> 00:05:44,639
so for example more impressions could

00:05:43,360 --> 00:05:46,320
result in more dollars

00:05:44,639 --> 00:05:50,320
which means you end up paying customers

00:05:46,320 --> 00:05:50,320
more or end up charging customers more

00:05:51,039 --> 00:05:54,639
uh if you have worked with uh large

00:05:52,800 --> 00:05:56,080
analytical workloads i'm i'm pretty sure

00:05:54,639 --> 00:05:57,919
you've come across a lot of small file

00:05:56,080 --> 00:06:00,080
problems it's very notorious

00:05:57,919 --> 00:06:01,919
uh so having a lot of small fries in

00:06:00,080 --> 00:06:03,199
your data set or results a lot of

00:06:01,919 --> 00:06:05,600
metadata over it

00:06:03,199 --> 00:06:06,479
in both managing and even it results in

00:06:05,600 --> 00:06:08,880
uh

00:06:06,479 --> 00:06:10,319
increasing your latency for equals on

00:06:08,880 --> 00:06:12,319
the other end of the spectrum

00:06:10,319 --> 00:06:13,440
you could actually stitch files in the

00:06:12,319 --> 00:06:16,479
ingestion path

00:06:13,440 --> 00:06:16,880
and then so that you you can avoid a lot

00:06:16,479 --> 00:06:18,560
of

00:06:16,880 --> 00:06:20,160
creation of a lot of small files but

00:06:18,560 --> 00:06:22,319
again this has its own cons

00:06:20,160 --> 00:06:23,520
it results in a lot of delays in your

00:06:22,319 --> 00:06:25,440
injection pipeline

00:06:23,520 --> 00:06:27,680
and it results in a lot of unnecessary

00:06:25,440 --> 00:06:30,240
uh file ios as well

00:06:27,680 --> 00:06:32,240
so for example it takes typically uh

00:06:30,240 --> 00:06:33,280
five to ten minutes to read like a 2gb

00:06:32,240 --> 00:06:35,039
part k5

00:06:33,280 --> 00:06:36,800
uh so you could take a hybrid approach

00:06:35,039 --> 00:06:38,960
of doing a file switching

00:06:36,800 --> 00:06:41,680
um but then so there is no real

00:06:38,960 --> 00:06:44,479
standardization on file switching

00:06:41,680 --> 00:06:46,479
but again while doing file stitching you

00:06:44,479 --> 00:06:48,319
need to ensure consistency guarantees

00:06:46,479 --> 00:06:50,080
and you also need to ensure snapshot

00:06:48,319 --> 00:06:50,720
isolation because your small files could

00:06:50,080 --> 00:06:53,120
have been

00:06:50,720 --> 00:06:54,880
exposed to your query engine so uh you

00:06:53,120 --> 00:06:56,880
need to ensure snapshot isolation is

00:06:54,880 --> 00:06:58,560
guaranteed

00:06:56,880 --> 00:06:59,919
uh next requirement is around

00:06:58,560 --> 00:07:02,560
transactional rights

00:06:59,919 --> 00:07:04,560
uh this is nothing but acid guarantees

00:07:02,560 --> 00:07:06,720
so i'll just skim through fastly

00:07:04,560 --> 00:07:08,400
uh first one is atomicity that could be

00:07:06,720 --> 00:07:11,680
uh a failure though

00:07:08,400 --> 00:07:13,599
halfway through your injection and

00:07:11,680 --> 00:07:15,199
do you know in case of bad data you

00:07:13,599 --> 00:07:16,960
might wish to roll back

00:07:15,199 --> 00:07:18,479
so at any point in time either your

00:07:16,960 --> 00:07:20,000
entire batch of write is

00:07:18,479 --> 00:07:21,520
actually published or nothing is

00:07:20,000 --> 00:07:23,599
published

00:07:21,520 --> 00:07:25,440
uh your data lake should also support

00:07:23,599 --> 00:07:27,440
ensure consistency guarantees

00:07:25,440 --> 00:07:29,360
so no partial data should be exposed to

00:07:27,440 --> 00:07:31,280
the end user

00:07:29,360 --> 00:07:32,400
uh since your data lake should support

00:07:31,280 --> 00:07:34,319
concurrent writer

00:07:32,400 --> 00:07:36,720
writers and readers uh snapshot

00:07:34,319 --> 00:07:38,720
isolation should be guaranteed

00:07:36,720 --> 00:07:40,000
and it should ensure strong durability

00:07:38,720 --> 00:07:41,599
uh which means that there

00:07:40,000 --> 00:07:43,919
doesn't there shouldn't be no data loss

00:07:41,599 --> 00:07:44,879
which means that if you made a comment

00:07:43,919 --> 00:07:46,960
and then if your

00:07:44,879 --> 00:07:48,560
storage node crashes for some reason

00:07:46,960 --> 00:07:51,120
your data lake should ensure there's no

00:07:48,560 --> 00:07:51,120
data loss

00:07:51,759 --> 00:07:54,879
so offline there have been a lot of

00:07:53,199 --> 00:07:55,599
regulations around data deletions and

00:07:54,879 --> 00:07:58,000
privacy

00:07:55,599 --> 00:08:01,199
you might have come across gdpr or even

00:07:58,000 --> 00:08:04,479
ccp a california consumer privacy act

00:08:01,199 --> 00:08:05,360
so there's a strong need for deleting

00:08:04,479 --> 00:08:09,039
records

00:08:05,360 --> 00:08:11,680
uh from from your data lake um

00:08:09,039 --> 00:08:13,199
so this essentially means that your data

00:08:11,680 --> 00:08:14,080
lake should support efficient delete

00:08:13,199 --> 00:08:16,000
operations

00:08:14,080 --> 00:08:17,919
uh which means that there should be like

00:08:16,000 --> 00:08:18,800
a point-ish lookups on your on your

00:08:17,919 --> 00:08:21,759
right path

00:08:18,800 --> 00:08:23,599
uh like a needle in a haystack and but

00:08:21,759 --> 00:08:25,599
still your data lake should be

00:08:23,599 --> 00:08:27,440
optimized for scans because most of the

00:08:25,599 --> 00:08:28,240
analytical workloads are right ones and

00:08:27,440 --> 00:08:31,680
read

00:08:28,240 --> 00:08:33,839
read heavy use cases and

00:08:31,680 --> 00:08:35,680
it should also support propagating these

00:08:33,839 --> 00:08:37,680
changes or deletions to the downstream

00:08:35,680 --> 00:08:39,360
tables as well

00:08:37,680 --> 00:08:41,039
so there are a few other requirements

00:08:39,360 --> 00:08:42,640
which data lake should support in

00:08:41,039 --> 00:08:43,360
general like it should support handling

00:08:42,640 --> 00:08:46,240
of later

00:08:43,360 --> 00:08:47,600
data the data format should be open in

00:08:46,240 --> 00:08:49,120
the sense like you don't lock in the

00:08:47,600 --> 00:08:51,040
user

00:08:49,120 --> 00:08:52,480
and it should enforce a schematization

00:08:51,040 --> 00:08:53,920
of data

00:08:52,480 --> 00:08:55,440
point in time queries if you want to

00:08:53,920 --> 00:08:57,120
take a look at how your data look like

00:08:55,440 --> 00:08:59,279
like two days back it should support

00:08:57,120 --> 00:09:01,920
point in time queries and save points

00:08:59,279 --> 00:09:04,640
for safer recovery and so on

00:09:01,920 --> 00:09:05,680
um so data lake has been her on for a

00:09:04,640 --> 00:09:08,000
few years now

00:09:05,680 --> 00:09:09,200
so the requirements for data lake has

00:09:08,000 --> 00:09:12,720
been evolving

00:09:09,200 --> 00:09:14,320
and uh in this section we can dive into

00:09:12,720 --> 00:09:16,000
uh to take a look at how the next

00:09:14,320 --> 00:09:18,320
generation data lake uh

00:09:16,000 --> 00:09:19,920
requirements might look like uh your

00:09:18,320 --> 00:09:20,720
data election should be dynamic in

00:09:19,920 --> 00:09:22,560
nature

00:09:20,720 --> 00:09:24,160
uh let let me walk through an

00:09:22,560 --> 00:09:26,959
illustration to to

00:09:24,160 --> 00:09:28,240
explain what do i mean by dynamic so

00:09:26,959 --> 00:09:31,360
going back to the

00:09:28,240 --> 00:09:32,240
impression example so from kafka you

00:09:31,360 --> 00:09:35,040
generate a

00:09:32,240 --> 00:09:36,000
aggregated stats table in your data lake

00:09:35,040 --> 00:09:37,600
so your schema might

00:09:36,000 --> 00:09:40,240
look something like this but you have

00:09:37,600 --> 00:09:43,440
even tidy even time user id date stream

00:09:40,240 --> 00:09:44,959
and so on so um

00:09:43,440 --> 00:09:46,959
sometimes that could be a mismatch

00:09:44,959 --> 00:09:48,959
between your query and ingestion pattern

00:09:46,959 --> 00:09:51,360
so for example in this case your

00:09:48,959 --> 00:09:53,120
ingestion could be based on arrival time

00:09:51,360 --> 00:09:55,600
but then your queries could be based on

00:09:53,120 --> 00:09:57,519
even time uh so this means that based on

00:09:55,600 --> 00:09:59,200
arrival time you have layered or you

00:09:57,519 --> 00:10:00,480
have laid out your data in such a manner

00:09:59,200 --> 00:10:01,839
depending on the arrival time

00:10:00,480 --> 00:10:03,279
whereas your queries are actually

00:10:01,839 --> 00:10:05,040
following a different pattern which is

00:10:03,279 --> 00:10:08,399
even time

00:10:05,040 --> 00:10:10,240
so the data lake requires primitives

00:10:08,399 --> 00:10:12,480
around reorganizing data

00:10:10,240 --> 00:10:13,680
after injection has happened because

00:10:12,480 --> 00:10:15,120
your queries are

00:10:13,680 --> 00:10:17,200
changing the patterns of queries are

00:10:15,120 --> 00:10:19,120
changing so

00:10:17,200 --> 00:10:21,360
uh while supporting such organ

00:10:19,120 --> 00:10:23,680
reorganizing or re-clustering of data

00:10:21,360 --> 00:10:25,519
uh these are reorganizations should be

00:10:23,680 --> 00:10:27,519
pluggable and it should be non-blocking

00:10:25,519 --> 00:10:28,800
non-blocking is very important because

00:10:27,519 --> 00:10:31,360
you can't actually

00:10:28,800 --> 00:10:32,240
uh in the in this pipeline you can't

00:10:31,360 --> 00:10:34,240
increase

00:10:32,240 --> 00:10:36,240
the latency in the right path or you

00:10:34,240 --> 00:10:38,240
can't actually stop the missionary do a

00:10:36,240 --> 00:10:40,000
rewrite of the entire data set and then

00:10:38,240 --> 00:10:46,240
open up for queries so it should be

00:10:40,000 --> 00:10:49,360
non-blocking as well

00:10:46,240 --> 00:10:50,959
yeah so here you could see a a simple uh

00:10:49,360 --> 00:10:53,440
streaming analytical

00:10:50,959 --> 00:10:55,440
uh architecture so on the right you

00:10:53,440 --> 00:10:56,560
could see a batch injection maintained

00:10:55,440 --> 00:10:58,640
in a data lake

00:10:56,560 --> 00:10:59,920
uh where it's refreshed let's say every

00:10:58,640 --> 00:11:01,440
few hours

00:10:59,920 --> 00:11:03,760
on the left you could see a stream

00:11:01,440 --> 00:11:06,399
processing pipeline

00:11:03,760 --> 00:11:08,000
where the freshness guarantees or it's

00:11:06,399 --> 00:11:11,440
expected to be near real time

00:11:08,000 --> 00:11:14,560
to the order of one to five minutes

00:11:11,440 --> 00:11:15,519
so typically the data lakes are often

00:11:14,560 --> 00:11:18,959
overlooked

00:11:15,519 --> 00:11:21,760
uh to be meant only for batch processing

00:11:18,959 --> 00:11:23,600
uh because of the lack of primitives and

00:11:21,760 --> 00:11:26,959
support for stream processing

00:11:23,600 --> 00:11:29,440
uh but data lake should should

00:11:26,959 --> 00:11:30,720
uh go to the next next generation so it

00:11:29,440 --> 00:11:32,240
should actually support

00:11:30,720 --> 00:11:33,920
stream proxy because there's a lot of

00:11:32,240 --> 00:11:35,600
overhead in maintaining different

00:11:33,920 --> 00:11:37,120
uh specialized systems for stream

00:11:35,600 --> 00:11:39,440
processing and for

00:11:37,120 --> 00:11:40,880
your batch crashing crossing needs so

00:11:39,440 --> 00:11:42,480
your data election should evolve to

00:11:40,880 --> 00:11:44,640
support stream crossing pipelines as

00:11:42,480 --> 00:11:44,640
well

00:11:45,920 --> 00:11:50,240
one of the most critical uh requirement

00:11:48,240 --> 00:11:52,000
nowadays is your your injection should

00:11:50,240 --> 00:11:53,760
be auto managed

00:11:52,000 --> 00:11:55,440
as you might have noticed the sources of

00:11:53,760 --> 00:11:58,000
raw data is of heterogeneous

00:11:55,440 --> 00:12:00,079
nature it ranges from daily change logs

00:11:58,000 --> 00:12:02,320
to kafka even streams

00:12:00,079 --> 00:12:03,920
listening to s3 buckets and so on so

00:12:02,320 --> 00:12:04,560
typically these are implemented as

00:12:03,920 --> 00:12:06,000
separate

00:12:04,560 --> 00:12:08,560
custom injection pipelines and

00:12:06,000 --> 00:12:10,480
maintained by users

00:12:08,560 --> 00:12:12,000
so again there's a lot of overhead in

00:12:10,480 --> 00:12:13,040
maintaining these separate custom

00:12:12,000 --> 00:12:14,880
pipelines so

00:12:13,040 --> 00:12:17,040
your data data lake should support

00:12:14,880 --> 00:12:20,160
automating most of these operations

00:12:17,040 --> 00:12:22,160
uh if not all uh to be explicit

00:12:20,160 --> 00:12:24,240
i mean that uh data lake should support

00:12:22,160 --> 00:12:26,160
incremental read and checkpointing

00:12:24,240 --> 00:12:27,920
uh it should support merge and uh

00:12:26,160 --> 00:12:28,800
deletes basically you you take a bunch

00:12:27,920 --> 00:12:31,120
of updates

00:12:28,800 --> 00:12:32,480
and just apply to your data lake uh it

00:12:31,120 --> 00:12:35,200
should support rollback

00:12:32,480 --> 00:12:37,760
uh whenever it's required and then it

00:12:35,200 --> 00:12:39,600
should support compaction

00:12:37,760 --> 00:12:41,120
in line if not it should support

00:12:39,600 --> 00:12:42,639
asynchronous compaction which is

00:12:41,120 --> 00:12:43,920
self-managed by its own and it's

00:12:42,639 --> 00:12:46,480
continuous

00:12:43,920 --> 00:12:46,480
in nature

00:12:47,440 --> 00:12:51,279
so typically data lake exposed the read

00:12:49,920 --> 00:12:53,680
path of the data lake is

00:12:51,279 --> 00:12:55,440
exposed by a different query engines uh

00:12:53,680 --> 00:12:57,519
query optimization is done to different

00:12:55,440 --> 00:12:59,839
measures one is partition pruning

00:12:57,519 --> 00:13:01,839
and another one is column level or group

00:12:59,839 --> 00:13:04,839
level pruning

00:13:01,839 --> 00:13:06,000
so again if your pattern of query

00:13:04,839 --> 00:13:08,079
changes uh

00:13:06,000 --> 00:13:10,320
let's say if so if the columns are

00:13:08,079 --> 00:13:11,680
filtered based on a non-partition column

00:13:10,320 --> 00:13:13,760
you end up reading most of your

00:13:11,680 --> 00:13:15,600
partitions so your

00:13:13,760 --> 00:13:18,320
read performance might take a hit so

00:13:15,600 --> 00:13:20,399
data lakes can do much better with

00:13:18,320 --> 00:13:22,399
having some auxiliary structures for

00:13:20,399 --> 00:13:24,320
example uh data lake should

00:13:22,399 --> 00:13:25,839
could store some uh secondary industries

00:13:24,320 --> 00:13:28,240
or bloom intake secondary

00:13:25,839 --> 00:13:29,519
industries i mean uh like a sparse index

00:13:28,240 --> 00:13:31,839
like a range index

00:13:29,519 --> 00:13:34,800
uh which you could maintain for commonly

00:13:31,839 --> 00:13:36,639
uh queried or filtered columns

00:13:34,800 --> 00:13:38,160
and then bloom index you could maintain

00:13:36,639 --> 00:13:42,639
for every data file

00:13:38,160 --> 00:13:45,760
so uh frequently uh query column so

00:13:42,639 --> 00:13:47,440
again uh this uh the basic uh

00:13:45,760 --> 00:13:49,279
objective here is that you reduce the

00:13:47,440 --> 00:13:50,880
number of data files to be looked up to

00:13:49,279 --> 00:13:54,000
serve your deep query so

00:13:50,880 --> 00:13:57,760
these all will actually uh uh ensure you

00:13:54,000 --> 00:13:57,760
keep your read latency under control

00:13:57,839 --> 00:14:01,279
so i it's over to biology for the rest

00:13:59,839 --> 00:14:05,279
of the

00:14:01,279 --> 00:14:05,279
section okay let me share my screen

00:14:10,000 --> 00:14:17,199
uh you can see my screen right yes

00:14:14,399 --> 00:14:18,000
okay so i'll just uh talk about apache

00:14:17,199 --> 00:14:20,560
hoodie

00:14:18,000 --> 00:14:24,000
in general and how it can be used to

00:14:20,560 --> 00:14:26,880
implement the next generation data lake

00:14:24,000 --> 00:14:27,680
uh we'll start with the overview of ruby

00:14:26,880 --> 00:14:30,079
hoodie

00:14:27,680 --> 00:14:31,519
manages the storage of large analytical

00:14:30,079 --> 00:14:34,000
data sets

00:14:31,519 --> 00:14:35,519
there are many facets to hoodie as a

00:14:34,000 --> 00:14:38,399
storage abstraction

00:14:35,519 --> 00:14:41,360
it provides efficient absurd and

00:14:38,399 --> 00:14:41,360
incremental primitives

00:14:41,839 --> 00:14:45,760
hoodies injection framework called delta

00:14:43,600 --> 00:14:48,959
streamer spark data source

00:14:45,760 --> 00:14:50,560
and structured streaming integration are

00:14:48,959 --> 00:14:53,199
like a building blocks for

00:14:50,560 --> 00:14:55,839
building and unified streaming and batch

00:14:53,199 --> 00:14:57,760
processing pipelines

00:14:55,839 --> 00:15:00,000
from the implementation perspective it

00:14:57,760 --> 00:15:02,959
can you can think of it as a library

00:15:00,000 --> 00:15:05,040
uh which can uh scale with the

00:15:02,959 --> 00:15:05,760
processing engine like spark for writes

00:15:05,040 --> 00:15:08,880
and for

00:15:05,760 --> 00:15:09,600
query engines for rate and it basically

00:15:08,880 --> 00:15:12,399
supports

00:15:09,600 --> 00:15:13,040
uh storing the analytical data sets in

00:15:12,399 --> 00:15:17,120
any

00:15:13,040 --> 00:15:17,120
hadoop compatible stores

00:15:17,440 --> 00:15:21,600
on the right side hoodie provides three

00:15:19,360 --> 00:15:23,360
levels of apis

00:15:21,600 --> 00:15:26,399
the first one is the low level right

00:15:23,360 --> 00:15:29,199
client api which operates at

00:15:26,399 --> 00:15:31,360
rdd abstraction and provides low level

00:15:29,199 --> 00:15:34,160
absurd insert apis

00:15:31,360 --> 00:15:35,440
there's also the spark data source and

00:15:34,160 --> 00:15:39,279
streaming

00:15:35,440 --> 00:15:41,360
integration which you can use to build

00:15:39,279 --> 00:15:42,480
uh batch or streaming injection

00:15:41,360 --> 00:15:44,399
pipelines

00:15:42,480 --> 00:15:45,680
uh hoodie also comes with its own

00:15:44,399 --> 00:15:46,480
injection framework called delta

00:15:45,680 --> 00:15:48,320
streamer

00:15:46,480 --> 00:15:50,720
uh which integrates with various

00:15:48,320 --> 00:15:53,440
different upstream sources

00:15:50,720 --> 00:15:54,480
and provides uh injection framework for

00:15:53,440 --> 00:15:57,199
ingesting two

00:15:54,480 --> 00:15:57,199
hoodie tables

00:15:58,079 --> 00:16:01,440
on the read side hoodie provides three

00:16:00,560 --> 00:16:03,680
distinct views

00:16:01,440 --> 00:16:04,720
uh on there's one view called read

00:16:03,680 --> 00:16:08,240
optimized view

00:16:04,720 --> 00:16:10,720
which provides the latest snapshot

00:16:08,240 --> 00:16:11,519
of the data site for different query

00:16:10,720 --> 00:16:14,079
engines

00:16:11,519 --> 00:16:16,000
and it provides columnar performance uh

00:16:14,079 --> 00:16:16,880
it basically stores the data in parking

00:16:16,000 --> 00:16:19,279
format and

00:16:16,880 --> 00:16:20,880
provides columnar performance the other

00:16:19,279 --> 00:16:23,120
one is the real-time view

00:16:20,880 --> 00:16:24,399
which we will look into further in the

00:16:23,120 --> 00:16:25,920
next subsequent slides

00:16:24,399 --> 00:16:28,240
but it again actually provides a

00:16:25,920 --> 00:16:31,360
snapshot views uh it throw it

00:16:28,240 --> 00:16:32,160
it takes a slight read cost in

00:16:31,360 --> 00:16:34,800
performing

00:16:32,160 --> 00:16:36,639
uh on the flight merge for but for

00:16:34,800 --> 00:16:40,560
providing the latest

00:16:36,639 --> 00:16:43,519
committed data and there is this novel

00:16:40,560 --> 00:16:44,560
incremental view which we support in

00:16:43,519 --> 00:16:46,880
hoodie

00:16:44,560 --> 00:16:47,839
it provides basically a change log view

00:16:46,880 --> 00:16:50,720
for

00:16:47,839 --> 00:16:52,480
any incremental processing that that's

00:16:50,720 --> 00:16:56,000
needed for building

00:16:52,480 --> 00:16:56,000
edl pipelines

00:16:57,360 --> 00:17:02,480
when it comes to the storage layout

00:16:59,279 --> 00:17:05,520
hoodie has two different types

00:17:02,480 --> 00:17:08,000
one is uh called copy and write uh here

00:17:05,520 --> 00:17:08,880
uh records are always written in column

00:17:08,000 --> 00:17:11,199
format

00:17:08,880 --> 00:17:13,360
uh that basically means that any changes

00:17:11,199 --> 00:17:14,400
uh to existing records any updates or

00:17:13,360 --> 00:17:16,959
deletes

00:17:14,400 --> 00:17:18,079
will result in a new version of the

00:17:16,959 --> 00:17:21,439
columnar file

00:17:18,079 --> 00:17:22,000
getting added uh there is also this

00:17:21,439 --> 00:17:24,720
another

00:17:22,000 --> 00:17:26,319
uh storage step called merge on read uh

00:17:24,720 --> 00:17:27,280
it's primarily used to speed up the

00:17:26,319 --> 00:17:30,480
ingestion

00:17:27,280 --> 00:17:33,360
uh here uh basically it supports i mean

00:17:30,480 --> 00:17:35,600
the format is both columnar and row

00:17:33,360 --> 00:17:38,320
based delta files

00:17:35,600 --> 00:17:39,280
uh we uh updates are basically uh

00:17:38,320 --> 00:17:41,200
instead of

00:17:39,280 --> 00:17:42,799
creating a new version of parque files

00:17:41,200 --> 00:17:43,679
we actually append them into a row

00:17:42,799 --> 00:17:46,640
format

00:17:43,679 --> 00:17:48,880
and there is a background compaction job

00:17:46,640 --> 00:17:50,640
that merges these delta files

00:17:48,880 --> 00:17:53,840
to create a new version of the column of

00:17:50,640 --> 00:17:57,360
files we will look at

00:17:53,840 --> 00:17:58,000
the illustration of how a copy on write

00:17:57,360 --> 00:18:01,280
table

00:17:58,000 --> 00:18:02,160
and how a merchandise table manages the

00:18:01,280 --> 00:18:04,559
layout

00:18:02,160 --> 00:18:06,240
and provides a consistent view to the

00:18:04,559 --> 00:18:08,000
data set

00:18:06,240 --> 00:18:09,440
we'll start with the copy and write

00:18:08,000 --> 00:18:10,960
storage layout uh

00:18:09,440 --> 00:18:13,120
let's assume that's a hoodie managed

00:18:10,960 --> 00:18:16,080
data set and hoodie

00:18:13,120 --> 00:18:18,480
uses something called a commit timeline

00:18:16,080 --> 00:18:19,200
uh it's basically a folder inside a data

00:18:18,480 --> 00:18:22,000
set uh

00:18:19,200 --> 00:18:23,679
hoodie uses that to to basically record

00:18:22,000 --> 00:18:24,640
all the operations that are happening on

00:18:23,679 --> 00:18:26,799
the data set

00:18:24,640 --> 00:18:29,440
and also along with the state at which

00:18:26,799 --> 00:18:31,360
these operations are present

00:18:29,440 --> 00:18:32,480
let's imagine for this illustration

00:18:31,360 --> 00:18:34,960
there are four keys

00:18:32,480 --> 00:18:35,919
uh records with four keys key one to key

00:18:34,960 --> 00:18:37,600
four

00:18:35,919 --> 00:18:39,919
which needs to be ingested to this data

00:18:37,600 --> 00:18:42,080
set hoodie begins with

00:18:39,919 --> 00:18:44,080
creating a marker file called inflight

00:18:42,080 --> 00:18:46,720
file in the comment timeline

00:18:44,080 --> 00:18:48,000
announcing the intent to ingest this new

00:18:46,720 --> 00:18:50,559
data

00:18:48,000 --> 00:18:51,280
it then goes ahead and creates these

00:18:50,559 --> 00:18:54,160
files

00:18:51,280 --> 00:18:55,360
the column of parquet files so in this

00:18:54,160 --> 00:18:58,960
example assume that

00:18:55,360 --> 00:19:03,280
key 1 and key 3 are co-located in file 1

00:18:58,960 --> 00:19:06,799
whereas key 2 and key 4 are located in

00:19:03,280 --> 00:19:08,960
file 2. so once this is

00:19:06,799 --> 00:19:10,160
written if you look at the commit

00:19:08,960 --> 00:19:12,880
timeline

00:19:10,160 --> 00:19:13,840
hoodie will mark the the action as

00:19:12,880 --> 00:19:16,720
completed

00:19:13,840 --> 00:19:17,760
so subsequently any of the read

00:19:16,720 --> 00:19:21,280
operations that are

00:19:17,760 --> 00:19:23,919
performed on these data set will see the

00:19:21,280 --> 00:19:26,320
files that have been returned as part of

00:19:23,919 --> 00:19:29,280
the commit c1

00:19:26,320 --> 00:19:30,880
so far so good so uh like once this is

00:19:29,280 --> 00:19:32,480
done like let's assume that in future

00:19:30,880 --> 00:19:34,480
there's an update that's going to happen

00:19:32,480 --> 00:19:37,440
on key one and key three

00:19:34,480 --> 00:19:38,080
uh hoodie again like uh like creates an

00:19:37,440 --> 00:19:39,840
in-flight

00:19:38,080 --> 00:19:41,280
uh marker file in the timeline to

00:19:39,840 --> 00:19:44,000
announce the

00:19:41,280 --> 00:19:44,480
new in ingestion that's going to happen

00:19:44,000 --> 00:19:48,240
for the

00:19:44,480 --> 00:19:49,600
batch too uh it in this case hoodie is

00:19:48,240 --> 00:19:52,160
going to do an index lookup

00:19:49,600 --> 00:19:54,320
and it figures out that key one and key

00:19:52,160 --> 00:19:57,600
three are located in file one

00:19:54,320 --> 00:20:00,720
so it basically creates a

00:19:57,600 --> 00:20:01,520
new version of the parquet file with the

00:20:00,720 --> 00:20:04,960
updated

00:20:01,520 --> 00:20:08,640
records key one and key three in it

00:20:04,960 --> 00:20:11,120
so once this is done um the

00:20:08,640 --> 00:20:12,080
timeline is marked for the commit c2 is

00:20:11,120 --> 00:20:16,080
marked done

00:20:12,080 --> 00:20:18,400
and like after this point in time

00:20:16,080 --> 00:20:20,640
any uh read queries that are happening

00:20:18,400 --> 00:20:21,679
on this data set is going to see the

00:20:20,640 --> 00:20:25,200
latest version

00:20:21,679 --> 00:20:28,000
of the file one which is at c2

00:20:25,200 --> 00:20:28,559
and it's going to read the data this is

00:20:28,000 --> 00:20:33,120
how

00:20:28,559 --> 00:20:35,679
copy and write manages the data set

00:20:33,120 --> 00:20:36,960
now with merge on read um the first

00:20:35,679 --> 00:20:39,600
batch is almost same

00:20:36,960 --> 00:20:41,039
where we can again assume that there are

00:20:39,600 --> 00:20:44,720
four keys here

00:20:41,039 --> 00:20:48,720
um we uh write these four keys are

00:20:44,720 --> 00:20:51,440
ingested to do different rk files and

00:20:48,720 --> 00:20:52,080
the comment is done uh at this point

00:20:51,440 --> 00:20:54,640
let's say

00:20:52,080 --> 00:20:56,240
uh a new batch comes in and just this is

00:20:54,640 --> 00:20:58,960
where it actually differs from the

00:20:56,240 --> 00:21:02,000
previous copy on write storage layout

00:20:58,960 --> 00:21:03,520
so hoodie again is going to perform an

00:21:02,000 --> 00:21:04,240
index lookup it's going to figure out

00:21:03,520 --> 00:21:07,520
that key 1

00:21:04,240 --> 00:21:10,559
and key 3 belongs to file 1

00:21:07,520 --> 00:21:12,320
whereas key 2 belongs to file 2. and

00:21:10,559 --> 00:21:14,320
but here instead of creating a new

00:21:12,320 --> 00:21:18,000
version of a packet file

00:21:14,320 --> 00:21:20,159
it is going to append these uh records

00:21:18,000 --> 00:21:21,280
without merging the changes to the

00:21:20,159 --> 00:21:24,000
previous uh

00:21:21,280 --> 00:21:24,720
record uh and it's going to append them

00:21:24,000 --> 00:21:28,480
to

00:21:24,720 --> 00:21:30,720
a delta file in in row format in avro

00:21:28,480 --> 00:21:32,880
and uh this is going to be uh fast

00:21:30,720 --> 00:21:34,880
wherein like it doesn't have to

00:21:32,880 --> 00:21:36,000
like take the heat of creating a

00:21:34,880 --> 00:21:39,280
columnar file

00:21:36,000 --> 00:21:42,480
and performing uh updates um mergers if

00:21:39,280 --> 00:21:45,360
it needed uh and so uh this is how like

00:21:42,480 --> 00:21:49,039
uh merge on read basically

00:21:45,360 --> 00:21:51,440
gets basically uh

00:21:49,039 --> 00:21:53,440
gets a quicker injection uh as opposed

00:21:51,440 --> 00:21:55,919
to a copy on write table

00:21:53,440 --> 00:21:57,280
so uh important view here is this

00:21:55,919 --> 00:22:00,480
real-time view

00:21:57,280 --> 00:22:03,200
here what happens is that

00:22:00,480 --> 00:22:05,200
if you want the latest snapshot that

00:22:03,200 --> 00:22:06,960
includes both the changes that have been

00:22:05,200 --> 00:22:10,880
committed in c1 which is in

00:22:06,960 --> 00:22:14,320
in columnar format and c2 which is in

00:22:10,880 --> 00:22:16,640
the unmerged row format uh we

00:22:14,320 --> 00:22:18,799
use this view called real-time view this

00:22:16,640 --> 00:22:21,600
performs on-the-fly merge of

00:22:18,799 --> 00:22:22,400
both the columnar uh parquet file and

00:22:21,600 --> 00:22:26,240
the unmerged

00:22:22,400 --> 00:22:29,360
uh record records and provide a latest

00:22:26,240 --> 00:22:32,159
snapshot view for the user

00:22:29,360 --> 00:22:32,880
so this way any queries that uses this

00:22:32,159 --> 00:22:34,880
view

00:22:32,880 --> 00:22:37,360
will be able to see the changes that are

00:22:34,880 --> 00:22:39,520
happened in both c1 and c2

00:22:37,360 --> 00:22:40,720
but it actually is going to take a small

00:22:39,520 --> 00:22:42,720
hit on of

00:22:40,720 --> 00:22:45,280
on the read side of taking the of

00:22:42,720 --> 00:22:47,679
merging these two changes

00:22:45,280 --> 00:22:48,720
on the read side uh but if you don't

00:22:47,679 --> 00:22:50,880
want to take the hit

00:22:48,720 --> 00:22:52,960
uh there's a trade-off here wherein like

00:22:50,880 --> 00:22:53,600
uh there's a view called raid optimized

00:22:52,960 --> 00:22:56,400
view

00:22:53,600 --> 00:22:58,320
which will only read c1's committed data

00:22:56,400 --> 00:23:01,440
which is already there in

00:22:58,320 --> 00:23:03,600
merged format in columnar format

00:23:01,440 --> 00:23:04,640
and it's going to just like get columnar

00:23:03,600 --> 00:23:06,960
performance for it

00:23:04,640 --> 00:23:08,559
but it's not going to see the changes

00:23:06,960 --> 00:23:11,919
that happened after c1

00:23:08,559 --> 00:23:15,200
in this case c2 um so

00:23:11,919 --> 00:23:15,200
these are two different views and

00:23:15,360 --> 00:23:19,760
there is a background compaction job

00:23:17,840 --> 00:23:22,640
which will happen eventually which will

00:23:19,760 --> 00:23:23,440
compact uh these unmerged records and

00:23:22,640 --> 00:23:26,159
the

00:23:23,440 --> 00:23:28,480
on the changes that are there in c1 and

00:23:26,159 --> 00:23:30,960
to create a new file version so that way

00:23:28,480 --> 00:23:32,320
uh the read optimized view will be able

00:23:30,960 --> 00:23:35,600
to see

00:23:32,320 --> 00:23:38,720
both c1 and c2 changes

00:23:35,600 --> 00:23:41,360
so now that we uh saw like

00:23:38,720 --> 00:23:43,360
the like basics of uh hoodie uh in terms

00:23:41,360 --> 00:23:45,360
of how it uh manages the

00:23:43,360 --> 00:23:46,559
uh the records and how does it manage

00:23:45,360 --> 00:23:48,960
the storage layout and

00:23:46,559 --> 00:23:50,720
uh what views it provides uh we will

00:23:48,960 --> 00:23:52,480
look at some of the functionalities that

00:23:50,720 --> 00:23:55,679
it provides in order to build

00:23:52,480 --> 00:23:57,360
an efficient data lake

00:23:55,679 --> 00:24:00,080
so we'll start with uh delta streamer

00:23:57,360 --> 00:24:03,520
basically uh uh okay so ingestion

00:24:00,080 --> 00:24:05,279
uh is uh not just about the data format

00:24:03,520 --> 00:24:06,880
that gets written or the

00:24:05,279 --> 00:24:08,960
the processing engine that's getting

00:24:06,880 --> 00:24:09,679
used right so it's a basically a

00:24:08,960 --> 00:24:12,559
framework

00:24:09,679 --> 00:24:13,039
uh platform i would say which needs to

00:24:12,559 --> 00:24:16,320
deal with

00:24:13,039 --> 00:24:19,039
a lot of other things uh you need to

00:24:16,320 --> 00:24:20,559
keep track of where you are in your

00:24:19,039 --> 00:24:22,159
consumption of upstream source when

00:24:20,559 --> 00:24:24,480
you're seeing data

00:24:22,159 --> 00:24:26,400
uh there is going to be uh various types

00:24:24,480 --> 00:24:28,640
of upstream sources and need to support

00:24:26,400 --> 00:24:30,880
all of them when ingesting the data to

00:24:28,640 --> 00:24:33,919
your data lake

00:24:30,880 --> 00:24:36,880
uh we need to handle partial failures

00:24:33,919 --> 00:24:38,559
during injection which is just a norm uh

00:24:36,880 --> 00:24:40,320
you need to perform rollbacks

00:24:38,559 --> 00:24:42,240
you need to clean up old versions of

00:24:40,320 --> 00:24:45,039
these files that are getting added

00:24:42,240 --> 00:24:46,640
and any compactions that needs to be

00:24:45,039 --> 00:24:48,320
done for my generation because all needs

00:24:46,640 --> 00:24:50,159
to be done in an atomic

00:24:48,320 --> 00:24:51,840
automatic fashion so that you don't have

00:24:50,159 --> 00:24:56,240
to deal with

00:24:51,840 --> 00:24:58,400
them separately and it's also

00:24:56,240 --> 00:24:59,440
norm that you don't sometimes you don't

00:24:58,400 --> 00:25:02,000
actually like

00:24:59,440 --> 00:25:03,279
ingest all the raw data in its in its

00:25:02,000 --> 00:25:04,720
primitive form

00:25:03,279 --> 00:25:06,799
but you would have to apply some

00:25:04,720 --> 00:25:09,200
filtering or transformations for example

00:25:06,799 --> 00:25:10,880
you might need to remove some pa columns

00:25:09,200 --> 00:25:12,320
uh all these needs to be happened on the

00:25:10,880 --> 00:25:13,440
source data before you write the hoodie

00:25:12,320 --> 00:25:15,840
table so you need

00:25:13,440 --> 00:25:17,520
some framework where you can apply these

00:25:15,840 --> 00:25:20,159
kind of transformations

00:25:17,520 --> 00:25:22,000
and more often than not there are going

00:25:20,159 --> 00:25:24,960
to be multiple tables

00:25:22,000 --> 00:25:25,679
in your data pipeline in your data lake

00:25:24,960 --> 00:25:27,679
basically

00:25:25,679 --> 00:25:29,279
and one of them and you need to make

00:25:27,679 --> 00:25:31,520
sure that you're able to ingest

00:25:29,279 --> 00:25:33,679
and have an etl framework that can that

00:25:31,520 --> 00:25:36,880
can provide

00:25:33,679 --> 00:25:39,039
like basically a new data to uh each of

00:25:36,880 --> 00:25:42,000
these cables

00:25:39,039 --> 00:25:43,600
so hoodie comes with a delta streamer

00:25:42,000 --> 00:25:45,919
it's an ingestion framework which takes

00:25:43,600 --> 00:25:48,400
care of all these above functionalities

00:25:45,919 --> 00:25:50,880
it supports popular upstream sources

00:25:48,400 --> 00:25:53,200
like kafka dfs log files

00:25:50,880 --> 00:25:53,919
and even upstream od tables if you want

00:25:53,200 --> 00:25:56,960
to build

00:25:53,919 --> 00:26:00,559
a chained pipeline in your data lake

00:25:56,960 --> 00:26:02,480
uh it supports continuous streaming and

00:26:00,559 --> 00:26:03,760
mode for like a streaming kind of an

00:26:02,480 --> 00:26:06,720
ingestion or in

00:26:03,760 --> 00:26:10,559
or in batch mode like run once if you

00:26:06,720 --> 00:26:13,760
want to do a batch emission job

00:26:10,559 --> 00:26:14,240
uh so we can um like we look into some

00:26:13,760 --> 00:26:17,360
streaming

00:26:14,240 --> 00:26:18,080
just a little more in depth and there

00:26:17,360 --> 00:26:21,200
are basically

00:26:18,080 --> 00:26:21,919
some use cases uh like uh building near

00:26:21,200 --> 00:26:25,279
real-time

00:26:21,919 --> 00:26:26,799
dashboards use cases like fro

00:26:25,279 --> 00:26:28,480
or risk where you want to do some

00:26:26,799 --> 00:26:30,559
business anomaly detection

00:26:28,480 --> 00:26:32,480
which requires like a near real-time

00:26:30,559 --> 00:26:35,440
data freshness

00:26:32,480 --> 00:26:36,320
near real time gear refers to i mean a

00:26:35,440 --> 00:26:39,200
latency of

00:26:36,320 --> 00:26:39,919
around five minutes uh early would be

00:26:39,200 --> 00:26:42,559
too long

00:26:39,919 --> 00:26:44,480
for such cases and traditionally like

00:26:42,559 --> 00:26:46,960
specialized databases have been the

00:26:44,480 --> 00:26:48,960
go-to systems for such use cases

00:26:46,960 --> 00:26:52,000
uh so even though that is a powerful

00:26:48,960 --> 00:26:55,679
ecosystem on the data lake side

00:26:52,000 --> 00:26:57,279
with query engines sql support

00:26:55,679 --> 00:26:59,200
notebooks and other things even with

00:26:57,279 --> 00:27:02,720
such a support like data

00:26:59,200 --> 00:27:02,720
lake has often been overlooked

00:27:03,600 --> 00:27:11,200
so hoodies merge on read design

00:27:07,520 --> 00:27:14,640
with the structured streaming support

00:27:11,200 --> 00:27:16,159
delta streamer and the incremental

00:27:14,640 --> 00:27:18,480
processing capabilities that hoodie

00:27:16,159 --> 00:27:22,240
provides kind of helps

00:27:18,480 --> 00:27:24,080
achieve these low sla you need it for

00:27:22,240 --> 00:27:27,120
these business use cases and that way

00:27:24,080 --> 00:27:30,480
you can you can think of using the

00:27:27,120 --> 00:27:32,320
the data lake as a provider for

00:27:30,480 --> 00:27:33,600
solving these kind of business use cases

00:27:32,320 --> 00:27:35,520
i don't need to go to specialized

00:27:33,600 --> 00:27:38,640
databases

00:27:35,520 --> 00:27:40,159
for for doing such some of these

00:27:38,640 --> 00:27:42,559
for supporting some of these use cases

00:27:40,159 --> 00:27:42,559
basically

00:27:43,120 --> 00:27:47,600
so now that we looked at the features

00:27:45,840 --> 00:27:50,080
provided by houdi for

00:27:47,600 --> 00:27:51,120
managing your pipeline let's look at

00:27:50,080 --> 00:27:54,640
what it takes to

00:27:51,120 --> 00:27:55,919
migrate an existing parquet data set to

00:27:54,640 --> 00:27:58,399
hoodie

00:27:55,919 --> 00:27:59,440
there are a few ways to bootstrap but

00:27:58,399 --> 00:28:01,760
first let's look at

00:27:59,440 --> 00:28:02,960
the record structure maintained by houdi

00:28:01,760 --> 00:28:05,039
hoodie maintains

00:28:02,960 --> 00:28:06,799
raw level metadata to support efficient

00:28:05,039 --> 00:28:09,440
upsells and incremental views

00:28:06,799 --> 00:28:11,039
we need to generate these metadata so in

00:28:09,440 --> 00:28:12,559
this case if you look at it

00:28:11,039 --> 00:28:15,039
there's this commit time there are other

00:28:12,559 --> 00:28:17,279
few other fields that that are part of

00:28:15,039 --> 00:28:18,960
what we call as pro metadata that hoodie

00:28:17,279 --> 00:28:21,120
maintains

00:28:18,960 --> 00:28:23,039
we need to have these metadata for

00:28:21,120 --> 00:28:25,600
hoodie to provide absurd and incremental

00:28:23,039 --> 00:28:25,600
primitives

00:28:26,880 --> 00:28:32,399
one way to to build a parquet

00:28:30,399 --> 00:28:34,559
uh so hoodie data set is to basically

00:28:32,399 --> 00:28:36,799
read the data set in parking whole or

00:28:34,559 --> 00:28:37,760
in parts and write them to new location

00:28:36,799 --> 00:28:39,200
in honey format

00:28:37,760 --> 00:28:41,279
this will be like a one-time course that

00:28:39,200 --> 00:28:44,399
you will do

00:28:41,279 --> 00:28:45,200
but you can also uh like if you think

00:28:44,399 --> 00:28:46,480
that you are

00:28:45,200 --> 00:28:48,320
you are this one time causes like

00:28:46,480 --> 00:28:50,240
prohibitive uh you can

00:28:48,320 --> 00:28:52,720
also do it in an incremental fashion

00:28:50,240 --> 00:28:53,840
where uh there are like new partitions

00:28:52,720 --> 00:28:55,840
that are getting added

00:28:53,840 --> 00:28:58,799
which you can write it in hoodie format

00:28:55,840 --> 00:29:01,919
but keep the older partitions intact

00:28:58,799 --> 00:29:03,200
in in read-only mode and untouched so

00:29:01,919 --> 00:29:06,399
hoodie will be able to

00:29:03,200 --> 00:29:08,000
seamlessly handle uh this case

00:29:06,399 --> 00:29:10,159
when you want to read from such a data

00:29:08,000 --> 00:29:13,520
set so in this case you have an example

00:29:10,159 --> 00:29:13,919
where the data set is date partitioned

00:29:13,520 --> 00:29:16,159
and

00:29:13,919 --> 00:29:17,840
some of the old days is in non hoodie

00:29:16,159 --> 00:29:18,720
partitions and one point in time you

00:29:17,840 --> 00:29:20,640
want to

00:29:18,720 --> 00:29:22,159
let's say move to hoodie so you don't

00:29:20,640 --> 00:29:24,000
have to do bootstrap you can start

00:29:22,159 --> 00:29:27,039
writing to new partitions with

00:29:24,000 --> 00:29:29,360
with hoodie primitives and um

00:29:27,039 --> 00:29:31,360
yeah so this essentially basically

00:29:29,360 --> 00:29:34,720
there's a trade-off if you can see here

00:29:31,360 --> 00:29:34,720
uh one is uh

00:29:34,880 --> 00:29:38,480
like you take a one-time bootstrap cost

00:29:37,120 --> 00:29:39,760
um

00:29:38,480 --> 00:29:41,360
but and then you get like all the

00:29:39,760 --> 00:29:42,880
benefits that you get from foodie that's

00:29:41,360 --> 00:29:44,559
the other one is like you you don't want

00:29:42,880 --> 00:29:47,760
to take the cost and you want to

00:29:44,559 --> 00:29:49,520
do a partial

00:29:47,760 --> 00:29:51,039
bootstrapping in this case a partial

00:29:49,520 --> 00:29:54,240
migration in this case

00:29:51,039 --> 00:29:57,360
but you will miss out of absurd features

00:29:54,240 --> 00:30:01,840
uh in in the old partitions

00:29:57,360 --> 00:30:01,840
but i mean it is a new partition

00:30:02,320 --> 00:30:05,760
a quick reminder we are 30 minutes into

00:30:04,240 --> 00:30:08,640
the presentation sure

00:30:05,760 --> 00:30:09,039
sure so we have a we have introduced a

00:30:08,640 --> 00:30:10,880
new

00:30:09,039 --> 00:30:13,039
bootstrap mechanism which uh doesn't

00:30:10,880 --> 00:30:14,720
require uh complete rewrites

00:30:13,039 --> 00:30:16,080
uh but the same time provides audi

00:30:14,720 --> 00:30:19,520
subset support

00:30:16,080 --> 00:30:20,240
uh the idea is to break a hoodie file to

00:30:19,520 --> 00:30:21,840
two files

00:30:20,240 --> 00:30:23,520
uh one is called the hoodie skeleton

00:30:21,840 --> 00:30:26,559
file which contains all the

00:30:23,520 --> 00:30:27,760
metadata files and there's this other

00:30:26,559 --> 00:30:31,200
file which is the actual

00:30:27,760 --> 00:30:32,960
existing uh parquet file and uh

00:30:31,200 --> 00:30:35,279
hoodie uh maintains an auxiliary

00:30:32,960 --> 00:30:36,960
structures uh to link these two files

00:30:35,279 --> 00:30:38,799
so that way when you want to bootstrap

00:30:36,960 --> 00:30:39,279
you don't have to rewrite the whole data

00:30:38,799 --> 00:30:42,000
set

00:30:39,279 --> 00:30:42,559
but just create this metadata skeleton

00:30:42,000 --> 00:30:45,919
file

00:30:42,559 --> 00:30:46,320
for each of these data files uh this way

00:30:45,919 --> 00:30:48,240
it's

00:30:46,320 --> 00:30:49,600
bootstrapping is very fast and you don't

00:30:48,240 --> 00:30:52,640
need to

00:30:49,600 --> 00:30:53,279
and hoodie internally manages uh like

00:30:52,640 --> 00:30:55,120
providing

00:30:53,279 --> 00:30:56,640
absurd and incremental providers on such

00:30:55,120 --> 00:30:58,159
a data set so this way

00:30:56,640 --> 00:30:59,840
bootstrap is faster and you also get the

00:30:58,159 --> 00:31:03,200
upside functionalities

00:30:59,840 --> 00:31:05,039
so quickly i'm moving quickly on to the

00:31:03,200 --> 00:31:07,120
uh upcoming features again like this is

00:31:05,039 --> 00:31:10,559
with respect to how it's going to help

00:31:07,120 --> 00:31:12,640
the building and efficient data lake um

00:31:10,559 --> 00:31:14,320
so we'll start with like uh quickly like

00:31:12,640 --> 00:31:16,000
so as we mentioned in the previous

00:31:14,320 --> 00:31:18,799
slides uh julius

00:31:16,000 --> 00:31:20,320
uh like maintains a lot of metadata like

00:31:18,799 --> 00:31:23,519
one is at row level

00:31:20,320 --> 00:31:24,240
uh fields and it also maintains uh bloom

00:31:23,519 --> 00:31:27,600
filters

00:31:24,240 --> 00:31:30,640
range statistics and all the timeline um

00:31:27,600 --> 00:31:31,519
related uh metadata that it keeps uh but

00:31:30,640 --> 00:31:33,919
it primarily

00:31:31,519 --> 00:31:35,600
uses this information to speed up record

00:31:33,919 --> 00:31:38,880
key lookups for offsets

00:31:35,600 --> 00:31:42,480
uh reduce file listings and supporting

00:31:38,880 --> 00:31:44,480
incremental parameters

00:31:42,480 --> 00:31:46,480
but we can actually do a better with

00:31:44,480 --> 00:31:47,840
this metadata so we can organize file

00:31:46,480 --> 00:31:51,360
listing metadata

00:31:47,840 --> 00:31:53,840
uh so that we can totally avoid uh

00:31:51,360 --> 00:31:54,880
file listings which could be slow we can

00:31:53,840 --> 00:31:57,679
also extend

00:31:54,880 --> 00:31:58,559
uh the range statistics that we have

00:31:57,679 --> 00:32:00,960
already

00:31:58,559 --> 00:32:02,880
and reorganize them so that we can

00:32:00,960 --> 00:32:04,159
basically maintain column indexes for

00:32:02,880 --> 00:32:06,559
any any columns

00:32:04,159 --> 00:32:08,320
to speed up query planning and uh faster

00:32:06,559 --> 00:32:11,279
lookups

00:32:08,320 --> 00:32:12,320
uh this can also be done in in a sparse

00:32:11,279 --> 00:32:16,240
way or in

00:32:12,320 --> 00:32:18,559
or in and dense way as well

00:32:16,240 --> 00:32:20,559
uh but if you see some of these metadata

00:32:18,559 --> 00:32:22,320
could be significantly large and scales

00:32:20,559 --> 00:32:25,519
in footprint with data

00:32:22,320 --> 00:32:26,880
um also uh metadata and data need to be

00:32:25,519 --> 00:32:29,120
consistent with each other

00:32:26,880 --> 00:32:31,760
um so what that basically means like any

00:32:29,120 --> 00:32:33,519
partial failures that happens in data

00:32:31,760 --> 00:32:35,760
needs to be handled in the other method

00:32:33,519 --> 00:32:38,960
as well so they need to be

00:32:35,760 --> 00:32:40,000
operated in an atomic fashion so the

00:32:38,960 --> 00:32:42,960
question is

00:32:40,000 --> 00:32:43,919
how do we design storage for such

00:32:42,960 --> 00:32:47,840
metadata

00:32:43,919 --> 00:32:51,600
which would scale and also provide

00:32:47,840 --> 00:32:52,799
such guarantees okay a simple and

00:32:51,600 --> 00:32:54,480
elegant solution is to

00:32:52,799 --> 00:32:56,559
treat metadata in the same way as we

00:32:54,480 --> 00:32:58,960
treat data

00:32:56,559 --> 00:33:00,559
basically make it an internal table and

00:32:58,960 --> 00:33:02,399
change commit protocol such that

00:33:00,559 --> 00:33:03,679
both data and metadata table is

00:33:02,399 --> 00:33:06,399
consistent

00:33:03,679 --> 00:33:07,679
so if you look at this example here

00:33:06,399 --> 00:33:09,600
let's assume this is a

00:33:07,679 --> 00:33:11,039
base path for a data set and a

00:33:09,600 --> 00:33:12,480
conceptual od table would look like

00:33:11,039 --> 00:33:14,559
something like this it has

00:33:12,480 --> 00:33:16,159
um many data partitions which where it

00:33:14,559 --> 00:33:18,480
contains all the data files

00:33:16,159 --> 00:33:19,440
it also has a hoodie folder where it

00:33:18,480 --> 00:33:21,679
keeps the timeline

00:33:19,440 --> 00:33:22,720
of all the operations that happened and

00:33:21,679 --> 00:33:25,840
in and

00:33:22,720 --> 00:33:26,320
here we are going to maintain we're

00:33:25,840 --> 00:33:29,600
creating

00:33:26,320 --> 00:33:32,960
an internal table called metadata table

00:33:29,600 --> 00:33:34,880
which again would support

00:33:32,960 --> 00:33:36,080
absurd and all other operations that

00:33:34,880 --> 00:33:38,720
hoodie provides

00:33:36,080 --> 00:33:39,600
but this is for updating the metadata

00:33:38,720 --> 00:33:42,240
itself

00:33:39,600 --> 00:33:43,679
so um in the case of file listing here

00:33:42,240 --> 00:33:46,320
the metadata files is going to

00:33:43,679 --> 00:33:47,679
uh point to uh which you're going to

00:33:46,320 --> 00:33:50,559
have the file parts which is

00:33:47,679 --> 00:33:52,240
going to point to the data files and you

00:33:50,559 --> 00:33:53,840
can simply see that like how this model

00:33:52,240 --> 00:33:54,559
can actually extend to any of the

00:33:53,840 --> 00:33:56,320
metadata

00:33:54,559 --> 00:33:57,600
for columnar index or any other index

00:33:56,320 --> 00:33:59,840
flag you can

00:33:57,600 --> 00:34:00,640
um you can you can employ the same

00:33:59,840 --> 00:34:04,640
strategy to

00:34:00,640 --> 00:34:07,200
to maintain those metadata files

00:34:04,640 --> 00:34:07,840
so in summary actually this this uh

00:34:07,200 --> 00:34:11,280
model

00:34:07,840 --> 00:34:13,119
uh uh provides uh

00:34:11,280 --> 00:34:15,200
it's kind of very elegant and you get

00:34:13,119 --> 00:34:17,119
all the hoodie machinery benefits for

00:34:15,200 --> 00:34:18,240
for free here you don't have to reinvent

00:34:17,119 --> 00:34:20,000
the wheel and

00:34:18,240 --> 00:34:21,280
do something different for managing

00:34:20,000 --> 00:34:24,720
metadata as opposed to

00:34:21,280 --> 00:34:26,159
data uh like rollbacks and any partial

00:34:24,720 --> 00:34:28,159
failures are handled in an atomic

00:34:26,159 --> 00:34:30,399
fashion and uh

00:34:28,159 --> 00:34:32,159
the the different types are also

00:34:30,399 --> 00:34:34,480
extendable like meaning like you

00:34:32,159 --> 00:34:37,679
uh it's extended to many different

00:34:34,480 --> 00:34:41,119
metadata formats as well

00:34:37,679 --> 00:34:42,159
so quickly moving to um like the final

00:34:41,119 --> 00:34:45,839
part of it it's like

00:34:42,159 --> 00:34:47,839
uh re-clustering data so reorganizing

00:34:45,839 --> 00:34:49,599
this data to optimize for query is often

00:34:47,839 --> 00:34:51,919
necessary to speed up

00:34:49,599 --> 00:34:52,879
common case or business critical queries

00:34:51,919 --> 00:34:55,440
in just pattern

00:34:52,879 --> 00:34:56,240
and query patterns often differs you

00:34:55,440 --> 00:34:58,880
would want to

00:34:56,240 --> 00:34:59,839
reorganize such that you can merge many

00:34:58,880 --> 00:35:03,119
small files

00:34:59,839 --> 00:35:05,040
to a large file and this can be done in

00:35:03,119 --> 00:35:05,680
an as part of write operation but it's

00:35:05,040 --> 00:35:08,160
going to

00:35:05,680 --> 00:35:10,000
basically uh slow down your rights your

00:35:08,160 --> 00:35:12,560
injection latency

00:35:10,000 --> 00:35:13,119
and you don't want that so you want to

00:35:12,560 --> 00:35:15,520
design

00:35:13,119 --> 00:35:16,960
a model where you can basically do this

00:35:15,520 --> 00:35:18,240
as a background operation in an

00:35:16,960 --> 00:35:21,520
asynchronous fashion

00:35:18,240 --> 00:35:24,560
uh without without uh blocking the

00:35:21,520 --> 00:35:26,560
rights so hoodie already uh

00:35:24,560 --> 00:35:28,240
supports uh asynchronous machinery which

00:35:26,560 --> 00:35:30,160
is used to uh this is

00:35:28,240 --> 00:35:31,760
just how compaction is basically

00:35:30,160 --> 00:35:34,320
performed right now

00:35:31,760 --> 00:35:35,839
um so it basically uh does this in a

00:35:34,320 --> 00:35:36,560
non-blocking fashion without blocking

00:35:35,839 --> 00:35:38,240
the rights

00:35:36,560 --> 00:35:40,480
and there's no need for any central

00:35:38,240 --> 00:35:41,839
coordination and we you we are going to

00:35:40,480 --> 00:35:44,079
use the same missionary

00:35:41,839 --> 00:35:44,960
for extend and extend this model to

00:35:44,079 --> 00:35:48,320
support

00:35:44,960 --> 00:35:48,320
uh reorganizing the data

00:35:48,480 --> 00:35:51,760
uh so finally like uh last part before

00:35:51,040 --> 00:35:54,560
end of the

00:35:51,760 --> 00:35:57,280
meeting so uh so these are the features

00:35:54,560 --> 00:35:59,520
that we are

00:35:57,280 --> 00:36:01,119
i mean it's either available or in the

00:35:59,520 --> 00:36:01,839
works uh the first one is like we

00:36:01,119 --> 00:36:05,119
recently i

00:36:01,839 --> 00:36:06,240
landed uh the hive style insert

00:36:05,119 --> 00:36:09,040
overwrite mode

00:36:06,240 --> 00:36:11,200
for overwriting uh either hoodie

00:36:09,040 --> 00:36:14,800
complete cable or partition

00:36:11,200 --> 00:36:15,599
uh spark 3.0 and think uh support is

00:36:14,800 --> 00:36:17,520
underway

00:36:15,599 --> 00:36:20,960
and there's also work happening to

00:36:17,520 --> 00:36:23,119
support virtual record keys in hoodie

00:36:20,960 --> 00:36:24,560
so yeah so these are interesting times

00:36:23,119 --> 00:36:25,680
uh in hoodie and if you want to

00:36:24,560 --> 00:36:27,359
contribute to od

00:36:25,680 --> 00:36:30,880
please let us know we'll be happy to

00:36:27,359 --> 00:36:30,880
help you on board thank you

00:36:32,720 --> 00:36:45,280
any questions

00:36:41,440 --> 00:36:46,480
okay so yeah i will share the we will

00:36:45,280 --> 00:36:48,320
share this presentation

00:36:46,480 --> 00:36:49,920
uh uh after this meeting uh all right

00:36:48,320 --> 00:36:51,599
let me

00:36:49,920 --> 00:36:53,920
see what can you actually share the link

00:36:51,599 --> 00:36:54,960
and say keep put it here as well if you

00:36:53,920 --> 00:36:57,040
can okay

00:36:54,960 --> 00:36:58,800
sure i will upload it to the google

00:36:57,040 --> 00:36:59,760
drive link or something which we got in

00:36:58,800 --> 00:37:01,839
the email okay

00:36:59,760 --> 00:37:03,359
so yeah i think uh so the other question

00:37:01,839 --> 00:37:05,920
is about uh

00:37:03,359 --> 00:37:07,680
apache houdini and apache high school um

00:37:05,920 --> 00:37:09,839
so um

00:37:07,680 --> 00:37:10,720
i mean there are definitely some common

00:37:09,839 --> 00:37:13,440
functionalities

00:37:10,720 --> 00:37:13,839
uh there in terms of how the you know in

00:37:13,440 --> 00:37:16,160
what

00:37:13,839 --> 00:37:17,200
what in terms of what it uh what we try

00:37:16,160 --> 00:37:21,040
to achieve

00:37:17,200 --> 00:37:22,960
um but uh i mean from hudi's perspective

00:37:21,040 --> 00:37:24,079
we wanted it's just not like uh the

00:37:22,960 --> 00:37:25,599
metadata format

00:37:24,079 --> 00:37:28,079
itself right so basically it's like we

00:37:25,599 --> 00:37:32,400
are planning to have a

00:37:28,079 --> 00:37:34,160
like a complete framework where we can

00:37:32,400 --> 00:37:35,440
manage your entire data lake where you

00:37:34,160 --> 00:37:38,400
can perform

00:37:35,440 --> 00:37:38,960
streaming ingestion provide this kind of

00:37:38,400 --> 00:37:41,599
uh

00:37:38,960 --> 00:37:43,599
reorganizing the data in a way where um

00:37:41,599 --> 00:37:46,160
which you can speed up the query so

00:37:43,599 --> 00:37:46,640
um so some of the parts like in terms of

00:37:46,160 --> 00:37:48,400
uh

00:37:46,640 --> 00:37:50,000
where apache specified you know nothing

00:37:48,400 --> 00:37:50,560
at least like they're trying to replace

00:37:50,000 --> 00:37:53,440
the

00:37:50,560 --> 00:37:55,040
high metastore um so there are like some

00:37:53,440 --> 00:37:56,720
commonalities in it but they're like

00:37:55,040 --> 00:37:58,880
like fundamentally we see it as a

00:37:56,720 --> 00:38:02,320
different uh uh different uh

00:37:58,880 --> 00:38:05,040
pieces of uh uh the data lake puzzle

00:38:02,320 --> 00:38:05,040
is that how

00:38:11,680 --> 00:38:18,079
so yeah i think that's like uh they also

00:38:14,880 --> 00:38:20,880
support uh upsets and uh

00:38:18,079 --> 00:38:22,560
uh in just the data uh so again but i

00:38:20,880 --> 00:38:24,720
would i would like put it in a way like

00:38:22,560 --> 00:38:27,839
we are uh

00:38:24,720 --> 00:38:31,119
we have our own uh way of uh uh like

00:38:27,839 --> 00:38:32,000
view of uh uh like project where we

00:38:31,119 --> 00:38:34,320
wanted to support

00:38:32,000 --> 00:38:36,079
these like many different uh like as a

00:38:34,320 --> 00:38:38,079
platform what we wanted to add

00:38:36,079 --> 00:38:39,760
uh so that i think there's like a big

00:38:38,079 --> 00:38:41,200
divergence there in terms of

00:38:39,760 --> 00:38:42,400
uh the functionalities that we want to

00:38:41,200 --> 00:38:44,160
support and we don't want to just stick

00:38:42,400 --> 00:38:45,280
with spark we want to support other

00:38:44,160 --> 00:38:48,480
process engines like spa

00:38:45,280 --> 00:38:50,079
flink and other things so um that is uh

00:38:48,480 --> 00:38:53,680
that's like traditionally like that

00:38:50,079 --> 00:38:53,680
that's a dichotomy there basically

00:38:56,240 --> 00:38:59,839
and uh okay if i can also add one one

00:38:58,400 --> 00:39:00,560
feature that's like kind of unique to

00:38:59,839 --> 00:39:03,599
hoodies like

00:39:00,560 --> 00:39:06,640
incremental processing basically um so

00:39:03,599 --> 00:39:08,240
uh just like kind of novel to uh

00:39:06,640 --> 00:39:10,079
hoodie and none of the other systems

00:39:08,240 --> 00:39:12,000
actually provide like if you think of it

00:39:10,079 --> 00:39:15,119
as like it's a powerful primitive

00:39:12,000 --> 00:39:15,760
you can if you want to build etl chains

00:39:15,119 --> 00:39:17,200
right your

00:39:15,760 --> 00:39:19,040
your data lake is not going to be just

00:39:17,200 --> 00:39:21,680
like one one single

00:39:19,040 --> 00:39:22,480
data table right it's going to be as uh

00:39:21,680 --> 00:39:24,960
like a

00:39:22,480 --> 00:39:26,800
collection of tables where with with

00:39:24,960 --> 00:39:28,160
some kind of a dac dependencies between

00:39:26,800 --> 00:39:31,920
them

00:39:28,160 --> 00:39:33,359
so uh like every time you want to either

00:39:31,920 --> 00:39:35,280
re-rate the whole table or rate the

00:39:33,359 --> 00:39:36,800
whole partition uh based on your

00:39:35,280 --> 00:39:38,720
upstream data changes

00:39:36,800 --> 00:39:41,119
you that's like not there needs to be an

00:39:38,720 --> 00:39:43,200
efficient primitive to support that

00:39:41,119 --> 00:39:44,880
and hoodie provides that in the sense

00:39:43,200 --> 00:39:47,040
that like it can so it's a change log

00:39:44,880 --> 00:39:49,920
stream where

00:39:47,040 --> 00:39:51,359
you can basically uh just read the

00:39:49,920 --> 00:39:53,040
changes that happened in the upstream

00:39:51,359 --> 00:39:56,720
table

00:39:53,040 --> 00:39:57,520
and just apply or process those changes

00:39:56,720 --> 00:40:00,000
and

00:39:57,520 --> 00:40:01,200
keep your downstream table up to date uh

00:40:00,000 --> 00:40:04,800
without having to

00:40:01,200 --> 00:40:05,920
uh read like a large footprint of your

00:40:04,800 --> 00:40:07,760
upstream table

00:40:05,920 --> 00:40:09,200
and relating the whole table that's like

00:40:07,760 --> 00:40:12,480
much more

00:40:09,200 --> 00:40:12,480
inefficient in our opinion

00:40:14,560 --> 00:40:20,000
okay so any other questions

00:40:20,640 --> 00:40:25,680
okay uh

00:40:24,240 --> 00:40:27,520
uh thanks a lot guys thanks thank you

00:40:25,680 --> 00:40:29,520
everyone

00:40:27,520 --> 00:40:30,720
let me put the link actually so it's

00:40:29,520 --> 00:40:39,839
shareable i believe so

00:40:30,720 --> 00:40:39,839
let me share and uh

00:40:39,920 --> 00:40:43,680
i hope this is readable um but we will

00:40:42,720 --> 00:40:46,640
share it in the

00:40:43,680 --> 00:40:46,960
in the apache uh con as well so that

00:40:46,640 --> 00:40:50,839
like

00:40:46,960 --> 00:40:53,839
it will be available when the video is

00:40:50,839 --> 00:40:53,839
available

00:41:16,160 --> 00:41:18,240

YouTube URL: https://www.youtube.com/watch?v=yC5TXyOownw


