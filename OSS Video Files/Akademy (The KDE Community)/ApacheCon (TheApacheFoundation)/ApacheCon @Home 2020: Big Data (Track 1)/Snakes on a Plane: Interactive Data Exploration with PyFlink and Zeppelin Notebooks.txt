Title: Snakes on a Plane: Interactive Data Exploration with PyFlink and Zeppelin Notebooks
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Snakes on a Plane: Interactive Data Exploration with PyFlink and Zeppelin Notebooks
Marta Paes

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Stream processing has fundamentally changed the way we build and think about data pipelines — but the technologies that unlock the value of this powerful paradigm haven’t always been friendly to non-Java/Scala developers. Apache Flink has recently introduced PyFlink, allowing developers to tap into streaming data in real-time with the flexibility of Python and its wide ecosystem for data analytics and Machine Learning. In this talk, we will explore the basics of PyFlink and showcase how developers can make use of a simple tool like interactive notebooks (Apache Zeppelin) to unleash the full power of an advanced stream processor like Flink.

Marta is a Developer Advocate at Ververica (formerly data Artisans) and a contributor to Apache Flink. After finding her mojo in open source, she is committed to making sense of Data Engineering through the eyes of those using its by-products. Marta holds a Master’s in Biomedical Engineering, where she developed a particular taste for multi-dimensional data visualization, and previously worked as a Data Warehouse Engineer at Zalando and Accenture.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,760 --> 00:00:29,439
okay

00:00:26,320 --> 00:00:30,320
hi everyone welcome to the session thank

00:00:29,439 --> 00:00:33,440
you so much for

00:00:30,320 --> 00:00:35,760
taking the time my name is marta

00:00:33,440 --> 00:00:37,360
i'm a developer advocate and today i

00:00:35,760 --> 00:00:38,160
will be talking to you a little bit

00:00:37,360 --> 00:00:39,680
about

00:00:38,160 --> 00:00:44,879
interactive data exploration with

00:00:39,680 --> 00:00:46,800
pythlink and zeppelin notebooks

00:00:44,879 --> 00:00:48,800
just let me know in the chat if you can

00:00:46,800 --> 00:00:49,760
hear me i think i'm not muted but just

00:00:48,800 --> 00:00:51,840
in case

00:00:49,760 --> 00:00:53,600
uh something's wrong just ping me in the

00:00:51,840 --> 00:00:56,160
chat

00:00:53,600 --> 00:00:57,440
uh so for those of you that have never

00:00:56,160 --> 00:01:00,399
heard about

00:00:57,440 --> 00:01:01,760
it's a company i work for you might know

00:01:00,399 --> 00:01:04,320
it as the company

00:01:01,760 --> 00:01:06,240
that of the folks that created apache

00:01:04,320 --> 00:01:09,760
flink

00:01:06,240 --> 00:01:10,000
and what viverica does besides working

00:01:09,760 --> 00:01:12,560
on

00:01:10,000 --> 00:01:13,280
open source flank is offer enterprise

00:01:12,560 --> 00:01:16,640
product

00:01:13,280 --> 00:01:17,360
that makes it a bit easier to to deal

00:01:16,640 --> 00:01:20,159
and tame

00:01:17,360 --> 00:01:21,200
uh your streaming applications in in

00:01:20,159 --> 00:01:23,360
production

00:01:21,200 --> 00:01:25,360
so and since the beginning of last year

00:01:23,360 --> 00:01:28,720
the company is part of the alibaba group

00:01:25,360 --> 00:01:30,799
and you may or may not know that alibaba

00:01:28,720 --> 00:01:31,520
is one of the biggest users of flank out

00:01:30,799 --> 00:01:34,000
there

00:01:31,520 --> 00:01:36,079
and they're also one of the biggest

00:01:34,000 --> 00:01:38,400
contributors to open source flank these

00:01:36,079 --> 00:01:38,400
days

00:01:38,880 --> 00:01:42,560
so if it's you're also your first time

00:01:41,200 --> 00:01:45,600
hearing about flink

00:01:42,560 --> 00:01:49,759
uh i'm going to try to give a really

00:01:45,600 --> 00:01:51,600
quick and really simple uh

00:01:49,759 --> 00:01:53,759
introduction to what it does so in the

00:01:51,600 --> 00:01:56,399
simplest form uh how you can think about

00:01:53,759 --> 00:01:58,479
flink it's just something that

00:01:56,399 --> 00:01:59,520
allows you to continuously process data

00:01:58,479 --> 00:02:02,799
that comes from

00:01:59,520 --> 00:02:04,719
disparate amount of data sources then it

00:02:02,799 --> 00:02:06,880
allows you to do some computations and

00:02:04,719 --> 00:02:09,840
transformations on that data

00:02:06,880 --> 00:02:10,720
as data flows through flink it's also

00:02:09,840 --> 00:02:12,640
able to

00:02:10,720 --> 00:02:14,640
kind of remember the context of the data

00:02:12,640 --> 00:02:17,920
that it's processing

00:02:14,640 --> 00:02:19,040
um and yeah then it just lets you sync

00:02:17,920 --> 00:02:21,760
this data

00:02:19,040 --> 00:02:22,080
and to do whatever you need so it could

00:02:21,760 --> 00:02:25,599
be

00:02:22,080 --> 00:02:26,160
api calls you can just sync it to a file

00:02:25,599 --> 00:02:30,480
system

00:02:26,160 --> 00:02:30,480
updates to a database uh whatever

00:02:30,560 --> 00:02:35,519
and what makes flink really powerful is

00:02:33,440 --> 00:02:36,160
the way that it handles this context or

00:02:35,519 --> 00:02:40,000
the way it

00:02:36,160 --> 00:02:42,400
is able to remember these events

00:02:40,000 --> 00:02:44,879
and this gives you some really nice

00:02:42,400 --> 00:02:46,720
properties like low latency processing

00:02:44,879 --> 00:02:48,640
fault tolerance when when something

00:02:46,720 --> 00:02:51,440
blows up

00:02:48,640 --> 00:02:52,879
and still give you the highest level of

00:02:51,440 --> 00:02:54,400
guarantees that you can get which is

00:02:52,879 --> 00:02:57,440
exactly once

00:02:54,400 --> 00:03:00,560
and what makes flink really flexible

00:02:57,440 --> 00:03:03,040
is doing uh one at a time

00:03:00,560 --> 00:03:04,000
event processing so it's uh some people

00:03:03,040 --> 00:03:07,280
out there call it like

00:03:04,000 --> 00:03:10,319
a real stream processor uh

00:03:07,280 --> 00:03:13,040
but yeah so this uh

00:03:10,319 --> 00:03:14,640
this gives you a really because it's in

00:03:13,040 --> 00:03:18,400
if you think about it such a

00:03:14,640 --> 00:03:21,360
simple uh concept that it gives you

00:03:18,400 --> 00:03:23,360
a really good primer to cover a really

00:03:21,360 --> 00:03:25,280
wide range of use cases so

00:03:23,360 --> 00:03:27,280
from streaming analytics and machine

00:03:25,280 --> 00:03:30,319
learning to more classical

00:03:27,280 --> 00:03:33,840
uh stream processing pipelines to lambda

00:03:30,319 --> 00:03:33,840
style event driven applications

00:03:35,040 --> 00:03:40,879
and at the very base

00:03:38,400 --> 00:03:41,599
there are like these more classical like

00:03:40,879 --> 00:03:44,080
i said

00:03:41,599 --> 00:03:46,080
core stream processing use cases that

00:03:44,080 --> 00:03:48,959
really build on these primitives

00:03:46,080 --> 00:03:50,480
uh that that kind of uh composed links

00:03:48,959 --> 00:03:54,159
so like event streams

00:03:50,480 --> 00:03:56,400
uh states and time and

00:03:54,159 --> 00:03:57,439
these are use cases where engineers are

00:03:56,400 --> 00:03:59,360
really

00:03:57,439 --> 00:04:01,519
exploring these low-level primitives

00:03:59,360 --> 00:04:05,360
deep in the flink api stack

00:04:01,519 --> 00:04:07,280
so when you need to do very complex or

00:04:05,360 --> 00:04:10,400
very heavy computations and you have to

00:04:07,280 --> 00:04:12,480
do a lot of customization on your code

00:04:10,400 --> 00:04:14,159
and where the goal is really to maximize

00:04:12,480 --> 00:04:15,519
the performance reliability

00:04:14,159 --> 00:04:17,680
of whatever you're building so you

00:04:15,519 --> 00:04:18,560
really want to milk all the really good

00:04:17,680 --> 00:04:21,040
things

00:04:18,560 --> 00:04:22,800
that fling can offer you and some

00:04:21,040 --> 00:04:26,479
examples of these

00:04:22,800 --> 00:04:28,400
use cases are for example companies

00:04:26,479 --> 00:04:30,880
like netflix they're using flint to

00:04:28,400 --> 00:04:34,320
build their core data infrastructure

00:04:30,880 --> 00:04:37,120
so netflix uh has this uh

00:04:34,320 --> 00:04:38,960
internal data platform that i'm not

00:04:37,120 --> 00:04:41,440
forgetting the name of but

00:04:38,960 --> 00:04:43,199
they are processing three petabytes of

00:04:41,440 --> 00:04:45,919
events per day with flink

00:04:43,199 --> 00:04:48,160
you also have a lot of banks doing doing

00:04:45,919 --> 00:04:51,199
fraud detection

00:04:48,160 --> 00:04:52,720
fraud detection infrastructure built

00:04:51,199 --> 00:04:56,000
with flink like ing

00:04:52,720 --> 00:04:57,680
they have a machine learning based fraud

00:04:56,000 --> 00:04:58,880
detection infrastructure built with

00:04:57,680 --> 00:05:02,400
flink

00:04:58,880 --> 00:05:05,280
and other kind of use cases

00:05:02,400 --> 00:05:07,120
like at aws they do log analysis for

00:05:05,280 --> 00:05:11,120
service monitoring and

00:05:07,120 --> 00:05:14,560
anomaly detection

00:05:11,120 --> 00:05:16,880
and then there is uh

00:05:14,560 --> 00:05:19,120
the side of streaming analytics and

00:05:16,880 --> 00:05:22,000
machine learning

00:05:19,120 --> 00:05:23,360
and here flank is used a bit more on a

00:05:22,000 --> 00:05:26,639
higher level

00:05:23,360 --> 00:05:28,479
for or for more domain specific use

00:05:26,639 --> 00:05:30,960
cases that can be

00:05:28,479 --> 00:05:33,440
easily modeled with something uh like

00:05:30,960 --> 00:05:34,960
sql or python and like a more simple

00:05:33,440 --> 00:05:37,919
abstraction

00:05:34,960 --> 00:05:38,960
like dynamic tips and here the focus is

00:05:37,919 --> 00:05:42,240
not so much

00:05:38,960 --> 00:05:44,400
on not so much on on

00:05:42,240 --> 00:05:45,600
on really going deep on flink and really

00:05:44,400 --> 00:05:48,720
maxing it out

00:05:45,600 --> 00:05:50,479
but more like on logic to me to meet

00:05:48,720 --> 00:05:54,080
some business requirements

00:05:50,479 --> 00:05:56,400
and so you kind of want uh you kind of

00:05:54,080 --> 00:05:58,479
expect some things like uh state

00:05:56,400 --> 00:05:59,919
handling execution optimization to be

00:05:58,479 --> 00:06:02,319
handled by flink and you don't really

00:05:59,919 --> 00:06:04,800
want to think about it

00:06:02,319 --> 00:06:07,120
and these are use cases where you also

00:06:04,800 --> 00:06:08,880
might have

00:06:07,120 --> 00:06:10,880
you also might have requirements where

00:06:08,880 --> 00:06:12,800
you need to do some mixed batch

00:06:10,880 --> 00:06:15,120
and stream processing for example if you

00:06:12,800 --> 00:06:16,720
need to backfill some historical data

00:06:15,120 --> 00:06:19,840
into your pipelines

00:06:16,720 --> 00:06:21,759
and the goal here of using this more

00:06:19,840 --> 00:06:22,960
higher level abstractions in flank is to

00:06:21,759 --> 00:06:24,880
maximize

00:06:22,960 --> 00:06:26,319
uh developer speed and autonomy so

00:06:24,880 --> 00:06:28,720
basically

00:06:26,319 --> 00:06:29,759
you want to make users even if they

00:06:28,720 --> 00:06:31,680
don't use

00:06:29,759 --> 00:06:33,440
even if they don't know java you want to

00:06:31,680 --> 00:06:35,440
make

00:06:33,440 --> 00:06:38,880
you want to make users independent with

00:06:35,440 --> 00:06:41,759
whatever data needs they have

00:06:38,880 --> 00:06:42,720
and some examples of use cases on this

00:06:41,759 --> 00:06:44,960
side of things

00:06:42,720 --> 00:06:46,639
are for example you have weibo which is

00:06:44,960 --> 00:06:48,319
one of the biggest social media

00:06:46,639 --> 00:06:50,720
platforms in china

00:06:48,319 --> 00:06:53,039
they're using flink to do unified online

00:06:50,720 --> 00:06:55,520
and offline model training

00:06:53,039 --> 00:06:56,880
at uber they also they have their

00:06:55,520 --> 00:06:59,280
internal platform called

00:06:56,880 --> 00:07:00,240
athena x you may have you may have heard

00:06:59,280 --> 00:07:02,720
of it

00:07:00,240 --> 00:07:04,000
it just allows basically everyone in the

00:07:02,720 --> 00:07:07,039
company to just

00:07:04,000 --> 00:07:10,080
using puresql uh to build their own

00:07:07,039 --> 00:07:13,680
end-to-end streaming analytics pipelines

00:07:10,080 --> 00:07:15,840
and at criteo they also have

00:07:13,680 --> 00:07:17,360
they also have their feature generation

00:07:15,840 --> 00:07:20,160
platform

00:07:17,360 --> 00:07:20,160
built with flink

00:07:20,880 --> 00:07:27,759
and kind of a

00:07:24,000 --> 00:07:31,199
testimonial of how flexible and how

00:07:27,759 --> 00:07:32,720
wide the usage of flink is is that it

00:07:31,199 --> 00:07:34,000
really powers some of the largest

00:07:32,720 --> 00:07:36,240
companies in the world

00:07:34,000 --> 00:07:37,680
and serves really different industry

00:07:36,240 --> 00:07:40,560
verticals so you have

00:07:37,680 --> 00:07:42,960
things from entertainment like netflix

00:07:40,560 --> 00:07:44,720
all the way to agrotech with

00:07:42,960 --> 00:07:46,319
companies like john deerey everyone

00:07:44,720 --> 00:07:47,680
using flank for

00:07:46,319 --> 00:07:49,520
super different things really

00:07:47,680 --> 00:07:51,680
interesting use cases some of them using

00:07:49,520 --> 00:07:54,319
flink at a scale that is kind of

00:07:51,680 --> 00:07:54,319
mind-blowing

00:07:55,520 --> 00:07:59,599
but the problem it's not really a

00:07:57,840 --> 00:08:00,960
problem but the thing with flink is that

00:07:59,599 --> 00:08:03,120
it's jvm based

00:08:00,960 --> 00:08:04,479
right so for a long time it was never

00:08:03,120 --> 00:08:06,720
suited for

00:08:04,479 --> 00:08:08,560
anyone that didn't know how to code in

00:08:06,720 --> 00:08:12,319
java or skull

00:08:08,560 --> 00:08:14,240
and i don't think i need to convince

00:08:12,319 --> 00:08:16,080
anyone here of the importance of python

00:08:14,240 --> 00:08:19,840
these days for data processing

00:08:16,080 --> 00:08:23,039
or to convince anyone to switch to java

00:08:19,840 --> 00:08:26,080
anyways so but

00:08:23,039 --> 00:08:29,360
just in case just to make a case here um

00:08:26,080 --> 00:08:29,840
python is pretty stacked so it has a

00:08:29,360 --> 00:08:32,800
very

00:08:29,840 --> 00:08:34,159
mature analytics stack with very

00:08:32,800 --> 00:08:37,360
well-loved libraries

00:08:34,159 --> 00:08:40,479
like numpy panda scikit-learn

00:08:37,360 --> 00:08:41,360
and these libraries are really intuitive

00:08:40,479 --> 00:08:44,320
for users

00:08:41,360 --> 00:08:45,040
and they're actually pretty fast and a

00:08:44,320 --> 00:08:47,120
lot of

00:08:45,040 --> 00:08:48,640
a lot a lot of the newer libraries for

00:08:47,120 --> 00:08:50,640
machine learning natural language

00:08:48,640 --> 00:08:53,200
processing and this kind of domains

00:08:50,640 --> 00:08:54,080
more like in the data science space you

00:08:53,200 --> 00:08:57,120
see that they are

00:08:54,080 --> 00:08:59,920
either being um either being written

00:08:57,120 --> 00:09:00,800
in or they are built they're being built

00:08:59,920 --> 00:09:04,160
first for

00:09:00,800 --> 00:09:05,360
python users so there's a huge ecosystem

00:09:04,160 --> 00:09:08,000
around python

00:09:05,360 --> 00:09:10,399
uh especially for people doing data

00:09:08,000 --> 00:09:12,800
science

00:09:10,399 --> 00:09:13,440
and python is also pretty timeless so if

00:09:12,800 --> 00:09:16,720
you look

00:09:13,440 --> 00:09:18,560
at the most popular libraries i found

00:09:16,720 --> 00:09:20,240
this very curious because

00:09:18,560 --> 00:09:21,680
i had to kind of like do a bit of

00:09:20,240 --> 00:09:23,360
research because i haven't touched

00:09:21,680 --> 00:09:25,519
python in a while

00:09:23,360 --> 00:09:27,519
but if you look at the most popular

00:09:25,519 --> 00:09:28,560
libraries that developers use with

00:09:27,519 --> 00:09:31,760
python still

00:09:28,560 --> 00:09:32,480
these days in 2020 five out of the top

00:09:31,760 --> 00:09:35,760
six

00:09:32,480 --> 00:09:38,480
are older than 12 years or

00:09:35,760 --> 00:09:39,920
are more than 12 years old and python

00:09:38,480 --> 00:09:43,760
itself is actually

00:09:39,920 --> 00:09:43,760
way older than java too so

00:09:44,399 --> 00:09:50,640
and although newer libraries like

00:09:48,640 --> 00:09:53,839
tensorflow and that kind of stuff are

00:09:50,640 --> 00:09:56,959
already being built in a way that

00:09:53,839 --> 00:09:58,880
really can distribute

00:09:56,959 --> 00:10:01,680
compute the problem with these really

00:09:58,880 --> 00:10:03,440
older libraries is that

00:10:01,680 --> 00:10:05,440
they're mostly restricted to data that

00:10:03,440 --> 00:10:07,600
can fit in memory

00:10:05,440 --> 00:10:09,760
and are mostly designed to run in a

00:10:07,600 --> 00:10:12,160
single core of a cpu so

00:10:09,760 --> 00:10:13,040
this basically means that you're you're

00:10:12,160 --> 00:10:16,839
limited to

00:10:13,040 --> 00:10:19,200
whatever machine you're using to to run

00:10:16,839 --> 00:10:22,800
python

00:10:19,200 --> 00:10:25,360
and this is a this is a problem because

00:10:22,800 --> 00:10:27,680
i think we've all experienced it by now

00:10:25,360 --> 00:10:30,399
like these days data is just splitting

00:10:27,680 --> 00:10:32,240
out of uh everywhere constantly from

00:10:30,399 --> 00:10:33,279
different sources it's being produced

00:10:32,240 --> 00:10:36,000
faster

00:10:33,279 --> 00:10:38,000
and more formats and then it's also at

00:10:36,000 --> 00:10:41,760
the same time requiring you to

00:10:38,000 --> 00:10:44,720
extract value from it faster and

00:10:41,760 --> 00:10:46,480
for more different reasons for more uh

00:10:44,720 --> 00:10:49,839
different stakeholders so

00:10:46,480 --> 00:10:53,519
it kind of is a challenge

00:10:49,839 --> 00:10:55,200
and despite this and despite and despite

00:10:53,519 --> 00:10:55,839
that python doesn't really scale you

00:10:55,200 --> 00:10:58,880
still

00:10:55,839 --> 00:11:00,640
kind of want to use it uh and you

00:10:58,880 --> 00:11:02,959
also want to use these powerful

00:11:00,640 --> 00:11:07,760
libraries right and you want to use it

00:11:02,959 --> 00:11:07,760
at scale and uh in parallel

00:11:08,640 --> 00:11:12,880
and this is one of the motivations

00:11:10,399 --> 00:11:16,079
behind pipeline so pythlink

00:11:12,880 --> 00:11:18,800
is if you've heard maybe of

00:11:16,079 --> 00:11:19,839
pi spark before pi spark has a much

00:11:18,800 --> 00:11:23,120
bigger legacy

00:11:19,839 --> 00:11:25,680
pythlink is pretty recent but

00:11:23,120 --> 00:11:28,000
the motivation behind it is basically

00:11:25,680 --> 00:11:30,079
twofold so

00:11:28,000 --> 00:11:31,040
people in the community thought about

00:11:30,079 --> 00:11:34,240
starting to

00:11:31,040 --> 00:11:34,959
support python in flink because in one

00:11:34,240 --> 00:11:38,079
way

00:11:34,959 --> 00:11:41,360
flink is a really really powerful engine

00:11:38,079 --> 00:11:42,320
for large-scale large-scale data

00:11:41,360 --> 00:11:45,120
processing

00:11:42,320 --> 00:11:46,959
and so uh in one in one hand you want to

00:11:45,120 --> 00:11:48,000
expose all the functionality that you

00:11:46,959 --> 00:11:51,760
have in flink

00:11:48,000 --> 00:11:54,399
to users beyond the jvm

00:11:51,760 --> 00:11:56,320
and the second motivation is that you

00:11:54,399 --> 00:11:58,000
also want to distribute and scale the

00:11:56,320 --> 00:12:00,399
functionality of python

00:11:58,000 --> 00:12:01,760
through flink so you want to be able to

00:12:00,399 --> 00:12:04,880
run for example

00:12:01,760 --> 00:12:05,200
pandas and all its friends in parallel

00:12:04,880 --> 00:12:07,760
and

00:12:05,200 --> 00:12:07,760
at scale

00:12:09,440 --> 00:12:13,040
and flink if there's one thing that

00:12:12,079 --> 00:12:15,279
fling can do

00:12:13,040 --> 00:12:16,399
it's definitely scale so to give you an

00:12:15,279 --> 00:12:18,639
idea of

00:12:16,399 --> 00:12:20,480
uh the scale that fling can go to

00:12:18,639 --> 00:12:21,600
everyone calls it the alibaba scale

00:12:20,480 --> 00:12:25,279
because it's the

00:12:21,600 --> 00:12:28,720
biggest um biggest scale

00:12:25,279 --> 00:12:30,800
use case known to us so the biggest

00:12:28,720 --> 00:12:33,600
production installation that we know is

00:12:30,800 --> 00:12:36,800
uh what alibaba runs on this

00:12:33,600 --> 00:12:38,720
crazy shopping festival they they have

00:12:36,800 --> 00:12:40,880
in november every year called a double

00:12:38,720 --> 00:12:43,440
11 and

00:12:40,880 --> 00:12:46,160
on this day flink is backing most of its

00:12:43,440 --> 00:12:49,360
uh real-time data applications so like

00:12:46,160 --> 00:12:52,240
search and recommendation uh ads

00:12:49,360 --> 00:12:54,560
and even like this huge jbm dashboard

00:12:52,240 --> 00:12:57,600
that you see all over the media

00:12:54,560 --> 00:13:00,560
when double 11 happens

00:12:57,600 --> 00:13:02,079
what is kind of crunching everything

00:13:00,560 --> 00:13:04,839
behind the scenes

00:13:02,079 --> 00:13:06,560
so that you can see the nice metrics is

00:13:04,839 --> 00:13:10,240
flink

00:13:06,560 --> 00:13:13,360
and so they are running flink on over

00:13:10,240 --> 00:13:14,399
5000 machines at peak they are

00:13:13,360 --> 00:13:18,800
processing

00:13:14,399 --> 00:13:20,480
2.5 billion events per second and

00:13:18,800 --> 00:13:22,160
they do all of this with sub second

00:13:20,480 --> 00:13:25,519
latency so even uh

00:13:22,160 --> 00:13:28,160
serving like um doing their updates to

00:13:25,519 --> 00:13:30,399
uh the feature vectors that go into the

00:13:28,160 --> 00:13:31,040
direct recommendation system this is all

00:13:30,399 --> 00:13:34,639
done with

00:13:31,040 --> 00:13:37,680
second literacy despite the volume of

00:13:34,639 --> 00:13:37,680
data that they're handling

00:13:38,160 --> 00:13:45,279
so in a nutshell our very overview what

00:13:42,560 --> 00:13:46,560
python gives you is a high-level

00:13:45,279 --> 00:13:49,199
abstraction

00:13:46,560 --> 00:13:50,560
kind of um relational like way to

00:13:49,199 --> 00:13:54,800
process data

00:13:50,560 --> 00:13:58,160
uh with um with a table like

00:13:54,800 --> 00:14:00,320
uh yeah with a table like uh

00:13:58,160 --> 00:14:02,720
abstraction that is very similar to

00:14:00,320 --> 00:14:05,360
using sql on a database

00:14:02,720 --> 00:14:06,959
or it's pretty familiar if you already

00:14:05,360 --> 00:14:09,199
work with python and

00:14:06,959 --> 00:14:11,279
you work with tabular formats so like if

00:14:09,199 --> 00:14:14,480
you use pandas this should be pretty

00:14:11,279 --> 00:14:16,320
pretty straightforward for you and

00:14:14,480 --> 00:14:18,000
besides all the operations that you

00:14:16,320 --> 00:14:20,800
would expect from a regular

00:14:18,000 --> 00:14:22,000
sequel like engine uh like complex joins

00:14:20,800 --> 00:14:24,720
aggregations

00:14:22,000 --> 00:14:26,880
it also has support for um advanced

00:14:24,720 --> 00:14:27,680
operations like time traveling pattern

00:14:26,880 --> 00:14:31,839
matching

00:14:27,680 --> 00:14:31,839
and uh that kind of stuff

00:14:32,720 --> 00:14:38,399
and i didn't mention it before

00:14:35,839 --> 00:14:38,959
well i i did mention it in the use cases

00:14:38,399 --> 00:14:40,880
for

00:14:38,959 --> 00:14:43,199
uh streaming analytics that sometimes

00:14:40,880 --> 00:14:47,920
you want to do batch and streaming

00:14:43,199 --> 00:14:50,320
and flink or pythlink allows you to do

00:14:47,920 --> 00:14:52,079
to handle batten streaming workloads

00:14:50,320 --> 00:14:54,320
each with their own

00:14:52,079 --> 00:14:58,160
cost optimizer so a lot of times

00:14:54,320 --> 00:15:01,360
basically you can use the same code to

00:14:58,160 --> 00:15:01,680
to run to process real-time data say

00:15:01,360 --> 00:15:04,880
from

00:15:01,680 --> 00:15:07,360
kafka or something like that and then

00:15:04,880 --> 00:15:10,000
to statically process a bunch of files

00:15:07,360 --> 00:15:13,920
that are stored in an s3 bucket

00:15:10,000 --> 00:15:16,320
and it also allows you to extend

00:15:13,920 --> 00:15:18,160
uh to extend your logic with

00:15:16,320 --> 00:15:21,519
user-defined functions

00:15:18,160 --> 00:15:24,079
also including vectorized udfs or uh

00:15:21,519 --> 00:15:24,880
known as pandas udfs they are much more

00:15:24,079 --> 00:15:28,240
performant

00:15:24,880 --> 00:15:28,240
in terms of serialization

00:15:30,000 --> 00:15:34,279
and all of this is supported by a

00:15:32,399 --> 00:15:37,279
growing ecosystem you have

00:15:34,279 --> 00:15:39,120
a lot of native connectors some of them

00:15:37,279 --> 00:15:41,759
directly supported by the community

00:15:39,120 --> 00:15:44,079
others are kind of externally maintained

00:15:41,759 --> 00:15:48,079
by other communities

00:15:44,079 --> 00:15:50,000
you can consume every basically every

00:15:48,079 --> 00:15:52,079
every common format that you find out

00:15:50,000 --> 00:15:54,079
there and there's also a machine

00:15:52,079 --> 00:15:57,360
learning library in the works

00:15:54,079 --> 00:16:00,560
so you'll also soon be able to

00:15:57,360 --> 00:16:00,560
to use that with python

00:16:01,600 --> 00:16:05,519
so but today i want to go a little bit

00:16:04,480 --> 00:16:07,920
into

00:16:05,519 --> 00:16:10,480
using python with apache zeppelin

00:16:07,920 --> 00:16:10,480
notebooks

00:16:11,759 --> 00:16:15,360
and here i'm not really going to

00:16:14,320 --> 00:16:18,800
advocate for

00:16:15,360 --> 00:16:20,240
or against notebooks here i

00:16:18,800 --> 00:16:22,160
i wasn't really experienced with

00:16:20,240 --> 00:16:22,639
notebooks before i came up with this

00:16:22,160 --> 00:16:24,800
talk

00:16:22,639 --> 00:16:26,800
but on the internet i saw that there's a

00:16:24,800 --> 00:16:27,920
really great divide between people that

00:16:26,800 --> 00:16:31,360
love notebooks

00:16:27,920 --> 00:16:34,560
people that hate notebooks netflix using

00:16:31,360 --> 00:16:36,880
notebooks to productionize stuff so

00:16:34,560 --> 00:16:38,720
this is not a hill i'm willing to die on

00:16:36,880 --> 00:16:39,360
today it's not one of the goals of this

00:16:38,720 --> 00:16:41,360
talk

00:16:39,360 --> 00:16:43,279
to tell you if you should use notebooks

00:16:41,360 --> 00:16:45,360
or you shouldn't use notebooks

00:16:43,279 --> 00:16:46,399
turns out they're pretty great for data

00:16:45,360 --> 00:16:49,199
exploration

00:16:46,399 --> 00:16:50,399
uh and they're pretty good for demos so

00:16:49,199 --> 00:16:53,360
that's kind of like

00:16:50,399 --> 00:16:53,360
where i'm leaving it

00:16:54,079 --> 00:16:58,399
and so zeppelin uh if you've never heard

00:16:56,720 --> 00:17:01,040
about it before as well

00:16:58,399 --> 00:17:01,920
open source of course uh web-based

00:17:01,040 --> 00:17:03,680
notebook

00:17:01,920 --> 00:17:06,240
provides you an interactive

00:17:03,680 --> 00:17:09,760
collaborative computing environment

00:17:06,240 --> 00:17:11,919
and it has uh

00:17:09,760 --> 00:17:14,000
like i said i'm not i've never used

00:17:11,919 --> 00:17:17,679
notebooks professionally

00:17:14,000 --> 00:17:19,360
but i've heard of

00:17:17,679 --> 00:17:22,079
other notebooks like jupiter or

00:17:19,360 --> 00:17:24,240
polynomial i've never tried them so

00:17:22,079 --> 00:17:26,240
uh these are basically some things that

00:17:24,240 --> 00:17:26,959
i think are really cool about zeppelin

00:17:26,240 --> 00:17:28,799
in particular

00:17:26,959 --> 00:17:30,640
not really zeppelin compared to anything

00:17:28,799 --> 00:17:33,919
else uh

00:17:30,640 --> 00:17:35,760
but so what's great about about zeppelin

00:17:33,919 --> 00:17:36,240
is that it has support for a lot of

00:17:35,760 --> 00:17:38,559
different

00:17:36,240 --> 00:17:40,880
interpreters so uh you can use it with

00:17:38,559 --> 00:17:41,840
flink you can use it with spark you can

00:17:40,880 --> 00:17:44,880
use it

00:17:41,840 --> 00:17:48,080
with r uh vanilla python you can

00:17:44,880 --> 00:17:49,280
use bash you can use markdown and you

00:17:48,080 --> 00:17:52,400
can use all of them

00:17:49,280 --> 00:17:54,080
in the same notebook i will show you

00:17:52,400 --> 00:17:56,640
i will show you in the demo afterwards

00:17:54,080 --> 00:18:00,799
you can just like compose a really nice

00:17:56,640 --> 00:18:03,840
uh composer really nice note with all

00:18:00,799 --> 00:18:06,720
very different interpreters and

00:18:03,840 --> 00:18:07,679
also it has built-in interactive

00:18:06,720 --> 00:18:10,720
visualization

00:18:07,679 --> 00:18:12,240
so it actually comes already with some

00:18:10,720 --> 00:18:14,400
really nice

00:18:12,240 --> 00:18:16,640
visualizations that you can interact

00:18:14,400 --> 00:18:19,120
with and zeppelin also provides

00:18:16,640 --> 00:18:20,400
uh its own visualization library so you

00:18:19,120 --> 00:18:23,760
can expand

00:18:20,400 --> 00:18:25,440
um called helium that you can

00:18:23,760 --> 00:18:28,000
use to expand the visualizations that

00:18:25,440 --> 00:18:31,200
are available out of the box

00:18:28,000 --> 00:18:33,440
and i think

00:18:31,200 --> 00:18:35,440
one of the differentiators of zeppelin

00:18:33,440 --> 00:18:38,720
from what i read is that it's

00:18:35,440 --> 00:18:39,520
built from scratch for multi-tenancy so

00:18:38,720 --> 00:18:41,679
it has

00:18:39,520 --> 00:18:43,440
a lot of features like different

00:18:41,679 --> 00:18:46,000
interpreter bindings

00:18:43,440 --> 00:18:47,200
binding modes for process isolation

00:18:46,000 --> 00:18:49,840
different authenticated

00:18:47,200 --> 00:18:51,760
authentication methods it also allows

00:18:49,840 --> 00:18:54,799
you to keep

00:18:51,760 --> 00:18:56,480
versioning on on git and this kind of

00:18:54,799 --> 00:18:58,640
stuff that makes it really good

00:18:56,480 --> 00:19:00,000
uh if you're if you have like if you

00:18:58,640 --> 00:19:02,559
want to share your notebooks and this

00:19:00,000 --> 00:19:02,559
kind of stuff

00:19:03,360 --> 00:19:08,840
okay so moving on to the demo i just

00:19:06,559 --> 00:19:11,919
need to

00:19:08,840 --> 00:19:14,240
disconnect my screen real quick and

00:19:11,919 --> 00:19:19,440
change it

00:19:14,240 --> 00:19:19,440
okay see

00:19:22,840 --> 00:19:27,760
cool

00:19:24,240 --> 00:19:30,320
okay it's here cool so this is how the

00:19:27,760 --> 00:19:32,880
zeppelin environment looks like

00:19:30,320 --> 00:19:33,600
all you have to do to make it work with

00:19:32,880 --> 00:19:36,000
flink

00:19:33,600 --> 00:19:37,440
really is come to the interpreters and

00:19:36,000 --> 00:19:40,400
you can see here

00:19:37,440 --> 00:19:42,000
the amount of interpreters that it has

00:19:40,400 --> 00:19:43,600
it's really a lot of stuff to choose

00:19:42,000 --> 00:19:45,760
from

00:19:43,600 --> 00:19:48,240
so all you have to do to make it work

00:19:45,760 --> 00:19:50,160
with flink is basically just point it at

00:19:48,240 --> 00:19:51,760
uh your flink installation if you want

00:19:50,160 --> 00:19:56,240
to use python

00:19:51,760 --> 00:19:59,919
you want it to your the python path

00:19:56,240 --> 00:19:59,919
where you have python installed

00:20:02,720 --> 00:20:09,679
cool so this demo is not a

00:20:06,080 --> 00:20:12,960
super complex demo basically

00:20:09,679 --> 00:20:16,000
we're just looking at a data set

00:20:12,960 --> 00:20:16,559
from kaggle and it's a data set about

00:20:16,000 --> 00:20:19,679
this

00:20:16,559 --> 00:20:23,039
uh film platform called movie it's like

00:20:19,679 --> 00:20:25,039
netflix but for hipsters and

00:20:23,039 --> 00:20:26,080
i'm not going to a lot of adventures

00:20:25,039 --> 00:20:28,880
here and i

00:20:26,080 --> 00:20:30,480
basically pre-ran everything because i'm

00:20:28,880 --> 00:20:31,600
really afraid that my computer will

00:20:30,480 --> 00:20:35,440
freeze

00:20:31,600 --> 00:20:38,320
because it's not uh yeah very well

00:20:35,440 --> 00:20:39,760
in doubt so what we're doing here is

00:20:38,320 --> 00:20:42,640
we're just looking

00:20:39,760 --> 00:20:43,360
plainly looking at some csv files so

00:20:42,640 --> 00:20:46,720
we're doing

00:20:43,360 --> 00:20:46,720
everything patch mode

00:20:46,880 --> 00:20:51,360
but yeah later you can just explore and

00:20:49,039 --> 00:20:53,520
you can easily just switch to streaming

00:20:51,360 --> 00:20:57,120
and also use it

00:20:53,520 --> 00:20:58,880
to to look at some streaming data

00:20:57,120 --> 00:21:00,640
so what we have is basically three

00:20:58,880 --> 00:21:03,520
different csv files

00:21:00,640 --> 00:21:05,280
first file has just some reference data

00:21:03,520 --> 00:21:06,960
from all the movies that are registered

00:21:05,280 --> 00:21:10,159
in this platform

00:21:06,960 --> 00:21:13,200
another file which is a little bigger

00:21:10,159 --> 00:21:14,240
has ratings for all this move for all

00:21:13,200 --> 00:21:16,960
these films

00:21:14,240 --> 00:21:17,679
from the movie users and this file has

00:21:16,960 --> 00:21:20,960
around

00:21:17,679 --> 00:21:23,520
15 million rows and the

00:21:20,960 --> 00:21:25,039
third file just has some aggregated data

00:21:23,520 --> 00:21:28,400
aggregated user data

00:21:25,039 --> 00:21:31,919
regarding also two ratings

00:21:28,400 --> 00:21:35,280
so zeppelin offers i think five

00:21:31,919 --> 00:21:36,159
different interpreters for flink here we

00:21:35,280 --> 00:21:39,440
are using

00:21:36,159 --> 00:21:40,000
a mix of python interpreter and also the

00:21:39,440 --> 00:21:43,360
batch

00:21:40,000 --> 00:21:44,159
sql interpreter and one of the good

00:21:43,360 --> 00:21:46,720
things

00:21:44,159 --> 00:21:48,400
of zeppelin if you just want to get up

00:21:46,720 --> 00:21:52,640
and running is that it kind of

00:21:48,400 --> 00:21:54,159
um it kind of just creates the

00:21:52,640 --> 00:21:56,000
environment variables for us so you

00:21:54,159 --> 00:21:58,080
don't really have to you don't really

00:21:56,000 --> 00:21:59,360
have to worry with

00:21:58,080 --> 00:22:01,200
all the imports and all the

00:21:59,360 --> 00:22:03,440
configuration of your environment unless

00:22:01,200 --> 00:22:05,280
you want to do

00:22:03,440 --> 00:22:07,440
unless you want to do some more

00:22:05,280 --> 00:22:09,679
customization but like just to

00:22:07,440 --> 00:22:13,760
get started you don't really need to to

00:22:09,679 --> 00:22:17,760
worry about any of that

00:22:13,760 --> 00:22:20,480
so okay

00:22:17,760 --> 00:22:21,840
so the first thing like i said we had we

00:22:20,480 --> 00:22:24,640
have these uh

00:22:21,840 --> 00:22:27,200
csv files first thing we want to do is

00:22:24,640 --> 00:22:29,919
we want to kind of

00:22:27,200 --> 00:22:30,880
load them onto blank so we want to

00:22:29,919 --> 00:22:33,280
create

00:22:30,880 --> 00:22:34,720
some uh we're going to create a source

00:22:33,280 --> 00:22:38,640
table from the movie

00:22:34,720 --> 00:22:41,280
csv file and the recommended and

00:22:38,640 --> 00:22:43,919
quickest way to define a source table

00:22:41,280 --> 00:22:44,880
in python is to use sql ddl so like i

00:22:43,919 --> 00:22:48,480
said

00:22:44,880 --> 00:22:51,520
pythlink support fully supports

00:22:48,480 --> 00:22:52,159
sql so you can just define you can just

00:22:51,520 --> 00:22:54,880
define

00:22:52,159 --> 00:22:55,520
your your tables using sql dtl you can

00:22:54,880 --> 00:22:59,600
also use

00:22:55,520 --> 00:23:02,799
you can also choose to use the python

00:22:59,600 --> 00:23:05,200
other specific interfaces but this

00:23:02,799 --> 00:23:06,960
is the recommended and the most

00:23:05,200 --> 00:23:08,880
straightforward way to really create a

00:23:06,960 --> 00:23:12,559
table so

00:23:08,880 --> 00:23:16,880
here i just for the sake of

00:23:12,559 --> 00:23:18,400
a demonstration i just here gave you two

00:23:16,880 --> 00:23:20,720
different ways that you can create a

00:23:18,400 --> 00:23:22,720
table so you can use

00:23:20,720 --> 00:23:25,120
you can use the pythlink interpreter or

00:23:22,720 --> 00:23:28,799
you can use the batch sql interpreter

00:23:25,120 --> 00:23:29,760
uh and in both places you basically can

00:23:28,799 --> 00:23:33,440
just use

00:23:29,760 --> 00:23:36,240
standard sql uh to really create a table

00:23:33,440 --> 00:23:39,440
this is a regular create table

00:23:36,240 --> 00:23:42,640
statement that would run in

00:23:39,440 --> 00:23:44,640
any database so here in the width part

00:23:42,640 --> 00:23:47,039
you just define your connector

00:23:44,640 --> 00:23:47,919
in our case um it's a file system

00:23:47,039 --> 00:23:51,039
connector

00:23:47,919 --> 00:23:52,240
i'm just connecting to um i'm just

00:23:51,039 --> 00:23:55,679
directing it to a

00:23:52,240 --> 00:23:58,080
um a csv file that is stored locally

00:23:55,679 --> 00:23:59,919
and i'm just telling flink that it

00:23:58,080 --> 00:24:02,480
should serialize this as

00:23:59,919 --> 00:24:02,480
csv

00:24:03,600 --> 00:24:08,880
and oh sorry i forgot to mention

00:24:07,120 --> 00:24:10,480
something that uh like i said

00:24:08,880 --> 00:24:12,720
you can use multiple interpreters you

00:24:10,480 --> 00:24:14,960
can see here that uh

00:24:12,720 --> 00:24:17,120
here i'm using some markdown so if you

00:24:14,960 --> 00:24:21,039
see like the

00:24:17,120 --> 00:24:24,559
you see here is the markdown interpreter

00:24:21,039 --> 00:24:28,400
okay and here for example you can just

00:24:24,559 --> 00:24:31,360
you can just check with uh

00:24:28,400 --> 00:24:32,960
some shell you can have a look at your

00:24:31,360 --> 00:24:36,480
at your source files

00:24:32,960 --> 00:24:37,039
just to get an idea of just get an idea

00:24:36,480 --> 00:24:38,799
how your

00:24:37,039 --> 00:24:40,400
how your data looks like before you

00:24:38,799 --> 00:24:42,640
define your table

00:24:40,400 --> 00:24:42,640
so

00:24:43,919 --> 00:24:47,760
so yeah here you can have like a feel

00:24:45,520 --> 00:24:50,080
for the data you can see like what data

00:24:47,760 --> 00:24:52,000
types to use you can see what's wrong so

00:24:50,080 --> 00:24:55,120
you can see here that

00:24:52,000 --> 00:24:58,000
the year has a weird format uh

00:24:55,120 --> 00:24:59,919
there's also here you can see that for

00:24:58,000 --> 00:25:01,440
some fields you have multiple values so

00:24:59,919 --> 00:25:02,320
these are all things that when you're

00:25:01,440 --> 00:25:04,480
just like

00:25:02,320 --> 00:25:06,159
looking around and trying to export the

00:25:04,480 --> 00:25:08,960
data and maybe clean it

00:25:06,159 --> 00:25:09,440
uh it's pretty useful that you can just

00:25:08,960 --> 00:25:12,240
yeah

00:25:09,440 --> 00:25:15,200
get a little subset of the data and then

00:25:12,240 --> 00:25:18,320
you can start building around that

00:25:15,200 --> 00:25:19,840
so i pre-ran this i created my table i

00:25:18,320 --> 00:25:23,200
did

00:25:19,840 --> 00:25:26,320
also account here to see

00:25:23,200 --> 00:25:28,159
um from the table we created i just

00:25:26,320 --> 00:25:30,480
wanted to select and see

00:25:28,159 --> 00:25:32,720
how many how many records there are in

00:25:30,480 --> 00:25:35,919
this table we have around

00:25:32,720 --> 00:25:38,400
200 000 records here

00:25:35,919 --> 00:25:38,960
and now we can start just using this

00:25:38,400 --> 00:25:42,880
table

00:25:38,960 --> 00:25:43,840
for some exploration and see what we can

00:25:42,880 --> 00:25:46,880
find

00:25:43,840 --> 00:25:48,080
uh so see what what we can find in it so

00:25:46,880 --> 00:25:51,600
we will use this

00:25:48,080 --> 00:25:54,400
movie movies table to get um

00:25:51,600 --> 00:25:56,320
the average movie popularity per movie

00:25:54,400 --> 00:25:58,320
released here and

00:25:56,320 --> 00:26:01,279
the way you do it the way you get the

00:25:58,320 --> 00:26:04,240
average movie popularity release

00:26:01,279 --> 00:26:06,080
for release here you can see here that

00:26:04,240 --> 00:26:09,840
uh

00:26:06,080 --> 00:26:10,320
it looks it's it's very familiar if

00:26:09,840 --> 00:26:12,480
you're

00:26:10,320 --> 00:26:15,120
from if you already know sql it's kind

00:26:12,480 --> 00:26:18,320
of the same way of thinking about it so

00:26:15,120 --> 00:26:18,880
um from your batch table environment you

00:26:18,320 --> 00:26:22,480
just

00:26:18,880 --> 00:26:25,760
select from the movie movie tables

00:26:22,480 --> 00:26:27,120
you do a group by per year and then

00:26:25,760 --> 00:26:28,960
you just select whatever you're

00:26:27,120 --> 00:26:32,159
interested in seeing so

00:26:28,960 --> 00:26:34,159
um pipeline and

00:26:32,159 --> 00:26:35,840
python has a lot of built-in functions

00:26:34,159 --> 00:26:37,760
that you can use to

00:26:35,840 --> 00:26:41,840
wrangle around with data so like you

00:26:37,760 --> 00:26:41,840
have string functions

00:26:42,320 --> 00:26:46,799
yeah and here you see that we're doing

00:26:44,320 --> 00:26:51,600
the average

00:26:46,799 --> 00:26:53,520
so if you run this which i have before

00:26:51,600 --> 00:26:54,880
you can choose from one of these

00:26:53,520 --> 00:26:58,080
visualizations and

00:26:54,880 --> 00:27:00,559
zeppelin so i hope i don't destroy it

00:26:58,080 --> 00:27:02,080
please remember that it looked good

00:27:00,559 --> 00:27:04,159
before i switched

00:27:02,080 --> 00:27:05,360
so you can choose between really

00:27:04,159 --> 00:27:07,279
different

00:27:05,360 --> 00:27:08,559
visualizations for the data that you

00:27:07,279 --> 00:27:11,760
have

00:27:08,559 --> 00:27:15,279
and here you can see that it's like

00:27:11,760 --> 00:27:18,559
fully interactive you can see

00:27:15,279 --> 00:27:21,520
all the data points and

00:27:18,559 --> 00:27:23,520
okay cool because this is about kind of

00:27:21,520 --> 00:27:24,720
extracting some insights from whatever

00:27:23,520 --> 00:27:27,039
we're seeing

00:27:24,720 --> 00:27:28,720
uh what we can see here for example is

00:27:27,039 --> 00:27:30,880
that there are two clear

00:27:28,720 --> 00:27:32,640
uh outlier years when it comes to

00:27:30,880 --> 00:27:35,520
popularity so

00:27:32,640 --> 00:27:36,320
or the average popularity so you see

00:27:35,520 --> 00:27:40,000
here

00:27:36,320 --> 00:27:41,360
and here and if you check further there

00:27:40,000 --> 00:27:43,520
is only

00:27:41,360 --> 00:27:44,640
one movie release in each of these years

00:27:43,520 --> 00:27:47,760
on movie

00:27:44,640 --> 00:27:48,960
so they were just really really popular

00:27:47,760 --> 00:27:52,640
films

00:27:48,960 --> 00:27:55,360
that were released in this year and

00:27:52,640 --> 00:27:56,960
we can also see that here in the 1920s

00:27:55,360 --> 00:28:00,240
you have like a

00:27:56,960 --> 00:28:02,480
very busy period and uh

00:28:00,240 --> 00:28:03,520
i did some internet researching and this

00:28:02,480 --> 00:28:06,960
is apparently

00:28:03,520 --> 00:28:09,760
um this is apparently the decade for

00:28:06,960 --> 00:28:10,559
silent movies so it seems like silent

00:28:09,760 --> 00:28:14,960
movies are

00:28:10,559 --> 00:28:18,080
also pretty popular with um movie users

00:28:14,960 --> 00:28:21,520
and you can also see that

00:28:18,080 --> 00:28:24,640
here probably until the end of the 1960s

00:28:21,520 --> 00:28:26,960
beginning from the 20s uh

00:28:24,640 --> 00:28:28,240
you also have increased popularity for

00:28:26,960 --> 00:28:30,480
the movies and this is

00:28:28,240 --> 00:28:31,360
uh considered the golden era of

00:28:30,480 --> 00:28:33,840
hollywood

00:28:31,360 --> 00:28:36,159
so i guess this can this can explain the

00:28:33,840 --> 00:28:38,880
increased popularity for movies in this

00:28:36,159 --> 00:28:38,880
in this period

00:28:40,399 --> 00:28:44,880
okay so moving forward this is basically

00:28:43,039 --> 00:28:47,120
the same thing that we did above before

00:28:44,880 --> 00:28:48,080
the other two files so for the movie

00:28:47,120 --> 00:28:50,559
ratings file

00:28:48,080 --> 00:28:52,320
and from the aggregated data we are just

00:28:50,559 --> 00:28:54,720
creating here

00:28:52,320 --> 00:28:56,720
uh we are just creating two more tables

00:28:54,720 --> 00:29:00,000
from those data sources

00:28:56,720 --> 00:29:02,960
and the only thing i'm going to risk

00:29:00,000 --> 00:29:03,840
i'm going to risk to do live is for

00:29:02,960 --> 00:29:06,640
example here

00:29:03,840 --> 00:29:07,919
if we want to see all the tables that we

00:29:06,640 --> 00:29:12,320
have created so far

00:29:07,919 --> 00:29:16,159
all the tables are registered in blink

00:29:12,320 --> 00:29:18,159
we should have three tables okay

00:29:16,159 --> 00:29:21,760
and that checks out so we have movies

00:29:18,159 --> 00:29:24,960
table ratings table ratings user table

00:29:21,760 --> 00:29:26,559
and we can just continue or we can just

00:29:24,960 --> 00:29:28,000
use these two new tables that we just

00:29:26,559 --> 00:29:30,399
created

00:29:28,000 --> 00:29:31,520
so another thing or one of the first

00:29:30,399 --> 00:29:34,880
things that we can see

00:29:31,520 --> 00:29:36,720
based on the ratings uh data

00:29:34,880 --> 00:29:38,840
is we can check the number of active

00:29:36,720 --> 00:29:41,840
users in the platform

00:29:38,840 --> 00:29:44,880
uh

00:29:41,840 --> 00:29:45,200
over time and uh for this i'm just using

00:29:44,880 --> 00:29:47,200
the

00:29:45,200 --> 00:29:49,440
rating activity so i'm considering that

00:29:47,200 --> 00:29:51,039
a user was active if they did a rating

00:29:49,440 --> 00:29:54,720
on a certain day

00:29:51,039 --> 00:29:58,080
so again the

00:29:54,720 --> 00:30:00,720
the way you construct your

00:29:58,080 --> 00:30:02,640
your query is pretty straightforward you

00:30:00,720 --> 00:30:03,360
select from the ratings table you group

00:30:02,640 --> 00:30:06,640
by

00:30:03,360 --> 00:30:08,399
uh you group by year and month

00:30:06,640 --> 00:30:10,159
and you see here again that you have

00:30:08,399 --> 00:30:13,279
some uh built-in

00:30:10,159 --> 00:30:14,880
date date handling functions that you

00:30:13,279 --> 00:30:17,440
can use

00:30:14,880 --> 00:30:18,320
and we are just yeah doing an aggregate

00:30:17,440 --> 00:30:20,880
account

00:30:18,320 --> 00:30:22,720
and then ordering so that we have like a

00:30:20,880 --> 00:30:27,120
nice timeline

00:30:22,720 --> 00:30:28,720
and what can we extract from here so

00:30:27,120 --> 00:30:31,200
basically there is

00:30:28,720 --> 00:30:33,840
it's pretty funny that there is a peak

00:30:31,200 --> 00:30:37,120
of active users every january 1st

00:30:33,840 --> 00:30:38,320
in every year this is pretty likely when

00:30:37,120 --> 00:30:41,440
people are making their

00:30:38,320 --> 00:30:42,240
best of the previous year lists and you

00:30:41,440 --> 00:30:45,600
can see

00:30:42,240 --> 00:30:48,399
so you can see here there's a peak here

00:30:45,600 --> 00:30:52,240
there's a peak january 2014

00:30:48,399 --> 00:30:56,480
january oops january 2015.

00:30:52,240 --> 00:30:58,159
let's see this big peak january 2019

00:30:56,480 --> 00:30:59,919
so this is one of the first things that

00:30:58,159 --> 00:31:03,039
you can that you can

00:30:59,919 --> 00:31:05,600
take out of this and

00:31:03,039 --> 00:31:06,880
this data only goes until april of this

00:31:05,600 --> 00:31:10,640
year but it still

00:31:06,880 --> 00:31:11,840
already captures um already captures

00:31:10,640 --> 00:31:14,880
like a boosting

00:31:11,840 --> 00:31:15,679
activity from when the corona eats hits

00:31:14,880 --> 00:31:18,880
so

00:31:15,679 --> 00:31:21,200
you can see that starting from

00:31:18,880 --> 00:31:22,080
after february so starting from march

00:31:21,200 --> 00:31:25,519
you can see

00:31:22,080 --> 00:31:28,320
that the activity is going up

00:31:25,519 --> 00:31:30,240
as people are getting locked down so i

00:31:28,320 --> 00:31:32,720
guess people are

00:31:30,240 --> 00:31:35,120
seeing and reading more films than

00:31:32,720 --> 00:31:35,120
before

00:31:36,000 --> 00:31:43,360
whoops i forgot to run this work now

00:31:39,200 --> 00:31:45,200
another thing you can do um is just

00:31:43,360 --> 00:31:47,200
because so far we've been looking at

00:31:45,200 --> 00:31:48,799
individual tables

00:31:47,200 --> 00:31:50,480
but one of the things that is

00:31:48,799 --> 00:31:52,559
interesting to do and that

00:31:50,480 --> 00:31:54,080
you kind of want to do in a performant

00:31:52,559 --> 00:31:55,360
way if you have huge amounts of data

00:31:54,080 --> 00:31:59,039
it's not the case here but

00:31:55,360 --> 00:32:00,080
anyways you can enrich you can enrich

00:31:59,039 --> 00:32:03,600
data so here

00:32:00,080 --> 00:32:06,880
what we are going to do or what i

00:32:03,600 --> 00:32:10,399
uh wanted to do was to get

00:32:06,880 --> 00:32:13,279
the top 10 rated movies in a given

00:32:10,399 --> 00:32:14,799
in a given year and for this you need

00:32:13,279 --> 00:32:17,840
two different tables you need

00:32:14,799 --> 00:32:20,960
the movies tables where we will get

00:32:17,840 --> 00:32:23,279
uh the title and the release year and

00:32:20,960 --> 00:32:25,039
you need the ratings table where you can

00:32:23,279 --> 00:32:28,080
get

00:32:25,039 --> 00:32:29,600
uh the rating the average rating score

00:32:28,080 --> 00:32:31,519
and you can also get the number of

00:32:29,600 --> 00:32:35,039
critics

00:32:31,519 --> 00:32:37,360
so uh first here because this

00:32:35,039 --> 00:32:38,399
table like i told you before has 15

00:32:37,360 --> 00:32:40,399
million rows

00:32:38,399 --> 00:32:41,679
one thing we want to do before actually

00:32:40,399 --> 00:32:43,360
joining them is kind of

00:32:41,679 --> 00:32:46,480
reduce the amount of data that we are

00:32:43,360 --> 00:32:49,919
using for that so for the ratings table

00:32:46,480 --> 00:32:53,120
um i'm filtering out

00:32:49,919 --> 00:32:57,039
just by a specific year

00:32:53,120 --> 00:33:00,000
and then i'm aggravated ever

00:32:57,039 --> 00:33:00,720
grouping it sorry uh that i'm just

00:33:00,000 --> 00:33:02,559
grouping it

00:33:00,720 --> 00:33:04,000
and calculating the average so like

00:33:02,559 --> 00:33:07,279
doing all

00:33:04,000 --> 00:33:09,200
the kind of like just reducing it down

00:33:07,279 --> 00:33:11,200
and doing a lot of calculations already

00:33:09,200 --> 00:33:12,799
over this data before i actually join it

00:33:11,200 --> 00:33:16,320
with movie table

00:33:12,799 --> 00:33:18,240
and here for the year uh this is one

00:33:16,320 --> 00:33:20,559
cool thing that you can do with zeppelin

00:33:18,240 --> 00:33:22,640
you can use this dynamic forms

00:33:20,559 --> 00:33:24,159
so here this is basically the default

00:33:22,640 --> 00:33:27,440
year so if you don't

00:33:24,159 --> 00:33:28,880
input anything this is what um we'll be

00:33:27,440 --> 00:33:31,679
using the calculation

00:33:28,880 --> 00:33:34,000
but you can enter an arbitrary arbitrary

00:33:31,679 --> 00:33:37,200
year here and it will be picked up

00:33:34,000 --> 00:33:42,559
in the query so

00:33:37,200 --> 00:33:42,559
okay and then you can just

00:33:43,120 --> 00:33:49,519
join these two tables

00:33:46,159 --> 00:33:50,240
they have uh one field in common so

00:33:49,519 --> 00:33:51,919
movie id

00:33:50,240 --> 00:33:54,080
here i had to give an alliance to the

00:33:51,919 --> 00:33:55,760
movie id because

00:33:54,080 --> 00:33:58,159
they can't have this the columns can

00:33:55,760 --> 00:34:01,200
have uh the same name

00:33:58,159 --> 00:34:04,559
if you're if you're using them in a join

00:34:01,200 --> 00:34:07,760
and yeah so basically i'm just joining

00:34:04,559 --> 00:34:10,960
the two tables

00:34:07,760 --> 00:34:14,159
and i am

00:34:10,960 --> 00:34:16,320
so that i can get oh okay i'm joining

00:34:14,159 --> 00:34:19,440
the two tables and i'm

00:34:16,320 --> 00:34:20,079
ordering them by the average rating they

00:34:19,440 --> 00:34:21,839
had

00:34:20,079 --> 00:34:24,159
and because just using the average

00:34:21,839 --> 00:34:27,599
rating gave pretty

00:34:24,159 --> 00:34:29,040
bad results because uh the rating score

00:34:27,599 --> 00:34:31,760
goes from zero to five

00:34:29,040 --> 00:34:33,200
and they're a lot of uh similar so the

00:34:31,760 --> 00:34:34,320
scale is really small there were a lot

00:34:33,200 --> 00:34:37,040
of

00:34:34,320 --> 00:34:37,599
similar ratings so to make it a bit

00:34:37,040 --> 00:34:41,119
fairer

00:34:37,599 --> 00:34:43,200
or more fair i also used as a condition

00:34:41,119 --> 00:34:46,320
the

00:34:43,200 --> 00:34:50,639
number of user critics for each film

00:34:46,320 --> 00:34:54,480
and so yeah and then i just fetched the

00:34:50,639 --> 00:34:57,920
the top ten and this is what you get

00:34:54,480 --> 00:35:01,359
so this is for 2000 and i think

00:34:57,920 --> 00:35:04,400
i ran it with 19 so for 2019 these are

00:35:01,359 --> 00:35:07,599
the 10 most popular films

00:35:04,400 --> 00:35:10,960
according to user ratings and

00:35:07,599 --> 00:35:10,960
the number of user critics

00:35:11,599 --> 00:35:18,400
so one last thing i wanted to show you

00:35:15,200 --> 00:35:21,520
is how to use pandas and other python

00:35:18,400 --> 00:35:24,160
libraries with links so one way to use

00:35:21,520 --> 00:35:24,960
python with pandas is to first reduce

00:35:24,160 --> 00:35:28,160
the amount

00:35:24,960 --> 00:35:29,839
of data that you want to act upon so

00:35:28,160 --> 00:35:31,520
this is like i said one of the things

00:35:29,839 --> 00:35:33,359
that flink is really good at is dealing

00:35:31,520 --> 00:35:35,359
with huge amount of data

00:35:33,359 --> 00:35:37,359
uh if you want to use it with pandas

00:35:35,359 --> 00:35:39,200
then you don't you'll kind of want to

00:35:37,359 --> 00:35:41,760
pre-process that into something that is

00:35:39,200 --> 00:35:44,480
manageable on pandas

00:35:41,760 --> 00:35:46,320
um and so you can just use pipeline to

00:35:44,480 --> 00:35:48,800
shrink everything down

00:35:46,320 --> 00:35:50,800
and then convert the resulting table

00:35:48,800 --> 00:35:54,400
into a pandas data frame

00:35:50,800 --> 00:35:57,200
and this is what we are doing here so

00:35:54,400 --> 00:35:59,040
we are getting we are selecting from the

00:35:57,200 --> 00:36:00,480
biggest table the ratings table we are

00:35:59,040 --> 00:36:04,640
just filtering out

00:36:00,480 --> 00:36:06,720
we just want um data that is from 2018

00:36:04,640 --> 00:36:11,520
to 2020

00:36:06,720 --> 00:36:14,880
and then we just select uh the year

00:36:11,520 --> 00:36:16,640
and the rating score

00:36:14,880 --> 00:36:18,480
and then all you have to do so that you

00:36:16,640 --> 00:36:22,400
can kind of transfer this

00:36:18,480 --> 00:36:25,680
to work with pandas is just convert

00:36:22,400 --> 00:36:28,640
convert your pipeline table to

00:36:25,680 --> 00:36:28,640
append this data frame

00:36:28,800 --> 00:36:35,760
and then you can use pandas

00:36:31,920 --> 00:36:37,520
for example here we are using pandas too

00:36:35,760 --> 00:36:39,040
to render a histogram this is something

00:36:37,520 --> 00:36:39,839
of course that you could never do with

00:36:39,040 --> 00:36:42,800
pythlink

00:36:39,839 --> 00:36:44,800
by itself but by using pipeline with

00:36:42,800 --> 00:36:47,920
pandas you can actually crunch your data

00:36:44,800 --> 00:36:51,440
then move back to pandas plot a nice

00:36:47,920 --> 00:36:53,920
instagram not as nice as the

00:36:51,440 --> 00:36:55,760
as the built-in visualizations in

00:36:53,920 --> 00:36:57,359
zeppelin because it's not interactive at

00:36:55,760 --> 00:36:58,880
all also because i made it very

00:36:57,359 --> 00:37:01,359
rudimentary

00:36:58,880 --> 00:37:04,480
uh yeah but this is a way other way you

00:37:01,359 --> 00:37:07,040
could also use five-link

00:37:04,480 --> 00:37:09,440
with these libraries is to write some

00:37:07,040 --> 00:37:11,040
custom code as a pandas udf

00:37:09,440 --> 00:37:13,520
and then just ship that to the phone

00:37:11,040 --> 00:37:13,520
cluster

00:37:13,680 --> 00:37:19,920
and i have no idea how i

00:37:16,880 --> 00:37:24,960
am in terms of time but i have to switch

00:37:19,920 --> 00:37:24,960
again my monitor so that i can finish up

00:37:29,520 --> 00:37:34,240
okay so that's that's basically it

00:37:32,560 --> 00:37:36,000
for the demo if you want to learn more

00:37:34,240 --> 00:37:37,760
about flink we also have our

00:37:36,000 --> 00:37:39,760
community conference coming up in

00:37:37,760 --> 00:37:44,160
october it's free

00:37:39,760 --> 00:37:46,640
there will be some talks about by flink

00:37:44,160 --> 00:37:47,680
there will be some talks about other

00:37:46,640 --> 00:37:49,440
flink apis

00:37:47,680 --> 00:37:51,040
so if you're interested just in general

00:37:49,440 --> 00:37:52,720
to learn more about link you can join

00:37:51,040 --> 00:37:56,079
that as well

00:37:52,720 --> 00:37:58,320
and thank you so much for attending the

00:37:56,079 --> 00:37:58,320
talk

00:38:01,520 --> 00:38:05,599
i have no idea if i'm going if i have

00:38:04,000 --> 00:38:07,839
gone over time

00:38:05,599 --> 00:38:10,240
but if you have any questions just feel

00:38:07,839 --> 00:38:12,640
free to drop them here you can also

00:38:10,240 --> 00:38:14,480
reach out to me on the asf slack

00:38:12,640 --> 00:38:18,240
you can also reach out to me on twitter

00:38:14,480 --> 00:38:20,480
if you have any questions

00:38:18,240 --> 00:38:22,400
there's a question from martin any plan

00:38:20,480 --> 00:38:26,160
to support other notebooks like

00:38:22,400 --> 00:38:28,560
jupiter uh there have been

00:38:26,160 --> 00:38:29,359
discussions about supporting jupiter as

00:38:28,560 --> 00:38:31,920
well

00:38:29,359 --> 00:38:33,680
but uh i don't think this is in the

00:38:31,920 --> 00:38:38,240
immediate roadmap

00:38:33,680 --> 00:38:40,320
but you will be able to use like in

00:38:38,240 --> 00:38:41,359
the commercial offering from my company

00:38:40,320 --> 00:38:43,280
you can use

00:38:41,359 --> 00:38:44,960
uh jupiter notebooks it has inspiration

00:38:43,280 --> 00:38:47,520
with jupiter notebooks but

00:38:44,960 --> 00:38:48,160
you can only use it with sql not really

00:38:47,520 --> 00:38:52,000
python

00:38:48,160 --> 00:38:56,960
itself so the short answer is

00:38:52,000 --> 00:39:00,400
with in open source flink not for now

00:38:56,960 --> 00:39:03,440
but if there are enough users

00:39:00,400 --> 00:39:04,560
asking for it then the community will

00:39:03,440 --> 00:39:07,200
probably

00:39:04,560 --> 00:39:07,200
listen to you

00:39:12,400 --> 00:39:15,839
does that answer your question

00:39:17,119 --> 00:39:23,599
cool uh okay

00:39:20,720 --> 00:39:25,200
so if there are no more questions i

00:39:23,599 --> 00:39:26,720
shared the link to the slides if you're

00:39:25,200 --> 00:39:28,880
interested

00:39:26,720 --> 00:39:31,119
i will at some point put this up on my

00:39:28,880 --> 00:39:33,040
github on a github repo i didn't have

00:39:31,119 --> 00:39:34,800
time to make a nice setup so that

00:39:33,040 --> 00:39:36,560
everyone could use this

00:39:34,800 --> 00:39:38,480
uh but i will at some point so if you

00:39:36,560 --> 00:39:39,440
want to keep an eye on twitter i will

00:39:38,480 --> 00:39:43,839
just

00:39:39,440 --> 00:39:43,839
share it there once i have it

00:39:45,680 --> 00:39:57,839
okay thank you everyone

00:39:48,720 --> 00:39:57,839
and enjoy the rest of the conference

00:40:15,280 --> 00:40:17,359

YouTube URL: https://www.youtube.com/watch?v=KNGPXeA0AnY


