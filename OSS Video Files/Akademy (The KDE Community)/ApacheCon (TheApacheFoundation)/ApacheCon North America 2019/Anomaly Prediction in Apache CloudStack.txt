Title: Anomaly Prediction in Apache CloudStack
Publication date: 2019-09-20
Playlist: ApacheCon North America 2019
Description: 
	There is a chance of system, storage and network failures in the CloudStack environment at any point of time, which is unknown to the end users and administrators. The administrators are highly depend on the alert and monitoring system to raise an alarm on failures, upon the action is being taken. This can down the operations and will impact the end user services. The prediction of such failures ahead in CloudStack environment would be required instead of depending on the monitoring systems. Anomaly Prediction requires a model using the existing knowledge base and history of the CloudStack environment. This model would help in predicting the failures/abnormalities ahead of time. The necessary action is taken on these to minimize the down time or no down time, and making the CloudStack environment robust. The actions taken are learnt by the system and would automatically apply on the respective failures, make it a self-healing system.
Captions: 
	00:00:04,690 --> 00:00:12,370
good evening everyone so I am sue - and

00:00:08,860 --> 00:00:18,660
I am going to talk about the anomaly or

00:00:12,370 --> 00:00:23,769
fart prediction in patchy cloud stack so

00:00:18,660 --> 00:00:29,529
myself and he is a co speaker both were

00:00:23,769 --> 00:00:31,960
working in this POC so we are

00:00:29,529 --> 00:00:37,809
experimenting on some of the machine

00:00:31,960 --> 00:00:41,230
learning models so this is the agenda so

00:00:37,809 --> 00:00:44,350
I will just both of the introduction and

00:00:41,230 --> 00:00:46,739
the proposed system features and various

00:00:44,350 --> 00:00:49,210
data sources which can be used for

00:00:46,739 --> 00:00:52,899
applying the machine learning algorithms

00:00:49,210 --> 00:00:55,120
and on the architecture of the

00:00:52,899 --> 00:00:57,760
prediction system how it looks like and

00:00:55,120 --> 00:01:01,090
the solution approach and the various

00:00:57,760 --> 00:01:06,910
resources that has to be involved as

00:01:01,090 --> 00:01:14,020
part of the data and the algorithms and

00:01:06,910 --> 00:01:16,450
summary so in the cloud step we see like

00:01:14,020 --> 00:01:20,350
in the data center environment there are

00:01:16,450 --> 00:01:24,100
lot of chances for a system or host

00:01:20,350 --> 00:01:26,170
failures or storage failures so it means

00:01:24,100 --> 00:01:28,270
we'll get to know that failures either

00:01:26,170 --> 00:01:33,640
with the alerting system of the mounting

00:01:28,270 --> 00:01:40,270
system so even the efficiency of the IT

00:01:33,640 --> 00:01:43,740
ops is not it's not this pitted customer

00:01:40,270 --> 00:01:48,820
experience will always the support

00:01:43,740 --> 00:01:54,310
support team gets the problems reported

00:01:48,820 --> 00:02:01,110
from the customers so and they you do

00:01:54,310 --> 00:02:05,490
the huge data and the velocity of

00:02:01,110 --> 00:02:05,490
operational data in the from stack so

00:02:05,969 --> 00:02:11,450
you can see the incidence

00:02:08,630 --> 00:02:14,270
the customers before support this

00:02:11,450 --> 00:02:17,330
aerosol I think that there are some

00:02:14,270 --> 00:02:20,480
statistics which says seventy seventy

00:02:17,330 --> 00:02:23,060
six percent of incidents were detected

00:02:20,480 --> 00:02:31,430
by customers before customer is aware

00:02:23,060 --> 00:02:33,560
and sixty six percent he says existing

00:02:31,430 --> 00:02:36,890
monitoring solutions I didn't fail less

00:02:33,560 --> 00:02:42,740
than half of our performance issues or

00:02:36,890 --> 00:02:45,710
outages and 56 percent is a growth

00:02:42,740 --> 00:02:50,330
growth the growing complexity is leading

00:02:45,710 --> 00:02:54,080
to more outages so because of this the

00:02:50,330 --> 00:02:58,910
challenges in cloud stack or data center

00:02:54,080 --> 00:03:02,810
environment so instead of depending on

00:02:58,910 --> 00:03:07,790
alert our monitoring tools it is better

00:03:02,810 --> 00:03:12,200
to detect and take necessary actions

00:03:07,790 --> 00:03:14,800
before actual problems occur so for that

00:03:12,200 --> 00:03:17,360
there should be a smarter solution which

00:03:14,800 --> 00:03:19,850
complement the existing monitoring

00:03:17,360 --> 00:03:23,330
systems and which there which will

00:03:19,850 --> 00:03:26,120
maximize the operational efficiency so

00:03:23,330 --> 00:03:30,770
these are the proposed system features

00:03:26,120 --> 00:03:33,620
so I will go through one by one so data

00:03:30,770 --> 00:03:38,600
extraction so we have various resources

00:03:33,620 --> 00:03:40,700
data sources the log files so cloud

00:03:38,600 --> 00:03:44,570
stack generates used laughs we have

00:03:40,700 --> 00:03:48,100
management server logs system VM logs

00:03:44,570 --> 00:03:50,870
and we have logs in which your router

00:03:48,100 --> 00:03:56,360
console proxy VM and secondary storage

00:03:50,870 --> 00:03:58,820
VM and at where and we have data base

00:03:56,360 --> 00:04:01,870
which captures various events and the

00:03:58,820 --> 00:04:05,750
states of various resources like host

00:04:01,870 --> 00:04:09,590
storage or network had to be at

00:04:05,750 --> 00:04:12,680
different points of time and we have

00:04:09,590 --> 00:04:17,900
some alerts like storages outdoor space

00:04:12,680 --> 00:04:22,069
or host is not connected such alerts

00:04:17,900 --> 00:04:24,590
being captured and the events

00:04:22,069 --> 00:04:27,740
and the threshold limit switch which we

00:04:24,590 --> 00:04:31,249
set for a secondary storage using a

00:04:27,740 --> 00:04:36,560
global global parameters and the CPU

00:04:31,249 --> 00:04:39,080
uses and memory usage in the hosts so

00:04:36,560 --> 00:04:43,520
these are the various data sources which

00:04:39,080 --> 00:04:52,219
can be used in the ML which can be used

00:04:43,520 --> 00:04:57,710
in MN algorithms and failure failure

00:04:52,219 --> 00:05:04,849
prediction so after extracting the data

00:04:57,710 --> 00:05:06,889
data we a model will be created a

00:05:04,849 --> 00:05:09,590
machine learning model will be created

00:05:06,889 --> 00:05:13,909
which can be used to predict the

00:05:09,590 --> 00:05:18,469
failures and in turn we will get a

00:05:13,909 --> 00:05:22,099
notification the number of logins failed

00:05:18,469 --> 00:05:24,439
at a given point of time saying 1r5

00:05:22,099 --> 00:05:27,909
logins have been so we will get

00:05:24,439 --> 00:05:33,620
immediate notification and

00:05:27,909 --> 00:05:37,099
recommendations for example recurring

00:05:33,620 --> 00:05:41,089
snapshots has been said and the system

00:05:37,099 --> 00:05:45,830
is taking daily recurring snapshots at

00:05:41,089 --> 00:05:48,589
some point of time and it mean doesn't

00:05:45,830 --> 00:05:53,199
know at what time the threshold limit

00:05:48,589 --> 00:05:55,699
reaches so the system knows day by day

00:05:53,199 --> 00:05:58,460
recurring snapshots has been taken and

00:05:55,699 --> 00:06:01,789
some point of time your storage will get

00:05:58,460 --> 00:06:04,909
filled so the system will immediately

00:06:01,789 --> 00:06:07,490
suggest that at this point of time the

00:06:04,909 --> 00:06:12,680
system might fail so either clean up or

00:06:07,490 --> 00:06:17,360
add additional storage and integration

00:06:12,680 --> 00:06:22,330
with external issue tracker like in like

00:06:17,360 --> 00:06:22,330
JIRA so you know if they eat

00:06:22,870 --> 00:06:29,230
gets any issue between me based on the

00:06:27,190 --> 00:06:33,940
severity of the issue it immediately

00:06:29,230 --> 00:06:37,200
creates a ticket in JIRA so that any

00:06:33,940 --> 00:06:41,710
engineer can start working on that and

00:06:37,200 --> 00:06:43,510
integration of monitoring systems so the

00:06:41,710 --> 00:06:46,350
system will take input from the

00:06:43,510 --> 00:06:49,930
monitoring systems all the alerts or any

00:06:46,350 --> 00:06:53,410
event or the notifications from the

00:06:49,930 --> 00:06:59,560
monitoring system for different predict

00:06:53,410 --> 00:07:06,820
critical analysis so this is a picture

00:06:59,560 --> 00:07:09,970
of the proposed system so we we have

00:07:06,820 --> 00:07:13,240
various data sources and all all are at

00:07:09,970 --> 00:07:15,370
the real-time I passed through data

00:07:13,240 --> 00:07:17,590
extraction layer and we have a scanner

00:07:15,370 --> 00:07:21,820
and filter so we have a different

00:07:17,590 --> 00:07:24,220
scanners for each each input type for

00:07:21,820 --> 00:07:28,390
reputation for log for database for

00:07:24,220 --> 00:07:32,470
monitoring system for events and we

00:07:28,390 --> 00:07:35,910
apply certain filters on that maybe the

00:07:32,470 --> 00:07:42,750
threshold limits or the resource states

00:07:35,910 --> 00:07:46,120
or might be say it is failure type and

00:07:42,750 --> 00:07:49,950
we this unstructured data from the logs

00:07:46,120 --> 00:07:54,850
will be converted into structured data

00:07:49,950 --> 00:07:58,360
and that that is passed for the

00:07:54,850 --> 00:08:01,479
prediction so the same data is been

00:07:58,360 --> 00:08:04,479
captured in the database so that that

00:08:01,479 --> 00:08:06,930
can be used for retaining the model so

00:08:04,479 --> 00:08:14,590
the initial model which is being created

00:08:06,930 --> 00:08:19,169
doesn't give better accuracy so so that

00:08:14,590 --> 00:08:24,760
the data from the system is recorded and

00:08:19,169 --> 00:08:27,310
initially at free frequent intervals the

00:08:24,760 --> 00:08:30,350
model is trained so that we can achieve

00:08:27,310 --> 00:08:36,830
the better accuracy

00:08:30,350 --> 00:08:40,419
so so we have in the model it detects

00:08:36,830 --> 00:08:44,390
various patterns it can be a failure

00:08:40,419 --> 00:08:49,460
state change patterns various resources

00:08:44,390 --> 00:08:55,430
it can be stories network or a host and

00:08:49,460 --> 00:08:59,030
based on the output from the model so we

00:08:55,430 --> 00:09:00,680
basically get classification what kind

00:08:59,030 --> 00:09:06,650
of failure it is and what is the

00:09:00,680 --> 00:09:10,640
severity and based on that either

00:09:06,650 --> 00:09:16,910
the system will decide it should report

00:09:10,640 --> 00:09:21,860
or node fit to the administrate a ticket

00:09:16,910 --> 00:09:25,070
in the issue tracker or it will

00:09:21,860 --> 00:09:27,440
recommend base if it is a storage which

00:09:25,070 --> 00:09:30,620
will just for example in the recurring

00:09:27,440 --> 00:09:33,710
snapshot each just Rickman clean a paper

00:09:30,620 --> 00:09:38,270
storage or maybe you can expand your

00:09:33,710 --> 00:09:44,450
storage or if there are any autonomous

00:09:38,270 --> 00:09:49,870
operations even it can also may be self

00:09:44,450 --> 00:09:54,650
heal itself they based on the operations

00:09:49,870 --> 00:09:58,460
so this is a solution approach so create

00:09:54,650 --> 00:10:02,390
a data model with the historical data of

00:09:58,460 --> 00:10:07,510
the cloud stack using the various data

00:10:02,390 --> 00:10:10,310
sources and scan and filter the data

00:10:07,510 --> 00:10:13,010
this will you and rotate and rotate the

00:10:10,310 --> 00:10:17,420
data and extract the entities of

00:10:13,010 --> 00:10:23,290
corrupted and derive patterns from that

00:10:17,420 --> 00:10:26,330
and then send that to the model and

00:10:23,290 --> 00:10:32,410
process output from the model so

00:10:26,330 --> 00:10:36,140
basically we in ml team in our

00:10:32,410 --> 00:10:38,780
organization and they we have passed the

00:10:36,140 --> 00:10:41,420
data to them and they have shade some

00:10:38,780 --> 00:10:42,300
accuracy reports so which I will show in

00:10:41,420 --> 00:10:45,589
the next wave

00:10:42,300 --> 00:10:48,810
so once a process output from the model

00:10:45,589 --> 00:10:52,260
based on the output either we can notify

00:10:48,810 --> 00:10:57,060
that it means or recommended miss or it

00:10:52,260 --> 00:11:01,260
can sell skill okay so these are the

00:10:57,060 --> 00:11:04,220
various resources and attributes which

00:11:01,260 --> 00:11:06,930
can be used for analysis and prediction

00:11:04,220 --> 00:11:09,600
storage so storage device so we can

00:11:06,930 --> 00:11:12,000
consider the capacity and the state of

00:11:09,600 --> 00:11:15,750
the stories it's active enabled or

00:11:12,000 --> 00:11:17,910
disabled network devices we can any

00:11:15,750 --> 00:11:20,430
packet drops are there or any link

00:11:17,910 --> 00:11:24,959
status sometimes we see connection

00:11:20,430 --> 00:11:28,380
timeout you know in our logs so maybe

00:11:24,959 --> 00:11:36,630
instead of debugging instead of root

00:11:28,380 --> 00:11:39,630
causing we can automatically apply or

00:11:36,630 --> 00:11:43,860
figure out some patterns and based on

00:11:39,630 --> 00:11:45,630
that maybe we can notify that mean to or

00:11:43,860 --> 00:11:51,089
recommended min to take the necessary

00:11:45,630 --> 00:11:53,760
actions and compute nodes so the average

00:11:51,089 --> 00:11:57,079
CPU are the based on the average CP or

00:11:53,760 --> 00:12:01,740
enemy memory utilization we can

00:11:57,079 --> 00:12:05,040
recommend that maybe this host doesn't

00:12:01,740 --> 00:12:07,410
have enough memory or CPU maybe either

00:12:05,040 --> 00:12:09,870
you can add a new host or delete unused

00:12:07,410 --> 00:12:16,470
to be VMs or something something like

00:12:09,870 --> 00:12:19,680
that and database so even sometimes who

00:12:16,470 --> 00:12:27,420
we see there a lot of I was happening in

00:12:19,680 --> 00:12:31,020
the database so maybe based based on the

00:12:27,420 --> 00:12:34,520
usage if there is a different database

00:12:31,020 --> 00:12:37,500
never configured we can alert

00:12:34,520 --> 00:12:38,490
accordingly to the it mean and the main

00:12:37,500 --> 00:12:41,730
is main server

00:12:38,490 --> 00:12:45,029
so when sometimes because of the huge

00:12:41,730 --> 00:12:47,520
logs within the management server we see

00:12:45,029 --> 00:12:53,579
heap heap out of space in the system

00:12:47,520 --> 00:12:56,130
logs so before going to that state we

00:12:53,579 --> 00:12:57,839
can we can monitor the storage and

00:12:56,130 --> 00:13:01,529
memory of the main admin server and

00:12:57,839 --> 00:13:06,329
immediately we can alert that means and

00:13:01,529 --> 00:13:08,160
we can also consider system BMS virtual

00:13:06,329 --> 00:13:13,230
routers secondary storage and the

00:13:08,160 --> 00:13:22,470
console proxy VM and and the load on

00:13:13,230 --> 00:13:28,709
them so that there is zero downtime and

00:13:22,470 --> 00:13:31,230
these are the algorithm so basically we

00:13:28,709 --> 00:13:36,150
tried different models and algorithms

00:13:31,230 --> 00:13:45,360
for some use cases so these are the

00:13:36,150 --> 00:13:48,720
below methods are try it out and from

00:13:45,360 --> 00:13:51,449
that we can peek you what gives the best

00:13:48,720 --> 00:13:55,829
performance and the accuracy so the

00:13:51,449 --> 00:14:00,959
results which which we get from our a or

00:13:55,829 --> 00:14:01,500
MN thing is this we shade couple of log

00:14:00,959 --> 00:14:05,699
files

00:14:01,500 --> 00:14:12,709
so with that they were applied this alga

00:14:05,699 --> 00:14:17,160
these methods and so we have seen

00:14:12,709 --> 00:14:20,000
support classification use the best best

00:14:17,160 --> 00:14:20,000
accuracy

00:14:22,880 --> 00:14:28,589
prediction using the log file so how you

00:14:25,790 --> 00:14:31,829
we predict a long and so using a

00:14:28,589 --> 00:14:35,760
management server we tagged various

00:14:31,829 --> 00:14:38,070
debug files like it can be error warning

00:14:35,760 --> 00:14:38,790
or info and the entities can be mean

00:14:38,070 --> 00:14:42,060
adminserver

00:14:38,790 --> 00:14:44,160
the log can state its belong to

00:14:42,060 --> 00:14:46,649
management server or virtual router

00:14:44,160 --> 00:14:50,820
primary storage second restore is a

00:14:46,649 --> 00:14:53,490
database host or network and the

00:14:50,820 --> 00:14:58,589
particular log can be a normal love or

00:14:53,490 --> 00:15:02,040
error and we can say if the this is a

00:14:58,589 --> 00:15:03,400
error log what kind of error it is what

00:15:02,040 --> 00:15:09,490
kind of validities

00:15:03,400 --> 00:15:14,010
so we can define the failure type so if

00:15:09,490 --> 00:15:16,690
we have a set of data maybe we can train

00:15:14,010 --> 00:15:19,540
set of data maybe we can partition that

00:15:16,690 --> 00:15:23,980
and we can train with some set of data

00:15:19,540 --> 00:15:26,020
and using the same model we can test

00:15:23,980 --> 00:15:30,100
with the rest of the data so that we can

00:15:26,020 --> 00:15:41,830
see what what is the accuracy with that

00:15:30,100 --> 00:15:49,900
model this is just yeah something small

00:15:41,830 --> 00:15:54,940
to start with and see so what we're

00:15:49,900 --> 00:16:00,400
trying to do is classify the lines you

00:15:54,940 --> 00:16:16,950
know and then it subscripts to auto

00:16:00,400 --> 00:16:16,950
categorize etc and then T by 2 Omega C

00:16:18,180 --> 00:16:46,480
predict which one once we have that the

00:16:27,490 --> 00:16:55,630
live small apology so we kind of start

00:16:46,480 --> 00:16:59,040
small projects like these this is what

00:16:55,630 --> 00:16:59,040
we try is what we go on

00:17:24,470 --> 00:18:13,440
yes maybe we should you should start

00:18:11,340 --> 00:18:16,950
seeing if you can come into this

00:18:13,440 --> 00:18:19,140
systematic yeah architecture diagram the

00:18:16,950 --> 00:18:21,929
beginning yes we have something in there

00:18:19,140 --> 00:18:23,750
that I will just question yeah I guess

00:18:21,929 --> 00:18:27,090
like the way you have to take it further

00:18:23,750 --> 00:18:29,580
you have the assistant store that's a

00:18:27,090 --> 00:18:32,940
big data repository off to what side

00:18:29,580 --> 00:18:35,040
yes but you showed the flow through

00:18:32,940 --> 00:18:38,640
takes for extraction to the production

00:18:35,040 --> 00:18:42,059
model structure tension yes so you know

00:18:38,640 --> 00:18:46,530
this is not really a concrete physically

00:18:42,059 --> 00:18:50,100
see that so this is taking structure the

00:18:46,530 --> 00:18:54,000
idea of this is to suggest that some of

00:18:50,100 --> 00:18:57,170
the models will work on memory and some

00:18:54,000 --> 00:18:57,170
of the models will

00:19:09,650 --> 00:19:15,930
to retrain so that will address will be

00:19:15,000 --> 00:19:20,130
pleased

00:19:15,930 --> 00:19:23,310
so actually and it is you will make

00:19:20,130 --> 00:19:26,850
decisions on sliding window of data you

00:19:23,310 --> 00:19:28,430
so you will not typically create the

00:19:26,850 --> 00:19:30,750
same model from beginning at the time

00:19:28,430 --> 00:19:33,450
because you voted at the last few

00:19:30,750 --> 00:19:38,400
whatever just last eight days and see if

00:19:33,450 --> 00:19:41,010
your patterns change you have to pick up

00:19:38,400 --> 00:19:45,990
in the one which you want so you will

00:19:41,010 --> 00:19:47,850
have short term models now maybe you've

00:19:45,990 --> 00:19:49,890
seen emphasis here is we have a great

00:19:47,850 --> 00:19:52,320
fix so protected inside everything

00:19:49,890 --> 00:19:54,540
system but that can happen is it shows

00:19:52,320 --> 00:19:59,910
in see that is correct Rose happened

00:19:54,540 --> 00:20:02,010
along a passage like that so that's why

00:19:59,910 --> 00:20:03,810
this way if you see in the model type in

00:20:02,010 --> 00:20:04,380
the model watch the award Phyllis

00:20:03,810 --> 00:20:06,930
patterns

00:20:04,380 --> 00:20:09,950
you got usage by the usage families you

00:20:06,930 --> 00:20:13,680
know your usage analysis how is he doing

00:20:09,950 --> 00:20:15,180
with it well now or quicker than what we

00:20:13,680 --> 00:20:17,390
had thought because people have you know

00:20:15,180 --> 00:20:17,390
suddenly

00:20:26,510 --> 00:20:35,630
the only break in this presentation is

00:20:28,950 --> 00:20:35,630
we we started this

00:20:38,990 --> 00:20:46,850
or Amy sushi from the loft raccoons

00:20:44,279 --> 00:20:49,710
like repeated errors in certain way in

00:20:46,850 --> 00:20:52,080
copyright frequency if the model itself

00:20:49,710 --> 00:20:54,330
can say you are making these kind of

00:20:52,080 --> 00:20:57,330
problems or your system is running with

00:20:54,330 --> 00:21:00,299
footage TV sperm like you know Kapoor

00:20:57,330 --> 00:21:02,789
the I haven't occurred that's the cool

00:21:00,299 --> 00:21:05,480
now do you see this is a change that

00:21:02,789 --> 00:21:08,009
he's having to do a transaction

00:21:05,480 --> 00:21:13,350
structure naturally General Authorities

00:21:08,009 --> 00:21:16,110
and right so so this is the bigger thing

00:21:13,350 --> 00:21:19,049
about is we are not suggesting this to

00:21:16,110 --> 00:21:22,529
be II kind of closely associated with

00:21:19,049 --> 00:21:24,750
CloudStack any work that you do EML has

00:21:22,529 --> 00:21:30,809
to be for a particular domain particular

00:21:24,750 --> 00:21:42,090
thing so algorithms will be the same or

00:21:30,809 --> 00:21:44,129
similar in any infrastructure I mean

00:21:42,090 --> 00:21:51,509
that what I meant was they should be the

00:21:44,129 --> 00:21:54,960
polity similarity etcetera so it is

00:21:51,509 --> 00:21:59,820
those models are seen then we can

00:21:54,960 --> 00:22:10,590
probably similar thing but if you take

00:21:59,820 --> 00:22:12,240
logs right launch system is different so

00:22:10,590 --> 00:22:17,960
that's why some of these species are

00:22:12,240 --> 00:22:22,590
specific to our stack some of these so

00:22:17,960 --> 00:22:28,769
if you find a success in particular

00:22:22,590 --> 00:22:33,799
model for processing opens problems if

00:22:28,769 --> 00:22:33,799
you map if you give you the same model

00:23:02,050 --> 00:23:05,119
[Music]

00:23:30,330 --> 00:23:35,630
[Laughter]

00:23:42,870 --> 00:24:18,970
once we we are done with our concept

00:23:47,710 --> 00:24:21,090
maybe we will share so we have taken the

00:24:18,970 --> 00:24:21,090
real

00:26:10,290 --> 00:26:16,380
so how much

00:26:42,850 --> 00:26:45,850
yes

00:27:45,340 --> 00:27:49,660

YouTube URL: https://www.youtube.com/watch?v=sI0GXVVgMrA


