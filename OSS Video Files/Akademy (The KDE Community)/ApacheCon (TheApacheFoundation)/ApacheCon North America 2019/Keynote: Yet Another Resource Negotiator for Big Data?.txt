Title: Keynote: Yet Another Resource Negotiator for Big Data?
Publication date: 2019-09-12
Playlist: ApacheCon North America 2019
Description: 
	The core of Google Cloud's approach to data analytics is meeting customer needs, which means embracing the best of Apache technologies. In this session, we will give a brief overview of the work that the Google Cloud team is doing to make Apache big data software cloud native, yet portable, through Kubernetes enhancements.

Join this session to learn more about our Kubernetes approach to big data and our development of open source Kubernetes Operators that provide packaged control planes for running Apache applications inside of a Kubernetes cluster. These operators help abstract Google?s knowledge of running massive data analytics systems into open source software. This session will also include a short primer on using Kubernetes for data processing, a comparison of YARN vs Kubernetes, our current offerings, and our future plans
Captions: 
	00:00:00,210 --> 00:00:04,890
so quick show of hands who here already

00:00:02,129 --> 00:00:06,629
knows what kubernetes is okay vast

00:00:04,890 --> 00:00:07,890
majority that's what I assumed so you're

00:00:06,629 --> 00:00:11,460
not getting any of what kubernetes is

00:00:07,890 --> 00:00:13,410
I'm just jumping right in so as I was

00:00:11,460 --> 00:00:15,900
introduced I am a product manager here

00:00:13,410 --> 00:00:19,230
at Google cloud focused on our open

00:00:15,900 --> 00:00:20,630
source data and analytics space and I'm

00:00:19,230 --> 00:00:24,150
super excited to be at this conference

00:00:20,630 --> 00:00:26,490
because Google we have a long history of

00:00:24,150 --> 00:00:28,260
solving these data problems with the

00:00:26,490 --> 00:00:30,599
open source community especially with

00:00:28,260 --> 00:00:32,820
the Apache community this goes all the

00:00:30,599 --> 00:00:35,610
way back to the original MapReduce paper

00:00:32,820 --> 00:00:38,879
that we put out there that the community

00:00:35,610 --> 00:00:43,079
and Yahoo took implemented and became

00:00:38,879 --> 00:00:45,390
apache hadoop more recently we've put

00:00:43,079 --> 00:00:49,399
out entire software stacks for things

00:00:45,390 --> 00:00:53,160
like big data processing with beam or

00:00:49,399 --> 00:00:55,230
tensorflow for machine learning and on

00:00:53,160 --> 00:00:57,870
the flip side of that we also now have

00:00:55,230 --> 00:01:00,180
Google Cloud which gives you the ability

00:00:57,870 --> 00:01:03,239
to run a lot of these open source

00:01:00,180 --> 00:01:05,100
software stacks without having to deal

00:01:03,239 --> 00:01:07,650
with the underlying infrastructure

00:01:05,100 --> 00:01:09,330
components hard drive servers networking

00:01:07,650 --> 00:01:12,750
all that you can pretty much abstract

00:01:09,330 --> 00:01:14,670
away and just hand us the software so

00:01:12,750 --> 00:01:17,700
some very quick examples of what I'm

00:01:14,670 --> 00:01:20,250
talking about here is if you want to

00:01:17,700 --> 00:01:23,460
give us a tensorflow machine learning

00:01:20,250 --> 00:01:26,210
model and we will host that for you we

00:01:23,460 --> 00:01:28,170
will right-size all of the appropriate

00:01:26,210 --> 00:01:30,689
infrastructure to do the model training

00:01:28,170 --> 00:01:33,659
and you can hand that off to Cloud ml

00:01:30,689 --> 00:01:35,700
engine for pre-processing a lot of time

00:01:33,659 --> 00:01:38,790
for those same machine learning

00:01:35,700 --> 00:01:40,680
pipelines we have a patchy beam which

00:01:38,790 --> 00:01:43,409
helps you with both streaming and batch

00:01:40,680 --> 00:01:45,030
analytics you can send us your Java code

00:01:43,409 --> 00:01:48,170
and without having to deal with any

00:01:45,030 --> 00:01:51,360
servers we will just run that for you

00:01:48,170 --> 00:01:54,600
recently through an acquisition we've

00:01:51,360 --> 00:01:57,540
started working with cask or feed up

00:01:54,600 --> 00:02:01,500
again Woolman source software what this

00:01:57,540 --> 00:02:03,719
is it's a full GUI ETL where you can

00:02:01,500 --> 00:02:06,299
take you know an advanced Kafka pipe

00:02:03,719 --> 00:02:08,940
split that up start sending to our DBMS

00:02:06,299 --> 00:02:11,160
as various places without ever having to

00:02:08,940 --> 00:02:13,920
write a line of code and the managed

00:02:11,160 --> 00:02:16,110
version of that again Cloud Data fusion

00:02:13,920 --> 00:02:17,760
and finally probably something a lot of

00:02:16,110 --> 00:02:20,310
you are familiar with workflow

00:02:17,760 --> 00:02:22,290
orchestration with Apache airflow if you

00:02:20,310 --> 00:02:24,030
want to just hand off that dagger jobs

00:02:22,290 --> 00:02:25,560
to us and we'll take care of all doing

00:02:24,030 --> 00:02:27,540
before it's maintenance all of the

00:02:25,560 --> 00:02:29,750
server infrastructure that's cloud

00:02:27,540 --> 00:02:32,819
composer

00:02:29,750 --> 00:02:35,250
I'm personally from our Cloud Data proc

00:02:32,819 --> 00:02:38,190
team our elevator pitch here

00:02:35,250 --> 00:02:42,360
it's a pat managed Hadoop and Apache

00:02:38,190 --> 00:02:44,670
spark reality is that it's really this

00:02:42,360 --> 00:02:47,040
open source ecosystem I call it like an

00:02:44,670 --> 00:02:50,010
open source engine for running all sorts

00:02:47,040 --> 00:02:51,600
of various open source software and you

00:02:50,010 --> 00:02:53,640
might be looking at these logos and even

00:02:51,600 --> 00:02:55,709
thinking right now you know Google Cloud

00:02:53,640 --> 00:02:57,269
we offer a managed version of some of

00:02:55,709 --> 00:02:59,850
this you know these same open source

00:02:57,269 --> 00:03:02,370
icons so we have cloud BigTable a

00:02:59,850 --> 00:03:03,959
managed knows sequel database but if

00:03:02,370 --> 00:03:07,860
your preference is just to run Apache

00:03:03,959 --> 00:03:09,480
HBase our team is here our mission is to

00:03:07,860 --> 00:03:13,860
really help you just have the best open

00:03:09,480 --> 00:03:16,590
source experience regardless now most of

00:03:13,860 --> 00:03:18,900
our team's focus has been on yarn yet

00:03:16,590 --> 00:03:21,090
another resource negotiator this is a

00:03:18,900 --> 00:03:23,430
resource negotiator for big data put out

00:03:21,090 --> 00:03:24,959
under Apache Hadoop we're going on we're

00:03:23,430 --> 00:03:27,090
coming up on ten years pretty soon of

00:03:24,959 --> 00:03:29,010
this so pretty battle tested and what

00:03:27,090 --> 00:03:31,829
the cloud data proc service provides is

00:03:29,010 --> 00:03:33,930
we have a back end control plane that

00:03:31,829 --> 00:03:36,660
we'll take a look at a lot of this yarn

00:03:33,930 --> 00:03:38,340
management will make sure that your jobs

00:03:36,660 --> 00:03:40,890
are actually getting completed clusters

00:03:38,340 --> 00:03:45,510
are getting torn down and you know the

00:03:40,890 --> 00:03:47,970
jobs are resourced appropriately but

00:03:45,510 --> 00:03:51,510
yeren it has some pretty significant

00:03:47,970 --> 00:03:54,239
pain points first off the management can

00:03:51,510 --> 00:03:57,540
be pretty difficult because yarn it

00:03:54,239 --> 00:04:00,720
originally was developed for unfriend

00:03:57,540 --> 00:04:02,790
bare metal servers and so we ported it

00:04:00,720 --> 00:04:05,910
to VMs and that's helped a little bit

00:04:02,790 --> 00:04:08,459
but it's still a pretty loaded stack so

00:04:05,910 --> 00:04:10,470
you might find yourself with a lot of

00:04:08,459 --> 00:04:12,180
open-source components that have taken

00:04:10,470 --> 00:04:13,410
dependencies on other open-source

00:04:12,180 --> 00:04:15,810
components you weren't really expecting

00:04:13,410 --> 00:04:17,639
so you go to run from sparks equal jobs

00:04:15,810 --> 00:04:19,410
and then all of a sudden you know you

00:04:17,639 --> 00:04:21,570
have some weird hive dependency things

00:04:19,410 --> 00:04:25,260
like that and that really just

00:04:21,570 --> 00:04:27,139
complicates this open source stat and if

00:04:25,260 --> 00:04:28,610
like there's just a lot of version

00:04:27,139 --> 00:04:30,710
and dependency management you'll have to

00:04:28,610 --> 00:04:32,419
deal with in a typical yarn base cluster

00:04:30,710 --> 00:04:35,270
if you don't know what I'm talking about

00:04:32,419 --> 00:04:37,490
here go ask your Hadoop admin who's had

00:04:35,270 --> 00:04:39,020
to like install the newest version of

00:04:37,490 --> 00:04:40,879
the zeppelin all of a sudden like a

00:04:39,020 --> 00:04:42,710
butterfly flaps its wings you've just

00:04:40,879 --> 00:04:44,569
stepped on some java class path and path

00:04:42,710 --> 00:04:46,129
and like every spark job breaks so

00:04:44,569 --> 00:04:48,620
that's the kind of stuff that i'm

00:04:46,129 --> 00:04:50,150
talking about here and then finally you

00:04:48,620 --> 00:04:54,259
know because of all these dependencies

00:04:50,150 --> 00:04:56,629
isolation it gets really hard so what

00:04:54,259 --> 00:04:58,789
folks tend to do when they move from

00:04:56,629 --> 00:05:01,789
on-prem to the cloud with these big data

00:04:58,789 --> 00:05:04,340
stacks is they start to piece up all the

00:05:01,789 --> 00:05:07,009
different workloads to run those on an

00:05:04,340 --> 00:05:09,500
appropriate sized cluster or opiates

00:05:07,009 --> 00:05:12,169
isin shape really so you might have a

00:05:09,500 --> 00:05:13,819
lot of bi or reported applications that

00:05:12,169 --> 00:05:16,250
you'll try to stick on to a memory heavy

00:05:13,819 --> 00:05:17,539
cluster or you'll have a bunch of

00:05:16,250 --> 00:05:20,120
machine learning job so you'll stick on

00:05:17,539 --> 00:05:21,680
to these compute heavy clusters but

00:05:20,120 --> 00:05:23,300
piecing all that up and figuring those

00:05:21,680 --> 00:05:24,979
out and like what jobs align with each

00:05:23,300 --> 00:05:29,150
other that could be a pretty difficult

00:05:24,979 --> 00:05:31,339
task so this is why Google along with

00:05:29,150 --> 00:05:34,699
the open-source community we worked to

00:05:31,339 --> 00:05:40,039
bring kubernetes as another scheduler

00:05:34,699 --> 00:05:41,870
for Apache spark now this was definitely

00:05:40,039 --> 00:05:45,169
done with a lot of the community and so

00:05:41,870 --> 00:05:48,349
now with apache spark you can run as a

00:05:45,169 --> 00:05:52,099
scheduler yarn masive standalone mode or

00:05:48,349 --> 00:05:55,039
kubernetes which is now experimental and

00:05:52,099 --> 00:05:57,889
this hopefully is going to give you some

00:05:55,039 --> 00:05:59,479
pretty big benefits first of all it's

00:05:57,889 --> 00:06:03,620
gonna give you this unified resource

00:05:59,479 --> 00:06:06,409
management layer what I mean here is a

00:06:03,620 --> 00:06:08,360
lot of customers at least I work with

00:06:06,409 --> 00:06:09,409
they are trying to move all their web

00:06:08,360 --> 00:06:11,539
applications or their business

00:06:09,409 --> 00:06:14,000
applications into kubernetes have one

00:06:11,539 --> 00:06:16,279
cluster management system that use a

00:06:14,000 --> 00:06:18,319
trained people up on but then they have

00:06:16,279 --> 00:06:20,000
this kind of silo big data cluster

00:06:18,319 --> 00:06:21,259
that's still running yarn and they want

00:06:20,000 --> 00:06:22,699
to get away from that and start running

00:06:21,259 --> 00:06:25,339
the rest of their Apache Big Data

00:06:22,699 --> 00:06:28,129
software in that same crudities cluster

00:06:25,339 --> 00:06:29,839
and as they do that they definitely want

00:06:28,129 --> 00:06:32,300
to isolate a lot of these open source

00:06:29,839 --> 00:06:33,439
jobs so instead of running various

00:06:32,300 --> 00:06:34,789
different jobs with different

00:06:33,439 --> 00:06:37,849
dependencies and conflicting with each

00:06:34,789 --> 00:06:39,169
other on a single cluster you know take

00:06:37,849 --> 00:06:40,150
a lot of those dependencies a lot of

00:06:39,169 --> 00:06:42,130
that versions

00:06:40,150 --> 00:06:43,360
stick those into a container and then

00:06:42,130 --> 00:06:44,830
you can just be running you know two

00:06:43,360 --> 00:06:47,230
different versions of spark on the same

00:06:44,830 --> 00:06:49,240
underlying cluster and it's fine you

00:06:47,230 --> 00:06:51,010
don't have to go back and upgrade you

00:06:49,240 --> 00:06:53,830
know one version of spark just because

00:06:51,010 --> 00:06:55,600
you know some data scientist says hey I

00:06:53,830 --> 00:06:58,530
need this new version this new feature

00:06:55,600 --> 00:07:02,320
and that really is gonna lead to more

00:06:58,530 --> 00:07:07,270
resilient infrastructure where you don't

00:07:02,320 --> 00:07:08,889
necessarily have to be where you can

00:07:07,270 --> 00:07:10,419
basically be installing you know the

00:07:08,889 --> 00:07:12,280
latest security patches on the

00:07:10,419 --> 00:07:14,800
infrastructure itself and not have to

00:07:12,280 --> 00:07:20,200
worry about going back and manipulating

00:07:14,800 --> 00:07:22,450
all these old dud jobs so here at Google

00:07:20,200 --> 00:07:24,669
this is the way we're approaching Big

00:07:22,450 --> 00:07:27,490
Data accrue Benes applications I call it

00:07:24,669 --> 00:07:29,860
kind of our good better best approach so

00:07:27,490 --> 00:07:31,630
we are directly working with the open

00:07:29,860 --> 00:07:34,900
source community as much as we can to

00:07:31,630 --> 00:07:37,900
provide a self managed version where

00:07:34,900 --> 00:07:39,639
we're taking a lot of these Apache or

00:07:37,900 --> 00:07:41,740
just open source big data applications

00:07:39,639 --> 00:07:43,660
and making them more cloud or more

00:07:41,740 --> 00:07:46,630
kubernetes native and giving that back

00:07:43,660 --> 00:07:48,789
for folks to use wherever but then if

00:07:46,630 --> 00:07:50,650
you want to abstract away a lot of that

00:07:48,789 --> 00:07:52,600
infrastructure kind of how I showed you

00:07:50,650 --> 00:07:54,639
in the first couple slides with the

00:07:52,600 --> 00:07:56,110
management that we provide that's where

00:07:54,639 --> 00:07:58,780
something like a Cloud Data proc would

00:07:56,110 --> 00:08:01,599
come in on top of that kubernetes

00:07:58,780 --> 00:08:03,520
infrastructure and then there's what I

00:08:01,599 --> 00:08:05,800
call kind of our best option which is

00:08:03,520 --> 00:08:09,160
all the docker containers that we have

00:08:05,800 --> 00:08:12,370
for cloud data proc or open sourcing

00:08:09,160 --> 00:08:14,470
those four for customers but as well as

00:08:12,370 --> 00:08:17,620
our partners that might want to extend

00:08:14,470 --> 00:08:19,419
that abstraction of infrastructure so

00:08:17,620 --> 00:08:22,860
that way you know a lot of these open

00:08:19,419 --> 00:08:25,330
source partners that we have that offer

00:08:22,860 --> 00:08:27,520
focus on a particular software stack

00:08:25,330 --> 00:08:29,289
they don't have to get hung up on doing

00:08:27,520 --> 00:08:31,360
a bunch of cloud integration dealing

00:08:29,289 --> 00:08:33,719
with security and logging and basic API

00:08:31,360 --> 00:08:36,120
calls they can just build on top of the

00:08:33,719 --> 00:08:38,830
cloud data proc docker containers

00:08:36,120 --> 00:08:41,529
essentially get all that built in and

00:08:38,830 --> 00:08:43,570
then really focus on just the open

00:08:41,529 --> 00:08:45,610
source software stack itself and

00:08:43,570 --> 00:08:47,170
managing that for their customers

00:08:45,610 --> 00:08:50,290
providing them great you know service

00:08:47,170 --> 00:08:52,150
level agreement all of that now the way

00:08:50,290 --> 00:08:53,860
we're making this happen is via

00:08:52,150 --> 00:08:56,389
kubernetes operate

00:08:53,860 --> 00:08:58,670
if you don't know what that is that's

00:08:56,389 --> 00:09:00,699
essentially an application control plane

00:08:58,670 --> 00:09:03,980
for running these really complex

00:09:00,699 --> 00:09:06,740
applications in kubernetes it's really

00:09:03,980 --> 00:09:09,079
an opinionated way that Google is going

00:09:06,740 --> 00:09:12,079
to provide for doing a lot of this stuff

00:09:09,079 --> 00:09:14,300
so it takes a lot of the knowledge that

00:09:12,079 --> 00:09:16,910
Google has of running these applicated

00:09:14,300 --> 00:09:19,579
applications for years and then stick

00:09:16,910 --> 00:09:20,930
those into a very discriminating

00:09:19,579 --> 00:09:22,970
architecture here you go

00:09:20,930 --> 00:09:24,529
spin it up and then you have that you

00:09:22,970 --> 00:09:27,139
know Google provisioned infrastructure

00:09:24,529 --> 00:09:28,670
in kubernetes this is done through

00:09:27,139 --> 00:09:30,980
extending some functionality that

00:09:28,670 --> 00:09:34,430
kubernetes already has where you get a

00:09:30,980 --> 00:09:36,680
custom resource definition meaning you

00:09:34,430 --> 00:09:39,110
can extend the language of kubernetes to

00:09:36,680 --> 00:09:41,089
make kubernetes actually look and feel a

00:09:39,110 --> 00:09:42,800
lot more like that actual application

00:09:41,089 --> 00:09:45,620
that you're trying to run so you don't

00:09:42,800 --> 00:09:47,690
have to understand all the nuances or

00:09:45,620 --> 00:09:49,730
details of kubernetes you can work with

00:09:47,690 --> 00:09:53,120
something like spark how you'd expect to

00:09:49,730 --> 00:09:56,320
work with spark so our first venture

00:09:53,120 --> 00:09:59,810
into this was with the release of a

00:09:56,320 --> 00:10:02,209
spark kubernetes operator that is the

00:09:59,810 --> 00:10:03,829
github link we've been working with some

00:10:02,209 --> 00:10:06,019
other big companies on this both

00:10:03,829 --> 00:10:08,630
Microsoft and IBM have collaborated with

00:10:06,019 --> 00:10:11,420
us here and this just gives you a lot of

00:10:08,630 --> 00:10:13,459
that basic set up for running spark

00:10:11,420 --> 00:10:14,240
applications along with some cloud

00:10:13,459 --> 00:10:16,850
integrations

00:10:14,240 --> 00:10:19,399
things like direct access to bigquery

00:10:16,850 --> 00:10:23,060
our serverless data warehouse storage or

00:10:19,399 --> 00:10:25,130
a replacement for HDFS with cloud

00:10:23,060 --> 00:10:25,880
storage so with a simple prefix change

00:10:25,130 --> 00:10:29,959
in your code

00:10:25,880 --> 00:10:31,519
going from HDFS to GS you can have you

00:10:29,959 --> 00:10:33,440
know cloud storage made available to you

00:10:31,519 --> 00:10:36,980
there's also integration with logging

00:10:33,440 --> 00:10:39,769
and some functionality again this is

00:10:36,980 --> 00:10:43,000
that custom resource definition of spark

00:10:39,769 --> 00:10:45,110
control where it lets you do some pretty

00:10:43,000 --> 00:10:47,660
clever things like you'd be working

00:10:45,110 --> 00:10:49,519
locally and you have all these packages

00:10:47,660 --> 00:10:50,779
that are on your local laptop and then

00:10:49,519 --> 00:10:52,639
when you're ready to send those up to

00:10:50,779 --> 00:10:55,540
cribben Eddy's the spark control help

00:10:52,639 --> 00:10:58,040
you package all that and ship that on

00:10:55,540 --> 00:11:01,339
there's a couple ways to go and deploy

00:10:58,040 --> 00:11:04,370
this if you're in Google Cloud itself

00:11:01,339 --> 00:11:07,310
what you can do is you can come into the

00:11:04,370 --> 00:11:09,590
console and search our marketplace for

00:11:07,310 --> 00:11:12,110
our spark operator and you'll find a

00:11:09,590 --> 00:11:14,180
deployment option available where you

00:11:12,110 --> 00:11:15,380
can click Next a couple times and now

00:11:14,180 --> 00:11:17,300
you're up and running a kubernetes

00:11:15,380 --> 00:11:21,140
cluster or you've just deployed that

00:11:17,300 --> 00:11:22,550
operator into kubernetes but you don't

00:11:21,140 --> 00:11:23,390
want to be in Google Cloud that's

00:11:22,550 --> 00:11:26,480
totally fine by us

00:11:23,390 --> 00:11:27,860
we also offer a helmet art if you don't

00:11:26,480 --> 00:11:29,540
know what that is think of basically

00:11:27,860 --> 00:11:32,450
like an app to get for kubernetes and

00:11:29,540 --> 00:11:34,490
you can take that same operator with all

00:11:32,450 --> 00:11:36,710
the Google you know architecture

00:11:34,490 --> 00:11:39,350
patterns best practices and deploy that

00:11:36,710 --> 00:11:42,920
in AWS untrimmed wherever you want to go

00:11:39,350 --> 00:11:44,720
with it so spark was the first one of

00:11:42,920 --> 00:11:46,640
these operators we've put out there but

00:11:44,720 --> 00:11:49,430
it's not going to be the last just last

00:11:46,640 --> 00:11:51,650
week we open sourced our operator for

00:11:49,430 --> 00:11:53,779
flink which again is going to give the

00:11:51,650 --> 00:11:59,480
control plane best practices for running

00:11:53,779 --> 00:12:01,760
flink in the crowd with kubernetes ok so

00:11:59,480 --> 00:12:03,320
you all just got the sales pitch and you

00:12:01,760 --> 00:12:05,089
know strategy for what we're doing at

00:12:03,320 --> 00:12:06,470
Google cloud but in the last couple

00:12:05,089 --> 00:12:08,990
minutes here I just want to give a quick

00:12:06,470 --> 00:12:10,670
kind of realistic expectations and

00:12:08,990 --> 00:12:13,040
things to think about as you're making

00:12:10,670 --> 00:12:15,800
this transition from yarn to kubernetes

00:12:13,040 --> 00:12:16,610
because yarn it's been out there for

00:12:15,800 --> 00:12:17,960
almost a decade

00:12:16,610 --> 00:12:20,960
it's definitely battle tested its

00:12:17,960 --> 00:12:22,460
production already and so look it's

00:12:20,960 --> 00:12:23,930
probably not gonna be an easy flip of

00:12:22,460 --> 00:12:25,490
the switch just to move over to

00:12:23,930 --> 00:12:28,250
kubernetes so just a couple things to

00:12:25,490 --> 00:12:29,830
think about and I'm gonna compare these

00:12:28,250 --> 00:12:32,300
inter like ok what's good what's bad

00:12:29,830 --> 00:12:34,610
give you the yin and the yang here of

00:12:32,300 --> 00:12:36,830
going from yarn to kubernetes so first

00:12:34,610 --> 00:12:39,589
of all kubernetes it's gonna give you

00:12:36,830 --> 00:12:41,480
that unified interface if you are

00:12:39,589 --> 00:12:43,310
already moving to this kubernetes world

00:12:41,480 --> 00:12:45,740
if you already have applications that

00:12:43,310 --> 00:12:47,060
are running in kubernetes but if you're

00:12:45,740 --> 00:12:49,070
all on OpenStack

00:12:47,060 --> 00:12:51,020
already this might just feel like yet

00:12:49,070 --> 00:12:53,209
another cluster type to manage if you're

00:12:51,020 --> 00:12:56,660
not already investing in that kubernetes

00:12:53,209 --> 00:12:58,760
ecosystem it's going to let your data

00:12:56,660 --> 00:13:01,670
scientists and developers tap in to a

00:12:58,760 --> 00:13:02,930
lot of unused kubernetes resources so

00:13:01,670 --> 00:13:04,220
maybe you have all of your web

00:13:02,930 --> 00:13:06,230
application of your business

00:13:04,220 --> 00:13:08,270
applications running in kubernetes nine

00:13:06,230 --> 00:13:11,120
to five that servers cranky and it's

00:13:08,270 --> 00:13:13,190
busy but then after five load gets

00:13:11,120 --> 00:13:14,959
loosened up you can start sending your

00:13:13,190 --> 00:13:17,730
big data applications to that same

00:13:14,959 --> 00:13:20,309
hardware and make use of that processing

00:13:17,730 --> 00:13:22,709
the flipside of that is if you've tried

00:13:20,309 --> 00:13:24,660
to do a lot of that already with yarn

00:13:22,709 --> 00:13:26,519
tuning meaning you know setting up your

00:13:24,660 --> 00:13:28,139
yarn keys using yarn labels to track

00:13:26,519 --> 00:13:29,429
what's what that's all going to get

00:13:28,139 --> 00:13:30,689
thrown out as you move to kubernetes

00:13:29,429 --> 00:13:31,859
just because kubernetes just has a

00:13:30,689 --> 00:13:34,040
different way that you're gonna manage

00:13:31,859 --> 00:13:34,040
your resources

00:13:34,429 --> 00:13:38,429
developers they are going to love

00:13:36,720 --> 00:13:40,739
kubernetes because they can start to put

00:13:38,429 --> 00:13:42,209
in all this custom configuration even if

00:13:40,739 --> 00:13:43,949
they want to rip out like what the base

00:13:42,209 --> 00:13:45,959
operating system looks like and bring in

00:13:43,949 --> 00:13:48,419
their own container to do that they can

00:13:45,959 --> 00:13:49,799
do that but with that you're definitely

00:13:48,419 --> 00:13:53,699
want to track what they're actually

00:13:49,799 --> 00:13:55,470
doing and so yarn this was I don't know

00:13:53,699 --> 00:13:57,839
if it was easy but it's definitely we

00:13:55,470 --> 00:13:59,879
know how to do it now the audit logs

00:13:57,839 --> 00:14:01,679
tend to be in two places one for the

00:13:59,879 --> 00:14:03,779
resource manager one for the node

00:14:01,679 --> 00:14:06,059
manager at this point most companies

00:14:03,779 --> 00:14:07,679
know exactly what those logs look like

00:14:06,059 --> 00:14:09,839
what to look for what to alert on

00:14:07,679 --> 00:14:12,389
kubernetes we definitely have logging

00:14:09,839 --> 00:14:14,129
that we're pulling in and integration

00:14:12,389 --> 00:14:15,629
with cloud logging but you're gonna have

00:14:14,129 --> 00:14:20,009
to rethink you know what those logs

00:14:15,629 --> 00:14:22,259
actually look like with kubernetes you

00:14:20,009 --> 00:14:24,749
can start to get away from having to

00:14:22,259 --> 00:14:26,399
like administer an entire Khanda

00:14:24,749 --> 00:14:28,769
environment to keep track of all the

00:14:26,399 --> 00:14:30,329
different various configurations see

00:14:28,769 --> 00:14:32,160
this is going to allow for really more

00:14:30,329 --> 00:14:34,350
targeted upgrades because you can

00:14:32,160 --> 00:14:36,689
package exactly the functionality you

00:14:34,350 --> 00:14:39,239
want into a particular docker container

00:14:36,689 --> 00:14:40,980
associated with that job so everybody

00:14:39,239 --> 00:14:42,839
might be on an older version of spark

00:14:40,980 --> 00:14:44,429
that's production tested you have one

00:14:42,839 --> 00:14:45,689
data scientist that's that's really want

00:14:44,429 --> 00:14:47,459
this new feature in the latest version

00:14:45,689 --> 00:14:48,959
of SPARC they can package that in the

00:14:47,459 --> 00:14:50,819
container run into all the same

00:14:48,959 --> 00:14:54,509
infrastructure with kubernetes and the

00:14:50,819 --> 00:14:58,049
jobs don't have to conflict but flipside

00:14:54,509 --> 00:14:59,970
of that with all that you know with all

00:14:58,049 --> 00:15:02,970
that power of developers also comes some

00:14:59,970 --> 00:15:05,999
additional responsibilities and so for a

00:15:02,970 --> 00:15:07,649
lot of use cases the developers might

00:15:05,999 --> 00:15:10,230
find themselves dealing with something

00:15:07,649 --> 00:15:13,169
that they didn't expect to maybe like a

00:15:10,230 --> 00:15:14,999
big one that comes up is like kubernetes

00:15:13,169 --> 00:15:16,649
network configurations to get to some

00:15:14,999 --> 00:15:18,959
data source that wasn't part of the

00:15:16,649 --> 00:15:20,129
standard and that's exactly the kind of

00:15:18,959 --> 00:15:23,279
thing that we're trying to address with

00:15:20,129 --> 00:15:24,989
our operators but you know reality is

00:15:23,279 --> 00:15:28,619
we're not gonna hit every use case out

00:15:24,989 --> 00:15:30,869
of the park in the first round you can

00:15:28,619 --> 00:15:31,560
also with kubernetes you don't have to

00:15:30,869 --> 00:15:33,209
start thinking

00:15:31,560 --> 00:15:35,490
you can go from thinking about things in

00:15:33,209 --> 00:15:37,949
a cluster level to just a particular job

00:15:35,490 --> 00:15:40,199
level and assign memory and CPU and

00:15:37,949 --> 00:15:42,499
resources at that job level which is

00:15:40,199 --> 00:15:45,959
great you can really isolate those

00:15:42,499 --> 00:15:48,480
isolate those containers the flipside to

00:15:45,959 --> 00:15:51,269
isolation is a lot of times there's data

00:15:48,480 --> 00:15:53,129
that you do want to share between jobs

00:15:51,269 --> 00:15:54,499
now with the operators we're going to

00:15:53,129 --> 00:15:57,420
give you integrations with cloud storage

00:15:54,499 --> 00:16:00,209
so you can very easily use like an s3 or

00:15:57,420 --> 00:16:03,509
a cloud storage at your source or you or

00:16:00,209 --> 00:16:05,100
your sink which will help but there's

00:16:03,509 --> 00:16:06,809
still a lot of intermediate data things

00:16:05,100 --> 00:16:08,009
like shuffle data things like temp data

00:16:06,809 --> 00:16:09,749
that occasionally you need to share

00:16:08,009 --> 00:16:11,639
between jobs and that can be a little

00:16:09,749 --> 00:16:15,269
bit more difficult in this more isolated

00:16:11,639 --> 00:16:18,269
world and finally the last one I have

00:16:15,269 --> 00:16:20,309
here is kubernetes has a lot of really

00:16:18,269 --> 00:16:22,290
cool features especially around security

00:16:20,309 --> 00:16:25,319
things like our secret manager where you

00:16:22,290 --> 00:16:27,870
can start to drop in you know JDBC

00:16:25,319 --> 00:16:29,999
connections keep those protected for all

00:16:27,870 --> 00:16:32,339
your different you know users but

00:16:29,999 --> 00:16:33,420
flipside is the security can get a

00:16:32,339 --> 00:16:35,100
little bit more complicated as you

00:16:33,420 --> 00:16:37,139
reminds me of like one of those Russian

00:16:35,100 --> 00:16:38,519
dolls where you have accounts within

00:16:37,139 --> 00:16:39,689
accounts within accounts where you have

00:16:38,519 --> 00:16:41,189
like a VM that's running a service

00:16:39,689 --> 00:16:42,420
account then within that there's

00:16:41,189 --> 00:16:43,920
actually a kubernetes service account

00:16:42,420 --> 00:16:46,410
and then with that you might have like

00:16:43,920 --> 00:16:48,059
Kerberos principles so tracking that all

00:16:46,410 --> 00:16:51,420
the way through can sometimes be a

00:16:48,059 --> 00:16:53,670
headache for folks so this is my World

00:16:51,420 --> 00:16:56,430
Wind tour of what we're working on here

00:16:53,670 --> 00:16:58,860
at Google in the open source Big Data

00:16:56,430 --> 00:17:00,750
space one of our developers on the flink

00:16:58,860 --> 00:17:03,089
operator dugong is actually here today

00:17:00,750 --> 00:17:04,740
and so we're very interested in working

00:17:03,089 --> 00:17:06,750
with anybody on these open source

00:17:04,740 --> 00:17:08,669
operators please come talk to me or to

00:17:06,750 --> 00:17:10,649
gong or Grizz here if you want to learn

00:17:08,669 --> 00:17:12,720
more about how to get involved in the

00:17:10,649 --> 00:17:14,610
community here with that I'm just going

00:17:12,720 --> 00:17:16,439
to leave some sessions on that we have

00:17:14,610 --> 00:17:18,240
Google presenting throughout the rest of

00:17:16,439 --> 00:17:20,140
the week and thank you all for coming to

00:17:18,240 --> 00:17:24,179
my session and appreciate the time

00:17:20,140 --> 00:17:24,179

YouTube URL: https://www.youtube.com/watch?v=8W88qAFdAUU


