Title: Tales From The Trenches: Solr Operations
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Search
Description: 
	Tales From The Trenches: Solr Operations
Mike Drob

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

There are many pitfalls that a team can fall into when designing and implementing a new Solr-based search application. We will draw on stories from the presenter's operational experience and distill the events into easy to understand patterns and anti-patterns. Topics covered would include query patterns, indexing patterns, and shard design.

An engineer with over a decade of distributed systems experience, Mike has spent most of his career helping enable others who are using big data platforms. He is a PMC member and committer on several Apache projects, and strongly believes that when people develop breadth in their expertise it builds better software all around. When not working, he enjoys photography, dogs, photography of dogs.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,800 --> 00:00:29,519
hello everybody

00:00:25,760 --> 00:00:32,880
uh welcome to apachecon at home 2020

00:00:29,519 --> 00:00:34,480
the first one that's virtual

00:00:32,880 --> 00:00:35,920
uh in the first session of the search

00:00:34,480 --> 00:00:38,640
track we have mike drop

00:00:35,920 --> 00:00:41,120
from apple uh an engineer with over a

00:00:38,640 --> 00:00:43,200
decade of distributed system experience

00:00:41,120 --> 00:00:44,719
a pmc member and committer on various

00:00:43,200 --> 00:00:46,800
apache projects

00:00:44,719 --> 00:00:49,280
mike will be sharing some tales uh from

00:00:46,800 --> 00:00:50,800
the trenches of solar operations

00:00:49,280 --> 00:00:52,320
uh hope you enjoy this session and the

00:00:50,800 --> 00:00:55,520
rest of the conference uh

00:00:52,320 --> 00:00:59,840
over to you mike thanks angstrom

00:00:55,520 --> 00:01:01,680
um like antrim said i'm a pmc member um

00:00:59,840 --> 00:01:03,680
there's my contact info right there i'm

00:01:01,680 --> 00:01:06,159
drove at apache

00:01:03,680 --> 00:01:07,600
so if you guys um end up really liking

00:01:06,159 --> 00:01:08,640
this or have questions afterwards that

00:01:07,600 --> 00:01:10,880
we don't get to

00:01:08,640 --> 00:01:12,400
um feel free to reach out to me

00:01:10,880 --> 00:01:14,640
hopefully

00:01:12,400 --> 00:01:15,439
we can make a conversation happen or

00:01:14,640 --> 00:01:19,040
even better yet

00:01:15,439 --> 00:01:21,920
ask questions on the mailing lists

00:01:19,040 --> 00:01:23,040
um i've been doing most recently i've

00:01:21,920 --> 00:01:25,119
been doing search stuff

00:01:23,040 --> 00:01:27,680
for the past couple years i've bounced

00:01:25,119 --> 00:01:30,479
around on hbase and hadoop for a little

00:01:27,680 --> 00:01:30,479
bit before that

00:01:30,880 --> 00:01:34,400
lots of different companies ranging from

00:01:32,560 --> 00:01:38,400
startups to now at apple

00:01:34,400 --> 00:01:39,600
a pretty large company and so i've seen

00:01:38,400 --> 00:01:41,680
lots of different sizes lots of

00:01:39,600 --> 00:01:43,840
different flavors uh and that's

00:01:41,680 --> 00:01:45,680
that's kind of colored my experience i

00:01:43,840 --> 00:01:47,920
spend a lot of time trying to make

00:01:45,680 --> 00:01:49,040
operators lives easier for debugging

00:01:47,920 --> 00:01:50,880
troubleshooting

00:01:49,040 --> 00:01:52,159
um i'm really passionate about better

00:01:50,880 --> 00:01:54,399
log messages too

00:01:52,159 --> 00:01:55,200
uh we don't we don't touch on that as

00:01:54,399 --> 00:01:59,119
much but i've got

00:01:55,200 --> 00:02:02,159
other talks about that um and then

00:01:59,119 --> 00:02:02,159
i'm just gonna dive in

00:02:03,200 --> 00:02:06,240
there we go so we're gonna talk a little

00:02:04,799 --> 00:02:08,399
bit about uh

00:02:06,240 --> 00:02:10,239
tails from the trenches different war

00:02:08,399 --> 00:02:13,440
stories you might say or

00:02:10,239 --> 00:02:15,040
um just what what we've experienced from

00:02:13,440 --> 00:02:17,760
solar operations

00:02:15,040 --> 00:02:19,520
um so first i want to i want to define

00:02:17,760 --> 00:02:21,680
this i want to set some expectations

00:02:19,520 --> 00:02:22,640
what are operations we're going to focus

00:02:21,680 --> 00:02:25,040
on

00:02:22,640 --> 00:02:26,239
things that are measurable maybe look at

00:02:25,040 --> 00:02:28,879
slas a little bit

00:02:26,239 --> 00:02:30,720
we're not going to cover um the

00:02:28,879 --> 00:02:31,760
deployment aspect of operations we're

00:02:30,720 --> 00:02:33,280
not going to cover

00:02:31,760 --> 00:02:35,200
you know this is how you should build it

00:02:33,280 --> 00:02:37,599
this is how you should

00:02:35,200 --> 00:02:39,360
um use these kinds of tools for

00:02:37,599 --> 00:02:40,400
automated deployments of course

00:02:39,360 --> 00:02:42,560
you should be automating your

00:02:40,400 --> 00:02:43,519
deployments but that's not that's not my

00:02:42,560 --> 00:02:45,519
focus today

00:02:43,519 --> 00:02:47,440
um we're going to look at a lot of cases

00:02:45,519 --> 00:02:50,080
where a user came and said oh this is

00:02:47,440 --> 00:02:53,200
slow or this is crashing

00:02:50,080 --> 00:02:55,840
and as i was working on this it almost

00:02:53,200 --> 00:02:58,640
turned into a query optimization talk

00:02:55,840 --> 00:03:00,480
which is interesting but it's we'll

00:02:58,640 --> 00:03:01,920
cover some of that we'll cover some more

00:03:00,480 --> 00:03:03,920
of the details

00:03:01,920 --> 00:03:06,640
of why things are the way they are we'll

00:03:03,920 --> 00:03:10,400
talk about solar internals

00:03:06,640 --> 00:03:12,239
and not not all of these have solutions

00:03:10,400 --> 00:03:14,080
that we can automate or solutions that

00:03:12,239 --> 00:03:15,920
we can build into solar

00:03:14,080 --> 00:03:17,280
um i'm happy to have conversations about

00:03:15,920 --> 00:03:19,120
all these but we're gonna we're gonna

00:03:17,280 --> 00:03:22,080
talk about five different stories

00:03:19,120 --> 00:03:24,400
um these are all real completely real

00:03:22,080 --> 00:03:27,360
i'm not going to tell you

00:03:24,400 --> 00:03:29,840
uh some details have been removed to you

00:03:27,360 --> 00:03:32,720
know protect the guilty

00:03:29,840 --> 00:03:35,200
but as they say these are based on real

00:03:32,720 --> 00:03:35,200
stories

00:03:35,360 --> 00:03:38,879
so we have the tale of slow queries uh

00:03:38,239 --> 00:03:41,680
everybody

00:03:38,879 --> 00:03:42,480
everybody gets this everybody always has

00:03:41,680 --> 00:03:44,879
you know hey

00:03:42,480 --> 00:03:46,159
why is my query slow what's going on and

00:03:44,879 --> 00:03:48,959
in this case it was

00:03:46,159 --> 00:03:51,360
it was a user um an application that had

00:03:48,959 --> 00:03:53,200
lots and lots of queries going

00:03:51,360 --> 00:03:54,959
and it was inconsistent performance some

00:03:53,200 --> 00:03:56,560
of them would be slow some of them would

00:03:54,959 --> 00:03:58,720
be fast

00:03:56,560 --> 00:04:01,519
uh occasionally there would be garbage

00:03:58,720 --> 00:04:04,080
collection pauses or out of memories

00:04:01,519 --> 00:04:06,080
and sometimes the the most consistent

00:04:04,080 --> 00:04:08,239
thing about the inconsistency

00:04:06,080 --> 00:04:11,120
was that the first query was fast and

00:04:08,239 --> 00:04:15,200
then later ones would be slower

00:04:11,120 --> 00:04:16,239
and this one this isn't really a new

00:04:15,200 --> 00:04:18,000
problem

00:04:16,239 --> 00:04:20,000
uh it's it's something that i even

00:04:18,000 --> 00:04:21,359
debated putting into this talk because i

00:04:20,000 --> 00:04:23,040
thought well

00:04:21,359 --> 00:04:25,520
you know it's it's like a joke where

00:04:23,040 --> 00:04:27,520
everybody knows the punch line

00:04:25,520 --> 00:04:29,759
um so we'll we'll talk a little bit

00:04:27,520 --> 00:04:31,120
about the solution here

00:04:29,759 --> 00:04:33,680
it's it's that they were doing deep

00:04:31,120 --> 00:04:36,880
paging instead of cursor mark

00:04:33,680 --> 00:04:39,680
and then i was really happy uh

00:04:36,880 --> 00:04:41,199
well sad and happy to see this exact

00:04:39,680 --> 00:04:42,479
same question come up on the mailing

00:04:41,199 --> 00:04:44,880
list on the user list

00:04:42,479 --> 00:04:46,400
just on thursday or friday so that

00:04:44,880 --> 00:04:48,479
really solidified that

00:04:46,400 --> 00:04:50,320
yes people people are still doing this

00:04:48,479 --> 00:04:51,360
people that i work with were doing this

00:04:50,320 --> 00:04:53,520
people were

00:04:51,360 --> 00:04:56,639
uh in the community are doing this and

00:04:53,520 --> 00:04:57,440
it is a problem deep paging uh cursor

00:04:56,639 --> 00:04:59,520
mark

00:04:57,440 --> 00:05:02,160
has been around for five six years now

00:04:59,520 --> 00:05:04,160
with so many blog posts so many

00:05:02,160 --> 00:05:06,720
so much information about why this is so

00:05:04,160 --> 00:05:08,400
much more efficient so much faster

00:05:06,720 --> 00:05:10,720
lots of resources available but people

00:05:08,400 --> 00:05:12,720
still do it um and in our case the the

00:05:10,720 --> 00:05:14,800
median query time went from 600

00:05:12,720 --> 00:05:16,800
milliseconds to 5 milliseconds the 95

00:05:14,800 --> 00:05:20,160
percentile queue time went from 6

00:05:16,800 --> 00:05:21,919
seconds to 200 milliseconds that's over

00:05:20,160 --> 00:05:24,560
that's like an order of magnitude this

00:05:21,919 --> 00:05:27,680
is amazing improvement

00:05:24,560 --> 00:05:28,960
um and what you know conceptually what

00:05:27,680 --> 00:05:31,039
does it mean it's like if you're looking

00:05:28,960 --> 00:05:32,720
for a book in the library

00:05:31,039 --> 00:05:35,280
and first you look at the first 10 books

00:05:32,720 --> 00:05:36,960
then you look at the next 10 books

00:05:35,280 --> 00:05:38,800
it's it gets really expensive if you

00:05:36,960 --> 00:05:40,000
have to start from the beginning each

00:05:38,800 --> 00:05:41,520
time and you have to start counting you

00:05:40,000 --> 00:05:42,800
know okay this book one two three four

00:05:41,520 --> 00:05:44,479
five six seven

00:05:42,800 --> 00:05:46,000
and eventually you get to okay give me

00:05:44,479 --> 00:05:49,360
the hundreds book on the shelf that

00:05:46,000 --> 00:05:49,360
takes a long time to count

00:05:49,440 --> 00:05:54,240
and it's it's much faster if you can

00:05:52,080 --> 00:05:54,560
remember and you say oh okay i got to

00:05:54,240 --> 00:05:56,720
the

00:05:54,560 --> 00:05:59,360
i was in the c's i was in the d's i was

00:05:56,720 --> 00:06:02,240
in the the q's the r's

00:05:59,360 --> 00:06:03,280
uh i was all the way down to x and when

00:06:02,240 --> 00:06:06,560
you can skip that

00:06:03,280 --> 00:06:09,039
far in then solar can

00:06:06,560 --> 00:06:10,639
skip as well it doesn't have to generate

00:06:09,039 --> 00:06:12,400
that entire result set it doesn't have

00:06:10,639 --> 00:06:15,919
to sort through everything

00:06:12,400 --> 00:06:16,960
um but why why is this a problem if

00:06:15,919 --> 00:06:19,919
everybody

00:06:16,960 --> 00:06:20,639
everybody um in quotes knows about the

00:06:19,919 --> 00:06:23,759
solution

00:06:20,639 --> 00:06:24,400
because not everybody does it uh deep

00:06:23,759 --> 00:06:25,840
paging

00:06:24,400 --> 00:06:27,759
is kind of the easiest thing it's the

00:06:25,840 --> 00:06:31,520
default thing it looks like

00:06:27,759 --> 00:06:34,800
in our admin ui it shows you

00:06:31,520 --> 00:06:37,360
you know start rows and adding an offset

00:06:34,800 --> 00:06:39,199
is really easy

00:06:37,360 --> 00:06:40,400
but when when you're doing that and it

00:06:39,199 --> 00:06:42,080
seems to work

00:06:40,400 --> 00:06:43,759
with small enough offsets it seems to

00:06:42,080 --> 00:06:46,880
work so this is

00:06:43,759 --> 00:06:49,039
a trap that's very easy to fall into and

00:06:46,880 --> 00:06:51,120
it's kind of a case of the boiling frog

00:06:49,039 --> 00:06:52,240
where it works and it works and it works

00:06:51,120 --> 00:06:55,759
until it doesn't

00:06:52,240 --> 00:06:58,080
eventually it just becomes

00:06:55,759 --> 00:06:59,599
too far into the paging and you have to

00:06:58,080 --> 00:07:03,039
re-architect you have to

00:06:59,599 --> 00:07:04,639
switch into something else now there's

00:07:03,039 --> 00:07:06,160
other interesting issues with cursor

00:07:04,639 --> 00:07:09,440
mark we'll talk about that in

00:07:06,160 --> 00:07:10,960
one of the later tales a little bit of

00:07:09,440 --> 00:07:14,000
foreshadowing a little bit of

00:07:10,960 --> 00:07:17,199
teasing but i want to

00:07:14,000 --> 00:07:19,520
i want to focus on um another similar

00:07:17,199 --> 00:07:20,639
case uh this was a different team i'll

00:07:19,520 --> 00:07:22,240
say uh

00:07:20,639 --> 00:07:23,919
all five of these are actually from

00:07:22,240 --> 00:07:25,919
different teams so it's not

00:07:23,919 --> 00:07:28,960
it's not like i'm picking on one of my

00:07:25,919 --> 00:07:30,560
co-workers or anything

00:07:28,960 --> 00:07:32,880
but this is it was a team that they were

00:07:30,560 --> 00:07:36,800
seeing frequent out of memory errors

00:07:32,880 --> 00:07:38,240
and you know solar is fine it recovers

00:07:36,800 --> 00:07:40,160
one of the replicas goes down it

00:07:38,240 --> 00:07:44,319
recovers

00:07:40,160 --> 00:07:46,960
and it nothing made sense here

00:07:44,319 --> 00:07:48,560
they weren't doing deep paging uh they

00:07:46,960 --> 00:07:51,919
weren't

00:07:48,560 --> 00:07:54,080
uh they didn't have like the big offsets

00:07:51,919 --> 00:07:55,759
their query volume was low the

00:07:54,080 --> 00:07:57,599
complexity was low

00:07:55,759 --> 00:07:59,759
um they were only getting a few results

00:07:57,599 --> 00:08:02,639
it was it wasn't like they were doing

00:07:59,759 --> 00:08:04,960
indexing and updating at the same time

00:08:02,639 --> 00:08:06,479
um so it didn't make sense it was it was

00:08:04,960 --> 00:08:09,120
really confusing their schema looked

00:08:06,479 --> 00:08:12,000
good their field cache

00:08:09,120 --> 00:08:12,960
wasn't an issue the commit settings you

00:08:12,000 --> 00:08:14,960
know auto commit

00:08:12,960 --> 00:08:16,400
soft commit hard commit all these

00:08:14,960 --> 00:08:19,360
everything seemed fine

00:08:16,400 --> 00:08:20,479
um and nobody could figure it out it was

00:08:19,360 --> 00:08:22,080
just

00:08:20,479 --> 00:08:24,720
occasionally frequently there would be

00:08:22,080 --> 00:08:27,360
out of memory errors

00:08:24,720 --> 00:08:28,400
and the solar instance would would fall

00:08:27,360 --> 00:08:30,160
over it would

00:08:28,400 --> 00:08:31,440
restart we had some automation for that

00:08:30,160 --> 00:08:33,599
that was great

00:08:31,440 --> 00:08:35,680
um it would restart and then things

00:08:33,599 --> 00:08:38,719
would be fine for another

00:08:35,680 --> 00:08:40,320
couple hours couple days um

00:08:38,719 --> 00:08:42,479
sometimes we'd see a bunch of these at

00:08:40,320 --> 00:08:44,080
the same time

00:08:42,479 --> 00:08:45,680
and it took us a really long time to

00:08:44,080 --> 00:08:48,160
find this

00:08:45,680 --> 00:08:48,880
and figure out what was going on and we

00:08:48,160 --> 00:08:50,560
discovered

00:08:48,880 --> 00:08:52,560
that so this team they had heard that

00:08:50,560 --> 00:08:54,959
paging is bad they

00:08:52,560 --> 00:08:56,880
they saw the stuff about cursor mark

00:08:54,959 --> 00:08:59,360
they saw they knew about the memory

00:08:56,880 --> 00:09:00,640
constraints of paging

00:08:59,360 --> 00:09:03,120
and so what they would do is they would

00:09:00,640 --> 00:09:04,640
issue two queries um the first one was

00:09:03,120 --> 00:09:06,320
just a match all docs query

00:09:04,640 --> 00:09:08,320
with no rows they would get the hit

00:09:06,320 --> 00:09:10,210
count the number of documents

00:09:08,320 --> 00:09:11,920
and then they would actually issue their

00:09:10,210 --> 00:09:14,800
[Music]

00:09:11,920 --> 00:09:16,160
their uh search on whatever fields they

00:09:14,800 --> 00:09:19,519
were and they'd say

00:09:16,160 --> 00:09:21,120
rows uh just the total the number of

00:09:19,519 --> 00:09:23,680
total documents that's how many rows i

00:09:21,120 --> 00:09:25,360
can try to give me up to that many

00:09:23,680 --> 00:09:29,279
give me the whole result set they wanted

00:09:25,360 --> 00:09:29,279
the whole result set every single time

00:09:29,440 --> 00:09:32,399
and they thought well this is going to

00:09:30,640 --> 00:09:33,519
be okay because normally we're getting

00:09:32,399 --> 00:09:37,519
100 to

00:09:33,519 --> 00:09:41,120
200 documents sometimes we're getting

00:09:37,519 --> 00:09:44,800
a thousand maybe two thousand once

00:09:41,120 --> 00:09:46,240
twice um but what was happening here it

00:09:44,800 --> 00:09:47,839
was

00:09:46,240 --> 00:09:49,120
they didn't they didn't understand why

00:09:47,839 --> 00:09:50,560
this would actually end up being a

00:09:49,120 --> 00:09:52,000
problem

00:09:50,560 --> 00:09:53,920
and for this one i want to use an

00:09:52,000 --> 00:09:55,120
analogy of you're you're going to

00:09:53,920 --> 00:09:56,959
the orchard and you're going to pick

00:09:55,120 --> 00:09:58,800
some apples and

00:09:56,959 --> 00:10:00,320
you only want a few apples so you're

00:09:58,800 --> 00:10:02,560
going to

00:10:00,320 --> 00:10:03,680
you're going to bring a little basket

00:10:02,560 --> 00:10:05,040
and

00:10:03,680 --> 00:10:06,640
you're gonna say okay this is how many

00:10:05,040 --> 00:10:07,279
apples i expect this is how many apples

00:10:06,640 --> 00:10:10,079
i need

00:10:07,279 --> 00:10:11,440
i'm just gonna put my dozen two dozen

00:10:10,079 --> 00:10:14,880
apples in here

00:10:11,440 --> 00:10:17,200
and i'm gonna be happy to go or

00:10:14,880 --> 00:10:18,720
if you're getting the same dozen two

00:10:17,200 --> 00:10:19,839
dozen apples and then suddenly your

00:10:18,720 --> 00:10:22,880
friend shows up with a

00:10:19,839 --> 00:10:26,640
with a giant tractor a giant combine

00:10:22,880 --> 00:10:29,519
that's a little bit wasteful and solar

00:10:26,640 --> 00:10:30,640
solar trusts you solar believes you when

00:10:29,519 --> 00:10:33,600
you say

00:10:30,640 --> 00:10:36,079
oh i want a million rows i want a

00:10:33,600 --> 00:10:38,079
hundred million rows

00:10:36,079 --> 00:10:40,079
it it believes that you know what you're

00:10:38,079 --> 00:10:43,440
doing and that

00:10:40,079 --> 00:10:45,279
it allocates memory for all of that

00:10:43,440 --> 00:10:47,440
it allocates memory for for the

00:10:45,279 --> 00:10:48,320
searchers for the sorts for the result

00:10:47,440 --> 00:10:52,160
sets

00:10:48,320 --> 00:10:54,160
um all these huge arrays and you get

00:10:52,160 --> 00:10:55,920
five ten queries in at the same time

00:10:54,160 --> 00:10:59,040
that are all look like this

00:10:55,920 --> 00:11:01,600
20 queries at the same time even

00:10:59,040 --> 00:11:03,600
a relatively large heap is going to be

00:11:01,600 --> 00:11:05,519
no match for

00:11:03,600 --> 00:11:07,839
25 of these queries it's it's going to

00:11:05,519 --> 00:11:09,680
fall over at some point

00:11:07,839 --> 00:11:10,880
and that's exactly what happened so they

00:11:09,680 --> 00:11:12,880
they

00:11:10,880 --> 00:11:14,480
lowered their expectations a little bit

00:11:12,880 --> 00:11:17,200
they again

00:11:14,480 --> 00:11:18,399
switched to doing some cursor mark some

00:11:17,200 --> 00:11:20,959
paging

00:11:18,399 --> 00:11:23,839
uh and things got better all these out

00:11:20,959 --> 00:11:26,320
of memory errors went away

00:11:23,839 --> 00:11:27,519
so we've talked about queries a little

00:11:26,320 --> 00:11:30,720
bit

00:11:27,519 --> 00:11:32,880
um let's switch track a little bit

00:11:30,720 --> 00:11:34,000
and we're gonna we're gonna talk about

00:11:32,880 --> 00:11:35,760
uh

00:11:34,000 --> 00:11:37,200
if your users your customers aren't

00:11:35,760 --> 00:11:38,240
coming to you and saying my queries are

00:11:37,200 --> 00:11:39,680
slow

00:11:38,240 --> 00:11:42,640
they're gonna be coming to you saying my

00:11:39,680 --> 00:11:46,000
updates are slow my indexing is slow

00:11:42,640 --> 00:11:47,200
and this was this was for a uh

00:11:46,000 --> 00:11:50,720
i don't want to call it a real-time

00:11:47,200 --> 00:11:53,600
system but a relatively live system

00:11:50,720 --> 00:11:55,279
and they had some kind of portal some

00:11:53,600 --> 00:11:58,160
kind of edit interface

00:11:55,279 --> 00:12:00,160
where users could make changes and then

00:11:58,160 --> 00:12:02,880
you click the save button

00:12:00,160 --> 00:12:03,760
and then that triggers off some workflow

00:12:02,880 --> 00:12:05,279
some data go

00:12:03,760 --> 00:12:06,959
you know bounces around all over the

00:12:05,279 --> 00:12:10,480
place eventually

00:12:06,959 --> 00:12:12,720
it sends an update to solar and

00:12:10,480 --> 00:12:13,760
users they they were expecting a save

00:12:12,720 --> 00:12:15,519
confirmation

00:12:13,760 --> 00:12:17,200
and they wanted to see you know know

00:12:15,519 --> 00:12:20,480
that their data had been persisted

00:12:17,200 --> 00:12:22,800
and then next time they do a search uh

00:12:20,480 --> 00:12:24,880
they they wanted to see the results so

00:12:22,800 --> 00:12:28,079
this is like updating the description

00:12:24,880 --> 00:12:31,760
and an e-commerce thing or updating um

00:12:28,079 --> 00:12:33,920
updating your profile on some kind of

00:12:31,760 --> 00:12:37,040
social media thing or updating

00:12:33,920 --> 00:12:38,800
you know whatever a user could update

00:12:37,040 --> 00:12:41,760
and then they want that to be reflected

00:12:38,800 --> 00:12:41,760
in searches right away

00:12:42,560 --> 00:12:47,200
and the overall update latency the the

00:12:45,680 --> 00:12:48,880
p99 in this case

00:12:47,200 --> 00:12:51,440
most of the time it was fast but

00:12:48,880 --> 00:12:53,760
sometimes two minutes

00:12:51,440 --> 00:12:55,600
two minutes what kind of user wants to

00:12:53,760 --> 00:12:58,079
wait two minutes to see

00:12:55,600 --> 00:12:59,279
that their results are actually saved if

00:12:58,079 --> 00:13:01,600
you're adding something to a shopping

00:12:59,279 --> 00:13:05,200
cart you know or if you're adding

00:13:01,600 --> 00:13:08,800
you're changing your profile like

00:13:05,200 --> 00:13:11,440
we're we want instant satisfaction

00:13:08,800 --> 00:13:13,920
instant gratification and that just

00:13:11,440 --> 00:13:17,040
didn't cut it

00:13:13,920 --> 00:13:21,839
so what what was happening um well

00:13:17,040 --> 00:13:24,240
they had like i said when they would

00:13:21,839 --> 00:13:26,240
click the save that would be a single

00:13:24,240 --> 00:13:28,720
document going in a single update

00:13:26,240 --> 00:13:30,399
to solar and then they would have a

00:13:28,720 --> 00:13:34,160
bunch of these and the idea it makes

00:13:30,399 --> 00:13:36,000
sense intuitively it makes sense

00:13:34,160 --> 00:13:38,399
the application team thought well i

00:13:36,000 --> 00:13:40,880
don't want my updates to get blocked by

00:13:38,399 --> 00:13:42,399
if your updates fail or your updates or

00:13:40,880 --> 00:13:45,279
somebody else is you know

00:13:42,399 --> 00:13:46,399
i want everybody independent so

00:13:45,279 --> 00:13:48,480
everybody's gonna

00:13:46,399 --> 00:13:50,800
have their own update and that's going

00:13:48,480 --> 00:13:53,680
to get written independently

00:13:50,800 --> 00:13:55,120
but that was actually causing a lot of a

00:13:53,680 --> 00:13:57,680
lot of network connections

00:13:55,120 --> 00:14:00,880
it was causing a lot more committing

00:13:57,680 --> 00:14:04,880
eventually down the line

00:14:00,880 --> 00:14:04,880
and it was just bad for the system

00:14:05,199 --> 00:14:09,920
so we had fortunately we had some

00:14:06,880 --> 00:14:13,600
metrics we had some

00:14:09,920 --> 00:14:16,000
uh some visibility into the system

00:14:13,600 --> 00:14:17,519
and we said hey you know start doing

00:14:16,000 --> 00:14:21,279
these micro batches

00:14:17,519 --> 00:14:24,800
even 10 10 updates in a batch

00:14:21,279 --> 00:14:25,120
is better than one and that still gave

00:14:24,800 --> 00:14:28,079
them

00:14:25,120 --> 00:14:29,279
a lot of the a lot of the granularity it

00:14:28,079 --> 00:14:30,880
was very

00:14:29,279 --> 00:14:32,480
instantaneous and on the application

00:14:30,880 --> 00:14:33,920
side they would batch it up

00:14:32,480 --> 00:14:35,760
10 updates and if there weren't 10

00:14:33,920 --> 00:14:36,800
updates they would time out after 100

00:14:35,760 --> 00:14:40,399
milliseconds

00:14:36,800 --> 00:14:43,760
send off what you have uh and

00:14:40,399 --> 00:14:45,600
the maximum update times the p99 i went

00:14:43,760 --> 00:14:47,440
from two minutes to eight seconds

00:14:45,600 --> 00:14:49,519
so they were they were really happy with

00:14:47,440 --> 00:14:51,519
this

00:14:49,519 --> 00:14:53,199
um and the idea is you know you have a

00:14:51,519 --> 00:14:57,920
lot of cars on the road

00:14:53,199 --> 00:14:58,560
maybe maybe get fewer cars on the road

00:14:57,920 --> 00:15:01,120
again

00:14:58,560 --> 00:15:02,959
solar the defaults it'll let you do this

00:15:01,120 --> 00:15:03,600
uh and people will think you know i have

00:15:02,959 --> 00:15:05,839
my

00:15:03,600 --> 00:15:07,279
uh auto commit auto soft commit my

00:15:05,839 --> 00:15:08,560
timeouts

00:15:07,279 --> 00:15:10,560
that should be taking care of all of

00:15:08,560 --> 00:15:11,920
this but

00:15:10,560 --> 00:15:13,760
even the network connections and the

00:15:11,920 --> 00:15:15,519
number of connections from from the

00:15:13,760 --> 00:15:17,360
leader to the shards to the

00:15:15,519 --> 00:15:18,560
uh replicas this was in a solar cloud

00:15:17,360 --> 00:15:21,600
setup

00:15:18,560 --> 00:15:24,240
uh the number of update requests

00:15:21,600 --> 00:15:27,600
all of this uh was just causing lots of

00:15:24,240 --> 00:15:30,079
operational problems

00:15:27,600 --> 00:15:32,959
so we're gonna go uh jump back into

00:15:30,079 --> 00:15:32,959
queries a little bit

00:15:33,680 --> 00:15:38,399
and this was this was a really

00:15:35,360 --> 00:15:38,399
interesting one i thought

00:15:38,880 --> 00:15:43,600
they had really complex queries uh it

00:15:43,360 --> 00:15:46,480
was

00:15:43,600 --> 00:15:48,079
it was score queries um lots of boosts

00:15:46,480 --> 00:15:50,480
lots of relevance um

00:15:48,079 --> 00:15:53,360
lots of computations you know proximity

00:15:50,480 --> 00:15:56,240
uh span terms

00:15:53,360 --> 00:15:57,600
lots of um and they're using cursor mark

00:15:56,240 --> 00:16:00,240
so you know that's great they're they're

00:15:57,600 --> 00:16:03,040
paging through the results

00:16:00,240 --> 00:16:03,680
um these really complex queries long

00:16:03,040 --> 00:16:06,839
queries

00:16:03,680 --> 00:16:09,360
lots of lots of information um lots of

00:16:06,839 --> 00:16:13,120
fields lots of analyzers

00:16:09,360 --> 00:16:14,560
um and lots of filtering

00:16:13,120 --> 00:16:16,240
a lot going on here basically is what

00:16:14,560 --> 00:16:18,800
i'm trying to say uh

00:16:16,240 --> 00:16:20,880
and sometimes the results would just

00:16:18,800 --> 00:16:23,120
stop at an arbitrary cursor

00:16:20,880 --> 00:16:24,880
you get through and on on the first one

00:16:23,120 --> 00:16:28,160
that says you know we expect

00:16:24,880 --> 00:16:30,000
500 results or 5 000 results and

00:16:28,160 --> 00:16:31,519
they're paging through 100 at a time

00:16:30,000 --> 00:16:33,680
with the cursor

00:16:31,519 --> 00:16:34,800
they get through sometimes it gets all

00:16:33,680 --> 00:16:36,079
the way through

00:16:34,800 --> 00:16:38,480
and sometimes it would stop at an

00:16:36,079 --> 00:16:42,480
arbitrary place

00:16:38,480 --> 00:16:44,800
uh and so they thought well is it

00:16:42,480 --> 00:16:47,040
is it updates we have concurrent updates

00:16:44,800 --> 00:16:51,839
going

00:16:47,040 --> 00:16:51,839
is that screwing up the scores is that

00:16:52,000 --> 00:16:56,240
we looked at that and that really wasn't

00:16:53,759 --> 00:16:58,079
it um

00:16:56,240 --> 00:16:59,759
and then so they kept playing with this

00:16:58,079 --> 00:17:01,600
and they thought well oh we're using

00:16:59,759 --> 00:17:04,400
we're actually using a ruby client

00:17:01,600 --> 00:17:05,919
is that is that the problem so then we

00:17:04,400 --> 00:17:07,520
switched to java

00:17:05,919 --> 00:17:09,039
same thing and then we switched to we're

00:17:07,520 --> 00:17:12,240
just doing some curl

00:17:09,039 --> 00:17:13,679
some straight http requests and okay

00:17:12,240 --> 00:17:15,199
yeah it's not it's not the client's

00:17:13,679 --> 00:17:16,880
fault

00:17:15,199 --> 00:17:18,400
and these results kept stopping at an

00:17:16,880 --> 00:17:20,880
arbitrary cursor

00:17:18,400 --> 00:17:22,400
um we never knew how far into the

00:17:20,880 --> 00:17:25,199
results that it would stop

00:17:22,400 --> 00:17:27,120
uh and what was really interesting is

00:17:25,199 --> 00:17:28,799
that we retry a couple times at that

00:17:27,120 --> 00:17:30,640
point because we know

00:17:28,799 --> 00:17:33,600
from the initial set we know we're

00:17:30,640 --> 00:17:37,360
expecting a certain amount of results

00:17:33,600 --> 00:17:39,440
if we just retry the cursor that

00:17:37,360 --> 00:17:40,720
stopped giving us results eventually it

00:17:39,440 --> 00:17:42,799
would give us more results

00:17:40,720 --> 00:17:45,520
it would come back it's a miracle it's

00:17:42,799 --> 00:17:48,080
amazing these results come back

00:17:45,520 --> 00:17:48,960
um and this one we were chasing this for

00:17:48,080 --> 00:17:51,600
weeks

00:17:48,960 --> 00:17:54,160
it was everybody was pulling their hair

00:17:51,600 --> 00:17:57,280
out they didn't understand this at all

00:17:54,160 --> 00:18:00,880
um and then we finally we

00:17:57,280 --> 00:18:04,240
finally realized what it was um

00:18:00,880 --> 00:18:07,840
so we had the same documents these were

00:18:04,240 --> 00:18:09,840
nrt replicas solar cloud uh near real

00:18:07,840 --> 00:18:13,120
time type replicas

00:18:09,840 --> 00:18:14,160
and they had their own their own merge

00:18:13,120 --> 00:18:16,160
patterns

00:18:14,160 --> 00:18:18,080
they had their different numbers of max

00:18:16,160 --> 00:18:19,360
docs which meant that the relevance

00:18:18,080 --> 00:18:21,360
scoring

00:18:19,360 --> 00:18:22,880
was different and the same document

00:18:21,360 --> 00:18:24,559
would have different scores on different

00:18:22,880 --> 00:18:27,440
replicas

00:18:24,559 --> 00:18:29,039
so when we hit the cursor mark when we

00:18:27,440 --> 00:18:31,679
were sending the cursor mark with

00:18:29,039 --> 00:18:33,760
sorting by score and by id we were

00:18:31,679 --> 00:18:35,520
saying okay i want i want from this

00:18:33,760 --> 00:18:37,520
score forward

00:18:35,520 --> 00:18:39,280
the docs the same documents scored

00:18:37,520 --> 00:18:42,320
differently

00:18:39,280 --> 00:18:43,919
and the second result the middle of the

00:18:42,320 --> 00:18:46,720
line result

00:18:43,919 --> 00:18:47,440
we would be way past we'd be past all of

00:18:46,720 --> 00:18:50,400
the results

00:18:47,440 --> 00:18:52,559
we would have missed everything we're

00:18:50,400 --> 00:18:53,919
like okay well how do we

00:18:52,559 --> 00:18:56,480
how do we fix this we didn't realize

00:18:53,919 --> 00:18:58,320
that it was the fault of nrt replicas at

00:18:56,480 --> 00:19:01,520
first

00:18:58,320 --> 00:19:03,280
but we just saw you know oh this one has

00:19:01,520 --> 00:19:05,200
more deletes more docs

00:19:03,280 --> 00:19:06,880
um the term frequency the document

00:19:05,200 --> 00:19:07,600
frequencies are different the stats are

00:19:06,880 --> 00:19:09,200
different

00:19:07,600 --> 00:19:10,960
so we look there's this thing called the

00:19:09,200 --> 00:19:13,039
global stat cache

00:19:10,960 --> 00:19:15,039
that didn't actually help because that's

00:19:13,039 --> 00:19:17,200
across

00:19:15,039 --> 00:19:19,679
more for working across shards not

00:19:17,200 --> 00:19:23,840
across replicas in the same

00:19:19,679 --> 00:19:25,200
shard and at the time i i think it's

00:19:23,840 --> 00:19:28,799
gotten better now

00:19:25,200 --> 00:19:30,240
um but at the time it was also

00:19:28,799 --> 00:19:33,120
causing some performance issues the

00:19:30,240 --> 00:19:36,320
queries started getting a lot slower

00:19:33,120 --> 00:19:38,720
um so we we finally looked at it when we

00:19:36,320 --> 00:19:39,280
realized well what if we have the same

00:19:38,720 --> 00:19:40,559
segments

00:19:39,280 --> 00:19:42,320
what if we have the same segments and

00:19:40,559 --> 00:19:44,000
then we can switch to t

00:19:42,320 --> 00:19:45,919
log replicas all the replicas are in

00:19:44,000 --> 00:19:47,600
sync everybody's going to have the same

00:19:45,919 --> 00:19:51,520
number of documents

00:19:47,600 --> 00:19:54,559
and that one that actually worked um

00:19:51,520 --> 00:19:58,160
and it's this wasn't as simple

00:19:54,559 --> 00:20:00,960
uh of a change as it sounds like because

00:19:58,160 --> 00:20:02,880
the old system wasn't on solar 7 yet so

00:20:00,960 --> 00:20:06,159
they had to also upgrade to solar 7 for

00:20:02,880 --> 00:20:08,720
this feature um

00:20:06,159 --> 00:20:09,520
but now now they are now they have this

00:20:08,720 --> 00:20:12,799
t-log

00:20:09,520 --> 00:20:13,840
replica types and the whole the whole

00:20:12,799 --> 00:20:16,559
query is

00:20:13,840 --> 00:20:16,559
much happier

00:20:17,039 --> 00:20:22,720
um and so we can see that we expected

00:20:21,039 --> 00:20:26,000
these things that we thought would be

00:20:22,720 --> 00:20:27,520
the same they were different

00:20:26,000 --> 00:20:28,880
we didn't have a great way of finding

00:20:27,520 --> 00:20:29,679
out that they were different but we we

00:20:28,880 --> 00:20:31,280
would see

00:20:29,679 --> 00:20:33,440
empirically like they're different

00:20:31,280 --> 00:20:35,280
they're giving us different results

00:20:33,440 --> 00:20:38,320
they're giving us different data what's

00:20:35,280 --> 00:20:38,320
actually different here

00:20:40,400 --> 00:20:45,600
and then so the last one this last tale

00:20:43,039 --> 00:20:48,960
that i want to tell you about

00:20:45,600 --> 00:20:51,280
is there's a little bit of a spoiler

00:20:48,960 --> 00:20:53,760
in the title in the name of this it's

00:20:51,280 --> 00:20:55,360
about range queries

00:20:53,760 --> 00:20:58,320
and this use case they were getting the

00:20:55,360 --> 00:20:59,679
results for a query up to a given time

00:20:58,320 --> 00:21:02,159
and you can think of this as we're

00:20:59,679 --> 00:21:03,200
running we're running some reports we

00:21:02,159 --> 00:21:06,880
want

00:21:03,200 --> 00:21:11,919
um the data you know up until yesterday

00:21:06,880 --> 00:21:14,000
for this particular type of information

00:21:11,919 --> 00:21:16,320
and it's uh we're working with a pretty

00:21:14,000 --> 00:21:19,600
large data set but a small result set

00:21:16,320 --> 00:21:20,400
you know we want all all the information

00:21:19,600 --> 00:21:23,760
on

00:21:20,400 --> 00:21:28,400
uh this particular facet

00:21:23,760 --> 00:21:30,720
let's say up to up to midnight or up to

00:21:28,400 --> 00:21:32,240
um an hour ago you know from the

00:21:30,720 --> 00:21:35,840
beginning of time

00:21:32,240 --> 00:21:38,559
up to now where now is the start of this

00:21:35,840 --> 00:21:40,000
multi-stage pipeline job multi-stage

00:21:38,559 --> 00:21:44,000
analytic

00:21:40,000 --> 00:21:46,080
um and solar has you know solar has this

00:21:44,000 --> 00:21:47,360
concept of saying now in the in the

00:21:46,080 --> 00:21:49,440
query

00:21:47,360 --> 00:21:51,440
that will propagate out to the shard so

00:21:49,440 --> 00:21:54,720
that you have it consistent now

00:21:51,440 --> 00:21:56,640
um this is also useful to consider

00:21:54,720 --> 00:21:58,480
for something like a dbq a delete by

00:21:56,640 --> 00:22:00,720
query you know if you want to

00:21:58,480 --> 00:22:02,559
delete all of the old data for for

00:22:00,720 --> 00:22:06,320
whatever reason you have

00:22:02,559 --> 00:22:08,080
an age off policy and you want to delete

00:22:06,320 --> 00:22:11,440
everything from

00:22:08,080 --> 00:22:15,039
start to yesterday start to

00:22:11,440 --> 00:22:18,240
beginning of the month um

00:22:15,039 --> 00:22:20,400
same kind of idea and we had we had

00:22:18,240 --> 00:22:22,799
several filter queries

00:22:20,400 --> 00:22:27,039
we were looking and this was this was

00:22:22,799 --> 00:22:30,559
really really slow like

00:22:27,039 --> 00:22:32,720
tens of seconds slow to get this query

00:22:30,559 --> 00:22:34,400
um and individually we tried we looked

00:22:32,720 --> 00:22:36,480
at all these filters we thought no

00:22:34,400 --> 00:22:40,240
none of none of these filters really

00:22:36,480 --> 00:22:43,200
look like they matter

00:22:40,240 --> 00:22:43,200
um and

00:22:43,360 --> 00:22:46,960
eventually uh we tried we tried lots of

00:22:46,080 --> 00:22:48,960
things about like

00:22:46,960 --> 00:22:50,240
caching you know disable the cache

00:22:48,960 --> 00:22:52,480
switch things from

00:22:50,240 --> 00:22:55,039
filter queries to the main query move

00:22:52,480 --> 00:22:58,320
things around

00:22:55,039 --> 00:23:01,120
and eventually we realized you know

00:22:58,320 --> 00:23:03,840
solar or lucine has to be looking at a

00:23:01,120 --> 00:23:07,120
lot of different segment files for this

00:23:03,840 --> 00:23:09,280
um and on a hunch kind of by accident we

00:23:07,120 --> 00:23:12,880
realized

00:23:09,280 --> 00:23:16,000
what if we switched this to a negation

00:23:12,880 --> 00:23:17,679
and the negation ended up being way

00:23:16,000 --> 00:23:20,640
faster

00:23:17,679 --> 00:23:23,280
so we switched this from from everything

00:23:20,640 --> 00:23:28,080
up until a certain point

00:23:23,280 --> 00:23:28,080
as well as all these other filters um

00:23:28,240 --> 00:23:32,400
we switched it to everything except the

00:23:31,280 --> 00:23:35,600
most recent stuff

00:23:32,400 --> 00:23:37,039
so instead of getting this this big data

00:23:35,600 --> 00:23:38,559
set we're just getting

00:23:37,039 --> 00:23:41,360
filtering out the little data set at the

00:23:38,559 --> 00:23:44,640
end and discarding that

00:23:41,360 --> 00:23:46,960
and this was a huge impact when

00:23:44,640 --> 00:23:51,279
the the time window at the very end is

00:23:46,960 --> 00:23:53,039
very short um

00:23:51,279 --> 00:23:54,640
it was less of an impact but we still

00:23:53,039 --> 00:23:56,720
saw for improvement

00:23:54,640 --> 00:23:57,760
even when this this window was three

00:23:56,720 --> 00:24:01,360
months

00:23:57,760 --> 00:24:04,720
um the the entirety of the data set was

00:24:01,360 --> 00:24:04,720
maybe a couple years

00:24:05,120 --> 00:24:10,080
we still saw improvements even at three

00:24:08,720 --> 00:24:12,640
months

00:24:10,080 --> 00:24:13,679
queries went from you know 10 seconds to

00:24:12,640 --> 00:24:15,919
10 milliseconds

00:24:13,679 --> 00:24:17,200
a different kind of clear query with a

00:24:15,919 --> 00:24:20,400
different set of filters

00:24:17,200 --> 00:24:23,200
went from 25 seconds

00:24:20,400 --> 00:24:24,640
to 300 milliseconds these were these

00:24:23,200 --> 00:24:27,679
were not interactive queries they were

00:24:24,640 --> 00:24:30,320
being used in reporting and analytics so

00:24:27,679 --> 00:24:32,240
sometimes people were you know they just

00:24:30,320 --> 00:24:33,600
accepted it they thought oh 25 seconds

00:24:32,240 --> 00:24:36,720
that's how long it takes

00:24:33,600 --> 00:24:38,400
um that's just solar

00:24:36,720 --> 00:24:40,400
and it was it was kind of kind of

00:24:38,400 --> 00:24:42,880
embarrassing you know we're looking down

00:24:40,400 --> 00:24:44,960
this this long line

00:24:42,880 --> 00:24:46,720
we're looking down this long line and

00:24:44,960 --> 00:24:49,919
not realizing that we can

00:24:46,720 --> 00:24:51,440
just compute the very end of it that's

00:24:49,919 --> 00:24:55,919
all that we need to actually

00:24:51,440 --> 00:24:55,919
filter on and examine the values deeply

00:24:56,559 --> 00:25:00,000
so these were these are lots of

00:24:59,600 --> 00:25:03,120
different

00:25:00,000 --> 00:25:03,919
stories um a lot of them involved

00:25:03,120 --> 00:25:05,840
looking at

00:25:03,919 --> 00:25:07,200
looking at logs looking at what the

00:25:05,840 --> 00:25:09,840
system is doing looking at the

00:25:07,200 --> 00:25:12,080
operations

00:25:09,840 --> 00:25:12,880
but in all of them the way that we

00:25:12,080 --> 00:25:14,320
discovered them

00:25:12,880 --> 00:25:16,640
discover the issue you know somebody

00:25:14,320 --> 00:25:19,600
says this is slow

00:25:16,640 --> 00:25:21,279
this is what i'm complaining about is we

00:25:19,600 --> 00:25:24,400
had those slas

00:25:21,279 --> 00:25:26,080
so here for all of this you know i

00:25:24,400 --> 00:25:27,679
i had things i could measure for each of

00:25:26,080 --> 00:25:30,320
the queries 10 seconds to 10

00:25:27,679 --> 00:25:31,760
milliseconds 25 seconds

00:25:30,320 --> 00:25:34,480
and we we talk about what the

00:25:31,760 --> 00:25:38,159
expectations are what the

00:25:34,480 --> 00:25:39,520
measurements are and

00:25:38,159 --> 00:25:41,440
you know that's that's one how we

00:25:39,520 --> 00:25:44,480
identify them we experiment

00:25:41,440 --> 00:25:47,039
and we continue to measure things

00:25:44,480 --> 00:25:48,080
and then and then we work on it we

00:25:47,039 --> 00:25:49,520
improve that

00:25:48,080 --> 00:25:50,960
there's other things that we haven't

00:25:49,520 --> 00:25:51,600
been able to improve we're still working

00:25:50,960 --> 00:25:53,360
on

00:25:51,600 --> 00:25:55,520
but hopefully we have measurements on

00:25:53,360 --> 00:25:58,480
all those too

00:25:55,520 --> 00:26:00,559
i just want to shout out um we are

00:25:58,480 --> 00:26:01,840
hiring at apple for lots of search stuff

00:26:00,559 --> 00:26:04,000
um if any of these

00:26:01,840 --> 00:26:05,760
debugging issues seemed interesting

00:26:04,000 --> 00:26:08,159
these stories seemed interesting

00:26:05,760 --> 00:26:09,520
uh if you're curious about what systems

00:26:08,159 --> 00:26:12,240
these actually applied to

00:26:09,520 --> 00:26:14,320
come work with us we have a slack

00:26:12,240 --> 00:26:16,640
channel on the apache conslack

00:26:14,320 --> 00:26:18,960
and some links for both internships and

00:26:16,640 --> 00:26:22,000
full-time positions

00:26:18,960 --> 00:26:22,799
and it looks like i think we have some

00:26:22,000 --> 00:26:26,159
time for

00:26:22,799 --> 00:26:28,000
q a some time for questions i don't know

00:26:26,159 --> 00:26:37,880
if we have questions in the chat

00:26:28,000 --> 00:26:41,169
or let me it's over here um

00:26:37,880 --> 00:26:41,169
[Music]

00:26:41,600 --> 00:26:46,080
the small batch is number three being

00:26:43,840 --> 00:26:48,720
slow

00:26:46,080 --> 00:26:48,720
yeah let me

00:26:49,440 --> 00:26:53,840
go back wait how do i get back to my

00:26:51,200 --> 00:26:53,840
slot okay

00:26:56,320 --> 00:27:00,799
um yeah so the question from

00:27:01,520 --> 00:27:04,880
the question from david is you know can

00:27:03,840 --> 00:27:07,520
you elaborate on

00:27:04,880 --> 00:27:10,559
on this being slow uh you'll understand

00:27:07,520 --> 00:27:14,000
the throughput issue but not latency

00:27:10,559 --> 00:27:17,520
so yeah the

00:27:14,000 --> 00:27:21,840
the basic issue here was

00:27:17,520 --> 00:27:25,200
um there were

00:27:21,840 --> 00:27:27,440
it was a combination of the number of

00:27:25,200 --> 00:27:29,440
commit uh the number of updates coming

00:27:27,440 --> 00:27:33,360
in and the number of commits happening

00:27:29,440 --> 00:27:37,039
and the number of um

00:27:33,360 --> 00:27:40,240
so throughput the throughput issue

00:27:37,039 --> 00:27:42,880
was actually causing the latency issue

00:27:40,240 --> 00:27:44,320
um there were a lot of connections a lot

00:27:42,880 --> 00:27:46,960
of

00:27:44,320 --> 00:27:47,760
stuff in memory floating around um

00:27:46,960 --> 00:27:51,120
mainly

00:27:47,760 --> 00:27:54,320
network related stuff um and

00:27:51,120 --> 00:27:56,640
commit related stuff um

00:27:54,320 --> 00:27:57,760
so that was leading to some memory

00:27:56,640 --> 00:28:00,840
pressure

00:27:57,760 --> 00:28:02,399
um each it wasn't each one being a

00:28:00,840 --> 00:28:06,320
commit

00:28:02,399 --> 00:28:07,600
um but uh how do i

00:28:06,320 --> 00:28:10,559
i'm trying to i'm trying to remember the

00:28:07,600 --> 00:28:10,559
exact details here

00:28:12,000 --> 00:28:14,960
um the

00:28:15,440 --> 00:28:20,320
the punch line of the joke basically is

00:28:18,480 --> 00:28:22,080
you know we had

00:28:20,320 --> 00:28:25,120
uh it was causing a lot of memory

00:28:22,080 --> 00:28:28,720
pressure and a lot of garbage collection

00:28:25,120 --> 00:28:32,000
um and then that was causing updates to

00:28:28,720 --> 00:28:36,000
to stall out um i

00:28:32,000 --> 00:28:38,480
think the main um

00:28:36,000 --> 00:28:40,399
the the main relevant piece was that

00:28:38,480 --> 00:28:42,840
there were a lot of replicas

00:28:40,399 --> 00:28:44,000
and so the communication from the the

00:28:42,840 --> 00:28:47,360
leader

00:28:44,000 --> 00:28:51,600
to the replicas for

00:28:47,360 --> 00:28:53,679
um for forwarding the updates

00:28:51,600 --> 00:28:55,279
uh that fan out was causing a lot of

00:28:53,679 --> 00:28:58,399
pressure

00:28:55,279 --> 00:29:00,159
um and then we would be getting slow

00:28:58,399 --> 00:29:01,919
responses from some of the replicas

00:29:00,159 --> 00:29:05,840
which was causing

00:29:01,919 --> 00:29:07,600
um the the leader to

00:29:05,840 --> 00:29:09,919
have to hold open and wait for a lot

00:29:07,600 --> 00:29:13,840
more of these uh connections

00:29:09,919 --> 00:29:13,840
or uh responses

00:29:14,640 --> 00:29:17,679
um trying to think if if there was more

00:29:16,880 --> 00:29:21,200
to it

00:29:17,679 --> 00:29:22,720
um yeah i'm pretty sure it was the the

00:29:21,200 --> 00:29:25,360
relationship between

00:29:22,720 --> 00:29:28,960
the the leader and the replicas that was

00:29:25,360 --> 00:29:28,960
causing the latency responses there

00:29:30,000 --> 00:29:36,240
um so a question from tim uh paging

00:29:33,600 --> 00:29:40,480
versus cursor mark how many documents

00:29:36,240 --> 00:29:43,520
should you have i'm just gonna leave

00:29:40,480 --> 00:29:43,520
thank you slide up here

00:29:43,840 --> 00:29:49,039
um you know like like all things in

00:29:46,559 --> 00:29:49,039
solar

00:29:49,279 --> 00:29:56,720
there's not one good answer

00:29:52,840 --> 00:29:58,720
um a thousand documents

00:29:56,720 --> 00:30:01,440
is you know very small a million

00:29:58,720 --> 00:30:04,480
documents is

00:30:01,440 --> 00:30:05,200
um you know not something that i would

00:30:04,480 --> 00:30:08,480
want to be

00:30:05,200 --> 00:30:10,480
getting in a single request 10 uh

00:30:08,480 --> 00:30:12,240
the comment from david is 10k i was i

00:30:10,480 --> 00:30:13,919
was gonna say yeah about probably five

00:30:12,240 --> 00:30:19,360
to ten k

00:30:13,919 --> 00:30:20,960
um we we had some stuff internally

00:30:19,360 --> 00:30:23,840
uh that anshum had talked about in a

00:30:20,960 --> 00:30:27,200
previous uh i think it was at buzzwords

00:30:23,840 --> 00:30:30,080
that he said 10k um

00:30:27,200 --> 00:30:30,559
was uh was a hard limit that we'd put in

00:30:30,080 --> 00:30:34,960
um

00:30:30,559 --> 00:30:36,640
i personally recommend 5k to folks

00:30:34,960 --> 00:30:38,880
but really you just need to measure like

00:30:36,640 --> 00:30:43,279
like i said um it's kind of a case for

00:30:38,880 --> 00:30:46,240
yeah sorry 10k rows in a single request

00:30:43,279 --> 00:30:47,120
um it's a it's kind of a case of the

00:30:46,240 --> 00:30:51,039
boiling frog

00:30:47,120 --> 00:30:51,440
um where it's gonna just gradually slow

00:30:51,039 --> 00:30:55,520
down

00:30:51,440 --> 00:30:57,120
gradually slow down and at some point

00:30:55,520 --> 00:30:58,559
cursor mark isn't gonna slow down it's

00:30:57,120 --> 00:31:02,960
gonna be uh

00:30:58,559 --> 00:31:05,039
linear so

00:31:02,960 --> 00:31:06,320
maybe whatever slowdown it is it's

00:31:05,039 --> 00:31:07,200
acceptable and it's still within your

00:31:06,320 --> 00:31:10,000
slas

00:31:07,200 --> 00:31:11,919
um having measurements again going back

00:31:10,000 --> 00:31:13,200
to that having measurements is

00:31:11,919 --> 00:31:16,880
the most important thing and that's how

00:31:13,200 --> 00:31:16,880
you're going to know um

00:31:17,679 --> 00:31:22,080
question from way for high update

00:31:19,600 --> 00:31:24,240
throughput is t log plus poll replicas

00:31:22,080 --> 00:31:27,919
the best choice for solar cloud

00:31:24,240 --> 00:31:32,159
um i

00:31:27,919 --> 00:31:35,600
i don't know um i haven't done any

00:31:32,159 --> 00:31:38,080
profiling with t log and poll replicas

00:31:35,600 --> 00:31:38,080
together

00:31:38,960 --> 00:31:42,000
i know that there's lots of strategies

00:31:40,720 --> 00:31:44,799
out there

00:31:42,000 --> 00:31:46,640
that involve you know kind of building

00:31:44,799 --> 00:31:48,080
building offline indices

00:31:46,640 --> 00:31:50,320
um and then using like the backup

00:31:48,080 --> 00:31:52,000
restore mechanism to load those in

00:31:50,320 --> 00:31:54,080
using some kind of batch system

00:31:52,000 --> 00:31:57,200
mapreduce spark

00:31:54,080 --> 00:31:58,840
um i've seen folks talk about having

00:31:57,200 --> 00:32:03,279
separate clusters

00:31:58,840 --> 00:32:06,480
um for live indexing i don't know

00:32:03,279 --> 00:32:07,679
uh again you have to measure it that's

00:32:06,480 --> 00:32:10,080
that's what it's all going to come back

00:32:07,679 --> 00:32:10,080
down to

00:32:10,720 --> 00:32:15,360
um go back to this slide has my contact

00:32:14,640 --> 00:32:18,399
info

00:32:15,360 --> 00:32:19,919
um i'd like to have you know if you have

00:32:18,399 --> 00:32:21,840
ideas you can start them

00:32:19,919 --> 00:32:24,240
on on the mailing lists have a

00:32:21,840 --> 00:32:28,000
discussion about that

00:32:24,240 --> 00:32:29,200
um t-log replicas poll replicas are

00:32:28,000 --> 00:32:32,159
definitely more

00:32:29,200 --> 00:32:34,240
performant than nrt replicas for for

00:32:32,159 --> 00:32:36,159
throughput

00:32:34,240 --> 00:32:37,840
but it comes at the the expense of you

00:32:36,159 --> 00:32:41,679
know not having that nrt search

00:32:37,840 --> 00:32:44,000
so that is a trade-off poll replicas

00:32:41,679 --> 00:32:46,480
they do less t log replicas do less than

00:32:44,000 --> 00:32:49,279
n r t replicas so

00:32:46,480 --> 00:32:51,440
uh intuitively they would be more

00:32:49,279 --> 00:32:53,120
performant

00:32:51,440 --> 00:32:55,279
um follow-up question there what is the

00:32:53,120 --> 00:32:59,039
best strategy for batch updates

00:32:55,279 --> 00:33:00,640
um it's been it's been said before by by

00:32:59,039 --> 00:33:03,919
other folks

00:33:00,640 --> 00:33:07,279
but batch as much as you can commit

00:33:03,919 --> 00:33:10,159
as infrequently as possible um as

00:33:07,279 --> 00:33:11,519
as few commits as possible uh that you

00:33:10,159 --> 00:33:15,120
can tolerate

00:33:11,519 --> 00:33:18,240
um as big of matches as you can tolerate

00:33:15,120 --> 00:33:21,120
and that's from a performance standpoint

00:33:18,240 --> 00:33:24,640
that's the best you know do do as little

00:33:21,120 --> 00:33:27,360
work do as little computation as you can

00:33:24,640 --> 00:33:29,279
to get the best performance if your

00:33:27,360 --> 00:33:30,960
application has needs

00:33:29,279 --> 00:33:32,559
for data visibility and you need to

00:33:30,960 --> 00:33:34,080
commit more frequently

00:33:32,559 --> 00:33:36,799
commit more frequently if your

00:33:34,080 --> 00:33:40,559
application has needs for

00:33:36,799 --> 00:33:42,320
um you know faster

00:33:40,559 --> 00:33:44,159
um smaller batches you know you can't

00:33:42,320 --> 00:33:47,039
wait 60 seconds for your data to be

00:33:44,159 --> 00:33:50,240
visible then don't batch for 60 seconds

00:33:47,039 --> 00:33:52,000
um at some point the the batches can

00:33:50,240 --> 00:33:55,120
become too big

00:33:52,000 --> 00:33:56,880
um you know data size just

00:33:55,120 --> 00:33:59,039
on the request and sending a single

00:33:56,880 --> 00:34:01,200
request then the cost of retrying that

00:33:59,039 --> 00:34:03,760
request becomes too big

00:34:01,200 --> 00:34:05,519
um i don't know exactly where that

00:34:03,760 --> 00:34:08,159
threshold is but i'm thinking

00:34:05,519 --> 00:34:09,520
somewhere in the order of megabytes um

00:34:08,159 --> 00:34:11,119
you know you don't want a single update

00:34:09,520 --> 00:34:12,240
request to be a gigabyte that's that's

00:34:11,119 --> 00:34:14,879
probably too big

00:34:12,240 --> 00:34:17,119
but a megabyte 10 megabytes probably

00:34:14,879 --> 00:34:17,119
fine

00:34:19,119 --> 00:34:24,159
uh question from matt do commit within

00:34:22,320 --> 00:34:26,879
stack up or do they merge together if

00:34:24,159 --> 00:34:26,879
they overlap

00:34:27,440 --> 00:34:32,320
um i'm i'm not sure i understand the

00:34:30,879 --> 00:34:35,200
question

00:34:32,320 --> 00:34:35,599
uh david responded that they don't stack

00:34:35,200 --> 00:34:38,320
uh

00:34:35,599 --> 00:34:40,720
i don't i don't understand the question

00:34:38,320 --> 00:34:40,720
i'm sorry

00:34:41,359 --> 00:34:49,839
um how are we on time

00:34:46,240 --> 00:34:49,839
are we

00:34:50,000 --> 00:34:53,119
i don't know when the next session is

00:34:51,679 --> 00:34:56,560
actually

00:34:53,119 --> 00:34:58,160
um i'm happy to i'm happy to keep

00:34:56,560 --> 00:35:00,640
answering questions in here

00:34:58,160 --> 00:35:02,480
um like i said we're gonna be we're

00:35:00,640 --> 00:35:05,359
available in

00:35:02,480 --> 00:35:06,480
i'll be in the search slack channel um

00:35:05,359 --> 00:35:09,599
i'll be doing

00:35:06,480 --> 00:35:12,240
booth duty for our uh

00:35:09,599 --> 00:35:13,440
apple sponsor booth we're hiring this

00:35:12,240 --> 00:35:16,960
afternoon

00:35:13,440 --> 00:35:20,800
at 1 p.m central time

00:35:16,960 --> 00:35:23,359
that's 11 a.m pacific 2 p.m

00:35:20,800 --> 00:35:23,359
eastern

00:35:26,720 --> 00:35:31,520
okay five minutes until the next session

00:35:28,400 --> 00:35:33,599
so uh i'll let folks

00:35:31,520 --> 00:35:35,040
make their way to the next session grab

00:35:33,599 --> 00:35:37,839
some water grab some coffee

00:35:35,040 --> 00:35:37,839
i need my water

00:35:39,359 --> 00:35:44,320
and thanks everybody for attending this

00:35:41,040 --> 00:35:47,359
was recorded it'll be posted on

00:35:44,320 --> 00:35:48,640
uh on youtube or on i actually have no

00:35:47,359 --> 00:35:51,680
idea where it's gonna be posted

00:35:48,640 --> 00:35:52,400
um i'm sending my slides into the to

00:35:51,680 --> 00:35:55,680
rich

00:35:52,400 --> 00:35:57,599
he'll take care of all that thanks for

00:35:55,680 --> 00:36:02,640
coming this was a lot of fun

00:35:57,599 --> 00:36:02,640
and uh i'm gonna go enjoy the next

00:36:10,839 --> 00:36:13,839
session

00:36:26,960 --> 00:36:29,040

YouTube URL: https://www.youtube.com/watch?v=W9wfY6RJPYQ


