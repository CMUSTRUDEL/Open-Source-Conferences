Title: Cassandra Internals
Publication date: 2013-10-18
Playlist: Apachecon NA 2013 - day 3
Description: 
	Aaron Morton
ApacheCon NA 2013
Cassandra
Captions: 
	00:00:00,000 --> 00:00:04,980
hi everyone thanks for sticking out my

00:00:03,750 --> 00:00:06,810
name is Aaron Morton I'm gonna be

00:00:04,980 --> 00:00:09,300
talking about it to Sandra internals

00:00:06,810 --> 00:00:12,509
today as you can imagine it's pretty

00:00:09,300 --> 00:00:15,990
complicated code base to implement a

00:00:12,509 --> 00:00:18,180
scalable distributed fault tolerant

00:00:15,990 --> 00:00:20,310
database so we won't be going through in

00:00:18,180 --> 00:00:21,539
great detail what I'm hope to do is go

00:00:20,310 --> 00:00:24,600
through and pull out some of the more

00:00:21,539 --> 00:00:25,859
important classes and join that back to

00:00:24,600 --> 00:00:27,779
some of the stuff you might see as a

00:00:25,859 --> 00:00:29,820
user so we're going to see where some of

00:00:27,779 --> 00:00:31,769
the exceptions are throwing that you see

00:00:29,820 --> 00:00:34,590
as a user we're going to see where the

00:00:31,769 --> 00:00:36,510
timeouts come from I'm also going to

00:00:34,590 --> 00:00:38,309
prevent a bit of an architectural

00:00:36,510 --> 00:00:39,899
overview so that if you're looking to

00:00:38,309 --> 00:00:42,000
contribute to the code base hopefully

00:00:39,899 --> 00:00:45,149
you've got a roadmap for where things go

00:00:42,000 --> 00:00:47,730
a little bit about myself I'm a free

00:00:45,149 --> 00:00:50,789
lunch Cassandra consultant I live in

00:00:47,730 --> 00:00:53,760
wellington in new zealand i'm a patchy

00:00:50,789 --> 00:00:55,559
committer and that datastax MVP I've

00:00:53,760 --> 00:00:59,309
been using Cassandra for about three and

00:00:55,559 --> 00:01:02,190
a half four years so start by looking at

00:00:59,309 --> 00:01:04,500
the architecture for Cassandra it's a

00:01:02,190 --> 00:01:07,320
pretty simple diagram like I can put

00:01:04,500 --> 00:01:10,350
clients up there they make calls to EP

00:01:07,320 --> 00:01:12,840
is those api's call down to some

00:01:10,350 --> 00:01:15,930
software components we can call cluster

00:01:12,840 --> 00:01:17,909
unaware that sorry cluster aware which

00:01:15,930 --> 00:01:20,610
in turn call down two things we can

00:01:17,909 --> 00:01:22,920
consider cluster aware and all those

00:01:20,610 --> 00:01:25,380
things do is take stuff from memory put

00:01:22,920 --> 00:01:28,409
it on desk take stuff from desk put it

00:01:25,380 --> 00:01:32,210
in memory pretty simple architecture if

00:01:28,409 --> 00:01:34,890
we've got multiple nodes in a request

00:01:32,210 --> 00:01:37,140
then the cluster aware components talk

00:01:34,890 --> 00:01:39,200
to each other and then they talk down to

00:01:37,140 --> 00:01:41,970
the cluster underwear components and

00:01:39,200 --> 00:01:44,520
we've got the cluster we've got machines

00:01:41,970 --> 00:01:47,820
collaborating to process you'll read or

00:01:44,520 --> 00:01:50,640
you're right request but we can change

00:01:47,820 --> 00:01:52,770
the boxes the labels on those boxes to

00:01:50,640 --> 00:01:56,009
be something a little more meaningful we

00:01:52,770 --> 00:01:57,840
can change the cluster a welcome to say

00:01:56,009 --> 00:02:00,869
dynamo because it's really just a dynamo

00:01:57,840 --> 00:02:03,210
based system we can change the cluster

00:02:00,869 --> 00:02:06,329
underwear box to say database because

00:02:03,210 --> 00:02:09,700
that's all it is it's a log structured

00:02:06,329 --> 00:02:12,489
column family-orientated fast database

00:02:09,700 --> 00:02:15,280
so we've got some bits and bobs that do

00:02:12,489 --> 00:02:18,940
dynamo and a database and you put it all

00:02:15,280 --> 00:02:21,310
together and you've got Cassandra so we

00:02:18,940 --> 00:02:24,730
can start to think about the API we can

00:02:21,310 --> 00:02:28,959
break that up into two different layers

00:02:24,730 --> 00:02:32,200
we have transports and services and we

00:02:28,959 --> 00:02:34,959
use thrift as a transport we also use it

00:02:32,200 --> 00:02:36,220
as a service Nate was talking about the

00:02:34,959 --> 00:02:39,040
different architect the different

00:02:36,220 --> 00:02:41,500
transports into Cassandra when we use

00:02:39,040 --> 00:02:45,130
and we have this new one called native

00:02:41,500 --> 00:02:47,920
binary which is new in 1.2 we use Jay

00:02:45,130 --> 00:02:50,440
line in the Cassandra CLI we might talk

00:02:47,920 --> 00:02:53,940
about that and RMA as a transport is

00:02:50,440 --> 00:02:57,370
used of course for the jmx extensions

00:02:53,940 --> 00:03:00,670
when it comes to thrift as a transport

00:02:57,370 --> 00:03:03,670
cassandra has some custom thrifty server

00:03:00,670 --> 00:03:06,579
implementations the one that's used is

00:03:03,670 --> 00:03:09,190
selected by the RPC server type config

00:03:06,579 --> 00:03:13,060
setting and the default one is called

00:03:09,190 --> 00:03:14,829
custom t thread pool server there's a

00:03:13,060 --> 00:03:16,150
custom implementation because it has

00:03:14,829 --> 00:03:18,670
some things in there that happen when

00:03:16,150 --> 00:03:21,299
those when your client disconnects so it

00:03:18,670 --> 00:03:23,889
can clean up the state that we keep

00:03:21,299 --> 00:03:27,010
there's also a custom team non-blocking

00:03:23,889 --> 00:03:29,079
server which is obsolete now in 1.2

00:03:27,010 --> 00:03:32,799
still in the code base so you might come

00:03:29,079 --> 00:03:36,940
across it and there's a custom th sha

00:03:32,799 --> 00:03:38,950
server that uses a thread pool and those

00:03:36,940 --> 00:03:40,870
threads process each request from a

00:03:38,950 --> 00:03:42,609
client and then go on to process a

00:03:40,870 --> 00:03:45,310
different request from a different

00:03:42,609 --> 00:03:47,799
client in the t thread pool server the

00:03:45,310 --> 00:03:49,799
default one the threads stick with the

00:03:47,799 --> 00:03:54,130
client as long as that connections there

00:03:49,799 --> 00:03:57,160
so hsha is the one is the setting in the

00:03:54,130 --> 00:03:59,950
Gamma to use the hsha server the default

00:03:57,160 --> 00:04:02,139
is custom t thread pulls over and if you

00:03:59,950 --> 00:04:04,180
don't do anything your request enters

00:04:02,139 --> 00:04:08,769
the cassandra codebase pretty much of

00:04:04,180 --> 00:04:12,280
that custom t thread pool server native

00:04:08,769 --> 00:04:15,760
binary is the new newness it's better in

00:04:12,280 --> 00:04:17,440
version 1.2 we've heard some talks about

00:04:15,760 --> 00:04:21,489
new things that has

00:04:17,440 --> 00:04:23,710
under the hood it uses Nettie 3.5 to

00:04:21,489 --> 00:04:26,110
actually implement the server it's

00:04:23,710 --> 00:04:28,750
disabled by default and you can enable

00:04:26,110 --> 00:04:31,360
it with start native transport in the am

00:04:28,750 --> 00:04:34,480
L file it uses a different port to

00:04:31,360 --> 00:04:37,210
thrift uses 904 to so you can run both

00:04:34,480 --> 00:04:40,510
thrift and native transport at the same

00:04:37,210 --> 00:04:42,070
time because this is some container code

00:04:40,510 --> 00:04:43,360
we can just have a bit of a look now and

00:04:42,070 --> 00:04:47,200
see how it works i'm not going to dig

00:04:43,360 --> 00:04:48,850
into the thrift code oec on this slide

00:04:47,200 --> 00:04:51,300
is means i'm talking about the package

00:04:48,850 --> 00:04:54,370
in the source code org apache cassandra

00:04:51,300 --> 00:04:57,010
transport package is then contains

00:04:54,370 --> 00:04:59,530
native binary protocol there's a server

00:04:57,010 --> 00:05:03,070
class in there when it starts up it

00:04:59,530 --> 00:05:06,330
creates an execution handler that has

00:05:03,070 --> 00:05:08,500
between 16 and 128 threads by default

00:05:06,330 --> 00:05:11,230
that's the thing that processes your

00:05:08,500 --> 00:05:12,970
requests on those exit the threads in

00:05:11,230 --> 00:05:14,380
their process one message from a client

00:05:12,970 --> 00:05:17,410
and then go on and find the next message

00:05:14,380 --> 00:05:21,010
to process a bit like the hsha server

00:05:17,410 --> 00:05:23,470
and to do that it uses niÃ±o under the

00:05:21,010 --> 00:05:26,740
hood and the last thing it does is it

00:05:23,470 --> 00:05:29,020
sets up the neti server and built this

00:05:26,740 --> 00:05:30,700
thing and nearly called a pipeline your

00:05:29,020 --> 00:05:32,680
request goes in through the pipeline and

00:05:30,700 --> 00:05:35,860
out again so the steps in the pipeline

00:05:32,680 --> 00:05:39,460
are things like message encoding decode

00:05:35,860 --> 00:05:42,960
message compress and decompress up to a

00:05:39,460 --> 00:05:46,090
dispatcher which processes the message

00:05:42,960 --> 00:05:48,669
native transport supports ssl on the

00:05:46,090 --> 00:05:50,230
client connections and the ssl actually

00:05:48,669 --> 00:05:54,340
just comes in as a different stage in

00:05:50,230 --> 00:05:56,380
this pipeline but once the server's up

00:05:54,340 --> 00:05:59,560
and running your messages are actually

00:05:56,380 --> 00:06:04,570
processed by the dispatcher by message

00:05:59,560 --> 00:06:06,940
dispatcher this gets a message and gets

00:06:04,570 --> 00:06:08,560
the client state and checks to see if

00:06:06,940 --> 00:06:10,810
the message is valid for the state that

00:06:08,560 --> 00:06:12,640
the client connections in so we don't

00:06:10,810 --> 00:06:15,970
want a credentials message if you're

00:06:12,640 --> 00:06:18,430
already authenticated takes the request

00:06:15,970 --> 00:06:20,430
and execute it that gives us a response

00:06:18,430 --> 00:06:23,590
that we can send back to the client

00:06:20,430 --> 00:06:24,340
before we do that it takes the response

00:06:23,590 --> 00:06:26,500
and

00:06:24,340 --> 00:06:29,350
ecstasy we should change the state on

00:06:26,500 --> 00:06:31,840
this on your connection so if we were

00:06:29,350 --> 00:06:34,720
authenticating the request was some

00:06:31,840 --> 00:06:36,040
requests redentor and now you're

00:06:34,720 --> 00:06:39,130
authenticated we want to change the

00:06:36,040 --> 00:06:42,610
state then we write the response out to

00:06:39,130 --> 00:06:44,889
the client those messages that that

00:06:42,610 --> 00:06:47,380
function processes are defined in the

00:06:44,889 --> 00:06:50,380
messages package some of them are here

00:06:47,380 --> 00:06:52,660
their request and response messages so

00:06:50,380 --> 00:06:55,240
if you're going to execute some CTL or

00:06:52,660 --> 00:06:57,430
you're going to prepare some cql you can

00:06:55,240 --> 00:06:59,110
see the message is there there's several

00:06:57,430 --> 00:07:02,320
is several more than what I've included

00:06:59,110 --> 00:07:05,650
here they're all documented in the

00:07:02,320 --> 00:07:09,070
native binary transport spec which is

00:07:05,650 --> 00:07:10,840
available in the source tree so that's

00:07:09,070 --> 00:07:13,150
the two transports we can come in on

00:07:10,840 --> 00:07:16,450
thrift into the custom t thread pool

00:07:13,150 --> 00:07:20,440
server or on native binary into the

00:07:16,450 --> 00:07:23,710
message dispatcher at the services level

00:07:20,440 --> 00:07:26,789
the API has jmx the command line

00:07:23,710 --> 00:07:29,590
interface we used to thrift and see 12-3

00:07:26,789 --> 00:07:32,320
the jmx interface is really important if

00:07:29,590 --> 00:07:33,490
you ever like doing ops or even when

00:07:32,320 --> 00:07:35,110
you're doing dev if you want to know

00:07:33,490 --> 00:07:38,520
what cassandra is doing get a handle on

00:07:35,110 --> 00:07:41,349
what the jmx interface provides you the

00:07:38,520 --> 00:07:43,120
management beans that are exposed are

00:07:41,349 --> 00:07:45,639
implemented all around the code base

00:07:43,120 --> 00:07:47,620
they're not in a single package but they

00:07:45,639 --> 00:07:50,380
all implement interfaces that end with

00:07:47,620 --> 00:07:52,419
mb and they're all implemented on the

00:07:50,380 --> 00:07:55,479
class of the thing that they're exposing

00:07:52,419 --> 00:07:57,970
management for so there's a storage

00:07:55,479 --> 00:08:01,410
proxy class there's an mbean called

00:07:57,970 --> 00:08:04,780
storage proxy mbean and that registers

00:08:01,410 --> 00:08:07,630
with jay as with a name like this so org

00:08:04,780 --> 00:08:12,940
apache cassandra DB storage proxy pretty

00:08:07,630 --> 00:08:17,560
easy to find next up at the API level is

00:08:12,940 --> 00:08:18,729
the CLI if you're using thrift RPC this

00:08:17,560 --> 00:08:21,070
is the way you go in and poke around

00:08:18,729 --> 00:08:25,419
your data or outside of your application

00:08:21,070 --> 00:08:26,830
if you're using c ql 3 you can still go

00:08:25,419 --> 00:08:29,370
in and poke around and have a look at

00:08:26,830 --> 00:08:31,240
how it stores that data internally

00:08:29,370 --> 00:08:32,860
whether or not you can update that

00:08:31,240 --> 00:08:35,050
depends a little bit on how your tables

00:08:32,860 --> 00:08:37,530
where do you find it's still a really

00:08:35,050 --> 00:08:40,360
great tool to know

00:08:37,530 --> 00:08:42,790
it's a separate app but the code lives

00:08:40,360 --> 00:08:46,200
inside Cassandra the entry point for

00:08:42,790 --> 00:08:49,000
this is the CLI main comes in connects

00:08:46,200 --> 00:08:50,920
you can pass in a file of statements and

00:08:49,000 --> 00:08:54,760
it will iterate over all those

00:08:50,920 --> 00:08:57,660
statements and execute them for you you

00:08:54,760 --> 00:09:00,940
can in lingo into an interactive mode

00:08:57,660 --> 00:09:03,550
where it just reads with J line and

00:09:00,940 --> 00:09:05,470
processes your statements of course

00:09:03,550 --> 00:09:07,920
those statements are in an antler

00:09:05,470 --> 00:09:11,050
grammar which is also in the code base

00:09:07,920 --> 00:09:13,330
and the real processing of those

00:09:11,050 --> 00:09:16,990
statements happens in the CLA client

00:09:13,330 --> 00:09:19,500
class it uses antler compiles the class

00:09:16,990 --> 00:09:22,030
and then just has a big switch statement

00:09:19,500 --> 00:09:24,010
so it's not particularly complicated in

00:09:22,030 --> 00:09:29,050
there easy to go in and have a look and

00:09:24,010 --> 00:09:31,090
see what the CLA is doing thrift as a

00:09:29,050 --> 00:09:33,850
service is something we've had in

00:09:31,090 --> 00:09:35,500
Cassandra since day one this is all of

00:09:33,850 --> 00:09:39,210
the functions that you're idiomatic

00:09:35,500 --> 00:09:41,380
client like Hector or Picasso expose

00:09:39,210 --> 00:09:44,470
roughly map back to something on the

00:09:41,380 --> 00:09:47,530
thrift interface it's implemented in a

00:09:44,470 --> 00:09:49,060
class called Cassandra server this does

00:09:47,530 --> 00:09:51,700
the sorts of things you might expect it

00:09:49,060 --> 00:09:54,700
has access control it has input

00:09:51,700 --> 00:09:57,070
validation it maps between the thrift

00:09:54,700 --> 00:09:59,200
classes of thrift structures and what we

00:09:57,070 --> 00:10:03,430
do internally is we don't use thrift

00:09:59,200 --> 00:10:06,070
inside Cassandra that thrift interface

00:10:03,430 --> 00:10:08,560
is defined in an IDL file that's

00:10:06,070 --> 00:10:11,230
available in the source tree there's an

00:10:08,560 --> 00:10:14,020
ant target to build the Java classes

00:10:11,230 --> 00:10:17,580
from that and once for Python and all

00:10:14,020 --> 00:10:20,710
the others as well so the most common

00:10:17,580 --> 00:10:23,290
function call on the thrift interface is

00:10:20,710 --> 00:10:27,760
get slice we have a bit of a look at

00:10:23,290 --> 00:10:29,410
that guy this is Cassandra 1.2 so-called

00:10:27,760 --> 00:10:32,280
doing gets sliced we've got tracing

00:10:29,410 --> 00:10:35,500
which is nearly 1.2 that gets set up

00:10:32,280 --> 00:10:37,120
then we get the clients state and this

00:10:35,500 --> 00:10:39,370
is where we store things like the key

00:10:37,120 --> 00:10:42,400
space you've selected and your

00:10:39,370 --> 00:10:43,840
authentication details so we use that to

00:10:42,400 --> 00:10:45,190
see if you've got access to the key

00:10:43,840 --> 00:10:47,800
space that you're trying to read from

00:10:45,190 --> 00:10:49,100
and because this is a pretty common type

00:10:47,800 --> 00:10:51,530
of operation

00:10:49,100 --> 00:10:55,160
then go into some internal functions to

00:10:51,530 --> 00:10:58,100
actually do the read multi get slice

00:10:55,160 --> 00:11:00,290
internal takes your request and just

00:10:58,100 --> 00:11:02,720
validates it and say well have you asked

00:11:00,290 --> 00:11:03,770
for a limit of minus two columns that

00:11:02,720 --> 00:11:06,160
doesn't make any sense have you

00:11:03,770 --> 00:11:08,480
specified the column family name and

00:11:06,160 --> 00:11:11,480
creates these objects we have called

00:11:08,480 --> 00:11:14,390
read commands which encapsulate the type

00:11:11,480 --> 00:11:21,230
of reads you want to do and we jump into

00:11:14,390 --> 00:11:22,730
another internal call this one takes our

00:11:21,230 --> 00:11:24,740
read commands and process them

00:11:22,730 --> 00:11:26,630
internally and then maps them back to

00:11:24,740 --> 00:11:28,940
thrift using a nice function called

00:11:26,630 --> 00:11:31,150
thrift to fire this is where we're

00:11:28,940 --> 00:11:35,030
standing to cross from the external API

00:11:31,150 --> 00:11:37,910
into Cassandra but we do that fully in

00:11:35,030 --> 00:11:40,430
the very column family function this

00:11:37,910 --> 00:11:42,230
takes our read commands and hands them

00:11:40,430 --> 00:11:44,300
down to a class called the storage proxy

00:11:42,230 --> 00:11:47,030
there's one storage proxy instance per

00:11:44,300 --> 00:11:48,890
server and the storage proxy actually

00:11:47,030 --> 00:11:51,350
comes from work or the Dynamo layer so

00:11:48,890 --> 00:11:53,780
this is the end of your API call after

00:11:51,350 --> 00:11:55,520
this your call is moved into Cassandra

00:11:53,780 --> 00:12:00,260
proper and it's going to be processed by

00:11:55,520 --> 00:12:02,300
those cluster aware dynamo components we

00:12:00,260 --> 00:12:06,860
can contrast that with C 2 L 3 and see

00:12:02,300 --> 00:12:09,590
how that process goes your query in situ

00:12:06,860 --> 00:12:12,110
l3 after it's either come through native

00:12:09,590 --> 00:12:15,380
transport or come through thrift on a

00:12:12,110 --> 00:12:19,790
call like execute cql 3 will end up at

00:12:15,380 --> 00:12:21,350
the sequel 3 query processor this guy

00:12:19,790 --> 00:12:23,540
does what you'd expect as well it has

00:12:21,350 --> 00:12:26,360
access control and input validation and

00:12:23,540 --> 00:12:29,060
what it returns is a message from our

00:12:26,360 --> 00:12:31,460
transport package so if this is used by

00:12:29,060 --> 00:12:32,690
native transport there's no more work to

00:12:31,460 --> 00:12:35,150
do because this is what we want to

00:12:32,690 --> 00:12:38,600
return back if it's called by thrift

00:12:35,150 --> 00:12:42,080
then we have to thrift fi the transport

00:12:38,600 --> 00:12:44,330
message before it can get sent back cql

00:12:42,080 --> 00:12:47,120
3 is defined in an antler grammar of

00:12:44,330 --> 00:12:48,290
course that's in the code base go and

00:12:47,120 --> 00:12:49,700
have a read of that it's got some

00:12:48,290 --> 00:12:54,230
comments in there that make it a little

00:12:49,700 --> 00:12:56,330
bit easier to understand and those

00:12:54,230 --> 00:12:57,550
statements go through a two-phase

00:12:56,330 --> 00:13:00,790
process to bx

00:12:57,550 --> 00:13:02,470
you that first their compiled by antler

00:13:00,790 --> 00:13:05,380
which creates these things we call paws

00:13:02,470 --> 00:13:07,540
statements and they do things like track

00:13:05,380 --> 00:13:10,089
the number of terms if you're preparing

00:13:07,540 --> 00:13:11,320
a statement and you've got some terms in

00:13:10,089 --> 00:13:13,779
there that you're going to bind later on

00:13:11,320 --> 00:13:15,910
it takes care of that it also has a

00:13:13,779 --> 00:13:19,000
prepared statement which makes what we

00:13:15,910 --> 00:13:20,290
call the real the cql statement this is

00:13:19,000 --> 00:13:22,870
the thing that we actually execute

00:13:20,290 --> 00:13:25,000
internally we don't execute prepared

00:13:22,870 --> 00:13:26,680
statements these do some additional

00:13:25,000 --> 00:13:29,110
access controls because now we know

00:13:26,680 --> 00:13:31,750
where you're going and validation

00:13:29,110 --> 00:13:35,160
because we know a bit more about your

00:13:31,750 --> 00:13:38,560
request has a simple execute call on it

00:13:35,160 --> 00:13:41,890
some query states and variables that

00:13:38,560 --> 00:13:44,290
your binding things like that if your

00:13:41,890 --> 00:13:46,600
statement includes a function call like

00:13:44,290 --> 00:13:48,790
the token function or the time you would

00:13:46,600 --> 00:13:51,010
function they're available in this

00:13:48,790 --> 00:13:54,910
package and going there they generally

00:13:51,010 --> 00:13:56,800
have some simple implementations when we

00:13:54,910 --> 00:13:59,620
can have a look now at the Select

00:13:56,800 --> 00:14:01,779
statement so the most useful and the

00:13:59,620 --> 00:14:03,970
most complicated cql statement in

00:14:01,779 --> 00:14:07,240
Cassandra roughly the the equivalent of

00:14:03,970 --> 00:14:09,940
get slice that we saw before it has an

00:14:07,240 --> 00:14:11,649
inner class called raw statement which

00:14:09,940 --> 00:14:13,270
is its implementation of the prepared

00:14:11,649 --> 00:14:17,079
statement this is the first thing we

00:14:13,270 --> 00:14:19,329
generate generated by antler some extra

00:14:17,079 --> 00:14:21,430
input validation and then it prepares it

00:14:19,329 --> 00:14:23,770
so it takes the essential syntax tree

00:14:21,430 --> 00:14:25,630
and now works out oh you actually want

00:14:23,770 --> 00:14:28,029
to get a range of columns I can like you

00:14:25,630 --> 00:14:31,660
know set some properties and it builds

00:14:28,029 --> 00:14:34,660
the Select statement these guys have

00:14:31,660 --> 00:14:37,060
execute function and it makes those same

00:14:34,660 --> 00:14:39,399
read command objects we saw on the

00:14:37,060 --> 00:14:41,199
thrift side and hands them down to the

00:14:39,399 --> 00:14:44,560
storage proxy just the same way we saw

00:14:41,199 --> 00:14:46,870
with thrift so your request comes into

00:14:44,560 --> 00:14:50,410
Cassandra into the custom t thread pool

00:14:46,870 --> 00:14:52,779
server or into the message dispatcher it

00:14:50,410 --> 00:14:54,760
then comes down to either the query

00:14:52,779 --> 00:14:57,399
processor or into the Cassandra server

00:14:54,760 --> 00:14:58,810
and then on to the query processor but

00:14:57,399 --> 00:15:01,660
either way it ends up down at the

00:14:58,810 --> 00:15:04,570
storage proxy to go and do your read or

00:15:01,660 --> 00:15:06,310
you're right so that storage properties

00:15:04,570 --> 00:15:07,600
the thing I put in the Dynamo layer into

00:15:06,310 --> 00:15:10,779
the entry point roughly

00:15:07,600 --> 00:15:13,959
into that layer the other packages we

00:15:10,779 --> 00:15:17,910
have in the Dynamo layer are the service

00:15:13,959 --> 00:15:22,620
package net package dhts the hash tree

00:15:17,910 --> 00:15:26,050
locator is where the snitch and the

00:15:22,620 --> 00:15:30,279
replication strategy exists and GMS

00:15:26,050 --> 00:15:33,069
contains the gossip so we've seen the

00:15:30,279 --> 00:15:34,480
storage of proxy this is the the entry

00:15:33,069 --> 00:15:36,790
point for all of the cluster wide

00:15:34,480 --> 00:15:39,069
storage operations if you're going to do

00:15:36,790 --> 00:15:42,759
read or write it goes into the storage

00:15:39,069 --> 00:15:45,699
proxy if it's a cluster wide some local

00:15:42,759 --> 00:15:47,759
operations don't use it it's pretty

00:15:45,699 --> 00:15:50,110
simple those things you'd expect it's

00:15:47,759 --> 00:15:52,449
going to go and find the endpoints your

00:15:50,110 --> 00:15:54,279
request wants to go to it's going to

00:15:52,449 --> 00:15:57,040
check that the consistency level that

00:15:54,279 --> 00:15:59,829
you've asked for is available and then

00:15:57,040 --> 00:16:02,230
it sends messages to stages in the local

00:15:59,829 --> 00:16:05,500
machine and rope and remote machines

00:16:02,230 --> 00:16:07,870
those stages are stages in the staged

00:16:05,500 --> 00:16:10,680
event driven architecture that Cassandra

00:16:07,870 --> 00:16:13,720
implements we'll talk about that shortly

00:16:10,680 --> 00:16:16,120
it waits around for a response from all

00:16:13,720 --> 00:16:17,980
the replicas that's spoken to and deals

00:16:16,120 --> 00:16:19,990
with them not responding it deals with

00:16:17,980 --> 00:16:22,630
storing hints that are then reap that

00:16:19,990 --> 00:16:26,589
can be replayed later on it orchestrates

00:16:22,630 --> 00:16:28,420
the cluster wide read and write another

00:16:26,589 --> 00:16:31,569
important class in this package is the

00:16:28,420 --> 00:16:33,519
storage service so this guy is an

00:16:31,569 --> 00:16:35,319
orchestration or a wrapper class for

00:16:33,519 --> 00:16:37,509
lots of things we do to do with the

00:16:35,319 --> 00:16:40,149
token ring to do with tracking ring

00:16:37,509 --> 00:16:43,779
state nodes up and down to do with

00:16:40,149 --> 00:16:46,209
starting gossip and starting thrift and

00:16:43,779 --> 00:16:49,089
it contains functions to map between

00:16:46,209 --> 00:16:51,310
nodes and tokens and tokens back to

00:16:49,089 --> 00:16:53,290
nodes it doesn't really have them

00:16:51,310 --> 00:16:56,019
Clemente shun for most of these things

00:16:53,290 --> 00:16:58,120
in here calls off to other parts in the

00:16:56,019 --> 00:17:00,189
in the code base but this is or the

00:16:58,120 --> 00:17:03,850
entry point if Europe and the collection

00:17:00,189 --> 00:17:05,980
for all of those function once the

00:17:03,850 --> 00:17:07,539
storage proxy is used the storage

00:17:05,980 --> 00:17:08,620
service and some other things and worked

00:17:07,539 --> 00:17:10,659
out where we're going to send those

00:17:08,620 --> 00:17:13,329
requests it sits around and waits for

00:17:10,659 --> 00:17:14,620
them to come back and if we're doing a

00:17:13,329 --> 00:17:16,240
read we're going to get a response back

00:17:14,620 --> 00:17:17,770
from multiple nodes we might get a

00:17:16,240 --> 00:17:19,160
response back from two different nodes

00:17:17,770 --> 00:17:21,050
and we need to understand

00:17:19,160 --> 00:17:24,020
do these things have the same response

00:17:21,050 --> 00:17:26,390
do as the data consistent that's

00:17:24,020 --> 00:17:29,770
implemented in in the eye response

00:17:26,390 --> 00:17:32,210
resolver interface pretty simple

00:17:29,770 --> 00:17:34,670
preprocess means is called when we get a

00:17:32,210 --> 00:17:37,550
response from a node so we get one comes

00:17:34,670 --> 00:17:40,580
off the wire called pre-process once

00:17:37,550 --> 00:17:43,460
we've got enough responses call resolve

00:17:40,580 --> 00:17:48,620
and check the sea of all those responses

00:17:43,460 --> 00:17:51,410
are consistent the road digest resolver

00:17:48,620 --> 00:17:54,560
is used the first time we do a read so

00:17:51,410 --> 00:17:56,270
your request comes in we determined

00:17:54,560 --> 00:17:58,490
we're going to send it to two replicas

00:17:56,270 --> 00:18:01,100
and we ask one of them to give us back

00:17:58,490 --> 00:18:03,440
the actual data and we ask the other one

00:18:01,100 --> 00:18:07,460
to just give us a digest of the data to

00:18:03,440 --> 00:18:09,710
save on some network overhead row digest

00:18:07,460 --> 00:18:11,750
resolver when in the result function

00:18:09,710 --> 00:18:14,330
builds a digest for the data that it got

00:18:11,750 --> 00:18:16,250
compares that to the other digests and

00:18:14,330 --> 00:18:19,430
if they don't match all it does is raise

00:18:16,250 --> 00:18:21,920
a digest mismatch exception if they do

00:18:19,430 --> 00:18:24,470
match it returns back the data regatta

00:18:21,920 --> 00:18:25,820
from one of the replicas and that's what

00:18:24,470 --> 00:18:29,300
ends up getting sent back to you as a

00:18:25,820 --> 00:18:31,910
client now when they don't match we have

00:18:29,300 --> 00:18:35,210
to go and run that read request again a

00:18:31,910 --> 00:18:37,640
second time and ask all the nodes

00:18:35,210 --> 00:18:39,470
involved to return back their data we

00:18:37,640 --> 00:18:42,110
don't want digest anymore we've got to

00:18:39,470 --> 00:18:44,990
resolve the differences that's handled

00:18:42,110 --> 00:18:47,810
in the row digest resolver so in this

00:18:44,990 --> 00:18:49,760
guy we've got the different results it

00:18:47,810 --> 00:18:53,030
works out the differences between the

00:18:49,760 --> 00:18:55,250
responses creates Delta mutations that

00:18:53,030 --> 00:18:57,680
are sent out to the nodes that are out

00:18:55,250 --> 00:19:00,920
of sync and without waiting around for

00:18:57,680 --> 00:19:03,050
those responses returns back from

00:19:00,920 --> 00:19:05,600
resolve the result we should send back

00:19:03,050 --> 00:19:08,030
to the client so we can have

00:19:05,600 --> 00:19:10,010
inconsistent data on desk we could have

00:19:08,030 --> 00:19:11,570
nodes that are just dropping mutations

00:19:10,010 --> 00:19:14,030
because their commit logs full or

00:19:11,570 --> 00:19:15,860
something like that but the work that

00:19:14,030 --> 00:19:17,600
the road digest resolve it does means

00:19:15,860 --> 00:19:20,480
that we can always actually return a

00:19:17,600 --> 00:19:24,140
consistent result from York to your

00:19:20,480 --> 00:19:26,120
client course the last response resolver

00:19:24,140 --> 00:19:28,010
is used on a range scan so this is where

00:19:26,120 --> 00:19:28,630
you're saying get me rose between foo

00:19:28,010 --> 00:19:31,520
and

00:19:28,630 --> 00:19:34,070
this deals with the problem of nodes

00:19:31,520 --> 00:19:36,350
returning different numbers of rows and

00:19:34,070 --> 00:19:38,120
different rows so one replica could

00:19:36,350 --> 00:19:41,210
return foo and bar and another replica

00:19:38,120 --> 00:19:45,320
returns bars the rain slice response

00:19:41,210 --> 00:19:47,090
resolve handles those differences that's

00:19:45,320 --> 00:19:49,490
used on the read path and not on the

00:19:47,090 --> 00:19:52,190
right path but both the read and the

00:19:49,490 --> 00:19:55,820
right path have this idea of a response

00:19:52,190 --> 00:19:57,260
handler or a call back this is something

00:19:55,820 --> 00:19:59,809
that implements an interface from the

00:19:57,260 --> 00:20:05,120
networking package called I async call

00:19:59,809 --> 00:20:06,710
back it's a simple interface it just

00:20:05,120 --> 00:20:09,830
allows the networking package to deliver

00:20:06,710 --> 00:20:13,850
a message to something in the code

00:20:09,830 --> 00:20:16,250
that's expecting a message back so we

00:20:13,850 --> 00:20:19,400
have a read callback has this nice

00:20:16,250 --> 00:20:21,320
function on that call get which waits on

00:20:19,400 --> 00:20:24,010
the condition and it waits with a

00:20:21,320 --> 00:20:29,179
timeout and the timeout is your RPC

00:20:24,010 --> 00:20:31,820
timeout that condition is set in the

00:20:29,179 --> 00:20:35,840
response implementation of the I

00:20:31,820 --> 00:20:38,179
asynccallback interface in that response

00:20:35,840 --> 00:20:40,580
we check to see if we've received the

00:20:38,179 --> 00:20:42,020
data we check to see if we've got the

00:20:40,580 --> 00:20:44,420
number of nodes were blocking for so

00:20:42,020 --> 00:20:46,780
we're waiting for two nodes and if

00:20:44,420 --> 00:20:50,030
that's true then the conditions set

00:20:46,780 --> 00:20:54,140
otherwise it of its unset and we have to

00:20:50,030 --> 00:20:56,059
a more waiting so something calls get it

00:20:54,140 --> 00:20:58,610
will come in here and that might block

00:20:56,059 --> 00:21:00,440
on that condition and will either pass

00:20:58,610 --> 00:21:03,770
that because the condition was set or we

00:21:00,440 --> 00:21:06,710
tied down if we timed out we throw a

00:21:03,770 --> 00:21:08,600
read timeout exception and that ends up

00:21:06,710 --> 00:21:10,400
being a timed out exception back to you

00:21:08,600 --> 00:21:13,850
as a client so this is where they come

00:21:10,400 --> 00:21:16,220
from inside the recall back if the

00:21:13,850 --> 00:21:18,410
conditions set and we go in and recall

00:21:16,220 --> 00:21:21,890
the response resolver that could then

00:21:18,410 --> 00:21:24,380
throw a digest mismatch exception and we

00:21:21,890 --> 00:21:26,300
have to go and run the read again if

00:21:24,380 --> 00:21:28,160
everything is working though we go

00:21:26,300 --> 00:21:30,650
through the condition we call the

00:21:28,160 --> 00:21:32,540
response resolver and we've got the

00:21:30,650 --> 00:21:36,110
result that we want to send back to you

00:21:32,540 --> 00:21:37,670
as a client call all this comes together

00:21:36,110 --> 00:21:39,920
in a function called fetch row

00:21:37,670 --> 00:21:42,410
on storage property this is the thing

00:21:39,920 --> 00:21:45,860
that orchestrates your cluster wide read

00:21:42,410 --> 00:21:47,680
in Cassandra it gets a list of read

00:21:45,860 --> 00:21:51,350
commands and iterates through them and

00:21:47,680 --> 00:21:53,570
it call it builds a list of endpoints

00:21:51,350 --> 00:21:56,480
there so that according to proximity so

00:21:53,570 --> 00:21:59,710
knows that the closer to you in the same

00:21:56,480 --> 00:22:02,270
data center in the same rack a higher

00:21:59,710 --> 00:22:04,040
the dynamic snitch kicks in there as

00:22:02,270 --> 00:22:07,160
well and might sort them according to

00:22:04,040 --> 00:22:09,500
their latencies and then that list is

00:22:07,160 --> 00:22:11,480
filtered according to your consistency

00:22:09,500 --> 00:22:14,930
level and whether you've got read repair

00:22:11,480 --> 00:22:16,640
running if you've got the replication

00:22:14,930 --> 00:22:19,280
factor of three and you do a quorum

00:22:16,640 --> 00:22:22,160
consistency and regroup air is not

00:22:19,280 --> 00:22:24,440
active we only go to two nodes every

00:22:22,160 --> 00:22:27,620
repairs active we go to all available up

00:22:24,440 --> 00:22:29,930
nodes build a digest resolver because

00:22:27,620 --> 00:22:32,450
we're just starting the read use that

00:22:29,930 --> 00:22:34,730
digest resolver to construct the read

00:22:32,450 --> 00:22:36,080
call back and include on that the number

00:22:34,730 --> 00:22:39,200
of notes were blocking for and things

00:22:36,080 --> 00:22:40,700
like that and then generate some

00:22:39,200 --> 00:22:42,890
messages that describe what we want to

00:22:40,700 --> 00:22:45,920
happen and send those using the

00:22:42,890 --> 00:22:49,040
messaging service send our our means

00:22:45,920 --> 00:22:51,260
send request response and we give the

00:22:49,040 --> 00:22:52,940
messaging service our call back so it

00:22:51,260 --> 00:22:56,330
knows where to deliver those responses

00:22:52,940 --> 00:22:58,130
back to after we've gone through all

00:22:56,330 --> 00:23:01,370
those read commands then we move on to

00:22:58,130 --> 00:23:03,620
the next part of this function where we

00:23:01,370 --> 00:23:05,720
iterate through all the reed callbacks

00:23:03,620 --> 00:23:08,450
and courgette and remember this can

00:23:05,720 --> 00:23:10,520
pluck so we could block on the first one

00:23:08,450 --> 00:23:12,560
while the other two are finished but it

00:23:10,520 --> 00:23:15,980
won't block if they've already finished

00:23:12,560 --> 00:23:17,930
when we get to call get that get can

00:23:15,980 --> 00:23:19,850
those exceptions bubble either they're

00:23:17,930 --> 00:23:21,080
the digest mismatch could bubble out and

00:23:19,850 --> 00:23:23,600
we'd have to go and run the read a

00:23:21,080 --> 00:23:25,670
second time the read timeout exception

00:23:23,600 --> 00:23:28,430
can bubble out we have to go and return

00:23:25,670 --> 00:23:30,200
the nearer back to the client but

00:23:28,430 --> 00:23:35,030
basically this is your read at the

00:23:30,200 --> 00:23:39,050
cluster level so the net service that we

00:23:35,030 --> 00:23:40,760
use to send those messages it takes care

00:23:39,050 --> 00:23:42,290
of their low level plumbing between all

00:23:40,760 --> 00:23:44,810
the nodes so it makes sure we've got

00:23:42,290 --> 00:23:45,970
connections back in put in and out it

00:23:44,810 --> 00:23:47,650
has Paul

00:23:45,970 --> 00:23:50,260
tools and thread pools and queues and

00:23:47,650 --> 00:23:51,310
stuff to handle all those messages but

00:23:50,260 --> 00:23:53,980
what we're going to talk about is the

00:23:51,310 --> 00:23:57,880
protocol that it handles that implements

00:23:53,980 --> 00:24:00,540
above that all the messages are sent

00:23:57,880 --> 00:24:03,820
between nodes are tagged with a verb so

00:24:00,540 --> 00:24:06,790
these verbs a request and response we

00:24:03,820 --> 00:24:09,400
request a mutation or we recruit or we

00:24:06,790 --> 00:24:12,400
request a read and then we get back a

00:24:09,400 --> 00:24:15,820
request response but for things like

00:24:12,400 --> 00:24:19,930
anti entropy there's a tree request and

00:24:15,820 --> 00:24:22,300
a tree response those verbs then a

00:24:19,930 --> 00:24:25,060
mapped to these objects we call verb

00:24:22,300 --> 00:24:28,120
handlers to implement a pretty simple

00:24:25,060 --> 00:24:30,840
interface it's just say do ok we pulled

00:24:28,120 --> 00:24:33,340
a message off the wire take care of it

00:24:30,840 --> 00:24:35,470
but just because we've got an object we

00:24:33,340 --> 00:24:39,820
can't run it what we actually need is a

00:24:35,470 --> 00:24:41,470
thread to run it on so the stages that

00:24:39,820 --> 00:24:43,990
we mentioned before the things that you

00:24:41,470 --> 00:24:45,910
see in no tool TP stats where you see

00:24:43,990 --> 00:24:48,550
all the different thread pools are also

00:24:45,910 --> 00:24:51,430
mapped to verbs so now we know if a

00:24:48,550 --> 00:24:53,140
message comes in and it's a mutation we

00:24:51,430 --> 00:24:54,580
know we can give it to this class this

00:24:53,140 --> 00:24:57,940
object and we can run it in this thread

00:24:54,580 --> 00:25:01,600
pool and that's what the receive

00:24:57,940 --> 00:25:05,470
function does takes a message wraps it

00:25:01,600 --> 00:25:08,320
in this message delivery task works out

00:25:05,470 --> 00:25:10,050
what stage that message maps to gets the

00:25:08,320 --> 00:25:12,940
thread pool the execution handler and

00:25:10,050 --> 00:25:14,170
drops the message in it this is

00:25:12,940 --> 00:25:15,610
happening on the thread that's just

00:25:14,170 --> 00:25:18,520
doing this it's just pulling stuff off

00:25:15,610 --> 00:25:22,030
the wire working out where the go and go

00:25:18,520 --> 00:25:23,830
in getting the next thing when the

00:25:22,030 --> 00:25:26,200
message delivery task gets a chance to

00:25:23,830 --> 00:25:29,310
run its over in the thread port like

00:25:26,200 --> 00:25:31,900
your mutation or your read thread pool

00:25:29,310 --> 00:25:34,390
it looks at the message and looks at

00:25:31,900 --> 00:25:36,310
when it was constructed and if it was

00:25:34,390 --> 00:25:40,090
constructed more than RPC timeout

00:25:36,310 --> 00:25:41,770
seconds ago it just drops it increments

00:25:40,090 --> 00:25:46,060
a counter that says we dropped a message

00:25:41,770 --> 00:25:48,940
and exits the function so when you see

00:25:46,060 --> 00:25:51,430
drop messages in TP stats or you see

00:25:48,940 --> 00:25:52,330
drop messages in your logs that's where

00:25:51,430 --> 00:25:55,539
they come from

00:25:52,330 --> 00:25:58,720
delivery task everything is good though

00:25:55,539 --> 00:26:02,950
we can grab that message get the verb

00:25:58,720 --> 00:26:05,320
handler get that object and ask it to

00:26:02,950 --> 00:26:06,970
process the message the verb handler

00:26:05,320 --> 00:26:08,980
then understands oh you're doing a

00:26:06,970 --> 00:26:12,279
mutation I know how to do with those and

00:26:08,980 --> 00:26:13,809
the execute on the thread in the thread

00:26:12,279 --> 00:26:16,419
pool that we've put it onto so in your

00:26:13,809 --> 00:26:21,190
mutation thread pool that's where you

00:26:16,419 --> 00:26:23,409
process that's where you're wrong to

00:26:21,190 --> 00:26:25,299
understand what nodes we want to get

00:26:23,409 --> 00:26:28,570
involved in the request where you have a

00:26:25,299 --> 00:26:31,230
distributed hash tree in the DHT package

00:26:28,570 --> 00:26:34,419
this is where your partitioner lives

00:26:31,230 --> 00:26:36,330
it's a bit more on this interface but

00:26:34,419 --> 00:26:39,760
the basic things that you'd expect our

00:26:36,330 --> 00:26:41,559
get the token for this key and get

00:26:39,760 --> 00:26:44,440
random token which is used by virtual

00:26:41,559 --> 00:26:46,419
knows when it's starting up for the

00:26:44,440 --> 00:26:49,750
first time or when you're running

00:26:46,419 --> 00:26:51,639
shuffle I think we've got a few

00:26:49,750 --> 00:26:54,669
implementations of those that the local

00:26:51,639 --> 00:26:58,090
partitioner that's used by system key

00:26:54,669 --> 00:27:00,399
spaces and secondary indexes it's their

00:26:58,090 --> 00:27:02,889
only sort locally local partition that

00:27:00,399 --> 00:27:06,460
doesn't have any knowledge of other

00:27:02,889 --> 00:27:09,100
nodes in the cluster ran the petitioner

00:27:06,460 --> 00:27:12,279
was the default prior to 1.2 they run

00:27:09,100 --> 00:27:16,149
seen that guy murmur 3 is the default in

00:27:12,279 --> 00:27:19,659
1.2 and above partition errs talk about

00:27:16,149 --> 00:27:22,419
tokens they're basically just a you know

00:27:19,659 --> 00:27:26,919
a bunch of wrapper stuff around storing

00:27:22,419 --> 00:27:29,590
one value the bytes token is used by the

00:27:26,919 --> 00:27:32,620
local petitioner and the byte ordered

00:27:29,590 --> 00:27:36,429
partitioner big industry token was used

00:27:32,620 --> 00:27:39,960
by random petitioner and long token is

00:27:36,429 --> 00:27:39,960
used by the murmur three partitioner

00:27:40,380 --> 00:27:45,100
locator is where we have the code that

00:27:43,299 --> 00:27:47,110
actually takes the token and our

00:27:45,100 --> 00:27:50,380
consistent mapping consistent hashing

00:27:47,110 --> 00:27:51,940
mapping and turns that into well you

00:27:50,380 --> 00:27:55,779
should now go and talk to these three

00:27:51,940 --> 00:27:58,240
nodes probably seen the snitch in there

00:27:55,779 --> 00:28:00,250
at some point its interface as roughly

00:27:58,240 --> 00:28:02,020
what you did mention that the idea of

00:28:00,250 --> 00:28:03,750
the snitch is to say hey I've got this

00:28:02,020 --> 00:28:06,320
IP address what Rack does it

00:28:03,750 --> 00:28:08,760
in what data center doesn't live in and

00:28:06,320 --> 00:28:12,060
also supports way to order those by

00:28:08,760 --> 00:28:14,400
proximity so I'm this node I'm in rack

00:28:12,060 --> 00:28:17,180
one in DC one please order all these

00:28:14,400 --> 00:28:19,410
nodes to be but once the closest to me

00:28:17,180 --> 00:28:22,220
we have our simple and property file

00:28:19,410 --> 00:28:25,680
sketch in ec2 and a bunch of other ones

00:28:22,220 --> 00:28:28,800
on top of that the replication strategy

00:28:25,680 --> 00:28:31,800
is the guy that actually says these are

00:28:28,800 --> 00:28:33,720
the three notes to go to as a function

00:28:31,800 --> 00:28:35,580
called calculate natural endpoints and

00:28:33,720 --> 00:28:38,070
this takes just some position in the

00:28:35,580 --> 00:28:40,650
ring and it works out the token range

00:28:38,070 --> 00:28:43,980
that includes that position in the ring

00:28:40,650 --> 00:28:47,340
and then it returns back then it works

00:28:43,980 --> 00:28:50,100
out what nodes actually are replicas for

00:28:47,340 --> 00:28:53,130
that token range that information is

00:28:50,100 --> 00:28:55,200
normally cached and that the crunchy bit

00:28:53,130 --> 00:28:58,050
that works that out is in calculate

00:28:55,200 --> 00:29:00,270
natural endpoints that's abstracting the

00:28:58,050 --> 00:29:02,550
base and implemented in a simple

00:29:00,270 --> 00:29:06,360
strategy and the network topology

00:29:02,550 --> 00:29:07,830
strategy the implementation of that has

00:29:06,360 --> 00:29:09,720
got nice comments in there and is pretty

00:29:07,830 --> 00:29:11,910
easy to understand and I know that there

00:29:09,720 --> 00:29:13,620
have been times when people do things

00:29:11,910 --> 00:29:15,630
with network topology strategy and

00:29:13,620 --> 00:29:17,450
actually have multiple data centers and

00:29:15,630 --> 00:29:20,370
racks and they end up with an unevenly

00:29:17,450 --> 00:29:22,260
balanced load if you have a look at that

00:29:20,370 --> 00:29:24,980
calculate natural endpoints function you

00:29:22,260 --> 00:29:28,710
can get a feel for what's going on there

00:29:24,980 --> 00:29:31,560
finally we have a list of token metadata

00:29:28,710 --> 00:29:34,200
so a map that goes from token to

00:29:31,560 --> 00:29:35,850
endpoint and a map that says oh these

00:29:34,200 --> 00:29:39,390
tokens are bootstrapping they're joining

00:29:35,850 --> 00:29:41,220
the cluster a list of no tokens are

00:29:39,390 --> 00:29:42,930
leaving the nodes that leaving the

00:29:41,220 --> 00:29:45,180
cluster and a bunch of other things like

00:29:42,930 --> 00:29:49,080
that they're just mapping around from

00:29:45,180 --> 00:29:51,510
one to the other last thing to look at

00:29:49,080 --> 00:29:53,520
the Dynamo layer is gossip and gossip

00:29:51,510 --> 00:29:56,670
can be a little bit confusing so trying

00:29:53,520 --> 00:29:59,580
to not get too deep into this guy what

00:29:56,670 --> 00:30:05,820
gossip does is allows nodes to swap

00:29:59,580 --> 00:30:07,740
values each value has a string and it's

00:30:05,820 --> 00:30:10,320
immutable and it has a version number

00:30:07,740 --> 00:30:12,030
that version number starts at zero when

00:30:10,320 --> 00:30:14,010
the service started every time the

00:30:12,030 --> 00:30:14,830
service starts and every time a new

00:30:14,010 --> 00:30:17,080
version

00:30:14,830 --> 00:30:21,370
value is created the version number is

00:30:17,080 --> 00:30:23,529
increased those version values are used

00:30:21,370 --> 00:30:25,929
to store values for things we call

00:30:23,529 --> 00:30:28,450
application state if you run the tool

00:30:25,929 --> 00:30:29,860
gossip info you'll see these come

00:30:28,450 --> 00:30:33,909
through the status and bowed and

00:30:29,860 --> 00:30:35,860
scheming things like that and there's a

00:30:33,909 --> 00:30:38,769
special piece of application state we

00:30:35,860 --> 00:30:40,840
have called the heartbeat statement this

00:30:38,769 --> 00:30:42,909
stores the version number and the

00:30:40,840 --> 00:30:45,549
heartbeat is increased every second when

00:30:42,909 --> 00:30:47,799
gossip runs and also has this thing we

00:30:45,549 --> 00:30:51,880
call the generation number you can see

00:30:47,799 --> 00:30:54,039
this bubble out in no tool info first

00:30:51,880 --> 00:30:56,080
time the service starts it gets the

00:30:54,039 --> 00:30:59,409
current millisecond time or second time

00:30:56,080 --> 00:31:01,149
can't remember and stores otherwise it's

00:30:59,409 --> 00:31:03,240
generation number every time the service

00:31:01,149 --> 00:31:05,289
starts after that is incremented by one

00:31:03,240 --> 00:31:07,779
every time there's some sort of major

00:31:05,289 --> 00:31:09,490
change on the server so it's token has

00:31:07,779 --> 00:31:12,669
changed or something like that it

00:31:09,490 --> 00:31:18,610
increases it by one essentially says

00:31:12,669 --> 00:31:20,740
this is a new instance of the server the

00:31:18,610 --> 00:31:22,659
application state the version values and

00:31:20,740 --> 00:31:25,210
the heartbeat are all used in gossip

00:31:22,659 --> 00:31:28,389
which is orchestrated by a gossip task

00:31:25,210 --> 00:31:31,870
runs on its own thread pool and the

00:31:28,389 --> 00:31:35,889
process here is send us in send out a

00:31:31,870 --> 00:31:40,299
sin message get an act back send an

00:31:35,889 --> 00:31:42,130
actor inside the gossiper we make these

00:31:40,299 --> 00:31:45,159
things called gossip digests and this

00:31:42,130 --> 00:31:47,950
says four nodes foo I have generation 6

00:31:45,159 --> 00:31:53,289
and the highest version value i have is

00:31:47,950 --> 00:31:55,149
600 calculates one of those for every

00:31:53,289 --> 00:31:57,990
know that knows about wraps those up in

00:31:55,149 --> 00:32:00,429
a send message and then sends them off

00:31:57,990 --> 00:32:02,620
calling send one way so we don't expect

00:32:00,429 --> 00:32:04,960
the response on this guy we're not using

00:32:02,620 --> 00:32:09,090
a recall back but we're still using the

00:32:04,960 --> 00:32:11,620
messaging service to its deliver it

00:32:09,090 --> 00:32:14,250
gossiping goes to one member that's lies

00:32:11,620 --> 00:32:17,289
one endpoint we know that's alive if

00:32:14,250 --> 00:32:19,419
there's a dead end point we gossip to

00:32:17,289 --> 00:32:21,970
that with a certain probability based on

00:32:19,419 --> 00:32:23,679
how many dead end points there are if we

00:32:21,970 --> 00:32:25,720
haven't gossiped to a seed by that point

00:32:23,679 --> 00:32:28,210
then we gossiped to see

00:32:25,720 --> 00:32:33,190
a certain probability based on how many

00:32:28,210 --> 00:32:35,350
seeds there are on the receiving end we

00:32:33,190 --> 00:32:39,160
use a verb handler like we talked about

00:32:35,350 --> 00:32:41,530
before it gets delivered that gossips in

00:32:39,160 --> 00:32:44,140
message that we constructed makes a call

00:32:41,530 --> 00:32:45,730
to the gossiper class and inside the

00:32:44,140 --> 00:32:48,460
examine gossiper we look at all those

00:32:45,730 --> 00:32:51,220
digest messages and do a bunch of things

00:32:48,460 --> 00:32:54,460
like this we say the sending message has

00:32:51,220 --> 00:32:56,830
it for service foo has a generation of

00:32:54,460 --> 00:32:58,660
six and I have a generation of five for

00:32:56,830 --> 00:33:00,820
that I'm going to get that going to give

00:32:58,660 --> 00:33:02,560
me everything he knows about that all

00:33:00,820 --> 00:33:05,020
the other version values about sue I

00:33:02,560 --> 00:33:06,640
want from him or the other way the

00:33:05,020 --> 00:33:08,650
sending message they're sending node has

00:33:06,640 --> 00:33:11,170
a generation of five I have a generation

00:33:08,650 --> 00:33:14,560
of sex I'm going to tell that all of my

00:33:11,170 --> 00:33:17,710
version values if the two generations

00:33:14,560 --> 00:33:20,890
match then only values which have a hut

00:33:17,710 --> 00:33:23,050
which are i will send that the other

00:33:20,890 --> 00:33:24,820
node the version values that I have that

00:33:23,050 --> 00:33:27,610
have a higher version number than what

00:33:24,820 --> 00:33:29,620
it's seen or I will request from the

00:33:27,610 --> 00:33:33,430
other node things that have a higher

00:33:29,620 --> 00:33:36,310
version number back on the initial nary

00:33:33,430 --> 00:33:39,060
node we have the act two verb handler

00:33:36,310 --> 00:33:41,320
this guy calls the failure detector and

00:33:39,060 --> 00:33:43,410
tells the failure detected hey we found

00:33:41,320 --> 00:33:45,970
out some more information about foo

00:33:43,410 --> 00:33:48,610
maybe you could market this up maybe it

00:33:45,970 --> 00:33:51,730
was dead before we can add in there some

00:33:48,610 --> 00:33:54,280
heartbeat information into the latency

00:33:51,730 --> 00:33:56,620
windows that we track to take notes down

00:33:54,280 --> 00:33:59,020
and then all the information that the

00:33:56,620 --> 00:34:01,180
other know told us we will add that to

00:33:59,020 --> 00:34:03,160
our stuff so if we now know that node

00:34:01,180 --> 00:34:06,910
foo has changed to a different version

00:34:03,160 --> 00:34:08,770
will update that in our list and we

00:34:06,910 --> 00:34:12,430
construct the act too and send that guy

00:34:08,770 --> 00:34:14,680
away that gets us out of the Dynamo

00:34:12,430 --> 00:34:18,130
layer and we got there the Dynamo layer

00:34:14,680 --> 00:34:21,040
by calling stages the stages are listed

00:34:18,130 --> 00:34:23,470
in control then the concurrent package

00:34:21,040 --> 00:34:25,410
there's also a database package down

00:34:23,470 --> 00:34:28,330
here which is the actual database

00:34:25,410 --> 00:34:31,380
there's cash flow and tracing packages

00:34:28,330 --> 00:34:33,670
which we don't have time for today

00:34:31,380 --> 00:34:36,820
stages are managed in this really simple

00:34:33,670 --> 00:34:38,169
class called stage manager in its static

00:34:36,820 --> 00:34:41,399
initialization it

00:34:38,169 --> 00:34:44,349
it's a map of stages to thread pools

00:34:41,399 --> 00:34:46,329
reads your configuration settings so if

00:34:44,349 --> 00:34:49,569
you're in your config it says concurrent

00:34:46,329 --> 00:34:51,760
rights is 32 it creates the mutation

00:34:49,569 --> 00:34:54,429
thread pool and says it can have 32

00:34:51,760 --> 00:34:55,960
threads in there provides a simple

00:34:54,429 --> 00:34:59,790
function to go and get that that we've

00:34:55,960 --> 00:35:02,559
seen before in the messaging service

00:34:59,790 --> 00:35:05,890
those stages are the stages that you see

00:35:02,559 --> 00:35:09,460
in no tool TP stats read the mutation

00:35:05,890 --> 00:35:12,880
and gossip and those sorts of things so

00:35:09,460 --> 00:35:14,950
it's about it for a concurrent DB is the

00:35:12,880 --> 00:35:16,839
actual database as I said before this is

00:35:14,950 --> 00:35:20,500
just a database and then on top of that

00:35:16,839 --> 00:35:23,290
is this orchestration of dynamo the

00:35:20,500 --> 00:35:27,750
thing that you think of as a key space

00:35:23,290 --> 00:35:29,980
is implemented in a class called table I

00:35:27,750 --> 00:35:33,069
like this because it makes me think of

00:35:29,980 --> 00:35:34,480
oh it's big table but sometimes that can

00:35:33,069 --> 00:35:36,369
be a bit confusing if you open up the

00:35:34,480 --> 00:35:40,299
code and you look at a class called

00:35:36,369 --> 00:35:42,099
table we open this up all the time if

00:35:40,299 --> 00:35:43,930
it's open once we don't open it again

00:35:42,099 --> 00:35:46,660
but you all they see calls the table

00:35:43,930 --> 00:35:48,520
don't open together reference it has a

00:35:46,660 --> 00:35:51,220
function on it called get column family

00:35:48,520 --> 00:35:52,780
store that's because the thing that you

00:35:51,220 --> 00:35:54,760
think of does a column family is

00:35:52,780 --> 00:35:58,599
implemented in a class called column

00:35:54,760 --> 00:36:00,700
family store it has some top-level entry

00:35:58,599 --> 00:36:03,270
points for getting a row and applying

00:36:00,700 --> 00:36:05,980
mutations so if you're going to update

00:36:03,270 --> 00:36:09,640
or read from a table you go to the table

00:36:05,980 --> 00:36:11,799
and do it so we have our column family

00:36:09,640 --> 00:36:14,530
store this is the actual column family

00:36:11,799 --> 00:36:18,010
it has to read from it has a function

00:36:14,530 --> 00:36:20,230
called get column family inside

00:36:18,010 --> 00:36:23,109
Cassandra the idea of a column family

00:36:20,230 --> 00:36:25,660
not Colin family store is just a sordid

00:36:23,109 --> 00:36:28,510
collection of columns it's a fragment or

00:36:25,660 --> 00:36:32,020
an entire row it doesn't represent the

00:36:28,510 --> 00:36:33,730
column family itself in its entirety all

00:36:32,020 --> 00:36:36,210
the action for doing read happens in a

00:36:33,730 --> 00:36:38,650
function called get top-level columns

00:36:36,210 --> 00:36:41,349
and there's an apply function just like

00:36:38,650 --> 00:36:43,240
we saw on the table it has some extra

00:36:41,349 --> 00:36:48,099
things on there to update in secondary

00:36:43,240 --> 00:36:50,280
indexes the column family is abstracted

00:36:48,099 --> 00:36:52,690
into this idea of a column

00:36:50,280 --> 00:36:54,910
it's got some functions you'd expect add

00:36:52,690 --> 00:36:58,690
and remove and it has two

00:36:54,910 --> 00:37:01,090
implementations Colin family is used on

00:36:58,690 --> 00:37:04,480
standard columns and top level is top

00:37:01,090 --> 00:37:06,910
level columns and super column is used

00:37:04,480 --> 00:37:10,780
for top-level columns in a Super Colon

00:37:06,910 --> 00:37:12,550
family so this is one of the reasons why

00:37:10,780 --> 00:37:15,010
most of the developers on Cassandra

00:37:12,550 --> 00:37:16,720
don't really like super column families

00:37:15,010 --> 00:37:18,940
because there are these sorts of

00:37:16,720 --> 00:37:23,470
exceptions and different code paths all

00:37:18,940 --> 00:37:27,160
through it the actual mapping of names

00:37:23,470 --> 00:37:30,130
two columns is handled by things that

00:37:27,160 --> 00:37:32,260
implement I saw the columns we have some

00:37:30,130 --> 00:37:35,470
different implementations here the array

00:37:32,260 --> 00:37:37,690
back saw the columns is a thread unsafe

00:37:35,470 --> 00:37:39,550
very fast collection it's used on the

00:37:37,690 --> 00:37:41,410
repast was one of the reasons we got

00:37:39,550 --> 00:37:43,210
like a big spoon speed boost in like

00:37:41,410 --> 00:37:45,280
point one I think it was from memory as

00:37:43,210 --> 00:37:47,560
because this went into the repast and

00:37:45,280 --> 00:37:50,230
had a lot less contention a lot less

00:37:47,560 --> 00:37:52,300
overhead on that wreath path we have one

00:37:50,230 --> 00:37:55,080
thread during the reeds and it just

00:37:52,300 --> 00:37:58,750
needs some where they put put some data

00:37:55,080 --> 00:38:01,600
atomic saw the columns implements the

00:37:58,750 --> 00:38:04,960
users the snappy tree this came in in

00:38:01,600 --> 00:38:07,090
point 11 from memory this is the thing

00:38:04,960 --> 00:38:10,240
that says oh you want to update 50

00:38:07,090 --> 00:38:15,070
columns well I'll use snap tree and it's

00:38:10,240 --> 00:38:17,530
0 copy to do then update and either 0 of

00:38:15,070 --> 00:38:21,070
those 0 or 50 columns will be available

00:38:17,530 --> 00:38:23,920
and visible at any one time this is only

00:38:21,070 --> 00:38:25,750
used on the mem table tree map back saw

00:38:23,920 --> 00:38:27,340
the columns is just another sort of

00:38:25,750 --> 00:38:31,750
thread safe collection that's used in a

00:38:27,340 --> 00:38:33,940
bunch of other places so there's a mem

00:38:31,750 --> 00:38:37,000
people that in-memory representation of

00:38:33,940 --> 00:38:38,620
your column family for once there's a

00:38:37,000 --> 00:38:40,540
funk there's a class that actually just

00:38:38,620 --> 00:38:45,940
Maps straight to the concept been tabled

00:38:40,540 --> 00:38:47,260
mm table has a put on it if you're want

00:38:45,940 --> 00:38:49,930
to understand how it does the rights

00:38:47,260 --> 00:38:53,320
that process starts up in the column

00:38:49,930 --> 00:38:55,330
family store that works out what mem

00:38:53,320 --> 00:38:57,160
tables we want to flush if the secondary

00:38:55,330 --> 00:38:59,740
index is involved we want to flush

00:38:57,160 --> 00:39:02,610
multiple because secondary indexes are

00:38:59,740 --> 00:39:05,290
just implemented as column families in

00:39:02,610 --> 00:39:07,450
that enters the mem table is about this

00:39:05,290 --> 00:39:08,950
function called flush and signal and the

00:39:07,450 --> 00:39:10,990
reason that latch is there is because

00:39:08,950 --> 00:39:12,580
we're going to count down like we're

00:39:10,990 --> 00:39:17,020
going to flush this column family and

00:39:12,580 --> 00:39:20,110
it's three secondary indexes it creates

00:39:17,020 --> 00:39:23,800
a runnable that gets put into the flush

00:39:20,110 --> 00:39:25,600
writer execution handler inside the yam

00:39:23,800 --> 00:39:28,920
all you'll see anything called a flush

00:39:25,600 --> 00:39:32,380
flush rider queue size from memory

00:39:28,920 --> 00:39:35,230
there's a thread pool and the queue that

00:39:32,380 --> 00:39:37,750
Q is normally for by default and this

00:39:35,230 --> 00:39:40,180
guy gets dropped in there when he gets a

00:39:37,750 --> 00:39:42,970
chance to run he makes this thing called

00:39:40,180 --> 00:39:45,220
an SS table writer iterates through all

00:39:42,970 --> 00:39:47,800
the rows and the column families that

00:39:45,220 --> 00:39:51,520
are in here calls the rider to append

00:39:47,800 --> 00:39:53,740
that writer creates that data DB

00:39:51,520 --> 00:39:56,830
component creates the bloom filter on

00:39:53,740 --> 00:39:58,930
the fly and creates the index the

00:39:56,830 --> 00:40:00,520
primary index component dash index takes

00:39:58,930 --> 00:40:04,720
care of all those other components that

00:40:00,520 --> 00:40:07,720
we have on disk for the SS table back to

00:40:04,720 --> 00:40:12,160
the reaper here's our read commands that

00:40:07,720 --> 00:40:14,320
we saw generated in the API layer simple

00:40:12,160 --> 00:40:16,540
interface called get row give it the

00:40:14,320 --> 00:40:20,020
table you want to read from and you'll

00:40:16,540 --> 00:40:22,660
get it back sliced by names read command

00:40:20,020 --> 00:40:26,380
is used when you say get me foo and bar

00:40:22,660 --> 00:40:28,330
and beds and slice from read command is

00:40:26,380 --> 00:40:32,400
used when you say get me 20 columns

00:40:28,330 --> 00:40:36,690
after foo internally these guys use

00:40:32,400 --> 00:40:39,400
these things we call disk Adam filters

00:40:36,690 --> 00:40:42,880
this cabin filter is something that can

00:40:39,400 --> 00:40:46,420
iterate over columns from a mem table or

00:40:42,880 --> 00:40:49,450
from one ss table according to some

00:40:46,420 --> 00:40:52,300
criteria the identity query filter is

00:40:49,450 --> 00:40:54,880
used in compaction and repair this just

00:40:52,300 --> 00:40:56,470
returns every column the names query

00:40:54,880 --> 00:40:59,500
filter is used when we've got the list

00:40:56,470 --> 00:41:01,060
of named columns that nowadays has lots

00:40:59,500 --> 00:41:05,530
of smarts in there to short-circuit the

00:41:01,060 --> 00:41:07,270
process so we getting columns by name is

00:41:05,530 --> 00:41:10,630
the fastest query you can do now

00:41:07,270 --> 00:41:12,760
sometimes the slice query filter is used

00:41:10,630 --> 00:41:13,340
when you say get me or 20 columns after

00:41:12,760 --> 00:41:16,500
food

00:41:13,340 --> 00:41:18,720
so in general what happens is your

00:41:16,500 --> 00:41:21,510
request comes into Cassandra comes into

00:41:18,720 --> 00:41:23,970
the custom t thread pool server or into

00:41:21,510 --> 00:41:27,980
the message dispatcher comes down to the

00:41:23,970 --> 00:41:30,830
Cassandra server or the query processor

00:41:27,980 --> 00:41:33,630
converges down into the storage proxy

00:41:30,830 --> 00:41:35,970
which delivers a message down into a

00:41:33,630 --> 00:41:39,420
stage that stage could either be local

00:41:35,970 --> 00:41:41,280
or on another machine the stage gets

00:41:39,420 --> 00:41:44,370
that message to run inside the thread

00:41:41,280 --> 00:41:47,010
pool we do the mutation we do the read

00:41:44,370 --> 00:41:48,930
we kind of reverse that process back to

00:41:47,010 --> 00:41:51,450
deliver your responses back deliver the

00:41:48,930 --> 00:41:54,510
response back to the coordinator back

00:41:51,450 --> 00:41:58,800
through the messaging protocol and back

00:41:54,510 --> 00:42:01,050
to the callback handler that we saw that

00:41:58,800 --> 00:42:03,990
that the storage proxy is probably

00:42:01,050 --> 00:42:06,600
waiting on that's the entire

00:42:03,990 --> 00:42:09,620
architecture and that's the end of my

00:42:06,600 --> 00:42:09,620
talk thanks

00:42:20,900 --> 00:42:24,310

YouTube URL: https://www.youtube.com/watch?v=x6YZN0Zg6Bc


