Title: Virtual Nodes: Rethinking Topology in Cassandra
Publication date: 2013-10-18
Playlist: Apachecon NA 2013 - day 3
Description: 
	Eric Evans
ApacheCon NA 2013
Cassandra
Captions: 
	00:00:00,260 --> 00:00:07,859
Wow as to the this is what you get for

00:00:04,170 --> 00:00:11,969
coming in after lunch right this is like

00:00:07,859 --> 00:00:13,380
twice as many of those I expect it so

00:00:11,969 --> 00:00:15,360
first first things first

00:00:13,380 --> 00:00:18,630
I'll go ahead and mention that I'm I'm

00:00:15,360 --> 00:00:22,320
pretty sick like cold and cold or flu or

00:00:18,630 --> 00:00:24,000
something so if I make funny sniffing

00:00:22,320 --> 00:00:28,199
noises or coughing or something like

00:00:24,000 --> 00:00:30,330
that please please ignore me and when it

00:00:28,199 --> 00:00:32,420
comes time to the for any questions if

00:00:30,330 --> 00:00:34,980
there are any if I could have somebody

00:00:32,420 --> 00:00:36,360
up close yep maybe Lois you know you

00:00:34,980 --> 00:00:39,149
yelling back at me because my ears are

00:00:36,360 --> 00:00:41,370
all plugged up and then I'm also on like

00:00:39,149 --> 00:00:42,989
a lot of strange combination of

00:00:41,370 --> 00:00:44,670
over-the-counter medication so if I do

00:00:42,989 --> 00:00:46,079
something bizarre or get naked or

00:00:44,670 --> 00:00:50,550
something Lois maybe again you could

00:00:46,079 --> 00:00:54,840
come up and stop me so my name is Eric

00:00:50,550 --> 00:00:56,670
Evans I am on the Cassandra PMC and have

00:00:54,840 --> 00:00:58,550
been involved Cassandra since pretty

00:00:56,670 --> 00:01:01,620
much the time it went into the incubator

00:00:58,550 --> 00:01:05,339
first for Rackspace and more recently

00:01:01,620 --> 00:01:07,110
for at Kuni and myself and a co-worker

00:01:05,339 --> 00:01:09,600
Dakini are the ones that implemented

00:01:07,110 --> 00:01:11,100
virtual notes so I thought I'd make the

00:01:09,600 --> 00:01:12,240
rounds and kind of explain it it's it's

00:01:11,100 --> 00:01:17,040
a topic that a lot of people are

00:01:12,240 --> 00:01:19,890
interested in to kind of understand the

00:01:17,040 --> 00:01:21,869
motivation for doing this and and the

00:01:19,890 --> 00:01:24,780
lots and the whys it helps to review the

00:01:21,869 --> 00:01:27,090
you know how content-based distribution

00:01:24,780 --> 00:01:29,220
in Cassandra works so I think it's

00:01:27,090 --> 00:01:31,799
probably some overlap here between some

00:01:29,220 --> 00:01:33,780
of the earlier presentations but in case

00:01:31,799 --> 00:01:35,340
you weren't here for that or just to

00:01:33,780 --> 00:01:38,220
have it on top of your mind I'll go over

00:01:35,340 --> 00:01:40,200
it again so the way this works is you

00:01:38,220 --> 00:01:42,180
know we take the the namespace that all

00:01:40,200 --> 00:01:44,100
of our keys will will exist in if you

00:01:42,180 --> 00:01:47,850
think of it as the primary key that's

00:01:44,100 --> 00:01:49,530
pretty pretty close we take the

00:01:47,850 --> 00:01:51,750
namespace that will cover all possible

00:01:49,530 --> 00:01:52,280
keys and we order them in ascending

00:01:51,750 --> 00:01:55,229
order

00:01:52,280 --> 00:01:56,729
and you would kind of visualize this as

00:01:55,229 --> 00:01:59,729
a ring or you can think of it as a clock

00:01:56,729 --> 00:02:03,450
face with the lowest value starts you

00:01:59,729 --> 00:02:05,430
know and works clockwise to higher and

00:02:03,450 --> 00:02:05,920
higher values until it rolls over again

00:02:05,430 --> 00:02:08,890
at the

00:02:05,920 --> 00:02:11,080
at the minimum value and then we

00:02:08,890 --> 00:02:13,830
position our nodes the nodes in our

00:02:11,080 --> 00:02:16,750
cluster around this ring or clock face

00:02:13,830 --> 00:02:18,430
and the partitions the data that gets

00:02:16,750 --> 00:02:20,680
stored in each node is that intervening

00:02:18,430 --> 00:02:23,880
space between we know where the node

00:02:20,680 --> 00:02:26,140
rests on the ring and the node before it

00:02:23,880 --> 00:02:27,730
and this works pretty well because all

00:02:26,140 --> 00:02:30,069
you really need to do is find where your

00:02:27,730 --> 00:02:31,750
key sorts in that namespace and you

00:02:30,069 --> 00:02:34,150
found the node to store it on and

00:02:31,750 --> 00:02:37,209
likewise to go back and make subsequent

00:02:34,150 --> 00:02:39,870
reads and since it is a distributed

00:02:37,209 --> 00:02:44,040
system you know we're gonna store

00:02:39,870 --> 00:02:46,510
redundant copies for fault tolerance and

00:02:44,040 --> 00:02:49,750
we just need something deterministic

00:02:46,510 --> 00:02:51,610
based on that first location so once

00:02:49,750 --> 00:02:54,370
these are all on this on the you know

00:02:51,610 --> 00:02:55,420
laid out on the cluster no copy is more

00:02:54,370 --> 00:02:57,370
important than the other there's no

00:02:55,420 --> 00:03:00,100
notion of primary and secondary or

00:02:57,370 --> 00:03:02,650
primary and backup copies but for

00:03:00,100 --> 00:03:05,319
purposes of positioning you know placing

00:03:02,650 --> 00:03:06,910
them in the cluster this first note is

00:03:05,319 --> 00:03:10,030
the partitioning and everything else is

00:03:06,910 --> 00:03:12,010
based upon that so the most obvious you

00:03:10,030 --> 00:03:13,900
know simple and this is this is

00:03:12,010 --> 00:03:16,359
implemented in Cassandra as the simple

00:03:13,900 --> 00:03:18,790
strategy is to take the next n minus 1

00:03:16,359 --> 00:03:20,470
nodes around the ring in this case we

00:03:18,790 --> 00:03:22,989
have a replication factor of 3 so we

00:03:20,470 --> 00:03:25,390
positioned the one on node a and then

00:03:22,989 --> 00:03:28,600
the next 2 copies of that data go on B

00:03:25,390 --> 00:03:31,120
and C something else that may be a

00:03:28,600 --> 00:03:33,040
little bit worn out but it's it's you

00:03:31,120 --> 00:03:36,760
know bares repeating for this for this

00:03:33,040 --> 00:03:39,579
topic as the cap theorem so this is a

00:03:36,760 --> 00:03:41,470
sort of a device we use to explain the

00:03:39,579 --> 00:03:44,650
contentious properties of a distributed

00:03:41,470 --> 00:03:46,630
storage system so the the C stands for

00:03:44,650 --> 00:03:50,049
consistency and you know once we have

00:03:46,630 --> 00:03:51,850
multiple copies of data in a cluster we

00:03:50,049 --> 00:03:55,329
have the possibility for them to be

00:03:51,850 --> 00:03:57,519
inconsistent with one another the a

00:03:55,329 --> 00:03:59,019
stands for availability now that we have

00:03:57,519 --> 00:04:02,620
multiple copies in the system we have

00:03:59,019 --> 00:04:04,239
the ability to survive a node failure to

00:04:02,620 --> 00:04:06,880
you know write from a read to another

00:04:04,239 --> 00:04:08,760
copy and partition tolerance just means

00:04:06,880 --> 00:04:11,709
you know if some portion of this cluster

00:04:08,760 --> 00:04:13,090
becomes unavailable from another you

00:04:11,709 --> 00:04:15,459
know being tolerant of that partition

00:04:13,090 --> 00:04:17,080
would mean that you know we can continue

00:04:15,459 --> 00:04:18,590
reading and writing from these disjoint

00:04:17,080 --> 00:04:21,530
sections of the cluster

00:04:18,590 --> 00:04:23,150
that would be okay and the easiest way

00:04:21,530 --> 00:04:26,240
to think about why these are contentious

00:04:23,150 --> 00:04:29,300
is if you imagine a simple sort of

00:04:26,240 --> 00:04:33,530
master master replication between a

00:04:29,300 --> 00:04:35,060
couple of machines you know that in

00:04:33,530 --> 00:04:37,070
order to make sure that's consistent you

00:04:35,060 --> 00:04:39,139
would want that right to succeed on both

00:04:37,070 --> 00:04:40,810
nodes before you you know before you

00:04:39,139 --> 00:04:42,740
consider that a successful transaction

00:04:40,810 --> 00:04:45,530
that's how you would maintain

00:04:42,740 --> 00:04:48,139
consistency in a very simple situation

00:04:45,530 --> 00:04:49,790
like that but obviously if that's the

00:04:48,139 --> 00:04:51,919
you know that's the conditions by which

00:04:49,790 --> 00:04:53,720
you make a successful right then if one

00:04:51,919 --> 00:04:56,090
note is down

00:04:53,720 --> 00:04:57,710
you've lost availability so this this is

00:04:56,090 --> 00:04:59,240
why this is why we describe these as

00:04:57,710 --> 00:05:02,540
being contentious usually someone will

00:04:59,240 --> 00:05:04,250
say pick two you know to say that at any

00:05:02,540 --> 00:05:06,940
given instance in time you can only have

00:05:04,250 --> 00:05:10,070
guarantees on two of these properties

00:05:06,940 --> 00:05:12,110
but what we do in Cassandra is we we

00:05:10,070 --> 00:05:14,630
have what was called tunable consistency

00:05:12,110 --> 00:05:16,250
and if you've read the the white paper

00:05:14,630 --> 00:05:17,770
on dynamo this is very similar to what

00:05:16,250 --> 00:05:21,199
they what they do there as well so

00:05:17,770 --> 00:05:25,220
assuming we have three copies we can

00:05:21,199 --> 00:05:26,330
decide on a per operation basis how much

00:05:25,220 --> 00:05:28,130
of that replication should be

00:05:26,330 --> 00:05:30,080
synchronous or asynchronous to let you

00:05:28,130 --> 00:05:33,169
trade off between consistency and

00:05:30,080 --> 00:05:36,440
availability so if you imagine a right

00:05:33,169 --> 00:05:38,120
happening at consistency level one and

00:05:36,440 --> 00:05:40,220
we're assuming that the the replication

00:05:38,120 --> 00:05:42,620
for these other two copies is going to

00:05:40,220 --> 00:05:45,320
happen asynchronously when the client

00:05:42,620 --> 00:05:47,870
hangs up we have no guarantees that that

00:05:45,320 --> 00:05:50,450
that data is consistent between all

00:05:47,870 --> 00:05:53,979
three copies but if for example we were

00:05:50,450 --> 00:05:56,750
able to do the the corresponding read at

00:05:53,979 --> 00:05:58,580
consistency level all and read from all

00:05:56,750 --> 00:06:00,260
of the copies there's no way we're we're

00:05:58,580 --> 00:06:02,000
not going to get that most recent value

00:06:00,260 --> 00:06:05,840
you know assuming that there was an

00:06:02,000 --> 00:06:07,340
inconsistency so we let you trade-off in

00:06:05,840 --> 00:06:09,370
this case we're saying you know we want

00:06:07,340 --> 00:06:12,950
maximum availability during the write

00:06:09,370 --> 00:06:17,030
and we want consistency at the expensive

00:06:12,950 --> 00:06:19,099
availability on the read so a more

00:06:17,030 --> 00:06:21,680
common scenario is we'll do something

00:06:19,099 --> 00:06:23,300
like quorum reads and writes and quorum

00:06:21,680 --> 00:06:25,760
just means is you know a simple majority

00:06:23,300 --> 00:06:28,130
of the nodes so using the same example

00:06:25,760 --> 00:06:30,310
of a replication factor of three we

00:06:28,130 --> 00:06:33,550
could do a right at quorum

00:06:30,310 --> 00:06:35,500
which would be two nodes and then you

00:06:33,550 --> 00:06:37,960
know we have no guarantees on that third

00:06:35,500 --> 00:06:40,960
third copy but if we did the right

00:06:37,960 --> 00:06:43,000
likewise a quorum there's no way we're

00:06:40,960 --> 00:06:44,440
not going to you know encounter the most

00:06:43,000 --> 00:06:46,960
recent value even if there's some

00:06:44,440 --> 00:06:49,690
inconsistency so this gives you read

00:06:46,960 --> 00:06:51,940
you're right consistency there may be in

00:06:49,690 --> 00:06:53,230
an inconsistency between there but

00:06:51,940 --> 00:06:54,760
you're always going to read you're right

00:06:53,230 --> 00:06:58,120
which is what people really can't count

00:06:54,760 --> 00:06:59,320
on care about and so the important

00:06:58,120 --> 00:07:00,850
property here is that you know so long

00:06:59,320 --> 00:07:03,430
as the number of reads you're blocking

00:07:00,850 --> 00:07:10,150
on or the number of reads that you're

00:07:03,430 --> 00:07:10,990
doing synchronously together with the

00:07:10,150 --> 00:07:13,030
number of writes you're doing

00:07:10,990 --> 00:07:14,260
synchronously synchronously so long as

00:07:13,030 --> 00:07:15,940
that is greater than your replication

00:07:14,260 --> 00:07:18,610
count you'll have that read your writing

00:07:15,940 --> 00:07:22,979
consistency so that's that's basically

00:07:18,610 --> 00:07:26,050
in a nutshell how how how the content

00:07:22,979 --> 00:07:27,010
distribution works in Cassandra or you

00:07:26,050 --> 00:07:30,370
know what I referred to in the first

00:07:27,010 --> 00:07:32,050
slide as a distributed hash table and it

00:07:30,370 --> 00:07:33,070
seems pretty elegant it seems pretty you

00:07:32,050 --> 00:07:34,870
know pretty simple pretty

00:07:33,070 --> 00:07:38,440
straightforward and produces you know

00:07:34,870 --> 00:07:40,120
like a really good results but it's not

00:07:38,440 --> 00:07:43,030
without you know like its share of

00:07:40,120 --> 00:07:45,400
problems it's a little bit too naive is

00:07:43,030 --> 00:07:48,370
the problem and so I'll explain why that

00:07:45,400 --> 00:07:50,560
is so the first the first example of

00:07:48,370 --> 00:07:53,130
this is that the the distribution of

00:07:50,560 --> 00:07:57,010
load of requests load is is not good

00:07:53,130 --> 00:07:58,419
with this this algorithm and if we go

00:07:57,010 --> 00:08:01,419
back and look at the example of a

00:07:58,419 --> 00:08:05,110
replication count of three you know four

00:08:01,419 --> 00:08:07,260
for the partition identified by a with

00:08:05,110 --> 00:08:09,610
the replicas being stored on B and C

00:08:07,260 --> 00:08:12,729
that's true for all of the nodes in the

00:08:09,610 --> 00:08:15,550
cluster so in this case the redundant

00:08:12,729 --> 00:08:21,789
copies for partition Z are also stored

00:08:15,550 --> 00:08:25,419
on a and B and likewise Y are being

00:08:21,789 --> 00:08:27,940
stored on Z and a so we've got a is

00:08:25,419 --> 00:08:32,710
sharing is you know it's participating

00:08:27,940 --> 00:08:36,860
in three replicas sets its Y za za B and

00:08:32,710 --> 00:08:38,360
ABC are all all share a in common

00:08:36,860 --> 00:08:40,400
so if you can imagine the situation

00:08:38,360 --> 00:08:42,110
where we lose one node it's just

00:08:40,400 --> 00:08:44,360
completely failed for some reason and

00:08:42,110 --> 00:08:45,710
it's not part of that quorum or part of

00:08:44,360 --> 00:08:49,400
that consistency level we can choose

00:08:45,710 --> 00:08:51,140
from and it's pretty obvious to imagine

00:08:49,400 --> 00:08:53,600
that this is going to play impose more

00:08:51,140 --> 00:08:55,370
load on the neighboring nodes because

00:08:53,600 --> 00:08:59,420
there's there's this locality property

00:08:55,370 --> 00:09:00,830
where the nodes around a however many

00:08:59,420 --> 00:09:03,860
depending on the other efficacious

00:09:00,830 --> 00:09:05,060
factor share in these replicas sets so

00:09:03,860 --> 00:09:06,680
if we're doing that consists that a

00:09:05,060 --> 00:09:09,200
quorum read and write like I described

00:09:06,680 --> 00:09:11,720
before instead of having three nodes

00:09:09,200 --> 00:09:14,360
available to do synchronous replication

00:09:11,720 --> 00:09:19,220
on to now we're doing them all on on

00:09:14,360 --> 00:09:20,390
just two because there's a failure so

00:09:19,220 --> 00:09:22,070
the way you would deal with a failure is

00:09:20,390 --> 00:09:25,880
obviously is you would replace that node

00:09:22,070 --> 00:09:27,710
or bring it back up and of course you

00:09:25,880 --> 00:09:29,540
know any any writes that occurred in the

00:09:27,710 --> 00:09:31,250
meantime need to be synchronized back

00:09:29,540 --> 00:09:34,220
over we need to get the consistency back

00:09:31,250 --> 00:09:36,290
up for that for that failed node and

00:09:34,220 --> 00:09:39,020
that involves streaming the root you

00:09:36,290 --> 00:09:43,160
know from the redundant copies on Y Z

00:09:39,020 --> 00:09:44,330
and B and C and it doesn't take you know

00:09:43,160 --> 00:09:45,950
it doesn't take much to figure out that

00:09:44,330 --> 00:09:49,130
that's going to impose even more load on

00:09:45,950 --> 00:09:51,260
those nodes probably at the time when we

00:09:49,130 --> 00:09:52,790
need to and we need to get load off of

00:09:51,260 --> 00:09:54,860
those nodes they're already incurring a

00:09:52,790 --> 00:09:56,780
heavy heavier than usual request load

00:09:54,860 --> 00:09:58,820
you know now we impose even more load on

00:09:56,780 --> 00:10:02,330
them in order to bring a back up to date

00:09:58,820 --> 00:10:07,430
with with the rest of the cluster so

00:10:02,330 --> 00:10:09,080
another example is the the distribution

00:10:07,430 --> 00:10:10,970
of the data so that was a distribution

00:10:09,080 --> 00:10:13,220
of load or request though I'll talk

00:10:10,970 --> 00:10:15,350
about the distribution of data so

00:10:13,220 --> 00:10:16,700
assuming you have a cluster of say four

00:10:15,350 --> 00:10:19,460
nodes

00:10:16,700 --> 00:10:21,770
you know nice even even partitions which

00:10:19,460 --> 00:10:23,870
incidentally this is this the the onus

00:10:21,770 --> 00:10:26,600
is on you to do this you would have to

00:10:23,870 --> 00:10:28,360
calculate four tokens that would

00:10:26,600 --> 00:10:30,710
position in this evenly

00:10:28,360 --> 00:10:33,830
you know the cluster grows and you need

00:10:30,710 --> 00:10:36,950
to you need to increase capacity you

00:10:33,830 --> 00:10:39,170
bootstrap a new note in but the very

00:10:36,950 --> 00:10:41,720
best you can do is bisect one of the

00:10:39,170 --> 00:10:43,320
existing partitions there's there's

00:10:41,720 --> 00:10:45,300
nothing else to do

00:10:43,320 --> 00:10:47,940
but that creates an imbalance you know

00:10:45,300 --> 00:10:50,100
now we have you know some some smaller

00:10:47,940 --> 00:10:52,200
partitions and some larger ones and the

00:10:50,100 --> 00:10:54,720
larger ones aren't receiving any relief

00:10:52,200 --> 00:10:56,760
from this so we're left to move the

00:10:54,720 --> 00:10:59,430
locations of those existing four nodes

00:10:56,760 --> 00:11:01,530
around the ring and of course that move

00:10:59,430 --> 00:11:04,890
of the of the partition means

00:11:01,530 --> 00:11:07,560
recalculating and moving data along with

00:11:04,890 --> 00:11:08,640
it and so that's not at all optimal we

00:11:07,560 --> 00:11:10,620
would we would want to just move the

00:11:08,640 --> 00:11:14,250
data necessary to accomplish this this

00:11:10,620 --> 00:11:17,490
add and so that's not a very good a very

00:11:14,250 --> 00:11:19,380
good way of expanding your cluster and

00:11:17,490 --> 00:11:20,940
we know this and so we usually tell you

00:11:19,380 --> 00:11:23,390
especially when your clusters are small

00:11:20,940 --> 00:11:25,770
and that that imbalance would be large

00:11:23,390 --> 00:11:27,270
we say double the size of your cluster

00:11:25,770 --> 00:11:28,590
and if you've been around Cassandra you

00:11:27,270 --> 00:11:30,060
probably heard someone say that you know

00:11:28,590 --> 00:11:32,130
if you have a small number of nodes and

00:11:30,060 --> 00:11:34,530
you're going to expand it for the first

00:11:32,130 --> 00:11:36,330
time double the size of the cluster and

00:11:34,530 --> 00:11:38,940
this is so that you could then bisect

00:11:36,330 --> 00:11:41,670
each range and not have to move move any

00:11:38,940 --> 00:11:44,610
but that's that's also a pretty pretty

00:11:41,670 --> 00:11:45,630
poor thing to have to do and we've

00:11:44,610 --> 00:11:48,690
probably gotten a little too comfortable

00:11:45,630 --> 00:11:52,500
at telling people to do that and so

00:11:48,690 --> 00:11:54,360
that's not good either so the top the

00:11:52,500 --> 00:11:55,890
topic of this talk is virtual nodes so

00:11:54,360 --> 00:12:00,170
it probably doesn't take much to guess

00:11:55,890 --> 00:12:03,570
that whoops that the solution to this is

00:12:00,170 --> 00:12:04,980
virtual nodes and so in that show what

00:12:03,570 --> 00:12:09,360
virtual nodes is we're just going to

00:12:04,980 --> 00:12:11,490
break that one-to-one relationship

00:12:09,360 --> 00:12:13,320
between a host and a token we're still

00:12:11,490 --> 00:12:15,210
going to divide the ring into you know

00:12:13,320 --> 00:12:17,010
into some number of partitions but it'll

00:12:15,210 --> 00:12:19,260
be more than the number of nodes we have

00:12:17,010 --> 00:12:21,530
and then we're going to you know

00:12:19,260 --> 00:12:23,670
randomly distribute these partitions to

00:12:21,530 --> 00:12:25,560
the to the nodes we have so the each

00:12:23,670 --> 00:12:30,480
node has more than one and that they're

00:12:25,560 --> 00:12:31,590
not you know contiguous and so the

00:12:30,480 --> 00:12:34,530
benefits of this is that it's

00:12:31,590 --> 00:12:37,560
operationally simpler we don't have to

00:12:34,530 --> 00:12:39,060
worry about moving moving our nodes

00:12:37,560 --> 00:12:42,030
around the ring if we're adding a new

00:12:39,060 --> 00:12:43,770
one because a new joining node can take

00:12:42,030 --> 00:12:46,170
an equal number of partitions from the

00:12:43,770 --> 00:12:47,580
existing nodes in the cluster which

00:12:46,170 --> 00:12:49,090
which is also results in better

00:12:47,580 --> 00:12:52,930
distribution of the

00:12:49,090 --> 00:12:54,430
of the load and the data and because you

00:12:52,930 --> 00:12:57,610
know it's now you know we're now sharing

00:12:54,430 --> 00:12:58,960
these ranges with with all of the other

00:12:57,610 --> 00:13:00,670
nodes in the cluster you know we can

00:12:58,960 --> 00:13:02,890
take advantage of concurrency instead of

00:13:00,670 --> 00:13:04,510
streaming from just the localized

00:13:02,890 --> 00:13:06,760
neighbors we can stream from all of the

00:13:04,510 --> 00:13:09,400
nodes in the cluster and this gives us

00:13:06,760 --> 00:13:11,470
smaller partitions you have a four node

00:13:09,400 --> 00:13:14,020
cluster convention in the cassandra you

00:13:11,470 --> 00:13:16,990
have four partitions you know we can now

00:13:14,020 --> 00:13:19,390
make make much more you know a much

00:13:16,990 --> 00:13:23,050
larger number of overall partitions that

00:13:19,390 --> 00:13:25,720
makes them smaller and you know if they

00:13:23,050 --> 00:13:28,180
fail for example failed to stream midway

00:13:25,720 --> 00:13:29,530
through we don't have to retry a great

00:13:28,180 --> 00:13:32,170
big partition we can do this more

00:13:29,530 --> 00:13:34,240
incrementally and it also provides a way

00:13:32,170 --> 00:13:36,190
of supporting heterogeneous Hardware

00:13:34,240 --> 00:13:41,740
because now not every node has to have

00:13:36,190 --> 00:13:43,210
the same percentage of data so there's a

00:13:41,740 --> 00:13:45,820
number of different strategies you could

00:13:43,210 --> 00:13:47,650
take for for implementing virtual nodes

00:13:45,820 --> 00:13:49,180
but I think they can all kind of be

00:13:47,650 --> 00:13:52,210
broken down into these three basic

00:13:49,180 --> 00:13:54,670
categories you'd have automatic sharding

00:13:52,210 --> 00:13:57,970
fixed partition assignment and random

00:13:54,670 --> 00:13:59,110
token assignment so by automatic

00:13:57,970 --> 00:14:02,050
sharding I'm saying

00:13:59,110 --> 00:14:03,339
kind of like way BigTable does or mongos

00:14:02,050 --> 00:14:05,710
Auto sharding if you're familiar with

00:14:03,339 --> 00:14:08,350
that is you know we have a a threshold

00:14:05,710 --> 00:14:09,670
on the size of the partition and when

00:14:08,350 --> 00:14:12,130
the partition exceeds that threshold

00:14:09,670 --> 00:14:15,670
then we split it and newly created

00:14:12,130 --> 00:14:17,230
partitions can be moved to nodes that

00:14:15,670 --> 00:14:21,550
have less data or you know are lower

00:14:17,230 --> 00:14:24,460
lower lower loaded the fixed partition

00:14:21,550 --> 00:14:26,620
assignment is where you would take you

00:14:24,460 --> 00:14:28,780
know a fixed number of total number of

00:14:26,620 --> 00:14:31,270
partitions for the for the entire key

00:14:28,780 --> 00:14:33,580
space you know you just start out with

00:14:31,270 --> 00:14:37,420
some number of partitions cluster wide

00:14:33,580 --> 00:14:39,280
we call that Q so then the the number of

00:14:37,420 --> 00:14:42,430
partitions per node would be Q over N

00:14:39,280 --> 00:14:45,760
where n is the number of nodes and then

00:14:42,430 --> 00:14:47,560
if you add a new node that new new node

00:14:45,760 --> 00:14:50,320
just simply gets some of the existing

00:14:47,560 --> 00:14:52,300
partitions from the other nodes in the

00:14:50,320 --> 00:14:53,860
cluster this is if you're familiar with

00:14:52,300 --> 00:14:56,230
the Dynamo paper this is what they refer

00:14:53,860 --> 00:14:57,760
to as strategy 3 what they what they

00:14:56,230 --> 00:14:58,690
eventually arrived at they tried three

00:14:57,760 --> 00:15:00,820
different approaches

00:14:58,690 --> 00:15:02,560
is the one they finally went with so

00:15:00,820 --> 00:15:06,010
it's also likewise the way Voldemort

00:15:02,560 --> 00:15:09,430
does it and finally we had the the

00:15:06,010 --> 00:15:11,110
random token assignment works by giving

00:15:09,430 --> 00:15:14,470
each node in the cluster a fixed number

00:15:11,110 --> 00:15:16,360
of tokens so we'll call that T T tokens

00:15:14,470 --> 00:15:20,800
and these are randomly calculated so we

00:15:16,360 --> 00:15:22,390
just randomly generate tokens partition

00:15:20,800 --> 00:15:25,630
identifiers if you will from within that

00:15:22,390 --> 00:15:26,860
you know namespace we talked about

00:15:25,630 --> 00:15:28,750
earlier you know we're having hash

00:15:26,860 --> 00:15:30,520
namespace you know murmur three or maybe

00:15:28,750 --> 00:15:35,400
md5 so this might be you know just

00:15:30,520 --> 00:15:37,570
randomly generate a 128-bit values and

00:15:35,400 --> 00:15:39,580
then you know we you know every new

00:15:37,570 --> 00:15:42,310
joining node simply generates and other

00:15:39,580 --> 00:15:45,520
you know T random tokens and those will

00:15:42,310 --> 00:15:47,770
inevitably you know cut into some

00:15:45,520 --> 00:15:49,900
existing key space that it's held by the

00:15:47,770 --> 00:15:51,490
other nodes and then if you're familiar

00:15:49,900 --> 00:15:53,950
with Lib Gotama this is kind of how it

00:15:51,490 --> 00:15:55,930
does it and if you generalize casandra's

00:15:53,950 --> 00:15:57,910
you know legacy system as you know where

00:15:55,930 --> 00:16:00,850
the where T equals one this is basically

00:15:57,910 --> 00:16:02,830
how we've been doing it all along

00:16:00,850 --> 00:16:05,710
and among these strategies the things to

00:16:02,830 --> 00:16:07,030
consider the important aspects are you

00:16:05,710 --> 00:16:09,850
know what is the number of partitions

00:16:07,030 --> 00:16:12,520
this this results in what is the size of

00:16:09,850 --> 00:16:15,160
the partitions and then you know how do

00:16:12,520 --> 00:16:17,290
these two properties change as we as we

00:16:15,160 --> 00:16:20,890
grow the cluster as we add more nodes or

00:16:17,290 --> 00:16:23,200
we add more more data and in summary

00:16:20,890 --> 00:16:25,720
this is kind of how they kind of how

00:16:23,200 --> 00:16:27,460
that shakes out I want to go over this

00:16:25,720 --> 00:16:28,900
this whole slide because I'm going to

00:16:27,460 --> 00:16:30,580
explain all these in depth but if you

00:16:28,900 --> 00:16:34,110
know for future reference these are this

00:16:30,580 --> 00:16:36,960
is this is how each of those things

00:16:34,110 --> 00:16:39,910
balances out so with automatic sharding

00:16:36,960 --> 00:16:43,210
the partition size is is constant

00:16:39,910 --> 00:16:44,500
because we have we have a threshold and

00:16:43,210 --> 00:16:46,720
we'll never grow beyond that threshold

00:16:44,500 --> 00:16:50,380
may be constants not the right word but

00:16:46,720 --> 00:16:52,089
they're at most some size and that's

00:16:50,380 --> 00:16:54,910
good having a nice predictable size of

00:16:52,089 --> 00:16:56,290
the partition is a good property but

00:16:54,910 --> 00:16:58,690
that means that the number of partitions

00:16:56,290 --> 00:17:00,220
scales with the size of the data set so

00:16:58,690 --> 00:17:03,430
you know as we add more and more data

00:17:00,220 --> 00:17:06,010
you know we result we create a you know

00:17:03,430 --> 00:17:09,699
linearly that results in an increase in

00:17:06,010 --> 00:17:11,050
partitions the number of partitions and

00:17:09,699 --> 00:17:12,160
that's not good you know if we're kind

00:17:11,050 --> 00:17:14,350
of aiming for

00:17:12,160 --> 00:17:15,880
you know infinite scalability which is

00:17:14,350 --> 00:17:18,420
maybe hubris but that we're kind of

00:17:15,880 --> 00:17:21,430
trying not to have an obvious bottleneck

00:17:18,420 --> 00:17:23,020
then you know you don't want the number

00:17:21,430 --> 00:17:24,400
of partitions to grow with the amount of

00:17:23,020 --> 00:17:28,330
data you know simply by the amount of

00:17:24,400 --> 00:17:29,560
data we add so that fixed partition

00:17:28,330 --> 00:17:31,840
assignment this is the one where we

00:17:29,560 --> 00:17:35,080
create Q tokens for the whole cluster

00:17:31,840 --> 00:17:37,450
upfront and then how many - how many of

00:17:35,080 --> 00:17:41,020
these tokens each node has is a function

00:17:37,450 --> 00:17:42,940
of how many nodes just Q over N this

00:17:41,020 --> 00:17:46,060
means that the number of partitions is

00:17:42,940 --> 00:17:48,670
constant which is also good but it means

00:17:46,060 --> 00:17:50,620
that the size of the partition scales

00:17:48,670 --> 00:17:52,930
with the amount of linearly with the

00:17:50,620 --> 00:17:54,160
amount of data we have so that just

00:17:52,930 --> 00:17:56,560
again that should kind of makes sense

00:17:54,160 --> 00:17:59,350
you know we add more nodes the

00:17:56,560 --> 00:18:02,560
partitions still continue to grow and

00:17:59,350 --> 00:18:04,870
that's not good and this method also has

00:18:02,560 --> 00:18:07,450
you know higher operational complexity

00:18:04,870 --> 00:18:10,330
because we have to sort of balance that

00:18:07,450 --> 00:18:12,610
you know token size and number of tokens

00:18:10,330 --> 00:18:15,700
when we create the cluster we have to

00:18:12,610 --> 00:18:17,920
you know that's these Q Q tokens we

00:18:15,700 --> 00:18:20,350
create we create up front and there's

00:18:17,920 --> 00:18:22,750
kind of no going back from that and so

00:18:20,350 --> 00:18:24,040
that's that's probably the wrong that's

00:18:22,750 --> 00:18:25,900
probably the point in time when you're

00:18:24,040 --> 00:18:27,070
least familiar with with what you're

00:18:25,900 --> 00:18:30,850
going to need is when you set it up

00:18:27,070 --> 00:18:33,220
initially so finally that leaves us with

00:18:30,850 --> 00:18:35,230
with a random token assignment this is

00:18:33,220 --> 00:18:39,550
the one where we we calculate T random

00:18:35,230 --> 00:18:41,790
tokens for each node T new T new random

00:18:39,550 --> 00:18:43,780
tokens for each joining node after that

00:18:41,790 --> 00:18:45,670
and this means that the number of

00:18:43,780 --> 00:18:48,100
partitions that we have scales linearly

00:18:45,670 --> 00:18:49,780
with the number of hosts which is that's

00:18:48,100 --> 00:18:51,640
not good but it's it's okay it's better

00:18:49,780 --> 00:18:54,550
with them better than scaling with the

00:18:51,640 --> 00:18:56,650
size of the data and it means that the

00:18:54,550 --> 00:19:00,220
partitions themselves will grow as more

00:18:56,650 --> 00:19:01,840
data is added but they'll decrease as we

00:19:00,220 --> 00:19:04,840
add more hosts and that and that's a

00:19:01,840 --> 00:19:06,550
good property to have to have so all in

00:19:04,840 --> 00:19:08,140
all that's really the right balance for

00:19:06,550 --> 00:19:10,660
Cassandra is this random token

00:19:08,140 --> 00:19:14,590
assignment and that's that's what we've

00:19:10,660 --> 00:19:15,850
implemented so on a little more of a

00:19:14,590 --> 00:19:17,770
practical note how many people here

00:19:15,850 --> 00:19:20,770
actually use Cassandra or have used

00:19:17,770 --> 00:19:22,060
Cassandra okay so quite a few so this

00:19:20,770 --> 00:19:23,180
would be more or less what you would

00:19:22,060 --> 00:19:25,030
need to do if you wanted to use

00:19:23,180 --> 00:19:28,090
this is kind of like the more practical

00:19:25,030 --> 00:19:32,180
side of things

00:19:28,090 --> 00:19:35,920
initial token is still respected so you

00:19:32,180 --> 00:19:39,160
can add your tokens manually

00:19:35,920 --> 00:19:41,210
comma-delimited if you if you want to

00:19:39,160 --> 00:19:45,200
this file is going to be pretty hard to

00:19:41,210 --> 00:19:47,600
manage with 256 128 bit you know 2 or 56

00:19:45,200 --> 00:19:50,890
being the default value of T that we use

00:19:47,600 --> 00:19:54,110
in Cassandra you know to separate 256

00:19:50,890 --> 00:19:56,030
128 bit tokens I mean we made you to

00:19:54,110 --> 00:19:58,640
calculate tokens in the past so that

00:19:56,030 --> 00:20:01,340
would be even more of a chore now so I

00:19:58,640 --> 00:20:03,080
don't recommend doing that we support it

00:20:01,340 --> 00:20:05,000
because technically we still have the

00:20:03,080 --> 00:20:06,740
the byte ordered partitioner which you

00:20:05,000 --> 00:20:09,110
shouldn't be using but if you did you

00:20:06,740 --> 00:20:12,770
would need some way of setting these and

00:20:09,110 --> 00:20:13,970
I probably more important is to make you

00:20:12,770 --> 00:20:17,600
know since we're generating these these

00:20:13,970 --> 00:20:19,490
tokens randomly the the share the per

00:20:17,600 --> 00:20:21,500
node share is really ends up balancing

00:20:19,490 --> 00:20:24,350
out quite closely but it's not perfect

00:20:21,500 --> 00:20:26,030
so there's the use case of say for

00:20:24,350 --> 00:20:27,710
example benchmarks where you'd want the

00:20:26,030 --> 00:20:30,160
per node share to be you know really

00:20:27,710 --> 00:20:33,400
really precise to get good results

00:20:30,160 --> 00:20:35,630
that's that's an option for that

00:20:33,400 --> 00:20:39,170
more commonly though you would just set

00:20:35,630 --> 00:20:40,760
this number num tokens 2 to 256 that's

00:20:39,170 --> 00:20:44,510
the default or something else if you

00:20:40,760 --> 00:20:46,970
know what you're doing and off you go a

00:20:44,510 --> 00:20:50,480
lot of things like small things change

00:20:46,970 --> 00:20:52,130
with this with this you know having

00:20:50,480 --> 00:20:54,830
multiple tokens per node and having a

00:20:52,130 --> 00:20:56,930
large number like 256 so we used to

00:20:54,830 --> 00:20:58,700
treat token almost as like the unique

00:20:56,930 --> 00:21:00,800
node identifier because they were

00:20:58,700 --> 00:21:02,840
one-to-one because they were unique I

00:21:00,800 --> 00:21:04,160
mean even an IP address you could change

00:21:02,840 --> 00:21:07,220
that you know you could change it

00:21:04,160 --> 00:21:08,960
remember your host the token was always

00:21:07,220 --> 00:21:10,640
kind of the unique way of identifying a

00:21:08,960 --> 00:21:13,130
node and so we sort of you know

00:21:10,640 --> 00:21:15,410
displayed it prominently but this is the

00:21:13,130 --> 00:21:17,000
the output of node tool info and if you

00:21:15,410 --> 00:21:18,410
use Cassandra this is a pretty commonly

00:21:17,000 --> 00:21:20,900
around command you know to kind of get a

00:21:18,410 --> 00:21:23,510
picture of you know sort of summary view

00:21:20,900 --> 00:21:25,970
of an individual node but you can

00:21:23,510 --> 00:21:27,710
imagine trying to display 256 tokens you

00:21:25,970 --> 00:21:28,930
know just get scroll shock you know it's

00:21:27,710 --> 00:21:32,570
not going to be very useful

00:21:28,930 --> 00:21:34,250
so we suppress that output now you know

00:21:32,570 --> 00:21:35,200
if you have more than one token and we

00:21:34,250 --> 00:21:38,500
make you throw another

00:21:35,200 --> 00:21:40,570
switch another example is no tool ring

00:21:38,500 --> 00:21:43,360
which is a pretty good pretty common way

00:21:40,570 --> 00:21:45,899
of that people would look at the the

00:21:43,360 --> 00:21:48,039
sort of status of the cluster as a whole

00:21:45,899 --> 00:21:50,130
but that's not terribly useful now

00:21:48,039 --> 00:21:53,200
because it's now expanded by you know

00:21:50,130 --> 00:21:55,330
255 times the number of nodes you have

00:21:53,200 --> 00:21:57,309
and so again it's scroll shocked and

00:21:55,330 --> 00:21:59,289
it's not a very good way of just kind of

00:21:57,309 --> 00:22:01,899
you know gathering the summary as a

00:21:59,289 --> 00:22:05,350
whole so we've introduced this no dual

00:22:01,899 --> 00:22:07,000
status command to kind of be the

00:22:05,350 --> 00:22:08,710
replacement you know ring was always

00:22:07,000 --> 00:22:10,990
meant to show the topology but it kind

00:22:08,710 --> 00:22:14,139
of double duty does like a way of

00:22:10,990 --> 00:22:16,389
looking at the whole cluster you'll see

00:22:14,139 --> 00:22:19,330
that no tool status shows this this host

00:22:16,389 --> 00:22:21,580
ID because we don't have you know

00:22:19,330 --> 00:22:24,360
anything unique and singular to a node

00:22:21,580 --> 00:22:27,130
anymore we had to create this this UUID

00:22:24,360 --> 00:22:29,169
field and we display that now instead

00:22:27,130 --> 00:22:31,990
I'll probably start to become more of a

00:22:29,169 --> 00:22:33,669
prominent feature and you can see things

00:22:31,990 --> 00:22:36,970
like the number of tokens he chose to

00:22:33,669 --> 00:22:41,740
have this is probably be the more common

00:22:36,970 --> 00:22:43,090
all in one shot command so if you have

00:22:41,740 --> 00:22:45,190
an existing cluster you want to migrate

00:22:43,090 --> 00:22:46,750
it that's possible the the way a

00:22:45,190 --> 00:22:49,120
migration would work is you know let's

00:22:46,750 --> 00:22:50,500
say you have you know three nodes you

00:22:49,120 --> 00:22:52,899
know you're coming off of Cassandra one

00:22:50,500 --> 00:22:55,510
one on the one two the first person that

00:22:52,899 --> 00:22:56,860
supports virtual nodes and you want to

00:22:55,510 --> 00:22:59,470
upgrade so you would just go into the

00:22:56,860 --> 00:23:02,409
the Cassandra demo file and you would

00:22:59,470 --> 00:23:04,240
uncomment that num tokens you would set

00:23:02,409 --> 00:23:07,419
it to something you know non something

00:23:04,240 --> 00:23:09,419
greater than one much like initial token

00:23:07,419 --> 00:23:11,590
this is like a one-shot deal once you've

00:23:09,419 --> 00:23:14,529
you know entered a value here and

00:23:11,590 --> 00:23:16,779
restarted it you can't change it later

00:23:14,529 --> 00:23:18,519
on it will have no effect but what

00:23:16,779 --> 00:23:21,700
that'll do is that'll take your existing

00:23:18,519 --> 00:23:24,100
partitions and it will split them num

00:23:21,700 --> 00:23:26,190
tokens way so you know if you've used a

00:23:24,100 --> 00:23:30,820
default it'll split each existing range

00:23:26,190 --> 00:23:32,620
256 ways but it's still 256 partitions

00:23:30,820 --> 00:23:36,130
that are all contiguous and within the

00:23:32,620 --> 00:23:37,889
range that you had before and so there's

00:23:36,130 --> 00:23:39,820
some value there you know you get that

00:23:37,889 --> 00:23:43,990
concurrency that

00:23:39,820 --> 00:23:45,190
that I promised before and you get to

00:23:43,990 --> 00:23:47,650
know the smaller more incremental

00:23:45,190 --> 00:23:49,600
streaming operations for repairs and

00:23:47,650 --> 00:23:51,610
bootstraps and eventually over time the

00:23:49,600 --> 00:23:53,920
placement of those you know as you add

00:23:51,610 --> 00:23:56,890
and remove notes will become somewhat

00:23:53,920 --> 00:23:59,050
randomized but if you don't want to wait

00:23:56,890 --> 00:24:03,760
for that there is a new operation called

00:23:59,050 --> 00:24:05,980
shuffle and what shuffle does is it just

00:24:03,760 --> 00:24:11,620
randomizes the placement of those now

00:24:05,980 --> 00:24:14,320
split ranges and it's kind of hairy and

00:24:11,620 --> 00:24:15,640
not very well tested so if you do if you

00:24:14,320 --> 00:24:18,480
do you shuffle make sure this is

00:24:15,640 --> 00:24:20,950
something that you've tried out in your

00:24:18,480 --> 00:24:23,860
you know your staging environments or

00:24:20,950 --> 00:24:25,720
development environments first I don't

00:24:23,860 --> 00:24:26,860
want to like destroy confidence in it

00:24:25,720 --> 00:24:28,360
and make everyone run away not wanting

00:24:26,860 --> 00:24:32,830
to use it but this is this is one of

00:24:28,360 --> 00:24:34,750
those unique it's filled with all sorts

00:24:32,830 --> 00:24:36,880
of corner cases and network races and

00:24:34,750 --> 00:24:38,970
it's just kind of a hard thing to do and

00:24:36,880 --> 00:24:41,830
this is going to be a piece of code that

00:24:38,970 --> 00:24:43,150
you know that people who migrate will

00:24:41,830 --> 00:24:45,520
use exactly once

00:24:43,150 --> 00:24:49,630
and anybody who uses a new cluster will

00:24:45,520 --> 00:24:51,310
never use so I don't have a high degree

00:24:49,630 --> 00:24:54,010
of confidence it'll ever be really well

00:24:51,310 --> 00:24:56,530
tested so you know just make sure you

00:24:54,010 --> 00:24:58,450
test it before you use it the way it

00:24:56,530 --> 00:25:00,910
works is is you know it's going to

00:24:58,450 --> 00:25:03,430
calculate a new mapping and it will

00:25:00,910 --> 00:25:05,140
queue on each ring on each node the

00:25:03,430 --> 00:25:06,850
range is that it should transfer and

00:25:05,140 --> 00:25:09,370
each rate each node will transfer the

00:25:06,850 --> 00:25:12,640
ranges to itself each each node is able

00:25:09,370 --> 00:25:14,800
to steal a range so we calculate which

00:25:12,640 --> 00:25:17,460
node which ranges each node should steal

00:25:14,800 --> 00:25:20,560
and we queue them up on a system table

00:25:17,460 --> 00:25:22,540
and then we tell it to go and we give it

00:25:20,560 --> 00:25:23,860
a few you know breaks so that you know

00:25:22,540 --> 00:25:25,690
one node doesn't get too far ahead of

00:25:23,860 --> 00:25:27,610
the other and then if everything goes

00:25:25,690 --> 00:25:30,550
well you've completely randomized your

00:25:27,610 --> 00:25:32,290
placement but pay attention to the logs

00:25:30,550 --> 00:25:33,880
and keep an eye on it you can always

00:25:32,290 --> 00:25:35,950
pause it or stop it if it was starting

00:25:33,880 --> 00:25:37,830
to do something wrong and and if it does

00:25:35,950 --> 00:25:41,980
it's probably not irrecoverable

00:25:37,830 --> 00:25:43,300
hopefully this is how that works there's

00:25:41,980 --> 00:25:45,790
a there's a new command called shuffle

00:25:43,300 --> 00:25:47,770
and you know you can create a new

00:25:45,790 --> 00:25:49,240
shuffle operation and start and stop it

00:25:47,770 --> 00:25:52,660
you can list the moves that are going to

00:25:49,240 --> 00:25:54,760
taking place you can limit it to only to

00:25:52,660 --> 00:25:58,600
only do a single DC at a time that sort

00:25:54,760 --> 00:26:00,430
of thing um and then there's the the

00:25:58,600 --> 00:26:02,020
performance aspect because you know we

00:26:00,430 --> 00:26:04,480
promise greater performance so I was

00:26:02,020 --> 00:26:06,670
throwing a couple of slides here you

00:26:04,480 --> 00:26:10,450
know this is like a 17 node cluster with

00:26:06,670 --> 00:26:12,190
a half a billion rows and this is the

00:26:10,450 --> 00:26:15,940
removed node operation which would cause

00:26:12,190 --> 00:26:18,580
you know you know basically all of the

00:26:15,940 --> 00:26:21,490
data to be moved around the cluster away

00:26:18,580 --> 00:26:22,960
from a failed mode and a bootstrap which

00:26:21,490 --> 00:26:24,760
does the exact opposite of that

00:26:22,960 --> 00:26:26,200
you know adds a new node and streams all

00:26:24,760 --> 00:26:30,070
of that all of the data that belongs to

00:26:26,200 --> 00:26:32,230
it and thus the the summary of that is

00:26:30,070 --> 00:26:34,060
it's it's a bit more than three times

00:26:32,230 --> 00:26:35,950
faster and the reason we show this

00:26:34,060 --> 00:26:37,360
remove node and bootstrap is because

00:26:35,950 --> 00:26:39,460
that's kind of the worst case scenario

00:26:37,360 --> 00:26:41,620
if you had a failed mode and you need to

00:26:39,460 --> 00:26:44,920
get your your your fault tolerance back

00:26:41,620 --> 00:26:47,560
up that's what that extra concurrency

00:26:44,920 --> 00:26:50,770
buys you is the full replacement of a

00:26:47,560 --> 00:26:55,630
node about you know 3.2 times faster

00:26:50,770 --> 00:26:57,700
which is pretty good and that is all I

00:26:55,630 --> 00:27:03,760
have other than any questions if anyone

00:26:57,700 --> 00:27:06,400
has it back what can you say about the

00:27:03,760 --> 00:27:09,070
compact ation level and strategy for a

00:27:06,400 --> 00:27:11,890
better storage usage because we have a

00:27:09,070 --> 00:27:15,070
problem when we use some kind of compact

00:27:11,890 --> 00:27:18,430
ation first it's double the size then

00:27:15,070 --> 00:27:21,610
it's shrink back so what what do you

00:27:18,430 --> 00:27:27,190
recommend for for the better stores

00:27:21,610 --> 00:27:31,120
users not your fault I got my ears are

00:27:27,190 --> 00:27:33,660
all plugged ok ok now I'm gonna have

00:27:31,120 --> 00:27:37,660
them repeated in a heavy Scottish accent

00:27:33,660 --> 00:27:42,760
let me repeat that what do you recommend

00:27:37,660 --> 00:27:45,550
for a compact ation strategy today I

00:27:42,760 --> 00:27:50,110
have a problem when we start compacting

00:27:45,550 --> 00:27:52,750
the the the SS tables first it grows the

00:27:50,110 --> 00:27:58,240
double of the SS tables then it shrink

00:27:52,750 --> 00:28:01,360
back to the right size so what what kind

00:27:58,240 --> 00:28:04,030
of compact ation are you guys using in

00:28:01,360 --> 00:28:05,559
what do you recommend that's clean so

00:28:04,030 --> 00:28:09,970
the question was which compaction

00:28:05,559 --> 00:28:11,200
strategy the the behavior you described

00:28:09,970 --> 00:28:13,750
sounds like you're talking about the

00:28:11,200 --> 00:28:15,480
size tiered compaction exactly which

00:28:13,750 --> 00:28:18,070
will need you know the working space

00:28:15,480 --> 00:28:19,900
equivalent to the size of the files it's

00:28:18,070 --> 00:28:26,890
compacting in order to be able to do

00:28:19,900 --> 00:28:29,350
that safely atomically yes I mean that's

00:28:26,890 --> 00:28:31,030
that's the size tiered size to your

00:28:29,350 --> 00:28:33,340
compaction is probably the best sort of

00:28:31,030 --> 00:28:34,870
general use compaction strategy and

00:28:33,340 --> 00:28:36,400
that's just a property of that of that

00:28:34,870 --> 00:28:38,140
strategy is that you know that the

00:28:36,400 --> 00:28:40,330
tables that it's compacting that it you

00:28:38,140 --> 00:28:44,230
know it needs the working space you know

00:28:40,330 --> 00:28:46,059
equal to the to them that's you know

00:28:44,230 --> 00:28:48,610
that's something people just usually you

00:28:46,059 --> 00:28:50,830
know they they allow for enough space to

00:28:48,610 --> 00:28:52,870
make that work and you can use the level

00:28:50,830 --> 00:28:54,429
of compaction strategy which would be

00:28:52,870 --> 00:28:57,010
you know there would be less less of

00:28:54,429 --> 00:28:59,169
that space in use because the tables are

00:28:57,010 --> 00:29:01,330
more fixed or predictable in size but

00:28:59,169 --> 00:29:02,950
the level of compaction strategy you

00:29:01,330 --> 00:29:05,919
know it kind of doubles the amount of

00:29:02,950 --> 00:29:08,260
right I oh it's it's designed to sort of

00:29:05,919 --> 00:29:10,150
amortize the read of the merge pour

00:29:08,260 --> 00:29:12,760
portion of the log structure merge over

00:29:10,150 --> 00:29:17,530
time and makes writes faster at the

00:29:12,760 --> 00:29:19,150
expense of right IO so I mean that's not

00:29:17,530 --> 00:29:26,530
a necessarily like a trigger I would

00:29:19,150 --> 00:29:28,980
pull you know lean lightly thank you any

00:29:26,530 --> 00:29:28,980
other questions

00:29:30,880 --> 00:29:37,340
so what a shoe on the cheetah

00:29:34,670 --> 00:29:41,240
Cassandra Jeter covers or what issues

00:29:37,340 --> 00:29:43,160
cover the topic for tracking previous

00:29:41,240 --> 00:29:47,590
correspondence ongoing correspondence

00:29:43,160 --> 00:29:49,970
between the development team and and

00:29:47,590 --> 00:29:52,280
obviously I was the conversation going

00:29:49,970 --> 00:29:54,430
particularly for those topic of the

00:29:52,280 --> 00:29:59,300
development on Cassandra

00:29:54,430 --> 00:30:01,490
which which Jerry issues covered this I

00:29:59,300 --> 00:30:02,870
don't know I could look him up for you

00:30:01,490 --> 00:30:04,610
but it was there was there was many of

00:30:02,870 --> 00:30:06,650
them it wasn't what it was there was

00:30:04,610 --> 00:30:09,050
probably one master with a bunch of sub

00:30:06,650 --> 00:30:12,200
tickets underneath of it but yeah it was

00:30:09,050 --> 00:30:13,880
many there's there was a there's also a

00:30:12,200 --> 00:30:14,930
good mailing list I don't have the link

00:30:13,880 --> 00:30:16,190
for that and you good me anything was

00:30:14,930 --> 00:30:18,290
thread where we kind of asked all this

00:30:16,190 --> 00:30:21,550
out might be a better summary than that

00:30:18,290 --> 00:30:21,550

YouTube URL: https://www.youtube.com/watch?v=8ZbZOKJvdCQ


