Title: #ACEU19: Nick Pentreath – Reinforcement Learning for Recommendations
Publication date: 2019-10-24
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	More: https://aceu19.apachecon.com/session/reinforcement-learning-recommendations

Recently, reinforcement learning (RL) and especially deep RL has achieved significant success in a variety of domains including board games such as Go, video games such as classic Atari games and online games such as DoTa 2, Capture-the-Flag, StarCraft II and others.

Simpler versions of RL techniques, notably bandit algorithms, have long been applied to personalization and recommendation systems. In this talk, I explore the history of reinforcement learning applied to personalization and recommendations as well as the state-of-the-art advances in deep RL techniques in this domain.
Captions: 
	00:00:05,880 --> 00:00:11,200
yeah good morning everybody just gone

00:00:08,740 --> 00:00:13,570
afternoon thanks very much

00:00:11,200 --> 00:00:15,330
to join my name is Nick country for

00:00:13,570 --> 00:00:17,160
principal engineer

00:00:15,330 --> 00:00:18,660
we talk about reinforcement learning as

00:00:17,160 --> 00:00:21,410
applied to recommendation and

00:00:18,660 --> 00:00:25,320
personalization so a little bit about me

00:00:21,410 --> 00:00:27,029
I'm ml Nick on Twitter and github I work

00:00:25,320 --> 00:00:29,400
at IBM in a team called the Center for

00:00:27,029 --> 00:00:31,050
open source data and AI technologies or

00:00:29,400 --> 00:00:34,110
Cote and I'll talk a little bit about

00:00:31,050 --> 00:00:35,670
that in a moment I focus on machine

00:00:34,110 --> 00:00:38,640
learning data science and AI

00:00:35,670 --> 00:00:40,890
applications I've got a long history in

00:00:38,640 --> 00:00:43,080
the Apache spark project where mikuma 10

00:00:40,890 --> 00:00:46,230
PMC member or for the book to do with

00:00:43,080 --> 00:00:49,470
spark and these days I'm involved in a

00:00:46,230 --> 00:00:50,970
lot of deep learning applications and

00:00:49,470 --> 00:00:54,809
recommender systems is something that's

00:00:50,970 --> 00:00:57,000
close to my heart I spent about three

00:00:54,809 --> 00:00:58,979
years building a start-up before IBM

00:00:57,000 --> 00:01:01,640
focused on recommendation models and

00:00:58,979 --> 00:01:03,420
before that in fact applied

00:01:01,640 --> 00:01:04,290
reinforcement learning techniques and

00:01:03,420 --> 00:01:07,290
some of the stuff that we'll talk about

00:01:04,290 --> 00:01:09,210
today to online advertising so this is

00:01:07,290 --> 00:01:12,390
the stuff that I've been working on in

00:01:09,210 --> 00:01:14,490
the past for a while so before we get

00:01:12,390 --> 00:01:16,590
going just a little bit about code a

00:01:14,490 --> 00:01:19,710
Center for open source data and AR

00:01:16,590 --> 00:01:21,480
technologies I joined this team at IBM a

00:01:19,710 --> 00:01:24,960
few years ago when it was called the

00:01:21,480 --> 00:01:27,750
spark Technology Center and the team is

00:01:24,960 --> 00:01:30,450
focused on at that point expanding

00:01:27,750 --> 00:01:32,430
Apache spark into the enterprise and and

00:01:30,450 --> 00:01:34,070
focusing on the communities and data

00:01:32,430 --> 00:01:36,840
science communities and tools around

00:01:34,070 --> 00:01:38,700
Apache spark since then the team has

00:01:36,840 --> 00:01:40,560
expanded its focus to encompass the

00:01:38,700 --> 00:01:42,360
end-to-end Enterprise AI life cycle

00:01:40,560 --> 00:01:44,460
hearts and open source technology

00:01:42,360 --> 00:01:46,530
service encompasses the Python data

00:01:44,460 --> 00:01:49,470
science stack more recently deep

00:01:46,530 --> 00:01:51,650
learning models and frameworks of course

00:01:49,470 --> 00:01:53,940
Apache spark is the living part of that

00:01:51,650 --> 00:01:55,650
some projects that I work on the model

00:01:53,940 --> 00:01:57,659
acid exchange and data asset exchange

00:01:55,650 --> 00:01:59,580
are around open curation of deep

00:01:57,659 --> 00:02:02,400
learning models and open datasets and

00:01:59,580 --> 00:02:04,890
more recently a lot of focus around

00:02:02,400 --> 00:02:06,979
model serving of in terms of open

00:02:04,890 --> 00:02:09,840
standards and frameworks as well as

00:02:06,979 --> 00:02:12,870
tackling fairness and trusted and in AI

00:02:09,840 --> 00:02:15,450
so explain ability have a Sarah

00:02:12,870 --> 00:02:17,069
robustness and fairness so we work on

00:02:15,450 --> 00:02:21,019
all of these different projects and we

00:02:17,069 --> 00:02:23,640
were on over 30 open source developers

00:02:21,019 --> 00:02:26,640
so today we'll start with a little

00:02:23,640 --> 00:02:28,440
overview of recommender systems and then

00:02:26,640 --> 00:02:28,890
do the same thing for reinforcement

00:02:28,440 --> 00:02:31,080
learning

00:02:28,890 --> 00:02:34,290
just at a high level and then look at

00:02:31,080 --> 00:02:36,500
how they have been used combined and

00:02:34,290 --> 00:02:39,630
then also along the way talk about

00:02:36,500 --> 00:02:41,220
advances in deep learning and the

00:02:39,630 --> 00:02:42,600
application of deep learning to

00:02:41,220 --> 00:02:46,560
reinforcement learning and how they've

00:02:42,600 --> 00:02:49,800
been combined and then wrap up so the

00:02:46,560 --> 00:02:52,620
each of these fields is in itself a huge

00:02:49,800 --> 00:02:54,810
area so we won't be able to go into too

00:02:52,620 --> 00:02:59,850
much detail about any to anyone and

00:02:54,810 --> 00:03:02,880
we'll keep it fairly high-level so

00:02:59,850 --> 00:03:04,290
recommender systems you know why are

00:03:02,880 --> 00:03:06,930
they important well you know what do we

00:03:04,290 --> 00:03:10,020
really care care about them for our

00:03:06,930 --> 00:03:11,480
recommenders are one of the earliest and

00:03:10,020 --> 00:03:13,140
probably most commercially successful

00:03:11,480 --> 00:03:15,209
applications of machine learning

00:03:13,140 --> 00:03:17,700
technology everywhere you look today

00:03:15,209 --> 00:03:21,270
you're interacting with a recommendation

00:03:17,700 --> 00:03:23,250
system or personalization system they're

00:03:21,270 --> 00:03:24,989
telling you what movies to watch what

00:03:23,250 --> 00:03:27,180
car to buy what games to play what

00:03:24,989 --> 00:03:29,370
websites deserve who to connect with on

00:03:27,180 --> 00:03:33,269
social media what clothes to buy where

00:03:29,370 --> 00:03:35,610
to go on holiday what jobs have and even

00:03:33,269 --> 00:03:38,640
in the medical field and dating so

00:03:35,610 --> 00:03:40,530
pretty much every aspect of modern and

00:03:38,640 --> 00:03:42,299
in particular online and connected life

00:03:40,530 --> 00:03:46,709
is touched by recommendations and

00:03:42,299 --> 00:03:50,190
personalizations according to most stats

00:03:46,709 --> 00:03:52,110
up to 30 or 35 percent of Amazon's

00:03:50,190 --> 00:03:53,190
bottom line comes from ultimately the

00:03:52,110 --> 00:03:56,010
recommendation and personalization

00:03:53,190 --> 00:03:59,010
system for Netflix that is rumored to be

00:03:56,010 --> 00:04:01,860
closer to 70 or 80 percent so anything

00:03:59,010 --> 00:04:04,470
where tastes plays a role we can

00:04:01,860 --> 00:04:06,110
dramatically increase revenue and a big

00:04:04,470 --> 00:04:12,840
portion of the revenue comes from

00:04:06,110 --> 00:04:14,660
recommendation system so the entities

00:04:12,840 --> 00:04:18,359
that are involved in our system are

00:04:14,660 --> 00:04:20,519
typically users and items our users

00:04:18,359 --> 00:04:22,080
obviously there can the ones coming into

00:04:20,519 --> 00:04:23,669
the system and interacting with it

00:04:22,080 --> 00:04:26,100
they're effectively creating the data

00:04:23,669 --> 00:04:29,400
that we're going to use and items can be

00:04:26,100 --> 00:04:32,820
anything as we saw before holidays books

00:04:29,400 --> 00:04:34,020
movies in this case we'll just pretend

00:04:32,820 --> 00:04:36,750
that they're going to be your videos

00:04:34,020 --> 00:04:39,240
online so we have all this metadata that

00:04:36,750 --> 00:04:41,200
we want to use about users and items to

00:04:39,240 --> 00:04:44,650
make better recommendations we

00:04:41,200 --> 00:04:48,100
of names demographic location

00:04:44,650 --> 00:04:49,810
demographic details for four users we

00:04:48,100 --> 00:04:51,520
have geolocation perhaps we have

00:04:49,810 --> 00:04:54,040
activity data when were there lost

00:04:51,520 --> 00:04:55,300
online when when was their profile lost

00:04:54,040 --> 00:04:57,760
updated we know that were there lost

00:04:55,300 --> 00:05:00,730
active and for items we similarly have a

00:04:57,760 --> 00:05:04,330
lot of metadata in and typically quite

00:05:00,730 --> 00:05:06,400
rich metadata around what is you know

00:05:04,330 --> 00:05:10,630
what is contained in that item so for

00:05:06,400 --> 00:05:13,030
video so it may be the actual video

00:05:10,630 --> 00:05:15,460
itself the frames the written or the

00:05:13,030 --> 00:05:18,340
images in the video effectively can be

00:05:15,460 --> 00:05:20,170
categories tags descriptions can also be

00:05:18,340 --> 00:05:21,580
activity data we know does at last play

00:05:20,170 --> 00:05:24,430
it how many likes has had had how many

00:05:21,580 --> 00:05:26,110
players is it had how popular is it we

00:05:24,430 --> 00:05:27,850
use the author what is the channel

00:05:26,110 --> 00:05:29,910
that's coming from all of these are

00:05:27,850 --> 00:05:32,620
important features that we want to use

00:05:29,910 --> 00:05:34,180
and users are coming into our system and

00:05:32,620 --> 00:05:38,050
interacting with the items in that

00:05:34,180 --> 00:05:41,140
system and every time they interact they

00:05:38,050 --> 00:05:42,910
are creating event data so most of the

00:05:41,140 --> 00:05:45,610
time in recommenders we hear about

00:05:42,910 --> 00:05:47,650
explicit data you know the canonical

00:05:45,610 --> 00:05:51,550
models and the canonical data sets and

00:05:47,650 --> 00:05:54,250
examples always talking about ratings so

00:05:51,550 --> 00:05:56,680
how many stars do they use a given

00:05:54,250 --> 00:05:58,390
rating in the movie lens data set but in

00:05:56,680 --> 00:06:01,390
reality most of the data comes from

00:05:58,390 --> 00:06:03,790
implicit data so ratings reviews our

00:06:01,390 --> 00:06:05,980
explicit statement of preference I give

00:06:03,790 --> 00:06:09,070
this a 5 I give us a 1 therefore I don't

00:06:05,980 --> 00:06:11,860
like it but implicit debt is all around

00:06:09,070 --> 00:06:13,060
user behavior where they're not actually

00:06:11,860 --> 00:06:15,730
telling you what they like and dislike

00:06:13,060 --> 00:06:19,060
they're just showing via their behavior

00:06:15,730 --> 00:06:21,490
so online this is page view you know

00:06:19,060 --> 00:06:23,440
coming to visit a website and looking at

00:06:21,490 --> 00:06:25,420
a product page may be a very light

00:06:23,440 --> 00:06:26,920
indicator preference it may not

00:06:25,420 --> 00:06:29,350
necessarily mean that the user likes

00:06:26,920 --> 00:06:31,540
that the product but it is a step

00:06:29,350 --> 00:06:34,360
towards showing that they like it if

00:06:31,540 --> 00:06:36,070
they're then at that product to the cart

00:06:34,360 --> 00:06:38,580
that's a much stronger indication of

00:06:36,070 --> 00:06:40,810
preference and finally if they buy it

00:06:38,580 --> 00:06:42,880
clearly that's the strongest indication

00:06:40,810 --> 00:06:44,620
of preference but even then we can't

00:06:42,880 --> 00:06:47,620
really say that that is an explicit

00:06:44,620 --> 00:06:49,630
indicator because perhaps someone what

00:06:47,620 --> 00:06:51,670
that product and actually didn't like it

00:06:49,630 --> 00:06:53,200
as much and and instead of leaving a

00:06:51,670 --> 00:06:54,400
negative review just threw it away we

00:06:53,200 --> 00:06:56,650
would never know that

00:06:54,400 --> 00:06:58,150
so again an implicit you know a

00:06:56,650 --> 00:06:59,290
purchases and is a strong implicit

00:06:58,150 --> 00:07:02,669
indicator but it doesn't necessarily

00:06:59,290 --> 00:07:05,500
tell us that a user likes that product

00:07:02,669 --> 00:07:09,370
we have all kinds of other information

00:07:05,500 --> 00:07:13,020
most of it most of implicit coming from

00:07:09,370 --> 00:07:15,490
social media as search queries etc and

00:07:13,020 --> 00:07:17,110
every time the user comes and interacts

00:07:15,490 --> 00:07:20,169
with the system and creates this event

00:07:17,110 --> 00:07:22,930
it's done in a certain context so this

00:07:20,169 --> 00:07:25,030
is a time of day to your location what

00:07:22,930 --> 00:07:27,370
kind of device are they using and we

00:07:25,030 --> 00:07:30,100
want to use this context data to further

00:07:27,370 --> 00:07:32,919
personalize and improve recommendation

00:07:30,100 --> 00:07:35,530
systems if someone is watching videos

00:07:32,919 --> 00:07:38,080
for example on their mobile on the way

00:07:35,530 --> 00:07:40,060
to work in on the train or the bus they

00:07:38,080 --> 00:07:41,830
may be open to different types of

00:07:40,060 --> 00:07:45,100
content then if they're at home watching

00:07:41,830 --> 00:07:47,320
Netflix on the TV alright so the

00:07:45,100 --> 00:07:50,020
contexts where they are what they're

00:07:47,320 --> 00:07:53,680
doing what device is quite important in

00:07:50,020 --> 00:07:55,660
deciding what you want to show them now

00:07:53,680 --> 00:07:58,360
prediction in machine learning is

00:07:55,660 --> 00:08:01,030
typically a fairly simple thing so if

00:07:58,360 --> 00:08:02,440
you want to you know classify you you

00:08:01,030 --> 00:08:06,250
take an image and you output a

00:08:02,440 --> 00:08:07,690
prediction of a cat or you take a piece

00:08:06,250 --> 00:08:09,820
of text in the output a pic one piece of

00:08:07,690 --> 00:08:12,789
sentiment you're predicting one training

00:08:09,820 --> 00:08:15,130
example at a time but prediction for

00:08:12,789 --> 00:08:16,720
recommendations is a lot more complex so

00:08:15,130 --> 00:08:19,539
we need to think about actually ranking

00:08:16,720 --> 00:08:23,199
items our goal really is to present the

00:08:19,539 --> 00:08:25,810
user at each interaction with a ranked

00:08:23,199 --> 00:08:27,370
list of items that they want to

00:08:25,810 --> 00:08:29,560
hopefully interact with so we want to

00:08:27,370 --> 00:08:31,539
rank them by some score and that score

00:08:29,560 --> 00:08:33,729
is an indication of how likely our model

00:08:31,539 --> 00:08:36,789
thinks they are to interact with that

00:08:33,729 --> 00:08:39,250
item so the prediction as always is all

00:08:36,789 --> 00:08:42,339
about ranking but the challenge that we

00:08:39,250 --> 00:08:45,370
have is that most machine learning

00:08:42,339 --> 00:08:47,320
models are optimized against one metric

00:08:45,370 --> 00:08:50,170
and that's typically some some form of

00:08:47,320 --> 00:08:51,910
loss and even the same is true for

00:08:50,170 --> 00:08:54,250
recommendation systems despite the fact

00:08:51,910 --> 00:08:56,560
that we are ranking and we know we need

00:08:54,250 --> 00:08:59,320
to rank all items for each prediction we

00:08:56,560 --> 00:09:02,680
are still ranking based on let's say an

00:08:59,320 --> 00:09:04,990
estimated rating or a estimated

00:09:02,680 --> 00:09:06,520
click-through rate but in reality in our

00:09:04,990 --> 00:09:07,680
real world system and in a real word

00:09:06,520 --> 00:09:09,770
business

00:09:07,680 --> 00:09:12,540
you have a lot of different objectives

00:09:09,770 --> 00:09:17,190
so in a business most of the time we

00:09:12,540 --> 00:09:18,450
want to you know maximize profit and you

00:09:17,190 --> 00:09:20,970
know one could argue perhaps that's the

00:09:18,450 --> 00:09:23,070
only goal but we need to take into

00:09:20,970 --> 00:09:25,020
account short-term and long-term factors

00:09:23,070 --> 00:09:27,660
so if we just chase short-term profit

00:09:25,020 --> 00:09:29,790
maximization at the at the risk of

00:09:27,660 --> 00:09:32,220
investing in for the long term you know

00:09:29,790 --> 00:09:33,810
we may run into challenges we also need

00:09:32,220 --> 00:09:36,630
to take into account regulatory factors

00:09:33,810 --> 00:09:40,950
and societal factors these are external

00:09:36,630 --> 00:09:43,020
to the business so just chase things for

00:09:40,950 --> 00:09:45,120
example click-through rates may not be

00:09:43,020 --> 00:09:47,550
the you know the best thing so even

00:09:45,120 --> 00:09:49,560
though our algorithm can only focus on

00:09:47,550 --> 00:09:51,029
one particular metric in reality the

00:09:49,560 --> 00:09:53,339
business is a much more complex and it

00:09:51,029 --> 00:09:56,970
needs to also take into account a longer

00:09:53,339 --> 00:09:59,190
timeframe so if we want to uplift

00:09:56,970 --> 00:10:00,570
revenue or increase user engagement that

00:09:59,190 --> 00:10:05,250
may not be the same thing as driving

00:10:00,570 --> 00:10:06,690
clickbait in the short term one of the

00:10:05,250 --> 00:10:08,459
other challenges we need to think about

00:10:06,690 --> 00:10:10,470
when it comes to recommenders is this

00:10:08,459 --> 00:10:12,990
call start problem so most

00:10:10,470 --> 00:10:14,160
recommendation models are operating

00:10:12,990 --> 00:10:16,260
under the assumption that we have all

00:10:14,160 --> 00:10:18,630
the data we have historical data about

00:10:16,260 --> 00:10:21,120
what users have viewed what items what

00:10:18,630 --> 00:10:23,459
they're purchased and so on clearly when

00:10:21,120 --> 00:10:26,520
we add a new item to the mix when it's

00:10:23,459 --> 00:10:28,560
brand new we have no historical data so

00:10:26,520 --> 00:10:31,050
we do have metadata about the item and

00:10:28,560 --> 00:10:33,120
many models will use them that metadata

00:10:31,050 --> 00:10:36,270
to try and improve prediction so for

00:10:33,120 --> 00:10:37,980
example if a user has watched a lot of

00:10:36,270 --> 00:10:40,140
action movies and a new action movie

00:10:37,980 --> 00:10:41,730
comes on you may want to use that

00:10:40,140 --> 00:10:43,709
information to recommend that movie to

00:10:41,730 --> 00:10:45,600
that user despite the fact that that

00:10:43,709 --> 00:10:48,750
movie has not been viewed by any similar

00:10:45,600 --> 00:10:50,430
users and similarly for new users or

00:10:48,750 --> 00:10:52,260
potentially users that were browsing

00:10:50,430 --> 00:10:53,820
anonymously or from a new device where

00:10:52,260 --> 00:10:57,060
we actually don't know them we can't

00:10:53,820 --> 00:10:59,250
link back that that particular device or

00:10:57,060 --> 00:11:00,570
anonymous user to a profile we don't

00:10:59,250 --> 00:11:01,670
know anything about them we haven't seen

00:11:00,570 --> 00:11:04,200
what they've interacted with

00:11:01,670 --> 00:11:05,810
historically we may have some context

00:11:04,200 --> 00:11:08,130
data what device they're using

00:11:05,810 --> 00:11:10,140
time-of-day geolocation which may help

00:11:08,130 --> 00:11:13,700
us if we have a contextual model but

00:11:10,140 --> 00:11:17,910
it's a challenge that we need to solve

00:11:13,700 --> 00:11:19,510
and finally another challenge is the

00:11:17,910 --> 00:11:23,589
dynamics of time

00:11:19,510 --> 00:11:25,990
so this applies to both the objective

00:11:23,589 --> 00:11:27,430
that the model is optimizing itself so

00:11:25,990 --> 00:11:29,470
click-through-rate behavior for example

00:11:27,430 --> 00:11:31,810
or watch behavior as well as these

00:11:29,470 --> 00:11:33,990
business metrics and and more complex

00:11:31,810 --> 00:11:39,190
objectives that we mentioned before

00:11:33,990 --> 00:11:41,910
so in many cases the dynamics of the

00:11:39,190 --> 00:11:46,360
short-term are changing very rapidly so

00:11:41,910 --> 00:11:50,889
news articles videos online these are

00:11:46,360 --> 00:11:53,079
examples where popularity is normally

00:11:50,889 --> 00:11:54,940
degrading very very fast and once that

00:11:53,079 --> 00:11:57,250
is released and the relevance to a

00:11:54,940 --> 00:11:59,920
particular user in the new space for

00:11:57,250 --> 00:12:02,470
example is degrading rapidly so if I'm

00:11:59,920 --> 00:12:06,310
coming to to read news about the latest

00:12:02,470 --> 00:12:08,440
you know scandal the newest politics yes

00:12:06,310 --> 00:12:11,110
some of the historical older articles

00:12:08,440 --> 00:12:13,120
all relevant if I'm and you know if my

00:12:11,110 --> 00:12:15,010
behavior is that I'm diving deeper and

00:12:13,120 --> 00:12:18,570
deeper into that story that becomes more

00:12:15,010 --> 00:12:20,829
relevant but in most cases you want to

00:12:18,570 --> 00:12:24,310
have that have the most popular and the

00:12:20,829 --> 00:12:28,149
most recent mix of articles that are

00:12:24,310 --> 00:12:31,380
displayed to the user so this far cycle

00:12:28,149 --> 00:12:33,339
of you know new items being added

00:12:31,380 --> 00:12:35,079
interacts with that cold start problem

00:12:33,339 --> 00:12:37,420
and creates a quite a big challenge and

00:12:35,079 --> 00:12:40,120
in the longer term taste can actually

00:12:37,420 --> 00:12:41,680
change over time so I may be really

00:12:40,120 --> 00:12:43,630
interested in you know US politics

00:12:41,680 --> 00:12:46,209
currently but in a few months I mean I

00:12:43,630 --> 00:12:48,279
may not be interested in that I'm I

00:12:46,209 --> 00:12:51,550
really like cat videos now but next year

00:12:48,279 --> 00:12:52,630
I may be into your mountain biking

00:12:51,550 --> 00:12:56,019
videos whatever the case may be

00:12:52,630 --> 00:13:00,220
so these dynamics are changing both in

00:12:56,019 --> 00:13:02,019
lis there's the non near real-time time

00:13:00,220 --> 00:13:03,730
frame so slightly longer than the

00:13:02,019 --> 00:13:06,490
instant or the you know a few days few

00:13:03,730 --> 00:13:07,930
weeks and also changing over months so

00:13:06,490 --> 00:13:10,209
we have to take into account that we

00:13:07,930 --> 00:13:11,680
wanna build a system that's going to be

00:13:10,209 --> 00:13:14,529
able to make those predictions that

00:13:11,680 --> 00:13:16,000
learn over long time frames and then

00:13:14,529 --> 00:13:17,829
finally we want to maximize those

00:13:16,000 --> 00:13:20,920
rewards over those long time frames so

00:13:17,829 --> 00:13:23,949
as we mentioned profitability is one as

00:13:20,920 --> 00:13:26,800
one aspect but user engagement is a is

00:13:23,949 --> 00:13:28,839
not a single metric that we can say well

00:13:26,800 --> 00:13:31,779
we've maximized it today therefore we've

00:13:28,839 --> 00:13:32,570
won we want to do that for a given user

00:13:31,779 --> 00:13:35,240
over a long

00:13:32,570 --> 00:13:37,550
period of time so we want to take into

00:13:35,240 --> 00:13:40,250
account all the changes in behavior and

00:13:37,550 --> 00:13:42,530
create a kind of more complex call where

00:13:40,250 --> 00:13:45,290
we want to optimize that user behavior

00:13:42,530 --> 00:13:49,820
optimize the profitability in aggregate

00:13:45,290 --> 00:13:52,910
over long periods of time so

00:13:49,820 --> 00:13:54,410
reinforcement learning is actually a

00:13:52,910 --> 00:13:57,050
good fit for this kind of problem space

00:13:54,410 --> 00:13:58,660
as we'll see so its most basic

00:13:57,050 --> 00:14:00,740
reinforcement learning is around

00:13:58,660 --> 00:14:03,040
learning by interacting with an

00:14:00,740 --> 00:14:06,080
environment in order to achieve a goal

00:14:03,040 --> 00:14:09,020
that's you know the very very high level

00:14:06,080 --> 00:14:12,350
basic of it and to expand it a little

00:14:09,020 --> 00:14:14,000
bit more will use a particular type of

00:14:12,350 --> 00:14:16,040
reinforcement learning a simple version

00:14:14,000 --> 00:14:17,630
which as you'll see has been applied to

00:14:16,040 --> 00:14:21,320
recommendation setting is called the

00:14:17,630 --> 00:14:23,600
multi-armed bandit so this is probably

00:14:21,320 --> 00:14:26,330
one of the most simple reinforcement

00:14:23,600 --> 00:14:30,910
learning problems but I say simple or

00:14:26,330 --> 00:14:33,170
simplest here in in a quotes because

00:14:30,910 --> 00:14:34,970
while it is quite simple on the face of

00:14:33,170 --> 00:14:36,500
it it can become really really complex

00:14:34,970 --> 00:14:38,600
and it actually does very well in

00:14:36,500 --> 00:14:40,580
real-world situations so one shouldn't

00:14:38,600 --> 00:14:43,670
write it off as being simple and

00:14:40,580 --> 00:14:45,830
therefore not useful so the idea behind

00:14:43,670 --> 00:14:49,670
multi-armed bandit is as the armed

00:14:45,830 --> 00:14:52,400
bandits parts is based on this idea of a

00:14:49,670 --> 00:14:53,870
slot machine or the one-armed bandit so

00:14:52,400 --> 00:14:55,820
the one-armed bandit is is a slot where

00:14:53,870 --> 00:14:58,180
the arm is on the side and you come

00:14:55,820 --> 00:15:01,700
along and you pull it and hopefully when

00:14:58,180 --> 00:15:04,450
gazillion euros but most of the time you

00:15:01,700 --> 00:15:07,820
don't so there's this concept of a

00:15:04,450 --> 00:15:10,100
reward when you pull or pay off when you

00:15:07,820 --> 00:15:12,590
pull the arm but it doesn't pay off

00:15:10,100 --> 00:15:14,660
every time so there's some sort of

00:15:12,590 --> 00:15:17,420
underlying probability distribution that

00:15:14,660 --> 00:15:19,280
is generating the reward and our goal if

00:15:17,420 --> 00:15:21,800
we're sitting at that machine is to

00:15:19,280 --> 00:15:22,820
hopefully maximize our reward now in

00:15:21,800 --> 00:15:25,430
this case we're going we're not going to

00:15:22,820 --> 00:15:26,960
be talking about your costs in the sense

00:15:25,430 --> 00:15:31,210
of having to pay for this for this

00:15:26,960 --> 00:15:31,210
bandit but a D that can be incorporated

00:15:31,480 --> 00:15:36,980
so just take an example with you know

00:15:34,100 --> 00:15:38,720
for arms or for bandits so it's

00:15:36,980 --> 00:15:41,510
effectively one bandit with four arms

00:15:38,720 --> 00:15:44,780
and this is the the simplest version of

00:15:41,510 --> 00:15:46,400
the problem where the payout is binary

00:15:44,780 --> 00:15:48,290
so you either get no payout

00:15:46,400 --> 00:15:52,310
get one so it's called the binary of the

00:15:48,290 --> 00:15:55,810
new bandit problem' so we can see here

00:15:52,310 --> 00:15:59,900
that each arm has a pair of probability

00:15:55,810 --> 00:16:01,310
but as the user was the gambler coming

00:15:59,900 --> 00:16:02,990
along to pull the arm I actually do not

00:16:01,310 --> 00:16:04,720
know what they are so they're they're

00:16:02,990 --> 00:16:08,090
unknown or unseen to me

00:16:04,720 --> 00:16:09,050
now at each time step and I can only

00:16:08,090 --> 00:16:11,330
pull one arm at a time

00:16:09,050 --> 00:16:13,340
you can obviously extend this to you to

00:16:11,330 --> 00:16:14,870
being able to pull multiple arms and

00:16:13,340 --> 00:16:16,400
having multiple players at a one time

00:16:14,870 --> 00:16:17,810
step but for now we'll assume that I can

00:16:16,400 --> 00:16:21,620
only pull one of these arms at a time

00:16:17,810 --> 00:16:23,660
and my goal is to maximize my reward so

00:16:21,620 --> 00:16:25,460
at each time step I must pull her choose

00:16:23,660 --> 00:16:29,750
which arm to pull and I want to get the

00:16:25,460 --> 00:16:31,880
greatest reward over the long term so

00:16:29,750 --> 00:16:34,370
this brings us into this concept of

00:16:31,880 --> 00:16:37,820
exploration versus exploitation so when

00:16:34,370 --> 00:16:41,120
we start off we I have no idea what is

00:16:37,820 --> 00:16:43,370
the payoff distribution and I have to

00:16:41,120 --> 00:16:45,080
balance two things in order to find out

00:16:43,370 --> 00:16:48,170
more about the world I need to explore

00:16:45,080 --> 00:16:52,340
so I need to pull an arm and see what

00:16:48,170 --> 00:16:54,560
happens and keep doing that and I could

00:16:52,340 --> 00:16:57,350
pull one arm or I could pull birth or

00:16:54,560 --> 00:16:59,420
for I could vary them but ultimately I

00:16:57,350 --> 00:17:01,910
need to start getting payoffs from the

00:16:59,420 --> 00:17:04,040
arm in order to start figuring out which

00:17:01,910 --> 00:17:06,830
one is going to be the most likely to

00:17:04,040 --> 00:17:09,050
payout but if I just keep randomly doing

00:17:06,830 --> 00:17:11,540
things the likelihood is I'm not going

00:17:09,050 --> 00:17:12,560
to really maximize my reward so at some

00:17:11,540 --> 00:17:14,510
point I need to actually start

00:17:12,560 --> 00:17:16,580
exploiting my knowledge so once I've

00:17:14,510 --> 00:17:18,709
learned hopefully which one of these

00:17:16,580 --> 00:17:20,089
arms is the most likely to payout

00:17:18,709 --> 00:17:21,920
well I'm just going to keep pulling that

00:17:20,089 --> 00:17:24,080
one because that over time is going to

00:17:21,920 --> 00:17:28,130
maximize the expectation that I have of

00:17:24,080 --> 00:17:29,780
getting a reward so this is this

00:17:28,130 --> 00:17:33,320
question question about how do you

00:17:29,780 --> 00:17:34,940
balance the need to explore to find out

00:17:33,320 --> 00:17:36,470
more about the world with a need to

00:17:34,940 --> 00:17:38,300
exploit in order to maximize a reward

00:17:36,470 --> 00:17:42,800
and this is the concept of a policy I

00:17:38,300 --> 00:17:44,690
need a plan that tells me at each time

00:17:42,800 --> 00:17:46,640
step given what I know about the world

00:17:44,690 --> 00:17:51,140
and given what I know about the arms

00:17:46,640 --> 00:17:54,560
which one am I going to pull and that is

00:17:51,140 --> 00:17:56,300
basically reinforcement learning so we

00:17:54,560 --> 00:17:57,800
have this concept of an agent in this

00:17:56,300 --> 00:18:00,590
case me

00:17:57,800 --> 00:18:03,380
we have an environment in this case the

00:18:00,590 --> 00:18:05,300
multi-armed bandit and at each time step

00:18:03,380 --> 00:18:06,950
I interact with the environment by

00:18:05,300 --> 00:18:09,800
taking some action in this case

00:18:06,950 --> 00:18:11,000
choosing which arm to pull now the

00:18:09,800 --> 00:18:14,240
bandit problem' doesn't actually have

00:18:11,000 --> 00:18:15,710
this concept of state but in full

00:18:14,240 --> 00:18:17,720
reinforcement learning settings we have

00:18:15,710 --> 00:18:19,160
a concept of state which is the state of

00:18:17,720 --> 00:18:21,080
the environments and so at each time

00:18:19,160 --> 00:18:23,450
step I receive the state of the

00:18:21,080 --> 00:18:26,630
environment which could be you know if

00:18:23,450 --> 00:18:28,880
I'm playing a an Atari game it would be

00:18:26,630 --> 00:18:29,780
the layouts of the of the image of the

00:18:28,880 --> 00:18:31,670
game yeah

00:18:29,780 --> 00:18:32,990
what is the screen look like or if I'm

00:18:31,670 --> 00:18:37,010
playing chess what is the board look

00:18:32,990 --> 00:18:40,250
like and when I pull taken action I

00:18:37,010 --> 00:18:42,920
receive a reward and we'll talk a bit

00:18:40,250 --> 00:18:45,320
about rewards next but the idea is that

00:18:42,920 --> 00:18:46,820
I receive the reward and then over time

00:18:45,320 --> 00:18:49,280
I learn the policy to maximize that

00:18:46,820 --> 00:18:51,500
reward so the policy is this mapping

00:18:49,280 --> 00:18:56,750
from the state of the world and what I

00:18:51,500 --> 00:18:58,330
know about it to taking an action so

00:18:56,750 --> 00:19:00,410
some of the challenges here is that

00:18:58,330 --> 00:19:02,660
while for the multi-armed bandit problem

00:19:00,410 --> 00:19:04,250
for example the reward is typically

00:19:02,660 --> 00:19:07,040
immediate I pull an arm and I get a

00:19:04,250 --> 00:19:08,650
reward immediately in most full

00:19:07,040 --> 00:19:10,700
reinforcement learning settings and

00:19:08,650 --> 00:19:13,670
complex scenarios in the real world

00:19:10,700 --> 00:19:15,950
that's not true so take the game of

00:19:13,670 --> 00:19:19,460
chess the reward only comes at the end

00:19:15,950 --> 00:19:21,290
right if you win a game you don't want

00:19:19,460 --> 00:19:23,600
to lose and you know draw is a kind of a

00:19:21,290 --> 00:19:25,550
split so you only know at the end of the

00:19:23,600 --> 00:19:29,540
game after taking all the actions what

00:19:25,550 --> 00:19:31,160
the reward was and similarly for your

00:19:29,540 --> 00:19:33,710
real-world scenarios or more complex

00:19:31,160 --> 00:19:37,760
games the other issue here is that is

00:19:33,710 --> 00:19:39,830
you have partial observability so I do

00:19:37,760 --> 00:19:41,360
not know what would have happened if I

00:19:39,830 --> 00:19:43,280
pulled the other arms I only get to

00:19:41,360 --> 00:19:46,250
choose to pull one arm so I could pull

00:19:43,280 --> 00:19:48,830
you know arm number one and get a reward

00:19:46,250 --> 00:19:50,360
or no no reward but if I had pulled the

00:19:48,830 --> 00:19:52,130
other arms I may have gotten a reward

00:19:50,360 --> 00:19:53,840
because that particular one has a higher

00:19:52,130 --> 00:19:55,430
probability of paying out but I don't

00:19:53,840 --> 00:19:57,980
know that so I can only observe my

00:19:55,430 --> 00:20:00,920
actions as the agent and I can't observe

00:19:57,980 --> 00:20:01,940
everything in some cases you know for

00:20:00,920 --> 00:20:05,270
example chess you have full

00:20:01,940 --> 00:20:06,650
observability yeah you can see it you

00:20:05,270 --> 00:20:07,760
can see all the different moves that

00:20:06,650 --> 00:20:09,290
could have happened and you see what

00:20:07,760 --> 00:20:10,710
your opponent did and everything but in

00:20:09,290 --> 00:20:14,160
most cases

00:20:10,710 --> 00:20:19,440
and most real-world situations you don't

00:20:14,160 --> 00:20:21,510
are not able to observe everything the

00:20:19,440 --> 00:20:23,910
other challenge is that this whole

00:20:21,510 --> 00:20:25,290
process and these data distributions are

00:20:23,910 --> 00:20:27,260
in fact changing over times as you

00:20:25,290 --> 00:20:29,790
mentioned in the recommendation space

00:20:27,260 --> 00:20:32,040
tests change over time the underlying

00:20:29,790 --> 00:20:34,530
data generating process is changing over

00:20:32,040 --> 00:20:36,210
time and in some cases the rules of the

00:20:34,530 --> 00:20:38,670
game are partially defined so for chess

00:20:36,210 --> 00:20:42,330
we're in luck because we know the rules

00:20:38,670 --> 00:20:43,860
of the game and fully defined for the

00:20:42,330 --> 00:20:45,240
simple bandit approach we're also in

00:20:43,860 --> 00:20:46,650
luck because the rules of the game

00:20:45,240 --> 00:20:48,930
undefined we just the only thing that's

00:20:46,650 --> 00:20:51,480
unknown is the probabilities underlying

00:20:48,930 --> 00:20:53,130
those rewards but in most cases for you

00:20:51,480 --> 00:20:56,760
know complex games like you know

00:20:53,130 --> 00:20:59,100
Starcraft 2 and what for games actually

00:20:56,760 --> 00:21:01,320
that's it's normally mostly fully

00:20:59,100 --> 00:21:03,630
defined but for real-world situations

00:21:01,320 --> 00:21:06,990
its partially defined so we may not know

00:21:03,630 --> 00:21:09,060
what the actual rules of you know of a

00:21:06,990 --> 00:21:12,750
recommendation space are we don't know

00:21:09,060 --> 00:21:14,850
the actual underlying behavior process

00:21:12,750 --> 00:21:18,570
of a user and we have these potentially

00:21:14,850 --> 00:21:22,050
huge state spaces so for the state space

00:21:18,570 --> 00:21:24,000
for multi-armed bandit is effectively

00:21:22,050 --> 00:21:25,710
you know the state of each arm one is

00:21:24,000 --> 00:21:28,080
what is the reward that we've received

00:21:25,710 --> 00:21:32,250
and so on and the actions that we can

00:21:28,080 --> 00:21:34,560
take our 1/4 and as we expand that more

00:21:32,250 --> 00:21:36,030
and more and more and expand that that

00:21:34,560 --> 00:21:38,760
bandit problem' more and more and more

00:21:36,030 --> 00:21:40,820
we get more greater greater greater size

00:21:38,760 --> 00:21:43,470
of state space and if you think about

00:21:40,820 --> 00:21:48,170
the game of go with his huge branching

00:21:43,470 --> 00:21:51,930
factors dota 2 and Starcraft with

00:21:48,170 --> 00:21:54,420
increasingly large branching factors and

00:21:51,930 --> 00:21:55,710
state space is effectively what the

00:21:54,420 --> 00:21:57,990
state of the world that we can receive

00:21:55,710 --> 00:21:59,670
and the actions that we can take that

00:21:57,990 --> 00:22:05,190
becomes one of the one of the main

00:21:59,670 --> 00:22:06,680
challenges so some of these these

00:22:05,190 --> 00:22:08,910
aspects really differentiate

00:22:06,680 --> 00:22:10,620
reinforcement learning from normal kinds

00:22:08,910 --> 00:22:12,030
of machine learning not normal cards but

00:22:10,620 --> 00:22:13,800
the other two kinds of machine learning

00:22:12,030 --> 00:22:16,920
so supervised learning and unsupervised

00:22:13,800 --> 00:22:19,830
learning so here in supervised learning

00:22:16,920 --> 00:22:21,810
we have we're given a label so we know

00:22:19,830 --> 00:22:24,430
you're one training example we given a

00:22:21,810 --> 00:22:26,320
matching label and we want to predict

00:22:24,430 --> 00:22:28,330
a model will learn a model that can

00:22:26,320 --> 00:22:30,010
predict a label giving the example an

00:22:28,330 --> 00:22:32,020
unsupervised learning will effectively

00:22:30,010 --> 00:22:34,090
learning structure of the data so we

00:22:32,020 --> 00:22:35,290
don't have labels but we want to learn

00:22:34,090 --> 00:22:37,900
the structure of the data effectively

00:22:35,290 --> 00:22:43,720
the probability distribution so that we

00:22:37,900 --> 00:22:47,710
can make your best estimates of that

00:22:43,720 --> 00:22:50,500
data and for reinforcement learning we

00:22:47,710 --> 00:22:52,059
don't have labels and so in some sense

00:22:50,500 --> 00:22:53,770
it's it's kind of unsupervised

00:22:52,059 --> 00:22:57,760
but unsupervised learning doesn't have

00:22:53,770 --> 00:23:00,160
this this concept of of taking actions

00:22:57,760 --> 00:23:03,580
partial observability the long term time

00:23:00,160 --> 00:23:06,850
frames huge state spaces so going back

00:23:03,580 --> 00:23:08,679
to our multi on bandit how do we go

00:23:06,850 --> 00:23:10,750
about solving this problem how do we

00:23:08,679 --> 00:23:12,610
generate this policy that map's the

00:23:10,750 --> 00:23:16,059
state of the world to the actions that

00:23:12,610 --> 00:23:19,210
we want to take well this is it again in

00:23:16,059 --> 00:23:22,960
itself a huge field and a really deep

00:23:19,210 --> 00:23:24,490
field of research but at a high level we

00:23:22,960 --> 00:23:27,010
can explore a couple of simple

00:23:24,490 --> 00:23:28,510
strategies so the first is obviously we

00:23:27,010 --> 00:23:29,679
can do nothing that's not going to be

00:23:28,510 --> 00:23:31,240
very useful because we're going to get

00:23:29,679 --> 00:23:32,020
no reward and that's definitely not

00:23:31,240 --> 00:23:36,070
going to be optimal

00:23:32,020 --> 00:23:38,140
the second is greedy policies and the

00:23:36,070 --> 00:23:42,160
third is more principled statistical

00:23:38,140 --> 00:23:46,030
policies so for greedy policies the idea

00:23:42,160 --> 00:23:47,200
is that we want to mostly exploit ok

00:23:46,030 --> 00:23:48,400
we're going to be greedy as we're going

00:23:47,200 --> 00:23:50,620
to use our knowledge as much as possible

00:23:48,400 --> 00:23:52,420
to increase our reward so we could

00:23:50,620 --> 00:23:55,540
obviously be purely greedy which is

00:23:52,420 --> 00:23:57,340
exploit only so in theory let's say we

00:23:55,540 --> 00:23:58,660
could pull each arm and then just go for

00:23:57,340 --> 00:24:01,030
the arm that that gave us the reward

00:23:58,660 --> 00:24:04,690
that would be that would be a close to

00:24:01,030 --> 00:24:06,550
purely greedy approach we then have a

00:24:04,690 --> 00:24:08,950
slightly more sophisticated approach

00:24:06,550 --> 00:24:11,470
which is a pape greedy so the idea there

00:24:08,950 --> 00:24:13,809
is to explore first and then exploit all

00:24:11,470 --> 00:24:16,059
to have phases epochs of exploration

00:24:13,809 --> 00:24:18,940
followed by exploitation so the idea is

00:24:16,059 --> 00:24:20,920
I would spend a bit of time you know 100

00:24:18,940 --> 00:24:23,410
time steps pulling each arm and figuring

00:24:20,920 --> 00:24:25,360
out what is my estimate of the reward

00:24:23,410 --> 00:24:26,830
distribution and once I've done a

00:24:25,360 --> 00:24:28,120
certain number of time steps I'm going

00:24:26,830 --> 00:24:29,530
to go for the one that has the highest

00:24:28,120 --> 00:24:32,140
expected reward and I'm just going to

00:24:29,530 --> 00:24:34,360
keep pulling it until I'm done I could

00:24:32,140 --> 00:24:36,460
as I said interleave those policies

00:24:34,360 --> 00:24:37,400
those epochs in between each other so I

00:24:36,460 --> 00:24:39,320
could

00:24:37,400 --> 00:24:40,760
explore for a while then exploit for a

00:24:39,320 --> 00:24:42,590
period and then go back to exploring

00:24:40,760 --> 00:24:46,010
just to just to check if something has

00:24:42,590 --> 00:24:48,440
changed and then if we generalize that

00:24:46,010 --> 00:24:50,990
approach we get to epsilon greedy which

00:24:48,440 --> 00:24:54,350
is that saying with probability Epsilon

00:24:50,990 --> 00:24:57,410
normally quite small you know 1% 10% 5%

00:24:54,350 --> 00:24:59,510
I'm going to pull a random arm and with

00:24:57,410 --> 00:25:02,120
probability 1 minus epsilon I'm going to

00:24:59,510 --> 00:25:03,470
pull the arm with the highest payoff so

00:25:02,120 --> 00:25:05,480
this is a slightly more principle that

00:25:03,470 --> 00:25:07,610
says I'm going to have a principled way

00:25:05,480 --> 00:25:09,140
of exploring but I'm just not going to

00:25:07,610 --> 00:25:13,850
spend most of my time exploiting what I

00:25:09,140 --> 00:25:17,090
know so if we take that a little bit

00:25:13,850 --> 00:25:22,130
further we have an approach called upper

00:25:17,090 --> 00:25:24,740
confidence bound and the idea here is we

00:25:22,130 --> 00:25:28,280
want to build a an estimate of the

00:25:24,740 --> 00:25:29,630
expected reward for each arm but we also

00:25:28,280 --> 00:25:31,429
want to take into account the fact that

00:25:29,630 --> 00:25:34,160
we don't know everything so we have

00:25:31,429 --> 00:25:35,240
uncertainty effectively in the if we

00:25:34,160 --> 00:25:38,000
think about the probability distribution

00:25:35,240 --> 00:25:39,500
of each arm we have the mean and we have

00:25:38,000 --> 00:25:42,410
a standard deviation so we can think of

00:25:39,500 --> 00:25:44,900
it as saying instead of just saying I'm

00:25:42,410 --> 00:25:47,059
going to pull the arm with the highest

00:25:44,900 --> 00:25:48,940
expected reward the highest mean I'm

00:25:47,059 --> 00:25:51,220
going to also take into account the

00:25:48,940 --> 00:25:53,720
standard deviation of that distribution

00:25:51,220 --> 00:25:55,640
what this means is that if you have an

00:25:53,720 --> 00:25:59,230
arm that has a lower expected value

00:25:55,640 --> 00:26:02,929
expected payoff but it has a high

00:25:59,230 --> 00:26:05,300
standard deviation variance you may

00:26:02,929 --> 00:26:07,550
still want to pull that arm to explore

00:26:05,300 --> 00:26:09,080
because you you don't yet know you

00:26:07,550 --> 00:26:12,800
haven't yet narrowed down your knowledge

00:26:09,080 --> 00:26:15,890
about that arm and similarly if an arm

00:26:12,800 --> 00:26:17,690
has very high expected return and a low

00:26:15,890 --> 00:26:19,280
standard deviation well then again you

00:26:17,690 --> 00:26:21,070
would want to pull that on because you

00:26:19,280 --> 00:26:23,120
have pretty high confidence what the

00:26:21,070 --> 00:26:25,460
underlying reward probability is going

00:26:23,120 --> 00:26:28,640
to be so effectively that's what you're

00:26:25,460 --> 00:26:30,980
doing you're you have one component of

00:26:28,640 --> 00:26:33,679
the you see B which is the expected

00:26:30,980 --> 00:26:35,360
reward and you have the bound the upper

00:26:33,679 --> 00:26:39,620
confidence bound which is formed by

00:26:35,360 --> 00:26:42,530
adding a a an adjustment based on

00:26:39,620 --> 00:26:43,700
standard deviation and in most cases you

00:26:42,530 --> 00:26:46,370
know if you can look at the literature

00:26:43,700 --> 00:26:48,920
you see people will outperform epsilon 3

00:26:46,370 --> 00:26:51,950
and 4 for non contextual models it's

00:26:48,920 --> 00:26:55,080
going to be one of the best performance

00:26:51,950 --> 00:26:56,159
so if we want to be even more rigorous

00:26:55,080 --> 00:26:58,320
about this from a statistical

00:26:56,159 --> 00:27:00,419
perspective we can apply Bayesian

00:26:58,320 --> 00:27:02,759
reasoning and that's why we get bayesian

00:27:00,419 --> 00:27:05,999
bandits or Thompson sampling so similar

00:27:02,759 --> 00:27:08,419
to UCB what we're trying to do here is

00:27:05,999 --> 00:27:10,830
say I'm going to start with a prior

00:27:08,419 --> 00:27:13,619
probability distribution on each arm so

00:27:10,830 --> 00:27:15,080
I'm going to assign a prior and if I

00:27:13,619 --> 00:27:17,820
don't know anything I'm just going to

00:27:15,080 --> 00:27:19,979
assign a kind of non informative prior

00:27:17,820 --> 00:27:21,809
that that is the same across all arms

00:27:19,979 --> 00:27:23,820
but I could also take into account any

00:27:21,809 --> 00:27:26,309
information I might have about you know

00:27:23,820 --> 00:27:28,169
about the world so if I if I may know

00:27:26,309 --> 00:27:30,210
that some armors could a higher

00:27:28,169 --> 00:27:32,669
probability prior probability I could

00:27:30,210 --> 00:27:34,049
incorporate that and then at each time

00:27:32,669 --> 00:27:36,239
step I simply sample from those

00:27:34,049 --> 00:27:38,820
distributions that I have and I picked

00:27:36,239 --> 00:27:42,299
the one with the highest highest sample

00:27:38,820 --> 00:27:45,089
the payoff and I pull that arm and then

00:27:42,299 --> 00:27:46,739
I update my probability distribution I

00:27:45,089 --> 00:27:48,269
played that prior with with the

00:27:46,739 --> 00:27:51,210
posterior effectively with that new

00:27:48,269 --> 00:27:54,779
information so in this way I'm also

00:27:51,210 --> 00:27:57,929
taking into account the expected value

00:27:54,779 --> 00:27:59,549
and the variation by sampling but I'm

00:27:57,929 --> 00:28:04,799
being more principled in the way that I

00:27:59,549 --> 00:28:07,049
update my my view of the world and then

00:28:04,799 --> 00:28:11,330
the next idea in Bandits is to add

00:28:07,049 --> 00:28:13,559
context so the simple multi-armed bandit

00:28:11,330 --> 00:28:16,080
environment we're setting is not really

00:28:13,559 --> 00:28:18,450
going to be particularly useful in the

00:28:16,080 --> 00:28:20,639
real world I mean it can be for for some

00:28:18,450 --> 00:28:22,619
situations for things like maybe

00:28:20,639 --> 00:28:24,929
effectively a be testing using bandits

00:28:22,619 --> 00:28:28,679
for a/b testing a small number of

00:28:24,929 --> 00:28:30,629
examples but if we think about most use

00:28:28,679 --> 00:28:32,070
cases in recommendations for example

00:28:30,629 --> 00:28:34,859
online advertising that we might one

00:28:32,070 --> 00:28:38,460
might want to use these approaches we're

00:28:34,859 --> 00:28:41,669
often going to have a lot of items so

00:28:38,460 --> 00:28:44,669
now if we think about every news article

00:28:41,669 --> 00:28:46,619
on our platform as as a an armed right

00:28:44,669 --> 00:28:48,559
that could be pulled to display to the

00:28:46,619 --> 00:28:51,929
user we can very quickly get into the

00:28:48,559 --> 00:28:54,089
hundreds of thousands tens of thousands

00:28:51,929 --> 00:28:55,589
even more in terms of items so that

00:28:54,089 --> 00:28:58,529
state space starts to become really

00:28:55,589 --> 00:29:00,570
really large and exploring that in

00:28:58,529 --> 00:29:02,099
anyway and exploring all of it in a

00:29:00,570 --> 00:29:03,179
principled way is is effectively going

00:29:02,099 --> 00:29:03,810
to be impossible we don't have enough

00:29:03,179 --> 00:29:05,370
time

00:29:03,810 --> 00:29:07,530
we don't have enough you know traffic or

00:29:05,370 --> 00:29:10,530
enough information so the idea behind

00:29:07,530 --> 00:29:15,390
contextual bandits is to say can we

00:29:10,530 --> 00:29:17,910
apply extra variables to our function

00:29:15,390 --> 00:29:20,130
mapping so instead of just having a

00:29:17,910 --> 00:29:22,490
simple expected value and you know

00:29:20,130 --> 00:29:25,440
variation type of approach for each

00:29:22,490 --> 00:29:27,090
distribution we can build a a model

00:29:25,440 --> 00:29:29,490
either you know linear model in the case

00:29:27,090 --> 00:29:31,710
of models like linear L and then you CD

00:29:29,490 --> 00:29:33,780
or non linear model which could be you

00:29:31,710 --> 00:29:36,930
know some kind of kernel model or deep

00:29:33,780 --> 00:29:39,870
neural network and the idea is to map a

00:29:36,930 --> 00:29:41,820
set of features to the to the reward so

00:29:39,870 --> 00:29:44,160
at each time step we're not we're not

00:29:41,820 --> 00:29:45,270
just updating an expectation in a

00:29:44,160 --> 00:29:48,570
variation and so on but actually

00:29:45,270 --> 00:29:50,490
updating a full linear model so again in

00:29:48,570 --> 00:29:51,480
the case of news articles you can you

00:29:50,490 --> 00:29:54,600
can think about the different features

00:29:51,480 --> 00:29:55,410
we might want to incorporate and we'll

00:29:54,600 --> 00:29:57,420
get to you a bit more about

00:29:55,410 --> 00:30:02,880
recommendations and contextual bandits

00:29:57,420 --> 00:30:05,030
shortly so before we move on to a little

00:30:02,880 --> 00:30:07,800
bit about deep reinforcement learning

00:30:05,030 --> 00:30:09,510
we'll just briefly cover some of deep

00:30:07,800 --> 00:30:12,240
learning and then see how they've have

00:30:09,510 --> 00:30:13,790
kind of combined and so deep learning

00:30:12,240 --> 00:30:17,700
has been around for quite a long time

00:30:13,790 --> 00:30:19,800
since the kind of 1940s and the original

00:30:17,700 --> 00:30:22,920
computer models originated in the 1960s

00:30:19,800 --> 00:30:25,320
this is actually a perceptron machine on

00:30:22,920 --> 00:30:26,580
the on the right there so that used to

00:30:25,320 --> 00:30:29,460
be that was the first kind of

00:30:26,580 --> 00:30:33,090
multi-layer perceptron back in the 1960s

00:30:29,460 --> 00:30:34,860
and these techniques fell out of favor

00:30:33,090 --> 00:30:37,040
in the in the 80s and 90s and the kind

00:30:34,860 --> 00:30:39,810
of AI winter when they didn't really

00:30:37,040 --> 00:30:41,490
live up to expectations but there's been

00:30:39,810 --> 00:30:43,860
a recent resurgence due to a number of

00:30:41,490 --> 00:30:46,830
factors so we've had bigger and better

00:30:43,860 --> 00:30:50,160
data if you think about all the mobile

00:30:46,830 --> 00:30:52,380
devices edge devices the web cameras

00:30:50,160 --> 00:30:54,360
that have created rich image data text

00:30:52,380 --> 00:30:55,560
data voice data and more and more and

00:30:54,360 --> 00:30:56,940
more of it and obviously the large

00:30:55,560 --> 00:30:59,010
internet companies have access to this

00:30:56,940 --> 00:31:01,110
kind of data as well as standard data

00:30:59,010 --> 00:31:03,750
sets so image net for example a shield

00:31:01,110 --> 00:31:05,940
in a whole new era of advances in image

00:31:03,750 --> 00:31:08,220
classification so having standard large

00:31:05,940 --> 00:31:11,070
standardized data sets available for

00:31:08,220 --> 00:31:13,220
researchers and academics and corporates

00:31:11,070 --> 00:31:15,930
to use to advance the state-of-the-art

00:31:13,220 --> 00:31:16,850
better hardware is a big factor GPUs in

00:31:15,930 --> 00:31:19,070
particular

00:31:16,850 --> 00:31:20,900
as the cost and availability of GPUs

00:31:19,070 --> 00:31:23,450
cost came down the availability

00:31:20,900 --> 00:31:25,340
increased you had more and more more

00:31:23,450 --> 00:31:27,440
compute power so you could take the same

00:31:25,340 --> 00:31:28,730
old model which couldn't be computed

00:31:27,440 --> 00:31:30,620
before now you can compute it and

00:31:28,730 --> 00:31:32,470
actually get the result and of course

00:31:30,620 --> 00:31:35,090
improvements to architectures

00:31:32,470 --> 00:31:37,220
optimization algorithms so this has led

00:31:35,090 --> 00:31:39,590
to state-of-the-art results in computer

00:31:37,220 --> 00:31:41,450
vision speech and text language

00:31:39,590 --> 00:31:44,480
translation pretty much across the board

00:31:41,450 --> 00:31:48,680
we've seen and in particular in domains

00:31:44,480 --> 00:31:51,650
with with rich data text images video

00:31:48,680 --> 00:31:54,770
and speech we've seen state of the art

00:31:51,650 --> 00:31:58,180
advances so modern deep learning is all

00:31:54,770 --> 00:31:59,900
around great deep multi-layer networks

00:31:58,180 --> 00:32:02,750
convolutional neural networks and

00:31:59,900 --> 00:32:04,100
computer vision sequences and time

00:32:02,750 --> 00:32:06,470
series and applying recurrent neural

00:32:04,100 --> 00:32:09,770
networks he'll STM's gated recurrent

00:32:06,470 --> 00:32:11,300
units and so on embeddings for text and

00:32:09,770 --> 00:32:12,740
categorical features and the deep

00:32:11,300 --> 00:32:15,650
learning frameworks have played a large

00:32:12,740 --> 00:32:17,270
role in fact so the frameworks act in

00:32:15,650 --> 00:32:19,550
Suffern pi torch give you this

00:32:17,270 --> 00:32:21,410
flexibility this concept of computation

00:32:19,550 --> 00:32:23,480
graphs automatic differentiation you

00:32:21,410 --> 00:32:25,700
don't need to go and do this by hand GPU

00:32:23,480 --> 00:32:28,280
support so it's allowed a lot of

00:32:25,700 --> 00:32:30,520
experimentation rapid development and

00:32:28,280 --> 00:32:32,780
increasing again the state-of-the-art

00:32:30,520 --> 00:32:35,570
recurrent neural networks for for

00:32:32,780 --> 00:32:37,610
example have not been very successful in

00:32:35,570 --> 00:32:39,560
sequence models so on the left you have

00:32:37,610 --> 00:32:41,390
the one-to-one approach you know image

00:32:39,560 --> 00:32:42,800
classification an honor non-recurrent

00:32:41,390 --> 00:32:45,740
model which will just take one input and

00:32:42,800 --> 00:32:48,020
give you a classification output you

00:32:45,740 --> 00:32:50,180
have a one-to-many sequence approach

00:32:48,020 --> 00:32:51,860
where you have an image input for

00:32:50,180 --> 00:32:53,780
example and you want to generate a text

00:32:51,860 --> 00:32:57,820
caption which is a sequence of your off

00:32:53,780 --> 00:33:00,890
words or tokens many-to-one which may be

00:32:57,820 --> 00:33:02,750
something like taking that sequence of

00:33:00,890 --> 00:33:05,750
tokens that represents text and spitting

00:33:02,750 --> 00:33:08,060
out something like a sentiment label and

00:33:05,750 --> 00:33:10,400
then you have many many to many

00:33:08,060 --> 00:33:12,140
relationships so for example taking a

00:33:10,400 --> 00:33:14,810
sequence of English language and

00:33:12,140 --> 00:33:17,780
translating it to German or labeling

00:33:14,810 --> 00:33:19,580
each piece of a sequence so named entity

00:33:17,780 --> 00:33:22,310
recognition would be taking each word

00:33:19,580 --> 00:33:24,800
and assigning an entity tag or labeling

00:33:22,310 --> 00:33:28,640
each frame in a video so our n ends have

00:33:24,800 --> 00:33:30,080
been really successful in this space and

00:33:28,640 --> 00:33:30,559
these deep learning approaches are

00:33:30,080 --> 00:33:33,379
increasing

00:33:30,559 --> 00:33:35,389
being applied to recommend us so most

00:33:33,379 --> 00:33:38,889
approaches have focused on this idea of

00:33:35,389 --> 00:33:42,740
deep learning as feature extractor so

00:33:38,889 --> 00:33:45,289
one example on the top right there is to

00:33:42,740 --> 00:33:47,919
actually take audio files from my finger

00:33:45,289 --> 00:33:49,820
to Spotify and represent them as

00:33:47,919 --> 00:33:51,980
spectrograms and then apply

00:33:49,820 --> 00:33:53,929
convolutional networks on top of that to

00:33:51,980 --> 00:33:56,029
get an embedding and then mapping that

00:33:53,929 --> 00:33:57,529
embedding to the embedding that is

00:33:56,029 --> 00:34:02,570
generated from a collaborative filtering

00:33:57,529 --> 00:34:04,399
of recommendation model so on the bottom

00:34:02,570 --> 00:34:05,809
we have a a wide and deep model which is

00:34:04,399 --> 00:34:07,789
trying to combine the power of linear

00:34:05,809 --> 00:34:09,230
models with deep learning so most of the

00:34:07,789 --> 00:34:11,119
advances in deep learning applied to

00:34:09,230 --> 00:34:12,560
recommenders have been trying to use

00:34:11,119 --> 00:34:14,060
deep learning for this really good at

00:34:12,560 --> 00:34:15,679
which is extracting automatically

00:34:14,060 --> 00:34:18,740
extracting features from rich content

00:34:15,679 --> 00:34:20,690
image text and then using those features

00:34:18,740 --> 00:34:22,639
in in some sort of model and we also

00:34:20,690 --> 00:34:24,800
have this concept of an end to end model

00:34:22,639 --> 00:34:27,440
so instead of combined you're taking one

00:34:24,800 --> 00:34:29,389
model and combining it with another in

00:34:27,440 --> 00:34:31,429
some ad hoc way training the entire

00:34:29,389 --> 00:34:33,500
network has become something that's

00:34:31,429 --> 00:34:34,669
increasingly common across all types of

00:34:33,500 --> 00:34:37,369
models and we as you'll see in

00:34:34,669 --> 00:34:38,990
reinforcement learning so the idea is to

00:34:37,369 --> 00:34:41,300
train everything at once in one

00:34:38,990 --> 00:34:42,919
architecture or connected one set of

00:34:41,300 --> 00:34:44,839
whites or shared parameters depending on

00:34:42,919 --> 00:34:48,800
the model and ultimately get a better

00:34:44,839 --> 00:34:50,450
result and suddenly we've seen advances

00:34:48,800 --> 00:34:53,119
in applying recurrent neural network

00:34:50,450 --> 00:34:55,369
architectures so two sequences and

00:34:53,119 --> 00:34:58,280
sequence modeling for recommenders so if

00:34:55,369 --> 00:35:01,069
you see if we treat each use a sequence

00:34:58,280 --> 00:35:03,859
with session as a sequence that can be

00:35:01,069 --> 00:35:06,349
learned and we can apply with some with

00:35:03,859 --> 00:35:08,450
a few changes we can apply much of the

00:35:06,349 --> 00:35:10,190
same RNN techniques as we apply in text

00:35:08,450 --> 00:35:15,410
or other sequence models to these

00:35:10,190 --> 00:35:18,109
challenges so for deep reinforcement

00:35:15,410 --> 00:35:20,540
learning and again for RL we haven't

00:35:18,109 --> 00:35:22,250
really had enough time to go through all

00:35:20,540 --> 00:35:25,060
of the different approaches that they go

00:35:22,250 --> 00:35:28,010
beyond obviously go well beyond Bandits

00:35:25,060 --> 00:35:30,950
but effectively that the idea is to

00:35:28,010 --> 00:35:33,319
combine those state-of-the-art

00:35:30,950 --> 00:35:35,359
reinforcement learning approaches with

00:35:33,319 --> 00:35:38,990
the deep learning techniques so one

00:35:35,359 --> 00:35:41,630
aspect of that is to take things like

00:35:38,990 --> 00:35:43,119
much the same as we as for the other way

00:35:41,630 --> 00:35:46,299
that deep learning does it is

00:35:43,119 --> 00:35:47,950
take aspects of these are deep learning

00:35:46,299 --> 00:35:49,630
for recommenders desert is to take

00:35:47,950 --> 00:35:51,849
aspects of the deep learning models

00:35:49,630 --> 00:35:54,579
convolutional networks are in ends and

00:35:51,849 --> 00:35:57,819
use them to model the inputs for for the

00:35:54,579 --> 00:36:01,029
the reinforcement learning and the other

00:35:57,819 --> 00:36:04,509
idea is to pain these models in to end

00:36:01,029 --> 00:36:06,490
so if you look at for example alpha star

00:36:04,509 --> 00:36:08,950
which is speeding you know Starcraft

00:36:06,490 --> 00:36:10,240
players a professional level it is a

00:36:08,950 --> 00:36:12,700
combination of many many different

00:36:10,240 --> 00:36:15,490
systems and part of that is you know

00:36:12,700 --> 00:36:19,200
lsdm networks for incorporating this

00:36:15,490 --> 00:36:21,279
idea of memory into into the system

00:36:19,200 --> 00:36:22,839
transformers for mapping sequences

00:36:21,279 --> 00:36:25,029
states-based sequences to action

00:36:22,839 --> 00:36:26,410
sequences and combining all of these and

00:36:25,029 --> 00:36:28,900
training them into end and this has been

00:36:26,410 --> 00:36:30,940
really one of the key advances in the

00:36:28,900 --> 00:36:32,589
space it's combining all these different

00:36:30,940 --> 00:36:35,799
pieces training them into and to get the

00:36:32,589 --> 00:36:39,190
results and results have been pretty

00:36:35,799 --> 00:36:43,019
phenomenal in certainly still in at the

00:36:39,190 --> 00:36:44,319
level of games or you know effectively

00:36:43,019 --> 00:36:46,809
well-defined

00:36:44,319 --> 00:36:49,509
sets sets of rules or well-defined

00:36:46,809 --> 00:36:52,539
structures so alphago beat the world

00:36:49,509 --> 00:36:54,039
champion and go alphas era is I'm not

00:36:52,539 --> 00:36:56,170
sure if it's actually played any you

00:36:54,039 --> 00:36:57,549
know the top champion but it's hard to

00:36:56,170 --> 00:36:59,980
be the top chess player in the world and

00:36:57,549 --> 00:37:02,559
his superhuman level performance alpha

00:36:59,980 --> 00:37:06,579
stars beaten professional level humans

00:37:02,559 --> 00:37:09,190
at Starcraft open AI 5 is B top

00:37:06,579 --> 00:37:11,769
professional defense of the Asian dota 2

00:37:09,190 --> 00:37:16,569
or defense of the ancients two teams you

00:37:11,769 --> 00:37:19,259
know and earlier this year so in areas

00:37:16,569 --> 00:37:21,369
where the full game rules are understood

00:37:19,259 --> 00:37:23,430
but you still have the challenges of

00:37:21,369 --> 00:37:25,539
huge state spaces partial observability

00:37:23,430 --> 00:37:27,730
you're a long time frames and then

00:37:25,539 --> 00:37:29,289
mapping different different different

00:37:27,730 --> 00:37:35,920
reward and kind of complex objectives

00:37:29,289 --> 00:37:37,720
we've seen huge advances so finally what

00:37:35,920 --> 00:37:39,400
are some of the applications of the

00:37:37,720 --> 00:37:40,569
reinforcement learning to recommender

00:37:39,400 --> 00:37:45,279
systems and I'll just highlight a couple

00:37:40,569 --> 00:37:47,920
of key ones so as we saw the multi-armed

00:37:45,279 --> 00:37:49,029
bandit approach and especially if we if

00:37:47,920 --> 00:37:52,690
we think about the contextual approach

00:37:49,029 --> 00:37:55,359
maps really nicely to recommend in

00:37:52,690 --> 00:37:56,200
recommending items that are changing

00:37:55,359 --> 00:37:59,020
frequently

00:37:56,200 --> 00:38:00,490
so this is an example from the fairly

00:37:59,020 --> 00:38:03,579
old example now from the Yahoo today

00:38:00,490 --> 00:38:05,710
front page where there's four articles

00:38:03,579 --> 00:38:09,309
that can be selected and then the main

00:38:05,710 --> 00:38:11,109
one is set as the story and this in this

00:38:09,309 --> 00:38:13,720
paper introduced the Linda NUCB

00:38:11,109 --> 00:38:15,309
algorithm which is effectively an

00:38:13,720 --> 00:38:17,170
extension of that upper confidence bound

00:38:15,309 --> 00:38:21,040
that we saw before but instead of

00:38:17,170 --> 00:38:23,109
mapping a simple simple reward

00:38:21,040 --> 00:38:27,339
distribution we use a bunch of features

00:38:23,109 --> 00:38:30,010
and use a linear model to predict to

00:38:27,339 --> 00:38:31,720
predict that and there's also a bound of

00:38:30,010 --> 00:38:33,910
sort of variation or standard deviation

00:38:31,720 --> 00:38:35,440
based bound on top of that so it's an

00:38:33,910 --> 00:38:38,380
extension of this you see the idea and

00:38:35,440 --> 00:38:39,849
the types of features that they that

00:38:38,380 --> 00:38:43,390
they included here are the kind of

00:38:39,849 --> 00:38:46,480
things that you might expect your bag of

00:38:43,390 --> 00:38:48,780
words tokens from the new stories and

00:38:46,480 --> 00:38:51,730
the descriptions and the titles

00:38:48,780 --> 00:38:54,069
demographic and geolocation data for the

00:38:51,730 --> 00:38:55,960
users so it means that we can bring all

00:38:54,069 --> 00:38:58,750
of the features that we that we want to

00:38:55,960 --> 00:38:59,980
use in our recommender models and that

00:38:58,750 --> 00:39:01,690
were kind of familiar with potentially

00:38:59,980 --> 00:39:03,520
from written from standard contextual

00:39:01,690 --> 00:39:06,970
recommendation approaches and apply them

00:39:03,520 --> 00:39:09,369
in in this in this case so learn UCB in

00:39:06,970 --> 00:39:11,500
this paper outperformed epsilon greedy

00:39:09,369 --> 00:39:19,329
and standard ECB approaches on live

00:39:11,500 --> 00:39:21,490
traffic and was implemented in yarn one

00:39:19,329 --> 00:39:24,309
of the other applications is

00:39:21,490 --> 00:39:27,400
personalizing artwork so Netflix has a

00:39:24,309 --> 00:39:31,420
couple of blog posts a little bit older

00:39:27,400 --> 00:39:34,420
now around which movie artwork to show

00:39:31,420 --> 00:39:36,670
to each user so when you if you're

00:39:34,420 --> 00:39:39,069
generating a recommendation

00:39:36,670 --> 00:39:41,799
obviously for for certain types of

00:39:39,069 --> 00:39:43,630
products while items it's quite visual

00:39:41,799 --> 00:39:46,089
what you're showing so you might show

00:39:43,630 --> 00:39:47,920
different types of the the poster for

00:39:46,089 --> 00:39:50,740
movie or different images from that

00:39:47,920 --> 00:39:52,349
movie something like that and users may

00:39:50,740 --> 00:39:56,230
respond to that in very different ways

00:39:52,349 --> 00:39:58,150
so they've started they started

00:39:56,230 --> 00:40:00,099
experimenting with just using Bandits

00:39:58,150 --> 00:40:00,980
effectively doing a kind of a bee test

00:40:00,099 --> 00:40:03,080
over

00:40:00,980 --> 00:40:05,570
more complex AP tests over different

00:40:03,080 --> 00:40:07,910
input images and then applied contextual

00:40:05,570 --> 00:40:09,740
bandits and what we see here is

00:40:07,910 --> 00:40:11,540
effectively the what they call the tech

00:40:09,740 --> 00:40:15,110
fraction so the percentage of times that

00:40:11,540 --> 00:40:17,900
a user effectively was selecting that

00:40:15,110 --> 00:40:22,010
image and and contextual bandits are

00:40:17,900 --> 00:40:24,890
again outperforming some of the other

00:40:22,010 --> 00:40:27,830
aspects that some of the other use cases

00:40:24,890 --> 00:40:31,910
have been thinking about using bandits

00:40:27,830 --> 00:40:34,340
as ensembles to create ensembles so the

00:40:31,910 --> 00:40:35,990
idea here is and I mentioned a little

00:40:34,340 --> 00:40:38,660
bit at a high level before is that

00:40:35,990 --> 00:40:40,970
banded algorithms can be used as an a/b

00:40:38,660 --> 00:40:43,790
test so if we treat each potential

00:40:40,970 --> 00:40:45,140
different type of content or content

00:40:43,790 --> 00:40:47,480
option that you want to show as an arm

00:40:45,140 --> 00:40:50,540
we can actually generalize a be testing

00:40:47,480 --> 00:40:52,160
and be more principled about it so you

00:40:50,540 --> 00:40:54,380
know if one arm could be a green button

00:40:52,160 --> 00:40:56,030
one one could be a red button one could

00:40:54,380 --> 00:40:57,200
be a yellow button and if we take that

00:40:56,030 --> 00:40:59,180
too

00:40:57,200 --> 00:41:00,920
you know expand that idea then we can

00:40:59,180 --> 00:41:03,200
have an ensemble of recommender models

00:41:00,920 --> 00:41:05,060
or in fact recommender systems so we can

00:41:03,200 --> 00:41:07,100
treat each recommender system as an arm

00:41:05,060 --> 00:41:08,510
and underlying that system could itself

00:41:07,100 --> 00:41:09,770
be abandoned it could be a collaborative

00:41:08,510 --> 00:41:13,130
filtering it could be a deep learning

00:41:09,770 --> 00:41:15,530
model could be an RNN and at each time

00:41:13,130 --> 00:41:17,090
step we use the the bandit approach to

00:41:15,530 --> 00:41:18,860
decide which of those systems do you

00:41:17,090 --> 00:41:24,740
feed it actually generate the

00:41:18,860 --> 00:41:27,260
recommendation other recent use cases

00:41:24,740 --> 00:41:29,420
come coming especially out of for

00:41:27,260 --> 00:41:31,430
example the Rex's conference in the last

00:41:29,420 --> 00:41:34,250
couple of years is a whole page

00:41:31,430 --> 00:41:35,450
recommendation so and then this is

00:41:34,250 --> 00:41:38,720
something that Netflix is also done

00:41:35,450 --> 00:41:41,390
recently so the idea is that on each

00:41:38,720 --> 00:41:44,060
page in particular the front page of

00:41:41,390 --> 00:41:47,000
let's say a Netflix or a news news site

00:41:44,060 --> 00:41:48,800
or a an online store you have many many

00:41:47,000 --> 00:41:51,680
different ways and options for showing

00:41:48,800 --> 00:41:53,450
recommendations you may have a featured

00:41:51,680 --> 00:41:55,490
tab a recommended tab you may have

00:41:53,450 --> 00:41:56,960
sidebars and even on product pages

00:41:55,490 --> 00:41:59,390
there's many different ways to show

00:41:56,960 --> 00:42:01,160
upsells cross cells you may also like

00:41:59,390 --> 00:42:04,970
and all kinds of different different

00:42:01,160 --> 00:42:06,890
locations so previously what you might

00:42:04,970 --> 00:42:09,860
want to do is use a different model for

00:42:06,890 --> 00:42:12,380
each of those and kind of try to in a

00:42:09,860 --> 00:42:14,390
fairly ad hoc way combine them while

00:42:12,380 --> 00:42:14,540
also trying to avoid let's say showing

00:42:14,390 --> 00:42:16,360
the

00:42:14,540 --> 00:42:19,250
same product twice in different boxes

00:42:16,360 --> 00:42:22,430
and the idea behind the whole page

00:42:19,250 --> 00:42:24,590
recommendation is to is again using a

00:42:22,430 --> 00:42:26,540
steep reinforcement learning concept of

00:42:24,590 --> 00:42:29,360
into n training and training the entire

00:42:26,540 --> 00:42:33,460
system by using reinforcement learning

00:42:29,360 --> 00:42:33,460
to learn that entire page structure

00:42:33,850 --> 00:42:41,270
another approach is augmenting Bandits

00:42:37,670 --> 00:42:44,000
with memory based techniques so we've

00:42:41,270 --> 00:42:45,980
seen that we have we have the idea of

00:42:44,000 --> 00:42:49,160
contextual bandits to have a more

00:42:45,980 --> 00:42:51,560
complex mapping of you know the state to

00:42:49,160 --> 00:42:53,960
the action space but one of the things

00:42:51,560 --> 00:42:55,910
that that can be a challenge is learning

00:42:53,960 --> 00:42:58,040
over time or retaining long-term memory

00:42:55,910 --> 00:43:00,920
so bandit algorithms or Bandit

00:42:58,040 --> 00:43:05,090
approaches are quite good at adapting

00:43:00,920 --> 00:43:07,670
especially to to initial initial

00:43:05,090 --> 00:43:09,440
uncertainty so if you start with those

00:43:07,670 --> 00:43:09,860
four arms that we saw you don't know

00:43:09,440 --> 00:43:11,870
anything

00:43:09,860 --> 00:43:14,570
these principle approaches you see be

00:43:11,870 --> 00:43:17,480
linear Allen SP and so on already good

00:43:14,570 --> 00:43:20,410
for rapidly converging on what you think

00:43:17,480 --> 00:43:22,520
is the the underlying distribution and

00:43:20,410 --> 00:43:25,160
exploring and exporting in a principled

00:43:22,520 --> 00:43:27,710
way and the same is true when a new item

00:43:25,160 --> 00:43:29,720
comes in when you think about a new item

00:43:27,710 --> 00:43:33,740
comes in you have no expectation or no

00:43:29,720 --> 00:43:36,230
estimate of the expected reward but the

00:43:33,740 --> 00:43:38,120
variance is very high so because the

00:43:36,230 --> 00:43:40,580
variance is very high you're you CPU you

00:43:38,120 --> 00:43:42,080
up a conference bound calculation you

00:43:40,580 --> 00:43:43,610
may end up being quite high and that

00:43:42,080 --> 00:43:45,890
means that you end up you're going to

00:43:43,610 --> 00:43:48,020
end up pulling that harm quite a lot so

00:43:45,890 --> 00:43:50,780
when a new new news article comes in or

00:43:48,020 --> 00:43:52,460
a new video comes in the the model is

00:43:50,780 --> 00:43:54,430
automatically going to be able to

00:43:52,460 --> 00:43:56,780
incorporate it and hasn't has a new arm

00:43:54,430 --> 00:43:58,160
explore a little bit and figure out is

00:43:56,780 --> 00:43:59,450
this going to be popular with users or

00:43:58,160 --> 00:44:01,490
not so they're quite good at

00:43:59,450 --> 00:44:03,830
incorporating that but in terms of

00:44:01,490 --> 00:44:07,070
changes in long term dynamics and long

00:44:03,830 --> 00:44:09,830
term learning as we saw bandits are

00:44:07,070 --> 00:44:12,230
really about immediate reward whereas

00:44:09,830 --> 00:44:13,370
the kind of reinforcement learning and

00:44:12,230 --> 00:44:17,150
the deep reinforcement learning is

00:44:13,370 --> 00:44:19,520
around very long term rewards so this is

00:44:17,150 --> 00:44:21,110
a technique for basically combining the

00:44:19,520 --> 00:44:23,660
mail you're augmenting the bandits are

00:44:21,110 --> 00:44:26,020
not that good over for modeling long

00:44:23,660 --> 00:44:27,700
term rewards and long term changes with

00:44:26,020 --> 00:44:34,050
RNN

00:44:27,700 --> 00:44:36,730
techniques okay so before wrapping up

00:44:34,050 --> 00:44:40,060
one final point to make that I want to

00:44:36,730 --> 00:44:42,400
make is that of evaluation so most of

00:44:40,060 --> 00:44:43,570
the time when we are evaluating a

00:44:42,400 --> 00:44:45,369
machine learning model let's take a

00:44:43,570 --> 00:44:47,740
class afire right it's very

00:44:45,369 --> 00:44:49,869
straightforward we have a bunch of

00:44:47,740 --> 00:44:52,780
training data we train our model on the

00:44:49,869 --> 00:44:55,570
labels for evaluation we maybe holdout

00:44:52,780 --> 00:44:57,460
it you know test set and evaluation set

00:44:55,570 --> 00:44:59,859
and then we can compute a metric

00:44:57,460 --> 00:45:01,570
accuracy area under the curve whatever

00:44:59,859 --> 00:45:03,400
the case may be that tells us how well

00:45:01,570 --> 00:45:05,320
we expect that model to generalize how

00:45:03,400 --> 00:45:08,349
well do we expect it to predict given

00:45:05,320 --> 00:45:11,020
new data it hasn't seen and we can use

00:45:08,349 --> 00:45:13,270
cross validation techniques for that and

00:45:11,020 --> 00:45:14,859
we can do the same thing in real life in

00:45:13,270 --> 00:45:17,770
real time we can kind of keep an

00:45:14,859 --> 00:45:19,780
evaluation going and that gives us an

00:45:17,770 --> 00:45:22,510
idea of how well it's performing in the

00:45:19,780 --> 00:45:23,710
wild the problem with these multi-armed

00:45:22,510 --> 00:45:26,890
bandit and reinforcement learning

00:45:23,710 --> 00:45:29,440
techniques is that because of we have

00:45:26,890 --> 00:45:32,040
this number one we have partial

00:45:29,440 --> 00:45:35,710
observability and number two we have

00:45:32,040 --> 00:45:38,650
this idea of a cyclicals system so

00:45:35,710 --> 00:45:41,950
because we we don't know what the best

00:45:38,650 --> 00:45:43,540
outcome was we only know when we pull

00:45:41,950 --> 00:45:44,980
the army we can only pull one arm that

00:45:43,540 --> 00:45:48,010
we chose we don't know what the actual

00:45:44,980 --> 00:45:51,250
best theoretical outcome was which we do

00:45:48,010 --> 00:45:53,410
in supervised learning so evaluating is

00:45:51,250 --> 00:45:55,270
especially offline so often this kind of

00:45:53,410 --> 00:45:58,180
concept of off policy evaluation becomes

00:45:55,270 --> 00:46:01,300
really challenging and the cyclic nature

00:45:58,180 --> 00:46:02,829
of the system means that when we are

00:46:01,300 --> 00:46:06,160
making recommendations we're actually

00:46:02,829 --> 00:46:08,200
influencing the future training data so

00:46:06,160 --> 00:46:09,640
there are techniques I won't get you

00:46:08,200 --> 00:46:10,720
don't have time to go really into them

00:46:09,640 --> 00:46:13,480
with a lot of techniques for dealing

00:46:10,720 --> 00:46:17,560
with us one way of doing it is obviously

00:46:13,480 --> 00:46:18,970
to to keep some random data aside so if

00:46:17,560 --> 00:46:20,859
you think about a be testing most of the

00:46:18,970 --> 00:46:23,890
time you would you just put tests

00:46:20,859 --> 00:46:25,930
between you know 50% graphics he's the

00:46:23,890 --> 00:46:28,690
green button 50% traffics is the red

00:46:25,930 --> 00:46:31,150
button but in this case we may want to

00:46:28,690 --> 00:46:33,010
keep 5% of traffic that sees something

00:46:31,150 --> 00:46:35,170
that's kind of random who has no

00:46:33,010 --> 00:46:38,140
recommendations for example and 95%

00:46:35,170 --> 00:46:40,750
which is which these recommendations and

00:46:38,140 --> 00:46:42,310
then we can use that 5% of traffic for

00:46:40,750 --> 00:46:45,400
training better models and for actually

00:46:42,310 --> 00:46:47,260
doing tree evaluation but this is a you

00:46:45,400 --> 00:46:53,109
know this is a challenge and an ongoing

00:46:47,260 --> 00:46:54,670
research effort so just to wrap up in

00:46:53,109 --> 00:46:55,960
summary your bandit based approaches

00:46:54,670 --> 00:46:58,630
have been used for a long time in the

00:46:55,960 --> 00:47:01,690
recommendation space and there's a huge

00:46:58,630 --> 00:47:05,170
history wealth of research I provide a

00:47:01,690 --> 00:47:07,630
few links yeah but but certainly there's

00:47:05,170 --> 00:47:09,130
that's just scratching the surface the

00:47:07,630 --> 00:47:10,359
more advanced reinforcement learning and

00:47:09,130 --> 00:47:12,280
especially deep reinforcement learning

00:47:10,359 --> 00:47:14,290
is really just getting started is being

00:47:12,280 --> 00:47:18,280
a few could could use cases a few

00:47:14,290 --> 00:47:19,900
applications but I think in in the sense

00:47:18,280 --> 00:47:21,340
that advanced you know deep

00:47:19,900 --> 00:47:23,560
reinforcement learning itself is really

00:47:21,340 --> 00:47:27,010
just getting started the application to

00:47:23,560 --> 00:47:29,400
recommenders has so far been a few in a

00:47:27,010 --> 00:47:31,720
few key papers if you keep applications

00:47:29,400 --> 00:47:33,550
but there's a lot more to go you know to

00:47:31,720 --> 00:47:35,619
come so I did encourage e if you want to

00:47:33,550 --> 00:47:37,270
know more as obviously as I mentioned a

00:47:35,619 --> 00:47:40,450
lot of links but the rexes conferences

00:47:37,270 --> 00:47:45,970
are the ACM rex rex this is where a lot

00:47:40,450 --> 00:47:47,830
of the latest advances can be found yeah

00:47:45,970 --> 00:47:50,910
so thank you very much I just want to

00:47:47,830 --> 00:47:53,380
briefly mention that we're having a free

00:47:50,910 --> 00:47:57,030
digital developer conference for air and

00:47:53,380 --> 00:48:00,099
cloud IBM is offering us for free online

00:47:57,030 --> 00:48:02,290
in North America India Europe and Asia

00:48:00,099 --> 00:48:06,339
during November so if you're interested

00:48:02,290 --> 00:48:08,380
in in your micro services cloud native

00:48:06,339 --> 00:48:10,420
development as well as machine learning

00:48:08,380 --> 00:48:12,070
and AI would encourage you to go and

00:48:10,420 --> 00:48:15,490
register and take a look it's completely

00:48:12,070 --> 00:48:17,650
free and I'm there's a few links at the

00:48:15,490 --> 00:48:20,770
end here so when I first when the slides

00:48:17,650 --> 00:48:22,080
are posted you know it says somewhere

00:48:20,770 --> 00:48:25,440
where you can find more information

00:48:22,080 --> 00:48:25,440
thank you very much

00:48:28,010 --> 00:48:45,380
I think we have time for a few questions

00:48:31,599 --> 00:48:48,140
one when dealing with systems and

00:48:45,380 --> 00:48:50,510
machines I guess is quite simple to get

00:48:48,140 --> 00:48:53,900
feedback in order for the model to be

00:48:50,510 --> 00:48:56,270
trained and but when dealing with humans

00:48:53,900 --> 00:49:01,130
is kind of more challenging might be I

00:48:56,270 --> 00:49:04,099
imagine so are there any tips and tricks

00:49:01,130 --> 00:49:07,820
and your experience that makes this

00:49:04,099 --> 00:49:11,440
feedback gathering more for the humans

00:49:07,820 --> 00:49:13,609
to be more compelling to - to feedback

00:49:11,440 --> 00:49:18,460
yeah I mean that's a really good

00:49:13,609 --> 00:49:21,349
question and yeah if getting getting

00:49:18,460 --> 00:49:24,560
true feedback in in any system certainly

00:49:21,349 --> 00:49:28,040
in recommender systems and machine

00:49:24,560 --> 00:49:29,450
learning is pretty tricky because kind

00:49:28,040 --> 00:49:30,140
of as I mentioned at the beginning or

00:49:29,450 --> 00:49:32,540
close to the beginning

00:49:30,140 --> 00:49:33,890
most of the data you have is implicit so

00:49:32,540 --> 00:49:36,470
you can ask for feedback you know what

00:49:33,890 --> 00:49:38,690
was your review what was your rating but

00:49:36,470 --> 00:49:42,050
this is from from from the human into

00:49:38,690 --> 00:49:43,640
the system but how do you you know

00:49:42,050 --> 00:49:45,500
number one most of the time they

00:49:43,640 --> 00:49:46,790
probably not going to respond number two

00:49:45,500 --> 00:49:49,220
how do you you know how do you align

00:49:46,790 --> 00:49:51,440
incentives you know are they are they're

00:49:49,220 --> 00:49:54,560
just are they giving a true true rating

00:49:51,440 --> 00:49:57,260
a true review yeah a lot of time someone

00:49:54,560 --> 00:49:58,819
may rate they may write something

00:49:57,260 --> 00:50:00,410
differently than what the behavior

00:49:58,819 --> 00:50:03,170
actually tells you so they may write

00:50:00,410 --> 00:50:05,540
something four or five but actually they

00:50:03,170 --> 00:50:06,950
don't the the preferences tell you that

00:50:05,540 --> 00:50:08,599
they don't really like that movie as as

00:50:06,950 --> 00:50:10,280
much as they indicate so these are all

00:50:08,599 --> 00:50:13,609
challengers I mean I don't have any you

00:50:10,280 --> 00:50:15,050
know hard and fast solutions in Center

00:50:13,609 --> 00:50:17,210
incentivizing people to provide feedback

00:50:15,050 --> 00:50:20,960
it's obviously you know one aspect I

00:50:17,210 --> 00:50:24,290
think I think adding explain ability is

00:50:20,960 --> 00:50:25,819
probably another area so again this is

00:50:24,290 --> 00:50:27,369
really the intersection of the human

00:50:25,819 --> 00:50:30,500
feedback and the systems I'd you know

00:50:27,369 --> 00:50:32,270
questions or concept but the more you

00:50:30,500 --> 00:50:34,490
can explain to users I think why they're

00:50:32,270 --> 00:50:36,319
getting recommendations so you know

00:50:34,490 --> 00:50:38,599
you're seeing this because you were to

00:50:36,319 --> 00:50:40,849
review that was positive on this and

00:50:38,599 --> 00:50:41,959
these are the factors yeah I think the

00:50:40,849 --> 00:50:45,119
more that we can explain

00:50:41,959 --> 00:50:46,380
in clear ways to users probably the more

00:50:45,119 --> 00:50:47,849
incentivize they are to actually give

00:50:46,380 --> 00:50:49,140
good feedback that'll let you know

00:50:47,849 --> 00:50:52,049
improve their recommendations so that

00:50:49,140 --> 00:50:53,069
that'll probably be there anything not

00:50:52,049 --> 00:50:56,549
the only but the key thing that I would

00:50:53,069 --> 00:50:58,319
think about is how to make it clear to

00:50:56,549 --> 00:50:59,940
you uses what why they're getting

00:50:58,319 --> 00:51:02,160
recommended things and then that would

00:50:59,940 --> 00:51:04,170
hopefully improve their kind of

00:51:02,160 --> 00:51:07,229
incentives to enhance their own

00:51:04,170 --> 00:51:16,949
experience by providing both consistent

00:51:07,229 --> 00:51:20,130
and you know true feedback it's there

00:51:16,949 --> 00:51:23,640
any is not related to a reinforcement

00:51:20,130 --> 00:51:26,849
learning but to spark more are there any

00:51:23,640 --> 00:51:30,989
reference architecture or of do you know

00:51:26,849 --> 00:51:34,969
machine learning systems implemented

00:51:30,989 --> 00:51:38,009
with spark oh and then we can leverage

00:51:34,969 --> 00:51:39,209
yeah I mean you're talking about machine

00:51:38,009 --> 00:51:40,079
learning systems in general or for

00:51:39,209 --> 00:51:42,449
particular application like

00:51:40,079 --> 00:51:45,599
recommendations or yeah in general I

00:51:42,449 --> 00:51:47,969
mean yeah I mean there are there are

00:51:45,599 --> 00:51:50,339
many I guess spark itself as a machine

00:51:47,969 --> 00:51:51,079
learning library spark is integrations

00:51:50,339 --> 00:51:53,969
to tensorflow

00:51:51,079 --> 00:51:55,709
and we you know you can run intensive

00:51:53,969 --> 00:51:59,190
run spark for doing doing those

00:51:55,709 --> 00:52:01,259
libraries doing those applications you

00:51:59,190 --> 00:52:02,609
know if the top of my head we can

00:52:01,259 --> 00:52:04,799
probably talk offline about I love to

00:52:02,609 --> 00:52:06,390
think of some done something that you're

00:52:04,799 --> 00:52:08,309
places to check out and the links but

00:52:06,390 --> 00:52:09,779
there are a lot of resources out there

00:52:08,309 --> 00:52:11,400
for you know how to do machine learning

00:52:09,779 --> 00:52:13,349
on spark how to integrate other machine

00:52:11,400 --> 00:52:15,779
learning systems with spark systems

00:52:13,349 --> 00:52:17,299
built on top of spark Salesforce has

00:52:15,779 --> 00:52:20,699
something for example called

00:52:17,299 --> 00:52:22,019
transmogrify which is an auto ml system

00:52:20,699 --> 00:52:23,759
built on spark so there's a lot out

00:52:22,019 --> 00:52:28,099
there we can probably chat offline to

00:52:23,759 --> 00:52:28,099
get more all right thank you thank you

00:52:30,570 --> 00:52:34,550
[Applause]

00:52:31,420 --> 00:52:34,550

YouTube URL: https://www.youtube.com/watch?v=F4NSTuYggt8


