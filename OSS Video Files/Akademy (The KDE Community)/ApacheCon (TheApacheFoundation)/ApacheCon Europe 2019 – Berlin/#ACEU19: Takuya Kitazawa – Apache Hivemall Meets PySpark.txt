Title: #ACEU19: Takuya Kitazawa – Apache Hivemall Meets PySpark
Publication date: 2019-10-26
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	Full title: Apache Hivemall Meets PySpark: Scalable Machine Learning with Hive, Spark, and Python

More: https://aceu19.apachecon.com/session/apache-hivemall-meets-pyspark-scalable-machine-learning-hive-spark-and-python

Apache Hivemall is a collection of Hive user-defined functions for machine learning (ML). The tool enables us to solve a wide variety of ML-related problems through the scalable SQL-like interface to Hive. To give a motivating example, simple regression and classification model can be efficiently trained by just executing 10 lines of a query.

This session demonstrates such Hivemall functionality with a special focus on integration with Apache Spark; the Hivemall contributors have been actively working on Spark integration since the project has entered the Apache Incubator. In particular, we deep-dive into how it works in PySpark. 

In PySpark, SparkSession with Hive support enabled gives direct access to the Hivemall capabilities at each of preprocessing, training, prediction, and evaluation phases. That is, we can simultaneously leverage the scalability of Hive/Spark and flexibility of Python ecosystem. We will eventually see how the combination can be a deeply satisfying way to implement a practical end-to-end ML solution.
Captions: 
	00:00:04,580 --> 00:00:09,760
let's get started and today I'm gonna

00:00:07,670 --> 00:00:12,050
talk about the combination of the

00:00:09,760 --> 00:00:16,279
incubating approach project called

00:00:12,050 --> 00:00:18,080
Apache IVA MO and about no Apache apart

00:00:16,279 --> 00:00:20,150
spark or price burg that's the

00:00:18,080 --> 00:00:21,860
combination and this is Doug here and

00:00:20,150 --> 00:00:25,189
I'm a data science and machine learning

00:00:21,860 --> 00:00:27,890
engineer at arm and also I'm a patch I'm

00:00:25,189 --> 00:00:30,410
on PMC alright

00:00:27,890 --> 00:00:32,660
so main topic I'm gonna talk I'm gonna

00:00:30,410 --> 00:00:35,329
speak in this session is about machine

00:00:32,660 --> 00:00:39,769
learning in query language it sounds

00:00:35,329 --> 00:00:42,530
like weird but it is so think about the

00:00:39,769 --> 00:00:44,149
scenario right there so you have to

00:00:42,530 --> 00:00:47,390
solve a machine learning problem

00:00:44,149 --> 00:00:50,539
a massive dataset storing some you're

00:00:47,390 --> 00:00:53,710
like data warehouse how to overcome the

00:00:50,539 --> 00:00:57,410
problem how you can provide a solution

00:00:53,710 --> 00:00:59,449
in the arity we have to understand a lot

00:00:57,410 --> 00:01:02,659
of things in order to achieve the goal

00:00:59,449 --> 00:01:04,460
for example in order to understand

00:01:02,659 --> 00:01:06,560
machine learning techniques we do need

00:01:04,460 --> 00:01:08,000
to run machine running salary or we have

00:01:06,560 --> 00:01:10,790
to understand the mass or stuff like

00:01:08,000 --> 00:01:12,980
that and also in order to work on the

00:01:10,790 --> 00:01:15,830
massive amount of data we have to make

00:01:12,980 --> 00:01:18,170
sure the scalability in some sense at

00:01:15,830 --> 00:01:19,340
the same time so when you think about

00:01:18,170 --> 00:01:21,590
data warehouse

00:01:19,340 --> 00:01:23,960
what kind of open source software or

00:01:21,590 --> 00:01:26,510
some enterprise software we can use and

00:01:23,960 --> 00:01:28,760
where the data is stored so many

00:01:26,510 --> 00:01:32,300
questions are coming as a result of this

00:01:28,760 --> 00:01:34,580
kind of consideration and what a patch

00:01:32,300 --> 00:01:38,170
high mode does is to provide a solution

00:01:34,580 --> 00:01:41,750
to that kind of situation so basically

00:01:38,170 --> 00:01:43,990
you can turn you can create machine

00:01:41,750 --> 00:01:47,450
learning query or machine learning code

00:01:43,990 --> 00:01:49,700
by lighting 10 lines of queries in the

00:01:47,450 --> 00:01:52,760
scalable manner so once you write this

00:01:49,700 --> 00:01:55,010
kind of a query that what high does is

00:01:52,760 --> 00:01:57,830
to train the machine learning model in

00:01:55,010 --> 00:02:02,480
the scalable manner in inside of the

00:01:57,830 --> 00:02:04,490
hive or spur course America so that's

00:02:02,480 --> 00:02:07,610
the solution what a patch Highmore

00:02:04,490 --> 00:02:09,530
provides you so a patch high mo is an

00:02:07,610 --> 00:02:12,290
open source query based machine learning

00:02:09,530 --> 00:02:16,900
library and it's in comparing from the

00:02:12,290 --> 00:02:19,900
20 60 and also we have six

00:02:16,900 --> 00:02:22,120
and three commenters and we are actively

00:02:19,900 --> 00:02:24,459
working toward graduation from the

00:02:22,120 --> 00:02:26,950
incubating projects so we do need to

00:02:24,459 --> 00:02:31,659
improve documentation and also we have

00:02:26,950 --> 00:02:33,430
to provide one more opportunities or you

00:02:31,659 --> 00:02:38,290
know we have to do a lot of things but

00:02:33,430 --> 00:02:39,730
it's ongoing a project so today I'm

00:02:38,290 --> 00:02:42,040
going to talk about first of all the

00:02:39,730 --> 00:02:44,530
brief introduction of a patch hi mo and

00:02:42,040 --> 00:02:45,849
also after that I'm going to move on to

00:02:44,530 --> 00:02:49,560
the main part of the talk

00:02:45,849 --> 00:02:54,129
so how Highmore works with iceberg or

00:02:49,560 --> 00:02:56,019
simply spark and why the combination of

00:02:54,129 --> 00:02:59,640
high more and the price berg is

00:02:56,019 --> 00:03:01,840
preferable in the practical situations

00:02:59,640 --> 00:03:05,799
so first of all let me start from the

00:03:01,840 --> 00:03:08,349
basics of high mo as you many of you may

00:03:05,799 --> 00:03:12,400
already know a patch hype is the open

00:03:08,349 --> 00:03:14,319
source data warehouse solution and you

00:03:12,400 --> 00:03:16,840
can store the massive amount of a query

00:03:14,319 --> 00:03:19,239
in the scalable manner and you can

00:03:16,840 --> 00:03:22,120
access to the data by lighting a query

00:03:19,239 --> 00:03:24,609
it just like a standard a sequel so you

00:03:22,120 --> 00:03:27,579
cannot procreate able or select join

00:03:24,609 --> 00:03:29,620
group by order by and also it's scalable

00:03:27,579 --> 00:03:34,720
so you can apply the cluster by

00:03:29,620 --> 00:03:38,379
operation on top of that so as the name

00:03:34,720 --> 00:03:40,510
suggests had mo is an extension of the

00:03:38,379 --> 00:03:43,480
hive capability it's basically a

00:03:40,510 --> 00:03:46,510
collection of user-defined functions of

00:03:43,480 --> 00:03:48,250
a patch hive so there are three

00:03:46,510 --> 00:03:50,650
different types of user-defined

00:03:48,250 --> 00:03:52,720
functions in a patch type and one is

00:03:50,650 --> 00:03:55,239
that just simple user-defined function

00:03:52,720 --> 00:03:58,419
which gives you the one-to-one mapping

00:03:55,239 --> 00:04:01,090
from the original table to the new table

00:03:58,419 --> 00:04:04,090
so in case of machine learning for

00:04:01,090 --> 00:04:07,599
example the normalization corresponds to

00:04:04,090 --> 00:04:09,750
this kind of open operation and another

00:04:07,599 --> 00:04:13,060
user-defined function is called you def

00:04:09,750 --> 00:04:15,879
user-defined aggregation function which

00:04:13,060 --> 00:04:18,940
actually aggregates multiple rows into

00:04:15,879 --> 00:04:21,070
single row so for example in case of

00:04:18,940 --> 00:04:23,940
machine learning evaluation could be

00:04:21,070 --> 00:04:27,400
part of that so you have to aggregate or

00:04:23,940 --> 00:04:30,070
combination of the expected output and

00:04:27,400 --> 00:04:30,380
prediction result and eventually you can

00:04:30,070 --> 00:04:33,850
get

00:04:30,380 --> 00:04:36,260
accuracy of prediction or another

00:04:33,850 --> 00:04:39,140
user-defined functions go to you DTF

00:04:36,260 --> 00:04:41,780
user I define the tabular function and

00:04:39,140 --> 00:04:44,720
which gives you the brand-new table as a

00:04:41,780 --> 00:04:46,040
result of specific operation so in the

00:04:44,720 --> 00:04:49,250
context of machine learning that

00:04:46,040 --> 00:04:51,980
training corresponds to that so you can

00:04:49,250 --> 00:04:53,960
feed you know your original record to

00:04:51,980 --> 00:04:56,480
the function and eventually you can

00:04:53,960 --> 00:04:57,760
obtain a brand new table as a machine

00:04:56,480 --> 00:05:00,560
learning model

00:04:57,760 --> 00:05:02,930
so basically Highmore internally

00:05:00,560 --> 00:05:05,180
implement those kind of user-defined

00:05:02,930 --> 00:05:10,400
functions and it's just aggregates and

00:05:05,180 --> 00:05:12,710
packages for your use and I would say

00:05:10,400 --> 00:05:15,410
there are four different characteristics

00:05:12,710 --> 00:05:17,870
basically in high mo and it's easy to

00:05:15,410 --> 00:05:20,470
use and it's scalable and it's versatile

00:05:17,870 --> 00:05:24,320
and also it lands in multi-platform and

00:05:20,470 --> 00:05:26,270
in terms of the first of all maybe let

00:05:24,320 --> 00:05:28,610
me show you some examples in the real

00:05:26,270 --> 00:05:31,970
world and actually this is coming from

00:05:28,610 --> 00:05:34,580
my company and my company is ARM chip

00:05:31,970 --> 00:05:38,630
company and our company is actively

00:05:34,580 --> 00:05:41,090
investing on some IOT industry on that

00:05:38,630 --> 00:05:43,070
point that error is very important and

00:05:41,090 --> 00:05:46,850
machine learning is a crucial portion of

00:05:43,070 --> 00:05:49,760
that for that purpose our internal data

00:05:46,850 --> 00:05:52,820
management platform bundles a patch hi

00:05:49,760 --> 00:05:55,640
moe and also the customers data is

00:05:52,820 --> 00:05:57,770
stored in the scalable manner in our

00:05:55,640 --> 00:06:00,650
customized database management system

00:05:57,770 --> 00:06:03,410
and customers can access the data by

00:06:00,650 --> 00:06:06,710
using either hype or pressed on both our

00:06:03,410 --> 00:06:09,770
query engines and for the hyper part we

00:06:06,710 --> 00:06:11,840
internally pandas a patch hi moe so our

00:06:09,770 --> 00:06:15,610
customers can apply machine learning

00:06:11,840 --> 00:06:19,150
techniques to the data stored in their

00:06:15,610 --> 00:06:22,520
you know their platform or our platform

00:06:19,150 --> 00:06:24,650
or another application is coming from

00:06:22,520 --> 00:06:26,300
the fear double recommender systems so

00:06:24,650 --> 00:06:29,000
photos who have attended the previous

00:06:26,300 --> 00:06:31,250
talk in this room he talked about some

00:06:29,000 --> 00:06:33,520
recommender system techniques and also

00:06:31,250 --> 00:06:35,510
Highmore can be useful for

00:06:33,520 --> 00:06:38,120
recommendations and I actually lost some

00:06:35,510 --> 00:06:39,650
research paper on it and we demonstrated

00:06:38,120 --> 00:06:41,150
how it works in the context of

00:06:39,650 --> 00:06:43,810
recommender systems so you can check it

00:06:41,150 --> 00:06:48,530
out if you are interested in

00:06:43,810 --> 00:06:50,810
or the good thing of high mo is actually

00:06:48,530 --> 00:06:53,480
you can do machine learning by using

00:06:50,810 --> 00:06:56,120
just sequel so the only thing you have

00:06:53,480 --> 00:07:00,860
to know is about you know sequel basics

00:06:56,120 --> 00:07:02,660
on that point it is very easy to to care

00:07:00,860 --> 00:07:05,210
the basic of machine learning techniques

00:07:02,660 --> 00:07:08,840
so it's actually used part of the a

00:07:05,210 --> 00:07:12,380
running which is available on udemy so

00:07:08,840 --> 00:07:14,330
you know the audiences have to know only

00:07:12,380 --> 00:07:15,650
about the sequel so they don't

00:07:14,330 --> 00:07:18,050
necessarily have to understand

00:07:15,650 --> 00:07:20,360
mathematical details or machine learning

00:07:18,050 --> 00:07:22,400
salary or stuff like that you can forget

00:07:20,360 --> 00:07:26,030
about it and how much in learning works

00:07:22,400 --> 00:07:27,950
and how intuitively you know it brands

00:07:26,030 --> 00:07:29,780
that that's the important part as a

00:07:27,950 --> 00:07:33,400
first step if you wanna run machine

00:07:29,780 --> 00:07:37,220
learning so in running Highmore is used

00:07:33,400 --> 00:07:41,180
and in terms of the easy to use and

00:07:37,220 --> 00:07:43,610
scalability like I showed so for example

00:07:41,180 --> 00:07:46,700
if you need to create a scalable

00:07:43,610 --> 00:07:49,910
logistic regression model for example if

00:07:46,700 --> 00:07:52,100
you use Apache mahout I don't know if

00:07:49,910 --> 00:07:52,670
you're using that but if you use a patch

00:07:52,100 --> 00:07:54,860
Mahad

00:07:52,670 --> 00:07:58,070
you have to write you know hundred lines

00:07:54,860 --> 00:08:00,890
of code to make it possible but in case

00:07:58,070 --> 00:08:03,290
of high mo internally everything is

00:08:00,890 --> 00:08:05,600
already bundled so the only thing you

00:08:03,290 --> 00:08:07,610
have to call is actually the bigger

00:08:05,600 --> 00:08:10,520
aggressive a trained logistic regression

00:08:07,610 --> 00:08:14,300
function as a part of the entire your

00:08:10,520 --> 00:08:16,900
sequel and it just completed by ten

00:08:14,300 --> 00:08:23,300
lines of sequel syntax or statement and

00:08:16,900 --> 00:08:26,390
of course it runs in parallel and at the

00:08:23,300 --> 00:08:28,820
same time that you know when you think

00:08:26,390 --> 00:08:31,280
about machine learning it's not only

00:08:28,820 --> 00:08:33,530
about purely machine learning you know

00:08:31,280 --> 00:08:35,570
you have to work on the data itself and

00:08:33,530 --> 00:08:37,910
you need to pre-process the data or

00:08:35,570 --> 00:08:40,340
maybe you need to evaluate the accuracy

00:08:37,910 --> 00:08:42,440
of prediction and eventually maybe you

00:08:40,340 --> 00:08:44,690
need to export the results to the some

00:08:42,440 --> 00:08:47,390
where errors or stuff like that for that

00:08:44,690 --> 00:08:50,450
purpose Highmore also provides multiple

00:08:47,390 --> 00:08:53,240
utility functions to it to the users

00:08:50,450 --> 00:08:55,550
like feature engineering or evaluation

00:08:53,240 --> 00:08:56,570
matrix or they are a bunch of you know

00:08:55,550 --> 00:08:58,820
useful

00:08:56,570 --> 00:09:01,720
la orb extra mop operations are

00:08:58,820 --> 00:09:03,760
available and also a recently we have

00:09:01,720 --> 00:09:06,440
incorporated some sort of JSON

00:09:03,760 --> 00:09:08,840
conversion operations which is kind of

00:09:06,440 --> 00:09:10,940
you know useful for pre-processing or

00:09:08,840 --> 00:09:16,450
exporting the table to the outside of

00:09:10,940 --> 00:09:19,700
hype and one of the most frequently used

00:09:16,450 --> 00:09:23,300
utility function is called each topic a

00:09:19,700 --> 00:09:25,400
the bottom of the slide so think about

00:09:23,300 --> 00:09:28,460
the scenario right you have this kind of

00:09:25,400 --> 00:09:31,310
table so item and user and score in the

00:09:28,460 --> 00:09:32,720
context of recommendation and if you

00:09:31,310 --> 00:09:36,320
want to pick top

00:09:32,720 --> 00:09:39,440
for example top three items for every

00:09:36,320 --> 00:09:42,140
single user what kind of query you have

00:09:39,440 --> 00:09:45,440
to write it is possible as I showed

00:09:42,140 --> 00:09:47,450
above the it's right it is possible but

00:09:45,440 --> 00:09:50,240
it's kind of complicated and also it's

00:09:47,450 --> 00:09:52,400
not that scare internally so when you

00:09:50,240 --> 00:09:54,140
issue the query it actually takes more

00:09:52,400 --> 00:09:56,900
than a day of course it depends on the

00:09:54,140 --> 00:09:59,450
podium of data but on our experimental

00:09:56,900 --> 00:10:02,480
environment it took more than a day

00:09:59,450 --> 00:10:04,760
but when you use hide more UDF each top

00:10:02,480 --> 00:10:08,150
kay it actually finishes in a couple

00:10:04,760 --> 00:10:11,300
hours because it internally bundles the

00:10:08,150 --> 00:10:13,580
bounded priority queue and it makes the

00:10:11,300 --> 00:10:15,860
entire computation much more efficient

00:10:13,580 --> 00:10:17,660
so that kind of utility functions are

00:10:15,860 --> 00:10:20,990
basically available as a part of the

00:10:17,660 --> 00:10:22,460
Highmore package and like I said it is

00:10:20,990 --> 00:10:26,290
the part of the application called

00:10:22,460 --> 00:10:28,700
recommender systems and we have many

00:10:26,290 --> 00:10:31,190
recommendation functions inside of the

00:10:28,700 --> 00:10:33,110
high mode so actually in the field of

00:10:31,190 --> 00:10:35,330
recommender systems there are multiple

00:10:33,110 --> 00:10:38,180
options in order to achieve the call

00:10:35,330 --> 00:10:39,920
let's say the recommendation so how to

00:10:38,180 --> 00:10:41,960
make recommendation there are many

00:10:39,920 --> 00:10:44,050
options and we do support multiple

00:10:41,960 --> 00:10:47,750
options in it

00:10:44,050 --> 00:10:50,360
and not only recommender systems but

00:10:47,750 --> 00:10:52,580
also we provide some natural language

00:10:50,360 --> 00:10:54,650
processing functions or geospatial

00:10:52,580 --> 00:10:57,590
functions so for example you can't

00:10:54,650 --> 00:11:01,430
organize the natural language text into

00:10:57,590 --> 00:11:05,030
every single words and as a result what

00:11:01,430 --> 00:11:07,400
you can do is for example you can apply

00:11:05,030 --> 00:11:09,380
the topic modeling technique which is

00:11:07,400 --> 00:11:10,370
also known as a clustering in the

00:11:09,380 --> 00:11:13,939
context of machine

00:11:10,370 --> 00:11:17,720
or if the data is more like time series

00:11:13,939 --> 00:11:19,670
data we experimentally implemented some

00:11:17,720 --> 00:11:21,949
sort of the anomaly detection or change

00:11:19,670 --> 00:11:24,860
point detection technique so you can

00:11:21,949 --> 00:11:28,040
apply the data store in hive by writing

00:11:24,860 --> 00:11:33,620
simple sequel and the result is kind of

00:11:28,040 --> 00:11:36,920
like much more sophisticated one and in

00:11:33,620 --> 00:11:39,620
terms of the efficiency or scalability

00:11:36,920 --> 00:11:42,709
we also provide some service caching the

00:11:39,620 --> 00:11:47,930
technique so-called which gives you some

00:11:42,709 --> 00:11:50,990
sort of approximate result to accelerate

00:11:47,930 --> 00:11:53,839
the entire computation so for example in

00:11:50,990 --> 00:11:57,110
the context of a seeker when you issued

00:11:53,839 --> 00:11:59,749
a query like select count distinct it

00:11:57,110 --> 00:12:03,740
might take a longer time than you expect

00:11:59,749 --> 00:12:06,889
it but there is a approximated function

00:12:03,740 --> 00:12:09,139
called aprox count distinct it actually

00:12:06,889 --> 00:12:13,370
in ternary text a little bit different

00:12:09,139 --> 00:12:15,829
approach to approximately calculated the

00:12:13,370 --> 00:12:18,319
number of distinct items from the target

00:12:15,829 --> 00:12:21,379
column and eventually that your career

00:12:18,319 --> 00:12:23,059
ends much more faster and also we

00:12:21,379 --> 00:12:26,240
internally bound a broom filtering

00:12:23,059 --> 00:12:29,569
technique for example so there are many

00:12:26,240 --> 00:12:31,249
options to do the you know before

00:12:29,569 --> 00:12:36,199
machine learning or after machine

00:12:31,249 --> 00:12:40,189
learning step and finally Highmore runs

00:12:36,199 --> 00:12:43,279
in the multi platform fashion so you

00:12:40,189 --> 00:12:45,550
know the good thing here is Highmore

00:12:43,279 --> 00:12:48,439
stands on the entire Apache

00:12:45,550 --> 00:12:51,230
ecosystem so it learns on Hadoop

00:12:48,439 --> 00:12:54,769
environment so what you can do is do run

00:12:51,230 --> 00:12:58,279
hive more functions on top of spirk for

00:12:54,769 --> 00:13:00,800
example so this is the hive query like I

00:12:58,279 --> 00:13:04,670
showed before and it can be converted

00:13:00,800 --> 00:13:08,029
into I don't think anybody is already

00:13:04,670 --> 00:13:09,499
using a patch Pig but you know if you

00:13:08,029 --> 00:13:13,160
were interested in a patch Pig

00:13:09,499 --> 00:13:15,459
it could be written like that or this is

00:13:13,160 --> 00:13:18,470
the main part of this topic this talk

00:13:15,459 --> 00:13:23,149
the query or highmaul functions can be

00:13:18,470 --> 00:13:23,779
called from spirt environment if you use

00:13:23,149 --> 00:13:28,879
high

00:13:23,779 --> 00:13:32,050
context so it's Puri spirk code but

00:13:28,879 --> 00:13:36,019
let's move on to the discussion to the

00:13:32,050 --> 00:13:38,480
PI spork pot so like I said hi more

00:13:36,019 --> 00:13:42,350
function can also land on top of spirk

00:13:38,480 --> 00:13:49,209
so it means highmaul function can also

00:13:42,350 --> 00:13:51,709
run on top ice burg as well and

00:13:49,209 --> 00:13:55,540
installation should be very

00:13:51,709 --> 00:13:58,670
straightforward so we provide a jar file

00:13:55,540 --> 00:14:00,949
to the users so the only thing you have

00:13:58,670 --> 00:14:03,499
to do is to download the spurge a file

00:14:00,949 --> 00:14:06,139
so high Morse perk to point something

00:14:03,499 --> 00:14:10,249
currently the latest version is 2.3 and

00:14:06,139 --> 00:14:12,620
2.3 and the 0.5 point 3 2 is the higher

00:14:10,249 --> 00:14:15,379
version and incubating with dependencies

00:14:12,620 --> 00:14:17,959
as the name suggests it already pantos

00:14:15,379 --> 00:14:22,069
everything so once you download the Java

00:14:17,959 --> 00:14:25,370
file you can pass the jar file to their

00:14:22,069 --> 00:14:27,949
pies perks sparcstation configuration

00:14:25,370 --> 00:14:29,750
like that so here's this perk station

00:14:27,949 --> 00:14:33,319
definition and we're gonna create this

00:14:29,750 --> 00:14:36,350
perk session it experimental runs on the

00:14:33,319 --> 00:14:38,540
local environment and the code is taking

00:14:36,350 --> 00:14:41,120
as much as possible and for the

00:14:38,540 --> 00:14:45,139
configuration we can add the configure

00:14:41,120 --> 00:14:48,230
called spur chairs we're gonna pass the

00:14:45,139 --> 00:14:49,490
high more Java file here and the

00:14:48,230 --> 00:14:52,100
important part of this entire

00:14:49,490 --> 00:14:55,759
configuration is here we have to put

00:14:52,100 --> 00:14:58,279
enable hype support and eventually you

00:14:55,759 --> 00:15:01,699
can learn any hyper UDF's

00:14:58,279 --> 00:15:03,920
from your PI spark core so you can you

00:15:01,699 --> 00:15:09,559
know jump between the Python environment

00:15:03,920 --> 00:15:12,740
and spirk scary little environment and

00:15:09,559 --> 00:15:15,139
like I said hi mo is basically the

00:15:12,740 --> 00:15:17,209
collection of user-defined functions so

00:15:15,139 --> 00:15:19,970
you need to load the user-defined

00:15:17,209 --> 00:15:23,089
function to your spark or PI Sparky

00:15:19,970 --> 00:15:25,040
environment somehow and to do that you

00:15:23,089 --> 00:15:27,279
just you know you're the only thing you

00:15:25,040 --> 00:15:30,769
have to do is do issued a single query

00:15:27,279 --> 00:15:33,319
spirk dot sequel and create temporary

00:15:30,769 --> 00:15:35,870
function and for example we have a

00:15:33,319 --> 00:15:38,060
function called hide more version we

00:15:35,870 --> 00:15:40,940
just returns the latest version of the

00:15:38,060 --> 00:15:45,320
current version of the hi mo so let's

00:15:40,940 --> 00:15:49,160
try to load the function from the pass

00:15:45,320 --> 00:15:51,170
of the function UDF and eventually you

00:15:49,160 --> 00:15:54,020
have access to the user-defined function

00:15:51,170 --> 00:15:56,480
so from your sparks session spark dot

00:15:54,020 --> 00:15:59,420
cql and the seeker statement comes here

00:15:56,480 --> 00:16:02,990
select hide more version and you want to

00:15:59,420 --> 00:16:05,300
see the result and you can get the you

00:16:02,990 --> 00:16:08,350
know the result and it says the high

00:16:05,300 --> 00:16:12,529
mode version is 0.5 point to incubating

00:16:08,350 --> 00:16:15,500
and the list of available functions are

00:16:12,529 --> 00:16:17,089
in the high mode repository so if you

00:16:15,500 --> 00:16:22,580
are interested in the you know the

00:16:17,089 --> 00:16:24,560
detail you can see it alright so let's

00:16:22,580 --> 00:16:26,660
take a look at the more practical

00:16:24,560 --> 00:16:27,950
example of the end-to-end machine

00:16:26,660 --> 00:16:30,350
learning workflow from there

00:16:27,950 --> 00:16:32,240
pre-processing to evaluation by using

00:16:30,350 --> 00:16:36,800
the sparks session that we created

00:16:32,240 --> 00:16:39,080
before so in this example I'm gonna take

00:16:36,800 --> 00:16:41,540
the binary classification example for

00:16:39,080 --> 00:16:43,520
the customer churn prediction so this is

00:16:41,540 --> 00:16:46,459
the public data and I'm gonna give the

00:16:43,520 --> 00:16:48,440
link if you're interested but the data

00:16:46,459 --> 00:16:51,680
basically looks like that the bottom of

00:16:48,440 --> 00:16:54,380
the slide so every single low has been

00:16:51,680 --> 00:16:56,930
the phone number and single phone number

00:16:54,380 --> 00:16:59,120
has a bunch of different attributes like

00:16:56,930 --> 00:17:02,000
that so weird is the font number coming

00:16:59,120 --> 00:17:03,680
from and raised area code and depending

00:17:02,000 --> 00:17:06,079
on the usage there are multiple

00:17:03,680 --> 00:17:08,630
attributes in it like dairy from call

00:17:06,079 --> 00:17:10,850
minutes for dairy cores or account

00:17:08,630 --> 00:17:13,339
lengths or if this customer has the

00:17:10,850 --> 00:17:17,660
international plan or not or some factor

00:17:13,339 --> 00:17:19,699
and the final column indicates if this

00:17:17,660 --> 00:17:22,429
phone number has already been churned

00:17:19,699 --> 00:17:25,520
from your service or not so this is the

00:17:22,429 --> 00:17:28,339
data we're going to use and once you get

00:17:25,520 --> 00:17:30,440
the data you can load the data in the

00:17:28,339 --> 00:17:32,240
form of data frame it's actually a good

00:17:30,440 --> 00:17:35,270
thing if your interview

00:17:32,240 --> 00:17:37,790
that's why we are using Python or PI

00:17:35,270 --> 00:17:41,030
spark here right if you're strongly

00:17:37,790 --> 00:17:42,920
focusing on only hype maybe you need to

00:17:41,030 --> 00:17:45,530
write the first of all create table and

00:17:42,920 --> 00:17:48,110
load from CSB and the schema is kind of

00:17:45,530 --> 00:17:49,470
like that it's kind of frustrating but

00:17:48,110 --> 00:17:52,350
if you have

00:17:49,470 --> 00:17:54,539
I am connector to your spark you can

00:17:52,350 --> 00:17:57,000
simply load the data in the form of data

00:17:54,539 --> 00:17:59,309
frame and it's you know visually easy to

00:17:57,000 --> 00:18:02,070
understand and it's easy to load the

00:17:59,309 --> 00:18:06,360
data to your spark environment or hive

00:18:02,070 --> 00:18:09,120
environment so this is the channel text

00:18:06,360 --> 00:18:11,520
the terrorised store in the form of csv

00:18:09,120 --> 00:18:14,460
file so you can simply load data here

00:18:11,520 --> 00:18:16,409
and if you want to do that you can apply

00:18:14,460 --> 00:18:18,630
some you know pre-processing to the

00:18:16,409 --> 00:18:20,520
Terra frame easily right you can modify

00:18:18,630 --> 00:18:24,780
the column name before creating the data

00:18:20,520 --> 00:18:28,409
frame for example it is also flexible if

00:18:24,780 --> 00:18:33,890
you only use type it's not that easy for

00:18:28,409 --> 00:18:33,890
sure so let's start from pre-processing

00:18:34,159 --> 00:18:42,570
so basically the Python code looks like

00:18:37,260 --> 00:18:46,470
that first of all in order to give the

00:18:42,570 --> 00:18:50,460
query itself access to their data frame

00:18:46,470 --> 00:18:53,820
you have to first create a view on the

00:18:50,460 --> 00:18:57,150
table or on the data frame so as the

00:18:53,820 --> 00:18:59,850
name DF we now know the chart data to

00:18:57,150 --> 00:19:02,280
the data frame so now we have to first

00:18:59,850 --> 00:19:05,820
call the create or pre-press temporal

00:19:02,280 --> 00:19:09,270
bill for the Chan data frame and the

00:19:05,820 --> 00:19:12,450
name is char as a result you can get

00:19:09,270 --> 00:19:15,570
access to the chant data from this

00:19:12,450 --> 00:19:19,710
entire sequel statement which is coming

00:19:15,570 --> 00:19:22,650
from hive more functionality so spirt

00:19:19,710 --> 00:19:24,929
dot sequel and inside of the three

00:19:22,650 --> 00:19:27,690
double quotes is actually the sequel

00:19:24,929 --> 00:19:31,590
statement which is issued to hive

00:19:27,690 --> 00:19:34,890
environment and this entire query does

00:19:31,590 --> 00:19:37,169
the pre-processing and eventually you

00:19:34,890 --> 00:19:40,610
can get the pre-processed data in the

00:19:37,169 --> 00:19:45,030
form of data frame DF pre-processed

00:19:40,610 --> 00:19:48,600
and what it does is i'm going to explain

00:19:45,030 --> 00:19:51,330
a little bit about it so basically the

00:19:48,600 --> 00:19:54,510
pre-processing also known as feature

00:19:51,330 --> 00:19:56,340
engineering can be done in the context

00:19:54,510 --> 00:19:58,950
of Hiva more like that

00:19:56,340 --> 00:20:01,840
so basically feature vector is

00:19:58,950 --> 00:20:04,919
represented as a lay

00:20:01,840 --> 00:20:08,230
strings and every single string

00:20:04,919 --> 00:20:12,070
corresponds to a single feature in the

00:20:08,230 --> 00:20:13,779
context of machine learning and in the

00:20:12,070 --> 00:20:16,090
field of machine learning there are two

00:20:13,779 --> 00:20:18,730
different types of features one is

00:20:16,090 --> 00:20:20,860
called qualitative features which

00:20:18,730 --> 00:20:23,820
corresponds to the some numerical

00:20:20,860 --> 00:20:27,100
representation of the attributes like

00:20:23,820 --> 00:20:29,190
maybe the number of cores or in case of

00:20:27,100 --> 00:20:32,080
the charm there are number of cores or

00:20:29,190 --> 00:20:32,650
the minutes of the day code or something

00:20:32,080 --> 00:20:34,779
like that

00:20:32,650 --> 00:20:37,210
that that can be numerically represented

00:20:34,779 --> 00:20:40,179
and another feature is called

00:20:37,210 --> 00:20:42,549
categorical feature which represents

00:20:40,179 --> 00:20:46,179
more active categorical information like

00:20:42,549 --> 00:20:50,260
if this customer has international plan

00:20:46,179 --> 00:20:53,080
or not it's boolean indicator or which

00:20:50,260 --> 00:20:55,450
state this customer is coming from this

00:20:53,080 --> 00:20:58,210
is also you know one of the multiple

00:20:55,450 --> 00:21:01,179
options that's the categorical region so

00:20:58,210 --> 00:21:03,340
for two of these features a higher mode

00:21:01,179 --> 00:21:05,470
provides a special its special function

00:21:03,340 --> 00:21:08,980
named the corner deliver features and

00:21:05,470 --> 00:21:11,260
the categorical features so you simply

00:21:08,980 --> 00:21:14,080
need to pass the name of a feature and

00:21:11,260 --> 00:21:17,620
the corresponding a column name or value

00:21:14,080 --> 00:21:20,890
itself to the second and third arguments

00:21:17,620 --> 00:21:23,500
and eventually you can get a lay of the

00:21:20,890 --> 00:21:26,500
string which actually means the feature

00:21:23,500 --> 00:21:28,960
vector so for the quantity the preacher

00:21:26,500 --> 00:21:32,080
you can get this kind of our price is

00:21:28,960 --> 00:21:35,380
600 and the size is 2.5 this is the

00:21:32,080 --> 00:21:38,440
representation of your data or in case

00:21:35,380 --> 00:21:40,899
of a categorical one you know the gender

00:21:38,440 --> 00:21:42,940
is male and the category product

00:21:40,899 --> 00:21:46,000
category would be book or something like

00:21:42,940 --> 00:21:49,149
that so those kind of pre-processing can

00:21:46,000 --> 00:21:51,549
be easily done by you can by you by

00:21:49,149 --> 00:21:55,929
using the simple functions quantitative

00:21:51,549 --> 00:21:57,700
features or categorical features so let

00:21:55,929 --> 00:22:01,450
me go back to the original the previous

00:21:57,700 --> 00:22:04,330
query issued in the spork session so

00:22:01,450 --> 00:22:06,370
inside of the query we have categorical

00:22:04,330 --> 00:22:08,770
features function and qualitative

00:22:06,370 --> 00:22:11,130
features function and the venturi

00:22:08,770 --> 00:22:13,539
we want to concatenate two of those

00:22:11,130 --> 00:22:14,460
qualitative and categorical feature

00:22:13,539 --> 00:22:16,890
vector into

00:22:14,460 --> 00:22:19,440
single feature vector so that we can

00:22:16,890 --> 00:22:22,860
directly pass the entire feature vector

00:22:19,440 --> 00:22:24,930
to their following training a query so

00:22:22,860 --> 00:22:26,610
what you can obtain as a result of this

00:22:24,930 --> 00:22:28,950
query it looks like that so

00:22:26,610 --> 00:22:31,800
international plan is no and state

00:22:28,950 --> 00:22:34,830
coming from KS area code is 4 1 5 and

00:22:31,800 --> 00:22:37,950
the PMF run is yes and night charge is

00:22:34,830 --> 00:22:40,470
11 point or 1 or stuff like that so you

00:22:37,950 --> 00:22:44,580
know basic attributes representing this

00:22:40,470 --> 00:22:50,040
specific customer is converted into a

00:22:44,580 --> 00:22:52,290
lay of string which is feature vector so

00:22:50,040 --> 00:22:55,140
this is basically the pre-processing for

00:22:52,290 --> 00:22:57,660
the terror and once you pre-process the

00:22:55,140 --> 00:23:01,430
terror the terror can easily be split

00:22:57,660 --> 00:23:06,060
into training set and tested if you want

00:23:01,430 --> 00:23:09,030
so fortunately the spice pork provides

00:23:06,060 --> 00:23:10,740
an interface called the London split so

00:23:09,030 --> 00:23:12,780
you can learn the Mori split every

00:23:10,740 --> 00:23:16,110
single records in the single data frame

00:23:12,780 --> 00:23:20,100
into for example 80% training set and

00:23:16,110 --> 00:23:22,200
20% test set so which gives you you know

00:23:20,100 --> 00:23:25,680
easier implementation for your

00:23:22,200 --> 00:23:28,890
evaluation workflow so now we have DF

00:23:25,680 --> 00:23:31,470
train table and DF tests table so what

00:23:28,890 --> 00:23:33,540
we do from now is to first create a

00:23:31,470 --> 00:23:35,940
prediction model customer churn

00:23:33,540 --> 00:23:38,940
prediction model by using the training

00:23:35,940 --> 00:23:41,850
set and eventually we want to evaluate

00:23:38,940 --> 00:23:48,870
the accuracy of prediction by using the

00:23:41,850 --> 00:23:55,620
holdout read the 20 % DF test records so

00:23:48,870 --> 00:23:58,320
it's time to train the model similarly

00:23:55,620 --> 00:24:01,410
to the before we're gonna first create a

00:23:58,320 --> 00:24:03,600
view for the training table so for the

00:24:01,410 --> 00:24:06,420
DF Train data frame we have to create a

00:24:03,600 --> 00:24:10,280
bill named the Train in order to issue a

00:24:06,420 --> 00:24:13,230
query and here we also provide a query

00:24:10,280 --> 00:24:15,750
spurted sequel and this time

00:24:13,230 --> 00:24:18,270
sequel roxrite death and keep earth

00:24:15,750 --> 00:24:21,600
culture the hive malfunction itself

00:24:18,270 --> 00:24:24,780
corresponds to the line is in brew the

00:24:21,600 --> 00:24:26,670
Train crossfire this function joins a

00:24:24,780 --> 00:24:28,830
classification model

00:24:26,670 --> 00:24:31,590
depending on a data passed to the

00:24:28,830 --> 00:24:34,260
function and in tannery that train

00:24:31,590 --> 00:24:37,920
crossfire runs in parallel in the spur

00:24:34,260 --> 00:24:39,930
environment and obviously the training

00:24:37,920 --> 00:24:43,290
is happening in the distribution manner

00:24:39,930 --> 00:24:45,450
distributed manner so eventually you you

00:24:43,290 --> 00:24:46,620
know maybe multiple outputs comes from

00:24:45,450 --> 00:24:49,770
multiple workers

00:24:46,620 --> 00:24:52,290
so finally you have to aggregate the

00:24:49,770 --> 00:24:55,560
prediction without the training result

00:24:52,290 --> 00:24:58,020
into single model so that's why we have

00:24:55,560 --> 00:24:59,820
feature and average and grouping by

00:24:58,020 --> 00:25:10,320
operation outside of the training

00:24:59,820 --> 00:25:12,450
function or statement so let me give a

00:25:10,320 --> 00:25:14,340
little bit more explanation on it so

00:25:12,450 --> 00:25:16,620
like I said the training classifier

00:25:14,340 --> 00:25:19,260
actually trains a classification model

00:25:16,620 --> 00:25:21,270
for the data passed to the first and the

00:25:19,260 --> 00:25:23,130
second argument and first argument

00:25:21,270 --> 00:25:25,350
corresponds to the feature vector that

00:25:23,130 --> 00:25:27,780
we created before and the second

00:25:25,350 --> 00:25:31,020
argument corresponds to the label which

00:25:27,780 --> 00:25:33,030
means the solution or answer passed to

00:25:31,020 --> 00:25:37,080
the machine learning model or machine

00:25:33,030 --> 00:25:39,900
learning algorithm and third argument

00:25:37,080 --> 00:25:42,510
corresponds to the hyper parameters to

00:25:39,900 --> 00:25:44,130
the machine learning model so the

00:25:42,510 --> 00:25:47,250
difficult part of the machine learning

00:25:44,130 --> 00:25:50,310
is to optimize this kind of hyper

00:25:47,250 --> 00:25:53,550
parameters and basically it can be done

00:25:50,310 --> 00:25:57,630
in this argument at a third argument in

00:25:53,550 --> 00:25:59,850
the context of Hiva MO so for example

00:25:57,630 --> 00:26:03,510
you can flexibly choose the loss

00:25:59,850 --> 00:26:06,960
function for the this function this

00:26:03,510 --> 00:26:09,090
option so loss and log loss if you pass

00:26:06,960 --> 00:26:10,770
the log loss option to us

00:26:09,090 --> 00:26:13,170
the machine learning model would be

00:26:10,770 --> 00:26:16,130
logistic regression model but if you

00:26:13,170 --> 00:26:18,570
prefer different loss function you can

00:26:16,130 --> 00:26:20,430
choose the hinge loss or low growth

00:26:18,570 --> 00:26:24,420
squared hinge loss or modified fumaroles

00:26:20,430 --> 00:26:26,400
there are multiple options and also this

00:26:24,420 --> 00:26:28,770
is the example between crossfire but

00:26:26,400 --> 00:26:30,360
like I commanded out we also have a

00:26:28,770 --> 00:26:33,090
function called the trained aggressor

00:26:30,360 --> 00:26:36,570
which actually predicts some value

00:26:33,090 --> 00:26:40,230
itself so crossfire gives the

00:26:36,570 --> 00:26:42,659
probability to you but regressor

00:26:40,230 --> 00:26:44,700
more like the actual predicted barrier

00:26:42,659 --> 00:26:46,740
like number of shares or number of

00:26:44,700 --> 00:26:48,659
course or suffrage that those kind of

00:26:46,740 --> 00:26:51,480
bodies can be predicted by using an

00:26:48,659 --> 00:26:53,809
aggressor but in any cases the usage is

00:26:51,480 --> 00:26:56,190
pretty much similar to that and

00:26:53,809 --> 00:27:00,809
regression case we also provide multiple

00:26:56,190 --> 00:27:03,630
Ross functions and also you can

00:27:00,809 --> 00:27:06,059
configure more by using a different

00:27:03,630 --> 00:27:10,260
options like for example you can

00:27:06,059 --> 00:27:12,809
configure arbitrary optimizers to train

00:27:10,260 --> 00:27:14,610
the model itself so the simplest

00:27:12,809 --> 00:27:18,029
approach is called a stochastic gradient

00:27:14,610 --> 00:27:20,340
descent SGD but if you perform advanced

00:27:18,029 --> 00:27:21,149
one we do support a de gratinéed

00:27:20,340 --> 00:27:25,559
adhirata

00:27:21,149 --> 00:27:27,120
and adam and also in order to make the

00:27:25,559 --> 00:27:30,539
machine learning model itself more

00:27:27,120 --> 00:27:32,450
robust for unforeseen future data there

00:27:30,539 --> 00:27:35,429
is a technique called regularization and

00:27:32,450 --> 00:27:35,850
we do provide multiple options to it as

00:27:35,429 --> 00:27:38,700
well

00:27:35,850 --> 00:27:41,399
l1 legalized Arale to regularize elastic

00:27:38,700 --> 00:27:45,659
net out the year stuff like that and

00:27:41,399 --> 00:27:49,049
also you can configure how to train the

00:27:45,659 --> 00:27:52,230
model itself iteratively so which is

00:27:49,049 --> 00:27:55,409
also called learning rate so you can

00:27:52,230 --> 00:27:59,549
configure those kind of advanced

00:27:55,409 --> 00:28:02,880
parameters inside of the option but

00:27:59,549 --> 00:28:05,279
basically usage itself stayed simple and

00:28:02,880 --> 00:28:06,570
if you're familiar with machine learning

00:28:05,279 --> 00:28:09,210
or if you have some preliminary

00:28:06,570 --> 00:28:11,669
understanding of your data you can do

00:28:09,210 --> 00:28:13,710
more on the sir argument so you know

00:28:11,669 --> 00:28:18,029
that that gives you flexibility but

00:28:13,710 --> 00:28:21,450
stays simple and once you've finalized

00:28:18,029 --> 00:28:25,080
the training what you can see is a table

00:28:21,450 --> 00:28:27,690
so when you use high mo machine learning

00:28:25,080 --> 00:28:29,519
model is always represented in the form

00:28:27,690 --> 00:28:31,200
of a table because it's originally

00:28:29,519 --> 00:28:35,669
coming from hype the data warehousing

00:28:31,200 --> 00:28:39,870
solution and what train crossfire gives

00:28:35,669 --> 00:28:42,269
you is called the linear model which is

00:28:39,870 --> 00:28:45,659
mathematically represented like the dis

00:28:42,269 --> 00:28:48,210
right hand side of the slide and inside

00:28:45,659 --> 00:28:50,610
of the hive you can see the table looks

00:28:48,210 --> 00:28:52,980
like the left hand side of the slide so

00:28:50,610 --> 00:28:56,580
for every single feature

00:28:52,980 --> 00:29:00,000
you can obtain the weight or importance

00:28:56,580 --> 00:29:02,159
of the or coefficient of the every

00:29:00,000 --> 00:29:05,640
single feature as a result of training

00:29:02,159 --> 00:29:07,830
so this is the basic output from the one

00:29:05,640 --> 00:29:12,480
of the most simplest machine learning

00:29:07,830 --> 00:29:15,539
techniques called linear model so what

00:29:12,480 --> 00:29:17,940
it means is okay we we now created a

00:29:15,539 --> 00:29:20,580
customer Chum prediction model on the

00:29:17,940 --> 00:29:24,659
data called DF to channel stuff like

00:29:20,580 --> 00:29:30,470
that and now it says for example okay if

00:29:24,659 --> 00:29:34,289
the customer is in Texas so it has 0.08

00:29:30,470 --> 00:29:38,490
possible not possibility but wait to

00:29:34,289 --> 00:29:42,750
Chan likely to Chan from the service so

00:29:38,490 --> 00:29:46,620
it's relatively higher than for example

00:29:42,750 --> 00:29:48,450
the other states for example yeah it's

00:29:46,620 --> 00:29:53,730
pretty much higher than the others all

00:29:48,450 --> 00:30:01,409
of the state starts training and it's

00:29:53,730 --> 00:30:04,470
time to predict so like I said I'm gonna

00:30:01,409 --> 00:30:07,470
use the test data frame for the

00:30:04,470 --> 00:30:09,960
prediction or evaluation so we're going

00:30:07,470 --> 00:30:12,690
to create a buccal test from the day of

00:30:09,960 --> 00:30:15,090
test data frame and also in order to

00:30:12,690 --> 00:30:17,280
make prediction obviously we have to use

00:30:15,090 --> 00:30:20,840
the machine logic model we created

00:30:17,280 --> 00:30:24,140
before so the model itself is also

00:30:20,840 --> 00:30:27,510
converted into AB you name the model and

00:30:24,140 --> 00:30:30,840
the query looks like that so what it

00:30:27,510 --> 00:30:33,840
basically does is to join the every

00:30:30,840 --> 00:30:38,370
single feature between the test samples

00:30:33,840 --> 00:30:40,549
and the model itself so okay so for this

00:30:38,370 --> 00:30:44,520
customer this customer is living in

00:30:40,549 --> 00:30:47,820
Texas and in the model Texas has the

00:30:44,520 --> 00:30:49,919
weight Oh point or ate and you know what

00:30:47,820 --> 00:30:51,659
what kind of value are discussed on my

00:30:49,919 --> 00:30:54,450
hands for the different attributes and

00:30:51,659 --> 00:30:57,120
what it basically does is to calculate a

00:30:54,450 --> 00:30:59,640
weighted sum about everything of

00:30:57,120 --> 00:31:03,240
attributes depending on the barrier they

00:30:59,640 --> 00:31:05,740
have and it's a logistic regression

00:31:03,240 --> 00:31:08,410
model so we finally need to

00:31:05,740 --> 00:31:13,030
take a sigmoid function so that the

00:31:08,410 --> 00:31:16,620
output converts into probability of the

00:31:13,030 --> 00:31:20,860
likelihood of a child so in this query

00:31:16,620 --> 00:31:23,590
for example sigmoid function or extract

00:31:20,860 --> 00:31:25,450
feature or extract weight these kind of

00:31:23,590 --> 00:31:28,300
functions are coming from home or

00:31:25,450 --> 00:31:31,210
package and it allows you to make

00:31:28,300 --> 00:31:35,260
prediction for the data stored in your

00:31:31,210 --> 00:31:37,270
data base and the final outcome the data

00:31:35,260 --> 00:31:40,420
frame called the DF prediction looks

00:31:37,270 --> 00:31:43,720
like that so for the every single test

00:31:40,420 --> 00:31:45,790
phone numbers we have expected to label

00:31:43,720 --> 00:31:47,530
so we already know if this customer has

00:31:45,790 --> 00:31:49,809
already been child from a service or not

00:31:47,530 --> 00:31:53,470
because we already have a complete data

00:31:49,809 --> 00:31:54,880
set at the beginning and very right hand

00:31:53,470 --> 00:31:57,850
side of the slide we have the

00:31:54,880 --> 00:32:00,370
probability predicted probability for

00:31:57,850 --> 00:32:03,040
the customer which represents if the

00:32:00,370 --> 00:32:06,760
customer is likely to represent the

00:32:03,040 --> 00:32:10,630
service or not so this is the entire

00:32:06,760 --> 00:32:13,480
process to make prediction and once you

00:32:10,630 --> 00:32:15,880
get the prediction result you can apply

00:32:13,480 --> 00:32:19,990
evaluation and you can see how it's

00:32:15,880 --> 00:32:22,059
accurate and the process is more like

00:32:19,990 --> 00:32:24,610
you know the same as before you can

00:32:22,059 --> 00:32:26,890
create a view for the prediction table

00:32:24,610 --> 00:32:28,330
and you can access to the prediction

00:32:26,890 --> 00:32:32,380
table from your sequel

00:32:28,330 --> 00:32:35,440
sparked or sequel and AUC or log loss at

00:32:32,380 --> 00:32:37,720
a specific evaluation matrix used for

00:32:35,440 --> 00:32:39,850
binary classification program and it's

00:32:37,720 --> 00:32:42,640
also coming from highball package and

00:32:39,850 --> 00:32:45,010
you can calculate the accuracy of

00:32:42,640 --> 00:32:48,850
prediction by using this kind of a query

00:32:45,010 --> 00:32:53,020
and it says ALC is 0.63 and log loss is

00:32:48,850 --> 00:32:57,100
0.64 and to be honest I'm not quite sure

00:32:53,020 --> 00:33:01,420
if it's good or bad but commonly

00:32:57,100 --> 00:33:02,980
speaking AUC 0.63 still has some

00:33:01,420 --> 00:33:07,630
potential to improve the accuracy

00:33:02,980 --> 00:33:11,200
because it's not that great so that's

00:33:07,630 --> 00:33:12,880
pretty much everything to you know build

00:33:11,200 --> 00:33:15,309
your entire machine learning model

00:33:12,880 --> 00:33:19,200
machine learning workflow by using pice

00:33:15,309 --> 00:33:20,880
burg and hypo and for training I have

00:33:19,200 --> 00:33:24,419
to emphasize that there are multiple

00:33:20,880 --> 00:33:27,210
options so what high mall offers is not

00:33:24,419 --> 00:33:30,539
only the simple linear classifiers or

00:33:27,210 --> 00:33:32,370
linear digresses Highmore can do more so

00:33:30,539 --> 00:33:34,860
we have this kind of options for

00:33:32,370 --> 00:33:37,200
classification and regression and the

00:33:34,860 --> 00:33:39,649
notable ones are maybe like the

00:33:37,200 --> 00:33:42,269
factorization machines or random forests

00:33:39,649 --> 00:33:45,990
which gives you much more complicated

00:33:42,269 --> 00:33:50,340
model but you can expect more accurate

00:33:45,990 --> 00:33:52,860
prediction result so in case of a

00:33:50,340 --> 00:33:55,889
factorization machines the mathematical

00:33:52,860 --> 00:33:58,440
representation looks like that and the

00:33:55,889 --> 00:34:00,659
model originally coming from the field

00:33:58,440 --> 00:34:03,840
of recommender systems and now it's

00:34:00,659 --> 00:34:08,429
widely used in the context of the

00:34:03,840 --> 00:34:09,960
prediction problems but in the hypo you

00:34:08,429 --> 00:34:11,879
don't necessarily have to understand the

00:34:09,960 --> 00:34:14,909
mathematical details and how it works

00:34:11,879 --> 00:34:17,940
the interface is quite same as the

00:34:14,909 --> 00:34:19,980
previous simplest one so but the only

00:34:17,940 --> 00:34:22,950
difference is the function name and

00:34:19,980 --> 00:34:25,619
options so the function name now is

00:34:22,950 --> 00:34:27,990
called trying FM the shorthand of a

00:34:25,619 --> 00:34:30,210
factorization machines join FM and the

00:34:27,990 --> 00:34:30,480
first and the second argument stays the

00:34:30,210 --> 00:34:33,270
same

00:34:30,480 --> 00:34:36,869
its feature vector in the label so that

00:34:33,270 --> 00:34:40,800
same answer argument is more for the

00:34:36,869 --> 00:34:42,359
configuration so this particular a

00:34:40,800 --> 00:34:45,690
little bit understanding of the machine

00:34:42,359 --> 00:34:47,579
learning this model itself but if you

00:34:45,690 --> 00:34:50,490
have no understanding you can stay here

00:34:47,579 --> 00:34:54,119
Bronk because there are some you know

00:34:50,490 --> 00:34:56,399
default variables in tanneries so you

00:34:54,119 --> 00:34:59,240
can try it without any you know

00:34:56,399 --> 00:35:02,099
preliminary understanding of the model

00:34:59,240 --> 00:35:04,950
and once you run the query you can

00:35:02,099 --> 00:35:07,440
obtain the query of the table as a

00:35:04,950 --> 00:35:10,230
machine learning model but it's not that

00:35:07,440 --> 00:35:14,510
simple as the previous one so this case

00:35:10,230 --> 00:35:16,710
the model looks like that and you know

00:35:14,510 --> 00:35:19,410
the mapping between the original

00:35:16,710 --> 00:35:22,440
mathematical equation and the salting

00:35:19,410 --> 00:35:24,510
table looks like that so if we use the

00:35:22,440 --> 00:35:27,930
wrong we want to understand what it

00:35:24,510 --> 00:35:30,210
means in that case maybe you have to

00:35:27,930 --> 00:35:32,140
deep type into the mathematical

00:35:30,210 --> 00:35:34,540
representation itself

00:35:32,140 --> 00:35:37,240
but if you want to make just simple

00:35:34,540 --> 00:35:40,630
prediction from that there's a function

00:35:37,240 --> 00:35:42,220
called the FM predict so you can simply

00:35:40,630 --> 00:35:44,110
pass these kind of columns to the

00:35:42,220 --> 00:35:49,090
function so you don't need to care about

00:35:44,110 --> 00:35:50,830
the you know how this pIJ is 0.18 and it

00:35:49,090 --> 00:35:52,600
looks like bad or something like that

00:35:50,830 --> 00:35:57,940
you don't need to do this kind of

00:35:52,600 --> 00:36:02,050
discussion or in case of land above

00:35:57,940 --> 00:36:06,760
forest here if you use a random forest

00:36:02,050 --> 00:36:08,440
the hype more requires you to pass the

00:36:06,760 --> 00:36:11,440
single special function called the

00:36:08,440 --> 00:36:15,100
feature hashing which actually

00:36:11,440 --> 00:36:18,010
compresses your original feature vector

00:36:15,100 --> 00:36:20,800
into much more space efficient format

00:36:18,010 --> 00:36:22,960
because unfortunately random forest is

00:36:20,800 --> 00:36:25,240
very complicated machine learning model

00:36:22,960 --> 00:36:27,700
and it consumes a lot of resources

00:36:25,240 --> 00:36:31,180
internally so this kind of compression

00:36:27,700 --> 00:36:33,700
and abstraction is mandatory to make it

00:36:31,180 --> 00:36:36,460
possible and you have to pass the

00:36:33,700 --> 00:36:37,990
original feature vector features into

00:36:36,460 --> 00:36:40,270
the function called the future hashing

00:36:37,990 --> 00:36:43,000
and what it gives you

00:36:40,270 --> 00:36:45,490
looks like that so if the feature vector

00:36:43,000 --> 00:36:48,400
looks like price 600 and category is

00:36:45,490 --> 00:36:51,340
booked the outcome output looks like

00:36:48,400 --> 00:36:54,160
that so the feature name itself is

00:36:51,340 --> 00:36:57,040
actually hashed into some specific

00:36:54,160 --> 00:36:58,720
numbers it gives you the compressed

00:36:57,040 --> 00:37:01,690
representation of your feature vector

00:36:58,720 --> 00:37:04,090
and it has to be passed to the random

00:37:01,690 --> 00:37:07,870
forest function which is called trying

00:37:04,090 --> 00:37:10,420
random forests across fire in hypo and

00:37:07,870 --> 00:37:12,400
the second argument stays simple it's a

00:37:10,420 --> 00:37:14,860
label and third argument also

00:37:12,400 --> 00:37:17,200
corresponds to the some options hyper

00:37:14,860 --> 00:37:20,080
parameters so in case of random forest

00:37:17,200 --> 00:37:22,300
how many trees you wanna have and which

00:37:20,080 --> 00:37:24,940
you know how to generate or how to

00:37:22,300 --> 00:37:26,560
initialize the parameters by using the

00:37:24,940 --> 00:37:32,860
specific cedar body or something like

00:37:26,560 --> 00:37:35,590
that so query stayed simple but model

00:37:32,860 --> 00:37:39,760
looks much more complicated actually the

00:37:35,590 --> 00:37:42,580
model itself is encoded into that kind

00:37:39,760 --> 00:37:44,430
of representation so you have no idea I

00:37:42,580 --> 00:37:48,200
have no idea how to

00:37:44,430 --> 00:37:52,140
understand this model unfortunately but

00:37:48,200 --> 00:37:55,470
here we do provide a function called

00:37:52,140 --> 00:37:58,110
tree export so when you pass the model

00:37:55,470 --> 00:38:01,050
to the tree export function with some

00:37:58,110 --> 00:38:03,330
options you can visually understand how

00:38:01,050 --> 00:38:07,200
the this you know random forest model

00:38:03,330 --> 00:38:10,050
looks like so basically the random

00:38:07,200 --> 00:38:12,750
forest model is known as the tree based

00:38:10,050 --> 00:38:15,450
prediction model so the output looks

00:38:12,750 --> 00:38:18,690
like this decision tree so you can see

00:38:15,450 --> 00:38:20,940
this kind of visualization or if you're

00:38:18,690 --> 00:38:22,830
a web programmer you can see the result

00:38:20,940 --> 00:38:24,900
in the form of JavaScript if-else

00:38:22,830 --> 00:38:27,720
statement and you can't actually embed

00:38:24,900 --> 00:38:32,690
into you know entire if error statement

00:38:27,720 --> 00:38:35,970
into your web application if we want and

00:38:32,690 --> 00:38:37,650
the prediction looks like that I'm not

00:38:35,970 --> 00:38:40,590
going to go you know deep dive into it

00:38:37,650 --> 00:38:42,810
but we also have the special function

00:38:40,590 --> 00:38:45,150
for prediction so tree predict and tree

00:38:42,810 --> 00:38:50,970
ensemble so you don't need to care about

00:38:45,150 --> 00:38:53,070
too much detail of the model itself so

00:38:50,970 --> 00:38:59,160
that's pretty much everything about how

00:38:53,070 --> 00:39:01,320
Highmore works with pie spork so why

00:38:59,160 --> 00:39:06,510
this combination is actually preferable

00:39:01,320 --> 00:39:09,270
I think the answer can be represented in

00:39:06,510 --> 00:39:12,750
the single statement so we can keep

00:39:09,270 --> 00:39:15,900
scalability but Python gives more

00:39:12,750 --> 00:39:18,120
flexibility and your entire machine

00:39:15,900 --> 00:39:22,410
learning workflow becomes much more

00:39:18,120 --> 00:39:24,600
programmable so like I said hype more

00:39:22,410 --> 00:39:26,190
gives you the simplicity of the machine

00:39:24,600 --> 00:39:28,380
learning model so you don't need to have

00:39:26,190 --> 00:39:31,130
you know understand mathematical details

00:39:28,380 --> 00:39:35,820
or stuff like that but interface itself

00:39:31,130 --> 00:39:37,830
stays sequel so you know where is the

00:39:35,820 --> 00:39:40,620
connection between our my sequel

00:39:37,830 --> 00:39:44,490
statement and my application itself on

00:39:40,620 --> 00:39:47,160
that point the Python connector is

00:39:44,490 --> 00:39:50,970
really important to bridge the gap

00:39:47,160 --> 00:39:55,620
between the hive statement and actual

00:39:50,970 --> 00:39:58,140
application so think about the entire

00:39:55,620 --> 00:40:00,869
machine and worker alike I showed before

00:39:58,140 --> 00:40:02,910
so for example for pre-processing of

00:40:00,869 --> 00:40:06,269
course Highmore offers multiple

00:40:02,910 --> 00:40:09,450
functions but in some cases you want to

00:40:06,269 --> 00:40:11,910
do more in that case of course you can

00:40:09,450 --> 00:40:16,980
neutralize the PI sperg functionalities

00:40:11,910 --> 00:40:19,319
for pre-processing for example so the

00:40:16,980 --> 00:40:21,989
example I showed before for the

00:40:19,319 --> 00:40:25,170
pre-processing was super simple one it's

00:40:21,989 --> 00:40:27,539
very basic but in the arity you need to

00:40:25,170 --> 00:40:31,519
do more to make machine learning model

00:40:27,539 --> 00:40:35,160
more accurate and low bust and scalable

00:40:31,519 --> 00:40:37,140
so in that case you have to do more

00:40:35,160 --> 00:40:40,589
actually before jumping into the

00:40:37,140 --> 00:40:43,079
training step so then you can do much

00:40:40,589 --> 00:40:45,779
more complicated thing by using PI spark

00:40:43,079 --> 00:40:49,470
interfaces for example so let's say for

00:40:45,779 --> 00:40:52,049
example I want to normalize the column

00:40:49,470 --> 00:40:55,170
called account links in that case the

00:40:52,049 --> 00:40:57,630
price perk provides multiple functions

00:40:55,170 --> 00:41:00,749
to realize that which is called for

00:40:57,630 --> 00:41:03,839
example mimics scarer which actually

00:41:00,749 --> 00:41:07,410
normalizes the entire barrio within the

00:41:03,839 --> 00:41:09,809
single column and twice per course of

00:41:07,410 --> 00:41:13,559
provider crew function called pipeline

00:41:09,809 --> 00:41:16,680
so you can create a single pipeline by

00:41:13,559 --> 00:41:18,869
concatenating multiple operations for

00:41:16,680 --> 00:41:21,900
the multiple different columns in the

00:41:18,869 --> 00:41:25,319
data frame so those kind of flexible you

00:41:21,900 --> 00:41:28,499
know code can be written for the price

00:41:25,319 --> 00:41:31,380
per data frame and once you do that you

00:41:28,499 --> 00:41:33,599
can obtain the normalized value for D

00:41:31,380 --> 00:41:37,019
for example this account lengths column

00:41:33,599 --> 00:41:40,259
and you know the ones pre-processing can

00:41:37,019 --> 00:41:43,890
be done it's time to make training and

00:41:40,259 --> 00:41:46,559
maybe you can utilize join degrees or

00:41:43,890 --> 00:41:48,930
train crossfire functions coming from

00:41:46,559 --> 00:41:51,539
hive more so this kind of hybrid

00:41:48,930 --> 00:41:56,839
approach can be possible as a result of

00:41:51,539 --> 00:42:01,650
the Pais Berg interface training as well

00:41:56,839 --> 00:42:04,380
so like I said the optimizing the hyper

00:42:01,650 --> 00:42:07,259
parameter which means the third argument

00:42:04,380 --> 00:42:09,119
to the training crossfire function is a

00:42:07,259 --> 00:42:12,270
very difficult part of machine learning

00:42:09,119 --> 00:42:14,700
so you need to try

00:42:12,270 --> 00:42:18,300
or combination of the possible options

00:42:14,700 --> 00:42:20,630
to the single machine learning model to

00:42:18,300 --> 00:42:24,180
achieve in the best of the best

00:42:20,630 --> 00:42:27,000
prediction accuracy in that case maybe

00:42:24,180 --> 00:42:29,720
you want to utilize you know for loop

00:42:27,000 --> 00:42:32,550
statement to try multiple combinations

00:42:29,720 --> 00:42:37,290
in that case you can do like that

00:42:32,550 --> 00:42:40,590
so query can be defined before the loop

00:42:37,290 --> 00:42:42,930
for loop and the possible hyper

00:42:40,590 --> 00:42:46,320
parameters the combinations can be

00:42:42,930 --> 00:42:48,720
defined here and inside of the loop you

00:42:46,320 --> 00:42:51,290
can you know issue the multiple

00:42:48,720 --> 00:42:54,510
different queries programmatically

00:42:51,290 --> 00:42:56,400
there's no way to do like that purely on

00:42:54,510 --> 00:42:58,560
the seeker statement right

00:42:56,400 --> 00:43:01,980
so you need to issue the multiple

00:42:58,560 --> 00:43:03,630
queries by yourself by hand so it's not

00:43:01,980 --> 00:43:08,040
that easy right

00:43:03,630 --> 00:43:10,440
so program ability is very important and

00:43:08,040 --> 00:43:13,260
the prediction in the evaluation as well

00:43:10,440 --> 00:43:16,260
make it programmable is very important

00:43:13,260 --> 00:43:17,910
so in case of evaluation for example of

00:43:16,260 --> 00:43:21,900
course hide more provides multiple

00:43:17,910 --> 00:43:24,870
evaluation metrics but there are more in

00:43:21,900 --> 00:43:28,350
the world and there's no optimal choice

00:43:24,870 --> 00:43:32,220
to measure the accuracy of prediction so

00:43:28,350 --> 00:43:35,580
in that case maybe you wanna try some

00:43:32,220 --> 00:43:37,680
you know customized evaluation metric

00:43:35,580 --> 00:43:41,040
which is coming from the PI's perk a

00:43:37,680 --> 00:43:43,320
memory package or maybe psych to run

00:43:41,040 --> 00:43:45,300
Python package or stuff like that so

00:43:43,320 --> 00:43:49,380
those kind of a function can also be

00:43:45,300 --> 00:43:52,770
applied to the data obtained as a result

00:43:49,380 --> 00:43:57,240
of the prediction done by hive more

00:43:52,770 --> 00:43:59,040
functions for example and the last thing

00:43:57,240 --> 00:44:01,760
I have to strongly emphasize here is

00:43:59,040 --> 00:44:03,960
about visualization because

00:44:01,760 --> 00:44:06,150
visualization is very important to

00:44:03,960 --> 00:44:09,660
understand your data understand your

00:44:06,150 --> 00:44:12,840
machine learning model so but the query

00:44:09,660 --> 00:44:15,870
itself has no way to do that so Python

00:44:12,840 --> 00:44:18,000
interface provides a way to do that so

00:44:15,870 --> 00:44:20,100
for example in case of the trying

00:44:18,000 --> 00:44:23,700
crossfire function the classification

00:44:20,100 --> 00:44:25,710
model we obtain before the model weight

00:44:23,700 --> 00:44:27,840
can be deployed

00:44:25,710 --> 00:44:30,210
visualize like that and you can

00:44:27,840 --> 00:44:33,330
understand which attribute is that it's

00:44:30,210 --> 00:44:38,010
actually important to predict customer

00:44:33,330 --> 00:44:40,140
churn or before pre-processing phase you

00:44:38,010 --> 00:44:42,240
can visualize your entire dataset and

00:44:40,140 --> 00:44:44,460
you can get a more deeper understanding

00:44:42,240 --> 00:44:46,230
over your data and how the data looks

00:44:44,460 --> 00:44:48,660
like and what's the characteristic of

00:44:46,230 --> 00:44:50,220
that and you know what what the ratio

00:44:48,660 --> 00:44:53,250
listening about is well stuff like that

00:44:50,220 --> 00:44:58,320
so everything is possible if you can

00:44:53,250 --> 00:45:00,390
utilize you know Python interfaces but

00:44:58,320 --> 00:45:03,300
for the machine learning part you can

00:45:00,390 --> 00:45:05,100
still utilize to have more functions so

00:45:03,300 --> 00:45:09,720
that that's the company combination you

00:45:05,100 --> 00:45:12,380
can do so I want to say you know from

00:45:09,720 --> 00:45:16,230
EDA which is the shorthand of the

00:45:12,380 --> 00:45:18,500
expiratory data analysis this is the

00:45:16,230 --> 00:45:22,770
step to understand your data to

00:45:18,500 --> 00:45:26,130
production Python actually gives the

00:45:22,770 --> 00:45:29,370
flexibility to the simple hive more

00:45:26,130 --> 00:45:32,850
functionalities and scalability remains

00:45:29,370 --> 00:45:34,380
as is so for example this kind of entire

00:45:32,850 --> 00:45:38,460
machine learning workflow can be

00:45:34,380 --> 00:45:40,620
organized by airflow maybe and for data

00:45:38,460 --> 00:45:44,010
analysis or visualization you can use

00:45:40,620 --> 00:45:46,890
here as a Jupiter notebook and the for

00:45:44,010 --> 00:45:50,100
training or pre-processing or evaluation

00:45:46,890 --> 00:45:52,110
you may be able to use section or once

00:45:50,100 --> 00:45:54,060
you get the entire machine learning

00:45:52,110 --> 00:45:57,360
model or prediction result you can

00:45:54,060 --> 00:45:59,250
deploy that to your application and

00:45:57,360 --> 00:46:01,550
maybe the application is written in

00:45:59,250 --> 00:46:01,550
Frost

00:46:01,820 --> 00:46:10,370
so that's the future you can obtain as a

00:46:05,040 --> 00:46:13,260
result of using a boss hypo and iceberg

00:46:10,370 --> 00:46:18,530
and that's pretty much everything about

00:46:13,260 --> 00:46:23,330
my talk and here github link is here and

00:46:18,530 --> 00:46:27,150
the second link is actually goes to the

00:46:23,330 --> 00:46:31,070
Jupiter not a book that has every single

00:46:27,150 --> 00:46:34,820
code snippet I showed in the slide so

00:46:31,070 --> 00:46:34,820
maybe I can shoulder

00:46:44,480 --> 00:46:52,410
yep so this is how the link looks like

00:46:50,790 --> 00:46:54,330
and the thread must be uploaded after

00:46:52,410 --> 00:46:57,180
the session so you can access the drink

00:46:54,330 --> 00:46:59,640
and basically every single code of

00:46:57,180 --> 00:47:02,760
fragments are in it and thanks to the

00:46:59,640 --> 00:47:05,369
Google collabo environment it's very

00:47:02,760 --> 00:47:08,400
easy to install its perky environment

00:47:05,369 --> 00:47:11,810
and ice perk environment so hide more

00:47:08,400 --> 00:47:11,810
inspiration has to be straightforward

00:47:12,290 --> 00:47:25,140
there we go yep and you can see how it

00:47:20,790 --> 00:47:30,480
looks like and data frame is visualized

00:47:25,140 --> 00:47:34,109
more can be seen in the visually

00:47:30,480 --> 00:47:36,540
stimulating way and like I said so the

00:47:34,109 --> 00:47:40,740
data is represented as the pan despite a

00:47:36,540 --> 00:47:43,170
deep iceberg data frame so data frame

00:47:40,740 --> 00:47:47,190
can be converted into pandas dataframe

00:47:43,170 --> 00:47:49,619
and for example you can understand the

00:47:47,190 --> 00:47:53,150
big picture of the entire data set by

00:47:49,619 --> 00:47:56,609
using pandas dataframe describe function

00:47:53,150 --> 00:47:59,400
so the data is not still rotted it's

00:47:56,609 --> 00:48:01,380
actually connected but you can see the

00:47:59,400 --> 00:48:03,570
you know understanding of the data or

00:48:01,380 --> 00:48:07,280
you can visualize the histogram of the

00:48:03,570 --> 00:48:09,660
specific karim looks like that and

00:48:07,280 --> 00:48:13,230
pre-processing can be done like that so

00:48:09,660 --> 00:48:17,030
these are basically same as the ones i

00:48:13,230 --> 00:48:17,030
showed in the slide

00:48:18,080 --> 00:48:24,320
alright so like this

00:48:28,119 --> 00:48:34,920
so that's it and thank you for listening

00:48:30,710 --> 00:48:40,670
and if you have any questions

00:48:34,920 --> 00:48:40,670

YouTube URL: https://www.youtube.com/watch?v=9FPrb6vYRqI


