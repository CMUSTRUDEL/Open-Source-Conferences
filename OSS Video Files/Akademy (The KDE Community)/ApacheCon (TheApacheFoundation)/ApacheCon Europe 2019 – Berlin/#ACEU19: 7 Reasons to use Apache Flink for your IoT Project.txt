Title: #ACEU19: 7 Reasons to use Apache Flink for your IoT Project
Publication date: 2019-10-31
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	Speaker: Marta Paes Moreira & Jakub Piasecki
More: https://aceu19.apachecon.com/session/7-reasons-use-apache-flink-your-iot-project-how-we-built-real-time-asset-tracking-system

IoT data poses several challenges to data processing systems. The volume of machine-generated data is huge, users expect timely reactions as soon as real-world events are detected by remote sensors, and connections to edge devices often suffer from varying and often high transfer latencies, resulting in data arriving out-of-order. Apache Flink is an open-source stream processor, that addresses the challenges that IoT data presents. Flink applications run in production at a massive scale at many enterprises and companies, including Alibaba, Netflix, and Uber. In this talk, we will discuss seven reasons why Apache Flink is well-suited for your IoT data project and present how we built a system for real-time RFID asset tracking that is backed by Apache Flink.
Captions: 
	00:00:05,259 --> 00:00:11,450
thank you so much thanks so much yeah

00:00:09,070 --> 00:00:14,299
thank you for joining us for this talk

00:00:11,450 --> 00:00:16,790
this is the second time we're giving

00:00:14,299 --> 00:00:19,490
this talk and last time it was in the

00:00:16,790 --> 00:00:22,340
very same room and almost the same time

00:00:19,490 --> 00:00:24,320
so there's very high chance that is

00:00:22,340 --> 00:00:28,420
going to be here again if we ever do it

00:00:24,320 --> 00:00:31,849
in theatre but thanks for coming

00:00:28,420 --> 00:00:33,890
my name is Jack coop I work for company

00:00:31,849 --> 00:00:37,400
called Freeport metrics I will be

00:00:33,890 --> 00:00:39,610
presenting with Martha from Verve Erica

00:00:37,400 --> 00:00:43,220
and basically we had support metrics

00:00:39,610 --> 00:00:46,250
used we used the great project budget

00:00:43,220 --> 00:00:50,510
link that Martha is working on with her

00:00:46,250 --> 00:00:53,119
called colleagues at very very calm and

00:00:50,510 --> 00:00:57,049
what we talked today about first of all

00:00:53,119 --> 00:01:00,680
I will talk a little bit about was so

00:00:57,049 --> 00:01:05,869
specific about IOT applications and I of

00:01:00,680 --> 00:01:10,730
the data then Martha week we will walk

00:01:05,869 --> 00:01:13,670
us through about half link is useful for

00:01:10,730 --> 00:01:15,230
these type of applications then I will

00:01:13,670 --> 00:01:17,780
talk about the specific use case that we

00:01:15,230 --> 00:01:20,510
created our three-part metrics using a

00:01:17,780 --> 00:01:24,920
patch evening and then Marta will follow

00:01:20,510 --> 00:01:29,900
up with additional use cases of people

00:01:24,920 --> 00:01:33,350
using frame for similar projects okay so

00:01:29,900 --> 00:01:37,450
just a brief recap what's so special

00:01:33,350 --> 00:01:40,670
about IOT and they attend applications

00:01:37,450 --> 00:01:44,060
so when creating this application I

00:01:40,670 --> 00:01:47,630
first I thought how much i OT data

00:01:44,060 --> 00:01:50,990
juries and the most specific how much

00:01:47,630 --> 00:01:54,979
IOT data I produce and I like you to

00:01:50,990 --> 00:02:00,920
work together with me for this very

00:01:54,979 --> 00:02:04,310
quick exercise when thinking about how

00:02:00,920 --> 00:02:07,729
much IOT data generated last month so

00:02:04,310 --> 00:02:09,830
maybe you drove through and automated

00:02:07,729 --> 00:02:12,709
talk about at the gateway or maybe you

00:02:09,830 --> 00:02:16,630
used an electric scooter or maybe you

00:02:12,709 --> 00:02:18,440
went from Iran and your pulse and your

00:02:16,630 --> 00:02:22,640
run was recorded

00:02:18,440 --> 00:02:25,010
and stored in the cloud and then thing

00:02:22,640 --> 00:02:34,660
about hammer data I drain generated last

00:02:25,010 --> 00:02:39,110
week so I did a check out hot automated

00:02:34,660 --> 00:02:39,980
I did an automated checkout at the

00:02:39,110 --> 00:02:42,650
retail store

00:02:39,980 --> 00:02:44,870
I used GPS and proud my data is stored

00:02:42,650 --> 00:02:47,780
somewhere for for statistics or other

00:02:44,870 --> 00:02:51,700
purpose and maybe on a smart fish or

00:02:47,780 --> 00:02:54,920
other smart device but I think when you

00:02:51,700 --> 00:02:56,450
what is the multi interesting is when I

00:02:54,920 --> 00:03:01,420
think how much data you produce

00:02:56,450 --> 00:03:03,860
constantly and maybe your house is

00:03:01,420 --> 00:03:06,200
connected to a smart meter the tracks

00:03:03,860 --> 00:03:08,420
your energy usage and every time time

00:03:06,200 --> 00:03:12,290
you turn on your coffee machine this

00:03:08,420 --> 00:03:16,460
spike of energy usage is recorded I may

00:03:12,290 --> 00:03:20,380
be you're constantly tracking your life

00:03:16,460 --> 00:03:22,850
with a with a health tracker or maybe

00:03:20,380 --> 00:03:28,910
unconsciously when your phone is

00:03:22,850 --> 00:03:31,910
connecting to a cellular based our this

00:03:28,910 --> 00:03:35,600
data is stored somewhere and you just

00:03:31,910 --> 00:03:37,400
hopefully per for good purposes so to

00:03:35,600 --> 00:03:39,200
summarize there is a lot of data in

00:03:37,400 --> 00:03:41,709
everyday life but also I'd like to talk

00:03:39,200 --> 00:03:45,200
today about some industrial use cases

00:03:41,709 --> 00:03:48,440
some can be very measuring solar energy

00:03:45,200 --> 00:03:52,760
production on at solar farms maybe some

00:03:48,440 --> 00:03:54,470
of you arrived using a plane here so

00:03:52,760 --> 00:03:57,320
maybe your luggage was tracked

00:03:54,470 --> 00:03:59,209
automatically Airport there may be also

00:03:57,320 --> 00:04:00,700
a use case of sensors tracking things

00:03:59,209 --> 00:04:02,900
very quickly when they go through

00:04:00,700 --> 00:04:05,690
production lines and there are more and

00:04:02,900 --> 00:04:07,430
more use cases some most some of the

00:04:05,690 --> 00:04:10,130
most interesting one or predictive

00:04:07,430 --> 00:04:13,190
maintenance when you track data from

00:04:10,130 --> 00:04:14,660
vehicles or other machines and you try

00:04:13,190 --> 00:04:17,540
to stop them and the maintenance before

00:04:14,660 --> 00:04:19,880
their they break the whole Trenton smart

00:04:17,540 --> 00:04:22,070
be smart buildings and smart cities also

00:04:19,880 --> 00:04:24,350
is very interesting from the data

00:04:22,070 --> 00:04:29,870
perspective as well as all the use cases

00:04:24,350 --> 00:04:32,110
in health care so like we study there's

00:04:29,870 --> 00:04:35,120
a lot of data

00:04:32,110 --> 00:04:38,330
Cisco says that is like five hundred

00:04:35,120 --> 00:04:40,490
zettabytes but this year the number

00:04:38,330 --> 00:04:43,040
seems a bit abstract but I think what is

00:04:40,490 --> 00:04:45,919
important this is more data produced

00:04:43,040 --> 00:04:48,410
that can be stored in all the data

00:04:45,919 --> 00:04:51,160
centers so it means that if we want to

00:04:48,410 --> 00:04:55,639
use the data effectively we need to

00:04:51,160 --> 00:04:57,229
process it before before it is start or

00:04:55,639 --> 00:05:01,850
we don't store it at all just just

00:04:57,229 --> 00:05:04,910
process it another IOT properties it is

00:05:01,850 --> 00:05:08,840
very often connect through a stable

00:05:04,910 --> 00:05:11,330
connection like mobile connection you

00:05:08,840 --> 00:05:14,290
were often use gateway defined devices

00:05:11,330 --> 00:05:17,060
which are devices living on-site and

00:05:14,290 --> 00:05:22,639
serving as an intermediary between IOT

00:05:17,060 --> 00:05:25,400
devices and your data center and those

00:05:22,639 --> 00:05:27,620
gateway devices can use boring because

00:05:25,400 --> 00:05:29,780
additional device and of course of

00:05:27,620 --> 00:05:33,740
course you need to take failures into

00:05:29,780 --> 00:05:35,479
account as well and the last properties

00:05:33,740 --> 00:05:38,479
the data flow is usually continuous

00:05:35,479 --> 00:05:43,970
especially in the when you track

00:05:38,479 --> 00:05:46,070
statistics or let's say you collect data

00:05:43,970 --> 00:05:48,770
from sensor every minute the saw is

00:05:46,070 --> 00:05:57,800
continuous and built-up data volume bits

00:05:48,770 --> 00:06:00,650
are pro with time and going to IOT

00:05:57,800 --> 00:06:03,410
applications I think what's unique about

00:06:00,650 --> 00:06:06,289
them is that first of all they collect

00:06:03,410 --> 00:06:08,840
data from real real world and they need

00:06:06,289 --> 00:06:12,380
to trigger reactions in the real world

00:06:08,840 --> 00:06:15,440
which needs to happen fast to give you

00:06:12,380 --> 00:06:20,120
an example from a system that we built

00:06:15,440 --> 00:06:22,940
which is an asset tracking system you

00:06:20,120 --> 00:06:28,150
can track all the hardware in your

00:06:22,940 --> 00:06:31,550
company and let's say you have and and

00:06:28,150 --> 00:06:33,380
precious laptop that is about to leave

00:06:31,550 --> 00:06:35,300
your facility so you want not know

00:06:33,380 --> 00:06:42,340
sooner than later that you can so can

00:06:35,300 --> 00:06:44,170
react and I will stop here and

00:06:42,340 --> 00:06:47,610
Marta talked about who I think is good

00:06:44,170 --> 00:06:50,820
for this scenarios that I talked about

00:06:47,610 --> 00:06:50,820
thank you

00:06:51,820 --> 00:06:57,790
okay so Yaakov just gave us a quick

00:06:54,940 --> 00:07:01,330
rundown of the basics of IOT data and

00:06:57,790 --> 00:07:05,230
IOT applications I will be trying to

00:07:01,330 --> 00:07:07,060
tell you about Apache plink and what

00:07:05,230 --> 00:07:09,430
characteristics it has that makes it a

00:07:07,060 --> 00:07:15,220
good fit for the requirements that these

00:07:09,430 --> 00:07:17,670
kind of applications have so I see some

00:07:15,220 --> 00:07:20,350
familiar faces from the Flint community

00:07:17,670 --> 00:07:23,800
but for those of you who are not

00:07:20,350 --> 00:07:28,210
familiar with what flink is just going

00:07:23,800 --> 00:07:30,520
to give a very quick overview flink is a

00:07:28,210 --> 00:07:33,400
distributed system for stateful stream

00:07:30,520 --> 00:07:36,280
processing it allows you to perform

00:07:33,400 --> 00:07:39,790
stateful computations over Imboden

00:07:36,280 --> 00:07:42,700
unbounded data streams this in practice

00:07:39,790 --> 00:07:45,400
means that it's capable of processing

00:07:42,700 --> 00:07:50,230
data that is continuously arriving in

00:07:45,400 --> 00:07:53,380
real time and also that it can it can

00:07:50,230 --> 00:07:57,360
remember data in flight that you can use

00:07:53,380 --> 00:08:00,610
in between computations flink is also

00:07:57,360 --> 00:08:04,710
optimized for to do these computations

00:08:00,610 --> 00:08:07,630
in at in-memory speed so very fast and

00:08:04,710 --> 00:08:11,880
pretty much at any scale that you

00:08:07,630 --> 00:08:15,280
require from it and one of its most

00:08:11,880 --> 00:08:19,050
notable properties is that it very

00:08:15,280 --> 00:08:22,090
easily maintains very large state

00:08:19,050 --> 00:08:25,240
application states so and in disaster

00:08:22,090 --> 00:08:27,160
proof manner manner which means that it

00:08:25,240 --> 00:08:28,510
guarantees that your application state

00:08:27,160 --> 00:08:32,080
is never lost

00:08:28,510 --> 00:08:36,730
even in and can always be easily

00:08:32,080 --> 00:08:39,420
recovered even in very bad disaster

00:08:36,730 --> 00:08:43,210
scenarios where everything fails and

00:08:39,420 --> 00:08:48,220
because it's distributed systems likely

00:08:43,210 --> 00:08:51,310
everything will fail at some point so

00:08:48,220 --> 00:08:54,550
now that we're hopefully all on the same

00:08:51,310 --> 00:08:56,000
page about what flink is I'm going to

00:08:54,550 --> 00:08:58,329
give you or

00:08:56,000 --> 00:09:01,819
guide you through several reasons why

00:08:58,329 --> 00:09:08,120
it's valuable in the IOT scenarios that

00:09:01,819 --> 00:09:12,259
Yakov was talking about so for the first

00:09:08,120 --> 00:09:15,259
scenario flink has a lot of native

00:09:12,259 --> 00:09:17,839
properties that allow it to handle and

00:09:15,259 --> 00:09:21,170
process events with very very low

00:09:17,839 --> 00:09:24,439
latency and this becomes very important

00:09:21,170 --> 00:09:27,829
if you consider that like Yaakov told us

00:09:24,439 --> 00:09:30,319
before IOT applications are operating

00:09:27,829 --> 00:09:34,009
continuously and they are already

00:09:30,319 --> 00:09:37,370
affected by latency that comes from

00:09:34,009 --> 00:09:44,300
network transmission or just failures

00:09:37,370 --> 00:09:46,269
that happen at the edge and so yeah and

00:09:44,300 --> 00:09:48,949
so we we should wish to try to minimize

00:09:46,269 --> 00:09:51,560
also the latency that is added on the

00:09:48,949 --> 00:09:56,290
data processing side and so flink is

00:09:51,560 --> 00:10:00,290
able to to achieve these low latencies

00:09:56,290 --> 00:10:02,899
by having state always locally

00:10:00,290 --> 00:10:05,000
maintained and accessed either on heap

00:10:02,899 --> 00:10:10,160
or a spill to disk if your state becomes

00:10:05,000 --> 00:10:12,170
too big so your application state is not

00:10:10,160 --> 00:10:14,569
maintained in some distributed database

00:10:12,170 --> 00:10:16,129
over the network that will add even more

00:10:14,569 --> 00:10:20,420
latency because they're trying to

00:10:16,129 --> 00:10:22,519
communicate with it so the and the state

00:10:20,420 --> 00:10:25,670
is just really co-located with the

00:10:22,519 --> 00:10:28,550
computations and in the machine that is

00:10:25,670 --> 00:10:31,879
processing the events so this gives you

00:10:28,550 --> 00:10:37,100
very very fast state access whenever you

00:10:31,879 --> 00:10:40,639
need to access it and this low latency

00:10:37,100 --> 00:10:43,730
is achieved also through links the way

00:10:40,639 --> 00:10:46,309
Flinx network stack is built it's

00:10:43,730 --> 00:10:48,680
optimized to achieve low latency but

00:10:46,309 --> 00:10:52,189
also offer you at the same time very

00:10:48,680 --> 00:10:54,529
high throughput this is done with a

00:10:52,189 --> 00:10:58,430
little help of something that is called

00:10:54,529 --> 00:11:02,300
credit based flow control which I'm not

00:10:58,430 --> 00:11:04,309
going into in detail but I just linked

00:11:02,300 --> 00:11:07,429
here there's a very good series of blog

00:11:04,309 --> 00:11:09,649
posts on the flink blog about Flinx

00:11:07,429 --> 00:11:15,230
network stack and how this is

00:11:09,649 --> 00:11:16,879
really materialized and another point is

00:11:15,230 --> 00:11:19,939
that the way that flink

00:11:16,879 --> 00:11:22,220
keeps this state consistent is by using

00:11:19,939 --> 00:11:24,410
something that is called a check

00:11:22,220 --> 00:11:27,490
pointing mechanism in the end this is

00:11:24,410 --> 00:11:31,189
just guaranteeing that you take periodic

00:11:27,490 --> 00:11:35,809
snapshots of your application state that

00:11:31,189 --> 00:11:40,100
that it could then use to just recover

00:11:35,809 --> 00:11:43,129
upon failure but this nap shots can be

00:11:40,100 --> 00:11:46,220
taken as synchronously and optionally

00:11:43,129 --> 00:11:48,249
also incrementally so it means that it

00:11:46,220 --> 00:11:51,139
can have a robust consistent application

00:11:48,249 --> 00:11:53,869
without actually adding any overhead

00:11:51,139 --> 00:12:01,939
latency that results just from that

00:11:53,869 --> 00:12:04,910
process okay as I said before blink is a

00:12:01,939 --> 00:12:10,779
distributed system and it can scale to

00:12:04,910 --> 00:12:14,300
very huge data volumes the weight scales

00:12:10,779 --> 00:12:16,220
is basically that streams are

00:12:14,300 --> 00:12:18,379
partitioned to distribute data and

00:12:16,220 --> 00:12:24,980
computation in computation on your

00:12:18,379 --> 00:12:27,850
computing cluster so out of the box all

00:12:24,980 --> 00:12:33,679
your applications are inherently

00:12:27,850 --> 00:12:36,139
parallel and highly scalable there are a

00:12:33,679 --> 00:12:38,089
lot of or so that you have an idea of

00:12:36,139 --> 00:12:40,249
the scale that you can achieve with

00:12:38,089 --> 00:12:44,240
flink there are production use cases

00:12:40,249 --> 00:12:49,100
that run flink on more than 10,000 cores

00:12:44,240 --> 00:12:52,279
and as a actual particular example

00:12:49,100 --> 00:12:55,730
Netflix is processing around 3 billion

00:12:52,279 --> 00:12:58,220
events per day with blink powering a lot

00:12:55,730 --> 00:13:03,290
of their business critical real-time

00:12:58,220 --> 00:13:05,209
applications along with the streams also

00:13:03,290 --> 00:13:07,459
the application state is partitioned

00:13:05,209 --> 00:13:12,319
with it so as I said before everything

00:13:07,459 --> 00:13:13,009
is co-located and the way that state is

00:13:12,319 --> 00:13:15,529
managed

00:13:13,009 --> 00:13:21,199
you can consistently consistently scale

00:13:15,529 --> 00:13:23,839
it to dozens of terabytes of state and

00:13:21,199 --> 00:13:25,579
yeah another advantage is that

00:13:23,839 --> 00:13:27,829
whenever you want to update state

00:13:25,579 --> 00:13:29,630
because everything is co-located you

00:13:27,829 --> 00:13:32,389
don't have a lot of transaction overhead

00:13:29,630 --> 00:13:38,930
just because updates are in the end just

00:13:32,389 --> 00:13:43,310
local operations and yeah even though

00:13:38,930 --> 00:13:45,829
most I Oh tea applications operate in a

00:13:43,310 --> 00:13:49,630
continuous fashion there's also the

00:13:45,829 --> 00:13:52,790
cases where you have load spikes and

00:13:49,630 --> 00:13:54,920
fling helps you with that because it can

00:13:52,790 --> 00:13:57,079
be scaled in and out on demand so you

00:13:54,920 --> 00:13:59,300
can easily adjust things like

00:13:57,079 --> 00:14:04,730
parallelism to ensure that the

00:13:59,300 --> 00:14:11,029
performance is accompanying the load

00:14:04,730 --> 00:14:13,699
that you have in your system so for

00:14:11,029 --> 00:14:16,639
number three

00:14:13,699 --> 00:14:19,190
I think it's widely known that IOT data

00:14:16,639 --> 00:14:22,459
or sensor data in general doesn't have

00:14:19,190 --> 00:14:26,120
the best quality so it can be super

00:14:22,459 --> 00:14:28,819
noisy and a lot of times it's out of

00:14:26,120 --> 00:14:34,430
order or late when it arrives to your

00:14:28,819 --> 00:14:38,930
system and I would say that flink

00:14:34,430 --> 00:14:42,829
handles this very gracefully because it

00:14:38,930 --> 00:14:46,790
has very rich time semantics so flink

00:14:42,829 --> 00:14:48,680
allows you to to process events based on

00:14:46,790 --> 00:14:50,779
the timestamp of when they actually

00:14:48,680 --> 00:14:53,930
occur occurred in the real world which

00:14:50,779 --> 00:14:55,250
is called event time as opposed to when

00:14:53,930 --> 00:14:57,470
they arrive in your machine for

00:14:55,250 --> 00:15:01,130
processing which is known as processing

00:14:57,470 --> 00:15:04,810
time and this means that regardless of

00:15:01,130 --> 00:15:07,250
the order in which the events arrive

00:15:04,810 --> 00:15:09,500
fling can guarantee that you processed

00:15:07,250 --> 00:15:11,420
them in the order that they actually

00:15:09,500 --> 00:15:11,959
occurred in the real world in the first

00:15:11,420 --> 00:15:17,980
place

00:15:11,959 --> 00:15:20,930
and in addition to in addition to the

00:15:17,980 --> 00:15:25,699
event time fling also implements this

00:15:20,930 --> 00:15:28,160
abstraction called watermarks yeah I

00:15:25,699 --> 00:15:32,180
think you can easily go on for hours

00:15:28,160 --> 00:15:33,980
about watermarks but the gist of it is

00:15:32,180 --> 00:15:36,139
that it's something that allows you to

00:15:33,980 --> 00:15:37,190
track the progress of time in your

00:15:36,139 --> 00:15:41,930
system

00:15:37,190 --> 00:15:44,870
it also allows you to say when you're

00:15:41,930 --> 00:15:47,210
done with processing so you're done

00:15:44,870 --> 00:15:50,560
waiting for data that is late and then

00:15:47,210 --> 00:15:54,350
you can just choose to do whatever is

00:15:50,560 --> 00:15:57,440
fit for your SL A's or for your

00:15:54,350 --> 00:15:59,480
completeness requirements so you can

00:15:57,440 --> 00:16:01,670
just wait for late data redirected to

00:15:59,480 --> 00:16:02,630
some other system to be processed in a

00:16:01,670 --> 00:16:04,520
different way

00:16:02,630 --> 00:16:06,920
or you can just if you had that

00:16:04,520 --> 00:16:11,240
privilege you can just discard those

00:16:06,920 --> 00:16:13,210
late events and here you always have to

00:16:11,240 --> 00:16:16,060
know you always have depending on

00:16:13,210 --> 00:16:18,230
depending of on your requirements

00:16:16,060 --> 00:16:20,450
there's always a trade-off between how

00:16:18,230 --> 00:16:23,650
complete your result will be and how

00:16:20,450 --> 00:16:29,260
much latency are adding to the system by

00:16:23,650 --> 00:16:29,260
by allowing this lateness to some extent

00:16:31,600 --> 00:16:39,220
also still regarding a noisy data flink

00:16:35,450 --> 00:16:43,580
has built-in primitives that help you

00:16:39,220 --> 00:16:47,390
smooth this data so things like

00:16:43,580 --> 00:16:49,010
windowing you can very quickly you can

00:16:47,390 --> 00:16:52,820
very quickly define some windowing

00:16:49,010 --> 00:16:55,850
function that lets you average over your

00:16:52,820 --> 00:16:58,940
data or just implement some other kind

00:16:55,850 --> 00:17:09,920
of more sophisticated pre-processing to

00:16:58,940 --> 00:17:12,500
your data some I like I mentioned for

00:17:09,920 --> 00:17:14,930
instance that Netflix they use fling to

00:17:12,500 --> 00:17:19,189
power a lot of applications that are

00:17:14,930 --> 00:17:22,939
business critical and so when you have

00:17:19,189 --> 00:17:25,640
business critical applications you also

00:17:22,939 --> 00:17:29,570
need to ensure that you can guarantee

00:17:25,640 --> 00:17:32,450
very high up times and flink

00:17:29,570 --> 00:17:34,730
as a very robust fault tolerance

00:17:32,450 --> 00:17:40,370
mechanism that backs this up quite

00:17:34,730 --> 00:17:42,440
nicely in production scenarios and yeah

00:17:40,370 --> 00:17:45,980
basically like I mentioned before it has

00:17:42,440 --> 00:17:49,100
a checkpointing mechanism that takes

00:17:45,980 --> 00:17:50,850
consistent snapshots of your application

00:17:49,100 --> 00:17:54,540
state stores it and

00:17:50,850 --> 00:17:58,060
persistent storage like s3 or HDFS and

00:17:54,540 --> 00:18:01,510
when something fails your system breaks

00:17:58,060 --> 00:18:04,210
down it just restored a state and your

00:18:01,510 --> 00:18:07,960
application is recovered like nothing

00:18:04,210 --> 00:18:11,140
happened before with exactly ones state

00:18:07,960 --> 00:18:13,740
guarantees and for some combinations of

00:18:11,140 --> 00:18:17,140
sinks so if for instance you're using

00:18:13,740 --> 00:18:20,530
flank with Kafka you can even achieve

00:18:17,140 --> 00:18:26,500
and to end exactly once guarantees which

00:18:20,530 --> 00:18:29,620
is pretty much all you want I guess in

00:18:26,500 --> 00:18:32,700
addition to this you can also you can

00:18:29,620 --> 00:18:37,720
also set up fling for high availability

00:18:32,700 --> 00:18:41,230
scenarios with many different resource

00:18:37,720 --> 00:18:43,600
managers so even if you have even if

00:18:41,230 --> 00:18:46,380
your master node goes down you can

00:18:43,600 --> 00:18:50,620
guarantee that your application is easy

00:18:46,380 --> 00:18:54,720
to recover and you can just like keep

00:18:50,620 --> 00:18:54,720
processing with very little downtime

00:18:58,200 --> 00:19:06,340
also like Jakob mentioned something that

00:19:02,230 --> 00:19:09,520
is very common in IOT is that you want

00:19:06,340 --> 00:19:13,030
to act upon a certain predefined series

00:19:09,520 --> 00:19:15,610
of events and then trigger some

00:19:13,030 --> 00:19:18,210
downstream actions so you might want to

00:19:15,610 --> 00:19:24,610
generate an alert or you want to trigger

00:19:18,210 --> 00:19:28,570
a cascade of business processes and so

00:19:24,610 --> 00:19:30,790
flink allows you to very easily define a

00:19:28,570 --> 00:19:33,820
regular expression like patterns and

00:19:30,790 --> 00:19:37,900
then match them to the event streams

00:19:33,820 --> 00:19:42,670
that are incoming so if you say hey I

00:19:37,900 --> 00:19:45,970
want to I want to send out an alarm if I

00:19:42,670 --> 00:19:49,360
see even a followed by event B or event

00:19:45,970 --> 00:19:52,840
C and then followed by event D you can

00:19:49,360 --> 00:19:55,360
just define that pattern you deploy your

00:19:52,840 --> 00:19:57,270
application and basically all the

00:19:55,360 --> 00:19:59,680
streams will be continuously monitored

00:19:57,270 --> 00:20:05,820
for looking for this pattern that you

00:19:59,680 --> 00:20:09,129
defined and the good thing is that

00:20:05,820 --> 00:20:11,529
pattern-matching can you kind of have a

00:20:09,129 --> 00:20:14,859
lot of choice in flink you can do it

00:20:11,529 --> 00:20:16,869
more Parramatta CLE just like with Java

00:20:14,859 --> 00:20:20,349
or Scala code just using there's a

00:20:16,869 --> 00:20:23,529
dedicated blink library is called blink

00:20:20,349 --> 00:20:26,259
CEP CP standing for complex event

00:20:23,529 --> 00:20:28,749
processing or you can do it in a more a

00:20:26,259 --> 00:20:31,299
high-level cultural style just using

00:20:28,749 --> 00:20:33,339
streaming sequel because the blink

00:20:31,299 --> 00:20:37,839
sequel semantics were extended with the

00:20:33,339 --> 00:20:40,599
match recognized clause I also linked a

00:20:37,839 --> 00:20:42,659
blog post here that I actually run you

00:20:40,599 --> 00:20:46,959
know if you want to learn more about

00:20:42,659 --> 00:20:51,779
pattern matching is using sequel for

00:20:46,959 --> 00:20:54,450
screaming and yeah because fling has

00:20:51,779 --> 00:20:56,859
more functionalities than just doing

00:20:54,450 --> 00:20:59,589
pattern matching you can very easily

00:20:56,859 --> 00:21:03,219
combine it with more advanced data

00:20:59,589 --> 00:21:07,179
analytics so you can basically cover a

00:21:03,219 --> 00:21:10,769
lot of more complex scenarios where you

00:21:07,179 --> 00:21:10,769
need to do some complex event processing

00:21:12,029 --> 00:21:18,519
yeah if you want to check I know another

00:21:15,039 --> 00:21:22,119
reference but the AWS guys last year

00:21:18,519 --> 00:21:25,899
really is the critical demo that is

00:21:22,119 --> 00:21:29,769
using the fling CP library to for a real

00:21:25,899 --> 00:21:31,259
time bushfire alerting and yeah I can

00:21:29,769 --> 00:21:33,940
also check it out it has really nice

00:21:31,259 --> 00:21:40,209
heat maps and a lot of visualizations

00:21:33,940 --> 00:21:44,349
it's it's a really cool demo okay number

00:21:40,209 --> 00:21:48,249
six is a very quick one blink is well

00:21:44,349 --> 00:21:50,589
connected with external systems so it is

00:21:48,249 --> 00:21:52,749
supported by a large ecosystem of

00:21:50,589 --> 00:21:55,329
connectors that are maintained by the

00:21:52,749 --> 00:21:57,940
Flint community and this means that you

00:21:55,329 --> 00:22:01,589
can basically integrate flink with most

00:21:57,940 --> 00:22:04,269
modern iot architectures so you can

00:22:01,589 --> 00:22:06,969
out-of-the-box just integrate flink with

00:22:04,269 --> 00:22:09,639
popular messaging systems like Kafka or

00:22:06,969 --> 00:22:13,629
Kinesis or pulsar same thing for

00:22:09,639 --> 00:22:14,950
databases and key value stores file

00:22:13,629 --> 00:22:17,350
systems like I mentioned for instance

00:22:14,950 --> 00:22:19,870
for checkpointing and then

00:22:17,350 --> 00:22:23,440
or you can just you know dump your data

00:22:19,870 --> 00:22:26,160
into parque something whatever fits your

00:22:23,440 --> 00:22:26,160
your needs

00:22:27,419 --> 00:22:32,260
okay and lastly a bit of a bold

00:22:30,400 --> 00:22:36,549
statement that's why I'm attributing it

00:22:32,260 --> 00:22:41,650
to Fabian we can argue that data

00:22:36,549 --> 00:22:44,020
streaming is conceptually simple at

00:22:41,650 --> 00:22:49,090
least for IOT data it's a very natural

00:22:44,020 --> 00:22:52,289
way of of processing of processing data

00:22:49,090 --> 00:22:54,840
because what you have is continuous

00:22:52,289 --> 00:22:57,630
streams of events that you want to

00:22:54,840 --> 00:23:01,809
process immediately and you want to

00:22:57,630 --> 00:23:03,909
process them event by event and this is

00:23:01,809 --> 00:23:07,080
something that is native to the way that

00:23:03,909 --> 00:23:12,850
flank the stream processing so it's

00:23:07,080 --> 00:23:14,950
actually a no-brainer young flink also

00:23:12,850 --> 00:23:16,960
gives you very good tools to reason

00:23:14,950 --> 00:23:21,820
about time and ordering like I already

00:23:16,960 --> 00:23:24,730
mentioned and yeah I think in general

00:23:21,820 --> 00:23:27,250
can just make stream processing less

00:23:24,730 --> 00:23:30,960
complex or at least less complex that it

00:23:27,250 --> 00:23:34,120
needs than it needs to be and offers you

00:23:30,960 --> 00:23:36,970
natively a lot of handy built-in

00:23:34,120 --> 00:23:40,480
operators like windowing operators or

00:23:36,970 --> 00:23:42,460
joints or complex event processing but

00:23:40,480 --> 00:23:45,309
it also still gives you access to the

00:23:42,460 --> 00:23:48,090
very core primitives of link like state

00:23:45,309 --> 00:23:50,289
and time that as we we've seen is really

00:23:48,090 --> 00:23:55,990
really important to tame your

00:23:50,289 --> 00:24:01,169
application and you can just use it at

00:23:55,990 --> 00:24:04,809
whatever scale you want because it will

00:24:01,169 --> 00:24:07,840
paralyze and scale applications in the

00:24:04,809 --> 00:24:10,600
pending independent of the size of your

00:24:07,840 --> 00:24:14,980
incoming data it will maintain whatever

00:24:10,600 --> 00:24:17,140
size of consistent state and guarantee

00:24:14,980 --> 00:24:25,000
that your application can always be

00:24:17,140 --> 00:24:27,850
recovered ok so this very long

00:24:25,000 --> 00:24:29,799
introduction I will end it over to Jakob

00:24:27,850 --> 00:24:30,970
again he will tell us about how they

00:24:29,799 --> 00:24:34,770
built an asset

00:24:30,970 --> 00:24:36,820
system with fling at Freeport metrics

00:24:34,770 --> 00:24:39,580
thank you

00:24:36,820 --> 00:24:43,480
so I will talk about our experience with

00:24:39,580 --> 00:24:45,040
Lync and how easy to build a product for

00:24:43,480 --> 00:24:48,070
one of our clients

00:24:45,040 --> 00:24:51,130
I wanna hope we didn't use all the great

00:24:48,070 --> 00:24:53,260
features that Marta presented but I will

00:24:51,130 --> 00:24:57,310
talk about those that I think were the

00:24:53,260 --> 00:25:00,040
most useful in our in our case just a

00:24:57,310 --> 00:25:03,430
very quick about my company I'm working

00:25:00,040 --> 00:25:05,680
for we are a b2b digital product

00:25:03,430 --> 00:25:08,350
development company and what it means we

00:25:05,680 --> 00:25:12,550
create software for companies that sell

00:25:08,350 --> 00:25:15,940
software to our companies that simple we

00:25:12,550 --> 00:25:19,720
were quite a few data at this project so

00:25:15,940 --> 00:25:22,060
far with solar and wind data we did

00:25:19,720 --> 00:25:25,150
quite a few projects in warehouse and

00:25:22,060 --> 00:25:27,130
inventory space we work with automated

00:25:25,150 --> 00:25:29,890
retail kiosks and also with

00:25:27,130 --> 00:25:31,300
sustainability reporting and I think

00:25:29,890 --> 00:25:34,590
what is important to mention that this

00:25:31,300 --> 00:25:39,670
was our first project with playing and

00:25:34,590 --> 00:25:42,460
before that we relied on standard ETL

00:25:39,670 --> 00:25:48,990
tools or custom solutions when we need

00:25:42,460 --> 00:25:52,660
to protect data in real time okay so

00:25:48,990 --> 00:25:57,640
what's what what an asset tracking

00:25:52,660 --> 00:26:02,080
system is first of all what we use it

00:25:57,640 --> 00:26:05,740
for the most basic use case is that is

00:26:02,080 --> 00:26:08,110
inventory management so basically you

00:26:05,740 --> 00:26:09,820
want to know what things you have do you

00:26:08,110 --> 00:26:13,030
have all the things you need and are if

00:26:09,820 --> 00:26:14,800
they are enticed they should be another

00:26:13,030 --> 00:26:19,300
use case is tracking shipments in

00:26:14,800 --> 00:26:21,390
warehouses and I think but inventory

00:26:19,300 --> 00:26:24,850
sorry asset tracking it's not about

00:26:21,390 --> 00:26:28,950
thanks one of the cool examples and

00:26:24,850 --> 00:26:32,560
actually our first pilot was tracking

00:26:28,950 --> 00:26:39,310
patients in hospitals so patients when

00:26:32,560 --> 00:26:41,080
when handed RFID wristbands and their

00:26:39,310 --> 00:26:43,630
progress from medical procedure was

00:26:41,080 --> 00:26:44,530
tracked and it was display on the

00:26:43,630 --> 00:26:47,590
dashboard and

00:26:44,530 --> 00:26:51,520
Ingram's so patients families could stay

00:26:47,590 --> 00:26:53,550
informed about who were they ladies are

00:26:51,520 --> 00:26:59,650
right now and what's happening to them

00:26:53,550 --> 00:27:03,760
and data sources in the inventory

00:26:59,650 --> 00:27:06,490
tracking system that we created are RFID

00:27:03,760 --> 00:27:10,680
tags which are dragged by RFID antenna

00:27:06,490 --> 00:27:10,680
seen in real-time

00:27:10,920 --> 00:27:16,680
operators can also used handheld

00:27:13,300 --> 00:27:19,330
scanners to scan our ID tags as well as

00:27:16,680 --> 00:27:21,060
barkos it and of course there is a user

00:27:19,330 --> 00:27:23,530
interface where you can override

00:27:21,060 --> 00:27:26,770
different values and enter data manually

00:27:23,530 --> 00:27:28,630
the system needs a scale from small

00:27:26,770 --> 00:27:31,660
example with small use cases to

00:27:28,630 --> 00:27:33,880
thousands of hundreds of of assets and

00:27:31,660 --> 00:27:37,000
on the tech side there's a classic

00:27:33,880 --> 00:27:40,330
causing IOT thing with a gateway and we

00:27:37,000 --> 00:27:44,200
had Kafka in the middle and Kafka

00:27:40,330 --> 00:27:46,900
forwarded attitudes link so speaking

00:27:44,200 --> 00:27:51,250
about features that we found very useful

00:27:46,900 --> 00:27:54,760
I think the most important is even time

00:27:51,250 --> 00:28:00,810
in our case we had multiple data sources

00:27:54,760 --> 00:28:05,290
we had gateway devices so all those even

00:28:00,810 --> 00:28:07,390
come to our system at different time and

00:28:05,290 --> 00:28:11,350
we need but we need to process them in

00:28:07,390 --> 00:28:13,120
the in order they occurred so I think

00:28:11,350 --> 00:28:16,300
even time processing thing was very

00:28:13,120 --> 00:28:22,150
useful in our case and another feature

00:28:16,300 --> 00:28:25,270
windowing so antennas can scan RFID tons

00:28:22,150 --> 00:28:27,820
tax constantly and you can imagine a

00:28:25,270 --> 00:28:31,200
situation when there are xx next to each

00:28:27,820 --> 00:28:36,760
other and they overlap so so we used

00:28:31,200 --> 00:28:39,940
windowing to apply some logic that led

00:28:36,760 --> 00:28:42,370
us not to tether mine which and antenna

00:28:39,940 --> 00:28:47,350
we should assign given tag to so again

00:28:42,370 --> 00:28:52,870
thanks to saying windowing so the second

00:28:47,350 --> 00:28:56,050
thing is Flinx state first of all is the

00:28:52,870 --> 00:28:57,130
concept of partitioning by keys which is

00:28:56,050 --> 00:29:00,370
a kind

00:28:57,130 --> 00:29:04,419
natural way to do in clink and what it

00:29:00,370 --> 00:29:07,179
gives to you is the basically your work

00:29:04,419 --> 00:29:13,809
salt can be easily parallelizable in our

00:29:07,179 --> 00:29:16,840
case we grouped event by by tag and when

00:29:13,809 --> 00:29:20,049
you combine even time and when you can

00:29:16,840 --> 00:29:22,390
combine how think handle stayed you can

00:29:20,049 --> 00:29:26,799
build simple state machines we haven't

00:29:22,390 --> 00:29:32,100
used complex event processing that fling

00:29:26,799 --> 00:29:34,809
provides we built our simple business

00:29:32,100 --> 00:29:37,419
processor and which let us do things

00:29:34,809 --> 00:29:40,900
like tracking if a patient is in an

00:29:37,419 --> 00:29:43,120
entire waiting room and when the patient

00:29:40,900 --> 00:29:45,250
enter doctors are pleased and then

00:29:43,120 --> 00:29:48,280
recover area and that could set the

00:29:45,250 --> 00:29:51,370
timer for 10 minutes and then we could

00:29:48,280 --> 00:29:52,960
automatically transition the patient to

00:29:51,370 --> 00:29:58,020
another state so again very useful

00:29:52,960 --> 00:30:01,870
combination of using state and even time

00:29:58,020 --> 00:30:05,919
very briefly our to have experience with

00:30:01,870 --> 00:30:08,350
flaying first of all we could we could

00:30:05,919 --> 00:30:10,150
have quite a few components to build in

00:30:08,350 --> 00:30:14,020
the system even processing was just one

00:30:10,150 --> 00:30:17,200
of them and we thanks to efficiency of

00:30:14,020 --> 00:30:19,659
link we were able to to achieve our goal

00:30:17,200 --> 00:30:23,620
which was building the system a second

00:30:19,659 --> 00:30:27,400
of all we could focus on logic referring

00:30:23,620 --> 00:30:31,960
to what Martha said about the Flint

00:30:27,400 --> 00:30:37,750
being conceptually simple I have to say

00:30:31,960 --> 00:30:41,340
that when you master this concept or

00:30:37,750 --> 00:30:43,539
these familiar with it it's it's

00:30:41,340 --> 00:30:45,960
relative natural way to think about

00:30:43,539 --> 00:30:48,190
processing continuous stream of events

00:30:45,960 --> 00:30:50,620
the first thing is high-level

00:30:48,190 --> 00:30:54,010
abstraction which is which is very good

00:30:50,620 --> 00:30:57,000
for entry barrier and also low level

00:30:54,010 --> 00:30:59,740
abstractions so you can expand given

00:30:57,000 --> 00:31:02,740
areas of your application as you

00:30:59,740 --> 00:31:05,590
progress in development and in your

00:31:02,740 --> 00:31:07,990
ethnic knowledge needless to say

00:31:05,590 --> 00:31:10,930
integrations with Kafka is really good

00:31:07,990 --> 00:31:13,090
in fingers of now so talk

00:31:10,930 --> 00:31:15,640
about challenges and again referring to

00:31:13,090 --> 00:31:17,380
think being conceptually simple for us

00:31:15,640 --> 00:31:20,710
coming from more traditional background

00:31:17,380 --> 00:31:23,890
and I don't just say that it may apply

00:31:20,710 --> 00:31:27,040
to many people entering stream

00:31:23,890 --> 00:31:29,680
processing this was like a totally new

00:31:27,040 --> 00:31:31,590
way of thinking it's just not not just

00:31:29,680 --> 00:31:34,120
another framework that you add to your

00:31:31,590 --> 00:31:36,730
Java spring application and you can

00:31:34,120 --> 00:31:40,000
immediately write think so you need to

00:31:36,730 --> 00:31:47,530
understand concepts to use it

00:31:40,000 --> 00:31:49,300
second thing is that in this model of

00:31:47,530 --> 00:31:51,610
processing CAD data together with

00:31:49,300 --> 00:31:54,280
processing code and you need to really

00:31:51,610 --> 00:31:57,340
think really carefully how you design

00:31:54,280 --> 00:31:58,510
your programs and how it affects the

00:31:57,340 --> 00:32:01,240
rest of the system to give you an

00:31:58,510 --> 00:32:04,900
example is for example how you partition

00:32:01,240 --> 00:32:08,260
your state what do you use what do you

00:32:04,900 --> 00:32:10,780
choose to for your keys might be a

00:32:08,260 --> 00:32:14,410
little bit hard to change later all

00:32:10,780 --> 00:32:16,660
these changes is to be made if the rest

00:32:14,410 --> 00:32:19,110
of the logic already relies on this

00:32:16,660 --> 00:32:21,970
partitioning so in to plan ahead and

00:32:19,110 --> 00:32:23,350
last thing is not valid in the more but

00:32:21,970 --> 00:32:25,270
four years ago

00:32:23,350 --> 00:32:26,830
learning materials were limited now

00:32:25,270 --> 00:32:29,820
there is a great fling book which I

00:32:26,830 --> 00:32:31,930
highly recommend came out this year

00:32:29,820 --> 00:32:34,690
there are trained there are a lot of

00:32:31,930 --> 00:32:37,360
online videos for different conferences

00:32:34,690 --> 00:32:41,370
so I think it's really good position

00:32:37,360 --> 00:32:46,180
right now for people who want to enter

00:32:41,370 --> 00:32:51,400
the free world and that's all from me

00:32:46,180 --> 00:32:53,620
and I pass it over to mark again to talk

00:32:51,400 --> 00:33:03,340
about whatever smart people did about

00:32:53,620 --> 00:33:06,730
IOT okay so now we heard about how ya

00:33:03,340 --> 00:33:10,780
cope in three-part metrics uses flink

00:33:06,730 --> 00:33:13,240
I'm not going to burden you with very

00:33:10,780 --> 00:33:16,750
detail I use cases but I would like to

00:33:13,240 --> 00:33:21,370
mention a couple of other use cases that

00:33:16,750 --> 00:33:24,130
we have with users in production all of

00:33:21,370 --> 00:33:24,850
these I will present or briefly explain

00:33:24,130 --> 00:33:28,150
three

00:33:24,850 --> 00:33:30,850
these are based on existing tox form

00:33:28,150 --> 00:33:33,220
from flint forward so again link

00:33:30,850 --> 00:33:35,920
dropping every slide has a link to the

00:33:33,220 --> 00:33:39,250
talk that the use case actually refers

00:33:35,920 --> 00:33:42,340
to so if you want to really dig into the

00:33:39,250 --> 00:33:44,580
nitty-gritty of how they used Fling for

00:33:42,340 --> 00:33:51,400
their particular use case you can just

00:33:44,580 --> 00:33:57,570
watch the recordings first up is John

00:33:51,400 --> 00:34:01,090
Deere gender is a fortune 500 pretty old

00:33:57,570 --> 00:34:03,310
company that manufactures high machinery

00:34:01,090 --> 00:34:07,390
or heavy machinery for agriculture

00:34:03,310 --> 00:34:09,790
construction and forestry and it uses

00:34:07,390 --> 00:34:11,680
flank to power a data platform that

00:34:09,790 --> 00:34:13,780
helps their customers improve improve

00:34:11,680 --> 00:34:17,770
the efficiency of their farming

00:34:13,780 --> 00:34:21,130
operations so just just so that you have

00:34:17,770 --> 00:34:24,160
a rough idea of the scale at which they

00:34:21,130 --> 00:34:26,350
operate and useful Inc their data

00:34:24,160 --> 00:34:30,460
platform receives around four billion

00:34:26,350 --> 00:34:33,730
events per day and a single machine can

00:34:30,460 --> 00:34:39,040
be producing around 2.5 k measurements

00:34:33,730 --> 00:34:41,650
per per second and they also yeah in the

00:34:39,040 --> 00:34:43,300
in the talk they explain how they face

00:34:41,650 --> 00:34:46,780
some of the challenges that we described

00:34:43,300 --> 00:34:50,010
here so for example having machines

00:34:46,780 --> 00:34:55,240
operating in rural areas that have

00:34:50,010 --> 00:34:58,000
pretty poor network coverage and yeah

00:34:55,240 --> 00:35:03,220
also like the spiky data that results

00:34:58,000 --> 00:35:05,230
from that and they also talk about how

00:35:03,220 --> 00:35:07,600
they use fling to do some pre-processing

00:35:05,230 --> 00:35:10,270
and some cleaning of their of their data

00:35:07,600 --> 00:35:13,000
to then just store it in data like for

00:35:10,270 --> 00:35:16,480
further processing or for further

00:35:13,000 --> 00:35:21,580
analysis and visualization and their

00:35:16,480 --> 00:35:25,290
stack is physically AWS space they

00:35:21,580 --> 00:35:28,930
deploy flink on EMR they use Kinesis for

00:35:25,290 --> 00:35:31,420
their ingestion flink to process and

00:35:28,930 --> 00:35:33,400
then they just dump everything into a

00:35:31,420 --> 00:35:38,710
data lake that is based on s3 and

00:35:33,400 --> 00:35:41,859
dynamodb another use case

00:35:38,710 --> 00:35:45,550
is from a company called here it

00:35:41,859 --> 00:35:49,619
provides mapping and geolocation data

00:35:45,550 --> 00:35:54,670
and other related services they use

00:35:49,619 --> 00:35:56,970
flink to to power something that sounds

00:35:54,670 --> 00:36:00,250
very is a chart is a teracle it's called

00:35:56,970 --> 00:36:03,130
living Maps but in the end what it does

00:36:00,250 --> 00:36:07,119
it's just it collects IOT events from

00:36:03,130 --> 00:36:11,440
multiple car sensors and then they use a

00:36:07,119 --> 00:36:15,820
blank application to enrich static map

00:36:11,440 --> 00:36:19,480
data with this live location context

00:36:15,820 --> 00:36:21,400
data to account for things like road

00:36:19,480 --> 00:36:23,770
conditions accidents parking

00:36:21,400 --> 00:36:28,180
availability of parking spots in real

00:36:23,770 --> 00:36:31,540
time for the customers that subscribe to

00:36:28,180 --> 00:36:39,880
their to some these living Maps in their

00:36:31,540 --> 00:36:43,089
data marketplace and lastly they have

00:36:39,880 --> 00:36:47,550
also track units it's a bit similar to

00:36:43,089 --> 00:36:50,830
John Deere track unit basically provides

00:36:47,550 --> 00:36:55,150
telematics solution for fleet management

00:36:50,830 --> 00:36:58,510
in the construction industry and they

00:36:55,150 --> 00:37:02,170
basically use flink to build a pipeline

00:36:58,510 --> 00:37:05,290
to process any telematic iot data that

00:37:02,170 --> 00:37:07,839
they receive from the machines so they

00:37:05,290 --> 00:37:10,510
track things like where a particular or

00:37:07,839 --> 00:37:13,000
where machines are what they are being

00:37:10,510 --> 00:37:16,480
used for when they are having idle time

00:37:13,000 --> 00:37:18,339
so when they are not being used and they

00:37:16,480 --> 00:37:21,820
use the insights that they derived from

00:37:18,339 --> 00:37:24,520
this to optimize the usage patterns of

00:37:21,820 --> 00:37:27,280
the machines and also to in fear about

00:37:24,520 --> 00:37:30,520
maintenance needs and other similar

00:37:27,280 --> 00:37:34,570
scenarios so they're also the planning

00:37:30,520 --> 00:37:37,450
flank on EMR using Kinesis flowing for

00:37:34,570 --> 00:37:47,500
processing and then as a saint are using

00:37:37,450 --> 00:37:51,790
cassandra okay so to to wrap it up we

00:37:47,500 --> 00:37:52,300
try to explain you how flink meets the

00:37:51,790 --> 00:37:59,290
high demo

00:37:52,300 --> 00:38:01,390
of IOT applications in very brief

00:37:59,290 --> 00:38:05,500
summary it gives you a very reliable

00:38:01,390 --> 00:38:07,860
performance and failure handling and it

00:38:05,500 --> 00:38:11,880
at the same time gives you enough

00:38:07,860 --> 00:38:14,800
flexibility to build very complex

00:38:11,880 --> 00:38:17,260
application logic with the right tools

00:38:14,800 --> 00:38:19,600
that allow you to overcome a lot of the

00:38:17,260 --> 00:38:26,290
bottlenecks that are inherent to working

00:38:19,600 --> 00:38:28,330
with IOT data in IOT pipelines so yeah

00:38:26,290 --> 00:38:30,490
Lync is being used a lot in production

00:38:28,330 --> 00:38:32,380
for dis use cases if you have nice

00:38:30,490 --> 00:38:35,230
project coming up with IOT you should

00:38:32,380 --> 00:38:48,460
maybe give it a try let us know how it

00:38:35,230 --> 00:38:52,200
went and thank you so anyone has

00:38:48,460 --> 00:38:52,200
questions we can start the Q&A

00:38:58,460 --> 00:39:04,760
I come from a very scientific background

00:39:01,369 --> 00:39:08,210
I am curious to know some numbers behind

00:39:04,760 --> 00:39:09,890
your Layton sees so for example the

00:39:08,210 --> 00:39:11,420
facilities that I work with fusion

00:39:09,890 --> 00:39:14,869
machines they have one kiddo its chain

00:39:11,420 --> 00:39:15,890
cutter it's control loops so can I mean

00:39:14,869 --> 00:39:17,900
the stream processing seems ideal

00:39:15,890 --> 00:39:19,520
they've got to look for a host of

00:39:17,900 --> 00:39:22,010
different situations that happen they've

00:39:19,520 --> 00:39:24,170
got to respond to them you know you're

00:39:22,010 --> 00:39:26,029
doing magnetic confinement but the

00:39:24,170 --> 00:39:28,910
latencies are quite low and you need

00:39:26,029 --> 00:39:31,910
reliable ten can hurt control loop is

00:39:28,910 --> 00:39:46,039
this possible is this achievable with

00:39:31,910 --> 00:39:49,640
link seconds so but possibilities in our

00:39:46,039 --> 00:39:52,240
case it was enough right yeah I don't I

00:39:49,640 --> 00:39:55,640
don't think I can tell you exact numbers

00:39:52,240 --> 00:40:00,079
but I think it's one of the things that

00:39:55,640 --> 00:40:02,750
flank is really valued for is that it

00:40:00,079 --> 00:40:04,819
really achieves very very low latency so

00:40:02,750 --> 00:40:08,029
you have a lot of use cases with filling

00:40:04,819 --> 00:40:12,010
for for example fraud detection which

00:40:08,029 --> 00:40:14,690
has very very low latency SSL ace and

00:40:12,010 --> 00:40:17,569
yeah if link is able to cover that

00:40:14,690 --> 00:40:20,869
I mean maybe Stefan can give you more

00:40:17,569 --> 00:40:30,740
precise numbers I think my brain is

00:40:20,869 --> 00:40:33,680
dumped by now okay um I think this

00:40:30,740 --> 00:40:36,190
depends a bit what equity or SLA on that

00:40:33,680 --> 00:40:38,599
latency is I guess right so there's

00:40:36,190 --> 00:40:40,130
let's say when the system is let's say

00:40:38,599 --> 00:40:41,630
running in a happy state not not a

00:40:40,130 --> 00:40:44,750
machine just crashed and it's recovering

00:40:41,630 --> 00:40:46,460
from that and depending on how you flip

00:40:44,750 --> 00:40:48,170
some switches yes you can actually get

00:40:46,460 --> 00:40:50,690
even with multiple aggregations and so

00:40:48,170 --> 00:40:54,230
on like easily single-digit milliseconds

00:40:50,690 --> 00:40:56,470
or single-digit milliseconds yeah that

00:40:54,230 --> 00:40:56,470
works

00:40:57,789 --> 00:41:03,710
no it's it's not built in like for for

00:41:01,130 --> 00:41:05,270
that area so it's it's not something

00:41:03,710 --> 00:41:06,920
that you would use I don't know a high

00:41:05,270 --> 00:41:08,900
frequency trading platform no it's it's

00:41:06,920 --> 00:41:11,599
really it's let's say the the low

00:41:08,900 --> 00:41:12,290
milliseconds range if you if you

00:41:11,599 --> 00:41:13,640
consider sea

00:41:12,290 --> 00:41:16,100
duration where you know machine just

00:41:13,640 --> 00:41:17,630
crashed and it recovers then and this is

00:41:16,100 --> 00:41:19,820
a bit of a tricky question I mean

00:41:17,630 --> 00:41:21,350
usually you know before you actually

00:41:19,820 --> 00:41:23,420
detect a failure you're waiting for a

00:41:21,350 --> 00:41:25,430
few heartbeats to timeout that usually

00:41:23,420 --> 00:41:28,190
means 10 seconds already passed before

00:41:25,430 --> 00:41:30,200
you detected the failure then you have a

00:41:28,190 --> 00:41:33,260
little bit of like rebalancing work and

00:41:30,200 --> 00:41:35,240
so on there is ongoing so that acti like

00:41:33,260 --> 00:41:36,980
after the after the nines where you

00:41:35,240 --> 00:41:38,750
guarantee the low latency it definitely

00:41:36,980 --> 00:41:40,760
goes more to the seconds unless you go

00:41:38,750 --> 00:41:42,770
for like some active active setup or so

00:41:40,760 --> 00:41:44,030
where you basically basically mirror

00:41:42,770 --> 00:41:47,600
other computation then you can actually

00:41:44,030 --> 00:41:48,800
like reliably keep it low yeah it really

00:41:47,600 --> 00:41:51,260
depends very much on how you set it up

00:41:48,800 --> 00:41:52,790
but I think microseconds is I don't know

00:41:51,260 --> 00:41:54,200
maybe nobody's really tried it out I

00:41:52,790 --> 00:41:56,000
would be really curious what the answer

00:41:54,200 --> 00:42:06,770
but that's not anything we've tried so

00:41:56,000 --> 00:42:10,520
far thanks Stefan and thank you actually

00:42:06,770 --> 00:42:13,010
I have a question about Apache beam so

00:42:10,520 --> 00:42:17,720
Apache beam users think as one of the

00:42:13,010 --> 00:42:21,530
runners I would call it a different

00:42:17,720 --> 00:42:25,610
layer of abstraction and my question

00:42:21,530 --> 00:42:30,700
would be if somebody was to decide if if

00:42:25,610 --> 00:42:33,920
use the Apache beam or use a plane thing

00:42:30,700 --> 00:42:39,560
what are the advantages of one or other

00:42:33,920 --> 00:42:42,590
approach again I can give you I think a

00:42:39,560 --> 00:42:45,830
very basic answer but good thing about

00:42:42,590 --> 00:42:55,070
SF conferences that we have PMC for beam

00:42:45,830 --> 00:42:59,000
here so so or at least the PMC for me so

00:42:55,070 --> 00:43:00,830
I think the advantage that I see out of

00:42:59,000 --> 00:43:02,960
the box with beam is just that if you

00:43:00,830 --> 00:43:05,660
need multi-language support so if you

00:43:02,960 --> 00:43:08,000
need to use pythons probably gives you

00:43:05,660 --> 00:43:09,620
an edge because flank is currently just

00:43:08,000 --> 00:43:12,890
being the Python support is being

00:43:09,620 --> 00:43:14,990
reworked right now but other than that I

00:43:12,890 --> 00:43:17,660
don't really I don't really know enough

00:43:14,990 --> 00:43:21,190
about being to to answer that but I can

00:43:17,660 --> 00:43:21,190
gladly pass it to max

00:43:21,780 --> 00:43:30,540
all right I also have another question

00:43:23,730 --> 00:43:32,640
or two to answer your question I think

00:43:30,540 --> 00:43:34,980
matter you already explained it really

00:43:32,640 --> 00:43:36,360
well so there are two main reasons are

00:43:34,980 --> 00:43:39,660
three main reasons why you would use

00:43:36,360 --> 00:43:42,240
beams so one is the unified API so you

00:43:39,660 --> 00:43:44,400
have a API that supports post bets and

00:43:42,240 --> 00:43:46,530
streaming where the string has to api's

00:43:44,400 --> 00:43:48,930
although eventually I guess think we'll

00:43:46,530 --> 00:43:52,650
also have unified API so that argument

00:43:48,930 --> 00:43:54,660
might go away in the future then you can

00:43:52,650 --> 00:43:56,280
choose your execution engine like you

00:43:54,660 --> 00:43:58,050
can run on flink but you can also run on

00:43:56,280 --> 00:44:01,500
other systems like Google Cloud dataflow

00:43:58,050 --> 00:44:03,450
spark and others and you mentioned

00:44:01,500 --> 00:44:06,030
already the multi-language support the

00:44:03,450 --> 00:44:07,320
Pisan support which can be really handy

00:44:06,030 --> 00:44:11,700
depending on whether your organization

00:44:07,320 --> 00:44:20,130
wants to use Python yeah and can I ask

00:44:11,700 --> 00:44:23,340
another question so yeah I was just

00:44:20,130 --> 00:44:25,020
curious about or maybe you touch them

00:44:23,340 --> 00:44:27,810
upon this and I didn't listen carefully

00:44:25,020 --> 00:44:32,820
but when you have data from like these

00:44:27,810 --> 00:44:34,650
machines let's say in a factory and like

00:44:32,820 --> 00:44:36,320
how do you typically read like you have

00:44:34,650 --> 00:44:38,370
a recommendation how to read that data

00:44:36,320 --> 00:44:40,920
because there might be some obscure

00:44:38,370 --> 00:44:42,360
interfaces I know there's some projects

00:44:40,920 --> 00:44:45,870
in Apache also which address this

00:44:42,360 --> 00:44:49,560
problem as far as I know if link doesn't

00:44:45,870 --> 00:44:53,160
have like MQTT support or I don't know

00:44:49,560 --> 00:44:56,390
how these crazy protocols are called how

00:44:53,160 --> 00:44:56,390
do you get the data into flink

00:45:04,300 --> 00:45:14,810
okay so from my experience you know you

00:45:09,610 --> 00:45:19,340
I think it's quite natural to use useful

00:45:14,810 --> 00:45:22,130
behind some kind of event something like

00:45:19,340 --> 00:45:26,500
basic sound like Kafka so how good have

00:45:22,130 --> 00:45:34,130
to get or another similar system so how

00:45:26,500 --> 00:45:37,600
to put events into this into that into

00:45:34,130 --> 00:45:42,350
consumer system it's outside of the

00:45:37,600 --> 00:45:43,970
concern of flink I would say but there

00:45:42,350 --> 00:45:44,990
there are a couple different connections

00:45:43,970 --> 00:45:48,800
but I don't know if there is anything

00:45:44,990 --> 00:45:50,000
for MQTT yeah I don't think it's

00:45:48,800 --> 00:45:54,440
actually something I never thought about

00:45:50,000 --> 00:45:56,660
before so thanks and yeah I think that

00:45:54,440 --> 00:45:59,900
at least all the IOT use cases that I'm

00:45:56,660 --> 00:46:03,740
familiar with use always a queuing

00:45:59,900 --> 00:46:07,250
system in between actually the edge and

00:46:03,740 --> 00:46:09,020
flink so I I don't know how to answer if

00:46:07,250 --> 00:46:12,710
you can just directly in just things to

00:46:09,020 --> 00:46:16,730
think not with the connectors I know but

00:46:12,710 --> 00:46:20,170
I'm also not an expert in IOT so you

00:46:16,730 --> 00:46:20,170
know I don't have a good answer for that

00:46:22,300 --> 00:46:27,770
so I have one for you so same as the

00:46:26,090 --> 00:46:31,220
beam one like it sounds like a

00:46:27,770 --> 00:46:33,590
processing system so like how does it

00:46:31,220 --> 00:46:36,110
differ from apache spark where you can

00:46:33,590 --> 00:46:38,960
again just stream stuff and process it

00:46:36,110 --> 00:46:40,490
and so I think maybe the stateful thing

00:46:38,960 --> 00:46:43,580
is something different but like can you

00:46:40,490 --> 00:46:45,020
touch upon it yeah I think there's

00:46:43,580 --> 00:46:46,730
always someone asking about the

00:46:45,020 --> 00:46:49,760
difference between flink and Kafka

00:46:46,730 --> 00:46:52,790
strings and spark and I mean the the

00:46:49,760 --> 00:46:55,820
answer is that it really really depends

00:46:52,790 --> 00:46:58,640
on your requirements and also depends on

00:46:55,820 --> 00:47:02,390
what you're already using so if you're

00:46:58,640 --> 00:47:05,180
already using Kafka Kafka streams is

00:47:02,390 --> 00:47:06,650
just an API on top of Kafka but if you

00:47:05,180 --> 00:47:10,190
maybe want to use a different queueing

00:47:06,650 --> 00:47:12,320
system then you blink is basically

00:47:10,190 --> 00:47:16,400
compatible with all of them

00:47:12,320 --> 00:47:18,530
and yeah then it just really boils down

00:47:16,400 --> 00:47:20,510
to your requirements if you're Moore if

00:47:18,530 --> 00:47:22,430
you're heavier like on the batch side on

00:47:20,510 --> 00:47:24,920
streaming site if you need low latency

00:47:22,430 --> 00:47:28,160
also how important state is for you

00:47:24,920 --> 00:47:30,050
because if link has a lot of things that

00:47:28,160 --> 00:47:35,150
are very mature so like state management

00:47:30,050 --> 00:47:38,030
and that kind of thing so yeah the

00:47:35,150 --> 00:47:40,460
politically correct answer is it depends

00:47:38,030 --> 00:47:42,200
very heavily on your use case and what

00:47:40,460 --> 00:47:46,420
you're already working with and what

00:47:42,200 --> 00:47:50,030
your team is familiar with so yeah and

00:47:46,420 --> 00:47:52,790
do we need any other set up on a

00:47:50,030 --> 00:47:55,910
database for this state thing where do

00:47:52,790 --> 00:47:57,710
you store these days now the city's

00:47:55,910 --> 00:48:02,930
store and either on heap so like in

00:47:57,710 --> 00:48:06,160
memory or it's or it's stored on on disk

00:48:02,930 --> 00:48:10,430
on embedded Roxie be a key value store

00:48:06,160 --> 00:48:13,280
so the idea is that you can keep very

00:48:10,430 --> 00:48:16,070
very high volumes of State without

00:48:13,280 --> 00:48:26,660
needing to have an external system to to

00:48:16,070 --> 00:48:28,520
keep it I have a question about apparent

00:48:26,660 --> 00:48:31,250
contradiction right it which is like you

00:48:28,520 --> 00:48:33,080
said oh you can have um you know 40

00:48:31,250 --> 00:48:35,870
terabytes of state I forgot the number

00:48:33,080 --> 00:48:38,420
but a lot of state and you can also

00:48:35,870 --> 00:48:40,580
always have the state available locally

00:48:38,420 --> 00:48:44,410
to do your processing very quickly but

00:48:40,580 --> 00:48:46,760
and it's sharted so but I I mean I think

00:48:44,410 --> 00:48:49,160
obviously you're not having all those 40

00:48:46,760 --> 00:48:51,530
terabytes on every node so they can't

00:48:49,160 --> 00:48:53,240
all be local so you have to have decided

00:48:51,530 --> 00:48:55,520
to shard it in an intelligent way so

00:48:53,240 --> 00:48:58,970
that the data that you need is near the

00:48:55,520 --> 00:49:01,090
events that use it but sometimes you

00:48:58,970 --> 00:49:03,820
have to do like Joint Operations

00:49:01,090 --> 00:49:06,800
grouping you know across multiple

00:49:03,820 --> 00:49:10,100
partitions and I wonder like this does

00:49:06,800 --> 00:49:12,980
flink handle that case and yeah you can

00:49:10,100 --> 00:49:14,600
just address that a little bit yeah I

00:49:12,980 --> 00:49:16,760
think that the way that is handle is

00:49:14,600 --> 00:49:22,280
just it just rashard state when those

00:49:16,760 --> 00:49:23,720
operations happen but Stefan yeah okay

00:49:22,280 --> 00:49:25,400
so there are there is support for

00:49:23,720 --> 00:49:26,090
grouping operations I guess that's

00:49:25,400 --> 00:49:28,940
really

00:49:26,090 --> 00:49:32,270
yeah so you shouldn't have any problem

00:49:28,940 --> 00:49:33,920
with like that so state management in

00:49:32,270 --> 00:49:41,060
general and flink I would say is very

00:49:33,920 --> 00:49:43,280
very advanced so yeah it yeah okay and

00:49:41,060 --> 00:49:45,170
what if you want to control the starting

00:49:43,280 --> 00:49:50,600
is that something that's under the users

00:49:45,170 --> 00:49:54,050
control if they no I see yeses but I

00:49:50,600 --> 00:49:59,860
wouldn't know how to justify that so if

00:49:54,050 --> 00:49:59,860
Stefan maybe he wants to say something

00:49:59,950 --> 00:50:04,910
yeah maybe one one confusion is that

00:50:03,020 --> 00:50:06,920
there's it's not one state it's not like

00:50:04,910 --> 00:50:08,690
there's one database but you have you

00:50:06,920 --> 00:50:10,280
have multiple stages in their processing

00:50:08,690 --> 00:50:13,790
pipeline let's say the first stage to

00:50:10,280 --> 00:50:15,710
some you know joining by I don't know by

00:50:13,790 --> 00:50:19,400
user ID the next one does an aggregation

00:50:15,710 --> 00:50:21,920
by some other other key right and

00:50:19,400 --> 00:50:23,990
between between these slight eats each

00:50:21,920 --> 00:50:25,670
stage has its own state charted in a

00:50:23,990 --> 00:50:27,560
different way and between stages the

00:50:25,670 --> 00:50:29,570
streams are basically rerouted so that

00:50:27,560 --> 00:50:30,890
always events and state basically become

00:50:29,570 --> 00:50:32,750
co-located so whenever the operation

00:50:30,890 --> 00:50:35,870
happens they happen together but the

00:50:32,750 --> 00:50:37,430
black events can can stream multiple

00:50:35,870 --> 00:50:39,740
times back and forth between multiple

00:50:37,430 --> 00:50:41,420
charts to to meet the state in different

00:50:39,740 --> 00:50:43,070
stages as they as they need to meet it

00:50:41,420 --> 00:50:46,670
and basically the sharding is yes you're

00:50:43,070 --> 00:50:47,840
defining by what what key do you want a

00:50:46,670 --> 00:50:49,070
group or aggregate or anything that

00:50:47,840 --> 00:50:50,090
defines both the sharding and the

00:50:49,070 --> 00:50:53,120
routing and everything kind of together

00:50:50,090 --> 00:50:54,560
and yeah basically like we reshuffles

00:50:53,120 --> 00:50:57,850
the streams and memory to meet the

00:50:54,560 --> 00:50:57,850
charts at each stage as needed

00:51:05,099 --> 00:51:11,349
anymore sorry man's a bit of a basic

00:51:09,940 --> 00:51:13,599
question but you mentioned that it

00:51:11,349 --> 00:51:16,180
handles unbounded data could you just

00:51:13,599 --> 00:51:21,700
clarify what that means and how it how

00:51:16,180 --> 00:51:24,940
it does it basically flankers or was

00:51:21,700 --> 00:51:32,440
purposely built to just handle strings

00:51:24,940 --> 00:51:34,630
that don't have really an end and yeah

00:51:32,440 --> 00:51:36,940
so the special thing about it is that it

00:51:34,630 --> 00:51:38,920
handles that continuous data event by

00:51:36,940 --> 00:51:44,730
event which is a bit different than how

00:51:38,920 --> 00:51:47,319
other systems do it but flink considers

00:51:44,730 --> 00:51:49,960
batch or bounded streams just like a

00:51:47,319 --> 00:51:54,400
different way of looking at an unbounded

00:51:49,960 --> 00:51:55,749
stream so flink is also by just

00:51:54,400 --> 00:51:57,640
considering that that it processes

00:51:55,749 --> 00:52:00,160
unbounded streams it just basically says

00:51:57,640 --> 00:52:03,279
that with the same system you can you

00:52:00,160 --> 00:52:06,430
can process infinite or finit datasets

00:52:03,279 --> 00:52:08,410
so for for flinky really I mean for now

00:52:06,430 --> 00:52:11,499
like max said it still has there's still

00:52:08,410 --> 00:52:15,130
some distinction it's not fully unified

00:52:11,499 --> 00:52:17,170
yet but in the future you will have yeah

00:52:15,130 --> 00:52:19,749
the ability to just process both

00:52:17,170 --> 00:52:24,579
streaming and batch under the same

00:52:19,749 --> 00:52:25,839
runtime with same api's so the unbounded

00:52:24,579 --> 00:52:29,230
processing really is the batch

00:52:25,839 --> 00:52:32,769
processing no I'm I'm Bernard is the

00:52:29,230 --> 00:52:34,660
streaming and bounded is just means it

00:52:32,769 --> 00:52:37,390
has a beginning and an end with which

00:52:34,660 --> 00:52:41,349
can just be a subset of an unbounded

00:52:37,390 --> 00:52:44,700
stream right so in terms of Kefka

00:52:41,349 --> 00:52:50,910
streams Kefka streams has the unbounded

00:52:44,700 --> 00:52:54,069
concept if memory serves me correctly

00:52:50,910 --> 00:52:57,789
yeah so Kafka streams also those stream

00:52:54,069 --> 00:52:59,380
processing I'm not sure if they have if

00:52:57,789 --> 00:53:01,569
they have support for a batch

00:52:59,380 --> 00:53:03,910
I'll see it sorry I meant yet there's no

00:53:01,569 --> 00:53:05,950
better concept and CAFTA stream so yes

00:53:03,910 --> 00:53:11,279
yeah so christian names the same thing

00:53:05,950 --> 00:53:11,279
is just unbounded continuous data yeah

00:53:13,329 --> 00:53:17,650
so we'll wrap up here if you have any

00:53:15,549 --> 00:53:20,229
more questions I think more time Jakub

00:53:17,650 --> 00:53:21,960
would be happy to help you offline thank

00:53:20,229 --> 00:53:26,019
you everyone thank you thank you

00:53:21,960 --> 00:53:26,019

YouTube URL: https://www.youtube.com/watch?v=Q0LBTmT4W9o


