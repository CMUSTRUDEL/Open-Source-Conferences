Title: #ACEU19: Alexey Zinoyev â€“ Ensembles of ML algorithms & distributed online ML
Publication date: 2019-10-26
Playlist: ApacheCon Europe 2019 â€“ Berlin
Description: 
	Full title: Ensembles of ML algorithms and Distributed Online Machine Learning with Apache Ignite

More: https://aceu19.apachecon.com/session/ensembles-ml-algorithms-and-distributed-online-machine-learning-apache-ignite-0

Ensembles of ML algorithms and Distributed Online Machine Learning with Apache Ignite

Currently, Apache Ignite has ML module that includes a lot of distributed ML algorithms, the bunch of approximate ML algorithms, easy integration with TensorFlow via TensorFlow Ignite Dataset (currently, this is a part of TF.contrib package) and also each algorithm supports the model updating that gives us ability to make online-learning not only for KMeans and LinReg unlike Apache Spark. 

We suggest to use Apache Ignite ML module to speedup your ML training and use Ignite as backend for distributed TensorFlow calculations. 

Also this talk lights issues of distributed machine learning algorithm implementations.
Captions: 
	00:00:04,590 --> 00:00:11,420
nice to meet you guten tag guten abend

00:00:07,490 --> 00:00:14,129
good evening can etcetera I will talk

00:00:11,420 --> 00:00:16,680
about machine learning model in a patch

00:00:14,129 --> 00:00:21,240
ignite project and of course let's start

00:00:16,680 --> 00:00:23,700
without some welcoming slides and please

00:00:21,240 --> 00:00:25,560
raise your hand who use a patch ignite

00:00:23,700 --> 00:00:31,470
for example in production maybe somebody

00:00:25,560 --> 00:00:34,890
ok I see one person cool I will talk

00:00:31,470 --> 00:00:36,960
some facts about myself for last six

00:00:34,890 --> 00:00:40,969
year I've been studying for source code

00:00:36,960 --> 00:00:45,269
for many distributed instruments like

00:00:40,969 --> 00:00:48,329
Apache spark like Apache hive Apache

00:00:45,269 --> 00:00:50,969
ignite and cetera etc maybe you used

00:00:48,329 --> 00:00:53,909
tens or not Apache project like Hereward

00:00:50,969 --> 00:00:56,460
like tensorflow and etc all of them are

00:00:53,909 --> 00:01:01,469
open sourced and last two years I've

00:00:56,460 --> 00:01:04,830
spent a lot of time to are built in half

00:01:01,469 --> 00:01:07,290
good ml module of the Apache ignite I

00:01:04,830 --> 00:01:09,479
will talk some words about what is a

00:01:07,290 --> 00:01:14,610
patch ignite because not so many users

00:01:09,479 --> 00:01:17,130
are here first of all it's I will give

00:01:14,610 --> 00:01:20,280
you a very simple definition it's kind

00:01:17,130 --> 00:01:23,310
of in memory database yes it has so many

00:01:20,280 --> 00:01:26,280
features how to persist on the disk how

00:01:23,310 --> 00:01:28,770
to support cache operation etc but now

00:01:26,280 --> 00:01:33,420
for many use cases in production it's in

00:01:28,770 --> 00:01:35,969
memory database maybe you could find

00:01:33,420 --> 00:01:39,049
some parallels with Cassandra of viv

00:01:35,969 --> 00:01:42,299
react but it fully support AC

00:01:39,049 --> 00:01:46,020
transactions first of all in a yes it

00:01:42,299 --> 00:01:50,579
has distributed SQL these two features

00:01:46,020 --> 00:01:53,100
are so good and two years we started a

00:01:50,579 --> 00:01:55,320
special model in Apache igniting a

00:01:53,100 --> 00:01:59,100
system it was a male model first of all

00:01:55,320 --> 00:02:02,009
it was like like linear algebra order

00:01:59,100 --> 00:02:06,179
the Apache ignite but now it's more than

00:02:02,009 --> 00:02:08,460
algebra now raise your hands who at

00:02:06,179 --> 00:02:10,619
least trained the model maybe in pet

00:02:08,460 --> 00:02:16,560
project me okay

00:02:10,619 --> 00:02:19,190
so many ml guys here but who used spark

00:02:16,560 --> 00:02:24,180
ml for this for example

00:02:19,190 --> 00:02:26,879
most of them okay cool I will talk some

00:02:24,180 --> 00:02:27,780
information about traditional

00:02:26,879 --> 00:02:30,840
programming about machine learning

00:02:27,780 --> 00:02:33,870
programmer and the usual program that

00:02:30,840 --> 00:02:36,180
the programmer writes resembles a finite

00:02:33,870 --> 00:02:38,940
state machine you know and generally

00:02:36,180 --> 00:02:41,580
consists of a strict set of instructions

00:02:38,940 --> 00:02:44,660
in the machinery rank we have the

00:02:41,580 --> 00:02:48,900
opposite situation we need to generate

00:02:44,660 --> 00:02:51,630
computations the model and ml task

00:02:48,900 --> 00:02:54,299
shortly could be described as a search

00:02:51,630 --> 00:02:57,870
of the best set of parameters for

00:02:54,299 --> 00:03:01,470
unknown function f which has known type

00:02:57,870 --> 00:03:04,190
for example it's the line or is a cubic

00:03:01,470 --> 00:03:06,269
line or something else it could be

00:03:04,190 --> 00:03:11,190
complicated most strictly we have the

00:03:06,269 --> 00:03:14,459
task of objects X Y is the set of

00:03:11,190 --> 00:03:18,390
answers and F is an unknown relation

00:03:14,459 --> 00:03:21,150
finding objects and answers and the goal

00:03:18,390 --> 00:03:24,540
is to approximate this unknown function

00:03:21,150 --> 00:03:27,420
by another function we've known hyper

00:03:24,540 --> 00:03:30,030
parameters for example let's start on

00:03:27,420 --> 00:03:32,609
the simplest model the linear regression

00:03:30,030 --> 00:03:34,769
this is an example of linear regression

00:03:32,609 --> 00:03:38,489
as I said it was mentioned two lectures

00:03:34,769 --> 00:03:40,350
two talks ago in five more talk which

00:03:38,489 --> 00:03:44,160
describes the linear dependency between

00:03:40,350 --> 00:03:48,530
input and output and our goal to find

00:03:44,160 --> 00:03:52,430
the best set of w WK efficient right and

00:03:48,530 --> 00:03:57,420
how to determine which line is the best

00:03:52,430 --> 00:04:00,450
here the loss function will help us ms e

00:03:57,420 --> 00:04:02,720
loss function with the difference sum of

00:04:00,450 --> 00:04:05,130
squared deviations between

00:04:02,720 --> 00:04:08,849
predictions and true answers known

00:04:05,130 --> 00:04:12,319
before for example on the label data set

00:04:08,849 --> 00:04:16,320
and we are trying to minimize the MSE

00:04:12,319 --> 00:04:19,380
result to find the best function I know

00:04:16,320 --> 00:04:21,720
that all ml guys know this fact but we

00:04:19,380 --> 00:04:24,270
need some introduction here sorry and a

00:04:21,720 --> 00:04:26,880
model can be represented not only via

00:04:24,270 --> 00:04:29,669
for example linear equation or something

00:04:26,880 --> 00:04:31,450
else but also as a tree for example and

00:04:29,669 --> 00:04:34,590
these sliders

00:04:31,450 --> 00:04:37,450
Shawn shows a Titanic passenger data set

00:04:34,590 --> 00:04:41,320
several prediction model as a binary

00:04:37,450 --> 00:04:45,220
tree this is very known image and the

00:04:41,320 --> 00:04:47,440
next we are trying to find a certain

00:04:45,220 --> 00:04:49,870
curve that divides the set of points

00:04:47,440 --> 00:04:51,850
into two classes there are a lot of

00:04:49,870 --> 00:04:54,130
these curves and we need to find a curve

00:04:51,850 --> 00:04:56,800
that separates these classes in the best

00:04:54,130 --> 00:04:59,230
way according to the last function we

00:04:56,800 --> 00:04:59,830
should remember this definition to the

00:04:59,230 --> 00:05:02,740
next slide

00:04:59,830 --> 00:05:07,120
imagine imagine that we have a lot of

00:05:02,740 --> 00:05:09,880
data in the cluster we will not talk

00:05:07,120 --> 00:05:12,610
that this is an ignite cluster in many

00:05:09,880 --> 00:05:13,690
cases maybe spark cluster Kafka cluster

00:05:12,610 --> 00:05:16,240
I don't know

00:05:13,690 --> 00:05:19,750
Roker burn carbonized cluster and etc

00:05:16,240 --> 00:05:22,810
and we are trying to find a way to

00:05:19,750 --> 00:05:25,090
predict on this data of course if you

00:05:22,810 --> 00:05:29,080
load this data to a batch ignite for

00:05:25,090 --> 00:05:31,870
example you can train on this data in

00:05:29,080 --> 00:05:33,730
the in memory in the real time and etc

00:05:31,870 --> 00:05:36,790
and apatriquen item a library supports

00:05:33,730 --> 00:05:39,010
not only distributed training it support

00:05:36,790 --> 00:05:41,830
distributed pre-processing evaluation

00:05:39,010 --> 00:05:43,780
parallel cross-validation for high

00:05:41,830 --> 00:05:47,650
cluster resource utilization and so many

00:05:43,780 --> 00:05:50,380
things let's start from the first point

00:05:47,650 --> 00:05:53,190
you have a row data for example in a

00:05:50,380 --> 00:05:56,470
batch igniter in Kafka you can use

00:05:53,190 --> 00:05:58,960
special data streamer to load the data

00:05:56,470 --> 00:06:02,290
into a batch ignite in in-memory tables

00:05:58,960 --> 00:06:04,630
and the distributed pipeline starts from

00:06:02,290 --> 00:06:07,330
the data loading and first of all after

00:06:04,630 --> 00:06:12,130
that via pre-processing we should

00:06:07,330 --> 00:06:16,120
prepare a set of vectors of course many

00:06:12,130 --> 00:06:17,530
algorithms likes data with double values

00:06:16,120 --> 00:06:20,200
not two strings

00:06:17,530 --> 00:06:22,480
not with strange in arms not with Java

00:06:20,200 --> 00:06:25,030
objects and etc and the goal of

00:06:22,480 --> 00:06:28,540
pre-processing to prepare vectors after

00:06:25,030 --> 00:06:30,400
that then dektor contained only doubles

00:06:28,540 --> 00:06:36,370
which range the model on the vector data

00:06:30,400 --> 00:06:40,660
after that we need sometimes not not not

00:06:36,370 --> 00:06:42,880
in 100% cases we need to repeat our

00:06:40,660 --> 00:06:43,950
training process again and again again

00:06:42,880 --> 00:06:46,260
and again and

00:06:43,950 --> 00:06:48,990
we don't want to spend the time to

00:06:46,260 --> 00:06:52,410
prepare this court in Java every time

00:06:48,990 --> 00:06:54,300
and here it's very easy to use something

00:06:52,410 --> 00:06:57,870
like piranha greed cross-validation

00:06:54,300 --> 00:07:00,750
pipeline API and etc for example like an

00:06:57,870 --> 00:07:02,490
apache spark but apache spark is not the

00:07:00,750 --> 00:07:05,910
in-memory database you should remember

00:07:02,490 --> 00:07:08,070
that and in the end the quality of the

00:07:05,910 --> 00:07:09,900
built model could be evaluated before

00:07:08,070 --> 00:07:12,750
deploying on the production's very

00:07:09,900 --> 00:07:14,520
interesting point we have a lot of

00:07:12,750 --> 00:07:17,310
different metrics how to estimate a

00:07:14,520 --> 00:07:19,320
model and for being binary

00:07:17,310 --> 00:07:21,510
classification tasks for regression for

00:07:19,320 --> 00:07:24,270
clustering sometimes and etcetera they

00:07:21,510 --> 00:07:26,520
all these steps are distributed in a

00:07:24,270 --> 00:07:29,430
batch ignite model you should ask me how

00:07:26,520 --> 00:07:31,770
we had the same thing in Apache spark

00:07:29,430 --> 00:07:33,660
yes we have the same finger patches part

00:07:31,770 --> 00:07:35,940
Apache spark and scikit-learn are

00:07:33,660 --> 00:07:39,660
reference libraries for the Apache

00:07:35,940 --> 00:07:43,050
ignite model but mmm I have one small

00:07:39,660 --> 00:07:47,250
problem the Apache spark has a stable of

00:07:43,050 --> 00:07:53,040
a mean emily has a stable core which has

00:07:47,250 --> 00:07:58,590
no I don't know ability to aggregate new

00:07:53,040 --> 00:08:00,270
pairs which located in many issues in

00:07:58,590 --> 00:08:03,510
the Apache spark juror there are a lot

00:08:00,270 --> 00:08:06,930
of different pairs and my goal in Apache

00:08:03,510 --> 00:08:09,030
igniter mile to use this experience to

00:08:06,930 --> 00:08:11,340
experience of all these users to involve

00:08:09,030 --> 00:08:13,800
them into the inter new distributed

00:08:11,340 --> 00:08:16,830
library which have a chance to emerge

00:08:13,800 --> 00:08:22,170
all these algorithms to support on your

00:08:16,830 --> 00:08:26,250
features and etc I'm not trying to for

00:08:22,170 --> 00:08:28,020
example for you for you say please drop

00:08:26,250 --> 00:08:29,700
a pouch apache spark and i'll live no

00:08:28,020 --> 00:08:31,560
this is for different use cases a

00:08:29,700 --> 00:08:34,229
partial spark and a leap is good so the

00:08:31,560 --> 00:08:36,930
real Big Data cases and you have a lot

00:08:34,229 --> 00:08:39,930
of data then this data is managed by

00:08:36,930 --> 00:08:42,570
Hadoop cluster but if you have medium

00:08:39,930 --> 00:08:44,670
data for in-memory cluster and Apache

00:08:42,570 --> 00:08:47,910
ignite you need prediction tool to in

00:08:44,670 --> 00:08:50,670
this is a bad idea to move into HDFS to

00:08:47,910 --> 00:08:52,380
train in spark in Apache spark but

00:08:50,670 --> 00:08:54,750
Apache ignite fast connection to Apache

00:08:52,380 --> 00:08:56,850
spark to and you can load a data from

00:08:54,750 --> 00:08:57,640
the Apache spark data frame operations

00:08:56,850 --> 00:09:01,690
to train

00:08:57,640 --> 00:09:04,780
in an memory mode also like an Apache

00:09:01,690 --> 00:09:08,290
spark it uses internally distributed

00:09:04,780 --> 00:09:11,320
data structures it's presented like a

00:09:08,290 --> 00:09:12,820
partition based data set this is an

00:09:11,320 --> 00:09:15,400
abstraction layer which sits between

00:09:12,820 --> 00:09:16,120
machine learning algorithms and the

00:09:15,400 --> 00:09:19,060
storage

00:09:16,120 --> 00:09:22,240
I mean Apache ignite caches and it uses

00:09:19,060 --> 00:09:24,790
MapReduce like approach to perform the

00:09:22,240 --> 00:09:27,010
computation of course in distributed

00:09:24,790 --> 00:09:33,070
world you couldn't avoid my produce

00:09:27,010 --> 00:09:36,100
right and for many tasks you can you can

00:09:33,070 --> 00:09:39,520
find you own you cast them approach to

00:09:36,100 --> 00:09:42,070
use MapReduce approach directly not via

00:09:39,520 --> 00:09:45,520
for example data frame common approach

00:09:42,070 --> 00:09:48,700
inspired not via write an RDD IPIN etc

00:09:45,520 --> 00:09:51,010
but you can implement your own partition

00:09:48,700 --> 00:09:53,290
data structures for each algorithm for

00:09:51,010 --> 00:09:55,930
example any aggression and the SVM

00:09:53,290 --> 00:09:58,510
algorithms has the same partition

00:09:55,930 --> 00:10:01,630
structure but for example the decision

00:09:58,510 --> 00:10:03,580
tree has another the and you can find

00:10:01,630 --> 00:10:06,330
the best formats to prepare them in

00:10:03,580 --> 00:10:08,890
Apache ignite ml we are trying to find

00:10:06,330 --> 00:10:10,780
the best way present data for each

00:10:08,890 --> 00:10:12,880
algorithm yes we have a common approach

00:10:10,780 --> 00:10:15,010
which can be used in your algorithm but

00:10:12,880 --> 00:10:18,460
if you want to implement your own

00:10:15,010 --> 00:10:20,680
storage you can as a contributor let's

00:10:18,460 --> 00:10:23,560
talk about our algorithms of course it

00:10:20,680 --> 00:10:26,440
has a lot of classification algorithms

00:10:23,560 --> 00:10:30,100
it's classic library you have logistic

00:10:26,440 --> 00:10:33,270
regression SVM Kanan I know that it's

00:10:30,100 --> 00:10:36,430
not distributed well but you can try and

00:10:33,270 --> 00:10:40,420
we implemented an approximate name

00:10:36,430 --> 00:10:42,880
neighbors I'm sorry to repeat this and

00:10:40,420 --> 00:10:45,400
then it's approximate Kanan

00:10:42,880 --> 00:10:48,070
decision trees random forest and random

00:10:45,400 --> 00:10:52,150
forest is not only one the example

00:10:48,070 --> 00:10:54,640
methods for the ML library also we

00:10:52,150 --> 00:10:58,630
support regression algorithms and of

00:10:54,640 --> 00:11:01,690
course and of course if you like the

00:10:58,630 --> 00:11:03,040
very very initial neural network

00:11:01,690 --> 00:11:05,230
architecture of course we support

00:11:03,040 --> 00:11:07,390
multi-layer perceptron and it uses

00:11:05,230 --> 00:11:11,500
internally in logistic regression like

00:11:07,390 --> 00:11:15,700
one layer perceptron there and let's

00:11:11,500 --> 00:11:19,690
look on the next I don't know I name it

00:11:15,700 --> 00:11:21,190
like architecture diagram let's say we

00:11:19,690 --> 00:11:23,230
use ignite caches to extract the

00:11:21,190 --> 00:11:25,990
in-memory data to build the partition

00:11:23,230 --> 00:11:29,280
based data set first of all a partition

00:11:25,990 --> 00:11:31,420
based data set are its special structure

00:11:29,280 --> 00:11:35,020
across the cluster distributed across

00:11:31,420 --> 00:11:37,390
the cluster but it's not the row Ignite

00:11:35,020 --> 00:11:40,390
format this is special format for the

00:11:37,390 --> 00:11:45,880
machine learning we don't want to reuse

00:11:40,390 --> 00:11:47,410
Ignite caches because they was built for

00:11:45,880 --> 00:11:50,380
another case it's not for machine

00:11:47,410 --> 00:11:53,740
learning but we use it like data source

00:11:50,380 --> 00:11:57,370
and we minimize the shuffling between

00:11:53,740 --> 00:12:00,390
the nodes partitions are created a near

00:11:57,370 --> 00:12:02,980
the data located in the apache ignite

00:12:00,390 --> 00:12:05,770
okay first of all fill the cache for

00:12:02,980 --> 00:12:08,200
example we read titanic passenger data

00:12:05,770 --> 00:12:10,900
set to predict who will survive or not

00:12:08,200 --> 00:12:13,300
and we built first of all ignite cached

00:12:10,900 --> 00:12:16,390
after that we make label two vectors

00:12:13,300 --> 00:12:18,760
vector in label from row data in caches

00:12:16,390 --> 00:12:21,970
via for example dummy vector risers we

00:12:18,760 --> 00:12:26,440
have a lot of them and this is a

00:12:21,970 --> 00:12:29,470
limitation now of apache ignite we have

00:12:26,440 --> 00:12:31,210
no full access to the data scheme in

00:12:29,470 --> 00:12:33,520
apache ignite and this is the reason why

00:12:31,210 --> 00:12:36,460
we work with indices now but i think

00:12:33,520 --> 00:12:40,420
that in next releases it will be fix it

00:12:36,460 --> 00:12:42,490
and we will work fully the meta

00:12:40,420 --> 00:12:46,630
information like the column names and

00:12:42,490 --> 00:12:48,550
etcetera yes this is issue after that we

00:12:46,630 --> 00:12:52,170
define the trainer decision tree

00:12:48,550 --> 00:12:55,990
classification trainer with five as

00:12:52,170 --> 00:13:00,220
marks deep of tree and minimum impurity

00:12:55,990 --> 00:13:02,230
as zero and after that we feed this

00:13:00,220 --> 00:13:04,750
training algorithm to the ignite

00:13:02,230 --> 00:13:08,350
instance for example ignite session

00:13:04,750 --> 00:13:11,350
declared before this slide data cache a

00:13:08,350 --> 00:13:14,170
declared earlier and vectorizer we

00:13:11,350 --> 00:13:17,380
combined them together and feed on this

00:13:14,170 --> 00:13:19,870
data to get the model and after that we

00:13:17,380 --> 00:13:24,420
use evaluator to evaluate our results

00:13:19,870 --> 00:13:26,620
okay it's java we use generics and so on

00:13:24,420 --> 00:13:27,730
preprocessors of course the Sun will

00:13:26,620 --> 00:13:30,220
date before we implement all

00:13:27,730 --> 00:13:32,290
pre-processing from scikit-learn and now

00:13:30,220 --> 00:13:34,900
the purchased park is not the reference

00:13:32,290 --> 00:13:37,060
here because cycling has more much more

00:13:34,900 --> 00:13:40,750
here and all pre-processing could be

00:13:37,060 --> 00:13:44,560
distributed well because in I don't know

00:13:40,750 --> 00:13:50,440
in ninety thousand cases it has only one

00:13:44,560 --> 00:13:52,030
Map Reduce step sometimes to not end the

00:13:50,440 --> 00:13:55,060
amount of preprocessor could be compared

00:13:52,030 --> 00:13:58,090
with spark except the NLP pre-processing

00:13:55,060 --> 00:14:00,340
and of course we have normalizing vector

00:13:58,090 --> 00:14:03,040
to special normas standard scaling

00:14:00,340 --> 00:14:05,770
minimax scaling max abhi scale and and

00:14:03,040 --> 00:14:08,070
etc etc and of course we support on hot

00:14:05,770 --> 00:14:12,310
encoding because we need to prepare our

00:14:08,070 --> 00:14:16,900
double vectors I think that may be part

00:14:12,310 --> 00:14:19,540
of you who sitting there not familiar

00:14:16,900 --> 00:14:23,320
with this please imagine you have

00:14:19,540 --> 00:14:24,880
something like Java G nom I know that

00:14:23,320 --> 00:14:29,260
most of Apache developer are Java

00:14:24,880 --> 00:14:33,070
developers but maybe not so oh and we

00:14:29,260 --> 00:14:36,820
need to present this janaab like a

00:14:33,070 --> 00:14:39,070
binary vector for example this digit 1

00:14:36,820 --> 00:14:40,900
or digit 0 on the special places for

00:14:39,070 --> 00:14:44,530
example we decided that the first column

00:14:40,900 --> 00:14:48,070
we will put one there each correlated

00:14:44,530 --> 00:14:51,190
the the first ordinary item in the Java

00:14:48,070 --> 00:14:52,990
and cetera et cetera let's combine

00:14:51,190 --> 00:14:54,700
together processing training and

00:14:52,990 --> 00:14:57,040
evaluation we could define their

00:14:54,700 --> 00:15:00,430
previous steps here for example in

00:14:57,040 --> 00:15:02,860
buting to feel mrs. values min max

00:15:00,430 --> 00:15:06,070
Keller to scale normalization if you

00:15:02,860 --> 00:15:10,120
need normalization and you could see

00:15:06,070 --> 00:15:12,490
that the second preprocessor used the

00:15:10,120 --> 00:15:14,860
link to the impudent preprocessor

00:15:12,490 --> 00:15:17,140
normalization preprocessor use min max

00:15:14,860 --> 00:15:19,870
Keller preprocessor and etc the decision

00:15:17,140 --> 00:15:22,480
tree classification trainer use the last

00:15:19,870 --> 00:15:24,280
one a normalization preprocessor and of

00:15:22,480 --> 00:15:28,090
course all calculation are lazy

00:15:24,280 --> 00:15:32,160
in reality where you call the feet no

00:15:28,090 --> 00:15:35,260
calculation are running but when you

00:15:32,160 --> 00:15:36,640
call the trainer feet all calculation

00:15:35,260 --> 00:15:41,110
are started from

00:15:36,640 --> 00:15:43,270
the first preprocessor let's go deeper

00:15:41,110 --> 00:15:45,760
to the linear regression linear

00:15:43,270 --> 00:15:47,470
regression could be are implemented via

00:15:45,760 --> 00:15:49,660
different approaches there are a lot of

00:15:47,470 --> 00:15:52,240
them on the Internet art key archive

00:15:49,660 --> 00:15:53,290
work and etc but let's talk about a few

00:15:52,240 --> 00:15:55,570
of the returns which could be

00:15:53,290 --> 00:15:58,150
implemented in distributed manner easily

00:15:55,570 --> 00:16:00,490
the first algorithm which was added

00:15:58,150 --> 00:16:02,790
historically to apache ignite was the

00:16:00,490 --> 00:16:07,030
linear regression via a less QR method

00:16:02,790 --> 00:16:11,110
alyce caron method eats it reminds me

00:16:07,030 --> 00:16:13,510
the SVD SVD I mean single or well you

00:16:11,110 --> 00:16:15,070
decomposition method the core of this

00:16:13,510 --> 00:16:16,450
method is the by de kernelization

00:16:15,070 --> 00:16:20,080
procedure

00:16:16,450 --> 00:16:21,760
Golub Catalan ker-slunk chose I don't

00:16:20,080 --> 00:16:27,460
remember exactly how to pronounce that

00:16:21,760 --> 00:16:30,880
and zero code could be listed like this

00:16:27,460 --> 00:16:34,000
and we need to understand how to move

00:16:30,880 --> 00:16:38,620
this court to the MapReduce operations

00:16:34,000 --> 00:16:43,510
we could improve the equations on the

00:16:38,620 --> 00:16:47,140
slide we could rewrite them for P parts

00:16:43,510 --> 00:16:51,870
for example at P index in each equation

00:16:47,140 --> 00:16:55,750
and are in the final we need two

00:16:51,870 --> 00:16:57,460
MapReduce at MapReduce stages for each

00:16:55,750 --> 00:16:59,920
step to perform the calculation and

00:16:57,460 --> 00:17:01,720
haven't selected P steps we can

00:16:59,920 --> 00:17:04,630
transform the set of expression like

00:17:01,720 --> 00:17:06,760
here and it was the first algorithm

00:17:04,630 --> 00:17:08,880
which was implemented maybe not very

00:17:06,760 --> 00:17:11,290
simple but it was very useful

00:17:08,880 --> 00:17:13,510
implementation which you used in many

00:17:11,290 --> 00:17:15,550
libraries and now they're interesting

00:17:13,510 --> 00:17:18,579
algorithm is stochastic gradient descent

00:17:15,550 --> 00:17:22,390
which is used in I don't know in half of

00:17:18,579 --> 00:17:25,449
our algorithms but not in all algorithms

00:17:22,390 --> 00:17:28,870
as a core of calculations for example we

00:17:25,449 --> 00:17:31,990
have you should remember this linear

00:17:28,870 --> 00:17:36,940
regression model and we need to find W

00:17:31,990 --> 00:17:40,570
coefficient right and let's write it

00:17:36,940 --> 00:17:43,510
once yet yet one time MSE a loss

00:17:40,570 --> 00:17:45,670
function and what we could do here

00:17:43,510 --> 00:17:48,190
we should minimize the MSE functional we

00:17:45,670 --> 00:17:50,280
should get the first derivative of each

00:17:48,190 --> 00:17:52,710
target variable right and the

00:17:50,280 --> 00:17:55,440
vector of such derivatives named as

00:17:52,710 --> 00:17:58,320
gradient and it indicates the direction

00:17:55,440 --> 00:18:00,480
of the maximum increment of the function

00:17:58,320 --> 00:18:04,320
at the point at the each point for

00:18:00,480 --> 00:18:06,900
example and we can change the W vector

00:18:04,320 --> 00:18:09,150
of our cafe coefficients according to

00:18:06,900 --> 00:18:11,250
the value of gradient by special

00:18:09,150 --> 00:18:13,110
formulas and so on iteration by

00:18:11,250 --> 00:18:16,010
iteration duration by iteration and

00:18:13,110 --> 00:18:19,620
these calculation could be distributed

00:18:16,010 --> 00:18:21,750
the gradient operation with the gradient

00:18:19,620 --> 00:18:23,970
operator is linear and could be

00:18:21,750 --> 00:18:26,400
calculated on that data partition in

00:18:23,970 --> 00:18:28,980
parallel on different nodes for example

00:18:26,400 --> 00:18:31,290
I don't know on different course if you

00:18:28,980 --> 00:18:35,610
run it in parallel mode notes and

00:18:31,290 --> 00:18:39,180
distributed model this is a simple

00:18:35,610 --> 00:18:41,100
visualization how it works there is no

00:18:39,180 --> 00:18:43,680
difference between distributed gradient

00:18:41,100 --> 00:18:48,720
and non distributed gradient it gives us

00:18:43,680 --> 00:18:51,540
the same result here this code shows how

00:18:48,720 --> 00:18:54,420
to use SGD to update the linear model

00:18:51,540 --> 00:18:57,750
coefficients this is not real production

00:18:54,420 --> 00:18:59,880
code but this is like a schema this is a

00:18:57,750 --> 00:19:03,570
place in court that wastes a lot of time

00:18:59,880 --> 00:19:06,680
I don't know 99 percent of time and it

00:19:03,570 --> 00:19:08,820
should be distributed you to the easily

00:19:06,680 --> 00:19:11,640
distributed nature of the gradient

00:19:08,820 --> 00:19:17,700
calculation as was mentioned on this

00:19:11,640 --> 00:19:20,940
slide this is how we work in apache

00:19:17,700 --> 00:19:24,030
ignite ml model we trying to find the

00:19:20,940 --> 00:19:26,880
paper how to distribute it one or

00:19:24,030 --> 00:19:29,340
another algorithm we find the placing

00:19:26,880 --> 00:19:31,980
quote which could be separated for

00:19:29,340 --> 00:19:33,960
different notice we are trying to find

00:19:31,980 --> 00:19:35,850
the reference implementation sometimes

00:19:33,960 --> 00:19:38,010
the reference implementation seen

00:19:35,850 --> 00:19:40,470
another distributed frameworks are not

00:19:38,010 --> 00:19:43,350
so good and has a special limitations

00:19:40,470 --> 00:19:45,660
but for the last two years on the

00:19:43,350 --> 00:19:48,540
archive orc were written a lot of new

00:19:45,660 --> 00:19:50,880
papers there you can find new approaches

00:19:48,540 --> 00:19:53,370
to distribute to distribute for example

00:19:50,880 --> 00:19:56,760
SVM algorithm and so on the new

00:19:53,370 --> 00:20:00,300
frameworks presented in C++ which could

00:19:56,760 --> 00:20:02,220
be I don't know became a part of a batch

00:20:00,300 --> 00:20:03,629
ignite ml or another library

00:20:02,220 --> 00:20:06,539
I hope that

00:20:03,629 --> 00:20:09,029
some new solutions which which were

00:20:06,539 --> 00:20:11,460
implemented in our library could be a

00:20:09,029 --> 00:20:13,580
reference for another libraries - this

00:20:11,460 --> 00:20:16,109
is not the closet sort of the world and

00:20:13,580 --> 00:20:18,659
why do we spend a lot of time on SGD

00:20:16,109 --> 00:20:20,629
method course it's working course for

00:20:18,659 --> 00:20:24,119
many our algorithms like logistic

00:20:20,629 --> 00:20:26,940
regression simple neural networks linear

00:20:24,119 --> 00:20:30,600
regression SVM and etc and if you

00:20:26,940 --> 00:20:33,119
understand how to implement the SGD

00:20:30,600 --> 00:20:36,239
method you could understand the sources

00:20:33,119 --> 00:20:40,429
for example of tensorflow by Georg in

00:20:36,239 --> 00:20:43,619
different neural network libraries

00:20:40,429 --> 00:20:46,139
sometimes is difficult to evaluate how

00:20:43,619 --> 00:20:48,299
good these are that binary

00:20:46,139 --> 00:20:51,960
classification model and the first idea

00:20:48,299 --> 00:20:54,090
is splitting of the initial data set on

00:20:51,960 --> 00:20:56,429
test and train data said okay we support

00:20:54,090 --> 00:20:58,919
this it's very simple but sometimes we

00:20:56,429 --> 00:21:01,799
could get on the separation the effect

00:20:58,919 --> 00:21:04,759
of the overfitting and the next the next

00:21:01,799 --> 00:21:07,739
idea is k-fold cross-validation and

00:21:04,759 --> 00:21:10,529
during this process signal splits our

00:21:07,739 --> 00:21:11,399
datasets in k consecutive faults and

00:21:10,529 --> 00:21:15,389
each fault

00:21:11,399 --> 00:21:18,299
is used once as a validation while k

00:21:15,389 --> 00:21:21,509
minus one remaining false form a

00:21:18,299 --> 00:21:24,450
training data set and of course we could

00:21:21,509 --> 00:21:26,639
run this calculation in calculations in

00:21:24,450 --> 00:21:28,679
parallel if you have enough resource

00:21:26,639 --> 00:21:32,629
cluster cores for example and etc

00:21:28,679 --> 00:21:35,429
because in reality the training not

00:21:32,629 --> 00:21:39,479
spent all your resources not all your

00:21:35,429 --> 00:21:44,399
memory not all you CPUs and etc also of

00:21:39,479 --> 00:21:47,909
course each ml developer like-likes ml

00:21:44,399 --> 00:21:51,179
pipelines and the spark has a very good

00:21:47,909 --> 00:21:54,299
support of pipeline api's and i'm trying

00:21:51,179 --> 00:21:56,309
to repeat these for newcomers in apache

00:21:54,299 --> 00:21:57,659
ignite ml it will be very easy to

00:21:56,309 --> 00:22:01,080
understand what's happened here on the

00:21:57,659 --> 00:22:03,299
slide but we implemented more a

00:22:01,080 --> 00:22:07,409
different parameter search strategies

00:22:03,299 --> 00:22:11,009
not i'm sorry stupid grid search but a

00:22:07,409 --> 00:22:14,129
random search and for example this

00:22:11,009 --> 00:22:17,430
summer I edit every evolution

00:22:14,129 --> 00:22:19,530
optimization strategy maybe you know for

00:22:17,430 --> 00:22:23,370
discrete optimization tasks there are a

00:22:19,530 --> 00:22:26,330
lot of different heuristics how to find

00:22:23,370 --> 00:22:30,000
the best solution on the set of discrete

00:22:26,330 --> 00:22:33,480
values for example and this is one of

00:22:30,000 --> 00:22:36,210
them or you could use another approach

00:22:33,480 --> 00:22:38,700
here you could add new approaches to our

00:22:36,210 --> 00:22:41,790
library too if you will be a contributor

00:22:38,700 --> 00:22:43,830
or commit on our project and if you

00:22:41,790 --> 00:22:45,930
don't know what is genetic algorithm

00:22:43,830 --> 00:22:49,830
maybe you remember something from the

00:22:45,930 --> 00:22:53,150
university courses and etc you saw on

00:22:49,830 --> 00:22:57,000
the set of hyper parameters like on the

00:22:53,150 --> 00:22:59,580
genes all the I don't know person in the

00:22:57,000 --> 00:23:03,120
population and you need to mute a them

00:22:59,580 --> 00:23:06,840
cross over them reproduce them to find

00:23:03,120 --> 00:23:10,440
the best solution also in our library

00:23:06,840 --> 00:23:13,370
they have not only genetic algorithm to

00:23:10,440 --> 00:23:16,230
tune hyper parameters we have support of

00:23:13,370 --> 00:23:19,140
genetic of distributed genetic

00:23:16,230 --> 00:23:22,860
algorithms - it was a very large

00:23:19,140 --> 00:23:26,130
donation it was in the early times of

00:23:22,860 --> 00:23:27,780
our model and it works well there are a

00:23:26,130 --> 00:23:30,420
lot of different examples if you need

00:23:27,780 --> 00:23:35,160
distributed genetic algorithms you can

00:23:30,420 --> 00:23:37,740
go and use this algorithms - after that

00:23:35,160 --> 00:23:39,690
we could finish all these pipelines with

00:23:37,740 --> 00:23:42,600
cross validation and hyper parameter

00:23:39,690 --> 00:23:46,290
tuning we need to declare the instance

00:23:42,600 --> 00:23:51,080
of cross validation and run the tune

00:23:46,290 --> 00:23:54,090
hyper parameters function to get to

00:23:51,080 --> 00:23:57,780
first of all to mix all previous

00:23:54,090 --> 00:24:00,350
separators together and find that for

00:23:57,780 --> 00:24:03,990
example for the decision training

00:24:00,350 --> 00:24:06,809
algorithm the best max deep and mean

00:24:03,990 --> 00:24:10,920
ability decrease hyper parameters which

00:24:06,809 --> 00:24:15,210
were presented here like set of

00:24:10,920 --> 00:24:17,670
different values that's not all of

00:24:15,210 --> 00:24:22,880
course we have n samples this is a part

00:24:17,670 --> 00:24:25,320
of topic today and maybe some one of you

00:24:22,880 --> 00:24:27,030
trying to join this talk to hear

00:24:25,320 --> 00:24:29,370
something about distributed and samples

00:24:27,030 --> 00:24:31,140
yes we have full feature distributed

00:24:29,370 --> 00:24:33,510
samples I mean staking begging

00:24:31,140 --> 00:24:34,950
and the boosting not limited-edition

00:24:33,510 --> 00:24:36,960
like an Apache spark

00:24:34,950 --> 00:24:39,560
sorry Apache spark but this is the

00:24:36,960 --> 00:24:42,240
reason why it's located in Apache ignite

00:24:39,560 --> 00:24:44,370
and symbol learning builds a set of

00:24:42,240 --> 00:24:46,950
classifiers in order to enhance the

00:24:44,370 --> 00:24:49,920
accuracy of a single classifier for

00:24:46,950 --> 00:24:53,160
example or the random forest you know

00:24:49,920 --> 00:24:55,860
random forests can you should train a

00:24:53,160 --> 00:24:59,190
lot of different trees on subset of data

00:24:55,860 --> 00:25:01,680
and merge the results voting for example

00:24:59,190 --> 00:25:03,750
like averaging or calculations the

00:25:01,680 --> 00:25:05,790
Maxwell your votes for example you votes

00:25:03,750 --> 00:25:09,230
for the first class or for the second

00:25:05,790 --> 00:25:12,630
class and 100 of trees trying to vote

00:25:09,230 --> 00:25:16,890
but of course you can use your own

00:25:12,630 --> 00:25:20,700
custom voters vote procedures and etc

00:25:16,890 --> 00:25:23,160
let's start on the begging this is also

00:25:20,700 --> 00:25:24,900
called like bootstrap aggregation this

00:25:23,160 --> 00:25:29,160
is special machine learning algorithm

00:25:24,900 --> 00:25:31,800
which has a common schema for mentioned

00:25:29,160 --> 00:25:34,740
earlier random forests random forests

00:25:31,800 --> 00:25:37,980
this kind of begging here and for

00:25:34,740 --> 00:25:39,690
example starting from the splitting the

00:25:37,980 --> 00:25:42,540
initial data sets on the train and test

00:25:39,690 --> 00:25:45,140
set and trace said could be splitted on

00:25:42,540 --> 00:25:47,850
the special bags and you can train

00:25:45,140 --> 00:25:50,550
different models on these bags and joins

00:25:47,850 --> 00:25:52,590
the results for example via voting Oh

00:25:50,550 --> 00:25:54,600
for example for regression tax via

00:25:52,590 --> 00:25:56,850
special mean procedure then you're

00:25:54,600 --> 00:25:59,790
trying to find the mean value of all

00:25:56,850 --> 00:26:02,160
predicted results also we have a

00:25:59,790 --> 00:26:04,620
boosting and in my opinion boosting is

00:26:02,160 --> 00:26:10,100
not a good candidate for distributing

00:26:04,620 --> 00:26:14,160
but it's good candidate for sequential

00:26:10,100 --> 00:26:15,780
execution of course but all training

00:26:14,160 --> 00:26:17,700
could be distributes for example you

00:26:15,780 --> 00:26:20,130
have a lot of data and your subset of

00:26:17,700 --> 00:26:22,290
data is too much and should be located

00:26:20,130 --> 00:26:25,860
in cluster to this is a reason to use

00:26:22,290 --> 00:26:28,140
our boost Inc not another one threat

00:26:25,860 --> 00:26:30,690
boosting from cool Python libraries I

00:26:28,140 --> 00:26:33,210
also we have is taken as take involves

00:26:30,690 --> 00:26:36,110
training a learning algorithm to combine

00:26:33,210 --> 00:26:42,030
the prediction of first step algorithm

00:26:36,110 --> 00:26:44,310
to predict the inputs for the second

00:26:42,030 --> 00:26:44,940
level algorithm for example we have

00:26:44,310 --> 00:26:47,670
partitioned by

00:26:44,940 --> 00:26:53,910
data set train free decision tree model

00:26:47,670 --> 00:26:56,250
and and train the logistic regression on

00:26:53,910 --> 00:26:58,950
the result of prediction of this model

00:26:56,250 --> 00:27:01,710
this very peppy popular technique on

00:26:58,950 --> 00:27:03,780
Kegel you know and then combine our

00:27:01,710 --> 00:27:06,330
algorithm is trained to make a final

00:27:03,780 --> 00:27:08,700
prediction used all of them it could be

00:27:06,330 --> 00:27:11,040
very easily used in a Patrick Knight ml

00:27:08,700 --> 00:27:14,360
for example via next code to gather the

00:27:11,040 --> 00:27:16,560
result you train here to decision tree

00:27:14,360 --> 00:27:19,980
classification models and combine them

00:27:16,560 --> 00:27:22,970
via special states take it vector data

00:27:19,980 --> 00:27:26,460
set trainer there you add easily you

00:27:22,970 --> 00:27:31,310
require trainers define it above and

00:27:26,460 --> 00:27:35,700
feet on the data to get this naked model

00:27:31,310 --> 00:27:38,310
also we support kind of online learning

00:27:35,700 --> 00:27:40,800
maybe it's not the fine at the final

00:27:38,310 --> 00:27:42,600
decision for our library but we support

00:27:40,800 --> 00:27:45,830
online learning for all of the

00:27:42,600 --> 00:27:48,420
algorithms because each developer who

00:27:45,830 --> 00:27:50,520
starting to add new algorithm to our

00:27:48,420 --> 00:27:54,270
library should implement the update

00:27:50,520 --> 00:27:56,850
model method to training partition it

00:27:54,270 --> 00:28:00,390
reminds me the partition training in

00:27:56,850 --> 00:28:02,670
cycler but as we know psychotronic has

00:28:00,390 --> 00:28:05,520
no distributed version to Train

00:28:02,670 --> 00:28:06,740
partitioned distributed the environment

00:28:05,520 --> 00:28:09,150
sorry

00:28:06,740 --> 00:28:11,550
unlike machine learning is the matter of

00:28:09,150 --> 00:28:14,340
machine learning to add new information

00:28:11,550 --> 00:28:17,400
to your model based on the new portion

00:28:14,340 --> 00:28:19,770
of data for example let's imagine you

00:28:17,400 --> 00:28:21,810
have a patchy spark which has connector

00:28:19,770 --> 00:28:24,960
to apache ignite you build the pipeline

00:28:21,810 --> 00:28:28,380
you prepare the data you reduce the data

00:28:24,960 --> 00:28:31,110
and load to a patch ignite and you need

00:28:28,380 --> 00:28:36,450
to update the training model via data

00:28:31,110 --> 00:28:38,430
pieces from Kafka you could use a patch

00:28:36,450 --> 00:28:40,590
spark but it has a very very limited

00:28:38,430 --> 00:28:43,550
support of an eye learning for free all

00:28:40,590 --> 00:28:46,680
five models I don't remember exactly via

00:28:43,550 --> 00:28:49,140
very complex formulas and cetera but

00:28:46,680 --> 00:28:52,800
here in Apache ignite you could easily

00:28:49,140 --> 00:28:55,290
use update method to update you previous

00:28:52,800 --> 00:28:57,690
model their new portion of data are

00:28:55,290 --> 00:29:00,510
loaded via data streamer

00:28:57,690 --> 00:29:02,400
I'm not going to add the code of loading

00:29:00,510 --> 00:29:04,640
but you could see how to use update

00:29:02,400 --> 00:29:09,300
method here you has a logistic

00:29:04,640 --> 00:29:11,550
regression trainer the the all hyper

00:29:09,300 --> 00:29:14,430
parameters with it on the first portion

00:29:11,550 --> 00:29:20,040
of data on the data cache one and a

00:29:14,430 --> 00:29:22,500
model two could be built via model one

00:29:20,040 --> 00:29:25,110
updated with portion data located in

00:29:22,500 --> 00:29:28,260
data cache two it works it works for all

00:29:25,110 --> 00:29:31,350
of our algorithm I agree I agree this is

00:29:28,260 --> 00:29:34,170
a very interesting field I mean that now

00:29:31,350 --> 00:29:38,550
on archive ork of course there are a lot

00:29:34,170 --> 00:29:41,130
of papers how to do these models we

00:29:38,550 --> 00:29:43,040
should be updated online in real time

00:29:41,130 --> 00:29:46,260
there are many many many different

00:29:43,040 --> 00:29:48,780
holistic approaches and you could go to

00:29:46,260 --> 00:29:50,460
how community and implement what you

00:29:48,780 --> 00:29:52,800
want for new algorithms and maybe

00:29:50,460 --> 00:29:58,260
something we've right for there are more

00:29:52,800 --> 00:30:03,510
for all algorithms to also in the end we

00:29:58,260 --> 00:30:05,220
will finish our talk I want to say some

00:30:03,510 --> 00:30:08,610
words about tons of for integration of

00:30:05,220 --> 00:30:12,840
course we have two ways are in our

00:30:08,610 --> 00:30:16,320
community to build our own neural

00:30:12,840 --> 00:30:19,260
network framework above apache ignite or

00:30:16,320 --> 00:30:22,610
use the existing we started from the

00:30:19,260 --> 00:30:24,930
multi-layer perceptron we played the

00:30:22,610 --> 00:30:27,480
reinforcement learning architecture in

00:30:24,930 --> 00:30:30,030
pairs and etc what we understand the

00:30:27,480 --> 00:30:32,910
power of community is limited and we

00:30:30,030 --> 00:30:35,310
should collaborate the existing cool

00:30:32,910 --> 00:30:37,710
solution liked and the flaw I know that

00:30:35,310 --> 00:30:40,770
many data scientists use pi torch too

00:30:37,710 --> 00:30:42,900
and I hope that somebody who will go to

00:30:40,770 --> 00:30:44,700
our community could help us with this

00:30:42,900 --> 00:30:48,780
integration but I am NOT a PI Church

00:30:44,700 --> 00:30:50,640
user I am enough goods if tensorflow I

00:30:48,780 --> 00:30:54,270
couldn't stand the source code on etc

00:30:50,640 --> 00:30:56,310
and it was a reason why three of us

00:30:54,270 --> 00:31:00,020
collaborated with tensorflow

00:30:56,310 --> 00:31:03,530
and we created a special bridge for that

00:31:00,020 --> 00:31:08,370
we made a few commits to apache ignite

00:31:03,530 --> 00:31:09,980
to prepare all infrastructure for

00:31:08,370 --> 00:31:12,059
integration to tensorflow

00:31:09,980 --> 00:31:15,210
for bridge it's like a

00:31:12,059 --> 00:31:17,909
special tensorflow a yo package or

00:31:15,210 --> 00:31:20,609
something else it was in one place in

00:31:17,909 --> 00:31:23,729
tensorflow one and it's in our

00:31:20,609 --> 00:31:26,909
repository in terms of flow - because of

00:31:23,729 --> 00:31:29,719
evolution of tensor flow and for tensor

00:31:26,909 --> 00:31:33,389
flow we supported saving to our

00:31:29,719 --> 00:31:35,940
in-memory file system supported loading

00:31:33,389 --> 00:31:37,919
from ignite data set data to the tensor

00:31:35,940 --> 00:31:41,460
flow to train on them internal internal

00:31:37,919 --> 00:31:44,099
flow and we support the distributed

00:31:41,460 --> 00:31:46,919
backend like heard about but

00:31:44,099 --> 00:31:49,739
historically our distributed weekend was

00:31:46,919 --> 00:31:52,649
earlier because we don't know about her

00:31:49,739 --> 00:31:54,779
hood and implemented this and guys in

00:31:52,649 --> 00:31:58,889
terms of all doesn't know about that

00:31:54,779 --> 00:32:00,749
two years ago we provide apache ignite

00:31:58,889 --> 00:32:02,969
integration like distributed backends

00:32:00,749 --> 00:32:05,309
you could use these babies back end like

00:32:02,969 --> 00:32:08,700
for distributed calculation of gradients

00:32:05,309 --> 00:32:10,919
in tensor flow for example here this is

00:32:08,700 --> 00:32:13,440
example of loading data from the Ignite

00:32:10,919 --> 00:32:18,210
data set for example from SQL public

00:32:13,440 --> 00:32:21,389
Kipton cash to iterate on them to print

00:32:18,210 --> 00:32:25,139
out this data this is the another

00:32:21,389 --> 00:32:28,950
example in on python how to use how to

00:32:25,139 --> 00:32:31,529
distribute your calculations via tender

00:32:28,950 --> 00:32:34,200
for device how to run the job and etc

00:32:31,529 --> 00:32:36,509
all examples are actual for the tens of

00:32:34,200 --> 00:32:39,989
for one i think that in next few months

00:32:36,509 --> 00:32:42,330
we will support tons of flow true again

00:32:39,989 --> 00:32:45,029
and again I will say if you want to

00:32:42,330 --> 00:32:47,519
support tons of flow to you could go our

00:32:45,029 --> 00:32:50,599
community and could help because you

00:32:47,519 --> 00:32:54,839
know we have not so many hands for this

00:32:50,599 --> 00:32:58,499
but we have so many opportunities and it

00:32:54,839 --> 00:33:01,649
works like how you are create separate

00:32:58,499 --> 00:33:05,700
terms of flow instances together these

00:33:01,649 --> 00:33:07,559
Apache ignite nodes and it could easily

00:33:05,700 --> 00:33:11,399
communicate with tens of floor boards

00:33:07,559 --> 00:33:14,099
and so on also sorry sorry in the end I

00:33:11,399 --> 00:33:17,129
should say about some integration points

00:33:14,099 --> 00:33:18,899
because for this talk for me this is

00:33:17,129 --> 00:33:21,269
purpose to invite you to our community

00:33:18,899 --> 00:33:23,519
to ignore to invite you to build the

00:33:21,269 --> 00:33:25,650
patch ignite ml and I want to talk about

00:33:23,519 --> 00:33:27,840
few opportunities because the first

00:33:25,650 --> 00:33:30,750
opportunity for newcomer its integration

00:33:27,840 --> 00:33:32,850
you know one to you could integrate it

00:33:30,750 --> 00:33:35,100
with our tool and this is a poet of

00:33:32,850 --> 00:33:38,610
collaboration and we have different

00:33:35,100 --> 00:33:41,460
support for model inference this is that

00:33:38,610 --> 00:33:43,950
means that we could get the model traded

00:33:41,460 --> 00:33:47,250
in the now the system law to us and

00:33:43,950 --> 00:33:49,830
predict on our data we have special for

00:33:47,250 --> 00:33:52,560
structure for distributed inference and

00:33:49,830 --> 00:33:56,460
for example you could load data from

00:33:52,560 --> 00:33:58,380
scikit-learn via per ml but this we have

00:33:56,460 --> 00:34:00,630
very limited support from a per ml

00:33:58,380 --> 00:34:02,940
format but of course we could extend

00:34:00,630 --> 00:34:07,350
this in future we have full support for

00:34:02,940 --> 00:34:11,070
exhibit models we have supports for all

00:34:07,350 --> 00:34:15,270
spark models which has the neighbor in

00:34:11,070 --> 00:34:19,710
our library we have full support of

00:34:15,270 --> 00:34:22,500
m'lip this special tool in many time for

00:34:19,710 --> 00:34:25,830
spark only model to load and predict on

00:34:22,500 --> 00:34:27,840
a training spark pipelines which were

00:34:25,830 --> 00:34:30,419
trained it in spark cluster for example

00:34:27,840 --> 00:34:33,780
you we have two points to integrate with

00:34:30,419 --> 00:34:36,300
spark we directly parse the spark binary

00:34:33,780 --> 00:34:38,730
format for example parkette files and so

00:34:36,300 --> 00:34:41,159
on and the another approach is in our

00:34:38,730 --> 00:34:43,710
pro sure to use emily runtime we don't

00:34:41,159 --> 00:34:45,960
know what will venus par community and

00:34:43,710 --> 00:34:48,770
trying to support all of them and you

00:34:45,960 --> 00:34:52,290
don't believe two days before the summit

00:34:48,770 --> 00:34:56,900
someone from h2o community prepares the

00:34:52,290 --> 00:35:00,180
poll requests how to parse h2o model to

00:34:56,900 --> 00:35:02,280
inference in apache ignite we didn't

00:35:00,180 --> 00:35:04,050
know it yet but i think that in the next

00:35:02,280 --> 00:35:07,260
release it will be merged and we will

00:35:04,050 --> 00:35:11,100
have more influence in a leap usage

00:35:07,260 --> 00:35:13,350
looks like that we run i'm a leap

00:35:11,100 --> 00:35:15,540
context we use special ignite

00:35:13,350 --> 00:35:18,210
distributed model builder to build the

00:35:15,540 --> 00:35:20,130
model from the file system of course a

00:35:18,210 --> 00:35:22,350
many cases we should part the file the

00:35:20,130 --> 00:35:24,990
special structure here and this is a

00:35:22,350 --> 00:35:28,410
work for integration this is not very

00:35:24,990 --> 00:35:30,480
difficult work but very meaningful if we

00:35:28,410 --> 00:35:34,080
have a lot of different integration

00:35:30,480 --> 00:35:36,990
points we could make our tools stronger

00:35:34,080 --> 00:35:39,390
of course and we could make predictions

00:35:36,990 --> 00:35:41,970
on our model like on

00:35:39,390 --> 00:35:45,600
training ignite for example the

00:35:41,970 --> 00:35:47,400
architecture diagram looks like that for

00:35:45,600 --> 00:35:50,370
spark male model parcel you have Apache

00:35:47,400 --> 00:35:55,530
spark for example were you trained the

00:35:50,370 --> 00:35:57,840
model there you use very usual method on

00:35:55,530 --> 00:36:00,960
a patchy spark model right over right

00:35:57,840 --> 00:36:02,580
safe you model to the directory or to

00:36:00,960 --> 00:36:04,980
the file to the parkade file of course

00:36:02,580 --> 00:36:08,070
or to the director of parkade files and

00:36:04,980 --> 00:36:11,040
we could parse for example this kind of

00:36:08,070 --> 00:36:14,880
model this is gradient boosted trees if

00:36:11,040 --> 00:36:17,670
I remember exactly and we could load for

00:36:14,880 --> 00:36:20,370
example we could load new data or we

00:36:17,670 --> 00:36:23,340
could upload you know new piece of

00:36:20,370 --> 00:36:27,210
information we could load this model the

00:36:23,340 --> 00:36:29,400
spark model parts or parts methods there

00:36:27,210 --> 00:36:31,470
we have a special Java enum for all

00:36:29,400 --> 00:36:33,930
supported models you can add new models

00:36:31,470 --> 00:36:38,520
there if you find them and predict on

00:36:33,930 --> 00:36:39,930
this data - it works it could be your

00:36:38,520 --> 00:36:42,660
application this is a motivation

00:36:39,930 --> 00:36:45,900
architecture diagram if you have a lot

00:36:42,660 --> 00:36:48,570
of different points where you need to

00:36:45,900 --> 00:36:50,970
load the data to one storage and you

00:36:48,570 --> 00:36:54,480
have no not petabytes but terabytes of

00:36:50,970 --> 00:36:56,940
data you could load the model from

00:36:54,480 --> 00:37:00,390
Apache spark via Sparkman model parser

00:36:56,940 --> 00:37:02,730
you could load new piece of data from

00:37:00,390 --> 00:37:05,160
cough-cough link via data streamers to

00:37:02,730 --> 00:37:10,800
apache ignite you could update the spark

00:37:05,160 --> 00:37:14,130
model via partition a train it models

00:37:10,800 --> 00:37:15,750
partition training models from piece of

00:37:14,130 --> 00:37:17,700
data from Kafka Fink and North then

00:37:15,750 --> 00:37:21,480
together combine them together in one

00:37:17,700 --> 00:37:24,510
large model which will be stored in our

00:37:21,480 --> 00:37:27,060
internal model storage Yahoo how to

00:37:24,510 --> 00:37:30,870
contribute how to contribute I hope that

00:37:27,060 --> 00:37:33,660
all these slides wasn't so hard I think

00:37:30,870 --> 00:37:37,260
that mathematical for most this is the

00:37:33,660 --> 00:37:39,600
reason why your have deal with machine

00:37:37,260 --> 00:37:42,930
learning etc and if you're interesting

00:37:39,600 --> 00:37:45,390
in I don't know in different challenges

00:37:42,930 --> 00:37:48,300
we've distributed nature with machine

00:37:45,390 --> 00:37:51,390
learning with integrations and etc you

00:37:48,300 --> 00:37:54,020
could join our community we have 200

00:37:51,390 --> 00:37:54,020
contributors

00:37:54,140 --> 00:38:01,200
50 maybe committers and the 30 PMC

00:37:58,260 --> 00:38:04,130
members we have block who we have ignite

00:38:01,200 --> 00:38:06,360
documentation separately documentation

00:38:04,130 --> 00:38:09,570
contribution to the documentation I

00:38:06,360 --> 00:38:11,520
welcoming to like in another PI chick

00:38:09,570 --> 00:38:16,680
night communities too and I want to

00:38:11,520 --> 00:38:19,920
publish some things for ignite 3.0 this

00:38:16,680 --> 00:38:22,650
is a very very long road map for us we

00:38:19,920 --> 00:38:24,480
want to have more deals with NLP natural

00:38:22,650 --> 00:38:25,470
language processing and more integration

00:38:24,480 --> 00:38:29,310
with tensorflow

00:38:25,470 --> 00:38:31,800
we want to work close if tensorflow with

00:38:29,310 --> 00:38:33,600
pi torch we with h2o add more clustering

00:38:31,800 --> 00:38:37,530
algorithm we have not enough clustering

00:38:33,600 --> 00:38:40,590
algorithms now only commence and yam

00:38:37,530 --> 00:38:42,750
algorithm and we want to add very basic

00:38:40,590 --> 00:38:45,080
statistical package now we miss this one

00:38:42,750 --> 00:38:47,100
because maybe we have no cool

00:38:45,080 --> 00:38:49,170
mathematicians or statisticians in our

00:38:47,100 --> 00:38:51,660
community and if you are cool and start

00:38:49,170 --> 00:38:53,370
let's join us and of course we have a

00:38:51,660 --> 00:38:56,160
lot of tasks for beginners for example

00:38:53,370 --> 00:38:59,640
last year I've mentored two guys who

00:38:56,160 --> 00:39:02,700
takes the task with label you for

00:38:59,640 --> 00:39:06,480
newbies and we spend a lot of time on

00:39:02,700 --> 00:39:08,820
github - for reviewing on dev least

00:39:06,480 --> 00:39:12,990
to discuss new proposals and cetera and

00:39:08,820 --> 00:39:16,020
now both of them are commuters and I

00:39:12,990 --> 00:39:18,420
think that one newcomer will be commits

00:39:16,020 --> 00:39:22,530
or to next year I hope because they did

00:39:18,420 --> 00:39:24,510
a lot of tickets if you're interested in

00:39:22,530 --> 00:39:28,350
this project of this opportunity follow

00:39:24,510 --> 00:39:30,600
me join the Twitter go to the github and

00:39:28,350 --> 00:39:32,730
join to the Apache ignite community of

00:39:30,600 --> 00:39:34,650
course we have not only mal tasks if you

00:39:32,730 --> 00:39:36,870
interesting can distribute it the SQL a

00:39:34,650 --> 00:39:40,140
distributed caches different operations

00:39:36,870 --> 00:39:42,060
cool Java multi-threading world and

00:39:40,140 --> 00:39:44,850
approaches you could join - this is

00:39:42,060 --> 00:39:47,870
really really hard job like like we all

00:39:44,850 --> 00:39:47,870
together thank you

00:39:53,050 --> 00:39:59,300
if you have time for question every

00:39:55,640 --> 00:40:03,790
every time do we have a time maybe

00:39:59,300 --> 00:40:11,480
somebody is ready to get the ticket I

00:40:03,790 --> 00:40:16,150
don't know okay I will be here after the

00:40:11,480 --> 00:40:16,150
this session I think 10 minutes here and

00:40:16,180 --> 00:40:20,390
I'm sorry

00:40:17,750 --> 00:40:22,720
now the downstairs near the tea and we

00:40:20,390 --> 00:40:24,880
could drink some tea and discuss

00:40:22,720 --> 00:40:28,020
distributed ml if you're interesting

00:40:24,880 --> 00:40:31,860
okay thank you

00:40:28,020 --> 00:40:31,860

YouTube URL: https://www.youtube.com/watch?v=tVbhlecuEyI


