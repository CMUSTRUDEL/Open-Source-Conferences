Title: #ACEU19: Apache IoTDB: Time Series Database for Industrial IoT
Publication date: 2019-10-31
Playlist: ApacheCon Europe 2019 â€“ Berlin
Description: 
	Speaker: Julian Feinauer & Xiangdong Huang
More: https://aceu19.apachecon.com/session/apache-iotdb-time-series-database-industrial-iot

Time series data management system is becoming increasingly important with the rise of the Internet of Things, especially in the industrial area. Apache IoTDB (incubating), one of the youngest projects in the Apache Incubator,  is born for that!
Apache IoTDB is an integrated data management engine designed for time series data, based on a file format which is able to store time series data very efficiently. It provides users specific services for data collection, storage and analysis. Due to its lightweight structure, high performance and usable features together with its good integration with Apache Hadoop and Apache Spark, it can meet the requirements of massive dataset storage, high-speed data ingestion and complex data analysis for industrial IoT applications.
As the project is very new to the ASF, the talk has four objectives. First, discuss the time series data management requirement in the IoT area; Second, introduce Apache IoTDB to the world; Third, to give an overview about the architecture of the project and the technologies used inside and Fourth, to give an idea about how IoTDB integrates with other Apache Projects and where it fits in the (IoT) data processing ecosystem. Some practical use cases are also introduced.
Captions: 
	00:00:04,920 --> 00:00:11,620
hello everyone today I will introduce an

00:00:08,770 --> 00:00:15,070
old coyote deep Apache incubating

00:00:11,620 --> 00:00:18,280
project who yo called IO TDB and it's a

00:00:15,070 --> 00:00:23,740
international since database does anyone

00:00:18,280 --> 00:00:26,440
care about IOT DB and up okay there are

00:00:23,740 --> 00:00:29,200
two persons that's why I stand here

00:00:26,440 --> 00:00:34,320
because I want to introduce today to all

00:00:29,200 --> 00:00:39,550
of you ok let's kinda see you who we are

00:00:34,320 --> 00:00:42,640
when we say IOT DB lttb's IOT + DP it's

00:00:39,550 --> 00:00:45,070
our database for managing IOT data

00:00:42,640 --> 00:00:48,520
especially for intellectual IOT data

00:00:45,070 --> 00:00:53,170
here and the IOT data means the time

00:00:48,520 --> 00:00:57,550
sure state has and when I introduced IOT

00:00:53,170 --> 00:01:01,510
DB to others many people ask me is IOT

00:00:57,550 --> 00:01:04,899
DP is there a blank space between the

00:01:01,510 --> 00:01:07,869
world IOT and the TP I said no lttb is a

00:01:04,899 --> 00:01:11,680
software product Wow

00:01:07,869 --> 00:01:14,049
io t DP means a kind of database so if

00:01:11,680 --> 00:01:19,600
we want to search something for about

00:01:14,049 --> 00:01:24,549
how GDP you can use them as one word do

00:01:19,600 --> 00:01:27,430
not use blank space and Ltd B is very

00:01:24,549 --> 00:01:29,619
young and I'm very glad to introduce IOT

00:01:27,430 --> 00:01:33,400
to be in Germany because you as you know

00:01:29,619 --> 00:01:37,990
Germany's famous country for internship

00:01:33,400 --> 00:01:41,290
and when I came this country I found

00:01:37,990 --> 00:01:45,010
many substances about IOT DP imported

00:01:41,290 --> 00:01:47,439
what society and these photos are taken

00:01:45,010 --> 00:01:50,380
around the body and we can see the

00:01:47,439 --> 00:01:53,400
window turbines the activators the

00:01:50,380 --> 00:01:57,520
trunks and modern work whole factories

00:01:53,400 --> 00:02:01,630
and all of this may belong to

00:01:57,520 --> 00:02:05,340
intellectual artists narrows and what is

00:02:01,630 --> 00:02:09,729
more important I found many DP logos

00:02:05,340 --> 00:02:14,940
imported especially as the metro

00:02:09,729 --> 00:02:18,160
stations actually it has different to me

00:02:14,940 --> 00:02:22,540
with I always database it's

00:02:18,160 --> 00:02:26,620
it seems is Germany tree

00:02:22,540 --> 00:02:29,830
short for German tree but anyway we got

00:02:26,620 --> 00:02:32,140
a IOT and DB in Germany now even though

00:02:29,830 --> 00:02:36,900
this is alko joke and then let's turn

00:02:32,140 --> 00:02:39,490
back and introduce what real IOT dbe's

00:02:36,900 --> 00:02:43,150
lttb is proposed from Shanghai

00:02:39,490 --> 00:02:47,470
University in China and is donated to a

00:02:43,150 --> 00:02:51,940
Vatican community at eighteen one bird

00:02:47,470 --> 00:02:54,490
2080 initially only some stuffs and the

00:02:51,940 --> 00:02:57,970
students into my university devoted for

00:02:54,490 --> 00:03:00,400
it but now there are some companies and

00:02:57,970 --> 00:03:04,560
the university students devoting for it

00:03:00,400 --> 00:03:09,190
for example like pragmatic bands Germany

00:03:04,560 --> 00:03:10,990
German company and you can see learn

00:03:09,190 --> 00:03:16,900
novo and Microsoft and some other

00:03:10,990 --> 00:03:21,640
companies and coding for each and Usedom

00:03:16,900 --> 00:03:24,070
for in some applications as for me I was

00:03:21,640 --> 00:03:27,070
a PhD student into my University and

00:03:24,070 --> 00:03:30,280
then appeared this poster inch margins

00:03:27,070 --> 00:03:32,770
University of Kent and now I'm an

00:03:30,280 --> 00:03:36,240
assistant researcher in to my university

00:03:32,770 --> 00:03:39,100
one circuit though I have no experience

00:03:36,240 --> 00:03:41,770
that working in some in cultural

00:03:39,100 --> 00:03:44,770
companies but I really worked for many

00:03:41,770 --> 00:03:48,700
managing time search data for several

00:03:44,770 --> 00:03:51,850
years and from 2012 I began to use

00:03:48,700 --> 00:03:56,710
Apache Cassandra to managing time search

00:03:51,850 --> 00:04:01,230
data in some companies in China to store

00:03:56,710 --> 00:04:05,410
the time search data and we found some

00:04:01,230 --> 00:04:08,140
performance problems for example the

00:04:05,410 --> 00:04:10,660
replic consistency problem for example

00:04:08,140 --> 00:04:13,690
when you scale out the character you

00:04:10,660 --> 00:04:15,640
have to wait long time to migrate data

00:04:13,690 --> 00:04:20,470
from one note or the know the two new

00:04:15,640 --> 00:04:23,500
notes and if the if your memory is not

00:04:20,470 --> 00:04:27,610
enough you will get a lot of small files

00:04:23,500 --> 00:04:30,520
and you will get an exception called too

00:04:27,610 --> 00:04:31,400
many open vowels and something something

00:04:30,520 --> 00:04:34,639
wrong

00:04:31,400 --> 00:04:37,520
so we began to develop IOT D beer and

00:04:34,639 --> 00:04:40,100
before you eloped developing IOT to be

00:04:37,520 --> 00:04:42,710
my favorite about project is like this

00:04:40,100 --> 00:04:49,850
Cassandra but now if I was a little a

00:04:42,710 --> 00:04:54,770
IOT DB and then let's introduce Julia so

00:04:49,850 --> 00:04:57,050
hi everybody my name is Julian I am the

00:04:54,770 --> 00:04:59,419
founder one of the founders of a small

00:04:57,050 --> 00:05:00,979
start-up called pragmatic Minds we have

00:04:59,419 --> 00:05:05,449
a spinoff pragmatic industry is where we

00:05:00,979 --> 00:05:07,669
do a lot of industry applications so we

00:05:05,449 --> 00:05:09,979
we hit a lot of the issues that identity

00:05:07,669 --> 00:05:13,190
be solving so when I heard first about

00:05:09,979 --> 00:05:14,990
aya TVB I was really really excited I

00:05:13,190 --> 00:05:17,120
was one of the first ones to join a

00:05:14,990 --> 00:05:19,550
mailing list and I had the luck that I

00:05:17,120 --> 00:05:23,680
was the first committer who is not the

00:05:19,550 --> 00:05:26,060
initial committee and and we really

00:05:23,680 --> 00:05:29,630
playing around a lot of our TDPS it

00:05:26,060 --> 00:05:32,060
solves many problems we have as you see

00:05:29,630 --> 00:05:33,740
with the other projects I'm involved we

00:05:32,060 --> 00:05:37,099
are really all around this industrial

00:05:33,740 --> 00:05:39,260
IOT space not only IOT yes I don't know

00:05:37,099 --> 00:05:44,870
smart thermostats but like real big

00:05:39,260 --> 00:05:47,210
machines and yeah this is basically I

00:05:44,870 --> 00:05:50,450
want to tell you a bit about the exact

00:05:47,210 --> 00:05:54,020
use case or the situations where you run

00:05:50,450 --> 00:05:57,200
into issues with the setup which you

00:05:54,020 --> 00:06:00,860
currently have a major driving forces

00:05:57,200 --> 00:06:03,260
what we in germany call industry 4.0 as

00:06:00,860 --> 00:06:08,900
you may know germany's very well known

00:06:03,260 --> 00:06:11,419
for its machine manufacturers in its big

00:06:08,900 --> 00:06:12,830
industrial machines and it's a pretty

00:06:11,419 --> 00:06:16,820
big thing here in China it's called

00:06:12,830 --> 00:06:20,870
industry internet in the u.s. also so

00:06:16,820 --> 00:06:22,370
people really start to think about not

00:06:20,870 --> 00:06:25,970
only about the building the machines but

00:06:22,370 --> 00:06:28,789
of using the data atom these machines

00:06:25,970 --> 00:06:31,070
are producing and this is something

00:06:28,789 --> 00:06:34,810
which we have it since three or four

00:06:31,070 --> 00:06:36,889
years really it goes it goes off and it

00:06:34,810 --> 00:06:38,630
people are really really interested in

00:06:36,889 --> 00:06:40,310
those data sets and they are huge

00:06:38,630 --> 00:06:42,849
because there's machines have tons of

00:06:40,310 --> 00:06:42,849
sensors

00:06:44,110 --> 00:06:50,560
go bit further so these are really

00:06:47,500 --> 00:06:52,000
examples of machines where people want

00:06:50,560 --> 00:06:55,000
for different reasons want to

00:06:52,000 --> 00:06:58,210
investigate the data and you have data

00:06:55,000 --> 00:07:02,439
in different levels but down to milli

00:06:58,210 --> 00:07:05,800
milli second frequencies where you're

00:07:02,439 --> 00:07:11,139
interested in in vibrations to see if a

00:07:05,800 --> 00:07:18,340
motor will soon break but also only only

00:07:11,139 --> 00:07:20,979
in a second range are ya some minute to

00:07:18,340 --> 00:07:24,490
some minutes but but overall such a

00:07:20,979 --> 00:07:28,090
machine has hundreds to thousands of

00:07:24,490 --> 00:07:30,159
sensors on it so it's it's really a lot

00:07:28,090 --> 00:07:31,719
of data coming together there are the

00:07:30,159 --> 00:07:34,479
domains where also tons of data is

00:07:31,719 --> 00:07:35,919
collected like meteorology it's very

00:07:34,479 --> 00:07:41,080
well known for collecting tons of data

00:07:35,919 --> 00:07:44,439
and also other other datasets play a

00:07:41,080 --> 00:07:46,810
role but we are mostly interested in and

00:07:44,439 --> 00:07:48,789
I would say there is two domains where

00:07:46,810 --> 00:07:56,979
there's really tons of data generated

00:07:48,789 --> 00:07:59,080
currently currently you you a typical

00:07:56,979 --> 00:08:01,479
setup basically is you have two machines

00:07:59,080 --> 00:08:05,349
generating the data some kind of

00:08:01,479 --> 00:08:10,690
messaging system and then you insert it

00:08:05,349 --> 00:08:13,599
in some kind of server and later on

00:08:10,690 --> 00:08:16,169
transfer to an our DBMS where you make

00:08:13,599 --> 00:08:22,330
it available for OLAP applications

00:08:16,169 --> 00:08:24,759
basically and the current the current

00:08:22,330 --> 00:08:27,789
set up could be something like this

00:08:24,759 --> 00:08:29,589
we're here at Apache console we show a

00:08:27,789 --> 00:08:32,409
lot of Apache projects which could be

00:08:29,589 --> 00:08:36,519
integrating their process sometimes PLC

00:08:32,409 --> 00:08:38,440
for exes like their source but when also

00:08:36,519 --> 00:08:40,539
other projects played a role we just

00:08:38,440 --> 00:08:42,579
heard about knife I or minify which you

00:08:40,539 --> 00:08:45,100
deploy at the edge to fetch the data

00:08:42,579 --> 00:08:47,470
transferred over to the server we have

00:08:45,100 --> 00:08:50,680
multiple message queues the the most

00:08:47,470 --> 00:08:53,649
famous one of course being Kafka and

00:08:50,680 --> 00:08:57,280
then as the database as hyeongdon said

00:08:53,649 --> 00:08:59,500
the first choice oftentimes is Cassandra

00:08:57,280 --> 00:09:02,530
for multiple reasons but one reason that

00:08:59,500 --> 00:09:04,270
the data is generated so fast that it's

00:09:02,530 --> 00:09:07,210
hard to find another system which is

00:09:04,270 --> 00:09:10,240
able to swallow it in time because

00:09:07,210 --> 00:09:12,820
you're really from tons tons of messages

00:09:10,240 --> 00:09:17,110
on there and it's something a regular

00:09:12,820 --> 00:09:18,520
our DBMS cannot handle so so we really

00:09:17,110 --> 00:09:22,120
have to look into this know sequel

00:09:18,520 --> 00:09:25,000
databases and analysis can be done for

00:09:22,120 --> 00:09:30,060
example with apache spark or based on an

00:09:25,000 --> 00:09:32,350
Hadoop or yeah whatever you like and

00:09:30,060 --> 00:09:36,750
some issues we have of this stack is

00:09:32,350 --> 00:09:39,660
that really here are some numbers of

00:09:36,750 --> 00:09:44,080
data we produce and in a simple example

00:09:39,660 --> 00:09:45,400
for the wind turbines we get 500 million

00:09:44,080 --> 00:09:48,630
points per second and this is really a

00:09:45,400 --> 00:09:51,070
challenge every database basically and

00:09:48,630 --> 00:09:55,780
the other thing which hits us is the

00:09:51,070 --> 00:09:58,660
volume because not not everybody can or

00:09:55,780 --> 00:10:01,060
wants to sustain a cluster just for

00:09:58,660 --> 00:10:02,710
storing the data and in some situations

00:10:01,060 --> 00:10:04,330
we want to make decisions faster at the

00:10:02,710 --> 00:10:06,630
edge and then it's even harder to

00:10:04,330 --> 00:10:09,730
control all the data storage there

00:10:06,630 --> 00:10:11,260
because systems like Cassandra are not

00:10:09,730 --> 00:10:16,690
made to be storage efficient they have

00:10:11,260 --> 00:10:18,100
other us piece and then yeah we have

00:10:16,690 --> 00:10:19,960
some other things like sparse tables

00:10:18,100 --> 00:10:23,470
because not every sensor reports at

00:10:19,960 --> 00:10:25,030
every timestamp and we have to deal with

00:10:23,470 --> 00:10:27,730
things like out of order which Cassandra

00:10:25,030 --> 00:10:29,350
just yeah stores the way it is but it

00:10:27,730 --> 00:10:40,089
would be better to sort in order

00:10:29,350 --> 00:10:41,200
probably so this is basically yeah one

00:10:40,089 --> 00:10:43,390
of the motivations we have and the

00:10:41,200 --> 00:10:45,339
second one is that ideally especially

00:10:43,390 --> 00:10:48,940
with no sequel you try to store the data

00:10:45,339 --> 00:10:52,839
in a way which makes it easier to query

00:10:48,940 --> 00:10:54,220
it later on so things like the time

00:10:52,839 --> 00:10:55,450
dimension is something special when

00:10:54,220 --> 00:10:57,070
thinking about time series data

00:10:55,450 --> 00:10:58,780
of course because it's like the primary

00:10:57,070 --> 00:11:01,030
index and you usually filter on an

00:10:58,780 --> 00:11:03,160
aggregate in it and the second thing is

00:11:01,030 --> 00:11:05,140
aggregation is important if you've ever

00:11:03,160 --> 00:11:07,150
worked with influx DB for example which

00:11:05,140 --> 00:11:09,339
is another famous times yes database

00:11:07,150 --> 00:11:11,170
direct is really a really lovely feature

00:11:09,339 --> 00:11:13,000
that you say okay give me data from this

00:11:11,170 --> 00:11:14,980
change but only one point per minute and

00:11:13,000 --> 00:11:16,779
just take the mean after where's in

00:11:14,980 --> 00:11:18,779
there because it for example I want to

00:11:16,779 --> 00:11:21,130
visualize something I do not need

00:11:18,779 --> 00:11:23,529
thousands of points but it only said

00:11:21,130 --> 00:11:25,570
okay I want 100 points in this range and

00:11:23,529 --> 00:11:28,000
aggregate things so this is something as

00:11:25,570 --> 00:11:29,709
an apron type API perspective which is

00:11:28,000 --> 00:11:31,660
really very crucial in working with time

00:11:29,709 --> 00:11:33,579
so your state huh because it helps you

00:11:31,660 --> 00:11:37,000
to keep your queries nice and easy

00:11:33,579 --> 00:11:47,709
but yeah make it way way better

00:11:37,000 --> 00:11:50,680
accessible and yeah yeah the second

00:11:47,709 --> 00:11:52,300
thing with with current current pramatta

00:11:50,680 --> 00:11:53,769
said you really do all the stuff on your

00:11:52,300 --> 00:11:55,600
own so first thing is you swallow the

00:11:53,769 --> 00:11:59,440
data then you transform it in some kind

00:11:55,600 --> 00:12:03,240
of ETL job which runs and synchronously

00:11:59,440 --> 00:12:06,910
takes it takes time but also takes

00:12:03,240 --> 00:12:09,519
computation computational resources so

00:12:06,910 --> 00:12:10,899
in the main things we want is something

00:12:09,519 --> 00:12:13,630
to start large volumes with high

00:12:10,899 --> 00:12:17,500
throughput ideally somewhere low-cost

00:12:13,630 --> 00:12:21,510
from a hardware perspective low latency

00:12:17,500 --> 00:12:24,760
for queries passed and easy aggregation

00:12:21,510 --> 00:12:27,940
and query capability for hybrid were

00:12:24,760 --> 00:12:30,279
close and if you monitor the current

00:12:27,940 --> 00:12:33,100
market of solutions for times yes they

00:12:30,279 --> 00:12:34,810
are quite some casandra's the HBase is

00:12:33,100 --> 00:12:36,940
there and then some systems based on

00:12:34,810 --> 00:12:39,850
Cassandra like hi raasta be open TST be

00:12:36,940 --> 00:12:43,990
a time scale which is a rod a yang

00:12:39,850 --> 00:12:45,820
project based on Postgres and well you

00:12:43,990 --> 00:12:48,100
can also do is post cross plane and in

00:12:45,820 --> 00:12:50,980
flexi be I just talked about and none of

00:12:48,100 --> 00:12:53,620
them really fits for this topic many of

00:12:50,980 --> 00:12:55,709
them come from the monitoring area where

00:12:53,620 --> 00:12:58,480
you want to monitor a cluster of

00:12:55,709 --> 00:13:01,510
computers and when when you push the

00:12:58,480 --> 00:13:02,949
workloads really really hard then they

00:13:01,510 --> 00:13:06,610
decorate they create a DES in

00:13:02,949 --> 00:13:09,459
performance or in storage efficiency or

00:13:06,610 --> 00:13:11,470
in both later on we have some numbers

00:13:09,459 --> 00:13:15,670
prepare to to really show that from a

00:13:11,470 --> 00:13:19,600
user's perspective you will hit it yeah

00:13:15,670 --> 00:13:22,050
issues there so this basically is some

00:13:19,600 --> 00:13:25,000
kind of motivation why the world needs

00:13:22,050 --> 00:13:26,770
such time seers database and now hand

00:13:25,000 --> 00:13:30,000
Shangdong again to tell you a bit more

00:13:26,770 --> 00:13:32,290
about how we tackle these problems to

00:13:30,000 --> 00:13:35,580
make it to make them easily accessible

00:13:32,290 --> 00:13:35,580
with IOT DB

00:13:36,960 --> 00:13:44,620
and now I will introduce the some

00:13:40,150 --> 00:13:49,030
details about about chayote DB and some

00:13:44,620 --> 00:13:51,760
real use case about how to it'd be okay

00:13:49,030 --> 00:13:54,400
to meet about requirements we build a

00:13:51,760 --> 00:13:56,650
time search database called IOT DB RT be

00:13:54,400 --> 00:14:00,490
focused on search data management and

00:13:56,650 --> 00:14:02,800
has outstanding performance for example

00:14:00,490 --> 00:14:05,620
a single server without a DB can manage

00:14:02,800 --> 00:14:07,900
a 10,000 mile second ten thousand

00:14:05,620 --> 00:14:10,330
millionth time series and the trillions

00:14:07,900 --> 00:14:13,630
of data points and the tens of terabytes

00:14:10,330 --> 00:14:19,500
data are one node and also support

00:14:13,630 --> 00:14:19,500
current big data analytics software's or

00:14:19,740 --> 00:14:25,870
LT B has the following features firstly

00:14:23,800 --> 00:14:28,810
it has high performance for data

00:14:25,870 --> 00:14:31,540
ingestion and provides high compression

00:14:28,810 --> 00:14:37,860
ratio for saving data and saving the

00:14:31,540 --> 00:14:41,860
disk IO secondly it has its latency

00:14:37,860 --> 00:14:44,950
remains low when querying on terabytes

00:14:41,860 --> 00:14:48,190
of data and it's sports time range query

00:14:44,950 --> 00:14:52,060
field query arm with way no filter and

00:14:48,190 --> 00:14:54,310
first aggregation queries third it's

00:14:52,060 --> 00:14:57,760
about many time series occlusive

00:14:54,310 --> 00:15:01,330
operations for example you can segment

00:14:57,760 --> 00:15:04,300
tensors into some sub screens for

00:15:01,330 --> 00:15:07,360
example you have have a car or I have a

00:15:04,300 --> 00:15:10,270
trunk but I Drive one over and sleep one

00:15:07,360 --> 00:15:15,040
over and drive our again and if you want

00:15:10,270 --> 00:15:18,160
to calculate the average fill cost you

00:15:15,040 --> 00:15:20,980
you cannot use the total time because

00:15:18,160 --> 00:15:24,270
your sleep several hours and you turn

00:15:20,980 --> 00:15:27,040
off your car so you have to cut

00:15:24,270 --> 00:15:29,620
temperatures into several sub schools

00:15:27,040 --> 00:15:33,339
and then you calculate in the subscript

00:15:29,620 --> 00:15:37,959
each subsequent and the average of the

00:15:33,339 --> 00:15:38,499
total result and for example we can

00:15:37,959 --> 00:15:40,359
support

00:15:38,499 --> 00:15:42,759
screams matching for example you have a

00:15:40,359 --> 00:15:44,589
long you have a long story of a time

00:15:42,759 --> 00:15:48,249
service and you want to find a similar

00:15:44,589 --> 00:15:50,229
pattern example like a curve or up and

00:15:48,249 --> 00:15:54,239
down curve and somehow something others

00:15:50,229 --> 00:15:57,759
and this had some operations only before

00:15:54,239 --> 00:16:00,789
only used for time source and I think

00:15:57,759 --> 00:16:02,769
many databases especially for relational

00:16:00,789 --> 00:16:07,559
database and the Camaro store they're

00:16:02,769 --> 00:16:11,709
not spot and we also integrate it with

00:16:07,559 --> 00:16:14,729
current big data systems and some

00:16:11,709 --> 00:16:18,069
traditional software's like MATLAB like

00:16:14,729 --> 00:16:21,729
graph Anna MapReduce and spark and

00:16:18,069 --> 00:16:24,779
something others therefore now lttb can

00:16:21,729 --> 00:16:28,659
cover the whole lifecycle of

00:16:24,779 --> 00:16:32,699
temperatures from cracked data from the

00:16:28,659 --> 00:16:36,989
machine and does in store the data and

00:16:32,699 --> 00:16:39,969
analysis they tend of realized the data

00:16:36,989 --> 00:16:42,339
this is the architecture of IOT DB we

00:16:39,969 --> 00:16:44,859
provide the two interfaces of tweezers

00:16:42,339 --> 00:16:48,119
the first level is the file level

00:16:44,859 --> 00:16:51,279
interface as shown in as a blue

00:16:48,119 --> 00:16:54,579
rectangle in the picture

00:16:51,279 --> 00:16:57,579
users can saved hamster State had as ES

00:16:54,579 --> 00:17:00,819
file as a file format as a file called

00:16:57,579 --> 00:17:03,279
the J's file shelf L is similar with

00:17:00,819 --> 00:17:04,959
Park height of our C files and I will I

00:17:03,279 --> 00:17:09,069
will introduce the difference between

00:17:04,959 --> 00:17:11,259
them and PS file the difference but the

00:17:09,069 --> 00:17:14,110
difference is TL tau is to optimize the

00:17:11,259 --> 00:17:18,159
for the time source data it plays the

00:17:14,110 --> 00:17:20,289
role of file layer of IOT DB TfL can be

00:17:18,159 --> 00:17:23,829
recognized to actually been spark and

00:17:20,289 --> 00:17:26,589
MapReduce so that you can use this big

00:17:23,829 --> 00:17:29,649
data analysis software to analyze the

00:17:26,589 --> 00:17:33,340
data in his file and the second one is

00:17:29,649 --> 00:17:35,980
the database layer interface users users

00:17:33,340 --> 00:17:38,919
can use the code to write data and query

00:17:35,980 --> 00:17:41,759
data from Ltd be and the lttb will

00:17:38,919 --> 00:17:45,129
organize the data as several TS vows

00:17:41,759 --> 00:17:48,159
until locally and now how did we can

00:17:45,129 --> 00:17:51,669
also write data to actually to HDFS

00:17:48,159 --> 00:17:52,210
so you needn't copy the data from your

00:17:51,669 --> 00:17:55,899
disk

00:17:52,210 --> 00:17:58,179
to HDFS and then your sparkle and

00:17:55,899 --> 00:18:02,640
Mario's or have to understand analyze it

00:17:58,179 --> 00:18:09,279
and we provide a synchronized model

00:18:02,640 --> 00:18:13,919
among IOT instances and for example it

00:18:09,279 --> 00:18:16,750
is quite useful for to crack data from

00:18:13,919 --> 00:18:21,880
manufacturers into one date warehouse

00:18:16,750 --> 00:18:24,789
for example if there's if a company has

00:18:21,880 --> 00:18:28,450
several factories around the world or

00:18:24,789 --> 00:18:31,659
around the German Germany you can't

00:18:28,450 --> 00:18:34,299
employ several you can't apply the IOT

00:18:31,659 --> 00:18:36,909
three instance on each factory and each

00:18:34,299 --> 00:18:40,210
Factory it manages at the Italian this

00:18:36,909 --> 00:18:43,750
Factory and then you can use synchronous

00:18:40,210 --> 00:18:47,980
model to synchronize the TS files from

00:18:43,750 --> 00:18:51,010
one this Ltd P instance to your

00:18:47,980 --> 00:18:52,899
datacenter the lttb is dancing in your

00:18:51,010 --> 00:18:57,580
datacenter and then you can get at it

00:18:52,899 --> 00:19:00,490
warehouse Ltd be schema is quite

00:18:57,580 --> 00:19:03,070
different for with other symptoms there

00:19:00,490 --> 00:19:05,559
are three concepts device merriment and

00:19:03,070 --> 00:19:07,860
the time source but us is a machine

00:19:05,559 --> 00:19:10,659
instance for example my car your car

00:19:07,860 --> 00:19:14,529
mouthful my iPhone your iPhone and so on

00:19:10,659 --> 00:19:17,080
and using the V code management as an

00:19:14,529 --> 00:19:19,260
example we can use the country the state

00:19:17,080 --> 00:19:24,340
and the plates number to represent

00:19:19,260 --> 00:19:26,289
represent Rico and amend or some or in

00:19:24,340 --> 00:19:29,320
some application applications is called

00:19:26,289 --> 00:19:32,250
a metric or variable is a variable that

00:19:29,320 --> 00:19:36,370
users are interested in for example the

00:19:32,250 --> 00:19:39,940
vehicle speed your phones remained power

00:19:36,370 --> 00:19:42,279
or something others and at us can have

00:19:39,940 --> 00:19:44,649
many measurements in our applications

00:19:42,279 --> 00:19:49,090
but when the turbine can has more than

00:19:44,649 --> 00:19:53,710
one more than 500 merriments and metro

00:19:49,090 --> 00:19:56,140
metro train has more than 3000 Americans

00:19:53,710 --> 00:20:00,039
and I will show you the applications

00:19:56,140 --> 00:20:03,429
later cumulative ice and merriment we

00:20:00,039 --> 00:20:05,990
call is data format ham series therefore

00:20:03,429 --> 00:20:08,720
the device and the moment

00:20:05,990 --> 00:20:13,870
merriment name from the time society and

00:20:08,720 --> 00:20:17,090
the videos are passed to that contact

00:20:13,870 --> 00:20:21,559
concatenated by thought to represent it

00:20:17,090 --> 00:20:27,230
for example root dot Cadillac at five

00:20:21,559 --> 00:20:29,510
dot dot I see a dot Alabama and to help

00:20:27,230 --> 00:20:31,640
organizing these devices we propose a

00:20:29,510 --> 00:20:36,909
new concept called a storage storage

00:20:31,640 --> 00:20:40,789
group if some devices have similar

00:20:36,909 --> 00:20:44,570
measurements we can assign this devices

00:20:40,789 --> 00:20:47,179
into a store ago but remember that we do

00:20:44,570 --> 00:20:52,450
not require that all the devices in one

00:20:47,179 --> 00:20:57,080
store group have has the same moments

00:20:52,450 --> 00:20:59,330
actually we can map our schema to other

00:20:57,080 --> 00:21:02,059
symptoms for example supposed to be

00:20:59,330 --> 00:21:06,370
house three temperatures like the top

00:21:02,059 --> 00:21:09,320
left top left and the both black

00:21:06,370 --> 00:21:11,720
students for example root thought

00:21:09,320 --> 00:21:14,419
Cadillac active file is the storage

00:21:11,720 --> 00:21:18,860
group name and the black and red color

00:21:14,419 --> 00:21:21,710
and students at us names its plate

00:21:18,860 --> 00:21:26,600
number and the blue students the

00:21:21,710 --> 00:21:28,669
filament has beat and two moments if we

00:21:26,600 --> 00:21:31,100
treating it as a relational table it

00:21:28,669 --> 00:21:33,320
looks like that and the we can consider

00:21:31,100 --> 00:21:36,409
the stored group name as the table name

00:21:33,320 --> 00:21:39,860
and the words in the devices device

00:21:36,409 --> 00:21:42,740
names form the country and the state and

00:21:39,860 --> 00:21:46,940
the device name columns and to represent

00:21:42,740 --> 00:21:50,210
to the devices and the moment for

00:21:46,940 --> 00:21:52,789
example filament and speed very calm our

00:21:50,210 --> 00:21:56,929
value columns and there's another column

00:21:52,789 --> 00:21:59,299
called the timestamp if we treat this

00:21:56,929 --> 00:22:01,909
table as a Cassandra schema you can

00:21:59,299 --> 00:22:04,880
consider the country in state and the

00:22:01,909 --> 00:22:08,720
West name and partition key and the

00:22:04,880 --> 00:22:11,059
timestamp as the catherine key and the

00:22:08,720 --> 00:22:15,110
few women and speed as the value columns

00:22:11,059 --> 00:22:18,110
and the if we treat it has other some

00:22:15,110 --> 00:22:19,880
other time sorted based like Jim Pratley

00:22:18,110 --> 00:22:24,340
be open GS tbo

00:22:19,880 --> 00:22:28,820
cos P P then the first three columns are

00:22:24,340 --> 00:22:34,120
taxed in the systems and the last two

00:22:28,820 --> 00:22:37,130
columns are felt in these systems and

00:22:34,120 --> 00:22:39,740
one more thing when we treat the schema

00:22:37,130 --> 00:22:42,799
as a relational table like this we can

00:22:39,740 --> 00:22:46,250
found there are two cells always been

00:22:42,799 --> 00:22:50,120
now because this this vehicle has no

00:22:46,250 --> 00:22:52,640
maybe has no field women merriment

00:22:50,120 --> 00:22:55,309
that's why Julian said that well if we

00:22:52,640 --> 00:23:00,799
already noted database we may get a

00:22:55,309 --> 00:23:04,309
stable and the IOT DB now provides a

00:23:00,799 --> 00:23:07,400
sicko interface to simply buy how to use

00:23:04,309 --> 00:23:09,950
lttb and we can create her temperatures

00:23:07,400 --> 00:23:13,549
inserts data delete date hands about

00:23:09,950 --> 00:23:17,900
many queries for example and in this in

00:23:13,549 --> 00:23:22,150
this picture you can see a value filter

00:23:17,900 --> 00:23:24,470
and we can actually we can transform the

00:23:22,150 --> 00:23:26,960
temperatures from time domain to the

00:23:24,470 --> 00:23:31,659
frequency domain but this function is

00:23:26,960 --> 00:23:35,030
not implemented till now and aggregation

00:23:31,659 --> 00:23:37,190
aggregation operations and this in

00:23:35,030 --> 00:23:40,970
certain sampling applications for you

00:23:37,190 --> 00:23:44,750
can done several data but each our work

00:23:40,970 --> 00:23:49,400
just get its count state count value all

00:23:44,750 --> 00:23:52,130
Maxwell's and others and now lttb

00:23:49,400 --> 00:23:54,950
suppose this basic database till type

00:23:52,130 --> 00:23:58,789
and then we may support some conflict

00:23:54,950 --> 00:24:02,630
paid data type for example GPS or array

00:23:58,789 --> 00:24:07,760
truth about more applications and this

00:24:02,630 --> 00:24:11,900
is the printed a result by our command

00:24:07,760 --> 00:24:17,600
line and now I will introduce some

00:24:11,900 --> 00:24:19,970
detail about how our TD implements the

00:24:17,600 --> 00:24:23,030
first one is EA's file we caught here as

00:24:19,970 --> 00:24:25,360
well as a zip file born the for time

00:24:23,030 --> 00:24:30,169
series data

00:24:25,360 --> 00:24:33,080
ts file is the calm file and it is about

00:24:30,169 --> 00:24:34,310
the main technology in IOT DB

00:24:33,080 --> 00:24:36,980
videos

00:24:34,310 --> 00:24:39,110
thought to reduce a disk i/o and improve

00:24:36,980 --> 00:24:41,510
the compression ratio and the we

00:24:39,110 --> 00:24:43,880
proposed some encoding methods to lights

00:24:41,510 --> 00:24:46,820
in the compression better and actually

00:24:43,880 --> 00:24:49,250
tell file is quite similar with package

00:24:46,820 --> 00:24:50,900
but it has better performance for time

00:24:49,250 --> 00:24:54,680
source data just for time source data

00:24:50,900 --> 00:24:57,650
and let's see the difference J's file is

00:24:54,680 --> 00:25:00,410
seen inspired from Parker under the

00:24:57,650 --> 00:25:03,260
concept ints file and Parker are similar

00:25:00,410 --> 00:25:05,660
but a style has its definition for

00:25:03,260 --> 00:25:08,020
example the trunk group who Intel's very

00:25:05,660 --> 00:25:13,280
similar with the local group in her head

00:25:08,020 --> 00:25:17,840
but in TS file a chunk of ol install the

00:25:13,280 --> 00:25:19,820
data that belongs to one devices 20s

00:25:17,840 --> 00:25:22,580
under the trunk structure is similar

00:25:19,820 --> 00:25:25,400
with the corn in her head and the trunk

00:25:22,580 --> 00:25:28,220
all stores the data that becomes to one

00:25:25,400 --> 00:25:30,730
T was and one moment and the concept of

00:25:28,220 --> 00:25:34,070
the page is the same with per credit and

00:25:30,730 --> 00:25:36,290
there are some detailed differences for

00:25:34,070 --> 00:25:38,540
example the first thing is that each

00:25:36,290 --> 00:25:41,690
page we do not all install the

00:25:38,540 --> 00:25:44,480
merriments value values but also there

00:25:41,690 --> 00:25:47,270
are timestamps it is because that we say

00:25:44,480 --> 00:25:48,350
we have said the timestamp column is

00:25:47,270 --> 00:25:53,420
always be read

00:25:48,350 --> 00:25:56,120
so in this way when we read in one

00:25:53,420 --> 00:25:59,060
temperatures we do not need to stick on

00:25:56,120 --> 00:26:03,740
tisk and and do need do not in the to

00:25:59,060 --> 00:26:06,860
use the repeat and duplicate duplication

00:26:03,740 --> 00:26:10,640
fields in like in Parker to align the

00:26:06,860 --> 00:26:13,400
two columns and to by reducing the disks

00:26:10,640 --> 00:26:16,460
big disk stick high all those

00:26:13,400 --> 00:26:19,340
performance can be improved secondly

00:26:16,460 --> 00:26:21,950
because we know that the data is time

00:26:19,340 --> 00:26:24,830
source data so the page and the trunk

00:26:21,950 --> 00:26:25,610
header can have many and useful

00:26:24,830 --> 00:26:27,500
informations

00:26:25,610 --> 00:26:30,110
information such as the minimum and

00:26:27,500 --> 00:26:32,380
maximum values in the page or in the

00:26:30,110 --> 00:26:35,750
trunk and the maxima and the minima

00:26:32,380 --> 00:26:39,140
timestamps in the trunk using this info

00:26:35,750 --> 00:26:41,830
we can accelerate the query filter and

00:26:39,140 --> 00:26:45,320
sort the biomass date has also changed

00:26:41,830 --> 00:26:47,600
because one film a how many times maybe

00:26:45,320 --> 00:26:51,410
one pair has

00:26:47,600 --> 00:26:55,250
tons of southern data and the time

00:26:51,410 --> 00:26:56,240
service and we may be and able to load

00:26:55,250 --> 00:26:59,960
all the time

00:26:56,240 --> 00:27:03,980
Velma data into memory therefore vo the

00:26:59,960 --> 00:27:06,710
two levels men had in her now and we do

00:27:03,980 --> 00:27:09,920
not use thrift thrift a protocol to

00:27:06,710 --> 00:27:10,580
civilize they attack in TS file it is

00:27:09,920 --> 00:27:14,330
because that

00:27:10,580 --> 00:27:17,150
lttb's and our GPS system and the

00:27:14,330 --> 00:27:20,870
meaning the to repair the data file if

00:27:17,150 --> 00:27:24,020
the process is charting down abnormally

00:27:20,870 --> 00:27:27,890
and so we need to know what is the mean

00:27:24,020 --> 00:27:31,460
of each pad in the file and if there is

00:27:27,890 --> 00:27:33,230
a broken file we can fix it and append

00:27:31,460 --> 00:27:36,620
the new date had to the tail of the day

00:27:33,230 --> 00:27:39,470
of the tail of the well and however it

00:27:36,620 --> 00:27:41,570
is hard to do that using part I'd be

00:27:39,470 --> 00:27:46,100
called Park in percodans earth to

00:27:41,570 --> 00:27:49,040
civilize the vermin data and tears fell

00:27:46,100 --> 00:27:52,310
implements a list of encoding measured

00:27:49,040 --> 00:27:55,010
technology for different types we adopt

00:27:52,310 --> 00:27:57,740
third half third high encoding called TS

00:27:55,010 --> 00:28:01,100
to deep that starts time to restate

00:27:57,740 --> 00:28:03,380
handle and the value data by calculating

00:28:01,100 --> 00:28:06,590
the third half daughter the the deep

00:28:03,380 --> 00:28:11,270
value is very small and then we can use

00:28:06,590 --> 00:28:13,580
few pits to store them and but the

00:28:11,270 --> 00:28:17,270
compression the compression will fall if

00:28:13,580 --> 00:28:21,560
there are many outlier points or the

00:28:17,270 --> 00:28:24,220
data standpoint is in irregular but this

00:28:21,560 --> 00:28:28,340
scenario is very common industrial

00:28:24,220 --> 00:28:30,950
applications so before the design and

00:28:28,340 --> 00:28:33,590
adaptive version of he has to deep

00:28:30,950 --> 00:28:35,630
encoding which can encode in the time

00:28:33,590 --> 00:28:38,990
source data with the outer layers and or

00:28:35,630 --> 00:28:44,060
missing the points efficiently and the

00:28:38,990 --> 00:28:49,160
technologies we can cut a page into

00:28:44,060 --> 00:28:51,980
several pages to to avoid some missing

00:28:49,160 --> 00:28:57,320
the parrot have some missing points and

00:28:51,980 --> 00:29:00,880
we can use we can't save the outlier

00:28:57,320 --> 00:29:04,420
points separately to

00:29:00,880 --> 00:29:07,870
get a high compression ratio and the for

00:29:04,420 --> 00:29:09,880
intercourse with screenshot repeated the

00:29:07,870 --> 00:29:11,380
values for example five five five five

00:29:09,880 --> 00:29:15,010
four four four four four

00:29:11,380 --> 00:29:17,920
we adopt a re namely run-ins in Cody and

00:29:15,010 --> 00:29:20,380
it encodes the repeated screens by

00:29:17,920 --> 00:29:24,220
storing data store in one value and the

00:29:20,380 --> 00:29:26,680
count and bid packing is for storing the

00:29:24,220 --> 00:29:30,100
integrated chat by using compared piece

00:29:26,680 --> 00:29:33,700
rather than using for paths and for

00:29:30,100 --> 00:29:36,340
floating data we implement gorilla or

00:29:33,700 --> 00:29:39,240
lossless encoding of them provided by

00:29:36,340 --> 00:29:42,460
Facebook and after encoding this

00:29:39,240 --> 00:29:45,040
sequence into binary array and the we

00:29:42,460 --> 00:29:48,130
further compress it once again using

00:29:45,040 --> 00:29:51,640
some well well-known compression

00:29:48,130 --> 00:29:55,950
algorithm like snappy and others and

00:29:51,640 --> 00:30:01,180
this is a detail about our adaptive

00:29:55,950 --> 00:30:03,940
encoding method actually we just and one

00:30:01,180 --> 00:30:08,440
in these are using this Ergon algorithm

00:30:03,940 --> 00:30:10,780
we can check which point which point is

00:30:08,440 --> 00:30:14,890
better to split a painting into several

00:30:10,780 --> 00:30:16,930
pages and the we may feel some missing

00:30:14,890 --> 00:30:21,280
points to get a better compression

00:30:16,930 --> 00:30:24,220
visual and the left is a part of seeker

00:30:21,280 --> 00:30:27,850
that ltd supports and we propose some

00:30:24,220 --> 00:30:30,670
indexes to accelerate the queries for

00:30:27,850 --> 00:30:34,450
example this proposed the pizza index

00:30:30,670 --> 00:30:37,420
PSA index in presenters can let iot

00:30:34,450 --> 00:30:40,890
because the aggregation results in few

00:30:37,420 --> 00:30:45,820
milliseconds billions of data points

00:30:40,890 --> 00:30:49,690
actually it is pre calculated data first

00:30:45,820 --> 00:30:55,120
and organize the pre calculation results

00:30:49,690 --> 00:30:58,330
in forest not whole tree and this these

00:30:55,120 --> 00:31:00,460
are two indices for supporting planting

00:30:58,330 --> 00:31:03,340
the subsequent saul funding the pattern

00:31:00,460 --> 00:31:05,830
among the hot hamsters did set and the

00:31:03,340 --> 00:31:07,300
left is for streaming data and the right

00:31:05,830 --> 00:31:11,160
is for historical data

00:31:07,300 --> 00:31:14,120
this actually these are two papers and

00:31:11,160 --> 00:31:19,880
butter we have two

00:31:14,120 --> 00:31:23,150
to implement them into Ltd be and Ltd be

00:31:19,880 --> 00:31:25,880
can be deployed for a rarity of powers

00:31:23,150 --> 00:31:29,059
for example sometimes we can just use

00:31:25,880 --> 00:31:31,159
file to store data and sometimes we can

00:31:29,059 --> 00:31:34,610
use out a DB choose about the conflict

00:31:31,159 --> 00:31:38,450
queries because tell Fiona support red

00:31:34,610 --> 00:31:41,809
state and some simple query and we can't

00:31:38,450 --> 00:31:45,559
apply Ltd be ot as well and embedded

00:31:41,809 --> 00:31:48,500
devices like Raspberry Pi computer you

00:31:45,559 --> 00:31:54,020
know Factory and deploy Ltd be cluster

00:31:48,500 --> 00:31:56,090
on caste and it's under and let's see

00:31:54,020 --> 00:31:59,690
how LTP works for tensors data

00:31:56,090 --> 00:32:03,529
management because out it with provider

00:31:59,690 --> 00:32:06,230
cycle and we also provided GBC and the

00:32:03,529 --> 00:32:08,809
you can use gdb C to resonate how core

00:32:06,230 --> 00:32:12,950
data and before your red state are you

00:32:08,809 --> 00:32:16,100
need to defend the schema for example

00:32:12,950 --> 00:32:18,169
here V we call them as story group and

00:32:16,100 --> 00:32:21,470
time series

00:32:18,169 --> 00:32:24,830
you can't defend village users different

00:32:21,470 --> 00:32:26,870
answers because temp users know what

00:32:24,830 --> 00:32:29,090
state of type that they'd highest and

00:32:26,870 --> 00:32:32,120
which encoding method is better

00:32:29,090 --> 00:32:36,649
maybe euler's maybe know that because if

00:32:32,120 --> 00:32:41,240
you know that your your your data looks

00:32:36,649 --> 00:32:45,799
like this you can use cause is a sin or

00:32:41,240 --> 00:32:49,429
cos our function so maybe you can do

00:32:45,799 --> 00:32:51,770
some good in coding matters and some or

00:32:49,429 --> 00:32:54,950
you you can implement yourself in coding

00:32:51,770 --> 00:32:57,230
math but you feel maybe you will spend

00:32:54,950 --> 00:33:00,679
that if you know your data looks like

00:32:57,230 --> 00:33:03,440
this then you can use your re encoding

00:33:00,679 --> 00:33:07,010
method of bit packing or some others and

00:33:03,440 --> 00:33:10,399
then you can insert data using insert

00:33:07,010 --> 00:33:16,130
clouds and now actually in our latest

00:33:10,399 --> 00:33:19,760
version we support to defend the claim

00:33:16,130 --> 00:33:23,059
the tempters schema automatically and

00:33:19,760 --> 00:33:27,620
that means others do not do not need to

00:33:23,059 --> 00:33:30,190
answer red Templars sentence

00:33:27,620 --> 00:33:34,340
maybe we cannot gather the better

00:33:30,190 --> 00:33:37,520
encoding method but maybe later we can

00:33:34,340 --> 00:33:41,840
do it introduce some AI technology to

00:33:37,520 --> 00:33:45,500
get a better encoding method and if

00:33:41,840 --> 00:33:48,620
you'll embarrass the performance we can

00:33:45,500 --> 00:33:52,070
use raw data row level API to skip

00:33:48,620 --> 00:33:54,950
parsing a Seco currently videos on

00:33:52,070 --> 00:33:58,520
Thursday to parcel psychopath avi fund

00:33:54,950 --> 00:34:02,870
that the time cost of party music oh

00:33:58,520 --> 00:34:06,020
it's very heavy so we provide a low

00:34:02,870 --> 00:34:09,620
level API to skip party in a Seco and it

00:34:06,020 --> 00:34:13,790
is far efficient and in our latest

00:34:09,620 --> 00:34:17,690
version you can use that API and using

00:34:13,790 --> 00:34:22,040
TDP see you can use result set to create

00:34:17,690 --> 00:34:25,159
handle scan result and if you we have an

00:34:22,040 --> 00:34:27,679
integrated IOT DB into graph ana by

00:34:25,159 --> 00:34:33,980
using the graph analysis team purchase

00:34:27,679 --> 00:34:38,480
and plugin you just we provide restful

00:34:33,980 --> 00:34:41,780
server to provide the to convert I

00:34:38,480 --> 00:34:44,179
wanted to be resulted to JSON format so

00:34:41,780 --> 00:34:51,770
you can use the simple cheese and plug

00:34:44,179 --> 00:34:54,560
in to view the data in IOT DB and you

00:34:51,770 --> 00:35:00,380
can in MATLAB for example there are

00:34:54,560 --> 00:35:03,260
there are many many factories the data

00:35:00,380 --> 00:35:05,660
taste scientists do not know how to Big

00:35:03,260 --> 00:35:08,750
Data symptoms she doesn't know how to

00:35:05,660 --> 00:35:11,240
use MATLAB or how to use other are our

00:35:08,750 --> 00:35:15,260
languages and others then you can use

00:35:11,240 --> 00:35:18,260
gdb in MATLAB you can use gdb C to query

00:35:15,260 --> 00:35:21,140
data from lttb and then you can you can

00:35:18,260 --> 00:35:24,220
use MATLAB to do some some sense

00:35:21,140 --> 00:35:30,620
calculus calculations for example like

00:35:24,220 --> 00:35:33,440
FFT and the sparks Nico you can query

00:35:30,620 --> 00:35:37,430
data and analyze data from what here's

00:35:33,440 --> 00:35:39,780
file or you can also query data from our

00:35:37,430 --> 00:35:45,150
ttv instance

00:35:39,780 --> 00:35:49,050
and I will show two demos here and the

00:35:45,150 --> 00:35:51,030
first demo is how to using lttb to read

00:35:49,050 --> 00:35:54,140
data and the show that happens graph an

00:35:51,030 --> 00:35:57,720
and inspire sequel to analysis data and

00:35:54,140 --> 00:36:00,780
the source code is here and in the

00:35:57,720 --> 00:36:05,760
second demo I will show how to write

00:36:00,780 --> 00:36:08,370
data by using lttb to read a town HDFS

00:36:05,760 --> 00:36:11,730
directory and the using how to analysis

00:36:08,370 --> 00:36:14,730
analyze the data because I didn't

00:36:11,730 --> 00:36:18,240
install high room in my laptop so this

00:36:14,730 --> 00:36:30,590
is the video and let's see the first

00:36:18,240 --> 00:36:30,590
time in this demo

00:36:51,720 --> 00:36:58,530
actually I want to integrate PRC facts

00:36:54,770 --> 00:37:03,690
with Ltd P to show the demo but I have

00:36:58,530 --> 00:37:08,240
no pure C here and and there are some

00:37:03,690 --> 00:37:11,970
cylinders so I write data generator

00:37:08,240 --> 00:37:15,750
crass to correct the data from my laptop

00:37:11,970 --> 00:37:20,970
it can create the CP or the memory and

00:37:15,750 --> 00:37:24,720
the total CPU and the process if you the

00:37:20,970 --> 00:37:31,230
and the interval is one second

00:37:24,720 --> 00:37:34,520
and you actually match the source code

00:37:31,230 --> 00:37:40,140
contents you can crack the data and

00:37:34,520 --> 00:37:42,810
center the data into Kafka a Mac I'm

00:37:40,140 --> 00:37:45,810
Quillan Durden consume data from and

00:37:42,810 --> 00:37:48,090
killed an astable 18 20 DB you can send

00:37:45,810 --> 00:37:51,540
the data into our I'm Cody the server

00:37:48,090 --> 00:37:54,090
and consume data from I'm I'm cool I'm

00:37:51,540 --> 00:37:57,510
cote de server and the res 18 to lttb

00:37:54,090 --> 00:38:01,109
you can also read data directly into IOT

00:37:57,510 --> 00:38:02,790
DB and you can also read data you do

00:38:01,109 --> 00:38:04,410
nothing in the start I wanted to be

00:38:02,790 --> 00:38:07,920
instance in which I already had into T

00:38:04,410 --> 00:38:11,490
as file token I see the first one you

00:38:07,920 --> 00:38:20,820
can rotate how to actually in into J's

00:38:11,490 --> 00:38:23,310
file you know this one and when you read

00:38:20,820 --> 00:38:28,609
state I directly input health is very

00:38:23,310 --> 00:38:31,680
easily for you can see okay you can

00:38:28,609 --> 00:38:35,550
create her T as well but if I write her

00:38:31,680 --> 00:38:39,390
and then for each moment alone you need

00:38:35,550 --> 00:38:42,510
to adjust it by to tell T as well what

00:38:39,390 --> 00:38:46,530
they type it is and which the encoding

00:38:42,510 --> 00:38:49,589
Arizona Mathilde and when you read data

00:38:46,530 --> 00:38:52,680
you just tell tell file the device name

00:38:49,589 --> 00:38:57,480
and the moment name and the time and the

00:38:52,680 --> 00:38:59,580
value and you can read data by this and

00:38:57,480 --> 00:39:03,290
family you just close the data across

00:38:59,580 --> 00:39:03,290
that here's file so let's see

00:39:09,750 --> 00:39:19,030
okay we have at least the fall

00:39:13,480 --> 00:39:21,730
if I was five 10 min 10 seconds and you

00:39:19,030 --> 00:39:24,970
can catch her tears file here and this

00:39:21,730 --> 00:39:31,420
far is worse more maybe several stolen

00:39:24,970 --> 00:39:34,650
bats and actually we provide two to

00:39:31,420 --> 00:39:38,320
print a little file because this is a

00:39:34,650 --> 00:39:42,280
little tool provided by OTT be you can

00:39:38,320 --> 00:39:45,130
print to the sketch healthiest file it's

00:39:42,280 --> 00:39:49,480
to help to you'll know more about the

00:39:45,130 --> 00:39:52,090
earth power it print which bad means

00:39:49,480 --> 00:39:54,280
until file you can find okay there are

00:39:52,090 --> 00:39:57,790
many there are many truant groups and

00:39:54,280 --> 00:39:59,860
for example a trunk or like a temp

00:39:57,790 --> 00:40:06,490
service called the name like this and

00:39:59,860 --> 00:40:08,350
the star x here and an x so this this is

00:40:06,490 --> 00:40:12,040
our what what I mentioned that before

00:40:08,350 --> 00:40:20,470
our info information about her for

00:40:12,040 --> 00:40:24,180
fasting faster query and we can use for

00:40:20,470 --> 00:40:29,140
Salinas let's back to redo the test file

00:40:24,180 --> 00:40:31,510
for some some of analysis for example we

00:40:29,140 --> 00:40:34,000
just read the data from the excel file

00:40:31,510 --> 00:40:36,990
and the you and the runners pass pass

00:40:34,000 --> 00:40:36,990
spark Seco

00:40:44,450 --> 00:40:54,120
okay there are two to comment the first

00:40:49,860 --> 00:40:57,090
one is show that data and the second one

00:40:54,120 --> 00:41:01,310
is show the count of radio okay this is

00:40:57,090 --> 00:41:04,200
a count value under this is data in

00:41:01,310 --> 00:41:09,240
relational table and you can see is a

00:41:04,200 --> 00:41:14,090
quite sparse table and we can also use

00:41:09,240 --> 00:41:14,090
Ltd be two children

00:41:24,920 --> 00:41:41,660
I run IOT TP first today and because I

00:41:37,190 --> 00:41:46,660
run this demo several one year or one

00:41:41,660 --> 00:41:46,660
hours ago so now they are state handed

00:42:05,170 --> 00:42:23,870
so many times yours not you can see

00:42:20,540 --> 00:42:26,930
there are two devices have which have

00:42:23,870 --> 00:42:29,390
the merriment called passes EPO

00:42:26,930 --> 00:42:30,950
the first one is system CPU and the

00:42:29,390 --> 00:42:34,130
second one is the processes if you

00:42:30,950 --> 00:42:39,250
because just an enjoyment process and we

00:42:34,130 --> 00:42:49,420
can use like this to get both the two

00:42:39,250 --> 00:42:52,810
devices in for the recipient Oh oops

00:42:49,420 --> 00:42:52,810
root thought

00:42:57,020 --> 00:43:04,560
sorry it's true to yours if we all do

00:43:01,380 --> 00:43:07,290
not want to wanted right a PPO how to

00:43:04,560 --> 00:43:11,420
write like this and you can catch there

00:43:07,290 --> 00:43:17,130
are two devices have having the CPU

00:43:11,420 --> 00:43:20,270
result and using this is a now we have

00:43:17,130 --> 00:43:25,890
an IOT DB instance and we can and run

00:43:20,270 --> 00:43:28,340
spark to 2 XA analysis data from my OTP

00:43:25,890 --> 00:43:32,280
instances you do not need to define

00:43:28,340 --> 00:43:39,090
which TL file you are looking and by

00:43:32,280 --> 00:43:43,440
just by this this program and it's very

00:43:39,090 --> 00:43:45,060
quite similar with the previous program

00:43:43,440 --> 00:43:49,680
but because I have no time so I will

00:43:45,060 --> 00:43:55,040
skip this and the second demo is how to

00:43:49,680 --> 00:44:02,270
read data on HDFS and you'll have to

00:43:55,040 --> 00:44:02,270
analyze data okay the first one is

00:44:03,380 --> 00:44:09,690
firstly in our latest version you can

00:44:06,540 --> 00:44:12,330
define which date where you store data

00:44:09,690 --> 00:44:16,080
local or means your local date local

00:44:12,330 --> 00:44:20,430
disk and HDFS means your save data HDFS

00:44:16,080 --> 00:44:26,130
and the Union to defend claim where your

00:44:20,430 --> 00:44:29,220
HDFS need and then if we all start Ltd P

00:44:26,130 --> 00:44:34,020
you can say you can find attach your

00:44:29,220 --> 00:44:38,130
data will be stored on HDFS directly so

00:44:34,020 --> 00:44:41,300
this this architecture is quite similar

00:44:38,130 --> 00:44:44,910
with HBase right

00:44:41,300 --> 00:44:49,590
this is hamsters and insert some data

00:44:44,910 --> 00:44:53,160
and then we call flash to be right we

00:44:49,590 --> 00:44:57,270
call command called flash to let out in

00:44:53,160 --> 00:45:00,660
flash data and you can find the on staff

00:44:57,270 --> 00:45:04,320
SEO company data and because the data is

00:45:00,660 --> 00:45:08,480
own HDFS now and the weekend using have

00:45:04,320 --> 00:45:08,480
to analyze the data

00:45:09,520 --> 00:45:20,230
I'm sorry we will start having the you

00:45:16,090 --> 00:45:23,320
need to add a jar jar file to let her

00:45:20,230 --> 00:45:24,780
have know how to understand how to parse

00:45:23,320 --> 00:45:29,430
that here fell

00:45:24,780 --> 00:45:34,119
and then you just create external table

00:45:29,430 --> 00:45:37,630
like this and then if unclaimed world it

00:45:34,119 --> 00:45:39,580
is on the HDFS and then you can you

00:45:37,630 --> 00:45:44,530
describe table and the serac and the

00:45:39,580 --> 00:45:48,030
runs and selected comment for example I

00:45:44,530 --> 00:45:48,030
sneeze okay

00:46:00,930 --> 00:46:11,380
moment it says okay and you will want to

00:46:06,910 --> 00:46:18,160
know more about the exam this demo you

00:46:11,380 --> 00:46:21,490
can see from here and currently we just

00:46:18,160 --> 00:46:25,510
provided Java API but to be healthy the

00:46:21,490 --> 00:46:27,580
RPC level obviously model is implemented

00:46:25,510 --> 00:46:31,120
by so you can maybe you can get other

00:46:27,580 --> 00:46:33,010
language APs and we have been created

00:46:31,120 --> 00:46:35,170
actually we have integrated IOT to be

00:46:33,010 --> 00:46:38,020
with Kafka with dr. McKie with the MQ

00:46:35,170 --> 00:46:41,620
and we want to implement each with cow

00:46:38,020 --> 00:46:44,020
site and Paris works with human to

00:46:41,620 --> 00:46:46,690
insecurity into with cow side we can get

00:46:44,020 --> 00:46:51,190
we can support a standard standard the

00:46:46,690 --> 00:46:55,450
Seco language and there are some use

00:46:51,190 --> 00:46:59,350
cases for the processing be yours

00:46:55,450 --> 00:47:02,530
Ltd be to replace my secret to manage

00:46:59,350 --> 00:47:08,110
some metrological stations in China

00:47:02,530 --> 00:47:11,500
there are about 150 50 meteorological

00:47:08,110 --> 00:47:15,280
stations and each 5 minute per per

00:47:11,500 --> 00:47:19,710
minute it reports the some they talk to

00:47:15,280 --> 00:47:23,860
the dead center and the second scene is

00:47:19,710 --> 00:47:26,230
we use lttb to risk replace the Seco

00:47:23,860 --> 00:47:28,090
single server in German company is a

00:47:26,230 --> 00:47:31,060
Chinese company to help managing the

00:47:28,090 --> 00:47:33,310
devices built by commercial in Japan and

00:47:31,060 --> 00:47:36,850
in the inside applications we manage

00:47:33,310 --> 00:47:43,720
about 200,000 devices and each device

00:47:36,850 --> 00:47:47,620
has generated up 2,400 measurings so

00:47:43,720 --> 00:47:50,440
they own even though they just cracked

00:47:47,620 --> 00:47:52,810
the data per minute but because there

00:47:50,440 --> 00:47:58,030
are so many devices the data point can

00:47:52,810 --> 00:48:01,810
reach up to 1.5 trillion points per day

00:47:58,030 --> 00:48:03,370
and the last one it's the next one is

00:48:01,810 --> 00:48:06,550
videos out it be to replace

00:48:03,370 --> 00:48:09,310
cars peepee in Shanghai Metro management

00:48:06,550 --> 00:48:12,460
instead in this application Ltd be

00:48:09,310 --> 00:48:13,369
managing about stories all 300 railways

00:48:12,460 --> 00:48:16,880
twins

00:48:13,369 --> 00:48:19,489
but now it's about 500 railway trains

00:48:16,880 --> 00:48:24,650
and the each train tracks Switzerland

00:48:19,489 --> 00:48:27,440
and 200 metrics are a 200-mile sentence

00:48:24,650 --> 00:48:29,950
so there are about 400 billion in points

00:48:27,440 --> 00:48:34,150
per day and before we use IOT DB

00:48:29,950 --> 00:48:36,529
Shanghai Metro knows caris DB and

00:48:34,150 --> 00:48:39,589
Cassandra backhand to correct

00:48:36,529 --> 00:48:43,099
just the ones 100 on the 44th trains and

00:48:39,589 --> 00:48:46,640
the correct frequency can't just reach

00:48:43,099 --> 00:48:49,579
how to buy 500 milliseconds better now

00:48:46,640 --> 00:48:55,160
we just use one how did we instance to

00:48:49,579 --> 00:48:59,450
correct a more 2089 it and the real

00:48:55,160 --> 00:49:04,569
applicable application of the Shanghai

00:48:59,450 --> 00:49:04,569
Metro data management

00:49:14,680 --> 00:49:20,720
because this this application is

00:49:17,089 --> 00:49:24,499
deployed in in Shanghai so the network

00:49:20,720 --> 00:49:29,329
is not very good so in Shanghai in

00:49:24,499 --> 00:49:36,200
Shanghai Metro there are about about 15

00:49:29,329 --> 00:49:41,390
15 or 17 low teens in Shanghai and we

00:49:36,200 --> 00:49:45,319
can see the status of some big some

00:49:41,390 --> 00:49:50,900
trains for example we can see this is

00:49:45,319 --> 00:49:57,230
the idea of these trains in 17 loading

00:49:50,900 --> 00:50:00,799
or 17 LAN and this is this is these are

00:49:57,230 --> 00:50:02,660
the metrics of the trim for example

00:50:00,799 --> 00:50:09,440
whether the door is open or whether

00:50:02,660 --> 00:50:12,140
doors closed and maybe the common people

00:50:09,440 --> 00:50:19,279
and persons in the tree and something on

00:50:12,140 --> 00:50:28,489
doors and we can't crack today we can

00:50:19,279 --> 00:50:30,589
correlate how using using page for

00:50:28,489 --> 00:50:33,680
example now we can there are totally

00:50:30,589 --> 00:50:38,119
about LAN one land to land straight

00:50:33,680 --> 00:50:40,730
until land 17 but there's no 14 and 15

00:50:38,119 --> 00:50:45,130
so there are totally 15 lands in

00:50:40,730 --> 00:50:52,549
Shanghai and each land there are about

00:50:45,130 --> 00:50:56,779
26 trans so there are about 400 or 500

00:50:52,549 --> 00:51:01,940
trans totally in Shanghai Metro and for

00:50:56,779 --> 00:51:05,539
each for example let's the I just tried

00:51:01,940 --> 00:51:07,430
this this district and for each tree

00:51:05,539 --> 00:51:11,029
let's see how many metrics or how many

00:51:07,430 --> 00:51:14,509
measurements there are you can see there

00:51:11,029 --> 00:51:20,989
are ten merits in this tree and there

00:51:14,509 --> 00:51:26,040
are 319 pages so they are totally 3000

00:51:20,989 --> 00:51:29,430
and 190 measurements

00:51:26,040 --> 00:51:41,930
each way and we can see that we have

00:51:29,430 --> 00:51:47,150
Karla data it is a chinese' transport

00:51:41,930 --> 00:51:51,660
and the weekends can one taste data

00:51:47,150 --> 00:51:57,990
because this this system use the cars TP

00:51:51,660 --> 00:52:01,260
prover so the developer gave tips to say

00:51:57,990 --> 00:52:03,900
that if you want to scan one taste data

00:52:01,260 --> 00:52:07,920
it's because there are not there are so

00:52:03,900 --> 00:52:10,650
many data so the time cost maybe 30

00:52:07,920 --> 00:52:14,670
minutes so 30 seconds seconds or later

00:52:10,650 --> 00:52:17,540
or longer so but now if you use IOT DP

00:52:14,670 --> 00:52:23,910
is very fast to gather the data

00:52:17,540 --> 00:52:26,970
yeah and the because the time comes and

00:52:23,910 --> 00:52:32,070
and Juliet do you want to introduce this

00:52:26,970 --> 00:52:35,880
one ok ok ok so in the future work we

00:52:32,070 --> 00:52:39,030
want to integrate IOT DB with maybe with

00:52:35,880 --> 00:52:41,640
the cow side to make it easy to use

00:52:39,030 --> 00:52:44,780
integrate it with higher to support

00:52:41,640 --> 00:52:48,480
using how to read in a directory and

00:52:44,780 --> 00:52:52,410
deploy our cluster version and it

00:52:48,480 --> 00:52:56,300
support more advanced functions so

00:52:52,410 --> 00:52:56,300
that's all thank you

00:53:03,460 --> 00:53:16,609
and Danny questions yeah first of all

00:53:13,789 --> 00:53:18,170
very nice talk very interesting the

00:53:16,609 --> 00:53:20,150
first question I have I actually have

00:53:18,170 --> 00:53:21,950
multiple questions the questions the

00:53:20,150 --> 00:53:24,079
first question I have is what is the

00:53:21,950 --> 00:53:26,059
resolution of the time stamp is it fake

00:53:24,079 --> 00:53:28,579
oh their resolution of the time

00:53:26,059 --> 00:53:32,140
stamp what is the minimum unit of time I

00:53:28,579 --> 00:53:35,329
can I can put into the database actually

00:53:32,140 --> 00:53:37,609
we just install time stamp eyes a long

00:53:35,329 --> 00:53:39,950
time and the week and let's use or

00:53:37,609 --> 00:53:41,690
defend also mean of the long time you

00:53:39,950 --> 00:53:45,079
can't define charge another time you

00:53:41,690 --> 00:53:47,599
can't define as I was that my cycle time

00:53:45,079 --> 00:53:49,400
dollars okay and the second question

00:53:47,599 --> 00:53:52,099
that arose in my mind as you were

00:53:49,400 --> 00:53:54,710
starting I owe TVB have you experienced

00:53:52,099 --> 00:53:56,299
any difference in the performance which

00:53:54,710 --> 00:53:57,890
file system is used for example if

00:53:56,299 --> 00:54:00,380
you're spinning it up on Windows is it

00:53:57,890 --> 00:54:04,460
significantly worse than on Linux for

00:54:00,380 --> 00:54:09,440
example with the TP t DB performance

00:54:04,460 --> 00:54:11,029
okay actually I have this file let's say

00:54:09,440 --> 00:54:13,130
it's a quickly for example of

00:54:11,029 --> 00:54:15,799
compression you can see we compare IOT

00:54:13,130 --> 00:54:18,910
DP with if our DB with tom scarda Tonica

00:54:15,799 --> 00:54:22,160
how speedy under positive ian is another

00:54:18,910 --> 00:54:26,479
chinese that times our database you can

00:54:22,160 --> 00:54:27,440
see we stole 100 gigabytes data and our

00:54:26,479 --> 00:54:33,229
TV :

00:54:27,440 --> 00:54:35,749
use 880 are bad proposal my question was

00:54:33,229 --> 00:54:37,999
more or less if the underlying operating

00:54:35,749 --> 00:54:40,369
system or the underlying file system to

00:54:37,999 --> 00:54:41,599
which you are writing your files aha if

00:54:40,369 --> 00:54:45,319
this has changed for example you're

00:54:41,599 --> 00:54:47,269
using NTFS on Windows and using x3 or

00:54:45,319 --> 00:54:52,789
something like that on Linux

00:54:47,269 --> 00:54:56,509
ok rewrites data by using our API so we

00:54:52,789 --> 00:54:58,849
write they'd had to actually disk yeah

00:54:56,509 --> 00:55:00,680
but you're still relying on the on the

00:54:58,849 --> 00:55:03,200
on the OS layer

00:55:00,680 --> 00:55:05,450
yeah the father says chambre is that the

00:55:03,200 --> 00:55:08,660
database is based on these TS files

00:55:05,450 --> 00:55:10,969
which are basically a bit like in HDFS

00:55:08,660 --> 00:55:12,469
there's packages which are like 200

00:55:10,969 --> 00:55:12,920
megabytes or something so we have not

00:55:12,469 --> 00:55:14,390
the issue

00:55:12,920 --> 00:55:16,850
you with guitar something that they have

00:55:14,390 --> 00:55:18,440
too many small files so therefore it

00:55:16,850 --> 00:55:21,550
basically doesn't matter which file

00:55:18,440 --> 00:55:23,750
system case there's no really in-depth

00:55:21,550 --> 00:55:25,100
explanation we have done but it doesn't

00:55:23,750 --> 00:55:33,680
matter that much as we just do book

00:55:25,100 --> 00:55:36,950
rights in big big minor found a doctor

00:55:33,680 --> 00:55:38,600
on HDFS and you're right state however

00:55:36,950 --> 00:55:43,040
fast you started a very fast that

00:55:38,600 --> 00:55:47,530
there's maybe a post like her post yeah

00:55:43,040 --> 00:55:50,960
because you write data in HDFS too fast

00:55:47,530 --> 00:55:54,650
and my last question at the moment here

00:55:50,960 --> 00:55:56,740
you're supporting JDBC are there any

00:55:54,650 --> 00:55:59,540
plans to support other language or other

00:55:56,740 --> 00:56:01,370
that possibilities for example to

00:55:59,540 --> 00:56:01,940
extract data directly from Python or

00:56:01,370 --> 00:56:06,200
stuff like that

00:56:01,940 --> 00:56:10,580
yeah we can use Swift to compile the API

00:56:06,200 --> 00:56:14,480
of pencil and the C pass pass but we do

00:56:10,580 --> 00:56:17,030
not support ODBC or some others okay

00:56:14,480 --> 00:56:19,730
okay if you feel you feel ready to the

00:56:17,030 --> 00:56:22,940
source code of actually we have two

00:56:19,730 --> 00:56:24,980
peers per class now and one is for

00:56:22,940 --> 00:56:29,300
supported passer and another is for

00:56:24,980 --> 00:56:31,130
support has passed yeah okay and a

00:56:29,300 --> 00:56:33,530
comment from my side probably we have

00:56:31,130 --> 00:56:35,210
some arrow people around here because I

00:56:33,530 --> 00:56:37,580
think error will also be a pretty nice

00:56:35,210 --> 00:56:39,800
fit as we've run a large data settings

00:56:37,580 --> 00:56:43,010
nice way to transport it basically from

00:56:39,800 --> 00:56:44,870
Java to Python or whatever so so

00:56:43,010 --> 00:56:46,070
probably if we get on this conference in

00:56:44,870 --> 00:56:47,500
contact with some error guys would be

00:56:46,070 --> 00:57:01,420
awesome to discuss with them how to

00:56:47,500 --> 00:57:01,420
basically make the data into thank you

00:57:11,690 --> 00:57:18,450
do you support any retention policy on

00:57:15,000 --> 00:57:22,440
your data now we are supporting that you

00:57:18,450 --> 00:57:28,890
can see this one and supports historical

00:57:22,440 --> 00:57:30,839
level data et al okay and maybe either

00:57:28,890 --> 00:57:33,390
questions overlap but I was going to ask

00:57:30,839 --> 00:57:35,640
about aggregation of past data such as I

00:57:33,390 --> 00:57:38,849
think you have in open TSG be the

00:57:35,640 --> 00:57:40,230
capacity for the system to aggregate so

00:57:38,849 --> 00:57:47,880
you don't keep too much data in the

00:57:40,230 --> 00:57:51,480
system so now we are we to which there's

00:57:47,880 --> 00:57:56,369
no limitation about the application you

00:57:51,480 --> 00:57:58,650
can you can say I want to do a down

00:57:56,369 --> 00:58:01,410
sampling or accretion for one year's

00:57:58,650 --> 00:58:05,010
data or 14 years data for just one

00:58:01,410 --> 00:58:08,130
tastes data but the performance may pity

00:58:05,010 --> 00:58:14,150
for me better because how many data

00:58:08,130 --> 00:58:14,150
you're we need to rate from the disk

00:58:17,869 --> 00:58:22,170
every time I have all the data

00:58:20,400 --> 00:58:26,190
accessible all the time

00:58:22,170 --> 00:58:28,650
currently we do not turn sample and data

00:58:26,190 --> 00:58:31,079
automatically automatically but now we

00:58:28,650 --> 00:58:34,200
are we are writing we have implemented

00:58:31,079 --> 00:58:37,589
the pre scientist into how to be after

00:58:34,200 --> 00:58:39,869
that the down sampling data will be

00:58:37,589 --> 00:58:42,680
started automatically before your query

00:58:39,869 --> 00:58:42,680
data but

00:58:52,109 --> 00:58:56,529
and at a another point which probably

00:58:54,789 --> 00:58:58,569
comes with the calcite integration when

00:58:56,529 --> 00:59:00,759
we integrate calcite then we have a

00:58:58,569 --> 00:59:03,249
powerful query planner and COO can use

00:59:00,759 --> 00:59:04,749
material is used as kind of indexes and

00:59:03,249 --> 00:59:07,920
then it will from our perspective to be

00:59:04,749 --> 00:59:11,019
more elegant to provide indexes for

00:59:07,920 --> 00:59:13,359
aggregations you you search oftentimes

00:59:11,019 --> 00:59:15,700
or you configure because for industrial

00:59:13,359 --> 00:59:17,319
data usually you need the raw data at

00:59:15,700 --> 00:59:19,630
any point in time but probably you want

00:59:17,319 --> 00:59:21,910
to have some regular intervals forests

00:59:19,630 --> 00:59:23,230
so the idea is if we have the cows that

00:59:21,910 --> 00:59:24,519
integration and we could add something

00:59:23,230 --> 00:59:26,980
like this that you say ok I want an

00:59:24,519 --> 00:59:29,079
index for 10 minutes samples for one day

00:59:26,980 --> 00:59:31,690
samples and you basically get both of

00:59:29,079 --> 00:59:35,589
data but with with the speed you want

00:59:31,690 --> 00:59:40,690
basically and another question if I may

00:59:35,589 --> 00:59:45,329
do you have and so on so you have many

00:59:40,690 --> 00:59:55,509
moving things you integrate some

00:59:45,329 --> 00:59:58,539
coordinates not your graph data that's

00:59:55,509 --> 01:00:00,359
why I say maybe later we can support GPS

00:59:58,539 --> 01:00:03,059
data or something others support

01:00:00,359 --> 01:00:14,759
trajectory data or something others

01:00:03,059 --> 01:00:14,759

YouTube URL: https://www.youtube.com/watch?v=LLFc3yPyTFY


