Title: #ACEU19: Paul Brebner â€“ Kafka, Cassandra and Kubernetes at Scale
Publication date: 2019-10-24
Playlist: ApacheCon Europe 2019 â€“ Berlin
Description: 
	Full title: Kafka, Cassandra and Kubernetes at Scale - Real-time Anomaly detection on 19 billion events a day

More: https://aceu19.apachecon.com/session/kafka-cassandra-and-kubernetes-scale-real-time-anomaly-detection-19-billion-events-day

Apache Kafka, Apache Cassandra and Kubernetes are open source big data technologies enabling applications and business operations to scale massively and rapidly. While Kafka and Cassandra underpins the data layer of the stack providing capability to stream, disseminate, store and retrieve data at very low latency, Kubernetes is a container orchestration technology that helps in automated application deployment and scaling of application clusters. In this presentation, we will reveal how we architected a massive scale deployment of a streaming data pipeline with Kafka and Cassandra to cater to an example Anomaly detection application running on a Kubernetes cluster and generating and processing massive amount of events. Anomaly detection is a method used to detect unusual events in an event stream. It is widely used in a range of applications such as financial fraud detection, security, threat detection, website user analytics, sensors, IoT, system health monitoring, etc. When such applications operate at massive scale generating millions or billions of events, they impose significant computational, performance and scalability challenges to anomaly detection algorithms and data layer technologies. We will demonstrate the scalability, performance and cost effectiveness of Apache Kafka, Cassandra and Kubernetes, with results from our experiments allowing the Anomaly detection application to scale to 19 Billion anomaly checks per day.
Captions: 
	00:00:04,630 --> 00:00:09,940
I'm Paul I'm a technology evangelist for

00:00:08,440 --> 00:00:12,610
insta cluster which is a company based

00:00:09,940 --> 00:00:15,279
in Canberra so I apologize in advance

00:00:12,610 --> 00:00:16,960
I'm probably slightly jet-lagged still I

00:00:15,279 --> 00:00:20,830
noticed the first day I was in building

00:00:16,960 --> 00:00:22,330
I was using my google map application on

00:00:20,830 --> 00:00:24,430
my phone and it was thinking I was going

00:00:22,330 --> 00:00:26,470
in the opposite direction to what I was

00:00:24,430 --> 00:00:29,619
I think it thought that I was still sort

00:00:26,470 --> 00:00:31,570
of down under and 180 degrees out of

00:00:29,619 --> 00:00:35,320
sync and my brain was a bit like that

00:00:31,570 --> 00:00:37,629
still unfortunately so this is quite

00:00:35,320 --> 00:00:40,390
exciting talking in a machine learning

00:00:37,629 --> 00:00:42,789
track at Apache con it's actually I was

00:00:40,390 --> 00:00:45,300
thinking back at the last time I was at

00:00:42,789 --> 00:00:48,370
a machine learning conference was 1987

00:00:45,300 --> 00:00:50,319
at UCI it was the fourth International

00:00:48,370 --> 00:00:52,300
machine learning workshop and I was

00:00:50,319 --> 00:00:54,969
doing my PhD in machine learning at the

00:00:52,300 --> 00:00:56,649
time and I didn't finish it and I ended

00:00:54,969 --> 00:00:59,260
up doing a software engineering research

00:00:56,649 --> 00:01:03,399
and distributed systems and it's sort of

00:00:59,260 --> 00:01:06,100
interesting that 32 years on I'm giving

00:01:03,399 --> 00:01:07,510
a talk with machine learning flavor and

00:01:06,100 --> 00:01:09,700
I think there's an interesting

00:01:07,510 --> 00:01:11,259
convergence because part of the problem

00:01:09,700 --> 00:01:13,119
we had 30 years ago with machine

00:01:11,259 --> 00:01:14,950
learning was the computers just weren't

00:01:13,119 --> 00:01:17,560
powerful enough and you couldn't connect

00:01:14,950 --> 00:01:19,920
multiple computers together easily but

00:01:17,560 --> 00:01:22,329
you could but it was really hard work

00:01:19,920 --> 00:01:24,460
but now the there's sort of a

00:01:22,329 --> 00:01:26,530
convergence of some of the scalability

00:01:24,460 --> 00:01:28,990
technologies for including things like

00:01:26,530 --> 00:01:30,399
cafe and Cassandra with some of the

00:01:28,990 --> 00:01:32,380
machine learning research which actually

00:01:30,399 --> 00:01:34,600
gives you the ability to do really

00:01:32,380 --> 00:01:36,009
scalable machine learning type type

00:01:34,600 --> 00:01:38,109
things so so I think there is a

00:01:36,009 --> 00:01:40,659
connection so I hope I'm not too much of

00:01:38,109 --> 00:01:42,789
an imposter at the machine learning

00:01:40,659 --> 00:01:45,070
track and that there's some benefit in

00:01:42,789 --> 00:01:47,020
this talk so because I'm a technology

00:01:45,070 --> 00:01:48,640
evangelist I sort of get to play with

00:01:47,020 --> 00:01:52,390
some of the technologies that the

00:01:48,640 --> 00:01:54,789
company I work for insta hosts on cloud

00:01:52,390 --> 00:01:56,710
providers so in particular category and

00:01:54,789 --> 00:01:58,840
Cassandra so I get to sort of learn the

00:01:56,710 --> 00:02:00,939
technology is myself I didn't know

00:01:58,840 --> 00:02:04,450
either of these technologies two years

00:02:00,939 --> 00:02:06,219
ago so I had to learn them I then had to

00:02:04,450 --> 00:02:08,380
think of it interesting use case for

00:02:06,219 --> 00:02:09,880
them and then start to build an

00:02:08,380 --> 00:02:11,650
application and get it working and then

00:02:09,880 --> 00:02:15,100
blog about it and even talk about it in

00:02:11,650 --> 00:02:16,390
some cases like today so this first talk

00:02:15,100 --> 00:02:18,460
I make sure giving two talks today this

00:02:16,390 --> 00:02:19,990
first one is based on the most

00:02:18,460 --> 00:02:21,910
recent work that I've done over the last

00:02:19,990 --> 00:02:24,670
six months so it's the most up-to-date

00:02:21,910 --> 00:02:26,440
hopefully the the most relevant there's

00:02:24,670 --> 00:02:28,420
a talk I'm giving again I think at two

00:02:26,440 --> 00:02:31,180
o'clock in the afternoon which is some

00:02:28,420 --> 00:02:33,670
work I did just using Kafka for an IAT

00:02:31,180 --> 00:02:35,110
application which I did last year so

00:02:33,670 --> 00:02:36,820
it's a bit out of date but that was my

00:02:35,110 --> 00:02:38,890
first attempt at learning catharsis and

00:02:36,820 --> 00:02:40,750
it's got a lot more about some of the

00:02:38,890 --> 00:02:45,340
mistakes I made hopefully and what not

00:02:40,750 --> 00:02:47,830
to do so let's get going um so here's

00:02:45,340 --> 00:02:50,580
the overview of this talk it's going to

00:02:47,830 --> 00:02:54,700
have six parts the WoW part headlines

00:02:50,580 --> 00:02:56,320
why did we do it what does it do how

00:02:54,700 --> 00:02:59,680
does it work and how well did it work

00:02:56,320 --> 00:03:02,350
and finally some so what sort of issues

00:02:59,680 --> 00:03:04,270
first of all the well part the headlines

00:03:02,350 --> 00:03:05,980
some sort of doing this back to front

00:03:04,270 --> 00:03:09,310
I'm giving you the results first and

00:03:05,980 --> 00:03:10,750
then explain how we get there so fifty

00:03:09,310 --> 00:03:11,590
years ago it was quite a significant

00:03:10,750 --> 00:03:14,130
year in history of the world

00:03:11,590 --> 00:03:16,960
particularly in the US and sorry

00:03:14,130 --> 00:03:19,690
European people this is a bit us focused

00:03:16,960 --> 00:03:22,330
I gave this talk in Patrick on Las Vegas

00:03:19,690 --> 00:03:24,820
last month so I don't think Elvis is

00:03:22,330 --> 00:03:27,820
coming home again anytime soon but sure

00:03:24,820 --> 00:03:29,770
enough man did walk on the moon and

00:03:27,820 --> 00:03:32,050
there was this funny festival thing that

00:03:29,770 --> 00:03:35,200
took all fun there was a new president

00:03:32,050 --> 00:03:37,360
and a few other things like that so yep

00:03:35,200 --> 00:03:40,150
man walked on the moon that was pretty

00:03:37,360 --> 00:03:43,000
exciting at the time I remember it this

00:03:40,150 --> 00:03:47,230
other other thing three days of peace

00:03:43,000 --> 00:03:48,910
and music Woodstock 50,000 people were

00:03:47,230 --> 00:03:51,670
expected that was how many tickets that

00:03:48,910 --> 00:03:54,730
sold in advance 1 million people

00:03:51,670 --> 00:03:56,260
actually tried to get in and about half

00:03:54,730 --> 00:03:57,640
a million people actually were there for

00:03:56,260 --> 00:04:01,660
some of the time they reckon so it was a

00:03:57,640 --> 00:04:05,170
pretty big event fast forwarding 50

00:04:01,660 --> 00:04:07,510
years we announced the results of this

00:04:05,170 --> 00:04:09,070
project a couple of months ago we had a

00:04:07,510 --> 00:04:11,650
press release and they got picked up by

00:04:09,070 --> 00:04:13,000
quite a few agencies so it was quite I

00:04:11,650 --> 00:04:16,180
was quite pleased that people were

00:04:13,000 --> 00:04:18,700
interested in this type of topic and the

00:04:16,180 --> 00:04:22,390
sort of the headline that we we had 19

00:04:18,700 --> 00:04:25,240
billion anomaly checks a day but as 19

00:04:22,390 --> 00:04:29,020
billion anomaly checks a day a lot or

00:04:25,240 --> 00:04:30,949
not well depends bit let's have a look

00:04:29,020 --> 00:04:33,479
at this in terms of a more traditional

00:04:30,949 --> 00:04:36,840
transactions per second metric so in

00:04:33,479 --> 00:04:38,220
terms of checks per second the system

00:04:36,840 --> 00:04:40,470
that we built could achieve two hundred

00:04:38,220 --> 00:04:42,449
and twenty thousand checks per second

00:04:40,470 --> 00:04:44,729
well it's a slightly lower and less

00:04:42,449 --> 00:04:46,590
impressive number but in terms of the

00:04:44,729 --> 00:04:49,080
daily throughput it's the same thing I

00:04:46,590 --> 00:04:51,990
had to look around for some similar

00:04:49,080 --> 00:04:55,229
results so these aren't exactly the same

00:04:51,990 --> 00:04:57,240
in terms of how they achieved them but I

00:04:55,229 --> 00:04:59,100
did use a similar technology stack to us

00:04:57,240 --> 00:05:00,720
they use caf-co cassandra and spark

00:04:59,100 --> 00:05:03,570
there was some actual machine learning

00:05:00,720 --> 00:05:05,039
going on and model building but they

00:05:03,570 --> 00:05:08,010
only managed to achieve four hundred and

00:05:05,039 --> 00:05:09,210
forty chicks a second so we're a lot

00:05:08,010 --> 00:05:14,940
more than that so that's sort of

00:05:09,210 --> 00:05:16,680
exciting parts of the system actually

00:05:14,940 --> 00:05:18,570
achieve a higher throughput than that

00:05:16,680 --> 00:05:21,930
we've actually used Kafka as a buffer

00:05:18,570 --> 00:05:24,690
and that part of the system has a peak

00:05:21,930 --> 00:05:26,340
of 2.3 million Kefka writes a second

00:05:24,690 --> 00:05:27,690
going into it which is about an order of

00:05:26,340 --> 00:05:30,389
magnitude more than the rest of the

00:05:27,690 --> 00:05:33,210
pipeline so in this case captors kefka's

00:05:30,389 --> 00:05:34,860
absorbing a load spike up to ten times

00:05:33,210 --> 00:05:38,699
what the rest of the system can cope

00:05:34,860 --> 00:05:41,940
with so it's another big number so back

00:05:38,699 --> 00:05:43,949
to the the billions add a numbers is

00:05:41,940 --> 00:05:45,900
this big in terms of the planet well yes

00:05:43,949 --> 00:05:48,479
population of the planets is about eight

00:05:45,900 --> 00:05:50,370
billion people at the moment to 19

00:05:48,479 --> 00:05:53,090
billion checks a day corresponds to

00:05:50,370 --> 00:05:56,039
about 2.5 events per person per day

00:05:53,090 --> 00:05:57,690
which seems pretty cool we did have to

00:05:56,039 --> 00:05:59,460
stop somewhere there wasn't theory in

00:05:57,690 --> 00:06:02,370
that upper limit to what we could have

00:05:59,460 --> 00:06:03,960
got out of this system and there was

00:06:02,370 --> 00:06:08,070
sort of arbitrary where we decided to

00:06:03,960 --> 00:06:10,349
stop so why did we do it what were the

00:06:08,070 --> 00:06:12,120
project goals were first of all we had

00:06:10,349 --> 00:06:15,260
multiple goals like this slightly

00:06:12,120 --> 00:06:17,729
obscure Australian game called AFL

00:06:15,260 --> 00:06:20,280
rather than just two goals they have

00:06:17,729 --> 00:06:23,880
this concept of having four I've never

00:06:20,280 --> 00:06:25,979
understood it myself so the first goals

00:06:23,880 --> 00:06:27,990
that we were talking about fast data

00:06:25,979 --> 00:06:30,150
processing we won't have to be able to

00:06:27,990 --> 00:06:33,210
process streams data essentially in real

00:06:30,150 --> 00:06:35,789
time which means sub-second so a lot of

00:06:33,210 --> 00:06:38,460
the events to come in run the algorithms

00:06:35,789 --> 00:06:41,370
and make a decision in under one second

00:06:38,460 --> 00:06:43,440
consistently so the second project goal

00:06:41,370 --> 00:06:44,190
was to do with big data in terms of

00:06:43,440 --> 00:06:46,920
throughput

00:06:44,190 --> 00:06:47,850
and the size of the system we didn't

00:06:46,920 --> 00:06:50,250
want the assistant they have any

00:06:47,850 --> 00:06:52,340
arbitrary upper limit - scalability and

00:06:50,250 --> 00:06:55,020
I mean we actually did want to produce

00:06:52,340 --> 00:06:58,410
some big benchmark numbers that we could

00:06:55,020 --> 00:07:00,770
publicize as well to be honest we also

00:06:58,410 --> 00:07:03,150
wanted the system to be cost effective

00:07:00,770 --> 00:07:05,850
essentially this means that we wanted to

00:07:03,150 --> 00:07:07,890
be incrementally scalable so that users

00:07:05,850 --> 00:07:09,090
could only pay for what they used and

00:07:07,890 --> 00:07:11,700
you could say you could say as the

00:07:09,090 --> 00:07:15,480
system for the demand that you expected

00:07:11,700 --> 00:07:17,580
and with a high benefit to cost ratio so

00:07:15,480 --> 00:07:19,530
if you only need a system half the size

00:07:17,580 --> 00:07:22,620
you should be able to to achieve that

00:07:19,530 --> 00:07:24,300
this is a picture of a half a car from

00:07:22,620 --> 00:07:27,480
an Australian movie called milk and it's

00:07:24,300 --> 00:07:29,940
quite fun if you haven't seen it we

00:07:27,480 --> 00:07:31,740
wanted to use two of the technologies

00:07:29,940 --> 00:07:35,370
that insta cluster offers as a manager

00:07:31,740 --> 00:07:36,720
service Apache Kafka and Cassandra so we

00:07:35,370 --> 00:07:38,700
had to come up with the use case that

00:07:36,720 --> 00:07:42,930
used both of these together in some

00:07:38,700 --> 00:07:44,580
interesting way so our platform does

00:07:42,930 --> 00:07:46,230
virtually everything for you including

00:07:44,580 --> 00:07:49,230
things like provisioning monitoring

00:07:46,230 --> 00:07:51,060
scaling and more so and I use those sort

00:07:49,230 --> 00:07:52,910
of features extensively as I was

00:07:51,060 --> 00:07:55,530
experimenting with the technologies and

00:07:52,910 --> 00:07:59,669
training the system and finally scaling

00:07:55,530 --> 00:08:01,230
it for the final results the other thing

00:07:59,669 --> 00:08:02,820
we were interested in was using Kafka as

00:08:01,230 --> 00:08:05,430
a buffer so this is one of the use cases

00:08:02,820 --> 00:08:07,260
of Kafka you sort of get it for free it

00:08:05,430 --> 00:08:09,510
just it acts as a buffer by this

00:08:07,260 --> 00:08:11,820
definition that decouples incoming and

00:08:09,510 --> 00:08:13,830
outgoing events the consumers don't have

00:08:11,820 --> 00:08:15,720
to be ready to read the events until

00:08:13,830 --> 00:08:18,510
they're ready and it will absorb

00:08:15,720 --> 00:08:20,970
essentially an infinite amount of load

00:08:18,510 --> 00:08:24,030
short periods of time so it's cost

00:08:20,970 --> 00:08:25,500
effective for short load spikes that

00:08:24,030 --> 00:08:27,540
prevents eye overloading the rest of the

00:08:25,500 --> 00:08:29,520
pipeline and all events are eventually

00:08:27,540 --> 00:08:32,310
processed so it's fine if you don't have

00:08:29,520 --> 00:08:34,110
a particularly strict SLI on on all the

00:08:32,310 --> 00:08:38,760
events and you've got some leeway to um

00:08:34,110 --> 00:08:40,200
the process some of them later on the

00:08:38,760 --> 00:08:41,669
other part of the experiment we were

00:08:40,200 --> 00:08:43,409
quite interested in was the ability to

00:08:41,669 --> 00:08:45,990
automate the application and this came

00:08:43,409 --> 00:08:48,660
out of my previous attempts to to build

00:08:45,990 --> 00:08:51,390
scalable applications and with Kefka in

00:08:48,660 --> 00:08:53,160
particular the applications themselves

00:08:51,390 --> 00:08:54,780
are actually quite difficult to scale

00:08:53,160 --> 00:08:56,160
and you want to be able to automate the

00:08:54,780 --> 00:08:57,990
scaling

00:08:56,160 --> 00:09:01,700
for those and the other aspect is

00:08:57,990 --> 00:09:04,170
observability so in order to be able to

00:09:01,700 --> 00:09:06,450
understand the system sufficiently to be

00:09:04,170 --> 00:09:09,000
able to tune it and scale it and debug

00:09:06,450 --> 00:09:11,070
it in some cases you have to have good

00:09:09,000 --> 00:09:15,420
quality monitoring of the application

00:09:11,070 --> 00:09:17,220
not just the the clusters themselves so

00:09:15,420 --> 00:09:18,330
in order to do that we decided we were

00:09:17,220 --> 00:09:20,250
going to use kubernetes for the

00:09:18,330 --> 00:09:21,900
application automation Prometheus for

00:09:20,250 --> 00:09:23,880
the application monitoring and something

00:09:21,900 --> 00:09:29,640
called open trace English the Jaeger

00:09:23,880 --> 00:09:32,760
tracer for the distributed tracing so

00:09:29,640 --> 00:09:33,720
I've sort of assumed that that we we

00:09:32,760 --> 00:09:35,220
know what we're doing or we don't

00:09:33,720 --> 00:09:37,170
actually know what it does yet so let's

00:09:35,220 --> 00:09:40,260
have a bit more of a drill down into the

00:09:37,170 --> 00:09:42,030
actual application itself it's an

00:09:40,260 --> 00:09:46,890
anomaly detection use case so it's

00:09:42,030 --> 00:09:48,720
designed for spotting unusual events so

00:09:46,890 --> 00:09:51,300
it was interesting when they did put men

00:09:48,720 --> 00:09:52,440
on the moon fifty years ago there were

00:09:51,300 --> 00:09:54,500
lots of people that got them there but

00:09:52,440 --> 00:09:57,720
there was actually any one woman in the

00:09:54,500 --> 00:09:59,820
Apollo 11 control room so she was a bit

00:09:57,720 --> 00:10:01,590
of an anomaly but a critical one because

00:09:59,820 --> 00:10:05,220
she was in charge of the monitoring of

00:10:01,590 --> 00:10:07,470
the second five rocket itself so the

00:10:05,220 --> 00:10:12,150
goals for anomaly detection now to spot

00:10:07,470 --> 00:10:14,550
the difference at speed and scale so

00:10:12,150 --> 00:10:16,260
first of all the speed as I mentioned we

00:10:14,550 --> 00:10:18,300
wanted to be able to spot the difference

00:10:16,260 --> 00:10:20,610
in potentially quite large amounts of

00:10:18,300 --> 00:10:22,020
data in under one second so we're

00:10:20,610 --> 00:10:23,670
definitely talking about something in

00:10:22,020 --> 00:10:26,790
the space of streams processing not

00:10:23,670 --> 00:10:28,260
batch processing at this point so that

00:10:26,790 --> 00:10:33,150
that determined the type of technology

00:10:28,260 --> 00:10:36,180
we we picked scalability was in terms of

00:10:33,150 --> 00:10:37,890
both the the size of a key space and the

00:10:36,180 --> 00:10:41,280
concurrency of the data coming into the

00:10:37,890 --> 00:10:42,930
system and subsequently stored so we

00:10:41,280 --> 00:10:46,080
were dealing with data that potentially

00:10:42,930 --> 00:10:48,240
had billions of keys so we needed a big

00:10:46,080 --> 00:10:51,900
data database to be able to store and

00:10:48,240 --> 00:10:53,490
query that amount of data and also for

00:10:51,900 --> 00:10:56,610
the processing it had implications for

00:10:53,490 --> 00:10:57,900
the processing scalability as well so

00:10:56,610 --> 00:11:01,010
some of the dimensions for scalability

00:10:57,900 --> 00:11:03,870
are the the load including data velocity

00:11:01,010 --> 00:11:06,300
the load was expected to increase

00:11:03,870 --> 00:11:09,860
there was no upper bound and you would

00:11:06,300 --> 00:11:09,860
get load spikes as well

00:11:09,960 --> 00:11:13,830
the other part of that story is the

00:11:11,760 --> 00:11:15,420
affordability sure you could build a

00:11:13,830 --> 00:11:17,940
system that was big enough perhaps to

00:11:15,420 --> 00:11:19,890
process the maximum possible load spike

00:11:17,940 --> 00:11:21,480
but in order to be affordable you

00:11:19,890 --> 00:11:22,980
actually have one that has linear

00:11:21,480 --> 00:11:25,680
resource scalability so it has to be

00:11:22,980 --> 00:11:29,130
elastic on demand you need incremental

00:11:25,680 --> 00:11:33,510
resources which change with increasing

00:11:29,130 --> 00:11:35,010
load those lots of not only protection

00:11:33,510 --> 00:11:36,900
use cases which is another reason that

00:11:35,010 --> 00:11:39,210
we picked it as a as a sort of

00:11:36,900 --> 00:11:41,670
application area in a sense it didn't

00:11:39,210 --> 00:11:44,510
matter what the application domain was

00:11:41,670 --> 00:11:47,550
people could imagine their own use cases

00:11:44,510 --> 00:11:50,190
so there are many and varied including

00:11:47,550 --> 00:11:54,810
infrastructure monitoring application

00:11:50,190 --> 00:11:57,920
monitoring IOT finance fraud detection

00:11:54,810 --> 00:11:59,610
quite a big one and for this sort of

00:11:57,920 --> 00:12:02,280
space-click stream analytics

00:11:59,610 --> 00:12:04,620
increasingly and even odd things

00:12:02,280 --> 00:12:08,480
potentially like drone deliveries which

00:12:04,620 --> 00:12:11,040
have got a lot of moving parts in fact

00:12:08,480 --> 00:12:14,670
ok so the next part of the story is well

00:12:11,040 --> 00:12:16,050
how does it actually work how is it made

00:12:14,670 --> 00:12:18,300
we're gonna have a look at the anomaly

00:12:16,050 --> 00:12:19,950
detection aspect the architecture and

00:12:18,300 --> 00:12:22,410
how the technologies actually fit

00:12:19,950 --> 00:12:24,900
together to make it work

00:12:22,410 --> 00:12:29,070
is this our machine this is the audio

00:12:24,900 --> 00:12:31,110
kellyo teleo count machine from dr zeus

00:12:29,070 --> 00:12:33,720
it's actually a stream processing

00:12:31,110 --> 00:12:35,370
machine for counting sleepers and to

00:12:33,720 --> 00:12:37,530
some extent we have advanced a bit from

00:12:35,370 --> 00:12:39,330
this 1960s technology but some of the

00:12:37,530 --> 00:12:44,490
some of the pieces are actually present

00:12:39,330 --> 00:12:47,010
in that machine an algorithm from the

00:12:44,490 --> 00:12:49,470
same vintage is actually what we chose

00:12:47,010 --> 00:12:52,020
eventually it's called a cumulative sum

00:12:49,470 --> 00:12:55,050
control chart it's pretty old as still

00:12:52,020 --> 00:12:56,760
in use in some situations and it's used

00:12:55,050 --> 00:12:59,550
for a statistical analysis of historical

00:12:56,760 --> 00:13:01,500
data so it relies upon the fact that as

00:12:59,550 --> 00:13:04,740
each new event comes in you can compare

00:13:01,500 --> 00:13:08,250
it based on the key of that event with

00:13:04,740 --> 00:13:10,200
the previous events for the same key

00:13:08,250 --> 00:13:12,930
back to almost an arbitrary amount of

00:13:10,200 --> 00:13:14,250
time in the past so it has a number of

00:13:12,930 --> 00:13:17,070
properties which are quite nice for us

00:13:14,250 --> 00:13:20,670
you know it doesn't assume equal time

00:13:17,070 --> 00:13:22,520
periods the events can be quite widely

00:13:20,670 --> 00:13:24,440
spaced across

00:13:22,520 --> 00:13:28,130
it doesn't require a particular number

00:13:24,440 --> 00:13:29,720
of events either that the downside is

00:13:28,130 --> 00:13:32,570
that as new events come in there can be

00:13:29,720 --> 00:13:34,190
a bit of a lag between when the anomaly

00:13:32,570 --> 00:13:36,650
actually occurred and when it's detected

00:13:34,190 --> 00:13:38,150
but that's okay so we picked this mainly

00:13:36,650 --> 00:13:39,590
because it's not computationally

00:13:38,150 --> 00:13:42,800
expensive there's no machine learning

00:13:39,590 --> 00:13:46,430
required as you don't need its untrained

00:13:42,800 --> 00:13:47,750
its unsupervised there were other

00:13:46,430 --> 00:13:48,950
alternatives you looked at but this

00:13:47,750 --> 00:13:52,280
seemed to fit the bill for our

00:13:48,950 --> 00:13:55,220
particular problem so in terms of the

00:13:52,280 --> 00:13:59,270
logical steps involved the events are

00:13:55,220 --> 00:14:03,260
coming in as a stream these are assumed

00:13:59,270 --> 00:14:05,300
to be coming from some real source in

00:14:03,260 --> 00:14:07,190
the financial sector it might be coming

00:14:05,300 --> 00:14:10,460
from the banking systems or from the

00:14:07,190 --> 00:14:12,320
ATMs or from people's mobile phone apps

00:14:10,460 --> 00:14:14,650
that are doing the transactions and

00:14:12,320 --> 00:14:17,420
things in our case it was simulated data

00:14:14,650 --> 00:14:19,160
some once the events come in the

00:14:17,420 --> 00:14:20,810
application picks the next event

00:14:19,160 --> 00:14:22,880
available from the stream writes the

00:14:20,810 --> 00:14:25,160
event to the database so that it can

00:14:22,880 --> 00:14:28,340
remember it for the future and then

00:14:25,160 --> 00:14:30,590
queries the historic from the historic

00:14:28,340 --> 00:14:32,540
data from the database for the key of

00:14:30,590 --> 00:14:34,970
that event if there are sufficient

00:14:32,540 --> 00:14:37,310
observations it then runs the anomaly

00:14:34,970 --> 00:14:38,810
detector algorithm and makes a decision

00:14:37,310 --> 00:14:41,090
about whether there's a potential

00:14:38,810 --> 00:14:42,650
anomaly detected and in a real system

00:14:41,090 --> 00:14:44,750
some sort of action would be taken at

00:14:42,650 --> 00:14:46,550
that point you might prevent someone's

00:14:44,750 --> 00:14:47,630
credit card from being used which is

00:14:46,550 --> 00:14:50,870
what happened to me the day before I

00:14:47,630 --> 00:14:54,170
flew here unfortunately so yeah there

00:14:50,870 --> 00:14:56,000
can be some real-time action or perhaps

00:14:54,170 --> 00:14:59,810
it might just get queued for people to

00:14:56,000 --> 00:15:01,010
investigate subsequently now here's the

00:14:59,810 --> 00:15:03,530
design of the the whole pipeline

00:15:01,010 --> 00:15:07,160
including the technologies were using

00:15:03,530 --> 00:15:09,530
this shows the interaction between the

00:15:07,160 --> 00:15:12,530
transaction generator on the Left the

00:15:09,530 --> 00:15:15,530
events going into the calf cluster the

00:15:12,530 --> 00:15:18,560
the anomaly detection pipeline and then

00:15:15,530 --> 00:15:20,240
Cassandra on the right there's a few

00:15:18,560 --> 00:15:23,300
details here which turned out to be

00:15:20,240 --> 00:15:25,370
quite critical we used two thread pools

00:15:23,300 --> 00:15:27,080
for the anomaly detection pipeline one

00:15:25,370 --> 00:15:29,420
thread pool is purely for the caf-co

00:15:27,080 --> 00:15:31,160
consumer and then the events get handed

00:15:29,420 --> 00:15:33,200
off to another thread pool which hands

00:15:31,160 --> 00:15:34,700
handles the rights to the Cassandra

00:15:33,200 --> 00:15:34,980
cluster running the algorithm and then

00:15:34,700 --> 00:15:37,500
do

00:15:34,980 --> 00:15:39,480
something with the results so it was

00:15:37,500 --> 00:15:42,060
quite important to decouple the consumer

00:15:39,480 --> 00:15:47,250
from the the processing part of the

00:15:42,060 --> 00:15:51,449
pipeline in terms of the cloud

00:15:47,250 --> 00:15:55,350
deployment context the Kefka and

00:15:51,449 --> 00:15:57,560
Cassandra clusters hosted on AWS running

00:15:55,350 --> 00:16:00,329
in our managed service infrastructure

00:15:57,560 --> 00:16:02,070
the application the event generator and

00:16:00,329 --> 00:16:06,899
the detection pipeline are also running

00:16:02,070 --> 00:16:11,970
in AWS but running in kubernetes X

00:16:06,899 --> 00:16:14,070
service provided by AWS I don't know how

00:16:11,970 --> 00:16:16,500
much people are familiar with Cassandra

00:16:14,070 --> 00:16:18,720
and Kafka so bear with me if you're an

00:16:16,500 --> 00:16:20,519
expert but I will just briefly cover

00:16:18,720 --> 00:16:22,260
some of the important points

00:16:20,519 --> 00:16:25,199
it's a Cassandra open source it's an

00:16:22,260 --> 00:16:27,329
Apache project it's a no SQL database

00:16:25,199 --> 00:16:30,180
it's a masterless ringing architecture

00:16:27,329 --> 00:16:33,029
and uses petition data for scalability

00:16:30,180 --> 00:16:35,519
and high availability it's very fast for

00:16:33,029 --> 00:16:37,290
writes not so fast for reads but fast

00:16:35,519 --> 00:16:40,050
enough and it's got a reasonably

00:16:37,290 --> 00:16:44,639
powerful and the query language with

00:16:40,050 --> 00:16:46,889
indexes we offer this as a managed

00:16:44,639 --> 00:16:48,959
service as some of the the advantages

00:16:46,889 --> 00:16:52,680
there if you're interested in in our

00:16:48,959 --> 00:16:54,360
service reps on all this mentioned we

00:16:52,680 --> 00:16:55,470
actually do a certification program for

00:16:54,360 --> 00:16:58,440
Apache Cassandra

00:16:55,470 --> 00:17:01,470
so we produce a report which is quite

00:16:58,440 --> 00:17:03,660
rigorous and detailed and based on our

00:17:01,470 --> 00:17:05,750
actual testing Prabhat and benchmarking

00:17:03,660 --> 00:17:08,459
which is some of our customers find

00:17:05,750 --> 00:17:10,860
quite useful and give us them a lot a

00:17:08,459 --> 00:17:13,799
lot more confidence in using it in

00:17:10,860 --> 00:17:15,480
mission critical application areas Kafka

00:17:13,799 --> 00:17:18,089
is a distributed streams processing

00:17:15,480 --> 00:17:19,620
system it enables distributed producers

00:17:18,089 --> 00:17:24,120
to send messages to distribution

00:17:19,620 --> 00:17:26,549
consumers via a cafe or cluster again

00:17:24,120 --> 00:17:29,070
and some of the benefits that it's fast

00:17:26,549 --> 00:17:31,200
high throughput low latency its scalable

00:17:29,070 --> 00:17:34,049
and the same way that Cassandra is you

00:17:31,200 --> 00:17:36,120
just add nodes to get more throughput

00:17:34,049 --> 00:17:37,350
it's reliable that's distributed and

00:17:36,120 --> 00:17:39,900
fault tolerant in the same way that

00:17:37,350 --> 00:17:42,690
Cassandra is you can make sure you've

00:17:39,900 --> 00:17:45,320
got zero data loss again it's open

00:17:42,690 --> 00:17:47,250
source it's an Apache project

00:17:45,320 --> 00:17:47,700
interestingly enough that was actually

00:17:47,250 --> 00:17:48,300
designed

00:17:47,700 --> 00:17:50,250
Barling

00:17:48,300 --> 00:17:51,750
didn't solve their application

00:17:50,250 --> 00:17:53,220
integration architecture problem they

00:17:51,750 --> 00:17:56,010
had a set of many many too many

00:17:53,220 --> 00:17:57,570
spaghetti type integration architecture

00:17:56,010 --> 00:17:59,970
which is obviously unsustainable and

00:17:57,570 --> 00:18:01,890
they wanted to be able to essentially to

00:17:59,970 --> 00:18:04,260
move to a hub-and-spoke architecture

00:18:01,890 --> 00:18:07,530
model so the solution that it was

00:18:04,260 --> 00:18:09,510
delightful really was to get data from

00:18:07,530 --> 00:18:11,430
heterogeneous data sources and make them

00:18:09,510 --> 00:18:12,900
available to heterogeneous data sinks

00:18:11,430 --> 00:18:15,390
it's actually really good for that so

00:18:12,900 --> 00:18:17,370
the use case as well and again it's

00:18:15,390 --> 00:18:21,330
available as a managed service but

00:18:17,370 --> 00:18:24,090
instead cluster so looking at a bit more

00:18:21,330 --> 00:18:25,920
detail the application automation was

00:18:24,090 --> 00:18:28,380
done with kubernetes so this was a new

00:18:25,920 --> 00:18:32,810
technology to me I was obviously one

00:18:28,380 --> 00:18:34,890
that that's been increasing in use and

00:18:32,810 --> 00:18:36,330
awareness in the last few years that

00:18:34,890 --> 00:18:41,550
seemed like a particularly good choice

00:18:36,330 --> 00:18:42,960
to try out or used iws x to do the to

00:18:41,550 --> 00:18:46,920
provide the masternodes

00:18:42,960 --> 00:18:48,900
for it and we hosted our load generator

00:18:46,920 --> 00:18:55,380
and the anomaly detection pipeline on

00:18:48,900 --> 00:18:57,060
the work nodes I understand so yeah this

00:18:55,380 --> 00:18:59,070
this sort of explains some of the story

00:18:57,060 --> 00:19:00,900
on how the application communicates for

00:18:59,070 --> 00:19:04,080
there are clusters that we we manage as

00:19:00,900 --> 00:19:06,030
well so we had to use VPC peering to get

00:19:04,080 --> 00:19:08,060
things to work properly and also our

00:19:06,030 --> 00:19:11,850
provisioning API that we have opened

00:19:08,060 --> 00:19:13,560
which enabled people to actually spin up

00:19:11,850 --> 00:19:15,600
clusters do things with them and killed

00:19:13,560 --> 00:19:17,760
morph again programmatically as well

00:19:15,600 --> 00:19:19,380
which is quite useful in DevOps type

00:19:17,760 --> 00:19:20,640
situations which is more or less the

00:19:19,380 --> 00:19:23,700
sort of environment that I was working

00:19:20,640 --> 00:19:25,380
in because I was this wasn't a

00:19:23,700 --> 00:19:26,970
production environment it was purely a

00:19:25,380 --> 00:19:30,630
development test environment so that was

00:19:26,970 --> 00:19:33,660
really really handy again I'm not quite

00:19:30,630 --> 00:19:35,790
sure what level of knowledge people have

00:19:33,660 --> 00:19:37,470
about kubernetes this at a very high

00:19:35,790 --> 00:19:38,790
level it's an automation system for the

00:19:37,470 --> 00:19:41,340
management scaling and deployment of

00:19:38,790 --> 00:19:43,200
containerized applications there's a lot

00:19:41,340 --> 00:19:43,560
of buzzwords and their ads but some ass

00:19:43,200 --> 00:19:47,280
to work

00:19:43,560 --> 00:19:48,570
architecture the pods are the unit of

00:19:47,280 --> 00:19:50,370
concurrency that's probably the most

00:19:48,570 --> 00:19:51,960
important thing in terms of scaling the

00:19:50,370 --> 00:19:55,440
system shimming you've got sufficient

00:19:51,960 --> 00:19:57,270
worker nodes and you can spin up more

00:19:55,440 --> 00:19:59,210
pods and scale the application up and

00:19:57,270 --> 00:20:03,510
down

00:19:59,210 --> 00:20:05,280
again it's open source it is cloud

00:20:03,510 --> 00:20:07,050
provider and programming language

00:20:05,280 --> 00:20:09,090
agnostic which is perhaps the main

00:20:07,050 --> 00:20:11,150
differentiator compared to some previous

00:20:09,090 --> 00:20:13,740
attempts at doing things in the space

00:20:11,150 --> 00:20:17,880
you can develop and test code locally

00:20:13,740 --> 00:20:20,190
and deploy at scale into a public cloud

00:20:17,880 --> 00:20:22,080
provider it helps with resource

00:20:20,190 --> 00:20:23,610
management you can deploy applications

00:20:22,080 --> 00:20:25,590
to Kuban at ease and it manages the

00:20:23,610 --> 00:20:27,540
scaling up and down I am keeping the

00:20:25,590 --> 00:20:29,460
application alive so it's not dust about

00:20:27,540 --> 00:20:32,160
scalability it's about availability as

00:20:29,460 --> 00:20:34,200
well interestingly there's more powerful

00:20:32,160 --> 00:20:36,660
frameworks coming but a bit built on

00:20:34,200 --> 00:20:38,400
kubernetes api x' and some of them look

00:20:36,660 --> 00:20:40,110
really really useful and quite cool

00:20:38,400 --> 00:20:45,750
particularly things for the service mesh

00:20:40,110 --> 00:20:47,310
type environments and things so I

00:20:45,750 --> 00:20:48,930
mentioned before observability so we

00:20:47,310 --> 00:20:50,610
decided to use Prometheus for monitoring

00:20:48,930 --> 00:20:52,200
I've actually got a whole talk so I gave

00:20:50,610 --> 00:20:54,060
it a pet chicken in Las Vegas on the

00:20:52,200 --> 00:20:55,950
monitoring aspect which i think is

00:20:54,060 --> 00:20:58,620
available certainly on our website if

00:20:55,950 --> 00:21:02,250
not somewhere else so it goes into a lot

00:20:58,620 --> 00:21:03,960
more detail but the trick turned out to

00:21:02,250 --> 00:21:07,770
be to use the kubernetes prometheus

00:21:03,960 --> 00:21:11,370
operator running in the kubernetes

00:21:07,770 --> 00:21:13,560
cluster itself that made it a lot easier

00:21:11,370 --> 00:21:15,450
to get the monitoring to work we use

00:21:13,560 --> 00:21:20,340
Griffon autographing the built-in

00:21:15,450 --> 00:21:21,900
Prometheus GUI is next to useless and

00:21:20,340 --> 00:21:24,480
the way we used it was essentially to

00:21:21,900 --> 00:21:26,460
debug tune and observe the sort of the

00:21:24,480 --> 00:21:28,140
top-level business metric that we

00:21:26,460 --> 00:21:30,750
actually wanted to report on for the

00:21:28,140 --> 00:21:33,390
benchmark result so that was thus the

00:21:30,750 --> 00:21:34,920
total throughput of the system and also

00:21:33,390 --> 00:21:38,280
to ensure that the response time was

00:21:34,920 --> 00:21:39,960
sub-second for that reported result we'd

00:21:38,280 --> 00:21:42,930
span up eventually a hundred application

00:21:39,960 --> 00:21:44,370
pods so this this these two graphs

00:21:42,930 --> 00:21:46,740
actually show if you look carefully

00:21:44,370 --> 00:21:47,940
there's a whole sort of layer of um

00:21:46,740 --> 00:21:50,760
graphs there and each of those is

00:21:47,940 --> 00:21:52,830
reporting the metrics for one pod and

00:21:50,760 --> 00:21:54,570
this is the stacked result to the

00:21:52,830 --> 00:21:55,620
aggregate result for all those pods and

00:21:54,570 --> 00:21:57,990
it works really well at scales

00:21:55,620 --> 00:21:59,820
dynamically you can scale the

00:21:57,990 --> 00:22:02,880
application up and down and it it copes

00:21:59,820 --> 00:22:04,710
with keeping track of which things are

00:22:02,880 --> 00:22:08,520
being monitored and how many of them

00:22:04,710 --> 00:22:10,500
there are again I don't know how much

00:22:08,520 --> 00:22:14,490
people know about to me theose

00:22:10,500 --> 00:22:17,789
I'm not an expert exactly I've used at

00:22:14,490 --> 00:22:19,350
once now and it works pretty well here's

00:22:17,789 --> 00:22:22,169
the high level architecture it's for

00:22:19,350 --> 00:22:24,990
monitoring applications and servers sort

00:22:22,169 --> 00:22:26,789
of dual purpose which is quite handy you

00:22:24,990 --> 00:22:29,549
have to have instrumentation that's the

00:22:26,789 --> 00:22:32,220
down side without the instrumentation

00:22:29,549 --> 00:22:34,230
it'll allow you to monitor servers okay

00:22:32,220 --> 00:22:35,039
but in order to monitor applications you

00:22:34,230 --> 00:22:37,710
actually do have to have some

00:22:35,039 --> 00:22:39,570
instrumentation in your code and it's

00:22:37,710 --> 00:22:41,639
quite easy to make mistakes as I

00:22:39,570 --> 00:22:44,309
discovered it's pool based which scales

00:22:41,639 --> 00:22:48,120
really well so the Prometheus server

00:22:44,309 --> 00:22:50,100
basically you have to tell it where the

00:22:48,120 --> 00:22:52,799
the end points are to get the metrics

00:22:50,100 --> 00:22:55,110
from the Prometheus operator does that

00:22:52,799 --> 00:22:56,399
automatically for you and so you don't

00:22:55,110 --> 00:22:58,500
have to do it manually and you just

00:22:56,399 --> 00:23:00,960
can't for large systems anyway so it has

00:22:58,500 --> 00:23:02,700
to be done automatically so there are

00:23:00,960 --> 00:23:06,690
all the different components involved in

00:23:02,700 --> 00:23:08,100
kubernetes so the operator I mentioned

00:23:06,690 --> 00:23:09,950
this before so in a production

00:23:08,100 --> 00:23:12,840
environment you really have to use this

00:23:09,950 --> 00:23:15,419
so it manages the application complexity

00:23:12,840 --> 00:23:18,470
and most importantly the dynamics so you

00:23:15,419 --> 00:23:20,279
set it up there is some configuration

00:23:18,470 --> 00:23:23,720
initially but once you've done that

00:23:20,279 --> 00:23:26,159
it'll it'll work well the second part of

00:23:23,720 --> 00:23:27,330
observable observability which perhaps a

00:23:26,159 --> 00:23:30,990
lot of people aren't familiar with is

00:23:27,330 --> 00:23:33,539
the tracing aspect so this enables you

00:23:30,990 --> 00:23:35,700
to get sort of an end-to-end viewpoint

00:23:33,539 --> 00:23:37,259
of the application either for every

00:23:35,700 --> 00:23:39,659
single transaction going through the

00:23:37,259 --> 00:23:41,940
system or at some aggregate level that

00:23:39,659 --> 00:23:43,740
sort of topology view level now there

00:23:41,940 --> 00:23:44,970
are some APM products commercial ones

00:23:43,740 --> 00:23:48,330
around which have done this for quite a

00:23:44,970 --> 00:23:50,490
few years and quite well but this is

00:23:48,330 --> 00:23:54,389
open source and it worked and that was

00:23:50,490 --> 00:23:56,460
quite quite pleased with the results the

00:23:54,389 --> 00:23:59,159
the picture at the top there shows an

00:23:56,460 --> 00:24:00,389
example of a single trace of a single

00:23:59,159 --> 00:24:03,240
transaction going through the system

00:24:00,389 --> 00:24:04,980
what I find more more useful and

00:24:03,240 --> 00:24:08,100
interesting in some ways is the the

00:24:04,980 --> 00:24:10,470
aggregate topology view so it's

00:24:08,100 --> 00:24:12,389
analyzing the data in real time and then

00:24:10,470 --> 00:24:13,860
you can basically say well show me what

00:24:12,389 --> 00:24:15,330
the topology of the system at this

00:24:13,860 --> 00:24:17,929
particular point in time looks like and

00:24:15,330 --> 00:24:19,980
that's what the lower picture there does

00:24:17,929 --> 00:24:21,570
even though we had a pretty simple

00:24:19,980 --> 00:24:23,480
topology for this pipeline it really is

00:24:21,570 --> 00:24:25,190
just a pipeline with

00:24:23,480 --> 00:24:26,330
a couple of different things going on in

00:24:25,190 --> 00:24:28,340
terms of the interaction with the

00:24:26,330 --> 00:24:30,710
Cassandra database at the bottom and

00:24:28,340 --> 00:24:32,330
running the anonyme detector it was

00:24:30,710 --> 00:24:35,480
actually quite valuable for debugging

00:24:32,330 --> 00:24:38,630
and I mentioned that with Prometheus you

00:24:35,480 --> 00:24:40,850
have to instrument your code in one

00:24:38,630 --> 00:24:42,710
particular run of my system I was not

00:24:40,850 --> 00:24:45,260
getting the the the results that I was

00:24:42,710 --> 00:24:46,520
expecting and I actually wondered

00:24:45,260 --> 00:24:48,020
whether I'd made a mistake in the

00:24:46,520 --> 00:24:50,750
instrumentation and I was able to use

00:24:48,020 --> 00:24:52,760
the the the this view of the system

00:24:50,750 --> 00:24:55,130
provided by Yaeger to actually double

00:24:52,760 --> 00:24:57,680
check that the system that all the calls

00:24:55,130 --> 00:25:00,530
actually were being made correctly and

00:24:57,680 --> 00:25:03,620
so I was able to debug the system quite

00:25:00,530 --> 00:25:04,910
quickly at that point again if you're

00:25:03,620 --> 00:25:07,180
not familiar with open tracing it's a

00:25:04,910 --> 00:25:09,410
standard API for distributed tracing

00:25:07,180 --> 00:25:12,500
it's a specification not an

00:25:09,410 --> 00:25:14,870
implementation so in order to work you

00:25:12,500 --> 00:25:17,600
need a the application instrumentation

00:25:14,870 --> 00:25:19,430
and be an actual tracer and there's a

00:25:17,600 --> 00:25:21,020
whole bunch of them some are open source

00:25:19,430 --> 00:25:23,720
summer proprietary so you can just take

00:25:21,020 --> 00:25:28,390
a pic I picked Yaeger I also tried

00:25:23,720 --> 00:25:28,390
Zipkin that worked pretty well as well

00:25:28,510 --> 00:25:35,150
it's got quite quite an interesting

00:25:30,950 --> 00:25:38,630
internal architecture Yaeger it's

00:25:35,150 --> 00:25:40,310
scalable it's got sort of it almost sort

00:25:38,630 --> 00:25:43,640
of a feedback loop mechanism as well

00:25:40,310 --> 00:25:46,520
which enables it to tune itself and so

00:25:43,640 --> 00:25:48,800
it can select the right amount of data

00:25:46,520 --> 00:25:52,490
coming out of the the monitored

00:25:48,800 --> 00:25:55,990
infrastructure so yes it uses Cassandra

00:25:52,490 --> 00:25:55,990
and spark in the production environment

00:25:56,680 --> 00:26:01,460
so that was a pretty simple example I

00:25:59,210 --> 00:26:03,590
actually decided well it looks like

00:26:01,460 --> 00:26:06,080
quite a powerful technology what can we

00:26:03,590 --> 00:26:08,330
do with it monitor more complex

00:26:06,080 --> 00:26:10,130
Kefka replication for example an

00:26:08,330 --> 00:26:12,470
enterprise service bus when you we might

00:26:10,130 --> 00:26:15,770
have potentially had many hundreds of

00:26:12,470 --> 00:26:17,480
topics and events being consumed and

00:26:15,770 --> 00:26:20,600
process and then push back into topics

00:26:17,480 --> 00:26:21,830
with with quite complicated networks so

00:26:20,600 --> 00:26:23,930
it actually worked pretty well for a

00:26:21,830 --> 00:26:25,970
simulated example so I'll just emphasize

00:26:23,930 --> 00:26:28,370
this isn't a real application it was a

00:26:25,970 --> 00:26:32,020
simulated one but I've been tracing

00:26:28,370 --> 00:26:38,740
first Yaeger did actually produce

00:26:32,020 --> 00:26:41,410
a good insight into what was going on so

00:26:38,740 --> 00:26:44,680
the final part of the talk is how well

00:26:41,410 --> 00:26:47,950
did it work in terms of the scalability

00:26:44,680 --> 00:26:50,740
of the system so I started out with

00:26:47,950 --> 00:26:52,060
three default Cassandra nodes that's the

00:26:50,740 --> 00:26:53,710
minimum number that we allow people to

00:26:52,060 --> 00:26:56,560
run in order to make sure you've got the

00:26:53,710 --> 00:26:58,570
replication factor of three so I was

00:26:56,560 --> 00:27:04,510
interested in the question of how easy

00:26:58,570 --> 00:27:07,180
and how far can I scale the system so

00:27:04,510 --> 00:27:09,820
there was a bit of work to do even for

00:27:07,180 --> 00:27:11,890
the three node Cassandra cluster system

00:27:09,820 --> 00:27:13,600
to actually optimize it which is what I

00:27:11,890 --> 00:27:16,450
did initially so I spent some time

00:27:13,600 --> 00:27:18,250
optimizing that system in order to get

00:27:16,450 --> 00:27:20,650
the best sort of throughput and the

00:27:18,250 --> 00:27:23,590
least response time that I could so this

00:27:20,650 --> 00:27:25,540
is one of the tricks that I used it was

00:27:23,590 --> 00:27:27,400
it's quite a high read read to write

00:27:25,540 --> 00:27:28,930
ratio this particular application for

00:27:27,400 --> 00:27:31,270
every event you're writing into the

00:27:28,930 --> 00:27:34,330
system you have to read 50 or more

00:27:31,270 --> 00:27:37,300
events back again to check if there's an

00:27:34,330 --> 00:27:39,220
anomaly so it turns out you can tune

00:27:37,300 --> 00:27:41,650
Cassandra to cope with that and that's

00:27:39,220 --> 00:27:43,830
to do with the compression chunk size as

00:27:41,650 --> 00:27:47,410
the magic number that I used

00:27:43,830 --> 00:27:49,420
interestingly enough this car was the

00:27:47,410 --> 00:27:53,050
first vehicle to reach 100 kilometres an

00:27:49,420 --> 00:27:54,310
hour in 1899 and it was electric isn't

00:27:53,050 --> 00:27:55,480
that cool I thought that was cool so

00:27:54,310 --> 00:27:58,390
it's a bit like Viper three node a

00:27:55,480 --> 00:28:01,240
cassandra cluster it's sort of it's cool

00:27:58,390 --> 00:28:05,800
but it's not the best in the world just

00:28:01,240 --> 00:28:08,920
yet in terms of scaling what can we

00:28:05,800 --> 00:28:10,360
tweak well the thing which determines

00:28:08,920 --> 00:28:12,880
the load on the system is the load

00:28:10,360 --> 00:28:14,950
generator in red there so there's a knob

00:28:12,880 --> 00:28:18,700
essentially to change the the rate of

00:28:14,950 --> 00:28:20,680
events coming into the system the second

00:28:18,700 --> 00:28:23,560
thing that I could modify was the

00:28:20,680 --> 00:28:24,760
cluster sizes and the worker pods and

00:28:23,560 --> 00:28:27,340
the number of worker pods in the

00:28:24,760 --> 00:28:29,440
application which are the orange knobs

00:28:27,340 --> 00:28:32,170
and then the final thing which is a bit

00:28:29,440 --> 00:28:34,450
sort of lower level is the thread pools

00:28:32,170 --> 00:28:36,630
the partitions and the connections in

00:28:34,450 --> 00:28:41,590
yellow and they turned out to be all

00:28:36,630 --> 00:28:44,020
more important than I thought so this

00:28:41,590 --> 00:28:46,140
graph shows my my first attempts at

00:28:44,020 --> 00:28:48,330
scaling the system out

00:28:46,140 --> 00:28:49,980
so one thing about kubernetes is it's

00:28:48,330 --> 00:28:51,420
really easy to scale the application you

00:28:49,980 --> 00:28:53,310
just increase the number of pods

00:28:51,420 --> 00:28:57,540
assuming you've got sufficient working

00:28:53,310 --> 00:28:59,100
loads in your your kubernetes cluster in

00:28:57,540 --> 00:29:02,280
some ways it's too easy because you can

00:28:59,100 --> 00:29:03,810
just spin up more pods without actually

00:29:02,280 --> 00:29:06,440
thinking about whether the system as a

00:29:03,810 --> 00:29:08,790
whole is going to work correctly or not

00:29:06,440 --> 00:29:10,410
so this was my first attempt after

00:29:08,790 --> 00:29:12,270
tuning the three node Cassandra

00:29:10,410 --> 00:29:14,610
clustered the best that I could

00:29:12,270 --> 00:29:17,130
I then to scale that out to 24 Cassandra

00:29:14,610 --> 00:29:18,480
nodes and that's the the blue line there

00:29:17,130 --> 00:29:20,580
as you can see that was a slightly

00:29:18,480 --> 00:29:22,170
disappointing result I wasn't getting

00:29:20,580 --> 00:29:25,650
the sort of the scalability increase

00:29:22,170 --> 00:29:28,080
that I was expecting from that what had

00:29:25,650 --> 00:29:30,120
gone wrong well basically I was ignoring

00:29:28,080 --> 00:29:32,820
the the settings for the thread pools

00:29:30,120 --> 00:29:34,080
and the relationship between those the

00:29:32,820 --> 00:29:37,710
number of pods in the number of

00:29:34,080 --> 00:29:40,680
Cassandra connections so essentially my

00:29:37,710 --> 00:29:44,400
tuning methodology changed I've tuned

00:29:40,680 --> 00:29:46,020
each of those things for each class for

00:29:44,400 --> 00:29:49,680
each size

00:29:46,020 --> 00:29:51,330
Cassandra cluster each time and that

00:29:49,680 --> 00:29:55,350
produced a far better result which is

00:29:51,330 --> 00:29:56,940
the orange line that's there so that was

00:29:55,350 --> 00:29:59,880
on the Cassandra side the Kefka side

00:29:56,940 --> 00:30:03,090
turned out to be important as well this

00:29:59,880 --> 00:30:05,190
is perhaps a little known aspect of

00:30:03,090 --> 00:30:07,830
Kefka which I discovered the hard way

00:30:05,190 --> 00:30:10,800
unfortunately this is a graph showing

00:30:07,830 --> 00:30:13,170
the number of partitions in a Kefka

00:30:10,800 --> 00:30:15,300
topic on the x-axis versus the

00:30:13,170 --> 00:30:16,680
throughput so as you can see as you

00:30:15,300 --> 00:30:18,450
increase the number of partitions the

00:30:16,680 --> 00:30:20,490
triple it drops significantly which is

00:30:18,450 --> 00:30:23,840
not a desirable feature of a scalable

00:30:20,490 --> 00:30:28,340
system so this this is a problem because

00:30:23,840 --> 00:30:31,350
in my application number of pods

00:30:28,340 --> 00:30:33,750
increase the number of kefka consumer

00:30:31,350 --> 00:30:35,010
threads which results in more Kafka team

00:30:33,750 --> 00:30:36,870
streamers and and all if they have more

00:30:35,010 --> 00:30:39,810
cavity consumers in Kafka you need more

00:30:36,870 --> 00:30:41,340
kappa partitions but unfortunately it

00:30:39,810 --> 00:30:43,890
lowers and throughput which is counter

00:30:41,340 --> 00:30:45,510
intuitive at that point now what's the

00:30:43,890 --> 00:30:47,550
solution least I hope I've got it here

00:30:45,510 --> 00:30:49,020
yep so the obvious solution and the only

00:30:47,550 --> 00:30:50,850
one that I actually tried was to make

00:30:49,020 --> 00:30:53,940
your caf-co cluster bigger which is sort

00:30:50,850 --> 00:30:56,700
of like it's a it's that the low-tech

00:30:53,940 --> 00:30:59,630
solution for this problem but inevitably

00:30:56,700 --> 00:31:03,299
it worked so

00:30:59,630 --> 00:31:05,580
essentially I was able to size my kafka

00:31:03,299 --> 00:31:08,100
cluster to achieve the sort of target

00:31:05,580 --> 00:31:10,350
throughput that I had for the number of

00:31:08,100 --> 00:31:14,010
repetitions that I needed which in my

00:31:10,350 --> 00:31:15,690
case was about 200 so I was able to get

00:31:14,010 --> 00:31:18,510
the throughput of about 2.3 million

00:31:15,690 --> 00:31:20,610
rights a second just by making lean the

00:31:18,510 --> 00:31:22,230
Khafre clusters bigger there are some

00:31:20,610 --> 00:31:24,270
other tricks that I've subsequently

00:31:22,230 --> 00:31:26,460
heard about which I again I haven't

00:31:24,270 --> 00:31:29,429
tried but it turns out that in kapha the

00:31:26,460 --> 00:31:31,260
the number of replicas fetches by

00:31:29,429 --> 00:31:32,700
default is 1 which seems like a

00:31:31,260 --> 00:31:34,980
particularly silly number because at

00:31:32,700 --> 00:31:36,330
that point you've got a bottleneck in

00:31:34,980 --> 00:31:38,700
the system and as the number of

00:31:36,330 --> 00:31:41,549
petitions goes up yep sure enough this

00:31:38,700 --> 00:31:46,049
report goes down so I have read some

00:31:41,549 --> 00:31:49,080
studies recently that suggest they're

00:31:46,049 --> 00:31:50,820
increasing that number at us to another

00:31:49,080 --> 00:31:51,929
number which is not too high I actually

00:31:50,820 --> 00:31:58,140
can increase the throughput

00:31:51,929 --> 00:31:59,880
significantly this slide explains what

00:31:58,140 --> 00:32:01,640
the file system sort of looked like for

00:31:59,880 --> 00:32:04,980
those interested in this level of detail

00:32:01,640 --> 00:32:06,600
most people aren't but essentially there

00:32:04,980 --> 00:32:08,940
are three clusters running there's the

00:32:06,600 --> 00:32:13,380
caf-co cluster we ended up with nine

00:32:08,940 --> 00:32:17,190
nodes and that the Cassandra cluster 48

00:32:13,380 --> 00:32:20,730
nodes and the kubernetes cluster we had

00:32:17,190 --> 00:32:23,010
in we ended up with two virtual machines

00:32:20,730 --> 00:32:29,100
with 72 cores in total for that part of

00:32:23,010 --> 00:32:30,720
the system so ok as we scaled out this

00:32:29,100 --> 00:32:33,480
is currently actually is the fastest car

00:32:30,720 --> 00:32:35,340
in the world does 0 to 100 in two

00:32:33,480 --> 00:32:37,950
seconds and it's also electric which i

00:32:35,340 --> 00:32:40,289
think is cool so let's see how the

00:32:37,950 --> 00:32:43,320
scaling worked so we ended up scaling

00:32:40,289 --> 00:32:46,500
from 3 to 48 no it's not not all at once

00:32:43,320 --> 00:32:49,049
we did increase incrementally and

00:32:46,500 --> 00:32:51,419
retuned as we went and so this got us

00:32:49,049 --> 00:32:52,890
the magic number of 19 billion checks a

00:32:51,419 --> 00:32:55,080
day again

00:32:52,890 --> 00:32:56,940
there's no upper limit we could have

00:32:55,080 --> 00:33:00,480
increased the system size significantly

00:32:56,940 --> 00:33:06,960
if we'd wanted to in terms of the

00:33:00,480 --> 00:33:10,890
resource usage yep so this is basically

00:33:06,960 --> 00:33:12,380
just showing visually that for the

00:33:10,890 --> 00:33:14,960
bigger system we use 500

00:33:12,380 --> 00:33:16,610
74 cause Cassandra was by far the

00:33:14,960 --> 00:33:21,950
biggest resource hog followed by

00:33:16,610 --> 00:33:26,000
kubernetes and Kafka yep 67 percent of

00:33:21,950 --> 00:33:29,420
the cause for Cassandra and the rest for

00:33:26,000 --> 00:33:32,630
the other two parts of the system yep

00:33:29,420 --> 00:33:34,880
similar graph showing that so the other

00:33:32,630 --> 00:33:38,270
aspect of this is the affordability at

00:33:34,880 --> 00:33:40,760
scale because we could and incrementally

00:33:38,270 --> 00:33:43,670
change the size of the system and tune

00:33:40,760 --> 00:33:45,470
it for each each size we were able to

00:33:43,670 --> 00:33:48,380
achieve what we were hoping to in terms

00:33:45,470 --> 00:33:50,960
of the the cost-benefit aspect so the

00:33:48,380 --> 00:33:54,320
biggest system was any costing $1,000 a

00:33:50,960 --> 00:33:57,080
day that was just in terms of the AWS

00:33:54,320 --> 00:33:58,610
instance costs themselves not including

00:33:57,080 --> 00:34:02,390
the insta cluster management cost which

00:33:58,610 --> 00:34:03,950
we we add to that yeah and on the other

00:34:02,390 --> 00:34:06,140
hand the smaller system of three nodes

00:34:03,950 --> 00:34:07,670
was any hundred dollars a day so the

00:34:06,140 --> 00:34:11,450
system can be sized depending on the

00:34:07,670 --> 00:34:13,220
business demand I mentioned that one of

00:34:11,450 --> 00:34:14,810
the use cases was using Kepler as a

00:34:13,220 --> 00:34:16,850
buffer all it worked really well for

00:34:14,810 --> 00:34:19,520
that so it certainly acted as a as a

00:34:16,850 --> 00:34:21,679
very effective buffer we were able to

00:34:19,520 --> 00:34:23,629
increase the events coming in up to 2.3

00:34:21,679 --> 00:34:27,010
million events today the rest of the

00:34:23,629 --> 00:34:29,540
pipeline and thus kept on processing at

00:34:27,010 --> 00:34:31,010
220,000 events until all the events were

00:34:29,540 --> 00:34:34,610
finally processed and the caught up

00:34:31,010 --> 00:34:36,800
again why would you do this well in some

00:34:34,610 --> 00:34:39,080
cases Judas may not be able to or may be

00:34:36,800 --> 00:34:40,909
too slow or too expensive to increase

00:34:39,080 --> 00:34:43,460
the size of the rest of the pipeline and

00:34:40,909 --> 00:34:45,050
in this particular application you could

00:34:43,460 --> 00:34:47,090
easily increase the application part

00:34:45,050 --> 00:34:47,780
with the kubernetes scalability

00:34:47,090 --> 00:34:51,110
mechanisms

00:34:47,780 --> 00:34:53,540
however resizing cassandra is actually

00:34:51,110 --> 00:34:55,340
quite difficult and quite slow and you'd

00:34:53,540 --> 00:34:57,980
have to have sufficient warning that you

00:34:55,340 --> 00:35:00,080
were expecting a load spike this I mean

00:34:57,980 --> 00:35:03,080
you can do it you can add nodes in our

00:35:00,080 --> 00:35:06,410
managed Cassandra and it will take time

00:35:03,080 --> 00:35:08,360
to increase the capacity of the cluster

00:35:06,410 --> 00:35:10,190
and it's pretty difficult to go back to

00:35:08,360 --> 00:35:12,020
a smaller system as the main problem so

00:35:10,190 --> 00:35:15,110
caf-co is a really good solution for

00:35:12,020 --> 00:35:17,960
that type of problem so there's so what

00:35:15,110 --> 00:35:20,240
about of question and this is a famous

00:35:17,960 --> 00:35:23,440
takeaway food van in Canberra if you're

00:35:20,240 --> 00:35:27,160
ever there it still still there

00:35:23,440 --> 00:35:29,890
so from the technical side kubernetes

00:35:27,160 --> 00:35:32,920
and AWS X in this case did enable

00:35:29,890 --> 00:35:34,660
automation of the application it did

00:35:32,920 --> 00:35:36,520
take some effort to understand and set

00:35:34,660 --> 00:35:38,500
up initially I think they've made the X

00:35:36,520 --> 00:35:41,830
setup a bit easier in the last few

00:35:38,500 --> 00:35:43,450
months but once working it makes the

00:35:41,830 --> 00:35:46,960
application deployment fast scalable

00:35:43,450 --> 00:35:49,840
repeatable and low cost prometheus and

00:35:46,960 --> 00:35:51,940
opened tracing and jeager were critical

00:35:49,840 --> 00:35:54,060
for debugging tuning and reporting the

00:35:51,940 --> 00:35:56,770
application performance and scalability

00:35:54,060 --> 00:35:59,260
it is tricky to monitor applications and

00:35:56,770 --> 00:36:01,750
kubernetes but using the prometheus

00:35:59,260 --> 00:36:04,060
operator sort of automated all that

00:36:01,750 --> 00:36:07,000
setup and the dynamic aspects of

00:36:04,060 --> 00:36:08,860
monitoring really well to achieve near

00:36:07,000 --> 00:36:11,080
linear scalability and maximize the

00:36:08,860 --> 00:36:12,970
throughput we needed to optimize the

00:36:11,080 --> 00:36:15,400
pipeline in particular in this case by

00:36:12,970 --> 00:36:18,100
tuning the thread pool sizes and the

00:36:15,400 --> 00:36:19,990
number of kubernetes pods and the goal

00:36:18,100 --> 00:36:23,650
of that was to minimize the number of

00:36:19,990 --> 00:36:25,420
Cassandra connections we found that sha

00:36:23,650 --> 00:36:27,250
Cassandra can cope with an arbitrary

00:36:25,420 --> 00:36:28,720
number of connections but there is an

00:36:27,250 --> 00:36:31,480
impact on the throughput so you need to

00:36:28,720 --> 00:36:33,520
try and minimize those you also need to

00:36:31,480 --> 00:36:36,010
minimize the number of Khafre consumers

00:36:33,520 --> 00:36:37,840
because that results in more Kafka

00:36:36,010 --> 00:36:41,170
petitions which as the graph showed

00:36:37,840 --> 00:36:43,450
results in lower throughput and we also

00:36:41,170 --> 00:36:44,980
needed to maximize the detector thread

00:36:43,450 --> 00:36:47,920
pool concurrency in order to actually

00:36:44,980 --> 00:36:49,630
just get the results that we did so

00:36:47,920 --> 00:36:52,960
there a bit of a sort of a trade-off

00:36:49,630 --> 00:36:55,660
sort of capabilities you need to keep

00:36:52,960 --> 00:36:56,280
those in mind from a business

00:36:55,660 --> 00:36:58,720
perspective

00:36:56,280 --> 00:37:02,620
Kefka and Cassandra enable fast

00:36:58,720 --> 00:37:04,690
streaming and storage at scale now in

00:37:02,620 --> 00:37:07,120
stochastic managed Kefka and Cassandra

00:37:04,690 --> 00:37:09,160
service did make it easy to automate the

00:37:07,120 --> 00:37:11,410
cluster provisioning including creation

00:37:09,160 --> 00:37:12,910
deletion and scaling and the monitoring

00:37:11,410 --> 00:37:15,400
of the clusters themselves which was

00:37:12,910 --> 00:37:17,290
quite important for me I was sort of new

00:37:15,400 --> 00:37:18,640
to Cassandra and Kafka so I needed to be

00:37:17,290 --> 00:37:22,990
confident of the clusters themselves

00:37:18,640 --> 00:37:25,810
when a bottleneck at all in the system

00:37:22,990 --> 00:37:27,580
we've got highly available inlays and

00:37:25,810 --> 00:37:28,840
proactive cluster monitoring alerting

00:37:27,580 --> 00:37:31,780
and maintenance for production

00:37:28,840 --> 00:37:33,730
environments as well it's affordability

00:37:31,780 --> 00:37:34,780
at scale you can have a low cost open

00:37:33,730 --> 00:37:37,349
source and commodity cloud

00:37:34,780 --> 00:37:39,989
infrastructure you only pay for what you

00:37:37,349 --> 00:37:41,970
in terms of the application and the calf

00:37:39,989 --> 00:37:44,819
gain Cassandra clusters which scale

00:37:41,970 --> 00:37:47,519
linearly with load so cost only

00:37:44,819 --> 00:37:49,739
increases incrementally as well the

00:37:47,519 --> 00:37:52,289
application can be easily resized up or

00:37:49,739 --> 00:37:55,319
down for any work cloud and there's no

00:37:52,289 --> 00:37:57,180
upper limit and of course there's lots

00:37:55,319 --> 00:37:59,700
more use cases involving applications

00:37:57,180 --> 00:38:01,739
using cafe and Cassandra together and

00:37:59,700 --> 00:38:04,650
other Apache open source projects as

00:38:01,739 --> 00:38:06,809
well so they're all quite they will work

00:38:04,650 --> 00:38:08,640
quite well together and you're only

00:38:06,809 --> 00:38:12,229
limited by your imagination in a sense

00:38:08,640 --> 00:38:14,489
too as to what you can build news flash

00:38:12,229 --> 00:38:16,739
so this wasn't quite the end of the

00:38:14,489 --> 00:38:18,599
story so I decided to have a look at

00:38:16,739 --> 00:38:24,989
geospatial anomaly detection as well

00:38:18,599 --> 00:38:26,400
just for fun so I compared the

00:38:24,989 --> 00:38:29,190
performance of multiple spatial

00:38:26,400 --> 00:38:32,249
representations and different Cassandra

00:38:29,190 --> 00:38:34,170
implementations I extended the

00:38:32,249 --> 00:38:36,809
application detect to detect anomalies

00:38:34,170 --> 00:38:38,519
over time and in space for example we

00:38:36,809 --> 00:38:40,710
wanted to find out if an event was

00:38:38,519 --> 00:38:46,319
unusual relative to the say it's nearest

00:38:40,710 --> 00:38:49,710
50 neighbors where but the the spatial

00:38:46,319 --> 00:38:52,079
dimension wasn't fixed so a neighbor

00:38:49,710 --> 00:38:53,249
might be either right next to you or 100

00:38:52,079 --> 00:38:56,309
kilometers away depending on how many

00:38:53,249 --> 00:38:59,309
were available at a particular in a

00:38:56,309 --> 00:39:00,960
particular area so I used a few tricks

00:38:59,309 --> 00:39:03,420
to find neighbors including

00:39:00,960 --> 00:39:05,489
just latitude longitude points and

00:39:03,420 --> 00:39:09,420
distance calculations bounding boxes

00:39:05,489 --> 00:39:11,789
geohashes and also some 3d geohashes as

00:39:09,420 --> 00:39:12,749
well the different Cassandra

00:39:11,789 --> 00:39:14,880
implementations

00:39:12,749 --> 00:39:16,979
I tried were clustering columns

00:39:14,880 --> 00:39:19,859
secondary indexes the normalized

00:39:16,979 --> 00:39:24,749
multiple tables and the Cassandra Lucene

00:39:19,859 --> 00:39:26,130
index plugin and yeah so I haven't given

00:39:24,749 --> 00:39:28,739
you the results there if you understand

00:39:26,130 --> 00:39:30,989
and that which was sort of an extension

00:39:28,739 --> 00:39:33,180
of this work there's another four part

00:39:30,989 --> 00:39:37,559
blog series on our website which goes

00:39:33,180 --> 00:39:39,450
into the details and the results so if

00:39:37,559 --> 00:39:40,920
you understood in any of this in order

00:39:39,450 --> 00:39:43,259
to find out a bit more there's a lot

00:39:40,920 --> 00:39:46,049
more information online the whole

00:39:43,259 --> 00:39:49,229
anomaly machine block series was ten

00:39:46,049 --> 00:39:50,340
parts over about six months there's the

00:39:49,229 --> 00:39:54,750
full path

00:39:50,340 --> 00:39:56,760
geospatial extension the code itself the

00:39:54,750 --> 00:39:58,170
application is on our github if anyone

00:39:56,760 --> 00:39:59,970
wants to have a look it's not

00:39:58,170 --> 00:40:02,520
particularly clever or well-written but

00:39:59,970 --> 00:40:04,350
it's a reasonable example of an

00:40:02,520 --> 00:40:06,990
application that uses both calf:cow and

00:40:04,350 --> 00:40:10,260
cassandra together and all of my blogs

00:40:06,990 --> 00:40:11,580
are available as well so on over two

00:40:10,260 --> 00:40:13,320
years I've written a fair amount of

00:40:11,580 --> 00:40:15,150
stuff hopefully some of its useful for

00:40:13,320 --> 00:40:17,190
the community and it's all about open

00:40:15,150 --> 00:40:21,870
source technologies so hopefully there's

00:40:17,190 --> 00:40:24,390
a good tie in to Apache con so finally

00:40:21,870 --> 00:40:28,980
some anomalies are easy to detect

00:40:24,390 --> 00:40:32,030
hopefully some anomalies can be detected

00:40:28,980 --> 00:40:35,940
given sufficient time this was a quote

00:40:32,030 --> 00:40:39,630
some some decades after Woodstock had

00:40:35,940 --> 00:40:41,670
come and gone I think I don't know other

00:40:39,630 --> 00:40:44,100
potential anomalies are harder to detect

00:40:41,670 --> 00:40:46,290
so I don't know this is a few months old

00:40:44,100 --> 00:40:48,120
the slide but a few months ago there was

00:40:46,290 --> 00:40:51,030
quite a lot in the news about some of

00:40:48,120 --> 00:40:52,890
the fires burning around the world and

00:40:51,030 --> 00:40:55,110
interestingly enough they were burning

00:40:52,890 --> 00:40:57,150
in the Amazon the Congo and the Siberia

00:40:55,110 --> 00:40:59,430
and there was quite a lot of discussion

00:40:57,150 --> 00:41:02,460
about which fires were worse than normal

00:40:59,430 --> 00:41:04,850
ye anomalies and it was actually quite

00:41:02,460 --> 00:41:07,140
hard to work that out

00:41:04,850 --> 00:41:08,940
so if you want to detect complex

00:41:07,140 --> 00:41:11,220
spatio-temporal or not only is reliable

00:41:08,940 --> 00:41:13,760
yet scale with kefka Cassandra and

00:41:11,220 --> 00:41:18,030
kubernetes please feel free to try our

00:41:13,760 --> 00:41:19,890
managed platform for open source and if

00:41:18,030 --> 00:41:21,840
you want to know which was the anomaly

00:41:19,890 --> 00:41:26,250
in terms of the fires it was actually

00:41:21,840 --> 00:41:26,910
Siberia interestingly enough so so

00:41:26,250 --> 00:41:30,780
that's it I think

00:41:26,910 --> 00:41:33,170
I think that's about my time and thank

00:41:30,780 --> 00:41:33,170
you very much

00:41:34,580 --> 00:41:39,010
[Music]

00:41:36,950 --> 00:41:39,010

YouTube URL: https://www.youtube.com/watch?v=dffis9XoYn0


