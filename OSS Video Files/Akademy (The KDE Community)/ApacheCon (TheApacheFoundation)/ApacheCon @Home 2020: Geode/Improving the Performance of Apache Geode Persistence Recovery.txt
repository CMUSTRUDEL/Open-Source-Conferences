Title: Improving the Performance of Apache Geode Persistence Recovery
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020: Geode
Description: 
	Improving the Performance of Apache Geode Persistence Recovery
Jianxia Chen

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Geode offers super fast write-ahead-logging (WAL) persistence with a shared-nothing architecture that is optimized for fast parallel recovery of nodes or an entire cluster. In this talk, we will first introduce how Geode disk stores work. Then we will present the recent work to improve the performance of persistence recovery. With the analysis of Geode logs, we find that the performance of persistence recovery can be significantly improved by unblocking some of the server initialization threads and parallelizing the process of disk stores recovery. Our experiments have proved that persistence recovery becomes remarkably more scalable and efficient with the improved process.

Jianxia is a PMC member and committer of Apache Geode. He enjoys working on open source software.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,480 --> 00:00:29,519
okay

00:00:25,519 --> 00:00:32,239
let's get started welcome join us

00:00:29,519 --> 00:00:33,840
i'm jinxia i'm a party geocommuter today

00:00:32,239 --> 00:00:36,719
i'm going to share with

00:00:33,840 --> 00:00:37,200
my experience about apache geode it's

00:00:36,719 --> 00:00:39,200
about

00:00:37,200 --> 00:00:44,239
improving the performance of apache

00:00:39,200 --> 00:00:46,719
geode positions recovery

00:00:44,239 --> 00:00:48,160
so here's what i'm going to talk about

00:00:46,719 --> 00:00:50,559
first i will introduce

00:00:48,160 --> 00:00:52,879
what apache geo disk store is and how

00:00:50,559 --> 00:00:54,800
persistence recovery works

00:00:52,879 --> 00:00:56,160
and then i'm going to talk about the

00:00:54,800 --> 00:00:59,840
performance improvements

00:00:56,160 --> 00:01:02,640
of persistence recovery i've done

00:00:59,840 --> 00:01:03,600
it's avoiding log contention and

00:01:02,640 --> 00:01:07,360
introducing

00:01:03,600 --> 00:01:07,360
parallel disk store recovery

00:01:08,320 --> 00:01:12,000
so apache geo offers a low latency and

00:01:10,720 --> 00:01:15,520
high concurrency data

00:01:12,000 --> 00:01:16,720
management solution it has database like

00:01:15,520 --> 00:01:19,119
consistency model

00:01:16,720 --> 00:01:20,159
and the shared nothing architecture is

00:01:19,119 --> 00:01:22,720
modern

00:01:20,159 --> 00:01:24,000
cache so on the screen you can see a

00:01:22,720 --> 00:01:28,320
screenshot from

00:01:24,000 --> 00:01:32,240
the official apache website it shows

00:01:28,320 --> 00:01:34,720
the some of the features that geode has

00:01:32,240 --> 00:01:36,159
so they have there are application and

00:01:34,720 --> 00:01:39,200
partitioning of your data

00:01:36,159 --> 00:01:42,399
you can position persist your data

00:01:39,200 --> 00:01:44,320
into a hard drive and offers superior

00:01:42,399 --> 00:01:45,759
performance and your data is in memory

00:01:44,320 --> 00:01:47,680
in addition

00:01:45,759 --> 00:01:49,360
to in memory data against persistent

00:01:47,680 --> 00:01:51,920
data to disk

00:01:49,360 --> 00:01:54,479
the overalls function execution and

00:01:51,920 --> 00:01:58,079
transactions and oql and

00:01:54,479 --> 00:02:00,880
event processing and clustering and

00:01:58,079 --> 00:02:01,759
you can do one replication across

00:02:00,880 --> 00:02:05,119
different

00:02:01,759 --> 00:02:07,759
geographic locations and you have the

00:02:05,119 --> 00:02:09,759
it has the continuous query and offer

00:02:07,759 --> 00:02:11,520
different declines for different client

00:02:09,759 --> 00:02:16,000
languages for example

00:02:11,520 --> 00:02:19,360
java c c sharp c plus plus and dot net

00:02:16,000 --> 00:02:22,800
and also your first adapters for radius

00:02:19,360 --> 00:02:24,319
and the main cache d i believe the

00:02:22,800 --> 00:02:27,040
community is actively working

00:02:24,319 --> 00:02:29,280
on developing new features and bug fixes

00:02:27,040 --> 00:02:31,120
for apache geode making apache only an

00:02:29,280 --> 00:02:32,640
even better product

00:02:31,120 --> 00:02:34,879
so today i'm going to focus on the

00:02:32,640 --> 00:02:36,959
feature of persistence

00:02:34,879 --> 00:02:39,519
so here's the feature about positions

00:02:36,959 --> 00:02:42,080
apache geo offers super fast

00:02:39,519 --> 00:02:44,319
right ahead logging positions and it has

00:02:42,080 --> 00:02:46,640
shared nothing architecture

00:02:44,319 --> 00:02:48,000
so the system is optimized for fast

00:02:46,640 --> 00:02:51,920
parallel recovery of

00:02:48,000 --> 00:02:55,040
nodes or or an entire cluster

00:02:51,920 --> 00:02:55,519
let's look into the architecture so here

00:02:55,040 --> 00:02:57,920
is the

00:02:55,519 --> 00:02:59,760
overview of the architecture of apache

00:02:57,920 --> 00:03:01,599
geo persistence

00:02:59,760 --> 00:03:03,840
on this picture you can see a number of

00:03:01,599 --> 00:03:06,239
cache servers and we can see that

00:03:03,840 --> 00:03:07,920
we can horizontally scale the number of

00:03:06,239 --> 00:03:10,800
cache servers

00:03:07,920 --> 00:03:13,760
and and also for each cache server we

00:03:10,800 --> 00:03:16,000
can attach a hard drive or disk

00:03:13,760 --> 00:03:16,879
that in addition to the memory data we

00:03:16,000 --> 00:03:19,840
can persist

00:03:16,879 --> 00:03:21,840
data into the hard drive so that in the

00:03:19,840 --> 00:03:23,440
event of a node failure or even a

00:03:21,840 --> 00:03:26,959
cluster failure we can

00:03:23,440 --> 00:03:29,120
efficiently infe fastly recover data

00:03:26,959 --> 00:03:31,680
from the hard drive

00:03:29,120 --> 00:03:33,120
so let's zoom a little bit more into the

00:03:31,680 --> 00:03:34,480
cache server see what's going on in the

00:03:33,120 --> 00:03:37,519
cache server in terms of

00:03:34,480 --> 00:03:39,440
positions and recovery

00:03:37,519 --> 00:03:41,120
let's look at your cache server so in

00:03:39,440 --> 00:03:44,159
the cache server there are

00:03:41,120 --> 00:03:45,280
a number of regions and a number of disk

00:03:44,159 --> 00:03:47,280
stores

00:03:45,280 --> 00:03:49,040
the regions can be replicated or

00:03:47,280 --> 00:03:49,920
partitioned and the region is

00:03:49,040 --> 00:03:52,879
essentially a

00:03:49,920 --> 00:03:54,080
distributed hash table so you can

00:03:52,879 --> 00:03:57,680
imagine that

00:03:54,080 --> 00:04:00,159
regions has the keys and values

00:03:57,680 --> 00:04:02,480
and each cache server has a set of disk

00:04:00,159 --> 00:04:05,120
stores

00:04:02,480 --> 00:04:06,000
some of the region can has the dedicated

00:04:05,120 --> 00:04:08,000
disk store

00:04:06,000 --> 00:04:09,519
or some regions can share the same disk

00:04:08,000 --> 00:04:12,560
store

00:04:09,519 --> 00:04:14,720
and the cache server is a java process

00:04:12,560 --> 00:04:15,840
outside the java process the disk store

00:04:14,720 --> 00:04:17,919
posits the data

00:04:15,840 --> 00:04:19,840
the region data into the operating

00:04:17,919 --> 00:04:21,359
system file system

00:04:19,840 --> 00:04:24,160
on the file system on the bottom two

00:04:21,359 --> 00:04:27,520
boxes you can see there are two

00:04:24,160 --> 00:04:31,120
set of files for the disk stores

00:04:27,520 --> 00:04:33,280
we call them upload files upload stands

00:04:31,120 --> 00:04:35,520
for operation log

00:04:33,280 --> 00:04:37,680
you can see in the directories there are

00:04:35,520 --> 00:04:40,960
files with extensions krf

00:04:37,680 --> 00:04:43,120
cif and dif so

00:04:40,960 --> 00:04:44,160
the krf are the files that stores the

00:04:43,120 --> 00:04:47,040
keys and offsets

00:04:44,160 --> 00:04:47,520
for the values in the cip files and the

00:04:47,040 --> 00:04:50,880
ci

00:04:47,520 --> 00:04:51,759
files stores the operations of the entry

00:04:50,880 --> 00:04:56,000
create

00:04:51,759 --> 00:04:58,720
update and invalidate the di file stores

00:04:56,000 --> 00:04:59,919
entry delete operations so those are the

00:04:58,720 --> 00:05:03,280
files that stores

00:04:59,919 --> 00:05:04,639
all these region operations and during

00:05:03,280 --> 00:05:07,440
recovery

00:05:04,639 --> 00:05:07,759
the cache server will read the data from

00:05:07,440 --> 00:05:10,160
the

00:05:07,759 --> 00:05:11,919
these files to rebuild the region in

00:05:10,160 --> 00:05:14,080
memory

00:05:11,919 --> 00:05:15,600
so let's take a look at the startup

00:05:14,080 --> 00:05:19,120
process

00:05:15,600 --> 00:05:21,520
so during startup entry keys from the kl

00:05:19,120 --> 00:05:24,080
files are loaded before entry values

00:05:21,520 --> 00:05:26,160
in the ci files this is a performance

00:05:24,080 --> 00:05:28,320
optimization geode has

00:05:26,160 --> 00:05:30,080
so that we can recover the cluster as

00:05:28,320 --> 00:05:32,080
fast as possible

00:05:30,080 --> 00:05:35,199
and once all the keys are loaded entry

00:05:32,080 --> 00:05:35,199
values are loaded

00:05:36,840 --> 00:05:39,919
asynchronously

00:05:38,320 --> 00:05:41,840
now it's a time to talk about the

00:05:39,919 --> 00:05:44,240
performance of persistence recovery

00:05:41,840 --> 00:05:45,680
since geode offers superior performance

00:05:44,240 --> 00:05:48,080
for the applications

00:05:45,680 --> 00:05:48,960
so we always seek ways to improve the

00:05:48,080 --> 00:05:51,360
performance

00:05:48,960 --> 00:05:53,120
in this case this is a performance for

00:05:51,360 --> 00:05:55,360
persistence recovery

00:05:53,120 --> 00:05:56,479
for a cluster or for some nodes so we're

00:05:55,360 --> 00:05:59,120
doing some persistent

00:05:56,479 --> 00:06:01,199
recovery tests for whole cluster we're

00:05:59,120 --> 00:06:05,120
doing that on the geo cloud

00:06:01,199 --> 00:06:08,400
only google cloud so on google cloud

00:06:05,120 --> 00:06:11,600
we have chosen some google vms

00:06:08,400 --> 00:06:15,199
which each has 16 vcpus

00:06:11,600 --> 00:06:18,960
and 128 gig memory memory

00:06:15,199 --> 00:06:22,800
and each vm also has

00:06:18,960 --> 00:06:24,720
a 200 gig this ssd hard drive

00:06:22,800 --> 00:06:26,720
so we start with a small cluster one

00:06:24,720 --> 00:06:28,000
locator and the two servers and then we

00:06:26,720 --> 00:06:30,880
can scale up to

00:06:28,000 --> 00:06:32,319
up to for example 70 servers we start

00:06:30,880 --> 00:06:35,759
from small scale first

00:06:32,319 --> 00:06:37,680
and for the data we have

00:06:35,759 --> 00:06:40,319
the partition the redundant position

00:06:37,680 --> 00:06:44,800
regions with two billion entries

00:06:40,319 --> 00:06:47,440
and each region key is of size 100 bytes

00:06:44,800 --> 00:06:49,280
and we keep the default values for

00:06:47,440 --> 00:06:51,280
out-of-the-box yield calculations

00:06:49,280 --> 00:06:54,639
so that we can fix some of the

00:06:51,280 --> 00:06:58,000
configurations and tune the others

00:06:54,639 --> 00:07:00,560
now how do we measure the performance

00:06:58,000 --> 00:07:02,800
we measure performance using a very

00:07:00,560 --> 00:07:03,440
simple magic the cluster recovery time

00:07:02,800 --> 00:07:05,919
meaning

00:07:03,440 --> 00:07:07,039
the time the cluster takes to recover

00:07:05,919 --> 00:07:10,319
data

00:07:07,039 --> 00:07:14,160
from the hard drive and

00:07:10,319 --> 00:07:14,160
until it's ready to serve the client

00:07:14,840 --> 00:07:19,840
requests

00:07:16,960 --> 00:07:20,880
so here's what we have observed we have

00:07:19,840 --> 00:07:23,759
loaded data

00:07:20,880 --> 00:07:25,599
into the memory first and the data are

00:07:23,759 --> 00:07:27,599
balanced among the servers

00:07:25,599 --> 00:07:29,039
and we when we shut down the servers and

00:07:27,599 --> 00:07:32,080
restart the whole

00:07:29,039 --> 00:07:34,720
cluster so we expect that because the

00:07:32,080 --> 00:07:36,319
the data is balanced among the servers

00:07:34,720 --> 00:07:38,080
so we expect that

00:07:36,319 --> 00:07:39,360
each server takes a similar amount of

00:07:38,080 --> 00:07:41,759
time to recover

00:07:39,360 --> 00:07:42,880
but according to a figure on the screen

00:07:41,759 --> 00:07:45,680
you can see that

00:07:42,880 --> 00:07:47,520
um there are four servers and each

00:07:45,680 --> 00:07:49,919
server takes some time to recover

00:07:47,520 --> 00:07:52,400
and most of the server takes about 700

00:07:49,919 --> 00:07:52,879
milli 700 seconds to recover for example

00:07:52,400 --> 00:07:55,520
server

00:07:52,879 --> 00:07:57,599
server one server 3 and server 4 they

00:07:55,520 --> 00:08:00,720
take about

00:07:57,599 --> 00:08:02,000
700 seconds to recover but server 2

00:08:00,720 --> 00:08:05,199
takes a longer time

00:08:02,000 --> 00:08:09,039
server 2 takes about more than 1000

00:08:05,199 --> 00:08:10,720
seconds to recover which is a 50 percent

00:08:09,039 --> 00:08:14,240
longer than the other servers

00:08:10,720 --> 00:08:16,080
so there are some outliers in this case

00:08:14,240 --> 00:08:18,800
and jio cluster has to wait for the

00:08:16,080 --> 00:08:22,319
slowest server slowest server to recover

00:08:18,800 --> 00:08:24,639
in order to serve the client requests

00:08:22,319 --> 00:08:25,360
so in this case the whole cluster will

00:08:24,639 --> 00:08:28,240
take about

00:08:25,360 --> 00:08:30,800
more than a thousand seconds to recover

00:08:28,240 --> 00:08:33,680
in order to serve the client requests

00:08:30,800 --> 00:08:35,760
so what happens under hood there are

00:08:33,680 --> 00:08:40,479
multiple ways to investigate

00:08:35,760 --> 00:08:42,719
let's start from the log analysis so for

00:08:40,479 --> 00:08:43,919
the jio cluster each server has its own

00:08:42,719 --> 00:08:47,680
log

00:08:43,919 --> 00:08:48,880
typically the server the the server logs

00:08:47,680 --> 00:08:52,959
range this lens

00:08:48,880 --> 00:08:55,680
range from a few hundred lines of text

00:08:52,959 --> 00:08:57,920
to thousands of lines of text it sounds

00:08:55,680 --> 00:09:01,519
scary right it's a long

00:08:57,920 --> 00:09:02,560
log but for our case we have control of

00:09:01,519 --> 00:09:06,320
our setup so

00:09:02,560 --> 00:09:09,440
our case the log size about

00:09:06,320 --> 00:09:11,760
a few thousand lines of log

00:09:09,440 --> 00:09:12,959
and there are some tips to analyze the

00:09:11,760 --> 00:09:14,640
logs

00:09:12,959 --> 00:09:16,800
for example typically i always search

00:09:14,640 --> 00:09:19,200
for some suspicious strings

00:09:16,800 --> 00:09:20,959
for example exceptions whether there's

00:09:19,200 --> 00:09:24,000
an exception in the log

00:09:20,959 --> 00:09:26,160
or stack traces in a log or search some

00:09:24,000 --> 00:09:30,160
keywords in the log level for example

00:09:26,160 --> 00:09:32,880
warning severe level lock level

00:09:30,160 --> 00:09:33,600
so these are the tips to analyze the

00:09:32,880 --> 00:09:36,160
logs

00:09:33,600 --> 00:09:37,120
also in geo there is something called

00:09:36,160 --> 00:09:40,480
threat monitor

00:09:37,120 --> 00:09:43,680
which is a feature that reports start

00:09:40,480 --> 00:09:47,040
threats in the server so let me show you

00:09:43,680 --> 00:09:48,560
an example here is a screenshot of the

00:09:47,040 --> 00:09:49,839
threat monitor you don't have to read

00:09:48,560 --> 00:09:52,000
every line

00:09:49,839 --> 00:09:53,760
here the screenshot just you give you an

00:09:52,000 --> 00:09:56,480
impression of what a threat

00:09:53,760 --> 00:09:59,279
monitor looks like you can see that it's

00:09:56,480 --> 00:09:59,279
a stack trace

00:09:59,360 --> 00:10:03,519
let's zoom in a little bit more about

00:10:01,120 --> 00:10:05,680
the thread monitor

00:10:03,519 --> 00:10:07,120
so i've minimized this lines of the

00:10:05,680 --> 00:10:10,160
stacks

00:10:07,120 --> 00:10:11,279
that is not interest of to us at this

00:10:10,160 --> 00:10:13,200
point of time

00:10:11,279 --> 00:10:14,720
so let's read from the top i've

00:10:13,200 --> 00:10:17,600
highlighted the keywords so

00:10:14,720 --> 00:10:18,959
this is this is a warning of the log

00:10:17,600 --> 00:10:20,800
level

00:10:18,959 --> 00:10:22,160
and this is australian monitor reports

00:10:20,800 --> 00:10:25,920
that the one thread is

00:10:22,160 --> 00:10:26,959
executed as has been stocked for 112

00:10:25,920 --> 00:10:29,760
seconds it's about

00:10:26,959 --> 00:10:31,440
two minutes it's pretty long time and

00:10:29,760 --> 00:10:32,560
australia is waiting on a lock on the

00:10:31,440 --> 00:10:34,560
hash map

00:10:32,560 --> 00:10:36,480
and on the bottom you have two you see

00:10:34,560 --> 00:10:39,519
you see two stacks

00:10:36,480 --> 00:10:40,880
of two threads so let's go through the

00:10:39,519 --> 00:10:44,000
thread stacks

00:10:40,880 --> 00:10:44,720
i have highlighted the lines of the

00:10:44,000 --> 00:10:46,560
stack

00:10:44,720 --> 00:10:48,720
so on the top stack we can see that

00:10:46,560 --> 00:10:51,600
there is a some

00:10:48,720 --> 00:10:52,079
um something about gemfirecast input so

00:10:51,600 --> 00:10:55,279
it's

00:10:52,079 --> 00:10:56,800
doing some get region operation and next

00:10:55,279 --> 00:10:58,480
to that we can see that there's a

00:10:56,800 --> 00:11:00,640
creation processor and

00:10:58,480 --> 00:11:03,600
region message looks like this thread is

00:11:00,640 --> 00:11:06,000
doing some create region operation

00:11:03,600 --> 00:11:06,640
and let's move on on the bottom of the

00:11:06,000 --> 00:11:10,640
thread

00:11:06,640 --> 00:11:12,720
we can see that there is an upload read

00:11:10,640 --> 00:11:14,880
kif remember we talk about ki

00:11:12,720 --> 00:11:16,240
files on the disk before in the previous

00:11:14,880 --> 00:11:18,160
slide

00:11:16,240 --> 00:11:20,399
so it looks like this thread is doing

00:11:18,160 --> 00:11:23,040
some reading from the disk to restore

00:11:20,399 --> 00:11:24,720
the data into the memory

00:11:23,040 --> 00:11:26,720
and also in the middle of the stack we

00:11:24,720 --> 00:11:29,360
can see that it also refers to the same

00:11:26,720 --> 00:11:31,519
java code gen file cache import

00:11:29,360 --> 00:11:32,560
so both thread refers to the same java

00:11:31,519 --> 00:11:34,880
code

00:11:32,560 --> 00:11:35,920
let's look into the java code of that

00:11:34,880 --> 00:11:37,920
file and then

00:11:35,920 --> 00:11:40,079
look into the lines that is referenced

00:11:37,920 --> 00:11:41,920
by the thread stack

00:11:40,079 --> 00:11:44,320
so here's the source code of that gen

00:11:41,920 --> 00:11:45,920
file caching.java

00:11:44,320 --> 00:11:48,800
in the source code on top you can see

00:11:45,920 --> 00:11:52,480
that the code defines a hash map

00:11:48,800 --> 00:11:55,360
it's a hashmap called root regions and

00:11:52,480 --> 00:11:56,480
on the bottom there are two public

00:11:55,360 --> 00:11:58,160
methods

00:11:56,480 --> 00:12:00,240
one is called get region and the other

00:11:58,160 --> 00:12:02,720
is called vm region and

00:12:00,240 --> 00:12:04,720
it within each method there's a

00:12:02,720 --> 00:12:06,720
synchronized block

00:12:04,720 --> 00:12:08,959
so the synchronized blocks are

00:12:06,720 --> 00:12:11,920
synchronizing on the hash map

00:12:08,959 --> 00:12:13,120
so if one thread holding the lock on one

00:12:11,920 --> 00:12:14,399
synchronizer block

00:12:13,120 --> 00:12:16,399
and the other thread is trying to

00:12:14,399 --> 00:12:17,920
acquire the same lock that thread will

00:12:16,399 --> 00:12:20,959
be blocked

00:12:17,920 --> 00:12:23,200
so for our case one thread is using is

00:12:20,959 --> 00:12:26,399
running the correct vm region

00:12:23,200 --> 00:12:27,040
and it acquires a log and is creating

00:12:26,399 --> 00:12:30,160
the vm

00:12:27,040 --> 00:12:33,200
of creating the region by reading the

00:12:30,160 --> 00:12:35,360
keys from the disk and restore data and

00:12:33,200 --> 00:12:38,639
the other thread is trying to

00:12:35,360 --> 00:12:42,079
get a region create accessories but it

00:12:38,639 --> 00:12:45,120
has to be it has to wait until it has

00:12:42,079 --> 00:12:48,399
access to a log now you might ask why

00:12:45,120 --> 00:12:50,560
this will slow down the cluster recovery

00:12:48,399 --> 00:12:53,200
let's take a look at the figure so there

00:12:50,560 --> 00:12:56,959
are certain steps to recover

00:12:53,200 --> 00:12:59,839
the data on a server so here i list

00:12:56,959 --> 00:13:00,560
a number of threads in the figure only

00:12:59,839 --> 00:13:03,040
on the

00:13:00,560 --> 00:13:03,920
figure you can see that there are two

00:13:03,040 --> 00:13:07,120
servers

00:13:03,920 --> 00:13:07,760
each server has a disk to recover data

00:13:07,120 --> 00:13:09,519
from

00:13:07,760 --> 00:13:10,959
so for server one it's recovering the

00:13:09,519 --> 00:13:13,920
data

00:13:10,959 --> 00:13:15,519
from the disk now let's follow the steps

00:13:13,920 --> 00:13:16,639
in this middle of the screen you can see

00:13:15,519 --> 00:13:18,839
that

00:13:16,639 --> 00:13:20,399
there the first step is create region

00:13:18,839 --> 00:13:23,360
message here

00:13:20,399 --> 00:13:24,480
in the geode cluster we in the

00:13:23,360 --> 00:13:28,240
distribute system

00:13:24,480 --> 00:13:29,920
the geo servers exchange

00:13:28,240 --> 00:13:32,079
talk to each other by exchanging the

00:13:29,920 --> 00:13:34,079
messages for example in this case the

00:13:32,079 --> 00:13:36,079
server tool actually is trying to create

00:13:34,079 --> 00:13:39,360
a metadata region

00:13:36,079 --> 00:13:40,079
on the cluster so server 2 is sending a

00:13:39,360 --> 00:13:43,839
query region

00:13:40,079 --> 00:13:46,000
message to server 1 but as we described

00:13:43,839 --> 00:13:49,040
before in the previous slide server 1

00:13:46,000 --> 00:13:50,639
is busy recovering data from the disk so

00:13:49,040 --> 00:13:53,040
that thread block

00:13:50,639 --> 00:13:54,880
the processing of query create region

00:13:53,040 --> 00:13:57,920
message

00:13:54,880 --> 00:14:01,279
so server two has to wait

00:13:57,920 --> 00:14:04,560
for the reply from server one for

00:14:01,279 --> 00:14:06,560
the create region reply message so that

00:14:04,560 --> 00:14:09,120
resume reply message is blocked

00:14:06,560 --> 00:14:10,240
on server one and server two has to wait

00:14:09,120 --> 00:14:14,240
for that

00:14:10,240 --> 00:14:17,360
now on server two it has to read ki

00:14:14,240 --> 00:14:18,959
after creating a region so you can see

00:14:17,360 --> 00:14:22,000
on server two step five

00:14:18,959 --> 00:14:23,760
read kf is also blocked because

00:14:22,000 --> 00:14:25,680
server two hasn't received the reply

00:14:23,760 --> 00:14:27,199
from server one so everything on server

00:14:25,680 --> 00:14:29,199
two is blocked

00:14:27,199 --> 00:14:30,880
until server one finished reading the

00:14:29,199 --> 00:14:32,880
data from the disk

00:14:30,880 --> 00:14:33,920
so you can see that the blocking thread

00:14:32,880 --> 00:14:37,839
on server one

00:14:33,920 --> 00:14:40,639
actually slowed down the other servers

00:14:37,839 --> 00:14:41,760
this is why the cluster restart is

00:14:40,639 --> 00:14:45,839
slowed down by

00:14:41,760 --> 00:14:45,839
the threat lock contention

00:14:46,399 --> 00:14:50,480
having analyzed the root cause of the

00:14:48,839 --> 00:14:53,680
slowness of the

00:14:50,480 --> 00:14:55,120
recovery the solution seems

00:14:53,680 --> 00:14:57,199
straightforward

00:14:55,120 --> 00:14:58,800
we can just replace the hashmap with

00:14:57,199 --> 00:15:02,160
concurrent hashmap

00:14:58,800 --> 00:15:05,040
so that we can avoid log contentions

00:15:02,160 --> 00:15:06,160
so after replacing the hashmap with

00:15:05,040 --> 00:15:09,040
concurrent hashmap

00:15:06,160 --> 00:15:10,560
we can see significantly faster recovery

00:15:09,040 --> 00:15:11,680
let's look at a figure on the lower

00:15:10,560 --> 00:15:14,160
right corner

00:15:11,680 --> 00:15:15,680
so here's the figure shows the recovery

00:15:14,160 --> 00:15:17,920
time in terms of

00:15:15,680 --> 00:15:20,399
minutes for the cluster there are two

00:15:17,920 --> 00:15:22,959
bars it compares the

00:15:20,399 --> 00:15:25,040
result before and after the code change

00:15:22,959 --> 00:15:27,760
so before our improvement

00:15:25,040 --> 00:15:28,880
the cluster takes about 37 minutes to

00:15:27,760 --> 00:15:31,680
recover

00:15:28,880 --> 00:15:34,240
but after our improvement the cluster

00:15:31,680 --> 00:15:37,360
takes about 25 to 26 minutes

00:15:34,240 --> 00:15:39,440
in this case so the improvement is about

00:15:37,360 --> 00:15:42,079
30 percent

00:15:39,440 --> 00:15:44,079
and we can keep looking into the logs

00:15:42,079 --> 00:15:46,560
and we can see that after improvement

00:15:44,079 --> 00:15:47,600
there's no more threat monitor stack

00:15:46,560 --> 00:15:49,680
choices

00:15:47,600 --> 00:15:50,800
which confirms that after the

00:15:49,680 --> 00:15:54,480
improvement there is

00:15:50,800 --> 00:15:57,120
no more log contentions for this case

00:15:54,480 --> 00:15:58,800
and if you if i interested in this case

00:15:57,120 --> 00:16:00,480
and want to know more about the details

00:15:58,800 --> 00:16:02,320
how this is implemented how it's

00:16:00,480 --> 00:16:03,120
analyzed you can look into geogebra

00:16:02,320 --> 00:16:07,519
tickets

00:16:03,120 --> 00:16:10,079
it's the ticket yield 75 79.45

00:16:07,519 --> 00:16:12,079
so that ticket will show you the

00:16:10,079 --> 00:16:13,440
analysis and also refers to a pull

00:16:12,079 --> 00:16:16,800
request

00:16:13,440 --> 00:16:16,800
on the code on github

00:16:18,959 --> 00:16:23,279
let's see what can we do what what else

00:16:21,440 --> 00:16:25,279
can we do to improve the performance of

00:16:23,279 --> 00:16:27,759
the persistence recovery

00:16:25,279 --> 00:16:29,839
so what we have improved so far is that

00:16:27,759 --> 00:16:30,959
we have done a single region recovery

00:16:29,839 --> 00:16:34,639
with single disk

00:16:30,959 --> 00:16:35,120
store we have improved about 30 percent

00:16:34,639 --> 00:16:38,320
in

00:16:35,120 --> 00:16:39,839
in terms of performance now

00:16:38,320 --> 00:16:42,079
what about multiple regions with

00:16:39,839 --> 00:16:45,120
multiple disk store

00:16:42,079 --> 00:16:46,480
so micro region is more realistic in the

00:16:45,120 --> 00:16:50,720
real world

00:16:46,480 --> 00:16:53,440
right so in the setup

00:16:50,720 --> 00:16:54,959
we then introduce two regions each with

00:16:53,440 --> 00:16:58,079
a disk store

00:16:54,959 --> 00:16:58,560
now we run the same set of the tests to

00:16:58,079 --> 00:17:02,560
measure

00:16:58,560 --> 00:17:02,560
performance of the cluster recovery time

00:17:03,759 --> 00:17:10,240
let's look into the logs again

00:17:07,039 --> 00:17:11,039
so here is this part of the log in the

00:17:10,240 --> 00:17:13,600
logic there are

00:17:11,039 --> 00:17:15,679
quite a few lines of text but they are

00:17:13,600 --> 00:17:16,480
similar so it's trying to recover some

00:17:15,679 --> 00:17:19,520
upload

00:17:16,480 --> 00:17:21,439
or operation log from the hard drive

00:17:19,520 --> 00:17:23,439
so on the top half you can see that it's

00:17:21,439 --> 00:17:24,720
trying to recover some upload from disk

00:17:23,439 --> 00:17:27,439
store one

00:17:24,720 --> 00:17:28,880
and then on the lower bottom you can see

00:17:27,439 --> 00:17:30,000
on the bottom half you can see that it's

00:17:28,880 --> 00:17:33,679
trying to recover

00:17:30,000 --> 00:17:35,360
the data from disk store 2. and

00:17:33,679 --> 00:17:38,160
you can see that i have highlighted the

00:17:35,360 --> 00:17:41,679
tid equals to 0x1 the tid

00:17:38,160 --> 00:17:44,960
means thread id so we can see that

00:17:41,679 --> 00:17:45,520
the samsung id demands red take care of

00:17:44,960 --> 00:17:47,760
the

00:17:45,520 --> 00:17:49,440
disk store recovery for both sticks

00:17:47,760 --> 00:17:51,840
stores which means

00:17:49,440 --> 00:17:53,520
the disk store recovery is sequential

00:17:51,840 --> 00:17:57,600
it's handled by the same

00:17:53,520 --> 00:17:59,360
thread so we may think about oh can we

00:17:57,600 --> 00:18:03,200
improve that can we make it parallel so

00:17:59,360 --> 00:18:03,200
that we can improve the performance

00:18:03,280 --> 00:18:07,520
then let's analyze the code then we can

00:18:06,640 --> 00:18:10,480
search the

00:18:07,520 --> 00:18:12,559
strings in the code the log strings in

00:18:10,480 --> 00:18:15,120
the code after analyzing the code we can

00:18:12,559 --> 00:18:17,520
we find that there's a for loop

00:18:15,120 --> 00:18:18,400
that iterate over the disk stores and

00:18:17,520 --> 00:18:21,039
restore

00:18:18,400 --> 00:18:22,480
the disk stores during a startup so

00:18:21,039 --> 00:18:26,400
here's the for loop

00:18:22,480 --> 00:18:29,600
and it created this stores

00:18:26,400 --> 00:18:32,960
one by one now thinking about

00:18:29,600 --> 00:18:33,520
making it parallel we can use parallel

00:18:32,960 --> 00:18:36,000
stream

00:18:33,520 --> 00:18:36,799
to make the recovery disk store recovery

00:18:36,000 --> 00:18:38,799
parallel

00:18:36,799 --> 00:18:40,880
also we have introduced a new boolean

00:18:38,799 --> 00:18:44,480
system properties called

00:18:40,880 --> 00:18:45,120
parallel disk store recovery so depends

00:18:44,480 --> 00:18:48,000
on the

00:18:45,120 --> 00:18:50,640
property value we can introduce a

00:18:48,000 --> 00:18:52,480
parallel stream or non-parallel stream

00:18:50,640 --> 00:18:54,000
and based on the stream for each element

00:18:52,480 --> 00:18:57,120
of the stream we

00:18:54,000 --> 00:19:00,080
restore the disk store so if we set that

00:18:57,120 --> 00:19:02,400
the value of the system property to chew

00:19:00,080 --> 00:19:04,240
we will introduce parallel stream

00:19:02,400 --> 00:19:05,760
and it will recover these stores in

00:19:04,240 --> 00:19:07,679
parallel and

00:19:05,760 --> 00:19:09,200
in this code you can see that we have

00:19:07,679 --> 00:19:11,679
the parallel stream and we

00:19:09,200 --> 00:19:14,240
use the stream to recover this store in

00:19:11,679 --> 00:19:14,240
parallel

00:19:14,320 --> 00:19:19,120
after this improvement we can see that

00:19:16,880 --> 00:19:22,080
let's take a look at the figure first

00:19:19,120 --> 00:19:23,760
on the upper right corner so again we

00:19:22,080 --> 00:19:24,559
show the recovery time of the whole

00:19:23,760 --> 00:19:26,720
cluster

00:19:24,559 --> 00:19:28,480
this is a comparison of recovery time

00:19:26,720 --> 00:19:31,200
between the sequential recovery and the

00:19:28,480 --> 00:19:32,960
parallel recovery of the disk stores

00:19:31,200 --> 00:19:35,679
on the left hand side is the sequential

00:19:32,960 --> 00:19:39,039
recovery time for a cluster

00:19:35,679 --> 00:19:42,080
and it takes about 13 minutes

00:19:39,039 --> 00:19:43,600
for the whole cluster to recover but

00:19:42,080 --> 00:19:46,400
after we improve

00:19:43,600 --> 00:19:47,760
that into a parallel disk store recovery

00:19:46,400 --> 00:19:50,080
you can see that on the figure

00:19:47,760 --> 00:19:52,000
right hand side that right hand side bar

00:19:50,080 --> 00:19:54,320
it takes about only about six or seven

00:19:52,000 --> 00:19:56,160
minutes to recover a whole cluster so we

00:19:54,320 --> 00:19:58,080
double the speed of the recovery and

00:19:56,160 --> 00:20:01,360
reduce the recovery time by

00:19:58,080 --> 00:20:04,400
in half again we look into

00:20:01,360 --> 00:20:06,320
the logs to confirm our improvement

00:20:04,400 --> 00:20:08,720
after improvement if you look into the

00:20:06,320 --> 00:20:11,360
logs we can see that

00:20:08,720 --> 00:20:12,559
there are two threads each take care of

00:20:11,360 --> 00:20:14,559
the recovery of

00:20:12,559 --> 00:20:16,799
a different disk store so in this case

00:20:14,559 --> 00:20:21,360
we i have highlighted that the tid

00:20:16,799 --> 00:20:23,679
equals 0x 1 and trd equals 0x48

00:20:21,360 --> 00:20:24,559
so there are two different threads and

00:20:23,679 --> 00:20:28,159
they're thinking of

00:20:24,559 --> 00:20:30,400
taking care of the disk store recovery

00:20:28,159 --> 00:20:31,440
separately and we can see that the

00:20:30,400 --> 00:20:33,440
recovery

00:20:31,440 --> 00:20:36,880
of the two strata in their living this

00:20:33,440 --> 00:20:36,880
is what we expected

00:20:38,400 --> 00:20:42,000
now let's summarize what we have done to

00:20:40,320 --> 00:20:45,520
improve the performance

00:20:42,000 --> 00:20:48,159
of the persistence recovery so i've done

00:20:45,520 --> 00:20:50,159
something to avoid blocking thread by

00:20:48,159 --> 00:20:51,360
replacing the hash map with concurrent

00:20:50,159 --> 00:20:56,320
hash map

00:20:51,360 --> 00:20:58,559
this is tracked by geogebra geo7945

00:20:56,320 --> 00:21:00,960
and we also have introduced a new

00:20:58,559 --> 00:21:02,960
parallel disk storage recovery

00:21:00,960 --> 00:21:04,520
instead of sequential recovery this is

00:21:02,960 --> 00:21:07,679
tracked by

00:21:04,520 --> 00:21:08,640
85 you can refer to these two jiras to

00:21:07,679 --> 00:21:11,760
for more details

00:21:08,640 --> 00:21:17,840
as well as the code all right

00:21:11,760 --> 00:21:17,840
that's about it time for the questions

00:21:20,640 --> 00:21:31,840
if you have questions you can type on

00:21:22,480 --> 00:21:31,840
the chat window

00:21:43,440 --> 00:21:47,280
karen says night presentation thank you

00:21:46,000 --> 00:21:51,039
karen

00:21:47,280 --> 00:21:59,840
and thank you alberto and thanks diane

00:21:51,039 --> 00:21:59,840
aaron thanks michael

00:22:07,919 --> 00:22:13,840
any questions

00:22:23,919 --> 00:22:28,320
okay if there is no question we'll

00:22:26,640 --> 00:22:41,840
finish this presentation

00:22:28,320 --> 00:22:41,840
thank you everyone

00:22:49,200 --> 00:22:51,280

YouTube URL: https://www.youtube.com/watch?v=9GuqZ4ymn7E


