Title: Running a Apache Geode on Kubernetes
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020: Geode
Description: 
	Running a Apache Geode on Kubernetes
Michael Oleske, Aaron Lindsey

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

As application developers move workloads to Kubernetes, they expect data services to run on Kubernetes alongside their applications. Kubernetes excels at running stateless workloads, but how does it handle complex stateful applications such as Apache Geode, a distributed in-memory database? We will describe challenges faced while building a Geode operator for Kubernetes, including controlling pod terminations, working with a dynamic network, and ensuring state management during the lifecycle of the deployment. We will explain the solutions we took to control these challenges, and dive into how we tested these solutions. You will leave with an understanding of how we moved a system designed to run on bare metal to a Kubernetes environment, uniting your workloads with your data services.

Michael Oleske:
Michael is a software engineer on Apache Geode and Tanzu GemFire. He works on making Geode both run well and easy to deploy for Kubernetes.

Aaron Lindsey:
Aaron works as a software engineer on Apache Geode and VMware Tanzu GemFire, focusing on making Geode run well on Kubernetes. Outside of work, he enjoys hiking and backpacking in the Pacific Northwest mountains, and volunteering with his local community and church.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,320 --> 00:00:28,240
uh so

00:00:25,599 --> 00:00:30,720
welcome everyone i'm michael oleski i'm

00:00:28,240 --> 00:00:31,119
a software engineer at vmware based out

00:00:30,720 --> 00:00:34,399
of

00:00:31,119 --> 00:00:34,960
santa monica california i'm aaron

00:00:34,399 --> 00:00:37,760
lindsey

00:00:34,960 --> 00:00:38,640
i'm an apache geocommitter and engineer

00:00:37,760 --> 00:00:42,239
at vmware

00:00:38,640 --> 00:00:44,000
i'm based in oregon in the united states

00:00:42,239 --> 00:00:46,320
and today we're going to talk to you

00:00:44,000 --> 00:00:47,280
about uh running apache geode on

00:00:46,320 --> 00:00:49,680
kubernetes

00:00:47,280 --> 00:00:52,879
or as uh i have called it much to aaron

00:00:49,680 --> 00:00:55,039
chagrin it's kubernetes geotime

00:00:52,879 --> 00:00:57,520
so let's uh let's dive into the outline

00:00:55,039 --> 00:00:59,600
and see what we'll talk about today so

00:00:57,520 --> 00:01:01,039
uh we got a pretty pretty short outlined

00:00:59,600 --> 00:01:02,239
looks here but there's a lot of depth in

00:01:01,039 --> 00:01:02,879
here so we're going to talk a little bit

00:01:02,239 --> 00:01:05,280
about

00:01:02,879 --> 00:01:06,400
motivation uh we're going to talk about

00:01:05,280 --> 00:01:08,799
some of the challenges

00:01:06,400 --> 00:01:11,360
of geode and kubernetes we're going to

00:01:08,799 --> 00:01:13,760
talk about what a kubernetes operator is

00:01:11,360 --> 00:01:14,880
uh then we'll dive into uh what building

00:01:13,760 --> 00:01:19,680
that the solution

00:01:14,880 --> 00:01:22,560
uh looked like and then a summary so

00:01:19,680 --> 00:01:24,080
at vmware we've been working on making

00:01:22,560 --> 00:01:27,200
uh

00:01:24,080 --> 00:01:28,720
geode work on kubernetes um

00:01:27,200 --> 00:01:30,320
and we hope after this talk you'll be

00:01:28,720 --> 00:01:31,360
able to apply some of these lessons if

00:01:30,320 --> 00:01:34,799
you are trying to run

00:01:31,360 --> 00:01:35,200
geodon kubernetes so let's dive into

00:01:34,799 --> 00:01:38,960
some

00:01:35,200 --> 00:01:41,439
motivation here so why geode well geo's

00:01:38,960 --> 00:01:44,399
a low latency high concern seed

00:01:41,439 --> 00:01:46,159
concurrency data management solution uh

00:01:44,399 --> 00:01:47,520
so if you attended any of the wonderful

00:01:46,159 --> 00:01:48,479
talks yesterday you would have heard a

00:01:47,520 --> 00:01:51,759
little bit more about

00:01:48,479 --> 00:01:54,320
um how how geode works itself but it's

00:01:51,759 --> 00:01:57,520
very very good at these fast

00:01:54,320 --> 00:02:00,240
data transactions and so

00:01:57,520 --> 00:02:00,719
why containers why would i consider

00:02:00,240 --> 00:02:04,240
running

00:02:00,719 --> 00:02:06,960
geode in a container well containers

00:02:04,240 --> 00:02:09,200
abstract from the operating system so

00:02:06,960 --> 00:02:09,520
this abstraction means we no longer have

00:02:09,200 --> 00:02:12,480
to

00:02:09,520 --> 00:02:14,560
think about the operating system piece

00:02:12,480 --> 00:02:16,239
on virtualized hardware we just have the

00:02:14,560 --> 00:02:18,319
operating system with our logical

00:02:16,239 --> 00:02:21,200
resources

00:02:18,319 --> 00:02:21,680
we can also decouple the application

00:02:21,200 --> 00:02:24,400
from the

00:02:21,680 --> 00:02:25,680
infrastructure so normally when you

00:02:24,400 --> 00:02:28,319
deploy an application

00:02:25,680 --> 00:02:30,000
um say gu you have to think about what

00:02:28,319 --> 00:02:31,599
the infrastructure is you're running on

00:02:30,000 --> 00:02:32,239
you got to make sure it's all set up and

00:02:31,599 --> 00:02:34,400
all

00:02:32,239 --> 00:02:36,000
has all the required pieces that you

00:02:34,400 --> 00:02:39,040
need in geotechnic

00:02:36,000 --> 00:02:41,360
java and so in

00:02:39,040 --> 00:02:42,720
containers we can just say oh i'm going

00:02:41,360 --> 00:02:46,480
to make sure my container

00:02:42,720 --> 00:02:48,000
has all the pieces i need to run geode

00:02:46,480 --> 00:02:50,000
and then i'm just going to look for some

00:02:48,000 --> 00:02:52,000
infrastructure that can run containers

00:02:50,000 --> 00:02:55,360
i don't really care what it is i just it

00:02:52,000 --> 00:02:58,400
just needs to say i can run containers

00:02:55,360 --> 00:03:00,080
and then consistent deploys so uh the

00:02:58,400 --> 00:03:02,159
neat thing about being in container

00:03:00,080 --> 00:03:03,920
is that it has all the pieces we need to

00:03:02,159 --> 00:03:07,200
run and knows exactly how to run

00:03:03,920 --> 00:03:10,640
it and so when i uh needs

00:03:07,200 --> 00:03:12,159
a new deployment i already have all the

00:03:10,640 --> 00:03:13,840
pieces needed to

00:03:12,159 --> 00:03:16,400
run it the same way as i did the first

00:03:13,840 --> 00:03:19,519
time i ran it it's all in the container

00:03:16,400 --> 00:03:21,120
and so uh i don't need to configure the

00:03:19,519 --> 00:03:23,519
infrastructure i just say here's my

00:03:21,120 --> 00:03:25,760
container please run it

00:03:23,519 --> 00:03:27,840
so yeah that sounds really cool and

00:03:25,760 --> 00:03:30,879
awesome and all that so

00:03:27,840 --> 00:03:32,879
why kubernetes well um

00:03:30,879 --> 00:03:35,040
kubernetes is a tool that allows for

00:03:32,879 --> 00:03:38,239
those easy configuration deployment and

00:03:35,040 --> 00:03:41,200
management and scaling events and so

00:03:38,239 --> 00:03:41,840
um you have your container but you got

00:03:41,200 --> 00:03:43,680
to still

00:03:41,840 --> 00:03:46,159
do things with your container such as

00:03:43,680 --> 00:03:47,599
like i want multiple copies of this

00:03:46,159 --> 00:03:49,599
container running so

00:03:47,599 --> 00:03:51,120
in the case of a geode server we

00:03:49,599 --> 00:03:54,799
probably want more than one

00:03:51,120 --> 00:03:58,400
server and so this

00:03:54,799 --> 00:04:00,640
uh this tool kubernetes will allow us to

00:03:58,400 --> 00:04:02,080
just deploy easily more containers in

00:04:00,640 --> 00:04:05,680
the same way instead of us having to

00:04:02,080 --> 00:04:05,680
manually deploy containers

00:04:06,159 --> 00:04:10,159
service discovery and load balancing is

00:04:08,159 --> 00:04:12,480
another important part of kubernetes so

00:04:10,159 --> 00:04:15,040
great i got all these containers running

00:04:12,480 --> 00:04:16,720
they need to talk to each other and so

00:04:15,040 --> 00:04:18,320
what kubernetes provides is provides

00:04:16,720 --> 00:04:20,799
both service discovery so

00:04:18,320 --> 00:04:22,639
applications i want to consume geode

00:04:20,799 --> 00:04:24,560
have an easy way to find it

00:04:22,639 --> 00:04:26,880
and load balancing so there's an easy

00:04:24,560 --> 00:04:29,280
way to route to the multiple copies of

00:04:26,880 --> 00:04:30,800
your container that you have

00:04:29,280 --> 00:04:35,040
so you basically don't have to do all

00:04:30,800 --> 00:04:37,360
the networking yourself

00:04:35,040 --> 00:04:39,600
recovery is another important piece for

00:04:37,360 --> 00:04:41,199
for kubernetes this is the ability to

00:04:39,600 --> 00:04:42,800
restart containers so

00:04:41,199 --> 00:04:45,759
if something bad happens if the

00:04:42,800 --> 00:04:48,080
container starts to crash

00:04:45,759 --> 00:04:49,680
kubernetes will recognize that and go

00:04:48,080 --> 00:04:51,759
replace

00:04:49,680 --> 00:04:52,800
the container and more importantly won't

00:04:51,759 --> 00:04:56,080
tell

00:04:52,800 --> 00:04:57,199
other pieces of your system that the

00:04:56,080 --> 00:05:01,039
container is

00:04:57,199 --> 00:05:03,919
ready until it truly is ready

00:05:01,039 --> 00:05:05,520
um and lastly running geode in your

00:05:03,919 --> 00:05:07,199
applications so

00:05:05,520 --> 00:05:08,720
uh we've been hearing more and more from

00:05:07,199 --> 00:05:10,560
customers about how they're either

00:05:08,720 --> 00:05:12,639
currently moving to kubernetes or

00:05:10,560 --> 00:05:15,280
they're looking to move to kubernetes

00:05:12,639 --> 00:05:16,240
and they would like to have geode

00:05:15,280 --> 00:05:20,080
available

00:05:16,240 --> 00:05:20,720
near those uh those applications that

00:05:20,080 --> 00:05:23,840
they run

00:05:20,720 --> 00:05:25,680
just uh it makes it easier to have folks

00:05:23,840 --> 00:05:27,680
communicate with their geod

00:05:25,680 --> 00:05:29,520
cluster if they're both on kubernetes

00:05:27,680 --> 00:05:32,880
for them

00:05:29,520 --> 00:05:34,000
so with that motivation in mind um

00:05:32,880 --> 00:05:36,080
why don't we dive into some of the

00:05:34,000 --> 00:05:38,479
challenges aaron

00:05:36,080 --> 00:05:40,000
yeah thanks michael um so at this point

00:05:38,479 --> 00:05:41,120
you might be wondering well that sounds

00:05:40,000 --> 00:05:44,720
like a great idea

00:05:41,120 --> 00:05:46,479
geod on kubernetes so can't we just

00:05:44,720 --> 00:05:49,120
take geode put it in some docker

00:05:46,479 --> 00:05:52,080
containers and deploy it and be done

00:05:49,120 --> 00:05:53,199
um well it turns out it's not quite that

00:05:52,080 --> 00:05:55,360
straightforward

00:05:53,199 --> 00:05:56,880
so i'm going to discuss some of the

00:05:55,360 --> 00:06:00,160
challenges

00:05:56,880 --> 00:06:02,240
of running geode on kubernetes

00:06:00,160 --> 00:06:04,479
so the first thing that i have listed

00:06:02,240 --> 00:06:05,600
here is operating a geo cluster requires

00:06:04,479 --> 00:06:08,800
specialized

00:06:05,600 --> 00:06:11,440
knowledge to explain this

00:06:08,800 --> 00:06:12,080
i want to give a quick example so

00:06:11,440 --> 00:06:15,039
suppose

00:06:12,080 --> 00:06:15,759
that i am in charge of operating a

00:06:15,039 --> 00:06:18,720
production

00:06:15,759 --> 00:06:21,360
geode cluster and i want to upgrade from

00:06:18,720 --> 00:06:23,280
one geode version to the next

00:06:21,360 --> 00:06:24,639
there are quite a few steps i need to go

00:06:23,280 --> 00:06:27,759
through to make sure that this

00:06:24,639 --> 00:06:30,080
is successful so first of all

00:06:27,759 --> 00:06:32,160
i would need to know that there are two

00:06:30,080 --> 00:06:35,120
different kinds of processes in geode

00:06:32,160 --> 00:06:36,080
some are locators some are servers and i

00:06:35,120 --> 00:06:38,080
would need to know that

00:06:36,080 --> 00:06:40,000
i need to upgrade the locators before

00:06:38,080 --> 00:06:42,479
the servers

00:06:40,000 --> 00:06:44,720
another thing is that i would need to

00:06:42,479 --> 00:06:46,400
make sure that before i upgraded any of

00:06:44,720 --> 00:06:48,240
the servers

00:06:46,400 --> 00:06:50,479
i need to make sure that in memory data

00:06:48,240 --> 00:06:52,319
on that server is replicated to at least

00:06:50,479 --> 00:06:54,000
one other server otherwise i would lose

00:06:52,319 --> 00:06:55,919
that data

00:06:54,000 --> 00:06:57,599
so these are some of these examples of

00:06:55,919 --> 00:07:00,080
specialized knowledge that a

00:06:57,599 --> 00:07:01,759
human operator would need to know in

00:07:00,080 --> 00:07:03,360
order to operate a geo cluster

00:07:01,759 --> 00:07:05,360
and this might sound obvious but

00:07:03,360 --> 00:07:07,759
kubernetes doesn't know

00:07:05,360 --> 00:07:09,440
any of that stuff like kubernetes by

00:07:07,759 --> 00:07:11,360
default doesn't know about geode it

00:07:09,440 --> 00:07:14,639
doesn't know about geodes special

00:07:11,360 --> 00:07:16,639
requirements so we kind of need a way to

00:07:14,639 --> 00:07:19,440
encode some of that application specific

00:07:16,639 --> 00:07:21,440
knowledge into kubernetes

00:07:19,440 --> 00:07:23,440
the second thing listed here is geode

00:07:21,440 --> 00:07:25,360
was not primarily designed to run

00:07:23,440 --> 00:07:27,039
in a cloud environment now this

00:07:25,360 --> 00:07:28,240
certainly doesn't mean you can't run it

00:07:27,039 --> 00:07:30,479
in a cloud environment

00:07:28,240 --> 00:07:32,880
but when you look at where geode is

00:07:30,479 --> 00:07:35,440
traditionally deployed

00:07:32,880 --> 00:07:36,319
you notice that there's usually more of

00:07:35,440 --> 00:07:39,440
a static

00:07:36,319 --> 00:07:43,440
network so um

00:07:39,440 --> 00:07:44,080
by contrast in kubernetes ip addresses

00:07:43,440 --> 00:07:46,080
um

00:07:44,080 --> 00:07:47,919
don't get assigned to the pause until

00:07:46,080 --> 00:07:49,919
they start and

00:07:47,919 --> 00:07:51,919
um each each geode member when it

00:07:49,919 --> 00:07:54,319
restarts it'll receive a new ip address

00:07:51,919 --> 00:07:57,280
so this kind of dynamic network

00:07:54,319 --> 00:07:58,479
is kind of a new environment for geode

00:07:57,280 --> 00:08:00,479
and we'll see that

00:07:58,479 --> 00:08:02,319
we do run into some challenges when we

00:08:00,479 --> 00:08:04,960
try to run geode

00:08:02,319 --> 00:08:05,599
there so the last thing here is running

00:08:04,960 --> 00:08:08,080
stateful

00:08:05,599 --> 00:08:09,599
applications on kubernetes requires

00:08:08,080 --> 00:08:12,160
great care

00:08:09,599 --> 00:08:13,680
what i mean by this is usually with

00:08:12,160 --> 00:08:15,680
stateful applications

00:08:13,680 --> 00:08:18,240
we have some data and we need to make

00:08:15,680 --> 00:08:20,400
sure we don't lose that data

00:08:18,240 --> 00:08:22,479
oftentimes stateful apps have some sort

00:08:20,400 --> 00:08:24,000
of consensus mechanism

00:08:22,479 --> 00:08:26,080
and you don't want to get in a state

00:08:24,000 --> 00:08:27,759
where you have a network partition

00:08:26,080 --> 00:08:29,520
and you have the split brain scenario

00:08:27,759 --> 00:08:31,360
where two

00:08:29,520 --> 00:08:32,800
two or more halves of the system are

00:08:31,360 --> 00:08:34,080
operating independently

00:08:32,800 --> 00:08:35,919
so you need to make sure that that

00:08:34,080 --> 00:08:37,360
doesn't happen and so that will

00:08:35,919 --> 00:08:40,080
be another challenge that we have to

00:08:37,360 --> 00:08:42,719
deal with

00:08:40,080 --> 00:08:44,240
so with those challenges in mind um

00:08:42,719 --> 00:08:46,560
let's

00:08:44,240 --> 00:08:48,640
talk about the idea of a kubernetes

00:08:46,560 --> 00:08:52,160
operator

00:08:48,640 --> 00:08:54,800
uh kubernetes operator is a pattern

00:08:52,160 --> 00:08:57,920
for extending kubernetes to manage

00:08:54,800 --> 00:09:00,000
complex stateful applications

00:08:57,920 --> 00:09:01,200
an operator is usually composed of two

00:09:00,000 --> 00:09:02,959
things so

00:09:01,200 --> 00:09:04,480
first of all you have a set of custom

00:09:02,959 --> 00:09:08,480
resource definitions

00:09:04,480 --> 00:09:10,720
or crds and these are custom kubernetes

00:09:08,480 --> 00:09:13,200
types that represent some sort of domain

00:09:10,720 --> 00:09:15,920
specific schema

00:09:13,200 --> 00:09:17,760
so for example you might have a crd

00:09:15,920 --> 00:09:20,800
called a geo cluster

00:09:17,760 --> 00:09:23,839
and the schema would include things like

00:09:20,800 --> 00:09:28,000
the number of locators number of servers

00:09:23,839 --> 00:09:31,360
etc the second thing in the operator

00:09:28,000 --> 00:09:33,920
is a custom controller this is the part

00:09:31,360 --> 00:09:36,240
that encodes that application specific

00:09:33,920 --> 00:09:38,560
knowledge we talked about earlier

00:09:36,240 --> 00:09:40,240
and implements what's known as desired

00:09:38,560 --> 00:09:42,160
state management so desired state

00:09:40,240 --> 00:09:44,240
management is kind of this idea at the

00:09:42,160 --> 00:09:48,800
core of kubernetes

00:09:44,240 --> 00:09:50,560
where i as a user i declare some desired

00:09:48,800 --> 00:09:52,399
state that i want the system to be in

00:09:50,560 --> 00:09:53,360
and it's up to kubernetes to make it

00:09:52,399 --> 00:09:56,000
happen

00:09:53,360 --> 00:09:57,279
and the controller is the component that

00:09:56,000 --> 00:09:59,760
is going to

00:09:57,279 --> 00:10:02,399
actually implement this desired state

00:09:59,760 --> 00:10:02,399
management

00:10:03,120 --> 00:10:06,320
the next thing i want to do is just walk

00:10:05,200 --> 00:10:08,800
through a

00:10:06,320 --> 00:10:10,560
quick example of how we can use a

00:10:08,800 --> 00:10:14,079
kubernetes operator

00:10:10,560 --> 00:10:14,880
to deploy a geo cluster on the screen

00:10:14,079 --> 00:10:18,480
here i have

00:10:14,880 --> 00:10:21,200
a kubernetes cluster with three

00:10:18,480 --> 00:10:24,880
worker nodes on the right and a set of

00:10:21,200 --> 00:10:28,240
control plane nodes in the center

00:10:24,880 --> 00:10:29,920
the first step is to deploy the operator

00:10:28,240 --> 00:10:33,120
itself

00:10:29,920 --> 00:10:35,200
um so we do that using a yaml file and

00:10:33,120 --> 00:10:36,160
uh the eml file has a lot of stuff in it

00:10:35,200 --> 00:10:39,200
but i just

00:10:36,160 --> 00:10:43,040
i'm focusing on two parts here so

00:10:39,200 --> 00:10:43,040
uh the first part is a deployment

00:10:43,200 --> 00:10:50,000
a deployment in kubernetes is just a set

00:10:46,640 --> 00:10:52,399
of one or more identical pods

00:10:50,000 --> 00:10:53,040
and a pod in kubernetes is kind of like

00:10:52,399 --> 00:10:57,200
a set

00:10:53,040 --> 00:11:00,480
of containers or processes in this case

00:10:57,200 --> 00:11:01,920
for this example pod is just a single

00:11:00,480 --> 00:11:05,120
geode process

00:11:01,920 --> 00:11:07,519
so um

00:11:05,120 --> 00:11:08,240
for this deployment it is for the

00:11:07,519 --> 00:11:10,800
controller

00:11:08,240 --> 00:11:11,920
and we see that it has one replica so

00:11:10,800 --> 00:11:15,360
i'm just deploying one

00:11:11,920 --> 00:11:17,040
controller pod the second thing in the

00:11:15,360 --> 00:11:18,800
cml file is the custom resource

00:11:17,040 --> 00:11:20,640
definition that we talked about earlier

00:11:18,800 --> 00:11:24,160
so this is defining

00:11:20,640 --> 00:11:24,640
a kind in kubernetes called geode

00:11:24,160 --> 00:11:29,200
cluster

00:11:24,640 --> 00:11:32,800
and kind is like a custom type

00:11:29,200 --> 00:11:34,959
so we give this to the kubernetes api

00:11:32,800 --> 00:11:38,640
and the control plane will spin up the

00:11:34,959 --> 00:11:40,720
locator pod that we asked for

00:11:38,640 --> 00:11:42,640
the first thing the controller does when

00:11:40,720 --> 00:11:44,000
it starts is it creates what's called a

00:11:42,640 --> 00:11:46,959
watch

00:11:44,000 --> 00:11:49,120
on geo cluster so a watch is kind of

00:11:46,959 --> 00:11:51,920
like a subscription for events

00:11:49,120 --> 00:11:54,480
so this is saying hey kubernetes api

00:11:51,920 --> 00:11:58,480
tell me anytime somebody creates

00:11:54,480 --> 00:11:58,480
updates or deletes a geo cluster

00:11:58,959 --> 00:12:03,120
great so now the operator is fully

00:12:01,040 --> 00:12:06,800
deployed so the next step

00:12:03,120 --> 00:12:08,880
is to deploy our geo cluster

00:12:06,800 --> 00:12:10,240
so that you see at the bottom here is an

00:12:08,880 --> 00:12:13,440
example of

00:12:10,240 --> 00:12:16,560
defining a geo cluster in yaml

00:12:13,440 --> 00:12:18,320
and this uh

00:12:16,560 --> 00:12:19,600
just right now it just has a few fields

00:12:18,320 --> 00:12:21,920
so we're saying

00:12:19,600 --> 00:12:25,839
we're asking for three locator replicas

00:12:21,920 --> 00:12:25,839
and three server replicas

00:12:26,320 --> 00:12:29,440
so we give that to the kubernetes api

00:12:28,320 --> 00:12:31,120
and now the api

00:12:29,440 --> 00:12:33,120
understands what a geo cluster is

00:12:31,120 --> 00:12:36,959
because we've defined that type

00:12:33,120 --> 00:12:37,839
and so now the controller receives an

00:12:36,959 --> 00:12:41,120
event

00:12:37,839 --> 00:12:43,600
because we've created a geocluster

00:12:41,120 --> 00:12:45,519
at this point the controller is doing

00:12:43,600 --> 00:12:47,200
its desired state management

00:12:45,519 --> 00:12:48,720
where it's going to look at the current

00:12:47,200 --> 00:12:50,880
state of the system

00:12:48,720 --> 00:12:52,880
and notice that there are no locators or

00:12:50,880 --> 00:12:55,200
servers but we asked for three locators

00:12:52,880 --> 00:12:57,760
and three servers

00:12:55,200 --> 00:12:58,560
so the controller the first thing it

00:12:57,760 --> 00:13:01,600
does

00:12:58,560 --> 00:13:04,160
is it is going to create

00:13:01,600 --> 00:13:05,920
a service and staple set for the

00:13:04,160 --> 00:13:08,240
locators

00:13:05,920 --> 00:13:09,760
just a few more terms in kubernetes so a

00:13:08,240 --> 00:13:11,680
staple set

00:13:09,760 --> 00:13:13,360
is really similar to a deployment in

00:13:11,680 --> 00:13:16,000
that it

00:13:13,360 --> 00:13:17,040
contains one or more pods but unlike a

00:13:16,000 --> 00:13:19,600
deployment the

00:13:17,040 --> 00:13:20,240
pods can have a unique ordering and

00:13:19,600 --> 00:13:22,720
identity

00:13:20,240 --> 00:13:24,240
so it's really useful for stateful

00:13:22,720 --> 00:13:27,360
applications like the name

00:13:24,240 --> 00:13:30,000
indicates and a service is the

00:13:27,360 --> 00:13:31,839
just the component that allows all of

00:13:30,000 --> 00:13:35,279
these things to talk to each other

00:13:31,839 --> 00:13:40,000
and for apps to talk to the

00:13:35,279 --> 00:13:41,920
pods so the controller asks the api to

00:13:40,000 --> 00:13:44,000
create those things and then the control

00:13:41,920 --> 00:13:47,040
plane will spin up the

00:13:44,000 --> 00:13:49,519
the locator pods that it asks for

00:13:47,040 --> 00:13:51,519
and the last part is just the same thing

00:13:49,519 --> 00:13:53,199
but for the geode servers so the

00:13:51,519 --> 00:13:56,160
controller asks for a

00:13:53,199 --> 00:13:56,959
safely set and service for the servers

00:13:56,160 --> 00:14:00,959
and

00:13:56,959 --> 00:14:03,279
the control plane will create those

00:14:00,959 --> 00:14:04,399
so this is the end of this example but

00:14:03,279 --> 00:14:06,160
um

00:14:04,399 --> 00:14:07,440
i just wanted to illustrate that this is

00:14:06,160 --> 00:14:10,880
kind of the high level

00:14:07,440 --> 00:14:14,160
solution that we've taken to deploying

00:14:10,880 --> 00:14:16,320
geode on kubernetes and i hope that you

00:14:14,160 --> 00:14:18,800
will see how this kind of addresses some

00:14:16,320 --> 00:14:21,360
of those challenges we mentioned earlier

00:14:18,800 --> 00:14:24,160
we can encode some of the application

00:14:21,360 --> 00:14:27,199
specific knowledge into the controller

00:14:24,160 --> 00:14:28,959
so that it can do things like deploying

00:14:27,199 --> 00:14:31,440
the locators before the servers

00:14:28,959 --> 00:14:34,160
and make sure we upgrade things in the

00:14:31,440 --> 00:14:34,160
correct order

00:14:34,959 --> 00:14:39,440
so at this point

00:14:39,519 --> 00:14:45,839
we would like to dive into discussing

00:14:42,959 --> 00:14:47,040
our experience building a kubernetes

00:14:45,839 --> 00:14:49,040
operator

00:14:47,040 --> 00:14:51,839
and i guess through through discussing

00:14:49,040 --> 00:14:54,000
this we're hoping that

00:14:51,839 --> 00:14:56,000
everyone will be able to take away some

00:14:54,000 --> 00:14:58,399
of the solutions that we found

00:14:56,000 --> 00:15:00,000
and apply them whether you're building a

00:14:58,399 --> 00:15:02,240
coupe an operator

00:15:00,000 --> 00:15:04,480
or not um just anyone who's trying to

00:15:02,240 --> 00:15:06,560
run geode on kubernetes could hopefully

00:15:04,480 --> 00:15:10,320
benefit from some of these

00:15:06,560 --> 00:15:10,320
uh experiences that we've had

00:15:11,279 --> 00:15:15,600
yeah so let's dive into the first one

00:15:13,279 --> 00:15:15,839
here that's um geode member life cycles

00:15:15,600 --> 00:15:19,279
and

00:15:15,839 --> 00:15:21,360
um preserving data so uh as we manage

00:15:19,279 --> 00:15:22,000
the life cycle of geode we want to make

00:15:21,360 --> 00:15:25,120
sure we

00:15:22,000 --> 00:15:28,639
never lose any data that was put

00:15:25,120 --> 00:15:31,440
into geode and so um the first thing we

00:15:28,639 --> 00:15:34,000
we learned about was the kubernetes

00:15:31,440 --> 00:15:37,279
expected life cycle versus the geode

00:15:34,000 --> 00:15:37,759
uh life cycle and uh it turns out that

00:15:37,279 --> 00:15:40,399
those

00:15:37,759 --> 00:15:41,680
life cycles were different kubernetes

00:15:40,399 --> 00:15:44,720
says hey

00:15:41,680 --> 00:15:45,519
i might reschedule a pod that's running

00:15:44,720 --> 00:15:48,160
at any time

00:15:45,519 --> 00:15:50,880
and so as aaron described before in our

00:15:48,160 --> 00:15:51,680
case a pod is running a server or a

00:15:50,880 --> 00:15:53,759
locator

00:15:51,680 --> 00:15:55,279
and so if it's running a server which it

00:15:53,759 --> 00:15:58,639
has all our data

00:15:55,279 --> 00:15:59,120
um kubernetes could just say oh i need

00:15:58,639 --> 00:16:02,639
to

00:15:59,120 --> 00:16:05,120
move this pod around because um

00:16:02,639 --> 00:16:07,360
the amount of ram i'm using on this

00:16:05,120 --> 00:16:08,880
particular worker node is

00:16:07,360 --> 00:16:11,120
is a little too high and i've got other

00:16:08,880 --> 00:16:14,800
worker nodes i can distribute this to

00:16:11,120 --> 00:16:17,279
so um what came about was

00:16:14,800 --> 00:16:18,079
how how do we prevent this data loss

00:16:17,279 --> 00:16:20,639
from happening

00:16:18,079 --> 00:16:22,160
well uh if you talk to folks familiar

00:16:20,639 --> 00:16:24,240
with geo they would say well i've got

00:16:22,160 --> 00:16:25,360
this thing called restore redundancy and

00:16:24,240 --> 00:16:29,120
that makes sure that

00:16:25,360 --> 00:16:31,360
um the there are extra copies of the

00:16:29,120 --> 00:16:33,839
pieces of data in this is what

00:16:31,360 --> 00:16:35,600
geode uses this idea of redundancy is

00:16:33,839 --> 00:16:38,959
what geode uses to make sure

00:16:35,600 --> 00:16:40,959
that you are highly available and that

00:16:38,959 --> 00:16:44,160
you're not going to lose data when

00:16:40,959 --> 00:16:47,360
it when when a server

00:16:44,160 --> 00:16:49,519
crashes or restarts or something

00:16:47,360 --> 00:16:50,800
so we actually had some fun fun the

00:16:49,519 --> 00:16:52,959
first time we

00:16:50,800 --> 00:16:55,279
got it up and running because we we got

00:16:52,959 --> 00:16:57,199
data loss on our very first try because

00:16:55,279 --> 00:16:59,600
we just didn't call restore redundancy

00:16:57,199 --> 00:17:01,440
between our multiple servers um

00:16:59,600 --> 00:17:04,000
uh aaron can tell you later how excited

00:17:01,440 --> 00:17:07,039
i was about that um

00:17:04,000 --> 00:17:08,559
so we went to go ad restore vanessa we

00:17:07,039 --> 00:17:08,880
were like okay the controller can just

00:17:08,559 --> 00:17:11,520
say

00:17:08,880 --> 00:17:12,640
make sure you call restore redundancy uh

00:17:11,520 --> 00:17:14,640
before

00:17:12,640 --> 00:17:15,679
you shut down but restoring redundancy

00:17:14,640 --> 00:17:18,799
can take some

00:17:15,679 --> 00:17:21,039
time and kubernetes uh does not like

00:17:18,799 --> 00:17:23,439
waiting it's not a very patient

00:17:21,039 --> 00:17:25,839
uh system so um kubernetes expects you

00:17:23,439 --> 00:17:28,559
know the containers to be ephemeral and

00:17:25,839 --> 00:17:29,760
you can kind of restart them but um

00:17:28,559 --> 00:17:33,600
geode's a data store

00:17:29,760 --> 00:17:35,520
it's got to store that data um so

00:17:33,600 --> 00:17:38,160
what what could we do what could we do

00:17:35,520 --> 00:17:40,240
to stop this so

00:17:38,160 --> 00:17:41,280
um this is where we're gonna introduce a

00:17:40,240 --> 00:17:43,120
new concept uh

00:17:41,280 --> 00:17:45,039
in kubernetes which are pre-stop hooks

00:17:43,120 --> 00:17:48,240
and finalizers

00:17:45,039 --> 00:17:48,720
so a pre-stop hook is called immediately

00:17:48,240 --> 00:17:51,280
before

00:17:48,720 --> 00:17:53,520
a container is terminated and it's

00:17:51,280 --> 00:17:55,520
synchronous so it must complete before

00:17:53,520 --> 00:17:58,000
the call to delete the container can be

00:17:55,520 --> 00:17:59,360
sent and so this is important because uh

00:17:58,000 --> 00:18:01,760
this is a method

00:17:59,360 --> 00:18:02,799
for preventing the container that's

00:18:01,760 --> 00:18:05,840
running um

00:18:02,799 --> 00:18:09,760
that kubernetes wants to move

00:18:05,840 --> 00:18:11,600
uh it prevents it from

00:18:09,760 --> 00:18:14,080
it prevents kubernetes from destroying

00:18:11,600 --> 00:18:16,720
that container and then um

00:18:14,080 --> 00:18:18,160
finalizers is the uh it allows a

00:18:16,720 --> 00:18:20,240
controller to implement

00:18:18,160 --> 00:18:22,000
asynchronous free delete hooks so

00:18:20,240 --> 00:18:25,360
they're basically just arbitrary string

00:18:22,000 --> 00:18:28,320
values but when they're present on um

00:18:25,360 --> 00:18:29,760
on our on our pod then they ensure a

00:18:28,320 --> 00:18:30,640
hard delete of the resource is not

00:18:29,760 --> 00:18:32,240
possible so

00:18:30,640 --> 00:18:33,520
that makes this these are the two

00:18:32,240 --> 00:18:34,160
concepts that we're going to use to make

00:18:33,520 --> 00:18:37,720
sure that

00:18:34,160 --> 00:18:40,840
um we couldn't delete

00:18:37,720 --> 00:18:42,320
a pod and reschedule it before it was

00:18:40,840 --> 00:18:45,039
ready

00:18:42,320 --> 00:18:47,919
so what does this mean for geode so in

00:18:45,039 --> 00:18:50,320
geode we have our pre-stop hook ignore

00:18:47,919 --> 00:18:51,600
all sig events until geode finishes

00:18:50,320 --> 00:18:55,440
restoring redundancy

00:18:51,600 --> 00:18:57,280
so an example of this might be we uh

00:18:55,440 --> 00:18:59,039
kubernetes will very politely send

00:18:57,280 --> 00:19:01,120
things like sync term

00:18:59,039 --> 00:19:02,320
and we will just say yeah i'm gonna

00:19:01,120 --> 00:19:04,000
ignore that for now

00:19:02,320 --> 00:19:06,320
i'm still doing my thing i'm still

00:19:04,000 --> 00:19:10,000
trying to restore redundancy

00:19:06,320 --> 00:19:12,080
um so what the pre-stop hook

00:19:10,000 --> 00:19:13,039
instead watches for is it watches for a

00:19:12,080 --> 00:19:14,720
finalizer

00:19:13,039 --> 00:19:16,960
on the server pod to determine the

00:19:14,720 --> 00:19:19,760
redundancy status

00:19:16,960 --> 00:19:22,480
and if it errors we just hang because um

00:19:19,760 --> 00:19:25,760
if we can't restore redundancy

00:19:22,480 --> 00:19:28,240
then um we're probably going to need

00:19:25,760 --> 00:19:29,440
manual intervention from uh from a human

00:19:28,240 --> 00:19:32,320
um

00:19:29,440 --> 00:19:33,679
who's in charge of this geode cluster

00:19:32,320 --> 00:19:35,760
that's running and that can

00:19:33,679 --> 00:19:38,000
allow them to you know save off the data

00:19:35,760 --> 00:19:40,000
somewhere else uh or what not before

00:19:38,000 --> 00:19:42,160
the pod does and so to actually help

00:19:40,000 --> 00:19:44,320
that kubernetes has this other concept

00:19:42,160 --> 00:19:45,679
called uh termination grace period which

00:19:44,320 --> 00:19:48,720
is like um

00:19:45,679 --> 00:19:49,360
hey after this amount of time i'm just

00:19:48,720 --> 00:19:51,679
gonna

00:19:49,360 --> 00:19:53,760
remove everything you you took long

00:19:51,679 --> 00:19:54,720
enough to do this and in our case we set

00:19:53,760 --> 00:19:57,840
it for one year

00:19:54,720 --> 00:20:00,720
we hope that's enough time for folks to

00:19:57,840 --> 00:20:00,720
figure out their hang

00:20:02,960 --> 00:20:09,679
so great let's uh let's see how

00:20:06,640 --> 00:20:13,200
we actually know when to add or remove

00:20:09,679 --> 00:20:16,320
this finalizer so uh

00:20:13,200 --> 00:20:18,640
on server pod creation and

00:20:16,320 --> 00:20:21,120
uh it's not scheduled for deletion then

00:20:18,640 --> 00:20:21,600
we set this finalizer so the finalizer

00:20:21,120 --> 00:20:22,880
as i

00:20:21,600 --> 00:20:25,600
mentioned before is just kind of a

00:20:22,880 --> 00:20:27,240
string value the example in here is we

00:20:25,600 --> 00:20:29,760
just have a finalizer called

00:20:27,240 --> 00:20:32,480
server.finalizer

00:20:29,760 --> 00:20:33,120
this is what the pre-stop hook is

00:20:32,480 --> 00:20:36,159
waiting

00:20:33,120 --> 00:20:39,440
uh to be removed

00:20:36,159 --> 00:20:41,919
and so uh let's dive into some

00:20:39,440 --> 00:20:43,120
diagrams explaining the steps so we

00:20:41,919 --> 00:20:45,919
start off with our pod

00:20:43,120 --> 00:20:46,960
and we say uh i'm going to try to delete

00:20:45,919 --> 00:20:49,039
this pod

00:20:46,960 --> 00:20:50,000
so the first thing we check is we see is

00:20:49,039 --> 00:20:52,720
there a

00:20:50,000 --> 00:20:54,159
running restore redundancy and so in

00:20:52,720 --> 00:20:56,000
this particular case since we

00:20:54,159 --> 00:20:57,679
just scheduled it for deletion and

00:20:56,000 --> 00:20:59,039
there's no other identifier saying

00:20:57,679 --> 00:21:01,760
there's a running

00:20:59,039 --> 00:21:02,960
uh restore redundancy then we're going

00:21:01,760 --> 00:21:06,320
to schedule

00:21:02,960 --> 00:21:09,440
a restore redundancy uh

00:21:06,320 --> 00:21:13,520
command so uh what happens is we

00:21:09,440 --> 00:21:16,880
uh go to use the geo management endpoint

00:21:13,520 --> 00:21:20,559
and we send a request to say hey can you

00:21:16,880 --> 00:21:23,039
start restoring redundancy

00:21:20,559 --> 00:21:23,600
and the return of that output is

00:21:23,039 --> 00:21:28,000
actually an

00:21:23,600 --> 00:21:29,600
id of the restore redundancy request

00:21:28,000 --> 00:21:31,840
since we mentioned before restore

00:21:29,600 --> 00:21:34,880
redundancy can take time

00:21:31,840 --> 00:21:36,559
uh we're going to save off this id to

00:21:34,880 --> 00:21:38,320
check in the future so at the end of all

00:21:36,559 --> 00:21:41,360
this we now have our pod with an

00:21:38,320 --> 00:21:43,440
annotation that has a restore id

00:21:41,360 --> 00:21:45,280
and in this case i i don't know why i

00:21:43,440 --> 00:21:47,600
picked 12 but i picked 12 as our id

00:21:45,280 --> 00:21:49,280
here so our 12 is the id that we got

00:21:47,600 --> 00:21:51,200
back from uh

00:21:49,280 --> 00:21:52,960
um our restore redundancy command so

00:21:51,200 --> 00:21:56,480
great we've now scheduled

00:21:52,960 --> 00:21:59,520
and are currently running uh the restore

00:21:56,480 --> 00:22:02,320
so now we need to uh

00:21:59,520 --> 00:22:02,640
worry about checking hey when am i ready

00:22:02,320 --> 00:22:04,240
to

00:22:02,640 --> 00:22:06,720
actually call it done so now we're in a

00:22:04,240 --> 00:22:08,240
pod it's in deletion state so we're

00:22:06,720 --> 00:22:11,039
waiting for it to be deleted and it has

00:22:08,240 --> 00:22:14,320
an annotation for restore id

00:22:11,039 --> 00:22:16,880
and so the next step is we

00:22:14,320 --> 00:22:18,400
see oh i've got a running uh restore

00:22:16,880 --> 00:22:21,360
redundancy let's go

00:22:18,400 --> 00:22:22,640
check the status of that restore

00:22:21,360 --> 00:22:26,559
redundancy

00:22:22,640 --> 00:22:28,559
and so um if the check were to

00:22:26,559 --> 00:22:29,840
say hey i'm still running then we go

00:22:28,559 --> 00:22:32,799
back to the first pod

00:22:29,840 --> 00:22:34,400
in uh the first uh section here where

00:22:32,799 --> 00:22:36,080
we're like nah nothing's changed i don't

00:22:34,400 --> 00:22:39,440
need to schedule a new one

00:22:36,080 --> 00:22:41,760
i'm just waiting um so we

00:22:39,440 --> 00:22:43,120
we do that for some number of times and

00:22:41,760 --> 00:22:44,320
then eventually check restore

00:22:43,120 --> 00:22:47,600
redundancies will say

00:22:44,320 --> 00:22:51,440
hey restart redundancy is finished and

00:22:47,600 --> 00:22:54,960
at that point in time we now remove

00:22:51,440 --> 00:22:56,960
the finalizer and the restore id and so

00:22:54,960 --> 00:22:57,919
at this point in time the finalizer has

00:22:56,960 --> 00:22:59,840
been removed

00:22:57,919 --> 00:23:01,200
and our pre-stop hook that was waiting

00:22:59,840 --> 00:23:02,799
for this finalizer

00:23:01,200 --> 00:23:04,559
will finally finish and with the

00:23:02,799 --> 00:23:08,640
pre-stop hook finished

00:23:04,559 --> 00:23:11,039
we will actually run through the regular

00:23:08,640 --> 00:23:13,039
delete cycle of removing the pod in

00:23:11,039 --> 00:23:15,440
kubernetes and so this is how

00:23:13,039 --> 00:23:16,159
we determine the efficient way to

00:23:15,440 --> 00:23:22,240
preserve

00:23:16,159 --> 00:23:25,440
data during server shutdowns

00:23:22,240 --> 00:23:28,880
great thanks michael um so

00:23:25,440 --> 00:23:30,720
uh kind of like we alluded to earlier uh

00:23:28,880 --> 00:23:32,159
some ways that the kubernetes network is

00:23:30,720 --> 00:23:33,840
different from a traditional network

00:23:32,159 --> 00:23:37,039
where you might deploy geode

00:23:33,840 --> 00:23:38,000
is uh members don't receive the same ip

00:23:37,039 --> 00:23:41,200
address

00:23:38,000 --> 00:23:43,360
after they restart so this was initially

00:23:41,200 --> 00:23:45,760
a problem because some of the geode

00:23:43,360 --> 00:23:48,000
clients uh were kind of aggressively

00:23:45,760 --> 00:23:51,039
caching locator ip addresses

00:23:48,000 --> 00:23:53,039
but there was um this issue was actually

00:23:51,039 --> 00:23:53,440
fixed by some other geo committers that

00:23:53,039 --> 00:23:56,080
weren't

00:23:53,440 --> 00:23:56,720
directly involved with our work and so

00:23:56,080 --> 00:23:58,640
that's

00:23:56,720 --> 00:24:01,120
really great i'm i just want to give a

00:23:58,640 --> 00:24:04,320
shout out to the open source community

00:24:01,120 --> 00:24:06,640
for working together on that um

00:24:04,320 --> 00:24:07,360
another thing is that the dns cache may

00:24:06,640 --> 00:24:09,919
be stale

00:24:07,360 --> 00:24:11,279
so this is something that you'll have to

00:24:09,919 --> 00:24:13,440
deal with

00:24:11,279 --> 00:24:15,039
a tip here is to use the locator wait

00:24:13,440 --> 00:24:18,880
time

00:24:15,039 --> 00:24:20,320
property on geode to tolerate the dns

00:24:18,880 --> 00:24:22,720
delay

00:24:20,320 --> 00:24:25,200
last thing here is that sometimes the

00:24:22,720 --> 00:24:26,240
dns isn't resolvable until pods are

00:24:25,200 --> 00:24:29,120
ready

00:24:26,240 --> 00:24:29,840
and ready in kubernetes just kind of

00:24:29,120 --> 00:24:32,880
means like

00:24:29,840 --> 00:24:34,320
it's the pod is um up and ready to

00:24:32,880 --> 00:24:36,880
receive connections

00:24:34,320 --> 00:24:38,799
so in geode we define that as the geode

00:24:36,880 --> 00:24:41,039
member is online it's joined the

00:24:38,799 --> 00:24:41,919
distributed system and it's now ready to

00:24:41,039 --> 00:24:46,240
accept

00:24:41,919 --> 00:24:49,279
client connections so um

00:24:46,240 --> 00:24:50,559
a tip here is to use the publish not

00:24:49,279 --> 00:24:52,559
ready addresses

00:24:50,559 --> 00:24:55,600
field on the staple sets and set that to

00:24:52,559 --> 00:24:58,400
true so that you can actually

00:24:55,600 --> 00:24:59,679
get back the ip addresses for the pods

00:24:58,400 --> 00:25:02,559
from the service

00:24:59,679 --> 00:25:04,240
before they're actually ready and then

00:25:02,559 --> 00:25:06,840
be careful with probes

00:25:04,240 --> 00:25:10,720
and just know how those affect dns

00:25:06,840 --> 00:25:10,720
resolution for services

00:25:11,600 --> 00:25:14,880
let's talk a little bit about

00:25:13,039 --> 00:25:17,840
containerizing geode

00:25:14,880 --> 00:25:18,960
so the first step here is make sure the

00:25:17,840 --> 00:25:21,679
geode process

00:25:18,960 --> 00:25:23,919
is process one in the container this is

00:25:21,679 --> 00:25:26,640
really important because process one

00:25:23,919 --> 00:25:28,320
receives signals from kubernetes which

00:25:26,640 --> 00:25:29,520
is important in geode for graceful

00:25:28,320 --> 00:25:32,159
shutdown

00:25:29,520 --> 00:25:35,360
also process one standard out is

00:25:32,159 --> 00:25:37,039
captured as logs and kubernetes

00:25:35,360 --> 00:25:38,400
so the second point here is kind of

00:25:37,039 --> 00:25:40,559
related to that but

00:25:38,400 --> 00:25:42,080
use locator launcher and server launcher

00:25:40,559 --> 00:25:44,480
instead of gfish to start

00:25:42,080 --> 00:25:45,279
members and that gfish in case you're

00:25:44,480 --> 00:25:47,520
not familiar

00:25:45,279 --> 00:25:49,120
is the command line interface for geod

00:25:47,520 --> 00:25:52,320
and it's normally how you would

00:25:49,120 --> 00:25:55,919
probably start members but

00:25:52,320 --> 00:25:57,760
gfish creates new processes which won't

00:25:55,919 --> 00:25:59,520
be process one in the container

00:25:57,760 --> 00:26:01,279
so we found that using the locator

00:25:59,520 --> 00:26:04,320
launcher and server launcher

00:26:01,279 --> 00:26:08,080
classes are

00:26:04,320 --> 00:26:10,000
achieve what we need the last thing here

00:26:08,080 --> 00:26:12,080
is use a minimal base image that's

00:26:10,000 --> 00:26:13,600
kind of just a good practice in general

00:26:12,080 --> 00:26:16,480
but it reduces this

00:26:13,600 --> 00:26:17,120
risk the security risk and it's faster

00:26:16,480 --> 00:26:20,799
since

00:26:17,120 --> 00:26:20,799
the container is going to be smaller

00:26:21,679 --> 00:26:27,120
a couple of notes on configuring geode

00:26:24,960 --> 00:26:28,159
so there are a lot of ways to configure

00:26:27,120 --> 00:26:30,559
geode

00:26:28,159 --> 00:26:34,159
one way is gfish like we just mentioned

00:26:30,559 --> 00:26:37,600
there's also this thing called cachexml

00:26:34,159 --> 00:26:40,720
we have rest apis and jmx

00:26:37,600 --> 00:26:42,400
so of all these different methods we

00:26:40,720 --> 00:26:45,200
found the best method

00:26:42,400 --> 00:26:47,760
for us was to use the swagger based

00:26:45,200 --> 00:26:50,240
management rest api

00:26:47,760 --> 00:26:51,200
and this is because our controller is

00:26:50,240 --> 00:26:53,600
what's doing

00:26:51,200 --> 00:26:55,679
is what's actually configuring geode and

00:26:53,600 --> 00:26:59,440
the controller is a go application

00:26:55,679 --> 00:27:01,679
and so with this tool called swagger

00:26:59,440 --> 00:27:05,679
cogen we're able to easily generate

00:27:01,679 --> 00:27:07,760
a go client and that gives us a strongly

00:27:05,679 --> 00:27:10,000
typed client so if something breaks in

00:27:07,760 --> 00:27:12,240
the api we can detect that at compile

00:27:10,000 --> 00:27:13,840
time instead of runtime and

00:27:12,240 --> 00:27:15,919
the client also handles all the

00:27:13,840 --> 00:27:17,120
serialization for us so we don't have to

00:27:15,919 --> 00:27:20,480
do things like

00:27:17,120 --> 00:27:23,279
parsing gfish output ourselves

00:27:20,480 --> 00:27:23,279
which is nice

00:27:24,960 --> 00:27:29,120
let's talk a little bit about data

00:27:26,640 --> 00:27:31,840
safety and performance

00:27:29,120 --> 00:27:33,760
the main point here is to avoid

00:27:31,840 --> 00:27:34,320
co-locating cache servers on the same

00:27:33,760 --> 00:27:36,960
machine

00:27:34,320 --> 00:27:39,120
this is because in geode redundancy is

00:27:36,960 --> 00:27:40,880
achieved by replicating data to multiple

00:27:39,120 --> 00:27:42,480
cache servers so if you have

00:27:40,880 --> 00:27:44,480
cache servers on the same machine you

00:27:42,480 --> 00:27:46,320
have a greater chance of losing data if

00:27:44,480 --> 00:27:48,799
that machine fails

00:27:46,320 --> 00:27:51,039
also geode is multi-threaded and it's

00:27:48,799 --> 00:27:53,919
designed to take advantage of all of

00:27:51,039 --> 00:27:55,679
a machine's cpu resources so running

00:27:53,919 --> 00:27:58,799
multiple servers on the same machine can

00:27:55,679 --> 00:28:01,279
cause cpu resource contention

00:27:58,799 --> 00:28:02,000
the tip here that we found is to use

00:28:01,279 --> 00:28:04,399
kubernetes

00:28:02,000 --> 00:28:06,559
affinity and anti-affinity policies to

00:28:04,399 --> 00:28:07,600
avoid co-locating the cache servers on

00:28:06,559 --> 00:28:10,000
the same

00:28:07,600 --> 00:28:10,000
machine

00:28:11,039 --> 00:28:16,559
okay moving on to persistence and

00:28:14,840 --> 00:28:19,039
serialization

00:28:16,559 --> 00:28:19,919
um so one of the tips that we found is

00:28:19,039 --> 00:28:22,720
that we need to

00:28:19,919 --> 00:28:24,480
start the geode members in parallel so

00:28:22,720 --> 00:28:27,600
that geode can determine the correct

00:28:24,480 --> 00:28:29,840
order to recover persistent regions

00:28:27,600 --> 00:28:30,880
we we ran into an issue where we were

00:28:29,840 --> 00:28:34,320
just using the

00:28:30,880 --> 00:28:36,240
the default ordered ready policy that

00:28:34,320 --> 00:28:39,039
the staple set gives us

00:28:36,240 --> 00:28:40,240
and um yeah we ran into a problem

00:28:39,039 --> 00:28:43,200
recovering from

00:28:40,240 --> 00:28:45,679
uh persistent regions because kubernetes

00:28:43,200 --> 00:28:47,279
wants to start the members one by one

00:28:45,679 --> 00:28:49,279
but we actually need to start them in

00:28:47,279 --> 00:28:51,039
parallel so that they can

00:28:49,279 --> 00:28:53,440
determine the correct order to recover

00:28:51,039 --> 00:28:55,200
the data so the tip here is to use the

00:28:53,440 --> 00:28:56,720
parallel pod management policy which is

00:28:55,200 --> 00:29:00,399
not the default

00:28:56,720 --> 00:29:03,360
in kubernetes and then one more

00:29:00,399 --> 00:29:04,960
tip here is about pdx serialization so

00:29:03,360 --> 00:29:07,760
ngo pdx

00:29:04,960 --> 00:29:08,880
is a serialization format that is

00:29:07,760 --> 00:29:12,880
commonly used

00:29:08,880 --> 00:29:14,720
and if we are using pdx it should be

00:29:12,880 --> 00:29:18,640
configured to use a non-default disk

00:29:14,720 --> 00:29:20,720
store if metadata persistence is enabled

00:29:18,640 --> 00:29:21,760
so the team that i was working with

00:29:20,720 --> 00:29:24,399
developed

00:29:21,760 --> 00:29:25,039
some apis that allow creating disk

00:29:24,399 --> 00:29:27,279
stores

00:29:25,039 --> 00:29:30,240
and configuring pdx before starting

00:29:27,279 --> 00:29:30,240
geode servers so

00:29:30,320 --> 00:29:35,360
you can take advantage of those apis if

00:29:32,159 --> 00:29:35,360
you're doing something similar

00:29:35,840 --> 00:29:40,559
so a couple more things here a quick

00:29:38,159 --> 00:29:42,159
note about rolling upgrades

00:29:40,559 --> 00:29:45,520
kubernetes actually makes this really

00:29:42,159 --> 00:29:47,360
easy the built-in stateful sets

00:29:45,520 --> 00:29:49,840
perform rolling upgrades in a manner

00:29:47,360 --> 00:29:51,039
compatible with geode based on a change

00:29:49,840 --> 00:29:54,640
the container image

00:29:51,039 --> 00:29:58,480
so doing a rolling upgrade once you have

00:29:54,640 --> 00:30:00,960
everything in place is basically just

00:29:58,480 --> 00:30:03,039
editing a yaml file and changing the

00:30:00,960 --> 00:30:04,640
container image in that yaml file and

00:30:03,039 --> 00:30:08,240
kubernetes handles

00:30:04,640 --> 00:30:10,480
the process of going through

00:30:08,240 --> 00:30:12,000
and re and upgrading the members one by

00:30:10,480 --> 00:30:13,679
one starting with the locators and

00:30:12,000 --> 00:30:15,200
making sure we don't lose data

00:30:13,679 --> 00:30:18,640
and that's kind of the advantage that we

00:30:15,200 --> 00:30:20,880
get from using the kubernetes operator

00:30:18,640 --> 00:30:22,640
so just some tips here like i said

00:30:20,880 --> 00:30:24,000
already make sure the locators finish

00:30:22,640 --> 00:30:27,360
upgrading before

00:30:24,000 --> 00:30:29,919
starting upgrading the servers and use

00:30:27,360 --> 00:30:31,279
probes to ensure each member becomes

00:30:29,919 --> 00:30:36,559
online before

00:30:31,279 --> 00:30:40,240
upgrading the next member

00:30:36,559 --> 00:30:42,399
um observability so prometheus is

00:30:40,240 --> 00:30:44,399
becoming a really widely used format

00:30:42,399 --> 00:30:45,440
for publishing metrics in cloud native

00:30:44,399 --> 00:30:47,760
environments and

00:30:45,440 --> 00:30:48,880
geode can expose prometheus scrapable

00:30:47,760 --> 00:30:51,760
endpoints

00:30:48,880 --> 00:30:52,320
using a library called micrometer this

00:30:51,760 --> 00:30:54,480
allows

00:30:52,320 --> 00:30:55,679
easy integration with a variety of tools

00:30:54,480 --> 00:30:58,240
like

00:30:55,679 --> 00:30:59,919
grafana and wavefront so the tip here is

00:30:58,240 --> 00:31:02,399
just to utilize those

00:30:59,919 --> 00:31:05,600
prometheus endpoints and in order to

00:31:02,399 --> 00:31:09,840
integrate with observability tools

00:31:05,600 --> 00:31:13,120
the last note about our experience is

00:31:09,840 --> 00:31:15,120
about development and testing so

00:31:13,120 --> 00:31:16,320
i just wanted to describe our testing

00:31:15,120 --> 00:31:18,480
pyramid so

00:31:16,320 --> 00:31:20,159
kind of at the bottom level we have unit

00:31:18,480 --> 00:31:22,080
tests which are testing

00:31:20,159 --> 00:31:24,880
individual functions and packages in the

00:31:22,080 --> 00:31:29,919
code for these we're using the

00:31:24,880 --> 00:31:32,159
ginkgo and gomega libraries for go

00:31:29,919 --> 00:31:33,360
the next level is the integration tests

00:31:32,159 --> 00:31:35,120
which are testing how

00:31:33,360 --> 00:31:37,519
components interact with each other but

00:31:35,120 --> 00:31:40,159
not necessarily the whole system

00:31:37,519 --> 00:31:40,960
so in this in this level we're utilizing

00:31:40,159 --> 00:31:44,480
the

00:31:40,960 --> 00:31:47,200
env test package which is part of the

00:31:44,480 --> 00:31:49,120
controller runtime library in kubernetes

00:31:47,200 --> 00:31:50,799
and what th what's really nice about

00:31:49,120 --> 00:31:52,480
this is that it gives us a partial

00:31:50,799 --> 00:31:56,240
kubernetes cluster

00:31:52,480 --> 00:31:58,840
with just xcd and the api server

00:31:56,240 --> 00:32:00,960
which is a really nice way to test our

00:31:58,840 --> 00:32:02,720
controller

00:32:00,960 --> 00:32:04,880
and then the last thing here is system

00:32:02,720 --> 00:32:07,519
test so these are testing the system

00:32:04,880 --> 00:32:09,200
as a whole the way a user would do it

00:32:07,519 --> 00:32:10,960
and we have a couple different kinds of

00:32:09,200 --> 00:32:13,360
system tests some are deploying a

00:32:10,960 --> 00:32:15,360
spring-based java app which uses a geode

00:32:13,360 --> 00:32:17,200
client and doing operations with that

00:32:15,360 --> 00:32:19,360
client even while

00:32:17,200 --> 00:32:20,640
inject injecting failures into the

00:32:19,360 --> 00:32:23,760
system

00:32:20,640 --> 00:32:25,840
and some other tests are just using the

00:32:23,760 --> 00:32:26,720
kubernetes api to configure a geo

00:32:25,840 --> 00:32:30,159
cluster

00:32:26,720 --> 00:32:33,600
and checking the correctness of that

00:32:30,159 --> 00:32:34,080
some notes on ci cd our tests are

00:32:33,600 --> 00:32:36,399
running

00:32:34,080 --> 00:32:37,120
on every commit against full kubernetes

00:32:36,399 --> 00:32:39,440
clusters

00:32:37,120 --> 00:32:41,039
and then just wanted to list some of the

00:32:39,440 --> 00:32:43,840
tools that we use every day

00:32:41,039 --> 00:32:43,840
here at the bottom

00:32:46,080 --> 00:32:53,760
so that concludes the section on

00:32:49,600 --> 00:32:56,640
building a solution and to wrap up here

00:32:53,760 --> 00:32:57,519
we just wanted to summarize the key

00:32:56,640 --> 00:33:00,240
takeaways

00:32:57,519 --> 00:33:02,399
again from this talk so what we hope you

00:33:00,240 --> 00:33:04,480
will take away from this

00:33:02,399 --> 00:33:06,159
uh first of all is an awareness of some

00:33:04,480 --> 00:33:08,399
of the challenges of running geodon

00:33:06,159 --> 00:33:10,159
kubernetes

00:33:08,399 --> 00:33:12,399
just like we mentioned earlier the the

00:33:10,159 --> 00:33:14,320
challenges are mainly the specialized

00:33:12,399 --> 00:33:15,600
knowledge that is required to operate a

00:33:14,320 --> 00:33:17,760
geo cluster

00:33:15,600 --> 00:33:19,279
the fact that geode wasn't primarily

00:33:17,760 --> 00:33:22,720
designed to run in the cloud

00:33:19,279 --> 00:33:23,200
and that uh great care must be taken to

00:33:22,720 --> 00:33:26,799
run

00:33:23,200 --> 00:33:27,919
stateful applications the next thing we

00:33:26,799 --> 00:33:30,880
hope you'll take away

00:33:27,919 --> 00:33:32,799
is seeing how the operator pattern helps

00:33:30,880 --> 00:33:36,399
address some of these challenges

00:33:32,799 --> 00:33:37,840
so for example we we can encode the

00:33:36,399 --> 00:33:39,279
specialized knowledge into the

00:33:37,840 --> 00:33:42,480
controller

00:33:39,279 --> 00:33:44,080
and we can configure geode in such a way

00:33:42,480 --> 00:33:46,559
through the controller that it works

00:33:44,080 --> 00:33:48,640
well in a cloud environment

00:33:46,559 --> 00:33:51,440
and then with with the controller we can

00:33:48,640 --> 00:33:53,279
do things to make sure that the

00:33:51,440 --> 00:33:54,799
data is not lost like michael was

00:33:53,279 --> 00:33:58,480
describing earlier

00:33:54,799 --> 00:34:00,159
so the last thing is we hope that you'll

00:33:58,480 --> 00:34:01,200
learn from our experience in building a

00:34:00,159 --> 00:34:04,399
geode operator

00:34:01,200 --> 00:34:05,039
and even if you're not building an

00:34:04,399 --> 00:34:06,960
operator

00:34:05,039 --> 00:34:08,560
just anyone who's trying to run geode on

00:34:06,960 --> 00:34:11,359
kubernetes we hope that some of these

00:34:08,560 --> 00:34:13,760
tips that we mentioned will be useful

00:34:11,359 --> 00:34:13,760
to you

00:34:15,440 --> 00:34:18,639
and with that we'll open it up for

00:34:17,520 --> 00:34:21,679
questions

00:34:18,639 --> 00:34:24,159
in the chat so it looks like we got one

00:34:21,679 --> 00:34:26,079
so uh alberto starts us off with uh when

00:34:24,159 --> 00:34:28,960
you talk about tests to what components

00:34:26,079 --> 00:34:34,159
were you referring to

00:34:28,960 --> 00:34:37,599
um so let me go back to tess

00:34:34,159 --> 00:34:38,879
um i okay so i'm assuming you're talking

00:34:37,599 --> 00:34:40,960
about the second

00:34:38,879 --> 00:34:42,720
part here about integration tests how

00:34:40,960 --> 00:34:43,679
multiple components interact with each

00:34:42,720 --> 00:34:46,000
other

00:34:43,679 --> 00:34:48,000
so the the specific example i can give

00:34:46,000 --> 00:34:51,599
is testing the controller

00:34:48,000 --> 00:34:55,200
so one way we could test the controller

00:34:51,599 --> 00:34:57,200
is by deploying it to a real kubernetes

00:34:55,200 --> 00:35:00,560
cluster with geode running

00:34:57,200 --> 00:35:01,760
and do operations and make sure

00:35:00,560 --> 00:35:03,440
everything is correct

00:35:01,760 --> 00:35:05,119
but the problem with that is that it's

00:35:03,440 --> 00:35:08,400
really slow

00:35:05,119 --> 00:35:10,400
to spin up a kubernetes cluster

00:35:08,400 --> 00:35:12,160
deploy the controller when we want to

00:35:10,400 --> 00:35:13,920
run these tests all the time to make

00:35:12,160 --> 00:35:17,200
sure we're not breaking stuff

00:35:13,920 --> 00:35:18,640
so the integration test in this case for

00:35:17,200 --> 00:35:22,079
the controller

00:35:18,640 --> 00:35:24,079
is just spinning up a partial kubernetes

00:35:22,079 --> 00:35:25,040
cluster with just the components that we

00:35:24,079 --> 00:35:26,640
actually

00:35:25,040 --> 00:35:28,240
sort of care about when we're testing

00:35:26,640 --> 00:35:32,400
the controller which are

00:35:28,240 --> 00:35:35,440
at cd which is a database of kubernetes

00:35:32,400 --> 00:35:38,400
the api server which is the rest api

00:35:35,440 --> 00:35:40,079
of kubernetes and then our controller of

00:35:38,400 --> 00:35:41,839
course so we just spin up those three

00:35:40,079 --> 00:35:43,520
things and test how those

00:35:41,839 --> 00:35:45,119
three components interact with each

00:35:43,520 --> 00:35:48,160
other instead of spinning up a whole

00:35:45,119 --> 00:35:48,160
kubernetes cluster

00:35:49,119 --> 00:35:53,040
okay and then there's a question from

00:35:51,680 --> 00:35:56,320
evaristo

00:35:53,040 --> 00:35:59,680
which asks did you try to

00:35:56,320 --> 00:36:01,520
scale out automatically no that's a

00:35:59,680 --> 00:36:03,440
really interesting

00:36:01,520 --> 00:36:05,119
topic though but we haven't we haven't

00:36:03,440 --> 00:36:13,839
gotten to that yet so we haven't tried

00:36:05,119 --> 00:36:13,839
to do automatic scaling yet

00:36:17,920 --> 00:36:21,359
we still have a few minutes if you want

00:36:19,599 --> 00:36:23,119
to ask some questions um

00:36:21,359 --> 00:36:24,960
if you come up with a question later

00:36:23,119 --> 00:36:26,720
though don't forget that there's the

00:36:24,960 --> 00:36:28,640
conference uh slack

00:36:26,720 --> 00:36:30,960
and there's a geochannel in there that

00:36:28,640 --> 00:36:32,640
um at least i'm there but uh

00:36:30,960 --> 00:36:34,560
and i think aaron is there too so we

00:36:32,640 --> 00:36:38,079
might be able to catch you there too if

00:36:34,560 --> 00:36:38,079
you uh come up with questions later

00:36:38,720 --> 00:36:42,000
yeah we can hang out for a little bit

00:36:40,640 --> 00:36:44,000
longer here in case

00:36:42,000 --> 00:36:46,160
anybody has more questions yeah we're

00:36:44,000 --> 00:36:46,880
also um our emails are on the first

00:36:46,160 --> 00:36:49,599
slide

00:36:46,880 --> 00:36:51,040
and we're we're both active on the geo

00:36:49,599 --> 00:36:53,359
dev list so that'd be another

00:36:51,040 --> 00:36:54,400
great place to ask questions somebody

00:36:53,359 --> 00:36:56,800
smarter than us

00:36:54,400 --> 00:36:58,960
can answer it too if we're not able to

00:36:56,800 --> 00:36:58,960
so

00:36:59,359 --> 00:37:02,400
um if you pull up the first slide aaron

00:37:01,040 --> 00:37:08,000
might give people a second chance to

00:37:02,400 --> 00:37:09,920
grab sure

00:37:08,000 --> 00:37:12,800
yeah thanks for taking time this morning

00:37:09,920 --> 00:37:12,800
to attend everyone

00:37:18,560 --> 00:37:22,000
great well if there aren't any more

00:37:20,240 --> 00:37:24,960
questions i guess we will close

00:37:22,000 --> 00:37:37,839
the presentation thank you yeah enjoy

00:37:24,960 --> 00:37:37,839
the rest of your conference folks

00:37:45,119 --> 00:37:47,200

YouTube URL: https://www.youtube.com/watch?v=iNSObr15E9o


