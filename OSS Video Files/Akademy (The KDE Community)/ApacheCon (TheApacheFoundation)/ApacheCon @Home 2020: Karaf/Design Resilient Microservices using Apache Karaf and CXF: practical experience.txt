Title: Design Resilient Microservices using Apache Karaf and CXF: practical experience
Publication date: 2020-10-16
Playlist: ApacheCon @Home 2020: Karaf
Description: 
	Design Resilient Microservices using Apache Karaf and CXF: practical experience
Andrei Shakirin

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Development team just has finished the last feature after months of hard work. Does this mean the software is production ready now? What aspects need to be considered before deployment your Microservices to the production environment? What should you do in emergency situations on production? How to make your software more reliable and resilient by deployment in Cloud? What are the stability and resiliency patterns and anti-patterns? All these questions will be addressed in the talk. Based on practical experience, presenter will demonstrate the best engineering practices to design resilient software using Apache Karaf, CXF, Kafka, ActiveMQ and illustrate them with real life cases. The following topics will be covered in presentation: • Stability anti-patterns (chain reactions, cascading failures, blocked threads) • Stability patterns (Timeouts, Circuit Breaker, Bulkheads, Fail Fast, Async) • Clustering and Load Balancing • Logging and Monitoring • Production Diagnostic • Pooling and Caching • Load and Stress Testing

Andrei is a software architect in the Talend team developing the open source Application Integration platform based on Apache projects. The areas of his interest are REST API design, Microservices, Cloud, resilient distributed systems, security and agile development. Andrei is PMC and committer of Apache CXF and committer of Syncope projects. He is member of OASIS S-RAMP Work Group and speaker at Java and Apache conferences. Last speaking experience: • DOAG 2019, Nov 2019, Nurnberg, Design Production-Ready Software • Karlsruhe Entwickertag 2017, Mai 2017, Karlsruhe, Microservices with OSGi • ApacheCon Europe 2016, Nov 2016, Seville, Microservices with Apache Karaf and Apache CXF: Practical Experience • ApacheCon Europe 2015, Oct 2015, Budapest, Create and Secure Your REST API with Apache CXF • ApacheCon Europe 2014, Nov 2014, Budapest, Design REST Services With CXF JAX-RS Implementation: Lessons Learned • WJAX 2011, Nov 2011, Munich, Apache Days, Enabling Services with Apache CXF
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,039 --> 00:00:30,320
okay so um

00:00:26,240 --> 00:00:34,000
i guess i can start it's a time

00:00:30,320 --> 00:00:36,800
uh so i will next 40 minutes will

00:00:34,000 --> 00:00:38,399
um dive between the resilience

00:00:36,800 --> 00:00:41,680
microservice design

00:00:38,399 --> 00:00:43,280
firms using apache 6 have cara effective

00:00:41,680 --> 00:00:44,480
mq and some other technologies so

00:00:43,280 --> 00:00:46,879
basically it's

00:00:44,480 --> 00:00:46,879
about

00:00:48,559 --> 00:00:53,520
about the basics of

00:00:51,760 --> 00:00:54,800
design resilience microsoft

00:00:53,520 --> 00:00:58,800
microservices and

00:00:54,800 --> 00:01:00,399
about resilience um so my presentation

00:00:58,800 --> 00:01:02,800
will contain some some

00:01:00,399 --> 00:01:03,840
parts uh in the first i i will start

00:01:02,800 --> 00:01:06,080
with um

00:01:03,840 --> 00:01:07,439
describing the real case happens some

00:01:06,080 --> 00:01:10,080
years ago with uh

00:01:07,439 --> 00:01:11,360
customer projects in ecommerce so the

00:01:10,080 --> 00:01:14,080
outage of

00:01:11,360 --> 00:01:15,040
ecommerce shop and just analyze the

00:01:14,080 --> 00:01:16,720
reason

00:01:15,040 --> 00:01:19,600
what really happens so perhaps you have

00:01:16,720 --> 00:01:21,119
ideas as well you can share it

00:01:19,600 --> 00:01:22,880
to make some formal definition of

00:01:21,119 --> 00:01:25,280
stability go through the

00:01:22,880 --> 00:01:26,000
stability anti-patterns and stability

00:01:25,280 --> 00:01:27,680
patterns

00:01:26,000 --> 00:01:29,680
and make some conclusion and lesson

00:01:27,680 --> 00:01:31,520
learned

00:01:29,680 --> 00:01:33,280
shortly about me i'm worked as a

00:01:31,520 --> 00:01:34,640
software architect and a talent team to

00:01:33,280 --> 00:01:37,119
tell and produce

00:01:34,640 --> 00:01:37,759
some products based on the open source

00:01:37,119 --> 00:01:41,119
apache

00:01:37,759 --> 00:01:43,200
projects and i'm also a bit involved in

00:01:41,119 --> 00:01:43,920
the party community in my bmc in apache

00:01:43,200 --> 00:01:45,759
6f

00:01:43,920 --> 00:01:47,439
and make some contributions in apache

00:01:45,759 --> 00:01:49,040
synchro areas and budget cut-off

00:01:47,439 --> 00:01:52,880
projects

00:01:49,040 --> 00:01:56,000
so as you can see earl the most of

00:01:52,880 --> 00:01:58,399
time i was as a technical person so

00:01:56,000 --> 00:02:00,000
software engineer software developer and

00:01:58,399 --> 00:02:01,840
a team lead

00:02:00,000 --> 00:02:03,280
but in one project service exactly with

00:02:01,840 --> 00:02:06,640
ecommerce customer i

00:02:03,280 --> 00:02:09,679
um also shoot e beat defaults

00:02:06,640 --> 00:02:11,840
so i care also about the deployments

00:02:09,679 --> 00:02:12,959
about production system about the

00:02:11,840 --> 00:02:15,440
outages

00:02:12,959 --> 00:02:18,160
failures and the first impression was

00:02:15,440 --> 00:02:18,160
was like this

00:02:18,640 --> 00:02:22,480
it's really hard to work under pressure

00:02:21,840 --> 00:02:24,959
because

00:02:22,480 --> 00:02:26,800
uh every minute so every second every

00:02:24,959 --> 00:02:27,840
minutes of outpatient shopping cost a

00:02:26,800 --> 00:02:31,599
lot of money

00:02:27,840 --> 00:02:34,480
so we have a lot of pressure if

00:02:31,599 --> 00:02:36,959
some some some failure and some some uh

00:02:34,480 --> 00:02:41,360
back really prevents a customer to buy

00:02:36,959 --> 00:02:41,360
in the shop and um

00:02:41,440 --> 00:02:44,720
to fixing a defining problem under this

00:02:43,440 --> 00:02:46,879
pressure was

00:02:44,720 --> 00:02:48,000
psychologically very very hard for me to

00:02:46,879 --> 00:02:49,920
be honest

00:02:48,000 --> 00:02:51,519
therefore i thought a bit in the

00:02:49,920 --> 00:02:54,560
direction how to

00:02:51,519 --> 00:02:57,920
other ways to make your system

00:02:54,560 --> 00:03:01,120
more or less uh robust to more reliable

00:02:57,920 --> 00:03:04,239
to react on the field

00:03:01,120 --> 00:03:04,800
um so i just start with a one case

00:03:04,239 --> 00:03:07,519
happens

00:03:04,800 --> 00:03:08,800
um some years ago exactly in this for

00:03:07,519 --> 00:03:10,319
this is a customer

00:03:08,800 --> 00:03:12,480
just describe a little bit uh

00:03:10,319 --> 00:03:15,599
architecture of the system it was a

00:03:12,480 --> 00:03:16,400
web web shop with a front-end web-based

00:03:15,599 --> 00:03:19,599
front-end

00:03:16,400 --> 00:03:22,080
the web browser our front end

00:03:19,599 --> 00:03:23,200
is implemented by javascript and access

00:03:22,080 --> 00:03:26,720
rest api

00:03:23,200 --> 00:03:26,720
with the weekend back-end

00:03:27,280 --> 00:03:33,200
we can't access sap some

00:03:30,319 --> 00:03:34,640
data storages is a document oriented and

00:03:33,200 --> 00:03:36,400
sql database

00:03:34,640 --> 00:03:38,400
also use actively messaging queue

00:03:36,400 --> 00:03:40,560
messaging and access some

00:03:38,400 --> 00:03:41,920
online finance and degraded fastness

00:03:40,560 --> 00:03:43,680
system

00:03:41,920 --> 00:03:45,360
itself inside the middleweights was

00:03:43,680 --> 00:03:48,400
implemented as microservices

00:03:45,360 --> 00:03:50,799
and every uh

00:03:48,400 --> 00:03:52,959
bounded context contains three parts is

00:03:50,799 --> 00:03:56,080
a kind of facade that

00:03:52,959 --> 00:03:57,680
has a task to orchestrate result of the

00:03:56,080 --> 00:04:00,080
core service the marketplace service

00:03:57,680 --> 00:04:00,959
core service is responsible to care

00:04:00,080 --> 00:04:03,840
about products

00:04:00,959 --> 00:04:05,200
inside their own products basically for

00:04:03,840 --> 00:04:08,400
ecommerce provider

00:04:05,200 --> 00:04:11,360
and marketplace case about products

00:04:08,400 --> 00:04:14,319
in the other sellers who use platform to

00:04:11,360 --> 00:04:17,680
sell own products

00:04:14,319 --> 00:04:19,440
um both of the marketplace even even

00:04:17,680 --> 00:04:21,759
facade but marketplace service and core

00:04:19,440 --> 00:04:23,919
service has some asynchronous separation

00:04:21,759 --> 00:04:25,680
and the sender publish something in the

00:04:23,919 --> 00:04:26,560
queue at least and they just listen for

00:04:25,680 --> 00:04:29,120
example if you

00:04:26,560 --> 00:04:29,919
commit the order it's not necessary to

00:04:29,120 --> 00:04:32,240
wait um

00:04:29,919 --> 00:04:34,000
synchronously for the response you can

00:04:32,240 --> 00:04:36,160
return immediately and the result

00:04:34,000 --> 00:04:37,040
for the customer and say yes uh order is

00:04:36,160 --> 00:04:39,120
accepted

00:04:37,040 --> 00:04:43,840
and internal processing happens through

00:04:39,120 --> 00:04:43,840
the asynchronous queue

00:04:45,680 --> 00:04:49,440
early morning i was responsible to

00:04:48,000 --> 00:04:53,759
deploy

00:04:49,440 --> 00:04:56,560
the updates in the core order or service

00:04:53,759 --> 00:04:58,960
so just normal production deployment and

00:04:56,560 --> 00:05:01,360
we have a kind of monitoring

00:04:58,960 --> 00:05:03,680
this name uptrends as a kind of semantic

00:05:01,360 --> 00:05:07,039
synthetic monitoring that they produce

00:05:03,680 --> 00:05:09,759
uh user user make

00:05:07,039 --> 00:05:11,759
a fake order let's add something to the

00:05:09,759 --> 00:05:14,800
card and go through the whole system

00:05:11,759 --> 00:05:15,520
and see the result and uh describe quite

00:05:14,800 --> 00:05:17,360
quite a good

00:05:15,520 --> 00:05:19,120
quality of monitoring that's if this

00:05:17,360 --> 00:05:22,720
process fails something wrong with

00:05:19,120 --> 00:05:23,600
the whole shop and deployment itself was

00:05:22,720 --> 00:05:27,280
successful

00:05:23,600 --> 00:05:31,199
it takes about 15 minutes or i was happy

00:05:27,280 --> 00:05:34,320
but and even it works for about

00:05:31,199 --> 00:05:37,280
20 minutes was uptrends was green so i

00:05:34,320 --> 00:05:38,000
really relaxed this time but suddenly

00:05:37,280 --> 00:05:40,479
after 20

00:05:38,000 --> 00:05:41,120
minutes i see the uh red color in the

00:05:40,479 --> 00:05:42,639
uptrends

00:05:41,120 --> 00:05:44,240
and something happened something that

00:05:42,639 --> 00:05:47,840
happens

00:05:44,240 --> 00:05:50,400
uh i restart the core services or

00:05:47,840 --> 00:05:50,880
it helps but only for 15 minutes and

00:05:50,400 --> 00:05:54,000
after

00:05:50,880 --> 00:05:57,520
50 minutes the problem happens again

00:05:54,000 --> 00:05:59,120
um after that i restart uh mercedes

00:05:57,520 --> 00:06:01,440
um and marketplace service doesn't

00:05:59,120 --> 00:06:02,880
happen at all and in the result i

00:06:01,440 --> 00:06:04,840
roll back the core orders as my

00:06:02,880 --> 00:06:07,199
deployment and it helps it prepare the

00:06:04,840 --> 00:06:10,560
system

00:06:07,199 --> 00:06:13,120
after that fortunately i'll be able to

00:06:10,560 --> 00:06:17,520
also to take as a threat dumps

00:06:13,120 --> 00:06:19,680
after the failure and in the

00:06:17,520 --> 00:06:21,120
post-mortem i analyze the thread dumps

00:06:19,680 --> 00:06:23,680
and it

00:06:21,120 --> 00:06:24,800
a bit or has i have idea what happens at

00:06:23,680 --> 00:06:26,720
least

00:06:24,800 --> 00:06:30,240
so this thread dump a lot of threads

00:06:26,720 --> 00:06:32,800
hanging by active mq create session

00:06:30,240 --> 00:06:35,280
so connection was a successful uh

00:06:32,800 --> 00:06:37,120
created but when the active queue try to

00:06:35,280 --> 00:06:39,199
create a session it hangs

00:06:37,120 --> 00:06:41,360
and a lot of thread are blocked in this

00:06:39,199 --> 00:06:44,479
state

00:06:41,360 --> 00:06:47,039
then i look what exactly was changed

00:06:44,479 --> 00:06:48,400
is some active mq configuration there is

00:06:47,039 --> 00:06:51,840
a url

00:06:48,400 --> 00:06:53,680
for broker maximum connection

00:06:51,840 --> 00:06:54,960
and there is a parameter maximum active

00:06:53,680 --> 00:06:58,160
session per connection was

00:06:54,960 --> 00:07:00,800
10 as a parameter concurrent consumer

00:06:58,160 --> 00:07:03,039
consumers uh concurred consumer

00:07:00,800 --> 00:07:07,039
basically is a parameter for listener

00:07:03,039 --> 00:07:08,560
to list to to to get the information

00:07:07,039 --> 00:07:09,440
with you in parallel so it's not only

00:07:08,560 --> 00:07:11,919
one thread that

00:07:09,440 --> 00:07:13,599
gets the information as some threads in

00:07:11,919 --> 00:07:16,720
this case is chain

00:07:13,599 --> 00:07:19,840
and i see what changed five

00:07:16,720 --> 00:07:22,000
it was five before and now becomes ten

00:07:19,840 --> 00:07:24,000
this was one of the changes in my

00:07:22,000 --> 00:07:27,520
deployment

00:07:24,000 --> 00:07:30,400
um looking at the courthouse implemented

00:07:27,520 --> 00:07:31,120
as a message listener just create a

00:07:30,400 --> 00:07:34,240
thread

00:07:31,120 --> 00:07:36,639
for concurrent consumer and every thread

00:07:34,240 --> 00:07:38,160
gets a great recession from the

00:07:36,639 --> 00:07:39,599
connection

00:07:38,160 --> 00:07:41,759
then session create a consumer from

00:07:39,599 --> 00:07:43,120
destination and activate that set

00:07:41,759 --> 00:07:44,879
message listing

00:07:43,120 --> 00:07:46,160
after that the consumer session was

00:07:44,879 --> 00:07:49,840
closed and

00:07:46,160 --> 00:07:51,280
interestingly the connection was uh

00:07:49,840 --> 00:07:54,240
obtained from the pulled connection

00:07:51,280 --> 00:07:54,240
factor in this case

00:07:54,960 --> 00:08:00,080
the message center site which was it use

00:07:58,319 --> 00:08:00,960
the same connection factor so also

00:08:00,080 --> 00:08:02,879
create a connection

00:08:00,960 --> 00:08:04,800
start the connection create a session

00:08:02,879 --> 00:08:07,199
create a producer and send the message

00:08:04,800 --> 00:08:09,680
and close all the things

00:08:07,199 --> 00:08:10,879
do you have already ideas what happens

00:08:09,680 --> 00:08:13,680
what why

00:08:10,879 --> 00:08:15,039
such kind of problems was here perhaps

00:08:13,680 --> 00:08:17,520
somebody has an idea and

00:08:15,039 --> 00:08:17,520
type here

00:08:18,560 --> 00:08:29,360
any ideas about that

00:08:27,039 --> 00:08:29,360
okay

00:08:30,560 --> 00:08:34,640
uh the problem was basically those are

00:08:32,880 --> 00:08:35,279
two problems so the main problem was

00:08:34,640 --> 00:08:37,440
that

00:08:35,279 --> 00:08:39,360
message sender and message listener

00:08:37,440 --> 00:08:40,000
share the same bullet connection factory

00:08:39,360 --> 00:08:43,039
it's a really

00:08:40,000 --> 00:08:45,360
really bad idea and what happens

00:08:43,039 --> 00:08:47,839
exactly because of this change it was

00:08:45,360 --> 00:08:50,080
five consumers before

00:08:47,839 --> 00:08:50,080
and

00:08:51,120 --> 00:08:56,880
consumer gets the connection create five

00:08:54,640 --> 00:08:58,640
session and now it creates 10 session

00:08:56,880 --> 00:09:00,000
and every connection is configured by

00:08:58,640 --> 00:09:03,600
maximum

00:09:00,000 --> 00:09:07,279
active session in a 10 so just get

00:09:03,600 --> 00:09:09,760
all possible sessions create a listeners

00:09:07,279 --> 00:09:12,080
and return a connection back to the pool

00:09:09,760 --> 00:09:15,680
with close connection

00:09:12,080 --> 00:09:19,200
then uh sender uh the message producer

00:09:15,680 --> 00:09:22,160
comes in in in a game and

00:09:19,200 --> 00:09:24,080
it gets a connection i try to create a

00:09:22,160 --> 00:09:27,360
session here

00:09:24,080 --> 00:09:29,839
and as far as soon as i get the session

00:09:27,360 --> 00:09:32,000
already used by listener there are no

00:09:29,839 --> 00:09:32,880
any free session and just hangs on this

00:09:32,000 --> 00:09:36,000
place

00:09:32,880 --> 00:09:38,880
so it happens not immediately but

00:09:36,000 --> 00:09:40,399
after some work they just get such a

00:09:38,880 --> 00:09:42,000
session and more and more centers

00:09:40,399 --> 00:09:44,640
hanging and

00:09:42,000 --> 00:09:45,839
basically the system stops to work so

00:09:44,640 --> 00:09:48,240
the main problem here

00:09:45,839 --> 00:09:50,240
was to share the same pulled connection

00:09:48,240 --> 00:09:52,880
factory for cyanistana even for

00:09:50,240 --> 00:09:56,240
listening makes no sense at all because

00:09:52,880 --> 00:09:57,360
uh for uh because it the life cycle of

00:09:56,240 --> 00:10:00,880
eastern is the same

00:09:57,360 --> 00:10:02,560
lifecycles or container it created a

00:10:00,880 --> 00:10:04,160
session in the beginning and closed it

00:10:02,560 --> 00:10:06,560
in the at the end

00:10:04,160 --> 00:10:07,360
so it use connection for the whole life

00:10:06,560 --> 00:10:09,760
cycle

00:10:07,360 --> 00:10:10,560
make no sense to use pool of connection

00:10:09,760 --> 00:10:14,320
here

00:10:10,560 --> 00:10:16,880
so just can inject the native

00:10:14,320 --> 00:10:18,480
connection factory and use it for sender

00:10:16,880 --> 00:10:20,160
it makes sense because it's only short

00:10:18,480 --> 00:10:21,839
communication it sends something in

00:10:20,160 --> 00:10:22,480
close it don't connect to the pool it

00:10:21,839 --> 00:10:25,680
can use

00:10:22,480 --> 00:10:28,320
full connection another

00:10:25,680 --> 00:10:29,279
problem of course uh from activemq it's

00:10:28,320 --> 00:10:31,680
a bit strange it's

00:10:29,279 --> 00:10:32,399
a clear click session has no timeout so

00:10:31,680 --> 00:10:35,040
i i

00:10:32,399 --> 00:10:36,959
just submitted as a bug and i have a

00:10:35,040 --> 00:10:38,480
default timeout after that it's with a

00:10:36,959 --> 00:10:42,240
there's a user for exception because

00:10:38,480 --> 00:10:44,800
before it just hangs forever

00:10:42,240 --> 00:10:46,880
yeah so this is a interesting case what

00:10:44,800 --> 00:10:49,200
happens on the on the customer project

00:10:46,880 --> 00:10:50,160
and after that i look a bit more in

00:10:49,200 --> 00:10:52,959
detail

00:10:50,160 --> 00:10:54,880
how are there some formal or some

00:10:52,959 --> 00:10:56,079
patterns how to make the system more

00:10:54,880 --> 00:10:58,839
resilient and

00:10:56,079 --> 00:11:00,079
avoid such situations as it happens for

00:10:58,839 --> 00:11:02,880
me

00:11:00,079 --> 00:11:04,800
um let's go to the definitions as uh

00:11:02,880 --> 00:11:08,000
what is the system stability at all

00:11:04,800 --> 00:11:09,279
so stability is a

00:11:08,000 --> 00:11:10,880
characteristic of the system or

00:11:09,279 --> 00:11:12,560
attribute of the system that they keeps

00:11:10,880 --> 00:11:14,959
processing transaction

00:11:12,560 --> 00:11:16,240
stays available even under some

00:11:14,959 --> 00:11:18,480
disruption impulses so

00:11:16,240 --> 00:11:19,279
even something bad happens with the

00:11:18,480 --> 00:11:21,120
network

00:11:19,279 --> 00:11:23,040
this database with asynchronous

00:11:21,120 --> 00:11:26,240
communication some components

00:11:23,040 --> 00:11:28,399
failures big load but systems still

00:11:26,240 --> 00:11:30,240
be able to proceed the transaction

00:11:28,399 --> 00:11:34,720
perhaps not in the

00:11:30,240 --> 00:11:36,320
uh 100 uh functionality yes perhaps some

00:11:34,720 --> 00:11:39,040
features available not available

00:11:36,320 --> 00:11:40,560
perhaps also some users will be blocked

00:11:39,040 --> 00:11:42,800
but the most of user

00:11:40,560 --> 00:11:43,600
can you still use this system tips

00:11:42,800 --> 00:11:46,000
useful

00:11:43,600 --> 00:11:49,200
this is a stability definition and

00:11:46,000 --> 00:11:52,000
availability is basically

00:11:49,200 --> 00:11:52,480
derived terms from the stability and

00:11:52,000 --> 00:11:55,120
shows

00:11:52,480 --> 00:11:56,880
if the system is operational and time t

00:11:55,120 --> 00:11:59,360
of course is a system stable it's also

00:11:56,880 --> 00:11:59,360
available

00:11:59,600 --> 00:12:06,160
uh important here to uh also get that

00:12:03,360 --> 00:12:07,279
it's not really possible to so it

00:12:06,160 --> 00:12:10,560
disregards how

00:12:07,279 --> 00:12:12,399
good your qa quality assurance is

00:12:10,560 --> 00:12:14,480
the unit test level integration test

00:12:12,399 --> 00:12:16,880
system tests lower tests

00:12:14,480 --> 00:12:19,600
but it's not possibly not possible to

00:12:16,880 --> 00:12:21,440
completely avoid production failures

00:12:19,600 --> 00:12:22,880
so you can you should you should somehow

00:12:21,440 --> 00:12:25,040
deal with that some

00:12:22,880 --> 00:12:26,639
problems in the production will happen

00:12:25,040 --> 00:12:28,560
for sure

00:12:26,639 --> 00:12:30,160
and every production failure is unique

00:12:28,560 --> 00:12:31,519
by there are common ways to improve the

00:12:30,160 --> 00:12:34,079
system robustness

00:12:31,519 --> 00:12:36,000
and exactly this was interesting for me

00:12:34,079 --> 00:12:38,639
how to do it

00:12:36,000 --> 00:12:40,320
um also some formal stuff which could

00:12:38,639 --> 00:12:41,360
happen with the system there are some

00:12:40,320 --> 00:12:44,800
three

00:12:41,360 --> 00:12:45,839
different kind of problems the first is

00:12:44,800 --> 00:12:49,200
a fault

00:12:45,839 --> 00:12:52,240
the fault is incorrect internal state

00:12:49,200 --> 00:12:55,040
it could uh caused by unexpected

00:12:52,240 --> 00:12:55,360
external system behavior or internal bug

00:12:55,040 --> 00:12:58,399
but

00:12:55,360 --> 00:12:59,279
fault is not always bad uh so for some

00:12:58,399 --> 00:13:02,560
fault you can

00:12:59,279 --> 00:13:03,680
deal you can just uh inter-subset and uh

00:13:02,560 --> 00:13:05,279
correct

00:13:03,680 --> 00:13:08,000
there are two approaches how to do this

00:13:05,279 --> 00:13:11,600
fault is for tolerance and you try to

00:13:08,000 --> 00:13:15,279
catch that and continue to work

00:13:11,600 --> 00:13:17,360
despite the fault or another strategy is

00:13:15,279 --> 00:13:18,959
for intolerance you propagate as soon as

00:13:17,360 --> 00:13:22,160
possible in the top system so

00:13:18,959 --> 00:13:24,639
some cases make sense as well

00:13:22,160 --> 00:13:26,320
second error is observable and correct

00:13:24,639 --> 00:13:26,880
behavior so if you add something into

00:13:26,320 --> 00:13:29,519
the cart

00:13:26,880 --> 00:13:30,240
but you still know have items it's

00:13:29,519 --> 00:13:33,040
basically so

00:13:30,240 --> 00:13:34,079
some some bugs in the system and the

00:13:33,040 --> 00:13:37,279
last type of

00:13:34,079 --> 00:13:39,519
problem is a failure that's the the

00:13:37,279 --> 00:13:41,440
the worst case that we would like to

00:13:39,519 --> 00:13:43,519
avoid is unresponsive system

00:13:41,440 --> 00:13:44,720
so the system stops to proceed user

00:13:43,519 --> 00:13:47,360
transaction and

00:13:44,720 --> 00:13:48,160
system mission cannot be accomplished

00:13:47,360 --> 00:13:52,240
they can

00:13:48,160 --> 00:13:52,240
try to avoid the last situation

00:13:52,480 --> 00:13:56,160
let's go for the stability enterprise

00:13:54,079 --> 00:13:57,199
basically empty patterns is a way to

00:13:56,160 --> 00:13:59,920
build the system

00:13:57,199 --> 00:14:01,120
or build architecture that make even

00:13:59,920 --> 00:14:03,040
problem even worse

00:14:01,120 --> 00:14:04,240
as before yeah so something that happens

00:14:03,040 --> 00:14:06,399
the architecture

00:14:04,240 --> 00:14:07,680
amplifies the problems makes even even

00:14:06,399 --> 00:14:09,600
better

00:14:07,680 --> 00:14:11,519
i mean even worse the first

00:14:09,600 --> 00:14:12,079
anti-patterns is unprotected integration

00:14:11,519 --> 00:14:15,360
points

00:14:12,079 --> 00:14:18,800
so uh integration points is all

00:14:15,360 --> 00:14:20,160
are places in the system to communicate

00:14:18,800 --> 00:14:22,320
outside it could be

00:14:20,160 --> 00:14:24,000
excellent service calls could be

00:14:22,320 --> 00:14:26,560
low-level microservices

00:14:24,000 --> 00:14:28,160
could be some as the case remote sdks

00:14:26,560 --> 00:14:29,920
and frameworks could be database access

00:14:28,160 --> 00:14:30,240
could be messaging so basically all what

00:14:29,920 --> 00:14:33,360
goes

00:14:30,240 --> 00:14:35,279
outside is integration point why

00:14:33,360 --> 00:14:37,519
integration point is so important

00:14:35,279 --> 00:14:38,880
because sometimes external system

00:14:37,519 --> 00:14:42,639
produce not

00:14:38,880 --> 00:14:44,320
or behaves not as we expect for example

00:14:42,639 --> 00:14:46,320
this is expected behavior that the

00:14:44,320 --> 00:14:48,959
system the external system uh has some

00:14:46,320 --> 00:14:51,360
weight gateway or internal server

00:14:48,959 --> 00:14:52,399
but instead external system just accepts

00:14:51,360 --> 00:14:55,440
the connection

00:14:52,399 --> 00:14:57,360
and reads the req but your gdp request

00:14:55,440 --> 00:15:00,959
was never reached as hanks

00:14:57,360 --> 00:15:04,000
for undefined time or you expect some

00:15:00,959 --> 00:15:06,000
json response and a system will use some

00:15:04,000 --> 00:15:08,079
and less mp3

00:15:06,000 --> 00:15:09,120
or even worse your unexpected xml

00:15:08,079 --> 00:15:11,839
application xml

00:15:09,120 --> 00:15:12,399
and external system has produced

00:15:11,839 --> 00:15:15,199
infinite

00:15:12,399 --> 00:15:18,320
stream with open xml text so it's a good

00:15:15,199 --> 00:15:20,959
test how robust your system

00:15:18,320 --> 00:15:22,320
therefore uh why why it's so so so

00:15:20,959 --> 00:15:24,399
important to protect the

00:15:22,320 --> 00:15:26,480
integration points because our

00:15:24,399 --> 00:15:29,519
unprotected uh integration points

00:15:26,480 --> 00:15:30,639
um produce cascading failure so let's

00:15:29,519 --> 00:15:32,639
imagine you have some

00:15:30,639 --> 00:15:34,399
micro services typically you have some

00:15:32,639 --> 00:15:36,240
chain calls one microservice invoke

00:15:34,399 --> 00:15:38,240
another microservice

00:15:36,240 --> 00:15:39,600
and you perhaps invoke some database or

00:15:38,240 --> 00:15:42,800
messaging system and

00:15:39,600 --> 00:15:43,040
the database has outpage for example and

00:15:42,800 --> 00:15:46,160
if

00:15:43,040 --> 00:15:48,560
if your iteration point is not protected

00:15:46,160 --> 00:15:50,320
the threads using this database

00:15:48,560 --> 00:15:52,639
connection are

00:15:50,320 --> 00:15:54,399
exhausted so that all the sets from the

00:15:52,639 --> 00:15:57,360
thread pool i used

00:15:54,399 --> 00:15:59,600
and microservice the whole microservices

00:15:57,360 --> 00:16:03,279
stopped response

00:15:59,600 --> 00:16:05,920
uh in other in in turns

00:16:03,279 --> 00:16:08,000
microservices call the low level

00:16:05,920 --> 00:16:10,639
microservice has also connection

00:16:08,000 --> 00:16:12,079
threat pool that has exhausted and also

00:16:10,639 --> 00:16:14,560
stopped response and

00:16:12,079 --> 00:16:15,680
as this case failure propagated

00:16:14,560 --> 00:16:18,720
vertically

00:16:15,680 --> 00:16:20,880
so you have some low level problems it

00:16:18,720 --> 00:16:24,240
very quickly propagated on the top and

00:16:20,880 --> 00:16:26,639
make your system unresponsive

00:16:24,240 --> 00:16:27,759
how to deal with that what you can do

00:16:26,639 --> 00:16:29,600
the first of course

00:16:27,759 --> 00:16:32,480
protect your integration point with the

00:16:29,600 --> 00:16:35,839
pattern so we'll look on some of that

00:16:32,480 --> 00:16:37,040
later but makes a lot of sense to

00:16:35,839 --> 00:16:39,920
introduce timeouts

00:16:37,040 --> 00:16:41,519
at least and i really recommend to

00:16:39,920 --> 00:16:42,240
protect integration for the circuit

00:16:41,519 --> 00:16:45,279
breaker

00:16:42,240 --> 00:16:47,680
uh pattern will look a bit more

00:16:45,279 --> 00:16:49,920
in details also synchronous processing

00:16:47,680 --> 00:16:50,320
helps in some some cases is not blocked

00:16:49,920 --> 00:16:53,920
not

00:16:50,320 --> 00:16:57,360
uh so make a failed

00:16:53,920 --> 00:16:59,279
failure tolerance uh of course test from

00:16:57,360 --> 00:17:01,839
any form of failure makes sense

00:16:59,279 --> 00:17:03,360
not only expected errors but also

00:17:01,839 --> 00:17:04,559
especially in the case and system

00:17:03,360 --> 00:17:06,400
external system hangs

00:17:04,559 --> 00:17:08,799
and blocks a request what happens with

00:17:06,400 --> 00:17:11,760
your system

00:17:08,799 --> 00:17:13,039
fail fast make make of course sense you

00:17:11,760 --> 00:17:14,959
should use retries as

00:17:13,039 --> 00:17:16,559
very carefully so from my experience it

00:17:14,959 --> 00:17:19,600
tries makes in a lot of

00:17:16,559 --> 00:17:21,439
cases even worse so the image

00:17:19,600 --> 00:17:23,280
let's imagine the external system has a

00:17:21,439 --> 00:17:27,839
high low the peak load

00:17:23,280 --> 00:17:30,960
and uh therefore makes a response or

00:17:27,839 --> 00:17:31,760
response slowly and what makes the

00:17:30,960 --> 00:17:35,039
clients

00:17:31,760 --> 00:17:36,559
they try to retry request retire and

00:17:35,039 --> 00:17:38,240
try and decline so instead try and

00:17:36,559 --> 00:17:41,360
request external system

00:17:38,240 --> 00:17:43,039
uh receive hundred or thousand and have

00:17:41,360 --> 00:17:46,400
no chance to recover so you may

00:17:43,039 --> 00:17:47,679
make situation even more and of course

00:17:46,400 --> 00:17:49,360
makes sense to provide at least

00:17:47,679 --> 00:17:52,400
information and tools to

00:17:49,360 --> 00:17:55,760
find the euro what exactly is cause

00:17:52,400 --> 00:17:55,760
of the other of the year

00:17:57,280 --> 00:18:02,480
the next anti-pattern so

00:18:00,640 --> 00:18:04,640
the failure could propagate it

00:18:02,480 --> 00:18:07,440
vertically but unfortunately it could

00:18:04,640 --> 00:18:10,799
propagate it horizontally as well

00:18:07,440 --> 00:18:14,400
so normally uh in the modern

00:18:10,799 --> 00:18:17,360
systems there is on not only one mode

00:18:14,400 --> 00:18:17,679
and you deploy your microservice but you

00:18:17,360 --> 00:18:19,919
have

00:18:17,679 --> 00:18:22,080
a number of nodes number of instances

00:18:19,919 --> 00:18:23,440
you have to ensure failover and

00:18:22,080 --> 00:18:25,840
load balance and there is a load

00:18:23,440 --> 00:18:28,160
balancer uh on top of that

00:18:25,840 --> 00:18:28,960
and load balancer distribute check the

00:18:28,160 --> 00:18:30,799
house

00:18:28,960 --> 00:18:33,679
of the instance and redistribute their

00:18:30,799 --> 00:18:36,160
request to the single instances

00:18:33,679 --> 00:18:37,440
what happens if i have some bark for

00:18:36,160 --> 00:18:40,640
example memory leak

00:18:37,440 --> 00:18:41,200
or a deadlock in the code in one system

00:18:40,640 --> 00:18:44,320
and

00:18:41,200 --> 00:18:47,200
one node my node is blocked the

00:18:44,320 --> 00:18:48,240
load balancer uh recognized that so

00:18:47,200 --> 00:18:51,120
chelsea check

00:18:48,240 --> 00:18:52,160
doesn't work anymore and load balancer

00:18:51,120 --> 00:18:55,039
redistributes

00:18:52,160 --> 00:18:56,640
requests to other servers so in the

00:18:55,039 --> 00:18:58,799
result other servers will

00:18:56,640 --> 00:19:00,400
receive more traffic so at least in one

00:18:58,799 --> 00:19:03,760
search traffic more

00:19:00,400 --> 00:19:04,799
and in theory there are independence

00:19:03,760 --> 00:19:07,600
modes

00:19:04,799 --> 00:19:08,240
but in reality the same software the

00:19:07,600 --> 00:19:10,000
same

00:19:08,240 --> 00:19:11,520
microservice content send codebase

00:19:10,000 --> 00:19:12,720
running on the whole servers

00:19:11,520 --> 00:19:14,799
and if you increase the traffic

00:19:12,720 --> 00:19:16,320
probability that this error happens on

00:19:14,799 --> 00:19:19,280
the other instances as well

00:19:16,320 --> 00:19:20,480
are high and very soon you will see such

00:19:19,280 --> 00:19:22,480
kind of situation

00:19:20,480 --> 00:19:25,600
so they're all servers unavailable and

00:19:22,480 --> 00:19:27,919
system is unresponsive

00:19:25,600 --> 00:19:29,039
what we can do against there is a

00:19:27,919 --> 00:19:31,039
pattern

00:19:29,039 --> 00:19:32,720
with the name bulkhead so it's a like in

00:19:31,039 --> 00:19:35,840
chip then the isolate

00:19:32,720 --> 00:19:39,360
even isolates the users otherwise

00:19:35,840 --> 00:19:41,600
instances is it even if

00:19:39,360 --> 00:19:43,200
some instances have a problem you still

00:19:41,600 --> 00:19:46,240
can

00:19:43,200 --> 00:19:48,240
sell for example most important users

00:19:46,240 --> 00:19:49,520
also make sense to carry about failures

00:19:48,240 --> 00:19:52,000
very quickly don't

00:19:49,520 --> 00:19:52,960
think that i have 12 knots more and if

00:19:52,000 --> 00:19:56,799
one fails

00:19:52,960 --> 00:19:59,200
nothing happens it makes sense to

00:19:56,799 --> 00:20:01,360
okay about it automatically so at least

00:19:59,200 --> 00:20:02,159
restart it and of course analyze what

00:20:01,360 --> 00:20:06,320
really happens

00:20:02,159 --> 00:20:08,640
so hand for it very soon

00:20:06,320 --> 00:20:10,000
next empty patterns um basically i

00:20:08,640 --> 00:20:12,799
already showed in the on the

00:20:10,000 --> 00:20:14,960
on the first case what happens if i

00:20:12,799 --> 00:20:16,640
share connection pool

00:20:14,960 --> 00:20:18,240
the same basically with the thread pool

00:20:16,640 --> 00:20:20,320
if my clients inside the application

00:20:18,240 --> 00:20:23,520
share the same thread pool

00:20:20,320 --> 00:20:25,840
and uh service c has a problem

00:20:23,520 --> 00:20:27,600
as for example slow down slow down or

00:20:25,840 --> 00:20:30,960
completely

00:20:27,600 --> 00:20:33,120
outage then the client

00:20:30,960 --> 00:20:34,080
just client c use more and more threads

00:20:33,120 --> 00:20:36,159
so more more

00:20:34,080 --> 00:20:37,120
more threads waiting for resources and

00:20:36,159 --> 00:20:39,600
just

00:20:37,120 --> 00:20:41,440
use all the threads from this red pool

00:20:39,600 --> 00:20:44,799
and as a result client a

00:20:41,440 --> 00:20:47,679
client b cannot obtain their

00:20:44,799 --> 00:20:50,159
threats from that pool as well so

00:20:47,679 --> 00:20:51,760
instead of that it makes sense to

00:20:50,159 --> 00:20:53,520
separate the threat pool that every

00:20:51,760 --> 00:20:55,520
client has their own one

00:20:53,520 --> 00:20:58,080
and in this case even the clients here

00:20:55,520 --> 00:21:01,440
inside vc has a problem and clients here

00:20:58,080 --> 00:21:02,000
has no uh any threats from from that

00:21:01,440 --> 00:21:05,760
pool

00:21:02,000 --> 00:21:05,760
the client inc ib still works

00:21:07,600 --> 00:21:11,760
the next anti-pattern as my my favorite

00:21:10,000 --> 00:21:15,600
one is

00:21:11,760 --> 00:21:18,880
accessing to shared storage

00:21:15,600 --> 00:21:21,360
so what happens in this case uh

00:21:18,880 --> 00:21:22,960
it's very very typical for the customer

00:21:21,360 --> 00:21:23,600
who split in manually to microservices

00:21:22,960 --> 00:21:26,799
as a split

00:21:23,600 --> 00:21:29,280
api speaking code but

00:21:26,799 --> 00:21:31,120
the most important is the splitting data

00:21:29,280 --> 00:21:34,400
it's not trivial task is quite

00:21:31,120 --> 00:21:37,679
difficult and the customers

00:21:34,400 --> 00:21:40,080
are just uh

00:21:37,679 --> 00:21:41,679
keep it so keep the microservices

00:21:40,080 --> 00:21:42,159
success and system database and even

00:21:41,679 --> 00:21:45,280
worse

00:21:42,159 --> 00:21:46,960
they use drawing between the tables so

00:21:45,280 --> 00:21:49,039
the one microservices

00:21:46,960 --> 00:21:51,520
make a join for the table of other

00:21:49,039 --> 00:21:53,600
microservices it's really a bad idea

00:21:51,520 --> 00:21:56,080
but idea for two reasons because of the

00:21:53,600 --> 00:21:58,159
teams should share data models so if the

00:21:56,080 --> 00:22:01,360
data model change from one team the next

00:21:58,159 --> 00:22:03,360
other team should adapt the things and

00:22:01,360 --> 00:22:05,760
the second reason why it's a bad um

00:22:03,360 --> 00:22:08,960
because

00:22:05,760 --> 00:22:12,159
it's a runtime imagine the customer

00:22:08,960 --> 00:22:13,280
table has some lock or you forget to add

00:22:12,159 --> 00:22:15,360
the index

00:22:13,280 --> 00:22:16,480
and the card microservice immediately

00:22:15,360 --> 00:22:18,480
will uh

00:22:16,480 --> 00:22:22,080
depend on that and influenced by that as

00:22:18,480 --> 00:22:25,120
well so affected by that by this problem

00:22:22,080 --> 00:22:28,320
uh how to resolve this uh the

00:22:25,120 --> 00:22:31,280
some approaches but one of them is

00:22:28,320 --> 00:22:32,880
uh you do it on the ipi level so kind of

00:22:31,280 --> 00:22:36,000
ipa composer

00:22:32,880 --> 00:22:38,840
in this case you just

00:22:36,000 --> 00:22:40,080
get necessary information from the

00:22:38,840 --> 00:22:41,919
service

00:22:40,080 --> 00:22:43,679
for example customer needs some

00:22:41,919 --> 00:22:45,600
information from the cart and api is

00:22:43,679 --> 00:22:46,640
responsible api composer is possible to

00:22:45,600 --> 00:22:49,840
get it from the

00:22:46,640 --> 00:22:51,520
api level and send through the ipis

00:22:49,840 --> 00:22:52,720
information to the microservice which

00:22:51,520 --> 00:22:55,200
works quite well

00:22:52,720 --> 00:22:56,799
if you have not a huge amount

00:22:55,200 --> 00:22:59,760
information and said not

00:22:56,799 --> 00:23:01,520
change it's very very very very often so

00:22:59,760 --> 00:23:05,440
more or less stable

00:23:01,520 --> 00:23:07,440
if the information changed very quickly

00:23:05,440 --> 00:23:09,440
then perhaps make sense to implement

00:23:07,440 --> 00:23:13,440
other solutions and the

00:23:09,440 --> 00:23:14,880
microservices uh reflect all changes in

00:23:13,440 --> 00:23:17,440
the queue so it will just

00:23:14,880 --> 00:23:18,480
uh make some updates in the card

00:23:17,440 --> 00:23:20,880
database

00:23:18,480 --> 00:23:22,400
and publish these updates into the queue

00:23:20,880 --> 00:23:23,039
and custom microservice listing for the

00:23:22,400 --> 00:23:25,360
queue and

00:23:23,039 --> 00:23:26,240
build by basically copy of the card

00:23:25,360 --> 00:23:30,320
table

00:23:26,240 --> 00:23:32,240
and push these changes there uh

00:23:30,320 --> 00:23:33,919
in this case of course there's eventual

00:23:32,240 --> 00:23:35,520
consistencies i'd say

00:23:33,919 --> 00:23:37,200
of course the changes will propagated

00:23:35,520 --> 00:23:39,679
not uh immediately

00:23:37,200 --> 00:23:41,279
it's with some delay and possible to get

00:23:39,679 --> 00:23:44,400
a stale data from the customer

00:23:41,279 --> 00:23:46,799
depends on your use case is it okay not

00:23:44,400 --> 00:23:49,440
and one step further to implement

00:23:46,799 --> 00:23:50,880
implement a kind of event sourcing

00:23:49,440 --> 00:23:52,880
event sourcing quite interesting

00:23:50,880 --> 00:23:54,880
patterns so in this case you don't

00:23:52,880 --> 00:23:56,159
you're not only interested in the target

00:23:54,880 --> 00:23:59,120
state

00:23:56,159 --> 00:24:00,240
in the database in the table but you are

00:23:59,120 --> 00:24:03,279
basically persist

00:24:00,240 --> 00:24:05,440
all steps so you produce kind of

00:24:03,279 --> 00:24:08,320
immutable events so for example card

00:24:05,440 --> 00:24:10,559
created item one added item to edit

00:24:08,320 --> 00:24:13,360
all events immutable you cannot update

00:24:10,559 --> 00:24:16,159
them you can just add the new one

00:24:13,360 --> 00:24:16,960
and you perceive the whole uh stream of

00:24:16,159 --> 00:24:19,600
events so

00:24:16,960 --> 00:24:21,840
basically your master of information so

00:24:19,600 --> 00:24:22,720
the stream of immutable events in this

00:24:21,840 --> 00:24:25,200
case

00:24:22,720 --> 00:24:26,400
if you like for some customers normally

00:24:25,200 --> 00:24:29,440
you publish in the messaging

00:24:26,400 --> 00:24:33,039
system like a kafka or activemcu

00:24:29,440 --> 00:24:35,279
and if the customer is a consumer

00:24:33,039 --> 00:24:36,960
would like us normally we build kind of

00:24:35,279 --> 00:24:39,520
storage materialized view

00:24:36,960 --> 00:24:41,679
and replace these events and they have a

00:24:39,520 --> 00:24:45,039
target state

00:24:41,679 --> 00:24:46,720
when this events event sourcing uh good

00:24:45,039 --> 00:24:48,080
for the cases if you for example need

00:24:46,720 --> 00:24:49,360
the history if you're interested not

00:24:48,080 --> 00:24:51,039
only the target state you need the

00:24:49,360 --> 00:24:52,960
history of events

00:24:51,039 --> 00:24:54,480
they're also very nice works for the

00:24:52,960 --> 00:24:55,679
concurrent modification because you

00:24:54,480 --> 00:24:57,679
normally you have no

00:24:55,679 --> 00:24:59,120
concurrent issues in this case you just

00:24:57,679 --> 00:25:03,360
persist in mutable events

00:24:59,120 --> 00:25:06,080
and a plate disadvantage of course

00:25:03,360 --> 00:25:07,760
the same is some eventual consistency so

00:25:06,080 --> 00:25:10,000
these events

00:25:07,760 --> 00:25:11,520
published take some time to synchronize

00:25:10,000 --> 00:25:14,320
it

00:25:11,520 --> 00:25:14,720
but for some cases or the event sourcing

00:25:14,320 --> 00:25:18,159
works

00:25:14,720 --> 00:25:18,159
works very very very good to know

00:25:19,279 --> 00:25:22,799
let's look uh this is about stability

00:25:21,279 --> 00:25:26,320
anti-patterns let's look at the

00:25:22,799 --> 00:25:28,640
stability patterns as well

00:25:26,320 --> 00:25:31,679
the first and most important from my

00:25:28,640 --> 00:25:35,279
point of view is circuit breaker

00:25:31,679 --> 00:25:38,320
what is the idea of circuit breaker

00:25:35,279 --> 00:25:40,400
idea is the following uh if the target

00:25:38,320 --> 00:25:43,760
system so imagine you have a client

00:25:40,400 --> 00:25:46,559
and the server and the server don't uh

00:25:43,760 --> 00:25:50,320
answer for example for five requests so

00:25:46,559 --> 00:25:50,320
all five requests has a timeout

00:25:50,400 --> 00:25:57,039
then you make a assumption that very

00:25:54,559 --> 00:25:57,440
very probably the server doesn't answer

00:25:57,039 --> 00:25:59,760
for

00:25:57,440 --> 00:26:01,600
will not answer so for six for request

00:25:59,760 --> 00:26:04,240
number six

00:26:01,600 --> 00:26:05,760
and therefore you say okay now circuit

00:26:04,240 --> 00:26:08,960
breaker is active

00:26:05,760 --> 00:26:09,279
after five or requests are failed and

00:26:08,960 --> 00:26:10,799
all

00:26:09,279 --> 00:26:12,480
other requests will be rejected

00:26:10,799 --> 00:26:14,480
immediately so you don't communicate

00:26:12,480 --> 00:26:16,400
with the target server at all

00:26:14,480 --> 00:26:18,080
and what you do either you return the

00:26:16,400 --> 00:26:19,360
failure on the client immediately but

00:26:18,080 --> 00:26:22,400
without timeout

00:26:19,360 --> 00:26:22,960
immediately or you make some fallback

00:26:22,400 --> 00:26:24,799
solution

00:26:22,960 --> 00:26:26,880
so for example typical case in the

00:26:24,799 --> 00:26:29,600
ecommerce you have a prices

00:26:26,880 --> 00:26:30,640
yeah and nice to communicate with the

00:26:29,600 --> 00:26:32,000
price servers

00:26:30,640 --> 00:26:33,679
but if the price service is not

00:26:32,000 --> 00:26:34,240
available you just get the price from

00:26:33,679 --> 00:26:36,640
the cash

00:26:34,240 --> 00:26:38,480
perhaps it gets still but your system

00:26:36,640 --> 00:26:40,480
can continue to work

00:26:38,480 --> 00:26:42,720
why circuit breaker is so important and

00:26:40,480 --> 00:26:46,000
so nice uh from my point of view

00:26:42,720 --> 00:26:47,279
as we also see the three states of the

00:26:46,000 --> 00:26:49,679
circuit breaker

00:26:47,279 --> 00:26:50,720
normal is a in closed state as a normal

00:26:49,679 --> 00:26:53,919
function the

00:26:50,720 --> 00:26:56,720
external server response

00:26:53,919 --> 00:26:57,520
if there's normally configuration of

00:26:56,720 --> 00:27:00,320
threshold

00:26:57,520 --> 00:27:01,200
so if you exceed this threshold reach

00:27:00,320 --> 00:27:03,760
the threshold

00:27:01,200 --> 00:27:04,799
circuit breaker goes into the open state

00:27:03,760 --> 00:27:07,039
and

00:27:04,799 --> 00:27:08,559
all requests are rejected immediately so

00:27:07,039 --> 00:27:12,080
either you return the fallback

00:27:08,559 --> 00:27:14,480
or hero after configure it time

00:27:12,080 --> 00:27:16,880
you try again the external system and

00:27:14,480 --> 00:27:19,039
even is it available again

00:27:16,880 --> 00:27:20,799
uh you goes to the closed state if it

00:27:19,039 --> 00:27:21,520
not available return back to the open

00:27:20,799 --> 00:27:23,440
state so

00:27:21,520 --> 00:27:25,200
this is strictly breaker works why it's

00:27:23,440 --> 00:27:27,600
so good it's so important

00:27:25,200 --> 00:27:29,279
uh because even the small timeout

00:27:27,600 --> 00:27:31,039
sometimes it's critical for the system

00:27:29,279 --> 00:27:32,880
so you have a high load

00:27:31,039 --> 00:27:35,039
and every request to trade in timeout is

00:27:32,880 --> 00:27:36,320
not acceptable sql breaker to help help

00:27:35,039 --> 00:27:40,640
just fail fast

00:27:36,320 --> 00:27:42,480
or provide a fallback solution another

00:27:40,640 --> 00:27:43,840
feature effect of circuit breakers it's

00:27:42,480 --> 00:27:46,640
also very important for me

00:27:43,840 --> 00:27:48,320
it selects the target server so normally

00:27:46,640 --> 00:27:48,880
why the target server has a problem

00:27:48,320 --> 00:27:51,279
because

00:27:48,880 --> 00:27:52,399
a lot big load for example yeah and if

00:27:51,279 --> 00:27:53,679
you make it a choice you make

00:27:52,399 --> 00:27:55,600
problemation worse

00:27:53,679 --> 00:27:56,880
and circuit breaker stops communication

00:27:55,600 --> 00:28:02,320
so the target server

00:27:56,880 --> 00:28:06,799
has a possibility has an option to

00:28:02,320 --> 00:28:11,039
so to be honest again to

00:28:06,799 --> 00:28:14,799
restart or replace the things and

00:28:11,039 --> 00:28:14,799
you give a chance to target what you

00:28:14,840 --> 00:28:18,399
recover

00:28:16,880 --> 00:28:20,320
it's fundamental pattern to protect your

00:28:18,399 --> 00:28:22,320
integration points

00:28:20,320 --> 00:28:24,240
it makes sense also to configure alarm

00:28:22,320 --> 00:28:25,840
and when it forms a circuit breaker

00:28:24,240 --> 00:28:27,760
activation you see on the monitoring

00:28:25,840 --> 00:28:29,200
system if circuit breakers actives and

00:28:27,760 --> 00:28:31,919
very probably something happens with the

00:28:29,200 --> 00:28:33,679
target system is communicating too

00:28:31,919 --> 00:28:35,279
uh there are a lot of configuration

00:28:33,679 --> 00:28:37,360
parameters you see you can configure a

00:28:35,279 --> 00:28:38,720
threshold you can configure a timeout

00:28:37,360 --> 00:28:40,880
configure number of threads you

00:28:38,720 --> 00:28:42,960
configure the time

00:28:40,880 --> 00:28:44,240
and second breakup goes into half often

00:28:42,960 --> 00:28:46,880
state

00:28:44,240 --> 00:28:47,679
uh it should make sense to configure for

00:28:46,880 --> 00:28:49,600
endpoints

00:28:47,679 --> 00:28:51,679
manually and tune a bit so there's a

00:28:49,600 --> 00:28:52,960
different load with different endpoints

00:28:51,679 --> 00:28:55,840
and the parameters should be

00:28:52,960 --> 00:28:57,600
configured carefully and there's nice

00:28:55,840 --> 00:28:59,360
circuit breaker so part implementation

00:28:57,600 --> 00:29:00,960
like uh

00:28:59,360 --> 00:29:03,120
netflix histics for example is not

00:29:00,960 --> 00:29:06,159
supported anymore but it's still

00:29:03,120 --> 00:29:06,559
works very stable i still use it or you

00:29:06,159 --> 00:29:08,480
can

00:29:06,559 --> 00:29:09,600
take its explanation from the resilience

00:29:08,480 --> 00:29:13,840
for j if you

00:29:09,600 --> 00:29:16,240
like to have the last things

00:29:13,840 --> 00:29:17,440
i will make a short demo how to quit

00:29:16,240 --> 00:29:22,559
breakers works

00:29:17,440 --> 00:29:22,559
just change my screen sharing

00:29:39,039 --> 00:29:43,039
so i have a two services very simple

00:29:42,480 --> 00:29:45,919
case

00:29:43,039 --> 00:29:47,600
uh the composite server is just called

00:29:45,919 --> 00:29:50,159
the basic service

00:29:47,600 --> 00:29:51,440
and composite service uh is protected by

00:29:50,159 --> 00:29:53,760
histrix uh

00:29:51,440 --> 00:29:56,000
circuit breaker so it's a configuration

00:29:53,760 --> 00:29:59,440
of historics it's a quick breaker this

00:29:56,000 --> 00:29:59,440
fallback and

00:29:59,520 --> 00:30:04,720
that pool command key configuration so i

00:30:02,000 --> 00:30:04,720
just start

00:30:07,200 --> 00:30:10,799
basic application first

00:30:20,000 --> 00:30:23,840
and start the compost application as

00:30:21,760 --> 00:30:23,840
well

00:30:35,039 --> 00:30:37,840
okay perfect

00:30:39,200 --> 00:30:43,279
now um i make it just a call of the

00:30:42,240 --> 00:30:46,640
composite service

00:30:43,279 --> 00:30:48,559
and let's see

00:30:46,640 --> 00:30:50,080
i want to return back the first call

00:30:48,559 --> 00:30:51,679
takes some time

00:30:50,080 --> 00:30:54,080
here it has a timeout but now it's

00:30:51,679 --> 00:30:54,080
answered

00:30:54,880 --> 00:30:58,559
let's just return the response from the

00:30:57,120 --> 00:31:02,720
basic service

00:30:58,559 --> 00:31:05,840
and what i do now i stop

00:31:02,720 --> 00:31:05,840
basic service

00:31:13,039 --> 00:31:16,720
but you see it returns hysterics time

00:31:14,960 --> 00:31:19,919
out exception and it's weight sometimes

00:31:16,720 --> 00:31:21,600
it has a big delay but after some

00:31:19,919 --> 00:31:23,600
invocation

00:31:21,600 --> 00:31:25,600
now second break is open you see i have

00:31:23,600 --> 00:31:27,840
immediately response

00:31:25,600 --> 00:31:29,919
i see the exception that circuit breaker

00:31:27,840 --> 00:31:34,159
is open

00:31:29,919 --> 00:31:37,360
and if i wait some time uh it will retry

00:31:34,159 --> 00:31:40,000
and has a timeout again

00:31:37,360 --> 00:31:42,399
progresses about 10 seconds yeah now it

00:31:40,000 --> 00:31:47,840
has the time out again

00:31:42,399 --> 00:31:47,840
and if i start now my basic service

00:32:03,120 --> 00:32:07,840
so it works again so this is a circuit

00:32:06,080 --> 00:32:09,039
breakers box very important this here

00:32:07,840 --> 00:32:11,279
now so i

00:32:09,039 --> 00:32:12,320
have immediate response either this year

00:32:11,279 --> 00:32:14,559
or i can

00:32:12,320 --> 00:32:16,159
provide some default fallback response

00:32:14,559 --> 00:32:16,880
for example take the things from the

00:32:16,159 --> 00:32:19,120
cache

00:32:16,880 --> 00:32:21,440
subject like this this is fundamental

00:32:19,120 --> 00:32:24,799
pattern which i really recommend to look

00:32:21,440 --> 00:32:30,559
to make your system resilient and stable

00:32:24,799 --> 00:32:30,559
so i say again my

00:32:40,840 --> 00:32:43,840
presentation

00:33:00,080 --> 00:33:06,000
so how um integrate

00:33:03,200 --> 00:33:06,640
hysterics in springboot um it's quite

00:33:06,000 --> 00:33:09,360
easy

00:33:06,640 --> 00:33:10,320
you can just create a starter at this

00:33:09,360 --> 00:33:13,279
cloud starter

00:33:10,320 --> 00:33:14,559
in the uh spring boot and provide

00:33:13,279 --> 00:33:16,399
annotation

00:33:14,559 --> 00:33:18,399
it's also possible to integrate with

00:33:16,399 --> 00:33:21,760
visits xf

00:33:18,399 --> 00:33:25,039
uh n6f is a bit more work but still

00:33:21,760 --> 00:33:27,679
quite quite easy just wrap

00:33:25,039 --> 00:33:28,720
your call of the service into the

00:33:27,679 --> 00:33:31,120
hystics command

00:33:28,720 --> 00:33:32,720
create hystics command with a

00:33:31,120 --> 00:33:36,240
configuration

00:33:32,720 --> 00:33:39,919
and just wrap your your call here

00:33:36,240 --> 00:33:41,679
and you either make a um execution

00:33:39,919 --> 00:33:42,960
synchronous execution or asynchronous

00:33:41,679 --> 00:33:45,519
use in the future

00:33:42,960 --> 00:33:46,480
or you can also make a reactive with

00:33:45,519 --> 00:33:50,240
some handler

00:33:46,480 --> 00:33:52,960
call handler and receive the response

00:33:50,240 --> 00:33:56,399
so it's very nice integrated in the 6f

00:33:52,960 --> 00:34:00,480
and it's been booted even easier

00:33:56,399 --> 00:34:00,480
um the next pattern is a bulkhead

00:34:01,279 --> 00:34:07,919
this is a pattern is protect

00:34:05,200 --> 00:34:09,520
horizontal propagation of the euro this

00:34:07,919 --> 00:34:11,200
could be applied at different levels i

00:34:09,520 --> 00:34:13,119
really recommend to do it always in the

00:34:11,200 --> 00:34:15,200
connection pool there are third pool

00:34:13,119 --> 00:34:16,960
but sometimes it even makes sense to do

00:34:15,200 --> 00:34:19,359
it on the container level that's

00:34:16,960 --> 00:34:20,159
for example some important clients very

00:34:19,359 --> 00:34:22,320
important

00:34:20,159 --> 00:34:24,879
clients dedicated to the separate

00:34:22,320 --> 00:34:27,359
service instances

00:34:24,879 --> 00:34:28,879
so client two and five three are very

00:34:27,359 --> 00:34:30,560
let's say it's very important they have

00:34:28,879 --> 00:34:32,240
dedicated service instances only for

00:34:30,560 --> 00:34:34,159
these clients and even you have

00:34:32,240 --> 00:34:36,000
problem because of client requests

00:34:34,159 --> 00:34:37,839
declared on the others

00:34:36,000 --> 00:34:39,040
the client two clients we still continue

00:34:37,839 --> 00:34:43,679
to work so sometimes

00:34:39,040 --> 00:34:47,119
make sense to bulkhead and

00:34:43,679 --> 00:34:50,480
route some customers to the dedicated

00:34:47,119 --> 00:34:54,079
instance pool of the service

00:34:50,480 --> 00:34:56,879
um surveillance communication is also

00:34:54,079 --> 00:34:58,240
nice pattern that i always recommend to

00:34:56,879 --> 00:35:00,240
use

00:34:58,240 --> 00:35:02,800
the synchronous communication is nice

00:35:00,240 --> 00:35:04,160
and really easy to implement

00:35:02,800 --> 00:35:05,680
but uh the problems occurrence

00:35:04,160 --> 00:35:06,000
communication that this service should

00:35:05,680 --> 00:35:08,880
should

00:35:06,000 --> 00:35:09,599
wait uh for the response and in some

00:35:08,880 --> 00:35:11,359
cases there

00:35:09,599 --> 00:35:12,640
has a blocked thread of course we have

00:35:11,359 --> 00:35:16,079
with a new

00:35:12,640 --> 00:35:18,960
communication we can make a thread three

00:35:16,079 --> 00:35:20,240
and provide a resume for the resume

00:35:18,960 --> 00:35:24,400
response later

00:35:20,240 --> 00:35:27,760
but still your service depends uh on the

00:35:24,400 --> 00:35:29,760
uh service too and the response

00:35:27,760 --> 00:35:31,440
our asynchronous communication helps to

00:35:29,760 --> 00:35:33,920
resolve this problem for example

00:35:31,440 --> 00:35:36,079
if uh the throughput of services service

00:35:33,920 --> 00:35:39,280
to have some slowdown and cannot

00:35:36,079 --> 00:35:39,680
proceed all messages was sent uh in this

00:35:39,280 --> 00:35:41,359
case

00:35:39,680 --> 00:35:43,119
message is just accumulated in the queue

00:35:41,359 --> 00:35:45,040
and when service 2 is

00:35:43,119 --> 00:35:46,880
available again it's just proceed the

00:35:45,040 --> 00:35:49,920
messages and even the services

00:35:46,880 --> 00:35:51,760
service 2 is completely down you still

00:35:49,920 --> 00:35:52,160
the service wants to continue to works

00:35:51,760 --> 00:35:54,320
and

00:35:52,160 --> 00:35:56,240
i'll just just put the messages to the

00:35:54,320 --> 00:35:59,040
queue of course it

00:35:56,240 --> 00:35:59,359
only for solution for sometimes uh if

00:35:59,040 --> 00:36:02,000
you

00:35:59,359 --> 00:36:02,800
if it's always down in one day you have

00:36:02,000 --> 00:36:04,960
over

00:36:02,800 --> 00:36:06,640
so overload overwhelm of the queue and

00:36:04,960 --> 00:36:09,440
still have a problem

00:36:06,640 --> 00:36:11,520
but for some time for peak loads and for

00:36:09,440 --> 00:36:14,160
short

00:36:11,520 --> 00:36:17,040
outages is really helpful to have a

00:36:14,160 --> 00:36:20,960
severance communication

00:36:17,040 --> 00:36:21,440
um as i said though it's really helpful

00:36:20,960 --> 00:36:23,920
for

00:36:21,440 --> 00:36:24,800
downward spread spiking load and it

00:36:23,920 --> 00:36:26,880
opens also

00:36:24,800 --> 00:36:28,160
some other communication patterns like

00:36:26,880 --> 00:36:30,960
pops up and you publish

00:36:28,160 --> 00:36:31,440
and propagate the message to broadcast

00:36:30,960 --> 00:36:34,000
uh

00:36:31,440 --> 00:36:36,480
for for some consumers not only with a

00:36:34,000 --> 00:36:36,480
single one

00:36:36,800 --> 00:36:40,800
uh unfortunately as well as

00:36:38,000 --> 00:36:44,079
communication is not a freelance

00:36:40,800 --> 00:36:44,800
you need normally to redesign a business

00:36:44,079 --> 00:36:47,680
system

00:36:44,800 --> 00:36:49,359
different architecture style also

00:36:47,680 --> 00:36:50,640
sometimes it implies a business rule

00:36:49,359 --> 00:36:53,119
that not

00:36:50,640 --> 00:36:55,359
answers immediately or make some

00:36:53,119 --> 00:36:57,760
eventual consistency

00:36:55,359 --> 00:36:58,480
and of course adds complexity to tracing

00:36:57,760 --> 00:37:01,440
and debugging

00:36:58,480 --> 00:37:03,520
and necessary to normally administrate

00:37:01,440 --> 00:37:08,560
and set up additional messaging

00:37:03,520 --> 00:37:12,320
broker like a kafka repertoire

00:37:08,560 --> 00:37:15,119
um as a with conclusion

00:37:12,320 --> 00:37:17,760
what is the stability goal is so the

00:37:15,119 --> 00:37:20,880
typical situation then we have a failure

00:37:17,760 --> 00:37:23,280
some external servers stopped responding

00:37:20,880 --> 00:37:25,200
threatpool exhausted and system

00:37:23,280 --> 00:37:25,680
architect implies the failure so it

00:37:25,200 --> 00:37:29,359
makes

00:37:25,680 --> 00:37:32,000
basically the problem uh critical

00:37:29,359 --> 00:37:33,440
and the system stop to responds and no

00:37:32,000 --> 00:37:35,839
features are available at all

00:37:33,440 --> 00:37:37,599
this is a weight case what you would

00:37:35,839 --> 00:37:39,920
like to achieve

00:37:37,599 --> 00:37:41,680
that's basically the same uh beginning

00:37:39,920 --> 00:37:44,320
is the same the failure is cigarette the

00:37:41,680 --> 00:37:46,320
external server stopped respawns but the

00:37:44,320 --> 00:37:47,200
system architecture somehow to dump the

00:37:46,320 --> 00:37:49,680
fault so we have

00:37:47,200 --> 00:37:51,520
uses patterns like circuit break

00:37:49,680 --> 00:37:52,960
bulkheads asynchronous communication

00:37:51,520 --> 00:37:56,400
event sourcing to

00:37:52,960 --> 00:37:58,480
dump the fold and perhaps you

00:37:56,400 --> 00:37:59,680
not all hundred percent of the feature

00:37:58,480 --> 00:38:02,880
is available not

00:37:59,680 --> 00:38:03,599
uh all users are served but your system

00:38:02,880 --> 00:38:05,359
still

00:38:03,599 --> 00:38:07,280
works and most of the features are

00:38:05,359 --> 00:38:08,400
available and the system is still still

00:38:07,280 --> 00:38:10,480
useful

00:38:08,400 --> 00:38:11,599
and the rest of the features remind

00:38:10,480 --> 00:38:14,320
unaffected

00:38:11,599 --> 00:38:16,240
and of course quite important to uh

00:38:14,320 --> 00:38:18,960
provide a possibility to recover

00:38:16,240 --> 00:38:22,480
analyze the issue quickly or how to

00:38:18,960 --> 00:38:22,480
recover the things or restart

00:38:22,640 --> 00:38:27,040
the nodes and make this the server

00:38:25,680 --> 00:38:30,000
available in the

00:38:27,040 --> 00:38:32,320
system in the full for 100 functionality

00:38:30,000 --> 00:38:35,119
back again

00:38:32,320 --> 00:38:35,839
so conclusion um the importance should

00:38:35,119 --> 00:38:38,800
understand

00:38:35,839 --> 00:38:39,920
uh as well service axiom for me that is

00:38:38,800 --> 00:38:42,960
not possible

00:38:39,920 --> 00:38:45,280
disregards how good your qa

00:38:42,960 --> 00:38:46,000
is how much test your i mean but you

00:38:45,280 --> 00:38:48,320
still have

00:38:46,000 --> 00:38:49,920
some problems on the production you have

00:38:48,320 --> 00:38:51,599
unreliable networks

00:38:49,920 --> 00:38:53,520
we have dependency of the external

00:38:51,599 --> 00:38:58,000
system payment system

00:38:53,520 --> 00:39:00,400
some data external databases

00:38:58,000 --> 00:39:03,040
cloud providers and outages happens in

00:39:00,400 --> 00:39:03,040
the production

00:39:03,839 --> 00:39:08,800
therefore you need for the system to be

00:39:07,520 --> 00:39:11,440
to be prepared for the fail

00:39:08,800 --> 00:39:13,680
embrace the failures and this could be

00:39:11,440 --> 00:39:16,000
achieved to

00:39:13,680 --> 00:39:19,520
use apply some stability patterns and

00:39:16,000 --> 00:39:22,640
avoid some stability antibiotics

00:39:19,520 --> 00:39:24,480
also important to consider stability not

00:39:22,640 --> 00:39:26,079
when the system already in production is

00:39:24,480 --> 00:39:27,680
a bit too late

00:39:26,079 --> 00:39:29,680
but in the architecture and design

00:39:27,680 --> 00:39:31,760
phases uh to make sense

00:39:29,680 --> 00:39:34,000
to things a bit how to make your system

00:39:31,760 --> 00:39:37,280
resilient and stable

00:39:34,000 --> 00:39:40,560
and robust this is basically all

00:39:37,280 --> 00:39:44,480
from the presentation perhaps you have

00:39:40,560 --> 00:39:44,480
some questions i would like to

00:39:45,359 --> 00:39:48,000
let's look

00:39:50,240 --> 00:39:53,760
okay these are some comments about my my

00:39:52,560 --> 00:39:55,359
example

00:39:53,760 --> 00:39:57,359
why is the connection not introduced is

00:39:55,359 --> 00:39:58,880
a try connection catch

00:39:57,359 --> 00:40:02,480
uh the problem is not a connect

00:39:58,880 --> 00:40:04,079
connection still still still

00:40:02,480 --> 00:40:06,480
getting this not exception by get

00:40:04,079 --> 00:40:06,880
connection the problem is the connection

00:40:06,480 --> 00:40:09,440
has

00:40:06,880 --> 00:40:11,119
a limited number of session and if i try

00:40:09,440 --> 00:40:14,240
to get a session it just hangs

00:40:11,119 --> 00:40:16,720
so the try catch don't really helps in

00:40:14,240 --> 00:40:16,720
this case

00:40:20,800 --> 00:40:25,040
yeah so it makes yeah so close makes

00:40:22,839 --> 00:40:27,520
sense

00:40:25,040 --> 00:40:28,240
oh it's a good good question how do you

00:40:27,520 --> 00:40:29,839
test

00:40:28,240 --> 00:40:32,720
your resiliency measures across

00:40:29,839 --> 00:40:32,720
microservices

00:40:33,839 --> 00:40:39,440
so normally i have i do it on the either

00:40:37,280 --> 00:40:41,440
the integration test level on system

00:40:39,440 --> 00:40:43,760
test level it's quite difficult to

00:40:41,440 --> 00:40:45,040
do it on the unit test because you mock

00:40:43,760 --> 00:40:46,160
normal external system by two

00:40:45,040 --> 00:40:48,880
integration tests

00:40:46,160 --> 00:40:50,640
and system tests you can uh simulate

00:40:48,880 --> 00:40:52,800
some some some problems on the external

00:40:50,640 --> 00:40:55,119
system for example

00:40:52,800 --> 00:40:57,839
if system not available and return or

00:40:55,119 --> 00:41:01,280
failure immediately is not a problem

00:40:57,839 --> 00:41:04,400
but the worst case if this external

00:41:01,280 --> 00:41:06,880
system hangs so don't return anything

00:41:04,400 --> 00:41:07,760
and this is a nice case you just put

00:41:06,880 --> 00:41:09,760
your timeout

00:41:07,760 --> 00:41:11,440
well there's a big big time out on the

00:41:09,760 --> 00:41:14,480
on the server side simulate

00:41:11,440 --> 00:41:15,760
uh the noticeable hanging on the

00:41:14,480 --> 00:41:18,960
external system

00:41:15,760 --> 00:41:21,760
and try to uh increase the load uh

00:41:18,960 --> 00:41:22,240
on your method for example in your

00:41:21,760 --> 00:41:25,680
entire

00:41:22,240 --> 00:41:29,280
endpoint and see what happens iso system

00:41:25,680 --> 00:41:33,200
also hangs and beginning to respond

00:41:29,280 --> 00:41:36,000
very slowly or completely hangs

00:41:33,200 --> 00:41:37,760
or it provides some useful fallbacks or

00:41:36,000 --> 00:41:40,240
at least fail faster

00:41:37,760 --> 00:41:40,800
fail faster is even better as a hanging

00:41:40,240 --> 00:41:44,000
system

00:41:40,800 --> 00:41:46,800
at least the customers see some errors

00:41:44,000 --> 00:41:48,640
and perhaps there some percentage of the

00:41:46,800 --> 00:41:52,079
customer still can

00:41:48,640 --> 00:41:55,680
works but 30 or 50 percent of customers

00:41:52,079 --> 00:41:55,680
will receive our euros quickly

00:42:00,079 --> 00:42:07,359
some other questions

00:42:03,440 --> 00:42:09,119
okay thanks uh

00:42:07,359 --> 00:42:10,640
yeah called easy listen for jay yeah

00:42:09,119 --> 00:42:11,280
he's excited applicate i already say

00:42:10,640 --> 00:42:13,359
that yes

00:42:11,280 --> 00:42:15,520
statistics not supported anymore but

00:42:13,359 --> 00:42:17,119
still it's very stable a good solution

00:42:15,520 --> 00:42:20,319
very good documented

00:42:17,119 --> 00:42:23,440
so i still use the histics in my uh

00:42:20,319 --> 00:42:26,640
system if i have no good reasons to

00:42:23,440 --> 00:42:28,400
uh replace it yes uh this uh i already

00:42:26,640 --> 00:42:30,079
decided seriously in sport js it's

00:42:28,400 --> 00:42:33,520
basically a follow follow

00:42:30,079 --> 00:42:36,160
or system observations but um

00:42:33,520 --> 00:42:36,880
i like east x implementation i still use

00:42:36,160 --> 00:42:39,680
it so

00:42:36,880 --> 00:42:40,640
why not even if it's not supported but

00:42:39,680 --> 00:42:43,040
if it's stable

00:42:40,640 --> 00:42:45,839
good document it works it's also an

00:42:43,040 --> 00:42:45,839
option

00:42:47,920 --> 00:42:55,359
uh some other questions

00:42:52,319 --> 00:42:59,520
looks very similar to cluster and not

00:42:55,359 --> 00:43:02,800
really so as um

00:42:59,520 --> 00:43:05,520
high availability cluster uh just

00:43:02,800 --> 00:43:07,200
distribute your requests to to another

00:43:05,520 --> 00:43:09,760
instance this is idea of

00:43:07,200 --> 00:43:11,680
a high availability cluster the circuit

00:43:09,760 --> 00:43:14,720
breaker

00:43:11,680 --> 00:43:16,240
recognize is basically similar behavior

00:43:14,720 --> 00:43:19,680
like circuit breaker in your

00:43:16,240 --> 00:43:21,920
elect so electrochain if you have some

00:43:19,680 --> 00:43:21,920
uh

00:43:22,800 --> 00:43:28,079
short circuit it's just

00:43:26,240 --> 00:43:30,240
breaks the connection completely it

00:43:28,079 --> 00:43:30,800
happens with circuit breaker so you stop

00:43:30,240 --> 00:43:32,640
the

00:43:30,800 --> 00:43:33,920
communication with external system stop

00:43:32,640 --> 00:43:36,960
stops completely

00:43:33,920 --> 00:43:37,920
and either provide a full brick or your

00:43:36,960 --> 00:43:41,839
own

00:43:37,920 --> 00:43:41,839
in this case

00:43:42,560 --> 00:43:46,640
okay any any more questions

00:43:50,480 --> 00:43:56,720
i think already a bit out of time

00:43:53,599 --> 00:43:59,119
25 45 minutes yeah

00:43:56,720 --> 00:44:00,240
okay so thanks a lot for your attention

00:43:59,119 --> 00:44:02,560
and um

00:44:00,240 --> 00:44:04,079
if you have some questions during the

00:44:02,560 --> 00:44:06,880
conference i will be available

00:44:04,079 --> 00:44:08,480
uh tomorrow and on thursday you can also

00:44:06,880 --> 00:44:13,359
ask me uh ping me

00:44:08,480 --> 00:44:13,359
ask me find me on link andy link it in

00:44:14,720 --> 00:44:23,839
so thanks a lot and have a nice day

00:44:18,160 --> 00:44:23,839
the rest of conference and bye

00:44:43,599 --> 00:44:45,680

YouTube URL: https://www.youtube.com/watch?v=D1I17-2lJvs


