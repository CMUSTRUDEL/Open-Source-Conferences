Title: Change Data Capture with Flink SQL and Debezium
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Change Data Capture with Flink SQL and Debezium
Marta Paes

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Change Data Capture (CDC) has become the standard to capture and propagate committed changes from a database to downstream consumers, for example to keep multiple datastores in sync and avoid common pitfalls such as dual writes (remember? "Friends don't let friends do dual writes"). Consuming these changelogs with Apache Flink used to be a pain, but the latest release (Flink 1.11) introduced not only support for CDC, but support for CDC from the comfort of your SQL couch. In this talk, we'll demo how to use Flink SQL to easily process database changelog data generated with Debezium. About the speaker(s):

Marta is a Developer Advocate at Ververica (formerly data Artisans) and a contributor to Apache Flink. After finding her mojo in open source, she is committed to making sense of Data Engineering through the eyes of those using its by-products. Marta holds a Masterâ€™s in Biomedical Engineering, where she developed a particular taste for multi-dimensional data visualization, and previously worked as a Data Warehouse Engineer at Zalando and Accenture.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:26,640 --> 00:00:30,080
okay

00:00:27,199 --> 00:00:31,760
so i guess i'll just get started hi

00:00:30,080 --> 00:00:34,160
everyone thanks for

00:00:31,760 --> 00:00:35,040
uh taking the time to attend this talk

00:00:34,160 --> 00:00:37,440
today or

00:00:35,040 --> 00:00:38,879
tonight uh here in berlin it's already

00:00:37,440 --> 00:00:41,840
tonight so

00:00:38,879 --> 00:00:43,040
uh i'm marta i'm a developer advocate at

00:00:41,840 --> 00:00:44,800
riverica

00:00:43,040 --> 00:00:47,039
and today i want to talk to you a little

00:00:44,800 --> 00:00:47,680
bit about change data capture with flink

00:00:47,039 --> 00:00:50,960
sequel

00:00:47,680 --> 00:00:52,879
and so

00:00:50,960 --> 00:00:55,680
to start with for those of you who have

00:00:52,879 --> 00:00:57,199
never heard about ververica

00:00:55,680 --> 00:00:59,760
you might know it as a company of the

00:00:57,199 --> 00:01:03,280
people who created apache flink

00:00:59,760 --> 00:01:05,199
which is one of the most active projects

00:01:03,280 --> 00:01:06,560
in the apache software foundation these

00:01:05,199 --> 00:01:08,720
days

00:01:06,560 --> 00:01:10,320
and i'm really fortunate to work every

00:01:08,720 --> 00:01:12,000
day with

00:01:10,320 --> 00:01:13,760
this great community that is just

00:01:12,000 --> 00:01:15,759
growing year over year

00:01:13,760 --> 00:01:18,960
the project is just getting huger and

00:01:15,759 --> 00:01:21,439
huger and more and more interesting so

00:01:18,960 --> 00:01:22,320
it's a really interesting position to be

00:01:21,439 --> 00:01:24,560
in

00:01:22,320 --> 00:01:27,280
and since the beginning of last year the

00:01:24,560 --> 00:01:30,479
company is part of the alibaba group

00:01:27,280 --> 00:01:32,400
who if you also don't know is one of the

00:01:30,479 --> 00:01:34,159
biggest users but also one of the

00:01:32,400 --> 00:01:35,600
biggest contributors to open source

00:01:34,159 --> 00:01:38,000
flank

00:01:35,600 --> 00:01:38,000
today

00:01:38,880 --> 00:01:42,240
so i wanted to start with a bit of the

00:01:40,479 --> 00:01:43,200
motivation behind this talk it's the

00:01:42,240 --> 00:01:45,920
second time i

00:01:43,200 --> 00:01:47,840
give this talk and it's always very um

00:01:45,920 --> 00:01:51,040
very personal topic for me

00:01:47,840 --> 00:01:52,960
because before turning to flink i was

00:01:51,040 --> 00:01:56,159
working as a data warehouse engineer

00:01:52,960 --> 00:01:58,399
and for a couple years and this is

00:01:56,159 --> 00:02:00,640
pretty much sums up what my day-to-day

00:01:58,399 --> 00:02:03,759
work used to look like so

00:02:00,640 --> 00:02:06,399
if you were ever involved in

00:02:03,759 --> 00:02:08,479
building data integration pipelines for

00:02:06,399 --> 00:02:09,280
analytics in a company that might be

00:02:08,479 --> 00:02:13,360
running

00:02:09,280 --> 00:02:15,120
a lot of legacy tech you know that

00:02:13,360 --> 00:02:18,000
there is a lot of ungrateful things

00:02:15,120 --> 00:02:20,000
about being a data warehouse engineer

00:02:18,000 --> 00:02:23,040
and like i said there's a lot of legacy

00:02:20,000 --> 00:02:25,120
technology you have to work with

00:02:23,040 --> 00:02:26,959
i don't want to say names maybe but

00:02:25,120 --> 00:02:29,120
there's a lot of technical depth

00:02:26,959 --> 00:02:30,480
um there's a lot of late we'll just fix

00:02:29,120 --> 00:02:32,800
it later data

00:02:30,480 --> 00:02:33,599
and with time you kind of just learn to

00:02:32,800 --> 00:02:37,120
live with it

00:02:33,599 --> 00:02:38,400
even though it leads to like pretty bad

00:02:37,120 --> 00:02:41,840
results and a lot of pain

00:02:38,400 --> 00:02:45,200
in your life so my

00:02:41,840 --> 00:02:45,920
everyday uh went a bit like this we had

00:02:45,200 --> 00:02:48,400
this

00:02:45,920 --> 00:02:50,640
monolithic application backed by some

00:02:48,400 --> 00:02:52,560
transactional databases for example if

00:02:50,640 --> 00:02:55,599
you had like some

00:02:52,560 --> 00:02:58,319
order system and you would write these

00:02:55,599 --> 00:02:59,440
really heavyweight etl jobs to integrate

00:02:58,319 --> 00:03:01,840
all this data

00:02:59,440 --> 00:03:02,879
in richard and so on to actually make it

00:03:01,840 --> 00:03:05,599
useful

00:03:02,879 --> 00:03:08,000
for your stakeholders for some specific

00:03:05,599 --> 00:03:11,360
business purpose

00:03:08,000 --> 00:03:14,560
and in this universe databases

00:03:11,360 --> 00:03:18,400
were and are a lot still these days

00:03:14,560 --> 00:03:20,400
um seen as these static sources of

00:03:18,400 --> 00:03:22,720
data records so it's like this

00:03:20,400 --> 00:03:24,720
collection of states

00:03:22,720 --> 00:03:26,159
about your business that is just sitting

00:03:24,720 --> 00:03:28,560
there and it's just waiting to be

00:03:26,159 --> 00:03:28,560
queried

00:03:29,599 --> 00:03:33,120
but in reality what happens if you think

00:03:32,159 --> 00:03:34,879
about it

00:03:33,120 --> 00:03:36,400
most of the data that is stored in your

00:03:34,879 --> 00:03:39,280
transactional databases

00:03:36,400 --> 00:03:39,840
is continuously produced and it it's

00:03:39,280 --> 00:03:42,640
also

00:03:39,840 --> 00:03:44,080
continuously changing so it's the logic

00:03:42,640 --> 00:03:45,920
that you use

00:03:44,080 --> 00:03:47,440
to query it that doesn't change that

00:03:45,920 --> 00:03:49,200
frequent frequently so

00:03:47,440 --> 00:03:50,879
although your data is changing all the

00:03:49,200 --> 00:03:56,480
time you're

00:03:50,879 --> 00:03:56,480
querying it almost always the same way

00:03:56,879 --> 00:04:00,400
and when you start looking at it from

00:03:59,040 --> 00:04:02,720
this perspective

00:04:00,400 --> 00:04:03,760
you seriously start questioning a lot of

00:04:02,720 --> 00:04:07,680
things

00:04:03,760 --> 00:04:10,879
so if you have all these

00:04:07,680 --> 00:04:12,560
dynamic properties about your data then

00:04:10,879 --> 00:04:14,159
why are you giving your stakeholders

00:04:12,560 --> 00:04:17,840
yesterday's data

00:04:14,159 --> 00:04:19,440
and why are you not offloading um

00:04:17,840 --> 00:04:21,519
why are you not offloading the load on

00:04:19,440 --> 00:04:24,000
your database along the day

00:04:21,519 --> 00:04:25,840
instead of having people wake up in the

00:04:24,000 --> 00:04:28,800
middle of the night because one of your

00:04:25,840 --> 00:04:29,759
huge eight hour running query just

00:04:28,800 --> 00:04:32,960
exploded

00:04:29,759 --> 00:04:35,520
because it ran out of memory and

00:04:32,960 --> 00:04:37,520
why are we just letting all this data

00:04:35,520 --> 00:04:42,320
sit there and lose value before we

00:04:37,520 --> 00:04:44,320
actually process it

00:04:42,320 --> 00:04:46,160
and let's be honest this is like was

00:04:44,320 --> 00:04:46,880
probably the last thing on my mind every

00:04:46,160 --> 00:04:50,240
day

00:04:46,880 --> 00:04:52,320
i never woke up thinking oh i

00:04:50,240 --> 00:04:53,600
just can't wait to scan a production

00:04:52,320 --> 00:04:56,400
database for changes

00:04:53,600 --> 00:04:58,160
using 100 line query with a thousand

00:04:56,400 --> 00:04:59,680
business logic conditions

00:04:58,160 --> 00:05:03,280
knowing that this query will probably

00:04:59,680 --> 00:05:03,280
fail at some at some point

00:05:04,720 --> 00:05:10,880
and what i described so far

00:05:07,759 --> 00:05:14,000
is actually change data capture i know

00:05:10,880 --> 00:05:16,160
this is a very also for me i

00:05:14,000 --> 00:05:18,160
i didn't use to associate change data

00:05:16,160 --> 00:05:19,919
capture with something i already did

00:05:18,160 --> 00:05:22,080
uh when i first heard about it i thought

00:05:19,919 --> 00:05:24,639
it was something really fancy

00:05:22,080 --> 00:05:26,880
and something really new but in essence

00:05:24,639 --> 00:05:30,320
what change data capture is really

00:05:26,880 --> 00:05:34,080
is uh pretty simple

00:05:30,320 --> 00:05:36,720
and it's been around for a long time so

00:05:34,080 --> 00:05:37,840
what it is is just tracking and

00:05:36,720 --> 00:05:41,199
propagating

00:05:37,840 --> 00:05:44,160
data changes in database to

00:05:41,199 --> 00:05:44,560
downstream consumers so whenever you get

00:05:44,160 --> 00:05:47,520
an

00:05:44,560 --> 00:05:48,400
insert and update or delete operation in

00:05:47,520 --> 00:05:50,479
your

00:05:48,400 --> 00:05:52,560
transactional databases you want to

00:05:50,479 --> 00:05:54,400
capture this change

00:05:52,560 --> 00:05:58,160
and just make it available to for

00:05:54,400 --> 00:05:58,160
example your analytics workload

00:05:59,199 --> 00:06:02,800
and when done right cdc can be a huge

00:06:01,840 --> 00:06:04,960
enabler

00:06:02,800 --> 00:06:06,960
for different use cases not just i'm i

00:06:04,960 --> 00:06:07,680
will focus more on streaming analytics

00:06:06,960 --> 00:06:09,280
here

00:06:07,680 --> 00:06:10,960
uh because that's kind of my comfort

00:06:09,280 --> 00:06:13,039
zone and it's also

00:06:10,960 --> 00:06:14,560
i think a very interesting use for a

00:06:13,039 --> 00:06:16,639
change that capture

00:06:14,560 --> 00:06:18,080
uh but you can use it for a bunch of

00:06:16,639 --> 00:06:19,759
other things so you can do cache

00:06:18,080 --> 00:06:22,160
invalidation you can do things like

00:06:19,759 --> 00:06:25,440
syncing between different microservices

00:06:22,160 --> 00:06:27,759
and so on and i really recommend that

00:06:25,440 --> 00:06:31,199
you follow the guy that is actually here

00:06:27,759 --> 00:06:34,240
and making me very nervous uh gunnar

00:06:31,199 --> 00:06:36,560
because uh and explore his collection of

00:06:34,240 --> 00:06:38,319
talks to learn more about cdc and all

00:06:36,560 --> 00:06:41,360
these use cases and patterns that

00:06:38,319 --> 00:06:43,199
that you can implement with it uh

00:06:41,360 --> 00:06:45,680
he's the main division guy the

00:06:43,199 --> 00:06:48,400
technology i'll also talk about today

00:06:45,680 --> 00:06:49,360
uh and he really has super great

00:06:48,400 --> 00:06:52,240
insights about

00:06:49,360 --> 00:06:52,240
about this topic

00:06:53,199 --> 00:06:59,120
well about the cdc itself there are

00:06:56,479 --> 00:06:59,840
basically two ways you can go about it

00:06:59,120 --> 00:07:02,479
so

00:06:59,840 --> 00:07:03,680
the first scenario is using what is

00:07:02,479 --> 00:07:07,520
called query

00:07:03,680 --> 00:07:10,000
career-based cdc this is probably what

00:07:07,520 --> 00:07:13,360
you're used to if you come from a heavy

00:07:10,000 --> 00:07:15,919
relational database background like me

00:07:13,360 --> 00:07:16,560
and what it is is you periodically just

00:07:15,919 --> 00:07:19,599
pull

00:07:16,560 --> 00:07:20,080
your database for changes and each time

00:07:19,599 --> 00:07:23,280
you just

00:07:20,080 --> 00:07:24,960
ask did anything change since the last

00:07:23,280 --> 00:07:27,520
time i checked

00:07:24,960 --> 00:07:28,560
and this has its advantages for some use

00:07:27,520 --> 00:07:32,160
cases like it's

00:07:28,560 --> 00:07:34,319
much simpler and involves much less work

00:07:32,160 --> 00:07:37,039
um if you want to do something like

00:07:34,319 --> 00:07:38,319
track say a slowly changing dimension

00:07:37,039 --> 00:07:40,479
for example

00:07:38,319 --> 00:07:42,319
but for most cases where you want to

00:07:40,479 --> 00:07:44,080
react to data as soon as possible and

00:07:42,319 --> 00:07:47,599
you have you want to have like a really

00:07:44,080 --> 00:07:50,319
wide and complete overview

00:07:47,599 --> 00:07:51,440
of all the changes that happen in your

00:07:50,319 --> 00:07:54,639
database

00:07:51,440 --> 00:07:56,160
uh it carries some problems so the

00:07:54,639 --> 00:07:58,400
problems with doing

00:07:56,160 --> 00:08:00,720
query based cdc is that some data

00:07:58,400 --> 00:08:01,599
changes might get lost in between these

00:08:00,720 --> 00:08:04,560
falls so

00:08:01,599 --> 00:08:05,280
imagine if you are querying or polling

00:08:04,560 --> 00:08:08,319
your

00:08:05,280 --> 00:08:09,919
your database every five minutes and in

00:08:08,319 --> 00:08:13,039
this interval

00:08:09,919 --> 00:08:14,400
a record changes states twice you will

00:08:13,039 --> 00:08:17,520
only capture them

00:08:14,400 --> 00:08:19,840
you will only capture the latest

00:08:17,520 --> 00:08:22,319
change in state so you kind of lost what

00:08:19,840 --> 00:08:25,440
happened in between

00:08:22,319 --> 00:08:27,440
in the same way if a record is deleted

00:08:25,440 --> 00:08:28,720
you just lose track of it you're not

00:08:27,440 --> 00:08:31,840
able to

00:08:28,720 --> 00:08:34,959
it's like it never existed

00:08:31,840 --> 00:08:36,320
and there's always this was the source

00:08:34,959 --> 00:08:38,240
of most of my pain

00:08:36,320 --> 00:08:40,479
there's always a trade-off between how

00:08:38,240 --> 00:08:43,200
frequently you can query your database

00:08:40,479 --> 00:08:44,320
and the load you're incurring on it so

00:08:43,200 --> 00:08:46,800
this is especially

00:08:44,320 --> 00:08:48,080
painful if you're if you have a if you

00:08:46,800 --> 00:08:50,560
have to share

00:08:48,080 --> 00:08:52,959
all this all these data sources with

00:08:50,560 --> 00:08:56,080
other teams

00:08:52,959 --> 00:08:57,279
and also you can really propagate schema

00:08:56,080 --> 00:09:00,320
changes

00:08:57,279 --> 00:09:01,200
so there's a big chance that um your

00:09:00,320 --> 00:09:04,320
pipeline

00:09:01,200 --> 00:09:05,200
will fail simply because someone changed

00:09:04,320 --> 00:09:07,600
the schema

00:09:05,200 --> 00:09:10,560
of the source table but they forgot to

00:09:07,600 --> 00:09:10,560
ping you on slack

00:09:12,480 --> 00:09:17,200
so there must be a better way to do this

00:09:15,839 --> 00:09:20,560
right and

00:09:17,200 --> 00:09:23,760
uh maybe you've also mused yourself

00:09:20,560 --> 00:09:25,680
at some point uh what if we just tapped

00:09:23,760 --> 00:09:28,959
into the transaction log

00:09:25,680 --> 00:09:29,839
so all the databases have a immutable

00:09:28,959 --> 00:09:32,560
log

00:09:29,839 --> 00:09:33,760
that registers all the transactions that

00:09:32,560 --> 00:09:36,480
have been successful

00:09:33,760 --> 00:09:37,279
um successfully done in within the

00:09:36,480 --> 00:09:40,399
system

00:09:37,279 --> 00:09:43,440
in postgres this is the right ahead log

00:09:40,399 --> 00:09:46,560
and my sequel is called the bin log

00:09:43,440 --> 00:09:49,200
and for example this is what

00:09:46,560 --> 00:09:50,959
a dba would use for a disaster recovery

00:09:49,200 --> 00:09:52,880
to kind of

00:09:50,959 --> 00:09:55,600
restore the state of the database of the

00:09:52,880 --> 00:10:00,720
database at some point in time before

00:09:55,600 --> 00:10:00,720
everything went to went to space

00:10:02,000 --> 00:10:09,040
and this is what log by cdc proposes so

00:10:05,600 --> 00:10:12,560
uh instead of periodically probing um

00:10:09,040 --> 00:10:14,880
your database you just probe this

00:10:12,560 --> 00:10:16,800
database transaction log and get the

00:10:14,880 --> 00:10:19,839
latest changes from there

00:10:16,800 --> 00:10:20,959
and this guarantees a lot of really nice

00:10:19,839 --> 00:10:24,160
things

00:10:20,959 --> 00:10:25,839
if you have like very dynamic data

00:10:24,160 --> 00:10:29,040
scenarios

00:10:25,839 --> 00:10:32,240
so you get more context

00:10:29,040 --> 00:10:32,880
on uh the changes that happen and you

00:10:32,240 --> 00:10:34,480
also get

00:10:32,880 --> 00:10:36,480
all the changes so the problem that i

00:10:34,480 --> 00:10:39,680
mentioned before of the leads

00:10:36,480 --> 00:10:40,079
or of uh consecutive updates uh you kind

00:10:39,680 --> 00:10:42,640
of

00:10:40,079 --> 00:10:44,320
um you kind of get like all the

00:10:42,640 --> 00:10:47,519
operations that happened

00:10:44,320 --> 00:10:50,640
no matter what and

00:10:47,519 --> 00:10:53,279
since uh since you're reading from

00:10:50,640 --> 00:10:54,959
the log and not the database the impact

00:10:53,279 --> 00:10:58,079
that you have in the source

00:10:54,959 --> 00:11:00,240
is minimal so it also tackles another uh

00:10:58,079 --> 00:11:01,920
another problem that i that i mentioned

00:11:00,240 --> 00:11:06,240
just before for queerbase

00:11:01,920 --> 00:11:08,480
cdc and

00:11:06,240 --> 00:11:11,279
uh you can propagate these uh changes in

00:11:08,480 --> 00:11:14,560
near real time as well

00:11:11,279 --> 00:11:17,040
and this doesn't mean that there is

00:11:14,560 --> 00:11:18,240
one right way to do ctc or wrong way to

00:11:17,040 --> 00:11:21,040
the cdc

00:11:18,240 --> 00:11:24,000
um but in this talk i will explore

00:11:21,040 --> 00:11:28,720
log-based cdc

00:11:24,000 --> 00:11:32,000
and why it's pretty cool

00:11:28,720 --> 00:11:32,560
so probably the most popular tool out

00:11:32,000 --> 00:11:35,040
there

00:11:32,560 --> 00:11:36,560
to do this log-based cdc these days is

00:11:35,040 --> 00:11:39,360
the bezium

00:11:36,560 --> 00:11:40,800
what's great about it is that it gives

00:11:39,360 --> 00:11:43,920
you a standard format

00:11:40,800 --> 00:11:46,480
for change events so you can process

00:11:43,920 --> 00:11:49,360
this data in the same way regardless

00:11:46,480 --> 00:11:50,160
of where it's coming from and it

00:11:49,360 --> 00:11:53,279
transforms

00:11:50,160 --> 00:11:55,920
basically transforms your databases into

00:11:53,279 --> 00:11:58,639
event streams that you can consume in

00:11:55,920 --> 00:11:58,639
your real time

00:12:00,480 --> 00:12:04,160
and the museum is there's a lot of ways

00:12:02,800 --> 00:12:06,079
that you can

00:12:04,160 --> 00:12:08,880
that you can deploy and use the bezium

00:12:06,079 --> 00:12:10,880
but typically

00:12:08,880 --> 00:12:14,000
it is just built on top of kafka and it

00:12:10,880 --> 00:12:15,839
provides some kafka connect connectors

00:12:14,000 --> 00:12:18,240
that's hard to pronounce for the most

00:12:15,839 --> 00:12:20,399
common databases so anything like mysql

00:12:18,240 --> 00:12:23,040
postgres mongodb

00:12:20,399 --> 00:12:24,000
uh i think even oracle is going to be

00:12:23,040 --> 00:12:26,639
supported soon or

00:12:24,000 --> 00:12:27,920
is already supported and the change

00:12:26,639 --> 00:12:29,920
events are then

00:12:27,920 --> 00:12:31,519
pushed to kafka from where you can

00:12:29,920 --> 00:12:35,200
basically

00:12:31,519 --> 00:12:35,200
process them with anything

00:12:35,920 --> 00:12:41,440
including flink which is

00:12:39,040 --> 00:12:43,839
part of what i want to talk to you about

00:12:41,440 --> 00:12:43,839
today

00:12:44,720 --> 00:12:47,839
so it might be that it's the first time

00:12:46,480 --> 00:12:51,600
as well that you're

00:12:47,839 --> 00:12:54,880
hearing about flink and if it is so

00:12:51,600 --> 00:12:56,800
then i think the simplest form in which

00:12:54,880 --> 00:12:59,760
you can think about flink

00:12:56,800 --> 00:13:00,720
is that uh it just allows you to

00:12:59,760 --> 00:13:04,720
continuously

00:13:00,720 --> 00:13:07,680
consume data from whatever

00:13:04,720 --> 00:13:09,279
data sources for example if we have

00:13:07,680 --> 00:13:11,519
change data events coming in through

00:13:09,279 --> 00:13:12,320
kafka you can just use flint to consume

00:13:11,519 --> 00:13:14,720
that

00:13:12,320 --> 00:13:16,320
and then it allows you to apply uh some

00:13:14,720 --> 00:13:18,880
stateful computations

00:13:16,320 --> 00:13:20,639
on these data streams uh it builds up

00:13:18,880 --> 00:13:21,600
some context of the data that it's

00:13:20,639 --> 00:13:25,040
processing so

00:13:21,600 --> 00:13:28,399
flink has this ability to remember um

00:13:25,040 --> 00:13:30,560
the events that it has seen before

00:13:28,399 --> 00:13:32,240
and apply it to the actual events it is

00:13:30,560 --> 00:13:34,240
processing now

00:13:32,240 --> 00:13:35,680
and then it produces some kind of output

00:13:34,240 --> 00:13:38,480
so it can be api

00:13:35,680 --> 00:13:41,600
calls updates to a database other data

00:13:38,480 --> 00:13:41,600
streams and so on

00:13:42,160 --> 00:13:45,279
and what what makes flink really really

00:13:44,480 --> 00:13:47,839
powerful

00:13:45,279 --> 00:13:48,639
is the way it handles this context or

00:13:47,839 --> 00:13:51,040
the way it does

00:13:48,639 --> 00:13:53,040
state management that gives you like

00:13:51,040 --> 00:13:54,720
some really really good properties

00:13:53,040 --> 00:13:57,519
especially if you're working

00:13:54,720 --> 00:13:58,639
um at scale and you need things to be

00:13:57,519 --> 00:14:01,279
fast

00:13:58,639 --> 00:14:03,920
uh so it gives you properties like low

00:14:01,279 --> 00:14:07,440
latency processing with really high

00:14:03,920 --> 00:14:08,079
correctness guarantees and what makes it

00:14:07,440 --> 00:14:11,279
really

00:14:08,079 --> 00:14:13,279
flexible is the fact that it does this

00:14:11,279 --> 00:14:15,440
processing one event at a time

00:14:13,279 --> 00:14:16,560
so it doesn't batch events it just does

00:14:15,440 --> 00:14:19,279
one at a time

00:14:16,560 --> 00:14:19,279
event process

00:14:20,480 --> 00:14:23,839
and this gives you a great primer it's

00:14:22,720 --> 00:14:28,079
like it's a it's

00:14:23,839 --> 00:14:30,720
such a simple concept in theory

00:14:28,079 --> 00:14:32,480
that allows you to address really a wide

00:14:30,720 --> 00:14:34,560
range of use cases

00:14:32,480 --> 00:14:36,000
uh so from streaming analytics and

00:14:34,560 --> 00:14:38,160
machine learning

00:14:36,000 --> 00:14:39,279
to lambda style event driven

00:14:38,160 --> 00:14:43,120
applications

00:14:39,279 --> 00:14:46,160
uh to more classical streaming pipelines

00:14:43,120 --> 00:14:48,880
um i think these are like the three big

00:14:46,160 --> 00:14:50,399
buckets of use cases that we see users

00:14:48,880 --> 00:14:52,560
using fling from

00:14:50,399 --> 00:14:52,560
for

00:14:53,680 --> 00:15:01,440
and like i said this vendor at a time

00:14:57,199 --> 00:15:04,720
processing gives you the ability to have

00:15:01,440 --> 00:15:06,880
a lot of flexibility so as a user

00:15:04,720 --> 00:15:07,760
flink gives you different apis to choose

00:15:06,880 --> 00:15:11,120
from

00:15:07,760 --> 00:15:12,480
that trade-off like how easy it is to

00:15:11,120 --> 00:15:14,240
use flink

00:15:12,480 --> 00:15:16,720
and how expressive you can get in

00:15:14,240 --> 00:15:19,680
building your streaming programs

00:15:16,720 --> 00:15:20,320
so at the higher level you have apis

00:15:19,680 --> 00:15:23,760
like

00:15:20,320 --> 00:15:26,880
sql the table api and pipelink

00:15:23,760 --> 00:15:28,880
there are closer to the relational way

00:15:26,880 --> 00:15:30,800
of thinking about data

00:15:28,880 --> 00:15:31,920
again if you come from a relational

00:15:30,800 --> 00:15:34,480
database

00:15:31,920 --> 00:15:35,199
background the abstractions are pretty

00:15:34,480 --> 00:15:39,519
similar

00:15:35,199 --> 00:15:42,160
to what you would expect there and

00:15:39,519 --> 00:15:42,639
these apis just allow you to kind of cut

00:15:42,160 --> 00:15:45,839
the

00:15:42,639 --> 00:15:47,759
cutter crap and like express your

00:15:45,839 --> 00:15:51,199
problems in a very con size

00:15:47,759 --> 00:15:54,320
and a very fast uh way and then flink

00:15:51,199 --> 00:15:57,440
does all the heavy lifting for you and

00:15:54,320 --> 00:15:59,680
as you progress down the api stack

00:15:57,440 --> 00:16:00,959
using the api starts getting a bit more

00:15:59,680 --> 00:16:03,279
complex

00:16:00,959 --> 00:16:04,320
but you get more and more control over

00:16:03,279 --> 00:16:07,120
the programs

00:16:04,320 --> 00:16:08,000
you're building and also how you how

00:16:07,120 --> 00:16:11,519
you're executing

00:16:08,000 --> 00:16:13,680
them and when you reach the the core

00:16:11,519 --> 00:16:15,120
building blocks of flink there really is

00:16:13,680 --> 00:16:17,120
no limit

00:16:15,120 --> 00:16:18,399
um if you see out there there are people

00:16:17,120 --> 00:16:22,079
doing pretty

00:16:18,399 --> 00:16:25,040
uh crazy crazy scale and crazy use cases

00:16:22,079 --> 00:16:27,920
with flink it really is super super

00:16:25,040 --> 00:16:30,560
flexible and very powerful

00:16:27,920 --> 00:16:31,360
so for some use cases you need all this

00:16:30,560 --> 00:16:34,639
power

00:16:31,360 --> 00:16:36,800
uh from flink but for a lot of others

00:16:34,639 --> 00:16:40,000
you actually don't

00:16:36,800 --> 00:16:42,560
and cdc is a great example of a use case

00:16:40,000 --> 00:16:45,279
that can benefit from the simplicity of

00:16:42,560 --> 00:16:45,279
link sql

00:16:47,279 --> 00:16:52,000
so the nice thing about providing a sql

00:16:50,800 --> 00:16:55,600
based api

00:16:52,000 --> 00:16:57,120
to do streaming is that everyone knows

00:16:55,600 --> 00:17:00,320
streaming right

00:16:57,120 --> 00:17:02,240
not streaming everyone knows equal right

00:17:00,320 --> 00:17:03,680
if you've used the database before

00:17:02,240 --> 00:17:06,000
you'll recognize

00:17:03,680 --> 00:17:06,959
the query that that's in the slide as

00:17:06,000 --> 00:17:10,640
simply

00:17:06,959 --> 00:17:10,640
standard or ansi sql

00:17:11,360 --> 00:17:16,480
and this means that you also know how to

00:17:14,160 --> 00:17:20,079
use flink sql from the get go because

00:17:16,480 --> 00:17:22,400
flinsql is just standard sql

00:17:20,079 --> 00:17:24,319
and you'll notice the resemblance in a

00:17:22,400 --> 00:17:28,240
bit uh in the demo

00:17:24,319 --> 00:17:31,120
or the on demo okay so

00:17:28,240 --> 00:17:31,520
just so that uh also to kind of like

00:17:31,120 --> 00:17:34,880
cover

00:17:31,520 --> 00:17:35,679
this um cover the distance between

00:17:34,880 --> 00:17:38,080
databases

00:17:35,679 --> 00:17:39,679
and uh uh a database engine and a

00:17:38,080 --> 00:17:41,600
streaming sql engine

00:17:39,679 --> 00:17:43,600
so what's different what's different

00:17:41,600 --> 00:17:46,640
about running sql on top of

00:17:43,600 --> 00:17:48,559
a streaming engine is that

00:17:46,640 --> 00:17:51,200
unlike what happens in a database where

00:17:48,559 --> 00:17:54,320
you run your query

00:17:51,200 --> 00:17:56,320
the engine takes a snapshot of

00:17:54,320 --> 00:17:58,559
the table at the time that the query is

00:17:56,320 --> 00:18:00,240
run and then it computes the results

00:17:58,559 --> 00:18:04,160
based on this static

00:18:00,240 --> 00:18:07,280
snapshot in a in streaming sql the query

00:18:04,160 --> 00:18:10,160
is continuous and never-ending so

00:18:07,280 --> 00:18:11,360
you first deploy your query and then

00:18:10,160 --> 00:18:15,120
whenever data is

00:18:11,360 --> 00:18:18,799
added to the table

00:18:15,120 --> 00:18:21,360
results are just continuously updated so

00:18:18,799 --> 00:18:29,600
this query will keep on running until

00:18:21,360 --> 00:18:33,120
you actually cancel the job

00:18:29,600 --> 00:18:35,520
so in a nutshell i hope you're still

00:18:33,120 --> 00:18:37,679
following along so in a nutshell

00:18:35,520 --> 00:18:38,720
uh what flings equal gives you is this

00:18:37,679 --> 00:18:41,280
really high level

00:18:38,720 --> 00:18:42,880
relational like way of thinking about

00:18:41,280 --> 00:18:46,080
data streams

00:18:42,880 --> 00:18:49,440
using a language that you already know

00:18:46,080 --> 00:18:51,200
and it's optimized to handle batch

00:18:49,440 --> 00:18:53,360
and streaming workloads so with the

00:18:51,200 --> 00:18:56,400
exact same query

00:18:53,360 --> 00:18:59,760
you can for example um

00:18:56,400 --> 00:19:02,880
you can for example query a kafka topic

00:18:59,760 --> 00:19:06,400
and a static

00:19:02,880 --> 00:19:09,280
a static file on s3 for example

00:19:06,400 --> 00:19:10,720
and besides all the operations that you

00:19:09,280 --> 00:19:14,720
might already

00:19:10,720 --> 00:19:18,799
know like joining aggregations filters

00:19:14,720 --> 00:19:21,840
it also supports advanced operations

00:19:18,799 --> 00:19:23,600
like time traveling pattern matching

00:19:21,840 --> 00:19:25,120
with things like the match recognized

00:19:23,600 --> 00:19:29,120
clause and all these

00:19:25,120 --> 00:19:29,520
things that would be pretty complicated

00:19:29,120 --> 00:19:32,720
to

00:19:29,520 --> 00:19:35,360
achieve just using sql but you can

00:19:32,720 --> 00:19:38,559
actually do it here

00:19:35,360 --> 00:19:40,000
and just in general with something as

00:19:38,559 --> 00:19:42,559
simple as

00:19:40,000 --> 00:19:43,679
sql you can build applications that are

00:19:42,559 --> 00:19:46,080
as resilient

00:19:43,679 --> 00:19:48,080
scalable and consistent as any other

00:19:46,080 --> 00:19:52,080
flink application written with

00:19:48,080 --> 00:19:52,480
the lower level apis and you also have a

00:19:52,080 --> 00:19:57,039
whole

00:19:52,480 --> 00:20:00,320
ecosystem around flink sql that

00:19:57,039 --> 00:20:02,480
uh just makes it really easy to write

00:20:00,320 --> 00:20:06,400
end-to-end streaming applications

00:20:02,480 --> 00:20:08,720
using sql and nothing else sorry i got

00:20:06,400 --> 00:20:12,080
distracted with the chat

00:20:08,720 --> 00:20:14,240
uh okay so now we can get down

00:20:12,080 --> 00:20:15,200
to what actually brought us here flame

00:20:14,240 --> 00:20:18,799
sql and

00:20:15,200 --> 00:20:22,320
change data capture so since flink111

00:20:18,799 --> 00:20:25,280
that was out last july um

00:20:22,320 --> 00:20:26,240
fling supports to consume json encoded

00:20:25,280 --> 00:20:29,840
dibysium

00:20:26,240 --> 00:20:32,400
changelog from kafka and for now that's

00:20:29,840 --> 00:20:34,159
that's all it's able to support but the

00:20:32,400 --> 00:20:36,880
way to do this is simply

00:20:34,159 --> 00:20:39,679
to use the dibysium json format in the

00:20:36,880 --> 00:20:42,640
properties of any kafka back table

00:20:39,679 --> 00:20:43,039
um you create and then flink is able to

00:20:42,640 --> 00:20:48,480
just

00:20:43,039 --> 00:20:51,440
serialize the the dibsum format

00:20:48,480 --> 00:20:51,919
okay now for the fun part so the last

00:20:51,440 --> 00:20:55,200
time

00:20:51,919 --> 00:20:57,120
i did this talk it was pre-recorded uh

00:20:55,200 --> 00:20:59,200
so i could do all the cutting and

00:20:57,120 --> 00:21:01,440
shortening in the world

00:20:59,200 --> 00:21:02,880
but this time because i don't trust my

00:21:01,440 --> 00:21:05,360
very limited machine

00:21:02,880 --> 00:21:06,640
and we don't have two hours to wait for

00:21:05,360 --> 00:21:08,960
things to run

00:21:06,640 --> 00:21:10,559
this is more of an on demo with a lot of

00:21:08,960 --> 00:21:14,080
kids uh

00:21:10,559 --> 00:21:17,919
more than an actual video or actually

00:21:14,080 --> 00:21:20,880
me doing something on my machine

00:21:17,919 --> 00:21:21,760
in any anyways the link i shared in the

00:21:20,880 --> 00:21:24,320
chat

00:21:21,760 --> 00:21:24,799
there is a record there is a there is a

00:21:24,320 --> 00:21:26,720
link

00:21:24,799 --> 00:21:27,919
to a previous recording where you can

00:21:26,720 --> 00:21:31,840
see

00:21:27,919 --> 00:21:34,159
me in action for real um

00:21:31,840 --> 00:21:35,440
if you're interested and you see it in a

00:21:34,159 --> 00:21:38,640
different way because

00:21:35,440 --> 00:21:40,480
here i will not just be coding live i

00:21:38,640 --> 00:21:41,200
will just be showing you some snippets

00:21:40,480 --> 00:21:45,840
and

00:21:41,200 --> 00:21:45,840
screenshots to not uh test my luck

00:21:47,440 --> 00:21:52,640
okay so this demo is a recycle demo but

00:21:51,039 --> 00:21:55,600
we're looking at

00:21:52,640 --> 00:21:56,799
um it has some fake insurance claim data

00:21:55,600 --> 00:22:00,000
that is related to

00:21:56,799 --> 00:22:02,240
animal attacks in australia and

00:22:00,000 --> 00:22:03,440
what we want to do with this demo is

00:22:02,240 --> 00:22:06,400
basically get

00:22:03,440 --> 00:22:07,039
change data capture going um pipeline

00:22:06,400 --> 00:22:09,280
going

00:22:07,039 --> 00:22:11,120
simplify some things by using some nice

00:22:09,280 --> 00:22:12,799
proper or some nice

00:22:11,120 --> 00:22:14,240
features of link has like integration

00:22:12,799 --> 00:22:16,799
with catalogs

00:22:14,240 --> 00:22:18,400
and then maintain a materialized view

00:22:16,799 --> 00:22:19,919
that would just then ship to

00:22:18,400 --> 00:22:23,679
elasticsearch

00:22:19,919 --> 00:22:27,840
so that we can see so we can do

00:22:23,679 --> 00:22:27,840
a small dashboard on kibana

00:22:30,320 --> 00:22:37,840
so the demo you can

00:22:33,840 --> 00:22:40,080
i i dropped a link for my github

00:22:37,840 --> 00:22:41,440
for the github github repository where

00:22:40,080 --> 00:22:44,559
you can find the demo

00:22:41,440 --> 00:22:46,880
you can try it out and it basically just

00:22:44,559 --> 00:22:51,280
uses a docker compose setup that you can

00:22:46,880 --> 00:22:54,080
um clone from there easily spin up and

00:22:51,280 --> 00:22:55,360
uh what it what it has or what this

00:22:54,080 --> 00:22:58,240
setup

00:22:55,360 --> 00:22:59,760
does is spin up a couple of containers

00:22:58,240 --> 00:23:02,799
that are running different uh

00:22:59,760 --> 00:23:03,679
services so we have uh postgres that we

00:23:02,799 --> 00:23:07,200
pre-loaded

00:23:03,679 --> 00:23:08,880
with some data which will be our source

00:23:07,200 --> 00:23:11,120
of change events

00:23:08,880 --> 00:23:12,080
we have kafka and kafka connect so that

00:23:11,120 --> 00:23:15,120
we can deploy

00:23:12,080 --> 00:23:16,640
the bzm to get this changed events uh we

00:23:15,120 --> 00:23:18,720
have a flink cluster

00:23:16,640 --> 00:23:20,240
and a sql client that will allow us to

00:23:18,720 --> 00:23:23,520
submit some queries of link

00:23:20,240 --> 00:23:24,000
and actually run things and finally

00:23:23,520 --> 00:23:26,080
we're

00:23:24,000 --> 00:23:28,080
syncing the whole thing to elasticsearch

00:23:26,080 --> 00:23:28,799
in kibana like i said so just so that we

00:23:28,080 --> 00:23:31,120
can see

00:23:28,799 --> 00:23:33,679
that something is actually happening

00:23:31,120 --> 00:23:33,679
visually

00:23:35,120 --> 00:23:40,559
so the first thing we want to do

00:23:38,159 --> 00:23:41,520
after we spin up everything after we

00:23:40,559 --> 00:23:44,400
imaginarily

00:23:41,520 --> 00:23:45,840
uh spin it spin up everything is start

00:23:44,400 --> 00:23:49,039
our postcards client

00:23:45,840 --> 00:23:50,320
that we will use um a bit later to just

00:23:49,039 --> 00:23:52,640
do some

00:23:50,320 --> 00:23:53,919
dml statements and the first thing we

00:23:52,640 --> 00:23:57,200
can do is just check

00:23:53,919 --> 00:23:58,960
what tables we have in there so

00:23:57,200 --> 00:24:01,600
if you if you check the information

00:23:58,960 --> 00:24:05,120
schema you see that we have two tables

00:24:01,600 --> 00:24:07,200
um members which will be the

00:24:05,120 --> 00:24:08,720
members that just has some reference

00:24:07,200 --> 00:24:11,600
data

00:24:08,720 --> 00:24:13,360
and then accident claims which will be a

00:24:11,600 --> 00:24:16,400
table that we're interested in

00:24:13,360 --> 00:24:17,279
tracking the changes and i kind of want

00:24:16,400 --> 00:24:20,000
you to rem

00:24:17,279 --> 00:24:20,400
to remember that this accident claims

00:24:20,000 --> 00:24:23,440
team

00:24:20,400 --> 00:24:25,679
table has 1000 records just

00:24:23,440 --> 00:24:26,559
so that you see that things are actually

00:24:25,679 --> 00:24:30,720
working

00:24:26,559 --> 00:24:32,080
uh down the road so what we need to do

00:24:30,720 --> 00:24:35,039
next is deploy our

00:24:32,080 --> 00:24:36,159
division connector and for that we'll

00:24:35,039 --> 00:24:38,320
just post a

00:24:36,159 --> 00:24:39,840
json document with the configuration of

00:24:38,320 --> 00:24:43,520
our connector

00:24:39,840 --> 00:24:45,440
to the running kafka connect service and

00:24:43,520 --> 00:24:46,799
what this should give you back is just

00:24:45,440 --> 00:24:50,240
basically

00:24:46,799 --> 00:24:53,120
that configuration of your connector and

00:24:50,240 --> 00:24:54,240
to make it a bit easier to read a bit

00:24:53,120 --> 00:24:56,799
prettier

00:24:54,240 --> 00:24:58,159
uh your property file looks something

00:24:56,799 --> 00:25:00,799
like this

00:24:58,159 --> 00:25:03,440
you can see that we're using the

00:25:00,799 --> 00:25:06,880
postgres division connector

00:25:03,440 --> 00:25:10,240
we want to track a database named

00:25:06,880 --> 00:25:13,200
postgres we are using

00:25:10,240 --> 00:25:15,760
pg claims as like the prefix for any

00:25:13,200 --> 00:25:19,120
kafka topic that is created

00:25:15,760 --> 00:25:22,159
from this and we are tracking the

00:25:19,120 --> 00:25:25,520
accident claims uh table

00:25:22,159 --> 00:25:25,520
that it just saw before

00:25:26,640 --> 00:25:31,120
so what the museum does the first time

00:25:29,200 --> 00:25:34,320
it connects to a postgres server

00:25:31,120 --> 00:25:36,720
is uh unless you tell it not to uh

00:25:34,320 --> 00:25:37,679
it takes a snapshot of where whatever

00:25:36,720 --> 00:25:40,320
you inst

00:25:37,679 --> 00:25:42,320
instruct the business to track so what

00:25:40,320 --> 00:25:45,520
should have happened

00:25:42,320 --> 00:25:48,159
when you deploy your connector is

00:25:45,520 --> 00:25:48,799
that division took a snapshot of your

00:25:48,159 --> 00:25:51,200
table

00:25:48,799 --> 00:25:52,960
and then pushed all the events into the

00:25:51,200 --> 00:25:55,360
kafka topic

00:25:52,960 --> 00:25:57,279
and what we're doing here is just

00:25:55,360 --> 00:25:59,200
basically making sure that that happened

00:25:57,279 --> 00:26:01,440
and so we can use the kafka console

00:25:59,200 --> 00:26:04,559
consumer to have a look in there

00:26:01,440 --> 00:26:05,440
and check that we have all those 1000

00:26:04,559 --> 00:26:09,360
events

00:26:05,440 --> 00:26:12,159
um i told you about before in there

00:26:09,360 --> 00:26:14,400
so this is what this uh is running in

00:26:12,159 --> 00:26:19,600
this terminal is just basically checking

00:26:14,400 --> 00:26:19,600
the kafka topic that we just set up

00:26:20,320 --> 00:26:24,880
and cool this checks out right we have a

00:26:22,559 --> 00:26:26,880
thousand records and

00:26:24,880 --> 00:26:28,000
what you see here is the typical

00:26:26,880 --> 00:26:31,120
structure of

00:26:28,000 --> 00:26:32,559
a division event so you'll have the key

00:26:31,120 --> 00:26:35,760
related information

00:26:32,559 --> 00:26:36,960
in that i sign up in red and then you

00:26:35,760 --> 00:26:38,799
have the event value

00:26:36,960 --> 00:26:42,159
information which is what we're

00:26:38,799 --> 00:26:42,159
interested in looking at here

00:26:42,320 --> 00:26:47,360
so in the value payload you can see that

00:26:45,120 --> 00:26:48,480
the before value or the previous state

00:26:47,360 --> 00:26:51,600
of your event

00:26:48,480 --> 00:26:52,640
was null and in this case it makes sense

00:26:51,600 --> 00:26:55,760
because

00:26:52,640 --> 00:26:57,840
uh all all the business did was uh

00:26:55,760 --> 00:26:59,120
read read what was already in the

00:26:57,840 --> 00:27:01,600
database

00:26:59,120 --> 00:27:03,600
and in the after object you get the

00:27:01,600 --> 00:27:06,960
actual state of your records

00:27:03,600 --> 00:27:09,760
and the database

00:27:06,960 --> 00:27:10,720
so what we're interested in seeing next

00:27:09,760 --> 00:27:12,640
is whether

00:27:10,720 --> 00:27:13,760
capturing changes with the bzm is

00:27:12,640 --> 00:27:16,240
working so

00:27:13,760 --> 00:27:17,039
is this first pi part of our pipeline

00:27:16,240 --> 00:27:19,440
actually

00:27:17,039 --> 00:27:21,200
doing what it's supposed to do so for

00:27:19,440 --> 00:27:23,279
this we can do a series of uh

00:27:21,200 --> 00:27:24,880
operations on postgre on the postcards

00:27:23,279 --> 00:27:27,600
client and see

00:27:24,880 --> 00:27:29,440
how they reflect on our kafka on our

00:27:27,600 --> 00:27:32,080
kafka topic

00:27:29,440 --> 00:27:32,799
so here i did a series of an insert an

00:27:32,080 --> 00:27:36,000
update and

00:27:32,799 --> 00:27:37,840
a delete operation and you can see that

00:27:36,000 --> 00:27:40,399
when i do an insert

00:27:37,840 --> 00:27:42,080
uh you get a create you get an event

00:27:40,399 --> 00:27:46,480
with a create

00:27:42,080 --> 00:27:51,039
operation in kafka when i do an update

00:27:46,480 --> 00:27:53,200
you see that you get uh

00:27:51,039 --> 00:27:55,600
you see that you get an event with an

00:27:53,200 --> 00:27:57,440
operation update and for updates you can

00:27:55,600 --> 00:28:00,840
see that now you have

00:27:57,440 --> 00:28:03,919
uh the before and the after objects uh

00:28:00,840 --> 00:28:06,480
populated and

00:28:03,919 --> 00:28:08,399
then delete same thing uh it just

00:28:06,480 --> 00:28:11,760
generates an event on kafka

00:28:08,399 --> 00:28:16,480
with uh delete operation so success

00:28:11,760 --> 00:28:18,559
things are working and we can move on

00:28:16,480 --> 00:28:21,200
so we cannot propagate this to flink

00:28:18,559 --> 00:28:22,880
that's like the ultimate goal is to

00:28:21,200 --> 00:28:25,760
propagate all of this to fling so that

00:28:22,880 --> 00:28:29,120
we can do some processing on it

00:28:25,760 --> 00:28:31,840
so here the first thing we can do now is

00:28:29,120 --> 00:28:32,880
start the fling sql client when you see

00:28:31,840 --> 00:28:36,080
the squirrel

00:28:32,880 --> 00:28:37,840
it's a good sign means it's up and you

00:28:36,080 --> 00:28:41,440
can start using it

00:28:37,840 --> 00:28:42,960
and like i said before this is basically

00:28:41,440 --> 00:28:43,679
a tool that you can use to submit

00:28:42,960 --> 00:28:47,760
queries

00:28:43,679 --> 00:28:50,240
or jobs to flink and

00:28:47,760 --> 00:28:52,320
what we want to start with is we will

00:28:50,240 --> 00:28:55,120
create a catalog

00:28:52,320 --> 00:28:56,880
or we will register with flink catalog

00:28:55,120 --> 00:28:59,919
that will give us access to

00:28:56,880 --> 00:29:00,240
the metadata of this all these tables

00:28:59,919 --> 00:29:03,039
there

00:29:00,240 --> 00:29:04,799
are in postgres and this will make it a

00:29:03,039 --> 00:29:07,520
bit easier

00:29:04,799 --> 00:29:07,520
further ahead

00:29:08,159 --> 00:29:14,640
and next and most importantly

00:29:12,000 --> 00:29:15,360
we want to create a table to consume

00:29:14,640 --> 00:29:18,880
this

00:29:15,360 --> 00:29:21,360
change data events from kafka so it

00:29:18,880 --> 00:29:23,679
should have

00:29:21,360 --> 00:29:25,440
the same schema and the same constraints

00:29:23,679 --> 00:29:26,240
as the original table and this is what

00:29:25,440 --> 00:29:29,679
we can use

00:29:26,240 --> 00:29:34,720
the catalog for so you can see here

00:29:29,679 --> 00:29:37,440
the ddl statement is a pretty normal

00:29:34,720 --> 00:29:39,520
pretty normal if you're used to if

00:29:37,440 --> 00:29:41,360
you're used to sql it's just the same

00:29:39,520 --> 00:29:43,360
the same syntax as you would use to

00:29:41,360 --> 00:29:46,480
create a table anywhere

00:29:43,360 --> 00:29:48,640
but then it has um the

00:29:46,480 --> 00:29:49,919
the part where you specify the connector

00:29:48,640 --> 00:29:52,000
so here

00:29:49,919 --> 00:29:53,200
uh the connector that is back in this

00:29:52,000 --> 00:29:56,399
table

00:29:53,200 --> 00:29:59,440
so here you can see now we're using the

00:29:56,399 --> 00:30:02,720
kafka connector we are tapping into

00:29:59,440 --> 00:30:05,440
our the kafka topic

00:30:02,720 --> 00:30:05,919
that we defined before so pg claims

00:30:05,440 --> 00:30:08,960
claims

00:30:05,919 --> 00:30:11,360
accident claims and then we're using the

00:30:08,960 --> 00:30:12,559
bzm json format to just deserialize the

00:30:11,360 --> 00:30:15,520
event so

00:30:12,559 --> 00:30:17,039
same as before i just submit this query

00:30:15,520 --> 00:30:18,720
using the sql client

00:30:17,039 --> 00:30:20,080
and we're good to go our table is

00:30:18,720 --> 00:30:23,600
created and

00:30:20,080 --> 00:30:26,000
ready to accept some uh change log

00:30:23,600 --> 00:30:26,000
action

00:30:27,120 --> 00:30:31,600
so this ring is still the next uh

00:30:29,919 --> 00:30:34,159
checkpoint so

00:30:31,600 --> 00:30:34,960
uh is the whole thing actually working

00:30:34,159 --> 00:30:36,960
now

00:30:34,960 --> 00:30:39,360
not just from postgres through the

00:30:36,960 --> 00:30:40,640
museum to kafka but from postgres

00:30:39,360 --> 00:30:44,159
through the business

00:30:40,640 --> 00:30:45,440
to kafka to flink so same as we did

00:30:44,159 --> 00:30:47,120
before for testing

00:30:45,440 --> 00:30:49,520
uh whether the events were making it

00:30:47,120 --> 00:30:52,799
into kafka or not

00:30:49,520 --> 00:30:53,760
uh here we here we run the same set of

00:30:52,799 --> 00:30:55,200
operations

00:30:53,760 --> 00:30:56,960
to see if the changes are being

00:30:55,200 --> 00:30:59,679
propagated all the way

00:30:56,960 --> 00:31:00,640
so in the bottom terminal you can see uh

00:30:59,679 --> 00:31:02,720
it's

00:31:00,640 --> 00:31:04,240
you can see the result of a select star

00:31:02,720 --> 00:31:06,399
from

00:31:04,240 --> 00:31:08,000
the changelog table that i just showed

00:31:06,399 --> 00:31:11,120
you

00:31:08,000 --> 00:31:11,519
and first time you run it what you will

00:31:11,120 --> 00:31:15,279
get

00:31:11,519 --> 00:31:18,159
is the 1000 records that were in there

00:31:15,279 --> 00:31:19,840
and you can see that as we run our

00:31:18,159 --> 00:31:22,399
statements in postgres

00:31:19,840 --> 00:31:22,880
same thing you see a new record popping

00:31:22,399 --> 00:31:26,080
up

00:31:22,880 --> 00:31:28,720
with the insert uh you see that record

00:31:26,080 --> 00:31:29,760
the receipt for that record being

00:31:28,720 --> 00:31:33,600
updated

00:31:29,760 --> 00:31:36,480
uh with an update on postgres and

00:31:33,600 --> 00:31:38,720
then it just simply disappears because

00:31:36,480 --> 00:31:41,919
we do a delete operation in the

00:31:38,720 --> 00:31:41,919
original source table

00:31:45,360 --> 00:31:52,080
and to make this um a really end-to-end

00:31:48,799 --> 00:31:54,720
example and also show uh fluency

00:31:52,080 --> 00:31:56,240
point sql in action a bit more uh

00:31:54,720 --> 00:32:00,399
imagine that we want

00:31:56,240 --> 00:32:03,279
to calculate and visualize

00:32:00,399 --> 00:32:04,399
uh the aggregated insurance costs per

00:32:03,279 --> 00:32:07,760
insurance company

00:32:04,399 --> 00:32:09,760
company in our in our data or basically

00:32:07,760 --> 00:32:10,320
we want to know what animals are causing

00:32:09,760 --> 00:32:13,519
the most

00:32:10,320 --> 00:32:17,360
the most harm in terms of costs

00:32:13,519 --> 00:32:19,760
so now that we

00:32:17,360 --> 00:32:21,440
what we can do is that we can just

00:32:19,760 --> 00:32:24,799
create a sync table

00:32:21,440 --> 00:32:25,519
that will sync our results or we'll just

00:32:24,799 --> 00:32:29,360
emit

00:32:25,519 --> 00:32:30,960
our results to an elasticsearch index

00:32:29,360 --> 00:32:32,799
you don't need to create the index in

00:32:30,960 --> 00:32:34,159
advance if it doesn't exist link will

00:32:32,799 --> 00:32:38,799
just

00:32:34,159 --> 00:32:41,519
create it for you and from elasticsearch

00:32:38,799 --> 00:32:43,039
after we have the data there we can just

00:32:41,519 --> 00:32:44,880
use kibana to make some pretty

00:32:43,039 --> 00:32:46,880
dashboards so that you can see things

00:32:44,880 --> 00:32:50,480
moving

00:32:46,880 --> 00:32:50,480
and so it's a bit more interesting

00:32:51,039 --> 00:32:58,000
so you can see that the query again it's

00:32:55,440 --> 00:32:58,960
like a query you could run anywhere you

00:32:58,000 --> 00:33:02,000
could probably

00:32:58,960 --> 00:33:03,440
um run this directly on postgres without

00:33:02,000 --> 00:33:06,720
doing any

00:33:03,440 --> 00:33:08,799
any modifications and

00:33:06,720 --> 00:33:10,159
it's just standard sql we're just doing

00:33:08,799 --> 00:33:14,480
like i said

00:33:10,159 --> 00:33:17,679
an aggregation of the total claims

00:33:14,480 --> 00:33:17,679
total claim costs

00:33:17,760 --> 00:33:21,360
over insurance companies and accident

00:33:20,399 --> 00:33:25,360
details so

00:33:21,360 --> 00:33:28,480
once we submit this query it will

00:33:25,360 --> 00:33:30,480
again using the from sql client uh

00:33:28,480 --> 00:33:32,720
it will just run continuously and

00:33:30,480 --> 00:33:35,600
continuously update the results

00:33:32,720 --> 00:33:39,840
on the elastic search index until you

00:33:35,600 --> 00:33:39,840
tell it to stop basically

00:33:40,320 --> 00:33:45,519
and then in kibana you can easily just

00:33:43,360 --> 00:33:46,080
create a dashboard that refreshes every

00:33:45,519 --> 00:33:48,799
second

00:33:46,080 --> 00:33:49,679
and you can see here in the original

00:33:48,799 --> 00:33:53,039
demo there

00:33:49,679 --> 00:33:53,919
is a data generator that i triggered

00:33:53,039 --> 00:33:56,640
just before

00:33:53,919 --> 00:33:58,159
this so that there are continuously

00:33:56,640 --> 00:34:01,840
eventually inserted in

00:33:58,159 --> 00:34:02,480
pulse race and this is the result so you

00:34:01,840 --> 00:34:06,480
see

00:34:02,480 --> 00:34:09,839
um you see that whatever

00:34:06,480 --> 00:34:13,440
whatever data changes uh happen in pro

00:34:09,839 --> 00:34:15,679
postgres are processed um

00:34:13,440 --> 00:34:18,159
through division to kafka and then in

00:34:15,679 --> 00:34:21,679
flink we're doing all these aggregations

00:34:18,159 --> 00:34:24,800
that we are with with this change data

00:34:21,679 --> 00:34:28,159
that then just end up in the

00:34:24,800 --> 00:34:28,159
in our dashboard and give on

00:34:29,040 --> 00:34:32,800
and all of this happens in your real

00:34:30,720 --> 00:34:35,760
time so if you try it yourself

00:34:32,800 --> 00:34:37,520
uh you can see that this is pretty this

00:34:35,760 --> 00:34:40,639
is pretty

00:34:37,520 --> 00:34:42,480
fast so that's it for the

00:34:40,639 --> 00:34:45,440
on demo again if you want to try the

00:34:42,480 --> 00:34:48,879
demo please go to the github repository

00:34:45,440 --> 00:34:49,760
go for it maybe watch the other video as

00:34:48,879 --> 00:34:52,560
well

00:34:49,760 --> 00:34:56,079
if you want to see something that if you

00:34:52,560 --> 00:34:56,079
really want to see all the steps

00:34:56,240 --> 00:35:00,720
all the steps in building the cdc

00:34:58,320 --> 00:35:04,240
pipeline

00:35:00,720 --> 00:35:06,640
and to wrap it up i would just like uh

00:35:04,240 --> 00:35:07,520
i would just like to say that fluent

00:35:06,640 --> 00:35:09,440
sequel is

00:35:07,520 --> 00:35:11,200
used really at massive scale in

00:35:09,440 --> 00:35:12,160
companies all around the world so like

00:35:11,200 --> 00:35:16,560
at alibaba

00:35:12,160 --> 00:35:19,839
uber yelp lyft i think alibaba is the

00:35:16,560 --> 00:35:22,880
biggest um is the

00:35:19,839 --> 00:35:24,160
biggest use case we know or at least

00:35:22,880 --> 00:35:27,520
that we know of

00:35:24,160 --> 00:35:29,599
to this day um on their on their

00:35:27,520 --> 00:35:32,480
shopping festival every year so like

00:35:29,599 --> 00:35:35,359
last year singles day 2019

00:35:32,480 --> 00:35:37,359
they were running um flink and their

00:35:35,359 --> 00:35:40,960
fling sql applications on over

00:35:37,359 --> 00:35:44,079
5000 notes and uh

00:35:40,960 --> 00:35:47,599
they were able to produce at peak uh 2.5

00:35:44,079 --> 00:35:50,800
billion events per second which is

00:35:47,599 --> 00:35:52,240
uh yeah it's pretty impressive and also

00:35:50,800 --> 00:35:53,920
like i like i mentioned one of the

00:35:52,240 --> 00:35:54,880
things that flink does really well is

00:35:53,920 --> 00:35:58,880
handle state

00:35:54,880 --> 00:36:02,320
and their biggest their biggest job was

00:35:58,880 --> 00:36:05,280
uh was handling 100 terabytes of state

00:36:02,320 --> 00:36:07,119
easily maybe not easily for whoever was

00:36:05,280 --> 00:36:08,720
maintaining that but

00:36:07,119 --> 00:36:10,880
this this is kind of the scale that it

00:36:08,720 --> 00:36:14,800
can really take flink

00:36:10,880 --> 00:36:18,160
to and also point sql by

00:36:14,800 --> 00:36:19,680
yeah by extension so just a recap fling

00:36:18,160 --> 00:36:22,240
sql standard

00:36:19,680 --> 00:36:23,359
provides unified apis for batch and

00:36:22,240 --> 00:36:27,040
streaming

00:36:23,359 --> 00:36:30,000
so you can do both with the same code

00:36:27,040 --> 00:36:31,760
and there's a really growing ecosystem

00:36:30,000 --> 00:36:32,960
of integrations around it that make it

00:36:31,760 --> 00:36:36,720
really easy to build

00:36:32,960 --> 00:36:38,079
streaming applications and for cdc in

00:36:36,720 --> 00:36:40,400
specific

00:36:38,079 --> 00:36:43,040
like i said it's very it's a very young

00:36:40,400 --> 00:36:46,560
feature in flink

00:36:43,040 --> 00:36:49,920
there are already flink 112 is

00:36:46,560 --> 00:36:50,560
is coming soon and it has already a lot

00:36:49,920 --> 00:36:53,920
of

00:36:50,560 --> 00:36:55,040
improvements on cdc so we will have

00:36:53,920 --> 00:36:58,079
overall encoded

00:36:55,040 --> 00:37:01,119
encoded change logs more than now we

00:36:58,079 --> 00:37:02,720
only support json and you'll be able to

00:37:01,119 --> 00:37:05,440
do temporal drawings with

00:37:02,720 --> 00:37:06,160
change log sources and also you'll be

00:37:05,440 --> 00:37:09,359
able to do

00:37:06,160 --> 00:37:12,960
batch right now you can only

00:37:09,359 --> 00:37:16,960
you can only use this for screen

00:37:12,960 --> 00:37:21,520
and jark which is the main developer

00:37:16,960 --> 00:37:24,480
or one of the main developers behind

00:37:21,520 --> 00:37:27,040
cdc supporting flink he has written a

00:37:24,480 --> 00:37:30,960
couple of connectors that use the

00:37:27,040 --> 00:37:32,960
the busium embedded engine so

00:37:30,960 --> 00:37:35,440
these are source connectors for flink

00:37:32,960 --> 00:37:37,040
that that allow you to do

00:37:35,440 --> 00:37:38,560
all of this that i showed you without

00:37:37,040 --> 00:37:43,200
deploying kafka

00:37:38,560 --> 00:37:44,320
or kafka connect or the museum itself so

00:37:43,200 --> 00:37:46,880
you can check it out if you're

00:37:44,320 --> 00:37:46,880
interested

00:37:47,359 --> 00:37:51,599
cool and if you want to learn more about

00:37:49,040 --> 00:37:55,040
flink or hear jerk

00:37:51,599 --> 00:37:56,800
actually talk more about cdc

00:37:55,040 --> 00:37:58,720
you can join flink forward it's our

00:37:56,800 --> 00:38:02,839
community conference

00:37:58,720 --> 00:38:05,920
coming up next month it's for free

00:38:02,839 --> 00:38:09,839
so uh yeah

00:38:05,920 --> 00:38:10,640
come around and thank you so much i have

00:38:09,839 --> 00:38:19,839
no idea

00:38:10,640 --> 00:38:19,839
if i'm over time or not but that's it

00:38:21,920 --> 00:38:26,800
uh felix do i have time for questions

00:38:24,240 --> 00:38:29,839
i'm a bit lost in

00:38:26,800 --> 00:38:35,040
calculations okay cool so

00:38:29,839 --> 00:38:37,920
um cool so there are two questions

00:38:35,040 --> 00:38:39,680
uh first question can cdc and flink be

00:38:37,920 --> 00:38:42,720
used in batch processing

00:38:39,680 --> 00:38:46,240
when oltp is and olap is

00:38:42,720 --> 00:38:49,920
s3 or athena

00:38:46,240 --> 00:38:54,079
like i said batch is not supported yet

00:38:49,920 --> 00:38:57,599
but once flink112 is out

00:38:54,079 --> 00:39:00,160
yes you should be able to should be able

00:38:57,599 --> 00:39:03,440
to use it with

00:39:00,160 --> 00:39:07,119
 and s3 yes

00:39:03,440 --> 00:39:07,119
athena i'm not sure

00:39:09,200 --> 00:39:13,520
okay and once i have cdc up and running

00:39:11,920 --> 00:39:15,599
which are the most important

00:39:13,520 --> 00:39:17,680
metrics to monitor so that i can

00:39:15,599 --> 00:39:20,640
identify some instability

00:39:17,680 --> 00:39:22,720
in the setup that's a really great

00:39:20,640 --> 00:39:26,000
question that i've never

00:39:22,720 --> 00:39:26,000
really thought about

00:39:26,240 --> 00:39:33,440
so i don't really know how to

00:39:29,760 --> 00:39:36,840
how to answer it but i would say

00:39:33,440 --> 00:39:36,840
[Music]

00:39:39,920 --> 00:39:43,599
yeah i don't know i i would say that

00:39:41,839 --> 00:39:48,000
probably there is something you should

00:39:43,599 --> 00:39:52,079
look at in terms of throughput uh

00:39:48,000 --> 00:39:55,280
maybe from kafka like how

00:39:52,079 --> 00:39:58,079
how fast events are being uh produced

00:39:55,280 --> 00:40:01,280
into kafka and how fast you're able to

00:39:58,079 --> 00:40:03,920
consume and process them in flink

00:40:01,280 --> 00:40:10,160
but i'm not really sure what to tell you

00:40:03,920 --> 00:40:12,160
here sorry

00:40:10,160 --> 00:40:14,079
will the joint be triggered whenever

00:40:12,160 --> 00:40:17,200
there's a new change event on either

00:40:14,079 --> 00:40:17,200
side of the joint

00:40:20,000 --> 00:40:23,200
yeah like like the nurse just building

00:40:22,880 --> 00:40:24,800
on

00:40:23,200 --> 00:40:26,800
on the previous answer i think what

00:40:24,800 --> 00:40:27,440
ganars is like end-to-end delay really

00:40:26,800 --> 00:40:28,960
is

00:40:27,440 --> 00:40:32,000
probably something that you should be

00:40:28,960 --> 00:40:34,800
they should be looking at

00:40:32,000 --> 00:40:36,720
and another question from gunner will

00:40:34,800 --> 00:40:38,000
the joint be triggered whenever there's

00:40:36,720 --> 00:40:41,200
a new change event

00:40:38,000 --> 00:40:46,000
on either side of the join

00:40:41,200 --> 00:40:48,240
uh not really triggered because

00:40:46,000 --> 00:40:50,079
like i mentioned what you have is this

00:40:48,240 --> 00:40:53,599
kind of materialized view

00:40:50,079 --> 00:40:55,599
so everything is handled in state um

00:40:53,599 --> 00:40:57,760
in the state so like it's either in

00:40:55,599 --> 00:41:00,880
memory or in disk depending

00:40:57,760 --> 00:41:02,720
on on the size of your state uh so

00:41:00,880 --> 00:41:06,960
basically whenever you have

00:41:02,720 --> 00:41:09,839
a new event things are just handled

00:41:06,960 --> 00:41:11,359
yeah in the background you don't really

00:41:09,839 --> 00:41:14,000
trigger the drawing every time you have

00:41:11,359 --> 00:41:14,000
a new event

00:41:18,000 --> 00:41:23,040
hope that answers the question is there

00:41:20,160 --> 00:41:26,720
a way to set an initial state for the

00:41:23,040 --> 00:41:26,720
phone queue result that is based

00:41:27,280 --> 00:41:30,480
cdc started

00:41:30,880 --> 00:41:36,480
uh yes

00:41:34,000 --> 00:41:37,920
the sorry i think i just mumbled the

00:41:36,480 --> 00:41:39,440
question is there a way to set an

00:41:37,920 --> 00:41:41,839
initial state

00:41:39,440 --> 00:41:42,880
for the flink query result that is based

00:41:41,839 --> 00:41:46,560
on data

00:41:42,880 --> 00:41:50,240
that existed before the cdc started

00:41:46,560 --> 00:41:52,640
so um here what you can do

00:41:50,240 --> 00:41:54,400
is that you can bootstrap your flink job

00:41:52,640 --> 00:41:56,160
or um

00:41:54,400 --> 00:41:58,960
the flink drop that will just be created

00:41:56,160 --> 00:42:02,079
with your query you can just

00:41:58,960 --> 00:42:02,800
use a save point or you can probably

00:42:02,079 --> 00:42:05,680
just uh

00:42:02,800 --> 00:42:07,599
bootstrap your job with a save point or

00:42:05,680 --> 00:42:10,720
something like that

00:42:07,599 --> 00:42:14,560
uh in flink so yeah it's possible

00:42:10,720 --> 00:42:14,560
to bootstrap it with data that existed

00:42:14,839 --> 00:42:19,440
before

00:42:16,000 --> 00:42:20,319
uh is fling able to write to a database

00:42:19,440 --> 00:42:23,040
in order

00:42:20,319 --> 00:42:23,760
while consuming from kafka let's say

00:42:23,040 --> 00:42:26,880
original

00:42:23,760 --> 00:42:31,760
database change data capture kafka

00:42:26,880 --> 00:42:35,200
flink different db

00:42:31,760 --> 00:42:37,839
uh yeah i mean um

00:42:35,200 --> 00:42:39,200
fling has a jdbc connector so you can

00:42:37,839 --> 00:42:42,839
write to

00:42:39,200 --> 00:42:44,480
any database that supports that then you

00:42:42,839 --> 00:42:46,880
have uh

00:42:44,480 --> 00:42:48,240
other connectors available like h phase

00:42:46,880 --> 00:42:50,000
if you want to write to hvac you can

00:42:48,240 --> 00:42:53,920
also do that

00:42:50,000 --> 00:42:57,040
and what is not supported yet i don't

00:42:53,920 --> 00:43:00,560
know if that's your question or not

00:42:57,040 --> 00:43:04,240
is actually serializing uh the events

00:43:00,560 --> 00:43:04,960
uh that go out uh in the division format

00:43:04,240 --> 00:43:06,800
too but

00:43:04,960 --> 00:43:08,640
in the future this this will also be

00:43:06,800 --> 00:43:10,640
supported so you can process your data

00:43:08,640 --> 00:43:10,960
and then you can serialize it uh again

00:43:10,640 --> 00:43:13,839
in the

00:43:10,960 --> 00:43:13,839
pc environment

00:43:14,720 --> 00:43:21,520
but yeah answering your question uh

00:43:18,800 --> 00:43:22,240
flink if and also if there isn't a

00:43:21,520 --> 00:43:25,119
connector

00:43:22,240 --> 00:43:26,720
that is natively supported by flink uh

00:43:25,119 --> 00:43:27,920
it should be pretty straightforward for

00:43:26,720 --> 00:43:31,280
you to implement your

00:43:27,920 --> 00:43:31,280
your own connectors yeah

00:43:37,280 --> 00:43:42,480
cool uh if that's it thanks a lot for

00:43:40,960 --> 00:43:44,079
thanks a lot for joining and if you have

00:43:42,480 --> 00:43:47,280
any other questions

00:43:44,079 --> 00:43:50,400
uh that i'll probably be less nervous

00:43:47,280 --> 00:43:54,400
replying to you can ping me on on slack

00:43:50,400 --> 00:43:56,160
or on twitter linkedin whatever

00:43:54,400 --> 00:44:09,839
and hope you enjoyed the presentation

00:43:56,160 --> 00:44:09,839
thank you so much

00:44:18,560 --> 00:44:20,640

YouTube URL: https://www.youtube.com/watch?v=wRIQqgI1gLA


