Title: Event Streaming and the Data Communication Layer
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Event Streaming and the Data Communication Layer
Adam Bellemare

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Streaming technologies unlock decoupled, near real-time services at scale. The most important part of any streaming platform is the event-broker (eg. Apache Kafka or Pulsar) as it plays the role of the Data Communication Layer (DCL). Many organizations fail to grasp the importance of the DCL and often relegate it to the role of a simple asynchronous message queue, leaving their key business domain events locked away in monolithic data stores. This is one of the many pitfalls that will be covered in this presentation, along with strategies and tipss for avoiding them. A well constructed DCL decouples both the ownership and production of data from the downstream services that require access to it. Access to clean, reliable, structured, and sorted data streams enables extremely powerful event-driven patterns. Data becomes much easier to access and no longer relies upon the producer's implementation to serve disparate business requirements. Teams and products can organize much more clearly along business bounded contexts, and modular, disposable, and compositional services become extremely easy to build and test. This presentation covers the best practices, responsibilities of the various actors, recommendations about specific technological implementations, and both the organizational changes required and those that will occur as a result of a reliable DCL implementation.

Adam Bellemare is the author of Building Event-Driven Microservices (O'Reilly, 2020). He has been working on event-driven architectures since 2010. His major accomplishments in this time include building an event-driven processing platform at BlackBerry, driving the migration to event-driven microservices at Flipp, and most recently, starting a new role to improve event-driven architectures at Shopify. He has contributed to both Apache Avro and Apache Kafka and is a keen supporter of the open-source community.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,680 --> 00:00:30,000
uh

00:00:26,960 --> 00:00:32,400
okay let's try this again there

00:00:30,000 --> 00:00:32,400
we go

00:00:33,520 --> 00:00:38,320
there we go that seems like it should be

00:00:35,920 --> 00:00:40,719
working

00:00:38,320 --> 00:00:44,000
all right sorry folks just uh you know

00:00:40,719 --> 00:00:47,280
first time here first time presenter

00:00:44,000 --> 00:00:50,079
all right well well uh welcome everybody

00:00:47,280 --> 00:00:51,520
i uh yeah can see okay thank you very

00:00:50,079 --> 00:00:54,079
much for the feedback

00:00:51,520 --> 00:00:55,760
uh all right well without further ado my

00:00:54,079 --> 00:00:57,039
name is adam bellmer

00:00:55,760 --> 00:00:59,199
and today i'm going to be talking to you

00:00:57,039 --> 00:01:01,600
about event driven architectures and the

00:00:59,199 --> 00:01:04,960
data communication layer

00:01:01,600 --> 00:01:06,080
and i need to there we go so first we're

00:01:04,960 --> 00:01:08,000
going to start talking

00:01:06,080 --> 00:01:09,680
about some of the communication

00:01:08,000 --> 00:01:13,040
structures involved in

00:01:09,680 --> 00:01:15,280
a business uh there's three of them

00:01:13,040 --> 00:01:16,560
and this of course is a gross

00:01:15,280 --> 00:01:19,680
simplification

00:01:16,560 --> 00:01:21,920
uh which i uh i'm hoping just to use to

00:01:19,680 --> 00:01:23,920
illustrate the points

00:01:21,920 --> 00:01:25,920
uh so what like why do we have a

00:01:23,920 --> 00:01:26,560
business or an organization of any sort

00:01:25,920 --> 00:01:28,799
you know what

00:01:26,560 --> 00:01:30,079
and this is basically to come together

00:01:28,799 --> 00:01:32,799
to

00:01:30,079 --> 00:01:33,439
uh become very efficient at certain

00:01:32,799 --> 00:01:38,000
tasks

00:01:33,439 --> 00:01:41,280
right and in a quintessential

00:01:38,000 --> 00:01:44,399
example business you may have certain

00:01:41,280 --> 00:01:44,960
um what do you call them divisions such

00:01:44,399 --> 00:01:49,439
as

00:01:44,960 --> 00:01:51,360
finance sales support engineering which

00:01:49,439 --> 00:01:54,079
you know builds things and you have

00:01:51,360 --> 00:01:56,240
these overarching responsibilities and

00:01:54,079 --> 00:01:59,680
we do this division of labor

00:01:56,240 --> 00:02:03,840
in such a way that we end up with

00:01:59,680 --> 00:02:05,200
a communication between these divisions

00:02:03,840 --> 00:02:08,160
but we also end up with a bunch of

00:02:05,200 --> 00:02:09,840
communication within these divisions

00:02:08,160 --> 00:02:11,360
now communication between these

00:02:09,840 --> 00:02:13,040
divisions is

00:02:11,360 --> 00:02:14,800
important because this is typically

00:02:13,040 --> 00:02:16,400
things such as um

00:02:14,800 --> 00:02:18,879
aside from the examples on the screen

00:02:16,400 --> 00:02:19,840
here you know uh when will you be done

00:02:18,879 --> 00:02:22,239
these things uh

00:02:19,840 --> 00:02:24,319
how can we coordinate our efforts how

00:02:22,239 --> 00:02:27,599
can we ensure that we're all working

00:02:24,319 --> 00:02:29,360
together to achieve the same things

00:02:27,599 --> 00:02:31,920
and what you're typically going to see

00:02:29,360 --> 00:02:32,480
is a structure that looks something like

00:02:31,920 --> 00:02:35,680
this

00:02:32,480 --> 00:02:38,400
at the business communication level and

00:02:35,680 --> 00:02:39,519
this is where we have very intensive

00:02:38,400 --> 00:02:43,440
communication

00:02:39,519 --> 00:02:45,599
within a business unit and within a team

00:02:43,440 --> 00:02:46,640
right the sub teams that form up these

00:02:45,599 --> 00:02:48,560
units

00:02:46,640 --> 00:02:50,560
and we have communication between them

00:02:48,560 --> 00:02:53,360
as well but this inter

00:02:50,560 --> 00:02:54,239
team inter-unit communication tends to

00:02:53,360 --> 00:02:57,040
be

00:02:54,239 --> 00:02:59,760
a bit a bit more formal a bit less

00:02:57,040 --> 00:03:03,040
frequent a bit sparser

00:02:59,760 --> 00:03:05,040
but this model here is uh is is

00:03:03,040 --> 00:03:06,800
replicated in many different parts of

00:03:05,040 --> 00:03:10,800
businesses and society

00:03:06,800 --> 00:03:10,800
all across our organizations

00:03:11,040 --> 00:03:14,560
so an implementation communication

00:03:13,760 --> 00:03:17,360
structure

00:03:14,560 --> 00:03:20,319
relates to this business structure and

00:03:17,360 --> 00:03:22,239
this is how do we optimize the processes

00:03:20,319 --> 00:03:24,159
that lead to whatever it is we're trying

00:03:22,239 --> 00:03:27,280
to do

00:03:24,159 --> 00:03:29,280
an implementation such as uh this

00:03:27,280 --> 00:03:30,159
example here this is an example this is

00:03:29,280 --> 00:03:34,159
a

00:03:30,159 --> 00:03:36,080
business process uh model notation

00:03:34,159 --> 00:03:37,200
uh example in this one where we're

00:03:36,080 --> 00:03:40,239
looking at

00:03:37,200 --> 00:03:41,519
uh fulfilling payments now obviously

00:03:40,239 --> 00:03:43,040
real payment fulfillment

00:03:41,519 --> 00:03:44,560
will probably be a bit more complicated

00:03:43,040 --> 00:03:46,480
than this but of course i had to fit

00:03:44,560 --> 00:03:50,080
this on my slide

00:03:46,480 --> 00:03:53,519
but what we have here is we have a

00:03:50,080 --> 00:03:55,360
more detailed description along with

00:03:53,519 --> 00:03:57,760
some of the processes involved

00:03:55,360 --> 00:03:59,040
of of how we want to fulfill this

00:03:57,760 --> 00:04:01,920
business function

00:03:59,040 --> 00:04:02,640
and this is leading towards how we

00:04:01,920 --> 00:04:06,319
implement

00:04:02,640 --> 00:04:07,280
this so in this case you can see that we

00:04:06,319 --> 00:04:11,360
have

00:04:07,280 --> 00:04:13,840
the uh implement the bpmn

00:04:11,360 --> 00:04:15,840
diagram from the previous slide we have

00:04:13,840 --> 00:04:19,359
this available

00:04:15,840 --> 00:04:22,639
at the top of our screen and this is

00:04:19,359 --> 00:04:25,520
basically illustrating that this is uh

00:04:22,639 --> 00:04:28,080
going into the these monoliths here and

00:04:25,520 --> 00:04:31,440
one of those are the other

00:04:28,080 --> 00:04:34,720
and the implementation

00:04:31,440 --> 00:04:36,880
of this business process goes into

00:04:34,720 --> 00:04:38,000
our application code goes into our data

00:04:36,880 --> 00:04:39,759
structures

00:04:38,000 --> 00:04:41,120
creates new data structures new data

00:04:39,759 --> 00:04:43,520
models and

00:04:41,120 --> 00:04:45,120
it becomes a hardened and it hardens

00:04:43,520 --> 00:04:48,000
around

00:04:45,120 --> 00:04:49,600
that what we that process that we want

00:04:48,000 --> 00:04:51,600
to do

00:04:49,600 --> 00:04:53,280
now there's many different ways you can

00:04:51,600 --> 00:04:56,320
implement

00:04:53,280 --> 00:04:57,680
you can like many fast-moving startup

00:04:56,320 --> 00:04:59,040
companies perhaps you

00:04:57,680 --> 00:05:00,800
you end up with perhaps more of a

00:04:59,040 --> 00:05:03,280
spaghetti monolith

00:05:00,800 --> 00:05:05,280
with a lot of tangle dependencies you

00:05:03,280 --> 00:05:07,759
may end up with something a bit more

00:05:05,280 --> 00:05:11,120
uh delineated where you have independent

00:05:07,759 --> 00:05:12,800
modules and more separation of concerns

00:05:11,120 --> 00:05:14,160
obviously there's many many many ways

00:05:12,800 --> 00:05:15,759
you can implement something

00:05:14,160 --> 00:05:18,800
these are just some of the common ones

00:05:15,759 --> 00:05:21,440
that you'll see here and there

00:05:18,800 --> 00:05:22,160
but one of the reasons why i chose the

00:05:21,440 --> 00:05:25,520
monolith

00:05:22,160 --> 00:05:28,960
is that i think it is probably the most

00:05:25,520 --> 00:05:30,240
familiar to everybody as a whole

00:05:28,960 --> 00:05:32,240
i mean obviously it depends on your

00:05:30,240 --> 00:05:34,960
audience um

00:05:32,240 --> 00:05:36,800
and it's not even a bad way to do things

00:05:34,960 --> 00:05:41,120
but there's a certain

00:05:36,800 --> 00:05:41,120
uh there's certain things about it that

00:05:42,000 --> 00:05:48,240
that induce you

00:05:45,120 --> 00:05:50,320
to put that implementation alongside all

00:05:48,240 --> 00:05:53,440
of your previous ones

00:05:50,320 --> 00:05:55,199
so in this case we have a new

00:05:53,440 --> 00:05:57,199
thing we want to do as a business this

00:05:55,199 --> 00:06:00,560
is a new thing we want to do

00:05:57,199 --> 00:06:02,560
um whatever it may be completely

00:06:00,560 --> 00:06:05,120
arbitrary

00:06:02,560 --> 00:06:07,120
usually we add it into a monolith or

00:06:05,120 --> 00:06:10,160
we'll add it into another existing

00:06:07,120 --> 00:06:12,080
service and

00:06:10,160 --> 00:06:14,160
this is not even necessarily because

00:06:12,080 --> 00:06:16,160
this is the best place for it to go

00:06:14,160 --> 00:06:19,840
maybe it is maybe it isn't and best is

00:06:16,160 --> 00:06:21,600
obviously a bit of a subjective word

00:06:19,840 --> 00:06:24,080
one of the reasons why it often ends up

00:06:21,600 --> 00:06:28,319
in there is because you need

00:06:24,080 --> 00:06:30,639
data or you need access to data

00:06:28,319 --> 00:06:33,280
from other modules other components

00:06:30,639 --> 00:06:36,319
other data structures within your

00:06:33,280 --> 00:06:40,080
service and that

00:06:36,319 --> 00:06:43,199
alone the fact that it may be there

00:06:40,080 --> 00:06:43,759
only or in part because of getting

00:06:43,199 --> 00:06:46,880
access

00:06:43,759 --> 00:06:47,360
to existing data sets and structures is

00:06:46,880 --> 00:06:50,479
a very

00:06:47,360 --> 00:06:50,960
limiting factor and this is where we

00:06:50,479 --> 00:06:54,240
look

00:06:50,960 --> 00:06:56,080
towards the data communication structure

00:06:54,240 --> 00:06:57,759
and it's quite simple of a question

00:06:56,080 --> 00:06:58,960
right how do we get the data we need for

00:06:57,759 --> 00:07:03,280
business processes

00:06:58,960 --> 00:07:07,120
and this is an of an abstract question

00:07:03,280 --> 00:07:08,160
it's um you know at the high level

00:07:07,120 --> 00:07:10,800
but there's certain things we're

00:07:08,160 --> 00:07:13,039
generally looking for we want

00:07:10,800 --> 00:07:14,720
uh i just realized now there's one thing

00:07:13,039 --> 00:07:16,160
i should add before i mention these

00:07:14,720 --> 00:07:20,000
three but we want it to be

00:07:16,160 --> 00:07:22,400
the correct data we want it to be

00:07:20,000 --> 00:07:24,000
refresh data so we don't want a copy

00:07:22,400 --> 00:07:26,240
that's you know

00:07:24,000 --> 00:07:27,440
maybe an hour old or a day old or a week

00:07:26,240 --> 00:07:29,440
old

00:07:27,440 --> 00:07:32,960
we generally want it to be as as

00:07:29,440 --> 00:07:35,840
relevant and as pertinent as possible

00:07:32,960 --> 00:07:37,520
we need to be able to access data at

00:07:35,840 --> 00:07:40,880
scale and this is not just by

00:07:37,520 --> 00:07:44,000
volume but from different

00:07:40,880 --> 00:07:46,080
business domains now obviously

00:07:44,000 --> 00:07:47,680
your requirements may vary but for

00:07:46,080 --> 00:07:50,800
example one simple requirement

00:07:47,680 --> 00:07:54,240
is you want to take things such as

00:07:50,800 --> 00:07:57,520
sales uh inventory

00:07:54,240 --> 00:07:59,280
and predicting like uh

00:07:57,520 --> 00:08:01,199
take those things group them together

00:07:59,280 --> 00:08:02,400
and then figure out predictively what

00:08:01,199 --> 00:08:05,840
things do we need to

00:08:02,400 --> 00:08:08,560
purchase uh more of in the future

00:08:05,840 --> 00:08:08,960
an ease of access and this is just to

00:08:08,560 --> 00:08:10,479
make

00:08:08,960 --> 00:08:12,000
like what's the point of having a data

00:08:10,479 --> 00:08:17,280
communication layer if it's hard to get

00:08:12,000 --> 00:08:21,280
the data so traditional data access

00:08:17,280 --> 00:08:23,919
uh there's a few ways to look at it but

00:08:21,280 --> 00:08:25,520
so we copied one i gave you like a sneak

00:08:23,919 --> 00:08:26,160
preview there of one right the monolith

00:08:25,520 --> 00:08:27,440
and you're gonna

00:08:26,160 --> 00:08:29,440
you might add stuff there because you

00:08:27,440 --> 00:08:30,879
want to get that data out of the

00:08:29,440 --> 00:08:32,640
existing monolith

00:08:30,879 --> 00:08:34,159
uh but other other things like right

00:08:32,640 --> 00:08:34,880
like where do i get where do i get this

00:08:34,159 --> 00:08:37,839
data

00:08:34,880 --> 00:08:39,120
what is this and where do i get it so

00:08:37,839 --> 00:08:40,959
many of you

00:08:39,120 --> 00:08:43,760
are probably familiar with what i call

00:08:40,959 --> 00:08:46,880
an ad hoc data communication structure

00:08:43,760 --> 00:08:50,080
and this is uh data

00:08:46,880 --> 00:08:52,399
that you you've gotten it on your own

00:08:50,080 --> 00:08:54,160
or or you got it because you know your

00:08:52,399 --> 00:08:55,839
friend in the other cubicle over there

00:08:54,160 --> 00:08:57,839
knows hey if you go to this service they

00:08:55,839 --> 00:09:00,560
seem to have copies of it

00:08:57,839 --> 00:09:02,399
and there's many these are not a

00:09:00,560 --> 00:09:04,160
comprehensive list but

00:09:02,399 --> 00:09:07,279
there are many different ways to get

00:09:04,160 --> 00:09:09,600
this data and they tend to be

00:09:07,279 --> 00:09:10,640
very purpose built and they kind of fall

00:09:09,600 --> 00:09:13,519
under the idea

00:09:10,640 --> 00:09:14,399
of you reach in and you grab the data

00:09:13,519 --> 00:09:16,080
you need

00:09:14,399 --> 00:09:17,519
and you do whatever it takes to get that

00:09:16,080 --> 00:09:20,800
data

00:09:17,519 --> 00:09:23,920
now sometimes you end up with a bit more

00:09:20,800 --> 00:09:26,800
uh cooperation let's call it and you can

00:09:23,920 --> 00:09:28,000
get maybe your monolith your major big

00:09:26,800 --> 00:09:29,200
upstream monolith that you need to get

00:09:28,000 --> 00:09:30,560
this data from

00:09:29,200 --> 00:09:32,240
you can get them to make some read-only

00:09:30,560 --> 00:09:33,680
replicas

00:09:32,240 --> 00:09:35,600
right or you can get them to schedule

00:09:33,680 --> 00:09:38,080
dumps of data at a period and

00:09:35,600 --> 00:09:39,760
you know maybe you can get these things

00:09:38,080 --> 00:09:40,640
but you still end up with certain issues

00:09:39,760 --> 00:09:42,640
around

00:09:40,640 --> 00:09:44,000
uh you're you're still coupling on the

00:09:42,640 --> 00:09:47,279
internal data model

00:09:44,000 --> 00:09:49,200
uh you're still reliant on uh coupling

00:09:47,279 --> 00:09:50,800
to their read-only replica

00:09:49,200 --> 00:09:52,640
and perhaps you have to share this

00:09:50,800 --> 00:09:54,959
replica with many other teams and

00:09:52,640 --> 00:09:56,800
and your performance needs and theirs

00:09:54,959 --> 00:09:59,120
like clash

00:09:56,800 --> 00:10:01,040
and so there's a variety of things you

00:09:59,120 --> 00:10:03,680
can do but all of these are sort of

00:10:01,040 --> 00:10:04,240
a band-aid and a lot of these are simply

00:10:03,680 --> 00:10:07,519
because

00:10:04,240 --> 00:10:08,640
you're using the implementation which is

00:10:07,519 --> 00:10:11,440
really good

00:10:08,640 --> 00:10:13,440
at getting a business process you know

00:10:11,440 --> 00:10:17,040
done up in a fast efficient way

00:10:13,440 --> 00:10:20,000
but it's not very good at helping other

00:10:17,040 --> 00:10:21,440
processes that may exist in the company

00:10:20,000 --> 00:10:24,560
get read-only access

00:10:21,440 --> 00:10:26,560
to that underlying data and of course

00:10:24,560 --> 00:10:27,519
this is what the entire etl industry is

00:10:26,560 --> 00:10:31,279
built on

00:10:27,519 --> 00:10:33,200
uh taking data from databases

00:10:31,279 --> 00:10:34,320
transforming it and putting it somewhere

00:10:33,200 --> 00:10:36,560
else and

00:10:34,320 --> 00:10:37,839
i mean there's there's a lot of them and

00:10:36,560 --> 00:10:41,040
they all go left to right

00:10:37,839 --> 00:10:44,000
for whatever reason and so this

00:10:41,040 --> 00:10:44,959
is not a problem that has been

00:10:44,000 --> 00:10:48,320
conclusively

00:10:44,959 --> 00:10:49,920
solved and it is a problem that

00:10:48,320 --> 00:10:53,040
i think that a lot of the existing

00:10:49,920 --> 00:10:55,120
solutions of making it easier just to

00:10:53,040 --> 00:10:57,120
extract data and put it somewhere else

00:10:55,120 --> 00:10:59,680
they're sort of missing the point here

00:10:57,120 --> 00:11:01,279
and the point here is that

00:10:59,680 --> 00:11:02,880
why do we need all these custom built

00:11:01,279 --> 00:11:05,839
tools to do all this when

00:11:02,880 --> 00:11:07,360
we can just be a bit more preventative

00:11:05,839 --> 00:11:10,959
at the start

00:11:07,360 --> 00:11:12,320
and make data more accessible and data

00:11:10,959 --> 00:11:15,120
is typically

00:11:12,320 --> 00:11:17,360
treated a bit as a second-class citizen

00:11:15,120 --> 00:11:20,959
uh it's sort of a form of exhaust

00:11:17,360 --> 00:11:23,250
that uh or a byproduct that

00:11:20,959 --> 00:11:24,800
comes off of a um

00:11:23,250 --> 00:11:28,560
[Music]

00:11:24,800 --> 00:11:30,079
comes off of an implementation and

00:11:28,560 --> 00:11:32,240
and there's just not a very good

00:11:30,079 --> 00:11:34,240
cohesive strategy around it i typically

00:11:32,240 --> 00:11:35,680
have found

00:11:34,240 --> 00:11:37,600
so now i'm going to take an aside into

00:11:35,680 --> 00:11:40,640
event driven basics just because this

00:11:37,600 --> 00:11:42,959
underpins how we're going to look at

00:11:40,640 --> 00:11:46,480
the monolith or sorry not the monolith

00:11:42,959 --> 00:11:48,720
the data communication layer sorry sorry

00:11:46,480 --> 00:11:49,600
so the first important thing is that uh

00:11:48,720 --> 00:11:51,920
the events

00:11:49,600 --> 00:11:53,680
of the data communication layer the

00:11:51,920 --> 00:11:54,480
event streams that we're looking to

00:11:53,680 --> 00:11:56,639
build

00:11:54,480 --> 00:11:58,000
they're business facts so these are

00:11:56,639 --> 00:11:59,040
things that have happened in their

00:11:58,000 --> 00:12:03,200
business

00:11:59,040 --> 00:12:04,800
that are important and that underpin

00:12:03,200 --> 00:12:06,639
exactly what it is we're doing as a

00:12:04,800 --> 00:12:10,320
business so for

00:12:06,639 --> 00:12:13,680
uh for example in the e-commerce space

00:12:10,320 --> 00:12:15,760
you may have uh events that declare

00:12:13,680 --> 00:12:17,519
how many units of stock a certain

00:12:15,760 --> 00:12:19,040
product has

00:12:17,519 --> 00:12:22,160
and you may also have events that

00:12:19,040 --> 00:12:24,000
declare this user has placed this order

00:12:22,160 --> 00:12:26,800
or this user has done this thing

00:12:24,000 --> 00:12:28,560
or this store has sold out of these

00:12:26,800 --> 00:12:32,320
things

00:12:28,560 --> 00:12:34,560
and these facts though they underpin uh

00:12:32,320 --> 00:12:36,639
they are the building blocks that we use

00:12:34,560 --> 00:12:40,079
to to share

00:12:36,639 --> 00:12:42,800
what's going on in our organization

00:12:40,079 --> 00:12:44,480
now this is a short presentation so i

00:12:42,800 --> 00:12:45,680
can't really get deeply into modeling

00:12:44,480 --> 00:12:49,279
stuff

00:12:45,680 --> 00:12:53,040
but in a nutshell we would focus on

00:12:49,279 --> 00:12:55,600
business entities and entities are

00:12:53,040 --> 00:12:56,320
things uh like i gave in that previous

00:12:55,600 --> 00:13:00,000
slide there

00:12:56,320 --> 00:13:03,279
like uh the shop the user uh

00:13:00,000 --> 00:13:05,040
payments um these are

00:13:03,279 --> 00:13:06,800
generally objects that you're dealing

00:13:05,040 --> 00:13:10,079
with

00:13:06,800 --> 00:13:11,440
non-entity events tend to be things that

00:13:10,079 --> 00:13:15,680
are relationships

00:13:11,440 --> 00:13:17,600
between these entities and

00:13:15,680 --> 00:13:19,279
there's a very simple flight example

00:13:17,600 --> 00:13:20,000
here and it's not always cut and dry

00:13:19,279 --> 00:13:22,160
it's a little bit

00:13:20,000 --> 00:13:24,079
like it's a little bit of science a

00:13:22,160 --> 00:13:27,440
little bit of art

00:13:24,079 --> 00:13:30,639
so in here we have stores and items

00:13:27,440 --> 00:13:32,480
and these are entities and they have

00:13:30,639 --> 00:13:34,240
uh all the properties about the store

00:13:32,480 --> 00:13:37,839
and all the properties about the item

00:13:34,240 --> 00:13:40,800
contained within them now this sales

00:13:37,839 --> 00:13:43,199
event stream has a relationship right we

00:13:40,800 --> 00:13:44,800
we have sold

00:13:43,199 --> 00:13:46,959
this item from this store and here's

00:13:44,800 --> 00:13:51,199
some price info and some payment info

00:13:46,959 --> 00:13:54,399
etc etc now you could leave sales

00:13:51,199 --> 00:13:57,199
without a primary key but you may be

00:13:54,399 --> 00:13:58,639
concerned about specific fields of a

00:13:57,199 --> 00:14:00,000
sale

00:13:58,639 --> 00:14:01,680
and you know maybe you want to issue

00:14:00,000 --> 00:14:03,360
refunds in the future and so you need

00:14:01,680 --> 00:14:04,959
like a receipt id

00:14:03,360 --> 00:14:06,399
and so you could also model it as an

00:14:04,959 --> 00:14:09,440
entity now

00:14:06,399 --> 00:14:10,800
these modelings depend on your business

00:14:09,440 --> 00:14:12,320
like how you're going to model it if

00:14:10,800 --> 00:14:14,079
it's going to be an entity or if it's

00:14:12,320 --> 00:14:15,120
going to be an event

00:14:14,079 --> 00:14:17,360
you know what are these going to look

00:14:15,120 --> 00:14:18,240
like and just for clarity's sake here

00:14:17,360 --> 00:14:21,279
entities

00:14:18,240 --> 00:14:24,320
are events it's just that an entity

00:14:21,279 --> 00:14:26,160
is a unique uh representation of

00:14:24,320 --> 00:14:28,160
something whereas an event

00:14:26,160 --> 00:14:30,240
uh you know there could be many events

00:14:28,160 --> 00:14:32,959
many sales

00:14:30,240 --> 00:14:34,240
whether you don't have that primary key

00:14:32,959 --> 00:14:37,760
now if you're familiar with the

00:14:34,240 --> 00:14:40,480
star and snowflake data modeling

00:14:37,760 --> 00:14:43,199
in warehousing stream modeling is very

00:14:40,480 --> 00:14:46,240
similar to that

00:14:43,199 --> 00:14:47,839
uh but once you have these entities what

00:14:46,240 --> 00:14:50,079
do you like why do we have these

00:14:47,839 --> 00:14:51,920
what are these facts for well this is

00:14:50,079 --> 00:14:54,959
where we start looking at the

00:14:51,920 --> 00:14:58,079
event broker with these event streams as

00:14:54,959 --> 00:14:59,839
the single source of truth

00:14:58,079 --> 00:15:02,480
and this is significant and this is a

00:14:59,839 --> 00:15:05,199
bit of a cultural mental shift

00:15:02,480 --> 00:15:07,519
because we want to ensure that people

00:15:05,199 --> 00:15:09,600
have the ability

00:15:07,519 --> 00:15:11,199
services i should say or people have the

00:15:09,600 --> 00:15:14,079
ability to

00:15:11,199 --> 00:15:15,279
source their data from one reliable

00:15:14,079 --> 00:15:16,880
place

00:15:15,279 --> 00:15:18,959
they don't need to go looking this is

00:15:16,880 --> 00:15:20,959
part of that ease of use

00:15:18,959 --> 00:15:22,880
for getting this data and this becomes a

00:15:20,959 --> 00:15:25,279
very very powerful pattern

00:15:22,880 --> 00:15:26,800
because a consumer no longer needs to

00:15:25,279 --> 00:15:29,279
worry about

00:15:26,800 --> 00:15:30,959
is this the truth is this the right

00:15:29,279 --> 00:15:32,959
source is it going to be updated is it

00:15:30,959 --> 00:15:34,560
going to be maintained monitored managed

00:15:32,959 --> 00:15:38,079
et cetera

00:15:34,560 --> 00:15:40,240
now the caveat is that the producer

00:15:38,079 --> 00:15:42,079
that owns the original creation of this

00:15:40,240 --> 00:15:43,759
data so whether it's the blue database

00:15:42,079 --> 00:15:45,279
or the red database or the black one

00:15:43,759 --> 00:15:48,240
here

00:15:45,279 --> 00:15:48,959
they too need to ensure that they

00:15:48,240 --> 00:15:50,880
produce

00:15:48,959 --> 00:15:54,399
the truth the whole truth and nothing

00:15:50,880 --> 00:15:55,839
but the truth

00:15:54,399 --> 00:15:57,440
now for those of you who may not be

00:15:55,839 --> 00:16:00,160
familiar with uh

00:15:57,440 --> 00:16:02,079
the event event streams or mutable logs

00:16:00,160 --> 00:16:04,560
um

00:16:02,079 --> 00:16:05,839
just to be clear we're talking about the

00:16:04,560 --> 00:16:07,920
immutable log

00:16:05,839 --> 00:16:10,160
we are not talking about a queue and

00:16:07,920 --> 00:16:12,240
we're not talking about cues where

00:16:10,160 --> 00:16:13,279
when a service consumes the event it's

00:16:12,240 --> 00:16:15,199
deleted

00:16:13,279 --> 00:16:17,040
these are not deleted events these are

00:16:15,199 --> 00:16:20,720
facts that are published

00:16:17,040 --> 00:16:21,920
in an append only way so the partitions

00:16:20,720 --> 00:16:25,759
and

00:16:21,920 --> 00:16:26,800
in the streams here data is added to

00:16:25,759 --> 00:16:29,600
them

00:16:26,800 --> 00:16:29,839
and it only ever gets larger with uh you

00:16:29,600 --> 00:16:31,199
know

00:16:29,839 --> 00:16:32,800
within reason you can there's some

00:16:31,199 --> 00:16:33,199
things you can do to ensure they don't

00:16:32,800 --> 00:16:36,720
grow

00:16:33,199 --> 00:16:39,040
a ridiculous amount but basically

00:16:36,720 --> 00:16:41,279
the services that consume from them are

00:16:39,040 --> 00:16:42,000
responsible for maintaining their own

00:16:41,279 --> 00:16:45,279
pointers

00:16:42,000 --> 00:16:48,079
as to where they've consumed from and

00:16:45,279 --> 00:16:49,680
they manage uh they're responsible for

00:16:48,079 --> 00:16:53,120
ingesting their data

00:16:49,680 --> 00:16:53,920
so again this is not cues and this data

00:16:53,120 --> 00:16:56,959
is not

00:16:53,920 --> 00:16:59,199
temporary it is permanent it is kept

00:16:56,959 --> 00:17:02,240
there as a permanent record

00:16:59,199 --> 00:17:05,039
such that a new service coming up

00:17:02,240 --> 00:17:06,959
or a service that had some bugs in it

00:17:05,039 --> 00:17:09,120
they can go back to a point in time and

00:17:06,959 --> 00:17:10,959
reprocess all that information again

00:17:09,120 --> 00:17:13,679
without having to disturb any other

00:17:10,959 --> 00:17:15,439
services or systems

00:17:13,679 --> 00:17:17,199
we also use these partitions that i

00:17:15,439 --> 00:17:19,120
alluded to on the previous slide

00:17:17,199 --> 00:17:21,439
we use them for scalability and for data

00:17:19,120 --> 00:17:24,160
locality so if we're dealing with

00:17:21,439 --> 00:17:25,439
you know hundreds of terabytes of data

00:17:24,160 --> 00:17:29,039
any particular

00:17:25,439 --> 00:17:32,160
instance of a service can subscribe to

00:17:29,039 --> 00:17:34,080
just a small subset of that information

00:17:32,160 --> 00:17:35,679
and process it locally so there's some

00:17:34,080 --> 00:17:38,000
data locality

00:17:35,679 --> 00:17:39,200
options that are presented there and

00:17:38,000 --> 00:17:42,880
this can help you also

00:17:39,200 --> 00:17:44,320
ensure you have sufficient throughput to

00:17:42,880 --> 00:17:44,960
make sure you can keep up to date with

00:17:44,320 --> 00:17:47,120
events of

00:17:44,960 --> 00:17:49,039
up to date with changes and again i kind

00:17:47,120 --> 00:17:51,679
of have to go through this bit a little

00:17:49,039 --> 00:17:55,600
uh quickly because you know it's like i

00:17:51,679 --> 00:17:55,600
said a shorter a shorter time slot

00:17:55,919 --> 00:18:00,080
importantly though the producer has a

00:17:58,000 --> 00:18:03,840
few responsibilities

00:18:00,080 --> 00:18:08,000
now the producer needs to ensure

00:18:03,840 --> 00:18:11,280
that all data of a given key

00:18:08,000 --> 00:18:14,559
a key is a unique id that represents

00:18:11,280 --> 00:18:18,160
that entity so for example um

00:18:14,559 --> 00:18:18,720
my let's say my my name my country of

00:18:18,160 --> 00:18:22,160
birth

00:18:18,720 --> 00:18:23,840
my birth date and uh whatever

00:18:22,160 --> 00:18:25,520
nation or if i have like a national

00:18:23,840 --> 00:18:28,000
identifier number of some sort that

00:18:25,520 --> 00:18:30,880
would be like my key

00:18:28,000 --> 00:18:32,400
so all events for me have to go to the

00:18:30,880 --> 00:18:34,960
same partition

00:18:32,400 --> 00:18:36,080
all of them now what this ensures is

00:18:34,960 --> 00:18:37,919
data locality

00:18:36,080 --> 00:18:40,559
so if you want to know anything about me

00:18:37,919 --> 00:18:42,559
you only have to listen to one partition

00:18:40,559 --> 00:18:44,160
any updates for it will it be there in

00:18:42,559 --> 00:18:45,760
sequential order

00:18:44,160 --> 00:18:47,919
so this is very very important for

00:18:45,760 --> 00:18:49,760
ensuring that consistent ordering

00:18:47,919 --> 00:18:53,679
and to make sure that you know where all

00:18:49,760 --> 00:18:56,880
that data is for any given key

00:18:53,679 --> 00:18:57,440
the producer is also responsibility or

00:18:56,880 --> 00:19:00,799
sorry is

00:18:57,440 --> 00:19:02,720
also responsible for consistency

00:19:00,799 --> 00:19:04,400
and the data can be eventually

00:19:02,720 --> 00:19:07,760
consistent

00:19:04,400 --> 00:19:09,919
but it has to be consistent it can't be

00:19:07,760 --> 00:19:11,360
wrong the producer can't write stuff to

00:19:09,919 --> 00:19:12,400
the event stream that it doesn't use

00:19:11,360 --> 00:19:15,679
internally

00:19:12,400 --> 00:19:20,559
and anything it uses internally that

00:19:15,679 --> 00:19:22,000
should be exposed must be exposed

00:19:20,559 --> 00:19:24,559
so i'll talk a little bit more about

00:19:22,000 --> 00:19:28,400
those boundaries later but

00:19:24,559 --> 00:19:30,000
uh if the the basics is that if the

00:19:28,400 --> 00:19:31,440
producer has a different set of

00:19:30,000 --> 00:19:33,520
information

00:19:31,440 --> 00:19:34,960
than the event stream and it never

00:19:33,520 --> 00:19:38,080
reconciles

00:19:34,960 --> 00:19:40,080
um you have two sources of truth

00:19:38,080 --> 00:19:42,080
and when you have two sources of truth

00:19:40,080 --> 00:19:44,799
consistency goes out the window

00:19:42,080 --> 00:19:46,880
um and you can have a lot of data

00:19:44,799 --> 00:19:47,760
consistency problems amongst downstream

00:19:46,880 --> 00:19:49,840
systems and

00:19:47,760 --> 00:19:53,360
and it just becomes very messy so this

00:19:49,840 --> 00:19:56,240
is a this is an absolute requirement

00:19:53,360 --> 00:19:57,360
additionally uh although some people i

00:19:56,240 --> 00:19:59,520
have seen in some

00:19:57,360 --> 00:20:01,280
teams and architects have break this

00:19:59,520 --> 00:20:02,640
principle i do not recommend it

00:20:01,280 --> 00:20:05,840
single writer principle is pretty

00:20:02,640 --> 00:20:08,640
straightforward uh for any given entity

00:20:05,840 --> 00:20:11,120
or event stream there is only one

00:20:08,640 --> 00:20:12,880
owner one single solitary owner that can

00:20:11,120 --> 00:20:14,640
write to it

00:20:12,880 --> 00:20:16,559
you don't let another service overwrite

00:20:14,640 --> 00:20:20,080
your data again this is how you get

00:20:16,559 --> 00:20:23,520
conflicting sources of truth

00:20:20,080 --> 00:20:26,799
now schemas are

00:20:23,520 --> 00:20:28,720
typically used and should be used when

00:20:26,799 --> 00:20:32,000
producing this data

00:20:28,720 --> 00:20:35,919
this is very very important because the

00:20:32,000 --> 00:20:36,320
intention is that the any consumer of

00:20:35,919 --> 00:20:39,679
this

00:20:36,320 --> 00:20:44,159
data should have enough context from

00:20:39,679 --> 00:20:47,280
the data itself and from all of the

00:20:44,159 --> 00:20:48,640
metadata descriptions taggings etc of

00:20:47,280 --> 00:20:50,480
the schema

00:20:48,640 --> 00:20:51,679
to fully interpret this data and to

00:20:50,480 --> 00:20:55,200
fully understand

00:20:51,679 --> 00:20:57,360
what it's reading if you you never

00:20:55,200 --> 00:20:59,440
you never write anything in there that

00:20:57,360 --> 00:21:01,520
can't be easily understood by the

00:20:59,440 --> 00:21:02,080
consumer because the point here is to

00:21:01,520 --> 00:21:04,400
provide

00:21:02,080 --> 00:21:05,840
that flexibility but also things such as

00:21:04,400 --> 00:21:07,760
data discovery

00:21:05,840 --> 00:21:09,440
um ensuring that consumers can

00:21:07,760 --> 00:21:12,559
understand any any

00:21:09,440 --> 00:21:15,840
uh corner cases that may be in the data

00:21:12,559 --> 00:21:16,880
so schemas whether apache avro which is

00:21:15,840 --> 00:21:20,000
the one that i

00:21:16,880 --> 00:21:22,080
favor or google protobuf which is also

00:21:20,000 --> 00:21:23,200
quite quite good from what i what i've

00:21:22,080 --> 00:21:24,799
seen

00:21:23,200 --> 00:21:27,840
those would be the kind of schemas

00:21:24,799 --> 00:21:30,480
you're looking to use

00:21:27,840 --> 00:21:31,840
so now for the consumer responsibilities

00:21:30,480 --> 00:21:33,840
one is the offsets

00:21:31,840 --> 00:21:36,240
consumer manages its own offsets from

00:21:33,840 --> 00:21:39,200
the streams it's reading from

00:21:36,240 --> 00:21:40,080
if it wants to reread stuff it's up to

00:21:39,200 --> 00:21:41,679
the consumer

00:21:40,080 --> 00:21:43,360
if it wants to store these offsets

00:21:41,679 --> 00:21:45,440
durably

00:21:43,360 --> 00:21:47,600
typically whether using like apache

00:21:45,440 --> 00:21:50,240
kafka pulsar kinesis all of those

00:21:47,600 --> 00:21:51,840
uh event brokers they will they will let

00:21:50,240 --> 00:21:53,600
you store it internally

00:21:51,840 --> 00:21:55,919
or you can also go store it in durable

00:21:53,600 --> 00:21:58,559
storage yourself

00:21:55,919 --> 00:21:59,760
the consumer is also responsible for

00:21:58,559 --> 00:22:02,480
excuse me

00:21:59,760 --> 00:22:05,280
for materializing building maintaining

00:22:02,480 --> 00:22:05,280
its own state

00:22:06,400 --> 00:22:09,679
this means that the event stream doesn't

00:22:08,000 --> 00:22:11,760
care what you do with it once you get it

00:22:09,679 --> 00:22:14,320
that's on you

00:22:11,760 --> 00:22:16,320
but you are but it is not the producer's

00:22:14,320 --> 00:22:18,080
responsibility to provide you a state

00:22:16,320 --> 00:22:20,480
it's not the data communication layers

00:22:18,080 --> 00:22:23,919
responsibility to provide you a state

00:22:20,480 --> 00:22:23,919
it is strictly up to the consumer

00:22:24,080 --> 00:22:27,760
so what you may see uh for example just

00:22:27,360 --> 00:22:31,039
uh

00:22:27,760 --> 00:22:33,200
maybe this might help frame it if you

00:22:31,039 --> 00:22:36,799
have a set of users

00:22:33,200 --> 00:22:38,400
as entities right so myself adam

00:22:36,799 --> 00:22:39,760
now let's say i'm a bad i'm bad at

00:22:38,400 --> 00:22:41,120
parking and i get lots of parking

00:22:39,760 --> 00:22:43,520
tickets

00:22:41,120 --> 00:22:45,039
so parking tickets are keyed events they

00:22:43,520 --> 00:22:45,760
have a key and it's probably the ticket

00:22:45,039 --> 00:22:47,760
id

00:22:45,760 --> 00:22:49,200
but what i want to do here is i want to

00:22:47,760 --> 00:22:51,760
take all the tickets that

00:22:49,200 --> 00:22:52,799
i would have and i want to take my user

00:22:51,760 --> 00:22:55,120
which is me

00:22:52,799 --> 00:22:55,840
right so i materialize myself which

00:22:55,120 --> 00:23:00,320
means i

00:22:55,840 --> 00:23:02,400
i just go right into the state

00:23:00,320 --> 00:23:04,080
i sum up parking tickets and i store

00:23:02,400 --> 00:23:07,919
that and then i join them together

00:23:04,080 --> 00:23:10,080
and boom i get out how much money i owe

00:23:07,919 --> 00:23:13,200
so this is a simple example of how

00:23:10,080 --> 00:23:15,520
you can take disparate streams that may

00:23:13,200 --> 00:23:17,440
mean different things

00:23:15,520 --> 00:23:19,039
combine it into some sort of application

00:23:17,440 --> 00:23:22,320
and get some business value

00:23:19,039 --> 00:23:26,400
yes this looks a lot like an uh an etl

00:23:22,320 --> 00:23:29,520
pipeline an extraction transform load

00:23:26,400 --> 00:23:32,880
one of the things i have found

00:23:29,520 --> 00:23:34,720
and i have been thrilled to hear many

00:23:32,880 --> 00:23:37,360
other

00:23:34,720 --> 00:23:39,120
software developers and data developer

00:23:37,360 --> 00:23:39,760
sorry data platform developers data

00:23:39,120 --> 00:23:43,200
scientists

00:23:39,760 --> 00:23:46,559
uh observe is that

00:23:43,200 --> 00:23:50,080
in a lot of event in in pretty much

00:23:46,559 --> 00:23:51,600
all event driven systems uh and

00:23:50,080 --> 00:23:53,440
mo anything that really has to do with

00:23:51,600 --> 00:23:55,520
streaming or real-time everything's

00:23:53,440 --> 00:23:59,840
gonna look like an etl

00:23:55,520 --> 00:24:02,480
when you start mixing in users um

00:23:59,840 --> 00:24:03,279
like with uh acting synchronously

00:24:02,480 --> 00:24:05,279
through

00:24:03,279 --> 00:24:06,400
like rest apis and you start mixing that

00:24:05,279 --> 00:24:07,360
into the mix things get a little

00:24:06,400 --> 00:24:08,799
different

00:24:07,360 --> 00:24:10,480
but you're generally going to find that

00:24:08,799 --> 00:24:12,080
in any sort of environment like this if

00:24:10,480 --> 00:24:13,840
you're like hey this looks like an etl

00:24:12,080 --> 00:24:16,080
yes you're right it does because it

00:24:13,840 --> 00:24:17,520
really is

00:24:16,080 --> 00:24:19,120
but the principle here is that we want

00:24:17,520 --> 00:24:21,279
to treat this domain event

00:24:19,120 --> 00:24:22,799
as a first class citizen we want to make

00:24:21,279 --> 00:24:24,799
it available we want to make it

00:24:22,799 --> 00:24:26,640
well defined we want to make sure it's

00:24:24,799 --> 00:24:28,159
consistent we want to make sure it's

00:24:26,640 --> 00:24:32,480
available

00:24:28,159 --> 00:24:33,919
and then we can decouple and again does

00:24:32,480 --> 00:24:35,279
this mean that any everything should be

00:24:33,919 --> 00:24:36,640
written to an event stream

00:24:35,279 --> 00:24:39,840
i think you all know the answer to this

00:24:36,640 --> 00:24:41,600
no um as a real life example

00:24:39,840 --> 00:24:42,960
uh one of my previous employers that i

00:24:41,600 --> 00:24:45,120
worked at uh

00:24:42,960 --> 00:24:47,360
flip which i'm going to talk about a bit

00:24:45,120 --> 00:24:49,760
more later on here as a real life

00:24:47,360 --> 00:24:54,080
example of what we did

00:24:49,760 --> 00:24:57,600
we had some 650 odd tables

00:24:54,080 --> 00:24:59,279
i think in our monolith

00:24:57,600 --> 00:25:00,880
one of our monoliths one of our major

00:24:59,279 --> 00:25:04,320
monoliths

00:25:00,880 --> 00:25:05,919
and after we basically

00:25:04,320 --> 00:25:07,840
did the things that i just talked about

00:25:05,919 --> 00:25:08,559
in the so far in this presentation we

00:25:07,840 --> 00:25:12,159
had about

00:25:08,559 --> 00:25:16,000
60 50 maybe 60

00:25:12,159 --> 00:25:20,000
individual entity and important event

00:25:16,000 --> 00:25:22,480
streams so a lot of the data

00:25:20,000 --> 00:25:24,400
within that monolith was is still

00:25:22,480 --> 00:25:26,559
encapsulated within that monolith

00:25:24,400 --> 00:25:28,240
strictly for that monolith these are

00:25:26,559 --> 00:25:30,320
only the important things

00:25:28,240 --> 00:25:31,600
that are important to the business as a

00:25:30,320 --> 00:25:35,679
whole

00:25:31,600 --> 00:25:37,279
that we were liberating out

00:25:35,679 --> 00:25:38,720
and again yes you make your data

00:25:37,279 --> 00:25:40,480
accessible by getting it to the event

00:25:38,720 --> 00:25:42,960
broker

00:25:40,480 --> 00:25:45,120
your single source of truth uh the place

00:25:42,960 --> 00:25:47,520
where you go to get this data

00:25:45,120 --> 00:25:48,960
so how do we get this data in here

00:25:47,520 --> 00:25:51,360
because i tell you okay put it in there

00:25:48,960 --> 00:25:53,840
and you say well that's nice adam but

00:25:51,360 --> 00:25:56,320
like like you said you know we have a

00:25:53,840 --> 00:25:57,919
monolith and it has 650 tables and like

00:25:56,320 --> 00:25:58,640
that seems like a lot of work and where

00:25:57,919 --> 00:26:00,480
do we start

00:25:58,640 --> 00:26:02,640
and you know i don't even know if this

00:26:00,480 --> 00:26:06,080
is going to fly

00:26:02,640 --> 00:26:09,039
well it's true there's there's many

00:26:06,080 --> 00:26:10,559
many things to consider how do you get

00:26:09,039 --> 00:26:12,480
that data out how do you get that data

00:26:10,559 --> 00:26:13,840
out for systems that are still critical

00:26:12,480 --> 00:26:15,279
that maybe you don't even have under

00:26:13,840 --> 00:26:17,120
active development

00:26:15,279 --> 00:26:19,120
uh how do you get that data out for

00:26:17,120 --> 00:26:20,799
systems that are under extensive active

00:26:19,120 --> 00:26:22,880
development and you can only devote so

00:26:20,799 --> 00:26:25,279
much resourcing

00:26:22,880 --> 00:26:27,440
so generally and this is a gross exagger

00:26:25,279 --> 00:26:30,000
not exaggeration generalization

00:26:27,440 --> 00:26:30,880
but this is what i tend to see this is

00:26:30,000 --> 00:26:33,039
what i've seen at

00:26:30,880 --> 00:26:34,080
numerous different organizations uh and

00:26:33,039 --> 00:26:36,320
with discussions of

00:26:34,080 --> 00:26:38,480
peers and colleagues but we generally

00:26:36,320 --> 00:26:39,919
start with the forked right ante pattern

00:26:38,480 --> 00:26:42,080
someone says okay let's just put this

00:26:39,919 --> 00:26:44,880
data in the event stream this is easy

00:26:42,080 --> 00:26:45,840
we're just going to write it so i want

00:26:44,880 --> 00:26:48,960
to store this blue

00:26:45,840 --> 00:26:50,320
triangle in my data store and then i'll

00:26:48,960 --> 00:26:52,000
write it to the event stream

00:26:50,320 --> 00:26:53,679
all right so i stored it in there i go

00:26:52,000 --> 00:26:54,799
to write it to the event stream i get a

00:26:53,679 --> 00:26:56,799
failure

00:26:54,799 --> 00:26:58,000
maybe the network fails maybe the

00:26:56,799 --> 00:26:59,360
producer fails

00:26:58,000 --> 00:27:01,840
maybe it's trying to write to an event

00:26:59,360 --> 00:27:03,840
stream that doesn't exist you know

00:27:01,840 --> 00:27:06,559
any number of things can happen but

00:27:03,840 --> 00:27:07,279
eventually this producer which may be

00:27:06,559 --> 00:27:08,640
serving

00:27:07,279 --> 00:27:10,159
its own you know doing its own work in

00:27:08,640 --> 00:27:11,120
real time says okay i give up i can't

00:27:10,159 --> 00:27:14,320
handle this

00:27:11,120 --> 00:27:16,400
i'm failing but now we have a problem

00:27:14,320 --> 00:27:18,480
now we have an inconsistent source with

00:27:16,400 --> 00:27:21,520
an inconsistent event stream

00:27:18,480 --> 00:27:22,399
this is the producer's responsibility to

00:27:21,520 --> 00:27:24,240
reconcile

00:27:22,399 --> 00:27:25,840
and i mentioned this earlier but this

00:27:24,240 --> 00:27:27,520
bears repeating because this is so

00:27:25,840 --> 00:27:31,600
important

00:27:27,520 --> 00:27:34,559
that you must not you must not mix this

00:27:31,600 --> 00:27:35,840
mix this up so another option what if

00:27:34,559 --> 00:27:37,039
you write it to the event stream first

00:27:35,840 --> 00:27:38,080
well i'm sure most of you know where

00:27:37,039 --> 00:27:40,720
this is going

00:27:38,080 --> 00:27:41,679
maybe you end up with a failure there

00:27:40,720 --> 00:27:43,279
you have a bad call

00:27:41,679 --> 00:27:44,799
you know bad column definitions your own

00:27:43,279 --> 00:27:48,080
bugs etc again

00:27:44,799 --> 00:27:51,679
they're inconsistent so what can you do

00:27:48,080 --> 00:27:54,880
and this is where options come in

00:27:51,679 --> 00:27:56,399
and this is where trade-offs come in so

00:27:54,880 --> 00:27:57,679
one option

00:27:56,399 --> 00:27:59,520
well why don't you just write it to the

00:27:57,679 --> 00:28:01,520
event stream and keep trying until it

00:27:59,520 --> 00:28:04,159
gets in there

00:28:01,520 --> 00:28:07,039
and then you read it back from the event

00:28:04,159 --> 00:28:09,840
stream into your database

00:28:07,039 --> 00:28:11,679
this is one thing you can do won't this

00:28:09,840 --> 00:28:14,240
slow down the application

00:28:11,679 --> 00:28:16,000
probably won't this prevent me from

00:28:14,240 --> 00:28:18,960
using some database centric features

00:28:16,000 --> 00:28:19,440
probably is this the best way to do

00:28:18,960 --> 00:28:22,320
things

00:28:19,440 --> 00:28:22,880
not at all not necessarily in many cases

00:28:22,320 --> 00:28:25,440
it is

00:28:22,880 --> 00:28:26,399
but not in all cases but again these are

00:28:25,440 --> 00:28:30,799
the trade-offs

00:28:26,399 --> 00:28:33,440
that as a engineering team as a company

00:28:30,799 --> 00:28:34,480
as a as you know the the many different

00:28:33,440 --> 00:28:37,440
producers and

00:28:34,480 --> 00:28:38,000
consumers of data within an organization

00:28:37,440 --> 00:28:40,960
this is

00:28:38,000 --> 00:28:42,960
the important conversations to have how

00:28:40,960 --> 00:28:46,080
do we make this stuff

00:28:42,960 --> 00:28:48,080
accessible what is it that we do

00:28:46,080 --> 00:28:49,120
another option and this tends to be

00:28:48,080 --> 00:28:51,440
after the

00:28:49,120 --> 00:28:52,720
the forked right anti-pattern has uh

00:28:51,440 --> 00:28:54,960
caused some production

00:28:52,720 --> 00:28:56,799
problems let's say you know with various

00:28:54,960 --> 00:29:00,320
systems reporting different

00:28:56,799 --> 00:29:00,720
data than others and you generally have

00:29:00,320 --> 00:29:03,200
like an

00:29:00,720 --> 00:29:04,720
outage so change data capture is another

00:29:03,200 --> 00:29:07,520
popular one

00:29:04,720 --> 00:29:09,919
and what change data capture is is a

00:29:07,520 --> 00:29:12,799
purpose-built system

00:29:09,919 --> 00:29:14,320
that consumes from a data store that

00:29:12,799 --> 00:29:17,360
pulls it

00:29:14,320 --> 00:29:19,120
and processes some data and writes it to

00:29:17,360 --> 00:29:21,279
the event stream

00:29:19,120 --> 00:29:22,240
so the change data capture service is

00:29:21,279 --> 00:29:25,520
taking on the

00:29:22,240 --> 00:29:27,679
ownership role of ensuring that it's

00:29:25,520 --> 00:29:30,240
consistent

00:29:27,679 --> 00:29:32,000
now there's many advantages of this for

00:29:30,240 --> 00:29:33,039
one you can connect to pretty much any

00:29:32,000 --> 00:29:35,440
data store

00:29:33,039 --> 00:29:37,919
whether it's a relational database

00:29:35,440 --> 00:29:41,200
document database

00:29:37,919 --> 00:29:43,039
you know a nosql db there's pretty much

00:29:41,200 --> 00:29:46,320
connectors for all of these things

00:29:43,039 --> 00:29:48,240
it's scalable you can scale it to very

00:29:46,320 --> 00:29:50,320
high volumes of data

00:29:48,240 --> 00:29:51,679
you can scale it out to many different

00:29:50,320 --> 00:29:53,679
services within your company

00:29:51,679 --> 00:29:54,720
you can even hook it into legacy systems

00:29:53,679 --> 00:29:57,039
that are no longer

00:29:54,720 --> 00:29:58,640
under active development which is uh one

00:29:57,039 --> 00:30:01,760
of the big selling points

00:29:58,640 --> 00:30:03,679
that uh sold it to me

00:30:01,760 --> 00:30:04,799
originally back back when i started

00:30:03,679 --> 00:30:07,279
doing these uh

00:30:04,799 --> 00:30:08,640
things and finally of course it enables

00:30:07,279 --> 00:30:10,080
our single source of truth which is the

00:30:08,640 --> 00:30:11,679
whole point of why we'd be doing this in

00:30:10,080 --> 00:30:13,760
the first place

00:30:11,679 --> 00:30:15,760
now obviously there's some downsides the

00:30:13,760 --> 00:30:19,120
first is that this polling loop

00:30:15,760 --> 00:30:20,480
is two words and it there's a lot packed

00:30:19,120 --> 00:30:22,559
into that

00:30:20,480 --> 00:30:24,480
so there's a lot of performance impacts

00:30:22,559 --> 00:30:24,799
that can occur particularly if you want

00:30:24,480 --> 00:30:27,600
to

00:30:24,799 --> 00:30:29,360
backfill data from you know you want all

00:30:27,600 --> 00:30:30,080
the data in that database in your event

00:30:29,360 --> 00:30:32,240
stream

00:30:30,080 --> 00:30:33,760
or in a particular table i should say in

00:30:32,240 --> 00:30:34,960
the event stream

00:30:33,760 --> 00:30:37,120
well i mean you may have 10 years of

00:30:34,960 --> 00:30:38,159
data you know that could be 150

00:30:37,120 --> 00:30:39,600
terabytes

00:30:38,159 --> 00:30:41,440
so if you start querying that out of

00:30:39,600 --> 00:30:44,240
your out of your producer service

00:30:41,440 --> 00:30:45,520
if it's your main production database

00:30:44,240 --> 00:30:47,679
you can cause some very serious

00:30:45,520 --> 00:30:50,320
performance impacts

00:30:47,679 --> 00:30:51,279
additionally incremental changes if

00:30:50,320 --> 00:30:53,120
someone does

00:30:51,279 --> 00:30:55,760
a let's say you're using a rails

00:30:53,120 --> 00:30:58,240
database and they do a large migration

00:30:55,760 --> 00:30:59,440
and it touches every every row in a

00:30:58,240 --> 00:31:01,840
table

00:30:59,440 --> 00:31:04,880
well you might find out that you end up

00:31:01,840 --> 00:31:06,640
reprocessing that entire data set again

00:31:04,880 --> 00:31:08,880
and of course that can be quite

00:31:06,640 --> 00:31:11,120
problematic

00:31:08,880 --> 00:31:12,159
you couple on the internal data model

00:31:11,120 --> 00:31:15,279
because

00:31:12,159 --> 00:31:17,039
the nature of change data capture is

00:31:15,279 --> 00:31:19,200
that you reach into the database and

00:31:17,039 --> 00:31:22,480
pull out the data you need

00:31:19,200 --> 00:31:24,960
and although you can somewhat

00:31:22,480 --> 00:31:26,640
design around that typically what

00:31:24,960 --> 00:31:28,640
happens is that the internal data model

00:31:26,640 --> 00:31:30,240
just gets exposed it's quick it's fast

00:31:28,640 --> 00:31:33,440
it's easy to do

00:31:30,240 --> 00:31:35,760
but it's also a good way to

00:31:33,440 --> 00:31:37,360
introduce some unintentional coupling

00:31:35,760 --> 00:31:38,799
change data capture service does not

00:31:37,360 --> 00:31:40,399
come for free

00:31:38,799 --> 00:31:42,640
you need teams you need people you have

00:31:40,399 --> 00:31:45,360
to maintain it you have to nurse it

00:31:42,640 --> 00:31:46,240
um often times you'll end up with

00:31:45,360 --> 00:31:48,159
breakages

00:31:46,240 --> 00:31:49,840
in it from various reasons you might run

00:31:48,159 --> 00:31:51,519
out of memory you might

00:31:49,840 --> 00:31:52,640
have problems connecting to a service

00:31:51,519 --> 00:31:53,840
you might have problems connecting to

00:31:52,640 --> 00:31:56,000
the sync

00:31:53,840 --> 00:31:56,880
and there's a lot of costs associated

00:31:56,000 --> 00:32:01,279
with that

00:31:56,880 --> 00:32:02,880
both financial personal and opportunity

00:32:01,279 --> 00:32:05,039
and there's also some idiosyncrasies

00:32:02,880 --> 00:32:09,760
with it because even though

00:32:05,039 --> 00:32:12,880
we can create a single source of truth

00:32:09,760 --> 00:32:15,440
for example hard deletes

00:32:12,880 --> 00:32:17,120
if the producer just flat out deletes a

00:32:15,440 --> 00:32:21,200
row from its table

00:32:17,120 --> 00:32:24,000
it's gone when you go to pull that

00:32:21,200 --> 00:32:24,720
if you're using a polling based system

00:32:24,000 --> 00:32:26,480
it's gone

00:32:24,720 --> 00:32:28,720
data's gone you don't know it's gone

00:32:26,480 --> 00:32:31,519
because it's been hard deleted

00:32:28,720 --> 00:32:32,080
so the event stream now deviates and

00:32:31,519 --> 00:32:34,640
again

00:32:32,080 --> 00:32:35,600
once you start deviating uh that single

00:32:34,640 --> 00:32:38,240
source of truth is

00:32:35,600 --> 00:32:39,760
is less useful so it's not that it's not

00:32:38,240 --> 00:32:41,440
change data capture is not useful it's

00:32:39,760 --> 00:32:43,039
that there's a lot of caveats that you

00:32:41,440 --> 00:32:44,240
need to be aware about when you're using

00:32:43,039 --> 00:32:45,840
it

00:32:44,240 --> 00:32:47,360
but this all comes down to like what is

00:32:45,840 --> 00:32:48,559
it that we're willing to do

00:32:47,360 --> 00:32:50,720
and there's trade-offs there's

00:32:48,559 --> 00:32:53,919
performance trade-offs there's

00:32:50,720 --> 00:32:55,279
you can have the producers use other

00:32:53,919 --> 00:32:56,000
patterns that i haven't talked about

00:32:55,279 --> 00:32:57,919
here

00:32:56,000 --> 00:32:59,840
to get more consistent data so there's

00:32:57,919 --> 00:33:02,960
many many options

00:32:59,840 --> 00:33:04,399
but these all come to a how do we

00:33:02,960 --> 00:33:05,519
what data is important how are we going

00:33:04,399 --> 00:33:07,039
to get it in there what are we going to

00:33:05,519 --> 00:33:10,320
do

00:33:07,039 --> 00:33:11,600
so let's see yeah i'm pulling up against

00:33:10,320 --> 00:33:12,320
my time boundary here so i'm going to go

00:33:11,600 --> 00:33:13,679
real quick

00:33:12,320 --> 00:33:15,600
uh here's an example from flips

00:33:13,679 --> 00:33:18,240
migration flip

00:33:15,600 --> 00:33:20,720
deals with circulars and flyers uh

00:33:18,240 --> 00:33:23,919
basically like e-commerce stuff

00:33:20,720 --> 00:33:25,760
and so what we did in the past was we

00:33:23,919 --> 00:33:28,000
wanted to add let's say coupons

00:33:25,760 --> 00:33:29,360
again we put it into the monolith

00:33:28,000 --> 00:33:31,360
because we needed to read

00:33:29,360 --> 00:33:32,960
a bunch of this information now the

00:33:31,360 --> 00:33:34,720
problem is that once we put it in there

00:33:32,960 --> 00:33:36,880
it's sort of hard to get it out

00:33:34,720 --> 00:33:38,880
and in the future we wanted to make sure

00:33:36,880 --> 00:33:40,799
that we had more flexibility we didn't

00:33:38,880 --> 00:33:42,000
want everything to have to be a rails

00:33:40,799 --> 00:33:43,760
application as well

00:33:42,000 --> 00:33:46,159
particularly as we were looking to

00:33:43,760 --> 00:33:48,880
enable things like go

00:33:46,159 --> 00:33:50,960
we're doing a bunch of scala based stuff

00:33:48,880 --> 00:33:54,000
javascript as well

00:33:50,960 --> 00:33:55,200
and so again so when i was talking about

00:33:54,000 --> 00:33:57,919
those read-only

00:33:55,200 --> 00:33:58,960
uh rep or sorry the ad-hoc communication

00:33:57,919 --> 00:34:01,039
structures

00:33:58,960 --> 00:34:02,480
we had all of them and we and i mean i'm

00:34:01,039 --> 00:34:04,000
no longer working there i know they

00:34:02,480 --> 00:34:06,080
probably still have a few

00:34:04,000 --> 00:34:07,840
uh but all of these ad hoc things were

00:34:06,080 --> 00:34:09,440
painful problematic they'd break we'd

00:34:07,840 --> 00:34:11,599
have to fix them and every new service

00:34:09,440 --> 00:34:14,240
sort of needed its own new one

00:34:11,599 --> 00:34:15,359
and so what we did was we started with a

00:34:14,240 --> 00:34:17,200
forked right pattern

00:34:15,359 --> 00:34:18,560
had some mistakes there moved on to

00:34:17,200 --> 00:34:20,320
change data capture

00:34:18,560 --> 00:34:22,560
and kind of didn't really like it that

00:34:20,320 --> 00:34:24,000
much and then we moved into finally

00:34:22,560 --> 00:34:26,079
native production

00:34:24,000 --> 00:34:28,399
directly to these event streams and we

00:34:26,079 --> 00:34:30,320
would expose things like flyers

00:34:28,399 --> 00:34:33,359
the items in them which merchants and

00:34:30,320 --> 00:34:37,119
stores into these event streams

00:34:33,359 --> 00:34:40,399
which then let us decouple

00:34:37,119 --> 00:34:41,119
and migrate the applications and the

00:34:40,399 --> 00:34:43,919
services

00:34:41,119 --> 00:34:45,679
that used to use these ad hoc data

00:34:43,919 --> 00:34:46,480
structures and we could migrate them one

00:34:45,679 --> 00:34:48,240
by one

00:34:46,480 --> 00:34:49,839
and we could do comparisons to make sure

00:34:48,240 --> 00:34:51,760
that we're still getting consistent

00:34:49,839 --> 00:34:54,399
information

00:34:51,760 --> 00:34:56,480
but then it also as we would expand our

00:34:54,399 --> 00:34:58,320
event streams into new

00:34:56,480 --> 00:34:59,520
uh you know the new business lines that

00:34:58,320 --> 00:35:01,520
our business

00:34:59,520 --> 00:35:02,720
was going into like coupons was a big

00:35:01,520 --> 00:35:06,000
one there

00:35:02,720 --> 00:35:08,079
um it just gave us the ability

00:35:06,000 --> 00:35:09,359
to take our existing data what we

00:35:08,079 --> 00:35:11,200
already had you know

00:35:09,359 --> 00:35:12,880
add some of the new domain that we're

00:35:11,200 --> 00:35:14,560
moving into

00:35:12,880 --> 00:35:16,960
and now you can just compose whatever

00:35:14,560 --> 00:35:19,760
services it is you need

00:35:16,960 --> 00:35:20,160
we built item detail services we built

00:35:19,760 --> 00:35:21,920
and

00:35:20,160 --> 00:35:24,560
experimented with a few different search

00:35:21,920 --> 00:35:27,280
services uh coupon service

00:35:24,560 --> 00:35:28,800
and in this case these services are also

00:35:27,280 --> 00:35:31,599
serving front end

00:35:28,800 --> 00:35:33,599
but we also had quite an extensive back

00:35:31,599 --> 00:35:35,359
end of event driven stuff to do a lot of

00:35:33,599 --> 00:35:37,760
our like asynchronous processing

00:35:35,359 --> 00:35:38,720
of flyers and events and data coming in

00:35:37,760 --> 00:35:40,800
so

00:35:38,720 --> 00:35:42,320
again you know by now this is ad nauseam

00:35:40,800 --> 00:35:44,240
how should you access important data

00:35:42,320 --> 00:35:45,839
please think about this

00:35:44,240 --> 00:35:48,000
what trade-offs are you prepared to make

00:35:45,839 --> 00:35:50,240
again conversations

00:35:48,000 --> 00:35:52,000
uh you know the whole point of this data

00:35:50,240 --> 00:35:53,280
communication layer i think it's great

00:35:52,000 --> 00:35:55,920
composition is awesome

00:35:53,280 --> 00:35:58,560
you should use it and by making your

00:35:55,920 --> 00:36:00,960
data available org wide you on

00:35:58,560 --> 00:36:03,680
so this is an important thing you unlock

00:36:00,960 --> 00:36:06,640
a ton of different options

00:36:03,680 --> 00:36:08,720
but you don't really restrict anything

00:36:06,640 --> 00:36:11,200
you don't say you have to do it this way

00:36:08,720 --> 00:36:13,839
it just makes it so you don't have to

00:36:11,200 --> 00:36:16,400
rely on these

00:36:13,839 --> 00:36:17,599
ad-hoc data communication mechanisms you

00:36:16,400 --> 00:36:19,920
have this formal one

00:36:17,599 --> 00:36:21,200
and everything becomes a lot easier to

00:36:19,920 --> 00:36:24,400
do business change

00:36:21,200 --> 00:36:25,520
et cetera here's my my shell i wrote a

00:36:24,400 --> 00:36:28,320
book

00:36:25,520 --> 00:36:29,280
this is i talk a lot more about these

00:36:28,320 --> 00:36:31,760
options in here

00:36:29,280 --> 00:36:32,560
it says event driven microservices but

00:36:31,760 --> 00:36:34,560
this is all

00:36:32,560 --> 00:36:35,599
underpinned by this data communication

00:36:34,560 --> 00:36:37,760
layer

00:36:35,599 --> 00:36:39,200
i'm sorry that i only have a few minutes

00:36:37,760 --> 00:36:41,280
left for questions but i hope you

00:36:39,200 --> 00:36:43,839
enjoyed this thank you very much

00:36:41,280 --> 00:36:54,160
if you have any questions please let me

00:36:43,839 --> 00:36:56,400
please go forth

00:36:54,160 --> 00:36:59,440
thank you uh thank you guys i'm glad uh

00:36:56,400 --> 00:37:02,720
i hope you appreciated it

00:36:59,440 --> 00:37:03,040
sorry if i was a bit uh a bit fast paced

00:37:02,720 --> 00:37:06,160
but

00:37:03,040 --> 00:37:06,480
uh i didn't want to didn't want to take

00:37:06,160 --> 00:37:09,520
up

00:37:06,480 --> 00:37:13,040
too much more of your extra time

00:37:09,520 --> 00:37:14,000
i'm all uh i mean i only recently signed

00:37:13,040 --> 00:37:16,480
up to twitter

00:37:14,000 --> 00:37:17,359
um i'm not the greatest at it but if you

00:37:16,480 --> 00:37:20,839
have any questions

00:37:17,359 --> 00:37:23,200
feel free to tweet them at me i suppose

00:37:20,839 --> 00:37:27,040
um

00:37:23,200 --> 00:37:28,880
yeah but otherwise uh thank you i'll

00:37:27,040 --> 00:37:31,200
wait here for a few more minutes

00:37:28,880 --> 00:37:32,800
if anyone has any questions uh but

00:37:31,200 --> 00:37:33,200
otherwise i hope all of you have a great

00:37:32,800 --> 00:37:36,000
day

00:37:33,200 --> 00:37:51,839
with the the rest of the apache con or

00:37:36,000 --> 00:37:51,839
wherever you may find yourself going

00:37:57,680 --> 00:38:05,839
thanks alex

00:38:17,839 --> 00:38:19,920
you

00:38:45,119 --> 00:38:47,839
hi kai yes

00:39:10,320 --> 00:39:14,320
is kafka more suitable for an org or can

00:39:12,480 --> 00:39:17,440
it also be leveraged by a team

00:39:14,320 --> 00:39:18,640
uh so honestly you can certainly

00:39:17,440 --> 00:39:21,599
leverage it for a team

00:39:18,640 --> 00:39:23,520
uh in fact the humble my or my original

00:39:21,599 --> 00:39:24,960
exposure to kafka started with very

00:39:23,520 --> 00:39:28,880
humble beginnings

00:39:24,960 --> 00:39:31,920
um for a team of

00:39:28,880 --> 00:39:36,240
oh not even 10 people really

00:39:31,920 --> 00:39:36,240
and we're predominantly doing um

00:39:36,640 --> 00:39:40,079
we're basically bootstrapping real time

00:39:39,359 --> 00:39:42,800
streaming

00:39:40,079 --> 00:39:43,680
into some experimental stuff and that's

00:39:42,800 --> 00:39:45,599
sort of how it's

00:39:43,680 --> 00:39:47,520
how it stayed with us for a good year or

00:39:45,599 --> 00:39:49,599
so um

00:39:47,520 --> 00:39:50,960
in fact it was easier to sell it to the

00:39:49,599 --> 00:39:53,920
org after

00:39:50,960 --> 00:39:55,200
starting just internally with our team

00:39:53,920 --> 00:39:57,520
because we were able to

00:39:55,200 --> 00:39:58,480
demonstrate um some of the things that

00:39:57,520 --> 00:40:01,520
did very well

00:39:58,480 --> 00:40:03,200
but we were also able to demonstrate uh

00:40:01,520 --> 00:40:04,800
you know where we would need support

00:40:03,200 --> 00:40:07,280
like infrastructural support

00:40:04,800 --> 00:40:09,839
uh you know estimations on server costs

00:40:07,280 --> 00:40:14,640
etc like that so

00:40:09,839 --> 00:40:14,640
the real answer is is both absolutely

00:40:22,839 --> 00:40:25,839
both

00:40:38,000 --> 00:40:42,480
uh did i d did i deal with any replay of

00:40:40,880 --> 00:40:45,680
historic data before

00:40:42,480 --> 00:40:49,040
uh yes yes actually quite extensively

00:40:45,680 --> 00:40:49,760
um generally we would replay historic

00:40:49,040 --> 00:40:52,960
data

00:40:49,760 --> 00:40:55,520
when so when

00:40:52,960 --> 00:40:56,480
in the flip example which i think is is

00:40:55,520 --> 00:40:59,760
a pertinent one

00:40:56,480 --> 00:41:02,640
uh any new services coming up uh

00:40:59,760 --> 00:41:03,280
because flyers and flyer items merchants

00:41:02,640 --> 00:41:05,200
stores

00:41:03,280 --> 00:41:07,599
because those things were such important

00:41:05,200 --> 00:41:10,480
aspects of our business

00:41:07,599 --> 00:41:13,040
any new service coming up would end up

00:41:10,480 --> 00:41:17,440
consuming all of that historically

00:41:13,040 --> 00:41:19,839
but sometimes we would also have to do

00:41:17,440 --> 00:41:20,800
code changes business changes and those

00:41:19,839 --> 00:41:24,560
changes

00:41:20,800 --> 00:41:26,640
would basically break the internal data

00:41:24,560 --> 00:41:28,560
models that we currently had

00:41:26,640 --> 00:41:29,839
so instead of trying to migrate the data

00:41:28,560 --> 00:41:33,520
models over

00:41:29,839 --> 00:41:36,800
which could be problematic in some cases

00:41:33,520 --> 00:41:38,560
we would just bring up a new application

00:41:36,800 --> 00:41:41,359
instance

00:41:38,560 --> 00:41:43,040
rebuild it all from the historic data

00:41:41,359 --> 00:41:44,480
and then once we were satisfied that

00:41:43,040 --> 00:41:46,960
everything was looking good

00:41:44,480 --> 00:41:49,680
we would swap over to using the new one

00:41:46,960 --> 00:41:53,119
and get rid of the old one

00:41:49,680 --> 00:41:55,839
so historic data was was regularly

00:41:53,119 --> 00:41:57,440
uh regularly replayed yeah yeah robert

00:41:55,839 --> 00:41:59,760
says he's done that a lot too yeah

00:41:57,440 --> 00:42:00,960
and and what's what's also really cool

00:41:59,760 --> 00:42:04,560
about that

00:42:00,960 --> 00:42:07,839
is it is the it is the same

00:42:04,560 --> 00:42:12,079
for what i would call disaster recovery

00:42:07,839 --> 00:42:14,640
so if you have a uh if you have

00:42:12,079 --> 00:42:16,560
like an application and a and it

00:42:14,640 --> 00:42:18,480
catastrophically fails

00:42:16,560 --> 00:42:20,160
and you don't have any snapshots and you

00:42:18,480 --> 00:42:22,240
have to bring it back up

00:42:20,160 --> 00:42:23,839
you'll already have an idea how long

00:42:22,240 --> 00:42:25,599
that could take because you're you know

00:42:23,839 --> 00:42:26,480
you're used to replaying this historical

00:42:25,599 --> 00:42:28,960
data

00:42:26,480 --> 00:42:30,560
and you know that like worst case

00:42:28,960 --> 00:42:32,079
scenario we're going to have to replay

00:42:30,560 --> 00:42:33,599
it all and this is what it'll be

00:42:32,079 --> 00:42:35,119
and then you can decide if that's good

00:42:33,599 --> 00:42:38,319
enough for your service level of

00:42:35,119 --> 00:42:41,119
service level objectives or not

00:42:38,319 --> 00:42:42,160
yeah and it's kind of cool because it's

00:42:41,119 --> 00:42:44,000
the same way

00:42:42,160 --> 00:42:45,440
you know whether whether you're building

00:42:44,000 --> 00:42:47,520
a new app whether you're

00:42:45,440 --> 00:42:50,000
changing and re-fixing bugs whether

00:42:47,520 --> 00:42:53,040
you're recovering from disasters

00:42:50,000 --> 00:42:54,079
uh so having that ex yeah uh what does

00:42:53,040 --> 00:42:55,839
robert here say

00:42:54,079 --> 00:42:58,000
replay a louder architecture user do

00:42:55,839 --> 00:43:00,880
does it yep yep and for testing yes

00:42:58,000 --> 00:43:02,160
and also for testing uh you can use the

00:43:00,880 --> 00:43:05,200
same sources of data

00:43:02,160 --> 00:43:07,520
to to pull that information in test it

00:43:05,200 --> 00:43:09,280
and without worrying about affecting

00:43:07,520 --> 00:43:10,640
production without you know making any

00:43:09,280 --> 00:43:12,240
mistakes there

00:43:10,640 --> 00:43:27,280
so once you get that communication

00:43:12,240 --> 00:43:29,280
available it's just very very useful

00:43:27,280 --> 00:43:30,560
uh kai when you rebuild from historic

00:43:29,280 --> 00:43:32,160
data what are the practices and

00:43:30,560 --> 00:43:33,839
recommendations

00:43:32,160 --> 00:43:35,839
you have to complete it safely and

00:43:33,839 --> 00:43:41,040
quickly right

00:43:35,839 --> 00:43:44,480
so this is where

00:43:41,040 --> 00:43:47,839
so if you're do yeah have a steady nerve

00:43:44,480 --> 00:43:50,079
so if you're doing this for disaster

00:43:47,839 --> 00:43:52,800
recovery purposes

00:43:50,079 --> 00:43:53,359
uh i think my best recommendation to you

00:43:52,800 --> 00:43:55,599
would be

00:43:53,359 --> 00:43:57,200
try not to get into that scenario in the

00:43:55,599 --> 00:44:02,400
first place it kind of sounds

00:43:57,200 --> 00:44:05,760
a bit uh evasive but for example if your

00:44:02,400 --> 00:44:07,599
consumer ha let's say you're using a

00:44:05,760 --> 00:44:08,400
consumer where you're materializing your

00:44:07,599 --> 00:44:12,960
data

00:44:08,400 --> 00:44:15,599
to like a managed relational database

00:44:12,960 --> 00:44:18,319
you can take snapshots of that and you

00:44:15,599 --> 00:44:20,319
can store the offsets of your consumer

00:44:18,319 --> 00:44:23,760
in those snapshots

00:44:20,319 --> 00:44:24,880
so in a worst case scenario if you go

00:44:23,760 --> 00:44:27,680
down

00:44:24,880 --> 00:44:28,960
you can say okay restore yesterday's

00:44:27,680 --> 00:44:31,920
midnight snapshot

00:44:28,960 --> 00:44:33,520
and and then replay from there so the

00:44:31,920 --> 00:44:35,920
snapshot restoration will

00:44:33,520 --> 00:44:37,280
be as fast as it normally is but the

00:44:35,920 --> 00:44:41,040
restoration will be very

00:44:37,280 --> 00:44:43,440
short now if you're in a situation where

00:44:41,040 --> 00:44:44,400
you have to reprocess from the very very

00:44:43,440 --> 00:44:46,240
beginning of time

00:44:44,400 --> 00:44:48,160
and you have no no you know there's no

00:44:46,240 --> 00:44:51,520
magical oats here

00:44:48,160 --> 00:44:54,640
um this is where

00:44:51,520 --> 00:44:56,240
a bit of foresight is needed so the

00:44:54,640 --> 00:44:57,520
number of partitions you have will be a

00:44:56,240 --> 00:45:00,960
limiting factor

00:44:57,520 --> 00:45:02,800
and the number of and how efficient your

00:45:00,960 --> 00:45:04,640
processing job is

00:45:02,800 --> 00:45:05,839
for consuming this data and doing work

00:45:04,640 --> 00:45:09,280
with it

00:45:05,839 --> 00:45:14,400
and there's

00:45:09,280 --> 00:45:16,079
tends to be a bit of a i find a bit of a

00:45:14,400 --> 00:45:17,839
struggle let's say between

00:45:16,079 --> 00:45:19,119
infrastructure that may manage these

00:45:17,839 --> 00:45:21,920
servers

00:45:19,119 --> 00:45:23,920
and pay for them right and the teams

00:45:21,920 --> 00:45:24,560
that want it to be as absolutely fast as

00:45:23,920 --> 00:45:26,640
possible

00:45:24,560 --> 00:45:28,400
so if you have say a terabyte of data

00:45:26,640 --> 00:45:30,079
yes you could have a thousand partitions

00:45:28,400 --> 00:45:32,160
and it would go very quickly with a

00:45:30,079 --> 00:45:32,720
thousand parallel instances processing

00:45:32,160 --> 00:45:34,720
it

00:45:32,720 --> 00:45:35,760
but that's sort of like financial

00:45:34,720 --> 00:45:37,920
overkill

00:45:35,760 --> 00:45:39,280
especially if in steady state you could

00:45:37,920 --> 00:45:42,160
get away with

00:45:39,280 --> 00:45:43,839
uh the parallelism provided by only one

00:45:42,160 --> 00:45:46,720
or two partitions

00:45:43,839 --> 00:45:48,000
um so that again is a it's a matter of

00:45:46,720 --> 00:45:51,359
trade-offs there

00:45:48,000 --> 00:45:54,800
and it kind of depends on the business

00:45:51,359 --> 00:45:57,440
requirements so if you have

00:45:54,800 --> 00:45:59,920
a very hard service level objective and

00:45:57,440 --> 00:46:02,400
it says it has to be at least one hour

00:45:59,920 --> 00:46:03,839
or sorry it has to be a maximum of one

00:46:02,400 --> 00:46:06,880
hour

00:46:03,839 --> 00:46:09,040
for um and generally you would kind of

00:46:06,880 --> 00:46:11,200
need like a sort of benchmark job you

00:46:09,040 --> 00:46:12,960
need something to measure it against

00:46:11,200 --> 00:46:14,960
and you'd say okay so we have to be able

00:46:12,960 --> 00:46:17,280
to reprocess this in one hour

00:46:14,960 --> 00:46:19,040
and this is our benchmark job so then

00:46:17,280 --> 00:46:21,200
you do some napkin math

00:46:19,040 --> 00:46:23,520
the amount of data size you have whether

00:46:21,200 --> 00:46:27,359
your topics are compacted or not

00:46:23,520 --> 00:46:27,760
and and sort of how fast you expect it

00:46:27,359 --> 00:46:30,720
to

00:46:27,760 --> 00:46:32,319
process do some division and you're

00:46:30,720 --> 00:46:33,440
going to end up with a partition count

00:46:32,319 --> 00:46:36,160
you know

00:46:33,440 --> 00:46:38,720
you know maybe you double it then and

00:46:36,160 --> 00:46:41,839
that would sort of be working backwards

00:46:38,720 --> 00:46:44,319
to ensure that you would have enough

00:46:41,839 --> 00:46:46,319
capacity

00:46:44,319 --> 00:46:49,280
so that if you do get in that scenario

00:46:46,319 --> 00:46:49,280
you can rebuild

00:46:52,480 --> 00:46:56,079
yeah and like what robert is saying here

00:46:54,560 --> 00:46:57,760
is perfect he says

00:46:56,079 --> 00:46:59,200
it's crucial for infrastructure to

00:46:57,760 --> 00:47:01,200
understand the business drivers for

00:46:59,200 --> 00:47:01,920
example if profit is generated entirely

00:47:01,200 --> 00:47:03,520
at peak

00:47:01,920 --> 00:47:04,960
infrastructure needs to accept over

00:47:03,520 --> 00:47:07,920
provisioning and

00:47:04,960 --> 00:47:09,359
yes and so again it there's you know

00:47:07,920 --> 00:47:12,960
there's no free lunch here

00:47:09,359 --> 00:47:15,520
it's it's really it really does depend

00:47:12,960 --> 00:47:16,480
um but this is why i always advocate

00:47:15,520 --> 00:47:19,280
it's best to

00:47:16,480 --> 00:47:20,319
it's best to sort of do a defensive

00:47:19,280 --> 00:47:23,440
approach

00:47:20,319 --> 00:47:25,119
where you you know ideally don't get

00:47:23,440 --> 00:47:27,119
into that scenario and i think with

00:47:25,119 --> 00:47:29,200
modern day cloud computing especially

00:47:27,119 --> 00:47:32,000
with being able to take snapshots of

00:47:29,200 --> 00:47:33,520
uh various data sets uh that that your

00:47:32,000 --> 00:47:37,040
state may generate

00:47:33,520 --> 00:47:38,079
um you're no longer so reliant upon an

00:47:37,040 --> 00:47:40,720
individual machine

00:47:38,079 --> 00:47:41,920
or server being alive so like even if

00:47:40,720 --> 00:47:43,520
you lose all of that

00:47:41,920 --> 00:47:45,760
yeah if you lose a whole data center for

00:47:43,520 --> 00:47:47,599
example and with the cloud

00:47:45,760 --> 00:47:48,800
uh you're it's like oh well we lost the

00:47:47,599 --> 00:47:50,400
whole data center but we still have your

00:47:48,800 --> 00:47:50,800
snapshot so we'll just move it over to

00:47:50,400 --> 00:47:52,319
this

00:47:50,800 --> 00:47:53,839
you know this region and boot it up

00:47:52,319 --> 00:47:56,960
there and here you go

00:47:53,839 --> 00:47:58,400
uh so so i would say like keep keep a

00:47:56,960 --> 00:47:59,760
look at those tools that prevent you

00:47:58,400 --> 00:48:02,319
from getting in that place in this in

00:47:59,760 --> 00:48:06,400
the first place yeah

00:48:02,319 --> 00:48:07,839
um all right folks uh uh

00:48:06,400 --> 00:48:10,800
let's see kai says one more thing here

00:48:07,839 --> 00:48:10,800
we play the whole history

00:48:14,160 --> 00:48:18,160
replay the whole history scale at the

00:48:15,760 --> 00:48:18,160
flag

00:48:18,720 --> 00:48:22,400
yeah yeah exactly um and so we

00:48:21,440 --> 00:48:26,480
successfully

00:48:22,400 --> 00:48:30,000
would uh we would store the offsets

00:48:26,480 --> 00:48:31,760
uh within our s within our

00:48:30,000 --> 00:48:34,640
so one pattern we use is we'd store the

00:48:31,760 --> 00:48:36,800
offsets within that within the

00:48:34,640 --> 00:48:39,359
snapshots uh we also did a lot of stuff

00:48:36,800 --> 00:48:42,240
with kafka streams where

00:48:39,359 --> 00:48:44,240
we we have change logs so all the change

00:48:42,240 --> 00:48:47,760
logs are backed by kafka

00:48:44,240 --> 00:48:50,079
and uh and then

00:48:47,760 --> 00:48:52,079
you can basically as long as you don't

00:48:50,079 --> 00:48:54,400
use lose your entire kafka cluster which

00:48:52,079 --> 00:48:57,440
would be catastrophic in its own right

00:48:54,400 --> 00:48:58,640
don't do that uh you would have the

00:48:57,440 --> 00:49:00,640
ability to just

00:48:58,640 --> 00:49:02,400
relaunch your application reload from

00:49:00,640 --> 00:49:04,160
the change log

00:49:02,400 --> 00:49:06,480
which is effectively your snapshot and

00:49:04,160 --> 00:49:08,800
then off you go

00:49:06,480 --> 00:49:09,920
all right folks i'm uh it was actually

00:49:08,800 --> 00:49:11,440
this was kind of cool

00:49:09,920 --> 00:49:13,760
it's nice talking to all you but i have

00:49:11,440 --> 00:49:16,960
to get going uh there's a couple of

00:49:13,760 --> 00:49:17,680
things i'd like to see myself but robert

00:49:16,960 --> 00:49:20,400
and kai

00:49:17,680 --> 00:49:22,800
uh nice chatting with you and whoever

00:49:20,400 --> 00:49:24,400
else may still be here yes take care

00:49:22,800 --> 00:49:26,960
thank you for the questions and robert i

00:49:24,400 --> 00:49:31,839
appreciate your input thank you

00:49:26,960 --> 00:49:31,839
you folks have yourselves a great day

00:49:49,839 --> 00:49:51,920

YouTube URL: https://www.youtube.com/watch?v=7p7eGRkdiLw


