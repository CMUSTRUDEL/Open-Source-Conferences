Title: Achieve the event-driven Nirvana with Apache Druid
Publication date: 2020-10-26
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Achieve the event-driven Nirvana with Apache Druid
Abdelkrim Hadjidj

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

After two decades of transforming into data-driven organizations, companies are moving to the next level: building event-driven organizations. An event-driven organization achieves faster insights, a better customer experience and more agility. However, this transformation requires advanced skills to make sense of all the events in real-time which put business people on the side. In this presentation, we will review the data architectures used by the most advanced event driven organizations today. We will discuss the challenges they face on delivering the promised business value and why stream processing technologies like Apache Kafka and Apache Flink are not enough to achieve the streaming nirvana. Finally, we will explain how Apache Druid enables self-service BI on event data and allows business users to ask their own questions leading to real-time insights.

Abdelkrim is a Data expert with 12 years experience on distributed systems (big data, IoT, peer to peer and cloud). He helps customers in EMEA using open source streaming technologies such as Apache Kafka, NiFi, Flink and Druid to pivot into event driven organizations. Abdelkrim is currently working as a Senior Solution Engineer at Imply. Previously, He held several positions including Senior Streaming Specialist at Cloudera, Solution Engineer at Hortonworks, Big Data Lead at Atos and CTO at Artheamis. He published several scientific papers at well-known IEEE and ACM journals. You can find him talking at Meetups or worldwide tech conferences such as Dataworks Summit, Strata or Flink Forward. He founded and runs the Future Of Data Meetup in Paris which is a group of 2300+ data and tech enthusiasts.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,720 --> 00:00:29,359
so

00:00:25,680 --> 00:00:31,279
let's move to the screen sharing

00:00:29,359 --> 00:00:33,360
so again hi everyone welcome to this

00:00:31,279 --> 00:00:34,800
session achieve the event driven nirvana

00:00:33,360 --> 00:00:36,880
with apache druid

00:00:34,800 --> 00:00:38,399
so i'm at the cream i'm senior solution

00:00:36,880 --> 00:00:41,760
engineer at imply

00:00:38,399 --> 00:00:44,000
uh which is a company created by the

00:00:41,760 --> 00:00:47,840
original creator of apache druid

00:00:44,000 --> 00:00:50,480
i'm based out of paris and yeah i like

00:00:47,840 --> 00:00:52,239
giving talk on different big data and

00:00:50,480 --> 00:00:53,760
streaming technologies

00:00:52,239 --> 00:00:56,559
uh before that i was streaming

00:00:53,760 --> 00:00:59,359
especially at cloudera

00:00:56,559 --> 00:01:00,320
so today um i'm going to talk about

00:00:59,359 --> 00:01:02,480
event driven

00:01:00,320 --> 00:01:03,440
uh so i will introduce quickly this

00:01:02,480 --> 00:01:06,560
concept

00:01:03,440 --> 00:01:08,640
i i won't try to define it um

00:01:06,560 --> 00:01:10,159
that that's something too too complex

00:01:08,640 --> 00:01:13,439
and challenging

00:01:10,159 --> 00:01:16,400
uh but i will really focus on why

00:01:13,439 --> 00:01:17,680
companies do event driven today and

00:01:16,400 --> 00:01:20,640
what's the

00:01:17,680 --> 00:01:22,240
most uh recurrent modern event-driven

00:01:20,640 --> 00:01:24,960
architecture based on

00:01:22,240 --> 00:01:26,960
apache technologies like kafka flink and

00:01:24,960 --> 00:01:28,159
and so on and what's the challenge

00:01:26,960 --> 00:01:30,960
associated with this

00:01:28,159 --> 00:01:32,320
especially for the analytics side of the

00:01:30,960 --> 00:01:34,079
house

00:01:32,320 --> 00:01:36,799
this is where i will introduce apache

00:01:34,079 --> 00:01:38,960
druid and what it brings to the

00:01:36,799 --> 00:01:40,640
equation and how it makes this

00:01:38,960 --> 00:01:44,000
architecture more

00:01:40,640 --> 00:01:44,479
well-suited for business and analyst

00:01:44,000 --> 00:01:47,439
people

00:01:44,479 --> 00:01:47,759
um that that's looking for for this data

00:01:47,439 --> 00:01:50,560
and

00:01:47,759 --> 00:01:53,200
and of course i i will ramp up with some

00:01:50,560 --> 00:01:56,240
conclusions

00:01:53,200 --> 00:01:57,680
so let's go uh let's go to it so if we

00:01:56,240 --> 00:02:00,159
look today to

00:01:57,680 --> 00:02:00,960
any system in the real world there is

00:02:00,159 --> 00:02:03,200
different

00:02:00,960 --> 00:02:04,719
parties and components exchanging

00:02:03,200 --> 00:02:08,479
information between them

00:02:04,719 --> 00:02:10,160
so if i'm a manufacturer i have some

00:02:08,479 --> 00:02:12,239
i have bank accounts in which i have

00:02:10,160 --> 00:02:13,680
different transaction with my providers

00:02:12,239 --> 00:02:15,440
with my customers and

00:02:13,680 --> 00:02:17,120
and so on i'm running different

00:02:15,440 --> 00:02:19,280
factories around the world that are

00:02:17,120 --> 00:02:21,200
producing pieces and parts

00:02:19,280 --> 00:02:23,760
and shipping them all around the world

00:02:21,200 --> 00:02:26,239
to my partners to my customers

00:02:23,760 --> 00:02:27,840
and there is the external ecosystem for

00:02:26,239 --> 00:02:30,800
instance the stock markets

00:02:27,840 --> 00:02:31,120
um that has an impact on my benefit on

00:02:30,800 --> 00:02:34,480
my

00:02:31,120 --> 00:02:37,360
revenue in terms of conversion from

00:02:34,480 --> 00:02:37,920
dollar to euro and this this kind of

00:02:37,360 --> 00:02:41,120
things

00:02:37,920 --> 00:02:43,040
and finally maybe i have a website uh so

00:02:41,120 --> 00:02:46,160
all of these generate data

00:02:43,040 --> 00:02:49,440
that i need to manage to

00:02:46,160 --> 00:02:50,400
capture and and and use in my day-to-day

00:02:49,440 --> 00:02:52,400
activities

00:02:50,400 --> 00:02:54,959
but if we look to the information system

00:02:52,400 --> 00:02:57,840
today it's mainly based on state

00:02:54,959 --> 00:02:59,840
so we have some sales report uh maybe

00:02:57,840 --> 00:03:01,760
you are updating this every hour

00:02:59,840 --> 00:03:03,200
that contains all the information how

00:03:01,760 --> 00:03:05,440
much product we sold

00:03:03,200 --> 00:03:07,599
how much revenue we we get how much

00:03:05,440 --> 00:03:10,159
benefit we get and so on

00:03:07,599 --> 00:03:12,640
uh the supply chain management is also

00:03:10,159 --> 00:03:13,200
has its own information system its own

00:03:12,640 --> 00:03:15,920
state

00:03:13,200 --> 00:03:17,760
maybe it's updated twice a day based on

00:03:15,920 --> 00:03:19,760
all this information that we get

00:03:17,760 --> 00:03:21,040
and the same thing for the quality maybe

00:03:19,760 --> 00:03:23,120
we are looking on

00:03:21,040 --> 00:03:24,799
how what what are the quality issues

00:03:23,120 --> 00:03:27,200
that we have every week and so on

00:03:24,799 --> 00:03:28,480
but when we look to the data that we are

00:03:27,200 --> 00:03:31,519
collecting everything is

00:03:28,480 --> 00:03:34,080
events uh it has a timestamp and

00:03:31,519 --> 00:03:35,840
we know that something happened so we

00:03:34,080 --> 00:03:37,360
received the money from from someone

00:03:35,840 --> 00:03:40,239
it's an event

00:03:37,360 --> 00:03:41,760
we produced a batch of of product in in

00:03:40,239 --> 00:03:44,319
one of our factories and

00:03:41,760 --> 00:03:45,760
events we delivered something as to

00:03:44,319 --> 00:03:48,959
someone as an event

00:03:45,760 --> 00:03:50,879
or maybe a customer has talked about our

00:03:48,959 --> 00:03:51,599
brand and our product on twitter is also

00:03:50,879 --> 00:03:54,159
an event

00:03:51,599 --> 00:03:54,879
and we need to be able to capture all of

00:03:54,159 --> 00:03:58,080
this

00:03:54,879 --> 00:04:01,040
and today we just captured them and

00:03:58,080 --> 00:04:03,200
built state based on it so the idea of

00:04:01,040 --> 00:04:06,080
event driven is really trying to

00:04:03,200 --> 00:04:08,000
go back to the source and make these

00:04:06,080 --> 00:04:10,319
events as a first class citizen

00:04:08,000 --> 00:04:11,599
this means that when we think about new

00:04:10,319 --> 00:04:15,599
product uh

00:04:11,599 --> 00:04:18,400
uh data product or or

00:04:15,599 --> 00:04:19,440
analytics uh or anything around this we

00:04:18,400 --> 00:04:21,919
need to think about

00:04:19,440 --> 00:04:23,919
events as the first place how we how we

00:04:21,919 --> 00:04:24,800
are able to capture them how we are able

00:04:23,919 --> 00:04:27,600
to use them

00:04:24,800 --> 00:04:28,000
and and so on and then for every event

00:04:27,600 --> 00:04:30,000
we need

00:04:28,000 --> 00:04:31,280
to define what are the information that

00:04:30,000 --> 00:04:34,080
we need to share

00:04:31,280 --> 00:04:35,600
and give all the contacts in in the

00:04:34,080 --> 00:04:38,000
event itself so it's

00:04:35,600 --> 00:04:39,600
a self-contained object that can be

00:04:38,000 --> 00:04:40,960
shared between the different teams

00:04:39,600 --> 00:04:42,479
between the different applications

00:04:40,960 --> 00:04:45,520
inside the organization

00:04:42,479 --> 00:04:46,720
and based off this we can use it without

00:04:45,520 --> 00:04:50,080
having to go and

00:04:46,720 --> 00:04:51,600
look into other state databases that

00:04:50,080 --> 00:04:54,479
that we already have

00:04:51,600 --> 00:04:55,840
so the main objective of all of this is

00:04:54,479 --> 00:04:58,240
really democratize

00:04:55,840 --> 00:05:00,160
these events make them available for

00:04:58,240 --> 00:05:01,520
anyone in the organization that are

00:05:00,160 --> 00:05:04,080
interested by

00:05:01,520 --> 00:05:04,880
consuming them using them maybe to

00:05:04,080 --> 00:05:07,520
update its

00:05:04,880 --> 00:05:09,840
internal state but also to interact with

00:05:07,520 --> 00:05:11,280
other teams and and systems inside the

00:05:09,840 --> 00:05:13,520
organization

00:05:11,280 --> 00:05:15,759
so anyone will be able to discover

00:05:13,520 --> 00:05:17,440
search and subscribe to these events

00:05:15,759 --> 00:05:18,880
and of course there is security around

00:05:17,440 --> 00:05:21,600
it it's not it's not

00:05:18,880 --> 00:05:23,280
uh make everything available for

00:05:21,600 --> 00:05:24,560
everyone but it's really this notion of

00:05:23,280 --> 00:05:27,280
of sharing this

00:05:24,560 --> 00:05:28,560
this if and make them available and when

00:05:27,280 --> 00:05:31,520
you look to the

00:05:28,560 --> 00:05:34,000
architectures today we see often that

00:05:31,520 --> 00:05:35,039
streaming technologies like apache kafka

00:05:34,000 --> 00:05:37,680
or pulsar

00:05:35,039 --> 00:05:38,400
are used as a technical enabler because

00:05:37,680 --> 00:05:40,240
they make

00:05:38,400 --> 00:05:41,680
possible this data sharing and

00:05:40,240 --> 00:05:43,759
subscription mechanism

00:05:41,680 --> 00:05:45,840
uh the replay of this event and and so

00:05:43,759 --> 00:05:47,759
on so that

00:05:45,840 --> 00:05:49,840
we use the streaming technologies as

00:05:47,759 --> 00:05:51,360
building blocks for our event-driven

00:05:49,840 --> 00:05:54,000
organization and the application and

00:05:51,360 --> 00:05:57,199
systems engineer

00:05:54,000 --> 00:05:58,080
so that's that's like the uh de facto

00:05:57,199 --> 00:06:01,120
standard today

00:05:58,080 --> 00:06:03,759
in around around this topic but

00:06:01,120 --> 00:06:06,240
why why it's it's it's really important

00:06:03,759 --> 00:06:08,319
or valuable to capture this event and

00:06:06,240 --> 00:06:09,280
share them in the organization the first

00:06:08,319 --> 00:06:11,520
thing is of course

00:06:09,280 --> 00:06:12,479
around low low latency we are talking

00:06:11,520 --> 00:06:15,039
about streaming

00:06:12,479 --> 00:06:16,080
so we need to be able to detect things

00:06:15,039 --> 00:06:19,199
and

00:06:16,080 --> 00:06:20,400
do action and um yeah react to them in

00:06:19,199 --> 00:06:22,400
in real time

00:06:20,400 --> 00:06:23,440
so when we look to the data

00:06:22,400 --> 00:06:25,840
architectures that

00:06:23,440 --> 00:06:27,759
that we usually have is we have

00:06:25,840 --> 00:06:29,600
different data sources it can be files

00:06:27,759 --> 00:06:31,520
click streams events or maybe our

00:06:29,600 --> 00:06:34,319
application are generating data

00:06:31,520 --> 00:06:36,160
we have some ingestion jobs to take this

00:06:34,319 --> 00:06:38,720
data from different sources

00:06:36,160 --> 00:06:39,680
store them in databases and then here we

00:06:38,720 --> 00:06:41,919
have two words

00:06:39,680 --> 00:06:43,520
we have the word of applications where

00:06:41,919 --> 00:06:45,759
we do the integration

00:06:43,520 --> 00:06:47,120
and we quickly put the data in

00:06:45,759 --> 00:06:49,360
operational database

00:06:47,120 --> 00:06:51,199
and then we expose it through apis or

00:06:49,360 --> 00:06:53,199
service or anything you want

00:06:51,199 --> 00:06:55,199
and then you have different application

00:06:53,199 --> 00:06:57,440
desktop application web applications

00:06:55,199 --> 00:06:59,039
that come and consume this this data and

00:06:57,440 --> 00:07:01,840
make it available

00:06:59,039 --> 00:07:03,520
on the other side we have all this uh

00:07:01,840 --> 00:07:06,240
analytics infrastructure

00:07:03,520 --> 00:07:07,199
where we do some etl jobs extracting

00:07:06,240 --> 00:07:10,400
data from

00:07:07,199 --> 00:07:13,599
uh different data sources um doing

00:07:10,400 --> 00:07:14,720
joins aggregation all all the kpis that

00:07:13,599 --> 00:07:16,800
we need to compute

00:07:14,720 --> 00:07:18,000
and then store them into data warehouse

00:07:16,800 --> 00:07:20,400
or data lake

00:07:18,000 --> 00:07:21,360
and because we have a lot of data and

00:07:20,400 --> 00:07:24,319
because

00:07:21,360 --> 00:07:24,960
it's it's hard to have like uh fast

00:07:24,319 --> 00:07:27,199
queries

00:07:24,960 --> 00:07:28,800
we may need to construct some data merge

00:07:27,199 --> 00:07:31,919
which is at the end of the day

00:07:28,800 --> 00:07:34,240
a view on in one part of my data

00:07:31,919 --> 00:07:35,440
to enable dashboard or analytics and

00:07:34,240 --> 00:07:37,840
things like this

00:07:35,440 --> 00:07:39,280
and for every step of this process we

00:07:37,840 --> 00:07:41,599
are adding latencies

00:07:39,280 --> 00:07:42,319
at the end of the day maybe there is one

00:07:41,599 --> 00:07:44,879
hour

00:07:42,319 --> 00:07:45,440
or several minutes or maybe one day

00:07:44,879 --> 00:07:48,160
between

00:07:45,440 --> 00:07:48,879
one something happen in real world and

00:07:48,160 --> 00:07:51,440
when it's

00:07:48,879 --> 00:07:53,120
it's visible on my application or in my

00:07:51,440 --> 00:07:56,160
dashboard

00:07:53,120 --> 00:07:58,560
so by using event driven architectures

00:07:56,160 --> 00:07:59,840
uh we we need to change this so

00:07:58,560 --> 00:08:02,800
everything happening

00:07:59,840 --> 00:08:04,720
is just published in an event broker i'm

00:08:02,800 --> 00:08:06,720
not talking about technologies here

00:08:04,720 --> 00:08:08,240
it can be kafka it can be pulsar it can

00:08:06,720 --> 00:08:11,039
be proprietary solution

00:08:08,240 --> 00:08:13,039
but the idea is we have a broker in

00:08:11,039 --> 00:08:15,520
which we can publish these events

00:08:13,039 --> 00:08:17,199
in real time and then we have different

00:08:15,520 --> 00:08:20,080
applications and services

00:08:17,199 --> 00:08:20,720
subscribing to different this this kind

00:08:20,080 --> 00:08:23,520
of

00:08:20,720 --> 00:08:24,960
information consuming them for updating

00:08:23,520 --> 00:08:26,720
their own internal

00:08:24,960 --> 00:08:28,960
state so for instance for the

00:08:26,720 --> 00:08:29,680
application word we can have our micro

00:08:28,960 --> 00:08:31,520
services

00:08:29,680 --> 00:08:32,800
that consume them and make them

00:08:31,520 --> 00:08:35,839
available to

00:08:32,800 --> 00:08:39,039
the business application through through

00:08:35,839 --> 00:08:41,360
the web services for instance

00:08:39,039 --> 00:08:43,680
for the stream analytics part we will be

00:08:41,360 --> 00:08:47,360
we will be able to do some

00:08:43,680 --> 00:08:49,519
simple uh analytics using uh streaming

00:08:47,360 --> 00:08:51,040
engine like flink or kafka streams and

00:08:49,519 --> 00:08:53,120
things like this

00:08:51,040 --> 00:08:55,760
so we are consuming this event in real

00:08:53,120 --> 00:08:58,240
time we are doing some aggregation on

00:08:55,760 --> 00:09:00,399
on time window for instance and then

00:08:58,240 --> 00:09:03,279
showing them in real time to the user

00:09:00,399 --> 00:09:03,760
but here we are really in the realm of

00:09:03,279 --> 00:09:07,360
uh

00:09:03,760 --> 00:09:10,560
dashboarding and not slice and dice and

00:09:07,360 --> 00:09:13,360
self-service analytics and the last part

00:09:10,560 --> 00:09:15,120
is around the advanced analytics um

00:09:13,360 --> 00:09:17,200
usually today

00:09:15,120 --> 00:09:19,200
even if you have like event-driven

00:09:17,200 --> 00:09:22,640
architecture and streaming architecture

00:09:19,200 --> 00:09:23,279
this part is still working in a batch

00:09:22,640 --> 00:09:26,320
fashion

00:09:23,279 --> 00:09:27,920
you can do some etl in real time but at

00:09:26,320 --> 00:09:30,959
the end of the day we will see

00:09:27,920 --> 00:09:32,480
why later data will end up in data

00:09:30,959 --> 00:09:34,959
warehouse or data like

00:09:32,480 --> 00:09:36,959
and and we lose uh some time there and

00:09:34,959 --> 00:09:40,000
we add some some latency to

00:09:36,959 --> 00:09:41,040
the end-to-end process so one can ask

00:09:40,000 --> 00:09:43,680
the question

00:09:41,040 --> 00:09:44,560
my business doesn't need like data in

00:09:43,680 --> 00:09:46,959
real time

00:09:44,560 --> 00:09:48,480
should i use event driven and streaming

00:09:46,959 --> 00:09:50,720
technologies and why

00:09:48,480 --> 00:09:52,640
um and the answer is yes one of the

00:09:50,720 --> 00:09:55,040
reasons is increased agility

00:09:52,640 --> 00:09:56,480
so let's say you have an application so

00:09:55,040 --> 00:09:59,279
usually what you do is

00:09:56,480 --> 00:10:01,600
you build an app specific data pipeline

00:09:59,279 --> 00:10:03,600
where you connect to different database

00:10:01,600 --> 00:10:05,760
you take the data that you need you

00:10:03,600 --> 00:10:08,480
shape it in the format that you need

00:10:05,760 --> 00:10:09,279
doing some filtering drawing group by

00:10:08,480 --> 00:10:11,440
enrichment

00:10:09,279 --> 00:10:12,640
things like this and then store it in a

00:10:11,440 --> 00:10:15,040
personal database

00:10:12,640 --> 00:10:16,399
so that your application can consume it

00:10:15,040 --> 00:10:18,720
but what happens

00:10:16,399 --> 00:10:20,880
if you need to deploy a new version of

00:10:18,720 --> 00:10:23,519
your application your data pipeline

00:10:20,880 --> 00:10:24,720
or maybe test new application you have

00:10:23,519 --> 00:10:26,480
an idea around

00:10:24,720 --> 00:10:28,720
an algorithms let's say for fraud

00:10:26,480 --> 00:10:32,240
detection you need to to try it

00:10:28,720 --> 00:10:35,279
on on production data how to do it

00:10:32,240 --> 00:10:38,320
so event driven can help you have more

00:10:35,279 --> 00:10:39,360
agility around this so since you have

00:10:38,320 --> 00:10:41,839
this event

00:10:39,360 --> 00:10:42,880
like the granular data the raw data in

00:10:41,839 --> 00:10:46,240
event broker

00:10:42,880 --> 00:10:49,040
you can have another pipeline

00:10:46,240 --> 00:10:50,640
that's deployed in parallel and consume

00:10:49,040 --> 00:10:52,880
the same events

00:10:50,640 --> 00:10:54,399
so that you can test things the first

00:10:52,880 --> 00:10:56,240
use case is around

00:10:54,399 --> 00:10:57,839
a b testing let's say you have new

00:10:56,240 --> 00:10:58,640
version of your application you need to

00:10:57,839 --> 00:11:01,279
deploy it

00:10:58,640 --> 00:11:03,120
so you can have another pipeline

00:11:01,279 --> 00:11:05,760
subscribing to some topic

00:11:03,120 --> 00:11:07,360
consuming this event doing maybe filter

00:11:05,760 --> 00:11:09,680
and group in different way

00:11:07,360 --> 00:11:12,240
but yes it's it's a new version of my

00:11:09,680 --> 00:11:14,000
application i store the data and test it

00:11:12,240 --> 00:11:15,440
when i'm happy with this new application

00:11:14,000 --> 00:11:18,000
i can roll it roll up

00:11:15,440 --> 00:11:18,880
uh really up to to production i can use

00:11:18,000 --> 00:11:21,839
it to upgrade

00:11:18,880 --> 00:11:22,880
i have this version before going uh

00:11:21,839 --> 00:11:25,360
upgrading my

00:11:22,880 --> 00:11:26,000
first up instance i can test this in

00:11:25,360 --> 00:11:28,000
production

00:11:26,000 --> 00:11:30,160
and see if i need to correct things

00:11:28,000 --> 00:11:33,600
maybe i found bugs or things like this

00:11:30,160 --> 00:11:36,560
before uh make a thing as a master

00:11:33,600 --> 00:11:37,920
instance of my application but also

00:11:36,560 --> 00:11:40,640
maybe i have an idea

00:11:37,920 --> 00:11:42,000
a business users uh has an app has an

00:11:40,640 --> 00:11:43,519
interesting features that

00:11:42,000 --> 00:11:45,200
need to be integrated into the

00:11:43,519 --> 00:11:48,240
application uh so

00:11:45,200 --> 00:11:50,320
we can consume the same data or uh do

00:11:48,240 --> 00:11:51,360
some branching from this data pipeline

00:11:50,320 --> 00:11:53,680
if we use like

00:11:51,360 --> 00:11:55,120
uh intermediate topics and and so on and

00:11:53,680 --> 00:11:58,160
test it and this is really

00:11:55,120 --> 00:12:01,200
important for the fast time to market

00:11:58,160 --> 00:12:03,279
because we are able to test new ideas

00:12:01,200 --> 00:12:05,839
build new services and features

00:12:03,279 --> 00:12:06,639
for our application without impacting

00:12:05,839 --> 00:12:09,440
what's what's

00:12:06,639 --> 00:12:11,040
what's available already and also we

00:12:09,440 --> 00:12:13,279
don't need to go over like

00:12:11,040 --> 00:12:14,160
long project of several months where we

00:12:13,279 --> 00:12:17,519
need to change

00:12:14,160 --> 00:12:20,320
our etl and think about the schema

00:12:17,519 --> 00:12:21,839
change it and and so on so it's uh it's

00:12:20,320 --> 00:12:25,360
really important to keep that

00:12:21,839 --> 00:12:27,839
in mind and the third

00:12:25,360 --> 00:12:29,200
reason why you would like to do event

00:12:27,839 --> 00:12:32,720
driven and streaming

00:12:29,200 --> 00:12:34,720
is higher scalability why because even

00:12:32,720 --> 00:12:36,720
if you need the data let's say

00:12:34,720 --> 00:12:39,680
at the end of the day you need to

00:12:36,720 --> 00:12:40,079
compute some kpis before midnight every

00:12:39,680 --> 00:12:42,160
day

00:12:40,079 --> 00:12:43,680
this is the sla that you shared with

00:12:42,160 --> 00:12:45,760
your business users

00:12:43,680 --> 00:12:47,040
so in in this case you are creating a

00:12:45,760 --> 00:12:50,079
lot of uh

00:12:47,040 --> 00:12:51,279
uh stress on the teams on the

00:12:50,079 --> 00:12:54,880
infrastructure

00:12:51,279 --> 00:12:58,000
to make sure for this window of

00:12:54,880 --> 00:12:58,560
one hour before midnight our jobs can

00:12:58,000 --> 00:13:00,560
compute

00:12:58,560 --> 00:13:02,880
everything compute all the aggregation

00:13:00,560 --> 00:13:04,320
the kpis and make them available

00:13:02,880 --> 00:13:06,480
so if there is any issue with your

00:13:04,320 --> 00:13:07,600
infrastructure will be you will have

00:13:06,480 --> 00:13:10,639
hard time to

00:13:07,600 --> 00:13:12,160
uh respect your sla the other thing

00:13:10,639 --> 00:13:14,480
is you will be sizing your

00:13:12,160 --> 00:13:17,519
infrastructure to do all this work

00:13:14,480 --> 00:13:18,079
in one hour for instance where if you do

00:13:17,519 --> 00:13:20,720
event

00:13:18,079 --> 00:13:21,200
uh and event driven and streaming you

00:13:20,720 --> 00:13:23,440
can

00:13:21,200 --> 00:13:25,600
you can distribute this load over the

00:13:23,440 --> 00:13:26,959
whole day so even if you don't need the

00:13:25,600 --> 00:13:30,079
final data

00:13:26,959 --> 00:13:30,720
uh at let's say 1pm but the fact that

00:13:30,079 --> 00:13:32,800
you are

00:13:30,720 --> 00:13:34,560
doing your aggregation your complica

00:13:32,800 --> 00:13:36,720
computation continuously

00:13:34,560 --> 00:13:38,560
you are preparing the work maybe there

00:13:36,720 --> 00:13:39,279
is some final aggregation that you need

00:13:38,560 --> 00:13:42,079
to do

00:13:39,279 --> 00:13:42,800
at 11 p.m before midnight that that's

00:13:42,079 --> 00:13:44,800
more heavy

00:13:42,800 --> 00:13:46,399
but at least you have done the biggest

00:13:44,800 --> 00:13:48,480
part of the work and

00:13:46,399 --> 00:13:50,480
this is really important like in in

00:13:48,480 --> 00:13:51,519
financial services where the regulation

00:13:50,480 --> 00:13:55,040
is changing

00:13:51,519 --> 00:13:56,079
and the the kpis are computed more

00:13:55,040 --> 00:13:59,360
frequently and

00:13:56,079 --> 00:14:02,880
and so on so uh

00:13:59,360 --> 00:14:04,480
we have we have seen now why its

00:14:02,880 --> 00:14:06,320
organization are using event driven

00:14:04,480 --> 00:14:07,680
architecture today but how they are

00:14:06,320 --> 00:14:11,040
implementing

00:14:07,680 --> 00:14:13,199
it in in real world usually

00:14:11,040 --> 00:14:14,959
there is an architecture around

00:14:13,199 --> 00:14:16,959
different streaming technologies and

00:14:14,959 --> 00:14:19,040
services so at the heart of it

00:14:16,959 --> 00:14:20,240
there is the broker that we saw in the

00:14:19,040 --> 00:14:23,519
previous

00:14:20,240 --> 00:14:24,639
slides where everything happening inside

00:14:23,519 --> 00:14:26,720
the organization

00:14:24,639 --> 00:14:28,160
whether it's technical or business

00:14:26,720 --> 00:14:30,880
events is published

00:14:28,160 --> 00:14:32,079
it's organized on two different topics

00:14:30,880 --> 00:14:34,079
and we have different

00:14:32,079 --> 00:14:36,720
applications and services subscribing to

00:14:34,079 --> 00:14:38,320
it so the data is coming from a variety

00:14:36,720 --> 00:14:40,800
of sources

00:14:38,320 --> 00:14:42,639
so if if the source is a database

00:14:40,800 --> 00:14:45,600
usually

00:14:42,639 --> 00:14:47,519
we try to use jetta capture technologies

00:14:45,600 --> 00:14:48,480
where we are able to connect into

00:14:47,519 --> 00:14:51,360
database

00:14:48,480 --> 00:14:52,560
and look into the log and capture

00:14:51,360 --> 00:14:54,880
everything it's happening

00:14:52,560 --> 00:14:55,920
in real time and publishing it to the

00:14:54,880 --> 00:14:57,839
broker

00:14:55,920 --> 00:15:00,160
while having a minimal impact on the

00:14:57,839 --> 00:15:01,839
database we can do query like we can

00:15:00,160 --> 00:15:04,880
query our database every

00:15:01,839 --> 00:15:06,240
one second 10 second but it's great this

00:15:04,880 --> 00:15:09,519
creates

00:15:06,240 --> 00:15:10,959
some latency it creates also stress on

00:15:09,519 --> 00:15:13,600
this databases

00:15:10,959 --> 00:15:14,880
and also we can miss some events so for

00:15:13,600 --> 00:15:18,160
instance if uh

00:15:14,880 --> 00:15:20,560
if a record have been created and then

00:15:18,160 --> 00:15:22,560
deleted from database we are sure to

00:15:20,560 --> 00:15:25,600
miss it if we if you don't do

00:15:22,560 --> 00:15:26,800
this frequently then we have different

00:15:25,600 --> 00:15:30,000
data sources like

00:15:26,800 --> 00:15:33,759
messaging systems files up and and

00:15:30,000 --> 00:15:35,600
and so on that we need to bring

00:15:33,759 --> 00:15:37,360
into the broker for this we have

00:15:35,600 --> 00:15:38,959
different connectors we have different

00:15:37,360 --> 00:15:42,160
systems that that can

00:15:38,959 --> 00:15:43,759
us that can help us do this and also of

00:15:42,160 --> 00:15:44,639
course it depends on the broker that we

00:15:43,759 --> 00:15:46,560
are using

00:15:44,639 --> 00:15:47,680
and finally we can have applications

00:15:46,560 --> 00:15:49,680
that rise that

00:15:47,680 --> 00:15:52,000
write directly into the broker using its

00:15:49,680 --> 00:15:54,880
api um

00:15:52,000 --> 00:15:55,920
which is a native integration usually

00:15:54,880 --> 00:15:58,079
this require

00:15:55,920 --> 00:16:00,000
uh rewriting some application if you

00:15:58,079 --> 00:16:01,440
have legacy application or at least

00:16:00,000 --> 00:16:03,040
existing application

00:16:01,440 --> 00:16:04,880
that's used to write this data into

00:16:03,040 --> 00:16:06,880
database so it means some

00:16:04,880 --> 00:16:08,000
some some work here that that you need

00:16:06,880 --> 00:16:10,160
to do to change

00:16:08,000 --> 00:16:11,120
the code and make it integrated directly

00:16:10,160 --> 00:16:13,279
with the broker

00:16:11,120 --> 00:16:14,639
once the events are in the broker we can

00:16:13,279 --> 00:16:16,880
do the etl

00:16:14,639 --> 00:16:18,079
with using streaming etl we can use

00:16:16,880 --> 00:16:20,320
stream processors

00:16:18,079 --> 00:16:21,519
um like flink or spark streaming and

00:16:20,320 --> 00:16:24,560
things like this

00:16:21,519 --> 00:16:26,639
to implement our data pipeline where we

00:16:24,560 --> 00:16:27,920
take raw data we do some aggregation

00:16:26,639 --> 00:16:30,720
based on windowing

00:16:27,920 --> 00:16:32,800
write back to the broker and then maybe

00:16:30,720 --> 00:16:35,440
do some filtering or alerting and

00:16:32,800 --> 00:16:37,759
and so on and and create different

00:16:35,440 --> 00:16:39,759
version of the

00:16:37,759 --> 00:16:41,360
our service and the application can also

00:16:39,759 --> 00:16:42,000
consume the data directly from the

00:16:41,360 --> 00:16:44,639
broker

00:16:42,000 --> 00:16:45,120
or we can export it into database and

00:16:44,639 --> 00:16:48,079
have

00:16:45,120 --> 00:16:50,639
classical application around it finally

00:16:48,079 --> 00:16:52,639
the same data can be exported cloud to

00:16:50,639 --> 00:16:53,279
data lakes or data warehouse where we

00:16:52,639 --> 00:16:56,880
can build

00:16:53,279 --> 00:16:58,800
dashboard and so on so from technology

00:16:56,880 --> 00:17:01,040
perspective we have the choice

00:16:58,800 --> 00:17:02,399
and we can choose and pick between

00:17:01,040 --> 00:17:04,720
different technologies

00:17:02,399 --> 00:17:05,919
so for anything around data ingestion

00:17:04,720 --> 00:17:09,039
and connectors

00:17:05,919 --> 00:17:10,640
um if we focus only the on the apache

00:17:09,039 --> 00:17:13,760
technologies since we are in in

00:17:10,640 --> 00:17:14,480
apache um conference there is the bzmc

00:17:13,760 --> 00:17:17,039
dc

00:17:14,480 --> 00:17:18,240
which is based on kafka connect and have

00:17:17,039 --> 00:17:21,280
connectors with different

00:17:18,240 --> 00:17:24,079
um databases like mysql and so on

00:17:21,280 --> 00:17:24,640
where we where this technology can go

00:17:24,079 --> 00:17:27,520
and look

00:17:24,640 --> 00:17:30,240
to the database logs and export the data

00:17:27,520 --> 00:17:30,240
in real time

00:17:30,320 --> 00:17:34,480
there is also apache nifi that's a tool

00:17:32,799 --> 00:17:37,280
with different connectors

00:17:34,480 --> 00:17:39,520
from sources to destination that's able

00:17:37,280 --> 00:17:42,880
to work in batch or in real time

00:17:39,520 --> 00:17:45,280
where we can connect to uh an http

00:17:42,880 --> 00:17:47,200
connection for instance bring the data

00:17:45,280 --> 00:17:49,760
do some lightweight transformation and

00:17:47,200 --> 00:17:52,880
then write it to kafka for instance

00:17:49,760 --> 00:17:55,600
uh if you use kafka as a broker you can

00:17:52,880 --> 00:17:58,240
also use kafka connect which is

00:17:55,600 --> 00:17:58,960
a sub project from from apache kafka

00:17:58,240 --> 00:18:01,679
that has

00:17:58,960 --> 00:18:02,720
also a high number of connectors where

00:18:01,679 --> 00:18:04,400
that you can use

00:18:02,720 --> 00:18:06,000
you can use also other systems like

00:18:04,400 --> 00:18:08,960
apache camera and so

00:18:06,000 --> 00:18:10,559
from broker perspective uh from my

00:18:08,960 --> 00:18:13,360
opinion apache kafka is

00:18:10,559 --> 00:18:15,200
is today the this thunder uh this is the

00:18:13,360 --> 00:18:15,919
thing this is the technology that we see

00:18:15,200 --> 00:18:18,880
in

00:18:15,919 --> 00:18:20,480
almost all the deployments out there uh

00:18:18,880 --> 00:18:22,799
where apache pulsar

00:18:20,480 --> 00:18:23,840
is getting attention and and we see it

00:18:22,799 --> 00:18:26,960
more often

00:18:23,840 --> 00:18:29,679
um so maybe something an

00:18:26,960 --> 00:18:30,960
area to look for in the coming months

00:18:29,679 --> 00:18:33,600
and

00:18:30,960 --> 00:18:34,400
and the years from stream processing

00:18:33,600 --> 00:18:35,840
perspective

00:18:34,400 --> 00:18:37,760
there is also a lot of apache

00:18:35,840 --> 00:18:38,480
technologies that can be used fling

00:18:37,760 --> 00:18:41,280
sparks

00:18:38,480 --> 00:18:42,880
storm samsa some of them are evolving uh

00:18:41,280 --> 00:18:43,360
really well and the community around

00:18:42,880 --> 00:18:45,919
them

00:18:43,360 --> 00:18:46,960
um is is continue to give some

00:18:45,919 --> 00:18:49,679
contribution

00:18:46,960 --> 00:18:50,000
and and so on some of them i'm losing of

00:18:49,679 --> 00:18:52,480
their

00:18:50,000 --> 00:18:53,679
speed like apache storm for instance but

00:18:52,480 --> 00:18:56,240
yeah there is there is a lot of

00:18:53,679 --> 00:18:59,520
technology there that can be used and

00:18:56,240 --> 00:19:01,200
you can have the choice so

00:18:59,520 --> 00:19:02,880
what's the challenge with this kind of

00:19:01,200 --> 00:19:05,919
of architectures and and

00:19:02,880 --> 00:19:08,480
what's the objective of this talk for

00:19:05,919 --> 00:19:08,480
for sure

00:19:09,280 --> 00:19:13,600
using streaming technologies as building

00:19:11,919 --> 00:19:15,760
blocks for event driven architecture

00:19:13,600 --> 00:19:18,000
introduced three main challenges

00:19:15,760 --> 00:19:18,960
the first one is actually stream

00:19:18,000 --> 00:19:21,600
processing is

00:19:18,960 --> 00:19:22,160
is really is different from analytics

00:19:21,600 --> 00:19:24,880
some

00:19:22,160 --> 00:19:26,400
um some of them call this streaming

00:19:24,880 --> 00:19:27,840
technology as streaming analytics but at

00:19:26,400 --> 00:19:29,200
the end of the day it's it's not the

00:19:27,840 --> 00:19:32,000
case

00:19:29,200 --> 00:19:34,160
the first challenge that we will face is

00:19:32,000 --> 00:19:34,799
using this technology's required coding

00:19:34,160 --> 00:19:38,640
skills

00:19:34,799 --> 00:19:42,160
so java scala and things like this

00:19:38,640 --> 00:19:46,960
so if we have if we go back to the first

00:19:42,160 --> 00:19:49,440
objective of building an event driven

00:19:46,960 --> 00:19:50,400
organization is make this event

00:19:49,440 --> 00:19:53,280
available for

00:19:50,400 --> 00:19:55,280
different teams and business and other

00:19:53,280 --> 00:19:56,080
business teams analyst teams don't have

00:19:55,280 --> 00:19:58,480
the skills

00:19:56,080 --> 00:20:00,080
so they cannot write java or scala code

00:19:58,480 --> 00:20:02,559
to consume these events

00:20:00,080 --> 00:20:03,520
so this is one of the challenge um to

00:20:02,559 --> 00:20:06,000
cope with it

00:20:03,520 --> 00:20:08,080
the this modern streaming technologies

00:20:06,000 --> 00:20:10,000
try to have support for sql

00:20:08,080 --> 00:20:11,120
for instance we have fling sql we have

00:20:10,000 --> 00:20:13,280
kafka sql

00:20:11,120 --> 00:20:14,400
uh we have spark structure streaming and

00:20:13,280 --> 00:20:17,120
so on but

00:20:14,400 --> 00:20:19,200
adding the sql support doesn't make this

00:20:17,120 --> 00:20:21,440
technology stream analytics

00:20:19,200 --> 00:20:22,720
technologies it's it's still stream

00:20:21,440 --> 00:20:26,320
processing

00:20:22,720 --> 00:20:28,480
and there is some uh differences from

00:20:26,320 --> 00:20:29,360
the operation perspective uh stream

00:20:28,480 --> 00:20:31,919
processing

00:20:29,360 --> 00:20:32,720
we try to do some windowing alerting

00:20:31,919 --> 00:20:34,480
filtering

00:20:32,720 --> 00:20:36,480
where analytics we try to compute

00:20:34,480 --> 00:20:38,400
statistics aggregation and so on

00:20:36,480 --> 00:20:40,480
we can do some of them using stream

00:20:38,400 --> 00:20:42,480
technologies but really focus on the

00:20:40,480 --> 00:20:43,919
real-time part of it so it's optimized

00:20:42,480 --> 00:20:46,159
for smaller windows and

00:20:43,919 --> 00:20:47,360
and so on where in the analytics we need

00:20:46,159 --> 00:20:49,919
to look to maybe

00:20:47,360 --> 00:20:51,840
longer history and compare between the

00:20:49,919 --> 00:20:53,679
trend that we are seeing in in the last

00:20:51,840 --> 00:20:56,640
hour compared to the six months and

00:20:53,679 --> 00:20:59,039
and so on the second difference is

00:20:56,640 --> 00:21:02,000
stream processing is long-running jobs

00:20:59,039 --> 00:21:02,320
where analytics is more adult so we have

00:21:02,000 --> 00:21:04,720
uh

00:21:02,320 --> 00:21:06,720
business users going asking questions

00:21:04,720 --> 00:21:08,080
based on the data getting answers and

00:21:06,720 --> 00:21:10,400
asking other questions

00:21:08,080 --> 00:21:11,440
and so on so there is law of interaction

00:21:10,400 --> 00:21:14,640
and uh

00:21:11,440 --> 00:21:15,679
discovery based based on this um and the

00:21:14,640 --> 00:21:17,440
problem with

00:21:15,679 --> 00:21:19,919
or the limitation with the streaming

00:21:17,440 --> 00:21:22,960
technologies is every question

00:21:19,919 --> 00:21:23,919
will end up being a new pipeline so if

00:21:22,960 --> 00:21:26,799
you need to compute

00:21:23,919 --> 00:21:28,480
kpis for instance the number of users

00:21:26,799 --> 00:21:30,960
connecting to your

00:21:28,480 --> 00:21:31,840
website every five minutes so it's a new

00:21:30,960 --> 00:21:34,159
pipeline

00:21:31,840 --> 00:21:35,200
you are creating a new topic maybe in

00:21:34,159 --> 00:21:38,320
kafka and

00:21:35,200 --> 00:21:40,080
uh yeah publishing this this computation

00:21:38,320 --> 00:21:42,000
if tomorrow you need to change it to one

00:21:40,080 --> 00:21:44,720
hour you need to go back and

00:21:42,000 --> 00:21:45,760
change things or deploy and deliver new

00:21:44,720 --> 00:21:47,919
pipelines so

00:21:45,760 --> 00:21:49,600
from time to market perspective it takes

00:21:47,919 --> 00:21:52,159
a lot of time and every time

00:21:49,600 --> 00:21:54,960
we need to go over a team that have this

00:21:52,159 --> 00:21:57,039
skills for writing java and scala code

00:21:54,960 --> 00:21:59,440
some of technologies has also

00:21:57,039 --> 00:22:01,679
materialized views where

00:21:59,440 --> 00:22:03,120
it it do some of this computation for

00:22:01,679 --> 00:22:06,640
you but it's it's limited

00:22:03,120 --> 00:22:07,520
as i said for um in terms of scalability

00:22:06,640 --> 00:22:11,600
and also how

00:22:07,520 --> 00:22:13,520
how long it goes back in the history

00:22:11,600 --> 00:22:15,520
the second challenge is really the

00:22:13,520 --> 00:22:16,240
integration with the analytics tool so

00:22:15,520 --> 00:22:19,039
if you are

00:22:16,240 --> 00:22:21,520
a data analyst today or business users

00:22:19,039 --> 00:22:22,799
you want to use your bi tools your data

00:22:21,520 --> 00:22:25,600
visualization tools

00:22:22,799 --> 00:22:26,720
and build some dashboard so going and

00:22:25,600 --> 00:22:29,200
connecting

00:22:26,720 --> 00:22:30,960
these things into kafka or pulsar is

00:22:29,200 --> 00:22:31,919
really a challenge it's not meant for

00:22:30,960 --> 00:22:35,039
this

00:22:31,919 --> 00:22:36,159
i'm sure that everyone listening today

00:22:35,039 --> 00:22:39,280
have heard their

00:22:36,159 --> 00:22:43,120
business users uh asking them to connect

00:22:39,280 --> 00:22:45,200
excel to kafka or to data lake and

00:22:43,120 --> 00:22:47,200
complaining about the performance and

00:22:45,200 --> 00:22:48,000
and so on so it's not only a matter of

00:22:47,200 --> 00:22:50,480
connectors but

00:22:48,000 --> 00:22:53,039
but they did they know they were not

00:22:50,480 --> 00:22:55,440
built for the same word

00:22:53,039 --> 00:22:57,760
um one part of the challenge comes from

00:22:55,440 --> 00:23:00,320
the connectors um like there is no

00:22:57,760 --> 00:23:01,360
standard connected like gdbc or dbc that

00:23:00,320 --> 00:23:05,280
we can find with

00:23:01,360 --> 00:23:08,799
databases uh and also every technologies

00:23:05,280 --> 00:23:12,320
even if it support sql will have its

00:23:08,799 --> 00:23:15,440
own specific language for managing the

00:23:12,320 --> 00:23:17,919
the timestamp offsets the joins and

00:23:15,440 --> 00:23:19,360
and so on so there is no standard uh

00:23:17,919 --> 00:23:22,720
here even if they

00:23:19,360 --> 00:23:25,520
try to use and be on c

00:23:22,720 --> 00:23:26,640
sql compliant there will be always some

00:23:25,520 --> 00:23:28,720
specificities

00:23:26,640 --> 00:23:31,360
because we are in the stream processing

00:23:28,720 --> 00:23:34,640
and not the analytics world

00:23:31,360 --> 00:23:36,640
and finally using these tools with

00:23:34,640 --> 00:23:38,799
the granularity of events that we are

00:23:36,640 --> 00:23:40,400
collecting we are talking about million

00:23:38,799 --> 00:23:42,240
of events per second here

00:23:40,400 --> 00:23:44,960
will be a challenge for these tools so

00:23:42,240 --> 00:23:46,720
we need some aggregation computation of

00:23:44,960 --> 00:23:47,679
statistics and so on to make it

00:23:46,720 --> 00:23:49,279
efficient and

00:23:47,679 --> 00:23:51,279
useful at the end of the day the

00:23:49,279 --> 00:23:52,799
business users don't don't care about

00:23:51,279 --> 00:23:54,960
one particular events

00:23:52,799 --> 00:23:56,000
but care about the trends about the

00:23:54,960 --> 00:23:59,120
statistics and

00:23:56,000 --> 00:24:01,520
so on and finally the

00:23:59,120 --> 00:24:02,320
the last challenge is uh round

00:24:01,520 --> 00:24:06,159
complexity

00:24:02,320 --> 00:24:09,360
and uh for having advanced analytics

00:24:06,159 --> 00:24:10,080
so usually you will have this really

00:24:09,360 --> 00:24:12,559
powerful

00:24:10,080 --> 00:24:14,640
low latency streaming architecture but

00:24:12,559 --> 00:24:17,279
when you want to do advanced analytics

00:24:14,640 --> 00:24:18,320
uh you don't have the choice uh and you

00:24:17,279 --> 00:24:20,640
need to

00:24:18,320 --> 00:24:21,679
put all this data into a data lake or

00:24:20,640 --> 00:24:24,880
data warehouse

00:24:21,679 --> 00:24:27,039
and use tools like hive or

00:24:24,880 --> 00:24:28,799
bigquery and things like this to do to

00:24:27,039 --> 00:24:30,640
build your advanced analytics

00:24:28,799 --> 00:24:32,799
this creates two problems the first one

00:24:30,640 --> 00:24:35,039
is these tools have

00:24:32,799 --> 00:24:36,400
maybe slow and just which introduce

00:24:35,039 --> 00:24:40,320
latency for your data

00:24:36,400 --> 00:24:42,640
so this is what we call slow data in or

00:24:40,320 --> 00:24:44,240
because you have all this granularity

00:24:42,640 --> 00:24:46,320
and events

00:24:44,240 --> 00:24:49,039
it makes the data that you are querying

00:24:46,320 --> 00:24:51,679
huge and the query will be really slow

00:24:49,039 --> 00:24:53,039
like uh slow data out so it works well

00:24:51,679 --> 00:24:55,760
for the batch word

00:24:53,039 --> 00:24:56,960
but if you do interactive queries with

00:24:55,760 --> 00:25:01,120
slice and dice

00:24:56,960 --> 00:25:04,240
you are looking for sub second query

00:25:01,120 --> 00:25:06,880
response time and the systems doesn't

00:25:04,240 --> 00:25:07,919
doesn't cope with your needs so one of

00:25:06,880 --> 00:25:10,480
the work runs

00:25:07,919 --> 00:25:11,679
is creating data merge which we where we

00:25:10,480 --> 00:25:14,480
try to have like

00:25:11,679 --> 00:25:15,840
a view on this data we take only the

00:25:14,480 --> 00:25:18,320
events that we need

00:25:15,840 --> 00:25:20,640
and we we try to do some computation

00:25:18,320 --> 00:25:23,440
using spark or

00:25:20,640 --> 00:25:25,440
flink and things like this but this

00:25:23,440 --> 00:25:27,440
again creates some complexity

00:25:25,440 --> 00:25:29,200
add latency to the data every time we

00:25:27,440 --> 00:25:31,760
are adding uh steps

00:25:29,200 --> 00:25:33,039
uh create security around gdpr and

00:25:31,760 --> 00:25:34,880
things like this where

00:25:33,039 --> 00:25:36,159
every time we are creating a copy of

00:25:34,880 --> 00:25:39,279
data and if

00:25:36,159 --> 00:25:41,120
someone if we need to delete data for

00:25:39,279 --> 00:25:44,080
for a user for instance we need to go

00:25:41,120 --> 00:25:47,600
over all of these copies

00:25:44,080 --> 00:25:50,799
and you also need to go over your

00:25:47,600 --> 00:25:53,039
iit or data engineering team

00:25:50,799 --> 00:25:54,880
to create this these views for you so

00:25:53,039 --> 00:25:56,159
again we are losing time we are losing

00:25:54,880 --> 00:25:59,120
time to market

00:25:56,159 --> 00:25:59,840
and of course you need to keep these two

00:25:59,120 --> 00:26:01,840
systems

00:25:59,840 --> 00:26:03,120
one for real-time analytics one for

00:26:01,840 --> 00:26:05,120
watch analytics

00:26:03,120 --> 00:26:06,480
which we called lambda architecture for

00:26:05,120 --> 00:26:10,000
for a few years ago

00:26:06,480 --> 00:26:13,039
um and um and and make things uh

00:26:10,000 --> 00:26:15,440
complex to manage so

00:26:13,039 --> 00:26:16,400
at the end of the day it's an unfair

00:26:15,440 --> 00:26:20,080
word because

00:26:16,400 --> 00:26:22,159
having this event-driven vision

00:26:20,080 --> 00:26:24,880
that's based on streaming technologies

00:26:22,159 --> 00:26:27,360
and architectures

00:26:24,880 --> 00:26:29,360
keep its promise for the itn developers

00:26:27,360 --> 00:26:30,000
because we can get low latency access to

00:26:29,360 --> 00:26:32,240
events

00:26:30,000 --> 00:26:34,320
we have all the agility that we saw

00:26:32,240 --> 00:26:36,400
granularity of data is kept because we

00:26:34,320 --> 00:26:37,919
are able to have this

00:26:36,400 --> 00:26:39,919
streaming technologies which are

00:26:37,919 --> 00:26:42,320
scalable and can compute

00:26:39,919 --> 00:26:43,919
some aggregation based on this uh it's

00:26:42,320 --> 00:26:46,000
it's scalable and we have all the

00:26:43,919 --> 00:26:46,480
tooling and systems because because we

00:26:46,000 --> 00:26:49,679
use

00:26:46,480 --> 00:26:51,440
java you are using a scalan and so on

00:26:49,679 --> 00:26:52,799
but for the analyst and the business

00:26:51,440 --> 00:26:55,760
users

00:26:52,799 --> 00:26:56,960
it's kind of missing the the main point

00:26:55,760 --> 00:27:00,240
of building this

00:26:56,960 --> 00:27:01,760
organization so how we can improve this

00:27:00,240 --> 00:27:05,360
how we can make things

00:27:01,760 --> 00:27:08,400
better one of the

00:27:05,360 --> 00:27:10,559
answer could be using apache druid

00:27:08,400 --> 00:27:11,840
so apache druid is an interesting

00:27:10,559 --> 00:27:15,679
technologies

00:27:11,840 --> 00:27:18,240
it's a kind of hybrid database

00:27:15,679 --> 00:27:19,039
that has features from time series

00:27:18,240 --> 00:27:20,799
database

00:27:19,039 --> 00:27:23,039
we are working we are talking about

00:27:20,799 --> 00:27:26,159
streaming so we have this

00:27:23,039 --> 00:27:28,960
time notion which is really important um

00:27:26,159 --> 00:27:29,679
we have also this notion from data

00:27:28,960 --> 00:27:31,679
warehouse

00:27:29,679 --> 00:27:32,799
so we have the tower housing features

00:27:31,679 --> 00:27:35,120
because we are chewing

00:27:32,799 --> 00:27:37,200
we are trying to build all up systems we

00:27:35,120 --> 00:27:40,080
are using um doing some

00:27:37,200 --> 00:27:40,559
aggregation group by computation and so

00:27:40,080 --> 00:27:42,880
on

00:27:40,559 --> 00:27:43,679
and finally search system because we

00:27:42,880 --> 00:27:46,080
need to go

00:27:43,679 --> 00:27:48,559
and do some addoc analysis and the

00:27:46,080 --> 00:27:52,000
exploration on the data

00:27:48,559 --> 00:27:54,159
so druid is uh taking features and uh

00:27:52,000 --> 00:27:55,520
concepts architectural concepts from all

00:27:54,159 --> 00:27:58,080
of these areas

00:27:55,520 --> 00:28:00,000
to have like past inkless architectures

00:27:58,080 --> 00:28:02,960
that combine between of them

00:28:00,000 --> 00:28:05,120
the the objective of druid is having

00:28:02,960 --> 00:28:07,520
fast data in and fast data out

00:28:05,120 --> 00:28:08,399
it means that we are able to ingest

00:28:07,520 --> 00:28:10,720
quickly

00:28:08,399 --> 00:28:13,360
large volume of data from broker like

00:28:10,720 --> 00:28:16,799
kafka and make them available for

00:28:13,360 --> 00:28:20,799
complex queries for a high concurrency

00:28:16,799 --> 00:28:23,279
number of users in real time so

00:28:20,799 --> 00:28:24,399
the latency between something happening

00:28:23,279 --> 00:28:27,919
in a real world

00:28:24,399 --> 00:28:29,840
and when it shows up in my query answer

00:28:27,919 --> 00:28:31,919
or my dashboard or my application will

00:28:29,840 --> 00:28:33,840
be sub second

00:28:31,919 --> 00:28:36,159
and of course it's apache project so

00:28:33,840 --> 00:28:38,799
it's built around open source community

00:28:36,159 --> 00:28:40,480
that's uh bringing new features and

00:28:38,799 --> 00:28:43,279
contribution to the project

00:28:40,480 --> 00:28:45,760
every month and if i release which is uh

00:28:43,279 --> 00:28:48,880
which is fun project to to work on

00:28:45,760 --> 00:28:51,919
so what makes uh druid really different

00:28:48,880 --> 00:28:54,480
and what makes druid a good

00:28:51,919 --> 00:28:55,840
feed for this event-driven architecture

00:28:54,480 --> 00:28:58,720
first is

00:28:55,840 --> 00:28:59,360
time-based data management so druids

00:28:58,720 --> 00:29:01,919
partition

00:28:59,360 --> 00:29:02,559
data by time and can additionally

00:29:01,919 --> 00:29:06,399
partition

00:29:02,559 --> 00:29:08,720
by other fields so this means that

00:29:06,399 --> 00:29:10,559
time-based queries will only access the

00:29:08,720 --> 00:29:11,279
partition that match the time range of

00:29:10,559 --> 00:29:13,679
the query

00:29:11,279 --> 00:29:14,960
which leads to significant performance

00:29:13,679 --> 00:29:16,799
improvement

00:29:14,960 --> 00:29:19,360
for time-based operations such as

00:29:16,799 --> 00:29:22,240
streaming and event-driven analytics

00:29:19,360 --> 00:29:23,279
the second thing is pool based real-time

00:29:22,240 --> 00:29:25,679
and batch ingestion

00:29:23,279 --> 00:29:26,880
so druid can adjust data either in real

00:29:25,679 --> 00:29:29,360
time or in batch

00:29:26,880 --> 00:29:31,600
it has native support for brokers like

00:29:29,360 --> 00:29:33,360
apache kafka or kinesis

00:29:31,600 --> 00:29:34,799
data is available for querying few

00:29:33,360 --> 00:29:37,520
milliseconds after

00:29:34,799 --> 00:29:39,279
the unjust thanks to dedicated and

00:29:37,520 --> 00:29:40,480
dexter and services that we can see in

00:29:39,279 --> 00:29:42,960
the next slide

00:29:40,480 --> 00:29:44,159
and it can also index data from hdfs or

00:29:42,960 --> 00:29:46,960
cloud storage

00:29:44,159 --> 00:29:47,279
so you can look to it you can see it as

00:29:46,960 --> 00:29:49,200
an

00:29:47,279 --> 00:29:51,679
out of the box managed lambda

00:29:49,200 --> 00:29:54,720
architecture which make things really

00:29:51,679 --> 00:29:56,720
simple and easy the third thing is is

00:29:54,720 --> 00:29:58,320
roll up so druid

00:29:56,720 --> 00:30:00,399
optionally supports relapse and

00:29:58,320 --> 00:30:02,880
summarization at just time

00:30:00,399 --> 00:30:03,520
uh so the rule ups merge the events

00:30:02,880 --> 00:30:05,520
together

00:30:03,520 --> 00:30:06,960
at the given frequency let's say you are

00:30:05,520 --> 00:30:10,399
receiving events and

00:30:06,960 --> 00:30:12,240
you need to look or do analytics at

00:30:10,399 --> 00:30:14,559
five minutes so you can you can ask

00:30:12,240 --> 00:30:17,200
through it to do it for you

00:30:14,559 --> 00:30:19,200
which leads to big cost savings in terms

00:30:17,200 --> 00:30:21,840
of performance boost as well

00:30:19,200 --> 00:30:23,520
for event driven application

00:30:21,840 --> 00:30:26,720
vectorization errors

00:30:23,520 --> 00:30:27,679
where drew would use the underlying

00:30:26,720 --> 00:30:30,080
infrastructure

00:30:27,679 --> 00:30:31,120
and and hardware to make your query

00:30:30,080 --> 00:30:33,760
faster

00:30:31,120 --> 00:30:34,559
and there is also a lot of optimization

00:30:33,760 --> 00:30:36,559
around

00:30:34,559 --> 00:30:38,320
approximation algorithm so for instance

00:30:36,559 --> 00:30:40,720
druid includes some

00:30:38,320 --> 00:30:42,000
account distinct approximate ranking

00:30:40,720 --> 00:30:45,039
computation

00:30:42,000 --> 00:30:48,240
uh approximate histogram and quantiles

00:30:45,039 --> 00:30:50,559
so this implementation offer limited

00:30:48,240 --> 00:30:51,360
memory usage and faster compute

00:30:50,559 --> 00:30:55,760
computation

00:30:51,360 --> 00:30:59,120
so compared to exact operation of course

00:30:55,760 --> 00:31:02,320
which is a very good fit for a large

00:30:59,120 --> 00:31:04,559
range of event driven application but

00:31:02,320 --> 00:31:07,440
for situations where you need exact

00:31:04,559 --> 00:31:09,440
uh count for instance druid is also able

00:31:07,440 --> 00:31:12,000
to to do it

00:31:09,440 --> 00:31:13,120
another optimization is around indexing

00:31:12,000 --> 00:31:15,919
so you would use

00:31:13,120 --> 00:31:18,799
uh roaring and concise compressed bitmap

00:31:15,919 --> 00:31:21,440
indexes to create index that power

00:31:18,799 --> 00:31:22,480
uh fast filtering searching across

00:31:21,440 --> 00:31:24,159
multiple columns

00:31:22,480 --> 00:31:27,039
which is really important for slicing

00:31:24,159 --> 00:31:29,279
and dicing and and cubing

00:31:27,039 --> 00:31:31,120
this is huge for for the usability for

00:31:29,279 --> 00:31:33,919
the and the user experience

00:31:31,120 --> 00:31:34,880
where business users go and look for

00:31:33,919 --> 00:31:37,440
something

00:31:34,880 --> 00:31:39,519
specific over millions of events per

00:31:37,440 --> 00:31:42,080
seconds

00:31:39,519 --> 00:31:43,440
that's really powerful and finally it's

00:31:42,080 --> 00:31:46,000
a horizontal

00:31:43,440 --> 00:31:48,960
horizontally scalable system that can be

00:31:46,000 --> 00:31:50,480
deployed in clusters of tens or hundreds

00:31:48,960 --> 00:31:52,640
or thousands of servers

00:31:50,480 --> 00:31:54,159
and can offer an adjustion rate of

00:31:52,640 --> 00:31:57,760
million millions

00:31:54,159 --> 00:31:59,679
requests per second we can keep

00:31:57,760 --> 00:32:02,240
some history of the data of around

00:31:59,679 --> 00:32:03,360
trillion of records and query latency of

00:32:02,240 --> 00:32:05,760
subsequence

00:32:03,360 --> 00:32:06,559
on all the all of this data so it's it's

00:32:05,760 --> 00:32:09,440
really a

00:32:06,559 --> 00:32:10,640
powerful system and the architecture is

00:32:09,440 --> 00:32:12,080
is really interesting

00:32:10,640 --> 00:32:14,320
so you can see the different data

00:32:12,080 --> 00:32:15,279
sources whether they are batch or

00:32:14,320 --> 00:32:18,480
streaming

00:32:15,279 --> 00:32:20,000
and it it's it's based on micro service

00:32:18,480 --> 00:32:22,480
architecture where we have

00:32:20,000 --> 00:32:23,600
indexes processes that are responsible

00:32:22,480 --> 00:32:25,919
for adjusting in

00:32:23,600 --> 00:32:28,080
this data in real time building these

00:32:25,919 --> 00:32:31,760
segments which are optimized column

00:32:28,080 --> 00:32:33,120
oriented data structure and put them on

00:32:31,760 --> 00:32:36,320
deep storage that can be

00:32:33,120 --> 00:32:38,399
s3 or hdfs or something like this

00:32:36,320 --> 00:32:39,600
and then loading them into her

00:32:38,399 --> 00:32:42,320
historical data

00:32:39,600 --> 00:32:44,080
that way you can query the data from

00:32:42,320 --> 00:32:47,279
real time or historical

00:32:44,080 --> 00:32:48,640
data sources transparently which is

00:32:47,279 --> 00:32:51,840
which is really interesting for this

00:32:48,640 --> 00:32:51,840
kind of use case

00:32:53,120 --> 00:32:57,200
so what about queries what about

00:32:55,120 --> 00:33:01,519
integration with the ecosystem

00:32:57,200 --> 00:33:04,640
so druids support two main ways of

00:33:01,519 --> 00:33:05,919
querying the data the first one is json

00:33:04,640 --> 00:33:08,720
over http

00:33:05,919 --> 00:33:10,159
which we call native queries so if you

00:33:08,720 --> 00:33:12,000
are building front-end

00:33:10,159 --> 00:33:13,840
this is something that you can use to

00:33:12,000 --> 00:33:16,320
quickly and easily integrate

00:33:13,840 --> 00:33:18,320
so at imply we have built a tool for a

00:33:16,320 --> 00:33:18,880
real-time slice and dice called imply

00:33:18,320 --> 00:33:20,880
pivo

00:33:18,880 --> 00:33:22,159
this is what we are using uh behind the

00:33:20,880 --> 00:33:25,279
scene the scene

00:33:22,159 --> 00:33:26,480
there is other open source solution like

00:33:25,279 --> 00:33:29,120
superset that that

00:33:26,480 --> 00:33:29,760
use and leverage this native queries as

00:33:29,120 --> 00:33:31,919
well

00:33:29,760 --> 00:33:33,760
and this is a very good way of

00:33:31,919 --> 00:33:37,279
integrating with druid

00:33:33,760 --> 00:33:39,919
for the bi and

00:33:37,279 --> 00:33:40,960
let's say traditional tools for for

00:33:39,919 --> 00:33:43,440
analytics

00:33:40,960 --> 00:33:44,799
there is uh apache druid also supports

00:33:43,440 --> 00:33:47,600
sql queries

00:33:44,799 --> 00:33:50,640
that's based on apache calcite so you

00:33:47,600 --> 00:33:53,840
can connect your bi tools directly to uh

00:33:50,640 --> 00:33:53,840
with as any

00:33:54,559 --> 00:34:02,159
database out there in in the world

00:33:58,799 --> 00:34:04,399
so if we take if we go back to our

00:34:02,159 --> 00:34:06,480
event driven architecture that based on

00:34:04,399 --> 00:34:09,359
on streaming technologies

00:34:06,480 --> 00:34:10,320
we can really augment it and make it

00:34:09,359 --> 00:34:13,119
more advanced

00:34:10,320 --> 00:34:14,720
by adding druid to the equation so we

00:34:13,119 --> 00:34:16,800
keep everything that we had

00:34:14,720 --> 00:34:18,839
we have our different data sources we

00:34:16,800 --> 00:34:21,359
have our cdc solution

00:34:18,839 --> 00:34:22,000
connectors uh whether it's app watching

00:34:21,359 --> 00:34:24,720
ifr

00:34:22,000 --> 00:34:25,679
kafka connect or camel to move data to

00:34:24,720 --> 00:34:28,960
brokers

00:34:25,679 --> 00:34:32,480
we keep also all the streaming etl and

00:34:28,960 --> 00:34:35,359
real-time microservices that can consume

00:34:32,480 --> 00:34:37,200
data from the broker directly but for

00:34:35,359 --> 00:34:40,000
anything around analytics

00:34:37,200 --> 00:34:41,280
the data will be adjusted to druid druid

00:34:40,000 --> 00:34:44,639
will index it

00:34:41,280 --> 00:34:47,119
optimize it maybe do some more like

00:34:44,639 --> 00:34:48,960
roll up of the data and compute all of

00:34:47,119 --> 00:34:49,280
these statistics to make it available

00:34:48,960 --> 00:34:51,440
for

00:34:49,280 --> 00:34:53,040
different kind of application so for

00:34:51,440 --> 00:34:54,079
instance if you need to do root cause

00:34:53,040 --> 00:34:56,000
analysis

00:34:54,079 --> 00:34:58,000
uh thanks to these features you can

00:34:56,000 --> 00:35:01,200
slice and dice and filter and

00:34:58,000 --> 00:35:02,640
and good go and try to understand what's

00:35:01,200 --> 00:35:05,359
happening

00:35:02,640 --> 00:35:07,200
let's say for your forward algorithms so

00:35:05,359 --> 00:35:10,400
druid will help you have this

00:35:07,200 --> 00:35:11,440
uh sub second query latency which which

00:35:10,400 --> 00:35:14,160
make your uh

00:35:11,440 --> 00:35:15,520
which make your you efficient for for

00:35:14,160 --> 00:35:17,119
the work

00:35:15,520 --> 00:35:18,960
there's also all the integration with

00:35:17,119 --> 00:35:21,280
the bi tools and

00:35:18,960 --> 00:35:23,520
the real-time analytics part of it you

00:35:21,280 --> 00:35:26,079
don't need to know

00:35:23,520 --> 00:35:27,520
java or scala to interact with it so you

00:35:26,079 --> 00:35:30,000
can use your own tooling as

00:35:27,520 --> 00:35:32,160
you are business user or analyst you can

00:35:30,000 --> 00:35:33,760
also use sql if something that that you

00:35:32,160 --> 00:35:36,000
like and that you use

00:35:33,760 --> 00:35:38,960
um and and finally you can build your

00:35:36,000 --> 00:35:40,079
custom visualization dashboard report or

00:35:38,960 --> 00:35:42,240
even having your

00:35:40,079 --> 00:35:43,119
machine learning yi algorithms that go

00:35:42,240 --> 00:35:46,400
and call

00:35:43,119 --> 00:35:49,839
draw it directly to have this

00:35:46,400 --> 00:35:52,480
statistics in in the prediction process

00:35:49,839 --> 00:35:54,400
so really making the architecture more

00:35:52,480 --> 00:35:55,119
swearable for analytics but also

00:35:54,400 --> 00:35:57,680
enabling

00:35:55,119 --> 00:35:59,280
new use case that we were not able to

00:35:57,680 --> 00:36:02,240
build

00:35:59,280 --> 00:36:03,760
and of course this make our analyst and

00:36:02,240 --> 00:36:06,079
business users happier

00:36:03,760 --> 00:36:07,520
because they can bring their tools they

00:36:06,079 --> 00:36:10,079
can use it they can act

00:36:07,520 --> 00:36:12,880
access data in low latency because we

00:36:10,079 --> 00:36:14,880
have this fast in fast out capabilities

00:36:12,880 --> 00:36:17,200
they have access to all the granularity

00:36:14,880 --> 00:36:19,280
of the data they can they can compute

00:36:17,200 --> 00:36:19,760
statistics and use them but if they need

00:36:19,280 --> 00:36:22,160
to go

00:36:19,760 --> 00:36:23,599
deeper into the event level that's

00:36:22,160 --> 00:36:25,680
something that they can do

00:36:23,599 --> 00:36:26,880
and of course the technology is scalable

00:36:25,680 --> 00:36:29,520
so that we can support

00:36:26,880 --> 00:36:30,560
million or billion of events uh that's

00:36:29,520 --> 00:36:32,640
interested

00:36:30,560 --> 00:36:34,240
and also compared between the real time

00:36:32,640 --> 00:36:37,680
and um

00:36:34,240 --> 00:36:41,440
and the batch part

00:36:37,680 --> 00:36:43,520
so as a conclusion a lot of companies

00:36:41,440 --> 00:36:44,960
today are using event driven for having

00:36:43,520 --> 00:36:47,599
low latency insights

00:36:44,960 --> 00:36:49,040
better agility and high scalability and

00:36:47,599 --> 00:36:51,119
to achieve this they use

00:36:49,040 --> 00:36:53,040
streaming technologies as a foundation

00:36:51,119 --> 00:36:55,520
from a technical standpoint

00:36:53,040 --> 00:36:56,240
so things like apache kafka flink and

00:36:55,520 --> 00:36:58,800
ifi are

00:36:56,240 --> 00:37:01,280
are often used but this architecture

00:36:58,800 --> 00:37:01,599
like uh delivers the promise that that

00:37:01,280 --> 00:37:04,880
they

00:37:01,599 --> 00:37:05,520
uh for our deployers but our business

00:37:04,880 --> 00:37:08,880
users

00:37:05,520 --> 00:37:11,359
are kept on the side um adding apache

00:37:08,880 --> 00:37:14,240
druid to this equation

00:37:11,359 --> 00:37:15,200
enabled these business users and uh and

00:37:14,240 --> 00:37:17,440
the

00:37:15,200 --> 00:37:19,359
analysts to consume this data in real

00:37:17,440 --> 00:37:21,200
time in self-service way

00:37:19,359 --> 00:37:22,800
and build all the analytics that they

00:37:21,200 --> 00:37:25,680
need

00:37:22,800 --> 00:37:27,200
if you need to uh or you want to learn

00:37:25,680 --> 00:37:31,040
more about apache druid

00:37:27,200 --> 00:37:34,160
we have two talks uh from jian which is

00:37:31,040 --> 00:37:34,800
the ctu and the co-founder of imply and

00:37:34,160 --> 00:37:37,040
also

00:37:34,800 --> 00:37:38,880
a vice president for apache druid so you

00:37:37,040 --> 00:37:41,280
have all the information here we can

00:37:38,880 --> 00:37:44,480
connect you to the talk and learn more

00:37:41,280 --> 00:37:46,880
thank you very much uh and

00:37:44,480 --> 00:37:47,839
we still have three minutes for question

00:37:46,880 --> 00:37:49,440
so

00:37:47,839 --> 00:37:51,839
um let's see if there is any question

00:37:49,440 --> 00:37:51,839
here

00:37:55,119 --> 00:38:02,720
i will stop sharing my screen

00:37:58,560 --> 00:38:02,720
and go back to the chat

00:38:03,520 --> 00:38:10,560
okay it looks like there is no question

00:38:07,200 --> 00:38:13,440
um so i will stay around

00:38:10,560 --> 00:38:14,720
for two or three minutes uh if you have

00:38:13,440 --> 00:38:19,839
any question

00:38:14,720 --> 00:38:19,839
just send them on the chat

00:38:20,720 --> 00:38:26,079
hi tim good to see you

00:38:23,760 --> 00:38:27,440
thanks victoria i'm glad that you

00:38:26,079 --> 00:38:30,839
enjoyed it

00:38:27,440 --> 00:38:33,839
thanks javier as well dimitri thanks

00:38:30,839 --> 00:38:33,839
everyone

00:39:40,720 --> 00:39:42,800

YouTube URL: https://www.youtube.com/watch?v=n8BNIn9n0bA


