Title: TVM: An End to End Deep Learning Compiler Stack
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	TVM: An End to End Deep Learning Compiler Stack
Tianqi Chen

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache(incubating) TVM is an open deep learning compiler stack for CPUs, GPUs, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends. TVM provides the following main features: - Compilation of deep learning models in Keras, MXNet, PyTorch, Tensorflow, CoreML, DarkNet into minimum deployable modules on diverse hardware backends. - Infrastructure to automatic generate and optimize tensor operators on more backend with better performance. In this talk, I will cover the new developments in TVM in the past year around the areas of more backend, automation and model support.

Tianqi Chen received his PhD. from the Paul G. Allen School of Computer Science & Engineering at the University of Washington, working with Carlos Guestrin on the intersection of machine learning and systems. He has created three major learning systems that are widely adopted: XGBoost, TVM, and MXNet(co-creator). He is a recipient of the Google Ph.D. Fellowship in Machine Learning. He is currently the CTO of OctoML.
YouTube URL: https://www.youtube.com/watch?v=QXp5ebZzLuE


