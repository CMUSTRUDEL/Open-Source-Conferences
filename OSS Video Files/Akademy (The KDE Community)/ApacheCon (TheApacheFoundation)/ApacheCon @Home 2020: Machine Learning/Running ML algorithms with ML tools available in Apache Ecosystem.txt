Title: Running ML algorithms with ML tools available in Apache Ecosystem
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	Running ML algorithms with ML tools available in Apache Ecosystem
Shekhar Prasad Rajak

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

In these days, having libraries to get abstract methods to use machine learning algorithm in the application is important but to train our model effectively in lesser time & resources; for our own customize algorithm is more important. Machine learning technology is changing every single day, so let's spend time on how Researchers and Software Developers can leverage the powerful features provided by Apache libraries & frameworks. In this talk we will focus on Apache libraries/frameworks available for distributed training, large scale & less costly data transfer during the whole Model training life cycle. Fundamentals and motive behind following Apache Projects: * Apache Spark MLlib: Simplifies large scale machine learning pipelines, using distributed memory-based Spark architecture. The best for building & experimenting new algorithms. * Apache MxNet: A lean, flexible, and ultra-scalable deep learning framework that supports state of the art in deep learning models * Apache Singa: It provides intelligent database system, distributed deep learning by partitioning the model and data onto nodes in a cluster and parallelize the training. * Apache Ignite: A distributed database , caching and processing platform designed to store and compute on large volumes of data across a cluster of nodes - which can be super useful to perform distributed training and inference instantly without massive data transmissions * Apache Mahout : A distributed linear algebra framework that support multiple distributed backends like Apache Spark, to use by data scientists to quickly implement algorithms and statistics analysis of data. Practical guide for above Apache projects, focusing following points: * Data processing, implementing existing & customised own ML algorithms, tuning, scaling up and finally deploying to optimising it using Apache cluster management tools and(or) Kubernetes. Performance and benchmark with Kubernetes. * Handling large-scale batch, streaming data & realtime processing. * Caching data or in-memory for faster ML predictions

Shekhar is passionate about Open Source Softwares and active in various Open Source Projects. During college days he has contributed SymPy - Python library for symbolic mathematics , Data Science related Ruby gems like: daru, dart-view(Author), nyaplot - which is under Ruby Science Foundation (SciRuby), Bundler: a gem to bundle gems, NumPy & SciPy for creating the interactive website and documentation website using sphinx and Hugo framework, CloudCV for migrating the Angular JS application to Angular 8, and few others. He has successfully completed Google Summer of Code 2016 & 2017 and mentored students after that on 2018, 2019. Shekhar also talked about daru-view gem in RubyConf India 2018 and PyCon India 2017 on SymPy & SymEngine.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:29,119 --> 00:00:31,840
okay cool

00:00:34,239 --> 00:00:39,600
so hello world thank you everyone for

00:00:37,840 --> 00:00:41,120
joining me in this talk tita running

00:00:39,600 --> 00:00:42,800
machine learning algorithms with

00:00:41,120 --> 00:00:44,160
machine learning tools available in

00:00:42,800 --> 00:00:46,000
apache ecosystem

00:00:44,160 --> 00:00:48,079
so in this talk we will learn more about

00:00:46,000 --> 00:00:49,920
what are the tools available in apache

00:00:48,079 --> 00:00:52,000
ecosystem to run the machine learning

00:00:49,920 --> 00:00:54,640
algorithm with efficient

00:00:52,000 --> 00:00:55,440
distributed manner with lesser data

00:00:54,640 --> 00:00:57,600
transmission

00:00:55,440 --> 00:00:59,840
over the whole machine learning life

00:00:57,600 --> 00:00:59,840
cycle

00:01:03,440 --> 00:01:10,080
so the researchers and data scientists

00:01:07,280 --> 00:01:11,520
are coming with new algorithms new

00:01:10,080 --> 00:01:13,280
machine learning algorithms with

00:01:11,520 --> 00:01:15,680
improvements and enhancement every

00:01:13,280 --> 00:01:18,720
single day so we can easily see

00:01:15,680 --> 00:01:20,880
various research papers coming up and

00:01:18,720 --> 00:01:22,080
the implementation of those new

00:01:20,880 --> 00:01:24,000
algorithms

00:01:22,080 --> 00:01:25,840
are very frequent in various deep

00:01:24,000 --> 00:01:29,680
learning and machine learning and

00:01:25,840 --> 00:01:32,799
libraries so

00:01:29,680 --> 00:01:34,320
this we got the performance better

00:01:32,799 --> 00:01:37,280
performance in our

00:01:34,320 --> 00:01:39,200
prototype version but most of them are

00:01:37,280 --> 00:01:41,600
not in production right now

00:01:39,200 --> 00:01:42,640
so what could be the reason so one of

00:01:41,600 --> 00:01:44,799
the reason could be

00:01:42,640 --> 00:01:45,680
because the whole machine learning life

00:01:44,799 --> 00:01:50,000
cycle

00:01:45,680 --> 00:01:50,000
is not high performance in production

00:01:52,840 --> 00:01:56,399
environment

00:01:54,159 --> 00:01:57,200
so what are the constant parts in each

00:01:56,399 --> 00:02:00,719
every

00:01:57,200 --> 00:02:00,719
ai or ml life cycle

00:02:01,200 --> 00:02:06,479
data is stored in the data source so we

00:02:03,759 --> 00:02:09,679
need to retrieve the data

00:02:06,479 --> 00:02:12,239
transform it and do filtering

00:02:09,679 --> 00:02:14,000
and get rid of all noisy data which is

00:02:12,239 --> 00:02:16,640
not required for prediction or

00:02:14,000 --> 00:02:18,400
any learning framework or any learning

00:02:16,640 --> 00:02:21,760
algorithms

00:02:18,400 --> 00:02:24,480
once we get the filter data we train it

00:02:21,760 --> 00:02:26,080
we test it and we repeat this process

00:02:24,480 --> 00:02:29,120
until we get the convergence

00:02:26,080 --> 00:02:30,720
and like basically the minimum error

00:02:29,120 --> 00:02:33,599
rate

00:02:30,720 --> 00:02:34,239
once model model is ready we put it in

00:02:33,599 --> 00:02:38,640
production

00:02:34,239 --> 00:02:42,000
and open it for uh other

00:02:38,640 --> 00:02:42,000
data outside world data

00:02:42,080 --> 00:02:46,080
so once we it got to production we also

00:02:44,239 --> 00:02:51,680
had to maintain it and

00:02:46,080 --> 00:02:54,480
update the version and monitoring it

00:02:51,680 --> 00:02:55,440
so we will see each lifecycle can how it

00:02:54,480 --> 00:02:58,400
can be

00:02:55,440 --> 00:03:00,080
improved so that we can get the high

00:02:58,400 --> 00:03:02,959
performance and

00:03:00,080 --> 00:03:04,080
better efficiency although there are

00:03:02,959 --> 00:03:06,640
many

00:03:04,080 --> 00:03:08,319
uh projects under a particular system

00:03:06,640 --> 00:03:09,040
for machine learning we will mostly

00:03:08,319 --> 00:03:12,319
focus

00:03:09,040 --> 00:03:16,159
in this talk on apache ignite singa

00:03:12,319 --> 00:03:18,319
spark mxnet and mahal

00:03:16,159 --> 00:03:19,599
we will go each of the project one by

00:03:18,319 --> 00:03:21,519
one and

00:03:19,599 --> 00:03:25,519
check the initial research paper and

00:03:21,519 --> 00:03:27,680
cone concepts how they are implemented

00:03:25,519 --> 00:03:29,599
and then we understand various scenarios

00:03:27,680 --> 00:03:33,120
and use cases together

00:03:29,599 --> 00:03:35,680
also we will see how we can do better

00:03:33,120 --> 00:03:37,920
deployment for all these projects

00:03:35,680 --> 00:03:39,760
frameworks

00:03:37,920 --> 00:03:41,680
about me my name is shekhar prasad

00:03:39,760 --> 00:03:42,959
reject i'm passionate about open source

00:03:41,680 --> 00:03:44,239
projects

00:03:42,959 --> 00:03:47,840
currently i'm working as a software

00:03:44,239 --> 00:03:49,760
engineer at apple

00:03:47,840 --> 00:03:51,280
i pursued my b-tech in computer science

00:03:49,760 --> 00:03:52,560
and engineering from nasa institute of

00:03:51,280 --> 00:03:55,920
technology warangal

00:03:52,560 --> 00:03:57,599
india i have

00:03:55,920 --> 00:03:59,120
also successfully completed google

00:03:57,599 --> 00:04:03,680
summer of code

00:03:59,120 --> 00:04:06,159
project in 2016 under sempiorg

00:04:03,680 --> 00:04:07,200
which is python library for symbolic

00:04:06,159 --> 00:04:11,599
mathematics

00:04:07,200 --> 00:04:11,599
and 2017 under ruby science foundation

00:04:12,319 --> 00:04:16,320
who built the various scientific uh

00:04:15,040 --> 00:04:18,560
scientific computing gym

00:04:16,320 --> 00:04:20,959
for ruby i also mentioned various

00:04:18,560 --> 00:04:24,400
students 2018-19 under various

00:04:20,959 --> 00:04:26,880
organization also i have

00:04:24,400 --> 00:04:29,840
contributed more than 10 organization in

00:04:26,880 --> 00:04:29,840
a different way

00:04:30,240 --> 00:04:34,400
like ruby bundler and empire sci-fi for

00:04:32,880 --> 00:04:37,520
building the

00:04:34,400 --> 00:04:38,960
main website and so on you can easily

00:04:37,520 --> 00:04:40,479
find me in

00:04:38,960 --> 00:04:42,000
with this handler shaker reject or

00:04:40,479 --> 00:04:45,680
seeker hyphen with

00:04:42,000 --> 00:04:45,680
twitter linkedin github

00:04:46,000 --> 00:04:49,520
so let's go back to the top

00:04:50,080 --> 00:04:54,320
so let's see uh history behind each ml

00:04:52,880 --> 00:04:56,400
frameworks

00:04:54,320 --> 00:04:58,080
so we start with apache singha singer or

00:04:56,400 --> 00:05:02,240
distributed deep learning

00:04:58,080 --> 00:05:05,520
platform so singapore mostly focus on

00:05:02,240 --> 00:05:06,960
distributed computing so how we can

00:05:05,520 --> 00:05:08,639
partition our data and

00:05:06,960 --> 00:05:10,320
run the training script in each of the

00:05:08,639 --> 00:05:13,360
machine parallelly so that

00:05:10,320 --> 00:05:17,120
the performance of overall training

00:05:13,360 --> 00:05:17,120
stage could be improved

00:05:18,639 --> 00:05:22,639
so what are the challenges in machine

00:05:21,600 --> 00:05:25,199
learning

00:05:22,639 --> 00:05:28,320
if we do distributed computing for uh

00:05:25,199 --> 00:05:28,320
deep learning framework

00:05:28,960 --> 00:05:33,840
so first challenge could be um so we

00:05:32,160 --> 00:05:37,039
have set up machines

00:05:33,840 --> 00:05:40,320
running running the training stage

00:05:37,039 --> 00:05:40,720
in paralleling so in each iteration we

00:05:40,320 --> 00:05:44,080
also

00:05:40,720 --> 00:05:47,440
need to update the model

00:05:44,080 --> 00:05:52,000
model parameters like weight or

00:05:47,440 --> 00:05:53,759
bias in each iteration so

00:05:52,000 --> 00:05:56,000
so that means we have to sync up the

00:05:53,759 --> 00:05:58,479
model and make it up to date in every

00:05:56,000 --> 00:06:00,160
iteration that means there is a huge

00:05:58,479 --> 00:06:03,520
communication over the

00:06:00,160 --> 00:06:04,400
system or set of machines that is

00:06:03,520 --> 00:06:07,360
running

00:06:04,400 --> 00:06:09,039
also we don't expect data scientists or

00:06:07,360 --> 00:06:12,319
machine learning to

00:06:09,039 --> 00:06:14,960
understand a distribution computation or

00:06:12,319 --> 00:06:16,319
set up the whole distributed system in

00:06:14,960 --> 00:06:19,600
their uh

00:06:16,319 --> 00:06:20,800
in their machine for various experiment

00:06:19,600 --> 00:06:23,520
and research

00:06:20,800 --> 00:06:25,120
so purchasing provides simple api to

00:06:23,520 --> 00:06:27,520
solve these issues

00:06:25,120 --> 00:06:28,160
so let's say in this example we have

00:06:27,520 --> 00:06:31,520
four

00:06:28,160 --> 00:06:35,680
gpu machines running parallely

00:06:31,520 --> 00:06:38,160
training stage ranking them 0 1 2

00:06:35,680 --> 00:06:38,160
and 3.

00:06:40,720 --> 00:06:45,440
so in each in each iteration

00:06:43,759 --> 00:06:48,080
after each iteration we get the loss

00:06:45,440 --> 00:06:52,720
function on gradient descent abcd

00:06:48,080 --> 00:06:56,400
and once we get them we

00:06:52,720 --> 00:06:59,360
do all reduce operation using nccr also

00:06:56,400 --> 00:07:01,039
known and can be pronounced as nickel

00:06:59,360 --> 00:07:04,240
which is nvidia

00:07:01,039 --> 00:07:06,720
collective communication their library

00:07:04,240 --> 00:07:07,520
to do various operations like all reduce

00:07:06,720 --> 00:07:12,000
broadcast

00:07:07,520 --> 00:07:15,199
or gather and so on so here

00:07:12,000 --> 00:07:16,319
all reduce operation will take all the

00:07:15,199 --> 00:07:19,680
results a b c

00:07:16,319 --> 00:07:22,000
d and uh get the average and

00:07:19,680 --> 00:07:24,080
distribute them uh brought and

00:07:22,000 --> 00:07:26,560
distribute them all to

00:07:24,080 --> 00:07:29,039
each of the machines to make the model

00:07:26,560 --> 00:07:32,800
up to date in each iteration

00:07:29,039 --> 00:07:35,840
so this is the very fast way to optimize

00:07:32,800 --> 00:07:37,120
optimized communication over the gpu

00:07:35,840 --> 00:07:40,000
system

00:07:37,120 --> 00:07:40,000
set up machines

00:07:40,800 --> 00:07:44,720
so apache single supports various deep

00:07:43,199 --> 00:07:47,680
learning feed forwarding

00:07:44,720 --> 00:07:49,520
models deep learning models like

00:07:47,680 --> 00:07:52,639
convolutional neural network

00:07:49,520 --> 00:07:55,039
energy models like restricted boltzmann

00:07:52,639 --> 00:07:55,039
machine

00:07:55,120 --> 00:08:00,160
rnn and national program language

00:07:57,680 --> 00:08:00,160
processing

00:08:00,240 --> 00:08:05,840
so main concept for each

00:08:03,680 --> 00:08:07,280
deep learning library or framework is

00:08:05,840 --> 00:08:10,639
computation graph

00:08:07,280 --> 00:08:13,759
so we have

00:08:10,639 --> 00:08:15,520
forward pass and backward pass so in

00:08:13,759 --> 00:08:18,479
each neural network we have

00:08:15,520 --> 00:08:19,199
input layer we have set up hidden layers

00:08:18,479 --> 00:08:22,479
and then

00:08:19,199 --> 00:08:25,599
output layers so we can think them

00:08:22,479 --> 00:08:26,960
as a graph basically so in that graph we

00:08:25,599 --> 00:08:30,080
have

00:08:26,960 --> 00:08:31,440
leaf node as an input layer all the

00:08:30,080 --> 00:08:33,680
nodes in input layer

00:08:31,440 --> 00:08:35,039
and there is one node in our output

00:08:33,680 --> 00:08:37,360
layer which is the

00:08:35,039 --> 00:08:40,560
our prediction that means there is only

00:08:37,360 --> 00:08:43,120
one node that is a root node

00:08:40,560 --> 00:08:44,880
so when once we go from input layer to

00:08:43,120 --> 00:08:48,560
output

00:08:44,880 --> 00:08:51,200
layer we

00:08:48,560 --> 00:08:52,640
go forward and calculate the loss

00:08:51,200 --> 00:08:55,839
function in each layer

00:08:52,640 --> 00:08:58,959
and the final launch function

00:08:55,839 --> 00:09:01,600
we calculate we understand a prediction

00:08:58,959 --> 00:09:04,880
accuracy

00:09:01,600 --> 00:09:07,040
since if the error is not minimum then

00:09:04,880 --> 00:09:09,920
we update the

00:09:07,040 --> 00:09:10,800
hyper parameters in our model in each

00:09:09,920 --> 00:09:13,519
layer

00:09:10,800 --> 00:09:15,120
uh the parameters could be number of

00:09:13,519 --> 00:09:18,959
node or

00:09:15,120 --> 00:09:22,000
weight or bias so for that we need to

00:09:18,959 --> 00:09:24,720
do back propagate and update the

00:09:22,000 --> 00:09:26,080
parameters in each layer so that means

00:09:24,720 --> 00:09:28,240
we need to calculate the gradient

00:09:26,080 --> 00:09:30,000
descent

00:09:28,240 --> 00:09:31,519
so forward pass is pretty

00:09:30,000 --> 00:09:32,399
straightforward we can calculate the

00:09:31,519 --> 00:09:33,680
loss function but

00:09:32,399 --> 00:09:36,800
for gradient descent we have to

00:09:33,680 --> 00:09:38,720
calculate the uh

00:09:36,800 --> 00:09:39,920
we have to do the some calculus

00:09:38,720 --> 00:09:42,399
operations like

00:09:39,920 --> 00:09:43,839
differentiation and that needs chain

00:09:42,399 --> 00:09:46,959
rule to be applied on

00:09:43,839 --> 00:09:50,399
each of the layers so if we uh track

00:09:46,959 --> 00:09:52,399
whole fruit we can and

00:09:50,399 --> 00:09:56,480
calculate the uh differentiation

00:09:52,399 --> 00:09:56,480
automatically after the forward pass so

00:09:57,279 --> 00:10:01,279
so that's why computer computation graph

00:09:59,839 --> 00:10:04,720
is very necessary and

00:10:01,279 --> 00:10:07,360
each deep learning framework

00:10:04,720 --> 00:10:08,399
it has optimized implementation of

00:10:07,360 --> 00:10:10,880
tensors

00:10:08,399 --> 00:10:12,720
and auto grade is all about like after

00:10:10,880 --> 00:10:14,320
forward pass automatically calculate the

00:10:12,720 --> 00:10:16,399
differentiation and get the gradient

00:10:14,320 --> 00:10:18,959
listen

00:10:16,399 --> 00:10:19,519
supported supports frameworks apache

00:10:18,959 --> 00:10:21,760
singa

00:10:19,519 --> 00:10:23,760
supports both synchronous and

00:10:21,760 --> 00:10:27,279
asynchronous training

00:10:23,760 --> 00:10:29,839
model uh framework so when we

00:10:27,279 --> 00:10:31,839
if we use synchronous training framework

00:10:29,839 --> 00:10:33,920
that means

00:10:31,839 --> 00:10:36,160
in each iteration the workload is

00:10:33,920 --> 00:10:39,519
distributed across the system

00:10:36,160 --> 00:10:42,880
so so this give good

00:10:39,519 --> 00:10:43,519
efficiency but when there is a large

00:10:42,880 --> 00:10:47,440
cluster

00:10:43,519 --> 00:10:50,959
then this this is not improved

00:10:47,440 --> 00:10:54,560
or not give a better performance so

00:10:50,959 --> 00:10:57,600
when but when we use the asynchronous

00:10:54,560 --> 00:11:00,000
training framework then we get better

00:10:57,600 --> 00:11:04,480
convergence rate

00:11:00,000 --> 00:11:04,480
but also that converges rate is not

00:11:05,040 --> 00:11:10,480
not improved when models uh number of

00:11:07,920 --> 00:11:13,680
replicas of the model is

00:11:10,480 --> 00:11:15,920
increasing so apache single

00:11:13,680 --> 00:11:17,279
comes with hybrid training framework

00:11:15,920 --> 00:11:19,440
also support hybrid training

00:11:17,279 --> 00:11:22,079
framework to balance out the efficiency

00:11:19,440 --> 00:11:25,600
and conventions rate

00:11:22,079 --> 00:11:27,200
communicator is uh implemented optimized

00:11:25,600 --> 00:11:31,040
way so that each

00:11:27,200 --> 00:11:34,399
set of machines can communicate very

00:11:31,040 --> 00:11:37,040
optimized way when nx format

00:11:34,399 --> 00:11:38,320
is supported by apache singer so if we

00:11:37,040 --> 00:11:41,760
have trained model

00:11:38,320 --> 00:11:44,720
through apache singer we can use the

00:11:41,760 --> 00:11:47,120
model we can load the mod uh we can save

00:11:44,720 --> 00:11:50,000
the model in on nx format and

00:11:47,120 --> 00:11:50,320
other deep learning framework can use it

00:11:50,000 --> 00:11:53,760
for

00:11:50,320 --> 00:11:53,760
inference and vice versa

00:11:55,360 --> 00:12:00,480
so let's go ahead with apache mxnet this

00:11:58,480 --> 00:12:03,120
is the initial research paper i found

00:12:00,480 --> 00:12:04,959
a flexible efficient machine learning

00:12:03,120 --> 00:12:07,360
library for heterogeneous distributed

00:12:04,959 --> 00:12:07,360
systems

00:12:07,600 --> 00:12:12,000
so main words are here flexible

00:12:09,920 --> 00:12:14,800
efficient

00:12:12,000 --> 00:12:17,200
heterogeneous distributed so we'll go

00:12:14,800 --> 00:12:20,639
one by one

00:12:17,200 --> 00:12:21,839
so apache mxnet provides flexibility in

00:12:20,639 --> 00:12:25,040
programming languages

00:12:21,839 --> 00:12:26,639
to in front end like we can code in c

00:12:25,040 --> 00:12:29,440
plus with java python r

00:12:26,639 --> 00:12:31,360
julia scala or js and in the backend

00:12:29,440 --> 00:12:35,200
side it will run in optimize c

00:12:31,360 --> 00:12:37,680
plus memory efficient

00:12:35,200 --> 00:12:39,839
it also it supports the communication

00:12:37,680 --> 00:12:39,839
graph

00:12:39,920 --> 00:12:44,000
the same concept at purchasing the

00:12:42,560 --> 00:12:47,360
programming interface

00:12:44,000 --> 00:12:49,680
both uh declarative and creative way

00:12:47,360 --> 00:12:51,839
that makes is more memory efficient

00:12:49,680 --> 00:12:55,760
because

00:12:51,839 --> 00:12:59,120
you want to do any imperative

00:12:55,760 --> 00:13:02,800
program like this numpy code

00:12:59,120 --> 00:13:02,800
we calculate c equals to b

00:13:03,440 --> 00:13:10,480
multiply by a so

00:13:06,560 --> 00:13:14,560
this calculation is done uh

00:13:10,480 --> 00:13:17,120
early so it just calculate it and

00:13:14,560 --> 00:13:17,760
give the same data by data type and

00:13:17,120 --> 00:13:20,800
result

00:13:17,760 --> 00:13:24,000
uh to see

00:13:20,800 --> 00:13:28,000
and then c is incremented by one and

00:13:24,000 --> 00:13:28,000
it passes the value to d

00:13:28,880 --> 00:13:32,880
but in declarative way we first have to

00:13:31,680 --> 00:13:37,760
define the variable

00:13:32,880 --> 00:13:40,880
and bind the value to that variable

00:13:37,760 --> 00:13:45,839
and it won't calculate the

00:13:40,880 --> 00:13:48,959
result but first it will calculate

00:13:45,839 --> 00:13:50,800
get the computation graph for it so as

00:13:48,959 --> 00:13:55,440
we can see

00:13:50,800 --> 00:13:58,800
in this image and once it is compiled

00:13:55,440 --> 00:14:02,079
we can use the function for

00:13:58,800 --> 00:14:05,279
various a or b values so

00:14:02,079 --> 00:14:05,279
it also uh

00:14:05,680 --> 00:14:12,079
check the reference counter so here d

00:14:08,880 --> 00:14:15,279
c is d is depend on c so

00:14:12,079 --> 00:14:16,800
c reference counter is 1 so if the

00:14:15,279 --> 00:14:19,440
reference counter is zero that means

00:14:16,800 --> 00:14:22,959
that variable life cycle is

00:14:19,440 --> 00:14:27,600
uh completed also apache

00:14:22,959 --> 00:14:30,720
in mxnet uh various neural network

00:14:27,600 --> 00:14:32,959
uh neural network or models

00:14:30,720 --> 00:14:35,760
can be used in various systems from

00:14:32,959 --> 00:14:39,600
mobile device to any cpu gpu or

00:14:35,760 --> 00:14:39,600
cloud cloud providers

00:14:42,320 --> 00:14:46,480
and predicting models apache maps net

00:14:45,760 --> 00:14:49,360
comes with

00:14:46,480 --> 00:14:50,079
uh very rich ecosystem we have gluon

00:14:49,360 --> 00:14:53,839
interface

00:14:50,079 --> 00:14:54,639
for uh if uh so in gluon interface we

00:14:53,839 --> 00:14:56,880
see

00:14:54,639 --> 00:14:58,560
various implementation of state

00:14:56,880 --> 00:15:01,680
state-of-the-art deep learning

00:14:58,560 --> 00:15:02,399
in computer vision natural language

00:15:01,680 --> 00:15:05,120
processing

00:15:02,399 --> 00:15:06,320
and time series probabilistic time

00:15:05,120 --> 00:15:09,360
series modeling

00:15:06,320 --> 00:15:12,560
so if we want any type of image

00:15:09,360 --> 00:15:15,279
classification or

00:15:12,560 --> 00:15:16,959
pose estimation or action recognition in

00:15:15,279 --> 00:15:19,440
the image we can

00:15:16,959 --> 00:15:20,959
do any of our researches and students

00:15:19,440 --> 00:15:24,000
can easily

00:15:20,959 --> 00:15:27,440
use the igbo on cv api

00:15:24,000 --> 00:15:28,320
it also have model 2 which have 170 plus

00:15:27,440 --> 00:15:31,120
high

00:15:28,320 --> 00:15:31,519
uh quality pretend model that can be

00:15:31,120 --> 00:15:35,040
used

00:15:31,519 --> 00:15:35,040
in the production directly

00:15:36,000 --> 00:15:42,880
mx4 and tensor rt are just to

00:15:39,120 --> 00:15:44,639
see how modern mxnet is so on top of it

00:15:42,880 --> 00:15:48,160
we can easily build any app to

00:15:44,639 --> 00:15:52,160
any library so if we

00:15:48,160 --> 00:15:55,120
log our execution part in

00:15:52,160 --> 00:15:56,240
folder and use the tensorboard to

00:15:55,120 --> 00:15:59,120
visualize it

00:15:56,240 --> 00:15:59,759
we can do it using the mx code and check

00:15:59,120 --> 00:16:01,120
all the

00:15:59,759 --> 00:16:03,920
how loss function you're getting

00:16:01,120 --> 00:16:06,959
decrease and how all these stages

00:16:03,920 --> 00:16:08,079
are completed and the communication gap

00:16:06,959 --> 00:16:11,839
and so on

00:16:08,079 --> 00:16:16,079
tensor that is nvidia library for

00:16:11,839 --> 00:16:19,279
uh optimize inference step

00:16:16,079 --> 00:16:19,839
so once train a model is trained uh we

00:16:19,279 --> 00:16:22,240
put it

00:16:19,839 --> 00:16:23,199
in production that is called inference

00:16:22,240 --> 00:16:26,480
and

00:16:23,199 --> 00:16:29,680
if we want optimized inference

00:16:26,480 --> 00:16:31,759
step or process we have to

00:16:29,680 --> 00:16:33,440
optimize our computation graph of the

00:16:31,759 --> 00:16:37,120
model so

00:16:33,440 --> 00:16:41,920
tensor rt first load the

00:16:37,120 --> 00:16:44,639
model pass the whole computation graph

00:16:41,920 --> 00:16:46,480
it get the all sub graphs that can be

00:16:44,639 --> 00:16:49,440
optimized by tensor rt

00:16:46,480 --> 00:16:50,480
if it can be optimized by tensority then

00:16:49,440 --> 00:16:54,079
it replace with

00:16:50,480 --> 00:16:56,959
its tension at node so basically

00:16:54,079 --> 00:16:57,920
if there is any set of layers that can

00:16:56,959 --> 00:17:01,360
be combined

00:16:57,920 --> 00:17:04,720
and uh can be calculated uh through

00:17:01,360 --> 00:17:08,720
single cuda call then

00:17:04,720 --> 00:17:08,720
then it is a optimized way and

00:17:12,160 --> 00:17:17,360
yeah so in this way we can optimize the

00:17:15,199 --> 00:17:22,079
inference step

00:17:17,360 --> 00:17:22,079
mxnet support various hardwares as well

00:17:23,360 --> 00:17:28,799
so by now we have seen uh

00:17:26,559 --> 00:17:29,600
why distributed competition is so

00:17:28,799 --> 00:17:32,720
helpful in

00:17:29,600 --> 00:17:35,039
machine learning we will understand the

00:17:32,720 --> 00:17:38,960
other step as well

00:17:35,039 --> 00:17:41,039
like data processing apache mohawk

00:17:38,960 --> 00:17:42,960
so apache mohawk initially was

00:17:41,039 --> 00:17:46,320
approaching apache lucene

00:17:42,960 --> 00:17:49,440
and then it become

00:17:46,320 --> 00:17:52,000
a good machine learning tool which works

00:17:49,440 --> 00:17:54,000
on top of hadoop map reduce

00:17:52,000 --> 00:17:56,320
for distributed scalable machine

00:17:54,000 --> 00:17:56,320
learning

00:17:57,840 --> 00:18:04,000
later on it became independent and

00:18:01,120 --> 00:18:04,000
only provide the

00:18:04,240 --> 00:18:11,440
distributed linear algebra operations

00:18:08,000 --> 00:18:14,400
and now it became back in agnostic that

00:18:11,440 --> 00:18:18,720
means it can run in a purchase spark

00:18:14,400 --> 00:18:18,720
or s2 or a budget thing

00:18:19,200 --> 00:18:25,600
so the here i am focusing on the

00:18:22,559 --> 00:18:30,799
distributed linear algebra uh

00:18:25,600 --> 00:18:34,240
operations using apache so it gives the

00:18:30,799 --> 00:18:39,039
very r like semantics

00:18:34,240 --> 00:18:41,440
for doing any matrix operations

00:18:39,039 --> 00:18:42,880
in a distributed way so r works in a

00:18:41,440 --> 00:18:46,400
single node now

00:18:42,880 --> 00:18:49,840
apache mode can do with the same syntax

00:18:46,400 --> 00:18:52,960
in a distributed manner with

00:18:49,840 --> 00:18:55,679
samsara so qr

00:18:52,960 --> 00:18:57,600
svd or pcr decomposition here is the

00:18:55,679 --> 00:19:00,160
summing some of the examples

00:18:57,600 --> 00:19:02,080
drm is distributed row matrix that means

00:19:00,160 --> 00:19:05,360
if there is a very large

00:19:02,080 --> 00:19:07,600
matrix we have that could be uh

00:19:05,360 --> 00:19:10,640
distributed over the setup system and

00:19:07,600 --> 00:19:14,240
operations will be in a distributed way

00:19:10,640 --> 00:19:16,000
in a optimized way so optimization is

00:19:14,240 --> 00:19:17,520
done under the hood like if you want to

00:19:16,000 --> 00:19:20,960
do it

00:19:17,520 --> 00:19:24,160
a a transpose matrix multiplier

00:19:20,960 --> 00:19:26,559
multiplication a then it will do some

00:19:24,160 --> 00:19:29,760
optimized algorithm like

00:19:26,559 --> 00:19:31,200
transport time self and we can visualize

00:19:29,760 --> 00:19:35,039
various things

00:19:31,200 --> 00:19:35,039
graph in using japanese

00:19:36,240 --> 00:19:40,240
so on top of these linear algebra

00:19:39,360 --> 00:19:43,280
operations

00:19:40,240 --> 00:19:46,320
it have you know some

00:19:43,280 --> 00:19:46,880
methods implemented for data processing

00:19:46,320 --> 00:19:50,320
like

00:19:46,880 --> 00:19:52,640
one hot encoding or getting the mean

00:19:50,320 --> 00:19:55,760
point of the data

00:19:52,640 --> 00:19:56,640
one of the popular one is the community

00:19:55,760 --> 00:19:59,440
system because

00:19:56,640 --> 00:20:00,080
in recommendation system we mostly uh

00:19:59,440 --> 00:20:03,760
work on

00:20:00,080 --> 00:20:03,760
various data metrics

00:20:04,640 --> 00:20:11,120
so mod also have command line

00:20:08,240 --> 00:20:11,919
for spark item similarity that will that

00:20:11,120 --> 00:20:15,360
means it will

00:20:11,919 --> 00:20:17,840
run on spark in the back inside

00:20:15,360 --> 00:20:20,159
so using this command we can get the

00:20:17,840 --> 00:20:23,760
indicator matrix log likelihood

00:20:20,159 --> 00:20:26,880
matrix and

00:20:23,760 --> 00:20:29,520
row similarity if we want to do

00:20:26,880 --> 00:20:29,520
recommendation

00:20:30,159 --> 00:20:35,760
content wise for a user

00:20:34,000 --> 00:20:38,640
so let's go ahead another data

00:20:35,760 --> 00:20:41,200
processing to spark

00:20:38,640 --> 00:20:42,240
we all know spark is one of the more

00:20:41,200 --> 00:20:46,840
most

00:20:42,240 --> 00:20:49,360
popular open source project for any data

00:20:46,840 --> 00:20:53,600
processing

00:20:49,360 --> 00:20:54,960
so it it it uh get the data from a data

00:20:53,600 --> 00:20:57,280
store and put it

00:20:54,960 --> 00:20:59,840
and try to catch them as much as

00:20:57,280 --> 00:21:03,600
possible so that in attritive jobs

00:20:59,840 --> 00:21:06,240
if we get chance to revisit the same

00:21:03,600 --> 00:21:09,039
data we don't go to db and

00:21:06,240 --> 00:21:09,440
do read or write operation we directly

00:21:09,039 --> 00:21:12,960
can

00:21:09,440 --> 00:21:16,320
do in immorality speed here

00:21:12,960 --> 00:21:18,320
on top of spark we have various packages

00:21:16,320 --> 00:21:19,360
for data processing like spark sql

00:21:18,320 --> 00:21:22,159
graphics

00:21:19,360 --> 00:21:24,000
streaming and it also can build powerful

00:21:22,159 --> 00:21:26,159
cluster management system that can work

00:21:24,000 --> 00:21:28,640
in any cluster managers like kubernetes

00:21:26,159 --> 00:21:31,600
yarn mesos

00:21:28,640 --> 00:21:32,559
so this makes is very popular for

00:21:31,600 --> 00:21:35,760
machine learning

00:21:32,559 --> 00:21:39,120
as well let's see what how

00:21:35,760 --> 00:21:42,559
cluster manager works in a spark

00:21:39,120 --> 00:21:46,799
stand spark application

00:21:42,559 --> 00:21:46,799
so we have once client

00:21:46,960 --> 00:21:52,320
submit the spark application it goes to

00:21:48,960 --> 00:21:54,400
spark master spark master talks to

00:21:52,320 --> 00:21:55,840
spark worker node to run the spark

00:21:54,400 --> 00:21:59,280
driver for this application

00:21:55,840 --> 00:22:00,960
and schedule the task that

00:21:59,280 --> 00:22:02,480
application want to run once drivers

00:22:00,960 --> 00:22:04,960
schedule the uh

00:22:02,480 --> 00:22:06,880
once sparked scheduler schedule the all

00:22:04,960 --> 00:22:08,960
the metadata about the task it wants to

00:22:06,880 --> 00:22:12,400
run and the memory or

00:22:08,960 --> 00:22:14,559
cpus wants to use to master master

00:22:12,400 --> 00:22:16,240
talks to worker note to start the

00:22:14,559 --> 00:22:17,120
executor for running those tasks

00:22:16,240 --> 00:22:19,120
parallely

00:22:17,120 --> 00:22:20,240
and once they are started there is

00:22:19,120 --> 00:22:22,480
directly communicate to

00:22:20,240 --> 00:22:25,440
scheduler and give the update about the

00:22:22,480 --> 00:22:25,440
task that is running

00:22:25,679 --> 00:22:30,000
so in this way uh it have very powerful

00:22:28,559 --> 00:22:32,559
execution system

00:22:30,000 --> 00:22:34,240
uh using directed acyclic graph and gas

00:22:32,559 --> 00:22:37,760
strategy

00:22:34,240 --> 00:22:40,240
various uh way of storing uh

00:22:37,760 --> 00:22:41,440
using the data rdd resilient data

00:22:40,240 --> 00:22:44,640
distributed data set

00:22:41,440 --> 00:22:47,039
for any kind of failure we

00:22:44,640 --> 00:22:49,919
there is no loss of data and various api

00:22:47,039 --> 00:22:52,240
on top of it data frame data set

00:22:49,919 --> 00:22:54,159
it is faster than for the faster than

00:22:52,240 --> 00:22:58,480
hadoop map reduce

00:22:54,159 --> 00:22:58,480
and we all know how spark

00:22:59,360 --> 00:23:06,000
hadoop and spark have sorting challenge

00:23:02,640 --> 00:23:10,720
hadoop did the shorting for

00:23:06,000 --> 00:23:13,360
petabyte of data in 22 minutes with 10x

00:23:10,720 --> 00:23:16,559
more resources inspected the same thing

00:23:13,360 --> 00:23:16,559
in 23 minutes

00:23:17,039 --> 00:23:23,360
so spark 3.0 also come with opt

00:23:20,159 --> 00:23:27,120
with good new features like

00:23:23,360 --> 00:23:29,360
adaptive query execution so we all know

00:23:27,120 --> 00:23:30,960
uh shuffle partitioning configuring

00:23:29,360 --> 00:23:33,200
shuffle partitioning

00:23:30,960 --> 00:23:34,159
is an art and like we have to understand

00:23:33,200 --> 00:23:37,520
the data

00:23:34,159 --> 00:23:38,480
and the stages that we want to run in

00:23:37,520 --> 00:23:41,840
runtime

00:23:38,480 --> 00:23:45,440
so now spark can go through

00:23:41,840 --> 00:23:48,559
these statistics for each stages

00:23:45,440 --> 00:23:50,480
and understand the data set and select

00:23:48,559 --> 00:23:53,520
the optimized query plan

00:23:50,480 --> 00:23:56,240
for the next iteration

00:23:53,520 --> 00:23:57,679
so also have various implementation on

00:23:56,240 --> 00:23:59,840
machine learning algorithms

00:23:57,679 --> 00:24:03,039
and machine learning pipeline

00:23:59,840 --> 00:24:06,400
transformer and estimator

00:24:03,039 --> 00:24:08,559
so we go step by and we update

00:24:06,400 --> 00:24:10,720
each stages in our pipeline and can run

00:24:08,559 --> 00:24:10,720
them

00:24:11,760 --> 00:24:16,799
okay i will go through this later on

00:24:14,000 --> 00:24:18,400
apache ignite

00:24:16,799 --> 00:24:19,919
which is horizontally scalable

00:24:18,400 --> 00:24:22,559
distributed in normally computing

00:24:19,919 --> 00:24:22,559
platform

00:24:24,320 --> 00:24:31,760
that means it it wants to do lesser and

00:24:28,640 --> 00:24:33,440
it wants to do basically zero etl very

00:24:31,760 --> 00:24:35,440
avoid the data transmission over the

00:24:33,440 --> 00:24:39,600
network over the system

00:24:35,440 --> 00:24:43,679
so it provides the data grid

00:24:39,600 --> 00:24:46,960
uh for uh it have data grid implemented

00:24:43,679 --> 00:24:49,200
under the hood so

00:24:46,960 --> 00:24:50,799
when we see data grid that means it have

00:24:49,200 --> 00:24:54,880
very high

00:24:50,799 --> 00:24:57,840
performance cache here

00:24:54,880 --> 00:24:58,480
so if we go to history of this cache we

00:24:57,840 --> 00:25:02,720
know about

00:24:58,480 --> 00:25:07,360
lru so we only stored the high

00:25:02,720 --> 00:25:10,080
most frequent used uh data

00:25:07,360 --> 00:25:11,360
to avoid single point of failure we can

00:25:10,080 --> 00:25:14,559
replicate the same

00:25:11,360 --> 00:25:16,720
lrd uk but but

00:25:14,559 --> 00:25:19,039
doing the replication is not a solution

00:25:16,720 --> 00:25:22,240
so we go through the next plan

00:25:19,039 --> 00:25:24,240
next optimize way data partition so each

00:25:22,240 --> 00:25:26,960
node have one primary data and this

00:25:24,240 --> 00:25:26,960
backup data

00:25:27,279 --> 00:25:30,559
and now the more optimized way is using

00:25:29,440 --> 00:25:33,600
affinity key

00:25:30,559 --> 00:25:36,080
and using the data grid

00:25:33,600 --> 00:25:38,720
so but if first distance is off it it

00:25:36,080 --> 00:25:38,720
works as a

00:25:38,960 --> 00:25:43,520
in memory data grid if persistence on it

00:25:42,080 --> 00:25:46,080
works as an in-memory

00:25:43,520 --> 00:25:46,080
database

00:25:47,600 --> 00:25:52,880
so third two main concept in apache

00:25:51,120 --> 00:25:55,520
ignite is compute grid

00:25:52,880 --> 00:25:57,120
and the service grid so in compute grid

00:25:55,520 --> 00:26:00,240
we understand it each

00:25:57,120 --> 00:26:04,080
in uh doing the computation uh

00:26:00,240 --> 00:26:07,360
independent uh task can be divided

00:26:04,080 --> 00:26:11,360
into set uh each ignite uh

00:26:07,360 --> 00:26:13,200
cluster nodes so here c1 c2 c3

00:26:11,360 --> 00:26:14,880
we are doing the computation and getting

00:26:13,200 --> 00:26:17,520
the result r1 r2 at three

00:26:14,880 --> 00:26:19,279
so it works like a map reduce fashion so

00:26:17,520 --> 00:26:21,440
like if you want to calculate the

00:26:19,279 --> 00:26:21,440
uh

00:26:23,200 --> 00:26:31,120
count the prime numbers in our uh

00:26:27,360 --> 00:26:34,159
list of uh in integer list then we can

00:26:31,120 --> 00:26:35,520
divide our data in into our ignite

00:26:34,159 --> 00:26:39,840
cluster

00:26:35,520 --> 00:26:39,840
and get the prime number count

00:26:40,000 --> 00:26:44,720
and once we get the result each for each

00:26:42,720 --> 00:26:47,120
node

00:26:44,720 --> 00:26:47,840
we sum sum them up and get the final

00:26:47,120 --> 00:26:51,039
result

00:26:47,840 --> 00:26:53,440
so if that works in a parallel way

00:26:51,039 --> 00:26:54,480
in service grid we have user defined

00:26:53,440 --> 00:26:56,799
services so

00:26:54,480 --> 00:26:57,840
it can so we can deploy the service in

00:26:56,799 --> 00:27:00,799
the two ways you know

00:26:57,840 --> 00:27:01,440
node singleton or singleton no singleton

00:27:00,799 --> 00:27:05,039
as

00:27:01,440 --> 00:27:06,000
saying uh each the service is deployed

00:27:05,039 --> 00:27:09,679
on each of the node

00:27:06,000 --> 00:27:12,799
and cluster mode that means um

00:27:09,679 --> 00:27:16,240
in one cluster there will be one uh

00:27:12,799 --> 00:27:20,080
services deploy in one of the node

00:27:16,240 --> 00:27:23,360
so so it also have if

00:27:20,080 --> 00:27:25,039
one of the node goes down

00:27:23,360 --> 00:27:27,279
we don't have to restart the whole

00:27:25,039 --> 00:27:30,399
cluster we just have to

00:27:27,279 --> 00:27:34,000
it started the new node

00:27:30,399 --> 00:27:35,919
and works smoothly so we can have

00:27:34,000 --> 00:27:38,559
deep learning framework or machine

00:27:35,919 --> 00:27:42,960
learning algorithm as a service here and

00:27:38,559 --> 00:27:42,960
running in each of the nodes parallelly

00:27:44,640 --> 00:27:48,080
so let's see one scenario here we have

00:27:47,679 --> 00:27:51,840
to

00:27:48,080 --> 00:27:54,880
ignite cluster node and each have two

00:27:51,840 --> 00:27:56,080
tables like one is country and one is

00:27:54,880 --> 00:27:59,360
city

00:27:56,080 --> 00:28:02,480
so one of the node is storing

00:27:59,360 --> 00:28:05,919
the uh

00:28:02,480 --> 00:28:08,080
data uh with where country

00:28:05,919 --> 00:28:09,039
code is india and other node is storing

00:28:08,080 --> 00:28:12,159
data

00:28:09,039 --> 00:28:14,559
where country was it us

00:28:12,159 --> 00:28:16,240
so if one if we have new record to be

00:28:14,559 --> 00:28:19,679
inserted in city table

00:28:16,240 --> 00:28:20,240
and where country code is india for that

00:28:19,679 --> 00:28:23,840
city

00:28:20,240 --> 00:28:27,919
that should go into our node one

00:28:23,840 --> 00:28:31,039
here and if there is any city

00:28:27,919 --> 00:28:33,679
which has country code us then

00:28:31,039 --> 00:28:35,120
it should go into node 2 so that we can

00:28:33,679 --> 00:28:38,000
do the

00:28:35,120 --> 00:28:40,320
any kind of group by or join operation

00:28:38,000 --> 00:28:43,919
inside the node itself

00:28:40,320 --> 00:28:44,399
so that will avoid the that will reduce

00:28:43,919 --> 00:28:46,640
the

00:28:44,399 --> 00:28:49,360
network latency so we can use the

00:28:46,640 --> 00:28:52,080
affinity key here

00:28:49,360 --> 00:28:53,200
as a country code for our city table so

00:28:52,080 --> 00:28:56,480
it make sure that

00:28:53,200 --> 00:28:59,919
uh the city related to the country

00:28:56,480 --> 00:29:03,840
goes to the uh respect

00:28:59,919 --> 00:29:03,840
and the same uh node

00:29:04,240 --> 00:29:08,640
in this way we try to avoid the data

00:29:06,799 --> 00:29:11,440
transmission over the network and

00:29:08,640 --> 00:29:12,480
any group banks on join operations kind

00:29:11,440 --> 00:29:16,159
of operations

00:29:12,480 --> 00:29:16,159
are now faster

00:29:17,840 --> 00:29:20,880
so we see partisan bit data set affinity

00:29:20,399 --> 00:29:24,399
key

00:29:20,880 --> 00:29:26,880
and there are different ways of policy

00:29:24,399 --> 00:29:29,760
if there is failover so it makes sure

00:29:26,880 --> 00:29:32,960
there is no failure and

00:29:29,760 --> 00:29:33,760
the overall time will will be same if as

00:29:32,960 --> 00:29:36,799
there is no

00:29:33,760 --> 00:29:36,799
uh node firewall

00:29:36,880 --> 00:29:40,000
it also have various load balancing

00:29:38,960 --> 00:29:43,279
policy so make sure

00:29:40,000 --> 00:29:45,600
that there is a load balance across the

00:29:43,279 --> 00:29:45,600
node

00:29:47,760 --> 00:29:52,480
so till now we have seen various

00:29:50,799 --> 00:29:56,080
distributed

00:29:52,480 --> 00:29:58,399
computation in uh you using these

00:29:56,080 --> 00:30:00,320
deep learning frameworks and then we see

00:29:58,399 --> 00:30:00,640
on distributed linear algebra operation

00:30:00,320 --> 00:30:02,640
can

00:30:00,640 --> 00:30:04,559
how disability linear every operation

00:30:02,640 --> 00:30:07,919
can be done using apache mohawk

00:30:04,559 --> 00:30:11,600
and then that is uh to create this

00:30:07,919 --> 00:30:14,080
various packages over spark for data

00:30:11,600 --> 00:30:14,080
processing

00:30:14,559 --> 00:30:19,360
and then we understand how we can

00:30:16,480 --> 00:30:22,559
accelerate the storage system as well

00:30:19,360 --> 00:30:25,679
by making them in memory and

00:30:22,559 --> 00:30:26,640
try to avoid data transmission over the

00:30:25,679 --> 00:30:30,080
system

00:30:26,640 --> 00:30:32,799
for any data processing so in

00:30:30,080 --> 00:30:34,559
so we if we want to build them together

00:30:32,799 --> 00:30:36,960
and

00:30:34,559 --> 00:30:39,600
get the powerful powerful uh machine

00:30:36,960 --> 00:30:41,840
learning system

00:30:39,600 --> 00:30:43,279
uh we can use apache knight as a cache

00:30:41,840 --> 00:30:46,080
or dv and then

00:30:43,279 --> 00:30:47,919
on top of it uh we use this spark core

00:30:46,080 --> 00:30:49,600
engine for any kind of data processing

00:30:47,919 --> 00:30:51,919
so a spark

00:30:49,600 --> 00:30:53,200
supports and can be configured to use

00:30:51,919 --> 00:30:56,399
the apache ignite

00:30:53,200 --> 00:30:58,000
data storage and once data is retrieved

00:30:56,399 --> 00:31:01,120
from the data source

00:30:58,000 --> 00:31:02,240
it apache ignite cluster provides the

00:31:01,120 --> 00:31:06,240
shared rdd

00:31:02,240 --> 00:31:10,080
on to each of these spark workers

00:31:06,240 --> 00:31:13,200
and can be accessible in memory speed

00:31:10,080 --> 00:31:16,559
so no data movement here

00:31:13,200 --> 00:31:17,279
we can do distributed sql or any rdd

00:31:16,559 --> 00:31:20,720
operations

00:31:17,279 --> 00:31:24,000
over the ignite cluster

00:31:20,720 --> 00:31:24,000
that ignite provides

00:31:26,000 --> 00:31:30,480
so also see we can have different data

00:31:29,679 --> 00:31:33,760
sources

00:31:30,480 --> 00:31:36,000
and if we can run them in

00:31:33,760 --> 00:31:37,840
spark that means we can deploy it in

00:31:36,000 --> 00:31:42,000
various lesser management

00:31:37,840 --> 00:31:43,039
tools or container orchestration tools

00:31:42,000 --> 00:31:47,120
like kubernetes

00:31:43,039 --> 00:31:50,320
yarn and measures so yeah and mostly

00:31:47,120 --> 00:31:52,399
as a resource manager and measures can

00:31:50,320 --> 00:31:55,360
work for container and non-containerized

00:31:52,399 --> 00:31:55,360
application both

00:31:57,440 --> 00:32:01,440
so if we are disabled learning

00:31:59,200 --> 00:32:04,880
frameworks then we should have a

00:32:01,440 --> 00:32:06,720
cluster manager our one resource manager

00:32:04,880 --> 00:32:09,039
to use the resources

00:32:06,720 --> 00:32:10,480
efficiently cluster manager to make sure

00:32:09,039 --> 00:32:12,960
there is

00:32:10,480 --> 00:32:15,840
if there is any failure it restart the

00:32:12,960 --> 00:32:15,840
worker node

00:32:16,480 --> 00:32:23,679
so this means um if we have a spark

00:32:20,240 --> 00:32:26,320
standalone flasher

00:32:23,679 --> 00:32:27,440
manager that can provide all these

00:32:26,320 --> 00:32:30,399
features

00:32:27,440 --> 00:32:32,399
if we want to use the mxnet server deep

00:32:30,399 --> 00:32:34,720
learning framework

00:32:32,399 --> 00:32:36,559
then if we have a trained model from

00:32:34,720 --> 00:32:38,640
mxnet we load our

00:32:36,559 --> 00:32:40,840
written spark application and do the

00:32:38,640 --> 00:32:43,679
inference using pi spark and

00:32:40,840 --> 00:32:47,039
mxnet and then this

00:32:43,679 --> 00:32:51,039
whole inference step can be done

00:32:47,039 --> 00:32:54,480
in a better performance in parallel

00:32:51,039 --> 00:32:57,919
so if we see the prototype version work

00:32:54,480 --> 00:33:00,720
pretty well in the small environment

00:32:57,919 --> 00:33:02,960
but it should work in a broad

00:33:00,720 --> 00:33:06,080
environment as well

00:33:02,960 --> 00:33:06,399
so that means it should isolate it from

00:33:06,080 --> 00:33:07,919
the

00:33:06,399 --> 00:33:10,480
environment that means we have to

00:33:07,919 --> 00:33:13,360
containerize it just a recap what is

00:33:10,480 --> 00:33:15,440
stocker container a process

00:33:13,360 --> 00:33:17,200
running on host machine that means a

00:33:15,440 --> 00:33:18,320
docker container if you run the docker

00:33:17,200 --> 00:33:20,159
container we can see the

00:33:18,320 --> 00:33:21,840
each of the container running in a host

00:33:20,159 --> 00:33:23,120
machine using the ps command or

00:33:21,840 --> 00:33:25,919
something

00:33:23,120 --> 00:33:27,039
it each container have own network and

00:33:25,919 --> 00:33:30,960
file system

00:33:27,039 --> 00:33:31,760
just fast and quick unlike vm we can't

00:33:30,960 --> 00:33:35,200
boot

00:33:31,760 --> 00:33:36,480
as a different os but they are all

00:33:35,200 --> 00:33:39,679
isolated

00:33:36,480 --> 00:33:40,720
so running in each host machine that

00:33:39,679 --> 00:33:44,080
means

00:33:40,720 --> 00:33:46,320
uh what makes it isolated so if we

00:33:44,080 --> 00:33:46,960
create a file system for one container

00:33:46,320 --> 00:33:49,840
it

00:33:46,960 --> 00:33:51,360
create a layer on top of it and make

00:33:49,840 --> 00:33:52,799
sure it is isolated from other

00:33:51,360 --> 00:33:55,519
containers

00:33:52,799 --> 00:33:58,480
so when we talk about containers we talk

00:33:55,519 --> 00:34:02,159
about kubernetes for managing them

00:33:58,480 --> 00:34:06,080
so various features in kubernetes like

00:34:02,159 --> 00:34:09,200
uh service we run the container or

00:34:06,080 --> 00:34:11,200
kubernetes pod and to get the replica of

00:34:09,200 --> 00:34:12,480
the instance we use replica controller

00:34:11,200 --> 00:34:15,760
replica set

00:34:12,480 --> 00:34:18,240
and once we get the replica we

00:34:15,760 --> 00:34:18,960
make it discoverable to outside the

00:34:18,240 --> 00:34:22,480
cluster

00:34:18,960 --> 00:34:24,720
and we can have very configuration

00:34:22,480 --> 00:34:26,720
communities object like config max

00:34:24,720 --> 00:34:29,119
secrets and also

00:34:26,720 --> 00:34:30,720
various low role based access control

00:34:29,119 --> 00:34:34,399
over it

00:34:30,720 --> 00:34:37,200
it is easy to roll out new version and

00:34:34,399 --> 00:34:39,359
monitor it as well so it helps in

00:34:37,200 --> 00:34:41,679
automatic deployments and dynamic

00:34:39,359 --> 00:34:43,280
scaling as per the demand and managing

00:34:41,679 --> 00:34:47,040
them

00:34:43,280 --> 00:34:48,240
it helps every developer system admin

00:34:47,040 --> 00:34:51,040
and operation operation

00:34:48,240 --> 00:34:52,800
can name easily monitor and get the

00:34:51,040 --> 00:34:53,440
setup the alert management or logging

00:34:52,800 --> 00:34:56,240
system

00:34:53,440 --> 00:34:57,280
system admin can easily configure the

00:34:56,240 --> 00:34:59,760
various

00:34:57,280 --> 00:35:02,720
infrastructure over it and developers

00:34:59,760 --> 00:35:03,680
can quickly start the equivalent

00:35:02,720 --> 00:35:06,800
discussion and

00:35:03,680 --> 00:35:08,480
and do the experiments over it when we

00:35:06,800 --> 00:35:11,119
see kubernetes for aion

00:35:08,480 --> 00:35:11,119
ml then

00:35:11,680 --> 00:35:15,599
it seems a little difficult because you

00:35:14,000 --> 00:35:17,359
have to containerize everything but

00:35:15,599 --> 00:35:18,880
you don't have to control everything

00:35:17,359 --> 00:35:21,520
because most of the

00:35:18,880 --> 00:35:24,240
things are already present in lm chart

00:35:21,520 --> 00:35:27,599
and we can use the operators

00:35:24,240 --> 00:35:29,680
and do our configuration on top of it so

00:35:27,599 --> 00:35:31,040
when we say spark on kubernetes operator

00:35:29,680 --> 00:35:34,240
that means we just say

00:35:31,040 --> 00:35:36,320
operator to i need spark cluster

00:35:34,240 --> 00:35:38,320
and creates the cluster for you with

00:35:36,320 --> 00:35:39,520
your given configuration and similarly

00:35:38,320 --> 00:35:43,520
for mxnet so

00:35:39,520 --> 00:35:47,200
also various cloud providers giving the

00:35:43,520 --> 00:35:47,200
i've already installed uh

00:35:47,359 --> 00:35:51,119
on on any instant there

00:35:51,440 --> 00:36:00,160
so we need we want also optimization

00:35:55,920 --> 00:36:03,119
should be same as our prototype version

00:36:00,160 --> 00:36:03,599
and it should be portable scalable and

00:36:03,119 --> 00:36:07,119
we can

00:36:03,599 --> 00:36:08,960
we should easily reuse the stages in

00:36:07,119 --> 00:36:11,040
machine learning life cycle

00:36:08,960 --> 00:36:14,240
and we should we should be able to

00:36:11,040 --> 00:36:16,800
easily experiment

00:36:14,240 --> 00:36:18,800
so cube flow provides a uniform platform

00:36:16,800 --> 00:36:20,640
it is a toolkit for kubernetes to run

00:36:18,800 --> 00:36:24,079
the machine learning

00:36:20,640 --> 00:36:27,599
in a simplified way

00:36:24,079 --> 00:36:30,000
so etl trend test tune the

00:36:27,599 --> 00:36:31,599
parameters and update inference

00:36:30,000 --> 00:36:34,160
monitorment and these are these

00:36:31,599 --> 00:36:35,520
stages in which is same for each machine

00:36:34,160 --> 00:36:39,680
learning

00:36:35,520 --> 00:36:42,320
system so we should have

00:36:39,680 --> 00:36:43,280
complete pipeline and that means set of

00:36:42,320 --> 00:36:45,280
steps

00:36:43,280 --> 00:36:46,400
over there and we can we should have a

00:36:45,280 --> 00:36:48,640
ui to visualize

00:36:46,400 --> 00:36:50,640
to understand each how is each step is

00:36:48,640 --> 00:36:53,119
going on

00:36:50,640 --> 00:36:53,680
so we can do the hyper parameter tuning

00:36:53,119 --> 00:36:56,720
missing

00:36:53,680 --> 00:36:58,640
cutting that means if we

00:36:56,720 --> 00:37:02,000
if we do it manually we have to run it

00:36:58,640 --> 00:37:02,000
again and again understand the

00:37:03,040 --> 00:37:08,160
parameter value for it it also provides

00:37:06,560 --> 00:37:11,040
various data scientist

00:37:08,160 --> 00:37:12,160
tools like jupiter notebook and can

00:37:11,040 --> 00:37:15,200
scale dynamically

00:37:12,160 --> 00:37:15,200
as per the demand

00:37:15,680 --> 00:37:19,760
so here one example if you want to do

00:37:17,760 --> 00:37:22,079
data processing and then decomposition

00:37:19,760 --> 00:37:23,839
step then returning step and so on we

00:37:22,079 --> 00:37:27,359
did the data pressing in python

00:37:23,839 --> 00:37:28,160
and some other framework and then we did

00:37:27,359 --> 00:37:33,119
the

00:37:28,160 --> 00:37:36,240
uh we use samsara for uh

00:37:33,119 --> 00:37:36,880
doing the decomposition or like reducing

00:37:36,240 --> 00:37:40,480
the

00:37:36,880 --> 00:37:45,680
resolution like data dimensionality

00:37:40,480 --> 00:37:45,680
reduction on each of the images

00:37:46,160 --> 00:37:51,200
basically filtering out all the noise

00:37:47,839 --> 00:37:52,880
data and only using the

00:37:51,200 --> 00:37:55,200
pixels that is required for the

00:37:52,880 --> 00:37:58,079
prediction and running on top of

00:37:55,200 --> 00:38:00,079
spark so that are all containerized each

00:37:58,079 --> 00:38:03,119
step is containerized here

00:38:00,079 --> 00:38:05,359
and then turning step and so on so my

00:38:03,119 --> 00:38:07,599
motive is here too so that each step is

00:38:05,359 --> 00:38:10,720
containerized and running in different

00:38:07,599 --> 00:38:14,400
languages or frameworks

00:38:10,720 --> 00:38:14,800
and using kubeflow we have a pipeline

00:38:14,400 --> 00:38:18,320
here

00:38:14,800 --> 00:38:20,720
we compile it and we create the

00:38:18,320 --> 00:38:22,400
experiment and run the pipeline so we

00:38:20,720 --> 00:38:27,760
can do the experiment over

00:38:22,400 --> 00:38:30,480
each steps and so on

00:38:27,760 --> 00:38:32,160
so once everything is fine you can enjoy

00:38:30,480 --> 00:38:35,200
this movement and monitor your

00:38:32,160 --> 00:38:35,760
uh or experiment the whole your machine

00:38:35,200 --> 00:38:37,680
and your life

00:38:35,760 --> 00:38:39,760
cycle cycle there are other cluster

00:38:37,680 --> 00:38:41,200
managers as well say like yarn mesos

00:38:39,760 --> 00:38:43,760
that works pretty fine

00:38:41,200 --> 00:38:46,240
but i think your flow is solving most of

00:38:43,760 --> 00:38:48,000
the challenges currently i

00:38:46,240 --> 00:38:49,680
i will also be talking about cluster

00:38:48,000 --> 00:38:50,800
management in apartheid ecosystem and

00:38:49,680 --> 00:38:53,520
kubernetes in

00:38:50,800 --> 00:38:54,400
extra which is after two hours from now

00:38:53,520 --> 00:38:58,839
and

00:38:54,400 --> 00:39:01,760
i hope you will join me there as well

00:38:58,839 --> 00:39:02,480
i so i we go through each research

00:39:01,760 --> 00:39:05,839
papers

00:39:02,480 --> 00:39:08,640
and i also read various books on top on

00:39:05,839 --> 00:39:10,000
for these technologies and official site

00:39:08,640 --> 00:39:13,680
are very

00:39:10,000 --> 00:39:15,040
we have very very good blog post and

00:39:13,680 --> 00:39:16,880
articles over it

00:39:15,040 --> 00:39:18,720
so thank you everyone for joining me in

00:39:16,880 --> 00:39:21,839
this talk i hope you learned

00:39:18,720 --> 00:39:22,960
a lot of things here so you can find me

00:39:21,839 --> 00:39:24,880
easily in any of

00:39:22,960 --> 00:39:26,560
uh with this android secretary jackson

00:39:24,880 --> 00:39:29,680
secretary

00:39:26,560 --> 00:39:32,119
my github url uh

00:39:29,680 --> 00:39:33,520
the personal website is hosted in

00:39:32,119 --> 00:39:35,680
security.github.io

00:39:33,520 --> 00:39:38,640
you can also also check out the various

00:39:35,680 --> 00:39:38,640
position on

00:39:39,280 --> 00:39:43,200
in apple in apple booth if you have any

00:39:42,640 --> 00:39:46,000
question

00:39:43,200 --> 00:39:47,839
feel free to text i am also available in

00:39:46,000 --> 00:39:50,880
machine learning big twitter track

00:39:47,839 --> 00:39:54,000
in slack channel apache con and also in

00:39:50,880 --> 00:39:55,760
apple boot thank you thank you very much

00:39:54,000 --> 00:39:57,280
thank you apache phone thank you apple

00:39:55,760 --> 00:40:10,720
and other companies for

00:39:57,280 --> 00:40:10,720

YouTube URL: https://www.youtube.com/watch?v=vV0Df0sk0zg


