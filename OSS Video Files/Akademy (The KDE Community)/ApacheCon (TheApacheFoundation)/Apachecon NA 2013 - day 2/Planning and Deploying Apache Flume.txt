Title: Planning and Deploying Apache Flume
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 2
Description: 
	Arvind Prabhakar
ApacheCon NA 2013
Big Data
Captions: 
	00:00:00,030 --> 00:00:05,220
all right we have our win profit from

00:00:02,480 --> 00:00:08,160
cloudera he's going to talk on Apache a

00:00:05,220 --> 00:00:12,150
flume thanks

00:00:08,160 --> 00:00:13,679
so hey everyone I'd appreciate if you

00:00:12,150 --> 00:00:17,160
know given the fact that this is between

00:00:13,679 --> 00:00:19,770
lunch and it's the last talk for for the

00:00:17,160 --> 00:00:21,300
morning I know it's going to be hard for

00:00:19,770 --> 00:00:24,269
you guys to focus but I'll try my best

00:00:21,300 --> 00:00:27,150
wrap it up on time so this talk is about

00:00:24,269 --> 00:00:28,320
planning your your deployment of Apache

00:00:27,150 --> 00:00:32,099
flume

00:00:28,320 --> 00:00:33,540
my name is Arvind from Hawker I am a PMC

00:00:32,099 --> 00:00:35,370
member and committer on a couple of

00:00:33,540 --> 00:00:37,920
Apache projects scoop and from

00:00:35,370 --> 00:00:40,890
specifically well these projects deal

00:00:37,920 --> 00:00:44,640
with data injustice it's it's it's a use

00:00:40,890 --> 00:00:47,910
case that's very close to the effective

00:00:44,640 --> 00:00:49,440
utilization for systems like - I'm also

00:00:47,910 --> 00:00:51,899
a member of the Apache Software

00:00:49,440 --> 00:00:55,350
Foundation and last but not least mo

00:00:51,899 --> 00:00:59,789
engineering manager at Cloudera what I

00:00:55,350 --> 00:01:02,160
have in this talk is basically to two

00:00:59,789 --> 00:01:03,510
broad sections of you know my mind my

00:01:02,160 --> 00:01:06,119
whole deck is divided into two broad

00:01:03,510 --> 00:01:09,119
sections one section goes through the

00:01:06,119 --> 00:01:10,950
overview of flume the other goes through

00:01:09,119 --> 00:01:14,670
planning and sizing for your deployment

00:01:10,950 --> 00:01:17,970
and the title of this talk is is how you

00:01:14,670 --> 00:01:19,320
plan and size your your topology in your

00:01:17,970 --> 00:01:21,630
deployment of capacity a flume so I'd

00:01:19,320 --> 00:01:24,390
like to spend more time on that but the

00:01:21,630 --> 00:01:26,670
same time I know that without having in

00:01:24,390 --> 00:01:29,159
a background and not everybody may have

00:01:26,670 --> 00:01:30,659
an exposure to what Apache flume is so

00:01:29,159 --> 00:01:33,240
I'd like to quickly go over the overview

00:01:30,659 --> 00:01:36,600
a quick show of hands how many of you

00:01:33,240 --> 00:01:39,270
have actually heard or used flume you

00:01:36,600 --> 00:01:46,100
know interactive is okay great thank you

00:01:39,270 --> 00:01:48,649
so we'll do the overview first so

00:01:46,100 --> 00:01:52,350
fundamentally what Apache flume is is

00:01:48,649 --> 00:01:55,280
it's a system that's designed to solve

00:01:52,350 --> 00:01:58,469
data aggregation problem from streaming

00:01:55,280 --> 00:02:00,719
sources so if you have trickle feed data

00:01:58,469 --> 00:02:04,229
coming in from various different

00:02:00,719 --> 00:02:07,200
locations in your data center flume is a

00:02:04,229 --> 00:02:08,640
system they'll help you set up the

00:02:07,200 --> 00:02:10,560
ingest for that you know have it

00:02:08,640 --> 00:02:12,970
aggregate in you know particular place

00:02:10,560 --> 00:02:17,950
where you can then later run your

00:02:12,970 --> 00:02:19,270
analysis and such so the the the big

00:02:17,950 --> 00:02:22,030
question is you know why would you need

00:02:19,270 --> 00:02:23,980
a system like lumen there there it seems

00:02:22,030 --> 00:02:25,630
to be a very commonplace occurrence in

00:02:23,980 --> 00:02:27,070
most data centers you have locked files

00:02:25,630 --> 00:02:29,440
that are scattered all over the place

00:02:27,070 --> 00:02:31,660
you know operators and administrators

00:02:29,440 --> 00:02:32,920
have figured out ways to deal with you

00:02:31,660 --> 00:02:36,520
know how those lock files are moved

00:02:32,920 --> 00:02:39,190
around so so the big question is why

00:02:36,520 --> 00:02:42,130
would you ever want to have a dedicated

00:02:39,190 --> 00:02:44,500
system for that use case the answer is

00:02:42,130 --> 00:02:46,240
fairly straightforward and if you have

00:02:44,500 --> 00:02:47,860
encountered this problem in your data

00:02:46,240 --> 00:02:50,500
center you probably would identify with

00:02:47,860 --> 00:02:52,000
all of these points stated here the fact

00:02:50,500 --> 00:02:55,270
that you need something that's reliable

00:02:52,000 --> 00:02:58,660
in that that scales out is of paramount

00:02:55,270 --> 00:02:59,830
importance if you were to do a solution

00:02:58,660 --> 00:03:03,940
you know implement a solution in a

00:02:59,830 --> 00:03:07,360
sustainable way if your data aggregation

00:03:03,940 --> 00:03:10,180
problem is such that your operators and

00:03:07,360 --> 00:03:12,550
administrators keep getting called you

00:03:10,180 --> 00:03:14,650
know based every every time there is a

00:03:12,550 --> 00:03:16,060
down note you have a problem at hand

00:03:14,650 --> 00:03:18,459
because you'd probably have to have an

00:03:16,060 --> 00:03:20,350
army of operators to go you know police

00:03:18,459 --> 00:03:23,920
that and and and make sure that keeps

00:03:20,350 --> 00:03:25,720
working there is there's other aspects

00:03:23,920 --> 00:03:28,570
of ad hoc solutions that don't quite

00:03:25,720 --> 00:03:31,680
work for instance you know the

00:03:28,570 --> 00:03:33,519
complexity of the whole aggregation

00:03:31,680 --> 00:03:35,739
requirement the complexity of the whole

00:03:33,519 --> 00:03:38,410
aggregation use case tends to only

00:03:35,739 --> 00:03:40,690
increase it'll never be the case like

00:03:38,410 --> 00:03:42,730
you know our business is expanding and

00:03:40,690 --> 00:03:44,769
lo behold like you know our data

00:03:42,730 --> 00:03:47,640
aggregation is simplified itself it's

00:03:44,769 --> 00:03:51,400
not going to happen it will always go

00:03:47,640 --> 00:03:52,780
towards the more complex and and that's

00:03:51,400 --> 00:03:54,100
when you would realize that you know

00:03:52,780 --> 00:03:55,570
what started out as an innocent

00:03:54,100 --> 00:03:57,850
experiment with like maybe a dozen

00:03:55,570 --> 00:04:01,209
servers in you know you know upcoming

00:03:57,850 --> 00:04:03,790
startup stage of your company quickly

00:04:01,209 --> 00:04:05,080
ran out of control and and now your face

00:04:03,790 --> 00:04:06,580
for like hundreds and thousands of

00:04:05,080 --> 00:04:09,340
servers you know all across maybe

00:04:06,580 --> 00:04:10,840
geographic boundaries and you're

00:04:09,340 --> 00:04:12,070
scratching your head so those are the

00:04:10,840 --> 00:04:13,450
kind of things like you know you have a

00:04:12,070 --> 00:04:15,760
batch of lumen in a situation like this

00:04:13,450 --> 00:04:18,010
you set it up and you forget about it

00:04:15,760 --> 00:04:21,520
it's a system if you plan in size

00:04:18,010 --> 00:04:24,190
correctly will self-sustained it also

00:04:21,520 --> 00:04:25,740
has other advantages like for example it

00:04:24,190 --> 00:04:26,789
has decorative

00:04:25,740 --> 00:04:29,250
in configuration which basically

00:04:26,789 --> 00:04:33,180
translates to you don't have to code to

00:04:29,250 --> 00:04:34,740
use it you you would interact with the

00:04:33,180 --> 00:04:37,560
system through property configuration

00:04:34,740 --> 00:04:39,990
files and the system is able to take up

00:04:37,560 --> 00:04:43,710
those files as the change and readjust

00:04:39,990 --> 00:04:45,810
itself retool itself you could have

00:04:43,710 --> 00:04:49,020
contextual routing contextual routing is

00:04:45,810 --> 00:04:50,960
is sort of the equivalent of what if I

00:04:49,020 --> 00:04:53,099
have fire in that part of my datacenter

00:04:50,960 --> 00:04:55,349
you would want your traffic to be routed

00:04:53,099 --> 00:04:58,830
in a separate data center or something

00:04:55,349 --> 00:05:00,509
of that nature and then it has a ton of

00:04:58,830 --> 00:05:01,919
features not every feature is going to

00:05:00,509 --> 00:05:04,530
appeal to everyone which is you know

00:05:01,919 --> 00:05:06,599
obviously the case the wave flume is

00:05:04,530 --> 00:05:09,000
designed as to deal with the majority

00:05:06,599 --> 00:05:11,729
use cases so it doesn't quite

00:05:09,000 --> 00:05:15,330
specialized it in one specific area of

00:05:11,729 --> 00:05:17,099
the the the aggregation problem you

00:05:15,330 --> 00:05:19,289
would find features that will enable you

00:05:17,099 --> 00:05:21,659
to do load balancing and failover and

00:05:19,289 --> 00:05:24,870
such which may or may not be applicable

00:05:21,659 --> 00:05:26,400
in other use cases and so on so over the

00:05:24,870 --> 00:05:29,550
next few slides I'm gonna go through

00:05:26,400 --> 00:05:32,340
these core concepts of what flume is and

00:05:29,550 --> 00:05:37,199
you know talk about how these concepts

00:05:32,340 --> 00:05:40,830
come together so the the first concept

00:05:37,199 --> 00:05:44,090
is is that of an event this is the

00:05:40,830 --> 00:05:48,690
definition that you know I think we have

00:05:44,090 --> 00:05:51,530
some consensus on you it's important to

00:05:48,690 --> 00:05:54,990
know that that you know an event is a

00:05:51,530 --> 00:05:58,590
very overloaded term means different

00:05:54,990 --> 00:06:01,680
things in different contexts in in in

00:05:58,590 --> 00:06:06,210
the context of flume what an event is is

00:06:01,680 --> 00:06:07,860
basically this bike payload an opaque

00:06:06,210 --> 00:06:12,840
payload that you're trying to send from

00:06:07,860 --> 00:06:15,210
one point to another so if you have you

00:06:12,840 --> 00:06:17,370
know for example log files that are

00:06:15,210 --> 00:06:21,080
being generated by you know servers

00:06:17,370 --> 00:06:23,099
running in you know thousands of nodes

00:06:21,080 --> 00:06:25,289
every entry in that log file could

00:06:23,099 --> 00:06:27,389
qualify as an event or it could be

00:06:25,289 --> 00:06:29,099
something totally different it entirely

00:06:27,389 --> 00:06:30,569
depends upon what you use cases what

00:06:29,099 --> 00:06:33,300
you're trying to aggregate where you try

00:06:30,569 --> 00:06:35,849
and take it and then you basically

00:06:33,300 --> 00:06:37,919
define here are my events and and flume

00:06:35,849 --> 00:06:38,390
as as a system comes with ways by which

00:06:37,919 --> 00:06:41,810
how

00:06:38,390 --> 00:06:44,150
you figure that out but once you have

00:06:41,810 --> 00:06:46,070
this event flume we'll be able to take

00:06:44,150 --> 00:06:49,730
it and route it to its destination and

00:06:46,070 --> 00:06:51,980
all across this routing flume doesn't

00:06:49,730 --> 00:06:55,870
care much about what exactly that event

00:06:51,980 --> 00:06:58,490
contains it could be you know just any

00:06:55,870 --> 00:06:59,990
random set of bytes for for all flume

00:06:58,490 --> 00:07:02,200
cares it doesn't have to be fixed sized

00:06:59,990 --> 00:07:05,720
it could be variable size by payload

00:07:02,200 --> 00:07:08,060
what flume does care about is optional

00:07:05,720 --> 00:07:10,970
headers which can be applied to this

00:07:08,060 --> 00:07:12,980
event and the concept is is inspired

00:07:10,970 --> 00:07:16,340
from you know how SMTP routing works

00:07:12,980 --> 00:07:19,430
where you have like string you know key

00:07:16,340 --> 00:07:21,760
value pairs that that that carry the

00:07:19,430 --> 00:07:24,290
envelope information for your payload

00:07:21,760 --> 00:07:25,850
and similar information can be added to

00:07:24,290 --> 00:07:29,360
the headers in flume and then once once

00:07:25,850 --> 00:07:31,190
it's in you can configure flume to react

00:07:29,360 --> 00:07:32,510
just presence of certain headers in this

00:07:31,190 --> 00:07:34,790
you know maybe achieve contextual

00:07:32,510 --> 00:07:36,830
routing and you can manipulate these

00:07:34,790 --> 00:07:41,390
other nests as the event is trickling

00:07:36,830 --> 00:07:43,970
down all events they start at the client

00:07:41,390 --> 00:07:47,390
place so so so client is the point of

00:07:43,970 --> 00:07:49,460
origination of events in in in the case

00:07:47,390 --> 00:07:51,320
of you know hundreds of web servers or

00:07:49,460 --> 00:07:53,930
hundreds of servers in your data center

00:07:51,320 --> 00:07:58,040
generating logs you'd probably have a

00:07:53,930 --> 00:08:01,400
client in those notes and then you would

00:07:58,040 --> 00:08:03,410
send you did the client would then parse

00:08:01,400 --> 00:08:05,840
out the logs take out the events from

00:08:03,410 --> 00:08:08,510
the logs and start routing them pumping

00:08:05,840 --> 00:08:10,550
them into the flume pipelines

00:08:08,510 --> 00:08:12,590
there are out-of-the-box clients there's

00:08:10,550 --> 00:08:15,740
the lock for J appender there there are

00:08:12,590 --> 00:08:19,610
you know you know that there's a very

00:08:15,740 --> 00:08:22,520
very simple to use API which runs with

00:08:19,610 --> 00:08:24,830
minimal configuration and and you

00:08:22,520 --> 00:08:29,600
basically take it you can write your own

00:08:24,830 --> 00:08:31,850
client using that API and the impact it

00:08:29,600 --> 00:08:36,520
has on on your overall solution is that

00:08:31,850 --> 00:08:39,260
it decouples your aggregation complexity

00:08:36,520 --> 00:08:42,290
from your your application that is that

00:08:39,260 --> 00:08:44,150
is using the client if you were to for

00:08:42,290 --> 00:08:47,920
example have your application directly

00:08:44,150 --> 00:08:50,210
write data into say an edge base cluster

00:08:47,920 --> 00:08:51,800
the kind of problems you'll be facing

00:08:50,210 --> 00:08:53,510
quite different from

00:08:51,800 --> 00:08:55,070
kind of problems you face when you know

00:08:53,510 --> 00:08:56,870
hey you have a simple client that's

00:08:55,070 --> 00:08:58,760
that's fairly straightforward you have a

00:08:56,870 --> 00:09:01,130
sized and plan topology of how that

00:08:58,760 --> 00:09:02,510
aggregation flow trickles into HBase and

00:09:01,130 --> 00:09:04,870
then flume kind of takes over and make

00:09:02,510 --> 00:09:06,740
sure that it correctly deposits itself

00:09:04,870 --> 00:09:08,000
clients are not necessarily needed in

00:09:06,740 --> 00:09:09,040
all case and we'll talk about that in a

00:09:08,000 --> 00:09:11,600
bit

00:09:09,040 --> 00:09:15,320
but so you have the point of origination

00:09:11,600 --> 00:09:17,779
as the client and when the client does

00:09:15,320 --> 00:09:19,519
is it sends data into agents and there's

00:09:17,779 --> 00:09:24,290
a set of agents you know one behind the

00:09:19,519 --> 00:09:27,140
other forming a pipeline an agent you

00:09:24,290 --> 00:09:29,300
know is basically a container which runs

00:09:27,140 --> 00:09:30,860
all of the flume components and we'll

00:09:29,300 --> 00:09:32,120
talk about these components that the

00:09:30,860 --> 00:09:35,240
first component that comes in the

00:09:32,120 --> 00:09:38,500
pipeline is a source so what a source is

00:09:35,240 --> 00:09:42,589
is essentially a component that that

00:09:38,500 --> 00:09:44,779
receives events from the upstream so you

00:09:42,589 --> 00:09:48,829
you know if your clients are running in

00:09:44,779 --> 00:09:51,140
your server nodes they would be using

00:09:48,829 --> 00:09:53,060
some IPC mechanism to send those events

00:09:51,140 --> 00:09:55,010
over to an age and the agent that it's

00:09:53,060 --> 00:09:57,709
configured with and there would be a

00:09:55,010 --> 00:10:01,550
source operating within the agent that

00:09:57,709 --> 00:10:04,220
will receive that event so there are a

00:10:01,550 --> 00:10:08,360
couple of IPC sources that flume comes

00:10:04,220 --> 00:10:10,490
with one is aro thrift is not yet in any

00:10:08,360 --> 00:10:13,149
of the upstream releases it should be in

00:10:10,490 --> 00:10:16,130
the next release the work is almost done

00:10:13,149 --> 00:10:17,390
and what the source does is it you know

00:10:16,130 --> 00:10:18,620
it says here it requires at least one

00:10:17,390 --> 00:10:21,620
channel to function but basically

00:10:18,620 --> 00:10:23,690
channel is a buffer that the source

00:10:21,620 --> 00:10:28,610
writes to so the source will take that

00:10:23,690 --> 00:10:30,260
event put it in the channel so there are

00:10:28,610 --> 00:10:32,060
different types of channels that flume

00:10:30,260 --> 00:10:33,560
comes with and you know again all of

00:10:32,060 --> 00:10:35,329
these components are basically

00:10:33,560 --> 00:10:36,680
interfaces and there's there's a rich

00:10:35,329 --> 00:10:38,810
framework that you could extend write

00:10:36,680 --> 00:10:40,750
your own channels if you wanted to but

00:10:38,810 --> 00:10:44,600
essentially what a channel is is is a

00:10:40,750 --> 00:10:46,130
transactional queue a double-ended

00:10:44,600 --> 00:10:48,980
transaction okay so you have sources

00:10:46,130 --> 00:10:50,630
writing into the tail of this queue

00:10:48,980 --> 00:10:52,820
using transactional semantics and and

00:10:50,630 --> 00:10:54,500
sinks which are draining this queue and

00:10:52,820 --> 00:10:56,899
this gives you this classic

00:10:54,500 --> 00:10:58,459
producer-consumer model where you have

00:10:56,899 --> 00:11:00,110
the channels as the buffer and the

00:10:58,459 --> 00:11:02,210
sources as the producers and sinks as

00:11:00,110 --> 00:11:07,070
the consumers

00:11:02,210 --> 00:11:09,050
the point in note here is the channels

00:11:07,070 --> 00:11:11,570
themselves are passive they're not doing

00:11:09,050 --> 00:11:15,920
any data transfer they essentially rely

00:11:11,570 --> 00:11:17,870
on the the fact that the sources are

00:11:15,920 --> 00:11:19,640
actively dumping information into the

00:11:17,870 --> 00:11:21,500
channel and things are actively draining

00:11:19,640 --> 00:11:24,590
the channel and when we talk about sing

00:11:21,500 --> 00:11:26,660
but you know just just for completeness

00:11:24,590 --> 00:11:28,490
there could be IPC syncs or there could

00:11:26,660 --> 00:11:30,950
be terminal syncs terminal sinks are

00:11:28,490 --> 00:11:33,650
things which will take event data out of

00:11:30,950 --> 00:11:35,210
the flume pipeline and deposit it in its

00:11:33,650 --> 00:11:37,750
destination so the most popular

00:11:35,210 --> 00:11:42,260
terminals saying that we encounter is

00:11:37,750 --> 00:11:44,000
HDFS sink and etch basing their people

00:11:42,260 --> 00:11:46,370
in the community have gone and written

00:11:44,000 --> 00:11:47,690
things for other systems as well and I

00:11:46,370 --> 00:11:51,260
think most of them are available on

00:11:47,690 --> 00:11:55,820
github and maybe even as patches but if

00:11:51,260 --> 00:11:57,680
you have a elongated pipeline this thing

00:11:55,820 --> 00:11:59,990
this source communication would be the

00:11:57,680 --> 00:12:03,020
IPC communication so you have the

00:11:59,990 --> 00:12:04,640
average source I've ever sync on one

00:12:03,020 --> 00:12:10,160
agent talking to the average source and

00:12:04,640 --> 00:12:12,170
another agent some secondary concepts

00:12:10,160 --> 00:12:15,680
there's this notion of interceptor this

00:12:12,170 --> 00:12:19,100
is this is a simple extension point that

00:12:15,680 --> 00:12:22,400
you can use to introspect inspect events

00:12:19,100 --> 00:12:24,170
and you know drop them filter them maybe

00:12:22,400 --> 00:12:26,180
even modify them although you know that

00:12:24,170 --> 00:12:28,460
not necessary ohm is not the best system

00:12:26,180 --> 00:12:32,000
for modifying events as they go along

00:12:28,460 --> 00:12:33,860
but filtering may be routing those kind

00:12:32,000 --> 00:12:35,780
of things you could inspect the event

00:12:33,860 --> 00:12:37,460
and apply headers based on their

00:12:35,780 --> 00:12:39,290
priorities whatever that that

00:12:37,460 --> 00:12:42,170
information that matters to your use

00:12:39,290 --> 00:12:44,570
case there's some out-of-the-box

00:12:42,170 --> 00:12:46,670
interceptors there's there's the

00:12:44,570 --> 00:12:48,350
timestamp interceptor and the host name

00:12:46,670 --> 00:12:50,510
interceptor and and the static header

00:12:48,350 --> 00:12:52,280
and receptor so for example you know

00:12:50,510 --> 00:12:54,200
there's this all kind of works very well

00:12:52,280 --> 00:12:55,730
across you know the different components

00:12:54,200 --> 00:12:59,420
for example the timestamp interceptor

00:12:55,730 --> 00:13:01,460
will will apply the UTC timestamp on on

00:12:59,420 --> 00:13:04,940
top of an event and then you can use

00:13:01,460 --> 00:13:07,460
that timestamp and specific parts of it

00:13:04,940 --> 00:13:11,180
like the hour the day the year the month

00:13:07,460 --> 00:13:15,420
and so on to be used as a path delimiter

00:13:11,180 --> 00:13:16,770
in your destination location and HDFS or

00:13:15,420 --> 00:13:18,450
you know if you have your own

00:13:16,770 --> 00:13:19,950
serializers you can use that information

00:13:18,450 --> 00:13:22,020
for doing anything else for that matter

00:13:19,950 --> 00:13:24,240
but out of the box it's fairly flexible

00:13:22,020 --> 00:13:27,990
how these interceptors can affect where

00:13:24,240 --> 00:13:30,300
your destination is we also have channel

00:13:27,990 --> 00:13:32,760
selectors what channel selectors do is

00:13:30,300 --> 00:13:34,500
is when you know a source is configured

00:13:32,760 --> 00:13:38,550
with multiple channels a channel

00:13:34,500 --> 00:13:40,740
selector can then decide which channel

00:13:38,550 --> 00:13:43,560
must a batch of event that just freshly

00:13:40,740 --> 00:13:47,760
arrived would go to doing this allows

00:13:43,560 --> 00:13:49,610
you to effectively have have custom

00:13:47,760 --> 00:13:52,290
routing logic if you want right there

00:13:49,610 --> 00:13:53,640
are things that there couple of channel

00:13:52,290 --> 00:13:55,320
selectors that come out of the box

00:13:53,640 --> 00:13:56,700
there's the replicating channel selector

00:13:55,320 --> 00:13:58,380
and the multiplexing channel selector

00:13:56,700 --> 00:13:59,760
what replicating does is copies your

00:13:58,380 --> 00:14:03,030
event data or worse this kind of a

00:13:59,760 --> 00:14:05,460
fan-out flow and the multiplexing

00:14:03,030 --> 00:14:08,190
selector allows you to make conditional

00:14:05,460 --> 00:14:10,440
hops so for example you can say if my

00:14:08,190 --> 00:14:13,050
event bat event data has the certain

00:14:10,440 --> 00:14:17,370
header I wanted to go to that channel

00:14:13,050 --> 00:14:20,870
and and so on so forth and then finally

00:14:17,370 --> 00:14:23,610
you know sync processor is basically an

00:14:20,870 --> 00:14:25,350
element that works with multiple syncs

00:14:23,610 --> 00:14:26,910
and it's capable of implementing complex

00:14:25,350 --> 00:14:29,100
logic such as failover and load

00:14:26,910 --> 00:14:30,690
balancing inside all of this is software

00:14:29,100 --> 00:14:32,460
logic set so you know there is no

00:14:30,690 --> 00:14:35,970
requirement for you to go you know have

00:14:32,460 --> 00:14:37,200
like a dedicated load balancer installed

00:14:35,970 --> 00:14:38,880
in your pipeline you can you can

00:14:37,200 --> 00:14:42,600
actually code this up through

00:14:38,880 --> 00:14:44,250
configuration so there there's the built

00:14:42,600 --> 00:14:46,020
in load balancing sync processor which

00:14:44,250 --> 00:14:48,390
allows you to have random or round-robin

00:14:46,020 --> 00:14:50,520
out-of-the-box mechanisms and then you

00:14:48,390 --> 00:14:52,320
can implement your own selection

00:14:50,520 --> 00:14:54,320
mechanism across different sinks and

00:14:52,320 --> 00:14:59,720
then you have failover and the default

00:14:54,320 --> 00:15:02,190
so this is how an agent takes in data

00:14:59,720 --> 00:15:04,790
what you see you know the little blue

00:15:02,190 --> 00:15:06,870
arrow if you can see it is is

00:15:04,790 --> 00:15:11,880
hypothetical data coming in to your

00:15:06,870 --> 00:15:14,280
agent and the source receives it it it

00:15:11,880 --> 00:15:15,630
talks to this gray out component called

00:15:14,280 --> 00:15:17,280
the channel processor which is grayed

00:15:15,630 --> 00:15:19,380
out because that's a framework component

00:15:17,280 --> 00:15:21,390
in flume it's not replaceable everything

00:15:19,380 --> 00:15:22,860
else is replaceable in this diagram you

00:15:21,390 --> 00:15:24,030
could write your own sources you could

00:15:22,860 --> 00:15:25,410
write your own interceptors you could

00:15:24,030 --> 00:15:26,850
write your own channel selectors and

00:15:25,410 --> 00:15:29,019
your own channel

00:15:26,850 --> 00:15:33,540
but that said you know the channel

00:15:29,019 --> 00:15:35,980
processor what it does is it applies the

00:15:33,540 --> 00:15:39,820
configure interceptors on top of these

00:15:35,980 --> 00:15:42,430
sources and then once once these

00:15:39,820 --> 00:15:44,230
interceptors pass like they allow these

00:15:42,430 --> 00:15:46,420
events to proceed down the ingest

00:15:44,230 --> 00:15:49,570
pipeline the channel processor would

00:15:46,420 --> 00:15:51,310
then invoke the configured selector to

00:15:49,570 --> 00:15:54,839
identify the set of channels that this

00:15:51,310 --> 00:15:54,839
event information needs to go to

00:15:54,959 --> 00:15:59,230
similarly on the other side you have

00:15:57,399 --> 00:16:01,300
like the agent draining of this

00:15:59,230 --> 00:16:04,269
information so you have a sink processor

00:16:01,300 --> 00:16:07,449
which again is a framework component

00:16:04,269 --> 00:16:09,339
it's not replaceable but I'm sorry the

00:16:07,449 --> 00:16:12,010
sink runner which identifies the sink

00:16:09,339 --> 00:16:14,500
processor to use and and the sink

00:16:12,010 --> 00:16:17,470
processor then figures out which set of

00:16:14,500 --> 00:16:19,449
sinks need to be involved and those

00:16:17,470 --> 00:16:23,410
things automatically drain the channels

00:16:19,449 --> 00:16:25,720
that they are configured with so this is

00:16:23,410 --> 00:16:28,440
a schematic of how you can think of the

00:16:25,720 --> 00:16:32,230
the pipeline the flow in fluent working

00:16:28,440 --> 00:16:34,750
you know that the top diagram is you

00:16:32,230 --> 00:16:37,060
know a healthy flow where you have data

00:16:34,750 --> 00:16:40,420
coming in and to you know first agent

00:16:37,060 --> 00:16:41,890
and it it dips into the channel the

00:16:40,420 --> 00:16:43,600
channel starts buffering the data and

00:16:41,890 --> 00:16:46,750
then there's a sink which is draining

00:16:43,600 --> 00:16:48,699
that data out so in a steady state you

00:16:46,750 --> 00:16:50,709
your channel sizes you know the the

00:16:48,699 --> 00:16:53,699
metaphoric like liquid inside this

00:16:50,709 --> 00:16:56,529
little drum will be at a minimal level

00:16:53,699 --> 00:16:58,449
you'd be you know draining it as it

00:16:56,529 --> 00:17:00,070
comes along and that's the effective

00:16:58,449 --> 00:17:01,600
that's the reason why you want to plan

00:17:00,070 --> 00:17:05,040
because you know make sure that your

00:17:01,600 --> 00:17:08,980
pipelines behave in a predictable manner

00:17:05,040 --> 00:17:10,419
so anyway but but but the the point the

00:17:08,980 --> 00:17:11,620
fact is that there will be failures

00:17:10,419 --> 00:17:13,569
there'll be net for congestion there

00:17:11,620 --> 00:17:16,179
will be like you know people tripping

00:17:13,569 --> 00:17:18,339
over wires and such when that happens

00:17:16,179 --> 00:17:21,610
and a communication link is lost

00:17:18,339 --> 00:17:24,429
what flume does is it starts using the

00:17:21,610 --> 00:17:25,929
channel buffers to absorb that shock to

00:17:24,429 --> 00:17:27,819
absolve that failure

00:17:25,929 --> 00:17:30,160
it'll keep buffering so in this

00:17:27,819 --> 00:17:32,290
particular diagram you know the the the

00:17:30,160 --> 00:17:35,020
communication to the last agent is lost

00:17:32,290 --> 00:17:37,570
so the second agent starts buffering up

00:17:35,020 --> 00:17:40,360
all the data that comes its way

00:17:37,570 --> 00:17:42,430
and if if the link is restored

00:17:40,360 --> 00:17:43,720
you know it'll drain back again and and

00:17:42,430 --> 00:17:46,210
and the system will come back to an

00:17:43,720 --> 00:17:47,800
equilibrium steady-state again but if it

00:17:46,210 --> 00:17:49,750
doesn't eventually this channel will

00:17:47,800 --> 00:17:52,150
fill up and when this channel fills up

00:17:49,750 --> 00:17:54,550
it'll fail itself that agent will stop

00:17:52,150 --> 00:17:56,080
accepting any data and that's when the

00:17:54,550 --> 00:17:58,810
first agent in this pipeline and start

00:17:56,080 --> 00:18:01,750
buffering things up so you know the

00:17:58,810 --> 00:18:03,490
longer you have the pipeline the better

00:18:01,750 --> 00:18:07,270
is your ability to you know sort of

00:18:03,490 --> 00:18:11,230
regain so much information across you

00:18:07,270 --> 00:18:13,390
know from downstream failures the the

00:18:11,230 --> 00:18:17,790
part of the system that actually makes

00:18:13,390 --> 00:18:21,610
it really important to notice is that

00:18:17,790 --> 00:18:22,900
the exchange between ages of of event

00:18:21,610 --> 00:18:25,270
data of the event badges is

00:18:22,900 --> 00:18:26,500
transactional which basically means that

00:18:25,270 --> 00:18:28,470
your data is not going to be lost

00:18:26,500 --> 00:18:32,760
regardless of any of these failures an

00:18:28,470 --> 00:18:38,770
agent will not let go of your event data

00:18:32,760 --> 00:18:40,870
unless it gets confirmed commits a

00:18:38,770 --> 00:18:42,700
transactional commit from the other age

00:18:40,870 --> 00:18:44,740
and that's accepting this data so if you

00:18:42,700 --> 00:18:47,890
look at this particular diagram that the

00:18:44,740 --> 00:18:51,760
blue channel and the blue sync are the

00:18:47,890 --> 00:18:53,590
upstream agent and the I guess ping or Y

00:18:51,760 --> 00:18:56,530
that whatever that is source and channel

00:18:53,590 --> 00:18:58,450
it's the downstream so the upstream

00:18:56,530 --> 00:19:00,960
agent starts the transaction the sync in

00:18:58,450 --> 00:19:03,610
the upstream agents transit transaction

00:19:00,960 --> 00:19:05,830
removes a batch of events send it across

00:19:03,610 --> 00:19:08,290
over the network to the source in the

00:19:05,830 --> 00:19:11,290
downstream agent which then starts its

00:19:08,290 --> 00:19:14,680
own transaction commits it to the

00:19:11,290 --> 00:19:17,470
channel ends the transaction and that's

00:19:14,680 --> 00:19:19,570
when the upstream sync commits its own

00:19:17,470 --> 00:19:21,810
transactions if if there's anything that

00:19:19,570 --> 00:19:24,730
goes wrong in this picture you know

00:19:21,810 --> 00:19:26,860
network failure disk full channel

00:19:24,730 --> 00:19:28,360
capacity exhausted that transaction the

00:19:26,860 --> 00:19:30,220
downstream transaction will fail the

00:19:28,360 --> 00:19:31,540
transient transaction fails the upstream

00:19:30,220 --> 00:19:34,270
transaction will fail your data doesn't

00:19:31,540 --> 00:19:36,130
go anywhere so that hop is is a

00:19:34,270 --> 00:19:39,400
guaranteed transactional exchange

00:19:36,130 --> 00:19:42,850
so this basically ends up you know

00:19:39,400 --> 00:19:45,460
forming that the backbone or how the

00:19:42,850 --> 00:19:48,430
system how the pipeline's work so with

00:19:45,460 --> 00:19:50,650
that I'm gonna shift gears focus on how

00:19:48,430 --> 00:19:51,430
do you plan and size your deployment so

00:19:50,650 --> 00:19:54,640
what

00:19:51,430 --> 00:19:56,560
talk about aggregation use case and I

00:19:54,640 --> 00:19:58,960
think I've mentioned in the passing what

00:19:56,560 --> 00:20:00,970
the add up solutions could be so here's

00:19:58,960 --> 00:20:02,890
here's an example of you got three web

00:20:00,970 --> 00:20:06,040
servers your you know upcoming startup

00:20:02,890 --> 00:20:07,510
you you launched your thing and you know

00:20:06,040 --> 00:20:09,790
you have this little cluster deployed

00:20:07,510 --> 00:20:11,380
somewhere you'd like to do machine

00:20:09,790 --> 00:20:12,970
learning or like data mining whatever

00:20:11,380 --> 00:20:14,410
you want to do in the logs here's your

00:20:12,970 --> 00:20:17,140
use case you've got you know three

00:20:14,410 --> 00:20:20,680
servers and you can code up like you

00:20:17,140 --> 00:20:23,020
know maybe shell scripts to do it but if

00:20:20,680 --> 00:20:26,260
you were to actually use a flume agent

00:20:23,020 --> 00:20:28,600
it gives you many advantages one if your

00:20:26,260 --> 00:20:31,270
cluster has any downtime you don't have

00:20:28,600 --> 00:20:33,640
to fill up the disks on the servers so

00:20:31,270 --> 00:20:37,900
that that's an important ice insulating

00:20:33,640 --> 00:20:41,140
factor it also allows you to quickly

00:20:37,900 --> 00:20:43,210
offset things off load the data into the

00:20:41,140 --> 00:20:45,730
flume agent and you know you you would

00:20:43,210 --> 00:20:47,680
not run up into a situation where the

00:20:45,730 --> 00:20:50,260
disks on your web servers are filling up

00:20:47,680 --> 00:20:52,330
so if out of these three web servers one

00:20:50,260 --> 00:20:55,390
of them is the most popular because of a

00:20:52,330 --> 00:20:58,720
bug in your load balancing mechanism you

00:20:55,390 --> 00:21:00,460
know chances are flume will be able to

00:20:58,720 --> 00:21:02,410
scale specifically to consume the

00:21:00,460 --> 00:21:06,420
excessive load coming in from that one

00:21:02,410 --> 00:21:09,040
place and and provide you that that

00:21:06,420 --> 00:21:10,300
shock absorbing capacities this is the

00:21:09,040 --> 00:21:13,030
part that you know it's very

00:21:10,300 --> 00:21:15,580
characteristic of characteristic of how

00:21:13,030 --> 00:21:18,280
flume operates and that it's designed to

00:21:15,580 --> 00:21:19,600
deal with the impedance mismatch between

00:21:18,280 --> 00:21:21,630
the upstream and the downstream and

00:21:19,600 --> 00:21:24,310
we'll talk about that in a little bit

00:21:21,630 --> 00:21:26,440
you also make sure that you're utilizing

00:21:24,310 --> 00:21:30,670
your network in in a more effective way

00:21:26,440 --> 00:21:32,320
if you go with limb so that was just

00:21:30,670 --> 00:21:33,910
with one flume agent if you were to add

00:21:32,320 --> 00:21:37,000
another flume aid and now you have more

00:21:33,910 --> 00:21:38,860
capacity you have redundancy you'd be

00:21:37,000 --> 00:21:41,350
able to failover in case this particular

00:21:38,860 --> 00:21:42,760
flume agent is you know has filled up

00:21:41,350 --> 00:21:43,900
its own channels and the downstream is

00:21:42,760 --> 00:21:46,120
not available so now you've got two

00:21:43,900 --> 00:21:47,470
places where you could go and you could

00:21:46,120 --> 00:21:50,470
you know layer them one after the other

00:21:47,470 --> 00:21:52,690
- it's a matter of like making the right

00:21:50,470 --> 00:21:57,280
choices which matter to your particular

00:21:52,690 --> 00:21:59,740
use case you could also route things you

00:21:57,280 --> 00:22:01,600
know if you go from three servers to 30

00:21:59,740 --> 00:22:02,800
servers and now you have a problem where

00:22:01,600 --> 00:22:04,780
like there's parts of your application

00:22:02,800 --> 00:22:05,340
that deal with you know one class of

00:22:04,780 --> 00:22:06,990
yours

00:22:05,340 --> 00:22:09,269
in another part of the application deals

00:22:06,990 --> 00:22:11,970
and other class of users you could you

00:22:09,269 --> 00:22:13,590
could through routing across flume

00:22:11,970 --> 00:22:14,159
pipelines you could address where they

00:22:13,590 --> 00:22:17,129
need to go

00:22:14,159 --> 00:22:19,320
so a typical converging flow is what

00:22:17,129 --> 00:22:21,870
that you know use case comes down to and

00:22:19,320 --> 00:22:25,710
and this is sort of a general high-level

00:22:21,870 --> 00:22:27,659
view of how you know you would lay

00:22:25,710 --> 00:22:30,870
things out you have any number of

00:22:27,659 --> 00:22:32,539
clients talking to any number of agents

00:22:30,870 --> 00:22:36,629
which talk doing more and more agents

00:22:32,539 --> 00:22:39,119
and eventually the last agent set will

00:22:36,629 --> 00:22:42,869
deliver it to the destination so these

00:22:39,119 --> 00:22:47,340
agents these these agent areas are you

00:22:42,869 --> 00:22:52,769
know informally called tears right you

00:22:47,340 --> 00:22:54,990
would you would basically have an outer

00:22:52,769 --> 00:22:59,309
tier of agents you know which is dealing

00:22:54,990 --> 00:23:01,559
directly with the clients and as these

00:22:59,309 --> 00:23:04,950
tears converge the number of agents will

00:23:01,559 --> 00:23:07,440
reduce and as these number of agents are

00:23:04,950 --> 00:23:09,210
reducing you know their capacity keeps

00:23:07,440 --> 00:23:11,519
going up and up so so a typical

00:23:09,210 --> 00:23:14,190
schematic is this so you have like you

00:23:11,519 --> 00:23:15,629
know once you're like fortune 500

00:23:14,190 --> 00:23:18,049
company after you start up experience

00:23:15,629 --> 00:23:21,059
you have this huge number of of

00:23:18,049 --> 00:23:22,559
outermost tear agents and there's a lot

00:23:21,059 --> 00:23:25,350
of clients writing to those agents and

00:23:22,559 --> 00:23:27,090
then they converge they aggregate into

00:23:25,350 --> 00:23:29,749
second tier and the second tier

00:23:27,090 --> 00:23:32,399
aggregates into third tier

00:23:29,749 --> 00:23:35,129
well we talk about you know why would

00:23:32,399 --> 00:23:36,659
you want to even do this but it suffice

00:23:35,129 --> 00:23:38,399
to say that that that you know this is

00:23:36,659 --> 00:23:40,259
not the only way it can be done there

00:23:38,399 --> 00:23:42,389
are many different ways i I just picked

00:23:40,259 --> 00:23:45,779
one use case and and went after that to

00:23:42,389 --> 00:23:48,419
set an example so the event volume in

00:23:45,779 --> 00:23:50,129
these tiers is different your are almost

00:23:48,419 --> 00:23:51,330
years like the hundreds of nodes the

00:23:50,129 --> 00:23:53,369
hundreds of agents that are running

00:23:51,330 --> 00:23:57,210
closest to where the data is being

00:23:53,369 --> 00:23:59,190
produced from will see the minimum the

00:23:57,210 --> 00:24:01,289
data volume the event fall in flow and

00:23:59,190 --> 00:24:04,049
as it goes into the aggregation tiers

00:24:01,289 --> 00:24:06,029
the amount of data that the aggregation

00:24:04,049 --> 00:24:08,190
tiers dealing with goes up a little bit

00:24:06,029 --> 00:24:11,100
and then up and up eventually you know

00:24:08,190 --> 00:24:13,409
leading to the terminal tier but you

00:24:11,100 --> 00:24:15,960
know if you do the math they all add up

00:24:13,409 --> 00:24:18,240
so all the data that is being produced

00:24:15,960 --> 00:24:19,270
at the outermost here is exactly

00:24:18,240 --> 00:24:21,040
constant

00:24:19,270 --> 00:24:22,750
you know across all the tears because

00:24:21,040 --> 00:24:25,090
all of this data is going it's just a

00:24:22,750 --> 00:24:28,690
matter of how many agencies is divided

00:24:25,090 --> 00:24:31,420
in two and that's basically the basis

00:24:28,690 --> 00:24:32,650
that's that the fundamental premise on

00:24:31,420 --> 00:24:34,780
how you would plan in size or

00:24:32,650 --> 00:24:37,750
deployments like you you know you want

00:24:34,780 --> 00:24:41,530
to strive for the steady state so the

00:24:37,750 --> 00:24:43,780
topology planning has these things to be

00:24:41,530 --> 00:24:45,310
considered one number of points of

00:24:43,780 --> 00:24:47,770
origination of events like the number of

00:24:45,310 --> 00:24:50,320
your end data nodes or whatever you know

00:24:47,770 --> 00:24:51,940
web servers and such where do they

00:24:50,320 --> 00:24:54,790
actually want where do you actually want

00:24:51,940 --> 00:24:56,890
to send these events to these two given

00:24:54,790 --> 00:24:58,720
factors what you need to find out

00:24:56,890 --> 00:25:01,390
through your topology and and and and

00:24:58,720 --> 00:25:03,940
and deployment planning is how can you

00:25:01,390 --> 00:25:06,820
cushion the spike offload that happen on

00:25:03,940 --> 00:25:08,740
your peripheral tiers and make sure that

00:25:06,820 --> 00:25:10,390
you know you're not bringing down your

00:25:08,740 --> 00:25:12,190
production systems your SD FS or

00:25:10,390 --> 00:25:13,990
whatever by having this like 9 a.m.

00:25:12,190 --> 00:25:17,350
spike that happens because everybody

00:25:13,990 --> 00:25:19,060
opens up their browsers and also what

00:25:17,350 --> 00:25:20,920
kind of maintenance down times are you

00:25:19,060 --> 00:25:23,830
gonna need you know how do you project

00:25:20,920 --> 00:25:25,360
that and and how you can actually plan

00:25:23,830 --> 00:25:26,710
for it so that flume continues to

00:25:25,360 --> 00:25:32,200
operate while you're downstream systems

00:25:26,710 --> 00:25:35,380
are not available so identifying the

00:25:32,200 --> 00:25:37,300
number of tiers you know what I briefly

00:25:35,380 --> 00:25:38,500
talked about it like each of these tiers

00:25:37,300 --> 00:25:42,370
are aggregating more and more

00:25:38,500 --> 00:25:44,230
information you could add specific logic

00:25:42,370 --> 00:25:45,510
in these tiers for handling load

00:25:44,230 --> 00:25:48,250
balancing and failover

00:25:45,510 --> 00:25:49,480
you know if you're well that's that's a

00:25:48,250 --> 00:25:52,600
very typical use case we see in the

00:25:49,480 --> 00:25:56,590
field very often and then you also

00:25:52,600 --> 00:25:58,690
realize that the the more volume of data

00:25:56,590 --> 00:26:02,470
that each of these agents and different

00:25:58,690 --> 00:26:04,090
tiers handle that you're utilizing the

00:26:02,470 --> 00:26:07,240
network better and you're utilizing the

00:26:04,090 --> 00:26:09,370
i/o capacity of your systems better as

00:26:07,240 --> 00:26:11,110
an example you know file channel which

00:26:09,370 --> 00:26:13,300
provides persistence of your events you

00:26:11,110 --> 00:26:15,030
know while while they exist on on the

00:26:13,300 --> 00:26:18,220
pipeline in their transient state

00:26:15,030 --> 00:26:20,740
heavily relies on F sinks disk level F

00:26:18,220 --> 00:26:22,960
sinks in order to provide transactional

00:26:20,740 --> 00:26:24,520
guarantees so if you have a larger batch

00:26:22,960 --> 00:26:27,640
size that goes into the final channel

00:26:24,520 --> 00:26:33,650
you have less F things

00:26:27,640 --> 00:26:35,750
so here's a rule of thumb you know when

00:26:33,650 --> 00:26:37,309
you're sizing your your topology we'll

00:26:35,750 --> 00:26:42,890
be planning your topology you want to

00:26:37,309 --> 00:26:45,590
take the outermost number of client you

00:26:42,890 --> 00:26:48,140
know points of origin and then apply a

00:26:45,590 --> 00:26:50,510
ratio like this like 4 to 16 and it

00:26:48,140 --> 00:26:52,549
doesn't have to be just 4 to 16 in some

00:26:50,510 --> 00:26:54,650
cases it they're the many factors that

00:26:52,549 --> 00:26:57,350
affect this like if your servers are

00:26:54,650 --> 00:27:00,679
generating like a few kilobytes per

00:26:57,350 --> 00:27:02,600
minute you know it's not much you know

00:27:00,679 --> 00:27:05,929
maybe maybe a few kilobytes per hour you

00:27:02,600 --> 00:27:09,890
you probably can do with having a ratio

00:27:05,929 --> 00:27:11,720
of hundred to one but if you're your

00:27:09,890 --> 00:27:13,460
servers are producing a large volume

00:27:11,720 --> 00:27:15,049
like you know a few megabytes a minute

00:27:13,460 --> 00:27:16,790
that's when you know you need to figure

00:27:15,049 --> 00:27:18,140
out whether 4 to 16 kind of makes sense

00:27:16,790 --> 00:27:19,520
for you some I'm just going to work with

00:27:18,140 --> 00:27:22,000
that number and that's on an average

00:27:19,520 --> 00:27:25,790
case with what you see from tier to tier

00:27:22,000 --> 00:27:30,650
you know ratio in a typical aggregation

00:27:25,790 --> 00:27:32,660
flow the kind of factors you will fold

00:27:30,650 --> 00:27:34,130
in into that planning is whether you

00:27:32,660 --> 00:27:36,020
want load balancing whether you won't

00:27:34,130 --> 00:27:37,370
fail over and if there's any contextual

00:27:36,020 --> 00:27:39,919
rounding and I'm not going to explore in

00:27:37,370 --> 00:27:42,470
detail what these things mean I would

00:27:39,919 --> 00:27:44,929
suggest if you ever try this by yourself

00:27:42,470 --> 00:27:47,510
please go ahead and disregard these

00:27:44,929 --> 00:27:50,360
factors and and and form your topology

00:27:47,510 --> 00:27:51,830
lay that out see how that works and then

00:27:50,360 --> 00:27:53,210
do a secondary analysis to see how you

00:27:51,830 --> 00:27:57,410
can do failover and load balancing at

00:27:53,210 --> 00:28:00,500
Oxford but basically using this rule of

00:27:57,410 --> 00:28:01,910
thumb you know let's say starting with

00:28:00,500 --> 00:28:05,169
an example of hundred web servers

00:28:01,910 --> 00:28:09,470
bumping in into say a destination HDFS

00:28:05,169 --> 00:28:10,880
cluster and using the one is 216 ratio

00:28:09,470 --> 00:28:12,980
for the outer tier you would you would

00:28:10,880 --> 00:28:16,429
basically get about seven tier one

00:28:12,980 --> 00:28:18,650
agents and then there are seven tier one

00:28:16,429 --> 00:28:20,540
agents you know you reduce this ratio

00:28:18,650 --> 00:28:23,450
for the next year bring it to one to

00:28:20,540 --> 00:28:24,770
four you would basically get to tier two

00:28:23,450 --> 00:28:27,590
agents and that's basically your

00:28:24,770 --> 00:28:28,970
topology for this use case so on the

00:28:27,590 --> 00:28:30,320
diagram it kind of looks like this you

00:28:28,970 --> 00:28:33,230
have sixteen web servers writing to the

00:28:30,320 --> 00:28:35,840
first agent agent one is all the agents

00:28:33,230 --> 00:28:39,570
in the same tier and the Tier one which

00:28:35,840 --> 00:28:42,360
then sends the aggregated information

00:28:39,570 --> 00:28:47,190
- agent - now this kind of exposes a

00:28:42,360 --> 00:28:49,260
real you know configuration requirement

00:28:47,190 --> 00:28:51,810
here which is in steady state all of

00:28:49,260 --> 00:28:55,620
that information that is coming in to

00:28:51,810 --> 00:28:57,900
say agent one should be exiting that

00:28:55,620 --> 00:29:00,120
agent one at the same state at the same

00:28:57,900 --> 00:29:01,890
rate in order for you to have

00:29:00,120 --> 00:29:04,320
equilibrium in in order for you to have

00:29:01,890 --> 00:29:07,230
steady state which basically translates

00:29:04,320 --> 00:29:10,740
to what we call the bat size on on the

00:29:07,230 --> 00:29:12,720
sings so your sources work with the

00:29:10,740 --> 00:29:14,820
upstream bat size you know whatever the

00:29:12,720 --> 00:29:17,010
client bat sizes doesn't matter whatever

00:29:14,820 --> 00:29:19,020
is coming in is coming in but assuming

00:29:17,010 --> 00:29:23,130
you know the clients for the sake of

00:29:19,020 --> 00:29:24,900
argument are sending maybe hundred

00:29:23,130 --> 00:29:27,870
events per cycle and the cycle could be

00:29:24,900 --> 00:29:29,130
like one sent one network home so if

00:29:27,870 --> 00:29:31,710
they're sending hundred events per cycle

00:29:29,130 --> 00:29:34,050
and there's sixteen you know clients

00:29:31,710 --> 00:29:35,760
writing to that agent you got sixteen

00:29:34,050 --> 00:29:37,290
hundred events first cycle coming in

00:29:35,760 --> 00:29:38,640
that should be the batch size for your

00:29:37,290 --> 00:29:41,430
exiting node

00:29:38,640 --> 00:29:44,130
so when agent one writes to agent two in

00:29:41,430 --> 00:29:47,640
that previous diagram the bat size that

00:29:44,130 --> 00:29:49,530
uses sixteen hundred so in one cycle all

00:29:47,640 --> 00:29:53,010
the events that came in to that agent

00:29:49,530 --> 00:29:55,410
have exerted the agent and the again

00:29:53,010 --> 00:29:58,590
fundamental requirement for you to plan

00:29:55,410 --> 00:30:00,420
your steady state if the bat sighs you

00:29:58,590 --> 00:30:03,570
know if the total ingest rate is way

00:30:00,420 --> 00:30:04,650
more than 2500 yes you have to break it

00:30:03,570 --> 00:30:08,220
down into multiple things and they just

00:30:04,650 --> 00:30:10,980
add up and then similarly you could do

00:30:08,220 --> 00:30:12,660
the sync bat size for you know the agent

00:30:10,980 --> 00:30:16,530
- I'm sorry I'm running out of times out

00:30:12,660 --> 00:30:18,600
to rush this the the next thing is you

00:30:16,530 --> 00:30:21,540
want to make sure you have enough

00:30:18,600 --> 00:30:22,980
failure tolerance capacity so your file

00:30:21,540 --> 00:30:24,600
channel or your memory channel whatever

00:30:22,980 --> 00:30:28,830
channels you're using it should have

00:30:24,600 --> 00:30:31,530
enough capacity to hold on to all these

00:30:28,830 --> 00:30:33,270
these event batches that are coming in

00:30:31,530 --> 00:30:35,250
in case of downstream failures and and

00:30:33,270 --> 00:30:36,720
you would basically do a very simple

00:30:35,250 --> 00:30:39,390
back of the envelope calculation to

00:30:36,720 --> 00:30:42,150
figure out if this is my expected worst

00:30:39,390 --> 00:30:45,510
case rate and I need to be down for two

00:30:42,150 --> 00:30:47,220
hours how many events must I store in

00:30:45,510 --> 00:30:49,650
the channel and that gives you the

00:30:47,220 --> 00:30:51,180
channel size if it's a file channel we

00:30:49,650 --> 00:30:52,930
encourage that you you know you use

00:30:51,180 --> 00:30:54,430
multiple different spindles

00:30:52,930 --> 00:30:58,930
that increases the throughput of the

00:30:54,430 --> 00:31:01,030
file channel so here's here's a a graph

00:30:58,930 --> 00:31:03,280
of how the channel capacity changes you

00:31:01,030 --> 00:31:06,040
know in cases of production and you have

00:31:03,280 --> 00:31:08,550
downstream failures so the first part

00:31:06,040 --> 00:31:10,960
you know you would see the channel size

00:31:08,550 --> 00:31:13,270
sort of fluctuating up and down up and

00:31:10,960 --> 00:31:15,340
down so pretty normal pretty healthy

00:31:13,270 --> 00:31:17,530
the important thing is that it keeps

00:31:15,340 --> 00:31:19,300
coming down it keeps coming to like

00:31:17,530 --> 00:31:20,680
close to zero and that's great

00:31:19,300 --> 00:31:22,600
but then if you at the moment you have a

00:31:20,680 --> 00:31:25,150
downstream failure the channel starts

00:31:22,600 --> 00:31:27,940
backing up and I'll back up to the point

00:31:25,150 --> 00:31:30,010
where it hits its capacity and this is

00:31:27,940 --> 00:31:31,930
the window you know from the steady

00:31:30,010 --> 00:31:34,810
state to the hitting of its capacity

00:31:31,930 --> 00:31:36,820
limit that's the window you plan for and

00:31:34,810 --> 00:31:39,070
if in that window you can address any

00:31:36,820 --> 00:31:40,510
downstream failures that's cool because

00:31:39,070 --> 00:31:42,520
then the system will come back to normal

00:31:40,510 --> 00:31:44,350
but in this particular graph that

00:31:42,520 --> 00:31:47,500
planning was done in such a way that you

00:31:44,350 --> 00:31:49,630
know that the failure was took much

00:31:47,500 --> 00:31:51,430
longer to restore itself so that's why

00:31:49,630 --> 00:31:52,960
you see that plateau on the top which is

00:31:51,430 --> 00:31:55,900
the channel is full and when the channel

00:31:52,960 --> 00:31:57,190
is full this agent has failed but then

00:31:55,900 --> 00:31:58,570
if the downstream agent becomes

00:31:57,190 --> 00:31:59,740
available again you know the

00:31:58,570 --> 00:32:04,590
communication is restored and the

00:31:59,740 --> 00:32:07,090
channel starts draining up again lastly

00:32:04,590 --> 00:32:09,400
the way you would size your hardware is

00:32:07,090 --> 00:32:13,060
again starting with the rule of thumb

00:32:09,400 --> 00:32:16,690
that the number of cores that you need

00:32:13,060 --> 00:32:17,920
on the agent box is equal to half of the

00:32:16,690 --> 00:32:20,290
total number of sources and sinks

00:32:17,920 --> 00:32:23,170
combined now sources are multi threaded

00:32:20,290 --> 00:32:24,880
by nature by default they they they are

00:32:23,170 --> 00:32:27,850
designed in a way that they will scale

00:32:24,880 --> 00:32:30,490
up to any capacity capacity that they

00:32:27,850 --> 00:32:32,170
need to where as things are single

00:32:30,490 --> 00:32:34,210
threaded and that's why you know the

00:32:32,170 --> 00:32:37,510
idea of using multiple sinks in case the

00:32:34,210 --> 00:32:41,950
batch sizes are large and the reason is

00:32:37,510 --> 00:32:43,270
that one one one major aspect of my

00:32:41,950 --> 00:32:45,010
thing I'd mentioned this in the passing

00:32:43,270 --> 00:32:46,450
earlier one major aspect is the

00:32:45,010 --> 00:32:48,580
impedance mismatch you know you have

00:32:46,450 --> 00:32:50,410
like huge spike of load happening on

00:32:48,580 --> 00:32:52,210
your peripheral tiers you don't want

00:32:50,410 --> 00:32:54,370
that to be that Joel to be translated

00:32:52,210 --> 00:32:57,370
directly on your production edge base or

00:32:54,370 --> 00:32:59,080
whatever that is so when you start up

00:32:57,370 --> 00:33:01,240
with this rule of thumb you're able to

00:32:59,080 --> 00:33:04,120
you know you might think oh this is kind

00:33:01,240 --> 00:33:06,210
of wasteful because I'm like I'm having

00:33:04,120 --> 00:33:08,040
too much computation capacity and not

00:33:06,210 --> 00:33:09,840
effectively utilizing it that's not

00:33:08,040 --> 00:33:14,790
entirely true depending upon which part

00:33:09,840 --> 00:33:15,990
of your pipeline you're in obviously if

00:33:14,790 --> 00:33:17,610
you're using the memory channel there

00:33:15,990 --> 00:33:18,570
are special requirements you know if

00:33:17,610 --> 00:33:19,590
you're using the file channel the

00:33:18,570 --> 00:33:22,530
special requirements you must have

00:33:19,590 --> 00:33:27,690
enough space on either in the memory or

00:33:22,530 --> 00:33:31,820
in the desk to keep all of that data so

00:33:27,690 --> 00:33:36,780
the most important thing and flume

00:33:31,820 --> 00:33:39,090
deployment is stage testing regardless

00:33:36,780 --> 00:33:41,880
all of what I've described here are just

00:33:39,090 --> 00:33:45,660
based on empirical you know evidence

00:33:41,880 --> 00:33:47,880
that we've gathered from the field every

00:33:45,660 --> 00:33:51,480
deployment is different every use case

00:33:47,880 --> 00:33:52,980
is different every characteristic of you

00:33:51,480 --> 00:33:55,410
know every floor that lumens ever

00:33:52,980 --> 00:33:57,720
touched is different there I cannot

00:33:55,410 --> 00:34:01,710
emphasize enough the need for you to do

00:33:57,720 --> 00:34:03,150
this you must ensure that you project

00:34:01,710 --> 00:34:05,160
you know you've a be grounded in your

00:34:03,150 --> 00:34:07,200
projections like yes I want my business

00:34:05,160 --> 00:34:09,270
to grow hundred times in this year but

00:34:07,200 --> 00:34:10,920
that's not what I would plan for like

00:34:09,270 --> 00:34:13,710
hundred times capacity is not needed

00:34:10,920 --> 00:34:16,230
that's me really really wasteful but

00:34:13,710 --> 00:34:19,290
that said you know if I project my

00:34:16,230 --> 00:34:21,150
business to go like five percent that's

00:34:19,290 --> 00:34:23,520
that's a reasonable projection and and

00:34:21,150 --> 00:34:25,140
yes I will plan for that extra five

00:34:23,520 --> 00:34:26,760
percent and I'll make sure that oh my

00:34:25,140 --> 00:34:28,590
you know sizing into quality planning is

00:34:26,760 --> 00:34:30,810
done with that in mind and when I start

00:34:28,590 --> 00:34:32,880
seeing that's about to hit that limit I

00:34:30,810 --> 00:34:35,130
will revisit this and expand the

00:34:32,880 --> 00:34:38,370
pipelines expand the the conversion

00:34:35,130 --> 00:34:40,440
flows but it's extremely important that

00:34:38,370 --> 00:34:42,480
you make these projections and these be

00:34:40,440 --> 00:34:45,030
grounded projections and and be able to

00:34:42,480 --> 00:34:48,600
test them out make sure this this plan

00:34:45,030 --> 00:34:51,720
topology deployment meets those

00:34:48,600 --> 00:34:53,340
requirements and you know once once you

00:34:51,720 --> 00:34:55,320
move it into production

00:34:53,340 --> 00:34:56,910
don't just let that go I mean you'd have

00:34:55,320 --> 00:34:58,950
to monitor it for a little while before

00:34:56,910 --> 00:35:00,750
it stabilizes and you are comfortable

00:34:58,950 --> 00:35:03,120
but then you understand what your flow

00:35:00,750 --> 00:35:04,770
characteristics are and once you're on

00:35:03,120 --> 00:35:05,610
you know at that point where you

00:35:04,770 --> 00:35:07,650
understand what your flow

00:35:05,610 --> 00:35:09,150
characteristics are and and you you you

00:35:07,650 --> 00:35:11,340
have done enough testing to ensure that

00:35:09,150 --> 00:35:14,750
this topology this deployments capable

00:35:11,340 --> 00:35:18,390
of handling it it kind of runs itself

00:35:14,750 --> 00:35:19,250
and you know until of course the time

00:35:18,390 --> 00:35:22,280
when you're low

00:35:19,250 --> 00:35:25,250
becomes much more and you know for

00:35:22,280 --> 00:35:28,400
whatever reasons you need to go revisit

00:35:25,250 --> 00:35:31,850
this topology if you have any questions

00:35:28,400 --> 00:35:34,490
any any need for like any clarifications

00:35:31,850 --> 00:35:36,530
or you're running into any problems

00:35:34,490 --> 00:35:39,080
doing any of this do not hesitate

00:35:36,530 --> 00:35:42,230
Dapeng us it's a very active community

00:35:39,080 --> 00:35:43,690
the Apache affluent community we take

00:35:42,230 --> 00:35:46,760
pride in the fact that we've seen a lot

00:35:43,690 --> 00:35:50,710
lots of deployments in early or very

00:35:46,760 --> 00:35:53,390
early stage of our you know completely

00:35:50,710 --> 00:35:55,580
round up implementation of flume what is

00:35:53,390 --> 00:35:58,700
now flume one point acts it's the latest

00:35:55,580 --> 00:36:00,170
version one-30 strongly encourage you to

00:35:58,700 --> 00:36:03,440
go try it out and if there's anything

00:36:00,170 --> 00:36:12,920
please let us know that's about it from

00:36:03,440 --> 00:36:16,670
my side of any questions you can have

00:36:12,920 --> 00:36:19,880
the mic seems really really

00:36:16,670 --> 00:36:24,470
time-sensitive thing is getting from the

00:36:19,880 --> 00:36:27,140
source to the very first tear agent and

00:36:24,470 --> 00:36:30,860
then the subsequent tears I would think

00:36:27,140 --> 00:36:33,290
are not as time-sensitive can you kind

00:36:30,860 --> 00:36:34,790
of fill me in what how if I'm thinking

00:36:33,290 --> 00:36:36,370
about this sort of the right way it

00:36:34,790 --> 00:36:40,490
seems like everything is happening

00:36:36,370 --> 00:36:41,960
synchronously no so every agent you know

00:36:40,490 --> 00:36:45,020
they're they're two active components in

00:36:41,960 --> 00:36:46,730
the agent the sources active on its own

00:36:45,020 --> 00:36:48,440
the sink is active on its own the

00:36:46,730 --> 00:36:50,000
channel acts as a buffer so they're

00:36:48,440 --> 00:36:52,460
basically asynchronous this whole

00:36:50,000 --> 00:36:55,760
pipeline is hop two hop asynchronous

00:36:52,460 --> 00:36:57,710
transfer now if you want to keep it as

00:36:55,760 --> 00:37:00,020
close to real time as you want

00:36:57,710 --> 00:37:01,730
you need to plan very carefully to make

00:37:00,020 --> 00:37:04,010
sure that there are no interruptions in

00:37:01,730 --> 00:37:05,240
this flow you know you have enough

00:37:04,010 --> 00:37:07,160
capacity you have enough hardware

00:37:05,240 --> 00:37:09,710
capacity for the kind of projected flow

00:37:07,160 --> 00:37:11,780
volume that that you're expecting and

00:37:09,710 --> 00:37:15,380
once you do that you can keep the

00:37:11,780 --> 00:37:17,810
end-to-end delivery times minimal but if

00:37:15,380 --> 00:37:21,320
anytime there is a failure in the system

00:37:17,810 --> 00:37:23,780
the channels will back up and it's only

00:37:21,320 --> 00:37:25,310
when that failures are stirred that that

00:37:23,780 --> 00:37:29,390
the channels will start draining again

00:37:25,310 --> 00:37:32,710
and that will introduce latency that's

00:37:29,390 --> 00:37:32,710
just how the system is

00:37:33,400 --> 00:37:38,599
so I have two questions first question

00:37:36,020 --> 00:37:40,460
is can we deploy flume in such a way

00:37:38,599 --> 00:37:42,890
that your tier 2 and tier 3 are in

00:37:40,460 --> 00:37:45,380
separate data centers and job perfectly

00:37:42,890 --> 00:37:50,859
like far so say tier 2 is in London and

00:37:45,380 --> 00:37:53,540
tier 3 is in California so typically it

00:37:50,859 --> 00:37:55,940
yes the question was can you deploy

00:37:53,540 --> 00:37:57,970
flume in such a way that different tiers

00:37:55,940 --> 00:38:00,079
are in different geographical locations

00:37:57,970 --> 00:38:01,820
or maybe different different data

00:38:00,079 --> 00:38:06,050
centers altogether

00:38:01,820 --> 00:38:10,070
the answer is yes although you you

00:38:06,050 --> 00:38:11,930
definitely want to ensure that the

00:38:10,070 --> 00:38:13,820
communication links between those tiers

00:38:11,930 --> 00:38:16,310
that span geographical boundaries or

00:38:13,820 --> 00:38:18,320
data center boundaries are minimal so

00:38:16,310 --> 00:38:20,630
you probably want to have a lot of

00:38:18,320 --> 00:38:23,030
aggregation happen right if within your

00:38:20,630 --> 00:38:26,270
first set of Tears before it gets piped

00:38:23,030 --> 00:38:31,329
over to the next and the second question

00:38:26,270 --> 00:38:33,770
is do you have checksumming and you know

00:38:31,329 --> 00:38:36,800
compression in between there is a juror

00:38:33,770 --> 00:38:40,790
for checksum interceptor but that said

00:38:36,800 --> 00:38:43,339
the exchange in the exchange for event

00:38:40,790 --> 00:38:46,130
data is transactional so we guarantee no

00:38:43,339 --> 00:38:48,680
data loss but at the same time you know

00:38:46,130 --> 00:38:51,140
at the file channel level there's work

00:38:48,680 --> 00:38:53,540
that's being done to to safeguard

00:38:51,140 --> 00:38:56,740
against bit rot for example if the data

00:38:53,540 --> 00:38:56,740
were to sit there for a long time

00:39:03,400 --> 00:39:11,060
so two questions is there an s3 sink the

00:39:08,300 --> 00:39:15,710
question is is there an s3 sink well so

00:39:11,060 --> 00:39:18,950
s3 works very well with you know as one

00:39:15,710 --> 00:39:21,070
of the file systems on HDFS so you could

00:39:18,950 --> 00:39:24,820
technically assume that you know the

00:39:21,070 --> 00:39:28,190
HDFS sink will be capable of writing to

00:39:24,820 --> 00:39:31,220
you know the DFS back by s3 okay but

00:39:28,190 --> 00:39:35,510
there is an open juror and we like

00:39:31,220 --> 00:39:38,960
batches okay the second question was so

00:39:35,510 --> 00:39:42,020
know there was an old flume project and

00:39:38,960 --> 00:39:44,210
there's a new generation flume project

00:39:42,020 --> 00:39:45,440
next generation what is the difference

00:39:44,210 --> 00:39:48,680
between those who what was it necessary

00:39:45,440 --> 00:39:50,690
to actually have flume and gee okay cold

00:39:48,680 --> 00:39:52,220
so so for those who may not be

00:39:50,690 --> 00:39:56,150
up-to-date on the history of the project

00:39:52,220 --> 00:39:58,700
a quick recap flume was deluged at

00:39:56,150 --> 00:40:01,660
Cloudera and it went through multiple

00:39:58,700 --> 00:40:04,490
releases I believe three years ago and

00:40:01,660 --> 00:40:06,170
at some point in time I think a year and

00:40:04,490 --> 00:40:09,260
a half ago we we decided to move it to

00:40:06,170 --> 00:40:11,330
incubation in Apache that's when flume

00:40:09,260 --> 00:40:14,000
came into the incubator and close to a

00:40:11,330 --> 00:40:16,250
year later it graduated the code base

00:40:14,000 --> 00:40:19,040
that came into Apache was that a flume

00:40:16,250 --> 00:40:22,670
point 9 X which is also referred to

00:40:19,040 --> 00:40:24,140
aslam OG but then you know over the

00:40:22,670 --> 00:40:26,270
course of these multiple releases that

00:40:24,140 --> 00:40:28,760
we saw through cloud or as experience in

00:40:26,270 --> 00:40:30,680
the field we realized that quite a few

00:40:28,760 --> 00:40:32,090
assumptions and design changes necessary

00:40:30,680 --> 00:40:34,010
in order to address the kind of

00:40:32,090 --> 00:40:36,200
requirements that film was being subject

00:40:34,010 --> 00:40:39,020
to in the field so we did a complete

00:40:36,200 --> 00:40:40,850
rewrite and that rewrite became what was

00:40:39,020 --> 00:40:43,160
called flume and G that was the first

00:40:40,850 --> 00:40:45,170
release be made through Apache the flume

00:40:43,160 --> 00:40:46,670
1.0 was the first release that point 9

00:40:45,170 --> 00:40:49,040
was never released to Apache it's a

00:40:46,670 --> 00:40:52,880
cloud era product you've still there you

00:40:49,040 --> 00:40:54,740
can try it out our active development is

00:40:52,880 --> 00:40:57,980
entirely based on flume one point X line

00:40:54,740 --> 00:41:00,009
and that has seen deployment in many

00:40:57,980 --> 00:41:06,049
many

00:41:00,009 --> 00:41:09,799
consumer sites alright so no more

00:41:06,049 --> 00:41:12,319
questions I have one you know a plug to

00:41:09,799 --> 00:41:15,650
highlight which is HBase Khan happening

00:41:12,319 --> 00:41:18,410
in on June 13 I believe call for papers

00:41:15,650 --> 00:41:21,230
open please share experiences with HBase

00:41:18,410 --> 00:41:25,519
I'm actually looking forward to one of

00:41:21,230 --> 00:41:27,499
our committers talk about how flume

00:41:25,519 --> 00:41:28,549
plugs into HBase and and it's something

00:41:27,499 --> 00:41:31,700
that a lot of our customers are using

00:41:28,549 --> 00:41:33,529
prevail'd time analysis so hope to see

00:41:31,700 --> 00:41:36,890
you all there as well and there's also a

00:41:33,529 --> 00:41:40,309
scoop talk but Kathleen and myself today

00:41:36,890 --> 00:41:42,230
in this room I think it's set for 15 and

00:41:40,309 --> 00:41:44,900
we'll have a demo it'll be a little bit

00:41:42,230 --> 00:41:47,420
more you know risky because demos hardly

00:41:44,900 --> 00:41:47,900
ever work we'll try it all right see you

00:41:47,420 --> 00:41:50,499
guys later

00:41:47,900 --> 00:41:50,499

YouTube URL: https://www.youtube.com/watch?v=0TNGXpUm97g


