Title: 7 Deadly Hadoop Misconfigurations
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 2
Description: 
	Kathleen Ting
ApacheCon NA 2013
Big Data
Captions: 
	00:00:01,399 --> 00:00:07,140
alright so next speaker is Kathleen ting

00:00:04,620 --> 00:00:10,410
and she's going to speak about seven

00:00:07,140 --> 00:00:15,710
deadly hadoop misconfigurations thank

00:00:10,410 --> 00:00:18,539
you vault our hardware is expensive

00:00:15,710 --> 00:00:21,150
Hadoop is a framework for processing and

00:00:18,539 --> 00:00:25,769
kareem big data on cheap commodity

00:00:21,150 --> 00:00:28,080
hardware that automatically handles data

00:00:25,769 --> 00:00:30,779
replication node failure from two years

00:00:28,080 --> 00:00:34,829
of supporting diverse clusters I've

00:00:30,779 --> 00:00:36,540
learned firsthand a lot of a lot of

00:00:34,829 --> 00:00:38,579
misconfigurations that can break

00:00:36,540 --> 00:00:40,200
clusters so Miss configurations and bugs

00:00:38,579 --> 00:00:43,170
break the most clusters fixing

00:00:40,200 --> 00:00:45,120
misconfigurations is up to you and what

00:00:43,170 --> 00:00:47,219
I what I want to share during this talk

00:00:45,120 --> 00:00:49,020
is what I've learned over the past two

00:00:47,219 --> 00:00:51,149
years of supporting clusters what are

00:00:49,020 --> 00:00:55,829
the most common misconfigurations we see

00:00:51,149 --> 00:00:57,629
that broken clusters I am a scoop

00:00:55,829 --> 00:00:59,070
commander and PFC member and following

00:00:57,629 --> 00:01:02,640
this talk I will be speaking with arvind

00:00:59,070 --> 00:01:06,840
/ bakr on scoop as well where we'll talk

00:01:02,640 --> 00:01:08,640
about scoop too i work at Cloudera as a

00:01:06,840 --> 00:01:12,810
customer operations engineering manager

00:01:08,640 --> 00:01:15,060
and the manager role at Cloudera is more

00:01:12,810 --> 00:01:16,439
of a player-coach model so in addition

00:01:15,060 --> 00:01:18,750
to the people management

00:01:16,439 --> 00:01:21,119
responsibilities I also work a lot of

00:01:18,750 --> 00:01:24,210
the support issues and I'm a subject

00:01:21,119 --> 00:01:28,619
matter expert on on not produced on

00:01:24,210 --> 00:01:31,409
zookeeper flume scoop and the bulk of of

00:01:28,619 --> 00:01:33,360
this talk is it's from the past two

00:01:31,409 --> 00:01:35,460
years of supporting clusters day in and

00:01:33,360 --> 00:01:38,720
day out and just seeing some of the the

00:01:35,460 --> 00:01:40,320
common scenes so specifically around

00:01:38,720 --> 00:01:42,600
miscommunication make it miss

00:01:40,320 --> 00:01:47,520
configuration with memory disk and

00:01:42,600 --> 00:01:52,920
threads for those of you not familiar

00:01:47,520 --> 00:01:55,439
with with MapReduce or with Hadoop the

00:01:52,920 --> 00:01:58,439
best way to explain MapReduce is with an

00:01:55,439 --> 00:02:00,570
example and the best example is the word

00:01:58,439 --> 00:02:03,689
count example so we're count is the

00:02:00,570 --> 00:02:06,869
hello world of Hadoop it's the example

00:02:03,689 --> 00:02:09,209
you run to to make sure your cluster is

00:02:06,869 --> 00:02:11,910
configured correctly I will say though

00:02:09,209 --> 00:02:13,280
that even if word count succeeds that

00:02:11,910 --> 00:02:15,530
doesn't automatically get

00:02:13,280 --> 00:02:18,830
auntie that your cluster is completely

00:02:15,530 --> 00:02:20,600
configured correctly often a somewhat

00:02:18,830 --> 00:02:23,720
innocuous change will break your cluster

00:02:20,600 --> 00:02:26,240
but your cluster was misconfigured from

00:02:23,720 --> 00:02:28,040
the beginning word count you know is it

00:02:26,240 --> 00:02:30,920
meant to test it or robustness and

00:02:28,040 --> 00:02:32,420
scalability of your cluster so it's even

00:02:30,920 --> 00:02:33,950
more reason to make sure you get your

00:02:32,420 --> 00:02:36,650
cluster configured correctly the first

00:02:33,950 --> 00:02:37,970
time because ultimately when you do run

00:02:36,650 --> 00:02:41,569
into issues you don't want to be

00:02:37,970 --> 00:02:43,010
spending time tracing back to what was

00:02:41,569 --> 00:02:46,519
the root cause and it's often hard

00:02:43,010 --> 00:02:48,920
because the first / symptom is often not

00:02:46,519 --> 00:02:52,040
correlated with the root cause of the

00:02:48,920 --> 00:02:55,340
issue and we'll talk more into into that

00:02:52,040 --> 00:02:56,840
on the preceding slides to take

00:02:55,340 --> 00:02:58,220
advantage of the parallel processing

00:02:56,840 --> 00:03:01,730
that Hadoop provides we need to provide

00:02:58,220 --> 00:03:04,370
we need express our query as a MapReduce

00:03:01,730 --> 00:03:06,440
job and Mac reduce works by breaking the

00:03:04,370 --> 00:03:09,950
processing into two phases the map phase

00:03:06,440 --> 00:03:12,680
and the reduced phase each phase has key

00:03:09,950 --> 00:03:14,660
value pairs as input and output so as

00:03:12,680 --> 00:03:16,910
you can see from this example we're

00:03:14,660 --> 00:03:21,950
starting from the last year so we have a

00:03:16,910 --> 00:03:24,200
text file of cities in Oregon so we have

00:03:21,950 --> 00:03:26,590
Eugene Salem Beaverton portland salem

00:03:24,200 --> 00:03:29,480
salem be returned eugene portland and

00:03:26,590 --> 00:03:32,299
what word count does is it takes this

00:03:29,480 --> 00:03:34,190
text file and it counts for you you know

00:03:32,299 --> 00:03:36,110
how many times this Eugene show up in

00:03:34,190 --> 00:03:41,209
this file how many times does Beaverton

00:03:36,110 --> 00:03:43,940
show up in this file etc so from going

00:03:41,209 --> 00:03:46,060
from the left to the right your first

00:03:43,940 --> 00:03:48,769
step is displayed so you're splitting

00:03:46,060 --> 00:03:49,820
your your text file into in two

00:03:48,769 --> 00:03:51,769
different lines right so you've got

00:03:49,820 --> 00:03:53,959
these two three distinct lines from

00:03:51,769 --> 00:03:57,470
there you start the mapping phase where

00:03:53,959 --> 00:04:01,220
you're you're breaking each line into a

00:03:57,470 --> 00:04:04,100
key value pair so Eugene is the key one

00:04:01,220 --> 00:04:06,200
is the value Salem is a key one is value

00:04:04,100 --> 00:04:07,220
and so forth from there you go into

00:04:06,200 --> 00:04:09,620
shuffling where you're grouping

00:04:07,220 --> 00:04:11,209
like-minded together so all of the genes

00:04:09,620 --> 00:04:15,560
go together all the stadiums go to

00:04:11,209 --> 00:04:17,570
gather and etc and then all to reducing

00:04:15,560 --> 00:04:20,329
where you're you're doing the counting

00:04:17,570 --> 00:04:22,210
part right where you look at you seeing

00:04:20,329 --> 00:04:24,860
this file you have two genes so you

00:04:22,210 --> 00:04:27,170
again key value pair right Eugene is the

00:04:24,860 --> 00:04:29,630
key to is the value in this case

00:04:27,170 --> 00:04:33,110
and finally you know you have a sorted

00:04:29,630 --> 00:04:39,710
list listing the word count of each city

00:04:33,110 --> 00:04:42,320
from this text file the agenda for today

00:04:39,710 --> 00:04:43,490
we're going to talk about first we're

00:04:42,320 --> 00:04:45,050
going to set the stage you know why do

00:04:43,490 --> 00:04:45,950
we even care about misconfigurations and

00:04:45,050 --> 00:04:47,870
often people when they hear

00:04:45,950 --> 00:04:49,730
misconfigurations i think well it's

00:04:47,870 --> 00:04:51,530
really something for performance tuning

00:04:49,730 --> 00:04:54,080
or you know worry about that when I have

00:04:51,530 --> 00:04:56,810
like large jobs and you know I want to

00:04:54,080 --> 00:04:59,270
make sure I have enough capacity etc but

00:04:56,810 --> 00:05:00,860
actually it just so happens that if you

00:04:59,270 --> 00:05:02,480
have a Miss configuration you will get

00:05:00,860 --> 00:05:05,450
failed jobs you can bring your cluster

00:05:02,480 --> 00:05:07,670
to its knees with a somewhat simple miss

00:05:05,450 --> 00:05:09,590
configuration the hard part is figuring

00:05:07,670 --> 00:05:11,900
out which miss configuration broker

00:05:09,590 --> 00:05:13,790
cluster and the best way to not be in

00:05:11,900 --> 00:05:15,380
that situation is to correctly configure

00:05:13,790 --> 00:05:16,790
your cluster the first time and so we're

00:05:15,380 --> 00:05:19,250
going to talk about some of the more

00:05:16,790 --> 00:05:21,350
common with configurations and hopefully

00:05:19,250 --> 00:05:25,030
overt a lot of those those needless

00:05:21,350 --> 00:05:25,030
hours spent figuring out what went wrong

00:05:27,910 --> 00:05:32,150
even though the bulk of our tickets that

00:05:31,040 --> 00:05:33,410
we're going to talk about the issues

00:05:32,150 --> 00:05:36,800
rather that we're going to talk about

00:05:33,410 --> 00:05:39,230
will be with MapReduce it's it's not

00:05:36,800 --> 00:05:40,970
that there are no misconfigurations with

00:05:39,230 --> 00:05:43,660
the other components in the Hadoop

00:05:40,970 --> 00:05:46,070
ecosystem of which there are 13 or so

00:05:43,660 --> 00:05:47,780
the reason I'm going to stretch

00:05:46,070 --> 00:05:50,360
MapReduce specifically is because

00:05:47,780 --> 00:05:52,520
MapReduce is central to this system and

00:05:50,360 --> 00:05:55,070
we've discovered that even say if you

00:05:52,520 --> 00:05:58,070
have a hive job so hive is it's a sequel

00:05:55,070 --> 00:06:01,700
like syntax I can use instead of writing

00:05:58,070 --> 00:06:04,700
MapReduce jobs in Java so by using hive

00:06:01,700 --> 00:06:06,590
you just use syntax like queries which

00:06:04,700 --> 00:06:10,160
will then spawn a MapReduce job on your

00:06:06,590 --> 00:06:13,790
behalf so we've discovered hive jobs

00:06:10,160 --> 00:06:16,010
that failed because of a faulty msconfig

00:06:13,790 --> 00:06:18,470
on MapReduce and we'll talk more in

00:06:16,010 --> 00:06:21,890
detail on the the preceding slides as

00:06:18,470 --> 00:06:24,440
you can see from this graph the the bulk

00:06:21,890 --> 00:06:27,320
of the time we've spent debugging issues

00:06:24,440 --> 00:06:29,390
has been in both time and tickets has

00:06:27,320 --> 00:06:33,850
been with MapReduce and this isn't to

00:06:29,390 --> 00:06:36,860
say that you know the other other

00:06:33,850 --> 00:06:38,940
components and Hadoop ecosystem are less

00:06:36,860 --> 00:06:41,190
buggy or less

00:06:38,940 --> 00:06:44,100
miss configuration prone on the contrary

00:06:41,190 --> 00:06:46,260
some more some less mature systems like

00:06:44,100 --> 00:06:49,400
hive and pig we might see more bugs

00:06:46,260 --> 00:06:52,350
right rather than misconfigurations but

00:06:49,400 --> 00:06:54,300
MapReduce is central to system therefore

00:06:52,350 --> 00:06:57,420
it's the hardest to configure there's so

00:06:54,300 --> 00:06:59,220
many knobs in HDFS I in Mac red side and

00:06:57,420 --> 00:07:01,890
coresight Orsola different knobs need a

00:06:59,220 --> 00:07:04,410
tweak and not only that but if you tweak

00:07:01,890 --> 00:07:07,110
one knob you better make sure you tweak

00:07:04,410 --> 00:07:10,680
the other knob too because they are

00:07:07,110 --> 00:07:12,660
dependent and it's hard as you know a

00:07:10,680 --> 00:07:14,400
new administrator to a Hadoop cluster

00:07:12,660 --> 00:07:16,500
it's hard to immediately know

00:07:14,400 --> 00:07:18,150
intuitively well which are the

00:07:16,500 --> 00:07:21,510
parameters are depend on each other and

00:07:18,150 --> 00:07:23,970
we'll see examples where the heap source

00:07:21,510 --> 00:07:25,530
art heap size was increased and then the

00:07:23,970 --> 00:07:27,810
map buffer was not subsequently

00:07:25,530 --> 00:07:32,520
increased and a result there were job

00:07:27,810 --> 00:07:35,040
failures on hive what are

00:07:32,520 --> 00:07:37,080
misconfigurations misconfigurations are

00:07:35,040 --> 00:07:38,640
any issues requiring a change to the

00:07:37,080 --> 00:07:41,640
Hadoop config file which is this course

00:07:38,640 --> 00:07:44,040
i HEV a site map right site or 20 s

00:07:41,640 --> 00:07:47,460
config files and as you can see from

00:07:44,040 --> 00:07:49,490
this graph the bulk of our tickets and

00:07:47,460 --> 00:07:52,110
our time is spent on misconfigurations

00:07:49,490 --> 00:07:53,550
when we've done root cause analysis and

00:07:52,110 --> 00:07:56,040
post mortems all support tickets we've

00:07:53,550 --> 00:07:57,450
really discovered across the board this

00:07:56,040 --> 00:07:59,790
isn't just one customer this is all of

00:07:57,450 --> 00:08:03,780
our customers looking at it over a

00:07:59,790 --> 00:08:05,250
statistically sound sample that you know

00:08:03,780 --> 00:08:08,190
it's really important to focus on

00:08:05,250 --> 00:08:09,930
misconfigurations because you know the

00:08:08,190 --> 00:08:12,180
due to the 8020 rule right you want to

00:08:09,930 --> 00:08:16,770
spend most of your time on the issues

00:08:12,180 --> 00:08:19,800
that pop up the most if your cluster is

00:08:16,770 --> 00:08:21,720
broken chances are it's a Miss

00:08:19,800 --> 00:08:23,850
configuration when I tell these people

00:08:21,720 --> 00:08:26,190
they don't believe me and and I don't

00:08:23,850 --> 00:08:30,570
blame them you know most of the time for

00:08:26,190 --> 00:08:33,180
for other systems other enterprise

00:08:30,570 --> 00:08:36,180
systems you know often it's a bug it's

00:08:33,180 --> 00:08:38,820
on Miss configuration but Hadoop for all

00:08:36,180 --> 00:08:40,530
of its benefits it's not there yet it's

00:08:38,820 --> 00:08:42,270
not amateur system yeah and this is why

00:08:40,530 --> 00:08:44,880
we need the community to help make you

00:08:42,270 --> 00:08:48,270
more mature to make the documentation

00:08:44,880 --> 00:08:51,889
more robust to make the API more user

00:08:48,270 --> 00:08:53,779
friendly etc but in the meantime we

00:08:51,889 --> 00:08:59,749
about misconfigurations because it

00:08:53,779 --> 00:09:01,100
contributes to broken clusters if you're

00:08:59,749 --> 00:09:02,929
over subscribing your cluster

00:09:01,100 --> 00:09:05,509
specifically if you're running a

00:09:02,929 --> 00:09:07,189
MapReduce job with a hive job you're

00:09:05,509 --> 00:09:08,899
oversubscribed you're going to see

00:09:07,189 --> 00:09:14,089
failures and they're going to be nasty

00:09:08,899 --> 00:09:17,239
brutish and short this is an error

00:09:14,089 --> 00:09:19,549
message we got when a customer upgraded

00:09:17,239 --> 00:09:22,369
their Hadoop cluster and previously

00:09:19,549 --> 00:09:25,399
working hydrops suddenly started failing

00:09:22,369 --> 00:09:28,069
right and left and they had no idea what

00:09:25,399 --> 00:09:29,929
happened because in their eyes all they

00:09:28,069 --> 00:09:31,459
did was upgrade and they'd even want to

00:09:29,929 --> 00:09:32,660
upgrade we told them to a great because

00:09:31,459 --> 00:09:36,139
would be good for them and look what

00:09:32,660 --> 00:09:38,299
happened and in you know shortly

00:09:36,139 --> 00:09:39,589
afterwards I had a very angry customer

00:09:38,299 --> 00:09:41,619
over the weekend trying to figure out

00:09:39,589 --> 00:09:45,889
what went wrong with these high jobs and

00:09:41,619 --> 00:09:48,319
this was all I had to go off of I had a

00:09:45,889 --> 00:09:50,389
return error code to and from that I

00:09:48,319 --> 00:09:55,129
needed to deduce you know what went

00:09:50,389 --> 00:09:56,689
wrong it turns out that the shuffle face

00:09:55,129 --> 00:09:58,189
for the curry so I remember from our

00:09:56,689 --> 00:10:00,410
word count example the shuffle phase

00:09:58,189 --> 00:10:03,199
came in between the map phase and and a

00:10:00,410 --> 00:10:06,919
reduced phase and that was really key to

00:10:03,199 --> 00:10:08,540
to figure out the puzzle there are so

00:10:06,919 --> 00:10:11,149
many miss sorry there's so many

00:10:08,540 --> 00:10:13,429
configurations so many knobs you can

00:10:11,149 --> 00:10:16,489
tune in Hadoop if you can't narrow it

00:10:13,429 --> 00:10:17,929
down it's just this relentless game of

00:10:16,489 --> 00:10:20,149
cat-and-mouse where you're tweaking

00:10:17,929 --> 00:10:21,709
something and you hope he'll work and it

00:10:20,149 --> 00:10:23,929
doesn't you tweak something else and it

00:10:21,709 --> 00:10:26,209
seems almost mindless after after a

00:10:23,929 --> 00:10:28,549
point but if you can pin down which

00:10:26,209 --> 00:10:30,699
fades your queries failing in then you

00:10:28,549 --> 00:10:33,619
can zero in on a subset of

00:10:30,699 --> 00:10:37,279
configurations to tweak as a result so

00:10:33,619 --> 00:10:39,889
in this case what ultimately helped us

00:10:37,279 --> 00:10:43,429
figure out what was going on was they

00:10:39,889 --> 00:10:45,649
had a successful job the xml file that

00:10:43,429 --> 00:10:48,379
we compared with a failed that job a

00:10:45,649 --> 00:10:51,230
failed job that xml file so job xml

00:10:48,379 --> 00:10:54,049
files are basically a core dump of all

00:10:51,230 --> 00:10:55,730
of the Hadoop configurations you you're

00:10:54,049 --> 00:10:57,799
using for that job so these could be

00:10:55,730 --> 00:10:59,660
config settings that you said on the

00:10:57,799 --> 00:11:03,499
command line or config settings from the

00:10:59,660 --> 00:11:04,870
course at file the HDFS site file etc so

00:11:03,499 --> 00:11:06,730
these are all of the

00:11:04,870 --> 00:11:09,490
configs that you use for that particular

00:11:06,730 --> 00:11:12,160
job so it was a literally trial and

00:11:09,490 --> 00:11:14,740
error I looked at the two XML files the

00:11:12,160 --> 00:11:17,080
two job XML files and just compared you

00:11:14,740 --> 00:11:19,380
know where where was it different and

00:11:17,080 --> 00:11:21,040
they turns out there were six

00:11:19,380 --> 00:11:23,529
configuration settings that were

00:11:21,040 --> 00:11:25,750
different and ultimately the culprit was

00:11:23,529 --> 00:11:28,330
the heap the child heap have been

00:11:25,750 --> 00:11:29,260
increased during the upgrade they had

00:11:28,330 --> 00:11:32,260
just figured you know what let's just

00:11:29,260 --> 00:11:34,630
add more heap to the child java ops but

00:11:32,260 --> 00:11:37,330
the map buffer size had not been

00:11:34,630 --> 00:11:39,070
increased and these two will talk about

00:11:37,330 --> 00:11:40,630
later but they have a relationship and

00:11:39,070 --> 00:11:42,640
that relationship needs to be honored

00:11:40,630 --> 00:11:44,650
otherwise you will see job failures like

00:11:42,640 --> 00:11:47,290
this so during the shuffle phase each

00:11:44,650 --> 00:11:49,870
map task has a circular memory buffer

00:11:47,290 --> 00:11:53,260
that writes the output to the buffer by

00:11:49,870 --> 00:11:55,870
default is 100 Meg's and it's a size

00:11:53,260 --> 00:11:58,300
that can be tuned by changing the iOS or

00:11:55,870 --> 00:12:00,400
MB property when the contents of the

00:11:58,300 --> 00:12:02,470
buffer reach a certain threshold size a

00:12:00,400 --> 00:12:04,900
background thread will start to spill

00:12:02,470 --> 00:12:06,610
the contents to disk map outputs will

00:12:04,900 --> 00:12:08,740
continue to be written to the buffer

00:12:06,610 --> 00:12:11,500
while this bill takes place but if the

00:12:08,740 --> 00:12:13,480
buffer fills up during that time the map

00:12:11,500 --> 00:12:16,630
will block until the spill is complete

00:12:13,480 --> 00:12:18,640
in this case increasing the IELTS or and

00:12:16,630 --> 00:12:21,610
be the map buffer size help the sorting

00:12:18,640 --> 00:12:24,250
phase by doing fewer disks bills but

00:12:21,610 --> 00:12:25,930
again this is not unto intuitive looking

00:12:24,250 --> 00:12:28,779
at that original error message which was

00:12:25,930 --> 00:12:31,959
a return code to it's very hard to go

00:12:28,779 --> 00:12:34,209
from that and to deduce that oh I didn't

00:12:31,959 --> 00:12:35,980
increase my map buffer size accordingly

00:12:34,209 --> 00:12:37,690
so all of our reason to make sure you

00:12:35,980 --> 00:12:39,130
configure it correctly the first time

00:12:37,690 --> 00:12:40,660
because later on you don't want to be

00:12:39,130 --> 00:12:42,850
scratching your head over to weekend

00:12:40,660 --> 00:12:44,560
trying to figure out and piece together

00:12:42,850 --> 00:12:49,510
like Sherlock Holmes what went wrong

00:12:44,560 --> 00:12:53,740
when it's a good segue into our first

00:12:49,510 --> 00:12:54,940
mismanagement topic memory you know it

00:12:53,740 --> 00:12:56,770
would have been nice if I had got in

00:12:54,940 --> 00:12:58,209
this error message instead of the the

00:12:56,770 --> 00:13:00,940
return code too but that's a different

00:12:58,209 --> 00:13:02,890
topic for a different session in this

00:13:00,940 --> 00:13:06,880
case if you're seeing this out of memory

00:13:02,890 --> 00:13:09,550
error it's most likely you have a memory

00:13:06,880 --> 00:13:13,270
leak somewhere you don't have memory

00:13:09,550 --> 00:13:17,320
configured correctly somewhere and as I

00:13:13,270 --> 00:13:17,720
mentioned you want to you want to set

00:13:17,320 --> 00:13:20,209
your

00:13:17,720 --> 00:13:22,399
your mat buffersize to be between a

00:13:20,209 --> 00:13:23,779
quarter and a half of your child heap

00:13:22,399 --> 00:13:25,579
size this is something that we've

00:13:23,779 --> 00:13:28,360
consistently seen across diverse

00:13:25,579 --> 00:13:31,220
clusters that has worked well that said

00:13:28,360 --> 00:13:34,069
it obviously should be overridden pro

00:13:31,220 --> 00:13:36,350
job as needed if if you're running a

00:13:34,069 --> 00:13:41,259
particular job you may need to allocate

00:13:36,350 --> 00:13:45,199
more or less Matt buffer as a result you

00:13:41,259 --> 00:13:46,639
know this is more reason why besides

00:13:45,199 --> 00:13:48,259
just making you're making sure you

00:13:46,639 --> 00:13:50,209
configure things correctly you want to

00:13:48,259 --> 00:13:52,639
make sure that you're doing monitoring

00:13:50,209 --> 00:13:54,410
so there are several tools out there one

00:13:52,639 --> 00:13:57,980
of which is clutter manager that can

00:13:54,410 --> 00:14:00,079
help you figure out is your failure

00:13:57,980 --> 00:14:01,879
occurring in the shuffle phase and if so

00:14:00,079 --> 00:14:03,860
you know what configs should you be

00:14:01,879 --> 00:14:07,490
looking at right there's so many configs

00:14:03,860 --> 00:14:11,569
where do you start you you need to

00:14:07,490 --> 00:14:14,029
monitor your Hadoop cluster in order to

00:14:11,569 --> 00:14:18,500
pinpoint failures like this and and to

00:14:14,029 --> 00:14:20,660
allocate your time accordingly you know

00:14:18,500 --> 00:14:22,189
other issues that could cause this out

00:14:20,660 --> 00:14:24,740
of a mirror it could be that your path

00:14:22,189 --> 00:14:28,790
names are too long we've seen instances

00:14:24,740 --> 00:14:30,920
where customers had instead of the using

00:14:28,790 --> 00:14:32,600
the best practice of just flash data / 1

00:14:30,920 --> 00:14:35,000
you know they had this using incredibly

00:14:32,600 --> 00:14:37,309
long past names that we're eating into

00:14:35,000 --> 00:14:39,470
their memory and and nobody realized

00:14:37,309 --> 00:14:40,879
that that you know these past names were

00:14:39,470 --> 00:14:44,959
actually eating up their memory

00:14:40,879 --> 00:14:48,470
accordingly one of our our cloudera

00:14:44,959 --> 00:14:51,769
engineers TopGun said it best when he

00:14:48,470 --> 00:14:54,589
tweeted if the sum of your max heap size

00:14:51,769 --> 00:14:57,319
exceeds your physical RAM minus 3

00:14:54,589 --> 00:15:00,800
gigabytes go directly to jail do not

00:14:57,319 --> 00:15:03,649
pass go do not collect 200 and we've

00:15:00,800 --> 00:15:05,509
seen time and time again where users

00:15:03,649 --> 00:15:08,449
have have gotten these error messages

00:15:05,509 --> 00:15:09,949
about out of memory and they just they

00:15:08,449 --> 00:15:11,360
just go crazy and they start increasing

00:15:09,949 --> 00:15:12,439
their data no heap and then they

00:15:11,360 --> 00:15:13,970
increase for good measure their trash

00:15:12,439 --> 00:15:15,410
tracker haven for good measure they

00:15:13,970 --> 00:15:18,500
increase some other things too but

00:15:15,410 --> 00:15:21,680
forgetting that at the end of the day it

00:15:18,500 --> 00:15:23,750
needs to add up to your total ram and

00:15:21,680 --> 00:15:26,000
and if not you know you will see errors

00:15:23,750 --> 00:15:28,309
and it's best to remember you know as

00:15:26,000 --> 00:15:30,080
you increase things that has

00:15:28,309 --> 00:15:33,320
consequences of other parameters

00:15:30,080 --> 00:15:35,180
as well another out of memory error

00:15:33,320 --> 00:15:36,590
could occur in your job tracker if

00:15:35,180 --> 00:15:38,330
you're seeing these error out of memory

00:15:36,590 --> 00:15:40,010
errors on your job tracker most likely

00:15:38,330 --> 00:15:43,390
your tasks are too small and you're

00:15:40,010 --> 00:15:45,710
keeping too much job history around

00:15:43,390 --> 00:15:49,970
before you start tweaking anything it's

00:15:45,710 --> 00:15:51,230
it's best to do run a J map analysis and

00:15:49,970 --> 00:15:54,350
that way you get a nice histogram of

00:15:51,230 --> 00:15:55,850
what objects a JVM has allocated because

00:15:54,350 --> 00:15:58,240
you can't improve what you can't measure

00:15:55,850 --> 00:16:01,310
right so it's important to first measure

00:15:58,240 --> 00:16:03,290
where are you allocating your memory and

00:16:01,310 --> 00:16:06,280
then you can improve increase/decrease

00:16:03,290 --> 00:16:08,980
tweak go from there and know for certain

00:16:06,280 --> 00:16:12,470
whether or not you're you're improving

00:16:08,980 --> 00:16:15,140
your memory usage or not by default the

00:16:12,470 --> 00:16:17,810
jobtracker keeps a hundred jobs per user

00:16:15,140 --> 00:16:19,430
so this is kind of a holdover from from

00:16:17,810 --> 00:16:22,250
the previous day's the early days I

00:16:19,430 --> 00:16:25,160
would say where you probably had

00:16:22,250 --> 00:16:27,650
everyone running jobs as the same user

00:16:25,160 --> 00:16:30,380
right security was on issue everybody

00:16:27,650 --> 00:16:32,240
ran as a Hadoop user and you know you

00:16:30,380 --> 00:16:34,700
probably had a proof of concept going so

00:16:32,240 --> 00:16:35,690
it didn't really matter right and so you

00:16:34,700 --> 00:16:37,910
know you want to keep all these jobs

00:16:35,690 --> 00:16:40,460
around for easy debugging and so the

00:16:37,910 --> 00:16:43,400
default was set at hundred jobs but now

00:16:40,460 --> 00:16:45,890
with with multiple users using the same

00:16:43,400 --> 00:16:48,890
cluster you can see how very quickly

00:16:45,890 --> 00:16:51,410
this can add up if you have say 100

00:16:48,890 --> 00:16:54,950
users and if you're keeping the defaults

00:16:51,410 --> 00:16:57,650
and saving 100 jobs per user that can

00:16:54,950 --> 00:16:59,960
very quickly exceed any keep you have

00:16:57,650 --> 00:17:04,699
allocated for your job tracker and and

00:16:59,960 --> 00:17:07,880
as a result we recommend decreasing that

00:17:04,699 --> 00:17:11,180
default 100 25 so set your complete user

00:17:07,880 --> 00:17:14,060
jobs to to a maximum of 5 and so this

00:17:11,180 --> 00:17:17,300
actually does not affect your ability to

00:17:14,060 --> 00:17:19,100
to debug because it doesn't mean that

00:17:17,300 --> 00:17:20,810
you know the rest of jobs are deleted

00:17:19,100 --> 00:17:22,430
they're just allocated and held

00:17:20,810 --> 00:17:24,320
elsewhere in memory so you're not using

00:17:22,430 --> 00:17:27,610
up your job tracker memory so you know

00:17:24,320 --> 00:17:31,220
your ability to debug fill jobs is not

00:17:27,610 --> 00:17:33,950
affected the only thing that is affected

00:17:31,220 --> 00:17:35,300
in a good way is you're using less ram

00:17:33,950 --> 00:17:38,179
on your job tracker so you're less

00:17:35,300 --> 00:17:41,509
likely to see the out of Emory errors

00:17:38,179 --> 00:17:45,139
finally our last most common memory

00:17:41,509 --> 00:17:48,619
mismanagement issue miss configuration

00:17:45,139 --> 00:17:50,360
is with native threads so if you're

00:17:48,619 --> 00:17:52,960
seeing these unable to create new native

00:17:50,360 --> 00:17:55,610
thread or too many open file issues it

00:17:52,960 --> 00:17:57,409
it could manifest as your data node

00:17:55,610 --> 00:18:00,590
showing up as dad even though processes

00:17:57,409 --> 00:18:02,090
are still running on those machines so

00:18:00,590 --> 00:18:05,330
in this case you're going to need to

00:18:02,090 --> 00:18:07,179
change your OS settings your OS configs

00:18:05,330 --> 00:18:10,039
rather than your Hadoop configs and

00:18:07,179 --> 00:18:14,179
increase your settings for open files

00:18:10,039 --> 00:18:16,789
processes max memory we recommend 64k

00:18:14,179 --> 00:18:20,330
and up I've not in the question well why

00:18:16,789 --> 00:18:22,159
not just fix that right just you know if

00:18:20,330 --> 00:18:23,450
you know it's the issue instead of just

00:18:22,159 --> 00:18:25,639
telling people to fix it why don't just

00:18:23,450 --> 00:18:27,889
fix it yourself the reason is twofold

00:18:25,639 --> 00:18:30,019
wine these are not sakti faults to these

00:18:27,889 --> 00:18:33,110
are not Hadoop defaults but OS defaults

00:18:30,019 --> 00:18:35,360
and I'm sure a lot of this admins will

00:18:33,110 --> 00:18:40,059
not be very happy if we're going around

00:18:35,360 --> 00:18:40,059
tweaking these OS settings accordingly

00:18:42,789 --> 00:18:51,980
on to thread mismanagement first up is

00:18:48,490 --> 00:18:54,649
fetch failures as you remember from our

00:18:51,980 --> 00:18:56,210
previous word counts lie this is the

00:18:54,649 --> 00:18:58,220
same word count slide but this time

00:18:56,210 --> 00:19:00,169
we're going to focus on just the shuffle

00:18:58,220 --> 00:19:03,259
phase so this is where defects raleys

00:19:00,169 --> 00:19:06,350
occur we're in between the MapReduce

00:19:03,259 --> 00:19:10,129
this is basically when you're not able

00:19:06,350 --> 00:19:12,649
to to fetch the reducers are not able to

00:19:10,129 --> 00:19:15,379
fetch the output from the mappers for a

00:19:12,649 --> 00:19:16,759
variety of reasons and fetch feelers

00:19:15,379 --> 00:19:19,190
I've worked that cloud error for two

00:19:16,759 --> 00:19:21,110
years I first started debugging fetch

00:19:19,190 --> 00:19:23,749
failure issues when i started i am still

00:19:21,110 --> 00:19:26,240
debugging fairly issues today and the

00:19:23,749 --> 00:19:28,549
reason for that is because fetch readers

00:19:26,240 --> 00:19:32,389
can be caused by a hardware issue maybe

00:19:28,549 --> 00:19:34,999
you have a corrupt disk it could be a

00:19:32,389 --> 00:19:38,629
Miss configuration it could be a jetty

00:19:34,999 --> 00:19:42,139
bug it could be auld above and we'll go

00:19:38,629 --> 00:19:43,249
into more detail later but to understand

00:19:42,139 --> 00:19:45,289
fetch fairness we need to first

00:19:43,249 --> 00:19:48,340
understand the reduced phase the moon's

00:19:45,289 --> 00:19:51,080
face is composed of three steps the copy

00:19:48,340 --> 00:19:53,360
distort also known as to merge and there

00:19:51,080 --> 00:19:55,940
deuce during the copy phase the reducer

00:19:53,360 --> 00:19:57,830
fetches the map output from the task

00:19:55,940 --> 00:20:01,070
tracker and stores it on reducer in

00:19:57,830 --> 00:20:03,769
memory or on disk the reduced tasks have

00:20:01,070 --> 00:20:05,720
to fetch the map outputs a map outputs

00:20:03,769 --> 00:20:08,659
from the remote servers of which there

00:20:05,720 --> 00:20:11,240
be many thousands veg traders occur when

00:20:08,659 --> 00:20:16,159
reducer fetch operations failed to

00:20:11,240 --> 00:20:18,649
retrieve mapper outputs we have we have

00:20:16,159 --> 00:20:21,860
seen this occur with with customers

00:20:18,649 --> 00:20:24,830
where they had first one node one single

00:20:21,860 --> 00:20:29,809
node out of there cluster of 100 or so

00:20:24,830 --> 00:20:31,789
notes to come to fetch failures and they

00:20:29,809 --> 00:20:36,350
saw hundreds of their mappers failing

00:20:31,789 --> 00:20:39,350
from too many fetch failures then like

00:20:36,350 --> 00:20:41,870
the Thundering Herd because that one

00:20:39,350 --> 00:20:44,779
host got blacklist blacklisted it then

00:20:41,870 --> 00:20:47,419
shifted to another node which then had

00:20:44,779 --> 00:20:49,159
to rerun all those failed jobs and as

00:20:47,419 --> 00:20:53,510
you can imagine that cost a quite a

00:20:49,159 --> 00:20:55,880
delay a delay of tens of hours and so

00:20:53,510 --> 00:20:57,830
even though the fetch failure warning

00:20:55,880 --> 00:20:59,870
when you see it in your lap oh it looks

00:20:57,830 --> 00:21:01,850
like a very innocuous info and you think

00:20:59,870 --> 00:21:04,039
oh it's a nympho I've worn so worried

00:21:01,850 --> 00:21:05,870
but I've errors were but I fatals worry

00:21:04,039 --> 00:21:09,139
but I don't have all day to worry about

00:21:05,870 --> 00:21:12,139
infos and you ignore infos but do not

00:21:09,139 --> 00:21:15,260
ignore this info this info is deadly and

00:21:12,139 --> 00:21:17,720
it could not just delay your jobs by

00:21:15,260 --> 00:21:19,580
tens of hours but it will do that to

00:21:17,720 --> 00:21:21,710
each node here in cluster until your

00:21:19,580 --> 00:21:23,990
entire cluster is brought to its knees

00:21:21,710 --> 00:21:31,789
because of this innocuous sounding info

00:21:23,990 --> 00:21:35,210
message but what causes this so-called

00:21:31,789 --> 00:21:37,580
info message it could be dns issues so

00:21:35,210 --> 00:21:39,409
it could be networking issues it could

00:21:37,580 --> 00:21:41,240
be a Miss configuration in that you

00:21:39,409 --> 00:21:43,850
don't have enough HTTP threads on your

00:21:41,240 --> 00:21:45,769
map recite it could be a JVM bug a jetty

00:21:43,850 --> 00:21:49,519
bug you know we've seen all of the above

00:21:45,769 --> 00:21:50,990
with with many of our customers and it's

00:21:49,519 --> 00:21:54,110
an issue that continues to rise

00:21:50,990 --> 00:21:57,700
specifically because the root cause is

00:21:54,110 --> 00:21:57,700
very varied

00:21:58,480 --> 00:22:04,340
so this is a recommended best practice

00:22:02,180 --> 00:22:06,200
of you know when you're seeing these

00:22:04,340 --> 00:22:08,990
miss configs you've rolled out the dns

00:22:06,200 --> 00:22:11,180
issues you've rolled out hardware issues

00:22:08,990 --> 00:22:13,430
you know these are the misconfigurations

00:22:11,180 --> 00:22:15,470
rather the configurations you should

00:22:13,430 --> 00:22:17,450
look into tweaking if you're seeing this

00:22:15,470 --> 00:22:20,060
and you've rolled out a lot of the

00:22:17,450 --> 00:22:22,190
hardware and networking issues the map

00:22:20,060 --> 00:22:27,340
output is fetched by the reducers from

00:22:22,190 --> 00:22:29,840
the task trackers via HTTP and so by by

00:22:27,340 --> 00:22:32,150
increasing the tax record HTTP threads

00:22:29,840 --> 00:22:35,870
you're increasing the number of threads

00:22:32,150 --> 00:22:37,610
that can be used to serve map output the

00:22:35,870 --> 00:22:40,190
reduced tasks have to fetch the map

00:22:37,610 --> 00:22:41,810
outputs from the remote servers of which

00:22:40,190 --> 00:22:43,910
there could be thousands so you want

00:22:41,810 --> 00:22:46,370
this copy process to be able to run in

00:22:43,910 --> 00:22:48,140
parallel and as a result what we

00:22:46,370 --> 00:22:50,480
recommend is studying your parallel

00:22:48,140 --> 00:22:52,940
copies to the square root of the note

00:22:50,480 --> 00:22:55,100
count with a floor of 10 so you can

00:22:52,940 --> 00:22:58,100
increase the number of parallel copies

00:22:55,100 --> 00:23:03,290
used by reducers to retrieve to fetch

00:22:58,100 --> 00:23:06,200
map output and and also we want to allow

00:23:03,290 --> 00:23:09,110
reducers from other jobs to be able to

00:23:06,200 --> 00:23:11,330
run while a big job waits on mappers and

00:23:09,110 --> 00:23:15,590
this is the rationale behind increasing

00:23:11,330 --> 00:23:19,670
the slow start to 2.8 finally if you're

00:23:15,590 --> 00:23:21,890
using the version 61 26 of jetty it's

00:23:19,670 --> 00:23:25,460
fetched further prone we have several

00:23:21,890 --> 00:23:27,740
jurors upstream jurors that discuss in

00:23:25,460 --> 00:23:29,030
more detail if you're interested but if

00:23:27,740 --> 00:23:31,790
you're seeing these issues you've really

00:23:29,030 --> 00:23:34,220
tweaked your configs you've wrote out

00:23:31,790 --> 00:23:35,390
any hardware issues you know you should

00:23:34,220 --> 00:23:37,190
look into what version of jetty you're

00:23:35,390 --> 00:23:39,170
running because it could very well be

00:23:37,190 --> 00:23:45,860
that you're running on on the fetch

00:23:39,170 --> 00:23:48,320
failure-prone jetty another another miss

00:23:45,860 --> 00:23:50,420
configuration we often see is when

00:23:48,320 --> 00:23:53,720
you're not able to place enough replicas

00:23:50,420 --> 00:23:57,560
and and what this means is that your

00:23:53,720 --> 00:24:00,350
your DFS replication exceeds your number

00:23:57,560 --> 00:24:02,210
of available data nodes and it could be

00:24:00,350 --> 00:24:04,670
that your number available data nodes is

00:24:02,210 --> 00:24:06,260
low due to load the space it could be

00:24:04,670 --> 00:24:08,930
that your your map read submit

00:24:06,260 --> 00:24:10,220
replication is too high it could be that

00:24:08,930 --> 00:24:12,980
your name Rhodes unable to

00:24:10,220 --> 00:24:16,760
aside the block placement policy for

00:24:12,980 --> 00:24:18,320
instance if if you've set your your

00:24:16,760 --> 00:24:21,080
number of racks to be greater than or

00:24:18,320 --> 00:24:24,919
equal to 2 that means that a blog has to

00:24:21,080 --> 00:24:26,360
exist in at least two racks and it also

00:24:24,919 --> 00:24:28,039
could be that your data is too much load

00:24:26,360 --> 00:24:31,340
on it etc there could be as you can see

00:24:28,039 --> 00:24:33,590
a variety of reasons even more so why

00:24:31,340 --> 00:24:36,380
it's important to to configure correctly

00:24:33,590 --> 00:24:39,200
because it's often hard to look at this

00:24:36,380 --> 00:24:41,210
this warned this warning message and

00:24:39,200 --> 00:24:43,250
figure out what caused this right as you

00:24:41,210 --> 00:24:45,409
think there's a slew of reasons that

00:24:43,250 --> 00:24:48,140
could cause it and subsequently there's

00:24:45,409 --> 00:24:51,409
also a slew of things you could do to

00:24:48,140 --> 00:24:53,840
alleviate it as a result so it's it's

00:24:51,409 --> 00:24:55,940
good to to check for your de space look

00:24:53,840 --> 00:24:58,280
for nodes that are down look for racks

00:24:55,940 --> 00:25:00,049
that are down you know make sure that

00:24:58,280 --> 00:25:03,140
this parameter the X severs cramped ER

00:25:00,049 --> 00:25:05,419
is is spelled wrong so it accidentally

00:25:03,140 --> 00:25:07,610
was misspelled and as a result you know

00:25:05,419 --> 00:25:10,250
people along the way they have spelled

00:25:07,610 --> 00:25:13,549
it correctly to their detriment so it's

00:25:10,250 --> 00:25:16,280
important to to honor that typo you may

00:25:13,549 --> 00:25:18,350
need to to rebalance on a replicated

00:25:16,280 --> 00:25:21,230
blocks and so these are some config

00:25:18,350 --> 00:25:23,480
settings we found to be useful you could

00:25:21,230 --> 00:25:25,789
also just shut down the data node and

00:25:23,480 --> 00:25:29,330
they use the Linux move command to move

00:25:25,789 --> 00:25:32,150
some of your files from your your DFS

00:25:29,330 --> 00:25:36,500
data dirt from a full volume to empty

00:25:32,150 --> 00:25:41,539
volume and finally on to data

00:25:36,500 --> 00:25:44,090
mismanagement if you're seeing this no

00:25:41,539 --> 00:25:46,610
such file directory one of the first

00:25:44,090 --> 00:25:49,400
things you want to look into is to make

00:25:46,610 --> 00:25:51,890
sure you have reserved enough space for

00:25:49,400 --> 00:25:55,520
the temporary data of the biggest Hadoop

00:25:51,890 --> 00:25:58,309
job expected so so we recommend

00:25:55,520 --> 00:26:01,780
allegheny roughly ten percent of your

00:25:58,309 --> 00:26:01,780
DFS space to Mac reduce

00:26:04,180 --> 00:26:10,730
if you're if you're seeing this error

00:26:06,770 --> 00:26:12,830
message you know usually it manifests as

00:26:10,730 --> 00:26:16,700
jobs are failing your past trackers

00:26:12,830 --> 00:26:18,470
failing to start it could be due to your

00:26:16,700 --> 00:26:21,530
task wrecker filling up and that's

00:26:18,470 --> 00:26:23,300
because you haven't said enough you

00:26:21,530 --> 00:26:25,370
haven't reserved enough space for your

00:26:23,300 --> 00:26:28,160
MapReduce jobs so it can also be wrong

00:26:25,370 --> 00:26:33,650
permissions a bad disk you'll make sure

00:26:28,160 --> 00:26:38,000
that your your permissions are 755 you

00:26:33,650 --> 00:26:42,620
know I I first gave this dis talk at

00:26:38,000 --> 00:26:44,510
Hadoop bro 2011 and for for this talk

00:26:42,620 --> 00:26:45,800
for a patch econ you know I didn't want

00:26:44,510 --> 00:26:47,810
to just give the same talk right because

00:26:45,800 --> 00:26:50,240
that would be pointless and a disservice

00:26:47,810 --> 00:26:52,370
to the community and so I started

00:26:50,240 --> 00:26:54,230
updating it and and so this is irie

00:26:52,370 --> 00:26:56,480
assure you for anyone who was in that

00:26:54,230 --> 00:26:58,910
previous talk this is an update but what

00:26:56,480 --> 00:27:00,620
amazed me was how many of these

00:26:58,910 --> 00:27:03,470
misconfigurations are still running

00:27:00,620 --> 00:27:05,270
rampant two years later I'm still seeing

00:27:03,470 --> 00:27:07,670
in the community I'm still seeing with

00:27:05,270 --> 00:27:10,610
our customers the permissions are set

00:27:07,670 --> 00:27:14,600
wrong you know the owner is set wrong

00:27:10,610 --> 00:27:16,700
they're running not as map-read or you

00:27:14,600 --> 00:27:18,520
know you're seeing bad disk so these

00:27:16,700 --> 00:27:22,040
same error messages yourselves seeing

00:27:18,520 --> 00:27:23,600
show up you know they're still showing

00:27:22,040 --> 00:27:25,070
up you know some of them are not which

00:27:23,600 --> 00:27:28,490
is why I've removed this from this deck

00:27:25,070 --> 00:27:31,720
but some of them still are and so what

00:27:28,490 --> 00:27:34,040
that tells me is mr. configurations are

00:27:31,720 --> 00:27:36,050
are really hard we need to monitor

00:27:34,040 --> 00:27:37,850
better you know we need to document

00:27:36,050 --> 00:27:39,530
better there are a lot of things we need

00:27:37,850 --> 00:27:41,360
to do better but you know being vigilant

00:27:39,530 --> 00:27:46,340
about configuring it right the first

00:27:41,360 --> 00:27:53,930
time is is key to to not seeing these

00:27:46,340 --> 00:27:56,990
going forward and finally a a final

00:27:53,930 --> 00:27:59,510
deadly miss configuration also more

00:27:56,990 --> 00:28:01,790
commonly known as as a fubar as a user

00:27:59,510 --> 00:28:04,520
error this actually happened we had a

00:28:01,790 --> 00:28:08,630
customer who remain nameless who who did

00:28:04,520 --> 00:28:10,940
rmr and their Hadoop trash was not

00:28:08,630 --> 00:28:12,380
configured and there was permanent data

00:28:10,940 --> 00:28:13,430
loss and there was much gnashing of

00:28:12,380 --> 00:28:15,830
teeth and yelling

00:28:13,430 --> 00:28:18,880
pointing and a lot of boss types got

00:28:15,830 --> 00:28:24,680
involved this is very unfortunate

00:28:18,880 --> 00:28:27,050
because the by default your trash is not

00:28:24,680 --> 00:28:30,350
configured and and here are some jurors

00:28:27,050 --> 00:28:32,150
to reference if you if you want to read

00:28:30,350 --> 00:28:34,040
about the history behind this so after

00:28:32,150 --> 00:28:35,780
this happened there's a lot of talking

00:28:34,040 --> 00:28:37,730
to me in the community about you know

00:28:35,780 --> 00:28:40,490
what should we should we change it so

00:28:37,730 --> 00:28:41,690
that trash is no longer a user lot of a

00:28:40,490 --> 00:28:43,220
future should we make it service hi

00:28:41,690 --> 00:28:46,010
trash right and i think that's still an

00:28:43,220 --> 00:28:47,780
ongoing debate I owners also talked

00:28:46,010 --> 00:28:49,520
about instead of having the default 0

00:28:47,780 --> 00:28:51,350
which means your trash is not configured

00:28:49,520 --> 00:28:52,880
so it's actually more of a recycling bin

00:28:51,350 --> 00:28:55,130
right it's more like whoa oops I didn't

00:28:52,880 --> 00:28:58,820
me to do an rmr let me go and rescue

00:28:55,130 --> 00:29:02,860
those files but instead you know the

00:28:58,820 --> 00:29:05,630
consensus was like let's keep it at zero

00:29:02,860 --> 00:29:07,370
so meaning that if you were to do an arm

00:29:05,630 --> 00:29:10,310
are you are going to be seeing permanent

00:29:07,370 --> 00:29:14,120
data loss as a result what we recommend

00:29:10,310 --> 00:29:15,380
is set it to once i was in four hundred

00:29:14,120 --> 00:29:17,390
forty minutes or your content stick

00:29:15,380 --> 00:29:19,700
around for a day so you know in case you

00:29:17,390 --> 00:29:21,320
do have this user error you know you can

00:29:19,700 --> 00:29:31,720
retrieve that you can go back in time

00:29:21,320 --> 00:29:37,190
and get that back so one bonus msconfig

00:29:31,720 --> 00:29:38,420
that is is not as deadly as some of the

00:29:37,190 --> 00:29:39,800
ones i've previously mentioned right

00:29:38,420 --> 00:29:42,110
there's nothing more deadly than losing

00:29:39,800 --> 00:29:44,720
data but this is on me that has has

00:29:42,110 --> 00:29:47,120
tripped up several of our users where

00:29:44,720 --> 00:29:48,470
you know they've they've got in this you

00:29:47,120 --> 00:29:50,420
know no groups too little for user dr.

00:29:48,470 --> 00:29:51,320
and arrow and who is dr. who i dont

00:29:50,420 --> 00:29:54,830
rember Doctor Who I don't remember

00:29:51,320 --> 00:29:57,500
authenticates Doctor Who so what we

00:29:54,830 --> 00:30:00,500
recommend if you see this is you know

00:29:57,500 --> 00:30:03,680
what this means is that you have an

00:30:00,500 --> 00:30:08,330
unauthenticated of you drop Itoh so for

00:30:03,680 --> 00:30:10,430
instance you know if if I I'm trying to

00:30:08,330 --> 00:30:14,210
view if i'm kate i'm trying to view

00:30:10,430 --> 00:30:17,600
mark's jobs you know i'm going to get

00:30:14,210 --> 00:30:18,920
this error right because i i'm not i'm

00:30:17,600 --> 00:30:20,540
not marked i shouldn't be able to view

00:30:18,920 --> 00:30:21,710
it but instead of instead of latino

00:30:20,540 --> 00:30:23,210
seeing error message that says you know

00:30:21,710 --> 00:30:24,240
you are not mark you're not able to

00:30:23,210 --> 00:30:27,840
you're going to

00:30:24,240 --> 00:30:30,059
I see justice Doctor Who message so you

00:30:27,840 --> 00:30:32,160
know you have a couple of options first

00:30:30,059 --> 00:30:34,590
you can just pass that specific user via

00:30:32,160 --> 00:30:37,950
the URL so you can just you know the

00:30:34,590 --> 00:30:40,380
localhost URL you can just like plop in

00:30:37,950 --> 00:30:42,030
you know username equals your name right

00:30:40,380 --> 00:30:46,650
so that's one way to bypass this another

00:30:42,030 --> 00:30:48,210
way is to configure Kerberos and you can

00:30:46,650 --> 00:30:56,850
also tweak this configuration setting

00:30:48,210 --> 00:30:59,250
from the Doctor Who default as well one

00:30:56,850 --> 00:31:02,820
large cluster doesn't necessarily show

00:30:59,250 --> 00:31:04,470
all the different dependencies and we've

00:31:02,820 --> 00:31:06,690
seen a lot of diverse operational

00:31:04,470 --> 00:31:08,280
clusters and part of what we do is is to

00:31:06,690 --> 00:31:10,679
provide tools that help diagnose to

00:31:08,280 --> 00:31:12,450
understand how Hadoop operates you know

00:31:10,679 --> 00:31:14,700
if if you didn't get anything from his

00:31:12,450 --> 00:31:15,900
talk you know first these slides will be

00:31:14,700 --> 00:31:17,190
available so you know for freed

00:31:15,900 --> 00:31:19,020
referenced in but if you need anything

00:31:17,190 --> 00:31:21,600
from his talk remember the seven deadly

00:31:19,020 --> 00:31:24,570
misconfigurations what I really want you

00:31:21,600 --> 00:31:25,950
to take away is Miss configurations are

00:31:24,570 --> 00:31:27,630
really hard to diagnose you know we

00:31:25,950 --> 00:31:30,210
don't have that documentation in place

00:31:27,630 --> 00:31:31,980
we don't have those descriptive detailed

00:31:30,210 --> 00:31:33,870
error messages in place at tell you oh

00:31:31,980 --> 00:31:35,700
this is where you misconfigured you know

00:31:33,870 --> 00:31:37,740
go tweak that go increase that go

00:31:35,700 --> 00:31:39,990
decrease that we don't have that yet

00:31:37,740 --> 00:31:42,690
therefore drooly important to correctly

00:31:39,990 --> 00:31:44,730
configure it the first time around and

00:31:42,690 --> 00:31:46,470
second you know get it right with

00:31:44,730 --> 00:31:48,960
monitoring tools it's there are a number

00:31:46,470 --> 00:31:52,740
out there and it's really important to

00:31:48,960 --> 00:31:54,840
use them you know for instance with the

00:31:52,740 --> 00:31:57,809
leap second but I don't know how many of

00:31:54,840 --> 00:32:00,660
you are affected by it but you know back

00:31:57,809 --> 00:32:02,910
in in May we got a support ticket from a

00:32:00,660 --> 00:32:04,320
customer asking you know is is Hadoop

00:32:02,910 --> 00:32:06,179
going to be affected with the upcoming

00:32:04,320 --> 00:32:09,210
leap second and with oh no you know

00:32:06,179 --> 00:32:10,679
Hadoop has automatic failover and and

00:32:09,210 --> 00:32:12,570
who do phool be able to take care of

00:32:10,679 --> 00:32:15,300
that and you know we all fell on our

00:32:12,570 --> 00:32:18,210
faces and then of June when a number of

00:32:15,300 --> 00:32:21,210
our customers 30 to be exact all hit the

00:32:18,210 --> 00:32:23,940
leap second but all at the same time and

00:32:21,210 --> 00:32:26,940
they're all freaking out because they

00:32:23,940 --> 00:32:27,990
were seen on a Saturday their cluster

00:32:26,940 --> 00:32:29,640
running at one hundred percent capacity

00:32:27,990 --> 00:32:31,950
all thought that the clusters were

00:32:29,640 --> 00:32:32,830
possessed they're possessed with a bug

00:32:31,950 --> 00:32:34,510
and

00:32:32,830 --> 00:32:36,370
and in this case by using clutter

00:32:34,510 --> 00:32:39,130
manager by using ganglia they were able

00:32:36,370 --> 00:32:41,950
to monitor and see that Oh suddenly our

00:32:39,130 --> 00:32:44,409
CPU spiked right you know it's something

00:32:41,950 --> 00:32:47,649
that that you want to be proactively

00:32:44,409 --> 00:32:49,870
monitoring and and to know when your

00:32:47,649 --> 00:32:51,909
cluster is broken or jobs or family like

00:32:49,870 --> 00:32:53,500
if you start noticing you know this node

00:32:51,909 --> 00:32:54,700
is going down and that note is going

00:32:53,500 --> 00:32:55,779
down maybe you want to look into fetch

00:32:54,700 --> 00:32:57,159
furthers maybe you know there's just

00:32:55,779 --> 00:33:00,250
done during heard going on with the

00:32:57,159 --> 00:33:03,730
fetch failures but at any rate miskin

00:33:00,250 --> 00:33:08,019
fix are hard and it's up to you to to

00:33:03,730 --> 00:33:11,490
diagnose and you know a lot of these

00:33:08,019 --> 00:33:13,450
issues dealt with map with MapReduce but

00:33:11,490 --> 00:33:16,210
often you know you're going to see these

00:33:13,450 --> 00:33:19,600
issues with hive also was HBase so a

00:33:16,210 --> 00:33:20,740
quick plug for HBase con call for

00:33:19,600 --> 00:33:26,289
speakers is up early bird registration

00:33:20,740 --> 00:33:28,620
is up for HPS con as well with that open

00:33:26,289 --> 00:33:28,620
up for questions

00:33:47,730 --> 00:33:55,320
don't be shy I'll have I have a question

00:33:52,070 --> 00:33:59,190
can you enlighten us anything on you

00:33:55,320 --> 00:34:01,260
know some of I know that on HDFS the

00:33:59,190 --> 00:34:05,400
administrator creates quotas for say

00:34:01,260 --> 00:34:07,880
different teams and many many times

00:34:05,400 --> 00:34:10,800
seven themes are given a less I quota

00:34:07,880 --> 00:34:12,870
but is there a more dynamic way that you

00:34:10,800 --> 00:34:15,750
know they can change the coat on on the

00:34:12,870 --> 00:34:18,600
fly or for say our day or something like

00:34:15,750 --> 00:34:20,430
that on HDFS so that you know the team

00:34:18,600 --> 00:34:27,960
can use that excessive quota for a day

00:34:20,430 --> 00:34:32,400
or so yeah the disk usage this usage c

00:34:27,960 --> 00:34:34,440
there there could be a configuration

00:34:32,400 --> 00:34:36,270
pamper for that it's not something we

00:34:34,440 --> 00:34:37,620
commonly see so i don't have a good

00:34:36,270 --> 00:34:40,020
answer for you right off that's what

00:34:37,620 --> 00:34:47,430
I've had it but i can i can look into it

00:34:40,020 --> 00:34:49,050
you're so many top to twig so well there

00:34:47,430 --> 00:34:51,510
you have it oh just a awesome this let

00:34:49,050 --> 00:34:56,010
me a mean yeah I thought that I w right

00:34:51,510 --> 00:35:01,560
it is right so now that I got your eyes

00:34:56,010 --> 00:35:06,540
but let me look into it it's something

00:35:01,560 --> 00:35:09,840
that we can hack we can like code if we

00:35:06,540 --> 00:35:15,320
have a way we can define strategy so or

00:35:09,840 --> 00:35:15,320
giving kotas on mischief that is what

00:35:19,830 --> 00:35:22,830
mark

00:35:24,160 --> 00:35:27,380
so is there something better we can do

00:35:26,390 --> 00:35:28,880
to actually document these

00:35:27,380 --> 00:35:30,799
configurations because I'm guessing a

00:35:28,880 --> 00:35:32,119
lot of people run into these and the end

00:35:30,799 --> 00:35:33,799
of google searching and sometimes they

00:35:32,119 --> 00:35:35,359
will find it and sometimes they want but

00:35:33,799 --> 00:35:37,160
do you have any particular thoughts on I

00:35:35,359 --> 00:35:40,759
mean how can we make the community

00:35:37,160 --> 00:35:42,470
better around who do projects to to make

00:35:40,759 --> 00:35:43,849
this information proactively available

00:35:42,470 --> 00:35:45,230
to people so they don't run into it

00:35:43,849 --> 00:35:47,359
waste a lot of time figuring out what it

00:35:45,230 --> 00:35:48,769
is and try to fix it right right so

00:35:47,359 --> 00:35:51,829
that's a great question mark so mark ass

00:35:48,769 --> 00:35:54,289
is there a way to proactively tell

00:35:51,829 --> 00:35:57,680
people audis misconfigurations an answer

00:35:54,289 --> 00:36:01,609
is is is twofold called our manager so

00:35:57,680 --> 00:36:02,960
as as I've seen these as I've seen as

00:36:01,609 --> 00:36:05,539
I've run into these misconfigurations

00:36:02,960 --> 00:36:07,999
I've passed that I fed that information

00:36:05,539 --> 00:36:11,470
to our color manager team and so with

00:36:07,999 --> 00:36:13,519
color manager the defaults are right

00:36:11,470 --> 00:36:16,039
because right now if you were to just

00:36:13,519 --> 00:36:17,839
install Hadoop out of the box you would

00:36:16,039 --> 00:36:22,369
run into these issues because the

00:36:17,839 --> 00:36:23,720
defaults are not suited for for running

00:36:22,369 --> 00:36:25,130
a Hadoop cluster right out the box

00:36:23,720 --> 00:36:27,529
without tweaking but call their manager

00:36:25,130 --> 00:36:30,349
is all of these these configs I've fed

00:36:27,529 --> 00:36:32,150
back to the contrary manager team so the

00:36:30,349 --> 00:36:34,789
default with cloud manager are right the

00:36:32,150 --> 00:36:37,249
first time that said in terms of more

00:36:34,789 --> 00:36:40,069
more proactiveness you know there's

00:36:37,249 --> 00:36:41,660
these slides will be made available and

00:36:40,069 --> 00:36:45,049
I've seen people on the user mailing

00:36:41,660 --> 00:36:49,099
list reference this deck from the hadoop

00:36:45,049 --> 00:36:51,259
world conference so you know this is

00:36:49,099 --> 00:36:52,940
something i don't intend to hold any of

00:36:51,259 --> 00:36:55,220
this knowledge this is some some

00:36:52,940 --> 00:36:57,410
knowledge that I've gleaned from looking

00:36:55,220 --> 00:36:59,089
at a diverse set of clusters so one

00:36:57,410 --> 00:37:00,549
large cluster isn't necessarily going to

00:36:59,089 --> 00:37:02,690
show you all of the the

00:37:00,549 --> 00:37:04,279
misconfigurations it's only by looking

00:37:02,690 --> 00:37:06,470
at a diverse set of clusters with a

00:37:04,279 --> 00:37:08,079
diverse workflow and many different use

00:37:06,470 --> 00:37:09,920
cases that you're going to see these

00:37:08,079 --> 00:37:12,890
misconfigurations are going to spot

00:37:09,920 --> 00:37:15,710
these patterns as well so this would f

00:37:12,890 --> 00:37:20,019
amade available color color manager is

00:37:15,710 --> 00:37:20,019
also freely available as well

00:37:31,480 --> 00:37:37,160
right right so the question is why is it

00:37:34,280 --> 00:37:42,200
that a small matt buffer size you know

00:37:37,160 --> 00:37:45,050
cause a hive job to fail and so you know

00:37:42,200 --> 00:37:48,410
that answer is multifold you know one it

00:37:45,050 --> 00:37:49,819
could very well be that underneath the

00:37:48,410 --> 00:37:53,559
Miss configuration what I do know is

00:37:49,819 --> 00:37:57,020
this when I increased the sort buffer

00:37:53,559 --> 00:37:58,700
sorry the mat buffer the job worked you

00:37:57,020 --> 00:38:00,559
know I didn't take anything else I just

00:37:58,700 --> 00:38:03,109
tweaked this one parameter and it worked

00:38:00,559 --> 00:38:04,819
you know but that said by digging into

00:38:03,109 --> 00:38:06,980
the theory behind it why would that

00:38:04,819 --> 00:38:09,260
cause it there could be underlying but

00:38:06,980 --> 00:38:13,099
for I know where dismissed configuration

00:38:09,260 --> 00:38:16,490
is masking a bug but you know it's

00:38:13,099 --> 00:38:19,309
another reason could be that because you

00:38:16,490 --> 00:38:23,450
know what you're doing is let me just go

00:38:19,309 --> 00:38:26,020
back to to that slide it goes pretty

00:38:23,450 --> 00:38:26,020
early on

00:38:31,650 --> 00:38:37,770
so by by increasing the iOS or mb the

00:38:35,460 --> 00:38:41,550
map buffer size we help the sorting face

00:38:37,770 --> 00:38:44,730
by doing fewer dis bills and so when

00:38:41,550 --> 00:38:48,960
when you do a dispels the map thread

00:38:44,730 --> 00:38:50,670
will block so you know this is this is

00:38:48,960 --> 00:38:53,220
what I'm guessing here like I understand

00:38:50,670 --> 00:38:54,930
like when I analyzed as further I kind

00:38:53,220 --> 00:38:56,940
of thought you know couldn't really be

00:38:54,930 --> 00:38:58,140
that that just you know because they

00:38:56,940 --> 00:38:59,220
should have still worked know there

00:38:58,140 --> 00:39:02,820
might have been some performance issues

00:38:59,220 --> 00:39:04,500
maybe it should have been you know it

00:39:02,820 --> 00:39:07,590
meant it may be taken out much longer

00:39:04,500 --> 00:39:09,750
time but the job should still in theory

00:39:07,590 --> 00:39:12,660
should have still succeeded even with a

00:39:09,750 --> 00:39:14,820
low Matt buffer size so so my theory

00:39:12,660 --> 00:39:18,030
behind this and looking to this more is

00:39:14,820 --> 00:39:19,800
that it could be that this was an

00:39:18,030 --> 00:39:21,870
especially long query you know I can't

00:39:19,800 --> 00:39:24,270
show you the query due to customer

00:39:21,870 --> 00:39:25,470
confidential data but it literally took

00:39:24,270 --> 00:39:27,360
up if I were to show you to hear it

00:39:25,470 --> 00:39:29,400
would later take up the whole page so it

00:39:27,360 --> 00:39:32,070
could just be that the size of your

00:39:29,400 --> 00:39:34,800
query caused too many Mac threads to

00:39:32,070 --> 00:39:37,890
block which then caused the shuffle

00:39:34,800 --> 00:39:39,750
phase two to fail so you know I'm not

00:39:37,890 --> 00:39:42,240
saying that when you have a completely

00:39:39,750 --> 00:39:43,440
configured correctly configured cluster

00:39:42,240 --> 00:39:45,330
you're never going to run to any issues

00:39:43,440 --> 00:39:47,450
but it will definitely decrease a lot of

00:39:45,330 --> 00:39:47,450
it

00:40:10,680 --> 00:40:15,369
acting all the slides from the speakers

00:40:13,119 --> 00:40:20,829
and a logic on should be posting some

00:40:15,369 --> 00:40:22,269
time we we are collecting these slides

00:40:20,829 --> 00:40:31,329
and apache con should be posting it

00:40:22,269 --> 00:40:34,989
sometime just out of curiosity on the

00:40:31,329 --> 00:40:38,230
issue that he saw here what was the data

00:40:34,989 --> 00:40:40,650
a binary comparable or it was just a

00:40:38,230 --> 00:40:43,869
writable comparable for which needed

00:40:40,650 --> 00:40:48,450
that not that occurred to be read off

00:40:43,869 --> 00:40:48,450
right off the memory to us an object

00:40:48,720 --> 00:41:02,009
because you were tracking and out of

00:40:50,890 --> 00:41:06,249
memory error here right that was youtube

00:41:02,009 --> 00:41:09,480
some compression codecs but adds up to

00:41:06,249 --> 00:41:09,480
the further details

00:41:16,239 --> 00:41:18,299

YouTube URL: https://www.youtube.com/watch?v=xwbSDr7dpl0


