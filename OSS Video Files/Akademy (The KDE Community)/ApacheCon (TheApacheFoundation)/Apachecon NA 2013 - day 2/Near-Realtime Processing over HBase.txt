Title: Near-Realtime Processing over HBase
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 2
Description: 
	Ryan Brush
ApacheCon NA 2013
Big Data
Captions: 
	00:00:00,000 --> 00:00:07,259
alright I hope everyone had a good lunch

00:00:03,049 --> 00:00:08,910
so next up we have Ryan brush and he's

00:00:07,259 --> 00:00:11,820
going to talk about near real-time

00:00:08,910 --> 00:00:15,240
processing over HP's what you write

00:00:11,820 --> 00:00:18,449
alright thanks alright so I'm going to

00:00:15,240 --> 00:00:19,920
jump in so at this point I'm just a

00:00:18,449 --> 00:00:21,660
happy that I got through lunch without a

00:00:19,920 --> 00:00:23,670
line of gravy down the front of my shirt

00:00:21,660 --> 00:00:25,350
so I figured that the rest of

00:00:23,670 --> 00:00:29,099
presentation should be downhill from

00:00:25,350 --> 00:00:31,430
here so I'm going to talk about some

00:00:29,099 --> 00:00:33,930
things that we're doing over HBase

00:00:31,430 --> 00:00:36,960
specifically doing low latency

00:00:33,930 --> 00:00:39,360
processing over it so we're going to

00:00:36,960 --> 00:00:41,969
cover a few things one is how the needs

00:00:39,360 --> 00:00:43,920
for doing real-time processing / HBase

00:00:41,969 --> 00:00:45,930
emerged some of the problems that we're

00:00:43,920 --> 00:00:48,629
trying to solve we're going to talk

00:00:45,930 --> 00:00:51,020
about complementing MapReduce and stream

00:00:48,629 --> 00:00:53,489
based processing over these technologies

00:00:51,020 --> 00:00:54,960
some techniques that and that we learned

00:00:53,489 --> 00:00:57,059
along the way and some lessons that we

00:00:54,960 --> 00:00:58,500
learned along the way we're going to

00:00:57,059 --> 00:01:01,199
touch on some ways that we're doing

00:00:58,500 --> 00:01:03,719
queries and searches against the data

00:01:01,199 --> 00:01:05,369
that we process and then some notes on

00:01:03,719 --> 00:01:08,369
on where I think we think that a lot of

00:01:05,369 --> 00:01:12,000
this is actually going so let's go ahead

00:01:08,369 --> 00:01:15,060
and jump in with the story so far so our

00:01:12,000 --> 00:01:18,600
first use of Hadoop was building a

00:01:15,060 --> 00:01:21,900
solution called chart search so really

00:01:18,600 --> 00:01:24,869
what what Turner does is uh we're

00:01:21,900 --> 00:01:26,520
focused on health care and technology

00:01:24,869 --> 00:01:27,840
around health care so one of the things

00:01:26,520 --> 00:01:28,979
that we found is that a lot of

00:01:27,840 --> 00:01:31,140
clinicians had a really hard time

00:01:28,979 --> 00:01:33,150
finding the appropriate clinical

00:01:31,140 --> 00:01:35,579
information to look at the history of

00:01:33,150 --> 00:01:37,619
their clients so we wanted to solve that

00:01:35,579 --> 00:01:39,720
by making the medical charts searchable

00:01:37,619 --> 00:01:41,600
which Believe It or Not took a long time

00:01:39,720 --> 00:01:45,079
for the healthcare industry to achieve

00:01:41,600 --> 00:01:49,110
so we do some interesting things here

00:01:45,079 --> 00:01:51,479
and all using Hadoop to leverage this so

00:01:49,110 --> 00:01:53,399
one is that we aggregate data into

00:01:51,479 --> 00:01:56,549
Hadoop and we do things like information

00:01:53,399 --> 00:01:59,159
extraction from from clinical documents

00:01:56,549 --> 00:02:02,040
or clinical notes will do semantic

00:01:59,159 --> 00:02:05,939
markup of clinical documents so if

00:02:02,040 --> 00:02:07,619
somebody has a myocardial infarction in

00:02:05,939 --> 00:02:09,390
their clinical document and I do a

00:02:07,619 --> 00:02:10,830
search for a heart disease I really want

00:02:09,390 --> 00:02:12,629
that document that has myocardial

00:02:10,830 --> 00:02:14,900
infarction to come back so we do

00:02:12,629 --> 00:02:17,580
semantics and

00:02:14,900 --> 00:02:20,220
which is the results and those related

00:02:17,580 --> 00:02:22,530
concepts in the searches and all of this

00:02:20,220 --> 00:02:24,690
since we are dealing with tenant

00:02:22,530 --> 00:02:27,450
generally historical data about a

00:02:24,690 --> 00:02:30,240
patient we processing latency of tens of

00:02:27,450 --> 00:02:31,770
minutes is generally ok so that was

00:02:30,240 --> 00:02:33,750
working pretty well and we decided well

00:02:31,770 --> 00:02:35,040
we've got all this data together would

00:02:33,750 --> 00:02:37,730
it be great if we can do some more stuff

00:02:35,040 --> 00:02:39,930
with it so we started looking at doing

00:02:37,730 --> 00:02:41,850
medical alerts of data that we're

00:02:39,930 --> 00:02:43,110
bringing together in the cloud I don't

00:02:41,850 --> 00:02:45,540
expect you to read this diagram it's

00:02:43,110 --> 00:02:50,310
just an example of some of the logic

00:02:45,540 --> 00:02:51,510
that we apply so giving this incoming

00:02:50,310 --> 00:02:54,120
data we want to be able to detect

00:02:51,510 --> 00:02:57,260
patients that are have certain health

00:02:54,120 --> 00:02:59,550
risks based on either literature or on

00:02:57,260 --> 00:03:01,530
research that we've done to indicate

00:02:59,550 --> 00:03:05,010
that they may have issues we want to

00:03:01,530 --> 00:03:07,680
notify clinicians about those risks so

00:03:05,010 --> 00:03:10,470
and then quickly include new knowledge

00:03:07,680 --> 00:03:13,320
to improve that so an example that we've

00:03:10,470 --> 00:03:14,670
actually rolled out is detecting

00:03:13,320 --> 00:03:16,980
patients that are at a high risk of

00:03:14,670 --> 00:03:18,570
sepsis so based on vital signs lab

00:03:16,980 --> 00:03:20,820
results number of their measures we can

00:03:18,570 --> 00:03:23,520
detect when a patient is has a good

00:03:20,820 --> 00:03:25,200
probability of being septic and what's

00:03:23,520 --> 00:03:27,030
interesting is this solutions actually

00:03:25,200 --> 00:03:28,650
saved lives we've actually being able to

00:03:27,030 --> 00:03:31,050
bring in these bring these pieces of

00:03:28,650 --> 00:03:32,910
information together detect this in the

00:03:31,050 --> 00:03:34,890
cloud and then send a notification back

00:03:32,910 --> 00:03:38,070
to the source system so the clinician

00:03:34,890 --> 00:03:40,770
can intervene appropriately so of course

00:03:38,070 --> 00:03:43,380
our processing demands only get stronger

00:03:40,770 --> 00:03:45,930
as we started also roll these out so

00:03:43,380 --> 00:03:48,060
rather than tens of minutes for a chart

00:03:45,930 --> 00:03:49,770
search solution in terms of processing

00:03:48,060 --> 00:03:52,830
now we really needs to be single digit

00:03:49,770 --> 00:03:54,239
minutes 0 or faster so you can probably

00:03:52,830 --> 00:03:55,260
guess where this is going next it's like

00:03:54,239 --> 00:03:57,600
well we have all this data in the cloud

00:03:55,260 --> 00:04:00,950
we want to look at it as it happens as

00:03:57,600 --> 00:04:03,120
it gets updated so here's a couple of

00:04:00,950 --> 00:04:06,170
screenshots from an ipad application

00:04:03,120 --> 00:04:09,810
that that that we're continuing to build

00:04:06,170 --> 00:04:12,390
that allows novel ways of exploring the

00:04:09,810 --> 00:04:14,070
medical records so what's interesting is

00:04:12,390 --> 00:04:16,350
that there's a lot of healthcare

00:04:14,070 --> 00:04:18,239
information systems out there and all of

00:04:16,350 --> 00:04:21,030
which have their own representation the

00:04:18,239 --> 00:04:23,430
own structure of the medical records and

00:04:21,030 --> 00:04:25,200
what happens is that oftentimes the way

00:04:23,430 --> 00:04:26,790
a clinician wants to view it or explore

00:04:25,200 --> 00:04:29,580
it may not

00:04:26,790 --> 00:04:31,050
aligned with the structure the way the

00:04:29,580 --> 00:04:34,230
data model is persisted in the source

00:04:31,050 --> 00:04:36,690
system so we actually use this Hadoop

00:04:34,230 --> 00:04:39,450
ecosystem to build precomputed models

00:04:36,690 --> 00:04:41,670
that match the way the access patterns

00:04:39,450 --> 00:04:43,200
that users have the system so I want to

00:04:41,670 --> 00:04:45,360
look at data in certain way we can

00:04:43,200 --> 00:04:47,160
compute a model a data model that's

00:04:45,360 --> 00:04:49,800
optimized to explore that certain data

00:04:47,160 --> 00:04:51,810
way in that way and the result is a very

00:04:49,800 --> 00:04:54,300
fast load time so you can bring up this

00:04:51,810 --> 00:04:56,460
iPad app and just flip through all kinds

00:04:54,300 --> 00:04:59,190
of history all kinds of information

00:04:56,460 --> 00:05:02,310
about a patient that normally would take

00:04:59,190 --> 00:05:05,040
in multiple seconds in order to compute

00:05:02,310 --> 00:05:07,530
that so since we want to explore real

00:05:05,040 --> 00:05:09,570
data will our latency demands get even

00:05:07,530 --> 00:05:11,940
stronger we need to be able to process

00:05:09,570 --> 00:05:13,830
incoming data enters the system we need

00:05:11,940 --> 00:05:16,830
to process it and make it available to

00:05:13,830 --> 00:05:18,450
users in seconds so at this point we're

00:05:16,830 --> 00:05:20,640
out of the realm of it what we can

00:05:18,450 --> 00:05:24,420
typically do with a Hadoop MapReduce

00:05:20,640 --> 00:05:26,280
bake ecosystem and of course there's a

00:05:24,420 --> 00:05:28,140
lot of others that are continuing to pop

00:05:26,280 --> 00:05:30,540
up in terms of doing care coordination

00:05:28,140 --> 00:05:33,450
analyze the health of populations coming

00:05:30,540 --> 00:05:35,310
up with personalized health plans our

00:05:33,450 --> 00:05:37,080
datasets are growing fast we're getting

00:05:35,310 --> 00:05:39,890
hundreds of gigabytes every day into the

00:05:37,080 --> 00:05:42,660
system we're approaching a petabyte of

00:05:39,890 --> 00:05:46,140
total data and the rate is only

00:05:42,660 --> 00:05:46,860
increasing so by the we totally expect

00:05:46,140 --> 00:05:49,230
that we're gonna be dealing with

00:05:46,860 --> 00:05:53,190
multiple petabytes if not this year then

00:05:49,230 --> 00:05:55,440
definitely in the coming years so what

00:05:53,190 --> 00:05:58,440
we have here with this is a trend

00:05:55,440 --> 00:06:00,060
towards these competing needs where some

00:05:58,440 --> 00:06:02,910
of these problems really call for the

00:06:00,060 --> 00:06:05,310
need to analyze entire sets of

00:06:02,910 --> 00:06:07,350
population data holistically in order to

00:06:05,310 --> 00:06:09,930
answer questions about those but other

00:06:07,350 --> 00:06:12,420
these problems need us to quickly apply

00:06:09,930 --> 00:06:14,520
incremental updates so if something new

00:06:12,420 --> 00:06:16,170
some new pieces of information about a

00:06:14,520 --> 00:06:17,760
patient occurs we need to be able to

00:06:16,170 --> 00:06:19,650
take that information we need to be able

00:06:17,760 --> 00:06:22,800
to update those views so they can be

00:06:19,650 --> 00:06:26,040
explored and these really are a trend

00:06:22,800 --> 00:06:27,930
towards competing needs in that format

00:06:26,040 --> 00:06:30,210
produce processing grab all the data

00:06:27,930 --> 00:06:33,600
with stream based processing won't get

00:06:30,210 --> 00:06:35,610
grab individual updates in what

00:06:33,600 --> 00:06:37,020
MapReduce we if you have to move the

00:06:35,610 --> 00:06:38,669
computation do the data because you

00:06:37,020 --> 00:06:41,030
can't there's too much data to move

00:06:38,669 --> 00:06:44,750
around where streams is the office

00:06:41,030 --> 00:06:48,360
and similarly mapreduce the output is

00:06:44,750 --> 00:06:51,030
logically a pure function of your input

00:06:48,360 --> 00:06:53,310
and that you generally have your set of

00:06:51,030 --> 00:06:55,290
input that you pass to a MapReduce job

00:06:53,310 --> 00:06:57,210
you execute that function and then

00:06:55,290 --> 00:06:59,100
produce a very large output whereas

00:06:57,210 --> 00:07:01,890
streaming processing you don't have a

00:06:59,100 --> 00:07:07,080
static state of the system things are

00:07:01,890 --> 00:07:08,970
changing around you and then further

00:07:07,080 --> 00:07:10,590
with streaming processing your pink pony

00:07:08,970 --> 00:07:12,750
damia may be incomplete and may be out

00:07:10,590 --> 00:07:16,380
of order just based on the way that it

00:07:12,750 --> 00:07:18,090
streams into the system and to make

00:07:16,380 --> 00:07:20,280
things even more complicated both of

00:07:18,090 --> 00:07:22,500
these processing models have to have the

00:07:20,280 --> 00:07:24,690
same underlying logic whether we

00:07:22,500 --> 00:07:27,570
construct a patient view or increase in

00:07:24,690 --> 00:07:29,490
construct a search index via MapReduce

00:07:27,570 --> 00:07:33,030
job or apply incremental changes to it

00:07:29,490 --> 00:07:37,020
we need the same output so how do we go

00:07:33,030 --> 00:07:39,540
about approaching this problem well the

00:07:37,020 --> 00:07:41,010
author of storm Nathan Mars there's

00:07:39,540 --> 00:07:43,740
written fairly extensively about this

00:07:41,010 --> 00:07:45,930
and he has a pretty good model of how to

00:07:43,740 --> 00:07:48,150
separate these needs and so he tends to

00:07:45,930 --> 00:07:50,160
describe things take these two competing

00:07:48,150 --> 00:07:51,660
age the streaming incremental processing

00:07:50,160 --> 00:07:53,250
and the MapReduce needs and keep them

00:07:51,660 --> 00:07:56,100
separate because they are very separate

00:07:53,250 --> 00:07:57,810
things so I'm borrowing to a terminology

00:07:56,100 --> 00:07:59,550
that Mars use is the speed layer and the

00:07:57,810 --> 00:08:00,870
batch layer thick and other times you

00:07:59,550 --> 00:08:04,380
may use the real-time layer in The

00:08:00,870 --> 00:08:08,310
Bachelor where our speed layer deals

00:08:04,380 --> 00:08:10,260
with hours of data we need to move it

00:08:08,310 --> 00:08:12,600
moves data to the computation it's very

00:08:10,260 --> 00:08:14,370
low latency or is the bachelor layer

00:08:12,600 --> 00:08:16,920
deals with many years of data I can do

00:08:14,370 --> 00:08:19,650
bulk loads so this pattern is working

00:08:16,920 --> 00:08:22,320
out pretty well and what we're doing is

00:08:19,650 --> 00:08:24,720
this is kind of our incarnation of that

00:08:22,320 --> 00:08:26,430
pattern where we're using Twitter's

00:08:24,720 --> 00:08:28,530
storm technology to do our incremental

00:08:26,430 --> 00:08:30,960
stream based processing we're using

00:08:28,530 --> 00:08:33,690
Hadoop MapReduce to do that the batch

00:08:30,960 --> 00:08:36,030
processing and what you'll notice that's

00:08:33,690 --> 00:08:38,240
interesting in here is that Apache HBase

00:08:36,030 --> 00:08:41,520
I kind of have it straddling the middle

00:08:38,240 --> 00:08:44,100
because HBase provides some properties

00:08:41,520 --> 00:08:45,960
of both of these worlds you can run

00:08:44,100 --> 00:08:47,880
MapReduce jobs directly against HBase

00:08:45,960 --> 00:08:50,400
and you get data locality since you can

00:08:47,880 --> 00:08:52,970
move your computation to the data but at

00:08:50,400 --> 00:08:55,279
the same time you can do incremental

00:08:52,970 --> 00:08:57,170
updates inserts and you can do rent you

00:08:55,279 --> 00:09:00,800
have a random access random reads and

00:08:57,170 --> 00:09:03,439
writes into HBase so I really think that

00:09:00,800 --> 00:09:05,959
the best way to think about HBase is

00:09:03,439 --> 00:09:09,439
it's really random access to the Hadoop

00:09:05,959 --> 00:09:10,879
distributed file system and a lot of

00:09:09,439 --> 00:09:12,110
problems you did for a lot of problems

00:09:10,879 --> 00:09:14,420
you don't need that random access but

00:09:12,110 --> 00:09:16,670
when you do need it you really do and

00:09:14,420 --> 00:09:17,839
HBase is a good tool for that so we'll

00:09:16,670 --> 00:09:20,509
talk about how some ways that we're

00:09:17,839 --> 00:09:22,129
using HBase specifically in conjunction

00:09:20,509 --> 00:09:26,329
to these other technologies to solve

00:09:22,129 --> 00:09:30,319
this this real-time processing so let's

00:09:26,329 --> 00:09:32,360
jump into the rabbit hole so I'm going

00:09:30,319 --> 00:09:34,939
to take a ride through the system that

00:09:32,360 --> 00:09:36,670
we built and talk about some of the

00:09:34,939 --> 00:09:39,110
lessons that we've learned along the way

00:09:36,670 --> 00:09:40,339
so let's start at the top and that is

00:09:39,110 --> 00:09:43,819
the first thing you have to do is get

00:09:40,339 --> 00:09:47,029
data into the system and this is pretty

00:09:43,819 --> 00:09:49,790
straightforward we simply have a set of

00:09:47,029 --> 00:09:52,279
HTTPS services that can accept content

00:09:49,790 --> 00:09:55,220
from a disparate set of data sources

00:09:52,279 --> 00:09:57,290
some of these may many of these live

00:09:55,220 --> 00:09:59,329
within our data center but we also

00:09:57,290 --> 00:10:01,819
accept connections authenticated secure

00:09:59,329 --> 00:10:03,529
connections from external clients either

00:10:01,819 --> 00:10:06,889
over dedicated pipes or over the

00:10:03,529 --> 00:10:09,170
Internet so we stream data and generally

00:10:06,889 --> 00:10:10,879
at this point the content that we're

00:10:09,170 --> 00:10:14,209
bringing in is stored as protocol

00:10:10,879 --> 00:10:17,629
buffers and this was largely done at the

00:10:14,209 --> 00:10:19,910
time time thrift wasn't particularly

00:10:17,629 --> 00:10:21,110
mature and a bro i'm not sure if it

00:10:19,910 --> 00:10:23,240
existed when we started doing this

00:10:21,110 --> 00:10:25,399
project so i think any of those

00:10:23,240 --> 00:10:27,350
technologies at work protobuf at the

00:10:25,399 --> 00:10:29,480
time was our best option and there

00:10:27,350 --> 00:10:32,959
hasn't been a sufficient need to change

00:10:29,480 --> 00:10:34,790
that and one thing that's important to

00:10:32,959 --> 00:10:36,649
call out is that what we're trying to

00:10:34,790 --> 00:10:40,220
achieve at this point is we want to

00:10:36,649 --> 00:10:43,449
mirror the raw representation of these

00:10:40,220 --> 00:10:47,480
sources data as simply as possible and

00:10:43,449 --> 00:10:49,850
the reason for that is is if there's any

00:10:47,480 --> 00:10:51,350
need to reprocess data if there's any if

00:10:49,850 --> 00:10:53,029
there's a bug or an enhancement that

00:10:51,350 --> 00:10:55,430
when you want to deploy we want to push

00:10:53,029 --> 00:10:57,559
as much of our logic as much as the

00:10:55,430 --> 00:11:00,019
complexity as possible into the cloud

00:10:57,559 --> 00:11:02,329
rather than in the systems that are

00:11:00,019 --> 00:11:05,089
sending or aggregating the data so that

00:11:02,329 --> 00:11:06,680
way we can quickly deploy any changes

00:11:05,089 --> 00:11:11,480
rerun a set of mapper t

00:11:06,680 --> 00:11:13,490
jobs and address them so let's talk

00:11:11,480 --> 00:11:18,680
about how we're processing this this

00:11:13,490 --> 00:11:20,600
incoming data in real time so some of

00:11:18,680 --> 00:11:22,670
you google wrote a pretty good paper a

00:11:20,600 --> 00:11:25,070
handful of years ago called percolator

00:11:22,670 --> 00:11:26,750
and in the percolator system they

00:11:25,070 --> 00:11:28,880
described some techniques they were

00:11:26,750 --> 00:11:31,360
doing in order to do much more low

00:11:28,880 --> 00:11:34,820
latency updates to their search index

00:11:31,360 --> 00:11:37,910
and the gist of the percolator paper is

00:11:34,820 --> 00:11:40,310
that it's built on BigTable which is you

00:11:37,910 --> 00:11:43,790
know the the Google's which the original

00:11:40,310 --> 00:11:47,300
source of the design for HBase and the

00:11:43,790 --> 00:11:50,270
pattern is is that they have a column

00:11:47,300 --> 00:11:52,790
family that's full of data and then they

00:11:50,270 --> 00:11:55,130
have a column family that indicates to

00:11:52,790 --> 00:11:57,050
which every time a specific cell of data

00:11:55,130 --> 00:12:00,470
is updated they write what they call a

00:11:57,050 --> 00:12:03,470
notification so your data may have in

00:12:00,470 --> 00:12:05,149
tens or hundreds of billions of rows but

00:12:03,470 --> 00:12:08,420
over any given period of time only a

00:12:05,149 --> 00:12:10,730
very small subset of those rows were

00:12:08,420 --> 00:12:11,750
actually changed so every time

00:12:10,730 --> 00:12:13,670
something's changed their vote in

00:12:11,750 --> 00:12:16,730
notification and then have an external

00:12:13,670 --> 00:12:19,339
process google calls them observer that

00:12:16,730 --> 00:12:21,650
scans your notification table for

00:12:19,339 --> 00:12:24,620
changes so we know what to pick up and

00:12:21,650 --> 00:12:27,110
process so this works pretty well and

00:12:24,620 --> 00:12:28,790
and so we basically we've taken this

00:12:27,110 --> 00:12:32,390
pattern and applied it to hbase but

00:12:28,790 --> 00:12:35,959
there's some catches the first is that a

00:12:32,390 --> 00:12:39,230
percolator style for notifications

00:12:35,959 --> 00:12:41,120
requires external coordination of the

00:12:39,230 --> 00:12:43,130
the processes that are scanning and

00:12:41,120 --> 00:12:44,630
doing the work since they're all

00:12:43,130 --> 00:12:46,250
scanning that notification table

00:12:44,630 --> 00:12:48,470
multiple processes could grab the same

00:12:46,250 --> 00:12:51,950
item and want to process it redundantly

00:12:48,470 --> 00:12:53,360
which is pretty inefficient it also and

00:12:51,950 --> 00:12:54,860
that external coordination means more

00:12:53,360 --> 00:12:57,740
infrastructure to build more structure

00:12:54,860 --> 00:13:00,529
to maintain so we wanted to find the

00:12:57,740 --> 00:13:02,810
approach in which we could use H basis

00:13:00,529 --> 00:13:05,270
primitives rather than standing up

00:13:02,810 --> 00:13:07,940
additional infrastructure so we've taken

00:13:05,270 --> 00:13:10,940
the Google's percolator model and evolve

00:13:07,940 --> 00:13:15,079
it somewhat to what looks like this and

00:13:10,940 --> 00:13:17,029
that it's similar to scanning for

00:13:15,079 --> 00:13:20,390
updates but what we actually are doing

00:13:17,029 --> 00:13:22,340
is we actually have a specific column

00:13:20,390 --> 00:13:24,620
hbase that we use to manage the

00:13:22,340 --> 00:13:27,950
coordination of the processes of the

00:13:24,620 --> 00:13:32,360
observer processes and and since HBase

00:13:27,950 --> 00:13:34,610
supports supports atomic operations what

00:13:32,360 --> 00:13:36,710
we can do is that if when if I grab a

00:13:34,610 --> 00:13:38,840
set of items that I want to process I

00:13:36,710 --> 00:13:41,090
can store all those items a row and I

00:13:38,840 --> 00:13:44,180
can do it atomically claim a lease

00:13:41,090 --> 00:13:46,430
column in that row at least qualifier in

00:13:44,180 --> 00:13:50,210
that row then process that and when I'm

00:13:46,430 --> 00:13:52,660
done i unlock my release the lease that

00:13:50,210 --> 00:13:54,800
was claimed and then lease has a timeout

00:13:52,660 --> 00:13:57,290
associated with it so in case there's a

00:13:54,800 --> 00:13:59,150
process failure that least will expire

00:13:57,290 --> 00:14:03,950
and somebody else can grab and pick it

00:13:59,150 --> 00:14:06,290
up so this works pretty well we were

00:14:03,950 --> 00:14:09,350
able to 3,000 notifications per second

00:14:06,290 --> 00:14:11,720
per node which was sufficient for our

00:14:09,350 --> 00:14:13,970
use cases this and it scales out you you

00:14:11,720 --> 00:14:16,130
can scale out to an arbitrary number of

00:14:13,970 --> 00:14:20,300
nodes it generally scary scales linearly

00:14:16,130 --> 00:14:21,920
with the size of your HBase cluster so

00:14:20,300 --> 00:14:23,510
we found a number advantages to this one

00:14:21,920 --> 00:14:25,010
is that we didn't need to operate

00:14:23,510 --> 00:14:26,690
additional infrastructure outside of

00:14:25,010 --> 00:14:29,930
HBase which the operational

00:14:26,690 --> 00:14:31,880
considerations are big deal the other

00:14:29,930 --> 00:14:34,490
big advantage is that it leverages the

00:14:31,880 --> 00:14:36,410
strong guarantees that HBase makes so

00:14:34,490 --> 00:14:38,560
everything that stored in HBase all of

00:14:36,410 --> 00:14:41,180
your data is persisted in HDFS

00:14:38,560 --> 00:14:43,070
replicated three times so this means

00:14:41,180 --> 00:14:45,200
that where there was no risk of any lost

00:14:43,070 --> 00:14:47,690
data and there was no risk of data being

00:14:45,200 --> 00:14:49,760
stranded or unavailable due to a failure

00:14:47,690 --> 00:14:51,740
of a single machine so if a machine or

00:14:49,760 --> 00:14:53,420
failed in the Hadoop cluster who Duke

00:14:51,740 --> 00:14:56,950
would detect that re replicate those

00:14:53,420 --> 00:14:59,990
blocks across the cluster and so we're

00:14:56,950 --> 00:15:04,070
being in health care we had very strong

00:14:59,990 --> 00:15:06,860
requirements of data availability so so

00:15:04,070 --> 00:15:09,110
this was a big deal for us and then the

00:15:06,860 --> 00:15:12,200
other advantage is that it's robust to

00:15:09,110 --> 00:15:13,760
volume very high volume spikes so one of

00:15:12,200 --> 00:15:15,820
the great things about HBase is it

00:15:13,760 --> 00:15:18,110
handles right throughputs very well

00:15:15,820 --> 00:15:20,630
assuming that you distribute your rights

00:15:18,110 --> 00:15:23,630
fairly evenly across the across many

00:15:20,630 --> 00:15:27,020
regions servers in your cluster so if we

00:15:23,630 --> 00:15:29,840
had some client that was and this turned

00:15:27,020 --> 00:15:32,060
out to be important because it turns out

00:15:29,840 --> 00:15:34,090
that the the usage patterns that we've

00:15:32,060 --> 00:15:36,290
seen in our clients were very uneven

00:15:34,090 --> 00:15:37,850
there's certain like if we're

00:15:36,290 --> 00:15:41,090
aggregating data from a particular data

00:15:37,850 --> 00:15:43,370
source they may run batch jobs or batch

00:15:41,090 --> 00:15:44,960
imports that goes and updates a huge

00:15:43,370 --> 00:15:46,940
number of rows in the database and then

00:15:44,960 --> 00:15:49,370
brings that all on to our clusters so

00:15:46,940 --> 00:15:52,040
being able to absorb these very large

00:15:49,370 --> 00:15:54,620
volume spikes from our clients was a big

00:15:52,040 --> 00:15:55,790
deal and this actually I mean this

00:15:54,620 --> 00:15:59,210
actually made it so we didn't have to

00:15:55,790 --> 00:16:02,120
create an artificial back pressure on to

00:15:59,210 --> 00:16:03,560
the producers of our data in this case

00:16:02,120 --> 00:16:04,820
we didn't have to make that more

00:16:03,560 --> 00:16:07,460
complicated to the people that are

00:16:04,820 --> 00:16:09,620
sending the data we are able to just

00:16:07,460 --> 00:16:12,680
absorb those spikes by spreading it out

00:16:09,620 --> 00:16:15,830
over an HBase cluster now there's some

00:16:12,680 --> 00:16:17,810
downsides as well one is weak they're

00:16:15,830 --> 00:16:20,390
the ordering guarantees are weak in this

00:16:17,810 --> 00:16:24,440
leasing model and that we may get

00:16:20,390 --> 00:16:27,860
processed data out of order it also is

00:16:24,440 --> 00:16:29,540
essentially an at least once processing

00:16:27,860 --> 00:16:31,100
model and that everything that comes in

00:16:29,540 --> 00:16:32,810
gets persisted it'll get picked up and

00:16:31,100 --> 00:16:35,450
processed but if there's a failure may

00:16:32,810 --> 00:16:36,710
be processed multiple times so it's

00:16:35,450 --> 00:16:39,080
important that all of our processing

00:16:36,710 --> 00:16:41,000
logic be idempotent which is generally a

00:16:39,080 --> 00:16:45,140
good practice in large distributed

00:16:41,000 --> 00:16:47,420
systems anyway lots of garbage from

00:16:45,140 --> 00:16:49,250
deleted cells if you're not deeply

00:16:47,420 --> 00:16:51,410
familiar with HBase when you do a delete

00:16:49,250 --> 00:16:56,020
you don't really delete it you mark it

00:16:51,410 --> 00:16:56,020
as pending for deletion so as a result

00:16:56,050 --> 00:17:00,140
garbage will pile up until you run a

00:16:58,160 --> 00:17:02,030
major compaction on it so you have to

00:17:00,140 --> 00:17:04,699
schedule and manage major compaction and

00:17:02,030 --> 00:17:07,730
then we also have to aggressively split

00:17:04,699 --> 00:17:09,560
data split these notification tables in

00:17:07,730 --> 00:17:11,300
order to avoid a hot region it's really

00:17:09,560 --> 00:17:13,550
easy we wrote if we were to put all

00:17:11,300 --> 00:17:15,079
these these rows on one region server we

00:17:13,550 --> 00:17:16,760
would light up that region server and

00:17:15,079 --> 00:17:18,949
and really limit our throughput and

00:17:16,760 --> 00:17:22,010
potentially in potentially starve users

00:17:18,949 --> 00:17:23,959
of it so but we know with these caveats

00:17:22,010 --> 00:17:26,390
this turns out to be a fairly scalable

00:17:23,959 --> 00:17:27,680
and robust system and the other downside

00:17:26,390 --> 00:17:29,750
that that I want to call it is that I

00:17:27,680 --> 00:17:32,630
think that there are likely to be better

00:17:29,750 --> 00:17:35,720
options emerging so there was a great

00:17:32,630 --> 00:17:37,870
talk yesterday about Kafka and its

00:17:35,720 --> 00:17:39,800
replication features that are coming in

00:17:37,870 --> 00:17:41,600
we're excited about that and I think

00:17:39,800 --> 00:17:45,350
we're going to be evaluating that as an

00:17:41,600 --> 00:17:47,059
alternative to this notification replace

00:17:45,350 --> 00:17:48,919
of this notification mechanism

00:17:47,059 --> 00:17:50,090
while keeping the rest of the

00:17:48,919 --> 00:17:52,519
architecture that we're subscribing

00:17:50,090 --> 00:17:56,539
today the same it's one of those things

00:17:52,519 --> 00:17:58,249
where and we're working with HBase we're

00:17:56,539 --> 00:18:00,289
kind of pushing some of the limits on

00:17:58,249 --> 00:18:02,450
the typical usage patterns of HBase and

00:18:00,289 --> 00:18:05,509
while we've been successful we've had to

00:18:02,450 --> 00:18:07,370
we I won't claim that all these things

00:18:05,509 --> 00:18:10,070
didn't come with a fair amount of

00:18:07,370 --> 00:18:11,960
lessons learnt along the way so we

00:18:10,070 --> 00:18:13,159
definitely prefer to be using open

00:18:11,960 --> 00:18:15,830
source projects in a way that's more

00:18:13,159 --> 00:18:17,629
broad legal used in the community and

00:18:15,830 --> 00:18:19,610
since Kafka seems like a good tool for

00:18:17,629 --> 00:18:22,669
this so haven't used it yet but we're

00:18:19,610 --> 00:18:24,409
going to be digging into it another

00:18:22,669 --> 00:18:26,559
thing that I want to call out this is

00:18:24,409 --> 00:18:29,509
the importance of really good

00:18:26,559 --> 00:18:30,799
measurements across the system so here's

00:18:29,509 --> 00:18:34,309
a couple of screenshots this is just a

00:18:30,799 --> 00:18:37,249
graphite view of it so but what we

00:18:34,309 --> 00:18:40,070
actually did is that we took our the

00:18:37,249 --> 00:18:41,450
HBase client and which provides a nice

00:18:40,070 --> 00:18:43,399
job interface that you can write your

00:18:41,450 --> 00:18:44,809
own implementation sub and we wrote an

00:18:43,399 --> 00:18:47,539
instrumented version of that Java

00:18:44,809 --> 00:18:50,539
interface that then passes through and

00:18:47,539 --> 00:18:52,159
we simply use the metrics API just

00:18:50,539 --> 00:18:54,710
excellent API if you're looking to do

00:18:52,159 --> 00:18:57,950
instrumentation in Java and a graphite

00:18:54,710 --> 00:19:00,940
reporter for that and some interesting

00:18:57,950 --> 00:19:03,740
properties emerged from this in that

00:19:00,940 --> 00:19:06,320
HBase modern version of HBase and Hadoop

00:19:03,740 --> 00:19:08,330
has really excellent instrumentation on

00:19:06,320 --> 00:19:10,669
the server side you can look at any

00:19:08,330 --> 00:19:12,110
region server you can look at its

00:19:10,669 --> 00:19:14,450
latency its throughput and I mean

00:19:12,110 --> 00:19:15,980
everything that's happening there so you

00:19:14,450 --> 00:19:17,840
get a good piece of information with

00:19:15,980 --> 00:19:20,360
that but what we found is that that

00:19:17,840 --> 00:19:24,350
didn't really reveal the full picture of

00:19:20,360 --> 00:19:26,600
our usage and our performance behavior

00:19:24,350 --> 00:19:29,749
when using HBase and that what we're

00:19:26,600 --> 00:19:31,009
seeing is that well under load under

00:19:29,749 --> 00:19:32,710
these real-time load we saw these a

00:19:31,009 --> 00:19:36,139
whole bunch of very very short-lived

00:19:32,710 --> 00:19:37,789
cubes pop up on their our region servers

00:19:36,139 --> 00:19:39,619
so a request would come in and it just

00:19:37,789 --> 00:19:42,080
it might be backed up because we didn't

00:19:39,619 --> 00:19:43,429
have enough request handlers set up but

00:19:42,080 --> 00:19:46,159
very bit for a very short period of time

00:19:43,429 --> 00:19:48,049
so we saw that we from the region server

00:19:46,159 --> 00:19:49,340
perspective and we're thinking okay well

00:19:48,049 --> 00:19:50,869
you know everything is going through

00:19:49,340 --> 00:19:54,230
we're processing a ton of requests per

00:19:50,869 --> 00:19:56,690
second it isn't a big deal but when we

00:19:54,230 --> 00:19:59,330
instrumented the client side all these

00:19:56,690 --> 00:20:00,920
very short-lived queues summed up to

00:19:59,330 --> 00:20:03,530
create a

00:20:00,920 --> 00:20:05,960
nificant performance degradation in the

00:20:03,530 --> 00:20:08,800
consumer from the consumer perspective

00:20:05,960 --> 00:20:11,690
of the system so the point being is that

00:20:08,800 --> 00:20:14,390
all the interpretation that exists on

00:20:11,690 --> 00:20:16,280
the server and Hadoop and HBase is great

00:20:14,390 --> 00:20:18,890
you can learn a lot from it but it's

00:20:16,280 --> 00:20:21,020
generally not sufficient so this

00:20:18,890 --> 00:20:23,450
basically revealed the impact of some

00:20:21,020 --> 00:20:24,800
hot regions on our cluster once we

00:20:23,450 --> 00:20:26,270
discover this with this instrumentation

00:20:24,800 --> 00:20:28,730
we were able to rebalance in our

00:20:26,270 --> 00:20:30,950
Parranda just age basis configuration

00:20:28,730 --> 00:20:35,870
which created a pretty significant

00:20:30,950 --> 00:20:40,610
performance impact all right so tie back

00:20:35,870 --> 00:20:43,010
in so so here's the story so far we

00:20:40,610 --> 00:20:45,050
bring data into the system and then we

00:20:43,010 --> 00:20:46,610
landed in HBase and we initiate our

00:20:45,050 --> 00:20:50,180
profit and we've talked about how to

00:20:46,610 --> 00:20:52,010
initiate processing and so now we're

00:20:50,180 --> 00:20:54,080
going to talk about how we actually host

00:20:52,010 --> 00:20:56,440
and operate the logic and the process

00:20:54,080 --> 00:20:58,310
themselves and so jumping into the storm

00:20:56,440 --> 00:21:00,530
so for those of you that aren't familiar

00:20:58,310 --> 00:21:03,770
with storm just touch on it really

00:21:00,530 --> 00:21:06,440
briefly it's essentially just a scalable

00:21:03,770 --> 00:21:10,040
mechanism of processing data in motion

00:21:06,440 --> 00:21:11,330
it's a compliment HBase in Hindu and

00:21:10,040 --> 00:21:14,630
that is focused on real time in

00:21:11,330 --> 00:21:16,700
incremental processing and the killer

00:21:14,630 --> 00:21:20,020
feature of storm and the reason that we

00:21:16,700 --> 00:21:23,840
had jumped into it is it has excellent

00:21:20,020 --> 00:21:26,990
guarantees of message processing in a

00:21:23,840 --> 00:21:28,880
distributed environment so you can you

00:21:26,990 --> 00:21:30,620
can google some references on I'm

00:21:28,880 --> 00:21:32,360
storming to talk about the mechanism it

00:21:30,620 --> 00:21:35,300
works but essentially it once a message

00:21:32,360 --> 00:21:37,340
enter storm is it it can process through

00:21:35,300 --> 00:21:39,950
a topology of processing nodes and be

00:21:37,340 --> 00:21:41,780
admitted to the other to some output and

00:21:39,950 --> 00:21:44,630
storm X very strong guarantees that

00:21:41,780 --> 00:21:46,880
either the message will be processed or

00:21:44,630 --> 00:21:48,440
if there was a failure it can go and ask

00:21:46,880 --> 00:21:50,030
the data source for the message say give

00:21:48,440 --> 00:21:52,870
it to me again so I can process it again

00:21:50,030 --> 00:21:55,520
later so a lot of the alternative

00:21:52,870 --> 00:21:57,740
complex event processing type systems

00:21:55,520 --> 00:21:59,840
don't have these kind of strong

00:21:57,740 --> 00:22:01,940
guarantees so so we looked at things

00:21:59,840 --> 00:22:04,220
like s4 we looked at a couple other

00:22:01,940 --> 00:22:07,070
sieepiess of some proprietary CEP

00:22:04,220 --> 00:22:08,930
systems and and and interestingly storm

00:22:07,070 --> 00:22:10,820
was the only only system like this that

00:22:08,930 --> 00:22:14,180
I'm aware of that provides these strong

00:22:10,820 --> 00:22:14,850
guarantees and so the way we essentially

00:22:14,180 --> 00:22:16,350
integrated

00:22:14,850 --> 00:22:20,900
with storm is that we wrote our own

00:22:16,350 --> 00:22:23,430
storm spout that pulls in that scans

00:22:20,900 --> 00:22:26,040
this notification table that described

00:22:23,430 --> 00:22:27,960
earlier and then emits events across the

00:22:26,040 --> 00:22:29,730
storm cluster this turns out to be

00:22:27,960 --> 00:22:32,220
pretty straightforward to do a

00:22:29,730 --> 00:22:34,230
statistics storm spout that does HBase

00:22:32,220 --> 00:22:41,280
scans against table and then emits

00:22:34,230 --> 00:22:45,390
output from it so our picture now looks

00:22:41,280 --> 00:22:47,460
like this so this works fairly well but

00:22:45,390 --> 00:22:50,430
of course there's a number of challenges

00:22:47,460 --> 00:22:52,440
that are just intrinsic to doing

00:22:50,430 --> 00:22:55,050
incremental updates incremental

00:22:52,440 --> 00:22:57,030
processing one is dealing with

00:22:55,050 --> 00:22:58,950
incomplete data and that you're getting

00:22:57,030 --> 00:23:00,480
data one piece at a time you may not

00:22:58,950 --> 00:23:04,350
have the whole picture now there's

00:23:00,480 --> 00:23:06,900
outdated state so a good example of this

00:23:04,350 --> 00:23:09,450
is if I if I change my name from Smith

00:23:06,900 --> 00:23:12,000
to Jones I have to go and i'm building a

00:23:09,450 --> 00:23:13,650
phone book i have to remove the smith

00:23:12,000 --> 00:23:14,910
entry from the phone book whereas an

00:23:13,650 --> 00:23:16,200
in-app reduce job i just throw out the

00:23:14,910 --> 00:23:18,210
phone book and rebuild the whole thing

00:23:16,200 --> 00:23:21,210
which actually adds a creates a number

00:23:18,210 --> 00:23:23,850
of complicated edge cases and also

00:23:21,210 --> 00:23:26,340
creates some timing and state conditions

00:23:23,850 --> 00:23:27,510
that can be difficult to reason about so

00:23:26,340 --> 00:23:30,210
i'm going to talk about some of the

00:23:27,510 --> 00:23:32,400
techniques that we're using to handle

00:23:30,210 --> 00:23:35,850
these complexities of doing real time

00:23:32,400 --> 00:23:38,220
processing so the first with regards to

00:23:35,850 --> 00:23:39,570
handling and complete data so this is

00:23:38,220 --> 00:23:42,990
kind of a pattern that we've emerged so

00:23:39,570 --> 00:23:45,090
imagine that you have a in a simple

00:23:42,990 --> 00:23:46,920
example you're building a document they

00:23:45,090 --> 00:23:48,510
have multiple pages so you're in you

00:23:46,920 --> 00:23:50,850
have one page streaming into the system

00:23:48,510 --> 00:23:52,530
at a time so page one enters the system

00:23:50,850 --> 00:23:54,030
and then and remember events can be

00:23:52,530 --> 00:23:58,080
entering the system out of order so page

00:23:54,030 --> 00:24:00,210
one and page three enters the system so

00:23:58,080 --> 00:24:03,270
what we're doing is so H interval page

00:24:00,210 --> 00:24:05,280
we can map that into you can do some

00:24:03,270 --> 00:24:07,590
pre-processing for the page and then

00:24:05,280 --> 00:24:11,010
essentially merge those pages together

00:24:07,590 --> 00:24:13,740
to create the output summary for our

00:24:11,010 --> 00:24:16,110
document so what we're doing here is

00:24:13,740 --> 00:24:18,240
that in HBase since we can do those

00:24:16,110 --> 00:24:20,280
random access those random reads we're

00:24:18,240 --> 00:24:23,190
able to actually take data that may not

00:24:20,280 --> 00:24:26,280
be complete yet and stage it persisted

00:24:23,190 --> 00:24:27,990
off in HBase so that when we have that

00:24:26,280 --> 00:24:29,580
complete document we can

00:24:27,990 --> 00:24:32,340
merge them together to produce our

00:24:29,580 --> 00:24:34,620
output artifact and so what's strange is

00:24:32,340 --> 00:24:38,250
that even though this isn't MapReduce in

00:24:34,620 --> 00:24:40,830
Hadoop sense these operations are fully

00:24:38,250 --> 00:24:42,870
analogous to a map function and a

00:24:40,830 --> 00:24:45,179
reduced function we're mapping a raw

00:24:42,870 --> 00:24:47,130
page element to a process page element

00:24:45,179 --> 00:24:49,320
and then we're reducing those sets of

00:24:47,130 --> 00:24:52,920
pages to an output document so you can

00:24:49,320 --> 00:24:54,840
kind of use the same mental model and

00:24:52,920 --> 00:24:57,720
there is so the result is is that this

00:24:54,840 --> 00:25:00,570
is kind of a rolling MapReduce as data

00:24:57,720 --> 00:25:02,730
enters the system and the nice thing is

00:25:00,570 --> 00:25:05,730
that because it's the similar mental

00:25:02,730 --> 00:25:07,800
model to a Hadoop style MapReduce a lot

00:25:05,730 --> 00:25:09,750
of the code is reusable between these

00:25:07,800 --> 00:25:13,170
possible processing infrastructures and

00:25:09,750 --> 00:25:16,350
I also call out is that many cases may

00:25:13,170 --> 00:25:17,940
not need a merge phase and a consuming

00:25:16,350 --> 00:25:19,559
application could read all the

00:25:17,940 --> 00:25:21,690
individual pages itself and merge it

00:25:19,559 --> 00:25:24,510
together at read time and this is much

00:25:21,690 --> 00:25:26,850
like how not all MapReduce jobs need a

00:25:24,510 --> 00:25:28,380
reduce phase and in fact its MapReduce

00:25:26,850 --> 00:25:31,230
jobs that don't need a reduce phase are

00:25:28,380 --> 00:25:35,010
more efficient and so some a lot of the

00:25:31,230 --> 00:25:36,900
same reasoning applies here so the next

00:25:35,010 --> 00:25:39,720
one I want to touch on is dealing with

00:25:36,900 --> 00:25:42,559
outdated state so imagine the scenario

00:25:39,720 --> 00:25:45,150
in which we were building indexes of

00:25:42,559 --> 00:25:48,780
residents of chicago residents of New

00:25:45,150 --> 00:25:52,050
York so at time 0 Alice lives in Chicago

00:25:48,780 --> 00:25:53,760
on and so she would live in the that

00:25:52,050 --> 00:25:56,070
index and then the time one she lives in

00:25:53,760 --> 00:25:57,990
New York she would exist and the other

00:25:56,070 --> 00:26:00,900
index it seems like a pretty simple

00:25:57,990 --> 00:26:02,429
problem so how do you so if you're just

00:26:00,900 --> 00:26:04,020
if you're dealing with a typical big

00:26:02,429 --> 00:26:06,570
data processing model it's pretty

00:26:04,020 --> 00:26:09,000
straightforward it's like running

00:26:06,570 --> 00:26:11,340
MapReduce job and you rebuild both

00:26:09,000 --> 00:26:13,860
indexes from scratch so outdated state

00:26:11,340 --> 00:26:17,040
is simply ignored so we throw away the

00:26:13,860 --> 00:26:19,350
indexes Alice new lives in New York that

00:26:17,040 --> 00:26:20,850
index is updated so that's very simple

00:26:19,350 --> 00:26:22,590
very easy to reason about which is one

00:26:20,850 --> 00:26:24,200
of the big advantages of MapReduce is

00:26:22,590 --> 00:26:26,790
the fact that we can reason about

00:26:24,200 --> 00:26:29,190
dealing with large data sets very simply

00:26:26,790 --> 00:26:30,390
it's it's really one of its values so

00:26:29,190 --> 00:26:34,110
that works for big data and now if we're

00:26:30,390 --> 00:26:36,000
dealing with fast updates and we can fit

00:26:34,110 --> 00:26:37,770
something in an acid database you know

00:26:36,000 --> 00:26:39,990
fit everything in some relational

00:26:37,770 --> 00:26:41,220
completely normalized data that's really

00:26:39,990 --> 00:26:43,200
simple to it's like we simple

00:26:41,220 --> 00:26:46,200
update Alice's location from one spot to

00:26:43,200 --> 00:26:48,510
another but where we want into trouble

00:26:46,200 --> 00:26:50,520
is that if we have data that is very

00:26:48,510 --> 00:26:54,720
large that it can't fit all into a

00:26:50,520 --> 00:26:56,940
single coordinated database but it's

00:26:54,720 --> 00:26:59,520
coming in too fast to apply MapReduce

00:26:56,940 --> 00:27:02,190
style reprocess the world events this

00:26:59,520 --> 00:27:04,679
come becomes a challenge because neither

00:27:02,190 --> 00:27:06,539
of these simple and easy to understand

00:27:04,679 --> 00:27:09,679
processing in metaphors can be applied

00:27:06,539 --> 00:27:12,210
in this context so this gets really hard

00:27:09,679 --> 00:27:15,210
so there's couple techniques to do it

00:27:12,210 --> 00:27:16,890
one is what I'm referring to as

00:27:15,210 --> 00:27:18,900
reconciling these on Reed and this is

00:27:16,890 --> 00:27:20,460
akin to the Nate's Martha's lambda

00:27:18,900 --> 00:27:24,530
architecture that he he speaks about

00:27:20,460 --> 00:27:27,289
sometime in that basically we can have

00:27:24,530 --> 00:27:30,150
data stores that are optimized for

00:27:27,289 --> 00:27:31,470
specific workloads so you can have your

00:27:30,150 --> 00:27:33,960
big data store for your MapReduce

00:27:31,470 --> 00:27:35,280
processing you have a smaller store for

00:27:33,960 --> 00:27:37,710
your incremental updates and then you

00:27:35,280 --> 00:27:40,380
simply merge the two items together at

00:27:37,710 --> 00:27:42,330
query time so in this slide I'm calling

00:27:40,380 --> 00:27:44,610
things in green that are positives and

00:27:42,330 --> 00:27:45,960
read that are negative so so this is

00:27:44,610 --> 00:27:48,150
great because the data stores can be

00:27:45,960 --> 00:27:49,440
optimized for their workloads they can

00:27:48,150 --> 00:27:50,850
do what they do well you don't have to

00:27:49,440 --> 00:27:52,770
have one data store that does both

00:27:50,850 --> 00:27:55,580
real-time and batch processing really

00:27:52,770 --> 00:27:59,580
well it also keeps the processing model

00:27:55,580 --> 00:28:01,080
independent so I can run my my batch

00:27:59,580 --> 00:28:03,330
processing my real time processing

00:28:01,080 --> 00:28:04,860
topologies independently one another I

00:28:03,330 --> 00:28:06,780
don't have to worry about combining them

00:28:04,860 --> 00:28:09,150
together and all the race and timing

00:28:06,780 --> 00:28:11,909
conditions that occur when dealing with

00:28:09,150 --> 00:28:14,400
that it does add complexity a tree time

00:28:11,909 --> 00:28:17,340
but I think overall it's a it's a simple

00:28:14,400 --> 00:28:19,409
model of the alternatives but I think

00:28:17,340 --> 00:28:22,890
that one of the big downside is that

00:28:19,409 --> 00:28:26,370
this type of pattern isn't available in

00:28:22,890 --> 00:28:29,010
commodity web or applications stacks it

00:28:26,370 --> 00:28:30,900
may emerge so if your application stack

00:28:29,010 --> 00:28:34,110
is a popular web framework if you're

00:28:30,900 --> 00:28:36,630
building a django or ruby on rails you

00:28:34,110 --> 00:28:38,190
probably want to use an existing data

00:28:36,630 --> 00:28:40,320
store that's well integrated with those

00:28:38,190 --> 00:28:41,820
app frameworks versus having to write

00:28:40,320 --> 00:28:44,130
code that merges you know these two

00:28:41,820 --> 00:28:46,650
outputs together so the result is is

00:28:44,130 --> 00:28:50,220
that we lose the ability to use existing

00:28:46,650 --> 00:28:53,010
commoditized stacks it would the need to

00:28:50,220 --> 00:28:54,809
to build our own but despite that

00:28:53,010 --> 00:28:55,230
drawback I think that this probably is

00:28:54,809 --> 00:28:57,500
the

00:28:55,230 --> 00:29:00,360
best approach to solving this problem

00:28:57,500 --> 00:29:02,910
particularly when an if higher level

00:29:00,360 --> 00:29:04,980
abstractions emerge to solve this for us

00:29:02,910 --> 00:29:08,280
likes right now like we have to write

00:29:04,980 --> 00:29:09,480
this merge logic ourselves there's no

00:29:08,280 --> 00:29:11,190
reason why this shouldn't be an

00:29:09,480 --> 00:29:13,410
abstraction where this data is

00:29:11,190 --> 00:29:14,700
transparently merged and nobody writing

00:29:13,410 --> 00:29:17,850
the application needs to be visible to

00:29:14,700 --> 00:29:19,380
it the technology that does it isn't

00:29:17,850 --> 00:29:21,120
widely available right now I know that

00:29:19,380 --> 00:29:24,690
there are some projects that are

00:29:21,120 --> 00:29:28,890
exploring that that be interesting to

00:29:24,690 --> 00:29:31,950
track one example is a rich hickies

00:29:28,890 --> 00:29:34,230
company at atomic as using a model very

00:29:31,950 --> 00:29:39,419
similar to this so I'm kind of keeping

00:29:34,230 --> 00:29:41,790
eyes on that with interest so the

00:29:39,419 --> 00:29:45,390
alternative that is is that to reconcile

00:29:41,790 --> 00:29:49,169
data all right so if we keep a history

00:29:45,390 --> 00:29:51,330
of our incoming data so like I said

00:29:49,169 --> 00:29:53,010
remember in our HBase live we keep all

00:29:51,330 --> 00:29:55,470
of our raw data so we have the fact that

00:29:53,010 --> 00:29:58,770
at time zero Alice lived in Chicago and

00:29:55,470 --> 00:30:00,390
at time one Alice moved to New York so

00:29:58,770 --> 00:30:02,309
what we can do is that when event at

00:30:00,390 --> 00:30:04,910
time one occurs we can look at that

00:30:02,309 --> 00:30:08,970
whole history and then no oh I need to

00:30:04,910 --> 00:30:12,030
remove Alice from the Chicago index and

00:30:08,970 --> 00:30:15,150
insert her into the New York index so

00:30:12,030 --> 00:30:17,100
which is great because the destination

00:30:15,150 --> 00:30:18,559
data store i'm basically bending my

00:30:17,100 --> 00:30:20,940
process to meets the needs of that

00:30:18,559 --> 00:30:23,460
destination store so i can use a

00:30:20,940 --> 00:30:27,090
commodity data store for a commodity web

00:30:23,460 --> 00:30:29,820
stack so that makes it easier there but

00:30:27,090 --> 00:30:31,590
it adds significant complexity to the

00:30:29,820 --> 00:30:33,240
processing logic and that every time

00:30:31,590 --> 00:30:35,280
something comes in i have to look at the

00:30:33,240 --> 00:30:38,010
history and i have to manually take care

00:30:35,280 --> 00:30:41,010
of obsolete state this is this

00:30:38,010 --> 00:30:43,559
complexity is can be a challenge to

00:30:41,010 --> 00:30:45,720
manage and so this is a pretty

00:30:43,559 --> 00:30:48,179
significant downside to this we are

00:30:45,720 --> 00:30:49,169
doing this in a number of cases but it's

00:30:48,179 --> 00:30:52,440
something you have to be very proactive

00:30:49,169 --> 00:30:54,330
to handle on handle because it's it is

00:30:52,440 --> 00:30:55,860
it can be prone to error when you need

00:30:54,330 --> 00:30:59,070
to manually be updating two separate

00:30:55,860 --> 00:31:00,570
things so it's so whenever possible I'd

00:30:59,070 --> 00:31:01,650
like to see patterns or frameworks to

00:31:00,570 --> 00:31:04,080
emerge so they don't have to think about

00:31:01,650 --> 00:31:06,090
that we've done some of this internally

00:31:04,080 --> 00:31:07,920
it's been a challenge to generalize

00:31:06,090 --> 00:31:11,130
these patterns to

00:31:07,920 --> 00:31:13,530
arbitrary use case but maybe maybe this

00:31:11,130 --> 00:31:16,610
will emerge at one point the other

00:31:13,530 --> 00:31:18,960
downside to that is that the data store

00:31:16,610 --> 00:31:21,290
that you're writing to must handle both

00:31:18,960 --> 00:31:23,700
MapReduce and real-time processing

00:31:21,290 --> 00:31:25,410
versus being having two separate data

00:31:23,700 --> 00:31:30,030
stores that are really optimized for

00:31:25,410 --> 00:31:31,890
these different models so through all

00:31:30,030 --> 00:31:34,770
this we have MapReduce in real-time

00:31:31,890 --> 00:31:38,640
processing in and they but they really

00:31:34,770 --> 00:31:40,590
had the same logic and so the

00:31:38,640 --> 00:31:42,600
implication here is that if you want to

00:31:40,590 --> 00:31:46,140
have the same logic between MapReduce

00:31:42,600 --> 00:31:47,970
and real-time processing functions have

00:31:46,140 --> 00:31:49,980
to be the center of your universe in

00:31:47,970 --> 00:31:51,990
MapReduce we often think of terms of

00:31:49,980 --> 00:31:54,000
like input formats and output formats or

00:31:51,990 --> 00:31:57,030
in a stream based processing my thing

00:31:54,000 --> 00:31:58,980
the things the messages but those aren't

00:31:57,030 --> 00:32:00,660
those do not intersect they do not

00:31:58,980 --> 00:32:03,540
overlap between these two processing

00:32:00,660 --> 00:32:05,910
models so the key is is that really we

00:32:03,540 --> 00:32:07,860
need to write logic as pure functions as

00:32:05,910 --> 00:32:09,510
much as possible so giving a defined

00:32:07,860 --> 00:32:11,430
input we always produce the same output

00:32:09,510 --> 00:32:13,380
and then we need to coordinate those

00:32:11,430 --> 00:32:15,180
functions with higher level libraries

00:32:13,380 --> 00:32:18,150
that can be deployed neither of these

00:32:15,180 --> 00:32:21,480
processing models so our choices for the

00:32:18,150 --> 00:32:23,910
higher-level libraries use mentions

00:32:21,480 --> 00:32:26,190
distort the storm project and then we're

00:32:23,910 --> 00:32:28,680
also early adopters of the apache crunch

00:32:26,190 --> 00:32:30,060
which just became a top-level project

00:32:28,680 --> 00:32:32,550
last week we've been using it for

00:32:30,060 --> 00:32:35,340
several months now doing our own builds

00:32:32,550 --> 00:32:36,560
out of the incubator check out apache

00:32:35,340 --> 00:32:38,640
crunch if you need to coordinate

00:32:36,560 --> 00:32:41,340
MapReduce jobs it has a number of it has

00:32:38,640 --> 00:32:43,500
the number of nice features has very

00:32:41,340 --> 00:32:47,520
strongly typed data structures that can

00:32:43,500 --> 00:32:49,320
be changed together so and so since we

00:32:47,520 --> 00:32:50,760
deal a lot with things like protocol

00:32:49,320 --> 00:32:52,740
buffers we're able to take our protocol

00:32:50,760 --> 00:32:54,660
buffers and build a strongly typed

00:32:52,740 --> 00:32:57,960
processing pipeline passing them from

00:32:54,660 --> 00:33:02,190
one to another and in the Crites a great

00:32:57,960 --> 00:33:03,870
development experience so when we go

00:33:02,190 --> 00:33:05,790
through all this one thing that's to be

00:33:03,870 --> 00:33:08,490
going up coming to mind is that this is

00:33:05,790 --> 00:33:11,610
getting complicated and complexity this

00:33:08,490 --> 00:33:13,170
is a really big deal incremental

00:33:11,610 --> 00:33:15,410
processing logic and dealing with these

00:33:13,170 --> 00:33:19,650
states is very complex it's error-prone

00:33:15,410 --> 00:33:21,720
and and as our challenges and since it's

00:33:19,650 --> 00:33:24,360
error-prone we need to make sure that

00:33:21,720 --> 00:33:28,260
we have a fail safe for them and to that

00:33:24,360 --> 00:33:30,120
failsafe is MapReduce so we take that

00:33:28,260 --> 00:33:32,490
original picture that we have and we

00:33:30,120 --> 00:33:33,600
have the storm processing topology but

00:33:32,490 --> 00:33:35,970
then we want to complement that with

00:33:33,600 --> 00:33:38,220
MapReduce and this is an important

00:33:35,970 --> 00:33:40,470
aspect I think when designing real time

00:33:38,220 --> 00:33:43,680
processing is you tend to might focus on

00:33:40,470 --> 00:33:44,610
the real time aspects of it but there's

00:33:43,680 --> 00:33:46,890
going to be a scenario in which

00:33:44,610 --> 00:33:48,510
something you have a bug or you have an

00:33:46,890 --> 00:33:50,130
enhancement you have something that goes

00:33:48,510 --> 00:33:51,930
wrong in your processing logic and you

00:33:50,130 --> 00:33:54,270
want to really be able to reprocess the

00:33:51,930 --> 00:33:56,520
world so it's important that we design

00:33:54,270 --> 00:33:59,100
everything so they think from that way

00:33:56,520 --> 00:34:02,690
it can be executed in a MapReduce style

00:33:59,100 --> 00:34:04,800
as well in order to address those things

00:34:02,690 --> 00:34:07,380
so I'm going to touch on one technique

00:34:04,800 --> 00:34:09,270
that we've been using to do that in this

00:34:07,380 --> 00:34:11,700
case our output our input data stories

00:34:09,270 --> 00:34:17,550
HBase and our output data store is also

00:34:11,700 --> 00:34:19,710
HBase so so imagine that we go back to

00:34:17,550 --> 00:34:23,730
that document example now one of the

00:34:19,710 --> 00:34:27,270
neat things about HBase is that you can

00:34:23,730 --> 00:34:29,760
mean that time is actually something of

00:34:27,270 --> 00:34:32,010
a malleable concept in HBase and that

00:34:29,760 --> 00:34:33,900
everything is written at a specific time

00:34:32,010 --> 00:34:35,970
stamp but you can override that time

00:34:33,900 --> 00:34:39,090
stamp so let's say in this example I

00:34:35,970 --> 00:34:41,100
have one document that's written at

00:34:39,090 --> 00:34:44,370
timestamp 50 another one that's written

00:34:41,100 --> 00:34:46,530
at time step 100 and so how do i do a

00:34:44,370 --> 00:34:48,450
rolling upgrade of logic and reprocess

00:34:46,530 --> 00:34:52,740
my whole history well the first thing we

00:34:48,450 --> 00:34:55,020
can do is deploy a new new logic using

00:34:52,740 --> 00:34:59,250
our storm incremental processing so new

00:34:55,020 --> 00:35:00,690
data may come in you know it sorry son

00:34:59,250 --> 00:35:05,130
you made it new data may come in at

00:35:00,690 --> 00:35:06,390
timestamp 300 so that gets our new data

00:35:05,130 --> 00:35:08,520
using our new logic but how do we

00:35:06,390 --> 00:35:11,880
retrofit all of our old data without

00:35:08,520 --> 00:35:14,910
overwriting new updates that came in as

00:35:11,880 --> 00:35:18,540
part of our real-time stream and we can

00:35:14,910 --> 00:35:21,120
actually do that by writing older data

00:35:18,540 --> 00:35:23,970
with older timestamps in HBase so this

00:35:21,120 --> 00:35:27,240
example we have a new entry that came in

00:35:23,970 --> 00:35:29,760
at timestamp 300 but I could run a

00:35:27,240 --> 00:35:31,830
MapReduce job that goes and writes it at

00:35:29,760 --> 00:35:34,380
an artificially lower timestamp so I

00:35:31,830 --> 00:35:35,100
don't my newer data will overwrite my

00:35:34,380 --> 00:35:37,870
old

00:35:35,100 --> 00:35:39,790
even though the MapReduce job is being

00:35:37,870 --> 00:35:44,650
executed underneath a real time

00:35:39,790 --> 00:35:46,870
processing topology so the gist is that

00:35:44,650 --> 00:35:49,120
the the most recent cell written in

00:35:46,870 --> 00:35:51,850
HBase doesn't need to be the logically

00:35:49,120 --> 00:35:54,220
newest cell written HBase and you can

00:35:51,850 --> 00:35:56,050
take advantage of that in order to run a

00:35:54,220 --> 00:35:57,430
MapReduce job against a set of data

00:35:56,050 --> 00:35:59,860
without having to worry about

00:35:57,430 --> 00:36:05,860
overwriting newer updates that may be

00:35:59,860 --> 00:36:07,600
streaming into the system so we're going

00:36:05,860 --> 00:36:10,720
to take this picture and we're going to

00:36:07,600 --> 00:36:12,790
complete it here and the the last major

00:36:10,720 --> 00:36:17,800
part of this architecture is building

00:36:12,790 --> 00:36:20,740
search indexes over HBase so there's a

00:36:17,800 --> 00:36:26,620
couple techniques for doing this one is

00:36:20,740 --> 00:36:28,300
building indexes with MapReduce and what

00:36:26,620 --> 00:36:30,070
will happen what's interesting that this

00:36:28,300 --> 00:36:36,160
is actually a fairly established pattern

00:36:30,070 --> 00:36:39,250
and then we can build a duo solar or

00:36:36,160 --> 00:36:41,770
leucine shard in each map tasks so we

00:36:39,250 --> 00:36:44,500
can scale it out / map tests we

00:36:41,770 --> 00:36:47,290
construct those indexes locally in that

00:36:44,500 --> 00:36:49,510
map or reduce process so it's all

00:36:47,290 --> 00:36:52,660
physically located on the MapReduce

00:36:49,510 --> 00:36:54,520
cluster which so it's very scalable and

00:36:52,660 --> 00:36:57,610
then once the processing is done we

00:36:54,520 --> 00:37:00,070
simply copy the index that's built in to

00:36:57,610 --> 00:37:02,710
do if we basically rsync it over to our

00:37:00,070 --> 00:37:04,510
cluster of solar servers so we do all

00:37:02,710 --> 00:37:05,800
the work in Hadoop so we're not

00:37:04,510 --> 00:37:07,780
competing with queries against those

00:37:05,800 --> 00:37:09,340
servers once we have a new version of

00:37:07,780 --> 00:37:11,890
our index built we can synchronize it

00:37:09,340 --> 00:37:15,120
over to the solar servers to get that

00:37:11,890 --> 00:37:18,340
latest index so that works great

00:37:15,120 --> 00:37:20,680
assuming that that assuming that your

00:37:18,340 --> 00:37:22,720
index updates don't need to be use this

00:37:20,680 --> 00:37:25,240
real-time processing metaphor so how do

00:37:22,720 --> 00:37:27,280
we make our index updates faster to

00:37:25,240 --> 00:37:30,160
complement this MapReduce tile

00:37:27,280 --> 00:37:31,690
processing a couple ways to do it one

00:37:30,160 --> 00:37:33,850
that's kind of a more typical approach

00:37:31,690 --> 00:37:36,480
that's supported by a solar out of the

00:37:33,850 --> 00:37:39,430
boxes is to essentially post new records

00:37:36,480 --> 00:37:42,070
so we have some sort of stream of data

00:37:39,430 --> 00:37:44,740
we post them to our solar shards on and

00:37:42,070 --> 00:37:47,170
then they get replicated over and are

00:37:44,740 --> 00:37:48,380
hosted in the search index so this works

00:37:47,170 --> 00:37:51,289
pretty well in a lot

00:37:48,380 --> 00:37:53,170
cases but one of the things that like I

00:37:51,289 --> 00:37:55,970
mentioned earlier is that we have very

00:37:53,170 --> 00:37:57,500
asymmetric data flow patterns from a

00:37:55,970 --> 00:38:00,170
variety of clients like sometimes we'll

00:37:57,500 --> 00:38:03,019
have huge bursts and as a result a huge

00:38:00,170 --> 00:38:04,789
burst can overwhelm this system so if I

00:38:03,019 --> 00:38:07,490
have a ton of data being pushed in to

00:38:04,789 --> 00:38:09,019
the solar shards and we can affect the

00:38:07,490 --> 00:38:11,119
performance and availability of those

00:38:09,019 --> 00:38:12,559
solar shards and there's ways you can

00:38:11,119 --> 00:38:17,930
mitigate that by creating artificial

00:38:12,559 --> 00:38:21,410
back pressure but but we also have to

00:38:17,930 --> 00:38:23,569
deal with any transient failures so if i

00:38:21,410 --> 00:38:26,380
have my data stream coming in what if a

00:38:23,569 --> 00:38:29,299
solar shard is temporarily unavailable

00:38:26,380 --> 00:38:31,099
maybe there's a network glitch that

00:38:29,299 --> 00:38:32,990
lasts you know only a handful of seconds

00:38:31,099 --> 00:38:34,309
but i can't temporarily send that you

00:38:32,990 --> 00:38:36,740
know i have to queue that up and then be

00:38:34,309 --> 00:38:39,170
able to try and resend that so it adds

00:38:36,740 --> 00:38:41,480
complexity to how we're pushing data on

00:38:39,170 --> 00:38:43,460
taller so on to our solar servers so

00:38:41,480 --> 00:38:46,789
this is workable but there's complexity

00:38:43,460 --> 00:38:50,720
there and it is it's somewhat vulnerable

00:38:46,789 --> 00:38:52,309
to spikes or transient failures so the

00:38:50,720 --> 00:38:54,500
approach that we've taken to save the to

00:38:52,309 --> 00:38:56,930
solve this is actually we've someone

00:38:54,500 --> 00:39:00,440
inverted this model in that rather than

00:38:56,930 --> 00:39:02,410
pushing data to solar we've written a

00:39:00,440 --> 00:39:05,240
customized solar plug-in that

00:39:02,410 --> 00:39:07,880
essentially each shard in our solar

00:39:05,240 --> 00:39:10,279
cluster is responsible for a subset of

00:39:07,880 --> 00:39:13,130
data in HBase and the solar shard

00:39:10,279 --> 00:39:16,609
actually is executing scans against a

00:39:13,130 --> 00:39:20,150
it's subset of data so it will just do a

00:39:16,609 --> 00:39:22,849
range scan / H base and that scan will

00:39:20,150 --> 00:39:24,829
be a time-based so it can scan over a

00:39:22,849 --> 00:39:27,769
range of HBase and say what has changed

00:39:24,829 --> 00:39:30,680
during this period of time and then it

00:39:27,769 --> 00:39:33,529
as it detects changes it can pull items

00:39:30,680 --> 00:39:35,329
into its index this turns out to be

00:39:33,529 --> 00:39:37,069
pretty efficient HBS has a nice

00:39:35,329 --> 00:39:39,799
optimization so if you do a time range

00:39:37,069 --> 00:39:42,200
based scan it won't even open won't even

00:39:39,799 --> 00:39:43,670
bother looking at some of its data files

00:39:42,200 --> 00:39:46,039
that don't have data in that time range

00:39:43,670 --> 00:39:47,869
so this turns out to be fairly efficient

00:39:46,039 --> 00:39:51,799
and the other thing is that this really

00:39:47,869 --> 00:39:55,039
cleanly recovers from any transient

00:39:51,799 --> 00:39:56,630
volume spikes or failures so if there's

00:39:55,039 --> 00:39:58,369
a temporary outage in the solar server

00:39:56,630 --> 00:40:00,319
if it's not pulling data or there's a

00:39:58,369 --> 00:40:02,060
huge volume spiked in HBase the solar

00:40:00,319 --> 00:40:06,620
server is going to pull data from H

00:40:02,060 --> 00:40:10,280
bass as fast as the solar server can and

00:40:06,620 --> 00:40:11,720
since HBase is just just being the read

00:40:10,280 --> 00:40:13,910
source of data this doesn't put that

00:40:11,720 --> 00:40:17,960
much load on HBase so this turns out to

00:40:13,910 --> 00:40:20,120
be a pretty resilient a pretty resilient

00:40:17,960 --> 00:40:21,470
system and I think that one of the

00:40:20,120 --> 00:40:22,940
engineers that's working on it's

00:40:21,470 --> 00:40:24,470
actually going to give a talk at that

00:40:22,940 --> 00:40:26,450
the next leucine revolution conference

00:40:24,470 --> 00:40:28,430
so get a chance to see ben Brown out

00:40:26,450 --> 00:40:30,950
there use the one of the main architects

00:40:28,430 --> 00:40:34,700
that design the system so I lemak highly

00:40:30,950 --> 00:40:36,830
recommend it going to see his talk so

00:40:34,700 --> 00:40:41,750
I'm going to close with one more note on

00:40:36,830 --> 00:40:45,080
on HBase and how we lay out data and

00:40:41,750 --> 00:40:48,320
that is hbase schema tends to get really

00:40:45,080 --> 00:40:49,850
complicated and you see a lot of tables

00:40:48,320 --> 00:40:52,310
that have this sort of pattern in which

00:40:49,850 --> 00:40:54,230
you have in the row key itself you have

00:40:52,310 --> 00:40:56,150
a person and you'll have some sort of

00:40:54,230 --> 00:40:57,590
entity that lives or have a parent

00:40:56,150 --> 00:41:02,660
entity than a child and maybe null

00:40:57,590 --> 00:41:03,800
nested multiple times deep and this sort

00:41:02,660 --> 00:41:06,740
of data structure is actually very

00:41:03,800 --> 00:41:09,230
efficient it's recommend i means you if

00:41:06,740 --> 00:41:10,670
you look through the history of H basis

00:41:09,230 --> 00:41:12,020
mailing lists and this is a common

00:41:10,670 --> 00:41:14,240
pattern that pops up that's recommended

00:41:12,020 --> 00:41:16,400
because you can if I want to scan

00:41:14,240 --> 00:41:19,190
everything that belongs to person one I

00:41:16,400 --> 00:41:21,620
can easily set up a scan across that

00:41:19,190 --> 00:41:24,050
person and since H basis sequential

00:41:21,620 --> 00:41:26,510
reads are actually very fast this is

00:41:24,050 --> 00:41:29,860
this is a good inefficient pattern the

00:41:26,510 --> 00:41:31,910
downside is it's hard to reason about

00:41:29,860 --> 00:41:34,970
and a lot of that is because I have to

00:41:31,910 --> 00:41:36,230
look at I can't look at a single row and

00:41:34,970 --> 00:41:38,990
say hey this is a person I have to look

00:41:36,230 --> 00:41:41,480
at the contents of a row in HBase and

00:41:38,990 --> 00:41:42,950
say okay well this is this is a person

00:41:41,480 --> 00:41:46,550
address and this next row is a person

00:41:42,950 --> 00:41:48,200
named and so it becomes more difficult

00:41:46,550 --> 00:41:50,060
to reason about because then we have to

00:41:48,200 --> 00:41:53,000
look at that conscious the row it also

00:41:50,060 --> 00:41:55,640
mismatches tooling that lives in the

00:41:53,000 --> 00:41:57,500
Hadoop ecosystem so a thing like hive or

00:41:55,640 --> 00:41:59,180
something like pig works really well if

00:41:57,500 --> 00:42:01,820
you think of your data is a tabular

00:41:59,180 --> 00:42:03,620
format where you have a bunch of Records

00:42:01,820 --> 00:42:05,690
and those records following oops they

00:42:03,620 --> 00:42:08,240
follow a tabular structure so they're

00:42:05,690 --> 00:42:10,430
all basically the same thing in this

00:42:08,240 --> 00:42:11,750
case it's very difficult to get this

00:42:10,430 --> 00:42:14,890
sort of thing to play nice with those

00:42:11,750 --> 00:42:16,720
those models so

00:42:14,890 --> 00:42:18,750
well we've migrated to a different

00:42:16,720 --> 00:42:21,940
structure for HBase and that is

00:42:18,750 --> 00:42:25,269
essentially a logical parent item / vo--

00:42:21,940 --> 00:42:27,250
and then having a very wide table for

00:42:25,269 --> 00:42:31,240
many for all the contents in that room

00:42:27,250 --> 00:42:33,670
and you can't think of HBase like a

00:42:31,240 --> 00:42:35,740
typical relational model HBase is very

00:42:33,670 --> 00:42:37,720
resilient to very wide rows there's some

00:42:35,740 --> 00:42:39,400
practical limits but typically tens of

00:42:37,720 --> 00:42:42,910
thousands of items in a single row in

00:42:39,400 --> 00:42:45,369
HBase is not a problem so this makes the

00:42:42,910 --> 00:42:47,140
RO unit the unit locality so everything

00:42:45,369 --> 00:42:49,480
that belongs to this person simply

00:42:47,140 --> 00:42:51,579
exists in that row it's a tabular layout

00:42:49,480 --> 00:42:53,529
so we've been able to take hive and just

00:42:51,579 --> 00:42:56,529
simply map it directly onto our HBase

00:42:53,529 --> 00:43:00,640
structure it's worked out worked out

00:42:56,529 --> 00:43:01,960
really well very easy to reason about in

00:43:00,640 --> 00:43:07,960
most cases there are no lost

00:43:01,960 --> 00:43:09,910
efficiencies so they're having a bunch

00:43:07,960 --> 00:43:11,589
of data in a single row if you're having

00:43:09,910 --> 00:43:14,470
tons of updates to that row you can

00:43:11,589 --> 00:43:15,970
create contention around that and then a

00:43:14,470 --> 00:43:19,059
row can never be split across multiple

00:43:15,970 --> 00:43:20,890
regions in HBase so if you have

00:43:19,059 --> 00:43:22,690
something obscene ridiculously big then

00:43:20,890 --> 00:43:24,430
that could be an issue but we have not

00:43:22,690 --> 00:43:26,170
approached either of these limits in our

00:43:24,430 --> 00:43:27,940
practical use so this has been a really

00:43:26,170 --> 00:43:29,529
great simple pattern if you're

00:43:27,940 --> 00:43:31,420
interested in more on that there was a

00:43:29,529 --> 00:43:35,009
talk at that at last year's the 2012

00:43:31,420 --> 00:43:38,650
HBase con even violate laid this out in

00:43:35,009 --> 00:43:40,480
fantastic form and so I highly recommend

00:43:38,650 --> 00:43:42,759
if you're looking at if you're looking

00:43:40,480 --> 00:43:45,730
at modeling data and HBase that this

00:43:42,759 --> 00:43:49,420
talk was excellent so it's well worth

00:43:45,730 --> 00:43:51,970
well worth the time doing so with all

00:43:49,420 --> 00:43:54,269
then I'm going to talk about our where

00:43:51,970 --> 00:43:57,069
we think a lot of this is going in that

00:43:54,269 --> 00:43:58,599
the patterns that we followed here that

00:43:57,069 --> 00:44:01,410
we've talked about this and we've been

00:43:58,599 --> 00:44:05,170
successful doing these sorts of things

00:44:01,410 --> 00:44:07,900
but it remains that complexity of these

00:44:05,170 --> 00:44:08,980
systems is our biggest enemy I mean

00:44:07,900 --> 00:44:12,490
there's a lot of pieces here and it's

00:44:08,980 --> 00:44:13,960
difficult to recent about and I feel

00:44:12,490 --> 00:44:16,180
like this that we may be living in a

00:44:13,960 --> 00:44:19,000
sort of assembly language era of big

00:44:16,180 --> 00:44:20,950
data and that all things are possible we

00:44:19,000 --> 00:44:22,750
can and that all these patterns that we

00:44:20,950 --> 00:44:24,630
described we can execute we can run data

00:44:22,750 --> 00:44:28,310
very large data sets we can process

00:44:24,630 --> 00:44:29,930
incrementally but it requires very deep

00:44:28,310 --> 00:44:33,800
knowledge about how these systems work

00:44:29,930 --> 00:44:35,150
there's there's not and we have there's

00:44:33,800 --> 00:44:38,750
a lot of additional code that you have

00:44:35,150 --> 00:44:41,450
to write which kind of sets the stage

00:44:38,750 --> 00:44:43,270
where I expect higher level abstractions

00:44:41,450 --> 00:44:46,160
are going to emerge for these patterns

00:44:43,270 --> 00:44:48,140
so much like I don't think anybody

00:44:46,160 --> 00:44:50,030
should be writing direct MapReduce jobs

00:44:48,140 --> 00:44:52,520
anymore against the Hadoop MapReduce API

00:44:50,030 --> 00:44:54,680
use use cascading or crunch or pig or

00:44:52,520 --> 00:44:56,690
hive there's no reason not to not use

00:44:54,680 --> 00:45:00,740
these frameworks it's much simpler much

00:44:56,690 --> 00:45:02,840
easier to build and compose jobs in this

00:45:00,740 --> 00:45:04,280
pattern I think that we're going to need

00:45:02,840 --> 00:45:06,230
to we're going to see higher level

00:45:04,280 --> 00:45:08,270
pattern start to emerge for dealing with

00:45:06,230 --> 00:45:10,700
incremental updates as well as

00:45:08,270 --> 00:45:13,340
large-scale MapReduce processing of this

00:45:10,700 --> 00:45:16,130
data I'm not sure what all those

00:45:13,340 --> 00:45:17,480
patterns are going to look like I think

00:45:16,130 --> 00:45:19,610
the summer starting to merge but one

00:45:17,480 --> 00:45:21,250
thing I'm sure is that seeing it and

00:45:19,610 --> 00:45:24,640
being involved and it's going to be fun

00:45:21,250 --> 00:45:27,640
so with that I'm going to open it up to

00:45:24,640 --> 00:45:27,640
questions

00:45:44,800 --> 00:45:51,010
okay so the question was how does crunch

00:45:48,020 --> 00:45:53,740
compared to something like storm so

00:45:51,010 --> 00:45:58,370
interesting there a fair number of

00:45:53,740 --> 00:45:59,840
similarities between the two I'll start

00:45:58,370 --> 00:46:01,810
with the biggest difference is that you

00:45:59,840 --> 00:46:05,420
know crunch is a way to compose

00:46:01,810 --> 00:46:07,540
MapReduce jobs take I mean take a an

00:46:05,420 --> 00:46:10,310
output of one function compose it to a

00:46:07,540 --> 00:46:13,700
sequence of other functions and produce

00:46:10,310 --> 00:46:16,520
some output yeah yeah crunch is

00:46:13,700 --> 00:46:19,280
specifically about MapReduce they are

00:46:16,520 --> 00:46:21,770
similar in that that they tend to they

00:46:19,280 --> 00:46:23,720
tend to be models of composing functions

00:46:21,770 --> 00:46:25,580
together to produce an output but storm

00:46:23,720 --> 00:46:28,190
is really focused as storm is purely

00:46:25,580 --> 00:46:30,500
focused on real-time processing crunch

00:46:28,190 --> 00:46:36,070
is purely focused on that produce we use

00:46:30,500 --> 00:46:36,070
both we like both they're good tools yes

00:46:46,060 --> 00:46:56,760
is HBase your secondary database do have

00:46:52,620 --> 00:47:01,000
men database kind of Nicole horikawa

00:46:56,760 --> 00:47:03,610
something yes so so the system that I'm

00:47:01,000 --> 00:47:06,850
describing here I guess its scope is

00:47:03,610 --> 00:47:09,640
really to aggregate data from a variety

00:47:06,850 --> 00:47:11,680
of data sources and person and you know

00:47:09,640 --> 00:47:13,990
hold it there so each one of those data

00:47:11,680 --> 00:47:16,060
sources will have its own database that

00:47:13,990 --> 00:47:19,660
is ultimately the ultimately the source

00:47:16,060 --> 00:47:21,370
of truth that we're bringing together so

00:47:19,660 --> 00:47:24,070
there's there's number of databases for

00:47:21,370 --> 00:47:29,350
the various sources at the end of the

00:47:24,070 --> 00:47:30,820
processing we will persist the data the

00:47:29,350 --> 00:47:32,680
output of a processing will persist it

00:47:30,820 --> 00:47:35,530
in a model that aligns with the needs of

00:47:32,680 --> 00:47:38,170
the applications and so how that's

00:47:35,530 --> 00:47:40,660
persisted in that case depends on really

00:47:38,170 --> 00:47:43,270
those needs so in some cases will

00:47:40,660 --> 00:47:45,490
produce and will produce the output will

00:47:43,270 --> 00:47:48,370
be a simple instead of postgres

00:47:45,490 --> 00:47:50,530
databases that can be easily queried in

00:47:48,370 --> 00:47:53,470
other cases for other applications their

00:47:50,530 --> 00:47:56,020
usage patterns do align with do a line

00:47:53,470 --> 00:47:57,520
really well with HBase so things like

00:47:56,020 --> 00:47:58,720
really efficient you know sequential

00:47:57,520 --> 00:48:00,160
reads or that sort of thing where you

00:47:58,720 --> 00:48:03,640
don't need more sophisticated query

00:48:00,160 --> 00:48:06,190
patterns so we have ultimately do have a

00:48:03,640 --> 00:48:07,690
number of database technologies in some

00:48:06,190 --> 00:48:10,030
cases HBase is the right tool for the

00:48:07,690 --> 00:48:11,620
job and that is the only database that's

00:48:10,030 --> 00:48:14,350
used in those applications in other

00:48:11,620 --> 00:48:15,760
cases of a relational model or some

00:48:14,350 --> 00:48:18,610
other store is a better tool for the job

00:48:15,760 --> 00:48:21,520
and when we use those there so we do

00:48:18,610 --> 00:48:22,990
have a convoy like most apparently like

00:48:21,520 --> 00:48:25,270
most companies we do have a number of

00:48:22,990 --> 00:48:29,100
different data stores so the answer is

00:48:25,270 --> 00:48:29,100
yes yes thanks

00:48:32,900 --> 00:48:39,300
can you talk about like maybe the number

00:48:37,230 --> 00:48:40,260
of kind of like the scale of nodes

00:48:39,300 --> 00:48:42,210
you're running on each different

00:48:40,260 --> 00:48:43,920
components like you know how many each

00:48:42,210 --> 00:48:45,060
base notes you haves and how many strong

00:48:43,920 --> 00:48:47,310
words you're running in how many solar

00:48:45,060 --> 00:48:49,740
indexes you have just to get a sense of

00:48:47,310 --> 00:48:51,330
you know how big this is and second is I

00:48:49,740 --> 00:48:53,880
can see how this is powering your

00:48:51,330 --> 00:48:56,070
real-time application where you know you

00:48:53,880 --> 00:48:58,020
have agreed on the use cases and the

00:48:56,070 --> 00:49:00,930
logic that means to be run to answer the

00:48:58,020 --> 00:49:03,690
question what about like ad hoc analysis

00:49:00,930 --> 00:49:05,100
that data miners want to do against the

00:49:03,690 --> 00:49:07,490
Big Data are you are they also

00:49:05,100 --> 00:49:10,350
interacting with against the same

00:49:07,490 --> 00:49:12,270
infrastructure to run their models or

00:49:10,350 --> 00:49:14,130
experiments sure okay yeah those are

00:49:12,270 --> 00:49:16,080
great questions so in terms of the the

00:49:14,130 --> 00:49:18,090
number of notes that we have we have

00:49:16,080 --> 00:49:20,490
multiple Hadoop clusters some of the

00:49:18,090 --> 00:49:22,350
products that I've shown were developed

00:49:20,490 --> 00:49:25,080
at different times and still run on

00:49:22,350 --> 00:49:27,030
their own clusters so our largest Hadoop

00:49:25,080 --> 00:49:30,450
cluster is I mean our first offering was

00:49:27,030 --> 00:49:32,010
that chart search offering and I don't

00:49:30,450 --> 00:49:33,780
think we're well over 100 nodes I don't

00:49:32,010 --> 00:49:36,960
think what you under nodes that too on

00:49:33,780 --> 00:49:39,030
that yet and then we have a similar

00:49:36,960 --> 00:49:41,820
number of solar nodes that hosted I

00:49:39,030 --> 00:49:42,930
don't know the specifics of that all of

00:49:41,820 --> 00:49:44,820
our Hardware stacks are fairly

00:49:42,930 --> 00:49:47,040
conventional based on Hadoop

00:49:44,820 --> 00:49:49,350
recommendations so I can know with HBase

00:49:47,040 --> 00:49:50,430
I mean we've upgraded it recently but I

00:49:49,350 --> 00:49:52,830
think our newer ones will have forty

00:49:50,430 --> 00:49:56,040
eight gigs of RAM either 8 or 12 core

00:49:52,830 --> 00:49:59,160
processors and you're pretty dumb within

00:49:56,040 --> 00:50:01,080
the bounds of recommendations there in

00:49:59,160 --> 00:50:02,130
terms of so some of the newer stuff

00:50:01,080 --> 00:50:04,080
where we're doing real-time processing

00:50:02,130 --> 00:50:06,690
isn't as widely rolled out as that

00:50:04,080 --> 00:50:09,120
original charge search so in those

00:50:06,690 --> 00:50:10,800
clusters were in tens of tens of knows I

00:50:09,120 --> 00:50:13,680
think that there were 20 and 30 nodes

00:50:10,800 --> 00:50:15,210
between those clusters the storm

00:50:13,680 --> 00:50:19,020
processing itself we tend to run on

00:50:15,210 --> 00:50:20,250
virtual machines just general virtual

00:50:19,020 --> 00:50:22,650
machines there's all those advantage

00:50:20,250 --> 00:50:24,840
there's management advantages of them so

00:50:22,650 --> 00:50:26,550
I don't know exactly how many we run of

00:50:24,840 --> 00:50:29,670
the run of those but and it's probably a

00:50:26,550 --> 00:50:32,940
similar hardware specification so tens

00:50:29,670 --> 00:50:35,760
of machines but not yet hundreds and

00:50:32,940 --> 00:50:37,830
then also analytics or how it supposed

00:50:35,760 --> 00:50:39,360
to basically ad hoc experiments that it

00:50:37,830 --> 00:50:40,830
won't run against the data does this all

00:50:39,360 --> 00:50:43,020
do they also run against the same

00:50:40,830 --> 00:50:44,290
cluster yeah or is it just my only for

00:50:43,020 --> 00:50:47,970
the application consumption

00:50:44,290 --> 00:50:50,670
right so so generally they we don't run

00:50:47,970 --> 00:50:52,690
analytics against the same cluster

00:50:50,670 --> 00:50:54,820
particularly in these real-time use

00:50:52,690 --> 00:50:56,650
cases because an analytics job we could

00:50:54,820 --> 00:50:59,050
saturate resources in the cluster and

00:50:56,650 --> 00:51:01,930
then which would take away from our time

00:50:59,050 --> 00:51:04,930
critical processing so we actually do a

00:51:01,930 --> 00:51:06,940
couple things one is HBase replication

00:51:04,930 --> 00:51:09,340
from one cluster to another and then

00:51:06,940 --> 00:51:13,180
we'll do think then once we have it in

00:51:09,340 --> 00:51:14,740
that secondary cluster we use hot we use

00:51:13,180 --> 00:51:16,870
both hive and pig we kind of have

00:51:14,740 --> 00:51:19,000
different groups that like them like the

00:51:16,870 --> 00:51:21,310
different tools so those both exists

00:51:19,000 --> 00:51:24,970
there and in terms of more interactive

00:51:21,310 --> 00:51:28,360
analytics we actually have a MPP system

00:51:24,970 --> 00:51:29,890
vertica that will load and and so the

00:51:28,360 --> 00:51:32,620
folks that are more comfortable in that

00:51:29,890 --> 00:51:34,810
sort of environment will just basically

00:51:32,620 --> 00:51:37,450
treat Hadoop as a large-scale ETL system

00:51:34,810 --> 00:51:40,930
load the MPP of lug vertica and then

00:51:37,450 --> 00:51:43,420
through the queries against that so

00:51:40,930 --> 00:51:46,450
another quick question so you work in a

00:51:43,420 --> 00:51:47,620
pretty highly regulated environment so

00:51:46,450 --> 00:51:49,300
you've had to deal with HIPAA I'm

00:51:47,620 --> 00:51:51,310
assuming and those kind of things can

00:51:49,300 --> 00:51:53,110
you talk a little bit more about that

00:51:51,310 --> 00:51:56,490
how that influenced some of the things

00:51:53,110 --> 00:51:58,510
you do yeah yeah so yeah so that this

00:51:56,490 --> 00:52:01,390
probably is heavily regulated as

00:51:58,510 --> 00:52:03,520
everybody on so which has a couple of a

00:52:01,390 --> 00:52:05,350
number of implications like one is that

00:52:03,520 --> 00:52:07,720
even though we're bringing this data

00:52:05,350 --> 00:52:10,960
into our Hadoop system the system still

00:52:07,720 --> 00:52:12,910
you know belongs to you the client the

00:52:10,960 --> 00:52:15,490
client which is the hospital or into

00:52:12,910 --> 00:52:17,830
individual patients so it's not our data

00:52:15,490 --> 00:52:19,600
I'm so we're basically in HIPAA terms

00:52:17,830 --> 00:52:21,400
you a business partner relationship

00:52:19,600 --> 00:52:24,040
which allows it to us to host it in our

00:52:21,400 --> 00:52:25,930
system but there's also some strong

00:52:24,040 --> 00:52:29,020
requirements that follow below that one

00:52:25,930 --> 00:52:30,520
is as a separation of clients data so

00:52:29,020 --> 00:52:33,520
you really don't want to mix them

00:52:30,520 --> 00:52:35,980
together so and we do that by basically

00:52:33,520 --> 00:52:38,740
keeping very separate you know Rose key

00:52:35,980 --> 00:52:40,390
spaces or if when we writing stuff just

00:52:38,740 --> 00:52:42,910
HDFS Barry's you know separate spaces

00:52:40,390 --> 00:52:46,270
for each client and then the other thing

00:52:42,910 --> 00:52:48,340
that I think that makes us pretty

00:52:46,270 --> 00:52:51,250
unusual in terms of Hadoop deployments

00:52:48,340 --> 00:52:56,620
that we actually encrypt everything that

00:52:51,250 --> 00:52:57,760
we store just to their Zack to meet the

00:52:56,620 --> 00:53:01,240
regulatory needs

00:52:57,760 --> 00:53:02,980
and actually is a reassurance to our

00:53:01,240 --> 00:53:04,930
clients in case anything gets

00:53:02,980 --> 00:53:09,340
compromised and the way that we're

00:53:04,930 --> 00:53:11,200
actually doing that is well we had been

00:53:09,340 --> 00:53:12,400
using actually encrypted hard drives but

00:53:11,200 --> 00:53:13,930
those are really expensive and they have

00:53:12,400 --> 00:53:16,900
low capacity don't align with the Hadoop

00:53:13,930 --> 00:53:19,450
model but what we can do what we've been

00:53:16,900 --> 00:53:21,400
doing more recently is using OS level

00:53:19,450 --> 00:53:24,430
encryption so the Red Hat luxe

00:53:21,400 --> 00:53:25,750
encryptions sweet so basically just

00:53:24,430 --> 00:53:28,180
using cryptid disk at the operating

00:53:25,750 --> 00:53:29,620
system level when we did some bunch of

00:53:28,180 --> 00:53:31,720
so we turned that on and we did a bunch

00:53:29,620 --> 00:53:33,610
of benchmarks and we were seeing a

00:53:31,720 --> 00:53:35,290
slight degradation in performance with

00:53:33,610 --> 00:53:37,780
that up with encryption turned on but it

00:53:35,290 --> 00:53:40,090
was within ten percent so the cost

00:53:37,780 --> 00:53:43,720
savings of using OS level encryption was

00:53:40,090 --> 00:53:47,280
definitely worth it all right we have

00:53:43,720 --> 00:53:50,350
time for one more one more question it

00:53:47,280 --> 00:53:52,960
become a situation you have the upgraded

00:53:50,350 --> 00:53:56,050
the earlier Hadoop or HBase the newer

00:53:52,960 --> 00:54:00,280
versions is impossible and how almost

00:53:56,050 --> 00:54:02,260
impossible now it's a upgrading I mean

00:54:00,280 --> 00:54:03,580
so I do piss Vance has advanced to the

00:54:02,260 --> 00:54:05,800
point where you can do it Hadoop

00:54:03,580 --> 00:54:07,150
upgrades HBase at least me on the

00:54:05,800 --> 00:54:08,770
versions that we've been using you have

00:54:07,150 --> 00:54:11,680
been able to do rolling upgrades of

00:54:08,770 --> 00:54:13,180
those so essentially hopefully we'll get

00:54:11,680 --> 00:54:14,320
there at some point hopefully HBase will

00:54:13,180 --> 00:54:16,300
get there some point I know it's being

00:54:14,320 --> 00:54:19,240
worked on so what we've done for

00:54:16,300 --> 00:54:21,280
upgrades is essentially i mentioned how

00:54:19,240 --> 00:54:23,710
we use how we use h based replication

00:54:21,280 --> 00:54:24,910
across clusters so we'll take one

00:54:23,710 --> 00:54:26,230
cluster that's live will make sure that

00:54:24,910 --> 00:54:29,350
all that data is either copied or

00:54:26,230 --> 00:54:31,960
replicated to another and then at that

00:54:29,350 --> 00:54:34,330
point we'll take down you know one

00:54:31,960 --> 00:54:36,010
cluster you know upgrade it and there's

00:54:34,330 --> 00:54:37,630
there's kind of a I mean it's kind of

00:54:36,010 --> 00:54:39,310
like a Bluegreen type model where we'll

00:54:37,630 --> 00:54:40,870
bring one up and then you have to grab

00:54:39,310 --> 00:54:43,180
everything that was written while that

00:54:40,870 --> 00:54:46,960
cluster was down so it's a fairly it's

00:54:43,180 --> 00:54:48,910
an involved process to be sure so and I

00:54:46,960 --> 00:54:50,350
think I'm not a time but i'll be happy

00:54:48,910 --> 00:54:55,410
to follow up with any other questions

00:54:50,350 --> 00:54:55,410
I'll just be up here so thank you

00:54:55,460 --> 00:54:57,520

YouTube URL: https://www.youtube.com/watch?v=MecziNEsg20


