Title: Powering CloudStack with Ceph RBD
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 2
Description: 
	Patrick McGarry
ApacheCon NA 2013
Cloud Crowd
Captions: 
	00:00:00,000 --> 00:00:07,049
hi good morning still I guess timezone

00:00:03,659 --> 00:00:09,389
appropriate greeting here here to talk a

00:00:07,049 --> 00:00:11,460
little bit about cloudstack and chefs

00:00:09,389 --> 00:00:12,900
RVD to do that I'm going to give you a

00:00:11,460 --> 00:00:15,210
little bit of an architecture overview

00:00:12,900 --> 00:00:17,520
of what's F is kind of how it works how

00:00:15,210 --> 00:00:18,779
it fits together and then at the end I

00:00:17,520 --> 00:00:21,270
can show you how it plugs into

00:00:18,779 --> 00:00:23,670
CloudStack scenes how we had such a

00:00:21,270 --> 00:00:27,510
great CloudStack introduction here just

00:00:23,670 --> 00:00:28,800
a minute ago so without further ado

00:00:27,510 --> 00:00:31,140
we'll get started and talk about Seth

00:00:28,800 --> 00:00:32,520
first a little bit about stuff why you

00:00:31,140 --> 00:00:36,210
should care about yet another

00:00:32,520 --> 00:00:37,649
distributed storage platform the big

00:00:36,210 --> 00:00:39,809
three obviously our time cost and

00:00:37,649 --> 00:00:41,640
requirements right so Seth does things a

00:00:39,809 --> 00:00:44,100
little bit differently in a number of

00:00:41,640 --> 00:00:45,950
different ways the first thing that we

00:00:44,100 --> 00:00:49,020
kind of wanted to take a crack at is

00:00:45,950 --> 00:00:50,879
time right and especially with the

00:00:49,020 --> 00:00:53,340
number of challenges being thrown at any

00:00:50,879 --> 00:00:55,500
given DevOps person your time is

00:00:53,340 --> 00:00:58,230
extremely valuable so we wanted to have

00:00:55,500 --> 00:01:00,449
to be incredibly easy to administer and

00:00:58,230 --> 00:01:02,489
this includes especially like some of

00:01:00,449 --> 00:01:05,810
the problems that you've seen managing

00:01:02,489 --> 00:01:07,820
data clusters of various flavors

00:01:05,810 --> 00:01:10,740
sometimes they're involves a lot of

00:01:07,820 --> 00:01:12,810
manual data migration load balancing a

00:01:10,740 --> 00:01:14,400
lot of attention they're like very small

00:01:12,810 --> 00:01:16,470
children sometimes and you kind of need

00:01:14,400 --> 00:01:18,650
to give them a lot of attention we

00:01:16,470 --> 00:01:21,410
wanted to kind of make Seth more of

00:01:18,650 --> 00:01:22,930
an adult or at least an adolescent so it

00:01:21,410 --> 00:01:25,370
doesn't need quite as much of your time

00:01:22,930 --> 00:01:27,290
and along with that also is painless

00:01:25,370 --> 00:01:29,540
scaling right scaling up and scaling

00:01:27,290 --> 00:01:32,930
down we realize that you know storage

00:01:29,540 --> 00:01:34,460
needs change your locations change how

00:01:32,930 --> 00:01:37,790
you're handling your data you know

00:01:34,460 --> 00:01:39,770
anything is up wide open for change so

00:01:37,790 --> 00:01:41,410
we wanted to be able to scale the

00:01:39,770 --> 00:01:43,880
storage system out to meet your needs

00:01:41,410 --> 00:01:45,830
dynamically so you wouldn't have to kind

00:01:43,880 --> 00:01:48,010
of shut it down you could just kind of

00:01:45,830 --> 00:01:50,630
plug and play and the cluster would

00:01:48,010 --> 00:01:53,840
rebalance and kind of adapt to whatever

00:01:50,630 --> 00:01:55,550
it was that you needed it to do of

00:01:53,840 --> 00:01:58,460
course the the the big numbers in

00:01:55,550 --> 00:02:01,160
everybody's eyes is cost right you want

00:01:58,460 --> 00:02:03,080
your storage to be a very kind of linear

00:02:01,160 --> 00:02:06,920
or as close to linear as you can get it

00:02:03,080 --> 00:02:09,020
function of size and performance to how

00:02:06,920 --> 00:02:11,720
much money you're pouring into the end

00:02:09,020 --> 00:02:14,090
you're given set up and the nice thing

00:02:11,720 --> 00:02:16,430
about SEF is it's designed to run on

00:02:14,090 --> 00:02:18,860
commodity hardware you know rather than

00:02:16,430 --> 00:02:20,270
kind of these forklift upgrades and we

00:02:18,860 --> 00:02:22,790
can talk a little bit more about kind of

00:02:20,270 --> 00:02:24,530
the architecture but cost was a big

00:02:22,790 --> 00:02:26,030
feature for us right we wanted it to be

00:02:24,530 --> 00:02:30,830
open source we wanted to stay open

00:02:26,030 --> 00:02:34,880
source and in doing so sage while the

00:02:30,830 --> 00:02:36,709
founder created not only is it kind of

00:02:34,880 --> 00:02:39,620
this copyleft license you know it's the

00:02:36,709 --> 00:02:41,900
lgpl too but also any contributor the

00:02:39,620 --> 00:02:44,510
project owns their own contributions so

00:02:41,900 --> 00:02:46,690
no matter what happens nobody no

00:02:44,510 --> 00:02:50,270
corporate entity is going to be able to

00:02:46,690 --> 00:02:52,580
clothes f down and charge crazy amounts

00:02:50,270 --> 00:02:54,560
of money for it so you know you're not

00:02:52,580 --> 00:02:56,870
going to get that vendor rock in and

00:02:54,560 --> 00:02:59,030
you'll be able to keep doing that linear

00:02:56,870 --> 00:03:01,940
progression so you want you want another

00:02:59,030 --> 00:03:03,530
host you want another 10 terabytes you

00:03:01,940 --> 00:03:06,080
want other 10 petabytes it doesn't

00:03:03,530 --> 00:03:09,440
matter you just load in whatever you

00:03:06,080 --> 00:03:12,680
want drops f on it and you're off to the

00:03:09,440 --> 00:03:14,810
races and of course the requirements

00:03:12,680 --> 00:03:17,030
because you know people when they talk

00:03:14,810 --> 00:03:19,490
about storage they can mean any one of a

00:03:17,030 --> 00:03:21,650
number of things because there's huge

00:03:19,490 --> 00:03:24,560
diverse storage requirements across the

00:03:21,650 --> 00:03:26,720
industry whether it be you know I just

00:03:24,560 --> 00:03:29,120
want an object store to drop a bunch of

00:03:26,720 --> 00:03:30,440
things into whether i want you know

00:03:29,120 --> 00:03:31,850
block storage we're going to probably

00:03:30,440 --> 00:03:34,100
talk a little bit more about today than

00:03:31,850 --> 00:03:36,860
anything else primary use case obviously

00:03:34,100 --> 00:03:39,410
being VMs right you want a place to to

00:03:36,860 --> 00:03:41,260
back your VMs or you know maybe you want

00:03:39,410 --> 00:03:43,700
some disks that you can mount somewhere

00:03:41,260 --> 00:03:46,310
or you know making your other option

00:03:43,700 --> 00:03:48,020
that you want is the shared file system

00:03:46,310 --> 00:03:51,470
with POSIX compliance that you can

00:03:48,020 --> 00:03:53,090
actually use across distributed hosts in

00:03:51,470 --> 00:03:55,050
any case you know you're going to want

00:03:53,090 --> 00:03:56,940
to have

00:03:55,050 --> 00:03:59,070
different ways to handle your data and

00:03:56,940 --> 00:04:00,600
there were a lot of different and kind

00:03:59,070 --> 00:04:03,060
of sometimes difficult problems that we

00:04:00,600 --> 00:04:04,380
kind of had to solve and obviously one

00:04:03,060 --> 00:04:06,170
of the biggest requirements out of any

00:04:04,380 --> 00:04:08,510
of them is going to be scale right

00:04:06,170 --> 00:04:10,950
people want to start small typically

00:04:08,510 --> 00:04:13,260
small being a relative term depending on

00:04:10,950 --> 00:04:15,030
your organization in your use case but

00:04:13,260 --> 00:04:16,470
you want to start small make sure

00:04:15,030 --> 00:04:19,260
everything's working you get all the

00:04:16,470 --> 00:04:21,000
kinks ironed out of your system and then

00:04:19,260 --> 00:04:24,120
you want the ability to scale that same

00:04:21,000 --> 00:04:27,270
system to you know it infinity with a

00:04:24,120 --> 00:04:29,430
little asterisks next to infinity and so

00:04:27,270 --> 00:04:31,500
the the read the way we wanted to scale

00:04:29,430 --> 00:04:34,580
was with that heterogenous hardware

00:04:31,500 --> 00:04:38,010
right so kind of some of the enterprise

00:04:34,580 --> 00:04:39,600
storage stuff in the past it's been you

00:04:38,010 --> 00:04:42,090
know I want a petabyte I go to storage

00:04:39,600 --> 00:04:44,880
vendor X they send me 1i fork left it in

00:04:42,090 --> 00:04:47,730
then fell on another petabyte it's

00:04:44,880 --> 00:04:49,920
another forklift either to replace the

00:04:47,730 --> 00:04:51,600
previous one or another forklift for the

00:04:49,920 --> 00:04:54,240
exact same which I'm going to spend you

00:04:51,600 --> 00:04:57,360
know and dollars times 2 on for the next

00:04:54,240 --> 00:04:59,850
one so we wanted to make it a lot easier

00:04:57,360 --> 00:05:02,910
you know you plug and play down to the

00:04:59,850 --> 00:05:06,690
disc level rather than the giant Rex or

00:05:02,910 --> 00:05:08,370
data centers and then on top of that we

00:05:06,690 --> 00:05:10,470
want a lot more reliability and fault

00:05:08,370 --> 00:05:12,750
tolerance because we're moving to this

00:05:10,470 --> 00:05:15,240
kind of heterogeneous hardware setup you

00:05:12,750 --> 00:05:17,400
know this commodity hardware you have to

00:05:15,240 --> 00:05:19,290
expect that at any given time you know

00:05:17,400 --> 00:05:22,230
maybe ten percent of your hardware is

00:05:19,290 --> 00:05:24,390
going to be on fire so you kind of have

00:05:22,230 --> 00:05:27,090
to plan your fault tolerance a little

00:05:24,390 --> 00:05:29,910
bit differently than something else that

00:05:27,090 --> 00:05:34,220
may be all of kind of a big giant black

00:05:29,910 --> 00:05:36,360
box so what is Seth Seth is a

00:05:34,220 --> 00:05:40,500
distributed it's a unified storage

00:05:36,360 --> 00:05:42,930
platform and it does primarily three

00:05:40,500 --> 00:05:46,410
things right we do object block and file

00:05:42,930 --> 00:05:48,180
storage um you know the object typically

00:05:46,410 --> 00:05:51,150
we'd say it's an object store you can

00:05:48,180 --> 00:05:52,800
treat it with the native interfaces but

00:05:51,150 --> 00:05:53,970
we also have a restful interface that

00:05:52,800 --> 00:05:56,190
you can get at it I'll get more into

00:05:53,970 --> 00:05:57,990
that a little bit later um we have a

00:05:56,190 --> 00:05:59,520
block device you can do it's a thinly

00:05:57,990 --> 00:06:01,980
provisioned block device on top of your

00:05:59,520 --> 00:06:05,550
object store you can do things like you

00:06:01,980 --> 00:06:07,740
know snapshots cloning your typical you

00:06:05,550 --> 00:06:10,080
don't and use it as a disc and then we

00:06:07,740 --> 00:06:12,180
also have a file store on top of the

00:06:10,080 --> 00:06:13,710
distributed object store which gives you

00:06:12,180 --> 00:06:15,270
thing you know strong consistency you

00:06:13,710 --> 00:06:19,680
can do snapshots on your file system all

00:06:15,270 --> 00:06:21,660
kinds of stuff so does it look like it

00:06:19,680 --> 00:06:23,970
looks like this so the part of it all

00:06:21,660 --> 00:06:27,150
Seph is a distributed object store right

00:06:23,970 --> 00:06:30,840
its rate us it's a very reliable

00:06:27,150 --> 00:06:33,210
autonomic distributed object it's or

00:06:30,840 --> 00:06:35,580
comprised of kind of these a lot of

00:06:33,210 --> 00:06:37,110
self-healing self-managing intelligent

00:06:35,580 --> 00:06:40,979
storage units right and you can see that

00:06:37,110 --> 00:06:42,510
if you can read the slides so the base

00:06:40,979 --> 00:06:43,560
of it it's this object store and on top

00:06:42,510 --> 00:06:45,330
of that we give you a number of

00:06:43,560 --> 00:06:48,360
different ways to talk to it to interact

00:06:45,330 --> 00:06:49,710
with it and you'll see it obviously the

00:06:48,360 --> 00:06:51,529
block and file is here but that's not

00:06:49,710 --> 00:06:53,149
all we do right

00:06:51,529 --> 00:06:55,279
so let me step back and we'll talk about

00:06:53,149 --> 00:06:59,089
the object for a minute so why do we

00:06:55,279 --> 00:07:00,019
start with object you know you can ask a

00:06:59,089 --> 00:07:01,099
million different people and you'd

00:07:00,019 --> 00:07:05,629
probably get a million different answers

00:07:01,099 --> 00:07:06,739
but the the biggest thing is when we

00:07:05,629 --> 00:07:08,389
started looking at what do we want to

00:07:06,739 --> 00:07:10,099
put as the underlying technology right

00:07:08,389 --> 00:07:11,929
some people started with with block

00:07:10,099 --> 00:07:13,369
devices and they kind of aggregate block

00:07:11,929 --> 00:07:14,689
devices and do crazy things but we

00:07:13,369 --> 00:07:17,209
wanted objects because it seemed to us

00:07:14,689 --> 00:07:19,669
that it was more useful it gives you

00:07:17,209 --> 00:07:22,579
names in a single flat namespace you can

00:07:19,669 --> 00:07:25,849
have wildly variable size and it gives

00:07:22,579 --> 00:07:27,769
you a very simple API with relatively

00:07:25,849 --> 00:07:30,199
rich semantics that you can work with

00:07:27,769 --> 00:07:32,539
from the very base level building block

00:07:30,199 --> 00:07:33,799
of what you're working with we also

00:07:32,539 --> 00:07:36,379
found it to be you know more scalable

00:07:33,799 --> 00:07:39,019
than individual files so you don't have

00:07:36,379 --> 00:07:41,029
to deal with that kind of hard to

00:07:39,019 --> 00:07:42,619
distribute hierarchy you don't have to

00:07:41,029 --> 00:07:44,269
worry about how your objects are

00:07:42,619 --> 00:07:48,019
spanning across multiple different

00:07:44,269 --> 00:07:50,929
blocks and things like that and the

00:07:48,019 --> 00:07:53,989
workload is very trivially parallel when

00:07:50,929 --> 00:07:57,769
you get into specific use cases and so

00:07:53,989 --> 00:08:00,769
Steph's object model we have a number of

00:07:57,769 --> 00:08:03,049
pools that we distribute so you'll

00:08:00,769 --> 00:08:05,629
define a pool you can have you know a

00:08:03,049 --> 00:08:09,289
single pool up to hundreds of pools in

00:08:05,629 --> 00:08:13,309
your cell cluster independent namespaces

00:08:09,289 --> 00:08:15,469
or object collections on those pools you

00:08:13,309 --> 00:08:16,639
can actually define rules of how you

00:08:15,469 --> 00:08:19,669
want your data placement your data

00:08:16,639 --> 00:08:21,259
storage to go based on individually on

00:08:19,669 --> 00:08:22,759
those pools so you can say you have one

00:08:21,259 --> 00:08:24,229
pool that says I want three copies of

00:08:22,759 --> 00:08:25,759
everything in this pool versus another

00:08:24,229 --> 00:08:28,620
that I walk you know ten copies of

00:08:25,759 --> 00:08:32,370
everything in this pool

00:08:28,620 --> 00:08:34,650
and then in those pools we have objects

00:08:32,370 --> 00:08:38,010
obviously which huge metric piles of

00:08:34,650 --> 00:08:39,719
data have the the actual data itself you

00:08:38,010 --> 00:08:41,370
know in these objects you can have blobs

00:08:39,719 --> 00:08:43,440
of data which is you know bytes two

00:08:41,370 --> 00:08:46,260
gigabytes in size you can have the

00:08:43,440 --> 00:08:48,270
attributes assigned to those you know

00:08:46,260 --> 00:08:49,770
bites to kilobytes kind of thing and

00:08:48,270 --> 00:08:53,100
then you can also store the key value

00:08:49,770 --> 00:08:56,070
bundles in there you skip forward a

00:08:53,100 --> 00:08:57,930
little so usually when you have a system

00:08:56,070 --> 00:09:00,720
will get into the architecture a little

00:08:57,930 --> 00:09:02,070
bit here you have a given system right

00:09:00,720 --> 00:09:04,680
it's a human talking to the computer

00:09:02,070 --> 00:09:07,350
that has any number of disks whether

00:09:04,680 --> 00:09:09,750
it's spinning roster SSD or whatever so

00:09:07,350 --> 00:09:11,220
it looks like this well a little bit

00:09:09,750 --> 00:09:13,230
more like this right so you usually have

00:09:11,220 --> 00:09:15,120
huge numbers of people trying to get at

00:09:13,230 --> 00:09:16,950
your data that's sitting on your disks

00:09:15,120 --> 00:09:20,190
and so that computer very quickly

00:09:16,950 --> 00:09:22,020
becomes a bottle ink and so SEF kind of

00:09:20,190 --> 00:09:22,950
dubs it a little bit differently we

00:09:22,020 --> 00:09:26,070
aggregated a whole bunch of different

00:09:22,950 --> 00:09:30,060
machines and we just treat it as a big

00:09:26,070 --> 00:09:31,589
pile I had to explain it to someone

00:09:30,060 --> 00:09:33,480
bring non-technical the other day and I

00:09:31,589 --> 00:09:34,880
said it was it was like there was a

00:09:33,480 --> 00:09:36,810
pretty girl to dance and there's a

00:09:34,880 --> 00:09:38,400
thousand guys like me you all want to

00:09:36,810 --> 00:09:39,959
dance with her and so we made her

00:09:38,400 --> 00:09:42,450
arbitrarily large so that everyone could

00:09:39,959 --> 00:09:46,860
dance with her at once they didn't think

00:09:42,450 --> 00:09:48,750
that was a terribly good way to simplify

00:09:46,860 --> 00:09:50,640
it but in a sense that's what we're

00:09:48,750 --> 00:09:52,830
doing is we're making it really we're

00:09:50,640 --> 00:09:56,160
making a thousand copies of her perhaps

00:09:52,830 --> 00:09:58,350
it would be a better way to say it so in

00:09:56,160 --> 00:10:00,540
your storage cluster you have a large

00:09:58,350 --> 00:10:02,670
number you know tends to tens thousands

00:10:00,540 --> 00:10:04,620
of these object storage Damon's right

00:10:02,670 --> 00:10:06,990
typically we will run the object storage

00:10:04,620 --> 00:10:08,490
Damon one per disk weather and that's

00:10:06,990 --> 00:10:12,000
you know the harder genus hardware thing

00:10:08,490 --> 00:10:13,620
whether that's an SSD or you know just a

00:10:12,000 --> 00:10:16,140
typical say to drive or a rate

00:10:13,620 --> 00:10:18,690
configuration of some sort you drop your

00:10:16,140 --> 00:10:20,160
OSD down on top of that and those are

00:10:18,690 --> 00:10:22,470
the things that are actually doing the

00:10:20,160 --> 00:10:24,540
the serving of Stewart objects to your

00:10:22,470 --> 00:10:26,010
clients right and they do a lot of

00:10:24,540 --> 00:10:29,790
things there are some intelligence has

00:10:26,010 --> 00:10:31,240
been built into them so rather than

00:10:29,790 --> 00:10:32,500
having

00:10:31,240 --> 00:10:36,550
we can get into the the lookups and

00:10:32,500 --> 00:10:38,529
stuff later but rather than having to

00:10:36,550 --> 00:10:39,850
worry about you know going through a

00:10:38,529 --> 00:10:42,070
single controller you're getting your

00:10:39,850 --> 00:10:43,570
clients that are eventually you know as

00:10:42,070 --> 00:10:45,040
the air traffic controllers which are

00:10:43,570 --> 00:10:47,050
the monitors I'll talk about a second

00:10:45,040 --> 00:10:49,630
they'll tell you where to hit your data

00:10:47,050 --> 00:10:51,279
and you'll go directly to that OSD and

00:10:49,630 --> 00:10:54,130
then the OSD will intelligently kind of

00:10:51,279 --> 00:10:56,130
peer with the rest of the cluster to

00:10:54,130 --> 00:10:58,930
worry about things like data replication

00:10:56,130 --> 00:11:01,930
data recovery when another OSD goes down

00:10:58,930 --> 00:11:05,080
they're always talking to each other can

00:11:01,930 --> 00:11:06,820
get a little chatty but the OSD is tend

00:11:05,080 --> 00:11:09,010
to have a certain amount of intelligence

00:11:06,820 --> 00:11:11,890
built into them so that you don't have

00:11:09,010 --> 00:11:13,360
to worry about always having traffic

00:11:11,890 --> 00:11:15,300
come off your cluster to do things and

00:11:13,360 --> 00:11:18,339
go back there's a lot of inter-cluster

00:11:15,300 --> 00:11:19,810
discussion if you will and then you also

00:11:18,339 --> 00:11:23,020
have these monitor notes typically a

00:11:19,810 --> 00:11:26,020
small odd number of monitors three or

00:11:23,020 --> 00:11:29,050
five is usually what we're seeing in you

00:11:26,020 --> 00:11:30,279
know early production clusters these

00:11:29,050 --> 00:11:31,959
guys are they air traffic controllers

00:11:30,279 --> 00:11:35,290
they're the ones that are maintaining

00:11:31,959 --> 00:11:36,760
the cluster state authentication they're

00:11:35,290 --> 00:11:37,959
the ones that are providing consensus

00:11:36,760 --> 00:11:40,390
for these kind of distributed

00:11:37,959 --> 00:11:43,750
decision-making they aren't actually

00:11:40,390 --> 00:11:45,070
involved in the data path so they're the

00:11:43,750 --> 00:11:47,200
kind of ones that tell everyone where to

00:11:45,070 --> 00:11:51,250
go and how things are what the current

00:11:47,200 --> 00:11:52,570
state of the cluster is so if you look a

00:11:51,250 --> 00:11:55,510
little closer it kind of looks like this

00:11:52,570 --> 00:11:57,339
right so the OSD notes typically will be

00:11:55,510 --> 00:11:58,720
more than just a single disk in a single

00:11:57,339 --> 00:12:01,420
OSD you'll have a machine that's running

00:11:58,720 --> 00:12:02,470
and those will actually have disks

00:12:01,420 --> 00:12:05,529
underneath them and you'll have some

00:12:02,470 --> 00:12:08,110
sort of filesystem on top of them we

00:12:05,529 --> 00:12:09,820
think the future should probably be

00:12:08,110 --> 00:12:12,100
butter FS that's where we'd like to be

00:12:09,820 --> 00:12:14,230
there are some performance concerns and

00:12:12,100 --> 00:12:16,570
stuff that it isn't quite there yet most

00:12:14,230 --> 00:12:19,209
people are using like XFS or you know x4

00:12:16,570 --> 00:12:20,620
is another good option but you have some

00:12:19,209 --> 00:12:22,730
file system sitting on top of your disk

00:12:20,620 --> 00:12:24,470
and then you drop your OSD on top of it

00:12:22,730 --> 00:12:27,079
and you know you maybe we're seeing like

00:12:24,470 --> 00:12:31,130
anywhere from the four to 12 disks in a

00:12:27,079 --> 00:12:32,899
machine for a single note and then kind

00:12:31,130 --> 00:12:38,209
of all of those you have many nodes that

00:12:32,899 --> 00:12:40,430
are put together to form your cluster so

00:12:38,209 --> 00:12:42,769
what makes SEF cool while the the one

00:12:40,430 --> 00:12:44,889
thing for me that was interesting and

00:12:42,769 --> 00:12:47,089
continues to be interesting as both a

00:12:44,889 --> 00:12:49,399
piece of technology as well as an

00:12:47,089 --> 00:12:51,260
academic interesting pursuit is crush

00:12:49,399 --> 00:12:54,290
crush is kind of at the heart of what

00:12:51,260 --> 00:12:56,329
makes SEF powerful it's a pseudo random

00:12:54,290 --> 00:12:59,570
placement algorithm it's a controlled

00:12:56,329 --> 00:13:01,699
replication under scaled hashing this is

00:12:59,570 --> 00:13:05,029
what allows SEF to be really fast when

00:13:01,699 --> 00:13:06,980
it comes to things like look up or you

00:13:05,029 --> 00:13:10,339
know data placement data retrieval and

00:13:06,980 --> 00:13:12,800
things like that so historically you had

00:13:10,339 --> 00:13:14,959
things like a single node and look up

00:13:12,800 --> 00:13:17,060
right you had to if you wanted some data

00:13:14,959 --> 00:13:18,320
and you had some where you had to do one

00:13:17,060 --> 00:13:21,860
of a couple of things right you had to

00:13:18,320 --> 00:13:23,449
kind of make a logical distribution of

00:13:21,860 --> 00:13:26,329
where your data is kind of the phonebook

00:13:23,449 --> 00:13:28,610
approach right ADA ADA see is on this

00:13:26,329 --> 00:13:30,260
one and and you know X to zed is on this

00:13:28,610 --> 00:13:34,160
one over here and and I'd know where to

00:13:30,260 --> 00:13:36,110
kind of go after it or you could say I

00:13:34,160 --> 00:13:38,420
have a bunch of data over here and what

00:13:36,110 --> 00:13:40,250
I want to find out where it is I'm going

00:13:38,420 --> 00:13:41,300
to write down in some lookup table where

00:13:40,250 --> 00:13:42,889
it is and so you have to go to the

00:13:41,300 --> 00:13:44,750
lookup table find out where it is and go

00:13:42,889 --> 00:13:49,240
find it with crush it kind of removes

00:13:44,750 --> 00:13:51,680
that whole step crush allows you to

00:13:49,240 --> 00:13:54,760
repeatedly calculate where your data

00:13:51,680 --> 00:13:56,990
should live or where it should go

00:13:54,760 --> 00:13:59,329
depending on a few things like the state

00:13:56,990 --> 00:14:02,480
of your cluster who's in who's out the

00:13:59,329 --> 00:14:04,550
crush rules the crush map is called that

00:14:02,480 --> 00:14:07,730
you actually define which allows you to

00:14:04,550 --> 00:14:10,069
say i want you know three copies of my

00:14:07,730 --> 00:14:12,500
data I don't want any two copies to live

00:14:10,069 --> 00:14:15,829
on the same row in my data center so

00:14:12,500 --> 00:14:17,720
it's actually too aware of your your

00:14:15,829 --> 00:14:21,139
topology you know your

00:14:17,720 --> 00:14:22,430
no data center what it looks like and so

00:14:21,139 --> 00:14:24,680
it's you know it's the nice thing about

00:14:22,430 --> 00:14:27,319
is it's repeatable as deterministic and

00:14:24,680 --> 00:14:30,170
so you know let's say I want to put some

00:14:27,319 --> 00:14:32,089
data into a cluster I will go and I'll

00:14:30,170 --> 00:14:34,069
figure out you know talk to the monitors

00:14:32,089 --> 00:14:36,800
who's in my monitor or who's in my

00:14:34,069 --> 00:14:38,509
cluster or other and I'll be able to say

00:14:36,800 --> 00:14:41,649
okay this is the state of my cluster

00:14:38,509 --> 00:14:43,850
here my crush rules here's the data and

00:14:41,649 --> 00:14:46,579
it will push it in and we can talk a

00:14:43,850 --> 00:14:50,089
little bit about how that placement

00:14:46,579 --> 00:14:56,230
happens in a minute but the nice thing

00:14:50,089 --> 00:14:58,399
about crush is it allows your data to

00:14:56,230 --> 00:14:59,959
you don't have to move your data around

00:14:58,399 --> 00:15:02,209
a lot depending on the changes in the

00:14:59,959 --> 00:15:03,920
state of your cluster because it always

00:15:02,209 --> 00:15:05,839
kind of knows where it's supposed to

00:15:03,920 --> 00:15:07,339
live and this is another part of that

00:15:05,839 --> 00:15:09,589
intelligence of your object storage

00:15:07,339 --> 00:15:13,009
Damon's let's say you're one of your OS

00:15:09,589 --> 00:15:14,360
DS goes down and it knows that battle

00:15:13,009 --> 00:15:16,610
you know all of the OSD is know that

00:15:14,360 --> 00:15:17,899
that one's out and it knows how to

00:15:16,610 --> 00:15:19,420
rebalance your data and so it doesn't

00:15:17,899 --> 00:15:22,009
have to move a whole lot of data around

00:15:19,420 --> 00:15:24,350
because crushed changes and it's going

00:15:22,009 --> 00:15:25,399
to know where it needs to move things so

00:15:24,350 --> 00:15:27,860
there's not a whole lot of data movement

00:15:25,399 --> 00:15:30,079
needs to happen so how does this work

00:15:27,860 --> 00:15:33,829
right so I want to push something into

00:15:30,079 --> 00:15:35,420
my cluster and so I talked to my

00:15:33,829 --> 00:15:37,579
monitors I found out the state of my

00:15:35,420 --> 00:15:41,720
cluster and I take this object whatever

00:15:37,579 --> 00:15:43,550
it is and I split it up into a number of

00:15:41,720 --> 00:15:45,740
placement groups tunable arbitrary

00:15:43,550 --> 00:15:48,110
placement groups and so these individual

00:15:45,740 --> 00:15:50,360
placement groups then get pushed into

00:15:48,110 --> 00:15:53,209
the cluster based on crush and as you

00:15:50,360 --> 00:15:55,220
can see here pretty color coding it goes

00:15:53,209 --> 00:15:57,470
and it looks at all of my OS DS in this

00:15:55,220 --> 00:15:59,959
case I have 10 OS DS and it takes I

00:15:57,470 --> 00:16:01,370
think I set a replication level of two

00:15:59,959 --> 00:16:04,410
and it takes two copies of each one of

00:16:01,370 --> 00:16:07,709
these and drops them on separate OS DS

00:16:04,410 --> 00:16:10,079
so really it's a little higher level

00:16:07,709 --> 00:16:11,819
you're pushing your data through crush

00:16:10,079 --> 00:16:14,940
the algorithm decides where it goes you

00:16:11,819 --> 00:16:16,440
talk to for each individual placement

00:16:14,940 --> 00:16:18,660
group then you take that placement group

00:16:16,440 --> 00:16:22,110
you push it into an OSD that OSD will

00:16:18,660 --> 00:16:24,750
intelligently pier with what the crush

00:16:22,110 --> 00:16:26,579
algorithm says the other place your copy

00:16:24,750 --> 00:16:28,709
of your data should live pushes it there

00:16:26,579 --> 00:16:29,850
for you and then it returns it says okay

00:16:28,709 --> 00:16:34,170
you successfully store that placement

00:16:29,850 --> 00:16:35,399
group move on to the next one so what

00:16:34,170 --> 00:16:37,259
happens is something breaks and this is

00:16:35,399 --> 00:16:39,540
what I was talking about let's this OSD

00:16:37,259 --> 00:16:41,100
here the one that's shaded out it's hard

00:16:39,540 --> 00:16:44,459
to see let's say this red yellow

00:16:41,100 --> 00:16:46,529
placement group OSD decides to set on

00:16:44,459 --> 00:16:48,870
fire alien invasion something somebody

00:16:46,529 --> 00:16:51,329
decides to trip over a cord goes down

00:16:48,870 --> 00:16:53,850
and so your cluster is aware that that

00:16:51,329 --> 00:16:56,129
particular OSD has gone down and then

00:16:53,850 --> 00:16:57,930
the two that are carrying the copies of

00:16:56,129 --> 00:17:00,689
that data of say hey I've got the copy

00:16:57,930 --> 00:17:02,819
of the data we need to get our

00:17:00,689 --> 00:17:05,610
replication back up to two copies of

00:17:02,819 --> 00:17:07,439
this data so it automatically peers with

00:17:05,610 --> 00:17:10,770
where it's supposed to be now based on

00:17:07,439 --> 00:17:13,199
the new crush rules and pushes it over

00:17:10,770 --> 00:17:18,329
to the OS DS where the data is supposed

00:17:13,199 --> 00:17:19,679
to live so let's revisit a bit of how

00:17:18,329 --> 00:17:21,390
we're talking to this cluster right

00:17:19,679 --> 00:17:23,280
we're talking to this this object store

00:17:21,390 --> 00:17:24,740
so there's really four different ways

00:17:23,280 --> 00:17:27,659
that you can talk to the object store

00:17:24,740 --> 00:17:31,110
the first being obviously liberate us

00:17:27,659 --> 00:17:33,809
this is our native way to talk to the

00:17:31,110 --> 00:17:36,390
object store has a lot of different

00:17:33,809 --> 00:17:38,539
features that you can that kind of

00:17:36,390 --> 00:17:42,150
define why it's cool but it's basically

00:17:38,539 --> 00:17:44,700
C C++ Python Java PHP whatever there's a

00:17:42,150 --> 00:17:47,250
number of different ways that you can

00:17:44,700 --> 00:17:50,490
talk to it but this allows you to have

00:17:47,250 --> 00:17:51,750
atomic single action transactions it you

00:17:50,490 --> 00:17:54,210
update your data all the attributes

00:17:51,750 --> 00:17:56,130
together you push it straight into the

00:17:54,210 --> 00:17:58,289
object store this is for guys that are

00:17:56,130 --> 00:18:01,230
writing apps right I want to build

00:17:58,289 --> 00:18:04,250
something that talks to SEF this is what

00:18:01,230 --> 00:18:04,250
I'm going to use to build it

00:18:04,360 --> 00:18:08,660
the next thing that you can use is the

00:18:06,860 --> 00:18:10,190
gateway and this is the other way to

00:18:08,660 --> 00:18:11,330
talk directly to your object store it's

00:18:10,190 --> 00:18:13,910
just to do a little bit different way to

00:18:11,330 --> 00:18:16,760
do it it's a restful gateway so you can

00:18:13,910 --> 00:18:19,640
talk you know over the HTTP protocol and

00:18:16,760 --> 00:18:22,070
we actually have instantiated both s3

00:18:19,640 --> 00:18:24,590
and Swift API so you can if you have

00:18:22,070 --> 00:18:28,400
something that already uses s3 as your

00:18:24,590 --> 00:18:31,220
endpoint or uses OpenStack Swift if you

00:18:28,400 --> 00:18:35,480
have one of those things you can spin up

00:18:31,220 --> 00:18:37,520
a class SEF cluster create a ratos

00:18:35,480 --> 00:18:39,890
gateway or a number of load balancer at

00:18:37,520 --> 00:18:43,580
those gateways change the endpoint and

00:18:39,890 --> 00:18:46,280
no one will ever know the difference and

00:18:43,580 --> 00:18:47,900
then there's the file system which I

00:18:46,280 --> 00:18:50,720
don't didn't want to go into too deeply

00:18:47,900 --> 00:18:52,790
today because the cephus is still a

00:18:50,720 --> 00:18:54,350
little wild Westy it's not where the

00:18:52,790 --> 00:18:56,420
majority of our work has been focused on

00:18:54,350 --> 00:18:58,460
there's a lot of love that's coming in

00:18:56,420 --> 00:19:02,240
the near future but it's a POSIX

00:18:58,460 --> 00:19:05,270
compliant distributed file system that

00:19:02,240 --> 00:19:07,430
is based on your object is based on the

00:19:05,270 --> 00:19:10,190
object store right so it allows you to

00:19:07,430 --> 00:19:11,360
have a distributed file system that you

00:19:10,190 --> 00:19:15,940
can mount from a number of different

00:19:11,360 --> 00:19:17,810
places that is then backed by this

00:19:15,940 --> 00:19:19,820
distributed object store which gives you

00:19:17,810 --> 00:19:21,980
all kinds of really cool stuff but is

00:19:19,820 --> 00:19:25,280
it's just not there yet so that brings

00:19:21,980 --> 00:19:27,770
us to our BD finally the part that

00:19:25,280 --> 00:19:30,110
everyone came here to hear about so it's

00:19:27,770 --> 00:19:32,630
like it says a reliable and fully be

00:19:30,110 --> 00:19:34,970
fully distributed block device so does

00:19:32,630 --> 00:19:37,580
linux kernel's client scheme you kvm

00:19:34,970 --> 00:19:39,860
drivers things like that native linux

00:19:37,580 --> 00:19:41,180
driver but basically it's a thinly

00:19:39,860 --> 00:19:43,220
provisioned block device sitting on top

00:19:41,180 --> 00:19:48,350
of your object store

00:19:43,220 --> 00:19:50,720
so what is it really burritos the rbd

00:19:48,350 --> 00:19:54,530
gives you a number of features that are

00:19:50,720 --> 00:19:56,360
pretty cool it allows you to store disk

00:19:54,530 --> 00:19:58,100
images in ratos really is what it comes

00:19:56,360 --> 00:20:00,230
down to but it kind of allows you to

00:19:58,100 --> 00:20:01,929
decouple the vm from the host because

00:20:00,230 --> 00:20:03,950
you have kind of this you know

00:20:01,929 --> 00:20:06,200
distributed storage platform that you're

00:20:03,950 --> 00:20:11,150
storing things on that so you have

00:20:06,200 --> 00:20:12,919
images VM images or disk images or

00:20:11,150 --> 00:20:15,380
whatever it is that gets striped across

00:20:12,919 --> 00:20:17,900
your entire cluster split up into those

00:20:15,380 --> 00:20:19,010
placement rupes we're talking about this

00:20:17,900 --> 00:20:22,940
allows you to do some really cool things

00:20:19,010 --> 00:20:25,100
but with rbd you have the ability than

00:20:22,940 --> 00:20:28,909
to do snapshots and and because it's a

00:20:25,100 --> 00:20:30,200
distributed platform you can do things

00:20:28,909 --> 00:20:33,320
like copy-on-write cloning and live

00:20:30,200 --> 00:20:35,929
migration and there's you know their

00:20:33,320 --> 00:20:38,750
support in as you can c qm u kv m

00:20:35,929 --> 00:20:41,750
there's a mainline Linux kernel after

00:20:38,750 --> 00:20:44,630
two at six thirty nine where you can

00:20:41,750 --> 00:20:46,669
just mount it right out of Linux and

00:20:44,630 --> 00:20:48,770
then there's support for CloudStack

00:20:46,669 --> 00:20:51,440
OpenStack and then there's xenserver

00:20:48,770 --> 00:20:53,510
stuff that's still being ironed out so

00:20:51,440 --> 00:20:55,970
what does it look like this is what I

00:20:53,510 --> 00:20:58,010
was talking about so you have some discs

00:20:55,970 --> 00:20:59,900
that's mounted somewhere as you can see

00:20:58,010 --> 00:21:01,909
and then it's Britain broken up into a

00:20:59,900 --> 00:21:05,390
logical number of components and split

00:21:01,909 --> 00:21:08,270
across your set cluster and really the

00:21:05,390 --> 00:21:10,220
the use case is for running VMS right

00:21:08,270 --> 00:21:12,440
and so that's kind of how it ends up

00:21:10,220 --> 00:21:14,539
looking like when you have vm running in

00:21:12,440 --> 00:21:16,220
cloudstack and then you have the disks

00:21:14,539 --> 00:21:18,470
behind them they get split up across

00:21:16,220 --> 00:21:19,789
yourself cluster and this this does a

00:21:18,470 --> 00:21:24,260
number of really cool things for you is

00:21:19,789 --> 00:21:27,980
if you have a extremely large disk or if

00:21:24,260 --> 00:21:30,620
you have a really really busy disk it

00:21:27,980 --> 00:21:32,419
doesn't care because it's split across a

00:21:30,620 --> 00:21:33,830
number of different hosts so it helps to

00:21:32,419 --> 00:21:36,559
even out some of your hot spots and

00:21:33,830 --> 00:21:38,600
whatnot so that you don't really have to

00:21:36,559 --> 00:21:40,400
worry about that as much because SEF is

00:21:38,600 --> 00:21:42,659
kind of has some again intelligence

00:21:40,400 --> 00:21:46,220
built into it to know

00:21:42,659 --> 00:21:46,220
all even out those hot spots basically

00:21:46,249 --> 00:21:50,090
how we doing on time oh

00:21:51,390 --> 00:21:58,120
so the idea obviously is it all of those

00:21:54,970 --> 00:22:01,630
distributed objects liberate us puts all

00:21:58,120 --> 00:22:04,890
those objects together into the block

00:22:01,630 --> 00:22:07,660
device so then our l-live rbd excuse me

00:22:04,890 --> 00:22:09,040
then puts those together for a

00:22:07,660 --> 00:22:12,400
virtualization container and then the

00:22:09,040 --> 00:22:14,230
container exposes it to the vm the long

00:22:12,400 --> 00:22:15,460
and short of it is this allows you to

00:22:14,230 --> 00:22:18,130
have something that is essentially

00:22:15,460 --> 00:22:21,940
Amazon's elastic block store you get

00:22:18,130 --> 00:22:22,900
your own but because it's a shared

00:22:21,940 --> 00:22:24,670
environment and this is what I was

00:22:22,900 --> 00:22:26,950
talking about before is you can do

00:22:24,670 --> 00:22:29,350
really fun things like migrating running

00:22:26,950 --> 00:22:32,200
instance between houses right so you

00:22:29,350 --> 00:22:33,640
have that container and you decide you

00:22:32,200 --> 00:22:35,920
want to move it to a new host but

00:22:33,640 --> 00:22:37,150
because it's backed by SEF it doesn't

00:22:35,920 --> 00:22:39,310
care you're not actually moving anything

00:22:37,150 --> 00:22:40,930
you just ain't changing the logical

00:22:39,310 --> 00:22:45,570
front end and as long as you're still

00:22:40,930 --> 00:22:45,570
talking to SEF the back end doesn't care

00:22:46,700 --> 00:22:50,690
and then yeah the driver in the mainland

00:22:49,250 --> 00:22:53,150
linux kernel allows you to map it as a

00:22:50,690 --> 00:22:55,780
native you know dev RVD 0 or whatever

00:22:53,150 --> 00:22:57,860
you can just mount it as a normal device

00:22:55,780 --> 00:23:00,050
so what's this copy all right cloning

00:22:57,860 --> 00:23:02,600
I'm surprised at the number of questions

00:23:00,050 --> 00:23:05,390
I get about copy-on-write cloning the

00:23:02,600 --> 00:23:07,100
the best example obviously is I would

00:23:05,390 --> 00:23:10,430
make a golden image let's say I have a

00:23:07,100 --> 00:23:13,730
boon to 1204 image that I want to make

00:23:10,430 --> 00:23:15,560
available to my CloudStack instance and

00:23:13,730 --> 00:23:18,020
I have you know usually it's not we're

00:23:15,560 --> 00:23:20,960
spinning up one copy its I want to spin

00:23:18,020 --> 00:23:23,990
up 100 copies of this instance and so

00:23:20,960 --> 00:23:25,700
what this allows me to do is I spin up a

00:23:23,990 --> 00:23:27,530
hundred copies of this instance but I do

00:23:25,700 --> 00:23:29,600
it instantly and I don't actually copy

00:23:27,530 --> 00:23:31,640
anything i just used this golden image

00:23:29,600 --> 00:23:34,250
and i instantiate 100 copies of it and

00:23:31,640 --> 00:23:37,340
it takes up 0 additional storage so now

00:23:34,250 --> 00:23:39,440
i have in this case for copies and it's

00:23:37,340 --> 00:23:41,030
taking up no additional storage and so

00:23:39,440 --> 00:23:42,800
what that means is then each of these

00:23:41,030 --> 00:23:45,470
individual machines will be you know

00:23:42,800 --> 00:23:47,330
boot up and and whatever end user is

00:23:45,470 --> 00:23:49,400
control of that particular machine will

00:23:47,330 --> 00:23:50,510
start writing data to it the only thing

00:23:49,400 --> 00:23:52,040
that you're going to have that's going

00:23:50,510 --> 00:23:54,230
to take up new storage now is the data

00:23:52,040 --> 00:23:55,340
that you're writing to it and then when

00:23:54,230 --> 00:23:56,900
they go back and they want to read

00:23:55,340 --> 00:24:00,200
things from that particular instance

00:23:56,900 --> 00:24:01,370
they will if it's if there's something

00:24:00,200 --> 00:24:03,290
that's changed they'll read it from

00:24:01,370 --> 00:24:04,850
their copy if it hasn't changed it'll

00:24:03,290 --> 00:24:06,680
just read straight through to the the

00:24:04,850 --> 00:24:09,520
golden copy if you will and so that's

00:24:06,680 --> 00:24:09,520
copy-on-write cloning

00:24:10,119 --> 00:24:13,619
is going through

00:24:15,890 --> 00:24:21,240
see

00:24:18,190 --> 00:24:21,240
say that again

00:24:24,060 --> 00:24:27,670
me

00:24:25,720 --> 00:24:29,200
okay so the question is does this lead

00:24:27,670 --> 00:24:30,580
to performance issues because you have a

00:24:29,200 --> 00:24:32,290
hundred copies that are reading from the

00:24:30,580 --> 00:24:33,820
same base image and the answer really is

00:24:32,290 --> 00:24:36,100
no because that base image is actually

00:24:33,820 --> 00:24:37,500
split into a huge number of placement

00:24:36,100 --> 00:24:40,780
groups across a wide number of machines

00:24:37,500 --> 00:24:43,600
and so I mean as long as your network is

00:24:40,780 --> 00:24:45,600
relatively reliable and fast the answer

00:24:43,600 --> 00:24:47,830
is no you shouldn't see I mean

00:24:45,600 --> 00:24:49,630
eventually you'll probably have to up

00:24:47,830 --> 00:24:52,300
your replication levels so that there's

00:24:49,630 --> 00:24:55,350
more copies of that of each of those

00:24:52,300 --> 00:24:58,830
placement groups to worry about but but

00:24:55,350 --> 00:24:58,830
essentially no not really

00:25:00,999 --> 00:25:06,740
so this brings us to cloudstack and

00:25:03,259 --> 00:25:10,669
while we're running really fast okay so

00:25:06,740 --> 00:25:14,210
what does this mean for cloudstack so

00:25:10,669 --> 00:25:17,059
with with CloudStack so for tato the the

00:25:14,210 --> 00:25:20,210
new for Dino release got RVD support for

00:25:17,059 --> 00:25:22,519
primary storage via kvm so what does

00:25:20,210 --> 00:25:25,159
that mean that means when you spin up a

00:25:22,519 --> 00:25:26,899
cloud stack if you were here during the

00:25:25,159 --> 00:25:27,980
last talked you'll you'll know a lot

00:25:26,899 --> 00:25:32,869
about this but if you spin up your

00:25:27,980 --> 00:25:37,580
cloudstack that you still need the very

00:25:32,869 --> 00:25:39,350
small NFS to serve those system VMS

00:25:37,580 --> 00:25:43,519
initially but then after that your

00:25:39,350 --> 00:25:45,409
primary storage can be CEFs RVD later

00:25:43,519 --> 00:25:46,909
versions are coming where you won't need

00:25:45,409 --> 00:25:48,429
that NFS you'll be able to use set for

00:25:46,909 --> 00:25:51,289
everything but we're not quite there yet

00:25:48,429 --> 00:25:54,470
there's no support right now for VMware

00:25:51,289 --> 00:25:56,119
or Zen and there's really no plan to the

00:25:54,470 --> 00:25:58,580
the guy who wrote the integration I'll

00:25:56,119 --> 00:26:00,220
get to him in a minute it he's not from

00:25:58,580 --> 00:26:04,039
ink tank he doesn't work directly for

00:26:00,220 --> 00:26:06,559
for us and he's not he's doing it for a

00:26:04,039 --> 00:26:08,749
very specific use case there's no real

00:26:06,559 --> 00:26:10,700
plans to to build and support for for

00:26:08,749 --> 00:26:12,980
VMware ins and patches are always

00:26:10,700 --> 00:26:16,940
welcome I would love to see someone else

00:26:12,980 --> 00:26:19,970
get really deeply involved and help Vito

00:26:16,940 --> 00:26:23,289
with a bit more of the building out of

00:26:19,970 --> 00:26:23,289
SEF and CloudStack integration

00:26:25,910 --> 00:26:29,510
it's the integration

00:26:32,200 --> 00:26:35,340
okay yep

00:26:36,509 --> 00:26:44,249
yeah so the live migration stuff that I

00:26:42,029 --> 00:26:46,739
was talking about is supported but we we

00:26:44,249 --> 00:26:48,059
don't have the the cloud stack

00:26:46,739 --> 00:26:50,849
integration doesn't have the ability to

00:26:48,059 --> 00:26:52,409
do to do snapshots from SEF yet that one

00:26:50,849 --> 00:26:54,929
is coming actually it's already written

00:26:52,409 --> 00:26:57,599
we're just vetoes just waiting for the

00:26:54,929 --> 00:27:01,499
the database backend refactor that's

00:26:57,599 --> 00:27:04,379
coming in for that too so yeah that's

00:27:01,499 --> 00:27:07,379
that's CloudStack now and the setup is

00:27:04,379 --> 00:27:09,059
actually really easy you spin up your

00:27:07,379 --> 00:27:10,979
class deck instance and then you just

00:27:09,059 --> 00:27:13,139
you go to your whether it's the UI or

00:27:10,979 --> 00:27:15,469
whether you're doing the the cloud

00:27:13,139 --> 00:27:17,969
monkey stuff or any of the various CLI

00:27:15,469 --> 00:27:22,469
stuff you just you add it as a primary

00:27:17,969 --> 00:27:25,709
storage there's a protocol selection you

00:27:22,469 --> 00:27:27,779
just do our BD and you point it at the

00:27:25,709 --> 00:27:29,309
particular place where your you know

00:27:27,779 --> 00:27:32,279
your monitor or whatever it is that you

00:27:29,309 --> 00:27:34,529
want to plug into and then you fill in

00:27:32,279 --> 00:27:36,629
your authentication entry info force FX

00:27:34,529 --> 00:27:38,789
maybe tag it is already so you can do

00:27:36,629 --> 00:27:40,529
some stuff later but there's there's

00:27:38,789 --> 00:27:41,729
really nothing special about it as long

00:27:40,529 --> 00:27:43,289
as you can spin up CloudStack and you

00:27:41,729 --> 00:27:46,979
can spin up Steph it's really easy to

00:27:43,289 --> 00:27:49,799
plug the two of them together so what's

00:27:46,979 --> 00:27:52,079
next the snapshot and the the backup

00:27:49,799 --> 00:27:55,619
support is probably going to be able to

00:27:52,079 --> 00:27:57,479
come in in for dot to with that that new

00:27:55,619 --> 00:28:00,629
storage code refactor stuff that's kind

00:27:57,479 --> 00:28:02,159
of a width for dot to most of the

00:28:00,629 --> 00:28:04,709
underlying stuff is already written so

00:28:02,159 --> 00:28:07,409
it's just managing a matter of making

00:28:04,709 --> 00:28:10,649
sure that how they did the new storage

00:28:07,409 --> 00:28:14,099
stuff it doesn't break anything cloning

00:28:10,649 --> 00:28:15,749
or if your European I was informed that

00:28:14,099 --> 00:28:18,659
that that is actually called layering

00:28:15,749 --> 00:28:20,029
support so the the the copy-on-write

00:28:18,659 --> 00:28:23,099
clone stuff that I was talking about

00:28:20,029 --> 00:28:25,859
that's also going to be coming in the

00:28:23,099 --> 00:28:28,199
future probably well maybe you for not

00:28:25,859 --> 00:28:30,119
one but probably before not to and then

00:28:28,199 --> 00:28:33,059
SEF support for being able to be a

00:28:30,119 --> 00:28:35,549
secondary storage so that the the

00:28:33,059 --> 00:28:37,979
storage of your images themselves the

00:28:35,549 --> 00:28:40,229
image catalog and for backup storage

00:28:37,979 --> 00:28:41,519
backup storage is new with for dot to I

00:28:40,229 --> 00:28:44,249
guess I don't know a whole lot about it

00:28:41,519 --> 00:28:46,529
but the secondary storage also will be

00:28:44,249 --> 00:28:48,299
coming with four dot two and that's

00:28:46,529 --> 00:28:51,329
actually a great use case for that for

00:28:48,299 --> 00:28:55,259
that gateway for your backup storage

00:28:51,329 --> 00:28:56,429
stuff so who's to blame the guy who

00:28:55,259 --> 00:28:58,289
actually wrote the integration his

00:28:56,429 --> 00:28:59,849
name's Vito den Hollander he's one of

00:28:58,289 --> 00:29:02,219
our partners in Europe and we actually

00:28:59,849 --> 00:29:03,329
like it this way where we're not writing

00:29:02,219 --> 00:29:05,639
all the integrations we would much

00:29:03,329 --> 00:29:07,889
rather see the community not only write

00:29:05,639 --> 00:29:10,829
them but own those integrations we're

00:29:07,889 --> 00:29:12,779
happy to help with whatever people are

00:29:10,829 --> 00:29:14,279
interested in doing or building but we

00:29:12,779 --> 00:29:17,069
would much prefer that it's not all

00:29:14,279 --> 00:29:19,529
centrally located you know if our la

00:29:17,069 --> 00:29:22,619
offices you know if the the big one hits

00:29:19,529 --> 00:29:25,019
and our la team gets swept out into the

00:29:22,619 --> 00:29:27,479
ocean we want to make sure that that the

00:29:25,019 --> 00:29:28,859
distributed knowledge is there but if

00:29:27,479 --> 00:29:30,629
you have questions on the integration

00:29:28,859 --> 00:29:32,700
between clouds deck and SEF this is the

00:29:30,629 --> 00:29:33,690
guy to ask he hangs on our IRC channel

00:29:32,700 --> 00:29:38,249
so if you want to come to our IRC

00:29:33,690 --> 00:29:40,459
channel run oft cnet at SF you can hit

00:29:38,249 --> 00:29:44,639
his website 42 I com if you're a

00:29:40,459 --> 00:29:46,289
european dude or chick and you want to

00:29:44,639 --> 00:29:48,659
talk about CloudStack and SEF he has a

00:29:46,289 --> 00:29:52,499
there 42 on the company that's what they

00:29:48,659 --> 00:29:54,300
do they spin up Constance F that's pedo

00:29:52,499 --> 00:29:57,429
yeah

00:29:54,300 --> 00:30:00,370
and but you know we we work in Europe

00:29:57,429 --> 00:30:02,080
and we kind of throw stuff over the wall

00:30:00,370 --> 00:30:04,300
of each other but but yeah he's the guy

00:30:02,080 --> 00:30:05,740
who wrote the integration he is actively

00:30:04,300 --> 00:30:08,830
working on it and I know he would

00:30:05,740 --> 00:30:11,860
welcome help or patches if he wanted to

00:30:08,830 --> 00:30:17,080
do anything that he isn't currently

00:30:11,860 --> 00:30:19,120
doing so that suggests i busted through

00:30:17,080 --> 00:30:26,950
that pretty quick do with questions yeah

00:30:19,120 --> 00:30:32,780
far away yesterday we had interesting

00:30:26,950 --> 00:30:36,020
life around never objects really needed

00:30:32,780 --> 00:30:38,540
be a massive scale objects or

00:30:36,020 --> 00:30:43,010
was the largest production deployment as

00:30:38,540 --> 00:30:44,330
far as number of all the number of

00:30:43,010 --> 00:30:49,580
objects not the size of the cluster

00:30:44,330 --> 00:30:56,360
right um and you can come any magnitude

00:30:49,580 --> 00:30:58,550
rather than yeah well I know the largest

00:30:56,360 --> 00:31:00,770
cluster that we currently have as a

00:30:58,550 --> 00:31:04,340
customer a support customer is the

00:31:00,770 --> 00:31:06,860
dreamhost cluster though those guys did

00:31:04,340 --> 00:31:09,110
the the dream objects and check it out

00:31:06,860 --> 00:31:12,110
it's it's probably was the first and it

00:31:09,110 --> 00:31:14,060
was currently the largest supported

00:31:12,110 --> 00:31:15,380
production cluster but that being said I

00:31:14,060 --> 00:31:17,210
was actually just talking to some of the

00:31:15,380 --> 00:31:21,080
other day who spun off a SEF cluster on

00:31:17,210 --> 00:31:21,830
their own who was not associated with us

00:31:21,080 --> 00:31:24,260
and we didn't even know they were

00:31:21,830 --> 00:31:26,420
running safe until a random conversation

00:31:24,260 --> 00:31:30,050
happened but they have an object store

00:31:26,420 --> 00:31:32,710
that was running in the high three-digit

00:31:30,050 --> 00:31:36,590
millions in terms of numbers of objects

00:31:32,710 --> 00:31:39,530
and you know they're working on various

00:31:36,590 --> 00:31:40,850
ways of manipulating that and some

00:31:39,530 --> 00:31:42,920
performance things that they wanted to

00:31:40,850 --> 00:31:45,740
tune which is why they started talking

00:31:42,920 --> 00:31:49,160
to us but I can't give any specifics but

00:31:45,740 --> 00:31:51,500
yeah there's huge numbers of object that

00:31:49,160 --> 00:31:53,600
are out there but also large class large

00:31:51,500 --> 00:31:57,730
clusters that are out there so does that

00:31:53,600 --> 00:31:57,730
answer your question well

00:31:59,649 --> 00:32:07,149
yeah so the discussion yesterday was

00:32:02,690 --> 00:32:10,149
about the HDFS back object storage

00:32:07,149 --> 00:32:10,149
current

00:32:12,070 --> 00:32:17,330
solving some problems that need to be

00:32:14,260 --> 00:32:19,960
solved this is

00:32:17,330 --> 00:32:19,960
make it back

00:32:20,590 --> 00:32:25,150
okay yeah well I mean that's what dream

00:32:22,990 --> 00:32:31,920
object is it's a competitor for us three

00:32:25,150 --> 00:32:34,980
sure there's issues scale-up think

00:32:31,920 --> 00:32:39,230
the number of objects in s3 that tends

00:32:34,980 --> 00:32:42,300
to be magnitude of where anyone elses

00:32:39,230 --> 00:32:45,210
yeah that's at that point it's just a

00:32:42,300 --> 00:32:46,590
question of your cluster implementation

00:32:45,210 --> 00:32:48,420
right because that gateway that's

00:32:46,590 --> 00:32:50,850
providing the ability to do that as

00:32:48,420 --> 00:32:52,500
three stuff you can spin up as many

00:32:50,850 --> 00:32:54,990
gateway machines as you want to and load

00:32:52,500 --> 00:33:01,340
balance somehow you want to the actual

00:32:54,990 --> 00:33:03,210
number of objects within SEF itself is I

00:33:01,340 --> 00:33:06,210
hate to use the word but it's

00:33:03,210 --> 00:33:08,280
essentially infinite right depends you

00:33:06,210 --> 00:33:11,040
just have to scale out more machines

00:33:08,280 --> 00:33:15,470
more disks more moro SDS to be able to

00:33:11,040 --> 00:33:15,470
accomplish that keep right

00:33:17,670 --> 00:33:24,180
theoretically be investors perfectly

00:33:20,280 --> 00:33:26,130
right well in reality until they hit the

00:33:24,180 --> 00:33:28,340
the single node lookups problems with it

00:33:26,130 --> 00:33:28,340
with

00:33:29,159 --> 00:33:33,679
visit limit then except then

00:33:36,170 --> 00:33:40,480
jiminy

00:33:38,919 --> 00:33:42,629
yeah that's where I was just going with

00:33:40,480 --> 00:33:42,629
it

00:33:44,740 --> 00:33:49,410
there's probably not there's probably

00:33:46,600 --> 00:33:49,410
not an explicit

00:33:50,450 --> 00:33:55,160
if you're going to get yeah the first

00:33:53,990 --> 00:33:58,400
constraint that you're going to hit with

00:33:55,160 --> 00:34:00,710
SEF is network so I mean if you can if

00:33:58,400 --> 00:34:03,230
you could in theory throw infinite nicks

00:34:00,710 --> 00:34:05,660
and and infinite you know various

00:34:03,230 --> 00:34:07,580
network connections at it you could

00:34:05,660 --> 00:34:08,780
could scale it to infinity but obviously

00:34:07,580 --> 00:34:10,430
that's going to be the first place where

00:34:08,780 --> 00:34:12,650
people are showing that it's bounded is

00:34:10,430 --> 00:34:15,740
by network traffic because there is a

00:34:12,650 --> 00:34:17,990
lot of that inter-cluster communication

00:34:15,740 --> 00:34:21,200
that really it's good fairly chatty and

00:34:17,990 --> 00:34:23,570
actually as an aside if you're spinning

00:34:21,200 --> 00:34:25,490
up OS DS in terms of the actual data

00:34:23,570 --> 00:34:29,420
storage on the OSD we recommend that you

00:34:25,490 --> 00:34:31,790
don't use SSDs for that the the best

00:34:29,420 --> 00:34:33,680
performance and longevity that we're

00:34:31,790 --> 00:34:36,080
seeing is using spinning rust for the

00:34:33,680 --> 00:34:38,300
actual data and then journaling for that

00:34:36,080 --> 00:34:41,270
OSD on an SSD because you can split them

00:34:38,300 --> 00:34:42,860
apart but because it's so chatty and

00:34:41,270 --> 00:34:44,410
there's so many reads and writes we're

00:34:42,860 --> 00:34:46,820
actually finding that with a

00:34:44,410 --> 00:34:48,920
significantly large and high traffic

00:34:46,820 --> 00:34:52,040
cluster that we're burning out o s DS at

00:34:48,920 --> 00:34:55,840
a prodigious rate if you use them for

00:34:52,040 --> 00:34:55,840
all of your data storage

00:35:04,310 --> 00:35:09,140
that's a that's a much longer question

00:35:07,640 --> 00:35:10,790
or an answer than I probably have time

00:35:09,140 --> 00:35:13,000
to answer and the guy to talk to about

00:35:10,790 --> 00:35:15,410
that actually is Mark Nelson he's our

00:35:13,000 --> 00:35:16,820
performance guy he does all things

00:35:15,410 --> 00:35:18,710
performance he's done a number of really

00:35:16,820 --> 00:35:20,480
nice lengthy blog entries on the ceph

00:35:18,710 --> 00:35:22,280
blog but if you'd like to know more

00:35:20,480 --> 00:35:23,960
specifically hit me afterwards I put you

00:35:22,280 --> 00:35:28,750
in touch with Mark and he can give you

00:35:23,960 --> 00:35:32,360
the brain dump of all brain dumps yeah

00:35:28,750 --> 00:35:35,600
is there any thought to kind of quality

00:35:32,360 --> 00:35:38,030
service walks for late I want to

00:35:35,600 --> 00:35:39,660
guarantee you this block is 400 I off

00:35:38,030 --> 00:35:43,180
second not

00:35:39,660 --> 00:35:46,680
ten thousand the annoying neighbor

00:35:43,180 --> 00:35:46,680
quality service that um

00:35:46,970 --> 00:35:53,000
there isn't as far as I know there isn't

00:35:49,520 --> 00:35:56,780
any explicit work being done on Foss in

00:35:53,000 --> 00:36:04,840
terms of something like that I know that

00:35:56,780 --> 00:36:04,840
there are a number of one guy

00:36:07,950 --> 00:36:12,970
I don't know the explicit answer to that

00:36:11,410 --> 00:36:16,210
I have an idea but I don't want to talk

00:36:12,970 --> 00:36:20,430
out of my other regions shoot me an

00:36:16,210 --> 00:36:20,430
email I'll get you the right answer

00:36:27,300 --> 00:36:34,880
she told which full time to DBS contain

00:36:35,160 --> 00:36:40,670
hi for losses

00:36:37,620 --> 00:36:42,900
my brave

00:36:40,670 --> 00:36:46,640
SolidFire trying to do a little work

00:36:42,900 --> 00:36:46,640
around you

00:36:47,970 --> 00:36:53,059
that's not

00:36:50,099 --> 00:36:53,059
that's necessarily yeah

00:36:56,440 --> 00:37:02,770
but I mean the short answer is that

00:36:59,410 --> 00:37:06,190
there are some things that you can do

00:37:02,770 --> 00:37:08,650
with staff where you can tell it how to

00:37:06,190 --> 00:37:09,819
handle particular data or requests or

00:37:08,650 --> 00:37:10,900
kind of where those things are supposed

00:37:09,819 --> 00:37:13,869
to live and how they're supposed to get

00:37:10,900 --> 00:37:18,550
there but there's no real answer to cost

00:37:13,869 --> 00:37:21,030
right now it's the short answer any

00:37:18,550 --> 00:37:21,030
other questions

00:37:28,079 --> 00:37:39,849
say it again Oh what's the latency for

00:37:36,880 --> 00:37:41,619
when an OSD fails in terms of in terms

00:37:39,849 --> 00:37:45,369
of what the recovery of the data before

00:37:41,619 --> 00:37:47,109
you reach full replication that is

00:37:45,369 --> 00:37:50,049
another one of those i'm going to add an

00:37:47,109 --> 00:37:52,769
asterisk to the answer but that's highly

00:37:50,049 --> 00:37:55,960
dependent on things like your network

00:37:52,769 --> 00:38:00,999
the actual size of your placement groups

00:37:55,960 --> 00:38:02,440
is tunable i think the this i'm not sure

00:38:00,999 --> 00:38:05,079
what the minimum placement group size is

00:38:02,440 --> 00:38:07,539
off top my head but yeah you can you can

00:38:05,079 --> 00:38:10,329
tune that to be based on your particular

00:38:07,539 --> 00:38:16,529
network infrastructure to speed that up

00:38:10,329 --> 00:38:19,239
and I'm no no the the OS DS will are

00:38:16,529 --> 00:38:20,859
aware of who's in and who's out because

00:38:19,239 --> 00:38:22,210
there's multiple monitors and they're

00:38:20,859 --> 00:38:24,160
talking to each other all the time and

00:38:22,210 --> 00:38:26,140
if it hits a timeout and it can't talk

00:38:24,160 --> 00:38:29,529
to a particular OSD after a certain

00:38:26,140 --> 00:38:33,279
amount of time also configurable it'll

00:38:29,529 --> 00:38:34,479
say this OSD is down it'll market is

00:38:33,279 --> 00:38:39,279
down and then it will start migrating

00:38:34,479 --> 00:38:43,180
data so yeah it's it's all configurable

00:38:39,279 --> 00:38:46,690
and that's that's actually why we invent

00:38:43,180 --> 00:38:48,999
we created ink tank the company Seph is

00:38:46,690 --> 00:38:51,849
incredibly powerful but with you know a

00:38:48,999 --> 00:38:53,319
huge amount of any system power there's

00:38:51,849 --> 00:38:54,969
also means there's tons and tons of

00:38:53,319 --> 00:38:56,799
tunable Zand configurable and stuff like

00:38:54,969 --> 00:38:58,749
that and so we were getting so many

00:38:56,799 --> 00:39:01,269
questions about hey come in and tune my

00:38:58,749 --> 00:39:03,099
cluster that that's why we finally made

00:39:01,269 --> 00:39:06,670
the decision to say we need a company to

00:39:03,099 --> 00:39:08,559
do that so yeah it's highly predicated

00:39:06,670 --> 00:39:13,269
on what your particular infrastructure

00:39:08,559 --> 00:39:17,039
building is but the answer is it can be

00:39:13,269 --> 00:39:17,039
as good as you need it to be really

00:39:27,099 --> 00:39:34,099
what's in bobtail so the question is can

00:39:31,940 --> 00:39:35,660
you have two different networks 14

00:39:34,099 --> 00:39:41,359
control messages and one for the actual

00:39:35,660 --> 00:39:43,750
data path all right I don't want to say

00:39:41,359 --> 00:39:46,849
yes because I don't know for sure I

00:39:43,750 --> 00:39:48,950
liked Steve's talk about how it's you

00:39:46,849 --> 00:39:50,810
know the guy teaching is not necessarily

00:39:48,950 --> 00:39:52,930
the guy with all the answers I'm far

00:39:50,810 --> 00:39:54,980
from the guy with all the answers um

00:39:52,930 --> 00:40:00,680
shoot me an email I'll get you the right

00:39:54,980 --> 00:40:04,390
answer any other questions all right

00:40:00,680 --> 00:40:04,390

YouTube URL: https://www.youtube.com/watch?v=DQ203KOfsuk


