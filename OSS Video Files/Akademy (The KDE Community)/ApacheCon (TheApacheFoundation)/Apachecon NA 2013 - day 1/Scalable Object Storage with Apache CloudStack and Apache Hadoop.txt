Title: Scalable Object Storage with Apache CloudStack and Apache Hadoop
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Chiradeep Vittal ApacheCon NA 2013
Cloud Crowd
Captions: 
	00:00:00,260 --> 00:00:11,820
so how's everybody doing today and my

00:00:08,849 --> 00:00:15,509
name is charlie i work for citrix

00:00:11,820 --> 00:00:20,070
systems and I've been working on Apache

00:00:15,509 --> 00:00:21,990
CloudStack since 2008 and I'm committed

00:00:20,070 --> 00:00:24,900
to the Apache CloudStack project and

00:00:21,990 --> 00:00:27,599
among the ppm c and today i'm going to

00:00:24,900 --> 00:00:30,119
talk to you about scalable object

00:00:27,599 --> 00:00:34,010
storage and what it means for patrick

00:00:30,119 --> 00:00:36,809
cloudstack and some kind of a design

00:00:34,010 --> 00:00:39,750
exercise we did with the Apache Hadoop

00:00:36,809 --> 00:00:43,110
project to explore whether Apache Hadoop

00:00:39,750 --> 00:00:47,789
could be a suitable object storage layer

00:00:43,110 --> 00:00:49,980
for CloudStack and beyond so before I

00:00:47,789 --> 00:00:52,410
started just like a show of hands who's

00:00:49,980 --> 00:00:57,329
heard of CloudStack and in actually

00:00:52,410 --> 00:01:07,530
installed and work with CloudStack okay

00:00:57,329 --> 00:01:10,770
and Hadoop folks okay so so so i'm going

00:01:07,530 --> 00:01:12,930
to introduce cloudstack so for those of

00:01:10,770 --> 00:01:15,450
you already know CloudStack hopefully

00:01:12,930 --> 00:01:17,850
it's not too much for you guys I'm not

00:01:15,450 --> 00:01:19,860
going to talk much about Hadoop assuming

00:01:17,850 --> 00:01:21,479
a lot of people know more about how did

00:01:19,860 --> 00:01:25,830
the cloud stacked it's been around for

00:01:21,479 --> 00:01:26,939
five years now so this is this is

00:01:25,830 --> 00:01:29,100
roughly the agenda i'll talk about

00:01:26,939 --> 00:01:32,610
CloudStack why we need object storage

00:01:29,100 --> 00:01:37,590
for infrastructure as a service software

00:01:32,610 --> 00:01:39,420
like cloudstack what we use today for in

00:01:37,590 --> 00:01:42,540
lieu object storage and what are those

00:01:39,420 --> 00:01:44,610
limitations and then we'll come up with

00:01:42,540 --> 00:01:46,740
from first principles what we would like

00:01:44,610 --> 00:01:49,200
for this for any object storage layer

00:01:46,740 --> 00:01:52,770
layer for for infrastructure as a

00:01:49,200 --> 00:01:54,750
service systems and I'll describe some

00:01:52,770 --> 00:01:57,450
of the current object store decorations

00:01:54,750 --> 00:02:00,240
in platte stack and then I'll talk about

00:01:57,450 --> 00:02:02,159
well how km/h TFS or Hadoop which is

00:02:00,240 --> 00:02:04,710
part of the Hadoop project be

00:02:02,159 --> 00:02:07,110
replacement or wire will contender for

00:02:04,710 --> 00:02:09,930
object storage in cloud

00:02:07,110 --> 00:02:12,750
and finally I talked about some future

00:02:09,930 --> 00:02:14,670
directions for those of you don't know

00:02:12,750 --> 00:02:18,210
you know cloudstack it's been incubating

00:02:14,670 --> 00:02:20,490
in the Apache foundation since April

00:02:18,210 --> 00:02:22,830
last year and but it's actually been

00:02:20,490 --> 00:02:26,130
open source longer than that and it's

00:02:22,830 --> 00:02:27,900
been in production since 2009 so fair

00:02:26,130 --> 00:02:34,310
bit of commercial and open source

00:02:27,900 --> 00:02:36,900
deployments fairly solid and mature and

00:02:34,310 --> 00:02:39,080
so the question you want to ask when

00:02:36,900 --> 00:02:41,640
you're building out a cloud is well

00:02:39,080 --> 00:02:43,470
obviously the big gorilla here is

00:02:41,640 --> 00:02:45,090
Amazon's well thought well hard as

00:02:43,470 --> 00:02:47,160
Amazon build a cloud so they start off

00:02:45,090 --> 00:02:49,770
with some commodity servers add some

00:02:47,160 --> 00:02:53,160
commodity networking that some commodity

00:02:49,770 --> 00:02:55,620
storage and then they use the

00:02:53,160 --> 00:02:57,450
open-source Xen hypervisor to kind of

00:02:55,620 --> 00:03:01,110
stitch together a lot of these

00:02:57,450 --> 00:03:02,760
components and then they have their own

00:03:01,110 --> 00:03:04,830
orchestration server I mean earlier to

00:03:02,760 --> 00:03:07,350
be andrea was talking about the app or

00:03:04,830 --> 00:03:10,980
the activity orchestration so they

00:03:07,350 --> 00:03:12,750
probably has something similar similar

00:03:10,980 --> 00:03:14,910
start set of services in there in the

00:03:12,750 --> 00:03:16,680
data center which orchestrates the

00:03:14,910 --> 00:03:20,760
hypervisor the networking the storage in

00:03:16,680 --> 00:03:23,310
league and the servers and then the Yu

00:03:20,760 --> 00:03:25,860
Yan very nice API a cc API to interact

00:03:23,310 --> 00:03:28,200
with their that cloud so that includes

00:03:25,860 --> 00:03:31,080
love ec2 which is the compute a p.i the

00:03:28,200 --> 00:03:34,019
s3 which is the storage API and then

00:03:31,080 --> 00:03:36,560
they give you a you know some way to

00:03:34,019 --> 00:03:39,299
purchase these services vending platform

00:03:36,560 --> 00:03:41,489
so well how do you build your cloud

00:03:39,299 --> 00:03:43,799
right you start off with the same thing

00:03:41,489 --> 00:03:45,510
networking server storage and it doesn't

00:03:43,799 --> 00:03:51,060
have to be commodity you can use the

00:03:45,510 --> 00:03:52,980
most expensive servers you can find you

00:03:51,060 --> 00:03:57,900
you can use any hypervisors you want

00:03:52,980 --> 00:04:00,959
then kvm VMware even Oracle VM if you if

00:03:57,900 --> 00:04:02,340
you wish to and then this is where

00:04:00,959 --> 00:04:04,260
cloudstack steps and where you can

00:04:02,340 --> 00:04:07,079
orchestrate the networking the

00:04:04,260 --> 00:04:09,030
hypervisor is the storage in order to

00:04:07,079 --> 00:04:12,810
give you the same crowd like experience

00:04:09,030 --> 00:04:14,519
and and then you can use that with the

00:04:12,810 --> 00:04:17,970
CloudStack api or you can use it with

00:04:14,519 --> 00:04:20,070
the ec2 api and then or and then you

00:04:17,970 --> 00:04:20,810
probably need if you're a public service

00:04:20,070 --> 00:04:22,860
provider

00:04:20,810 --> 00:04:25,920
cloud provider you probably need some

00:04:22,860 --> 00:04:28,890
kind of a portal 2 2 ND service lv

00:04:25,920 --> 00:04:32,010
services to the public so that's how you

00:04:28,890 --> 00:04:34,590
build a clock select cloud and so let's

00:04:32,010 --> 00:04:38,370
go inside what's inside a cloud stack

00:04:34,590 --> 00:04:40,470
matter so Club stock has this concept of

00:04:38,370 --> 00:04:43,620
a part which is a group of related

00:04:40,470 --> 00:04:45,420
servers which is typically a single

00:04:43,620 --> 00:04:47,490
failure domain so easily it's like a

00:04:45,420 --> 00:04:49,740
rack of servers so they had the same

00:04:47,490 --> 00:04:51,450
power supply they are connected at the

00:04:49,740 --> 00:04:52,890
same switch so the power supply fails

00:04:51,450 --> 00:04:55,260
the rack is going to fail the switch

00:04:52,890 --> 00:04:57,600
fails is the Iraq is going to fail and

00:04:55,260 --> 00:05:01,410
there's usually some you know associated

00:04:57,600 --> 00:05:05,940
primary storage with that and then you

00:05:01,410 --> 00:05:09,450
connect that rack to through the l3 core

00:05:05,940 --> 00:05:11,430
to the internet right and then as use as

00:05:09,450 --> 00:05:13,830
your cloud browse the bigger you keep

00:05:11,430 --> 00:05:20,580
you know I like a cookie cutter you keep

00:05:13,830 --> 00:05:23,310
adding more and more parts and and and

00:05:20,580 --> 00:05:26,820
then you have the crowd stack management

00:05:23,310 --> 00:05:29,070
server on the left hand side to to

00:05:26,820 --> 00:05:31,050
orchestrate everything for you and then

00:05:29,070 --> 00:05:33,750
you can interact with cloudstack using

00:05:31,050 --> 00:05:36,830
the end-user API and if the cloud admin

00:05:33,750 --> 00:05:39,450
you can use the admin epi to do that and

00:05:36,830 --> 00:05:41,820
finally the last piece is the secondary

00:05:39,450 --> 00:05:43,650
storage and I'll explain why we need

00:05:41,820 --> 00:05:46,140
secondary storage in addition to the

00:05:43,650 --> 00:05:49,680
primary storage secondary storage is

00:05:46,140 --> 00:05:53,250
mainly used for permanent immutable

00:05:49,680 --> 00:05:56,100
objects stuff that you don't ever modify

00:05:53,250 --> 00:06:02,310
once you create it so your templates

00:05:56,100 --> 00:06:04,380
your snapshots your your is ours so what

00:06:02,310 --> 00:06:06,180
you do is what clubs that orchestrates

00:06:04,380 --> 00:06:09,330
for you is that when you ask you to

00:06:06,180 --> 00:06:11,460
start a vm it takes an image or template

00:06:09,330 --> 00:06:13,950
of a virtual machine from the secondary

00:06:11,460 --> 00:06:15,510
store and then copies it to the the

00:06:13,950 --> 00:06:20,070
prime minister like you see on the

00:06:15,510 --> 00:06:22,890
bottom bottom to slide there and that

00:06:20,070 --> 00:06:28,440
that allows the vm to start running on a

00:06:22,890 --> 00:06:29,760
server in that part and now that you

00:06:28,440 --> 00:06:30,670
have been running the vm for some time

00:06:29,760 --> 00:06:32,800
you want to make up

00:06:30,670 --> 00:06:34,990
a cup of it so CloudStack will let you

00:06:32,800 --> 00:06:36,540
take a snapshot of it and the snapshot

00:06:34,990 --> 00:06:38,950
then moves to the secondary storage

00:06:36,540 --> 00:06:43,090
right so simple enough clutched a

00:06:38,950 --> 00:06:44,710
carcass traits that for you and then the

00:06:43,090 --> 00:06:46,150
the neat thing about snapshots is that

00:06:44,710 --> 00:06:48,640
you can use the snapshot to create

00:06:46,150 --> 00:06:51,040
additional templates and now you can say

00:06:48,640 --> 00:06:53,860
that I want a clone of that VMI just

00:06:51,040 --> 00:06:55,540
snap share it and then crushed acrylate

00:06:53,860 --> 00:06:57,910
the creative second vm which looks

00:06:55,540 --> 00:07:00,130
exactly at the first vm so this is the

00:06:57,910 --> 00:07:04,510
the main purpose of secondary storage to

00:07:00,130 --> 00:07:07,360
give you that scalable backup storage to

00:07:04,510 --> 00:07:10,170
store your immutable permanent objects

00:07:07,360 --> 00:07:15,700
like templates and snapshots and aisles

00:07:10,170 --> 00:07:16,930
and this becomes important because when

00:07:15,700 --> 00:07:18,970
you when you're running a cloud style

00:07:16,930 --> 00:07:20,740
workload which is quite a little bit

00:07:18,970 --> 00:07:24,510
different from your enterprise style

00:07:20,740 --> 00:07:26,200
workloads you're running on a

00:07:24,510 --> 00:07:27,970
standardized cookie cutter

00:07:26,200 --> 00:07:29,650
infrastructure it's not necessarily the

00:07:27,970 --> 00:07:31,390
greatest the most expensive the most

00:07:29,650 --> 00:07:34,780
reliable almost redundant hardware you

00:07:31,390 --> 00:07:37,680
can find but you rely on automation and

00:07:34,780 --> 00:07:40,870
efficiencies to get to cut Acosta and

00:07:37,680 --> 00:07:43,210
what key feature of running things on

00:07:40,870 --> 00:07:46,690
the cloud is that your IT department

00:07:43,210 --> 00:07:48,790
here the godly the server admin the the

00:07:46,690 --> 00:07:52,120
network admin doesn't only availability

00:07:48,790 --> 00:07:53,950
you only regular ability and and as we

00:07:52,120 --> 00:07:57,610
saw that the clouds can get very big and

00:07:53,950 --> 00:07:59,830
as you add these reliable and semi

00:07:57,610 --> 00:08:03,400
reliable components you find that things

00:07:59,830 --> 00:08:05,860
break all the time right and so what the

00:08:03,400 --> 00:08:08,410
you as an application writer you focus

00:08:05,860 --> 00:08:11,650
on is the mean time to repair as opposed

00:08:08,410 --> 00:08:15,100
to the mean time between failures and so

00:08:11,650 --> 00:08:16,870
here you know you're you're you've been

00:08:15,100 --> 00:08:18,370
running your vm and that hypervisor your

00:08:16,870 --> 00:08:21,280
hypervisor disappears well what do you

00:08:18,370 --> 00:08:23,710
do well thank God you got a snapshot on

00:08:21,280 --> 00:08:25,840
secondary storage from which you can

00:08:23,710 --> 00:08:29,350
create your vm and get back up running

00:08:25,840 --> 00:08:31,600
right boom you're Iraq disappears and

00:08:29,350 --> 00:08:35,050
then again the primary storage is gone

00:08:31,600 --> 00:08:38,820
so you again you go to your secondary

00:08:35,050 --> 00:08:41,320
storage and then restore from backup and

00:08:38,820 --> 00:08:44,519
then I a primary storage can and boom

00:08:41,320 --> 00:08:49,330
maybe your old zone disappears right

00:08:44,519 --> 00:08:51,370
so so the the typical deployment order

00:08:49,330 --> 00:08:55,360
to to handle this kind of reliability

00:08:51,370 --> 00:08:58,600
concerns is that you you deploy a set of

00:08:55,360 --> 00:09:00,760
data centers which are close to each

00:08:58,600 --> 00:09:03,579
other but not necessarily in the same

00:09:00,760 --> 00:09:04,779
feeling domain so for example they're

00:09:03,579 --> 00:09:08,560
not going to get flooded at the same

00:09:04,779 --> 00:09:09,880
time or they're not going to get hit by

00:09:08,560 --> 00:09:13,209
earthquake at the same time maybe they

00:09:09,880 --> 00:09:15,610
aren't different ethnic Forge so you

00:09:13,209 --> 00:09:18,190
replicate that that zone infrastructure

00:09:15,610 --> 00:09:20,200
is talking about into each of these data

00:09:18,190 --> 00:09:22,930
centers and then you connect them that's

00:09:20,200 --> 00:09:24,910
a very low latency backbone so you know

00:09:22,930 --> 00:09:27,550
maybe a millisecond the two millisecond

00:09:24,910 --> 00:09:30,459
back so you know these are within 10 20

00:09:27,550 --> 00:09:35,170
30 kilometers of each other to give you

00:09:30,459 --> 00:09:36,670
that kind of latency and so and then

00:09:35,170 --> 00:09:39,579
what you do is that if you want to look

00:09:36,670 --> 00:09:41,829
a global scale cloud you then you deploy

00:09:39,579 --> 00:09:44,260
multiple regions and then interconnect

00:09:41,829 --> 00:09:46,779
them over the Internet all right so this

00:09:44,260 --> 00:09:49,510
is the regions and zones concert which

00:09:46,779 --> 00:09:51,899
those of you who use Amazon should be

00:09:49,510 --> 00:09:51,899
familiar with

00:09:59,320 --> 00:10:04,900
so what's what's the current status of

00:10:02,980 --> 00:10:08,740
secondary storage in implied stock

00:10:04,900 --> 00:10:11,590
photography so by default the earthy you

00:10:08,740 --> 00:10:13,540
deploy in NFS server destroy your images

00:10:11,590 --> 00:10:16,150
and those immutable objects are talking

00:10:13,540 --> 00:10:18,190
about and it's great because it can be

00:10:16,150 --> 00:10:21,670
monitored by any hypervisor most

00:10:18,190 --> 00:10:24,130
hypervisors understand NFS everybody and

00:10:21,670 --> 00:10:28,510
his dog knows how to administer an NFS

00:10:24,130 --> 00:10:30,970
server it's really easy but then it has

00:10:28,510 --> 00:10:34,420
some problems we know it doesn't scale

00:10:30,970 --> 00:10:36,250
well it's a very chatty protocol you are

00:10:34,420 --> 00:10:37,720
deployed over long distances then you

00:10:36,250 --> 00:10:43,600
got to think about well I'll do I need a

00:10:37,720 --> 00:10:45,130
van optimizer and then you know and it

00:10:43,600 --> 00:10:47,020
is a bottleneck I mean you got a

00:10:45,130 --> 00:10:48,670
thousand hypervisors and they are trying

00:10:47,020 --> 00:10:51,700
to talk to this one and if a storage

00:10:48,670 --> 00:10:55,450
server right and then you know out of

00:10:51,700 --> 00:10:57,460
the box most of these NFS servers which

00:10:55,450 --> 00:10:59,860
you can get with enough with Linux or

00:10:57,460 --> 00:11:03,460
any of your operating systems then are

00:10:59,860 --> 00:11:06,280
replicated right sure i can use raid but

00:11:03,460 --> 00:11:08,670
what if the machine blows up right so

00:11:06,280 --> 00:11:10,840
there's some problems with NFS vicino

00:11:08,670 --> 00:11:16,270
undermined the reliability of your

00:11:10,840 --> 00:11:18,640
profit is injured so the solution now

00:11:16,270 --> 00:11:20,290
one solution is to use object storage

00:11:18,640 --> 00:11:23,290
and those of you are familiar with

00:11:20,290 --> 00:11:25,540
amazon s3 is what i'm talking about

00:11:23,290 --> 00:11:29,080
which is an object storage solution

00:11:25,540 --> 00:11:31,420
which gives you a seemingly infinite

00:11:29,080 --> 00:11:35,520
amount of storage capacity where you can

00:11:31,420 --> 00:11:39,300
store and retrieve objects through HTTP

00:11:35,520 --> 00:11:41,410
you know in a very very very simple API

00:11:39,300 --> 00:11:44,680
so it looks something like this year

00:11:41,410 --> 00:11:48,180
it's still inside the region and there's

00:11:44,680 --> 00:11:50,740
a bunch of disks inside each zone and

00:11:48,180 --> 00:11:57,490
that's managed by this object storage

00:11:50,740 --> 00:11:59,050
technology something like s3 and what

00:11:57,490 --> 00:12:01,000
the object storage technology gives you

00:11:59,050 --> 00:12:04,180
is it replicates the objects between

00:12:01,000 --> 00:12:05,590
these between these zones at it ordered

00:12:04,180 --> 00:12:08,050
stand make sure that there's always

00:12:05,590 --> 00:12:10,529
three copies for example it gives you

00:12:08,050 --> 00:12:12,180
repairs of one

00:12:10,529 --> 00:12:13,999
goes out of saying then it you know it

00:12:12,180 --> 00:12:16,170
makes sure there's three copies again

00:12:13,999 --> 00:12:17,970
maintenance usage records you know

00:12:16,170 --> 00:12:23,550
that's the that's what an object storage

00:12:17,970 --> 00:12:26,550
technology gives you and and this gives

00:12:23,550 --> 00:12:29,639
you reliability because let's say you're

00:12:26,550 --> 00:12:33,779
running some VMS in your region he

00:12:29,639 --> 00:12:36,180
thought we beams in all four zones so

00:12:33,779 --> 00:12:38,730
the object storage is ensuring that your

00:12:36,180 --> 00:12:43,860
volumes are replicated to to each fail

00:12:38,730 --> 00:12:47,189
in the main each zone and then and then

00:12:43,860 --> 00:12:49,740
as your snapshot your VMs it makes sure

00:12:47,189 --> 00:12:51,689
that those snapshots get copied to each

00:12:49,740 --> 00:12:55,829
of the different zones so that you never

00:12:51,689 --> 00:12:57,540
ever lose your snapshot and then so if

00:12:55,829 --> 00:12:59,129
your zone disappears you still have a

00:12:57,540 --> 00:13:00,899
copy of your snapshot and at least one

00:12:59,129 --> 00:13:08,009
of the zones in order to be able to

00:13:00,899 --> 00:13:12,990
carry on right and so there you go

00:13:08,009 --> 00:13:15,449
you're up and running again and then

00:13:12,990 --> 00:13:17,459
that's not the only thing it also

00:13:15,449 --> 00:13:20,339
enables other applications I mean if you

00:13:17,459 --> 00:13:22,529
look at s3 the kind of innovation of in

00:13:20,339 --> 00:13:24,949
the startups and other people have been

00:13:22,529 --> 00:13:28,740
able to bring about using just using s3

00:13:24,949 --> 00:13:31,319
by just putting an API a simple API

00:13:28,740 --> 00:13:34,589
server in front of the object storage

00:13:31,319 --> 00:13:37,559
technology you get you know application

00:13:34,589 --> 00:13:40,769
like Dropbox you get inability to store

00:13:37,559 --> 00:13:42,990
static content you do content delivery

00:13:40,769 --> 00:13:47,430
networks you can already use it for

00:13:42,990 --> 00:13:48,990
archival so not only does object storage

00:13:47,430 --> 00:13:52,529
technology have applications in

00:13:48,990 --> 00:13:55,860
infrastructure-as-a-service deployments

00:13:52,529 --> 00:13:59,129
but it you can repurpose or use it for

00:13:55,860 --> 00:14:02,750
other purposes like like neat

00:13:59,129 --> 00:14:02,750
applications like Dropbox right

00:14:04,920 --> 00:14:08,890
so what are the some of the correct

00:14:07,200 --> 00:14:12,130
characteristics of object storage

00:14:08,890 --> 00:14:15,130
technology which gives you this the span

00:14:12,130 --> 00:14:16,600
of advantages first it's in a highly

00:14:15,130 --> 00:14:20,230
reliable and durable if you look at

00:14:16,600 --> 00:14:22,870
amazon s3 it promises ninety-nine point

00:14:20,230 --> 00:14:25,540
nine percent availability and eleven

00:14:22,870 --> 00:14:27,280
lines of durability so that eleven 9s

00:14:25,540 --> 00:14:29,320
durability means that you've gotta wait

00:14:27,280 --> 00:14:36,160
10 million years before you lose

00:14:29,320 --> 00:14:39,760
anything in amazon s3 and its massive

00:14:36,160 --> 00:14:41,830
scale i mean it at last count s3 was at

00:14:39,760 --> 00:14:43,360
one point three trillion objects and

00:14:41,830 --> 00:14:46,840
it's toured across all their seven

00:14:43,360 --> 00:14:48,520
regions and they have throughput of you

00:14:46,840 --> 00:14:52,810
know nearly a million requests per

00:14:48,520 --> 00:14:55,600
second and so that's the kind of the top

00:14:52,810 --> 00:14:57,340
end of the scale of object storage and a

00:14:55,600 --> 00:15:04,510
growing and they probably go to be two

00:14:57,340 --> 00:15:07,000
trillion by the end of the year the key

00:15:04,510 --> 00:15:08,920
aspect of object storage is that these

00:15:07,000 --> 00:15:11,290
are immutable objects so once you store

00:15:08,920 --> 00:15:13,030
an object an object storage you can't

00:15:11,290 --> 00:15:15,850
delete you can't modify it you can't

00:15:13,030 --> 00:15:17,380
seek to any kind right to the tenth part

00:15:15,850 --> 00:15:21,310
of it you cannot do anything all you can

00:15:17,380 --> 00:15:23,470
do is delete it and replace it and so

00:15:21,310 --> 00:15:25,600
that gives you a lot of advantages when

00:15:23,470 --> 00:15:27,700
you're designing your object storage

00:15:25,600 --> 00:15:29,770
system because you don't suddenly have

00:15:27,700 --> 00:15:31,330
to worry about multiple people are

00:15:29,770 --> 00:15:33,310
trying to modify the same location in

00:15:31,330 --> 00:15:37,030
the file it becomes a much easier

00:15:33,310 --> 00:15:39,490
problem to solve the next aspect is that

00:15:37,030 --> 00:15:41,500
it has a simple API and this is true of

00:15:39,490 --> 00:15:43,840
almost any storage system out there

00:15:41,500 --> 00:15:48,310
whether you look at Google storage or as

00:15:43,840 --> 00:15:50,530
your storage or Amazon s3 is the simple

00:15:48,310 --> 00:15:52,600
put post of objects get objects delete

00:15:50,530 --> 00:15:55,870
objects and if you look at their API as

00:15:52,600 --> 00:15:57,640
they all look almost identical and so

00:15:55,870 --> 00:16:00,130
there's no seek there's no mutation

00:15:57,640 --> 00:16:05,279
there's no pausing semantics it is just

00:16:00,130 --> 00:16:07,930
a few operations to

00:16:05,279 --> 00:16:10,180
the last simplifying factor is that it's

00:16:07,930 --> 00:16:12,819
a flat namespace everything is stored in

00:16:10,180 --> 00:16:14,439
buckets are containers or what are we

00:16:12,819 --> 00:16:16,689
called and these bucket names are unique

00:16:14,439 --> 00:16:18,579
and buckets can only contain other

00:16:16,689 --> 00:16:20,589
objects they cannot contain other odd

00:16:18,579 --> 00:16:22,660
bucket so there's no a directory system

00:16:20,589 --> 00:16:26,319
there is no extra metadata there's no

00:16:22,660 --> 00:16:28,120
directed shower so there's no soft links

00:16:26,319 --> 00:16:31,350
hard links moving stuff between

00:16:28,120 --> 00:16:34,839
directories everything is simplified

00:16:31,350 --> 00:16:36,189
finally it's cheap as getting cheaper

00:16:34,839 --> 00:16:41,649
all the time so that's another advantage

00:16:36,189 --> 00:16:45,990
of our text origin so what cloud sect

00:16:41,649 --> 00:16:49,990
has is an s3 compatible API server and

00:16:45,990 --> 00:16:53,319
what it does is that it is just the API

00:16:49,990 --> 00:16:56,230
server if you when you deploy it it

00:16:53,319 --> 00:16:58,720
writes to a POSIX file system at the

00:16:56,230 --> 00:17:00,430
back end well it's probable in the sense

00:16:58,720 --> 00:17:06,339
that you can store you can plug in any

00:17:00,430 --> 00:17:09,159
object storage technology behind it and

00:17:06,339 --> 00:17:12,699
so in turn Stein's the Amazon s3 dress

00:17:09,159 --> 00:17:14,020
style and so KPIs and it has pluggable

00:17:12,699 --> 00:17:16,510
back end and the back and storage needs

00:17:14,020 --> 00:17:19,089
to do very simple things it's usually a

00:17:16,510 --> 00:17:21,720
few hundred lines of code to to

00:17:19,089 --> 00:17:25,390
integrate with the s3 API front-end

00:17:21,720 --> 00:17:26,919
example you know create container it

00:17:25,390 --> 00:17:28,600
means that you create a bucket you know

00:17:26,919 --> 00:17:31,450
save object means you know right the

00:17:28,600 --> 00:17:34,809
streaming right to the TR object storage

00:17:31,450 --> 00:17:37,809
system and the default back end is a is

00:17:34,809 --> 00:17:39,880
a POSIX file system helps you get up and

00:17:37,809 --> 00:17:44,169
running but not very in a useful in

00:17:39,880 --> 00:17:46,390
production and so there's one vendors

00:17:44,169 --> 00:17:48,130
who integrated their object storage with

00:17:46,390 --> 00:17:50,740
our is with the crouched access three

00:17:48,130 --> 00:17:54,190
aps over that's called currying object

00:17:50,740 --> 00:17:58,480
store out of austin and then we also

00:17:54,190 --> 00:18:03,370
integrated HDFS so if you look at this

00:17:58,480 --> 00:18:06,909
diagram here s the API server can talk

00:18:03,370 --> 00:18:09,720
to HDFS API to a cluster of it up Hadoop

00:18:06,909 --> 00:18:09,720
nodes right

00:18:16,780 --> 00:18:24,410
that's right it does yeah so it so what

00:18:22,670 --> 00:18:27,230
we showed you is that doesn't there's a

00:18:24,410 --> 00:18:29,830
MySQL database right there and flush

00:18:27,230 --> 00:18:32,570
stack is that my sequel storage to store

00:18:29,830 --> 00:18:35,480
for example bucket object mappings and

00:18:32,570 --> 00:18:47,150
then object metadata like axles and

00:18:35,480 --> 00:18:48,350
bucket policies depends on the object or

00:18:47,150 --> 00:18:56,270
a tech not one of the one of the

00:18:48,350 --> 00:18:58,670
constraints yeah so and then how well

00:18:56,270 --> 00:19:01,040
this is you know the s3 castle but

00:18:58,670 --> 00:19:04,250
object storage in general inside

00:19:01,040 --> 00:19:06,050
CloudStack how would you use it as i

00:19:04,250 --> 00:19:09,530
said before use it for mutable stuff

00:19:06,050 --> 00:19:11,210
like images and snapshots and as a

00:19:09,530 --> 00:19:14,030
replacement for the NFS secondary

00:19:11,210 --> 00:19:15,850
storage or you can augment your

00:19:14,030 --> 00:19:18,410
secondary storage and use it to

00:19:15,850 --> 00:19:20,860
distribute your templates and images

00:19:18,410 --> 00:19:23,300
throughout throughout the region and

00:19:20,860 --> 00:19:26,230
today we have integration is available

00:19:23,300 --> 00:19:29,210
with react CS and OpenStack Swift and

00:19:26,230 --> 00:19:31,520
what's up coming in in the next release

00:19:29,210 --> 00:19:33,680
which is the june release is a framework

00:19:31,520 --> 00:19:37,040
for integrating more storage providers

00:19:33,680 --> 00:19:40,010
which can store CloudStack snapshots and

00:19:37,040 --> 00:19:42,520
platt stock images and templates on to

00:19:40,010 --> 00:19:42,520
object storage

00:19:45,789 --> 00:19:51,700
so that's the current state of the art

00:19:49,210 --> 00:19:54,659
and so the next part of the talk we'll

00:19:51,700 --> 00:19:56,739
talk about well what could be billed

00:19:54,659 --> 00:19:58,989
given that we want a general-purpose

00:19:56,739 --> 00:20:01,059
object storage server which can not only

00:19:58,989 --> 00:20:04,409
satisfy your infrastructure-as-a-service

00:20:01,059 --> 00:20:08,769
need but also just a neat stuff like

00:20:04,409 --> 00:20:10,929
like a like a Dropbox application so

00:20:08,769 --> 00:20:12,489
here's some in a strong man requirements

00:20:10,929 --> 00:20:14,440
you want to be open source you wanted to

00:20:12,489 --> 00:20:17,710
be hopefully in the apache software

00:20:14,440 --> 00:20:19,059
foundation you wanted to scale to at

00:20:17,710 --> 00:20:21,940
least a billion object I mean that's

00:20:19,059 --> 00:20:23,559
table stakes these days I mean if if

00:20:21,940 --> 00:20:25,029
Amazon's that our trigger objects you

00:20:23,559 --> 00:20:28,869
know only between 10 million objects

00:20:25,029 --> 00:20:31,059
right deserve you want reliability and

00:20:28,869 --> 00:20:33,609
durability on par with s3 no that's

00:20:31,059 --> 00:20:37,929
that's good goal to have you want to be

00:20:33,609 --> 00:20:40,269
able to imitate the s3 API or the google

00:20:37,929 --> 00:20:41,859
storage API these are all very similar

00:20:40,269 --> 00:20:44,070
aight guys so once you get one you got

00:20:41,859 --> 00:20:46,330
pretty much caught the other ones and

00:20:44,070 --> 00:20:49,690
you want tooling around maintenance

00:20:46,330 --> 00:20:51,129
order to prepare usage record and you

00:20:49,690 --> 00:20:54,309
know which is specific to that Arctic

00:20:51,129 --> 00:20:56,399
storage so the following slides you know

00:20:54,309 --> 00:20:58,989
are talking about a theoretical design

00:20:56,399 --> 00:21:01,149
not much of us have actually been done

00:20:58,989 --> 00:21:03,599
but it's been discussed with the Hadoop

00:21:01,149 --> 00:21:03,599
community

00:21:07,149 --> 00:21:13,059
so what would an ark scalable object

00:21:09,789 --> 00:21:15,219
storage look like you first of all you

00:21:13,059 --> 00:21:17,440
get a bunch of API servers to serve your

00:21:15,219 --> 00:21:22,839
in tens of thousands of requests per

00:21:17,440 --> 00:21:25,239
second you're authorized that API call

00:21:22,839 --> 00:21:27,899
and then you would look you look up that

00:21:25,239 --> 00:21:32,499
object it's the API is talking about and

00:21:27,899 --> 00:21:35,229
find an object or so the object in in in

00:21:32,499 --> 00:21:37,769
a whole bunch of object servers so a

00:21:35,229 --> 00:21:41,559
fairly typical object storage

00:21:37,769 --> 00:21:43,929
architecture you you notice also have a

00:21:41,559 --> 00:21:46,269
set of replicators and auditors which

00:21:43,929 --> 00:21:48,519
make sure that there's at least three

00:21:46,269 --> 00:21:50,700
copies of your object at any given point

00:21:48,519 --> 00:21:50,700
in time

00:21:58,370 --> 00:22:04,730
so the the talk talks about well why you

00:22:01,760 --> 00:22:07,850
know about using Hadoop or HDFS as the

00:22:04,730 --> 00:22:11,000
as the scalable part of this object

00:22:07,850 --> 00:22:13,570
storage and why not I mean it's an ESF

00:22:11,000 --> 00:22:16,460
project just like a patchy cloud stack

00:22:13,570 --> 00:22:18,350
it has immutable objects so it has

00:22:16,460 --> 00:22:20,600
solved a very tough problem for us which

00:22:18,350 --> 00:22:25,309
is how do you store three copies of an

00:22:20,600 --> 00:22:26,690
object in an atomic fashion and make

00:22:25,309 --> 00:22:30,710
sure there's always three copies of that

00:22:26,690 --> 00:22:32,720
object it has an approval reliability

00:22:30,710 --> 00:22:34,880
proven scale proven performance

00:22:32,720 --> 00:22:40,670
everybody knows that you know writes and

00:22:34,880 --> 00:22:42,830
reads are very fast in in in HDFS we

00:22:40,670 --> 00:22:45,080
have data from the field for example

00:22:42,830 --> 00:22:47,420
from facebook that they store in a 200

00:22:45,080 --> 00:22:49,490
million objection one cluster they've

00:22:47,420 --> 00:22:53,510
been able to store 100 petabytes in one

00:22:49,490 --> 00:22:55,900
cluster and that's the kind of scale

00:22:53,510 --> 00:22:59,990
which has not been seen in any other

00:22:55,900 --> 00:23:03,559
object storage system open source out

00:22:59,990 --> 00:23:05,480
there and it's relatively simple to

00:23:03,559 --> 00:23:08,420
operate I mean really if you want to

00:23:05,480 --> 00:23:10,130
scale it up it's no balancing of rings

00:23:08,420 --> 00:23:11,750
and anything is just at a node and

00:23:10,130 --> 00:23:15,080
you're up and running if suddenly have

00:23:11,750 --> 00:23:17,000
you storage to play with right so these

00:23:15,080 --> 00:23:23,809
are some of the reasons why you know we

00:23:17,000 --> 00:23:25,670
looked into HDFS so so replacing the

00:23:23,809 --> 00:23:29,059
HDFS and that in the diagram I showed

00:23:25,670 --> 00:23:30,830
you before you got the s3 API servers

00:23:29,059 --> 00:23:33,860
getting authenticated by some

00:23:30,830 --> 00:23:35,990
authentication servers and then we have

00:23:33,860 --> 00:23:37,610
a name North pair to look up where you

00:23:35,990 --> 00:23:39,620
where to store the object or way to read

00:23:37,610 --> 00:23:43,070
the object firm and then you have the

00:23:39,620 --> 00:23:47,420
HDFS data nodes to to serve up the nodes

00:23:43,070 --> 00:23:50,059
or to store those objects so a fairly

00:23:47,420 --> 00:23:53,780
simple design and then the API servers

00:23:50,059 --> 00:23:55,910
just need to talk the HDFS API to the to

00:23:53,780 --> 00:24:01,520
the data nodes and to the name no so

00:23:55,910 --> 00:24:05,000
again a very simple architecture this in

00:24:01,520 --> 00:24:07,690
fact exists today is just not going to

00:24:05,000 --> 00:24:07,690
test to that scale

00:24:07,850 --> 00:24:14,910
so but so as engineers we love to have

00:24:12,120 --> 00:24:18,750
design challenges and so what's the

00:24:14,910 --> 00:24:22,170
gotchas in the simple architecture the

00:24:18,750 --> 00:24:27,810
first one is named naught scalability we

00:24:22,170 --> 00:24:31,380
know that each block in HDFS takes 150

00:24:27,810 --> 00:24:33,840
by to ram and so if you have hundreds of

00:24:31,380 --> 00:24:36,900
millions of blocks you quickly run into

00:24:33,840 --> 00:24:39,090
very large Ram requirements and then

00:24:36,900 --> 00:24:41,040
because this is java based it's not

00:24:39,090 --> 00:24:43,940
doesn't handle it very well you get GC

00:24:41,040 --> 00:24:46,140
pauses and all kinds of problems and

00:24:43,940 --> 00:24:49,260
some of these problems are at rest I

00:24:46,140 --> 00:24:51,240
mean I'm sure for example there's how do

00:24:49,260 --> 00:24:54,600
clusters are they're running with one

00:24:51,240 --> 00:24:56,670
hundreds of gigabytes of RAM the name

00:24:54,600 --> 00:24:59,550
node is you know widely known to be a

00:24:56,670 --> 00:25:00,900
single point of failure however that's

00:24:59,550 --> 00:25:03,150
being addressed in the community and

00:25:00,900 --> 00:25:06,630
that should soon be solved it has been

00:25:03,150 --> 00:25:13,260
solved but there's a few rough edges

00:25:06,630 --> 00:25:15,330
around on the H a process and then as we

00:25:13,260 --> 00:25:17,460
discuss this I remember I talked about

00:25:15,330 --> 00:25:21,120
this high speed low latency link between

00:25:17,460 --> 00:25:23,460
the data centers what if it's not there

00:25:21,120 --> 00:25:25,830
I mean Hadoop has rack the replacement

00:25:23,460 --> 00:25:28,760
which you could take advantage of to

00:25:25,830 --> 00:25:30,990
place each object in a different zone

00:25:28,760 --> 00:25:32,940
but what if they're a little far apart

00:25:30,990 --> 00:25:36,690
maybe there's a 4 millisecond latency

00:25:32,940 --> 00:25:41,760
maybe it's a you know well HDFS work

00:25:36,690 --> 00:25:44,550
just as well that was not clear and the

00:25:41,760 --> 00:25:46,740
general feedback from the community was

00:25:44,550 --> 00:25:52,040
yeah don't do that because we don't know

00:25:46,740 --> 00:25:56,540
what's going to happen and then finally

00:25:52,040 --> 00:25:59,610
we had mine in SQL before to store the

00:25:56,540 --> 00:26:03,810
mutable data the ackles the policies the

00:25:59,610 --> 00:26:05,670
timer's on the objects where do we store

00:26:03,810 --> 00:26:09,960
that right i mean you need to scale that

00:26:05,670 --> 00:26:12,120
aspect of the system as well so the

00:26:09,960 --> 00:26:15,630
first problem which is named not

00:26:12,120 --> 00:26:18,390
scalability so let's take our goal of a

00:26:15,630 --> 00:26:19,910
billion objects so that translates to

00:26:18,390 --> 00:26:22,680
three copies

00:26:19,910 --> 00:26:25,140
three billion blocks the reason I say

00:26:22,680 --> 00:26:27,570
three million is because I expect we

00:26:25,140 --> 00:26:31,590
expect the average object size to be

00:26:27,570 --> 00:26:33,360
fairly small five to ten MBS so assuming

00:26:31,590 --> 00:26:36,990
an average of five megabytes per object

00:26:33,360 --> 00:26:39,780
you get about 15 petabytes raw not a

00:26:36,990 --> 00:26:41,910
great deal of storage what that turns

00:26:39,780 --> 00:26:46,020
out to require about 450 gigabytes of

00:26:41,910 --> 00:26:50,160
RAM so quite beyond any current system

00:26:46,020 --> 00:26:51,920
currently right and then if you crunch

00:26:50,160 --> 00:26:54,450
number of data nodes that are required

00:26:51,920 --> 00:26:59,460
maybe about a thousand eight uh notes

00:26:54,450 --> 00:27:02,670
assuming 16 terabytes per know so it's

00:26:59,460 --> 00:27:04,110
still not a intractable problem because

00:27:02,670 --> 00:27:06,330
you have this nice thing thought name

00:27:04,110 --> 00:27:08,880
Lord Federation where you can split up

00:27:06,330 --> 00:27:11,250
the namespace into different name nodes

00:27:08,880 --> 00:27:13,350
so that now instead of running one

00:27:11,250 --> 00:27:17,760
signal in order for 50 gigabytes of RAM

00:27:13,350 --> 00:27:19,620
you could run maybe 10 of them with each

00:27:17,760 --> 00:27:23,340
of running with 50 gigabytes of RAM

00:27:19,620 --> 00:27:26,070
right the second approach you could do

00:27:23,340 --> 00:27:28,830
is Hadoop has a system called hard files

00:27:26,070 --> 00:27:31,200
where you can store many fouls together

00:27:28,830 --> 00:27:35,880
as one file and then you can still

00:27:31,200 --> 00:27:37,230
retrieve one file inside that archive so

00:27:35,880 --> 00:27:39,500
that's a different approach but again

00:27:37,230 --> 00:27:44,820
that requires a layer of management

00:27:39,500 --> 00:27:48,059
outside of HDX so name node Federation

00:27:44,820 --> 00:27:50,160
looks like this you have everybody

00:27:48,059 --> 00:27:53,250
manage each name mode manages the same

00:27:50,160 --> 00:27:55,380
set of data node thousand data nodes we

00:27:53,250 --> 00:27:58,830
are talking about but then they'd the

00:27:55,380 --> 00:28:02,520
namespace in our case the the bucket

00:27:58,830 --> 00:28:06,870
names will be partitioned between each

00:28:02,520 --> 00:28:09,840
of these name node so if you had a

00:28:06,870 --> 00:28:12,750
billion objects and ten name nodes then

00:28:09,840 --> 00:28:17,870
each name node would have a hundred

00:28:12,750 --> 00:28:21,360
million object to to take care of a

00:28:17,870 --> 00:28:23,730
tractable problem but then I don't think

00:28:21,360 --> 00:28:25,890
anybody's tested name more Federation

00:28:23,730 --> 00:28:28,580
along with HH so that's a new problem to

00:28:25,890 --> 00:28:28,580
be solved right there

00:28:31,230 --> 00:28:35,679
yeah and that the second problem is that

00:28:33,549 --> 00:28:39,210
now that you're decided to charge your

00:28:35,679 --> 00:28:42,130
namespace amongst these 10 named nodes

00:28:39,210 --> 00:28:45,160
how do you decide how to chart right so

00:28:42,130 --> 00:28:48,760
so you either need a consistent hashing

00:28:45,160 --> 00:28:51,520
skiing or you need another database to

00:28:48,760 --> 00:28:54,010
store this mapping between your object

00:28:51,520 --> 00:28:56,289
name and the particular name node that

00:28:54,010 --> 00:28:59,679
is being sold by that object name right

00:28:56,289 --> 00:29:02,590
and so you require another scalable key

00:28:59,679 --> 00:29:04,480
value store and that's not a problem we

00:29:02,590 --> 00:29:10,090
just use HBase for that because you know

00:29:04,480 --> 00:29:14,320
that scales to infinitely right and then

00:29:10,090 --> 00:29:15,610
the last problem is well as you run the

00:29:14,320 --> 00:29:17,380
system for a while you're going to get

00:29:15,610 --> 00:29:20,289
different name nodes with different sets

00:29:17,380 --> 00:29:21,669
of names right and so they got to be

00:29:20,289 --> 00:29:23,380
unbalanced and so you'll have to run

00:29:21,669 --> 00:29:25,600
some kind of background process to

00:29:23,380 --> 00:29:28,510
rebalance these 10 named nodes amongst

00:29:25,600 --> 00:29:30,840
each other and similarly when you add a

00:29:28,510 --> 00:29:33,309
new name note the scale of the system

00:29:30,840 --> 00:29:37,620
you're going to have to you know move

00:29:33,309 --> 00:29:37,620
some of the old load into the new name

00:29:39,240 --> 00:29:44,110
the second thorny problem here is and

00:29:42,370 --> 00:29:46,210
it's a really tiny problem because you

00:29:44,110 --> 00:29:50,880
want to do a replication over slower and

00:29:46,210 --> 00:29:56,169
lossy links and this actually was the

00:29:50,880 --> 00:30:01,000
Achilles heel which stop the project

00:29:56,169 --> 00:30:02,350
from going beyond its design stage so

00:30:01,000 --> 00:30:05,470
first thing you can do is asynchronous

00:30:02,350 --> 00:30:07,120
replication how do pious very neat till

00:30:05,470 --> 00:30:09,789
tool called this CP where you can

00:30:07,120 --> 00:30:11,289
replicate between clusters of course

00:30:09,789 --> 00:30:15,250
this now means that you have six copies

00:30:11,289 --> 00:30:16,270
instead of three copies and then you

00:30:15,250 --> 00:30:17,950
need to maintain some kind of

00:30:16,270 --> 00:30:19,600
master-slave relationship between the

00:30:17,950 --> 00:30:22,059
primary cluster and the end slave

00:30:19,600 --> 00:30:23,710
cluster and so you got the possibility

00:30:22,059 --> 00:30:25,929
of you know data loss when you do the

00:30:23,710 --> 00:30:27,520
failover and you need some kind of

00:30:25,929 --> 00:30:28,990
additional coordination logic to say

00:30:27,520 --> 00:30:33,280
well who's the master who's the slave

00:30:28,990 --> 00:30:35,289
and so on and so forth or you could say

00:30:33,280 --> 00:30:37,929
it's synchronous replication so the s3

00:30:35,289 --> 00:30:39,880
API server can say that aha I've got two

00:30:37,929 --> 00:30:42,310
clusters and until i right to both

00:30:39,880 --> 00:30:45,130
clusters I'm not going to write

00:30:42,310 --> 00:30:48,850
return and I cause went to be to the end

00:30:45,130 --> 00:30:50,440
user well what happens when your zone

00:30:48,850 --> 00:30:54,340
disappeared right the new then you're

00:30:50,440 --> 00:30:56,440
effectively dead in the water and then

00:30:54,340 --> 00:31:04,690
your last year elliptic availability

00:30:56,440 --> 00:31:07,240
goal by having two zones and this is not

00:31:04,690 --> 00:31:11,530
something new we just rediscovering the

00:31:07,240 --> 00:31:14,140
capture the cap theorem for just as a

00:31:11,530 --> 00:31:17,620
refresher it says that in a distributed

00:31:14,140 --> 00:31:19,360
system you can either choose consistency

00:31:17,620 --> 00:31:21,280
or you can choose availability when

00:31:19,360 --> 00:31:24,790
during a network partition you cannot

00:31:21,280 --> 00:31:28,090
have both right and of course there's

00:31:24,790 --> 00:31:29,500
many nuances on this but essentially

00:31:28,090 --> 00:31:32,980
this is the problem we were running it

00:31:29,500 --> 00:31:38,590
and so perhaps it was the wrong problem

00:31:32,980 --> 00:31:40,300
to be trying to solve so what so if we

00:31:38,590 --> 00:31:42,190
did the the synchronous approach we

00:31:40,300 --> 00:31:44,650
would get consistency but not

00:31:42,190 --> 00:31:46,270
availability if we did the disk CP

00:31:44,650 --> 00:31:53,380
approach we will get availability but

00:31:46,270 --> 00:31:56,190
not consistency the last problem is well

00:31:53,380 --> 00:31:59,920
where do we store our object meditator

00:31:56,190 --> 00:32:01,800
so one option is we stored with in HDFS

00:31:59,920 --> 00:32:05,200
along with the bucket and along with the

00:32:01,800 --> 00:32:06,910
object but then breeds tend to get a

00:32:05,200 --> 00:32:10,450
little expensive so if you want to check

00:32:06,910 --> 00:32:12,160
the akal before reading the object now

00:32:10,450 --> 00:32:15,010
you've got to read the object find a CO

00:32:12,160 --> 00:32:17,890
and then say well now so you might have

00:32:15,010 --> 00:32:19,660
read a 10 gigabyte object check the akal

00:32:17,890 --> 00:32:21,460
and then said no you can't access it so

00:32:19,660 --> 00:32:26,020
that that tends to be kind of wasteful

00:32:21,460 --> 00:32:27,790
and then the second problem is it's

00:32:26,020 --> 00:32:29,860
mutable data because people can change

00:32:27,790 --> 00:32:32,080
the rackets it's not like in the SGA

00:32:29,860 --> 00:32:35,170
it's not like an object which you would

00:32:32,080 --> 00:32:37,930
have to delete or to delete or to append

00:32:35,170 --> 00:32:40,930
only so you need some kind of layer on

00:32:37,930 --> 00:32:43,930
top of HDFS to make it mutable and it's

00:32:40,930 --> 00:32:47,130
not a new problem HBase does it all the

00:32:43,930 --> 00:32:49,630
time it has mutable data on top of HDFS

00:32:47,130 --> 00:32:53,080
but nonetheless it's a new piece of

00:32:49,630 --> 00:32:53,850
doubt mean that we need to do which HDFS

00:32:53,080 --> 00:33:01,770
does not

00:32:53,850 --> 00:33:05,870
out of the box so the second option is

00:33:01,770 --> 00:33:08,520
we use another storage system like HBase

00:33:05,870 --> 00:33:11,190
and as we saw before if you wanted to do

00:33:08,520 --> 00:33:12,690
something like name node Federation you

00:33:11,190 --> 00:33:15,450
would have to use HBase there or

00:33:12,690 --> 00:33:20,340
something like HBase over there anyway

00:33:15,450 --> 00:33:21,570
but then the problem using HBase or any

00:33:20,340 --> 00:33:23,480
other thing is that it's yet another

00:33:21,570 --> 00:33:25,860
match system you got to manage right

00:33:23,480 --> 00:33:27,870
HBase is like it's got the region

00:33:25,860 --> 00:33:29,789
servers you got the zookeeper you got a

00:33:27,870 --> 00:33:32,760
whole bunch of new infrastructure you

00:33:29,789 --> 00:33:36,809
got to manage to keep up and to enter

00:33:32,760 --> 00:33:38,669
scale so are suddenly our simple

00:33:36,809 --> 00:33:44,220
architecture settings becoming very very

00:33:38,669 --> 00:33:46,710
complex or we could store modify the

00:33:44,220 --> 00:33:48,450
name now to store the metadata and this

00:33:46,710 --> 00:33:51,690
was something the Hadoop community was

00:33:48,450 --> 00:33:54,240
quite open to doing it would be very

00:33:51,690 --> 00:33:57,000
high performance but it's not very

00:33:54,240 --> 00:34:01,080
extensible so tomorrow amazon adds the

00:33:57,000 --> 00:34:04,289
third kind of mutable thing into the in

00:34:01,080 --> 00:34:06,900
an object what do you do now I mean you

00:34:04,289 --> 00:34:08,909
can't go it's a very critical part of

00:34:06,900 --> 00:34:11,700
HDFS it's not something to be modified

00:34:08,909 --> 00:34:14,250
easily it's not something we can just

00:34:11,700 --> 00:34:17,429
say I just a plug-in or something it's

00:34:14,250 --> 00:34:19,710
got to be done with great care and and

00:34:17,429 --> 00:34:23,780
it's probably not a good idea to touch

00:34:19,710 --> 00:34:23,780
the main part of it is here

00:34:29,550 --> 00:34:40,830
so what is the future for object store

00:34:32,550 --> 00:34:42,540
in HDFS if we backtrack to the problems

00:34:40,830 --> 00:34:46,290
which I discuss I think I started with a

00:34:42,540 --> 00:34:48,060
billion object now if a billion objects

00:34:46,290 --> 00:34:49,320
is not in your future if you don't think

00:34:48,060 --> 00:34:53,429
ever that you're going to get to a

00:34:49,320 --> 00:34:56,580
billion objects then a lot of these

00:34:53,429 --> 00:34:58,950
problems will apply to you so maybe you

00:34:56,580 --> 00:35:01,230
just need it for 102 or 200 million

00:34:58,950 --> 00:35:03,150
objects and we know that works in a

00:35:01,230 --> 00:35:07,050
Facebook has deployed this they do it

00:35:03,150 --> 00:35:08,820
all the time and if you are data centers

00:35:07,050 --> 00:35:11,760
are close to each other or adjacent

00:35:08,820 --> 00:35:13,140
rooms with the far wall in between so

00:35:11,760 --> 00:35:16,160
that they're not going to catch fire at

00:35:13,140 --> 00:35:18,420
the same time it's a viable solution and

00:35:16,160 --> 00:35:21,060
you could still store your mask you'll

00:35:18,420 --> 00:35:23,700
they are mutable object in my skill my

00:35:21,060 --> 00:35:26,430
skill will scale to 100 million objects

00:35:23,700 --> 00:35:28,800
no problem so it's a viable solution for

00:35:26,430 --> 00:35:31,200
that kind of point the only question we

00:35:28,800 --> 00:35:33,690
have to ask ourselves that well what

00:35:31,200 --> 00:35:35,160
about scaling to a billion what about

00:35:33,690 --> 00:35:38,400
scaling to a 10 billion what about

00:35:35,160 --> 00:35:41,040
scaling to 100 billion object so a

00:35:38,400 --> 00:35:43,260
larger deployments needs development it

00:35:41,040 --> 00:35:45,540
needs solutions for the consistency in

00:35:43,260 --> 00:35:48,900
availability portion it needs solution

00:35:45,540 --> 00:35:54,680
for a scalable mutable data store for

00:35:48,900 --> 00:35:54,680
ackles and other stuff it it needs

00:35:55,730 --> 00:36:01,110
significant effort to manage the

00:35:58,170 --> 00:36:03,900
charting between the name nodes and to

00:36:01,110 --> 00:36:06,090
manage the h.a of the name nodes and so

00:36:03,900 --> 00:36:09,260
at this point in time there is no effort

00:36:06,090 --> 00:36:12,240
on going to solve this problem and

00:36:09,260 --> 00:36:14,880
something that I think the Clarks our

00:36:12,240 --> 00:36:18,920
community would welcome collaboration

00:36:14,880 --> 00:36:21,630
with tashi Hadoop community I think that

00:36:18,920 --> 00:36:23,870
HDFS is unparalleled in its simplicity

00:36:21,630 --> 00:36:27,290
in its reliability in its performance

00:36:23,870 --> 00:36:28,970
there's quite nothing like it and

00:36:27,290 --> 00:36:32,030
I think the two communities

00:36:28,970 --> 00:36:35,480
collaborating together could really put

00:36:32,030 --> 00:36:39,110
out a really great product which would

00:36:35,480 --> 00:36:41,210
be which would make that patchy name

00:36:39,110 --> 00:36:50,420
proud because Apache is all of our

00:36:41,210 --> 00:36:54,260
infrastructure in to conclude Clarke's

00:36:50,420 --> 00:36:58,180
like to make cloud stock workloads more

00:36:54,260 --> 00:37:03,470
reliable it needs object storage and

00:36:58,180 --> 00:37:06,020
it's it from 40 to 41 it acquired that

00:37:03,470 --> 00:37:11,510
with the integrations with react CS and

00:37:06,020 --> 00:37:14,540
and Swift however object store is not

00:37:11,510 --> 00:37:19,010
easy and depending on how much you want

00:37:14,540 --> 00:37:21,140
to scale it to HDFS it comes close it

00:37:19,010 --> 00:37:23,510
comes very close to solving the problem

00:37:21,140 --> 00:37:27,350
or it's not close enough for a

00:37:23,510 --> 00:37:29,270
general-purpose object storage and we

00:37:27,350 --> 00:37:31,670
would welcome any collaboration between

00:37:29,270 --> 00:37:36,460
the Hadoop and the couch the communities

00:37:31,670 --> 00:37:36,460
to solve this problem sure

00:37:39,470 --> 00:37:43,670
we do not look at glass surface now

00:37:55,900 --> 00:38:02,330
I think some of those claims could be

00:37:59,890 --> 00:38:07,700
questionable about multi data center

00:38:02,330 --> 00:38:11,200
especially and I think by trying to

00:38:07,700 --> 00:38:13,940
layer on a POSIX semantics I think it's

00:38:11,200 --> 00:38:17,240
I think it fundamentally makes the

00:38:13,940 --> 00:38:20,720
system less reliable but this was an

00:38:17,240 --> 00:38:23,750
effort to incubate or start a project in

00:38:20,720 --> 00:38:27,800
the apache software foundation with an

00:38:23,750 --> 00:38:35,450
Apache License not not necessarily so

00:38:27,800 --> 00:38:37,100
it's not just that but I mean we could

00:38:35,450 --> 00:38:39,170
discuss the problems with Russia sure

00:38:37,100 --> 00:38:41,620
there's marketing claims in this real

00:38:39,170 --> 00:38:41,620
reality

00:38:46,160 --> 00:38:48,220
you

00:39:00,150 --> 00:39:04,279
sure you do that sweet

00:39:05,869 --> 00:39:16,290
sure and and if you look at the diagram

00:39:09,750 --> 00:39:20,010
I drew here you know it's very similar

00:39:16,290 --> 00:39:21,690
to a social system where this is

00:39:20,010 --> 00:39:25,109
probably Keystone and this is probably

00:39:21,690 --> 00:39:29,850
the nikon container account servers and

00:39:25,109 --> 00:39:33,359
these are there I think it what they

00:39:29,850 --> 00:39:39,300
call an object servers I think it's very

00:39:33,359 --> 00:39:41,520
similar the again we wanted a project

00:39:39,300 --> 00:39:44,100
which would be congruent with Apache

00:39:41,520 --> 00:39:47,340
CloudStack you know perhaps use the

00:39:44,100 --> 00:39:49,320
Apache Hadoop and as I said before the

00:39:47,340 --> 00:39:52,020
performance these are the reliability

00:39:49,320 --> 00:39:56,550
and the maturity of tribes of Hadoop and

00:39:52,020 --> 00:39:59,460
HDFS is quite I mean it's a great deal

00:39:56,550 --> 00:40:02,160
of thought has gone into how to do it

00:39:59,460 --> 00:40:05,550
you know the fact that he can run

00:40:02,160 --> 00:40:09,030
immutable database like HBase on top of

00:40:05,550 --> 00:40:12,410
it it just means that there's additional

00:40:09,030 --> 00:40:12,410
things you can do with it

00:40:17,190 --> 00:40:21,720
anybody from the Hadoop community

00:40:36,120 --> 00:40:50,460
why curiosity why inject fat has

00:40:41,880 --> 00:40:53,360
effectively the s3 proxy yeah so the API

00:40:50,460 --> 00:40:57,360
servers i show you are not necessarily

00:40:53,360 --> 00:41:00,180
CloudStack API servers today for you

00:40:57,360 --> 00:41:02,610
deploy s3 API in CloudStack these run as

00:41:00,180 --> 00:41:04,640
a separate web app and so there's no

00:41:02,610 --> 00:41:08,340
actually no shared state between

00:41:04,640 --> 00:41:11,390
CloudStack and s3 API except for the

00:41:08,340 --> 00:41:11,390
authentication credentials

00:41:19,240 --> 00:41:22,080
it seems

00:41:22,960 --> 00:41:33,570
especially we Hawk we're making that as

00:41:31,359 --> 00:41:33,570
well

00:41:36,230 --> 00:41:38,890
share

00:41:40,610 --> 00:41:56,790
yeah sure I mean there's there's

00:41:54,690 --> 00:41:59,250
different ways to scale that either you

00:41:56,790 --> 00:42:03,240
can put a service layer on top of the AP

00:41:59,250 --> 00:42:06,990
of the credentials or you could do some

00:42:03,240 --> 00:42:10,590
kind of a lazy cache of you know this is

00:42:06,990 --> 00:42:13,080
performance Prix ikut are you know or

00:42:10,590 --> 00:42:16,490
architectural changes we can make the

00:42:13,080 --> 00:42:16,490
comment that definitely

00:42:36,690 --> 00:42:56,830
require solving yes yea I'm guessing you

00:42:52,000 --> 00:43:00,120
know typically when I when I'm Mike HDFS

00:42:56,830 --> 00:43:02,590
guru but what I've read and what I've

00:43:00,120 --> 00:43:04,480
seen on the mailing list except I let

00:43:02,590 --> 00:43:06,040
you know people get worried when there

00:43:04,480 --> 00:43:16,680
is more than four or five milliseconds

00:43:06,040 --> 00:43:16,680
of latency right yeah sure

00:43:23,729 --> 00:43:29,400
yeah I would say that it's probably best

00:43:27,160 --> 00:43:31,569
to get so by its own infrastructure

00:43:29,400 --> 00:43:38,589
simply because you can optimize the

00:43:31,569 --> 00:43:40,299
hardware measure when you deploy what I

00:43:38,589 --> 00:43:46,959
could see sharing the same

00:43:40,299 --> 00:43:48,969
infrastructure is a MapReduce compute on

00:43:46,959 --> 00:43:52,559
top of the same infrastructure but I

00:43:48,969 --> 00:43:52,559
wouldn't run VMs on that infrastructure

00:44:05,269 --> 00:44:09,719
you could do that I think and raised

00:44:07,529 --> 00:44:12,900
question was more about your running VMS

00:44:09,719 --> 00:44:15,599
as well as using the disk which is on

00:44:12,900 --> 00:44:19,499
that hypervisor to store your objects

00:44:15,599 --> 00:44:21,089
but yeah suddenly you could leverage so

00:44:19,499 --> 00:44:23,729
i talked of a great deal about the

00:44:21,089 --> 00:44:26,069
management complexity which which we

00:44:23,729 --> 00:44:27,869
introduced by tryna scale up like the

00:44:26,069 --> 00:44:31,950
spinning up additional named nerds

00:44:27,869 --> 00:44:35,279
maintaining ha4 the name nodes you know

00:44:31,950 --> 00:44:37,589
managing HBase cluster suddenly all that

00:44:35,279 --> 00:44:39,900
can be automated with CloudStack and so

00:44:37,589 --> 00:44:41,489
it so these problems not intractable but

00:44:39,900 --> 00:44:44,160
it does require additional develop and

00:44:41,489 --> 00:44:46,789
on CloudStack as well as on on

00:44:44,160 --> 00:44:46,789
achievements

00:44:58,170 --> 00:45:09,190
definitely a yeah so yeah so VMs exact

00:45:06,850 --> 00:45:13,060
for that one purpose yeah I could see

00:45:09,190 --> 00:45:16,510
that yeah that would be a big advantage

00:45:13,060 --> 00:45:23,640
yes that money we could probably speed

00:45:16,510 --> 00:45:23,640
up your computations a great deal sure

00:45:34,690 --> 00:45:42,609
so object storage is typically a

00:45:37,710 --> 00:45:45,119
http-based API so you can use HTTP verbs

00:45:42,609 --> 00:45:49,750
like put and get and delete and then

00:45:45,119 --> 00:45:51,700
that translates into whatever the

00:45:49,750 --> 00:45:55,240
backing follow the backing storage

00:45:51,700 --> 00:45:57,520
system it can do what over that request

00:45:55,240 --> 00:46:00,880
means for example if it was a POSIX file

00:45:57,520 --> 00:46:02,349
system if you put an object it could you

00:46:00,880 --> 00:46:05,500
know create the object on the file

00:46:02,349 --> 00:46:07,839
system similarly when you put an object

00:46:05,500 --> 00:46:11,440
in HDFS of the backing pulses so then it

00:46:07,839 --> 00:46:13,869
creates a block so it's a mapping

00:46:11,440 --> 00:46:16,079
between the HTTP layer to the back in

00:46:13,869 --> 00:46:16,079
the area

00:46:32,770 --> 00:46:35,670
right yeah

00:46:39,390 --> 00:46:43,769
yeah hp's doesn't have that many small

00:46:41,880 --> 00:46:48,269
files because what it does is it keeps

00:46:43,769 --> 00:46:50,010
appending to the same file so so you

00:46:48,269 --> 00:46:52,710
never run into the small-time problem

00:46:50,010 --> 00:46:54,630
with h pace and then and then they do

00:46:52,710 --> 00:46:56,789
periodic compaction is to make sure that

00:46:54,630 --> 00:47:00,650
if they have a lot of small files a

00:46:56,789 --> 00:47:12,210
compact them into into into larger files

00:47:00,650 --> 00:47:15,269
they are they that's right yeah you

00:47:12,210 --> 00:47:17,039
could but it's naar de it's not designed

00:47:15,269 --> 00:47:20,390
for that kind of system it's meant for a

00:47:17,039 --> 00:47:24,720
key value store it stores a Lardon RAM

00:47:20,390 --> 00:47:31,260
and so you will not see any performance

00:47:24,720 --> 00:47:36,079
gains necessarily and and you know and

00:47:31,260 --> 00:47:38,430
then it's possible and that's another

00:47:36,079 --> 00:47:46,190
design dimension to be high explore

00:47:38,430 --> 00:47:46,190
different that's right again

00:47:46,960 --> 00:47:53,180
yes and and and we know that there's

00:47:50,390 --> 00:47:57,109
other people who try to do object

00:47:53,180 --> 00:47:58,820
storage on Cassandra for instance and in

00:47:57,109 --> 00:48:01,869
that particular implementation they ran

00:47:58,820 --> 00:48:03,740
into performance problems because the

00:48:01,869 --> 00:48:06,050
cassandra is architected for a

00:48:03,740 --> 00:48:13,700
particular use case it just did not

00:48:06,050 --> 00:48:15,730
handle that kind of useless okay thank

00:48:13,700 --> 00:48:15,730

YouTube URL: https://www.youtube.com/watch?v=ILWLBgkkVrM


