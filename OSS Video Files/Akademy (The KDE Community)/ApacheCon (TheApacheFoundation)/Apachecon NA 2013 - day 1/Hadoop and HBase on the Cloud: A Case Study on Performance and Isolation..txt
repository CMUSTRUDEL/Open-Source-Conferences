Title: Hadoop and HBase on the Cloud: A Case Study on Performance and Isolation.
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Konstantin Shvachko, Jagane Sundar ApacheCon NA 2013
Cloud Crowd
Captions: 
	00:00:00,000 --> 00:00:09,630
so my name is constantly in rajkot I'm

00:00:05,730 --> 00:00:14,820
presenting today man a case study one

00:00:09,630 --> 00:00:19,500
very popular problem that was a bird in

00:00:14,820 --> 00:00:25,350
Hadoop from variation in inch x and

00:00:19,500 --> 00:00:33,680
presenting with the co-author chickens

00:00:25,350 --> 00:00:36,750
entire so we both began and and me and I

00:00:33,680 --> 00:00:40,110
were the founders of alte store and down

00:00:36,750 --> 00:00:43,680
to scale the startups were recently

00:00:40,110 --> 00:00:48,770
joined one disk and no chicken is CEO

00:00:43,680 --> 00:00:48,770
and vp of engineering there and I'm a

00:00:49,100 --> 00:00:58,410
chip architect at one disk so jiggin was

00:00:53,780 --> 00:01:02,100
working with Hadoop at Yahoo working on

00:00:58,410 --> 00:01:04,229
performance and operability of Hadoop he

00:01:02,100 --> 00:01:09,720
has a great deal of experience in Big

00:01:04,229 --> 00:01:15,299
Data cloud virtualization network

00:01:09,720 --> 00:01:17,420
experience I I'm interested in data

00:01:15,299 --> 00:01:24,560
structures and algorithms for

00:01:17,420 --> 00:01:28,350
distributed large storage systems and I

00:01:24,560 --> 00:01:32,159
started this new open source project

00:01:28,350 --> 00:01:36,600
which is a filesystem refer and file

00:01:32,159 --> 00:01:40,770
system it's a file system that has both

00:01:36,600 --> 00:01:44,490
distributed metadata and data and it is

00:01:40,770 --> 00:01:49,399
hosted on a budget extra I've done a lot

00:01:44,490 --> 00:01:49,399
of things in HDFS so let's get started

00:01:49,970 --> 00:01:56,220
what's a budget do so I used to ask the

00:01:54,299 --> 00:01:59,520
question who knows about

00:01:56,220 --> 00:02:04,170
do now I really did who doesn't know

00:01:59,520 --> 00:02:06,540
about a budget that's good so I'll be

00:02:04,170 --> 00:02:08,850
quick and brief on that I'll just

00:02:06,540 --> 00:02:16,860
introduce the terminology because we

00:02:08,850 --> 00:02:20,760
will use it a lot later on so the main

00:02:16,860 --> 00:02:23,880
thing is that the lower layer layer of

00:02:20,760 --> 00:02:26,670
Hadoop is distributed file system it's a

00:02:23,880 --> 00:02:31,490
reliable distributed storage layer and

00:02:26,670 --> 00:02:34,680
name node is the metadata server which

00:02:31,490 --> 00:02:36,510
serves which stores all the namespace

00:02:34,680 --> 00:02:39,209
information and provides block

00:02:36,510 --> 00:02:42,950
management for the file system and the

00:02:39,209 --> 00:02:45,060
data is replicated on data node

00:02:42,950 --> 00:02:48,440
different data nodes have multiple

00:02:45,060 --> 00:02:51,980
replicas of blocks MapReduce is

00:02:48,440 --> 00:02:55,920
distributed computational framework and

00:02:51,980 --> 00:03:01,200
it's based on a simple computational

00:02:55,920 --> 00:03:07,530
model invented by Google the job tracker

00:03:01,200 --> 00:03:10,230
is an alcove metadata service it's a it

00:03:07,530 --> 00:03:13,800
plays the central role on the MapReduce

00:03:10,230 --> 00:03:18,390
cluster task truckers are the slaves of

00:03:13,800 --> 00:03:23,450
the MapReduce they directly run the

00:03:18,390 --> 00:03:23,450
tasks of pod and execute the jobs

00:03:26,520 --> 00:03:36,990
so a little bit about HBase HBase is a

00:03:34,280 --> 00:03:41,930
key value storage system it's

00:03:36,990 --> 00:03:46,980
distributed storage system it is a table

00:03:41,930 --> 00:03:50,100
table based so it stores tables but the

00:03:46,980 --> 00:03:57,810
tables are very loosely structured don't

00:03:50,100 --> 00:04:04,670
have a strict strict schema Rose can

00:03:57,810 --> 00:04:08,190
have arbitrary number of columns the

00:04:04,670 --> 00:04:11,850
distributed nature of HBase is that it

00:04:08,190 --> 00:04:16,020
is it can display it into into

00:04:11,850 --> 00:04:19,770
horizontally into regions and also it

00:04:16,020 --> 00:04:21,299
can be split vertically if there is too

00:04:19,770 --> 00:04:25,200
many columns then you have to split

00:04:21,299 --> 00:04:28,380
vertically and those boundaries on which

00:04:25,200 --> 00:04:32,040
is bleach-based vertically called Colin

00:04:28,380 --> 00:04:35,420
founders HBase can be considered as a

00:04:32,040 --> 00:04:39,470
distributed cache because it serves its

00:04:35,420 --> 00:04:44,400
caches regions the pieces of the table

00:04:39,470 --> 00:04:47,310
in ROM and then serves it quickly so

00:04:44,400 --> 00:04:50,180
that applications and consumers of data

00:04:47,310 --> 00:04:57,480
from each base could do real-time

00:04:50,180 --> 00:05:00,210
processing of that data so I'm sure

00:04:57,480 --> 00:05:07,320
everybody knows what MapReduce it is but

00:05:00,210 --> 00:05:09,780
I need to emphasize some some important

00:05:07,320 --> 00:05:13,410
points for for this particular

00:05:09,780 --> 00:05:15,390
presentation so the main the main parts

00:05:13,410 --> 00:05:20,520
of MapReduce framework

00:05:15,390 --> 00:05:23,010
as a computational model is mopping GG

00:05:20,520 --> 00:05:27,120
use basically that's that's the only

00:05:23,010 --> 00:05:29,940
things that the user should define along

00:05:27,120 --> 00:05:34,790
along with location of input and output

00:05:29,940 --> 00:05:38,100
data the rest is taken care by the

00:05:34,790 --> 00:05:43,400
framework itself so the frameworks

00:05:38,100 --> 00:05:46,920
plates the input data into into into

00:05:43,400 --> 00:05:51,120
parts and feeds them into maps maps

00:05:46,920 --> 00:05:54,710
produce transform that data produce some

00:05:51,120 --> 00:05:58,680
results right and locally on the drives

00:05:54,710 --> 00:06:02,400
then there is a framework initiated

00:05:58,680 --> 00:06:05,280
shuffle phase which delivers the local

00:06:02,400 --> 00:06:08,190
results produced by maps to the reducers

00:06:05,280 --> 00:06:12,000
responsible for the particular key type

00:06:08,190 --> 00:06:15,750
and then reduced processes this reduces

00:06:12,000 --> 00:06:20,250
process the data and output usually

00:06:15,750 --> 00:06:27,090
again in distributed file system they

00:06:20,250 --> 00:06:29,970
output in in parallel so by the way this

00:06:27,090 --> 00:06:36,030
example accounts consonants and vowels

00:06:29,970 --> 00:06:39,240
in in the words so what's the problem

00:06:36,030 --> 00:06:41,570
many of you would would say that well

00:06:39,240 --> 00:06:45,450
with Hadoop there is lots of problems

00:06:41,570 --> 00:06:48,920
well I'm going to address just one that

00:06:45,450 --> 00:06:52,770
was standing for a very long time

00:06:48,920 --> 00:06:56,880
basically from Hadoop conception it's

00:06:52,770 --> 00:07:01,590
low CPU utilization on on the Hadoop

00:06:56,880 --> 00:07:03,840
clusters utilization yo utilization for

00:07:01,590 --> 00:07:08,660
example is not bad you can write a

00:07:03,840 --> 00:07:12,690
simple MapReduce job you can use test

00:07:08,660 --> 00:07:17,340
dfsa yo or very well to you pterosaurs

00:07:12,690 --> 00:07:19,860
and you will get high Yo utilization on

00:07:17,340 --> 00:07:24,960
the cluster network network utilization

00:07:19,860 --> 00:07:26,700
will nobody complained about network

00:07:24,960 --> 00:07:29,660
utilization

00:07:26,700 --> 00:07:33,510
in terms of predictor of course

00:07:29,660 --> 00:07:36,180
apparently problems in cloudstack with

00:07:33,510 --> 00:07:38,700
network please 10 of them but with

00:07:36,180 --> 00:07:43,680
Hadoop it's it it's designed to optimize

00:07:38,700 --> 00:07:48,120
on on the network so first of all the

00:07:43,680 --> 00:07:53,100
data is I mean sorry but the execution

00:07:48,120 --> 00:07:56,610
of tasks is ship is shipped locally to

00:07:53,100 --> 00:07:59,160
the data so there is no data transfer so

00:07:56,610 --> 00:08:06,300
there is no extensive transfer of input

00:07:59,160 --> 00:08:09,720
data block replication of the requires

00:08:06,300 --> 00:08:12,960
network and this is one one one of the

00:08:09,720 --> 00:08:15,930
things that require network traffic it

00:08:12,960 --> 00:08:18,420
requires to network transfers because

00:08:15,930 --> 00:08:23,270
the first copy goes local to the same

00:08:18,420 --> 00:08:29,790
node and two are into two different note

00:08:23,270 --> 00:08:33,060
ma prides transient output into local

00:08:29,790 --> 00:08:36,479
drive so there's no traffic there the

00:08:33,060 --> 00:08:39,390
shuffle is probably the biggest part of

00:08:36,479 --> 00:08:41,570
the MapReduce framework that produces a

00:08:39,390 --> 00:08:44,010
lot of network traffic traffic because

00:08:41,570 --> 00:08:46,320
reducers need to get information from

00:08:44,010 --> 00:08:48,300
all the mappers that bran I mean

00:08:46,320 --> 00:08:50,790
potentially all the mappers that rent

00:08:48,300 --> 00:08:56,670
and get those parts that belong to this

00:08:50,790 --> 00:08:58,440
reducer and transfer that to a single

00:08:56,670 --> 00:09:01,590
node so there is a lot of cross no

00:08:58,440 --> 00:09:04,620
traffic on the shuffle faith but the

00:09:01,590 --> 00:09:07,880
real problem is CPU utilization it

00:09:04,620 --> 00:09:13,500
usually low and what we think is that

00:09:07,880 --> 00:09:16,500
are the main the main reasons for low

00:09:13,500 --> 00:09:21,780
utilization is first of all are you

00:09:16,500 --> 00:09:27,330
bound workloads MapReduce and Hadoop in

00:09:21,780 --> 00:09:30,510
general is a big data problem framework

00:09:27,330 --> 00:09:33,270
for solving big data problems and big

00:09:30,510 --> 00:09:34,890
data means a lot of a lot of data right

00:09:33,270 --> 00:09:36,080
so you need to read it in order to

00:09:34,890 --> 00:09:39,200
process so that

00:09:36,080 --> 00:09:43,030
yio bombed it were closed on the other

00:09:39,200 --> 00:09:46,310
hand there is a cluster provisioning

00:09:43,030 --> 00:09:49,430
trade-off I'll talk about it a little

00:09:46,310 --> 00:09:52,370
bit later what it means that there is a

00:09:49,430 --> 00:09:55,040
trade-off between peak peak load

00:09:52,370 --> 00:09:58,430
performance you want to keep your

00:09:55,040 --> 00:10:02,390
cluster running fast when the peak load

00:09:58,430 --> 00:10:04,220
heads on the other hand that lowers

00:10:02,390 --> 00:10:09,400
therefore its utilization when there is

00:10:04,220 --> 00:10:09,400
no substantial traffic on the classroom

00:10:10,300 --> 00:10:18,380
so have we ever seen honey is it is

00:10:14,530 --> 00:10:20,540
everything so bad and when ever seen cpu

00:10:18,380 --> 00:10:25,820
intensive utilization of course we did

00:10:20,540 --> 00:10:29,810
so computation of Pi has virtually zero

00:10:25,820 --> 00:10:32,510
input and very little output data it's a

00:10:29,810 --> 00:10:35,690
lot of computation its enormous amount

00:10:32,510 --> 00:10:39,470
of fuss free transforms that compute

00:10:35,690 --> 00:10:42,850
amazingly large large numbers and in the

00:10:39,470 --> 00:10:49,490
end give you give you one digit after

00:10:42,850 --> 00:10:54,550
months of computation so we have seen

00:10:49,490 --> 00:10:57,910
that when nicholas started run of his

00:10:54,550 --> 00:11:03,340
record by computation they literally

00:10:57,910 --> 00:11:07,130
heated up the data center a cluster and

00:11:03,340 --> 00:11:10,970
it reached them critical levels at that

00:11:07,130 --> 00:11:17,050
point so it was very interesting but in

00:11:10,970 --> 00:11:17,050
reality it doesn't happen gone

00:11:17,650 --> 00:11:29,750
computational problem compute oriole

00:11:23,960 --> 00:11:35,240
problems are really CPU consuming jobs

00:11:29,750 --> 00:11:38,990
but how many of those jobs are we doing

00:11:35,240 --> 00:11:42,050
on and how often well to insert the

00:11:38,990 --> 00:11:46,820
terror sword is CPU is quite CPU

00:11:42,050 --> 00:11:50,870
intensive so again when Terra sort bit

00:11:46,820 --> 00:11:55,520
record we've seen a lot of CPU cycles

00:11:50,870 --> 00:11:59,090
consumed in traditional database is the

00:11:55,520 --> 00:12:02,090
solution to raise utilization of nodes

00:11:59,090 --> 00:12:07,130
is to is to use compression of the data

00:12:02,090 --> 00:12:09,890
but major big clusters of Hadoop have

00:12:07,130 --> 00:12:13,240
all the data compressed these days and

00:12:09,890 --> 00:12:17,150
we still don't see high cpu utilization

00:12:13,240 --> 00:12:19,520
so again protect production clusters run

00:12:17,150 --> 00:12:21,950
cold because of the two main reasons

00:12:19,520 --> 00:12:24,650
first of all they need to process a lot

00:12:21,950 --> 00:12:27,350
of information and most of the jobs are

00:12:24,650 --> 00:12:32,990
at your bunk and the second reason is

00:12:27,350 --> 00:12:36,440
that the workload is different and

00:12:32,990 --> 00:12:41,120
production production cluster are born

00:12:36,440 --> 00:12:42,920
with restrict SLA so it is so when you

00:12:41,120 --> 00:12:46,190
provision a cluster you you would rather

00:12:42,920 --> 00:12:50,660
think about okay this hard work will

00:12:46,190 --> 00:12:55,640
stay with it will stay idle on average

00:12:50,660 --> 00:12:57,910
but at big times my website will not

00:12:55,640 --> 00:12:57,910
crash

00:12:58,970 --> 00:13:11,730
so that's that basically explanation of

00:13:08,009 --> 00:13:14,910
that dilemma so suppose that we have 72

00:13:11,730 --> 00:13:19,739
gigabyte ram / note as a rule of thumb

00:13:14,910 --> 00:13:22,110
don't don't follow the this blindly is

00:13:19,739 --> 00:13:23,819
that I would allocate for gigabyte per

00:13:22,110 --> 00:13:27,239
data node because it does a lot of

00:13:23,819 --> 00:13:29,999
transfers I would allocate took gigabyte

00:13:27,239 --> 00:13:32,579
per task tracker if you run age based on

00:13:29,999 --> 00:13:38,449
the same cluster then it needs a lot of

00:13:32,579 --> 00:13:43,079
memory so I say 16 gigabytes so I

00:13:38,449 --> 00:13:46,619
occupied how much 22 gigabyte total so

00:13:43,079 --> 00:13:48,600
50s remaining some will take some will

00:13:46,619 --> 00:13:52,920
be taken by the system itself so

00:13:48,600 --> 00:13:57,059
basically every task slot right at

00:13:52,920 --> 00:14:00,360
either MapReduce will take two gigabytes

00:13:57,059 --> 00:14:06,720
so I can have up to 25 slots right then

00:14:00,360 --> 00:14:09,139
and that that's a provisioning for for

00:14:06,720 --> 00:14:12,119
all those services running without

00:14:09,139 --> 00:14:15,360
interaction with each other but if I

00:14:12,119 --> 00:14:17,610
want average utilization hi I over

00:14:15,360 --> 00:14:23,899
provision that I over subscribe and then

00:14:17,610 --> 00:14:28,980
I would assign 28 tasks instead of 25

00:14:23,899 --> 00:14:33,499
but and that's the trade-off when all

00:14:28,980 --> 00:14:33,499
the resources at the big time are

00:14:36,800 --> 00:14:46,290
are working now the the the different

00:14:43,020 --> 00:14:47,870
parts of this system of that node will

00:14:46,290 --> 00:14:50,570
start fighting with each other

00:14:47,870 --> 00:14:53,160
struggling for resources and

00:14:50,570 --> 00:14:57,810
particularly HBase doesn't like to be

00:14:53,160 --> 00:15:01,380
starved and may crash so so that's

00:14:57,810 --> 00:15:04,770
that's the problem and one of the

00:15:01,380 --> 00:15:07,980
solution is if you had a way to better

00:15:04,770 --> 00:15:11,430
isolate the resources you could go with

00:15:07,980 --> 00:15:17,220
more aggressive resource allocation and

00:15:11,430 --> 00:15:20,160
we'll see how how we approach to this so

00:15:17,220 --> 00:15:27,630
as I mentioned two problems right io

00:15:20,160 --> 00:15:32,160
bound workloads and that trade-off in

00:15:27,630 --> 00:15:37,310
provisionally so we decided to start and

00:15:32,160 --> 00:15:39,690
try to eliminate the i/o bound workload

00:15:37,310 --> 00:15:43,320
improving I oh how do you do that of

00:15:39,690 --> 00:15:47,910
course you use at something faster than

00:15:43,320 --> 00:15:53,730
drives and these days it's flash or rare

00:15:47,910 --> 00:15:57,810
so we ran two sets of benchmarks in one

00:15:53,730 --> 00:16:02,910
in both we use we use flash but the in

00:15:57,810 --> 00:16:07,590
the second one that includes HBase edge

00:16:02,910 --> 00:16:09,660
base was sort of caching mechanism so

00:16:07,590 --> 00:16:12,860
that you could serve data directly from

00:16:09,660 --> 00:16:17,910
Ram with the HBase and that that makes

00:16:12,860 --> 00:16:20,070
things run faster in the first in the

00:16:17,910 --> 00:16:26,880
first set of benchmark we just used

00:16:20,070 --> 00:16:29,910
plain HDFS and we used the simplest

00:16:26,880 --> 00:16:33,890
benchmark app for HDFS which is called

00:16:29,910 --> 00:16:40,530
test DFS I yo so what it does now it's

00:16:33,890 --> 00:16:44,340
it it measures performance of Rights

00:16:40,530 --> 00:16:46,950
sequential reads a pant and recently

00:16:44,340 --> 00:16:51,270
added specifically for

00:16:46,950 --> 00:16:53,250
for that study random breeds random

00:16:51,270 --> 00:16:57,540
reads measuring of random reads

00:16:53,250 --> 00:17:02,300
performance tfsi you is a MapReduce job

00:16:57,540 --> 00:17:05,730
the map actually produces all those

00:17:02,300 --> 00:17:08,220
requests basically reading every map

00:17:05,730 --> 00:17:15,800
task reads or writes into a single file

00:17:08,220 --> 00:17:18,540
and calculates the track the throughput

00:17:15,800 --> 00:17:21,420
then it outputs through the throughput

00:17:18,540 --> 00:17:23,790
and single reducer aggregates all the

00:17:21,420 --> 00:17:28,680
information produces you the average

00:17:23,790 --> 00:17:31,950
through of of the job form of all the

00:17:28,680 --> 00:17:33,930
maps so basically if you want to

00:17:31,950 --> 00:17:35,640
increase the number of files you're

00:17:33,930 --> 00:17:37,860
processing at the same time

00:17:35,640 --> 00:17:42,390
simultaneously then you increase the

00:17:37,860 --> 00:17:45,570
number of mappers in DFS a random reads

00:17:42,390 --> 00:17:48,830
so this is sort of a new features

00:17:45,570 --> 00:17:53,160
specifically for for that stat study

00:17:48,830 --> 00:17:57,480
it's in JIRA we of course have a jira

00:17:53,160 --> 00:18:02,840
for that so there are three types of

00:17:57,480 --> 00:18:05,970
random reads first is straightforward

00:18:02,840 --> 00:18:10,650
random reads when you randomly select an

00:18:05,970 --> 00:18:13,560
earth an offset and read for example 1

00:18:10,650 --> 00:18:16,530
kilobyte of data from that offset then

00:18:13,560 --> 00:18:19,560
you switch to another random apps offset

00:18:16,530 --> 00:18:24,420
and read 1 kilobyte of data and you go

00:18:19,560 --> 00:18:28,170
until you read the amount of bytes you

00:18:24,420 --> 00:18:31,650
were supposed to read so that's a random

00:18:28,170 --> 00:18:35,490
read and one disadvantage of that that

00:18:31,650 --> 00:18:38,850
sometimes you heat you your reads

00:18:35,490 --> 00:18:41,460
intersect so you hit the catch and we

00:18:38,850 --> 00:18:44,580
want to avoid that because flash really

00:18:41,460 --> 00:18:47,040
stresses it gives you gives you a better

00:18:44,580 --> 00:18:51,660
performing of randomly so in order to

00:18:47,040 --> 00:18:54,420
avoid those read ahead caching we

00:18:51,660 --> 00:18:56,220
introduced backward read if you read

00:18:54,420 --> 00:19:00,390
backwards there is no reader hat cap

00:18:56,220 --> 00:19:04,169
so you always heat cold data now and we

00:19:00,390 --> 00:19:07,890
introduce keeping that is you know what

00:19:04,169 --> 00:19:10,679
what you really had window is so you

00:19:07,890 --> 00:19:15,419
skip ahead of that and read cold data

00:19:10,679 --> 00:19:18,929
again so those word you know just things

00:19:15,419 --> 00:19:21,690
we could come up in order to avoid mmm

00:19:18,929 --> 00:19:24,539
hot days because in in sequential reason

00:19:21,690 --> 00:19:28,409
right we don't see much increase in

00:19:24,539 --> 00:19:33,360
performance so when when you do it

00:19:28,409 --> 00:19:36,260
correctly then all the three give very

00:19:33,360 --> 00:19:39,650
similar results and that means that

00:19:36,260 --> 00:19:43,440
we're doing really really random reads

00:19:39,650 --> 00:19:47,850
the benchmark environment for tfsi yo so

00:19:43,440 --> 00:19:51,630
we used to do pawn and HBase 92 we had

00:19:47,850 --> 00:19:54,210
one monster node and running name node

00:19:51,630 --> 00:19:57,570
and job tracker and we have three

00:19:54,210 --> 00:20:01,760
Slade's Slade's that have data nodes and

00:19:57,570 --> 00:20:06,470
dust trackers so typical configuration

00:20:01,760 --> 00:20:12,030
we have we had eight core hypertrichosis

00:20:06,470 --> 00:20:17,450
sirs 24 4 gigabyte from drives 44 1

00:20:12,030 --> 00:20:22,230
terabyte drives sorry 24 gigabyte of RAM

00:20:17,450 --> 00:20:27,530
and that's standard 1 gigabyte GB PS

00:20:22,230 --> 00:20:32,159
network so DFS I yield data set was

00:20:27,530 --> 00:20:35,809
overall sizes 10 gigabyte it is split

00:20:32,159 --> 00:20:39,929
into 72 files that means that we can run

00:20:35,809 --> 00:20:44,600
72 mopper simultaneously on that on that

00:20:39,929 --> 00:20:49,169
cluster and measure the performance

00:20:44,600 --> 00:20:52,650
total data read for every you you should

00:20:49,169 --> 00:20:55,670
not read the whole day right from from

00:20:52,650 --> 00:20:59,520
the file so we reach 70

00:20:55,670 --> 00:21:02,990
and single read is one megabyte and we

00:20:59,520 --> 00:21:05,670
used and we measured performance with

00:21:02,990 --> 00:21:08,310
different number of concurrent trees are

00:21:05,670 --> 00:21:13,440
starting from three which is one reader

00:21:08,310 --> 00:21:22,980
/ / take note and end it with 72 which

00:21:13,440 --> 00:21:28,620
is 24 readers per turn on so this is how

00:21:22,980 --> 00:21:30,870
how how this is the behavior we have

00:21:28,620 --> 00:21:34,430
seen on the classes so basically the

00:21:30,870 --> 00:21:37,730
hard drive the hard drive cluster

00:21:34,430 --> 00:21:41,460
stabilizes and doesn't go a performance

00:21:37,730 --> 00:21:45,140
remains the same because you hid the

00:21:41,460 --> 00:21:50,040
disk clinic at that point and flashes

00:21:45,140 --> 00:21:55,590
linearly goes up so we didn't saturate

00:21:50,040 --> 00:22:00,780
our button our flash arrays so it was

00:21:55,590 --> 00:22:03,810
good and since we removed sort of the

00:22:00,780 --> 00:22:10,710
contention on the yo we decided to go

00:22:03,810 --> 00:22:13,800
with why CSB and try to see what what

00:22:10,710 --> 00:22:17,310
the what the performance will be so

00:22:13,800 --> 00:22:23,660
what's why CSB cloud serving benchmark

00:22:17,310 --> 00:22:29,070
but it provides a load on on the age

00:22:23,660 --> 00:22:32,450
story on a storage cluster on a

00:22:29,070 --> 00:22:35,220
distributed storage cluster but why

00:22:32,450 --> 00:22:37,410
usually it's used for measuring edge

00:22:35,220 --> 00:22:42,390
base but it also can be used to compare

00:22:37,410 --> 00:22:46,070
HBase and other non non non no sequel

00:22:42,390 --> 00:22:51,400
databases and traditional relational

00:22:46,070 --> 00:22:51,400
databases work with Cassandra Baltimore

00:22:51,880 --> 00:23:03,860
many other things so data represented as

00:22:57,350 --> 00:23:07,070
a table with some big size field in it

00:23:03,860 --> 00:23:10,670
and they are identified by unique keys

00:23:07,070 --> 00:23:13,310
which is important for it for non sequel

00:23:10,670 --> 00:23:16,490
databases for HBase there are four main

00:23:13,310 --> 00:23:19,330
operations they're very simple the

00:23:16,490 --> 00:23:24,100
insert a new record read the record

00:23:19,330 --> 00:23:29,510
updated record which is right and scan

00:23:24,100 --> 00:23:33,340
scan a range of records so scan means

00:23:29,510 --> 00:23:37,310
that you that you scan a consequent

00:23:33,340 --> 00:23:42,670
consequence segment of records in in the

00:23:37,310 --> 00:23:45,670
key range from key X 1 through key x 2

00:23:42,670 --> 00:23:45,670
so

00:23:49,720 --> 00:23:54,790
so it's the same environment we used

00:23:52,330 --> 00:23:57,910
exactly the same environment we had to

00:23:54,790 --> 00:24:01,390
we added HBase master on the master node

00:23:57,910 --> 00:24:07,300
and zookeeper and we added region server

00:24:01,390 --> 00:24:12,010
for every for every slave node so in

00:24:07,300 --> 00:24:15,760
this case we also added a variation to

00:24:12,010 --> 00:24:22,120
our configuration so we're using DMS and

00:24:15,760 --> 00:24:25,330
we're trying to utilize utilize the

00:24:22,120 --> 00:24:29,920
environment with with those VMs so we

00:24:25,330 --> 00:24:31,960
tried two three and four VMs per load in

00:24:29,920 --> 00:24:35,590
this configuration and we'll see what

00:24:31,960 --> 00:24:40,900
the results are were on data set data

00:24:35,590 --> 00:24:43,360
set for why CSB was to dataset 10

00:24:40,900 --> 00:24:48,280
million records and 30 million records

00:24:43,360 --> 00:24:51,100
so 10 million records is more internal

00:24:48,280 --> 00:24:55,390
memory load and with 30 30 million

00:24:51,100 --> 00:24:58,810
record we hit we were having more are

00:24:55,390 --> 00:25:02,560
you so that's why it was necessary to

00:24:58,810 --> 00:25:06,190
have both of them while we're running

00:25:02,560 --> 00:25:09,520
this dstyle is running and collecting

00:25:06,190 --> 00:25:12,820
data from I mean simple simple to 2d

00:25:09,520 --> 00:25:16,420
step it's just collecting utilization of

00:25:12,820 --> 00:25:23,560
different resources on the cluster so in

00:25:16,420 --> 00:25:27,040
YCS be you can you can define the

00:25:23,560 --> 00:25:30,310
percentages of operations for your

00:25:27,040 --> 00:25:33,070
workload so how it works you first do a

00:25:30,310 --> 00:25:35,890
data load and this is one hundred

00:25:33,070 --> 00:25:39,070
percent inserts and then you run one of

00:25:35,890 --> 00:25:42,940
your workload with specified percentages

00:25:39,070 --> 00:25:46,000
so we chose these three workloads on the

00:25:42,940 --> 00:25:48,360
data load we did not expect any

00:25:46,000 --> 00:25:52,810
improvements with flash I mean

00:25:48,360 --> 00:25:55,630
marginally this is if you're right load

00:25:52,810 --> 00:25:59,500
so you wouldn't expect anything unusual

00:25:55,630 --> 00:26:01,470
here reads would have inserts so we're

00:25:59,500 --> 00:26:05,190
measuring read performance

00:26:01,470 --> 00:26:08,130
there is a lot of inserts why because in

00:26:05,190 --> 00:26:11,730
that case reads our code in most cases

00:26:08,130 --> 00:26:17,850
in many cases because there is a huge

00:26:11,730 --> 00:26:21,000
insert insert insert traffic and if you

00:26:17,850 --> 00:26:23,730
do it on hard drives then there's lots

00:26:21,000 --> 00:26:26,309
of six basically you right you see you

00:26:23,730 --> 00:26:29,940
read you see so there was an interesting

00:26:26,309 --> 00:26:32,940
sort of workload to consider and short

00:26:29,940 --> 00:26:40,559
range scan the skins are you know

00:26:32,940 --> 00:26:42,539
typical typical read scenario they have

00:26:40,559 --> 00:26:46,919
a little bit of inserts just to make

00:26:42,539 --> 00:26:53,340
sure that we don't do scans all the time

00:26:46,919 --> 00:26:56,070
so short range scale is is basically and

00:26:53,340 --> 00:27:01,230
and allowed an analogy of random read

00:26:56,070 --> 00:27:03,330
for each base so this is the true put

00:27:01,230 --> 00:27:11,370
that we have seen if you see on those

00:27:03,330 --> 00:27:14,039
crabs you see that black is flash for

00:27:11,370 --> 00:27:18,559
data load there is marginal as we

00:27:14,039 --> 00:27:24,840
expected marginal improvement with flash

00:27:18,559 --> 00:27:27,059
with reads over inserts there is a

00:27:24,840 --> 00:27:30,840
substantial improvement for each

00:27:27,059 --> 00:27:34,159
performance and short range scans random

00:27:30,840 --> 00:27:34,159
read a really good

00:27:36,210 --> 00:27:44,440
ok

00:27:38,320 --> 00:27:48,389
so now we decide to write number of VMs

00:27:44,440 --> 00:27:53,350
first of all we noticed we studied from

00:27:48,389 --> 00:27:56,710
physical note no vm sorbonne v1 p.m. for

00:27:53,350 --> 00:28:01,269
that matter and we noticed that if we

00:27:56,710 --> 00:28:06,870
add VMs every vm added gives us twenty

00:28:01,269 --> 00:28:11,649
percent advantage in in performance so

00:28:06,870 --> 00:28:14,620
with he so those are lines for different

00:28:11,649 --> 00:28:19,659
number of ems on the cluster so still

00:28:14,620 --> 00:28:23,529
three nodes but more gaps and we are

00:28:19,659 --> 00:28:26,019
trying to run multiple concurrent

00:28:23,529 --> 00:28:28,480
threads increasing the number of

00:28:26,019 --> 00:28:31,029
concurrent threads and we see that the

00:28:28,480 --> 00:28:37,509
physical cluster is flat as we have seen

00:28:31,029 --> 00:28:45,610
before but DMS our DM clusters are going

00:28:37,509 --> 00:28:49,320
higher latency is is really is really a

00:28:45,610 --> 00:28:52,899
distinguishing story I didn't plot here

00:28:49,320 --> 00:28:54,909
two more points for physical for

00:28:52,899 --> 00:28:58,659
physical cluster because it goes

00:28:54,909 --> 00:29:01,240
straight up with increases it doubles

00:28:58,659 --> 00:29:04,500
for every for every next point so so if

00:29:01,240 --> 00:29:09,820
you if I plotted the hope the hope thing

00:29:04,500 --> 00:29:17,730
I didn't want to I wanted the difference

00:29:09,820 --> 00:29:17,730
in in this be recognizable as well yes

00:29:17,990 --> 00:29:23,980
latency with the 1690s crossover right

00:29:25,840 --> 00:29:33,890
so it's the experiment it's that I know

00:29:31,460 --> 00:29:37,280
there was some one anomaly i bought by a

00:29:33,890 --> 00:29:42,010
coot really flattened it but yeah that

00:29:37,280 --> 00:29:42,010
that's what you know the actual data set

00:29:43,420 --> 00:29:50,720
okay and the most interesting part part

00:29:47,270 --> 00:29:52,760
of this was the cpu utilization again

00:29:50,720 --> 00:29:56,630
we're looking at the physical cluster

00:29:52,760 --> 00:29:59,420
and we see that this gray by looking at

00:29:56,630 --> 00:30:03,620
the gray area gray area is idle cpu

00:29:59,420 --> 00:30:06,020
utilization so idle and wait basically

00:30:03,620 --> 00:30:09,740
what we're interested in because i

00:30:06,020 --> 00:30:13,510
delayed wait wait is I yo it's stable

00:30:09,740 --> 00:30:17,210
it's one percent in both cases idol is

00:30:13,510 --> 00:30:23,000
huge in the physical node cluster on

00:30:17,210 --> 00:30:27,800
virtualized cluster we achieve much much

00:30:23,000 --> 00:30:31,750
better average utilization at Peaks we

00:30:27,800 --> 00:30:35,920
have seen almost one hundred percent so

00:30:31,750 --> 00:30:38,330
because I mean jobs are starting

00:30:35,920 --> 00:30:41,059
finishing so there there are always

00:30:38,330 --> 00:30:46,640
piques their so sometimes you see

00:30:41,059 --> 00:30:49,850
hundred percent had looks like that's

00:30:46,640 --> 00:30:52,190
the way to go and the last third

00:30:49,850 --> 00:30:55,160
experience of third workload that we

00:30:52,190 --> 00:30:57,920
consider it on the HBase cluster was

00:30:55,160 --> 00:31:00,140
forty percent breeds with fifty five

00:30:57,920 --> 00:31:03,830
percent inserts so this is what I told

00:31:00,140 --> 00:31:06,890
we're measuring reads when in present of

00:31:03,830 --> 00:31:09,890
lots of rights and we have seen

00:31:06,890 --> 00:31:15,190
basically at the same picture we ran it

00:31:09,890 --> 00:31:15,190
for two data sets this is more or less

00:31:15,309 --> 00:31:21,110
measuring performance of HBase itself

00:31:18,080 --> 00:31:23,549
serving data from cash and that involves

00:31:21,110 --> 00:31:26,669
more I use because you cannot

00:31:23,549 --> 00:31:34,200
we couldn't fit all 30 million records

00:31:26,669 --> 00:31:37,019
in memory the latency is right so this

00:31:34,200 --> 00:31:39,450
is this is a reverse picture it's not

00:31:37,019 --> 00:31:45,690
true puts its latency so for disk

00:31:39,450 --> 00:31:53,509
latency is much higher and here are some

00:31:45,690 --> 00:31:56,879
conclusions for HDFS sequential

00:31:53,509 --> 00:32:00,090
sequential disks handle sequential are

00:31:56,879 --> 00:32:03,269
you where ever I don't think we need to

00:32:00,090 --> 00:32:06,869
switch to still expensive flash storage

00:32:03,269 --> 00:32:09,570
at this point if your if your workloads

00:32:06,869 --> 00:32:11,340
are mostly sequential if you have lots

00:32:09,570 --> 00:32:17,269
of randomly it's that where flash

00:32:11,340 --> 00:32:23,340
storage kicks in and it definitely

00:32:17,269 --> 00:32:25,440
outperforms discs for HBase right only

00:32:23,340 --> 00:32:32,779
workloads are not data loading for

00:32:25,440 --> 00:32:36,149
example are not are not very interested

00:32:32,779 --> 00:32:39,289
you cannot do a lot of interesting

00:32:36,149 --> 00:32:44,549
things with flash for that type of flow

00:32:39,289 --> 00:32:50,279
I think one those those were expected

00:32:44,549 --> 00:32:56,960
results sort of the use of multiple VMs

00:32:50,279 --> 00:32:59,820
was not so expected result but the

00:32:56,960 --> 00:33:04,799
results show that the benchmarks show

00:32:59,820 --> 00:33:11,869
that using VMs can utilize your hardware

00:33:04,799 --> 00:33:11,869
your CPU much better than regular drives

00:33:14,400 --> 00:33:22,900
we also showed that random reads not

00:33:18,940 --> 00:33:26,130
only random reads are optimized with the

00:33:22,900 --> 00:33:29,980
ends but an flash combination of

00:33:26,130 --> 00:33:34,570
virtualization and flash storage but

00:33:29,980 --> 00:33:38,940
also reads in presence of lots of rights

00:33:34,570 --> 00:33:43,480
and there is a more wider use case in

00:33:38,940 --> 00:33:46,600
many in many applications right randomly

00:33:43,480 --> 00:33:49,660
it's probably not very but very popular

00:33:46,600 --> 00:33:54,790
but reading a lot when you're also

00:33:49,660 --> 00:33:56,920
reading is a typical workload so in my

00:33:54,790 --> 00:33:59,620
opinion virtualization serves two main

00:33:56,920 --> 00:34:03,000
functions here first of all you increase

00:33:59,620 --> 00:34:06,280
utilization of your resources so you can

00:34:03,000 --> 00:34:09,159
use less resources to compute or compute

00:34:06,280 --> 00:34:11,980
more the same resources you have and

00:34:09,159 --> 00:34:13,600
also the second important part is

00:34:11,980 --> 00:34:19,560
isolation when you need to provision

00:34:13,600 --> 00:34:23,740
your cluster for a strict SLA require

00:34:19,560 --> 00:34:27,370
environments then you can use than your

00:34:23,740 --> 00:34:30,490
VMS will be isolated they will not try

00:34:27,370 --> 00:34:32,110
to interfere with each other and you can

00:34:30,490 --> 00:34:34,679
provision your clusters more

00:34:32,110 --> 00:34:34,679
aggressively

00:34:42,570 --> 00:34:50,700
time for questions yes please

00:34:48,450 --> 00:34:53,339
so you've got a physical setup or four

00:34:50,700 --> 00:34:58,740
machines and when you get a

00:34:53,339 --> 00:35:02,880
virtualization you have kind of a 69 12e

00:34:58,740 --> 00:35:05,099
F on each of those four villa so what

00:35:02,880 --> 00:35:07,020
was the distribution are to make

00:35:05,099 --> 00:35:09,450
marriage and how you can add as an

00:35:07,020 --> 00:35:12,420
astronaut chick on your honeyed you

00:35:09,450 --> 00:35:14,780
always tell me how one master node or

00:35:12,420 --> 00:35:17,760
did you also end up running multiple

00:35:14,780 --> 00:35:19,650
multiple months ago yeah yeah in

00:35:17,760 --> 00:35:22,530
production of course you you were going

00:35:19,650 --> 00:35:25,859
to separate those musters you want to

00:35:22,530 --> 00:35:34,560
run the new single node job fair girl

00:35:25,859 --> 00:35:36,359
singer but they know zookeeper doesn't

00:35:34,560 --> 00:35:40,079
take you by the same should be to see

00:35:36,359 --> 00:35:42,300
read it on the other and I put in order

00:35:40,079 --> 00:35:45,359
to provide incentives but yes in

00:35:42,300 --> 00:35:47,869
production what you want then that was

00:35:45,359 --> 00:35:47,869
the point

00:35:48,770 --> 00:35:59,190
for this benchmarking because we didn't

00:35:53,760 --> 00:36:00,990
care about performance of what what

00:35:59,190 --> 00:36:02,400
virtualization technology to do this

00:36:00,990 --> 00:36:08,010
like always you're hyper visor because

00:36:02,400 --> 00:36:12,260
I'm going so that was being weird see

00:36:08,010 --> 00:36:12,260
the skirt still the three word

00:36:18,190 --> 00:36:23,030
how did you provide a better resources

00:36:20,480 --> 00:36:27,230
you just split up the memory or less

00:36:23,030 --> 00:36:29,540
evenly yes also with googly halfway YY

00:36:27,230 --> 00:36:31,960
phones only you don't even for me yes

00:36:29,540 --> 00:36:35,870
but no because we have only for pricing

00:36:31,960 --> 00:36:40,390
and was over was one girl curvy yes one

00:36:35,870 --> 00:36:45,290
driver but but but memory wise yes

00:36:40,390 --> 00:36:52,120
affection did you say hey doll he

00:36:45,290 --> 00:36:58,220
destroy us with nobody user yet it was I

00:36:52,120 --> 00:37:01,780
was announced with me before their so do

00:36:58,220 --> 00:37:01,780
you have mm-hmm

00:37:09,839 --> 00:37:12,469
alright

00:37:14,230 --> 00:37:16,290

YouTube URL: https://www.youtube.com/watch?v=dNNwywqIcqg


