Title: Advanced data modeling techniques for Cassandra
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Cassandra
Description: 
	Advanced data modeling techniques for Cassandra
Arturo Hinojosa, Michael Raney

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Whether your storing timeseries data for a messaging app or device metadata for an industrial IoT application, your Cassandra data model can have a massive impact on your applicationâ€™s performance and scalability. In this talk, we will walk through advanced techniques and best practices for building highly scalable, fast, and robust data models. You will learn how to model your data based on your queries and access patterns to ensure you have well-distributed data that will enable your application to scale up as traffic grows. We will talk through examples of deformalizing data, modeling complex relationships, and optimizations that you can apply to your schemas and data models to improve performance.

Arturo Hinojosa:
Arturo Hinojosa is a Principal Product Manager on the Amazon Keyspaces (for Apache Cassandra) team at Amazon Web Services (AWS). Arturo is responsible for Amazon Keyspaces' overall product strategy and has been with AWS for over four years.
Michael Raney:
Michael is the lead specialist solution architect (SA) for Amazon Keyspaces (for Apache Cassandra). As the lead SA for Amazon Keyspaces, Michael works with customers every day to design cloud-based NoSQL solutions for large-scale distributed systems.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,720 --> 00:00:27,519
thank you well thank you everyone for

00:00:25,840 --> 00:00:28,960
making time this afternoon or this

00:00:27,519 --> 00:00:31,199
morning depending on where you are

00:00:28,960 --> 00:00:33,040
uh join us for our session today uh my

00:00:31,199 --> 00:00:34,079
name is arturo nojosa and with me today

00:00:33,040 --> 00:00:35,600
is michael rainey

00:00:34,079 --> 00:00:37,920
and we're going to talk about advanced

00:00:35,600 --> 00:00:39,040
data modeling techniques uh for apache

00:00:37,920 --> 00:00:41,280
cassandra

00:00:39,040 --> 00:00:42,559
um so again apologizing in my garage

00:00:41,280 --> 00:00:45,120
today as i'm sure

00:00:42,559 --> 00:00:45,760
all of us are work from home sale uh so

00:00:45,120 --> 00:00:47,840
please

00:00:45,760 --> 00:00:49,120
ignore the the diapers yeah so what

00:00:47,840 --> 00:00:50,800
before we get going a little bit about

00:00:49,120 --> 00:00:52,399
each of us right so uh my name is retro

00:00:50,800 --> 00:00:54,079
and i'm the principal product manager

00:00:52,399 --> 00:00:57,360
for amazon key spaces

00:00:54,079 --> 00:00:59,840
here at aws i've been with aws for about

00:00:57,360 --> 00:01:01,680
five years and i i own the product and

00:00:59,840 --> 00:01:03,359
business strategy for key spaces

00:01:01,680 --> 00:01:05,519
uh before that i was on the another

00:01:03,359 --> 00:01:07,200
non-relational service team dynamodb

00:01:05,519 --> 00:01:08,720
and you know worked a little bit on our

00:01:07,200 --> 00:01:11,760
security products as well

00:01:08,720 --> 00:01:13,920
michael hi

00:01:11,760 --> 00:01:15,759
i'm michael rainey i'm a specialist

00:01:13,920 --> 00:01:17,119
solution architect at aws

00:01:15,759 --> 00:01:19,600
i've been working with distributed

00:01:17,119 --> 00:01:22,400
systems for about 11 years now

00:01:19,600 --> 00:01:23,840
and my primary focus at aws is helping

00:01:22,400 --> 00:01:27,520
users with cassandra

00:01:23,840 --> 00:01:28,720
and amazon key spaces great so as we get

00:01:27,520 --> 00:01:30,400
going here um

00:01:28,720 --> 00:01:32,159
before we get going let's talk a little

00:01:30,400 --> 00:01:33,280
bit about the agenda for today so first

00:01:32,159 --> 00:01:34,560
we're going to talk about the importance

00:01:33,280 --> 00:01:36,799
of good data modeling

00:01:34,560 --> 00:01:38,560
why developing good data models that are

00:01:36,799 --> 00:01:40,159
scalable and very performant

00:01:38,560 --> 00:01:42,320
it is critical for developers building

00:01:40,159 --> 00:01:43,360
these massive global scale applications

00:01:42,320 --> 00:01:44,479
and we'll talk about some of the

00:01:43,360 --> 00:01:45,920
challenges of building good

00:01:44,479 --> 00:01:47,439
non-relational data models

00:01:45,920 --> 00:01:48,960
especially for those folks coming to

00:01:47,439 --> 00:01:49,360
cassandra from the first time from with

00:01:48,960 --> 00:01:50,720
a

00:01:49,360 --> 00:01:52,960
a relational background in something

00:01:50,720 --> 00:01:54,079
like you know postgres or aurora or

00:01:52,960 --> 00:01:55,520
mysql

00:01:54,079 --> 00:01:57,680
then mica will talk about some of the

00:01:55,520 --> 00:01:58,960
key data modeling concepts he uses day

00:01:57,680 --> 00:01:59,520
in and day out with customers in the

00:01:58,960 --> 00:02:00,799
field

00:01:59,520 --> 00:02:02,560
as well as some of the techniques he

00:02:00,799 --> 00:02:04,960
uses with customers to help them

00:02:02,560 --> 00:02:05,920
build these massively scale applications

00:02:04,960 --> 00:02:07,360
finally we'll talk

00:02:05,920 --> 00:02:08,319
about some of the tooling we use we

00:02:07,360 --> 00:02:09,759
actually have a pretty exciting

00:02:08,319 --> 00:02:10,239
announcement in that part of the session

00:02:09,759 --> 00:02:11,280
today

00:02:10,239 --> 00:02:13,280
and then we'll leave some time at the

00:02:11,280 --> 00:02:14,879
end for questions so

00:02:13,280 --> 00:02:16,480
you know why is good data modeling

00:02:14,879 --> 00:02:17,760
important right you know one of the very

00:02:16,480 --> 00:02:20,720
interesting things we saw

00:02:17,760 --> 00:02:21,120
in the recent cassandra uh report was

00:02:20,720 --> 00:02:23,200
that

00:02:21,120 --> 00:02:24,640
you know 34 percent you know michael

00:02:23,200 --> 00:02:27,760
slide

00:02:24,640 --> 00:02:29,440
34 percent of uh

00:02:27,760 --> 00:02:31,200
practitioners courtesy cassandra said

00:02:29,440 --> 00:02:33,200
they sort of had problems

00:02:31,200 --> 00:02:35,040
you know driving cassandra adoption

00:02:33,200 --> 00:02:36,800
within the organization due to a lack of

00:02:35,040 --> 00:02:38,400
cassandra skills on the team right

00:02:36,800 --> 00:02:40,160
and a big part of that is knowing how to

00:02:38,400 --> 00:02:41,200
build these really massively scalable

00:02:40,160 --> 00:02:43,360
data models

00:02:41,200 --> 00:02:44,480
and you know and why is that hard well

00:02:43,360 --> 00:02:46,400
first and foremost

00:02:44,480 --> 00:02:48,319
you know it's a context shift right if

00:02:46,400 --> 00:02:49,120
you are used to building relational data

00:02:48,319 --> 00:02:51,920
models

00:02:49,120 --> 00:02:54,239
and building data model data michael

00:02:51,920 --> 00:02:54,239
slide

00:02:55,519 --> 00:02:58,720
your good data tomorrow is right it can

00:02:56,959 --> 00:02:59,760
be challenging so this so in the

00:02:58,720 --> 00:03:01,040
partnership with dataology it really

00:02:59,760 --> 00:03:02,560
helps you build these massively

00:03:01,040 --> 00:03:03,280
performance massively scaled

00:03:02,560 --> 00:03:04,640
applications

00:03:03,280 --> 00:03:06,080
when you build a good data model that

00:03:04,640 --> 00:03:07,440
has really good performance it's aligned

00:03:06,080 --> 00:03:08,720
with your with the query patterns

00:03:07,440 --> 00:03:10,640
that helps you read and write data

00:03:08,720 --> 00:03:11,760
faster as well as top of that you you

00:03:10,640 --> 00:03:13,280
can build these very well distributed

00:03:11,760 --> 00:03:14,959
data models are more scalable

00:03:13,280 --> 00:03:16,800
as your traffic and data evolve over

00:03:14,959 --> 00:03:18,000
time but there are challenges to these

00:03:16,800 --> 00:03:19,599
buildings data models

00:03:18,000 --> 00:03:20,959
first and foremost the context shift

00:03:19,599 --> 00:03:22,159
from going from the relational data

00:03:20,959 --> 00:03:24,400
models in our relational

00:03:22,159 --> 00:03:26,159
why well in a relational world you build

00:03:24,400 --> 00:03:27,519
your tables around entities you know for

00:03:26,159 --> 00:03:28,319
example if you're with an e-commerce

00:03:27,519 --> 00:03:30,319
application

00:03:28,319 --> 00:03:32,000
you might have a table for orders and

00:03:30,319 --> 00:03:33,920
for customers and

00:03:32,000 --> 00:03:35,680
for products and when you need to do a

00:03:33,920 --> 00:03:37,440
query across all this data

00:03:35,680 --> 00:03:39,280
you use a join right and i can say hey

00:03:37,440 --> 00:03:39,840
you know select from tables one two

00:03:39,280 --> 00:03:41,680
three

00:03:39,840 --> 00:03:43,440
or sort of share this common thing and

00:03:41,680 --> 00:03:45,120
bring me back all the information

00:03:43,440 --> 00:03:46,560
well non-relational roles it doesn't

00:03:45,120 --> 00:03:48,239
work quite the same way

00:03:46,560 --> 00:03:49,840
um you really want to make sure your

00:03:48,239 --> 00:03:52,159
table schemas are

00:03:49,840 --> 00:03:54,000
are reflecting of your actual query

00:03:52,159 --> 00:03:55,519
pattern so in a single query

00:03:54,000 --> 00:03:57,599
you can bring together all the

00:03:55,519 --> 00:03:58,560
information about products and customers

00:03:57,599 --> 00:04:01,200
and tables

00:03:58,560 --> 00:04:03,040
in a single command you know for example

00:04:01,200 --> 00:04:05,280
you might have a heterogeneous table

00:04:03,040 --> 00:04:07,120
where you have all the information about

00:04:05,280 --> 00:04:09,200
products and customers and orders in one

00:04:07,120 --> 00:04:10,239
place um because there are no quarries

00:04:09,200 --> 00:04:12,959
though right you might have

00:04:10,239 --> 00:04:14,319
rows that that are representing an order

00:04:12,959 --> 00:04:15,760
and a customer

00:04:14,319 --> 00:04:17,440
and and something like that all in a

00:04:15,760 --> 00:04:18,959
single table so it's it's a very

00:04:17,440 --> 00:04:20,400
fundamentally different way of sort of

00:04:18,959 --> 00:04:22,400
thinking about your schemas

00:04:20,400 --> 00:04:24,240
and your access patterns so to talk

00:04:22,400 --> 00:04:24,960
about some of the fundamentals of data

00:04:24,240 --> 00:04:26,080
modeling

00:04:24,960 --> 00:04:27,199
um you know i'm going to turn over to

00:04:26,080 --> 00:04:29,840
michael to go through some of these

00:04:27,199 --> 00:04:29,840
concepts

00:04:30,560 --> 00:04:35,120
thanks arturo yes uh thank you for

00:04:33,360 --> 00:04:38,320
outlining those challenges as well

00:04:35,120 --> 00:04:39,759
i find that uh most of what

00:04:38,320 --> 00:04:41,280
you know being comfortable with these

00:04:39,759 --> 00:04:42,479
challenges and transitioning from

00:04:41,280 --> 00:04:45,199
relational to not

00:04:42,479 --> 00:04:45,840
non-relational no sequel is really about

00:04:45,199 --> 00:04:48,800
prac

00:04:45,840 --> 00:04:50,160
gaining practical experience uh what i'm

00:04:48,800 --> 00:04:52,800
going to share in the next

00:04:50,160 --> 00:04:54,000
few slides here is some common cassandra

00:04:52,800 --> 00:04:55,440
terminology

00:04:54,000 --> 00:04:57,120
some constructs that are different in

00:04:55,440 --> 00:04:58,160
cassandra than nosql

00:04:57,120 --> 00:04:59,840
and then we're going to work on

00:04:58,160 --> 00:05:01,199
solutions that we use every day in the

00:04:59,840 --> 00:05:02,639
field to help customers with their

00:05:01,199 --> 00:05:04,160
cassandra models

00:05:02,639 --> 00:05:05,680
then finally we're going to go and look

00:05:04,160 --> 00:05:07,759
at a new tool that we're developing to

00:05:05,680 --> 00:05:09,199
help modeling easier for developers and

00:05:07,759 --> 00:05:10,880
get that hands-on experience and

00:05:09,199 --> 00:05:13,039
practical knowledge with

00:05:10,880 --> 00:05:14,000
modeling which is one of the key aspects

00:05:13,039 --> 00:05:17,680
to

00:05:14,000 --> 00:05:17,680
doing things well with cassandra

00:05:17,759 --> 00:05:23,120
so at a high level cassandra and nosql

00:05:21,680 --> 00:05:24,720
generally looks like what you would

00:05:23,120 --> 00:05:27,680
expect from a relational

00:05:24,720 --> 00:05:28,800
model you have a schema and cassandra

00:05:27,680 --> 00:05:30,639
it's a key space

00:05:28,800 --> 00:05:33,120
and then you have tables which are just

00:05:30,639 --> 00:05:35,759
collections of related data

00:05:33,120 --> 00:05:36,880
they contain schema or constructs of

00:05:35,759 --> 00:05:39,120
similar types

00:05:36,880 --> 00:05:41,840
so the server-side validation of that

00:05:39,120 --> 00:05:43,680
data when you insert it

00:05:41,840 --> 00:05:45,440
and then there's a rows which is every

00:05:43,680 --> 00:05:46,880
row is a similar structure

00:05:45,440 --> 00:05:49,199
in cassandra it's a little different

00:05:46,880 --> 00:05:51,759
we'll get into that

00:05:49,199 --> 00:05:52,960
but every row has a kind of sort of cell

00:05:51,759 --> 00:05:55,039
and the cell is a

00:05:52,960 --> 00:05:57,360
sort of the value of that row for that

00:05:55,039 --> 00:05:59,039
column

00:05:57,360 --> 00:06:00,639
so what's different in nosql than

00:05:59,039 --> 00:06:03,120
relational well one of the biggest

00:06:00,639 --> 00:06:04,960
difference is that nosql is distributed

00:06:03,120 --> 00:06:06,960
so when you have your model you got to

00:06:04,960 --> 00:06:08,880
think about how that data is distributed

00:06:06,960 --> 00:06:10,240
and the best way to do that is to evenly

00:06:08,880 --> 00:06:12,319
distribute it for

00:06:10,240 --> 00:06:13,840
for the the data that's distributed over

00:06:12,319 --> 00:06:16,479
multiple physical hardware

00:06:13,840 --> 00:06:17,280
but also distribute for the access and

00:06:16,479 --> 00:06:19,600
so that the

00:06:17,280 --> 00:06:21,600
applications are accessing data in a

00:06:19,600 --> 00:06:23,360
randomized way

00:06:21,600 --> 00:06:25,120
so your partitioning has to be built

00:06:23,360 --> 00:06:26,479
into your model so your partitioning is

00:06:25,120 --> 00:06:28,160
done at scale

00:06:26,479 --> 00:06:31,360
you're looking at more denormalized

00:06:28,160 --> 00:06:33,360
models so storing and duplicating data

00:06:31,360 --> 00:06:34,880
putting the cost more on the storage

00:06:33,360 --> 00:06:36,720
than the cpu

00:06:34,880 --> 00:06:38,600
and then you're doing more modeling from

00:06:36,720 --> 00:06:41,440
a developer-centric way or

00:06:38,600 --> 00:06:43,120
application-centric way then based on

00:06:41,440 --> 00:06:44,800
designing a model for your entire

00:06:43,120 --> 00:06:47,600
organization it's really

00:06:44,800 --> 00:06:48,000
a different way to have responsibility

00:06:47,600 --> 00:06:51,039
move

00:06:48,000 --> 00:06:52,960
from the dba to the developer and own

00:06:51,039 --> 00:06:55,680
sort of that individual or microservice

00:06:52,960 --> 00:06:55,680
or application

00:06:55,759 --> 00:06:58,800
so let's get into some of the

00:06:56,800 --> 00:07:02,400
differences between cassandra and

00:06:58,800 --> 00:07:04,000
and and sql or relational that is so you

00:07:02,400 --> 00:07:06,160
first off you have a partition key which

00:07:04,000 --> 00:07:06,720
is really critical to being successful

00:07:06,160 --> 00:07:08,880
in

00:07:06,720 --> 00:07:11,039
in nosql most of the times when i you

00:07:08,880 --> 00:07:13,520
see issues with cassandra's model

00:07:11,039 --> 00:07:15,440
it's usually here with the partition key

00:07:13,520 --> 00:07:17,520
right so participate is responsible for

00:07:15,440 --> 00:07:19,120
distributing the data and the access

00:07:17,520 --> 00:07:21,520
you want to choose something that has a

00:07:19,120 --> 00:07:22,400
high cardinality this way that your data

00:07:21,520 --> 00:07:24,720
is distributed

00:07:22,400 --> 00:07:26,080
but then also your data is accessed in a

00:07:24,720 --> 00:07:27,840
distributed way

00:07:26,080 --> 00:07:29,120
so it's key point there's a different

00:07:27,840 --> 00:07:30,720
fundamental difference between

00:07:29,120 --> 00:07:34,240
description of data and distributive

00:07:30,720 --> 00:07:34,240
access is the two different concerns

00:07:34,319 --> 00:07:37,120
from a partition key when you're

00:07:35,520 --> 00:07:38,639
querying it's required to provide the

00:07:37,120 --> 00:07:40,960
partitioning key now the partition key

00:07:38,639 --> 00:07:42,560
can be made of one or multiple columns

00:07:40,960 --> 00:07:44,879
so when you go ahead and you start

00:07:42,560 --> 00:07:45,759
querying uh your table you're going to

00:07:44,879 --> 00:07:47,520
need to provide

00:07:45,759 --> 00:07:48,879
those columns and both of those columns

00:07:47,520 --> 00:07:51,280
kind of represent or

00:07:48,879 --> 00:07:53,599
have additive properties to that

00:07:51,280 --> 00:07:56,400
cardinality that you choose so

00:07:53,599 --> 00:07:58,720
uh maybe customer is not that there's

00:07:56,400 --> 00:08:00,639
not that many values but then customer

00:07:58,720 --> 00:08:01,840
count combination provides you with a

00:08:00,639 --> 00:08:03,280
lot of values which

00:08:01,840 --> 00:08:07,120
means that most of your data is going to

00:08:03,280 --> 00:08:08,800
be distributed among those partitions

00:08:07,120 --> 00:08:10,560
now the partition is sort of like a

00:08:08,800 --> 00:08:11,840
bucket it contains many objects there's

00:08:10,560 --> 00:08:14,879
many rows within

00:08:11,840 --> 00:08:18,000
within a partition and that partition

00:08:14,879 --> 00:08:19,280
is sorted so you provide an optional

00:08:18,000 --> 00:08:22,240
clustering key

00:08:19,280 --> 00:08:23,039
which sorts your your data based on the

00:08:22,240 --> 00:08:24,639
columns

00:08:23,039 --> 00:08:27,120
and if you have multiple clustering

00:08:24,639 --> 00:08:29,280
columns there is a nested relationship

00:08:27,120 --> 00:08:30,560
there so first column is sorted then

00:08:29,280 --> 00:08:32,479
it's a hierarchy of

00:08:30,560 --> 00:08:34,159
of storing multiple columns within each

00:08:32,479 --> 00:08:35,599
other and that's where you start getting

00:08:34,159 --> 00:08:40,320
this wide row

00:08:35,599 --> 00:08:42,080
wide column sort of construct

00:08:40,320 --> 00:08:43,440
but it also allows you to do complex

00:08:42,080 --> 00:08:45,120
queries you know you want to be able to

00:08:43,440 --> 00:08:45,839
grab all the data you want in a single

00:08:45,120 --> 00:08:48,399
query

00:08:45,839 --> 00:08:49,680
so the ability to add different sort

00:08:48,399 --> 00:08:52,320
keys and new indexes

00:08:49,680 --> 00:08:54,240
into your partition allows you to then

00:08:52,320 --> 00:08:56,320
go ahead and start doing more complex

00:08:54,240 --> 00:08:59,040
queries not just equality

00:08:56,320 --> 00:08:59,519
or not equal or inequalities but then

00:08:59,040 --> 00:09:01,760
also

00:08:59,519 --> 00:09:03,680
range queries and be able to scan

00:09:01,760 --> 00:09:04,399
multiple rows return multiple result

00:09:03,680 --> 00:09:07,040
sets

00:09:04,399 --> 00:09:10,240
and really start doing more complex

00:09:07,040 --> 00:09:11,680
things than just the key value lookups

00:09:10,240 --> 00:09:12,959
so you go ahead same with the select

00:09:11,680 --> 00:09:13,760
statement you're going to go ahead and

00:09:12,959 --> 00:09:15,920
select

00:09:13,760 --> 00:09:17,040
from a table provide that partition key

00:09:15,920 --> 00:09:18,640
because it's required

00:09:17,040 --> 00:09:20,480
but then you're going to have to you

00:09:18,640 --> 00:09:21,839
know identify different clustering keys

00:09:20,480 --> 00:09:23,279
now you don't need to provide them all

00:09:21,839 --> 00:09:24,640
you just need to provide

00:09:23,279 --> 00:09:26,080
some of them or really you don't have to

00:09:24,640 --> 00:09:27,760
private if you don't provide any of them

00:09:26,080 --> 00:09:30,080
you'll return all the rows but if you're

00:09:27,760 --> 00:09:31,519
you start you know in this case you know

00:09:30,080 --> 00:09:34,640
providing an mdn will

00:09:31,519 --> 00:09:35,760
bring me into a set of rows related to

00:09:34,640 --> 00:09:37,200
that mdn

00:09:35,760 --> 00:09:40,000
and then i can then go ahead and do a

00:09:37,200 --> 00:09:42,160
range query on that bill cycle so i can

00:09:40,000 --> 00:09:44,160
return a range of bills for that

00:09:42,160 --> 00:09:46,480
phone number within my customer and

00:09:44,160 --> 00:09:46,480
account

00:09:46,560 --> 00:09:50,000
non-key columns are uh you know there's

00:09:48,880 --> 00:09:52,240
not just uh

00:09:50,000 --> 00:09:53,839
uh something that you define as a

00:09:52,240 --> 00:09:56,160
structure but they also have

00:09:53,839 --> 00:09:58,240
a particular purpose in cassandra as

00:09:56,160 --> 00:10:00,240
well i mean it's important to validate

00:09:58,240 --> 00:10:02,160
that your rose and the schema is similar

00:10:00,240 --> 00:10:04,880
so that you can centralize

00:10:02,160 --> 00:10:06,480
and govern the the data model for many

00:10:04,880 --> 00:10:07,839
different applications of many different

00:10:06,480 --> 00:10:09,920
apis

00:10:07,839 --> 00:10:11,839
but what you're able to do with non-key

00:10:09,920 --> 00:10:13,040
column is actually project data so then

00:10:11,839 --> 00:10:14,959
you can only return

00:10:13,040 --> 00:10:17,040
a certain amount of fields if i just

00:10:14,959 --> 00:10:20,079
want to return name and total charges

00:10:17,040 --> 00:10:22,399
i can return a smaller result set

00:10:20,079 --> 00:10:23,519
but additionally you can also filter on

00:10:22,399 --> 00:10:25,279
those data so

00:10:23,519 --> 00:10:27,200
you know i'll fill out filtering gets a

00:10:25,279 --> 00:10:29,279
bad wrap in cassandra but really when

00:10:27,200 --> 00:10:30,000
you're doing it against a smaller data

00:10:29,279 --> 00:10:31,360
set

00:10:30,000 --> 00:10:33,440
which you've already gone home and

00:10:31,360 --> 00:10:35,040
trigger and minimize the results with

00:10:33,440 --> 00:10:35,680
your partition key and your clustering

00:10:35,040 --> 00:10:37,920
key

00:10:35,680 --> 00:10:39,920
allow filtering is a nice uh feature to

00:10:37,920 --> 00:10:43,680
allow you to minimize that data set

00:10:39,920 --> 00:10:45,440
returned to your application

00:10:43,680 --> 00:10:47,120
so what you know now that we've designed

00:10:45,440 --> 00:10:49,040
those constructs now we want to go into

00:10:47,120 --> 00:10:51,519
a little bit of challenges that we see

00:10:49,040 --> 00:10:52,959
obviously we've mentioned the partition

00:10:51,519 --> 00:10:55,120
key and choosing the right key that's

00:10:52,959 --> 00:10:57,200
high cardinality it's very important but

00:10:55,120 --> 00:11:00,000
what you really have is this trade-off

00:10:57,200 --> 00:11:02,000
or this conflict between determining uh

00:11:00,000 --> 00:11:03,600
a partition key and what should go in my

00:11:02,000 --> 00:11:05,519
clustering keys and how much

00:11:03,600 --> 00:11:07,040
data should i read in a per select or

00:11:05,519 --> 00:11:08,480
per query

00:11:07,040 --> 00:11:10,000
so we have the you know we want to

00:11:08,480 --> 00:11:11,600
distribute our data over multiple

00:11:10,000 --> 00:11:13,600
partitions to you know

00:11:11,600 --> 00:11:15,360
improve the scale the distribution and

00:11:13,600 --> 00:11:17,440
utilization of our hardware

00:11:15,360 --> 00:11:18,720
but we also want to minimize the

00:11:17,440 --> 00:11:19,600
complexity of our client-side

00:11:18,720 --> 00:11:21,760
application

00:11:19,600 --> 00:11:23,360
we don't want to provide multiple

00:11:21,760 --> 00:11:25,360
partition lookups we want to do a single

00:11:23,360 --> 00:11:27,680
partition lookup return all my data

00:11:25,360 --> 00:11:28,959
let the database handle the retrieval of

00:11:27,680 --> 00:11:31,360
that information

00:11:28,959 --> 00:11:33,120
we also want to go ahead and you know

00:11:31,360 --> 00:11:34,720
after we got our application working

00:11:33,120 --> 00:11:36,079
the secondary sort of thing is like all

00:11:34,720 --> 00:11:37,040
right now let's make it more cost

00:11:36,079 --> 00:11:40,160
affordable

00:11:37,040 --> 00:11:40,880
more cost reduce cost and start taking

00:11:40,160 --> 00:11:42,240
that you know

00:11:40,880 --> 00:11:44,640
cost and start using it to other

00:11:42,240 --> 00:11:46,240
projects right so one of the ways we

00:11:44,640 --> 00:11:47,839
look at doing this is reducing the

00:11:46,240 --> 00:11:50,160
number of write operations

00:11:47,839 --> 00:11:51,519
increasing the size of your row and

00:11:50,160 --> 00:11:53,200
although you know you wanted to

00:11:51,519 --> 00:11:53,839
normalize your data and to normalize

00:11:53,200 --> 00:11:56,480
your model

00:11:53,839 --> 00:11:58,320
look for places where you can sort of

00:11:56,480 --> 00:12:00,320
normalize your data within a row or

00:11:58,320 --> 00:12:03,519
within a partition or within a column

00:12:00,320 --> 00:12:06,079
clustering key so that you can do more

00:12:03,519 --> 00:12:06,079
with less

00:12:07,760 --> 00:12:11,040
one of the fundamental things to get

00:12:09,120 --> 00:12:13,040
right with cassandra is when you look at

00:12:11,040 --> 00:12:14,160
your results you may think of it as rows

00:12:13,040 --> 00:12:16,240
and columns and

00:12:14,160 --> 00:12:18,000
this looks like a common result set that

00:12:16,240 --> 00:12:18,639
i would get with a relational database

00:12:18,000 --> 00:12:21,920
but

00:12:18,639 --> 00:12:23,279
in actuality your model looks more like

00:12:21,920 --> 00:12:24,000
this and you can think about your model

00:12:23,279 --> 00:12:25,760
as a hierarchy

00:12:24,000 --> 00:12:28,000
which which really works out well with

00:12:25,760 --> 00:12:29,920
oltp applications because a lot of the

00:12:28,000 --> 00:12:32,480
business functions that we model

00:12:29,920 --> 00:12:34,560
are actually hierarchies in themselves

00:12:32,480 --> 00:12:36,720
so if you look at this this is what the

00:12:34,560 --> 00:12:38,079
path the query needs to traverse to read

00:12:36,720 --> 00:12:41,200
that data and if you think about

00:12:38,079 --> 00:12:42,720
that it makes it a little bit more

00:12:41,200 --> 00:12:45,120
palatable to start to

00:12:42,720 --> 00:12:47,279
you know designing more complex queries

00:12:45,120 --> 00:12:49,600
within your clustering keys

00:12:47,279 --> 00:12:50,880
so but one thing to get right is that

00:12:49,600 --> 00:12:52,079
you know you want to provide

00:12:50,880 --> 00:12:53,519
you know school in this in this

00:12:52,079 --> 00:12:55,120
situation you have a school that has

00:12:53,519 --> 00:12:56,800
multiple departments departments have

00:12:55,120 --> 00:12:57,680
multiple teachers teachers have multiple

00:12:56,800 --> 00:12:59,519
classes

00:12:57,680 --> 00:13:01,040
classes have multiple semesters over

00:12:59,519 --> 00:13:01,680
time and those semesters and classes

00:13:01,040 --> 00:13:03,360
have

00:13:01,680 --> 00:13:05,040
uh you know a number of different

00:13:03,360 --> 00:13:08,560
students with uh you know

00:13:05,040 --> 00:13:10,480
different properties to those students

00:13:08,560 --> 00:13:12,079
so let's say i want to determine what

00:13:10,480 --> 00:13:13,920
should i have as my partition key and

00:13:12,079 --> 00:13:16,800
when my clustering key and what can i do

00:13:13,920 --> 00:13:17,680
to help improve one or the other well we

00:13:16,800 --> 00:13:19,519
kind of like to say

00:13:17,680 --> 00:13:20,959
in terms of shifting left or shifting

00:13:19,519 --> 00:13:23,200
right so shift left

00:13:20,959 --> 00:13:25,279
is a concept of moving your clustering

00:13:23,200 --> 00:13:26,880
key into your partition key

00:13:25,279 --> 00:13:29,040
and when you do this you're essentially

00:13:26,880 --> 00:13:30,639
naturally giving yourself more possible

00:13:29,040 --> 00:13:31,680
values which is going to lead in better

00:13:30,639 --> 00:13:34,480
distribution

00:13:31,680 --> 00:13:36,320
of your data so then you're evenly more

00:13:34,480 --> 00:13:37,360
evenly distributed your access is more

00:13:36,320 --> 00:13:39,120
random

00:13:37,360 --> 00:13:40,639
and your the value you get out of your

00:13:39,120 --> 00:13:41,760
hardware is going to be exponentially

00:13:40,639 --> 00:13:42,800
greater and the throughput that you get

00:13:41,760 --> 00:13:45,440
out of your cluster is going to be

00:13:42,800 --> 00:13:47,279
exponentially better

00:13:45,440 --> 00:13:49,199
but we also have situations where we

00:13:47,279 --> 00:13:51,040
need to improve the query capabilities

00:13:49,199 --> 00:13:52,880
and reduce the complex logic in the

00:13:51,040 --> 00:13:54,800
client side to send go ahead and order

00:13:52,880 --> 00:13:56,800
all these different partition lookups

00:13:54,800 --> 00:13:58,639
so we want to go ahead and shift right

00:13:56,800 --> 00:14:00,000
and when we shift right now we're moving

00:13:58,639 --> 00:14:02,160
data from our

00:14:00,000 --> 00:14:04,000
partition key to our clustering column

00:14:02,160 --> 00:14:05,279
right now what we've done is we have

00:14:04,000 --> 00:14:07,600
more relationships

00:14:05,279 --> 00:14:09,440
within this hierarchy so if school is

00:14:07,600 --> 00:14:11,600
not accessed often or there are enough

00:14:09,440 --> 00:14:13,040
schools to provide even distribution

00:14:11,600 --> 00:14:15,040
then i may want to keep all the

00:14:13,040 --> 00:14:16,480
departments teachers and classes etc

00:14:15,040 --> 00:14:17,839
within my clustering key

00:14:16,480 --> 00:14:19,440
this way i can provide a lot of

00:14:17,839 --> 00:14:20,160
different metrics and aggregations of

00:14:19,440 --> 00:14:23,040
data

00:14:20,160 --> 00:14:23,839
on that clustering key with a single

00:14:23,040 --> 00:14:25,680
query

00:14:23,839 --> 00:14:28,639
and really provide you know consistent

00:14:25,680 --> 00:14:28,639
lookup latencies

00:14:29,519 --> 00:14:32,480
so one of the challenges like we've

00:14:30,959 --> 00:14:33,839
mentioned is hotkey right we'll hear

00:14:32,480 --> 00:14:34,320
this a lot and you'll get a lot of this

00:14:33,839 --> 00:14:36,320
with

00:14:34,320 --> 00:14:37,600
dealing with cassandra it's common

00:14:36,320 --> 00:14:39,839
across

00:14:37,600 --> 00:14:41,440
most environments because it really

00:14:39,839 --> 00:14:42,079
really a factor of how you model your

00:14:41,440 --> 00:14:43,839
data

00:14:42,079 --> 00:14:45,600
in terms of your understanding your

00:14:43,839 --> 00:14:47,920
business application

00:14:45,600 --> 00:14:49,680
so when you have a distributed access

00:14:47,920 --> 00:14:51,279
when you see more one server is getting

00:14:49,680 --> 00:14:53,120
more requests than the other

00:14:51,279 --> 00:14:54,480
that server is then utilized more than

00:14:53,120 --> 00:14:56,399
others and that means that

00:14:54,480 --> 00:14:58,000
it has different maybe performance

00:14:56,399 --> 00:14:58,800
characteristics it may be a little bit

00:14:58,000 --> 00:15:01,839
slower

00:14:58,800 --> 00:15:04,959
you know it may be um you may

00:15:01,839 --> 00:15:07,920
have situations where uh your entire

00:15:04,959 --> 00:15:09,519
cluster is kind of uh based on the you

00:15:07,920 --> 00:15:10,160
know single performance of this node

00:15:09,519 --> 00:15:12,000
because

00:15:10,160 --> 00:15:14,639
uh transactions are dependent on each

00:15:12,000 --> 00:15:17,360
other so what you want to do is not have

00:15:14,639 --> 00:15:19,680
it all skewed against one partition

00:15:17,360 --> 00:15:21,680
but you want to go ahead and have

00:15:19,680 --> 00:15:23,279
provide even distribution across

00:15:21,680 --> 00:15:25,120
those clusters so your number of

00:15:23,279 --> 00:15:26,720
requests per second is evenly

00:15:25,120 --> 00:15:28,320
distributed and we find that there's

00:15:26,720 --> 00:15:29,279
essentially three different methods that

00:15:28,320 --> 00:15:31,519
you can do

00:15:29,279 --> 00:15:33,759
to help solve this problem and provide

00:15:31,519 --> 00:15:35,279
the the least amount of code changes in

00:15:33,759 --> 00:15:37,519
solving that problem right

00:15:35,279 --> 00:15:38,959
so that's improving the cardinality of

00:15:37,519 --> 00:15:40,880
your model or your key

00:15:38,959 --> 00:15:42,800
by improving you know inserting new

00:15:40,880 --> 00:15:44,480
fields into that cardinality

00:15:42,800 --> 00:15:46,720
and some other advanced methods we'll

00:15:44,480 --> 00:15:48,639
get into a little bit later

00:15:46,720 --> 00:15:50,959
randomizing the access a lot of times if

00:15:48,639 --> 00:15:52,800
you're having a set order of queries and

00:15:50,959 --> 00:15:54,160
you're going ahead and fire those order

00:15:52,800 --> 00:15:55,920
all at the same time a lot of times

00:15:54,160 --> 00:15:57,519
those ordered queries or their ordered

00:15:55,920 --> 00:15:58,639
parameters can end up being hit on the

00:15:57,519 --> 00:16:00,639
same partition

00:15:58,639 --> 00:16:01,920
so if you randomize that query set

00:16:00,639 --> 00:16:04,240
before you

00:16:01,920 --> 00:16:06,160
uh go ahead and access your data a lot

00:16:04,240 --> 00:16:08,720
of times that even distribution or

00:16:06,160 --> 00:16:10,639
of that queries can provide a more

00:16:08,720 --> 00:16:13,120
random distribution against your cluster

00:16:10,639 --> 00:16:15,120
so you get better throughput this is and

00:16:13,120 --> 00:16:16,560
then the other option is to use time

00:16:15,120 --> 00:16:18,800
right so you

00:16:16,560 --> 00:16:20,079
plan out your queries over time if the

00:16:18,800 --> 00:16:22,399
client doesn't need the

00:16:20,079 --> 00:16:23,680
sub you know the millisecond latency of

00:16:22,399 --> 00:16:25,440
cassandra provides

00:16:23,680 --> 00:16:27,040
and you can deal with seconds then you

00:16:25,440 --> 00:16:29,759
know rate limit and

00:16:27,040 --> 00:16:30,880
for you know b do q level loading or q

00:16:29,759 --> 00:16:32,160
level querying

00:16:30,880 --> 00:16:34,639
just go ahead and even out that

00:16:32,160 --> 00:16:36,079
distribution and the benefits of this

00:16:34,639 --> 00:16:38,320
distribution is that you get a

00:16:36,079 --> 00:16:38,880
consistent utilization across your

00:16:38,320 --> 00:16:41,040
servers

00:16:38,880 --> 00:16:42,560
so that when you scale up and down you

00:16:41,040 --> 00:16:45,519
know your performance is going to be

00:16:42,560 --> 00:16:45,519
consistent as well

00:16:46,320 --> 00:16:49,920
so let's talk about uh ways that we

00:16:48,320 --> 00:16:52,399
could add cardinality what does it look

00:16:49,920 --> 00:16:54,480
like so if we take this iot use case

00:16:52,399 --> 00:16:56,240
we're looking at you know a customer

00:16:54,480 --> 00:16:57,440
account has many phone numbers they're

00:16:56,240 --> 00:16:59,759
receiving

00:16:57,440 --> 00:17:01,680
many messages per second right that's

00:16:59,759 --> 00:17:03,680
not really that big of a problem when

00:17:01,680 --> 00:17:05,039
it's only a few phone numbers or keys

00:17:03,680 --> 00:17:06,240
per account but then when you have a

00:17:05,039 --> 00:17:08,400
business user and that

00:17:06,240 --> 00:17:09,600
may grow to millions and millions of uh

00:17:08,400 --> 00:17:12,880
of not millions

00:17:09,600 --> 00:17:13,600
uh maybe uh thousands of of phone

00:17:12,880 --> 00:17:15,600
numbers per

00:17:13,600 --> 00:17:16,799
partition and then all of the message

00:17:15,600 --> 00:17:20,319
that they're receiving you could start

00:17:16,799 --> 00:17:21,679
getting into hotkey situations

00:17:20,319 --> 00:17:24,000
what you want to do is shift left from

00:17:21,679 --> 00:17:27,439
your customer in columns

00:17:24,000 --> 00:17:29,360
and now provide uh move that mdn uh into

00:17:27,439 --> 00:17:30,880
the clustering key now you're you're

00:17:29,360 --> 00:17:33,520
receiving uh

00:17:30,880 --> 00:17:35,280
you know messages per partition on only

00:17:33,520 --> 00:17:36,960
a single mdn which is going to inc

00:17:35,280 --> 00:17:38,880
distribute your data evenly

00:17:36,960 --> 00:17:40,400
and then it's going to you know allow

00:17:38,880 --> 00:17:42,640
you greater throughput

00:17:40,400 --> 00:17:44,400
uh overall for that account because it

00:17:42,640 --> 00:17:48,320
is now based on several different

00:17:44,400 --> 00:17:48,320
uh servers

00:17:50,640 --> 00:17:55,520
another the other another issue is

00:17:53,360 --> 00:17:57,440
reducing the number of partitions read

00:17:55,520 --> 00:17:59,520
now this is the counter problem to

00:17:57,440 --> 00:18:00,559
now you know creating more partitions

00:17:59,520 --> 00:18:02,160
for your key

00:18:00,559 --> 00:18:03,760
now you have all of these partitions

00:18:02,160 --> 00:18:05,280
which you used to access with a single

00:18:03,760 --> 00:18:05,760
query now you need to access many

00:18:05,280 --> 00:18:08,000
different

00:18:05,760 --> 00:18:09,679
partitions to grab that data and there's

00:18:08,000 --> 00:18:10,240
two essential methods to try to solve

00:18:09,679 --> 00:18:12,400
this one

00:18:10,240 --> 00:18:14,080
is there's a server sideway so you go

00:18:12,400 --> 00:18:15,440
ahead and you know do allow filtering

00:18:14,080 --> 00:18:16,400
which is sort of an anti-pattern of

00:18:15,440 --> 00:18:18,640
cassandra

00:18:16,400 --> 00:18:20,559
or you do a cql in operation which is

00:18:18,640 --> 00:18:21,760
sort of an anti-pattern as you get to

00:18:20,559 --> 00:18:24,640
more

00:18:21,760 --> 00:18:26,559
operands in your in statement what

00:18:24,640 --> 00:18:28,080
happens is that you know your latencies

00:18:26,559 --> 00:18:29,600
increase and your availability

00:18:28,080 --> 00:18:32,000
drops because now you've involved the

00:18:29,600 --> 00:18:34,000
query that goes that reaches multiple

00:18:32,000 --> 00:18:35,200
physical hosts which then have to be

00:18:34,000 --> 00:18:36,400
then

00:18:35,200 --> 00:18:38,000
you know have a higher chance of

00:18:36,400 --> 00:18:40,080
availability when you go across all of

00:18:38,000 --> 00:18:42,160
those hosts

00:18:40,080 --> 00:18:43,760
and the other option is to do parallel

00:18:42,160 --> 00:18:46,640
requests from the client

00:18:43,760 --> 00:18:48,240
now this is uh gets you and satisfies

00:18:46,640 --> 00:18:52,080
the throughput constraints

00:18:48,240 --> 00:18:52,640
um of your query but there is a awful

00:18:52,080 --> 00:18:55,200
number of

00:18:52,640 --> 00:18:55,760
a lot of complexity deal from the client

00:18:55,200 --> 00:18:58,400
side

00:18:55,760 --> 00:18:59,679
to go ahead and then wrap all of these

00:18:58,400 --> 00:19:01,520
statements together

00:18:59,679 --> 00:19:03,120
uh you know do advanced things like

00:19:01,520 --> 00:19:04,880
pagination uh

00:19:03,120 --> 00:19:07,200
it takes a lot of custom work to go

00:19:04,880 --> 00:19:09,360
ahead and do parallel requests from

00:19:07,200 --> 00:19:10,880
uh to a multi-partition node so really

00:19:09,360 --> 00:19:13,120
the answer is to

00:19:10,880 --> 00:19:14,799
really reduce both of that problem and

00:19:13,120 --> 00:19:18,880
try to improve

00:19:14,799 --> 00:19:20,960
your clustering key and partition key

00:19:18,880 --> 00:19:23,600
configuration so that your model can

00:19:20,960 --> 00:19:26,880
handle a sort of best of both worlds

00:19:23,600 --> 00:19:28,080
in that situation so let's take a look

00:19:26,880 --> 00:19:28,640
at a shift right now we're going to move

00:19:28,080 --> 00:19:31,679
a column

00:19:28,640 --> 00:19:34,320
into clustering keys so if we take

00:19:31,679 --> 00:19:35,840
a similar use case but instead of an iot

00:19:34,320 --> 00:19:37,039
use case this is uh looking up the

00:19:35,840 --> 00:19:38,880
customer bill

00:19:37,039 --> 00:19:40,080
so this may happen once a month for a

00:19:38,880 --> 00:19:42,320
customer account

00:19:40,080 --> 00:19:44,720
not that heavy of a throughput even even

00:19:42,320 --> 00:19:47,200
as the number of mdns scale up

00:19:44,720 --> 00:19:48,720
so we can actually keep that in the

00:19:47,200 --> 00:19:51,200
partition key but if we want to do

00:19:48,720 --> 00:19:52,000
aggregations across multiple mdns across

00:19:51,200 --> 00:19:54,320
account

00:19:52,000 --> 00:19:56,080
it makes more sense to put that into the

00:19:54,320 --> 00:19:58,160
clustering column where now i can do an

00:19:56,080 --> 00:20:00,480
aggregation across my entire account

00:19:58,160 --> 00:20:02,400
grab all my data in a single query not

00:20:00,480 --> 00:20:03,919
have to do a complex in operation not

00:20:02,400 --> 00:20:06,320
have to do a complex uh

00:20:03,919 --> 00:20:07,919
server client-side sort of uh

00:20:06,320 --> 00:20:09,840
aggregation of that data

00:20:07,919 --> 00:20:13,039
you simply have a simple query and it

00:20:09,840 --> 00:20:14,799
works well with your partition key

00:20:13,039 --> 00:20:16,880
but there are often times where those

00:20:14,799 --> 00:20:19,600
that conflict is very real and

00:20:16,880 --> 00:20:20,320
it's very hard to avoid that situation

00:20:19,600 --> 00:20:22,240
so

00:20:20,320 --> 00:20:24,240
the common practice advanced modeling

00:20:22,240 --> 00:20:24,880
practice within cassandra is to go ahead

00:20:24,240 --> 00:20:28,080
and shard

00:20:24,880 --> 00:20:30,880
or go ahead and bucket that access so

00:20:28,080 --> 00:20:32,880
sharding is to be artificially create

00:20:30,880 --> 00:20:36,720
more partitions

00:20:32,880 --> 00:20:38,559
based on the values of another

00:20:36,720 --> 00:20:40,880
set of data so if you're inserting a

00:20:38,559 --> 00:20:43,360
clustering column with a phone number

00:20:40,880 --> 00:20:45,520
you may want to shard on the area code

00:20:43,360 --> 00:20:46,960
and you can bucket all of the area codes

00:20:45,520 --> 00:20:48,960
in the same partition

00:20:46,960 --> 00:20:50,720
but for all of your numbers you get many

00:20:48,960 --> 00:20:54,159
area codes that means you have many

00:20:50,720 --> 00:20:54,720
uh uh partitions for that customer

00:20:54,159 --> 00:20:57,679
account

00:20:54,720 --> 00:20:58,480
but not as many partitions as you do

00:20:57,679 --> 00:21:00,240
phone numbers

00:20:58,480 --> 00:21:02,240
so it's sort of the sliding scale that

00:21:00,240 --> 00:21:05,600
you have to trade off between

00:21:02,240 --> 00:21:06,400
partitioning and clustering clustering

00:21:05,600 --> 00:21:07,840
keys

00:21:06,400 --> 00:21:09,360
so here's some common methods that you

00:21:07,840 --> 00:21:10,960
can use for sharding which i find

00:21:09,360 --> 00:21:12,640
effective because you often have to

00:21:10,960 --> 00:21:14,320
change your sharding method based on the

00:21:12,640 --> 00:21:16,559
business and the business opera the

00:21:14,320 --> 00:21:18,159
business requirements so one is this

00:21:16,559 --> 00:21:19,760
essentially take a substring like we did

00:21:18,159 --> 00:21:20,480
before take the area code have a phone

00:21:19,760 --> 00:21:22,320
number

00:21:20,480 --> 00:21:24,720
there's more complex things like you

00:21:22,320 --> 00:21:26,400
know trimming and reversing the website

00:21:24,720 --> 00:21:27,200
so that they're ordered in a way that

00:21:26,400 --> 00:21:29,200
you can

00:21:27,200 --> 00:21:30,640
uh you know have better distribution

00:21:29,200 --> 00:21:32,640
based on that order

00:21:30,640 --> 00:21:34,559
you can use the you know subsets of like

00:21:32,640 --> 00:21:35,039
an ip address or a device to get you

00:21:34,559 --> 00:21:38,080
know

00:21:35,039 --> 00:21:41,280
a fixed number of partitions

00:21:38,080 --> 00:21:43,440
but based on a piece of data that you're

00:21:41,280 --> 00:21:45,760
inserting

00:21:43,440 --> 00:21:47,919
sharding by range so if you have a limit

00:21:45,760 --> 00:21:48,400
to the number of accounts you have in a

00:21:47,919 --> 00:21:50,080
in a

00:21:48,400 --> 00:21:51,600
in a number of phone numbers within an

00:21:50,080 --> 00:21:52,240
account maybe the number of phone

00:21:51,600 --> 00:21:54,080
numbers

00:21:52,240 --> 00:21:56,559
divided by n shards and now you have a

00:21:54,080 --> 00:21:59,280
shard a sharded range by n shards so if

00:21:56,559 --> 00:21:59,919
you took a thousand mdns you sharded it

00:21:59,280 --> 00:22:02,400
by

00:21:59,919 --> 00:22:03,919
uh you know 12 then what you have is

00:22:02,400 --> 00:22:06,080
something like 83

00:22:03,919 --> 00:22:07,679
per range which gives you a nice bucket

00:22:06,080 --> 00:22:11,360
to go ahead and

00:22:07,679 --> 00:22:13,440
start querying uh buckets of partitions

00:22:11,360 --> 00:22:14,960
but you're not as distributed and you're

00:22:13,440 --> 00:22:17,760
not querying you know thousands of

00:22:14,960 --> 00:22:19,440
partitions to get your answer

00:22:17,760 --> 00:22:21,919
sharding by hash this is another common

00:22:19,440 --> 00:22:24,080
method just take the hash code of a

00:22:21,919 --> 00:22:25,919
value and then bucket it modulo by some

00:22:24,080 --> 00:22:27,520
value number of shards that you want and

00:22:25,919 --> 00:22:28,960
then starting by time this is

00:22:27,520 --> 00:22:30,559
starting by time helps you more with

00:22:28,960 --> 00:22:32,080
distributing data not as much as

00:22:30,559 --> 00:22:33,440
distributing access because usually

00:22:32,080 --> 00:22:34,720
you're always inserting the latest

00:22:33,440 --> 00:22:36,720
information anyway so

00:22:34,720 --> 00:22:38,159
a lot of times sharding by time will

00:22:36,720 --> 00:22:40,720
help you on the

00:22:38,159 --> 00:22:42,320
read side but only if your you know your

00:22:40,720 --> 00:22:43,679
queries end up spanning you know

00:22:42,320 --> 00:22:46,559
multiple days or

00:22:43,679 --> 00:22:48,480
or weeks months etc like that um both

00:22:46,559 --> 00:22:48,799
sharding by time and charting by range

00:22:48,480 --> 00:22:50,320
are

00:22:48,799 --> 00:22:52,640
are model partitioning models that

00:22:50,320 --> 00:22:54,000
continue to grow so it continues to grow

00:22:52,640 --> 00:22:57,600
and you can create more shards

00:22:54,000 --> 00:22:59,760
as time goes on which is also nice

00:22:57,600 --> 00:23:01,280
all right and third and final option we

00:22:59,760 --> 00:23:03,039
talked about hot keys we talked about

00:23:01,280 --> 00:23:04,400
you know improving select or reducing

00:23:03,039 --> 00:23:06,960
partition keys now we just want to

00:23:04,400 --> 00:23:08,240
do we want to talk about reducing cost

00:23:06,960 --> 00:23:09,840
so take a look at this

00:23:08,240 --> 00:23:11,520
we're going to you know we'll look at

00:23:09,840 --> 00:23:11,840
the number of operations generally we

00:23:11,520 --> 00:23:14,000
know

00:23:11,840 --> 00:23:15,840
cassandra is right optimized and we want

00:23:14,000 --> 00:23:17,360
to you know insert a lot of data

00:23:15,840 --> 00:23:19,360
we don't care how many writes we do but

00:23:17,360 --> 00:23:20,960
that's not necessarily true

00:23:19,360 --> 00:23:22,720
a lot of times the primary key can be

00:23:20,960 --> 00:23:24,720
the biggest component of your row

00:23:22,720 --> 00:23:26,400
and that you're small it's throwing very

00:23:24,720 --> 00:23:27,200
many small rows we find this all the

00:23:26,400 --> 00:23:29,440
time

00:23:27,200 --> 00:23:31,520
so what we can do is start reducing the

00:23:29,440 --> 00:23:32,240
number of operations we can reduce the

00:23:31,520 --> 00:23:35,280
size of

00:23:32,240 --> 00:23:36,960
the data stored and the data accessed by

00:23:35,280 --> 00:23:39,919
essentially compressing down

00:23:36,960 --> 00:23:42,559
rows or making the row size larger and

00:23:39,919 --> 00:23:44,960
reducing the number of operations

00:23:42,559 --> 00:23:45,919
we can also go ahead and start you know

00:23:44,960 --> 00:23:47,840
taking

00:23:45,919 --> 00:23:49,039
similar tables and start combining them

00:23:47,840 --> 00:23:51,760
together

00:23:49,039 --> 00:23:54,720
in a way in a single row that manages

00:23:51,760 --> 00:23:57,120
sort of that entity relationship

00:23:54,720 --> 00:23:59,120
and the way that we do this is we use

00:23:57,120 --> 00:24:00,720
cassandra semi-structured types

00:23:59,120 --> 00:24:02,640
so a lot of people think that cassandra

00:24:00,720 --> 00:24:04,320
is just a fixed structure type

00:24:02,640 --> 00:24:06,400
that you don't have sort of ability to

00:24:04,320 --> 00:24:08,000
do add properties dynamically that's not

00:24:06,400 --> 00:24:09,840
necessarily true

00:24:08,000 --> 00:24:12,000
cassandra supports semi-structured data

00:24:09,840 --> 00:24:13,840
types so you can do map sets and lists

00:24:12,000 --> 00:24:15,360
so you can have custom properties

00:24:13,840 --> 00:24:16,960
you know the good use case for this is a

00:24:15,360 --> 00:24:19,760
product catalog where

00:24:16,960 --> 00:24:21,840
many of the uh each product may have its

00:24:19,760 --> 00:24:24,400
own set of properties that's a great use

00:24:21,840 --> 00:24:26,080
case for a map because you could store

00:24:24,400 --> 00:24:27,440
you know custom properties and all of

00:24:26,080 --> 00:24:29,440
the columns don't have to be the same

00:24:27,440 --> 00:24:30,880
for every row

00:24:29,440 --> 00:24:33,120
so we can go ahead and start compressing

00:24:30,880 --> 00:24:35,679
data even within a row by using

00:24:33,120 --> 00:24:37,279
lists and sets storing nested

00:24:35,679 --> 00:24:39,279
collections within a row

00:24:37,279 --> 00:24:41,840
and you could do contain statements

00:24:39,279 --> 00:24:43,679
against those collections

00:24:41,840 --> 00:24:45,279
as well as with maps and columns so you

00:24:43,679 --> 00:24:47,120
filter them just as you would filter

00:24:45,279 --> 00:24:48,960
a non-key column but this time you're

00:24:47,120 --> 00:24:50,159
going to do things like contains key or

00:24:48,960 --> 00:24:51,760
contains

00:24:50,159 --> 00:24:53,360
and using that allows filter predicate

00:24:51,760 --> 00:24:56,640
to go ahead and filter those

00:24:53,360 --> 00:24:58,000
number of rows so

00:24:56,640 --> 00:24:59,360
how does this work when we go ahead now

00:24:58,000 --> 00:25:00,400
we want to shift right we want to shrink

00:24:59,360 --> 00:25:01,840
the size of the

00:25:00,400 --> 00:25:03,279
rows we go ahead and we look at your

00:25:01,840 --> 00:25:04,880
clustering columns and we say all right

00:25:03,279 --> 00:25:07,279
you have event type

00:25:04,880 --> 00:25:08,400
this is uh you know a number of events

00:25:07,279 --> 00:25:10,080
is you know

00:25:08,400 --> 00:25:11,840
sort of creating this large number of

00:25:10,080 --> 00:25:13,200
rows we want to compress that to small

00:25:11,840 --> 00:25:15,279
number of rows so we go ahead and take

00:25:13,200 --> 00:25:15,760
that event type we go ahead and shift

00:25:15,279 --> 00:25:17,840
right

00:25:15,760 --> 00:25:18,799
we're going to move that into a map and

00:25:17,840 --> 00:25:20,640
what happens is we

00:25:18,799 --> 00:25:22,880
we reduce the size with the number of

00:25:20,640 --> 00:25:25,279
rows we increase the size of the row

00:25:22,880 --> 00:25:26,080
and now we have that event type

00:25:25,279 --> 00:25:28,480
predicated

00:25:26,080 --> 00:25:30,000
on the the different properties within

00:25:28,480 --> 00:25:33,039
that map so now you have

00:25:30,000 --> 00:25:35,919
the cardinality of your event type plus

00:25:33,039 --> 00:25:37,440
its custom field to now give you a lot

00:25:35,919 --> 00:25:40,080
you know a better map which has

00:25:37,440 --> 00:25:41,279
uh creates a larger size row and

00:25:40,080 --> 00:25:43,440
supports

00:25:41,279 --> 00:25:44,960
more dynamic properties for each

00:25:43,440 --> 00:25:46,640
individual events

00:25:44,960 --> 00:25:49,039
so the way that you query this instead

00:25:46,640 --> 00:25:51,760
of doing a a condition

00:25:49,039 --> 00:25:54,320
on the clustering columns what you can

00:25:51,760 --> 00:25:57,039
just do is go ahead and do a condition

00:25:54,320 --> 00:25:58,320
on that map with the cut the

00:25:57,039 --> 00:26:01,679
concatenation

00:25:58,320 --> 00:26:03,039
of the event type and the uh event

00:26:01,679 --> 00:26:05,279
property

00:26:03,039 --> 00:26:06,960
all right so that was uh just a high

00:26:05,279 --> 00:26:09,120
value overview we have a ton of

00:26:06,960 --> 00:26:10,880
information that we're gathering here uh

00:26:09,120 --> 00:26:13,600
working with cassandra users

00:26:10,880 --> 00:26:14,000
making their models better and what i'll

00:26:13,600 --> 00:26:16,000
just

00:26:14,000 --> 00:26:17,840
in review we have different ways to

00:26:16,000 --> 00:26:19,360
handle performance and availability

00:26:17,840 --> 00:26:20,799
concerns with your model distributing

00:26:19,360 --> 00:26:23,840
over multiple partitions

00:26:20,799 --> 00:26:25,440
reducing the number of reads per sorry

00:26:23,840 --> 00:26:27,679
reducing the number of reads or number

00:26:25,440 --> 00:26:29,600
of partitions to complete a read

00:26:27,679 --> 00:26:30,799
and then reducing cost looking at ways

00:26:29,600 --> 00:26:33,919
that you can

00:26:30,799 --> 00:26:36,000
you know reduce cost by combining

00:26:33,919 --> 00:26:37,679
write operations and denormalizing your

00:26:36,000 --> 00:26:39,840
data creating relationships

00:26:37,679 --> 00:26:41,760
with semi-structured data types such as

00:26:39,840 --> 00:26:42,960
maps and lists

00:26:41,760 --> 00:26:45,600
so i'm going to turn it back over to

00:26:42,960 --> 00:26:48,400
arturo for some developer tooling

00:26:45,600 --> 00:26:49,840
and how we can build more cadence and

00:26:48,400 --> 00:26:51,360
experience practical experience with

00:26:49,840 --> 00:26:53,760
cassandra modeling

00:26:51,360 --> 00:26:55,760
with uh our new tooling yeah thank you

00:26:53,760 --> 00:26:57,440
michael that was really good information

00:26:55,760 --> 00:26:58,640
everyone else found that valuable i

00:26:57,440 --> 00:27:00,400
always learned something new whenever i

00:26:58,640 --> 00:27:03,279
listen to michael talk about these

00:27:00,400 --> 00:27:04,480
data modeling techniques um you know one

00:27:03,279 --> 00:27:05,120
of the things that we're really thinking

00:27:04,480 --> 00:27:07,840
about

00:27:05,120 --> 00:27:08,960
at aws is how do we do more to empower

00:27:07,840 --> 00:27:11,840
developers right

00:27:08,960 --> 00:27:14,080
how do we make it easier for both um you

00:27:11,840 --> 00:27:15,440
know an advanced cassandra

00:27:14,080 --> 00:27:18,320
expert that's been working with this

00:27:15,440 --> 00:27:20,240
honor for 10 years to do more

00:27:18,320 --> 00:27:21,840
also how do we make cassandra more

00:27:20,240 --> 00:27:22,799
accessible how do we make it easier for

00:27:21,840 --> 00:27:23,760
developers that are new to

00:27:22,799 --> 00:27:24,960
non-relational

00:27:23,760 --> 00:27:26,799
you know open source technologies like

00:27:24,960 --> 00:27:29,039
asana to get started and

00:27:26,799 --> 00:27:29,919
uh to that end we're excited to announce

00:27:29,039 --> 00:27:33,200
today that we

00:27:29,919 --> 00:27:35,200
are adding apache cassandra support and

00:27:33,200 --> 00:27:38,080
amazon key spaces support

00:27:35,200 --> 00:27:38,559
to the aws nosql workbench and the

00:27:38,080 --> 00:27:40,399
newest

00:27:38,559 --> 00:27:42,159
workbench is a fantastic tool which

00:27:40,399 --> 00:27:44,399
really helps developers build

00:27:42,159 --> 00:27:46,799
much more scalable and performant data

00:27:44,399 --> 00:27:48,960
models you can basically go in there and

00:27:46,799 --> 00:27:50,559
design and visualize the data models

00:27:48,960 --> 00:27:52,960
using a point-and-click interface

00:27:50,559 --> 00:27:53,840
um after before this thing came out i've

00:27:52,960 --> 00:27:55,919
seen some

00:27:53,840 --> 00:27:57,279
some just incredible spreadsheets and

00:27:55,919 --> 00:27:58,880
some really excel

00:27:57,279 --> 00:28:00,640
excel foo that would that would blow

00:27:58,880 --> 00:28:02,320
your mind this makes this

00:28:00,640 --> 00:28:04,159
so much easier with the workbench now

00:28:02,320 --> 00:28:05,039
you can basically you know import a

00:28:04,159 --> 00:28:07,440
schema from a

00:28:05,039 --> 00:28:08,720
from a cluster start manipulating the

00:28:07,440 --> 00:28:10,159
data model to do some of the things that

00:28:08,720 --> 00:28:11,039
michael showed you about shifting left

00:28:10,159 --> 00:28:12,640
and shifting right

00:28:11,039 --> 00:28:14,960
and then you can even commit those

00:28:12,640 --> 00:28:16,720
resources right into your cluster or

00:28:14,960 --> 00:28:18,960
amazon key spaces so

00:28:16,720 --> 00:28:20,399
it's really really easy to use it makes

00:28:18,960 --> 00:28:22,799
cassandra much much easier

00:28:20,399 --> 00:28:24,240
uh to work with and uh and it gets you

00:28:22,799 --> 00:28:25,679
give you a quick demo that i'm going to

00:28:24,240 --> 00:28:27,360
turn it back over to michael to walk

00:28:25,679 --> 00:28:31,840
through how you can use the noah's field

00:28:27,360 --> 00:28:31,840
workbench for a really easy use case

00:28:31,919 --> 00:28:35,120
awesome thank you really excited about

00:28:33,840 --> 00:28:36,799
this and

00:28:35,120 --> 00:28:38,480
uh what i'm going to show you is just a

00:28:36,799 --> 00:28:41,360
demo of the tool of a

00:28:38,480 --> 00:28:42,640
you know similar developer you know

00:28:41,360 --> 00:28:44,880
experience and how

00:28:42,640 --> 00:28:46,320
developers go about using this workbench

00:28:44,880 --> 00:28:47,360
you know improve productivity and

00:28:46,320 --> 00:28:49,279
agility

00:28:47,360 --> 00:28:51,120
with working with cassandra you know

00:28:49,279 --> 00:28:53,360
that a lot of times with cassandra

00:28:51,120 --> 00:28:54,159
you know making model changes you got to

00:28:53,360 --> 00:28:56,080
create the

00:28:54,159 --> 00:28:57,600
you know to create a table in the

00:28:56,080 --> 00:28:59,200
database

00:28:57,600 --> 00:29:00,640
you can't really manipulate the data

00:28:59,200 --> 00:29:02,320
it's hard to see it with data so what

00:29:00,640 --> 00:29:02,799
this tool really helps you do is really

00:29:02,320 --> 00:29:05,919
go ahead

00:29:02,799 --> 00:29:07,440
import share models design and then go

00:29:05,919 --> 00:29:08,240
ahead and commit that back into

00:29:07,440 --> 00:29:10,240
cassandra

00:29:08,240 --> 00:29:11,600
and work in a very collaborative way so

00:29:10,240 --> 00:29:13,840
this is what i'm going to show

00:29:11,600 --> 00:29:14,640
is arturo sent me a model this morning

00:29:13,840 --> 00:29:17,679
i'm going to go

00:29:14,640 --> 00:29:19,360
load it up into the nosql workbench

00:29:17,679 --> 00:29:20,480
i'm going to do some modifications to

00:29:19,360 --> 00:29:23,520
that model and then i'm going to simply

00:29:20,480 --> 00:29:23,520
commit it to cassandra

00:29:23,600 --> 00:29:27,279
so i'm going to jump over to the works

00:29:25,039 --> 00:29:28,080
the workbench um can you see guys see

00:29:27,279 --> 00:29:31,440
the workbench

00:29:28,080 --> 00:29:34,000
everyone out there okay

00:29:31,440 --> 00:29:36,640
i'm gonna hope you do uh we have the

00:29:34,000 --> 00:29:39,440
nosql workbench here

00:29:36,640 --> 00:29:41,679
and on the right-hand side you could see

00:29:39,440 --> 00:29:43,840
that we i'm sorry on the left hand side

00:29:41,679 --> 00:29:45,760
we can see that we have our navigation

00:29:43,840 --> 00:29:46,799
pane where you can go ahead come to the

00:29:45,760 --> 00:29:48,720
home screen

00:29:46,799 --> 00:29:50,320
um go ahead and choose your data model

00:29:48,720 --> 00:29:52,480
this is where you can change

00:29:50,320 --> 00:29:54,320
the the properties and the and the

00:29:52,480 --> 00:29:56,000
structure of your model and then you go

00:29:54,320 --> 00:29:57,520
ahead and visualize that visualize it

00:29:56,000 --> 00:29:59,679
with different data sets

00:29:57,520 --> 00:30:01,360
uh sample data sets that we provide a

00:29:59,679 --> 00:30:03,279
sample a sample

00:30:01,360 --> 00:30:05,279
that you can enter in manually and share

00:30:03,279 --> 00:30:07,520
with your other teammates

00:30:05,279 --> 00:30:09,760
so i could see coming into the dashboard

00:30:07,520 --> 00:30:10,480
that i have my cassandra model already

00:30:09,760 --> 00:30:13,600
loaded

00:30:10,480 --> 00:30:15,760
in here and a model

00:30:13,600 --> 00:30:17,679
is essentially a workspace a model

00:30:15,760 --> 00:30:19,520
contains multiple key spaces and those

00:30:17,679 --> 00:30:21,039
key and also those key spaces contain

00:30:19,520 --> 00:30:22,480
multiple tables

00:30:21,039 --> 00:30:26,799
i can go ahead and import the model

00:30:22,480 --> 00:30:26,799
again which arturo sent me this morning

00:30:29,679 --> 00:30:32,480
and overwrite the existing model and

00:30:31,200 --> 00:30:33,840
it's going to take me right to the data

00:30:32,480 --> 00:30:35,360
model and visualize the

00:30:33,840 --> 00:30:37,919
the modeler so i can start manipulating

00:30:35,360 --> 00:30:39,520
this model the model contains multiple

00:30:37,919 --> 00:30:42,159
key spaces right so i can go ahead and

00:30:39,520 --> 00:30:44,480
choose that key space i can visualize my

00:30:42,159 --> 00:30:45,360
replication strategy and my key space

00:30:44,480 --> 00:30:47,360
settings

00:30:45,360 --> 00:30:49,360
as well as a number of tables that are

00:30:47,360 --> 00:30:51,120
in that key space as well so if i go

00:30:49,360 --> 00:30:52,799
ahead and click the customer

00:30:51,120 --> 00:30:54,240
table i can then view all of the

00:30:52,799 --> 00:30:57,279
properties of that customer

00:30:54,240 --> 00:30:59,200
customer it's schema the what makes a

00:30:57,279 --> 00:31:00,080
partition key what makes clustering

00:30:59,200 --> 00:31:02,720
columns

00:31:00,080 --> 00:31:03,360
uh the types that are in those those for

00:31:02,720 --> 00:31:06,159
those

00:31:03,360 --> 00:31:07,039
uh columns i can change those uh pretty

00:31:06,159 --> 00:31:09,760
quickly to

00:31:07,039 --> 00:31:10,720
to you know change the type of that

00:31:09,760 --> 00:31:12,559
column

00:31:10,720 --> 00:31:14,480
and then i can view my non-key columns

00:31:12,559 --> 00:31:16,240
does it contain any maps does it contain

00:31:14,480 --> 00:31:18,799
blobs et cetera this is very

00:31:16,240 --> 00:31:20,640
nice way to visualize this tool and even

00:31:18,799 --> 00:31:22,480
you know do things like change the order

00:31:20,640 --> 00:31:23,039
and descending order ascending order of

00:31:22,480 --> 00:31:24,960
your

00:31:23,039 --> 00:31:26,799
clustering columns so that you go ahead

00:31:24,960 --> 00:31:28,960
and understand that you can go

00:31:26,799 --> 00:31:29,919
uh you know query data and you're going

00:31:28,960 --> 00:31:32,720
to get results

00:31:29,919 --> 00:31:33,760
in the order that you expect after

00:31:32,720 --> 00:31:35,679
you've gone ahead and

00:31:33,760 --> 00:31:37,200
you know changed your model you can go

00:31:35,679 --> 00:31:38,880
ahead and visualize it and that

00:31:37,200 --> 00:31:40,880
visualization is something

00:31:38,880 --> 00:31:44,000
that you probably noticed from the

00:31:40,880 --> 00:31:46,559
slides so you can export these images

00:31:44,000 --> 00:31:47,919
that it creates uh and put them into

00:31:46,559 --> 00:31:49,519
your slides and then you can use them

00:31:47,919 --> 00:31:52,480
for your next presentation

00:31:49,519 --> 00:31:52,880
on apache cassandra use it to send it to

00:31:52,480 --> 00:31:56,000
uh

00:31:52,880 --> 00:31:57,440
internal external uh uh uh

00:31:56,000 --> 00:31:59,919
people that you're working with on

00:31:57,440 --> 00:32:02,159
cassandra and then bring it to the next

00:31:59,919 --> 00:32:04,960
apache con conference

00:32:02,159 --> 00:32:05,919
um so let's say if i wanted to do a

00:32:04,960 --> 00:32:08,159
shift left on

00:32:05,919 --> 00:32:10,320
uh this this table i wanted to move the

00:32:08,159 --> 00:32:11,200
mdm to the partition key and demonstrate

00:32:10,320 --> 00:32:14,320
some of the

00:32:11,200 --> 00:32:16,720
advanced the tactics we use in

00:32:14,320 --> 00:32:18,799
a schema modification we can go ahead

00:32:16,720 --> 00:32:21,440
and go back to the data modeler

00:32:18,799 --> 00:32:22,000
i'm going to choose the customer table

00:32:21,440 --> 00:32:24,640
and

00:32:22,000 --> 00:32:25,679
i'm going to edit the table i'm going to

00:32:24,640 --> 00:32:27,279
go down to

00:32:25,679 --> 00:32:30,960
the partition key i'm simply going to

00:32:27,279 --> 00:32:32,799
enter in the mdn field

00:32:30,960 --> 00:32:34,240
and then i'm going to remove it from the

00:32:32,799 --> 00:32:37,039
clustering columns

00:32:34,240 --> 00:32:37,760
and then i go ahead and save that data

00:32:37,039 --> 00:32:39,519
and

00:32:37,760 --> 00:32:42,240
then i go ahead and can go visualize

00:32:39,519 --> 00:32:46,000
this data model and now i see that

00:32:42,240 --> 00:32:47,279
the mdn is now part of the primary key

00:32:46,000 --> 00:32:50,159
or the partition key

00:32:47,279 --> 00:32:51,679
part of the primary key and now the

00:32:50,159 --> 00:32:53,919
clustering key only contains the build

00:32:51,679 --> 00:32:55,440
cycle

00:32:53,919 --> 00:32:57,519
the aggregate view of this you can

00:32:55,440 --> 00:33:00,720
actually visualize all of the data

00:32:57,519 --> 00:33:03,840
within your within your model um

00:33:00,720 --> 00:33:06,320
and then you can even export that png

00:33:03,840 --> 00:33:08,840
image uh so that you can put it into

00:33:06,320 --> 00:33:10,480
your you know presentations or email it

00:33:08,840 --> 00:33:12,320
etc

00:33:10,480 --> 00:33:13,760
after you've done committing and

00:33:12,320 --> 00:33:15,039
visualizing your data you're going to

00:33:13,760 --> 00:33:17,120
want to commit it to

00:33:15,039 --> 00:33:18,720
cassandra or you commit it to amazon key

00:33:17,120 --> 00:33:19,519
spaces and for the sake of this demo i'm

00:33:18,720 --> 00:33:25,200
going ahead and

00:33:19,519 --> 00:33:28,000
commit to amazon key spaces

00:33:25,200 --> 00:33:29,679
and i'm going to choose my ssl pem i'm

00:33:28,000 --> 00:33:31,279
going to go ahead and commit that to

00:33:29,679 --> 00:33:34,480
amazon key spaces

00:33:31,279 --> 00:33:37,200
which is a serverless no cassandra

00:33:34,480 --> 00:33:39,760
compatible database

00:33:37,200 --> 00:33:42,399
and what happened is i need to

00:33:39,760 --> 00:33:42,399
disconnect from

00:33:45,279 --> 00:33:49,679
go ahead and go ahead and commit that

00:33:47,360 --> 00:33:49,679
again

00:33:51,519 --> 00:33:58,720
and then while it's committing it to

00:33:54,559 --> 00:33:58,720
key spaces i'll go over and

00:33:59,679 --> 00:34:04,159
see my device services key spaces now

00:34:01,760 --> 00:34:04,159
came up

00:34:04,399 --> 00:34:08,000
and now you'll see that the device and

00:34:06,159 --> 00:34:11,440
the tables are being created

00:34:08,000 --> 00:34:13,599
uh in key spaces

00:34:11,440 --> 00:34:15,359
as the data is being committed uh to

00:34:13,599 --> 00:34:18,399
consent to cassandra here

00:34:15,359 --> 00:34:21,040
um in key spaces so um

00:34:18,399 --> 00:34:23,200
we can go ahead and view this data you

00:34:21,040 --> 00:34:26,639
can view the table's properties here

00:34:23,200 --> 00:34:28,560
um you can turn on uh you know

00:34:26,639 --> 00:34:30,720
point time recovery or change your

00:34:28,560 --> 00:34:32,720
capacity settings after it's completed

00:34:30,720 --> 00:34:34,079
then you can go ahead and export that

00:34:32,720 --> 00:34:37,280
model send it to

00:34:34,079 --> 00:34:39,280
your co-workers and uh you know

00:34:37,280 --> 00:34:41,440
start you know changing that model

00:34:39,280 --> 00:34:43,919
visually within the works the nosql

00:34:41,440 --> 00:34:43,919
workbench

00:34:46,839 --> 00:34:49,839
okay

00:34:50,079 --> 00:34:53,839
go ahead and get started with this no

00:34:52,159 --> 00:34:54,960
sql workbench uh you can go ahead and

00:34:53,839 --> 00:34:58,160
download it from

00:34:54,960 --> 00:34:59,280
uh uh or you know send a a request to

00:34:58,160 --> 00:35:01,520
see the preview

00:34:59,280 --> 00:35:03,280
uh in the email and that will put you in

00:35:01,520 --> 00:35:05,839
a waiting list to then

00:35:03,280 --> 00:35:08,079
uh be put on the waiting list for this

00:35:05,839 --> 00:35:10,079
preview which you can then download

00:35:08,079 --> 00:35:12,240
um and start playing around with your

00:35:10,079 --> 00:35:13,599
existing models

00:35:12,240 --> 00:35:15,440
yeah well kind of yeah we do have

00:35:13,599 --> 00:35:16,320
clients available for windows mac os and

00:35:15,440 --> 00:35:17,920
linux

00:35:16,320 --> 00:35:19,760
it's free to use uh there's no

00:35:17,920 --> 00:35:20,960
additional cost to use the data modeler

00:35:19,760 --> 00:35:22,880
and as michael mentioned we are

00:35:20,960 --> 00:35:23,440
extending extra early access to the

00:35:22,880 --> 00:35:24,880
preview

00:35:23,440 --> 00:35:27,920
to the attendance here at apachecon

00:35:24,880 --> 00:35:29,920
today just send us an email to nosqlwb

00:35:27,920 --> 00:35:32,400
amazon.com we could send you a link to

00:35:29,920 --> 00:35:33,920
download uh the client of your choice

00:35:32,400 --> 00:35:35,280
um again really excited to just give

00:35:33,920 --> 00:35:36,800
this tool to developers help them get

00:35:35,280 --> 00:35:38,079
going with the song a little bit easier

00:35:36,800 --> 00:35:39,839
and really appreciate everyone's time

00:35:38,079 --> 00:35:40,960
today for joining the session i think

00:35:39,839 --> 00:35:42,640
we've got a little bit of time left for

00:35:40,960 --> 00:35:47,440
questions uh so we'll go ahead and

00:35:42,640 --> 00:35:47,440
and let you guys ask any q a or gals

00:35:55,920 --> 00:35:59,119
i did see one question earlier from from

00:35:57,520 --> 00:36:00,480
giorgio about uh any use case

00:35:59,119 --> 00:36:01,920
improvement modeling skills

00:36:00,480 --> 00:36:03,520
one thing they'll point out is there are

00:36:01,920 --> 00:36:05,040
some baked in examples

00:36:03,520 --> 00:36:06,720
uh into the no skill workbench that will

00:36:05,040 --> 00:36:08,000
be available when it goes ga so

00:36:06,720 --> 00:36:09,520
if you want to see examples to play

00:36:08,000 --> 00:36:10,000
around with a more more complete data

00:36:09,520 --> 00:36:11,280
model

00:36:10,000 --> 00:36:14,160
uh they will be there to help you sort

00:36:11,280 --> 00:36:15,520
of learn and up your game a little bit

00:36:14,160 --> 00:36:17,359
uh and so michael one of the questions

00:36:15,520 --> 00:36:18,720
is is there any modeling strategies for

00:36:17,359 --> 00:36:20,079
wide partitions

00:36:18,720 --> 00:36:23,119
in cassandra what do you recommend in

00:36:20,079 --> 00:36:25,040
those cases yeah so this means a wide

00:36:23,119 --> 00:36:26,800
partition is meaning generally the case

00:36:25,040 --> 00:36:28,000
is that you have too much data in one

00:36:26,800 --> 00:36:30,079
partition and that

00:36:28,000 --> 00:36:31,359
that can cause issues on throughput like

00:36:30,079 --> 00:36:33,599
you're going to select

00:36:31,359 --> 00:36:35,839
a read and now you have to read a lot of

00:36:33,599 --> 00:36:36,880
data from that partition so you may want

00:36:35,839 --> 00:36:38,720
to go ahead

00:36:36,880 --> 00:36:40,320
and say hey i want to store this data

00:36:38,720 --> 00:36:41,920
over multiple partitions

00:36:40,320 --> 00:36:44,240
this way i can access that data in

00:36:41,920 --> 00:36:44,880
parallel and reduce the time that it

00:36:44,240 --> 00:36:47,839
needs to

00:36:44,880 --> 00:36:49,440
to grab all that information uh

00:36:47,839 --> 00:36:51,760
generally you can use a lot of those

00:36:49,440 --> 00:36:52,960
partitioning tricks sharding tricks that

00:36:51,760 --> 00:36:55,920
you would

00:36:52,960 --> 00:36:57,839
for distributing access you can also use

00:36:55,920 --> 00:36:58,320
those same tricks to start distributing

00:36:57,839 --> 00:37:01,119
data

00:36:58,320 --> 00:37:02,480
across multiple partitions as well and

00:37:01,119 --> 00:37:03,839
when you distribute your data over

00:37:02,480 --> 00:37:05,520
multiple partitions

00:37:03,839 --> 00:37:07,520
that will increase increase your

00:37:05,520 --> 00:37:08,079
throughput when you go ahead and

00:37:07,520 --> 00:37:11,920
aggregate

00:37:08,079 --> 00:37:11,920
that data from multiple partitions

00:37:16,000 --> 00:37:19,599
great well if there are no other

00:37:18,240 --> 00:37:21,280
questions i'm sure folks got a

00:37:19,599 --> 00:37:22,720
jam-packed afternoon today so

00:37:21,280 --> 00:37:24,240
feel free to reach out to us on twitter

00:37:22,720 --> 00:37:25,040
you know our twitter handles are are on

00:37:24,240 --> 00:37:26,720
the slides

00:37:25,040 --> 00:37:28,560
they love to hear from you um and again

00:37:26,720 --> 00:37:30,480
anyone interested in uh

00:37:28,560 --> 00:37:31,599
in downloading uh or getting access to

00:37:30,480 --> 00:37:34,839
the preview

00:37:31,599 --> 00:37:36,560
of the workbench please email uh nosql

00:37:34,839 --> 00:37:38,800
amazon.com thank you again for your time

00:37:36,560 --> 00:37:51,839
this afternoon

00:37:38,800 --> 00:37:51,839
thank you

00:38:19,760 --> 00:38:23,200
and many of those who still have

00:38:21,440 --> 00:38:24,720
questions or want to have a conversation

00:38:23,200 --> 00:38:27,599
about uh cassandra

00:38:24,720 --> 00:38:28,480
free appeals free uh feel free to ping

00:38:27,599 --> 00:38:32,079
me on this

00:38:28,480 --> 00:38:33,680
um apache con session uh software i'll

00:38:32,079 --> 00:38:35,920
be hanging around for a while to

00:38:33,680 --> 00:38:37,040
take any more questions if you if you if

00:38:35,920 --> 00:38:39,920
you want to

00:38:37,040 --> 00:38:42,160
start up a networking session okay thank

00:38:39,920 --> 00:38:42,160
you

00:38:44,839 --> 00:38:47,839
bye

00:40:20,640 --> 00:40:22,720

YouTube URL: https://www.youtube.com/watch?v=e55q6hdk0lY


