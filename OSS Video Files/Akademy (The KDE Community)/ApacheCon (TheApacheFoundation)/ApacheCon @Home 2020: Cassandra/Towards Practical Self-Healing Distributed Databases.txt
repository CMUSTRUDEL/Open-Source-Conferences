Title: Towards Practical Self-Healing Distributed Databases
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Cassandra
Description: 
	Towards Practical Self-Healing Distributed Databases
Dinesh Joshi, Joey Lynch

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

As distributed databases expand in popularity, there is ever-growing research into new database architectures that are designed from the start with built-in self-tuning and self- healing features. In real world deployments, however, migration to these entirely new systems is impractical and the challenge is to keep massive fleets of existing databases available under constant software and hardware change. Apache Cassandra is one such existing database that helped to popularize "scale-out" distributed databases and it runs some of the largest existing deployments of any open-source distributed database. In this talk, we demonstrate the techniques needed to transform the typical, highly manual, Apache Cassandra deployment into a self-healing system. We start by composing specialized agents together to surface the needed signals for a self-healing deployment and to execute local actions. Then we show how to combine the signals from the agents into the cluster level control- planes required to safely iterate and evolve existing deployments without compromising database availability. Finally, we show how to create simulated models of the database's behavior, allowing rapid iteration with minimal risk. With these systems in place, it is possible to create a truly self-healing database system within existing large-scale Apache Cassandra deployments.

Dinesh Joshi:
Dinesh A. Joshi has been a professional Software Engineer for over a decade building highly scalable realtime Web Services and Distributed Streaming Data Processing Architectures serving over 1 billion devices. Dinesh is an active contributor to the Apache Cassandra codebase. He has a Masters degree in Computer Science (Distributed Systems & Databases) from Georgia Tech, Atlanta, USA.
Joey Lynch:
Joey helps keep the wheels on the bus for Netflixâ€™s data infrastructure.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:28,840 --> 00:00:34,480
so all right

00:00:30,480 --> 00:00:34,480
shall we uh shall we get started

00:00:42,840 --> 00:00:45,840
yes

00:00:47,570 --> 00:00:52,879
[Music]

00:00:49,440 --> 00:00:56,559
okay uh do you see my

00:00:52,879 --> 00:00:58,960
presentation i see it

00:00:56,559 --> 00:01:00,559
okay great all right good morning

00:00:58,960 --> 00:01:01,600
everybody thank you for joining our

00:01:00,559 --> 00:01:05,119
session

00:01:01,600 --> 00:01:05,519
uh i'm dinesh joshi and uh i have with

00:01:05,119 --> 00:01:08,400
me

00:01:05,519 --> 00:01:09,439
uh joey lynch uh and today we are going

00:01:08,400 --> 00:01:11,040
to talk about uh

00:01:09,439 --> 00:01:13,040
towards practicals of healing

00:01:11,040 --> 00:01:15,840
distributed databases

00:01:13,040 --> 00:01:16,880
a little bit about me i am a senior

00:01:15,840 --> 00:01:20,080
software engineer

00:01:16,880 --> 00:01:22,720
at apple and uh joey

00:01:20,080 --> 00:01:23,360
can introduce himself uh but we both are

00:01:22,720 --> 00:01:25,040
apache

00:01:23,360 --> 00:01:28,159
standard committers working on the

00:01:25,040 --> 00:01:30,000
cassandra project for for a while now

00:01:28,159 --> 00:01:31,200
hi thanks dinesh uh yeah so my name is

00:01:30,000 --> 00:01:33,040
joey and i'm an engineer

00:01:31,200 --> 00:01:34,799
over at netflix on the cloud data

00:01:33,040 --> 00:01:37,040
engineering team um

00:01:34,799 --> 00:01:39,200
and i basically wrangle databases of

00:01:37,040 --> 00:01:42,960
varying forms

00:01:39,200 --> 00:01:46,640
so cassandra's one of them cool so

00:01:42,960 --> 00:01:49,119
today um we'll be uh doing a few things

00:01:46,640 --> 00:01:49,840
um so we'll introduce you to the

00:01:49,119 --> 00:01:53,280
problems

00:01:49,840 --> 00:01:55,439
piece um we'll talk about the goals of a

00:01:53,280 --> 00:01:58,560
self-healing distributed database

00:01:55,439 --> 00:02:02,799
and we will look at some of the

00:01:58,560 --> 00:02:05,439
large scale operations when we run a

00:02:02,799 --> 00:02:07,600
database and then we propose a

00:02:05,439 --> 00:02:10,000
self-healing architecture

00:02:07,600 --> 00:02:10,800
and talk about how we can safely

00:02:10,000 --> 00:02:14,319
experiment

00:02:10,800 --> 00:02:16,879
in production uh with this architecture

00:02:14,319 --> 00:02:19,920
and finally we'll talk about conclusions

00:02:16,879 --> 00:02:23,680
and our future work

00:02:19,920 --> 00:02:26,800
so um let's get started um

00:02:23,680 --> 00:02:28,720
running a database is hard so if

00:02:26,800 --> 00:02:29,840
people have ever tried running a

00:02:28,720 --> 00:02:32,160
database

00:02:29,840 --> 00:02:33,200
which is a stateful application uh they

00:02:32,160 --> 00:02:35,360
know that

00:02:33,200 --> 00:02:36,319
running a database is difficult in

00:02:35,360 --> 00:02:38,160
production

00:02:36,319 --> 00:02:39,840
but running a distributed database is

00:02:38,160 --> 00:02:42,319
even harder because

00:02:39,840 --> 00:02:43,280
there's more difficulty in uh the

00:02:42,319 --> 00:02:45,200
problem space

00:02:43,280 --> 00:02:46,480
and then the failure modes are very

00:02:45,200 --> 00:02:49,920
esoteric

00:02:46,480 --> 00:02:51,040
so um the current uh distributed

00:02:49,920 --> 00:02:54,080
database uh

00:02:51,040 --> 00:02:56,400
you know uh operations require a lot of

00:02:54,080 --> 00:02:57,920
manual intervention and it's a high

00:02:56,400 --> 00:03:00,319
latch activity

00:02:57,920 --> 00:03:02,159
um requiring uh intervention from the

00:03:00,319 --> 00:03:05,280
sis ops folks and

00:03:02,159 --> 00:03:05,760
developers at times uh there are some

00:03:05,280 --> 00:03:08,319
new

00:03:05,760 --> 00:03:09,280
autonomous databases that are coming up

00:03:08,319 --> 00:03:12,400
uh in the

00:03:09,280 --> 00:03:15,120
research space and uh they are

00:03:12,400 --> 00:03:17,519
not yet fully at a point that the

00:03:15,120 --> 00:03:19,440
industry can adopt them at scale

00:03:17,519 --> 00:03:21,599
and it is difficult to migrate to these

00:03:19,440 --> 00:03:26,640
new autonomous data users because

00:03:21,599 --> 00:03:30,400
the data model and uh the the

00:03:26,640 --> 00:03:31,760
the database uh response characteristics

00:03:30,400 --> 00:03:35,280
are very different

00:03:31,760 --> 00:03:37,200
from the existing databases so it's not

00:03:35,280 --> 00:03:39,440
a trivial task to take your existing

00:03:37,200 --> 00:03:41,840
system that's serving hundreds of

00:03:39,440 --> 00:03:42,879
petabytes of data and then move it over

00:03:41,840 --> 00:03:45,840
to

00:03:42,879 --> 00:03:49,920
a new autonomous database with which

00:03:45,840 --> 00:03:53,120
requires a full rewrite of applications

00:03:49,920 --> 00:03:54,799
so with that said i'm going to talk

00:03:53,120 --> 00:03:57,760
about the goals of a self-healing

00:03:54,799 --> 00:03:57,760
distributed database

00:03:57,920 --> 00:04:04,560
the goals are basically to self-preserve

00:04:01,840 --> 00:04:05,120
which means that the database has to

00:04:04,560 --> 00:04:08,400
function

00:04:05,120 --> 00:04:08,879
in the event of a failure within the

00:04:08,400 --> 00:04:12,640
system

00:04:08,879 --> 00:04:15,360
so that is the primary goal of database

00:04:12,640 --> 00:04:17,199
and to avoid oscillating scenes so what

00:04:15,360 --> 00:04:18,160
that means is if the database detects

00:04:17,199 --> 00:04:20,720
there's a problem

00:04:18,160 --> 00:04:22,960
it tries to fix the problem but that

00:04:20,720 --> 00:04:23,759
should not keep ping-ponging between two

00:04:22,960 --> 00:04:28,639
states

00:04:23,759 --> 00:04:29,840
um and which in itself is a problem

00:04:28,639 --> 00:04:32,320
and then the system has to be

00:04:29,840 --> 00:04:32,720
explainable so when you get a call at 2

00:04:32,320 --> 00:04:36,080
am

00:04:32,720 --> 00:04:38,240
as a sorry or an operator

00:04:36,080 --> 00:04:40,560
of the system you don't want to dig

00:04:38,240 --> 00:04:41,680
around too many places and look at what

00:04:40,560 --> 00:04:44,240
the

00:04:41,680 --> 00:04:45,520
issue is uh you want the system to be

00:04:44,240 --> 00:04:47,919
very understandable

00:04:45,520 --> 00:04:51,120
so these are three main goals of a

00:04:47,919 --> 00:04:52,800
distributed database that is selfie

00:04:51,120 --> 00:04:54,880
the non goals these are specifically

00:04:52,800 --> 00:04:58,240
things that we are not going to try

00:04:54,880 --> 00:05:00,400
addressing um basically the system does

00:04:58,240 --> 00:05:02,479
not handle all types of failures

00:05:00,400 --> 00:05:03,759
and it does not magically tune itself so

00:05:02,479 --> 00:05:06,160
it's not one of those

00:05:03,759 --> 00:05:07,520
systems where we observe the metrics and

00:05:06,160 --> 00:05:10,960
then we try to tune

00:05:07,520 --> 00:05:12,800
the database that's not the goal of the

00:05:10,960 --> 00:05:15,120
system

00:05:12,800 --> 00:05:17,600
we can leverage machine learning but uh

00:05:15,120 --> 00:05:19,360
not at the expense of understandability

00:05:17,600 --> 00:05:20,960
uh people who have worked with machine

00:05:19,360 --> 00:05:21,600
learning know that the machine learning

00:05:20,960 --> 00:05:23,600
models

00:05:21,600 --> 00:05:25,840
while they are useful they are very

00:05:23,600 --> 00:05:27,680
difficult to understand at times

00:05:25,840 --> 00:05:30,080
and and our system needs to be more

00:05:27,680 --> 00:05:33,199
explainable

00:05:30,080 --> 00:05:36,080
so let's dive into what uh what

00:05:33,199 --> 00:05:36,400
what is the difficulty in running these

00:05:36,080 --> 00:05:39,759
uh

00:05:36,400 --> 00:05:42,240
large-scale systems right so the primary

00:05:39,759 --> 00:05:44,080
uh difficulty is in software management

00:05:42,240 --> 00:05:46,800
software is constantly changing

00:05:44,080 --> 00:05:47,680
and uh and we need to keep that software

00:05:46,800 --> 00:05:50,320
updated

00:05:47,680 --> 00:05:51,039
and any sort of updater activity is a

00:05:50,320 --> 00:05:54,479
destructive

00:05:51,039 --> 00:05:56,800
activity to your database cluster

00:05:54,479 --> 00:05:57,520
the second is configuration management

00:05:56,800 --> 00:05:59,360
things change

00:05:57,520 --> 00:06:01,120
over time new data gets added to the

00:05:59,360 --> 00:06:01,840
database it's a database management

00:06:01,120 --> 00:06:06,160
system

00:06:01,840 --> 00:06:08,639
and so you need to constantly keep

00:06:06,160 --> 00:06:10,720
uh tuning it uh and configuration

00:06:08,639 --> 00:06:13,120
management is key to that

00:06:10,720 --> 00:06:14,319
and uh hardware is constantly degrading

00:06:13,120 --> 00:06:17,440
so hardware

00:06:14,319 --> 00:06:18,160
does fail our time and we need to keep

00:06:17,440 --> 00:06:21,440
replacing

00:06:18,160 --> 00:06:24,080
instances in the cluster but also uh

00:06:21,440 --> 00:06:24,880
hardware gets updated over time there

00:06:24,080 --> 00:06:27,120
are newer

00:06:24,880 --> 00:06:28,000
cpus on the market there's expanded

00:06:27,120 --> 00:06:31,440
memory

00:06:28,000 --> 00:06:32,000
uh more higher performance ssds that get

00:06:31,440 --> 00:06:35,280
added

00:06:32,000 --> 00:06:37,759
and so you need to just you know in in

00:06:35,280 --> 00:06:38,800
in situations you need to just update to

00:06:37,759 --> 00:06:42,560
on your hardware

00:06:38,800 --> 00:06:44,960
footprint and uh the

00:06:42,560 --> 00:06:45,759
there is difficulty also in another

00:06:44,960 --> 00:06:47,280
dimension

00:06:45,759 --> 00:06:49,280
is to understand the system you need to

00:06:47,280 --> 00:06:50,639
collect effective telemetry

00:06:49,280 --> 00:06:54,080
so you need to understand what the

00:06:50,639 --> 00:06:56,560
system is doing when it is doing it

00:06:54,080 --> 00:06:57,440
and finally the system when it detects a

00:06:56,560 --> 00:07:00,720
issue

00:06:57,440 --> 00:07:03,520
it should be an on-call and uh

00:07:00,720 --> 00:07:04,800
this is uh this is crucial for the

00:07:03,520 --> 00:07:07,919
continuous operations of

00:07:04,800 --> 00:07:09,599
any system so uh

00:07:07,919 --> 00:07:11,199
what what are we really talking about

00:07:09,599 --> 00:07:12,240
here right uh with all of these

00:07:11,199 --> 00:07:15,120
activities

00:07:12,240 --> 00:07:16,400
uh we're talking about risk and uh what

00:07:15,120 --> 00:07:19,840
we see on the slide

00:07:16,400 --> 00:07:22,960
is a graph that models um

00:07:19,840 --> 00:07:26,639
the risk and it models it

00:07:22,960 --> 00:07:30,160
in the using metric

00:07:26,639 --> 00:07:33,440
which is mean time to failure um and

00:07:30,160 --> 00:07:37,440
as the you know what you see is

00:07:33,440 --> 00:07:40,639
the the failures uh are gonna increase

00:07:37,440 --> 00:07:41,360
uh exponentially this uh graph is a log

00:07:40,639 --> 00:07:44,720
scale

00:07:41,360 --> 00:07:48,319
so uh it it it displays

00:07:44,720 --> 00:07:50,960
the the criticality of uh you know

00:07:48,319 --> 00:07:52,560
of handling risk or mitigating risk any

00:07:50,960 --> 00:07:54,319
form of change is a risk

00:07:52,560 --> 00:07:55,840
and the bigger cluster that you run or

00:07:54,319 --> 00:07:58,879
more nodes that you run the

00:07:55,840 --> 00:08:01,599
risk increases dramatically so

00:07:58,879 --> 00:08:03,520
what is this risk right so whenever we

00:08:01,599 --> 00:08:04,319
make any changes online changes in

00:08:03,520 --> 00:08:06,400
cassandra

00:08:04,319 --> 00:08:08,319
which involve changing uh settings via

00:08:06,400 --> 00:08:11,120
jmx or system tables

00:08:08,319 --> 00:08:12,319
um they it involves risk so this is

00:08:11,120 --> 00:08:15,520
usually done for

00:08:12,319 --> 00:08:17,599
database uh also at times we use it for

00:08:15,520 --> 00:08:19,199
reconfiguring cassandra

00:08:17,599 --> 00:08:20,720
so consumer doesn't need to be taken

00:08:19,199 --> 00:08:22,479
offline in order to

00:08:20,720 --> 00:08:25,199
configure certain parameters these are

00:08:22,479 --> 00:08:26,800
called hot props in cassandra

00:08:25,199 --> 00:08:28,720
and then there are offline changes that

00:08:26,800 --> 00:08:32,159
we do uh which are

00:08:28,720 --> 00:08:34,320
uh yamo uh base changes and uh

00:08:32,159 --> 00:08:36,000
this involves taking the instance

00:08:34,320 --> 00:08:38,880
offline when you take an instance

00:08:36,000 --> 00:08:40,399
offline it is an inherent risk uh to the

00:08:38,880 --> 00:08:42,880
entire investor because

00:08:40,399 --> 00:08:44,399
now you have uh traffic going to other

00:08:42,880 --> 00:08:47,600
instances in the cluster which

00:08:44,399 --> 00:08:48,880
may increase by 30 33 percent uh at

00:08:47,600 --> 00:08:52,800
times as well

00:08:48,880 --> 00:08:55,839
so uh the the idea here is that

00:08:52,800 --> 00:08:57,600
um any any offline change

00:08:55,839 --> 00:08:59,519
is gonna cause the instance to be

00:08:57,600 --> 00:09:01,360
unavailable and uh

00:08:59,519 --> 00:09:03,360
and this is something that we need to do

00:09:01,360 --> 00:09:06,000
because you know sometimes we need to

00:09:03,360 --> 00:09:07,360
update the kernel uh sometimes we need

00:09:06,000 --> 00:09:10,640
to update the runtime

00:09:07,360 --> 00:09:13,200
and other suspects right um

00:09:10,640 --> 00:09:14,720
including updating cassandra and then

00:09:13,200 --> 00:09:18,160
like we talked about earlier

00:09:14,720 --> 00:09:21,279
uh hardware upgrades um it also requires

00:09:18,160 --> 00:09:23,680
data instance offline because the the

00:09:21,279 --> 00:09:24,399
the instance has to be moved from one

00:09:23,680 --> 00:09:27,440
physical

00:09:24,399 --> 00:09:30,880
hardware to another physical hardware

00:09:27,440 --> 00:09:32,640
so let's uh look at the self-healing

00:09:30,880 --> 00:09:33,519
architecture how does it try to solve

00:09:32,640 --> 00:09:36,959
some of these

00:09:33,519 --> 00:09:39,200
uh issues and mitigates risk

00:09:36,959 --> 00:09:41,440
so the self-healing architecture is a

00:09:39,200 --> 00:09:44,640
whole goal-driven architecture

00:09:41,440 --> 00:09:48,480
and what that means is uh we are

00:09:44,640 --> 00:09:50,640
gonna set explicit goals for the system

00:09:48,480 --> 00:09:52,240
there are specialized agents and we will

00:09:50,640 --> 00:09:53,120
see what those agents are but these are

00:09:52,240 --> 00:09:56,160
agents that

00:09:53,120 --> 00:09:58,320
are primarily uh specialized in

00:09:56,160 --> 00:10:01,680
a specific domain and they are running

00:09:58,320 --> 00:10:03,680
on the nodes in the system

00:10:01,680 --> 00:10:06,000
there's a component called cluster

00:10:03,680 --> 00:10:07,560
manager this cluster manager allows

00:10:06,000 --> 00:10:09,760
the cluster to

00:10:07,560 --> 00:10:13,360
[Music]

00:10:09,760 --> 00:10:16,160
to heal itself when there are issues

00:10:13,360 --> 00:10:16,959
um and it probably is a facilitator

00:10:16,160 --> 00:10:18,560
rather than

00:10:16,959 --> 00:10:20,079
you know system that is controlling the

00:10:18,560 --> 00:10:22,160
cluster

00:10:20,079 --> 00:10:23,600
and there is local and global decision

00:10:22,160 --> 00:10:26,320
making in this architecture

00:10:23,600 --> 00:10:28,320
what that means is that uh individual

00:10:26,320 --> 00:10:29,760
agents can make local decisions

00:10:28,320 --> 00:10:33,360
and if they want to participate in a

00:10:29,760 --> 00:10:35,200
global decision they can do that as well

00:10:33,360 --> 00:10:36,640
and the system is constantly trying to

00:10:35,200 --> 00:10:39,200
converge to a good state

00:10:36,640 --> 00:10:40,959
so that's uh that's those are the uh

00:10:39,200 --> 00:10:42,000
main components of a self-healing

00:10:40,959 --> 00:10:44,000
architecture

00:10:42,000 --> 00:10:46,480
so now let's look at how this

00:10:44,000 --> 00:10:49,680
architecture looks

00:10:46,480 --> 00:10:52,720
will look like um so

00:10:49,680 --> 00:10:54,320
um in this architecture uh what we see

00:10:52,720 --> 00:10:55,120
is that there are these nodes these

00:10:54,320 --> 00:10:58,000
nodes are

00:10:55,120 --> 00:11:00,800
individual cassandra nodes and here we

00:10:58,000 --> 00:11:03,519
have three nodes n n1 n2 and n3

00:11:00,800 --> 00:11:04,160
and each node has a specialized agent

00:11:03,519 --> 00:11:07,200
some

00:11:04,160 --> 00:11:10,240
agent that is running on this node

00:11:07,200 --> 00:11:10,959
and this agent internally if you look

00:11:10,240 --> 00:11:13,600
here

00:11:10,959 --> 00:11:14,320
it has it understands the state of the

00:11:13,600 --> 00:11:16,320
node so it

00:11:14,320 --> 00:11:18,800
it observes what the current state of

00:11:16,320 --> 00:11:21,040
the node is it also understands

00:11:18,800 --> 00:11:22,079
what the node's goals are so that there

00:11:21,040 --> 00:11:25,440
are set of goals

00:11:22,079 --> 00:11:28,160
that it has to achieve and there is just

00:11:25,440 --> 00:11:29,279
some state that it reaps and it

00:11:28,160 --> 00:11:32,640
understands

00:11:29,279 --> 00:11:36,079
and reports these the state

00:11:32,640 --> 00:11:36,640
back to uh the system but uh based on

00:11:36,079 --> 00:11:39,120
the

00:11:36,640 --> 00:11:40,640
goals and this observed state the agent

00:11:39,120 --> 00:11:44,399
can potentially take

00:11:40,640 --> 00:11:47,360
some action on the on the cluster

00:11:44,399 --> 00:11:48,399
or sorry on the individual node and at a

00:11:47,360 --> 00:11:51,120
higher level

00:11:48,399 --> 00:11:53,040
there is a state machine here that is

00:11:51,120 --> 00:11:53,600
always transitioning from one state to

00:11:53,040 --> 00:11:55,839
the other

00:11:53,600 --> 00:11:56,800
so when the state is observed to be a

00:11:55,839 --> 00:11:58,880
certain state

00:11:56,800 --> 00:11:59,920
the node is going to now the agent is

00:11:58,880 --> 00:12:02,160
going to try to

00:11:59,920 --> 00:12:03,360
transition the node to another state

00:12:02,160 --> 00:12:05,839
that is

00:12:03,360 --> 00:12:07,519
provided as the node goals and at a

00:12:05,839 --> 00:12:08,720
higher level we have a cluster measure

00:12:07,519 --> 00:12:10,480
which is across

00:12:08,720 --> 00:12:11,839
all the nodes within the cluster and

00:12:10,480 --> 00:12:14,240
this cluster manager

00:12:11,839 --> 00:12:15,680
has world goals which means it has the

00:12:14,240 --> 00:12:17,600
cluster-wide goals

00:12:15,680 --> 00:12:18,800
uh programmed into it and then it has

00:12:17,600 --> 00:12:20,959
the state of the world

00:12:18,800 --> 00:12:22,320
so it understands what the what the

00:12:20,959 --> 00:12:25,040
state of the world currently

00:12:22,320 --> 00:12:26,240
is and it is also doing the same thing

00:12:25,040 --> 00:12:29,279
the difference between

00:12:26,240 --> 00:12:30,720
the node and the region that is running

00:12:29,279 --> 00:12:32,240
on the node and the cluster manager is

00:12:30,720 --> 00:12:34,639
that

00:12:32,240 --> 00:12:35,360
is has a higher level view whereas the

00:12:34,639 --> 00:12:38,639
node

00:12:35,360 --> 00:12:41,760
level agent has a single focused view

00:12:38,639 --> 00:12:45,760
which is the scope is the node

00:12:41,760 --> 00:12:47,120
um of that region so what kind of agents

00:12:45,760 --> 00:12:49,519
are we talking about here

00:12:47,120 --> 00:12:51,600
so there are hardware agents software

00:12:49,519 --> 00:12:54,560
agents and system reporting agents

00:12:51,600 --> 00:12:56,800
and these as the name suggests they are

00:12:54,560 --> 00:13:00,639
specialized to handle hardware

00:12:56,800 --> 00:13:04,480
uh or or the system uh reports

00:13:00,639 --> 00:13:05,120
uh and agents can rapidly react to local

00:13:04,480 --> 00:13:09,120
stimuli

00:13:05,120 --> 00:13:09,120
so much like a human's uh

00:13:09,519 --> 00:13:12,639
nervous system which senses an input and

00:13:12,320 --> 00:13:14,880
then

00:13:12,639 --> 00:13:15,839
it reacts to it uh similarly these

00:13:14,880 --> 00:13:18,880
agents

00:13:15,839 --> 00:13:21,360
react to the the state

00:13:18,880 --> 00:13:22,320
of the of the node and they make local

00:13:21,360 --> 00:13:24,720
decisions

00:13:22,320 --> 00:13:25,519
uh together these agents are going to

00:13:24,720 --> 00:13:27,839
cooperate

00:13:25,519 --> 00:13:29,200
to make a global decision via the

00:13:27,839 --> 00:13:31,040
cluster manager

00:13:29,200 --> 00:13:33,360
and they constantly converge to the gold

00:13:31,040 --> 00:13:36,480
state independently

00:13:33,360 --> 00:13:39,760
and when they can't uh get to

00:13:36,480 --> 00:13:42,160
a good state or they can't find a state

00:13:39,760 --> 00:13:46,160
a transition that is applicable

00:13:42,160 --> 00:13:48,720
they will page a human being

00:13:46,160 --> 00:13:50,399
and the natural question arises so why

00:13:48,720 --> 00:13:51,920
do we have domain specifications or why

00:13:50,399 --> 00:13:53,920
do we have specializations

00:13:51,920 --> 00:13:55,360
right uh these agents are easy to

00:13:53,920 --> 00:13:58,079
understand and debug

00:13:55,360 --> 00:13:59,519
each agent author gets to pick uh the

00:13:58,079 --> 00:14:01,680
ideal implementation

00:13:59,519 --> 00:14:04,240
uh so you're not stuck writing an agent

00:14:01,680 --> 00:14:04,240
in java

00:14:14,839 --> 00:14:17,839
if

00:14:18,959 --> 00:14:31,839
oh did we lose tonight

00:14:41,040 --> 00:14:45,920
oh all right i guess i will message

00:14:44,240 --> 00:14:47,199
dinesh offline and then i'm going to

00:14:45,920 --> 00:14:59,839
restart the presentation

00:14:47,199 --> 00:14:59,839
give me one second

00:15:15,360 --> 00:15:22,560
hey uh did did you all lose me

00:15:19,279 --> 00:15:23,360
yeah we lost it okay i'm sorry about

00:15:22,560 --> 00:15:27,040
that i think

00:15:23,360 --> 00:15:30,320
uh my internet's been flaky

00:15:27,040 --> 00:15:34,160
all right um

00:15:30,320 --> 00:15:38,240
i'm resharing my screen um

00:15:34,160 --> 00:15:41,279
so sorry where did we lose

00:15:38,240 --> 00:15:44,240
see you next time all right

00:15:41,279 --> 00:15:46,079
so domain specifications uh so they are

00:15:44,240 --> 00:15:49,120
easy to understand and debug

00:15:46,079 --> 00:15:52,000
and each agent can be composed uh or

00:15:49,120 --> 00:15:54,720
written in a different language that is

00:15:52,000 --> 00:15:57,360
ideal for the problem space and uh

00:15:54,720 --> 00:15:58,399
they are easy to compose which means if

00:15:57,360 --> 00:16:01,440
each agent is

00:15:58,399 --> 00:16:04,000
has a single responsibility then uh

00:16:01,440 --> 00:16:05,360
that those agents can be composed into a

00:16:04,000 --> 00:16:08,000
uh

00:16:05,360 --> 00:16:09,120
into system and which means that there

00:16:08,000 --> 00:16:12,240
is a higher

00:16:09,120 --> 00:16:13,519
level of reusability in these agents uh

00:16:12,240 --> 00:16:15,519
so what works for

00:16:13,519 --> 00:16:16,720
uh let's say cassandra or a distributed

00:16:15,519 --> 00:16:19,519
database like cassandra

00:16:16,720 --> 00:16:21,680
doesn't necessarily uh you don't need to

00:16:19,519 --> 00:16:24,880
use the same exact agents

00:16:21,680 --> 00:16:26,399
and so it brings brings in composability

00:16:24,880 --> 00:16:28,000
into the system

00:16:26,399 --> 00:16:29,759
so i'm going to hand it off to goei to

00:16:28,000 --> 00:16:33,199
talk about uh

00:16:29,759 --> 00:16:35,120
the next section uh joey

00:16:33,199 --> 00:16:37,199
all right thank you dinesh all right so

00:16:35,120 --> 00:16:38,560
we've heard uh kind of at a high level

00:16:37,199 --> 00:16:40,560
how we want to try and structure the

00:16:38,560 --> 00:16:42,000
self-healing database um for the next

00:16:40,560 --> 00:16:43,440
couple slides i'll take us

00:16:42,000 --> 00:16:45,279
kind of through progressively more

00:16:43,440 --> 00:16:47,279
complex examples that

00:16:45,279 --> 00:16:49,040
show you how we can actually do that and

00:16:47,279 --> 00:16:49,519
the very first step of any cell healing

00:16:49,040 --> 00:16:51,519
system

00:16:49,519 --> 00:16:53,199
or any declarative system is to write

00:16:51,519 --> 00:16:55,279
down what you want so

00:16:53,199 --> 00:16:57,680
here you can see a goal description for

00:16:55,279 --> 00:16:58,959
a given cassandra cluster

00:16:57,680 --> 00:17:00,959
and you can see that that goal is broken

00:16:58,959 --> 00:17:03,600
down into three main parts

00:17:00,959 --> 00:17:04,000
so the first one is we have desires or

00:17:03,600 --> 00:17:05,120
goals

00:17:04,000 --> 00:17:07,199
about what software we want to be

00:17:05,120 --> 00:17:09,199
running so what is the base

00:17:07,199 --> 00:17:11,039
operating system that we'll be uh

00:17:09,199 --> 00:17:13,199
installing on top of what are

00:17:11,039 --> 00:17:14,720
the side cars or other processes that

00:17:13,199 --> 00:17:17,039
have to run next to the database

00:17:14,720 --> 00:17:18,000
uh what is the configuration either yaml

00:17:17,039 --> 00:17:20,880
or other

00:17:18,000 --> 00:17:22,880
that we need for that database then we

00:17:20,880 --> 00:17:25,120
have hardware which is

00:17:22,880 --> 00:17:26,640
our desires or our goals about a kind of

00:17:25,120 --> 00:17:28,240
scale or shape

00:17:26,640 --> 00:17:30,880
so for example we might want to run a

00:17:28,240 --> 00:17:34,000
large cluster or smaller cluster

00:17:30,880 --> 00:17:36,960
and uh and and then finally we have

00:17:34,000 --> 00:17:38,000
high level system desires or goals uh

00:17:36,960 --> 00:17:39,520
things like

00:17:38,000 --> 00:17:41,200
uh how much money are we willing to

00:17:39,520 --> 00:17:43,760
spend on this cluster

00:17:41,200 --> 00:17:45,200
uh what are our target slos in terms of

00:17:43,760 --> 00:17:46,080
service level objectives in terms of

00:17:45,200 --> 00:17:49,440
like latency

00:17:46,080 --> 00:17:51,360
or availability and then the final

00:17:49,440 --> 00:17:53,360
aspect of this goal document is that it

00:17:51,360 --> 00:17:55,679
is parameterized by environment

00:17:53,360 --> 00:17:56,400
so a pretty important part of goal-based

00:17:55,679 --> 00:17:59,039
systems

00:17:56,400 --> 00:18:00,000
is that we can set those goals at a fine

00:17:59,039 --> 00:18:01,919
level so for example

00:18:00,000 --> 00:18:03,919
we can set the goals for a single node

00:18:01,919 --> 00:18:05,280
or for a rack of cassandra machines or

00:18:03,919 --> 00:18:06,720
for a cluster

00:18:05,280 --> 00:18:08,480
or for an entire environment like

00:18:06,720 --> 00:18:11,200
production or staging

00:18:08,480 --> 00:18:11,200
next slide please

00:18:12,720 --> 00:18:16,240
so i let's say that we've set some goals

00:18:15,919 --> 00:18:18,640
like

00:18:16,240 --> 00:18:19,919
some latency targets or uh for a given

00:18:18,640 --> 00:18:21,520
cassandra cluster to be

00:18:19,919 --> 00:18:23,679
uh running and let's kind of work

00:18:21,520 --> 00:18:25,760
through uh kind of every element

00:18:23,679 --> 00:18:27,120
of the of the goals that we need so at

00:18:25,760 --> 00:18:28,799
the very lowest level

00:18:27,120 --> 00:18:31,440
we're going to need to enforce goals for

00:18:28,799 --> 00:18:34,000
our hardware so for example here

00:18:31,440 --> 00:18:35,280
we can see a very simple startup

00:18:34,000 --> 00:18:37,039
supervisor agent

00:18:35,280 --> 00:18:38,640
which runs a really simple disk burn-in

00:18:37,039 --> 00:18:40,480
check

00:18:38,640 --> 00:18:42,400
and in production essentially what this

00:18:40,480 --> 00:18:43,520
is doing is it's enforcing a goal about

00:18:42,400 --> 00:18:45,600
our hardware's

00:18:43,520 --> 00:18:46,880
uh capabilities before we even start the

00:18:45,600 --> 00:18:48,400
database uh

00:18:46,880 --> 00:18:50,000
in here you can see something very

00:18:48,400 --> 00:18:53,280
simple like using a

00:18:50,000 --> 00:18:54,720
direct io dd command that does

00:18:53,280 --> 00:18:57,440
maybe one gigabyte of read and one

00:18:54,720 --> 00:18:59,919
gigabyte of write to a disk to

00:18:57,440 --> 00:19:02,720
a to assert that that disk can meet the

00:18:59,919 --> 00:19:05,600
latency requirements that we have

00:19:02,720 --> 00:19:05,600
uh next slide please

00:19:07,919 --> 00:19:11,440
okay thank you uh but it's not enough

00:19:09,679 --> 00:19:13,280
just to check that hardware is meeting

00:19:11,440 --> 00:19:14,480
our goals when it starts we also have to

00:19:13,280 --> 00:19:16,799
continuously

00:19:14,480 --> 00:19:18,559
assert that our hardware is meeting our

00:19:16,799 --> 00:19:20,799
our goals specifications

00:19:18,559 --> 00:19:22,799
um and if we're running on linux we get

00:19:20,799 --> 00:19:24,720
a lot of great tools out of the box

00:19:22,799 --> 00:19:26,640
for assessing the state of the world on

00:19:24,720 --> 00:19:28,400
that node to see if

00:19:26,640 --> 00:19:30,320
we are meeting our goals so for example

00:19:28,400 --> 00:19:31,280
we can find out if the disk is throwing

00:19:30,320 --> 00:19:33,200
i o errors

00:19:31,280 --> 00:19:35,760
through d message we can find out how

00:19:33,200 --> 00:19:37,280
long ios are spending

00:19:35,760 --> 00:19:39,360
being serviced by the disk versus

00:19:37,280 --> 00:19:42,320
queuing waiting for the disk using

00:19:39,360 --> 00:19:42,720
disk stats or we can even look into how

00:19:42,320 --> 00:19:44,960
long

00:19:42,720 --> 00:19:46,640
are our cassandra threads waiting to run

00:19:44,960 --> 00:19:49,200
on a cpu

00:19:46,640 --> 00:19:50,160
and this information is all really

00:19:49,200 --> 00:19:51,760
useful

00:19:50,160 --> 00:19:53,600
to be reported to that cluster manager

00:19:51,760 --> 00:19:55,919
uh to be able to make

00:19:53,600 --> 00:19:56,799
uh larger larger decisions so for

00:19:55,919 --> 00:19:58,720
example

00:19:56,799 --> 00:20:01,600
uh if you are seeing that a disk is

00:19:58,720 --> 00:20:03,280
taking one or two seconds to service ios

00:20:01,600 --> 00:20:05,840
the only possible remedy is to ask the

00:20:03,280 --> 00:20:07,039
cluster manager to terminate yourself

00:20:05,840 --> 00:20:08,880
because there's no way that you can run

00:20:07,039 --> 00:20:11,200
cassandra successfully

00:20:08,880 --> 00:20:12,159
uh conversely if you see and schedule

00:20:11,200 --> 00:20:14,640
and sketch that

00:20:12,159 --> 00:20:15,600
that that your threads are not waiting

00:20:14,640 --> 00:20:17,200
for a cpu

00:20:15,600 --> 00:20:18,400
it doesn't matter how much you scale up

00:20:17,200 --> 00:20:19,520
that cluster it's not going to get

00:20:18,400 --> 00:20:21,360
faster

00:20:19,520 --> 00:20:22,799
because if you add more cpus it's you

00:20:21,360 --> 00:20:24,640
know that's not

00:20:22,799 --> 00:20:26,960
the process itself is slow not that

00:20:24,640 --> 00:20:28,400
you're waiting for a cpu to run on

00:20:26,960 --> 00:20:30,240
these are a little bit more complicated

00:20:28,400 --> 00:20:31,919
than those startup checks but we can see

00:20:30,240 --> 00:20:33,600
how we can kind of use the same

00:20:31,919 --> 00:20:35,679
mental model thinking about like okay

00:20:33,600 --> 00:20:36,960
what's our goal and then we assess the

00:20:35,679 --> 00:20:38,400
state of the world and when they're not

00:20:36,960 --> 00:20:42,159
equal we take action

00:20:38,400 --> 00:20:45,360
uh next slide please but why do we

00:20:42,159 --> 00:20:49,120
no but why do we even care

00:20:45,360 --> 00:20:54,799
about goals at such a low level um

00:20:49,120 --> 00:20:56,640
yeah next slide i want to be seeing the

00:20:54,799 --> 00:20:58,240
dinner she's still there okay yeah this

00:20:56,640 --> 00:20:59,520
is good uh why hardware agents

00:20:58,240 --> 00:21:01,600
all right so the reason why we want

00:20:59,520 --> 00:21:02,080
hardware agents is because we have to

00:21:01,600 --> 00:21:04,080
assess

00:21:02,080 --> 00:21:05,280
that our goals are meeting their uh that

00:21:04,080 --> 00:21:06,720
we're meeting our goals at the lowest

00:21:05,280 --> 00:21:07,600
level in order to meet the highest level

00:21:06,720 --> 00:21:09,360
so for example

00:21:07,600 --> 00:21:11,280
if we want a two millisecond 99th

00:21:09,360 --> 00:21:12,640
percentile guarantee at the client

00:21:11,280 --> 00:21:14,240
we're going to need a one millisecond

00:21:12,640 --> 00:21:16,480
nine minute guarantee at the database

00:21:14,240 --> 00:21:17,679
next slide

00:21:16,480 --> 00:21:19,360
and in order to achieve a one

00:21:17,679 --> 00:21:21,360
millisecond at the database we're going

00:21:19,360 --> 00:21:23,280
to need a 100 microsecond

00:21:21,360 --> 00:21:24,799
uh goal to be met at our disk or our

00:21:23,280 --> 00:21:27,520
network level

00:21:24,799 --> 00:21:29,280
so so building up a goal-based system

00:21:27,520 --> 00:21:30,240
really starts with that lowest level and

00:21:29,280 --> 00:21:34,400
then we build up

00:21:30,240 --> 00:21:37,039
next slide please so let's see

00:21:34,400 --> 00:21:37,520
how we can build up the next step up the

00:21:37,039 --> 00:21:40,559
stack

00:21:37,520 --> 00:21:42,000
is going to be our specialized software

00:21:40,559 --> 00:21:43,360
supervisory agents

00:21:42,000 --> 00:21:45,120
and the simplest goal-based agent you

00:21:43,360 --> 00:21:47,600
can think of is this process supervisor

00:21:45,120 --> 00:21:48,320
so the goal is uh my cassandra database

00:21:47,600 --> 00:21:49,840
is running

00:21:48,320 --> 00:21:51,360
and then the observed city of the world

00:21:49,840 --> 00:21:52,640
is either running or it's not if it's

00:21:51,360 --> 00:21:55,520
not running i'll start it

00:21:52,640 --> 00:21:56,320
if it is running uh i like i leave it

00:21:55,520 --> 00:21:58,320
leave it be

00:21:56,320 --> 00:22:00,720
um but you can actually take this a step

00:21:58,320 --> 00:22:03,520
further and actually supervise your

00:22:00,720 --> 00:22:04,480
uh your cassandra jvms as well and you

00:22:03,520 --> 00:22:07,600
can establish

00:22:04,480 --> 00:22:09,120
goal uh throughput uh so how much time

00:22:07,600 --> 00:22:10,480
your cassandra database spends running

00:22:09,120 --> 00:22:11,280
application code versus garbage

00:22:10,480 --> 00:22:12,960
collecting

00:22:11,280 --> 00:22:14,720
you you presumably want that to be a

00:22:12,960 --> 00:22:15,679
pretty high percentage how many of you

00:22:14,720 --> 00:22:17,679
out there have ever seen

00:22:15,679 --> 00:22:19,200
a cassandra database enter a gc spiral

00:22:17,679 --> 00:22:21,039
of death um

00:22:19,200 --> 00:22:22,880
i'm imagining that a bunch of people are

00:22:21,039 --> 00:22:23,919
raising their hands it happens sometimes

00:22:22,880 --> 00:22:26,640
in java databases

00:22:23,919 --> 00:22:28,640
especially when a query comes in that

00:22:26,640 --> 00:22:30,320
asks it to load the entire data set into

00:22:28,640 --> 00:22:32,320
memory

00:22:30,320 --> 00:22:33,760
but in order to automatically recover

00:22:32,320 --> 00:22:35,679
from those and self-heal

00:22:33,760 --> 00:22:37,679
we have to be setting goals about that

00:22:35,679 --> 00:22:40,480
throughput so that we can actually see

00:22:37,679 --> 00:22:42,640
when the database enters that bad bad

00:22:40,480 --> 00:22:44,080
scenario and can remedy it by

00:22:42,640 --> 00:22:45,760
for example taking a core dump and

00:22:44,080 --> 00:22:46,400
restarting the database or killing the

00:22:45,760 --> 00:22:49,840
database

00:22:46,400 --> 00:22:49,840
uh next slide please

00:22:50,000 --> 00:22:54,159
um but we can take it even further so

00:22:52,400 --> 00:22:56,159
both of those previous ones again don't

00:22:54,159 --> 00:22:57,840
require any coordination or any

00:22:56,159 --> 00:22:59,760
real use of the cluster manager beyond

00:22:57,840 --> 00:23:01,919
please please terminate me

00:22:59,760 --> 00:23:03,919
but we can actually build even further

00:23:01,919 --> 00:23:05,280
and look at system-wide or cluster-wide

00:23:03,919 --> 00:23:06,799
orchestration

00:23:05,280 --> 00:23:08,880
using this goal-based system so for

00:23:06,799 --> 00:23:11,039
example we can set goals about

00:23:08,880 --> 00:23:12,799
our continuous backup process we can we

00:23:11,039 --> 00:23:14,559
can say like you know we desire that

00:23:12,799 --> 00:23:15,760
every file and disk is synced to our

00:23:14,559 --> 00:23:18,080
backup as of some

00:23:15,760 --> 00:23:19,360
you know uh delay so for example like a

00:23:18,080 --> 00:23:21,039
a 10 minute

00:23:19,360 --> 00:23:22,960
point in time backup should be taken

00:23:21,039 --> 00:23:24,640
every 10 minutes but we only want to

00:23:22,960 --> 00:23:26,559
upload the deltas

00:23:24,640 --> 00:23:28,159
or we can also use the same for hardware

00:23:26,559 --> 00:23:30,240
or software replacement

00:23:28,159 --> 00:23:32,080
or even for something super complex like

00:23:30,240 --> 00:23:33,520
a distributed repair where we're

00:23:32,080 --> 00:23:35,600
repairing the entire data set

00:23:33,520 --> 00:23:37,360
coordinating through the cluster manager

00:23:35,600 --> 00:23:38,000
and all of those we can phrase in terms

00:23:37,360 --> 00:23:39,679
of

00:23:38,000 --> 00:23:41,440
our goals and we can take actions in

00:23:39,679 --> 00:23:44,159
terms of state machine

00:23:41,440 --> 00:23:44,159
next slide please

00:23:45,039 --> 00:23:48,320
all right so let's let's dive into a

00:23:46,720 --> 00:23:50,159
couple of concrete examples of how to

00:23:48,320 --> 00:23:52,480
build those state machines

00:23:50,159 --> 00:23:53,679
and to do that i just want to cover uh

00:23:52,480 --> 00:23:55,760
really quickly that the

00:23:53,679 --> 00:23:57,440
difference between what we call

00:23:55,760 --> 00:23:57,919
imperative control planes which are very

00:23:57,440 --> 00:23:59,279
hard

00:23:57,919 --> 00:24:00,960
to scale and very hard to make

00:23:59,279 --> 00:24:01,520
self-healing and declarative control

00:24:00,960 --> 00:24:04,159
planes

00:24:01,520 --> 00:24:05,840
um imperative control planes are what i

00:24:04,159 --> 00:24:07,760
call ssh in a for loop so

00:24:05,840 --> 00:24:09,600
so how many cassandra operators have

00:24:07,760 --> 00:24:10,799
written a bash script that ssh is to

00:24:09,600 --> 00:24:12,159
every cassandra node and run some

00:24:10,799 --> 00:24:14,000
nodetool command

00:24:12,159 --> 00:24:16,480
i'm raising my hand i'm assuming a lot

00:24:14,000 --> 00:24:18,400
of other people are raising their hand

00:24:16,480 --> 00:24:20,159
because it's a very easy thing to do but

00:24:18,400 --> 00:24:21,360
it doesn't scale and it doesn't scale

00:24:20,159 --> 00:24:24,240
because it doesn't build in

00:24:21,360 --> 00:24:25,440
failure so you don't know when sch can't

00:24:24,240 --> 00:24:27,200
reach that node did it

00:24:25,440 --> 00:24:29,120
execute the command did it start

00:24:27,200 --> 00:24:29,679
executing the command what state is it

00:24:29,120 --> 00:24:32,000
in

00:24:29,679 --> 00:24:33,200
um there's no way to know what's going

00:24:32,000 --> 00:24:33,919
on with your system and so then you have

00:24:33,200 --> 00:24:35,919
to write a second

00:24:33,919 --> 00:24:37,760
ssh script which just checks the status

00:24:35,919 --> 00:24:38,720
of the first one and then the second one

00:24:37,760 --> 00:24:40,960
and you end up with turtles all the way

00:24:38,720 --> 00:24:43,520
down whereas on the right hand side here

00:24:40,960 --> 00:24:44,720
we have a declarative based system where

00:24:43,520 --> 00:24:46,159
the only thing the control plane is

00:24:44,720 --> 00:24:47,840
allowed to do is write into a highly

00:24:46,159 --> 00:24:48,640
available database i want the state

00:24:47,840 --> 00:24:50,960
transition

00:24:48,640 --> 00:24:51,919
and then all the nodes individually are

00:24:50,960 --> 00:24:54,880
going to

00:24:51,919 --> 00:24:55,919
constantly pull that state compare it to

00:24:54,880 --> 00:24:57,200
the state of their node

00:24:55,919 --> 00:25:01,840
and make transitions like dinesh

00:24:57,200 --> 00:25:01,840
mentioned next slide please

00:25:03,679 --> 00:25:07,120
and within the node we're going to

00:25:05,600 --> 00:25:08,799
follow a really simple algorithm we're

00:25:07,120 --> 00:25:10,080
going to assess the state of the world

00:25:08,799 --> 00:25:11,279
and we're going to assess the goal or

00:25:10,080 --> 00:25:12,400
desired state of the world and when

00:25:11,279 --> 00:25:13,600
those aren't equal we're going to

00:25:12,400 --> 00:25:14,960
compute a path

00:25:13,600 --> 00:25:16,799
and we're going to compute a path to the

00:25:14,960 --> 00:25:18,159
desired state that tries to minimize

00:25:16,799 --> 00:25:21,200
downtime

00:25:18,159 --> 00:25:22,880
next next slide and the reason why we

00:25:21,200 --> 00:25:24,240
want to minimize downtime is because we

00:25:22,880 --> 00:25:24,880
want to prefer maintenance that doesn't

00:25:24,240 --> 00:25:27,279
require

00:25:24,880 --> 00:25:28,799
us to take that availability risk but if

00:25:27,279 --> 00:25:29,200
we do have to take that availability

00:25:28,799 --> 00:25:30,320
risk

00:25:29,200 --> 00:25:32,400
we're going to try to do that

00:25:30,320 --> 00:25:33,840
maintenance as fast as possible so if we

00:25:32,400 --> 00:25:35,840
have a cluster of 100 nodes

00:25:33,840 --> 00:25:37,360
spread across three availability zones

00:25:35,840 --> 00:25:39,600
or cassandra racks

00:25:37,360 --> 00:25:40,880
we're going to try to do maintenance a

00:25:39,600 --> 00:25:43,200
half rack at a time

00:25:40,880 --> 00:25:44,799
so that we can have a mixed mode state

00:25:43,200 --> 00:25:45,919
which is very risky for a database

00:25:44,799 --> 00:25:49,200
because

00:25:45,919 --> 00:25:50,880
mixed mode states are the least tested

00:25:49,200 --> 00:25:52,480
so we want to have that risk for as

00:25:50,880 --> 00:25:53,520
little time as possible

00:25:52,480 --> 00:25:55,919
and it turns out that if you look at

00:25:53,520 --> 00:25:57,440
that paper earlier the math says that

00:25:55,919 --> 00:25:59,120
whether you do maintenance on a 100

00:25:57,440 --> 00:26:00,000
cluster one number a time or a half a

00:25:59,120 --> 00:26:01,919
rack at a time

00:26:00,000 --> 00:26:03,360
it's the same risk to your availability

00:26:01,919 --> 00:26:05,039
so you should just do it fast

00:26:03,360 --> 00:26:06,400
and then finally when you do need to

00:26:05,039 --> 00:26:09,039
coordinate you use the cluster manager

00:26:06,400 --> 00:26:09,039
next slide please

00:26:09,600 --> 00:26:12,720
all right so let's do a concrete example

00:26:11,200 --> 00:26:14,640
let's do a software upgrade so

00:26:12,720 --> 00:26:15,679
uh in a single environment let's say in

00:26:14,640 --> 00:26:17,279
this case the production environment

00:26:15,679 --> 00:26:19,279
we're going to upgrade our cassandra

00:26:17,279 --> 00:26:24,320
cluster to 40 alpha 5.

00:26:19,279 --> 00:26:28,320
next slide please

00:26:24,320 --> 00:26:30,640
and uh and and so we set that desire

00:26:28,320 --> 00:26:31,679
uh we set that goal now the upgrade

00:26:30,640 --> 00:26:32,720
agent which is running on all our

00:26:31,679 --> 00:26:34,559
cassandra nodes

00:26:32,720 --> 00:26:35,919
is going to see that the running version

00:26:34,559 --> 00:26:37,200
of software is not equal to the gold

00:26:35,919 --> 00:26:38,640
version and it's going to begin our

00:26:37,200 --> 00:26:39,760
upgrade state machine

00:26:38,640 --> 00:26:41,760
and one interesting thing about the

00:26:39,760 --> 00:26:43,200
upgrade state machine is that

00:26:41,760 --> 00:26:44,799
it inherently has to coordinate through

00:26:43,200 --> 00:26:45,919
the cluster manager because it's a low

00:26:44,799 --> 00:26:49,200
node can't know

00:26:45,919 --> 00:26:50,240
if replicas in another rack are already

00:26:49,200 --> 00:26:52,559
down

00:26:50,240 --> 00:26:54,080
so you can see that uh the very first

00:26:52,559 --> 00:26:54,720
step of our state machine is going to be

00:26:54,080 --> 00:26:56,640
acquire

00:26:54,720 --> 00:26:58,080
a lock uh through the cluster manager

00:26:56,640 --> 00:26:59,440
which is just a lock is just a state

00:26:58,080 --> 00:27:02,080
machine with two states

00:26:59,440 --> 00:27:03,120
uh acquire a lock that allows you uh to

00:27:02,080 --> 00:27:05,360
do maintenance

00:27:03,120 --> 00:27:07,120
um obviously uh something that i haven't

00:27:05,360 --> 00:27:09,039
put here but at all times you need to

00:27:07,120 --> 00:27:10,480
have your cassandra database provisioned

00:27:09,039 --> 00:27:11,600
with enough excess capacity for

00:27:10,480 --> 00:27:12,559
maintenance so that means that you

00:27:11,600 --> 00:27:15,279
should be able to survive

00:27:12,559 --> 00:27:17,039
an entire rack failing at any given time

00:27:15,279 --> 00:27:18,559
or entire replica set

00:27:17,039 --> 00:27:20,880
and netflix we have a separate set of

00:27:18,559 --> 00:27:23,039
agents which are continuously reporting

00:27:20,880 --> 00:27:25,039
information about our capacity and then

00:27:23,039 --> 00:27:26,240
we have like an offline process for

00:27:25,039 --> 00:27:27,360
if we're under capacity so our

00:27:26,240 --> 00:27:28,880
maintenance systems don't have to worry

00:27:27,360 --> 00:27:31,120
about that because they can just assume

00:27:28,880 --> 00:27:33,120
that we have enough capacity

00:27:31,120 --> 00:27:34,640
so first we acquire the rack lock and

00:27:33,120 --> 00:27:36,000
then we enter if you were seeing the

00:27:34,640 --> 00:27:36,720
keynote earlier jonathan was talking

00:27:36,000 --> 00:27:38,240
about

00:27:36,720 --> 00:27:39,919
you have to actually gracefully drain

00:27:38,240 --> 00:27:41,120
you have to wait for clients to drain

00:27:39,919 --> 00:27:42,159
off then you have to pull yourself out

00:27:41,120 --> 00:27:43,679
of gossip

00:27:42,159 --> 00:27:45,679
and then finally you're ready to do the

00:27:43,679 --> 00:27:47,039
software upgrade and uh we like to do

00:27:45,679 --> 00:27:48,720
software upgrades in a self-healing

00:27:47,039 --> 00:27:50,559
system with atomic state transitions so

00:27:48,720 --> 00:27:52,559
we boot into an imager

00:27:50,559 --> 00:27:54,080
next slide please and the imager will

00:27:52,559 --> 00:27:55,840
actually reach out to

00:27:54,080 --> 00:27:58,880
an image repository if you're running

00:27:55,840 --> 00:28:01,360
ec2 you can pull amis if you're

00:27:58,880 --> 00:28:03,440
running in kubernetes this is like a pod

00:28:01,360 --> 00:28:05,679
spec or like a like a set of containers

00:28:03,440 --> 00:28:07,120
and uh you're going to boot into the new

00:28:05,679 --> 00:28:08,720
image if you can and if you can't get

00:28:07,120 --> 00:28:10,480
the the new image you're going to reboot

00:28:08,720 --> 00:28:12,320
back into the old image

00:28:10,480 --> 00:28:15,039
and then restart the database and you

00:28:12,320 --> 00:28:16,720
can kind of see a key insight as is

00:28:15,039 --> 00:28:18,159
developing here which is every one of

00:28:16,720 --> 00:28:19,360
these state machines has to deal with

00:28:18,159 --> 00:28:20,880
failure and success

00:28:19,360 --> 00:28:22,240
so unlike that bash script where you

00:28:20,880 --> 00:28:23,200
implicitly were only dealing with the

00:28:22,240 --> 00:28:24,960
happy path

00:28:23,200 --> 00:28:26,880
uh when you phrase your agents in terms

00:28:24,960 --> 00:28:28,399
of distributed state machines

00:28:26,880 --> 00:28:29,760
and you actually draw out these state

00:28:28,399 --> 00:28:30,799
transitions you force yourself to

00:28:29,760 --> 00:28:32,080
actually think about like okay what

00:28:30,799 --> 00:28:33,440
happens if i can't get the

00:28:32,080 --> 00:28:34,720
the new image what happens if i can't

00:28:33,440 --> 00:28:35,360
pull that well i have to boot into the

00:28:34,720 --> 00:28:37,440
old one

00:28:35,360 --> 00:28:38,799
what happens if uh for whatever reason i

00:28:37,440 --> 00:28:41,679
can't boot into anything

00:28:38,799 --> 00:28:42,000
um well then you have to to to terminate

00:28:41,679 --> 00:28:43,520
then

00:28:42,000 --> 00:28:46,080
ask the cluster determine yourself next

00:28:43,520 --> 00:28:46,080
slide please

00:28:48,159 --> 00:28:53,840
and i that was an example of a

00:28:51,600 --> 00:28:55,360
kind of like i would say a slightly

00:28:53,840 --> 00:28:56,640
advanced maneuver using a distributed

00:28:55,360 --> 00:28:58,080
state machine which is a distributed

00:28:56,640 --> 00:28:59,200
software upgrade that did require

00:28:58,080 --> 00:29:00,159
downtime so we had to coordinate through

00:28:59,200 --> 00:29:01,760
the cluster manager

00:29:00,159 --> 00:29:04,000
uh now let's do a really fun one where

00:29:01,760 --> 00:29:05,360
we actually do state transfer as well

00:29:04,000 --> 00:29:07,200
so in this one we can see that the

00:29:05,360 --> 00:29:10,399
operator has declared a desire for

00:29:07,200 --> 00:29:12,559
a new or a goal for a new hardware type

00:29:10,399 --> 00:29:13,919
so maybe you want more disk space or

00:29:12,559 --> 00:29:15,039
somebody came along and said hey we

00:29:13,919 --> 00:29:16,399
bought a bunch of these new computers

00:29:15,039 --> 00:29:17,520
can you guys move your center databases

00:29:16,399 --> 00:29:19,360
onto them

00:29:17,520 --> 00:29:21,360
so so that's what we're going to do now

00:29:19,360 --> 00:29:23,679
next next slide please

00:29:21,360 --> 00:29:25,279
and to do that we have to do something

00:29:23,679 --> 00:29:26,240
kind of interesting which is we actually

00:29:25,279 --> 00:29:28,240
have to start

00:29:26,240 --> 00:29:29,279
with asking the cluster manager to help

00:29:28,240 --> 00:29:30,799
um so

00:29:29,279 --> 00:29:32,640
and this is because the agents on a

00:29:30,799 --> 00:29:33,360
single node they can't make new hardware

00:29:32,640 --> 00:29:34,799
exist

00:29:33,360 --> 00:29:36,640
they can't make it join the cluster they

00:29:34,799 --> 00:29:38,799
can't make it be available

00:29:36,640 --> 00:29:40,080
so the very first step is each agent is

00:29:38,799 --> 00:29:40,720
going to basically ask the cluster

00:29:40,080 --> 00:29:42,480
manager

00:29:40,720 --> 00:29:45,200
um you know essentially i'm waiting for

00:29:42,480 --> 00:29:47,600
a buddy i'm waiting for a new node

00:29:45,200 --> 00:29:48,640
of this type to exist um and then the

00:29:47,600 --> 00:29:50,880
cluster manager

00:29:48,640 --> 00:29:52,720
uh will reach out to your control plane

00:29:50,880 --> 00:29:55,440
api via kubernetes be it

00:29:52,720 --> 00:29:56,720
uh be it ec2 and ask it to launch some

00:29:55,440 --> 00:29:58,640
nodes

00:29:56,720 --> 00:30:00,000
and then simultaneously your old nodes

00:29:58,640 --> 00:30:01,200
and your new nodes now enter

00:30:00,000 --> 00:30:03,120
two separate state machines that

00:30:01,200 --> 00:30:05,440
cooperate through the cluster manager

00:30:03,120 --> 00:30:09,039
next slide please so let's start with

00:30:05,440 --> 00:30:12,640
the leaving state machine

00:30:09,039 --> 00:30:14,159
where we can take advantage of the

00:30:12,640 --> 00:30:15,679
distributed nature of cassandra and we

00:30:14,159 --> 00:30:17,840
can actually do

00:30:15,679 --> 00:30:19,120
this entire process in parallel across

00:30:17,840 --> 00:30:20,799
all of our old nodes

00:30:19,120 --> 00:30:23,039
so what we see here on the slide wait

00:30:20,799 --> 00:30:24,720
for new nodes uh to show up

00:30:23,039 --> 00:30:26,559
uh start checkpointing our state into

00:30:24,720 --> 00:30:29,120
our consistent backup system

00:30:26,559 --> 00:30:31,120
uh be it s3 via gfs wherever we're

00:30:29,120 --> 00:30:33,760
keeping our backups for our database

00:30:31,120 --> 00:30:35,120
and then finally our nodes will start

00:30:33,760 --> 00:30:36,159
advertising that they're ready for

00:30:35,120 --> 00:30:37,600
replacement

00:30:36,159 --> 00:30:40,000
by average we call it advertising a

00:30:37,600 --> 00:30:41,600
token uh you can also think of it like a

00:30:40,000 --> 00:30:42,960
shard or just generally like a data set

00:30:41,600 --> 00:30:43,679
a piece of data that's on a physical

00:30:42,960 --> 00:30:47,200
node

00:30:43,679 --> 00:30:48,399
um next next slide please and once we

00:30:47,200 --> 00:30:49,760
advertise the token

00:30:48,399 --> 00:30:51,840
we're going to enter the part that has

00:30:49,760 --> 00:30:53,840
to coordinate through the

00:30:51,840 --> 00:30:55,279
cluster manager um so just for brevity

00:30:53,840 --> 00:30:56,320
i'm just going to cover the happy path

00:30:55,279 --> 00:30:58,320
here but you can see

00:30:56,320 --> 00:30:59,519
that there is a sad path that has to be

00:30:58,320 --> 00:31:00,399
programmed for each possible state

00:30:59,519 --> 00:31:02,080
transition

00:31:00,399 --> 00:31:04,240
but generally speaking an old node is

00:31:02,080 --> 00:31:06,000
going to advertise a token it's going to

00:31:04,240 --> 00:31:08,799
buddy up with a new node

00:31:06,000 --> 00:31:10,480
at that point we take the database down

00:31:08,799 --> 00:31:11,840
so there are some pending mutations that

00:31:10,480 --> 00:31:12,880
have happened since the consistent

00:31:11,840 --> 00:31:15,039
checkpoint

00:31:12,880 --> 00:31:17,200
that we now need to sync so that's so we

00:31:15,039 --> 00:31:19,600
enter the sync delta phase

00:31:17,200 --> 00:31:21,679
after we sync deltas we're going to do a

00:31:19,600 --> 00:31:23,519
full data checksum using xxhash for

00:31:21,679 --> 00:31:24,559
speed um if you do shop26 it's going to

00:31:23,519 --> 00:31:25,440
take forever you're not going to do

00:31:24,559 --> 00:31:27,039
checksums

00:31:25,440 --> 00:31:28,640
um but if you use xhash you can actually

00:31:27,039 --> 00:31:30,000
get it done in like no time at all

00:31:28,640 --> 00:31:31,440
all right so you're going to verify that

00:31:30,000 --> 00:31:33,200
the data on the old node in the new node

00:31:31,440 --> 00:31:34,720
are bite for byte identical

00:31:33,200 --> 00:31:36,720
and then finally you're going to

00:31:34,720 --> 00:31:38,080
transfer ownership from the old node to

00:31:36,720 --> 00:31:40,000
the new node

00:31:38,080 --> 00:31:41,679
and then the leaving state machine will

00:31:40,000 --> 00:31:43,200
enter a termination state allows the

00:31:41,679 --> 00:31:44,240
cluster manager to help it by

00:31:43,200 --> 00:31:45,840
terminating

00:31:44,240 --> 00:31:47,200
because its job is over it's transferred

00:31:45,840 --> 00:31:47,919
its token it's transferred its ownership

00:31:47,200 --> 00:31:50,960
of data

00:31:47,919 --> 00:31:52,399
uh next slide please and we can look at

00:31:50,960 --> 00:31:53,039
the joining state machine for the kind

00:31:52,399 --> 00:31:54,880
of

00:31:53,039 --> 00:31:56,559
other side of the picture so we saw kind

00:31:54,880 --> 00:31:58,320
of how the leaving node works

00:31:56,559 --> 00:31:59,919
on the joining state machine we can also

00:31:58,320 --> 00:32:01,919
have all of our nodes that are joining

00:31:59,919 --> 00:32:04,559
in parallel

00:32:01,919 --> 00:32:05,679
acquire a pending token so buddy up with

00:32:04,559 --> 00:32:08,399
one of those old nodes

00:32:05,679 --> 00:32:09,360
and then sync that node's checkpoint

00:32:08,399 --> 00:32:11,200
down to disk

00:32:09,360 --> 00:32:12,399
and the key for why we're going to do

00:32:11,200 --> 00:32:13,919
that is twofold

00:32:12,399 --> 00:32:16,000
the first one is because we can do it

00:32:13,919 --> 00:32:17,120
massively parallel um so we can do all

00:32:16,000 --> 00:32:19,200
of our nodes in our cassandra cluster

00:32:17,120 --> 00:32:20,720
simultaneously we don't have to worry

00:32:19,200 --> 00:32:22,640
about like oh well we're going to put

00:32:20,720 --> 00:32:25,760
too much bandwidth pressure on cassandra

00:32:22,640 --> 00:32:27,200
or like streaming is fast or slow or so

00:32:25,760 --> 00:32:28,720
for example in cassandra trunk streaming

00:32:27,200 --> 00:32:30,720
is quite quick but in cassandra 3 or

00:32:28,720 --> 00:32:31,519
xander 2 streaming is very slow compared

00:32:30,720 --> 00:32:33,760
to

00:32:31,519 --> 00:32:35,360
the native native hardware capabilities

00:32:33,760 --> 00:32:36,799
um but this is actually really key

00:32:35,360 --> 00:32:37,519
because we're exploiting the inherent

00:32:36,799 --> 00:32:38,880
parallelism

00:32:37,519 --> 00:32:41,200
of the self-failing distributed state

00:32:38,880 --> 00:32:42,880
machine all of our new nodes are pulling

00:32:41,200 --> 00:32:44,320
from the backup simultaneously

00:32:42,880 --> 00:32:46,240
and then only once they pull that

00:32:44,320 --> 00:32:48,799
consistent snapshot do we wait

00:32:46,240 --> 00:32:50,880
for uh for do we wait for our buddy to

00:32:48,799 --> 00:32:54,840
stop next slide please

00:32:50,880 --> 00:32:56,640
and then uh and then once we uh wait for

00:32:54,840 --> 00:32:58,720
stop uh

00:32:56,640 --> 00:33:00,240
we wait for our buddy that's when we we

00:32:58,720 --> 00:33:00,880
start looking like that software upgrade

00:33:00,240 --> 00:33:03,600
where

00:33:00,880 --> 00:33:05,039
uh one half rack at a time uh nodes are

00:33:03,600 --> 00:33:06,640
going to stop cassandra

00:33:05,039 --> 00:33:08,559
and if we remember in the leaving state

00:33:06,640 --> 00:33:11,120
machine the stopped cassandra

00:33:08,559 --> 00:33:12,960
is sinking deltas so the new node has to

00:33:11,120 --> 00:33:15,600
receive those deltas

00:33:12,960 --> 00:33:17,039
and then we're going to verify checksums

00:33:15,600 --> 00:33:18,240
so you can see that this is mirroring

00:33:17,039 --> 00:33:21,120
the sending

00:33:18,240 --> 00:33:22,880
node and then finally we receive the

00:33:21,120 --> 00:33:25,279
token

00:33:22,880 --> 00:33:26,960
via other transfer and uh unlike the

00:33:25,279 --> 00:33:28,640
leaving one which terminates we start

00:33:26,960 --> 00:33:31,440
cassandra

00:33:28,640 --> 00:33:31,679
so uh yeah i think that's that that kind

00:33:31,440 --> 00:33:34,320
of

00:33:31,679 --> 00:33:35,519
describes it end to end um and we can

00:33:34,320 --> 00:33:36,960
see how we're building up these

00:33:35,519 --> 00:33:38,880
progressively more complex

00:33:36,960 --> 00:33:40,480
distributed state machines but we're

00:33:38,880 --> 00:33:41,919
always leveraging the same basic

00:33:40,480 --> 00:33:42,960
architecture

00:33:41,919 --> 00:33:45,200
and we can actually take it a step

00:33:42,960 --> 00:33:47,200
further and use this to provide

00:33:45,200 --> 00:33:49,600
any arbitrary maintenance tasks so for

00:33:47,200 --> 00:33:52,000
example you can use this architecture to

00:33:49,600 --> 00:33:53,120
run repair on your cassandra cluster

00:33:52,000 --> 00:33:54,640
your repair agent

00:33:53,120 --> 00:33:56,399
just observes the repaired state of the

00:33:54,640 --> 00:33:57,519
node and compares it with the desired

00:33:56,399 --> 00:33:58,960
repaired state

00:33:57,519 --> 00:34:00,640
um and then you just run through the

00:33:58,960 --> 00:34:01,200
distributed state machine as described

00:34:00,640 --> 00:34:04,080
in the

00:34:01,200 --> 00:34:06,399
open source cassandra 14346 ticket which

00:34:04,080 --> 00:34:09,919
is reproduced here

00:34:06,399 --> 00:34:12,079
um but i you know to kind of wrap up

00:34:09,919 --> 00:34:13,200
a next slide please i think what we've

00:34:12,079 --> 00:34:14,639
been able to see here

00:34:13,200 --> 00:34:17,280
is that no matter the type of

00:34:14,639 --> 00:34:18,960
maintenance activity that you have to do

00:34:17,280 --> 00:34:20,879
on a distributed database you just

00:34:18,960 --> 00:34:22,320
follow the same basic steps so number

00:34:20,879 --> 00:34:22,879
one instead of doing the thing you want

00:34:22,320 --> 00:34:24,480
to do

00:34:22,879 --> 00:34:26,240
write down what you want to do write it

00:34:24,480 --> 00:34:27,679
down in a database write it down in a

00:34:26,240 --> 00:34:29,119
state machine

00:34:27,679 --> 00:34:31,280
you know just write it down in a

00:34:29,119 --> 00:34:33,440
database in a highly available store

00:34:31,280 --> 00:34:35,040
then number two program your agents on

00:34:33,440 --> 00:34:37,280
every machine to follow the same

00:34:35,040 --> 00:34:39,119
identical distributed state machine

00:34:37,280 --> 00:34:40,480
it is a little bit harder to write

00:34:39,119 --> 00:34:41,679
because you have to actually think about

00:34:40,480 --> 00:34:45,119
those failure modes

00:34:41,679 --> 00:34:46,800
um but it ends up with an inherently

00:34:45,119 --> 00:34:48,000
parallel system instead of an inherently

00:34:46,800 --> 00:34:50,079
serial one so it's a very

00:34:48,000 --> 00:34:51,280
fast uh maintenance activity which

00:34:50,079 --> 00:34:52,079
lowers that risk that we're talking

00:34:51,280 --> 00:34:53,839
about earlier

00:34:52,079 --> 00:34:55,440
um and then finally if you ever find

00:34:53,839 --> 00:34:57,440
yourself needing to communicate

00:34:55,440 --> 00:34:58,640
do so with a state transition rather

00:34:57,440 --> 00:35:00,079
than a synchronous message

00:34:58,640 --> 00:35:01,520
so you'll notice that earlier in the

00:35:00,079 --> 00:35:02,000
like the hardware replacement one i

00:35:01,520 --> 00:35:03,280
didn't say

00:35:02,000 --> 00:35:05,280
that the new node and the old node are

00:35:03,280 --> 00:35:08,160
going to send each other an rpc um

00:35:05,280 --> 00:35:09,760
instead they record a state transition

00:35:08,160 --> 00:35:12,480
like i'm waiting for a buddy

00:35:09,760 --> 00:35:12,960
or i've acquired a lock or i'm acquiring

00:35:12,480 --> 00:35:16,240
uh

00:35:12,960 --> 00:35:17,920
my my uh my destination node

00:35:16,240 --> 00:35:19,280
so a key insight there is that the only

00:35:17,920 --> 00:35:20,880
form of communication is going to be

00:35:19,280 --> 00:35:23,040
through those state transitions

00:35:20,880 --> 00:35:24,079
um and and guess absolutely changing

00:35:23,040 --> 00:35:25,599
from an imperative

00:35:24,079 --> 00:35:26,720
uh programming model to the declarative

00:35:25,599 --> 00:35:28,079
programming model required by a

00:35:26,720 --> 00:35:29,760
self-healing system is hard

00:35:28,079 --> 00:35:31,280
um just like if you're used to writing

00:35:29,760 --> 00:35:33,040
synchronous code it's hard to write

00:35:31,280 --> 00:35:34,800
asynchronous with concurrent code

00:35:33,040 --> 00:35:36,079
um but but we've found that it's the

00:35:34,800 --> 00:35:37,920
only way to make our systems truly

00:35:36,079 --> 00:35:40,800
robust to failure because it forces you

00:35:37,920 --> 00:35:42,160
to program in those failure edge cases

00:35:40,800 --> 00:35:44,240
and with that i'm going to hand it back

00:35:42,160 --> 00:35:45,440
over to tanesh for so i

00:35:44,240 --> 00:35:47,520
kind of explained how we can use

00:35:45,440 --> 00:35:48,720
self-healing systems to make changes um

00:35:47,520 --> 00:35:49,599
dinesh now we'll cover a little bit

00:35:48,720 --> 00:35:53,440
about how

00:35:49,599 --> 00:35:53,440
we can know which changes to make

00:35:53,599 --> 00:35:57,040
all right thank you joey i hope i'm

00:35:55,920 --> 00:36:00,240
audible

00:35:57,040 --> 00:36:01,440
um so let's talk about safely

00:36:00,240 --> 00:36:03,920
experimenting in

00:36:01,440 --> 00:36:05,920
production and i have one sentence

00:36:03,920 --> 00:36:09,040
answer to that there's no way you can

00:36:05,920 --> 00:36:11,599
experiment safely in production

00:36:09,040 --> 00:36:13,440
so um so let's see what what's the next

00:36:11,599 --> 00:36:16,480
best thing that we can do

00:36:13,440 --> 00:36:19,119
so uh the next best thing is that we

00:36:16,480 --> 00:36:20,480
need to emulate production first and in

00:36:19,119 --> 00:36:22,640
order to do that

00:36:20,480 --> 00:36:23,920
uh we can copy the clusters gold

00:36:22,640 --> 00:36:26,800
document to

00:36:23,920 --> 00:36:28,560
a staging or test environment and the

00:36:26,800 --> 00:36:31,520
self-healing system should restore

00:36:28,560 --> 00:36:32,160
it from backup and once the system is

00:36:31,520 --> 00:36:34,400
restored

00:36:32,160 --> 00:36:35,520
what we can do is capture live traffic

00:36:34,400 --> 00:36:38,800
from production

00:36:35,520 --> 00:36:41,440
using fql in cassandra 4.0

00:36:38,800 --> 00:36:42,160
we have uh fql which is full query

00:36:41,440 --> 00:36:45,119
logging

00:36:42,160 --> 00:36:46,079
and it allows us to capture the traffic

00:36:45,119 --> 00:36:49,040
um

00:36:46,079 --> 00:36:49,599
and our we can use something like harry

00:36:49,040 --> 00:36:53,760
which is

00:36:49,599 --> 00:36:54,320
a um which is a system that's used for

00:36:53,760 --> 00:36:56,320
fuzz

00:36:54,320 --> 00:36:57,680
or property based testing to generate

00:36:56,320 --> 00:37:00,720
synthetic queries

00:36:57,680 --> 00:37:03,680
uh based on that uh what we can do

00:37:00,720 --> 00:37:05,119
is modify the configuration that we want

00:37:03,680 --> 00:37:06,720
to experiment so let's say if your

00:37:05,119 --> 00:37:10,400
configuration is c we

00:37:06,720 --> 00:37:12,400
move it to c data by changing some stuff

00:37:10,400 --> 00:37:14,880
and then that new configuration is

00:37:12,400 --> 00:37:17,200
applied uh to this experimental

00:37:14,880 --> 00:37:18,720
setup and then we apply load that is

00:37:17,200 --> 00:37:19,440
matching your production query

00:37:18,720 --> 00:37:21,599
distribution

00:37:19,440 --> 00:37:23,680
or synthetic queries that you generate

00:37:21,599 --> 00:37:26,160
and you throw at this system

00:37:23,680 --> 00:37:27,760
and we can run chaos agents to simulate

00:37:26,160 --> 00:37:31,040
real world failures

00:37:27,760 --> 00:37:32,160
like pulling a node offline um causing

00:37:31,040 --> 00:37:35,119
network jitter

00:37:32,160 --> 00:37:37,119
packet drops and stuff like that and if

00:37:35,119 --> 00:37:39,680
the if the system

00:37:37,119 --> 00:37:40,400
um you know survives uh which you can

00:37:39,680 --> 00:37:43,520
tell by

00:37:40,400 --> 00:37:46,320
uh waiting for this test to run and

00:37:43,520 --> 00:37:47,839
observe logs metrics and errors or

00:37:46,320 --> 00:37:49,440
increase latency but depending on

00:37:47,839 --> 00:37:51,440
whatever your goals are

00:37:49,440 --> 00:37:53,520
you can actually observe the system in

00:37:51,440 --> 00:37:56,720
in

00:37:53,520 --> 00:37:57,359
in motion and if it survives then what

00:37:56,720 --> 00:37:59,760
we can do

00:37:57,359 --> 00:38:01,520
is we can uh make this change in

00:37:59,760 --> 00:38:05,040
production

00:38:01,520 --> 00:38:07,520
when it comes to rolling this out we

00:38:05,040 --> 00:38:09,119
need to make sure that we don't roll it

00:38:07,520 --> 00:38:11,440
out all at once so

00:38:09,119 --> 00:38:12,560
we use live control interfaces that we

00:38:11,440 --> 00:38:14,800
talked about earlier

00:38:12,560 --> 00:38:17,040
to change this setting if it is possible

00:38:14,800 --> 00:38:20,000
to change it using a hot drop

00:38:17,040 --> 00:38:20,960
in in production uh we change it to a on

00:38:20,000 --> 00:38:23,680
a single node

00:38:20,960 --> 00:38:24,000
observe it and progressively roll it out

00:38:23,680 --> 00:38:26,800
to

00:38:24,000 --> 00:38:28,960
a whole rack uh whole data center in the

00:38:26,800 --> 00:38:32,720
next data center and so on so forth

00:38:28,960 --> 00:38:33,520
until uh the change is rolled out to all

00:38:32,720 --> 00:38:35,920
the instances

00:38:33,520 --> 00:38:37,440
in production and all the while we need

00:38:35,920 --> 00:38:40,000
to observe metrics

00:38:37,440 --> 00:38:41,040
um this ensures that if there are any

00:38:40,000 --> 00:38:43,760
failures you stop

00:38:41,040 --> 00:38:46,079
you can roll back the change that you

00:38:43,760 --> 00:38:48,640
just made

00:38:46,079 --> 00:38:49,920
so uh there are a bunch of conclusions

00:38:48,640 --> 00:38:51,760
and future work

00:38:49,920 --> 00:38:53,280
uh what we talked about here was a

00:38:51,760 --> 00:38:56,560
concrete architecture that

00:38:53,280 --> 00:39:01,359
is based on control theory uh it is

00:38:56,560 --> 00:39:04,320
uh it's rooted in some very well

00:39:01,359 --> 00:39:06,800
you know fundamentals of computer

00:39:04,320 --> 00:39:09,119
science like state machines

00:39:06,800 --> 00:39:11,520
and and it makes the system a lot easier

00:39:09,119 --> 00:39:14,640
to understand

00:39:11,520 --> 00:39:18,400
also to track changes across the

00:39:14,640 --> 00:39:21,440
across the nodes uh is easier

00:39:18,400 --> 00:39:22,320
with the system and we can apply the

00:39:21,440 --> 00:39:24,560
self-healing

00:39:22,320 --> 00:39:26,240
uh properties to a real world production

00:39:24,560 --> 00:39:29,119
database

00:39:26,240 --> 00:39:29,920
and a small set of cooperating agents

00:39:29,119 --> 00:39:32,480
are

00:39:29,920 --> 00:39:33,520
that's all that is needed along with a

00:39:32,480 --> 00:39:36,320
cluster manager

00:39:33,520 --> 00:39:37,839
there's no large-scale central planning

00:39:36,320 --> 00:39:41,599
that is necessary in this

00:39:37,839 --> 00:39:42,000
system and the the machine learning

00:39:41,599 --> 00:39:44,640
models

00:39:42,000 --> 00:39:46,240
are harder to explain and understand so

00:39:44,640 --> 00:39:49,440
we kind of stay away from

00:39:46,240 --> 00:39:52,640
uh jumping on that bandwagon right now

00:39:49,440 --> 00:39:55,440
and uh eventually what we could

00:39:52,640 --> 00:39:56,800
potentially do is if there's a log of

00:39:55,440 --> 00:40:00,000
all these state transitions

00:39:56,800 --> 00:40:04,000
you could take the state transitions and

00:40:00,000 --> 00:40:07,119
apply anomaly detection or

00:40:04,000 --> 00:40:10,480
you can use it to predict failures

00:40:07,119 --> 00:40:11,920
because it is very crust data that the

00:40:10,480 --> 00:40:13,599
system will generate

00:40:11,920 --> 00:40:16,240
and then finally uh this is a general

00:40:13,599 --> 00:40:18,160
approach to any distributed system so if

00:40:16,240 --> 00:40:20,480
you want to apply this to

00:40:18,160 --> 00:40:22,319
um let's say elasticsearch or some other

00:40:20,480 --> 00:40:25,200
distributed database which is

00:40:22,319 --> 00:40:26,319
um not running on a bunch of machines

00:40:25,200 --> 00:40:29,359
this approach is

00:40:26,319 --> 00:40:33,119
also generalizable so

00:40:29,359 --> 00:40:33,760
that's the those are the conclusions and

00:40:33,119 --> 00:40:38,560
and

00:40:33,760 --> 00:40:41,599
direction in which we can take this um

00:40:38,560 --> 00:40:42,480
so finally uh we are hiring if anybody

00:40:41,599 --> 00:40:45,680
is interested

00:40:42,480 --> 00:40:47,599
in uh working at apple or netflix uh

00:40:45,680 --> 00:40:48,560
please follow the links on on your

00:40:47,599 --> 00:40:51,680
screen

00:40:48,560 --> 00:40:54,880
and uh uh there are a bunch of open

00:40:51,680 --> 00:40:57,440
positions uh and uh we can

00:40:54,880 --> 00:40:58,240
be open for questions now um i have to

00:40:57,440 --> 00:41:01,119
hop off to

00:40:58,240 --> 00:41:01,680
uh my next session but joey uh can

00:41:01,119 --> 00:41:04,240
answer

00:41:01,680 --> 00:41:06,880
uh answer the questions uh thank you all

00:41:04,240 --> 00:41:10,000
for joining

00:41:06,880 --> 00:41:11,680
thank you nash all right so

00:41:10,000 --> 00:41:14,560
if anybody has any questions i guess

00:41:11,680 --> 00:41:15,599
just put them in the chat maybe

00:41:14,560 --> 00:41:17,680
and i'll hang out for a couple of

00:41:15,599 --> 00:41:19,440
minutes

00:41:17,680 --> 00:41:22,480
or also feel free to reach out to me on

00:41:19,440 --> 00:41:22,480
apache slack

00:41:25,200 --> 00:41:28,480
i don't see any questions i will hang up

00:41:27,599 --> 00:41:35,839
for a

00:41:28,480 --> 00:41:35,839
few more seconds

00:41:54,240 --> 00:41:59,760
all right it looks like uh we don't

00:41:57,680 --> 00:42:01,920
really have any questions so uh in that

00:41:59,760 --> 00:42:02,880
case let's head over to dinesh's next

00:42:01,920 --> 00:42:05,520
talk

00:42:02,880 --> 00:42:06,560
also in the cassandra track and thank

00:42:05,520 --> 00:42:17,839
you all for

00:42:06,560 --> 00:42:17,839
coming and watching

00:42:30,800 --> 00:42:34,480
all right yeah i don't think there are

00:42:32,160 --> 00:42:45,839
any questions okay i'm leaving

00:42:34,480 --> 00:42:45,839
bye everyone

00:42:57,920 --> 00:43:01,440
all right i've been told that there are

00:42:59,520 --> 00:43:02,400
questions um but i can't see them in

00:43:01,440 --> 00:43:04,560
chat so i have

00:43:02,400 --> 00:43:06,079
uh somebody who's sending me uh

00:43:04,560 --> 00:43:09,280
screenshots of their chat

00:43:06,079 --> 00:43:12,560
so that i can see what they're saying um

00:43:09,280 --> 00:43:14,079
all right so uh there's a question about

00:43:12,560 --> 00:43:15,440
are there any open source agents or

00:43:14,079 --> 00:43:16,480
cluster manager available which can do

00:43:15,440 --> 00:43:19,599
similar things

00:43:16,480 --> 00:43:21,680
um so netflix has open sourced uh

00:43:19,599 --> 00:43:24,319
various of the agents so for example we

00:43:21,680 --> 00:43:26,000
open sourced our net our pream

00:43:24,319 --> 00:43:27,680
agent which is a management sidecar it

00:43:26,000 --> 00:43:30,160
does like backup and restore

00:43:27,680 --> 00:43:30,800
and and uh controlling repair we we have

00:43:30,160 --> 00:43:33,280
that

00:43:30,800 --> 00:43:34,640
open source ticket for open sourcing the

00:43:33,280 --> 00:43:38,240
uh repair

00:43:34,640 --> 00:43:41,520
uh the repair uh agent as part of apache

00:43:38,240 --> 00:43:44,079
cassandra sidecar

00:43:41,520 --> 00:43:46,000
that works on going and we've open

00:43:44,079 --> 00:43:46,319
source for example like jvm quake so you

00:43:46,000 --> 00:43:49,599
can

00:43:46,319 --> 00:43:52,160
attach the jvm supervisor to

00:43:49,599 --> 00:43:53,760
your java process be it elasticsearch

00:43:52,160 --> 00:43:54,319
zookeeper and rescue it from garbage

00:43:53,760 --> 00:43:56,319
collection

00:43:54,319 --> 00:43:58,000
death spirals a lot of the agents are

00:43:56,319 --> 00:44:00,000
already open source unrelated to our

00:43:58,000 --> 00:44:03,119
company so for example like um

00:44:00,000 --> 00:44:06,079
you know systemd that's just or whatever

00:44:03,119 --> 00:44:08,079
net system you use uh and then in terms

00:44:06,079 --> 00:44:09,839
of software upgrading hardware

00:44:08,079 --> 00:44:11,599
that's not open source right now uh we

00:44:09,839 --> 00:44:14,800
did open source our

00:44:11,599 --> 00:44:17,680
uh so at netflix we call it cloud um

00:44:14,800 --> 00:44:18,160
i think it's i think it's um i forget

00:44:17,680 --> 00:44:20,880
the name

00:44:18,160 --> 00:44:23,040
um but it's an open source project that

00:44:20,880 --> 00:44:23,359
allows you to image ec2 instances in

00:44:23,040 --> 00:44:26,400
place

00:44:23,359 --> 00:44:27,440
so you can take a mi and you can

00:44:26,400 --> 00:44:30,240
actually like

00:44:27,440 --> 00:44:30,880
flash it's called s3 flash bootloader

00:44:30,240 --> 00:44:33,280
it's on our

00:44:30,880 --> 00:44:34,319
netflix gun works github and you can use

00:44:33,280 --> 00:44:37,040
that to

00:44:34,319 --> 00:44:39,920
image an ec2 machine from one known

00:44:37,040 --> 00:44:41,920
state to another known state

00:44:39,920 --> 00:44:43,760
and i think that there are preponderance

00:44:41,920 --> 00:44:44,880
of kubernetes operators that are open

00:44:43,760 --> 00:44:46,720
source right now and i think

00:44:44,880 --> 00:44:48,839
the main issue there is just we don't

00:44:46,720 --> 00:44:51,839
have one consistent one

00:44:48,839 --> 00:44:54,160
um so uh

00:44:51,839 --> 00:44:55,839
yeah in terms of cluster manager uh

00:44:54,160 --> 00:44:56,400
we've we've often found the cluster

00:44:55,839 --> 00:44:58,319
manager

00:44:56,400 --> 00:44:59,920
uh is is something that companies want

00:44:58,319 --> 00:45:03,200
to write themselves uh

00:44:59,920 --> 00:45:06,720
based on uh like their own

00:45:03,200 --> 00:45:08,319
control plane service so

00:45:06,720 --> 00:45:10,400
you know in the case of repair service

00:45:08,319 --> 00:45:11,599
we used the database itself as the

00:45:10,400 --> 00:45:14,400
cluster manager so

00:45:11,599 --> 00:45:16,240
so we communicate state transitions

00:45:14,400 --> 00:45:18,400
through like a system table

00:45:16,240 --> 00:45:19,280
that's because we didn't have a sidecar

00:45:18,400 --> 00:45:20,480
yet um

00:45:19,280 --> 00:45:22,560
now that we have the sidecar we could

00:45:20,480 --> 00:45:23,760
potentially use that uh using state

00:45:22,560 --> 00:45:25,440
stored in the database as a cluster

00:45:23,760 --> 00:45:28,960
manager

00:45:25,440 --> 00:45:30,960
um okay so uh yeah i guess

00:45:28,960 --> 00:45:32,079
i guess the the short answer is various

00:45:30,960 --> 00:45:35,040
agents or open source

00:45:32,079 --> 00:45:36,720
various agents are closed the cluster

00:45:35,040 --> 00:45:38,400
manager is really

00:45:36,720 --> 00:45:40,240
you know something that your operations

00:45:38,400 --> 00:45:43,200
team is going to define um it's very

00:45:40,240 --> 00:45:46,480
similar like a kubernetes operator

00:45:43,200 --> 00:45:48,400
all right um is this work related to the

00:45:46,480 --> 00:45:49,839
apache cassandra management process step

00:45:48,400 --> 00:45:52,000
one or separate work

00:45:49,839 --> 00:45:53,520
um so i would say that this work kind of

00:45:52,000 --> 00:45:56,960
motivated uh

00:45:53,520 --> 00:45:58,000
from our perspective uh our involvement

00:45:56,960 --> 00:45:59,839
in step one

00:45:58,000 --> 00:46:01,200
uh so so specifically there was a lot of

00:45:59,839 --> 00:46:02,880
questions of like well can you open

00:46:01,200 --> 00:46:05,680
source like the cluster manager

00:46:02,880 --> 00:46:06,800
um and we really kind of see the

00:46:05,680 --> 00:46:08,960
cassandra

00:46:06,800 --> 00:46:10,560
open source sidecar uh as kind of whole

00:46:08,960 --> 00:46:13,440
as kind of hosting

00:46:10,560 --> 00:46:14,000
a lot of these um state machines for you

00:46:13,440 --> 00:46:17,920
but like for

00:46:14,000 --> 00:46:20,319
example a state transition like hardware

00:46:17,920 --> 00:46:21,680
migration is very uh environment

00:46:20,319 --> 00:46:26,079
specific so like for example

00:46:21,680 --> 00:46:29,040
in netflix we use s3 for backup and we

00:46:26,079 --> 00:46:29,839
uh we have consistent snapshots ns3 that

00:46:29,040 --> 00:46:31,680
we can sync

00:46:29,839 --> 00:46:33,440
uh or we have ebs volumes that we can

00:46:31,680 --> 00:46:35,920
attach um

00:46:33,440 --> 00:46:36,480
and you know similar concepts exist in

00:46:35,920 --> 00:46:39,440
google

00:46:36,480 --> 00:46:39,760
so you have like gfs uh in google cloud

00:46:39,440 --> 00:46:42,640
um

00:46:39,760 --> 00:46:44,240
and and most on on prems have some

00:46:42,640 --> 00:46:45,040
equivalent of like a blob store and a

00:46:44,240 --> 00:46:48,079
disc

00:46:45,040 --> 00:46:49,520
uh system um but interacting with those

00:46:48,079 --> 00:46:52,640
like like writing that glue

00:46:49,520 --> 00:46:55,839
is actually most of the work um

00:46:52,640 --> 00:46:56,960
so we're not really sure how to uh

00:46:55,839 --> 00:46:58,800
properly

00:46:56,960 --> 00:47:00,160
basically we can we think we can get the

00:46:58,800 --> 00:47:01,280
state machines but we'll have to have

00:47:00,160 --> 00:47:04,880
plug-ins for like

00:47:01,280 --> 00:47:08,240
actually doing the state transfer

00:47:04,880 --> 00:47:10,640
um all right so those are

00:47:08,240 --> 00:47:13,280
the two questions that they've sent me

00:47:10,640 --> 00:47:13,280
let me ask

00:47:18,760 --> 00:47:21,899
[Music]

00:47:21,920 --> 00:47:39,839
details or technical difficulties i'm

00:47:23,599 --> 00:47:39,839
now being told that i'm on mute

00:47:50,480 --> 00:47:52,559

YouTube URL: https://www.youtube.com/watch?v=9wAM7L49agM


