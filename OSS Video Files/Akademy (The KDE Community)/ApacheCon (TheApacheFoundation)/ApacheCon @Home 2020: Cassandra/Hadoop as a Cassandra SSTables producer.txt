Title: Hadoop as a Cassandra SSTables producer
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Cassandra
Description: 
	Hadoop as a Cassandra SSTables producer
Serban Teodorescu, Adelina Vidovici

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

We’re using a lambda architecture, with Hadoop used for the main database and Cassandra deployed as persistent cache at edges, in total about 7-800 Cassandra nodes. One issue is daily push of data from Hadoop to Cassandra, which is the main factor that impacts the clusters performance and costs. We used to produce JSON data in Hadoop, then convert it to SSTables at the edges and streaming them to Cassandra. I’ll show why this architecture is unable to take advantage of Cassandra 4 streaming improvements, why is that important for us, how to combine Hadoop with Cassandra vnodes in order to achieve optimal streaming, and show some (preliminary) performance figures. The later is work in progress, but I hope it will be finished by the time the conference is startin

Serban Teodorescu
I'm at SRE at Adobe, part of a small team that manages 30+ Cassandra clusters for Adobe Audience Manager. Previously, I was a Python programmer, and I'm still trying to find out how a software developer who preferred SQL databases ended up as an SRE for a Cassandra team, and then started to work in Java.
Adelina Vidovici
I'm Software Engineer in Adobe Romania with a background in Computer Science and a big passion for Chemistry. In the last 2.5 years, I was part of the Adobe Audience Manager team and I’ve got the chance to learn and work with Big Data technologies: Trust me! We have cookies! :) Besides work, I enjoy reading, travelling and going for a bike ride from time to time.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,039 --> 00:00:30,240
okay

00:00:25,680 --> 00:00:33,760
let's start

00:00:30,240 --> 00:00:34,960
uh hi everyone i'm adrina computer

00:00:33,760 --> 00:00:37,520
science

00:00:34,960 --> 00:00:38,160
and uh together with sherban and saryat

00:00:37,520 --> 00:00:40,079
adobe

00:00:38,160 --> 00:00:42,840
i will present today the topic called

00:00:40,079 --> 00:00:45,840
hadoop as a cassandra as a stable

00:00:42,840 --> 00:00:45,840
producer

00:00:47,280 --> 00:00:50,960
let's go through the agenda we will

00:00:49,200 --> 00:00:54,160
start with the problem that we have

00:00:50,960 --> 00:00:54,800
now with the current architecture after

00:00:54,160 --> 00:00:57,039
that

00:00:54,800 --> 00:00:59,039
we will describe our proposed solution

00:00:57,039 --> 00:01:01,120
and how we have implemented it

00:00:59,039 --> 00:01:02,800
and the presentation will end with our

00:01:01,120 --> 00:01:05,520
results and what we plan to do in the

00:01:02,800 --> 00:01:05,520
near future

00:01:06,720 --> 00:01:13,600
first i'll go to the problem that we had

00:01:11,200 --> 00:01:15,920
we are both working in adobe audience

00:01:13,600 --> 00:01:19,200
manager a data management platform

00:01:15,920 --> 00:01:22,000
that collects data from various sources

00:01:19,200 --> 00:01:22,960
builds audience profiles and segments

00:01:22,000 --> 00:01:25,200
them

00:01:22,960 --> 00:01:26,880
it's using a lambda architecture is a

00:01:25,200 --> 00:01:29,280
hadoop central database

00:01:26,880 --> 00:01:31,040
as a source of truth and cassandra

00:01:29,280 --> 00:01:34,079
deployed on eight ages

00:01:31,040 --> 00:01:36,240
as a persistent cache system there is a

00:01:34,079 --> 00:01:36,960
daily push of data from hadoop to the

00:01:36,240 --> 00:01:39,360
edges

00:01:36,960 --> 00:01:40,799
and continuous push from the edges to

00:01:39,360 --> 00:01:43,840
hadoop

00:01:40,799 --> 00:01:46,799
all infrastructure is in aws and for

00:01:43,840 --> 00:01:52,799
cassandra we are using around 800

00:01:46,799 --> 00:01:52,799
i3 2x large instances in 34 clusters

00:01:53,759 --> 00:01:58,719
the problem that we have is the same

00:01:55,840 --> 00:01:58,719
push from hadoop

00:01:59,119 --> 00:02:02,320
this is the current architecture of it

00:02:01,520 --> 00:02:05,119
first

00:02:02,320 --> 00:02:09,599
hadoop output is uploaded to s3 and

00:02:05,119 --> 00:02:12,239
notification are sent to sql

00:02:09,599 --> 00:02:13,760
in each edge region there are several

00:02:12,239 --> 00:02:17,840
ec2 instances

00:02:13,760 --> 00:02:17,840
running two java applications

00:02:17,920 --> 00:02:24,959
the first consumes the sps notifications

00:02:22,000 --> 00:02:25,680
creates ss tables using sql as a stable

00:02:24,959 --> 00:02:28,239
writer

00:02:25,680 --> 00:02:29,760
and sends sqs notification to the other

00:02:28,239 --> 00:02:31,760
application

00:02:29,760 --> 00:02:33,599
it needs to do an aggregation on the

00:02:31,760 --> 00:02:36,080
table and on ttl

00:02:33,599 --> 00:02:39,200
on the ttl because that's due to a

00:02:36,080 --> 00:02:42,400
limitation of secularism tablewriter

00:02:39,200 --> 00:02:43,920
since the tpl must be specified in the

00:02:42,400 --> 00:02:49,920
insert statement

00:02:43,920 --> 00:02:52,239
and the writer has only one statement

00:02:49,920 --> 00:02:55,680
the second application strings the

00:02:52,239 --> 00:02:57,120
s tables created previously to the

00:02:55,680 --> 00:03:00,879
cassandra cluster

00:02:57,120 --> 00:03:04,000
with a capped throughput all data

00:03:00,879 --> 00:03:06,959
is kept on ephemeral disk

00:03:04,000 --> 00:03:06,959
until streamed

00:03:07,680 --> 00:03:11,760
there were several advantages of this

00:03:10,239 --> 00:03:14,640
architecture

00:03:11,760 --> 00:03:16,959
compared with merchants the streaming

00:03:14,640 --> 00:03:19,200
should be more efficient than that

00:03:16,959 --> 00:03:20,080
and throughput is easier to control

00:03:19,200 --> 00:03:22,480
allowing us

00:03:20,080 --> 00:03:24,959
to send more data when the daily traffic

00:03:22,480 --> 00:03:26,720
is lower

00:03:24,959 --> 00:03:28,239
but it's more interesting to look at

00:03:26,720 --> 00:03:30,560
disadvantages

00:03:28,239 --> 00:03:32,959
in practice there is a stable writer

00:03:30,560 --> 00:03:35,440
will start slowly accumulating data

00:03:32,959 --> 00:03:37,040
before being able to pass it to for

00:03:35,440 --> 00:03:40,720
streaming

00:03:37,040 --> 00:03:42,799
and sometimes not being able to produce

00:03:40,720 --> 00:03:44,840
enough data to fill the throughput cap

00:03:42,799 --> 00:03:48,560
as you can see in the

00:03:44,840 --> 00:03:52,239
graph then it will fill the disk

00:03:48,560 --> 00:03:55,439
due to sstable loader variation

00:03:52,239 --> 00:03:58,840
by the throughput cap and

00:03:55,439 --> 00:04:02,400
you can see in the lower graphic how it

00:03:58,840 --> 00:04:06,319
uh stops and starts when discs become

00:04:02,400 --> 00:04:06,319
available feeling it back to 100

00:04:07,760 --> 00:04:12,560
all this all this disk activity makes

00:04:10,879 --> 00:04:15,040
the service stateful

00:04:12,560 --> 00:04:17,600
if we lose one instance we have to

00:04:15,040 --> 00:04:20,799
choose between

00:04:17,600 --> 00:04:23,360
restarting the entire process for all

00:04:20,799 --> 00:04:24,080
for the entire cluster from zero or

00:04:23,360 --> 00:04:27,520
assuming

00:04:24,080 --> 00:04:31,040
or accepting that we lost some data

00:04:27,520 --> 00:04:34,560
it's also inefficient it's

00:04:31,040 --> 00:04:38,160
this and cpu intensive while running

00:04:34,560 --> 00:04:41,759
but only until it kills the disk then

00:04:38,160 --> 00:04:43,840
the disk and cpu are under neutralized

00:04:41,759 --> 00:04:45,360
or on the other hand sometimes they are

00:04:43,840 --> 00:04:48,479
not enough when

00:04:45,360 --> 00:04:49,520
the ttl cardinals is larger than the

00:04:48,479 --> 00:04:52,880
usual it will need

00:04:49,520 --> 00:04:54,960
more compute resources

00:04:52,880 --> 00:04:56,560
you can see in the graph here that there

00:04:54,960 --> 00:04:59,680
are long pauses between

00:04:56,560 --> 00:05:03,360
usages for some clusters the data

00:04:59,680 --> 00:05:06,160
push is not daily or is big

00:05:03,360 --> 00:05:06,720
resulting in ec2 instances that are not

00:05:06,160 --> 00:05:11,840
used

00:05:06,720 --> 00:05:11,840
for 80 percent of the time

00:05:13,039 --> 00:05:17,120
also it will distribute data uniformly

00:05:15,280 --> 00:05:19,759
across the entire cluster

00:05:17,120 --> 00:05:22,400
each ss table will have data for all

00:05:19,759 --> 00:05:22,400
instances

00:05:22,479 --> 00:05:27,360
because it will not create hotspots but

00:05:25,120 --> 00:05:31,199
on the other hand it will create

00:05:27,360 --> 00:05:34,240
small ss tables on each cassandra nodes

00:05:31,199 --> 00:05:35,840
see especially for for large clusters

00:05:34,240 --> 00:05:38,240
since we have the same

00:05:35,840 --> 00:05:39,360
as a stable size divided by let's say

00:05:38,240 --> 00:05:43,680
00:05:39,360 --> 00:05:45,360
nodes all these small access cables will

00:05:43,680 --> 00:05:49,840
then be contacted with

00:05:45,360 --> 00:05:50,240
spss on l0 some time ago i investigated

00:05:49,840 --> 00:05:54,160
this

00:05:50,240 --> 00:05:57,199
and it's about between 15 and 20 percent

00:05:54,160 --> 00:06:02,160
of the total number of compaction

00:05:57,199 --> 00:06:05,199
that is the normal lcs

00:06:02,160 --> 00:06:07,280
contractions between levels

00:06:05,199 --> 00:06:09,600
thus streaming and compaction becomes

00:06:07,280 --> 00:06:11,120
the main reason for scaling out for some

00:06:09,600 --> 00:06:13,600
clusters

00:06:11,120 --> 00:06:16,080
simply to have enough compute resources

00:06:13,600 --> 00:06:18,720
for compaction

00:06:16,080 --> 00:06:20,560
overall however there was little need to

00:06:18,720 --> 00:06:22,479
change this architecture

00:06:20,560 --> 00:06:24,720
there were little gains available the

00:06:22,479 --> 00:06:25,680
only tangible benefit would have been to

00:06:24,720 --> 00:06:27,600
decouple

00:06:25,680 --> 00:06:29,199
the two applications and make it

00:06:27,600 --> 00:06:31,440
stateless

00:06:29,199 --> 00:06:33,039
and while that would have been nice from

00:06:31,440 --> 00:06:34,720
an operational perspective

00:06:33,039 --> 00:06:36,880
which wouldn't have improved cassandra

00:06:34,720 --> 00:06:39,600
performance at all

00:06:36,880 --> 00:06:39,919
and that was true until screen changes

00:06:39,600 --> 00:06:43,199
were

00:06:39,919 --> 00:06:49,360
announced in cassandra 4. i won't get

00:06:43,199 --> 00:06:52,000
into details about this

00:06:49,360 --> 00:06:53,680
but there are there is there are two

00:06:52,000 --> 00:06:56,720
blog posts and at least

00:06:53,680 --> 00:06:59,919
one uh presentation about it

00:06:56,720 --> 00:07:03,039
by using the zero copy streaming becomes

00:06:59,919 --> 00:07:07,199
five times faster and should have less

00:07:03,039 --> 00:07:09,919
impact on cassandra node's performance

00:07:07,199 --> 00:07:12,080
it relies on copying the entire ss table

00:07:09,919 --> 00:07:14,240
to the destination servers

00:07:12,080 --> 00:07:15,120
and in order for this to work the

00:07:14,240 --> 00:07:18,240
sstable

00:07:15,120 --> 00:07:21,440
data must belong to a single token range

00:07:18,240 --> 00:07:24,080
just like they are on cassandra nodes

00:07:21,440 --> 00:07:27,199
so we started we started to think about

00:07:24,080 --> 00:07:27,199
the new architecture

00:07:28,080 --> 00:07:32,240
the new architecture implies moving dss

00:07:30,560 --> 00:07:35,120
stable writer from the

00:07:32,240 --> 00:07:36,000
hac2 instances to a new hadoop mapreduce

00:07:35,120 --> 00:07:37,840
job

00:07:36,000 --> 00:07:39,199
hadoop is a much better tool for

00:07:37,840 --> 00:07:41,440
aggregation and

00:07:39,199 --> 00:07:42,400
in order to produce token aware ss

00:07:41,440 --> 00:07:44,560
tables

00:07:42,400 --> 00:07:46,160
we need more partitioning and

00:07:44,560 --> 00:07:47,919
aggregation

00:07:46,160 --> 00:07:49,840
this could have been done also in the

00:07:47,919 --> 00:07:52,400
order architecture but

00:07:49,840 --> 00:07:54,080
it would have amplified the slow start

00:07:52,400 --> 00:07:56,160
and inefficiency

00:07:54,080 --> 00:07:59,840
and in some ways we would have

00:07:56,160 --> 00:07:59,840
implemented something like hadoop

00:07:59,919 --> 00:08:03,680
let's look at the simplified schema of

00:08:01,919 --> 00:08:05,520
the previous and current

00:08:03,680 --> 00:08:07,680
hadoop mapreduce component from the

00:08:05,520 --> 00:08:11,599
pipeline we are having a spark

00:08:07,680 --> 00:08:14,080
application which produces files in hdfs

00:08:11,599 --> 00:08:16,960
those files are representing the input

00:08:14,080 --> 00:08:20,400
of the mapreduce job which will be

00:08:16,960 --> 00:08:21,919
which will upload the data in s3 in json

00:08:20,400 --> 00:08:24,960
format

00:08:21,919 --> 00:08:26,000
for each file uploaded in s3 an sqs

00:08:24,960 --> 00:08:28,560
notification is

00:08:26,000 --> 00:08:30,160
sent the first part of the schema

00:08:28,560 --> 00:08:32,800
remains unchanged

00:08:30,160 --> 00:08:35,279
using the same files produced by the

00:08:32,800 --> 00:08:37,599
spark application in hdfs

00:08:35,279 --> 00:08:40,000
a new mapreduce job will be generating

00:08:37,599 --> 00:08:43,680
directly the token aware ss tables

00:08:40,000 --> 00:08:45,839
in s3 this mapreduce job can be executed

00:08:43,680 --> 00:08:49,360
by using our cloudera cluster

00:08:45,839 --> 00:08:52,160
or by using a separate emr cluster again

00:08:49,360 --> 00:08:55,040
for each file uploaded to s3 we will

00:08:52,160 --> 00:08:58,720
have an sks notification

00:08:55,040 --> 00:09:00,880
but each stable has eight files

00:08:58,720 --> 00:09:03,120
so for ensuring that we have a single

00:09:00,880 --> 00:09:05,200
sqs notification for each access table

00:09:03,120 --> 00:09:06,880
we are archiving the files corresponding

00:09:05,200 --> 00:09:10,320
to an ss table

00:09:06,880 --> 00:09:10,320
under a single tar file

00:09:11,519 --> 00:09:16,560
the bulk loader is now just one java

00:09:14,480 --> 00:09:17,839
application which is streaming data from

00:09:16,560 --> 00:09:21,360
s3 and sqs

00:09:17,839 --> 00:09:24,320
the solution has less sqs cues in

00:09:21,360 --> 00:09:27,360
tools no no more sustainable writer

00:09:24,320 --> 00:09:29,440
producer and consumer to manage

00:09:27,360 --> 00:09:31,440
since it's stateless it's much easier to

00:09:29,440 --> 00:09:34,240
be deployed in kubernetes

00:09:31,440 --> 00:09:34,640
with containers starting started only

00:09:34,240 --> 00:09:37,600
when

00:09:34,640 --> 00:09:38,240
sqs notifications needs to be processed

00:09:37,600 --> 00:09:42,080
making

00:09:38,240 --> 00:09:42,080
it more cost efficient

00:09:43,519 --> 00:09:48,000
we are also hoping to see other

00:09:45,360 --> 00:09:50,320
performance improvements in cassandra

00:09:48,000 --> 00:09:52,320
streaming should now produce normal that

00:09:50,320 --> 00:09:56,560
is 256 megabytes

00:09:52,320 --> 00:10:00,160
ss tables instead of small ones

00:09:56,560 --> 00:10:03,600
so it should be less spss on

00:10:00,160 --> 00:10:06,640
l0 and also

00:10:03,600 --> 00:10:09,279
we'll have better control

00:10:06,640 --> 00:10:10,000
we'll be able to delay streaming to

00:10:09,279 --> 00:10:13,839
specific

00:10:10,000 --> 00:10:16,240
instances such as nodes that are

00:10:13,839 --> 00:10:18,240
down or are being decommissioned

00:10:16,240 --> 00:10:22,000
regarding these ones

00:10:18,240 --> 00:10:24,320
uh we are constantly seeing a bug on one

00:10:22,000 --> 00:10:26,560
of the large cluster

00:10:24,320 --> 00:10:27,920
if we decommission during streaming the

00:10:26,560 --> 00:10:31,040
bloom filters

00:10:27,920 --> 00:10:34,079
are no longer working properly making

00:10:31,040 --> 00:10:36,000
one or more nodes very slow and the only

00:10:34,079 --> 00:10:36,800
reliable solution that i found is to

00:10:36,000 --> 00:10:39,839
replace

00:10:36,800 --> 00:10:40,480
the nodes i don't have more information

00:10:39,839 --> 00:10:43,360
about this

00:10:40,480 --> 00:10:44,240
since it's very difficult to reproduce

00:10:43,360 --> 00:10:48,399
and

00:10:44,240 --> 00:10:48,399
it only happens on the largest cluster

00:10:49,519 --> 00:10:53,760
there are some question marks in the new

00:10:52,000 --> 00:10:56,560
architecture

00:10:53,760 --> 00:10:57,839
to partition by a token range in hadoop

00:10:56,560 --> 00:10:59,839
we must have

00:10:57,839 --> 00:11:03,440
this information the token distribution

00:10:59,839 --> 00:11:06,959
data from all clusters

00:11:03,440 --> 00:11:09,839
also what happens when a cluster is

00:11:06,959 --> 00:11:12,839
scaled in or out that would make the

00:11:09,839 --> 00:11:14,640
token distribution data only partially

00:11:12,839 --> 00:11:18,079
correct

00:11:14,640 --> 00:11:20,560
on scaling the ss table being stream

00:11:18,079 --> 00:11:21,200
will still belong to a single token

00:11:20,560 --> 00:11:23,680
range

00:11:21,200 --> 00:11:24,959
actually to a sub range since what will

00:11:23,680 --> 00:11:27,680
happen is that

00:11:24,959 --> 00:11:30,000
some token ranges will just be

00:11:27,680 --> 00:11:34,320
consolidated in

00:11:30,000 --> 00:11:37,440
larger ones but on scale out it will not

00:11:34,320 --> 00:11:40,160
since token ranges will be split

00:11:37,440 --> 00:11:40,640
but that only means that we'll fall back

00:11:40,160 --> 00:11:43,360
to

00:11:40,640 --> 00:11:44,480
all style streaming to the new nodes and

00:11:43,360 --> 00:11:48,240
to those that

00:11:44,480 --> 00:11:50,240
had that data that's not ideal but it's

00:11:48,240 --> 00:11:52,720
not a disaster either

00:11:50,240 --> 00:11:53,440
and it will be impacting only a few

00:11:52,720 --> 00:11:58,480
instances

00:11:53,440 --> 00:11:58,480
and only on limited location

00:11:58,639 --> 00:12:03,120
so let's discuss the implementation

00:12:03,440 --> 00:12:08,639
the hadoop job needs to create ss tables

00:12:06,639 --> 00:12:11,760
for eight different clusters

00:12:08,639 --> 00:12:12,480
some of them cassandra 3 some cassandra

00:12:11,760 --> 00:12:15,519
00:12:12,480 --> 00:12:17,839
because we will not be able to upgrade

00:12:15,519 --> 00:12:20,720
all 8 clusters in the same time to

00:12:17,839 --> 00:12:20,720
cassandra 4.

00:12:20,800 --> 00:12:24,079
on cassandra 3 it's also not probably

00:12:23,120 --> 00:12:27,360
not a good idea

00:12:24,079 --> 00:12:30,079
to stream these token aware as a stable

00:12:27,360 --> 00:12:31,040
since we'll stream from more than one

00:12:30,079 --> 00:12:34,079
container

00:12:31,040 --> 00:12:36,399
it's just a matter of time until

00:12:34,079 --> 00:12:38,320
several containers will stream to the

00:12:36,399 --> 00:12:41,600
same cassandra node

00:12:38,320 --> 00:12:44,560
result resulting in hotspots

00:12:41,600 --> 00:12:46,079
at this moment there is no way to

00:12:44,560 --> 00:12:49,519
control this on the

00:12:46,079 --> 00:12:51,760
destination no cap on incoming streaming

00:12:49,519 --> 00:12:55,200
but as far as i remember there is an

00:12:51,760 --> 00:12:55,200
open jira ticket for this

00:12:56,639 --> 00:13:01,920
but that will be in cassandra 4 anyway

00:12:58,560 --> 00:13:04,399
so it's not really an issue

00:13:01,920 --> 00:13:06,639
for getting the token range information

00:13:04,399 --> 00:13:06,959
we will use the prometheus metrics which

00:13:06,639 --> 00:13:09,279
are

00:13:06,959 --> 00:13:10,079
already consolidated in tunnels in the

00:13:09,279 --> 00:13:14,000
same

00:13:10,079 --> 00:13:16,560
aws region with the hadoop cluster

00:13:14,000 --> 00:13:18,000
to generate token aware ss tables we

00:13:16,560 --> 00:13:20,240
need to ensure that

00:13:18,000 --> 00:13:22,639
reducer tasks are getting only data

00:13:20,240 --> 00:13:25,279
belonging to the same token range

00:13:22,639 --> 00:13:27,360
for this we're using hadoop partitioning

00:13:25,279 --> 00:13:30,480
which controls which reducer will get

00:13:27,360 --> 00:13:33,120
each record outputted by the map jobs

00:13:30,480 --> 00:13:35,040
usually this is a hash function but in

00:13:33,120 --> 00:13:38,480
our case it's just a map converting

00:13:35,040 --> 00:13:41,600
token values to a partition index

00:13:38,480 --> 00:13:44,079
to ensure uniformity this

00:13:41,600 --> 00:13:46,880
must be deterministic there must be no

00:13:44,079 --> 00:13:49,279
variation in how this is computing since

00:13:46,880 --> 00:13:52,160
since it will be executed at different

00:13:49,279 --> 00:13:54,639
times from different map tasks

00:13:52,160 --> 00:13:55,360
the partitioning is also useful for

00:13:54,639 --> 00:13:58,720
cassandra

00:13:55,360 --> 00:14:00,639
trig clusters since the number of tokens

00:13:58,720 --> 00:14:02,560
in each cluster should be a good enough

00:14:00,639 --> 00:14:07,040
approximation of the cluster size

00:14:02,560 --> 00:14:07,040
thus keeping hadoop reducers balanced

00:14:09,440 --> 00:14:14,000
currently in our solution the range of

00:14:12,320 --> 00:14:16,880
partition is equal to the number of

00:14:14,000 --> 00:14:19,360
token ranges

00:14:16,880 --> 00:14:21,680
for cassandra 3 each record is sent to a

00:14:19,360 --> 00:14:25,360
random partition within the interval

00:14:21,680 --> 00:14:27,680
thus eliminating hotspots

00:14:25,360 --> 00:14:29,600
for cassandra 4 all the records

00:14:27,680 --> 00:14:32,399
belonging to a token range will have the

00:14:29,600 --> 00:14:32,399
same partition

00:14:32,880 --> 00:14:37,199
in the future for increasing performance

00:14:35,120 --> 00:14:38,079
we can further divide the records from a

00:14:37,199 --> 00:14:41,360
token range

00:14:38,079 --> 00:14:41,360
to more than one partition

00:14:42,800 --> 00:14:47,920
there are several hadoop classes uh

00:14:46,240 --> 00:14:49,440
that we customize i'll go through the

00:14:47,920 --> 00:14:52,639
most important ones

00:14:49,440 --> 00:14:53,440
first custom mapper by definition a

00:14:52,639 --> 00:14:55,920
mapper

00:14:53,440 --> 00:14:56,720
gets as input key value pairs and

00:14:55,920 --> 00:14:59,839
outputs

00:14:56,720 --> 00:15:01,600
a set of intermediate key value pairs in

00:14:59,839 --> 00:15:03,199
our case it will pre-compute

00:15:01,600 --> 00:15:06,800
partitioning data structure

00:15:03,199 --> 00:15:10,839
and on each map set the partition index

00:15:06,800 --> 00:15:12,240
it knows which region has cassandra 3 or

00:15:10,839 --> 00:15:15,360
4.

00:15:12,240 --> 00:15:16,320
the partitioner usually contains a hash

00:15:15,360 --> 00:15:18,000
function which

00:15:16,320 --> 00:15:19,600
ensures that the data is uniformly

00:15:18,000 --> 00:15:21,839
distributed between tasks

00:15:19,600 --> 00:15:23,760
in our case it is a very simple class

00:15:21,839 --> 00:15:24,839
which contains a method to get the

00:15:23,760 --> 00:15:29,519
partition

00:15:24,839 --> 00:15:29,519
index set during the map phase

00:15:33,120 --> 00:15:39,360
the custom record writer usually just

00:15:36,560 --> 00:15:40,399
writes the output key value pairs to an

00:15:39,360 --> 00:15:43,839
output output

00:15:40,399 --> 00:15:45,519
file in our case this is where ss tables

00:15:43,839 --> 00:15:48,480
are being created

00:15:45,519 --> 00:15:51,199
it manages a map of string and sql as a

00:15:48,480 --> 00:15:54,399
stable writer instance

00:15:51,199 --> 00:15:57,839
since we can still get distinct details

00:15:54,399 --> 00:16:00,560
in each run each writer instance gets

00:15:57,839 --> 00:16:04,320
its unique folder to write to

00:16:00,560 --> 00:16:06,000
it overrides two methods right to send

00:16:04,320 --> 00:16:09,279
data to the proper writer

00:16:06,000 --> 00:16:12,399
and close to close all writer instances

00:16:09,279 --> 00:16:16,320
start each asset table and

00:16:12,399 --> 00:16:16,320
upload them to st

00:16:17,040 --> 00:16:22,240
all this looks great now but we had a

00:16:20,079 --> 00:16:24,639
lot of problems

00:16:22,240 --> 00:16:25,360
the traditional hadoop model at least

00:16:24,639 --> 00:16:28,240
how it's

00:16:25,360 --> 00:16:29,360
used in adobe audience manager and how i

00:16:28,240 --> 00:16:31,360
understand it

00:16:29,360 --> 00:16:32,399
is that the input is split in multiple

00:16:31,360 --> 00:16:36,000
parts and each

00:16:32,399 --> 00:16:38,000
user occurs there is no conflict

00:16:36,000 --> 00:16:39,440
even when one computer is running

00:16:38,000 --> 00:16:42,000
multiple reducers

00:16:39,440 --> 00:16:44,160
at the same time even if they are using

00:16:42,000 --> 00:16:45,680
the same output folder because the files

00:16:44,160 --> 00:16:48,720
are different

00:16:45,680 --> 00:16:51,120
on the other hand cassandra writer is

00:16:48,720 --> 00:16:54,079
very sensitive to having two writers

00:16:51,120 --> 00:16:55,120
using the same folder this triggered

00:16:54,079 --> 00:16:57,440
multiple bugs

00:16:55,120 --> 00:16:58,240
which were easily solved when i changed

00:16:57,440 --> 00:17:01,519
this

00:16:58,240 --> 00:17:03,759
to use unique temporary output folders

00:17:01,519 --> 00:17:04,720
but to get to this point i need to

00:17:03,759 --> 00:17:08,559
understand both

00:17:04,720 --> 00:17:08,559
cassandra and hadoop behaviors

00:17:08,640 --> 00:17:15,679
another issue a very difficult one was

00:17:12,559 --> 00:17:16,559
usage of vice writable class it's one of

00:17:15,679 --> 00:17:18,720
the classes that

00:17:16,559 --> 00:17:19,679
implements the writable interface in

00:17:18,720 --> 00:17:22,400
order to allow

00:17:19,679 --> 00:17:24,400
allow hadoop to serialize the serialized

00:17:22,400 --> 00:17:27,360
basic data types

00:17:24,400 --> 00:17:28,319
in custom record writer we need to get

00:17:27,360 --> 00:17:31,440
the byte array

00:17:28,319 --> 00:17:33,360
out of a byte writable instance

00:17:31,440 --> 00:17:34,640
in order to send it to the cassandra

00:17:33,360 --> 00:17:38,960
writer

00:17:34,640 --> 00:17:42,640
and the get bytes is perfect

00:17:38,960 --> 00:17:45,919
however as others have seen before

00:17:42,640 --> 00:17:47,760
get bytes returned zero padded bytes

00:17:45,919 --> 00:17:49,679
array

00:17:47,760 --> 00:17:51,760
since it's actually returning a

00:17:49,679 --> 00:17:54,320
reference to the underlying

00:17:51,760 --> 00:17:56,080
data structure not the data that was

00:17:54,320 --> 00:18:01,280
sent to it

00:17:56,080 --> 00:18:03,679
i should say that the title of this

00:18:01,280 --> 00:18:05,360
hadoop jira ticket that i included here

00:18:03,679 --> 00:18:09,840
is entirely appropriate it's a

00:18:05,360 --> 00:18:09,840
very bad name that leads to mistakes

00:18:10,240 --> 00:18:16,080
so in our case it was adding 50 percent

00:18:13,840 --> 00:18:19,679
of zeroes to any byte array

00:18:16,080 --> 00:18:22,799
this in turn triggered another issue

00:18:19,679 --> 00:18:25,760
a problem in the sql ss table writer

00:18:22,799 --> 00:18:28,400
which added the same record multiple

00:18:25,760 --> 00:18:30,480
times in the same ss table using the

00:18:28,400 --> 00:18:33,520
same partition key

00:18:30,480 --> 00:18:37,280
and that made which of course made the

00:18:33,520 --> 00:18:40,160
ss table invalid and triggered an error

00:18:37,280 --> 00:18:41,600
on streaming and it was almost

00:18:40,160 --> 00:18:44,480
impossible to diagnose

00:18:41,600 --> 00:18:46,880
as it was the last place to look at i

00:18:44,480 --> 00:18:49,280
was assuming that there is

00:18:46,880 --> 00:18:51,600
a problem with the map phase that we are

00:18:49,280 --> 00:18:52,400
somehow duplicating records anything but

00:18:51,600 --> 00:18:55,360
looking at

00:18:52,400 --> 00:18:56,160
this issue at some point i noticed the

00:18:55,360 --> 00:18:59,440
zeros

00:18:56,160 --> 00:19:00,480
but i i assumed it was a hadoop parsing

00:18:59,440 --> 00:19:03,679
issues

00:19:00,480 --> 00:19:04,880
on the testing environment and since the

00:19:03,679 --> 00:19:08,640
duplicate case was

00:19:04,880 --> 00:19:08,640
was a much more critical issue

00:19:09,039 --> 00:19:15,760
and the fabric zero seem

00:19:12,320 --> 00:19:16,640
totally unrelated so i only discovered

00:19:15,760 --> 00:19:19,120
this

00:19:16,640 --> 00:19:21,360
when i noticed that padding was always

00:19:19,120 --> 00:19:23,440
fifty percent of the length

00:19:21,360 --> 00:19:25,760
of the data length of the original data

00:19:23,440 --> 00:19:29,360
land and started to investigate this

00:19:25,760 --> 00:19:30,720
strange thing while being blocked on the

00:19:29,360 --> 00:19:33,120
other issue

00:19:30,720 --> 00:19:34,000
the solution was very simple and it's

00:19:33,120 --> 00:19:36,720
well known in

00:19:34,000 --> 00:19:38,480
the hadoop community to replace get

00:19:36,720 --> 00:19:41,760
bytes with copy bytes

00:19:38,480 --> 00:19:45,120
and that's immediately solved

00:19:41,760 --> 00:19:47,520
by amazement the duplicate record

00:19:45,120 --> 00:19:47,520
problem

00:19:48,960 --> 00:19:52,480
there are also multiple problems due to

00:19:51,120 --> 00:19:55,600
partitioning different scenes

00:19:52,480 --> 00:19:59,200
between within

00:19:55,600 --> 00:20:01,679
with cloudera controller and reducers

00:19:59,200 --> 00:20:03,679
it was time consuming but easy to solve

00:20:01,679 --> 00:20:07,280
by ensuring we have the same

00:20:03,679 --> 00:20:10,000
configuration files use ss payback soft

00:20:07,280 --> 00:20:12,480
we use stable sorting and be very

00:20:10,000 --> 00:20:14,720
careful with primo's output parsing

00:20:12,480 --> 00:20:17,600
since any error there will compromise

00:20:14,720 --> 00:20:17,600
the entire job

00:20:17,760 --> 00:20:23,840
next problem the java dependencies hell

00:20:20,960 --> 00:20:25,120
we had to combine our legacy hadoop code

00:20:23,840 --> 00:20:26,880
with cassandra

00:20:25,120 --> 00:20:29,679
and initially there was no problem in

00:20:26,880 --> 00:20:31,840
the job that we were testing

00:20:29,679 --> 00:20:32,799
but when we tried the full run of all

00:20:31,840 --> 00:20:37,200
hadoop jobs

00:20:32,799 --> 00:20:40,240
our daily pipeline we got a guava

00:20:37,200 --> 00:20:42,080
version conflict upgrading the version

00:20:40,240 --> 00:20:45,360
in our code was not an option

00:20:42,080 --> 00:20:47,440
it would have touched too much code

00:20:45,360 --> 00:20:49,440
the solution was to build the faber jar

00:20:47,440 --> 00:20:53,520
version of cassandra that included

00:20:49,440 --> 00:20:56,880
all libraries with guava in order to

00:20:53,520 --> 00:20:59,760
size that the version conflict

00:20:56,880 --> 00:21:00,480
and i should mention that i was able to

00:20:59,760 --> 00:21:02,880
do this

00:21:00,480 --> 00:21:05,760
only with the help of the cassandra fork

00:21:02,880 --> 00:21:08,559
that uses gradle instead of ankh

00:21:05,760 --> 00:21:09,280
i don't really know any of them gradle

00:21:08,559 --> 00:21:11,760
of aunt

00:21:09,280 --> 00:21:14,159
but gradle is more accessible at least

00:21:11,760 --> 00:21:14,159
for me

00:21:15,120 --> 00:21:19,440
there are also problems with other jars

00:21:17,440 --> 00:21:20,400
and we have to exclude several other

00:21:19,440 --> 00:21:23,760
libraries

00:21:20,400 --> 00:21:26,000
and shade the native jar

00:21:23,760 --> 00:21:26,000
too

00:21:27,039 --> 00:21:31,120
and i think that once the gradle support

00:21:30,320 --> 00:21:33,600
gets into

00:21:31,120 --> 00:21:34,240
cassandra trunk it should be easier to

00:21:33,600 --> 00:21:39,840
build

00:21:34,240 --> 00:21:39,840
cassandra as a fat jar with shady jars

00:21:41,280 --> 00:21:45,840
so let's look at what were the results

00:21:46,080 --> 00:21:49,679
the goal that we started with was to

00:21:47,919 --> 00:21:52,000
compare two clusters

00:21:49,679 --> 00:21:54,240
receiving production traffic one with

00:21:52,000 --> 00:21:56,799
cassandra 3 and the old streaming

00:21:54,240 --> 00:21:58,640
and the secondary one with cassandra 4

00:21:56,799 --> 00:22:02,880
and the new streaming

00:21:58,640 --> 00:22:04,320
and perhaps to have cassandra 4 then on

00:22:02,880 --> 00:22:06,720
both of them

00:22:04,320 --> 00:22:09,039
to compare the difference on the

00:22:06,720 --> 00:22:14,080
streaming impact between

00:22:09,039 --> 00:22:16,480
sstable aware and and not aware

00:22:14,080 --> 00:22:17,600
we didn't get there due to delays while

00:22:16,480 --> 00:22:20,640
solving the above

00:22:17,600 --> 00:22:20,960
problems and more the last problem we

00:22:20,640 --> 00:22:24,000
had

00:22:20,960 --> 00:22:28,400
was the recent cassandra static it is

00:22:24,000 --> 00:22:30,320
one way called now and it prevented the

00:22:28,400 --> 00:22:33,760
the bulk load application to connect the

00:22:30,320 --> 00:22:35,600
customer for cluster for streaming

00:22:33,760 --> 00:22:37,360
when we will when will be ready with

00:22:35,600 --> 00:22:40,799
that and other problems

00:22:37,360 --> 00:22:42,559
we'll try to push to publish the results

00:22:40,799 --> 00:22:45,360
on the adobe tech blog

00:22:42,559 --> 00:22:45,360
the comparison

00:22:46,400 --> 00:22:50,080
but uh what we have tested and verified

00:22:49,600 --> 00:22:52,400
is that

00:22:50,080 --> 00:22:54,799
zero copy screening works with how to

00:22:52,400 --> 00:22:59,120
generate ss paper

00:22:54,799 --> 00:23:01,600
and here is the log that proves it

00:22:59,120 --> 00:23:02,640
the log is from the server that received

00:23:01,600 --> 00:23:06,480
data

00:23:02,640 --> 00:23:08,640
streamed with the sstable loader

00:23:06,480 --> 00:23:12,159
and you can see it using the new class

00:23:08,640 --> 00:23:14,559
big table zero copywriter

00:23:12,159 --> 00:23:16,720
and we also found the minor bug in the

00:23:14,559 --> 00:23:18,640
ss table loader i did we didn't i didn't

00:23:16,720 --> 00:23:22,080
report it yet

00:23:18,640 --> 00:23:25,280
it's um it was report errors in

00:23:22,080 --> 00:23:27,360
the above situation for all nodes that

00:23:25,280 --> 00:23:30,400
are not getting data which will be

00:23:27,360 --> 00:23:31,360
most of most of the nodes but the

00:23:30,400 --> 00:23:35,440
streaming does

00:23:31,360 --> 00:23:35,440
work on the nodes that have the token

00:23:35,600 --> 00:23:39,679
now the plans for the future

00:23:42,400 --> 00:23:47,840
on hadoop side

00:23:45,520 --> 00:23:50,240
we might look into read and write

00:23:47,840 --> 00:23:52,400
directly on s3

00:23:50,240 --> 00:23:53,440
since we have plans to drop cloud data

00:23:52,400 --> 00:23:56,480
and hdfs

00:23:53,440 --> 00:23:58,640
moving to an s-e-backed file system

00:23:56,480 --> 00:24:00,320
this might be very difficult as it will

00:23:58,640 --> 00:24:04,000
require changes in how

00:24:00,320 --> 00:24:08,480
sql stable writer works we might

00:24:04,000 --> 00:24:08,480
keep using dmr hdfs instead

00:24:08,640 --> 00:24:12,400
as i was previously mentioning block or

00:24:11,200 --> 00:24:14,880
delay some

00:24:12,400 --> 00:24:15,440
string to some specific host or token

00:24:14,880 --> 00:24:19,200
ranges

00:24:15,440 --> 00:24:21,120
should be quite easy well

00:24:19,200 --> 00:24:25,039
we should investigate that as a stable

00:24:21,120 --> 00:24:25,039
error it should be simple as well

00:24:25,520 --> 00:24:30,000
on a longer term it will be interesting

00:24:28,320 --> 00:24:32,720
to see if we can

00:24:30,000 --> 00:24:33,279
offline merge and then split all access

00:24:32,720 --> 00:24:35,600
tables

00:24:33,279 --> 00:24:36,799
belong to the same token range before

00:24:35,600 --> 00:24:40,400
streaming

00:24:36,799 --> 00:24:42,799
sort of doing an offline compaction

00:24:40,400 --> 00:24:45,600
i expect this to further decrease the

00:24:42,799 --> 00:24:49,039
compaction pressure

00:24:45,600 --> 00:24:50,880
and that's all before finishing i should

00:24:49,039 --> 00:24:52,880
add that this was the therefore

00:24:50,880 --> 00:24:55,279
starting as it started as a garage week

00:24:52,880 --> 00:24:55,600
project then a dissertation case is over

00:24:55,279 --> 00:24:58,799
for

00:24:55,600 --> 00:25:00,720
our colleague then a project for an

00:24:58,799 --> 00:25:02,400
intern laura mitchell

00:25:00,720 --> 00:25:04,400
it couldn't be finished before the end

00:25:02,400 --> 00:25:08,480
of her internship produced a limited

00:25:04,400 --> 00:25:08,480
involvement from adelina and me

00:25:12,840 --> 00:25:15,840
questions

00:25:44,400 --> 00:25:48,080
no questions thank you

00:25:48,480 --> 00:25:51,919
we finished a bit earlier but i don't

00:25:51,039 --> 00:25:57,840
think anyone

00:25:51,919 --> 00:25:57,840
will be upset because of that

00:25:59,760 --> 00:26:05,840
for any questions you have the uh

00:26:02,799 --> 00:26:06,960
you can always reach us on our emails

00:26:05,840 --> 00:26:11,840
included at the

00:26:06,960 --> 00:26:11,840
beginning of the presentation thank you

00:26:22,840 --> 00:26:25,840
thanks

00:27:05,039 --> 00:27:07,120

YouTube URL: https://www.youtube.com/watch?v=K-vT4dHrKCE


