Title: CCCNA18 - Andrija Panić - Successfully Running Enterprise Workloads with CloudStack
Publication date: 2018-11-20
Playlist: Cloudstack Collaboration Conference North America 2018
Description: 
	Andrija Panić - Successfully Running Enterprise Workloads with CloudStack, KVM and Managed Primary Storage

A presentation from the Cloudstack Collaboration Conference, a part of ApacheCon
 North America 2018 in Montreal

http://ca.cloudstackcollab.org/
http://apachecon.com/acna18
Captions: 
	00:00:03,520 --> 00:00:10,000
hi everybody so we're just about ready

00:00:07,639 --> 00:00:11,750
to start with aundrea patches

00:00:10,000 --> 00:00:14,750
successfully running enterprise

00:00:11,750 --> 00:00:17,840
workloads with cloud stack and kbm with

00:00:14,750 --> 00:00:20,449
primary storage he's been involved in

00:00:17,840 --> 00:00:24,919
the clouds tech community for quite a

00:00:20,449 --> 00:00:26,749
while now in doing a lot of talking on

00:00:24,919 --> 00:00:29,689
the mailing list and kind of keeping up

00:00:26,749 --> 00:00:32,630
with what's going on and I've really

00:00:29,689 --> 00:00:34,940
been working now for maybe a year or two

00:00:32,630 --> 00:00:37,250
with aundrea and we've had a great time

00:00:34,940 --> 00:00:40,030
kind of integrating additional features

00:00:37,250 --> 00:00:43,219
into cloud stacks primary storage

00:00:40,030 --> 00:00:45,399
managed primary storage specifically so

00:00:43,219 --> 00:00:48,649
he's here today to kind of give us a

00:00:45,399 --> 00:00:52,370
overview of how its implemented at this

00:00:48,649 --> 00:00:53,620
company Yod yeah everyone thanks for

00:00:52,370 --> 00:00:57,379
joining me

00:00:53,620 --> 00:00:59,149
so yeah my name is on Japan it's just my

00:00:57,379 --> 00:01:01,640
father it is just me I work for tier

00:00:59,149 --> 00:01:03,289
data I will kind of spend one minute of

00:01:01,640 --> 00:01:04,730
your time just to briefly introduce the

00:01:03,289 --> 00:01:08,380
company I work for and their real moves

00:01:04,730 --> 00:01:10,550
are more interesting things that yes so

00:01:08,380 --> 00:01:13,700
in summation what for here data

00:01:10,550 --> 00:01:15,620
integrator is basically English the

00:01:13,700 --> 00:01:17,660
color of here I like to think of that

00:01:15,620 --> 00:01:20,030
above like a daughter company and here

00:01:17,660 --> 00:01:21,770
was actually founded over 120 years ago

00:01:20,030 --> 00:01:24,080
the original stock in a lumber or wood

00:01:21,770 --> 00:01:25,970
industry times change they're moving to

00:01:24,080 --> 00:01:29,030
real estates and that's the actual

00:01:25,970 --> 00:01:31,370
interesting point because later after

00:01:29,030 --> 00:01:34,430
they decided to move to more obvious the

00:01:31,370 --> 00:01:36,350
modern stuff like IT because of the

00:01:34,430 --> 00:01:39,500
previous state period they acquired a

00:01:36,350 --> 00:01:41,990
huge amount of physical properties all

00:01:39,500 --> 00:01:45,710
around Switzerland which are all owned

00:01:41,990 --> 00:01:48,560
by here this way basically we are able

00:01:45,710 --> 00:01:51,530
to what we did actually is that there is

00:01:48,560 --> 00:01:53,660
completely 100 percent private network

00:01:51,530 --> 00:01:55,520
of fiber optics across hospitals so it's

00:01:53,660 --> 00:01:58,460
not to leave from telecom providers

00:01:55,520 --> 00:01:59,780
it's absolutely over here and thus we

00:01:58,460 --> 00:02:02,930
are able to

00:01:59,780 --> 00:02:05,920
obviously offer some guarantees and

00:02:02,930 --> 00:02:09,080
latency on bandwidth and so on so forth

00:02:05,920 --> 00:02:11,180
we're all certified twelve less the most

00:02:09,080 --> 00:02:13,069
recent according some financial cinema

00:02:11,180 --> 00:02:15,830
standard the famous VDP are indifferent

00:02:13,069 --> 00:02:17,930
I see stuff and we are connected to of

00:02:15,830 --> 00:02:21,350
your fish location-specific C which I

00:02:17,930 --> 00:02:24,050
will mention later to actually do some

00:02:21,350 --> 00:02:27,980
special friendships with a big cloud

00:02:24,050 --> 00:02:29,360
Rider guys so basically because of the

00:02:27,980 --> 00:02:31,430
fiber optics next event Center we are

00:02:29,360 --> 00:02:34,340
able to connect customer premises to

00:02:31,430 --> 00:02:36,800
whatever service we offer we will very

00:02:34,340 --> 00:02:38,209
soon I begin I think beginning of the

00:02:36,800 --> 00:02:41,170
next year will be offering the container

00:02:38,209 --> 00:02:44,480
stuff we already do offer extra storage

00:02:41,170 --> 00:02:47,989
we do also offer a fear of Microsoft

00:02:44,480 --> 00:02:49,700
curve flavored cloud services recycle

00:02:47,989 --> 00:02:51,350
rest o'clock steglich we're running and

00:02:49,700 --> 00:02:54,319
a very special private connection

00:02:51,350 --> 00:02:57,530
private peering with my schedule of 365

00:02:54,319 --> 00:02:59,630
so basically the users can from their

00:02:57,530 --> 00:03:02,000
remote locations via fiber optic

00:02:59,630 --> 00:03:05,660
networks we are data centers again for

00:03:02,000 --> 00:03:07,430
fiber optic networks access some of the

00:03:05,660 --> 00:03:09,920
public route providers as I mentioned

00:03:07,430 --> 00:03:11,600
assuring office 365 and actually we do

00:03:09,920 --> 00:03:14,209
provide the very key latency that's

00:03:11,600 --> 00:03:16,370
nothing that per my knowledge and what I

00:03:14,209 --> 00:03:17,810
have been told from the Collegium in my

00:03:16,370 --> 00:03:19,910
company no god no other actually

00:03:17,810 --> 00:03:22,640
provider can offer the moment because

00:03:19,910 --> 00:03:26,209
usually you publicly access or enter and

00:03:22,640 --> 00:03:28,820
there is no guarantee some agency enough

00:03:26,209 --> 00:03:30,440
I guess on the company side and the

00:03:28,820 --> 00:03:32,750
history so let's move to the actual

00:03:30,440 --> 00:03:35,780
story so basically we started our

00:03:32,750 --> 00:03:40,609
coaster journey let's say beginning of

00:03:35,780 --> 00:03:44,120
2013 and in the early days basically

00:03:40,609 --> 00:03:46,190
next I should mention where as Mike

00:03:44,120 --> 00:03:48,440
pointed out running purely karyam

00:03:46,190 --> 00:03:50,140
we started with clouds that for three we

00:03:48,440 --> 00:03:52,150
are now more running for a

00:03:50,140 --> 00:03:55,770
release with a lot of deported stuff

00:03:52,150 --> 00:03:58,150
obviously and we didn't start with a

00:03:55,770 --> 00:04:00,880
self-storage solution for us because of

00:03:58,150 --> 00:04:02,319
some great properties it has you might

00:04:00,880 --> 00:04:03,940
guess you can all really don't need to

00:04:02,319 --> 00:04:06,130
repeat every sentence but it absolutely

00:04:03,940 --> 00:04:08,470
does provide some of the really nice

00:04:06,130 --> 00:04:11,110
things later we actually wanted a bit

00:04:08,470 --> 00:04:13,180
more performance and maybe a little bit

00:04:11,110 --> 00:04:15,100
better integration into cloud stack so

00:04:13,180 --> 00:04:17,440
we went with our and if the solution

00:04:15,100 --> 00:04:19,959
kind of a hybrid one and this was

00:04:17,440 --> 00:04:22,419
do-it-yourself ocean which is important

00:04:19,959 --> 00:04:24,220
I will mention later why then we remove

00:04:22,419 --> 00:04:25,960
the foliage this year after solution

00:04:24,220 --> 00:04:28,330
again there are some limitations for

00:04:25,960 --> 00:04:30,430
that obviously not good quality of

00:04:28,330 --> 00:04:33,760
service or not true quality of service

00:04:30,430 --> 00:04:34,150
with this and that's basically where we

00:04:33,760 --> 00:04:36,190
were

00:04:34,150 --> 00:04:38,890
during this original period that we did

00:04:36,190 --> 00:04:41,410
kind of face a lot of different

00:04:38,890 --> 00:04:45,100
challenges which is the nice way to say

00:04:41,410 --> 00:04:47,410
a lot of issues that not many people or

00:04:45,100 --> 00:04:50,230
too many vendors to actually share with

00:04:47,410 --> 00:04:52,390
you we do share because we believe that

00:04:50,230 --> 00:04:54,910
nice way to actually learning and maybe

00:04:52,390 --> 00:04:58,320
save some people or some trouble at

00:04:54,910 --> 00:05:00,820
least so we can basically do let's say

00:04:58,320 --> 00:05:03,010
filter these issues in a couple of

00:05:00,820 --> 00:05:06,250
categories so stability performance and

00:05:03,010 --> 00:05:08,110
maybe usability / management I do need

00:05:06,250 --> 00:05:10,540
to mention that we start our safe

00:05:08,110 --> 00:05:13,510
journey very early and most of issues

00:05:10,540 --> 00:05:15,880
are actually fixed so far so if you are

00:05:13,510 --> 00:05:18,040
now to start reading using and consuming

00:05:15,880 --> 00:05:20,770
set as the primary store it is much much

00:05:18,040 --> 00:05:24,790
much better situation performance wise

00:05:20,770 --> 00:05:26,620
your thing will vary depending on your

00:05:24,790 --> 00:05:28,450
set of chromosome testing hardware and

00:05:26,620 --> 00:05:30,700
so on and usability management stuff

00:05:28,450 --> 00:05:32,950
some things have been fixed or improved

00:05:30,700 --> 00:05:34,030
and some things are still a challenge to

00:05:32,950 --> 00:05:36,729
be solved

00:05:34,030 --> 00:05:39,460
so now let's dive into a couple of

00:05:36,729 --> 00:05:41,590
concrete so let's say original issues

00:05:39,460 --> 00:05:45,100
basically the very very early stage of

00:05:41,590 --> 00:05:47,410
losing self there was obviously some and

00:05:45,100 --> 00:05:49,419
handle stuff and and simply things that

00:05:47,410 --> 00:05:51,550
weren't accounted for and basically any

00:05:49,419 --> 00:05:54,550
kind of exception in the lib RBD which

00:05:51,550 --> 00:05:56,530
is our buddy client library for liberty

00:05:54,550 --> 00:05:57,689
to communicate itself cost or any kind

00:05:56,530 --> 00:05:59,819
of exceptional day

00:05:57,689 --> 00:06:01,649
basically crash stationed which means

00:05:59,819 --> 00:06:03,299
that the VM high-availability mechanism

00:06:01,649 --> 00:06:05,849
would kick in and you gets downtime

00:06:03,299 --> 00:06:08,519
couple of examples are given on the

00:06:05,849 --> 00:06:10,559
slides but I do need to mention this is

00:06:08,519 --> 00:06:12,629
mostly solve in the mean time so you're

00:06:10,559 --> 00:06:14,009
not you will not experience the same

00:06:12,629 --> 00:06:16,309
thing we did if you start now

00:06:14,009 --> 00:06:19,529
now also sepra balancing a deep scrubs

00:06:16,309 --> 00:06:21,899
basically can impact client I or heavily

00:06:19,529 --> 00:06:25,369
up to the point if you are if you don't

00:06:21,899 --> 00:06:29,759
have a let's say most properly sized a

00:06:25,369 --> 00:06:32,099
hardener if you do some other let's say

00:06:29,759 --> 00:06:34,439
slight miss configurations or or maybe

00:06:32,099 --> 00:06:37,050
lack of a bit of optimization then you

00:06:34,439 --> 00:06:38,969
get you can get to the point where the

00:06:37,050 --> 00:06:41,669
the client IO is basically not serve

00:06:38,969 --> 00:06:43,139
within 30 seconds or so and then a more

00:06:41,669 --> 00:06:44,959
sensitive file system will get to

00:06:43,139 --> 00:06:48,629
read-only mode and you can imagine

00:06:44,959 --> 00:06:50,999
clients joy then also a flapping which

00:06:48,629 --> 00:06:53,610
is actual official word for some sources

00:06:50,999 --> 00:06:56,069
going up and down of the safe monitors

00:06:53,610 --> 00:06:59,549
or employees which could happen due to

00:06:56,069 --> 00:07:02,009
network issues not because F itself they

00:06:59,549 --> 00:07:04,559
can also cause what I mentioned slow

00:07:02,009 --> 00:07:07,610
requests and basically dial was not

00:07:04,559 --> 00:07:10,649
circular client in in inappropriate time

00:07:07,610 --> 00:07:12,959
then also when we speak about stability

00:07:10,649 --> 00:07:14,999
volumes never says they're running up to

00:07:12,959 --> 00:07:16,139
go at least four level release as you

00:07:14,999 --> 00:07:17,759
know the snatchers they take on a

00:07:16,139 --> 00:07:20,129
primary storage and extend vehicle

00:07:17,759 --> 00:07:22,199
copied over to secondary storage and

00:07:20,129 --> 00:07:23,999
this container can actually take off

00:07:22,199 --> 00:07:27,599
along with a long time to complete and

00:07:23,999 --> 00:07:29,279
then it can actually block other you

00:07:27,599 --> 00:07:32,189
know joke you for the other items to the

00:07:29,279 --> 00:07:33,869
KB mentioned I believe maybe I will be

00:07:32,189 --> 00:07:35,399
one of the guys for sugar theories or

00:07:33,869 --> 00:07:37,559
somebody else actually originally

00:07:35,399 --> 00:07:39,959
documented this issue but basically what

00:07:37,559 --> 00:07:42,689
we will not be able for example to start

00:07:39,959 --> 00:07:44,699
a VM or to start over to router because

00:07:42,689 --> 00:07:47,309
there is no foot running sounds kind of

00:07:44,699 --> 00:07:50,399
silly but it can happen in very specific

00:07:47,309 --> 00:07:51,029
cases but it did happen to us a couple

00:07:50,399 --> 00:07:53,399
of times

00:07:51,029 --> 00:07:55,610
also as I mention we originally use the

00:07:53,399 --> 00:07:57,500
do-it-yourself NFS solution which was

00:07:55,610 --> 00:08:00,469
the fest we want to get a kernel panic

00:07:57,500 --> 00:08:03,080
because of the ZFS model this was by the

00:08:00,469 --> 00:08:07,580
way my last name with a additional thing

00:08:03,080 --> 00:08:10,759
so now we got our agents rebooting being

00:08:07,580 --> 00:08:12,139
rebooted actually due to the gaming

00:08:10,759 --> 00:08:13,819
carpet script which resumed for the

00:08:12,139 --> 00:08:16,250
storage is highly available and cannot

00:08:13,819 --> 00:08:19,039
die at all so it will actually boot the

00:08:16,250 --> 00:08:20,569
agent to be able to reconnect so imagine

00:08:19,039 --> 00:08:22,930
when the whole cloud you know very very

00:08:20,569 --> 00:08:26,139
early stages not see enough clients to

00:08:22,930 --> 00:08:29,930
be too stressed but still it went down

00:08:26,139 --> 00:08:33,729
not nice at all estimation the now we

00:08:29,930 --> 00:08:36,560
are electing moving to a performance

00:08:33,729 --> 00:08:38,479
sector I already mentioned set rebalance

00:08:36,560 --> 00:08:41,630
leaves crowds of flapping monitors can

00:08:38,479 --> 00:08:43,219
cause block slow requests in the

00:08:41,630 --> 00:08:44,779
previous section I kind of equality

00:08:43,219 --> 00:08:46,339
instability because some p.m. should

00:08:44,779 --> 00:08:49,130
completely die with every wonderful

00:08:46,339 --> 00:08:50,660
system this is now just a performance

00:08:49,130 --> 00:08:52,519
thing the way to come of this is

00:08:50,660 --> 00:08:54,290
actually to throttle etc balanced

00:08:52,519 --> 00:08:56,870
process so instead of one day it will

00:08:54,290 --> 00:08:58,339
take eight days to rebounds properly but

00:08:56,870 --> 00:09:00,890
at least the client private is not

00:08:58,339 --> 00:09:02,360
impacted and then you are good at some

00:09:00,890 --> 00:09:03,829
point in time we also hear serious

00:09:02,360 --> 00:09:05,300
issues with deep scrubs I do need to

00:09:03,829 --> 00:09:07,970
mention that this was hard drive base

00:09:05,300 --> 00:09:10,699
that set Buster with some SSD journals

00:09:07,970 --> 00:09:13,670
but with even disable the deep scrubs

00:09:10,699 --> 00:09:15,980
which is not a really smart thing to do

00:09:13,670 --> 00:09:17,810
at all but that's what we have to do in

00:09:15,980 --> 00:09:20,120
order to survive the slow

00:09:17,810 --> 00:09:22,550
requests also if you have a faulty

00:09:20,120 --> 00:09:25,339
network cable from experience

00:09:22,550 --> 00:09:29,060
foreigners which from experience or a

00:09:25,339 --> 00:09:32,930
fail failing or let's say problematic

00:09:29,060 --> 00:09:34,730
hard drive make sure to be ready to fix

00:09:32,930 --> 00:09:36,740
these issues on time otherwise you will

00:09:34,730 --> 00:09:39,740
get the flapping or as these the

00:09:36,740 --> 00:09:42,199
monitors and thing really go to help in

00:09:39,740 --> 00:09:44,540
you know literal way so this is not

00:09:42,199 --> 00:09:46,880
issue of self this is issue of the

00:09:44,540 --> 00:09:49,430
networking which can be obviously has

00:09:46,880 --> 00:09:50,660
too many moving parts in some way also

00:09:49,430 --> 00:09:52,250
back in the days I believe it's still

00:09:50,660 --> 00:09:53,990
the case in the way bra dos which is

00:09:52,250 --> 00:09:58,070
basically communicating

00:09:53,990 --> 00:09:59,600
to the defroster in general they're the

00:09:58,070 --> 00:10:01,100
reason alone that set client-side

00:09:59,600 --> 00:10:03,560
libraries there is a way to define a

00:10:01,100 --> 00:10:05,330
stripe size so for example if you write

00:10:03,560 --> 00:10:07,190
four megabytes of data to the self

00:10:05,330 --> 00:10:10,040
cluster you could theoretically instruct

00:10:07,190 --> 00:10:13,160
except Buster to stripe the data with

00:10:10,040 --> 00:10:15,290
stripe size of for example one megabyte

00:10:13,160 --> 00:10:17,120
and theoretically you will be writing to

00:10:15,290 --> 00:10:19,040
four different car drives and does to

00:10:17,120 --> 00:10:21,230
prove the performance but in Lee

00:10:19,040 --> 00:10:25,010
brothers there is still on access

00:10:21,230 --> 00:10:26,660
tracker ticket I would say there was

00:10:25,010 --> 00:10:28,370
still no response and it's not still

00:10:26,660 --> 00:10:32,360
implemented in the Lee brothers which is

00:10:28,370 --> 00:10:33,770
basically let's say a part of leveraging

00:10:32,360 --> 00:10:36,140
that sense of being consumed by Lee

00:10:33,770 --> 00:10:38,360
Burton and Lee barbg so there is no way

00:10:36,140 --> 00:10:41,540
to actually maybe do some performance

00:10:38,360 --> 00:10:43,690
improvements this is not penalty but you

00:10:41,540 --> 00:10:48,160
know it's lack of possibility reproves

00:10:43,690 --> 00:10:50,800
now also who I put it in performance

00:10:48,160 --> 00:10:56,930
section what they were mention it later

00:10:50,800 --> 00:10:59,870
but early days of of our us using self

00:10:56,930 --> 00:11:01,940
there was actually missing let's call it

00:10:59,870 --> 00:11:03,560
a proper a snapshot lifecycle so

00:11:01,940 --> 00:11:06,020
basically create a snapshot to delete

00:11:03,560 --> 00:11:07,760
the snapshot it's deleted from secondary

00:11:06,020 --> 00:11:10,250
storage excellent it's deleted from

00:11:07,760 --> 00:11:11,720
database of markers rated excellent but

00:11:10,250 --> 00:11:14,120
it's not deleted from except after

00:11:11,720 --> 00:11:15,860
itself and then you can imagine guys

00:11:14,120 --> 00:11:18,079
hearing a daily snapshots

00:11:15,860 --> 00:11:20,300
24 hours of its 24 steps of the day

00:11:18,079 --> 00:11:23,149
times out of days you are not good

00:11:20,300 --> 00:11:25,040
hearing 50 or 100 snapshot which I will

00:11:23,149 --> 00:11:26,720
explain later during to certain things

00:11:25,040 --> 00:11:29,209
can actually cause the call except

00:11:26,720 --> 00:11:32,329
cluster so really perform really really

00:11:29,209 --> 00:11:33,980
bad and we did kind of people disable

00:11:32,329 --> 00:11:39,050
our synapses in the GUI we hid in the

00:11:33,980 --> 00:11:40,670
button not too smart but less so thing

00:11:39,050 --> 00:11:42,529
and then we later did the extra demand

00:11:40,670 --> 00:11:44,209
itself or properly removes never

00:11:42,529 --> 00:11:46,070
consented to move our code or the life

00:11:44,209 --> 00:11:47,959
cycle thing which was committed that

00:11:46,070 --> 00:11:50,779
this was one of our humble contributions

00:11:47,959 --> 00:11:52,339
I believe also guys from is it super

00:11:50,779 --> 00:11:55,279
furious or the clouds yes and although

00:11:52,339 --> 00:11:58,660
some of these guys also actually be the

00:11:55,279 --> 00:12:01,750
same thing so it should be there in them

00:11:58,660 --> 00:12:03,519
swing from a long time ago then

00:12:01,750 --> 00:12:04,810
continuing with performance whatever

00:12:03,519 --> 00:12:06,759
kind of shared environment you use

00:12:04,810 --> 00:12:08,290
networking storage doesn't matter if you

00:12:06,759 --> 00:12:10,420
don't have a proper quality of service

00:12:08,290 --> 00:12:13,329
with a guaranteed let's say minimum or

00:12:10,420 --> 00:12:15,279
some whatever bandwidth or something

00:12:13,329 --> 00:12:17,500
then sooner or later going for which to

00:12:15,279 --> 00:12:24,490
experience North neighborhood nothing

00:12:17,500 --> 00:12:26,949
new there right so with which KVM I'm

00:12:24,490 --> 00:12:28,870
now speaking specific about a VM there

00:12:26,949 --> 00:12:30,759
is a way in the disk offering stuff to

00:12:28,870 --> 00:12:33,790
define a hypervisor part of service

00:12:30,759 --> 00:12:36,910
which basically will just instruct KDM

00:12:33,790 --> 00:12:40,810
to implement read the right frock link

00:12:36,910 --> 00:12:43,360
either in bytes per second or iOS per

00:12:40,810 --> 00:12:45,459
second irrelevant of dial size so this

00:12:43,360 --> 00:12:48,399
is not really a true quality of service

00:12:45,459 --> 00:12:51,040
at least from my point of view it's more

00:12:48,399 --> 00:12:52,480
of a say set damage control of a kind

00:12:51,040 --> 00:12:54,370
and those were some reason it's not

00:12:52,480 --> 00:12:56,500
apply to corporate volumes to dimension

00:12:54,370 --> 00:12:58,389
clients pinning a VM let me touch them

00:12:56,500 --> 00:13:00,339
drives let me do some benchmarking how

00:12:58,389 --> 00:13:03,880
their cloud works there is no throttling

00:13:00,339 --> 00:13:05,680
and then he basically will let's say

00:13:03,880 --> 00:13:09,939
cause performance degradation for all

00:13:05,680 --> 00:13:12,009
for all other clients and now I put this

00:13:09,939 --> 00:13:13,990
kind of fatty reverses practice exercise

00:13:12,009 --> 00:13:16,660
but this is actually also explained in a

00:13:13,990 --> 00:13:18,220
very nice white paper from Intel from

00:13:16,660 --> 00:13:20,170
Intel Corporation about how to reach

00:13:18,220 --> 00:13:21,069
what we are me occidentalis disease and

00:13:20,170 --> 00:13:23,529
some other stuff

00:13:21,069 --> 00:13:27,399
imagine you sent 100 purely sequential

00:13:23,529 --> 00:13:29,589
streams to our remote storage let's say

00:13:27,399 --> 00:13:31,509
you do our dedication on km or doesn't

00:13:29,589 --> 00:13:34,420
matter or NFS stuff it's purely

00:13:31,509 --> 00:13:36,279
optimized but basically yeah you can

00:13:34,420 --> 00:13:38,110
argue that any storage financial excel

00:13:36,279 --> 00:13:41,230
of a sequential fire and that will be

00:13:38,110 --> 00:13:43,899
true but because of too many parallel

00:13:41,230 --> 00:13:45,639
streams at the same time in the

00:13:43,899 --> 00:13:47,680
background this actually practice

00:13:45,639 --> 00:13:50,410
becomes one kind of percent random i/o

00:13:47,680 --> 00:13:53,139
no matter how you optimize the stuff is

00:13:50,410 --> 00:13:54,759
it complete eventually when your load is

00:13:53,139 --> 00:13:56,740
high enough and you have enough of

00:13:54,759 --> 00:13:58,600
parallel streams of data reading or

00:13:56,740 --> 00:13:59,300
writing doesn't matter it will become

00:13:58,600 --> 00:14:01,610
full

00:13:59,300 --> 00:14:04,209
DeMeo so you know this is the reason

00:14:01,610 --> 00:14:07,700
basically there's no magical caching

00:14:04,209 --> 00:14:09,200
will help eventually does help until up

00:14:07,700 --> 00:14:11,540
to the point where you exhaust the cache

00:14:09,200 --> 00:14:12,380
and you see to do more work out than the

00:14:11,540 --> 00:14:15,320
question hello

00:14:12,380 --> 00:14:17,930
and obviously you need is this nothing

00:14:15,320 --> 00:14:21,230
too smart here but I believe it's

00:14:17,930 --> 00:14:24,519
important to just mention moving through

00:14:21,230 --> 00:14:27,050
stability slash management let's say

00:14:24,519 --> 00:14:29,810
challenges wave I did dimensional snaps

00:14:27,050 --> 00:14:31,610
of life cycles which self but just

00:14:29,810 --> 00:14:35,240
understand the pain that we went through

00:14:31,610 --> 00:14:37,190
and it was fixed so now it's so fine as

00:14:35,240 --> 00:14:39,740
I mentioned you could have 50 or 100

00:14:37,190 --> 00:14:42,110
snapshots on single volume and multiple

00:14:39,740 --> 00:14:43,730
that by couple volumes and then due to a

00:14:42,110 --> 00:14:45,820
certain horribly layering issue the way

00:14:43,730 --> 00:14:48,350
it works insert it will cost remembers

00:14:45,820 --> 00:14:50,510
performance problems these are here

00:14:48,350 --> 00:14:53,560
extra links to the last slide where I

00:14:50,510 --> 00:14:56,360
have URLs to original issues raised by

00:14:53,560 --> 00:14:57,829
different guides in communities or the

00:14:56,360 --> 00:15:00,110
left side you can actually read about

00:14:57,829 --> 00:15:02,240
are we the layering stuff we also

00:15:00,110 --> 00:15:03,800
committed some kind of temporary snap so

00:15:02,240 --> 00:15:06,350
delicious sweet and so on and then a

00:15:03,800 --> 00:15:08,149
proper fixes already mentioned now at

00:15:06,350 --> 00:15:10,070
the same time when we got a bunch of

00:15:08,149 --> 00:15:12,709
snapshots really actually had a very

00:15:10,070 --> 00:15:16,130
another set of problem that was

00:15:12,709 --> 00:15:18,170
continuing or basically expanding the

00:15:16,130 --> 00:15:21,220
previous one when you want to delete the

00:15:18,170 --> 00:15:23,600
volume basically it will try to

00:15:21,220 --> 00:15:25,399
unprotect over serfs nationals but you

00:15:23,600 --> 00:15:27,800
start do specific variables you this

00:15:25,399 --> 00:15:30,290
used in run Java library if you have

00:15:27,800 --> 00:15:31,670
more than sixteen snapshots it will call

00:15:30,290 --> 00:15:33,380
it you're basically failure of course

00:15:31,670 --> 00:15:37,220
exception and again major disconnection

00:15:33,380 --> 00:15:40,750
stand again down time so a lot of let's

00:15:37,220 --> 00:15:43,250
say fun back in the days but this was

00:15:40,750 --> 00:15:48,740
fixed thanks to reader and the big

00:15:43,250 --> 00:15:50,600
things from this point so yeah there are

00:15:48,740 --> 00:15:51,709
bits of status to practice us dimension

00:15:50,600 --> 00:15:54,400
you already know how they are coping

00:15:51,709 --> 00:15:56,050
being copied over to secondary storage

00:15:54,400 --> 00:15:58,030
imaginative class of two even four

00:15:56,050 --> 00:16:00,190
terabytes trying to snap to the volume

00:15:58,030 --> 00:16:03,340
because it says exception but it's not

00:16:00,190 --> 00:16:05,800
release natural so yeah we can discuss

00:16:03,340 --> 00:16:07,570
the play tria but basically in a case

00:16:05,800 --> 00:16:10,870
where there is a failure of even

00:16:07,570 --> 00:16:13,150
manually a trigger snapshot depending on

00:16:10,870 --> 00:16:15,400
you in what state it will fail it can

00:16:13,150 --> 00:16:18,070
actually cause a Broken Arrow schedule

00:16:15,400 --> 00:16:19,420
for the particular volume which is not

00:16:18,070 --> 00:16:22,630
funny because then you need to dig into

00:16:19,420 --> 00:16:24,550
database and and solve issues and we

00:16:22,630 --> 00:16:29,860
will address how we will explain how

00:16:24,550 --> 00:16:32,200
extras this or later then also from my

00:16:29,860 --> 00:16:36,580
point of view you know these snapshots

00:16:32,200 --> 00:16:38,290
or movies they are copies backups of the

00:16:36,580 --> 00:16:40,960
snapshots you cannot revert back

00:16:38,290 --> 00:16:43,090
resources you can create new VM from the

00:16:40,960 --> 00:16:44,830
template which you created from the root

00:16:43,090 --> 00:16:46,900
volume snapshot or you can create new

00:16:44,830 --> 00:16:49,570
data discs which you created from a

00:16:46,900 --> 00:16:52,290
database snapshot so you really need to

00:16:49,570 --> 00:16:54,340
provide to create new resources

00:16:52,290 --> 00:16:55,750
theoretically with new IP addresses for

00:16:54,340 --> 00:16:58,630
the VM and so on there is no really way

00:16:55,750 --> 00:17:00,160
to just revert back which is the in my

00:16:58,630 --> 00:17:03,040
opinion the whole sense of out to

00:17:00,160 --> 00:17:05,230
snapshot then also some minor things

00:17:03,040 --> 00:17:06,790
which are not to spend too much time my

00:17:05,230 --> 00:17:10,209
profile extension and when you download

00:17:06,790 --> 00:17:12,459
volume from set for improper volume type

00:17:10,209 --> 00:17:14,110
or actually yeah what type in database

00:17:12,459 --> 00:17:16,600
which will cause issues down the road

00:17:14,110 --> 00:17:18,250
but this was initially locally fixed and

00:17:16,600 --> 00:17:21,970
I believe now it's all also fixed in

00:17:18,250 --> 00:17:24,600
upstream so again this is all kind of a

00:17:21,970 --> 00:17:28,900
history at least from this point of view

00:17:24,600 --> 00:17:31,360
now pretty kind of serious and

00:17:28,900 --> 00:17:33,070
interesting is the volume equations as

00:17:31,360 --> 00:17:34,990
you grow as the cloud provider will

00:17:33,070 --> 00:17:36,430
start and implement different storages

00:17:34,990 --> 00:17:38,920
or at least more storages for whatever

00:17:36,430 --> 00:17:41,380
reason for camión there was no things

00:17:38,920 --> 00:17:43,990
such as like all my storage my creation

00:17:41,380 --> 00:17:46,000
like the risk for the most like there is

00:17:43,990 --> 00:17:47,890
for than like the motion of emotion for

00:17:46,000 --> 00:17:49,570
behavior basically the only thing how

00:17:47,890 --> 00:17:50,430
you can migrate volumes between Seth and

00:17:49,570 --> 00:17:53,580
NFS for

00:17:50,430 --> 00:17:55,440
or 21st boxes doesn't matter is by

00:17:53,580 --> 00:17:56,820
actually doing the offline storage

00:17:55,440 --> 00:17:59,370
migration which means that you need to

00:17:56,820 --> 00:18:01,500
stop the VM migrate volume with a lot of

00:17:59,370 --> 00:18:03,240
downtime potentially this can take hours

00:18:01,500 --> 00:18:05,520
if you are talking about terabytes size

00:18:03,240 --> 00:18:07,290
volumes and then start the VM obviously

00:18:05,520 --> 00:18:10,290
the customers are not happy with that

00:18:07,290 --> 00:18:12,180
solution now I do need to mention a very

00:18:10,290 --> 00:18:13,380
special case there is I put it on the

00:18:12,180 --> 00:18:15,960
wall because there is nothing on

00:18:13,380 --> 00:18:19,050
unfortunately special about it but you

00:18:15,960 --> 00:18:22,200
can get issues we did when you have a

00:18:19,050 --> 00:18:24,540
very busy VM which has a huge amount of

00:18:22,200 --> 00:18:27,090
RAM memory generation that's usually

00:18:24,540 --> 00:18:29,910
it's their busy database server as

00:18:27,090 --> 00:18:31,800
example from the practice basically the

00:18:29,910 --> 00:18:33,330
migration thread in some circumstances

00:18:31,800 --> 00:18:35,820
you cannot simply catch up with the

00:18:33,330 --> 00:18:37,920
amount of RAM memory changes now you are

00:18:35,820 --> 00:18:39,780
asking well maybe when wondering how the

00:18:37,920 --> 00:18:41,670
hell this has anything to do with the

00:18:39,780 --> 00:18:44,430
storage because this is only live

00:18:41,670 --> 00:18:46,050
migration of CPU and VM let's say memory

00:18:44,430 --> 00:18:47,970
my collaboration from one host to

00:18:46,050 --> 00:18:49,500
another you will see later when we

00:18:47,970 --> 00:18:52,320
actually solve other issues but we get

00:18:49,500 --> 00:18:54,000
blocked by this one and basically you

00:18:52,320 --> 00:18:56,610
you are with this issue your block

00:18:54,000 --> 00:18:58,680
facial just blinding click put Austin

00:18:56,610 --> 00:19:01,560
maintenance mode and do your work or

00:18:58,680 --> 00:19:03,420
kernel camera whatever we address all of

00:19:01,560 --> 00:19:05,490
these issues I will explain basically

00:19:03,420 --> 00:19:07,110
later the way to reproduce it I'm not

00:19:05,490 --> 00:19:09,060
going to read it and spend your time but

00:19:07,110 --> 00:19:12,000
you can use the Google stress of test

00:19:09,060 --> 00:19:14,460
security in literally one line you can

00:19:12,000 --> 00:19:16,140
produce that officially the only one

00:19:14,460 --> 00:19:18,180
gigabyte of ram we're calling it when

00:19:16,140 --> 00:19:19,530
you can produce artificial a busy vm you

00:19:18,180 --> 00:19:21,510
can try the live migration tool

00:19:19,530 --> 00:19:24,000
absolutely never finish even on ten

00:19:21,510 --> 00:19:27,600
gigabit network so yeah you can

00:19:24,000 --> 00:19:30,360
basically read that and test it if you

00:19:27,600 --> 00:19:33,060
if you're interested and how we actually

00:19:30,360 --> 00:19:35,370
solve this or actually what's the way to

00:19:33,060 --> 00:19:39,030
solve this in theory is basically to use

00:19:35,370 --> 00:19:41,130
canvas a new version of our coconuts

00:19:39,030 --> 00:19:42,380
which is called dynamic and basically

00:19:41,130 --> 00:19:45,080
filter demo

00:19:42,380 --> 00:19:47,600
the throttle CPUs of the VM more and

00:19:45,080 --> 00:19:49,730
more and more up to the point that the

00:19:47,600 --> 00:19:52,490
seekers are not able to produce so much

00:19:49,730 --> 00:19:54,680
around memory changes and then we will

00:19:52,490 --> 00:19:57,440
not be so busy and it can actually

00:19:54,680 --> 00:19:59,330
finish the migration so I don't have any

00:19:57,440 --> 00:20:01,940
interesting squirrels penguins and other

00:19:59,330 --> 00:20:03,650
stuff to be humorous so I try to do it

00:20:01,940 --> 00:20:05,990
here a couple of slides how do we solve

00:20:03,650 --> 00:20:08,270
all these challenges should we stop

00:20:05,990 --> 00:20:11,600
expecting higher performance or delete

00:20:08,270 --> 00:20:13,910
us or simply forget about snapshots and

00:20:11,600 --> 00:20:15,050
wire migrations of course not you can

00:20:13,910 --> 00:20:17,120
solve this in different ways obviously

00:20:15,050 --> 00:20:19,430
there are some improvements being done

00:20:17,120 --> 00:20:21,770
as far as from 4:11 release some of

00:20:19,430 --> 00:20:23,330
those things and so on but the way it

00:20:21,770 --> 00:20:28,100
happened for us is that basically we

00:20:23,330 --> 00:20:30,020
went with solid yeah we went with the

00:20:28,100 --> 00:20:31,730
SolidFire guys and I will now explain

00:20:30,020 --> 00:20:35,450
how to address each of the problems

00:20:31,730 --> 00:20:37,790
briefed and we'll do two out of five

00:20:35,450 --> 00:20:40,120
very hopefully very interesting demos

00:20:37,790 --> 00:20:44,570
for you so just a few words about

00:20:40,120 --> 00:20:47,000
SolidFire it's basically based on the

00:20:44,570 --> 00:20:48,860
whole logic of SolidFire storage is

00:20:47,000 --> 00:20:51,470
based around quality of service which is

00:20:48,860 --> 00:20:54,080
defined by three values out of many

00:20:51,470 --> 00:20:56,630
properties for volume its volume in my

00:20:54,080 --> 00:20:58,430
UPS we'll explain later maximum I option

00:20:56,630 --> 00:21:00,080
and burst types you can read a bit of

00:20:58,430 --> 00:21:02,780
more details but I will explain now in a

00:21:00,080 --> 00:21:05,240
second so basically the minimum AFC is

00:21:02,780 --> 00:21:07,580
property of each volume that resides on

00:21:05,240 --> 00:21:11,090
lives on SolidFire storage which

00:21:07,580 --> 00:21:13,700
basically is guaranteed 100% in all

00:21:11,090 --> 00:21:16,280
normal and the most volatile conditions

00:21:13,700 --> 00:21:18,380
so including cluster expansion repair

00:21:16,280 --> 00:21:21,710
maintenance SSD health checks which are

00:21:18,380 --> 00:21:23,150
made an analogy to say fatigue scrubs

00:21:21,710 --> 00:21:25,550
and so on because there is a certain

00:21:23,150 --> 00:21:27,620
amount of whole cluster I or is there

00:21:25,550 --> 00:21:28,730
for internal operation and the other

00:21:27,620 --> 00:21:31,490
part of the i/o

00:21:28,730 --> 00:21:33,470
poor instance of how many hours you can

00:21:31,490 --> 00:21:36,440
offer to clients is actually reserved

00:21:33,470 --> 00:21:38,360
for the clients also nice thing but this

00:21:36,440 --> 00:21:40,340
is actually you can argue is true for

00:21:38,360 --> 00:21:41,810
any storage SolidFire snapshot stands

00:21:40,340 --> 00:21:44,780
for acquiring and this is how I support

00:21:41,810 --> 00:21:45,980
in cloud stack so creation and restore

00:21:44,780 --> 00:21:48,679
from Stanford is

00:21:45,980 --> 00:21:51,260
like two seconds or three seconds from

00:21:48,679 --> 00:21:54,049
the GUI of cloud stack in the backend is

00:21:51,260 --> 00:21:56,059
probably one even more a quick no more

00:21:54,049 --> 00:21:58,070
food no more blogs or queues stability

00:21:56,059 --> 00:22:00,860
issue solved no need a network traffic

00:21:58,070 --> 00:22:03,620
and sending data to secondary storage a

00:22:00,860 --> 00:22:06,440
bit of throughput issues solved

00:22:03,620 --> 00:22:07,760
no no basically issues a large number of

00:22:06,440 --> 00:22:10,090
snaps to the cause of the way it's

00:22:07,760 --> 00:22:12,799
implemented internally SolidFire and

00:22:10,090 --> 00:22:14,990
also when you do anything when you have

00:22:12,799 --> 00:22:17,570
a handle any kind of say new storage

00:22:14,990 --> 00:22:19,820
ease itself is it SolidFire is it any

00:22:17,570 --> 00:22:22,190
kind of other solutions you do need to

00:22:19,820 --> 00:22:24,140
have some management skills with some

00:22:22,190 --> 00:22:26,150
storage solution it really is really a

00:22:24,140 --> 00:22:27,290
challenge to actually solve this so but

00:22:26,150 --> 00:22:30,760
with SolidFire

00:22:27,290 --> 00:22:33,049
I personally really two business days

00:22:30,760 --> 00:22:35,150
register cost multiple occasions and

00:22:33,049 --> 00:22:37,580
scenarios is enough to actually handle

00:22:35,150 --> 00:22:39,980
what you need to handle as the client

00:22:37,580 --> 00:22:42,380
everything else is actually about by the

00:22:39,980 --> 00:22:46,790
I said SolidFire which is now net of

00:22:42,380 --> 00:22:48,650
support including upgrades expansions

00:22:46,790 --> 00:22:50,780
monitoring and alerting you get a dead

00:22:48,650 --> 00:22:52,610
power supply I came here that I gave

00:22:50,780 --> 00:22:53,210
that power supply the guys tonight off

00:22:52,610 --> 00:22:55,250
send me an email

00:22:53,210 --> 00:22:58,190
Andrea said power supply cable cable

00:22:55,250 --> 00:23:01,309
fine so a power supply dead we sent

00:22:58,190 --> 00:23:03,110
same day so I cannot my personal yet

00:23:01,309 --> 00:23:05,510
extremely a positive experience the

00:23:03,110 --> 00:23:07,790
SolidFire guys not a commercial for them

00:23:05,510 --> 00:23:10,460
but you don't actually care with all the

00:23:07,790 --> 00:23:12,140
vendors you will argue you will actually

00:23:10,460 --> 00:23:13,549
agree on that and I actually think of

00:23:12,140 --> 00:23:15,110
this system energy storage is I mean

00:23:13,549 --> 00:23:17,450
every way not in a cloud stack terms

00:23:15,110 --> 00:23:19,880
manage storage but in a basically every

00:23:17,450 --> 00:23:22,520
possible way and also no external live

00:23:19,880 --> 00:23:25,040
our live with library so on there is no

00:23:22,520 --> 00:23:29,020
way to crush that is delivered because

00:23:25,040 --> 00:23:31,400
it's only I scuzzy looms that are

00:23:29,020 --> 00:23:33,200
mounted remotely to the host and then

00:23:31,400 --> 00:23:36,260
just pass through like a local volumes

00:23:33,200 --> 00:23:38,750
within these are lists of some of the

00:23:36,260 --> 00:23:42,799
improvements that we have actually kind

00:23:38,750 --> 00:23:44,150
of requested or us might to implement as

00:23:42,799 --> 00:23:46,270
I said this is not a commercial to

00:23:44,150 --> 00:23:50,150
source fire so you do see that some of

00:23:46,270 --> 00:23:53,210
you I can say may be a big medium basic

00:23:50,150 --> 00:23:54,860
features were not there so you could not

00:23:53,210 --> 00:23:57,049
the template you cannot download volume

00:23:54,860 --> 00:23:59,450
from SolidFire and so on you could all

00:23:57,049 --> 00:24:01,399
lead to do some couple of steps so you

00:23:59,450 --> 00:24:03,799
know it's not all shiny initially and

00:24:01,399 --> 00:24:05,480
then we basically ask Mike to help us

00:24:03,799 --> 00:24:08,059
with this stuff which we did and this is

00:24:05,480 --> 00:24:10,640
now part of the 411 we use it in a 4h

00:24:08,059 --> 00:24:12,440
and basically we I believe we don't miss

00:24:10,640 --> 00:24:15,200
any kind of storage feature when you do

00:24:12,440 --> 00:24:16,700
any kind of storage just stuff in cloud

00:24:15,200 --> 00:24:18,529
stack we don't miss anything when we

00:24:16,700 --> 00:24:21,320
compare it to what we could offer to our

00:24:18,529 --> 00:24:24,649
clients with NFS itself so let's move

00:24:21,320 --> 00:24:26,630
forward how is basically snap to think

00:24:24,649 --> 00:24:28,750
which I already explained the issue of

00:24:26,630 --> 00:24:31,279
uh snapshots of being able to restore

00:24:28,750 --> 00:24:33,289
takes forever a small car it's all good

00:24:31,279 --> 00:24:35,240
so it's fire and by the way we

00:24:33,289 --> 00:24:37,130
internally on the renamed is for backups

00:24:35,240 --> 00:24:39,309
in the GUI because it's not sensitive

00:24:37,130 --> 00:24:42,289
very backup now we need to revert back

00:24:39,309 --> 00:24:44,750
so how we solve this so basically the

00:24:42,289 --> 00:24:46,340
way that the the SolidFire plug-in

00:24:44,750 --> 00:24:47,539
inside cloud stick is implemented is

00:24:46,340 --> 00:24:50,090
that it references

00:24:47,539 --> 00:24:52,070
next was a SolidFire there is no coping

00:24:50,090 --> 00:24:54,799
over to secondary storage so it's fast

00:24:52,070 --> 00:24:56,330
it's blazingly fast these are real

00:24:54,799 --> 00:24:58,549
snatches that semester because you can

00:24:56,330 --> 00:25:01,100
easily restore from original VM instance

00:24:58,549 --> 00:25:02,510
they this is whatever internal thing

00:25:01,100 --> 00:25:04,190
which you maybe don't care but there is

00:25:02,510 --> 00:25:06,320
almost no space consumption not

00:25:04,190 --> 00:25:08,260
literally but almost and no needs to

00:25:06,320 --> 00:25:12,440
move tons of data across your network

00:25:08,260 --> 00:25:15,200
and no block job using cloud stack so

00:25:12,440 --> 00:25:16,640
you can start your VM if you want now

00:25:15,200 --> 00:25:18,500
this is a three minute demo which I'm

00:25:16,640 --> 00:25:20,090
not going to do because I went in to

00:25:18,500 --> 00:25:22,100
kind of say these three minutes for

00:25:20,090 --> 00:25:24,380
other interesting one it literally takes

00:25:22,100 --> 00:25:27,110
three minutes to produce make a VM

00:25:24,380 --> 00:25:29,600
create important data create snapshot

00:25:27,110 --> 00:25:32,059
caused destruction by deleting the data

00:25:29,600 --> 00:25:33,950
and then restore the Cole VM or the root

00:25:32,059 --> 00:25:35,480
volume or whatever you want it takes

00:25:33,950 --> 00:25:39,200
literal three minutes so please you can

00:25:35,480 --> 00:25:40,730
do it at your own pace the demos are

00:25:39,200 --> 00:25:42,740
embedded and there is also links at the

00:25:40,730 --> 00:25:43,250
end of the slides so you can actually um

00:25:42,740 --> 00:25:45,980
directly

00:25:43,250 --> 00:25:47,660
YouTube if you want so let's move

00:25:45,980 --> 00:25:49,610
forward because I need to spend time for

00:25:47,660 --> 00:25:51,620
a more interesting that was so quality

00:25:49,610 --> 00:25:53,780
of Sandra's already mentioned it's kind

00:25:51,620 --> 00:25:56,060
of halfway there because you can only

00:25:53,780 --> 00:25:58,220
limit read and write rates in bytes per

00:25:56,060 --> 00:26:00,320
second or iOS what share what I owe per

00:25:58,220 --> 00:26:02,630
second whatever limit you sit first and

00:26:00,320 --> 00:26:04,400
the one we're very very important thing

00:26:02,630 --> 00:26:06,680
and not related to the safe anaphase

00:26:04,400 --> 00:26:08,300
it's ready to any storage you seriously

00:26:06,680 --> 00:26:10,850
need to have some hoodoo scale structure

00:26:08,300 --> 00:26:12,710
understand if your clients are having

00:26:10,850 --> 00:26:14,570
good performance after one year in

00:26:12,710 --> 00:26:16,940
production when you rolled to time more

00:26:14,570 --> 00:26:18,920
clients in the same storage how the hell

00:26:16,940 --> 00:26:21,260
do you know if the client still care

00:26:18,920 --> 00:26:23,540
good latency says they get one year ago

00:26:21,260 --> 00:26:24,890
because you usually don't your access to

00:26:23,540 --> 00:26:29,240
clients again there is no way to measure

00:26:24,890 --> 00:26:31,400
it and basically this is what well these

00:26:29,240 --> 00:26:34,760
are another positive benefit actually so

00:26:31,400 --> 00:26:36,530
it fire and basically we're leveraging

00:26:34,760 --> 00:26:39,140
part of service which I mentioned is

00:26:36,530 --> 00:26:40,810
their call let's say center over so far

00:26:39,140 --> 00:26:43,160
it was built around part of service

00:26:40,810 --> 00:26:45,440
there is ZERO possibility for noisy

00:26:43,160 --> 00:26:47,720
neighborhood so basically clients

00:26:45,440 --> 00:26:49,160
usually get much more what they pay for

00:26:47,720 --> 00:26:51,890
which is the maximum my UPS and in

00:26:49,160 --> 00:26:54,140
certain cases burst tiles which can give

00:26:51,890 --> 00:26:56,120
you a burst up to certain amount of I

00:26:54,140 --> 00:26:58,850
have seen a critical times during VM

00:26:56,120 --> 00:27:00,080
reboots database dumps and so on and it

00:26:58,850 --> 00:27:01,880
also here's a perfect contouring

00:27:00,080 --> 00:27:03,920
environment basically where you can see

00:27:01,880 --> 00:27:05,320
if you need more gigabytes terabytes or

00:27:03,920 --> 00:27:08,240
you need more ice

00:27:05,320 --> 00:27:11,360
so we will do a dis them and I will

00:27:08,240 --> 00:27:13,010
explain as we go through but when you

00:27:11,360 --> 00:27:17,320
maybe view the slide so fine you can

00:27:13,010 --> 00:27:17,320
read as well so

00:27:19,550 --> 00:27:26,470
I feel something something yeah

00:27:27,779 --> 00:27:33,179
the demos are recorded obviously for

00:27:30,590 --> 00:27:40,049
making sure are we going to do any kind

00:27:33,179 --> 00:27:44,309
of issues the demos are with sound is

00:27:40,049 --> 00:27:49,049
just now muted so we have a basically

00:27:44,309 --> 00:27:51,389
demo VM nothing special and you can

00:27:49,049 --> 00:27:54,119
observe extra develops of this VM we

00:27:51,389 --> 00:27:56,729
have one root volume it's located on

00:27:54,119 --> 00:27:58,289
manna-fest in this example this was of

00:27:56,729 --> 00:28:00,899
just the format deploy here from

00:27:58,289 --> 00:28:02,609
template will use the vision of xml with

00:28:00,899 --> 00:28:04,200
grep in the i/o tools section you can

00:28:02,609 --> 00:28:06,839
actually see that it read by side by

00:28:04,200 --> 00:28:08,549
smile implemented item section and

00:28:06,839 --> 00:28:11,429
inside there for some reason you guys

00:28:08,549 --> 00:28:14,849
decided to show you here one volume now

00:28:11,429 --> 00:28:16,739
i will actually what I will do and you

00:28:14,849 --> 00:28:20,460
can transplant this I will actually

00:28:16,739 --> 00:28:25,549
attach one NFS data volume one safe data

00:28:20,460 --> 00:28:32,549
volume and one SolidFire data volume and

00:28:25,549 --> 00:28:36,089
yeah let me just verify this or you can

00:28:32,549 --> 00:28:38,070
actually see we have route 1 and FS 1 so

00:28:36,089 --> 00:28:40,049
it fire on one cell for you but the

00:28:38,070 --> 00:28:41,969
SolidFire is the last one just for the

00:28:40,049 --> 00:28:43,589
reference later if we go back to the

00:28:41,969 --> 00:28:45,479
hypervisor actually excite the VM you

00:28:43,589 --> 00:28:48,119
see three new volumes are attached will

00:28:45,479 --> 00:28:49,769
be recvd whatever and if you go back to

00:28:48,119 --> 00:28:51,539
the cost physical cave in post if you

00:28:49,769 --> 00:28:53,820
repeat the same deep dumping smell

00:28:51,539 --> 00:28:56,759
command we still see item section 4 root

00:28:53,820 --> 00:28:59,909
volume no iTunes section 4 V DB which is

00:28:56,759 --> 00:29:00,989
an offense ability itself and VD e which

00:28:59,909 --> 00:29:02,940
is SolidFire

00:29:00,989 --> 00:29:04,529
but that's fine for the lifespan at

00:29:02,940 --> 00:29:06,839
least so there is no engine section

00:29:04,529 --> 00:29:09,719
either though we use the disc offerings

00:29:06,839 --> 00:29:11,330
which do specify select stop and start

00:29:09,719 --> 00:29:14,399
the VM so it will actually pick up

00:29:11,330 --> 00:29:16,529
properly the thing the meantime and just

00:29:14,399 --> 00:29:18,389
briefly showing you so if I reviewing

00:29:16,529 --> 00:29:21,330
this is our quality of service volume as

00:29:18,389 --> 00:29:22,710
to college there is minimum is maximum

00:29:21,330 --> 00:29:23,639
my ops and burst types which is

00:29:22,710 --> 00:29:26,249
multiplier

00:29:23,639 --> 00:29:28,889
let's say 2 times this 4,000 or the

00:29:26,249 --> 00:29:30,140
maximum miles and basically this how a

00:29:28,889 --> 00:29:31,370
quarter service is done

00:29:30,140 --> 00:29:34,790
all right what is done on the storage

00:29:31,370 --> 00:29:37,640
side not on the hearth of either side if

00:29:34,790 --> 00:29:41,030
you start the VM basically we will very

00:29:37,640 --> 00:29:43,580
briefly be able to see a proper KVM

00:29:41,030 --> 00:29:46,790
quality service implemented for NFS data

00:29:43,580 --> 00:29:50,030
this self data this so basically I will

00:29:46,790 --> 00:29:53,090
open here a new tab and copy are the

00:29:50,030 --> 00:30:00,110
same command of which the 50 ml then

00:29:53,090 --> 00:30:02,060
wrapping interesting sections and now if

00:30:00,110 --> 00:30:04,430
you repeat the same with lambics ml we

00:30:02,060 --> 00:30:06,470
see I tuned on route volume I attune

00:30:04,430 --> 00:30:09,320
section which is gave a way to connect

00:30:06,470 --> 00:30:11,810
something on NFS on self and still no

00:30:09,320 --> 00:30:13,070
item section 4 so it's part because it

00:30:11,810 --> 00:30:17,030
shouldn't be there it's in storage

00:30:13,070 --> 00:30:21,920
inside and just so basically now that's

00:30:17,030 --> 00:30:24,320
pretty much it on the KB level for some

00:30:21,920 --> 00:30:26,320
reason I decided to show a master Y that

00:30:24,320 --> 00:30:31,190
we still care for volumes inside the VM

00:30:26,320 --> 00:30:32,720
but if we go to if you go actually to

00:30:31,190 --> 00:30:35,120
show you how is this differently

00:30:32,720 --> 00:30:37,550
implemented hypervisor part asterisk or

00:30:35,120 --> 00:30:39,320
storage molecule series let's go to

00:30:37,550 --> 00:30:44,840
actually disk offering so let's actually

00:30:39,320 --> 00:30:46,280
see almost fold our mic so some regular

00:30:44,840 --> 00:30:48,560
part of it is named scription sighs I

00:30:46,280 --> 00:30:49,970
saw nothing special there but the

00:30:48,560 --> 00:30:53,210
interesting field is quality of service

00:30:49,970 --> 00:30:55,280
type if you choose hypervisor you see

00:30:53,210 --> 00:30:57,290
here this great discovery Drake this

00:30:55,280 --> 00:30:59,210
right rate in bytes and same for in I

00:30:57,290 --> 00:31:00,920
ops per second different one of this

00:30:59,210 --> 00:31:02,720
will be part of the cave area 20

00:31:00,920 --> 00:31:05,690
generate 60mph additional file and

00:31:02,720 --> 00:31:08,260
that's how you actually generate quality

00:31:05,690 --> 00:31:10,910
service on a cable side if we choose

00:31:08,260 --> 00:31:13,070
storage quality of service we can

00:31:10,910 --> 00:31:15,260
minimize maximize and this will be these

00:31:13,070 --> 00:31:17,810
values will be used by SOI fire plugging

00:31:15,260 --> 00:31:20,300
it will be best to the beckoned

00:31:17,810 --> 00:31:22,400
SolidFire cluster and it will be created

00:31:20,300 --> 00:31:23,960
it will become the properties over

00:31:22,400 --> 00:31:26,360
freshly created hole a minimum maximum

00:31:23,960 --> 00:31:29,150
you don't see here burst because burst

00:31:26,360 --> 00:31:31,850
is are basically card code so to speak

00:31:29,150 --> 00:31:34,490
by unity engine data base multiplier of

00:31:31,850 --> 00:31:35,130
the maximum volume maximum is whatever

00:31:34,490 --> 00:31:37,890
the maximum

00:31:35,130 --> 00:31:39,780
you can say but is this five times more

00:31:37,890 --> 00:31:42,630
because if the only birds for one minute

00:31:39,780 --> 00:31:45,090
or two so that's been basically to the

00:31:42,630 --> 00:31:53,330
differences of quality of service on KVM

00:31:45,090 --> 00:31:53,330
and sorry

00:31:56,750 --> 00:32:04,520
yeah so you spend a lot of money and

00:32:02,419 --> 00:32:06,380
your bar you have a nice SolidFire

00:32:04,520 --> 00:32:08,030
cluster you know and you change all of

00:32:06,380 --> 00:32:09,890
these offerings and they and to be

00:32:08,030 --> 00:32:11,299
talking structurally deploy new

00:32:09,890 --> 00:32:14,210
resources on the SolidFire that's

00:32:11,299 --> 00:32:16,640
perfect but you have probably tens of

00:32:14,210 --> 00:32:18,260
hundreds of volumes and how are that are

00:32:16,640 --> 00:32:21,230
left from several FS or whatever

00:32:18,260 --> 00:32:23,450
chemical reaction memories one way which

00:32:21,230 --> 00:32:25,039
was freshly implemented its offline

00:32:23,450 --> 00:32:27,080
storage migration but it's really not

00:32:25,039 --> 00:32:31,970
logistically possible due to

00:32:27,080 --> 00:32:33,860
negotiations simply not possible then

00:32:31,970 --> 00:32:35,419
what we become greedy and whereas Mike

00:32:33,860 --> 00:32:38,210
can you please implement online storage

00:32:35,419 --> 00:32:40,340
migration this is estimation Ewing 411

00:32:38,210 --> 00:32:42,820
we're using embed ported please pay

00:32:40,340 --> 00:32:45,320
attention to the extremely nasty bug

00:32:42,820 --> 00:32:47,900
read it when you have time because if

00:32:45,320 --> 00:32:50,510
you don't handle it it will limit your

00:32:47,900 --> 00:32:52,100
migration speed to 1.3 megabytes and

00:32:50,510 --> 00:32:53,299
then you will say why so his parish so

00:32:52,100 --> 00:32:56,539
slow if there's nothing to do with

00:32:53,299 --> 00:32:58,640
SolidFire it's a Liberty bug now how

00:32:56,539 --> 00:33:00,039
this works actually this works we will

00:32:58,640 --> 00:33:03,289
finish in a demo later but basically

00:33:00,039 --> 00:33:05,179
volumes of the VMS or what the VM which

00:33:03,289 --> 00:33:08,090
is for example existing or NFS are

00:33:05,179 --> 00:33:09,620
mirror on a block level that mirror to

00:33:08,090 --> 00:33:12,380
automatically provision volumes from

00:33:09,620 --> 00:33:15,679
SolidFire and then at 99% when it's done

00:33:12,380 --> 00:33:17,780
the kv real decided will kick in the VM

00:33:15,679 --> 00:33:20,450
live migration which is Cyprian raw

00:33:17,780 --> 00:33:22,730
memory basically or the ROM memory

00:33:20,450 --> 00:33:24,740
copying content copying content or

00:33:22,730 --> 00:33:28,400
remember from a source close to the

00:33:24,740 --> 00:33:29,809
destination cost but this is now the

00:33:28,400 --> 00:33:32,330
special case which I mentioned

00:33:29,809 --> 00:33:34,730
previously you migrate robotics two

00:33:32,330 --> 00:33:38,150
volumes potentially and then km says

00:33:34,730 --> 00:33:39,799
yeah finally let's wire migrate VM and

00:33:38,150 --> 00:33:42,440
then you are stuck because the VM is

00:33:39,799 --> 00:33:44,450
busy it's a lot of a sequel server so

00:33:42,440 --> 00:33:47,720
the way we saw this is again thanks to

00:33:44,450 --> 00:33:51,350
Mike actually we implemented or Mike

00:33:47,720 --> 00:33:54,470
implemented being able to consume

00:33:51,350 --> 00:33:56,059
dynamic auto conversions by bogus techo

00:33:54,470 --> 00:33:58,000
than a single global parameter and

00:33:56,059 --> 00:34:01,990
basically this is available only

00:33:58,000 --> 00:34:04,510
kamo 2.5 in newer by default it's kind

00:34:01,990 --> 00:34:06,760
of with the intelligent frothing sort of

00:34:04,510 --> 00:34:08,350
wave dynamically trickles more and more

00:34:06,760 --> 00:34:10,629
and more up to the point it actually

00:34:08,350 --> 00:34:12,730
works but there is also regular auto

00:34:10,629 --> 00:34:13,720
collisions from chem 1/6 and newer which

00:34:12,730 --> 00:34:19,210
is useless

00:34:13,720 --> 00:34:20,679
believe me so here we have two demos

00:34:19,210 --> 00:34:23,290
which are going to skip and I will

00:34:20,679 --> 00:34:25,030
explain them the first is just showing

00:34:23,290 --> 00:34:27,580
the dynamic out of colleges versus

00:34:25,030 --> 00:34:28,780
extremely busy VM fight so basically we

00:34:27,580 --> 00:34:31,030
are using again the stress of

00:34:28,780 --> 00:34:33,730
testability were writing 259 gigabytes

00:34:31,030 --> 00:34:37,240
of RAM with 24

00:34:33,730 --> 00:34:39,460
CPUs I'm not sure there's any data bases

00:34:37,240 --> 00:34:41,050
are not literal any but not many get

00:34:39,460 --> 00:34:43,750
that busy but this is extreme of the

00:34:41,050 --> 00:34:45,460
extreme and then you will observe with

00:34:43,750 --> 00:34:48,879
will topically to be observed of CPU

00:34:45,460 --> 00:34:50,290
utilization on the host a single VM or

00:34:48,879 --> 00:34:52,960
single host so you can use it very

00:34:50,290 --> 00:34:54,550
easily and you can then read it from

00:34:52,960 --> 00:34:59,470
your own pace I'm going to do a super

00:34:54,550 --> 00:35:01,930
them of this one and the name of the

00:34:59,470 --> 00:35:04,630
only storage migration of this one is a

00:35:01,930 --> 00:35:06,700
perfect world them off I will be able to

00:35:04,630 --> 00:35:10,330
nothing which is not what you have in

00:35:06,700 --> 00:35:14,710
production and how you migrate NFS and

00:35:10,330 --> 00:35:17,610
self volumes example to SolidFire this

00:35:14,710 --> 00:35:21,150
is a new API call with 411 release

00:35:17,610 --> 00:35:23,800
basically to specify the virtual machine

00:35:21,150 --> 00:35:25,890
the tape echo of course yes but then you

00:35:23,800 --> 00:35:29,080
need to specify a little machine ID or

00:35:25,890 --> 00:35:30,940
which volumes are migrating since we are

00:35:29,080 --> 00:35:32,260
doing also the live migration of the VM

00:35:30,940 --> 00:35:34,690
at the end we also need to specify

00:35:32,260 --> 00:35:37,780
destination host and of course we have

00:35:34,690 --> 00:35:40,180
pairs of destination pool and a source

00:35:37,780 --> 00:35:42,310
volume destination pool and a source

00:35:40,180 --> 00:35:44,860
volume so basically this is aa spoil

00:35:42,310 --> 00:35:48,970
SolidFire you are ID or ID from the GUI

00:35:44,860 --> 00:35:50,500
and NFS ID again of the source folder

00:35:48,970 --> 00:35:54,730
which won't migrate will repeat same

00:35:50,500 --> 00:35:57,730
pair of let's say variables for self so

00:35:54,730 --> 00:35:58,950
it's very easy it's only kind of five

00:35:57,730 --> 00:36:01,349
minutes and you can do it

00:35:58,950 --> 00:36:04,230
on base basically I'm going to do a

00:36:01,349 --> 00:36:07,079
super demo which is I guess something

00:36:04,230 --> 00:36:09,450
extra don't see that regular e/m is kind

00:36:07,079 --> 00:36:13,260
of almost crazy scientist so we will

00:36:09,450 --> 00:36:15,720
cause the busy VM by writing which one

00:36:13,260 --> 00:36:17,250
to four CPUs using the stress sub test

00:36:15,720 --> 00:36:18,720
utility we are going to write with 60

00:36:17,250 --> 00:36:21,450
gigabytes of RAM it's basically just

00:36:18,720 --> 00:36:22,859
pooping over our memory pages you can

00:36:21,450 --> 00:36:25,550
believe me you can see ocular at your

00:36:22,859 --> 00:36:28,410
own with over 12 gigabytes per second

00:36:25,550 --> 00:36:29,910
copy speed inside the VM and you can

00:36:28,410 --> 00:36:31,740
imagine if this is circled enough then

00:36:29,910 --> 00:36:33,119
it clearly works at the same time we are

00:36:31,740 --> 00:36:34,770
touching two NFS forums which is going

00:36:33,119 --> 00:36:37,950
to explain during the demo we'll do the

00:36:34,770 --> 00:36:40,230
feel benchmark heavy writing heavily to

00:36:37,950 --> 00:36:42,300
the source volume and then try to do a

00:36:40,230 --> 00:36:46,380
migration of the source volume which are

00:36:42,300 --> 00:36:48,089
every stress to the SolidFire I will

00:36:46,380 --> 00:36:53,070
explain during the demo so you don't

00:36:48,089 --> 00:36:54,450
need to actually read everything this is

00:36:53,070 --> 00:37:00,200
a fancy branding that they did just for

00:36:54,450 --> 00:37:00,200
this demo of the GUI completely amateur

00:37:05,890 --> 00:37:12,130
let's login to our fancy branded

00:37:08,020 --> 00:37:18,790
environment and we have a VM which were

00:37:12,130 --> 00:37:20,470
interested in interested in click you

00:37:18,790 --> 00:37:23,380
will actually see that this VM is with

00:37:20,470 --> 00:37:26,620
Africa 24 CPUs on SolidFire here you see

00:37:23,380 --> 00:37:28,470
actually 24 CPUs it exists on certain

00:37:26,620 --> 00:37:31,300
physical hosts we will see that later

00:37:28,470 --> 00:37:33,280
during the demo if you check the volume

00:37:31,300 --> 00:37:35,440
of this VM we see one root volume which

00:37:33,280 --> 00:37:36,970
is actually on SolidFire we don't care

00:37:35,440 --> 00:37:38,590
about this one forget that I ever

00:37:36,970 --> 00:37:40,360
mentioned but just for the reference

00:37:38,590 --> 00:37:43,030
this one is all there is a voice fire

00:37:40,360 --> 00:37:46,750
but we here to all volumes you can

00:37:43,030 --> 00:37:48,970
imagine of 100 gigabytes using NFS

00:37:46,750 --> 00:37:51,820
offerings existing or certain FS storage

00:37:48,970 --> 00:37:53,650
1 volume 2 volumes and basically we'll

00:37:51,820 --> 00:37:57,030
do our soft terrain inside the VM which

00:37:53,650 --> 00:38:00,370
you will see raid 0 actually on top of 2

00:37:57,030 --> 00:38:01,750
data volumes so we can actually go right

00:38:00,370 --> 00:38:05,220
in parallel and suppress them in

00:38:01,750 --> 00:38:05,220
parallel in a serious way

00:38:05,820 --> 00:38:12,760
so yeah source host we have a VM running

00:38:09,790 --> 00:38:15,400
obviously wishlist nothing special it

00:38:12,760 --> 00:38:16,900
says or source force as I mentioned the

00:38:15,400 --> 00:38:19,750
sound exists with the demo so I

00:38:16,900 --> 00:38:24,100
basically repeat myself as well on the

00:38:19,750 --> 00:38:25,270
destination cover sorry maybe us again

00:38:24,100 --> 00:38:27,250
on the source Coast we're using the

00:38:25,270 --> 00:38:29,760
victim job info which is a way to

00:38:27,250 --> 00:38:31,990
observe the live migration not online

00:38:29,760 --> 00:38:33,940
migration just the V my elaboration

00:38:31,990 --> 00:38:36,640
process they were also using the visual

00:38:33,940 --> 00:38:39,940
blocks job info on to source volumes

00:38:36,640 --> 00:38:41,530
both NFS aren't there any block level of

00:38:39,940 --> 00:38:43,810
jobs which actually means on my storage

00:38:41,530 --> 00:38:46,150
migration is it started or not for now

00:38:43,810 --> 00:38:48,850
nothing we started for now we will use

00:38:46,150 --> 00:38:50,950
first opportunity to 59.5 gigabyte of

00:38:48,850 --> 00:38:53,310
ram we only a kind of 500 operating

00:38:50,950 --> 00:38:56,800
system to breathe with certain

00:38:53,310 --> 00:38:58,720
parameters for for for us the stuff we

00:38:56,800 --> 00:39:02,230
have the route volume we have two data

00:38:58,720 --> 00:39:04,600
volumes to 100 gigs we have the ability

00:39:02,230 --> 00:39:06,820
C whatever and we have software raid 0

00:39:04,600 --> 00:39:08,850
on top of this if you check you see

00:39:06,820 --> 00:39:13,240
actually almost 200 gigabytes

00:39:08,850 --> 00:39:16,060
formatted file system with x4 and we are

00:39:13,240 --> 00:39:19,900
using a Futurity which you are all

00:39:16,060 --> 00:39:21,730
familiar with to actually write to write

00:39:19,900 --> 00:39:23,140
the 100 meter by file of course 200

00:39:21,730 --> 00:39:25,000
gigabyte volumes this is their

00:39:23,140 --> 00:39:27,250
environments time limiting the right

00:39:25,000 --> 00:39:29,860
rate 150 megabytes but you will agreed

00:39:27,250 --> 00:39:32,680
huge amount of megabytes per second zero

00:39:29,860 --> 00:39:35,350
CPU utilization I start zero this

00:39:32,680 --> 00:39:38,530
visualization nothing so let's now make

00:39:35,350 --> 00:39:40,420
a busy I'm really busy let's start let's

00:39:38,530 --> 00:39:42,520
say torture test so to speak

00:39:40,420 --> 00:39:45,760
this takes a couple of seconds to

00:39:42,520 --> 00:39:48,910
actually really build up 5 to 10 seconds

00:39:45,760 --> 00:39:51,100
not more let's also hit the feel we are

00:39:48,910 --> 00:39:55,150
limited to 150 megabytes you can see

00:39:51,100 --> 00:39:57,160
here it actually does as much and after

00:39:55,150 --> 00:39:58,720
a few seconds of the specificity running

00:39:57,160 --> 00:40:01,690
you will observe the CPUs they are

00:39:58,720 --> 00:40:04,390
literally 100% all of them are actually

00:40:01,690 --> 00:40:07,270
being used to copy memory inside the VM

00:40:04,390 --> 00:40:09,130
with a huge migration rate source we use

00:40:07,270 --> 00:40:11,380
the will top utility we see the super

00:40:09,130 --> 00:40:14,230
percentages on a source horse which

00:40:11,380 --> 00:40:17,650
various receive idiom around 75% which

00:40:14,230 --> 00:40:19,690
is 24 cpus out of 32 physical on the

00:40:17,650 --> 00:40:21,580
horse and this will get down and down

00:40:19,690 --> 00:40:24,520
and down and down as the camel

00:40:21,580 --> 00:40:26,380
dynamic ecologist later kicks him so

00:40:24,520 --> 00:40:28,780
next time the kernel monkey lets

00:40:26,380 --> 00:40:31,960
basically show array people to try to

00:40:28,780 --> 00:40:34,600
migrate only my greatest vm from and if

00:40:31,960 --> 00:40:37,450
that's south fire we have api call

00:40:34,600 --> 00:40:40,630
virtual machine ID host ID and then as I

00:40:37,450 --> 00:40:42,580
mentioned migrate of 0 which is and then

00:40:40,630 --> 00:40:45,760
pool which is SolidFire pool destination

00:40:42,580 --> 00:40:47,860
source for the one NFS one again so if I

00:40:45,760 --> 00:40:51,730
prove this nation the second volume

00:40:47,860 --> 00:40:53,410
ID so we can get hit enter and just wait

00:40:51,730 --> 00:40:55,870
for our two three seconds so it really

00:40:53,410 --> 00:40:59,940
actually starts on the let's say

00:40:55,870 --> 00:41:05,100
physical level on the storage end and

00:40:59,940 --> 00:41:07,110
hypervisor just a few seconds if you'll

00:41:05,100 --> 00:41:09,300
start now we still not actually start to

00:41:07,110 --> 00:41:11,250
do my immigration it's a preparing but

00:41:09,300 --> 00:41:13,500
here you see a block level drug copy

00:41:11,250 --> 00:41:16,380
process this is the mirroring of true

00:41:13,500 --> 00:41:18,990
and fs100 eggs to a automatically

00:41:16,380 --> 00:41:21,060
provision took two times 100 legal soy

00:41:18,990 --> 00:41:23,160
fire you will see that's roughly on

00:41:21,060 --> 00:41:25,440
destination cost we see the same name of

00:41:23,160 --> 00:41:27,180
the VM but in a paused state this room

00:41:25,440 --> 00:41:29,370
has identical extender finish the file

00:41:27,180 --> 00:41:30,660
of the source p.m. it's powder but it is

00:41:29,370 --> 00:41:33,470
not really light identical it's

00:41:30,660 --> 00:41:37,410
referencing SolidFire voters now not NFS

00:41:33,470 --> 00:41:40,040
this is a nice solid fire GUI these top

00:41:37,410 --> 00:41:42,030
two are the two 100 big volumes which we

00:41:40,040 --> 00:41:44,250
automatically provisioned by issuing the

00:41:42,030 --> 00:41:46,890
API call I hope basically briefly just

00:41:44,250 --> 00:41:49,170
increase the make maximum is to 25000 so

00:41:46,890 --> 00:41:53,070
actually this makes sense and I will

00:41:49,170 --> 00:41:57,150
repeat this I will repeat this for the

00:41:53,070 --> 00:41:59,810
second volume as well to see minimum

00:41:57,150 --> 00:42:02,820
maximum you can override the burst types

00:41:59,810 --> 00:42:05,550
for the single volume this discussion is

00:42:02,820 --> 00:42:07,470
part of series manual of course but you

00:42:05,550 --> 00:42:10,530
use it through the cloud secure a normal

00:42:07,470 --> 00:42:12,000
way if you observe the reporting tab and

00:42:10,530 --> 00:42:14,790
volume performance you can see hole and

00:42:12,000 --> 00:42:16,050
performance volume which is not

00:42:14,790 --> 00:42:17,790
something you can see the ratio of

00:42:16,050 --> 00:42:20,130
different storage solutions it takes a

00:42:17,790 --> 00:42:22,770
couple of seconds to actually build up

00:42:20,130 --> 00:42:25,190
with Rakuten and so on basically I will

00:42:22,770 --> 00:42:30,750
now try to do a kind of a split window

00:42:25,190 --> 00:42:32,610
view between the migration process of

00:42:30,750 --> 00:42:35,730
the volumes as seen from the hypervisor

00:42:32,610 --> 00:42:38,430
side and akkadian and the actual

00:42:35,730 --> 00:42:41,520
SolidFire and then I will rewind this so

00:42:38,430 --> 00:42:45,990
we don't need to spend 46 minutes for

00:42:41,520 --> 00:42:48,570
the demo of course so let me rewind this

00:42:45,990 --> 00:42:51,690
as the time goes by you can see the

00:42:48,570 --> 00:42:53,460
right ray from the two fresh with

00:42:51,690 --> 00:42:55,650
provision soil fire volumes which will

00:42:53,460 --> 00:42:58,380
be which are replacement for two NFS

00:42:55,650 --> 00:43:00,180
volumes if we rewind even more is

00:42:58,380 --> 00:43:02,790
they're basically written with 200

00:43:00,180 --> 00:43:04,530
megabytes and so on some Sun I will just

00:43:02,790 --> 00:43:07,380
the I pause this video many times

00:43:04,530 --> 00:43:09,810
obviously because it took 120 avoir 1.5

00:43:07,380 --> 00:43:11,670
hours they should do something the whole

00:43:09,810 --> 00:43:14,340
of migration process is still going on

00:43:11,670 --> 00:43:16,520
in the slides I already wrote

00:43:14,340 --> 00:43:20,100
interesting times and I happen to

00:43:16,520 --> 00:43:23,820
obviously know it's on 27 minutes and so

00:43:20,100 --> 00:43:26,730
let's rewind 27 minutes and 45 seconds

00:43:23,820 --> 00:43:29,030
or so you can observe the volume copy

00:43:26,730 --> 00:43:32,670
progress which goes up and up slowly

00:43:29,030 --> 00:43:37,320
very slowly actually but she does and

00:43:32,670 --> 00:43:44,100
let me just find this oops I did it so

00:43:37,320 --> 00:43:47,250
much so 99 percent of the volume some

00:43:44,100 --> 00:43:48,840
mirrors not finished yet and any kind of

00:43:47,250 --> 00:43:51,420
additional changes are still being

00:43:48,840 --> 00:43:54,390
dynamically mirror to the destination at

00:43:51,420 --> 00:43:56,370
some point about just now the KVM will

00:43:54,390 --> 00:43:58,830
decide okay I have mirrored enough of

00:43:56,370 --> 00:44:01,020
storage excellent let's now kick in the

00:43:58,830 --> 00:44:02,910
live migration and basically this is

00:44:01,020 --> 00:44:05,820
where the content of the memories being

00:44:02,910 --> 00:44:08,640
copied over to the destination course

00:44:05,820 --> 00:44:11,610
you can see bandwidth rate of almost

00:44:08,640 --> 00:44:14,130
kalfa gigabyte per second this is our 10

00:44:11,610 --> 00:44:16,740
T Network you can see iterations which

00:44:14,130 --> 00:44:19,560
is uh it's important interesting way

00:44:16,740 --> 00:44:21,300
this first migration after all 59

00:44:19,560 --> 00:44:24,630
gigabytes are migrated as you can see

00:44:21,300 --> 00:44:27,720
data processed and data remaining after

00:44:24,630 --> 00:44:30,210
it migrates all the 59 gigabytes the

00:44:27,720 --> 00:44:32,330
camera will say okay I migrated colder

00:44:30,210 --> 00:44:35,340
memory let's check what has changed

00:44:32,330 --> 00:44:38,010
since I hope with this ol 59 megabyte in

00:44:35,340 --> 00:44:39,990
it will say aw crap somebody stressing

00:44:38,010 --> 00:44:43,500
the VM everything changed I need to copy

00:44:39,990 --> 00:44:45,630
again all 59 gigabytes and then

00:44:43,500 --> 00:44:47,730
basically this is where kayvyun will

00:44:45,630 --> 00:44:50,570
start intelligence is thinking like me

00:44:47,730 --> 00:44:53,280
throttle the CPU more and more and more

00:44:50,570 --> 00:44:54,480
you can actually see after this is kind

00:44:53,280 --> 00:44:56,610
of finished and goes into second

00:44:54,480 --> 00:44:59,070
iteration you can see directly rate the

00:44:56,610 --> 00:45:01,590
amount of memory pages of second which

00:44:59,070 --> 00:45:03,810
is calculated on in a second run from

00:45:01,590 --> 00:45:05,790
the first run not so much important but

00:45:03,810 --> 00:45:09,060
you knew you put up

00:45:05,790 --> 00:45:10,230
going down so if you remind remind this

00:45:09,060 --> 00:45:14,100
a lot

00:45:10,230 --> 00:45:17,040
you see it's now in third iteration we

00:45:14,100 --> 00:45:19,290
see now it's the 11th iteration you see

00:45:17,040 --> 00:45:22,350
now it's diagonal in whatever iteration

00:45:19,290 --> 00:45:24,420
in 13 so it feels like 13 times that

00:45:22,350 --> 00:45:26,670
Khemu is every time it's rock next

00:45:24,420 --> 00:45:29,010
appears more and more and more up to the

00:45:26,670 --> 00:45:34,200
point cameraman down down down and

00:45:29,010 --> 00:45:36,330
basically very soon it will I believe it

00:45:34,200 --> 00:45:40,110
took around well that's not what I

00:45:36,330 --> 00:45:42,360
wanted you do but here we go

00:45:40,110 --> 00:45:45,630
I basically took around 20 something

00:45:42,360 --> 00:45:48,660
iterations to really completely throw Co

00:45:45,630 --> 00:45:50,040
enough to CPUs and lower the the page

00:45:48,660 --> 00:45:53,310
rate which is kind of not very

00:45:50,040 --> 00:45:55,050
indicative if you see but you can see

00:45:53,310 --> 00:45:57,810
actually totally move kind of six

00:45:55,050 --> 00:45:59,310
hundred gigabytes of data of RAM memory

00:45:57,810 --> 00:46:01,560
content was moved although there are

00:45:59,310 --> 00:46:04,010
memories only 60 gigabytes so it's

00:46:01,560 --> 00:46:09,030
repeating this again and again and again

00:46:04,010 --> 00:46:11,850
and then at some point at some point we

00:46:09,030 --> 00:46:14,340
can only it's finished yeah it's

00:46:11,850 --> 00:46:17,250
completed and second laters if the the

00:46:14,340 --> 00:46:19,590
command will say no such VM because the

00:46:17,250 --> 00:46:21,450
source VM has been the source field has

00:46:19,590 --> 00:46:23,640
been migrated it's destroyed the source

00:46:21,450 --> 00:46:26,760
host is again running on a destination

00:46:23,640 --> 00:46:29,670
host which I kind of failed to prove or

00:46:26,760 --> 00:46:34,140
to show you but let's pretty much it for

00:46:29,670 --> 00:46:37,950
for the demo here you actually have a

00:46:34,140 --> 00:46:39,810
couple of interesting links which is our

00:46:37,950 --> 00:46:42,900
own let's say commits argued aware

00:46:39,810 --> 00:46:44,850
English which I mentioned widows fix for

00:46:42,900 --> 00:46:46,350
others Java Isaac to in sequence where

00:46:44,850 --> 00:46:49,110
you cannot start the VM because

00:46:46,350 --> 00:46:51,810
naturally running also have a few of

00:46:49,110 --> 00:46:54,030
their own links to the demos these are

00:46:51,810 --> 00:46:55,890
at the moment on my personal account but

00:46:54,030 --> 00:46:58,020
they will be moved to a company yet

00:46:55,890 --> 00:47:01,200
account soon I will try to make sure

00:46:58,020 --> 00:47:02,880
these are valid for some time and then

00:47:01,200 --> 00:47:05,280
you have other couple of other links

00:47:02,880 --> 00:47:07,080
very excellent videos from my copies of

00:47:05,280 --> 00:47:10,230
different stuff related to soil foreign

00:47:07,080 --> 00:47:12,480
cloud stack a few of immigration

00:47:10,230 --> 00:47:14,970
geek low level details also what to

00:47:12,480 --> 00:47:16,170
expect with self from ear artists they

00:47:14,970 --> 00:47:18,750
actually know what they are doing with

00:47:16,170 --> 00:47:20,369
self and also some other self

00:47:18,750 --> 00:47:22,260
architecture for example from record if

00:47:20,369 --> 00:47:24,450
you want and so on and we have a small

00:47:22,260 --> 00:47:26,790
like I said kind of a bonus which is

00:47:24,450 --> 00:47:28,500
down when migrations fit from here we

00:47:26,790 --> 00:47:30,720
here to migrate a bunch of VMs from

00:47:28,500 --> 00:47:32,850
manifest itself to storage fires the

00:47:30,720 --> 00:47:34,920
basically just fit a simple JSON file to

00:47:32,850 --> 00:47:37,680
the genitive which presumes the script

00:47:34,920 --> 00:47:40,020
it will it just takes the idea of the VM

00:47:37,680 --> 00:47:43,230
it will dynamically find out in a

00:47:40,020 --> 00:47:45,480
certain way if volumes are existing what

00:47:43,230 --> 00:47:47,310
volumes are not on voice fire and it

00:47:45,480 --> 00:47:49,770
will generate the proper APA coal and

00:47:47,310 --> 00:47:52,050
shoot it so the management server and it

00:47:49,770 --> 00:47:53,340
will basically migrate the VM you do

00:47:52,050 --> 00:47:55,800
need to pay attention if you are using

00:47:53,340 --> 00:47:57,990
different tags on the disk offerings

00:47:55,800 --> 00:47:59,609
after you migrate you might be like we

00:47:57,990 --> 00:48:02,160
here because we have different storage

00:47:59,609 --> 00:48:04,170
tags on second NFS deprecated and you

00:48:02,160 --> 00:48:07,140
can solid pirate are gonna soy fire

00:48:04,170 --> 00:48:08,460
stuff so depending on your usage of tags

00:48:07,140 --> 00:48:11,160
and storage and offerings you might be

00:48:08,460 --> 00:48:13,710
to ask for the client please stop the VM

00:48:11,160 --> 00:48:15,960
change the offering to us the current so

00:48:13,710 --> 00:48:17,970
I won't start the VM so that's like you

00:48:15,960 --> 00:48:21,240
know 30 seconds of or one minute let's

00:48:17,970 --> 00:48:24,000
say of downtime so that would be whoa I

00:48:21,240 --> 00:48:27,619
guess if you have any questions I'm

00:48:24,000 --> 00:48:27,619
happy to answer

00:48:36,500 --> 00:48:43,069
Mike I believe the Kiva's if domicile

00:48:40,819 --> 00:48:46,210
immigration is working for NFS tooth

00:48:43,069 --> 00:48:46,210
self or the other way

00:48:57,840 --> 00:49:00,350
and

00:49:43,360 --> 00:49:47,980
yeah yeah but it's a great improvement

00:49:46,330 --> 00:49:49,990
there is no actually magic there it's

00:49:47,980 --> 00:49:51,820
just being missing in thumbstick world

00:49:49,990 --> 00:49:54,460
because this is you can do this manually

00:49:51,820 --> 00:49:57,280
by which mine online magnet whatever the

00:49:54,460 --> 00:49:59,410
command - - of your storage that's the

00:49:57,280 --> 00:50:01,720
basic command then you define the remote

00:49:59,410 --> 00:50:05,350
host and so on if you in you feed the

00:50:01,720 --> 00:50:08,050
XML we just changed volume references it

00:50:05,350 --> 00:50:10,420
doesn't reference NFS the to reference

00:50:08,050 --> 00:50:12,130
the SolidFire or it reference cell for

00:50:10,420 --> 00:50:16,180
example so you could manually just

00:50:12,130 --> 00:50:17,500
issued a migration job would say and

00:50:16,180 --> 00:50:19,900
this has been our very nicely

00:50:17,500 --> 00:50:23,110
implemented by Mike I believe that there

00:50:19,900 --> 00:50:24,490
was in certain cases we had this kind of

00:50:23,110 --> 00:50:26,710
scenario because of tablets and

00:50:24,490 --> 00:50:29,830
everything there is certain cases where

00:50:26,710 --> 00:50:33,580
the migration from self to soil fire

00:50:29,830 --> 00:50:36,250
will maybe not work again if you're

00:50:33,580 --> 00:50:39,760
using tags in a certain way and this is

00:50:36,250 --> 00:50:44,050
from core 11 0 but it has been fixed in

00:50:39,760 --> 00:50:46,930
for 11 1 or 2 so it hasn't fixed a now

00:50:44,050 --> 00:50:49,860
in the most recent release a public

00:50:46,930 --> 00:50:49,860
release speak

00:50:53,720 --> 00:51:04,429
that's one

00:50:56,750 --> 00:51:04,429

YouTube URL: https://www.youtube.com/watch?v=9-jEhSGxp-M


