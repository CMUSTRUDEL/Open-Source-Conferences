Title: Beam day on Google Open Source Live | Dataflow Templates
Publication date: 2021-05-06
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:00:00,790 --> 00:00:08,360
[Music]

00:00:10,000 --> 00:00:14,960
welcome to dataflow templates

00:00:12,719 --> 00:00:17,199
today i'm going to share an example use

00:00:14,960 --> 00:00:20,160
case for data flow templates

00:00:17,199 --> 00:00:21,039
how to create and execute data flow flex

00:00:20,160 --> 00:00:23,439
templates

00:00:21,039 --> 00:00:24,160
and how to get started contributing

00:00:23,439 --> 00:00:27,840
templates

00:00:24,160 --> 00:00:28,640
to open source community my name is alex

00:00:27,840 --> 00:00:31,760
casalabo

00:00:28,640 --> 00:00:33,440
i'm program manager at aqualon a custom

00:00:31,760 --> 00:00:35,760
software engineering company

00:00:33,440 --> 00:00:37,280
google cloud partner headquartered in

00:00:35,760 --> 00:00:39,520
seattle washington

00:00:37,280 --> 00:00:40,800
with engineering offices in europe and

00:00:39,520 --> 00:00:43,200
north america

00:00:40,800 --> 00:00:44,800
over the last 20 years we helped over

00:00:43,200 --> 00:00:48,399
100 clients to build

00:00:44,800 --> 00:00:52,239
software data analytics machine learning

00:00:48,399 --> 00:00:54,960
enterprise and mobile apps

00:00:52,239 --> 00:00:57,520
i will share an example use case where

00:00:54,960 --> 00:00:58,800
data flow was a great solution for data

00:00:57,520 --> 00:01:02,320
transformation

00:00:58,800 --> 00:01:05,360
and custom template helped to integrate

00:01:02,320 --> 00:01:06,799
external data protection service into

00:01:05,360 --> 00:01:09,439
pipeline

00:01:06,799 --> 00:01:10,799
a customer use case was to ingest large

00:01:09,439 --> 00:01:13,200
volumes of data

00:01:10,799 --> 00:01:15,119
that included sensitive data like credit

00:01:13,200 --> 00:01:17,360
card or pii

00:01:15,119 --> 00:01:20,320
and a third-party data protection

00:01:17,360 --> 00:01:22,720
service was used for tokenization

00:01:20,320 --> 00:01:23,840
converting data elements like credit

00:01:22,720 --> 00:01:28,080
card number

00:01:23,840 --> 00:01:30,720
into non-sensitive token representations

00:01:28,080 --> 00:01:31,520
that can be stored safely based on

00:01:30,720 --> 00:01:34,000
access role

00:01:31,520 --> 00:01:34,799
a user may see a token representation of

00:01:34,000 --> 00:01:37,360
data

00:01:34,799 --> 00:01:38,240
a partially masked data like last four

00:01:37,360 --> 00:01:41,520
digits of

00:01:38,240 --> 00:01:42,079
credit card number or an original credit

00:01:41,520 --> 00:01:45,360
card

00:01:42,079 --> 00:01:47,920
number data flow provides an

00:01:45,360 --> 00:01:50,479
auto scaling fully managed high

00:01:47,920 --> 00:01:52,880
performance data transformation service

00:01:50,479 --> 00:01:55,600
with consumption based pricing that was

00:01:52,880 --> 00:01:56,799
perfect fit for data processing

00:01:55,600 --> 00:01:59,520
challenge was that

00:01:56,799 --> 00:02:02,159
out of box there was no support for

00:01:59,520 --> 00:02:06,000
external calls to third-party

00:02:02,159 --> 00:02:09,119
service to tokenize data

00:02:06,000 --> 00:02:12,239
a custom pipeline has to be built

00:02:09,119 --> 00:02:15,120
to support this and

00:02:12,239 --> 00:02:16,480
different input and output options need

00:02:15,120 --> 00:02:18,959
to be supported

00:02:16,480 --> 00:02:20,640
on google cloud storage we have json

00:02:18,959 --> 00:02:24,080
csvs avro

00:02:20,640 --> 00:02:26,640
we have data from pubsub and also

00:02:24,080 --> 00:02:27,120
pipeline had to support different output

00:02:26,640 --> 00:02:30,239
things

00:02:27,120 --> 00:02:33,840
bigquery bigtable gcs

00:02:30,239 --> 00:02:34,319
so this variety of options on the i o

00:02:33,840 --> 00:02:37,840
side

00:02:34,319 --> 00:02:39,200
that required a same pipeline to do same

00:02:37,840 --> 00:02:41,200
processing

00:02:39,200 --> 00:02:43,440
templatizing pipeline allowed to

00:02:41,200 --> 00:02:46,160
implement pipeline just once

00:02:43,440 --> 00:02:48,720
and support all required i o options

00:02:46,160 --> 00:02:51,360
using template parameters

00:02:48,720 --> 00:02:52,800
to implement tokenization we created

00:02:51,360 --> 00:02:55,360
data flow pipeline

00:02:52,800 --> 00:02:56,400
with processing logic to call external

00:02:55,360 --> 00:02:59,040
rest service

00:02:56,400 --> 00:02:59,680
we had several challenges to solve uh

00:02:59,040 --> 00:03:01,760
including

00:02:59,680 --> 00:03:04,000
optimizing communication with rest

00:03:01,760 --> 00:03:06,879
service for performance

00:03:04,000 --> 00:03:07,519
stateful do fun bim feature helped us to

00:03:06,879 --> 00:03:10,480
buffer

00:03:07,519 --> 00:03:12,239
and batch data into bundles that worker

00:03:10,480 --> 00:03:14,400
can send in one request

00:03:12,239 --> 00:03:16,720
and even maintain order enough data in

00:03:14,400 --> 00:03:19,920
this patches

00:03:16,720 --> 00:03:22,400
then we packaged pipeline into template

00:03:19,920 --> 00:03:22,959
and that enabled us to support different

00:03:22,400 --> 00:03:26,159
input

00:03:22,959 --> 00:03:29,599
output sources execution parameters and

00:03:26,159 --> 00:03:31,519
shared template easily next we will look

00:03:29,599 --> 00:03:34,799
how to get from a pipeline

00:03:31,519 --> 00:03:37,440
to data flow template

00:03:34,799 --> 00:03:39,280
what is a pipeline a pipeline is a flow

00:03:37,440 --> 00:03:42,879
of operations in beam

00:03:39,280 --> 00:03:45,519
in simplest way it starts with io read

00:03:42,879 --> 00:03:46,319
data transformations are applied and

00:03:45,519 --> 00:03:50,319
finally

00:03:46,319 --> 00:03:54,319
io write transform is applied to output

00:03:50,319 --> 00:03:57,439
data into output sync

00:03:54,319 --> 00:04:01,519
data flow templates is a way to package

00:03:57,439 --> 00:04:04,319
and stage pipelines it has two phases

00:04:01,519 --> 00:04:05,280
template construction is implementing

00:04:04,319 --> 00:04:07,840
pipeline

00:04:05,280 --> 00:04:08,560
compiling it into execution graph and

00:04:07,840 --> 00:04:11,920
staging

00:04:08,560 --> 00:04:14,640
on google cloud execution phase

00:04:11,920 --> 00:04:16,000
allows to run templates using google

00:04:14,640 --> 00:04:19,199
cloud console

00:04:16,000 --> 00:04:21,759
gcloud tool or rest api without

00:04:19,199 --> 00:04:22,800
development environment without

00:04:21,759 --> 00:04:26,400
configuring

00:04:22,800 --> 00:04:29,280
associated dependencies on your computer

00:04:26,400 --> 00:04:30,320
runtime parameter allows to customize

00:04:29,280 --> 00:04:33,360
execution

00:04:30,320 --> 00:04:36,000
of the pipeline running your pipeline

00:04:33,360 --> 00:04:38,160
does not require you to recompile code

00:04:36,000 --> 00:04:40,560
every time

00:04:38,160 --> 00:04:42,560
there are two template types classic

00:04:40,560 --> 00:04:43,040
templates are staged as permanently

00:04:42,560 --> 00:04:46,160
fixed

00:04:43,040 --> 00:04:48,960
execution graphs on google cloud storage

00:04:46,160 --> 00:04:50,400
classic templates require bim pipeline

00:04:48,960 --> 00:04:53,199
code modification to

00:04:50,400 --> 00:04:54,160
implement value provider interface to

00:04:53,199 --> 00:04:56,479
defer

00:04:54,160 --> 00:04:57,280
reading of variables until template is

00:04:56,479 --> 00:05:00,160
run

00:04:57,280 --> 00:05:01,440
for example name of pub sub subscription

00:05:00,160 --> 00:05:04,960
that user might want to

00:05:01,440 --> 00:05:05,360
pass at execution a minor changing

00:05:04,960 --> 00:05:07,600
option

00:05:05,360 --> 00:05:08,639
will require recompiling template by

00:05:07,600 --> 00:05:10,960
developers

00:05:08,639 --> 00:05:13,280
for example to change to google cloud

00:05:10,960 --> 00:05:16,560
storage instead of pub sub

00:05:13,280 --> 00:05:19,759
flex templates package an existing beam

00:05:16,560 --> 00:05:23,280
pipeline as a docker image on your

00:05:19,759 --> 00:05:26,400
project's container registry and create

00:05:23,280 --> 00:05:29,520
template specification metadata on

00:05:26,400 --> 00:05:30,720
google cloud storage bucket to execute

00:05:29,520 --> 00:05:32,960
flex template

00:05:30,720 --> 00:05:34,560
then just refer to the template spec

00:05:32,960 --> 00:05:37,199
file

00:05:34,560 --> 00:05:39,039
benefits of flex templates are dynamic

00:05:37,199 --> 00:05:42,080
execution graph

00:05:39,039 --> 00:05:45,120
that is constructed and validated at

00:05:42,080 --> 00:05:48,240
job launch time based on final

00:05:45,120 --> 00:05:51,440
parameters that user supplied

00:05:48,240 --> 00:05:52,080
flex templates eliminated need for value

00:05:51,440 --> 00:05:57,280
provider

00:05:52,080 --> 00:05:57,280
interface improving developer experience

00:05:57,680 --> 00:06:01,600
creating data flow flex template has

00:06:00,400 --> 00:06:05,199
three steps

00:06:01,600 --> 00:06:08,160
implement data pipeline create metadata

00:06:05,199 --> 00:06:09,199
and build the template pipeline for flex

00:06:08,160 --> 00:06:12,960
templates

00:06:09,199 --> 00:06:16,160
can be implemented in java or python

00:06:12,960 --> 00:06:20,160
i'm going to switch to my demo

00:06:16,160 --> 00:06:23,199
example that shows our tokenization

00:06:20,160 --> 00:06:26,080
pipeline tokenization pipeline was

00:06:23,199 --> 00:06:28,800
implemented in java

00:06:26,080 --> 00:06:30,000
beam pipeline in java has main method as

00:06:28,800 --> 00:06:33,360
an entry point

00:06:30,000 --> 00:06:34,000
for pipeline execution that often

00:06:33,360 --> 00:06:37,199
processes

00:06:34,000 --> 00:06:40,479
options and calls pipeline

00:06:37,199 --> 00:06:42,960
run method that implements

00:06:40,479 --> 00:06:44,720
transforms and other business logic for

00:06:42,960 --> 00:06:49,199
the pipeline

00:06:44,720 --> 00:06:52,000
next bim pipeline is compiled

00:06:49,199 --> 00:06:53,120
into uber file that contains all

00:06:52,000 --> 00:06:57,199
dependencies

00:06:53,120 --> 00:07:00,960
using maven or cradle and

00:06:57,199 --> 00:07:04,000
next step to get from pipeline

00:07:00,960 --> 00:07:06,880
to template is using

00:07:04,000 --> 00:07:07,840
gcloud dataflow flex template build

00:07:06,880 --> 00:07:10,560
command

00:07:07,840 --> 00:07:11,440
that will package that will create

00:07:10,560 --> 00:07:14,720
docker

00:07:11,440 --> 00:07:18,479
container metadata file and

00:07:14,720 --> 00:07:20,479
stage them in google cloud

00:07:18,479 --> 00:07:22,160
i'm going to switch to my demo

00:07:20,479 --> 00:07:25,840
environment and

00:07:22,160 --> 00:07:26,880
start this process here i'm going to

00:07:25,840 --> 00:07:31,840
start

00:07:26,880 --> 00:07:35,120
maven maven compile process

00:07:31,840 --> 00:07:38,319
and just to verify that it started

00:07:35,120 --> 00:07:42,240
it's going to take a little while to

00:07:38,319 --> 00:07:46,960
complete and while it's compiling

00:07:42,240 --> 00:07:51,759
i'm going to switch to metadata file

00:07:46,960 --> 00:07:51,759
and provide an overview what it is

00:07:52,400 --> 00:07:55,759
template metadata file provides template

00:07:55,199 --> 00:07:59,520
name

00:07:55,759 --> 00:08:02,879
description and parameters

00:07:59,520 --> 00:08:06,000
that user can specify when creating

00:08:02,879 --> 00:08:09,120
job from template in web console

00:08:06,000 --> 00:08:12,240
in gcloud or rest api

00:08:09,120 --> 00:08:16,160
template parameters i have name

00:08:12,240 --> 00:08:20,879
label help text ram type properties

00:08:16,160 --> 00:08:23,599
and also can be optional or required

00:08:20,879 --> 00:08:25,759
and support regular expression

00:08:23,599 --> 00:08:29,680
validation of user input

00:08:25,759 --> 00:08:33,440
that comes really user friendly when

00:08:29,680 --> 00:08:37,039
job is created in web console

00:08:33,440 --> 00:08:40,719
maven build just completed

00:08:37,039 --> 00:08:45,279
and we can switch to the second

00:08:40,719 --> 00:08:45,279
step of the template build

00:08:48,080 --> 00:08:51,120
the second step is running gcloud

00:08:50,320 --> 00:08:54,959
dataflow

00:08:51,120 --> 00:08:58,240
flex template build i will start it

00:08:54,959 --> 00:09:00,880
right away and will explain what i have

00:08:58,240 --> 00:09:00,880
on the screen

00:09:01,120 --> 00:09:07,519
i use environment variables to help

00:09:04,160 --> 00:09:08,800
organize organize my commands and

00:09:07,519 --> 00:09:12,399
scripts

00:09:08,800 --> 00:09:13,440
so i have project id storage and

00:09:12,399 --> 00:09:17,680
template

00:09:13,440 --> 00:09:21,440
path google container registry

00:09:17,680 --> 00:09:24,000
image path and base

00:09:21,440 --> 00:09:26,240
container image as an environment

00:09:24,000 --> 00:09:28,800
variable

00:09:26,240 --> 00:09:29,839
when i start the flex template build

00:09:28,800 --> 00:09:32,959
command

00:09:29,839 --> 00:09:36,800
i provide template path

00:09:32,959 --> 00:09:40,240
where metadata will be stored

00:09:36,800 --> 00:09:43,920
i specify container image path and

00:09:40,240 --> 00:09:47,839
sdk language that that bim pipeline

00:09:43,920 --> 00:09:51,040
is using i also specify

00:09:47,839 --> 00:09:52,959
base container image one of the google

00:09:51,040 --> 00:09:57,440
provided

00:09:52,959 --> 00:10:00,640
images for data flow containers

00:09:57,440 --> 00:10:04,160
and lastly i specify some source

00:10:00,640 --> 00:10:05,279
files source for metadata source for

00:10:04,160 --> 00:10:08,399
uber

00:10:05,279 --> 00:10:09,920
jar file that we just completed in

00:10:08,399 --> 00:10:13,600
previous step

00:10:09,920 --> 00:10:14,320
and specify main class that will be

00:10:13,600 --> 00:10:17,360
called

00:10:14,320 --> 00:10:19,839
when dataflow creates job from this

00:10:17,360 --> 00:10:19,839
template

00:10:22,079 --> 00:10:29,120
i'm going to pause here

00:10:25,279 --> 00:10:29,120
for this command to complete

00:10:31,360 --> 00:10:38,240
flex template build just completed

00:10:34,720 --> 00:10:41,440
i'm going to navigate to web console

00:10:38,240 --> 00:10:44,880
to verify container registry

00:10:41,440 --> 00:10:44,880
and metadata file

00:10:45,680 --> 00:10:52,480
in my projects container registry

00:10:48,720 --> 00:10:55,680
i just go inside of the path

00:10:52,480 --> 00:11:01,360
and the

00:10:55,680 --> 00:11:05,440
image has just been uploaded

00:11:01,360 --> 00:11:10,399
next i'm switching to template path

00:11:05,440 --> 00:11:13,760
that was used for metadata file

00:11:10,399 --> 00:11:17,279
and validating that tokenization demo

00:11:13,760 --> 00:11:20,320
json metadata file has been created

00:11:17,279 --> 00:11:24,000
and completed we can go in and

00:11:20,320 --> 00:11:28,079
actually check the json it has

00:11:24,000 --> 00:11:31,279
a reference to docker image metadata

00:11:28,079 --> 00:11:34,959
and down at the bottom it has

00:11:31,279 --> 00:11:37,920
sdk info that specifies

00:11:34,959 --> 00:11:37,920
template language

00:11:38,560 --> 00:11:43,040
we are ready to run the template in data

00:11:41,360 --> 00:11:46,720
flow

00:11:43,040 --> 00:11:51,120
i'm going to navigate to data flow and

00:11:46,720 --> 00:11:51,120
here create job from template

00:11:55,600 --> 00:11:59,519
in this drop down i can select different

00:11:58,639 --> 00:12:01,600
templates

00:11:59,519 --> 00:12:03,440
and there is many templates that come

00:12:01,600 --> 00:12:07,279
from google

00:12:03,440 --> 00:12:10,639
with data flow for this project

00:12:07,279 --> 00:12:11,120
we created custom template and i will

00:12:10,639 --> 00:12:15,200
select

00:12:11,120 --> 00:12:17,680
custom template and navigate to cloud

00:12:15,200 --> 00:12:17,680
storage

00:12:20,079 --> 00:12:25,839
to select template metadata file

00:12:28,240 --> 00:12:32,399
you notice that description of the

00:12:30,560 --> 00:12:35,120
template

00:12:32,399 --> 00:12:37,120
that you can see here just changed to

00:12:35,120 --> 00:12:40,959
the template we created

00:12:37,120 --> 00:12:44,320
and now i will provide

00:12:40,959 --> 00:12:45,680
required parameter with json schema with

00:12:44,320 --> 00:12:48,959
data schema

00:12:45,680 --> 00:12:48,959
for my input

00:12:49,040 --> 00:12:52,240
and expand optional parameters to

00:12:51,440 --> 00:12:54,399
specify

00:12:52,240 --> 00:12:56,720
other templates that i will need for the

00:12:54,399 --> 00:12:56,720
demo

00:12:58,240 --> 00:13:04,560
path to input data

00:13:02,000 --> 00:13:05,680
format of the input files and i'm going

00:13:04,560 --> 00:13:09,040
to put

00:13:05,680 --> 00:13:13,440
jason with a typo

00:13:09,040 --> 00:13:17,120
just to demonstrate how validation works

00:13:13,440 --> 00:13:20,880
uh it allows me to easily notice a typo

00:13:17,120 --> 00:13:24,000
go back and fix it

00:13:20,880 --> 00:13:26,560
and continue with other parameters for

00:13:24,000 --> 00:13:26,560
my demo

00:13:27,760 --> 00:13:36,240
press endpoint for tokenization

00:13:31,600 --> 00:13:39,600
mock service that we use and

00:13:36,240 --> 00:13:42,720
next parameter is payload configuration

00:13:39,600 --> 00:13:43,360
that defines which fields which data

00:13:42,720 --> 00:13:46,399
elements

00:13:43,360 --> 00:13:49,519
i want to tokenize template

00:13:46,399 --> 00:13:52,880
also supports

00:13:49,519 --> 00:13:53,440
that letter path that we can specify to

00:13:52,880 --> 00:13:57,920
store

00:13:53,440 --> 00:14:00,839
any any errors

00:13:57,920 --> 00:14:03,839
in data processing on google cloud

00:14:00,839 --> 00:14:03,839
storage

00:14:04,079 --> 00:14:10,800
and lastly i will use

00:14:07,760 --> 00:14:14,480
bigquery table as my output

00:14:10,800 --> 00:14:18,000
and will specify table

00:14:14,480 --> 00:14:21,120
here as a parameter when we scroll

00:14:18,000 --> 00:14:24,160
uh scroll down we see all the

00:14:21,120 --> 00:14:25,360
uh all the data flow parameters that all

00:14:24,160 --> 00:14:29,519
data flow

00:14:25,360 --> 00:14:32,399
jobs support we can also specify

00:14:29,519 --> 00:14:34,560
any of those parameters with a custom

00:14:32,399 --> 00:14:37,360
template

00:14:34,560 --> 00:14:38,240
i'm going to stick with defaults and now

00:14:37,360 --> 00:14:41,519
we'll click

00:14:38,240 --> 00:14:47,040
run the job to create

00:14:41,519 --> 00:14:47,040
dataflow job from this custom template

00:14:47,839 --> 00:14:51,199
it will take a moment to analyze the job

00:14:50,480 --> 00:14:54,079
and create

00:14:51,199 --> 00:14:56,079
execution graph next the job will be

00:14:54,079 --> 00:14:58,320
staged and executed

00:14:56,079 --> 00:14:59,680
it might take few minutes and i will

00:14:58,320 --> 00:15:02,160
demo result

00:14:59,680 --> 00:15:04,720
of running this job that i did before

00:15:02,160 --> 00:15:04,720
the demo

00:15:05,040 --> 00:15:12,240
when job is created from custom template

00:15:08,800 --> 00:15:15,199
job execution graph will be constructed

00:15:12,240 --> 00:15:15,600
based on data processing in the pipeline

00:15:15,199 --> 00:15:19,040
that

00:15:15,600 --> 00:15:22,240
was implemented uh you can see

00:15:19,040 --> 00:15:23,040
dsg tokenization is one of the one of

00:15:22,240 --> 00:15:26,720
the

00:15:23,040 --> 00:15:29,920
transforms here and also

00:15:26,720 --> 00:15:33,279
we're gonna write successful transforms

00:15:29,920 --> 00:15:35,759
into our output and we have

00:15:33,279 --> 00:15:36,320
error handling transforms that will

00:15:35,759 --> 00:15:40,720
catch

00:15:36,320 --> 00:15:43,279
any errors when the job is run

00:15:40,720 --> 00:15:45,680
the output will be stored in bigquery

00:15:43,279 --> 00:15:45,680
table

00:15:46,720 --> 00:15:56,639
switching to bigquery i used

00:15:52,320 --> 00:15:59,680
demo json gcs to bigquery as a table

00:15:56,639 --> 00:15:59,680
for output

00:16:01,040 --> 00:16:08,480
the table was created by dataflow job

00:16:05,440 --> 00:16:11,920
and table schema matches

00:16:08,480 --> 00:16:14,320
input schema of the input data

00:16:11,920 --> 00:16:17,920
when i click on preview i will see a

00:16:14,320 --> 00:16:21,199
snippet of the output data

00:16:17,920 --> 00:16:22,079
here you can see that ssn and last name

00:16:21,199 --> 00:16:25,759
columns

00:16:22,079 --> 00:16:29,040
have been tokenized and some randomly

00:16:25,759 --> 00:16:32,079
looking data is stored

00:16:29,040 --> 00:16:35,759
in the table

00:16:32,079 --> 00:16:38,320
this is exactly what we wanted to see as

00:16:35,759 --> 00:16:41,519
a result

00:16:38,320 --> 00:16:44,560
now we built andrea

00:16:41,519 --> 00:16:47,199
dataflow template you can build

00:16:44,560 --> 00:16:48,000
custom dataflowflex templates for your

00:16:47,199 --> 00:16:51,279
projects

00:16:48,000 --> 00:16:55,839
and easily share them with users

00:16:51,279 --> 00:16:58,160
to create data flow jobs that they need

00:16:55,839 --> 00:16:59,680
you might also be thinking how do i

00:16:58,160 --> 00:17:02,800
share template

00:16:59,680 --> 00:17:05,039
with open source community

00:17:02,800 --> 00:17:06,799
to share templates with community you

00:17:05,039 --> 00:17:08,559
might contribute them to data flow

00:17:06,799 --> 00:17:11,679
templates repo

00:17:08,559 --> 00:17:14,559
it's google cloud repository

00:17:11,679 --> 00:17:15,919
and templates from google and other open

00:17:14,559 --> 00:17:18,720
source contributors

00:17:15,919 --> 00:17:20,640
are stored there it's easy to get

00:17:18,720 --> 00:17:23,839
started with contributions

00:17:20,640 --> 00:17:27,280
fork repository to develop your template

00:17:23,839 --> 00:17:31,360
develop pipeline following style guides

00:17:27,280 --> 00:17:34,000
and best practices provided by google

00:17:31,360 --> 00:17:36,880
sign contributor license agreement and

00:17:34,000 --> 00:17:39,520
submit template pull request to merge it

00:17:36,880 --> 00:17:40,400
pull request will go through code review

00:17:39,520 --> 00:17:44,480
and will get

00:17:40,400 --> 00:17:47,280
get merged with data flow templates

00:17:44,480 --> 00:17:47,679
summarizing today's session q takeaways

00:17:47,280 --> 00:17:49,919
are

00:17:47,679 --> 00:17:51,840
data flow flex templates are

00:17:49,919 --> 00:17:54,720
containerized packages

00:17:51,840 --> 00:17:55,520
with all necessary environments to run

00:17:54,720 --> 00:17:58,240
data flow

00:17:55,520 --> 00:18:00,640
pipeline so users can create job from

00:17:58,240 --> 00:18:03,919
template in cloud console

00:18:00,640 --> 00:18:07,120
gcloud command line tool or rest api

00:18:03,919 --> 00:18:08,000
flex templates use dynamic execution

00:18:07,120 --> 00:18:11,520
graph

00:18:08,000 --> 00:18:14,240
creating and validating job

00:18:11,520 --> 00:18:15,200
execution graph with final user

00:18:14,240 --> 00:18:18,640
parameters

00:18:15,200 --> 00:18:19,840
when job is launched flex templates are

00:18:18,640 --> 00:18:23,440
more portable

00:18:19,840 --> 00:18:26,640
easier to support with input and output

00:18:23,440 --> 00:18:27,440
options and we also learned how to

00:18:26,640 --> 00:18:31,760
create

00:18:27,440 --> 00:18:34,320
build and run dataflow flex template

00:18:31,760 --> 00:18:35,039
thank you for joining dataflow template

00:18:34,320 --> 00:18:37,200
session

00:18:35,039 --> 00:18:39,200
reach out to us at aqualon if you are

00:18:37,200 --> 00:18:41,840
interested in building google cloud

00:18:39,200 --> 00:18:56,720
solution with dataflow or apache beam

00:18:41,840 --> 00:18:56,720

YouTube URL: https://www.youtube.com/watch?v=aZOuLMwP04s


