Title: Beam day on Google Open Source Live | Full Event
Publication date: 2021-05-06
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:00:00,670 --> 00:00:07,670
[Music]

00:00:10,960 --> 00:00:13,519
good morning good afternoon and good

00:00:12,799 --> 00:00:15,280
evening

00:00:13,519 --> 00:00:16,960
thank you for joining apache beam day on

00:00:15,280 --> 00:00:18,880
google open source live

00:00:16,960 --> 00:00:20,560
this is a monthly series of sessions led

00:00:18,880 --> 00:00:21,359
by experts from google and community

00:00:20,560 --> 00:00:23,119
leaders

00:00:21,359 --> 00:00:24,880
we really appreciate everyone taking the

00:00:23,119 --> 00:00:26,560
time to come here digitally to learn

00:00:24,880 --> 00:00:28,160
more about apache beam

00:00:26,560 --> 00:00:29,760
my name is samuel rhode and i'm a

00:00:28,160 --> 00:00:30,720
software engineer working on google

00:00:29,760 --> 00:00:33,840
cloud dataflow

00:00:30,720 --> 00:00:34,559
in sunny seattle washington and i'm

00:00:33,840 --> 00:00:36,320
robert burke

00:00:34,559 --> 00:00:38,079
i also work at google as a software

00:00:36,320 --> 00:00:39,040
engineer working on the apache beam go

00:00:38,079 --> 00:00:41,600
sdk

00:00:39,040 --> 00:00:43,360
i'm located in rainey seattle washington

00:00:41,600 --> 00:00:44,800
and i'm excited for you to learn about

00:00:43,360 --> 00:00:46,719
apache beam

00:00:44,800 --> 00:00:48,640
for those of you new to apache beam it's

00:00:46,719 --> 00:00:50,800
a unified model to write both batch and

00:00:48,640 --> 00:00:52,399
streaming data processing pipelines

00:00:50,800 --> 00:00:54,239
this allows you to write portable

00:00:52,399 --> 00:00:55,600
pipelines once and then

00:00:54,239 --> 00:00:57,600
run in a number of different

00:00:55,600 --> 00:00:59,680
environments like cloud flow

00:00:57,600 --> 00:01:01,039
apache flink and spark just to name a

00:00:59,680 --> 00:01:02,160
few

00:01:01,039 --> 00:01:03,600
if you're looking to learn more about

00:01:02,160 --> 00:01:04,879
beam you're in the right place as we

00:01:03,600 --> 00:01:05,439
have some really interesting topics

00:01:04,879 --> 00:01:06,799
today

00:01:05,439 --> 00:01:08,720
we'll kick it off with the talk from

00:01:06,799 --> 00:01:10,320
inigo san jose walking us through the

00:01:08,720 --> 00:01:11,119
life of google technical solutions

00:01:10,320 --> 00:01:12,720
engineer

00:01:11,119 --> 00:01:14,640
and speaking about distributed data

00:01:12,720 --> 00:01:17,280
processing with apache beam

00:01:14,640 --> 00:01:19,600
this presentation is a it's a great

00:01:17,280 --> 00:01:20,640
overview and introduction to apache beam

00:01:19,600 --> 00:01:23,920
going into

00:01:20,640 --> 00:01:26,560
some specifics uh some generalizations

00:01:23,920 --> 00:01:28,080
and kind of a broad overview about how

00:01:26,560 --> 00:01:30,000
apache game works

00:01:28,080 --> 00:01:31,520
uh next is boey yuan talking about the

00:01:30,000 --> 00:01:33,360
new data source api

00:01:31,520 --> 00:01:34,960
that allows for more expressive batch

00:01:33,360 --> 00:01:36,960
and streaming reads

00:01:34,960 --> 00:01:38,640
before this we had something called the

00:01:36,960 --> 00:01:41,680
custom source api

00:01:38,640 --> 00:01:44,880
which was not very expressive and

00:01:41,680 --> 00:01:48,240
led to some very cluttered and

00:01:44,880 --> 00:01:50,240
just buggy sources that were in apache

00:01:48,240 --> 00:01:53,200
beam and so this new

00:01:50,240 --> 00:01:54,000
splittable do fun api will allow you to

00:01:53,200 --> 00:01:58,159
optimize your

00:01:54,000 --> 00:02:01,600
sources and write it in a more clean way

00:01:58,159 --> 00:02:03,520
then next is alex kosalapa from actvlon

00:02:01,600 --> 00:02:04,799
about how they use beam with dataflow

00:02:03,520 --> 00:02:06,640
templates

00:02:04,799 --> 00:02:08,399
finally we'll wrap it up with william

00:02:06,640 --> 00:02:09,440
sykes from wiseline describing building

00:02:08,399 --> 00:02:12,400
beam pipelines

00:02:09,440 --> 00:02:14,239
with cdc and museum before we hand

00:02:12,400 --> 00:02:16,560
things off to our first speaker though

00:02:14,239 --> 00:02:18,080
we have a couple of housekeeping items

00:02:16,560 --> 00:02:20,319
don't forget to put your questions in

00:02:18,080 --> 00:02:21,120
the live q a forum below the live stream

00:02:20,319 --> 00:02:22,560
window

00:02:21,120 --> 00:02:24,959
speakers are ready to answer your

00:02:22,560 --> 00:02:25,440
questions live if you're viewing in full

00:02:24,959 --> 00:02:27,120
screen

00:02:25,440 --> 00:02:29,360
you'll need to exit full screen to see

00:02:27,120 --> 00:02:31,040
the live q a forum

00:02:29,360 --> 00:02:33,120
our sessions have been pre-recorded to

00:02:31,040 --> 00:02:35,280
allow for accurate transcripts

00:02:33,120 --> 00:02:37,519
and that so speakers can focus on

00:02:35,280 --> 00:02:39,120
answering your questions live

00:02:37,519 --> 00:02:40,959
we would love to hear your thoughts on

00:02:39,120 --> 00:02:42,959
these sessions so feel free to make

00:02:40,959 --> 00:02:44,800
comments in the live forum

00:02:42,959 --> 00:02:47,120
once we're done don't forget to join us

00:02:44,800 --> 00:02:48,800
all at the after party on google meet

00:02:47,120 --> 00:02:50,400
we will share a link to google meet at

00:02:48,800 --> 00:02:53,120
the end of the last session

00:02:50,400 --> 00:02:53,760
oh and last but not least use the

00:02:53,120 --> 00:02:56,080
hashtag

00:02:53,760 --> 00:02:57,680
google os live to share your experience

00:02:56,080 --> 00:03:05,840
on social media

00:02:57,680 --> 00:03:05,840
and with that enjoy

00:03:42,000 --> 00:03:46,480
hi everyone i'm minion jose i'm a

00:03:44,400 --> 00:03:48,159
technical solutions engineer at google

00:03:46,480 --> 00:03:49,920
and i'm very very happy to be here with

00:03:48,159 --> 00:03:51,680
you kicking off bim day

00:03:49,920 --> 00:03:53,840
and of course i'm going to be talking

00:03:51,680 --> 00:03:54,720
about apache which i hope is not a

00:03:53,840 --> 00:03:57,760
surprise

00:03:54,720 --> 00:04:00,720
for any of you and the agenda for today

00:03:57,760 --> 00:04:01,519
is we're going to start talking about

00:04:00,720 --> 00:04:04,720
apache beam

00:04:01,519 --> 00:04:05,040
what it is and what is its model what's

00:04:04,720 --> 00:04:06,799
the

00:04:05,040 --> 00:04:09,519
business proposition that it has and

00:04:06,799 --> 00:04:12,400
we're going to end with one demo

00:04:09,519 --> 00:04:13,519
so let's start with what's apache beam

00:04:12,400 --> 00:04:15,680
so apache beam

00:04:13,519 --> 00:04:17,359
is an open source data parallel

00:04:15,680 --> 00:04:19,680
processing model that works

00:04:17,359 --> 00:04:20,400
both with batch and streaming and it can

00:04:19,680 --> 00:04:22,079
be used

00:04:20,400 --> 00:04:24,720
in different distributed processing

00:04:22,079 --> 00:04:26,479
backends and you might be asking

00:04:24,720 --> 00:04:28,560
what's the difference between apache

00:04:26,479 --> 00:04:32,320
beam and other data processing

00:04:28,560 --> 00:04:33,199
engines so here it comes the apache beam

00:04:32,320 --> 00:04:34,639
model

00:04:33,199 --> 00:04:36,639
there are three main things that make

00:04:34,639 --> 00:04:37,759
capacitive different the first one is

00:04:36,639 --> 00:04:39,759
that it's unified

00:04:37,759 --> 00:04:42,000
we have a single programming model for

00:04:39,759 --> 00:04:44,880
both but some streaming

00:04:42,000 --> 00:04:45,440
is portable so it can be executed in

00:04:44,880 --> 00:04:47,840
different

00:04:45,440 --> 00:04:49,120
execution environments and is extensible

00:04:47,840 --> 00:04:51,520
so we can write

00:04:49,120 --> 00:04:54,160
and share new sdks higher connectors and

00:04:51,520 --> 00:04:56,240
transformation libraries

00:04:54,160 --> 00:04:58,000
what do we exactly mean when we say that

00:04:56,240 --> 00:04:59,759
apache beam is unified

00:04:58,000 --> 00:05:01,120
this means that for both paths and

00:04:59,759 --> 00:05:03,840
streaming the sdk

00:05:01,120 --> 00:05:05,039
is the same so if we write something in

00:05:03,840 --> 00:05:05,759
parts and we want to move it to

00:05:05,039 --> 00:05:07,840
streaming

00:05:05,759 --> 00:05:09,600
we don't need to change the code we only

00:05:07,840 --> 00:05:10,479
need to add some standard logic that

00:05:09,600 --> 00:05:12,320
comes from

00:05:10,479 --> 00:05:13,759
from streaming when because we need to

00:05:12,320 --> 00:05:16,000
use unpointed sources

00:05:13,759 --> 00:05:17,120
this can be adding windows or splitting

00:05:16,000 --> 00:05:20,400
the boundary source

00:05:17,120 --> 00:05:22,080
in different chunks of bounded data so

00:05:20,400 --> 00:05:24,080
an example of this could be if we have

00:05:22,080 --> 00:05:26,880
some batch pipeline

00:05:24,080 --> 00:05:28,400
that uses a map or uses a flat map we

00:05:26,880 --> 00:05:30,560
don't really need to change anything

00:05:28,400 --> 00:05:33,520
from it to a streaming the same

00:05:30,560 --> 00:05:35,520
code will work from one to the other and

00:05:33,520 --> 00:05:38,080
the same with a group by if we're

00:05:35,520 --> 00:05:40,000
adding a group by from batch to

00:05:38,080 --> 00:05:42,960
streaming we only will need to add these

00:05:40,000 --> 00:05:46,000
windows that i mentioned before

00:05:42,960 --> 00:05:46,800
apache is portable so we have different

00:05:46,000 --> 00:05:48,720
sdks

00:05:46,800 --> 00:05:49,840
different languages that you can print

00:05:48,720 --> 00:05:51,600
your pipeline in

00:05:49,840 --> 00:05:53,039
and you can run this pipeline in

00:05:51,600 --> 00:05:55,120
different runners

00:05:53,039 --> 00:05:57,199
so for this case we have java we have

00:05:55,120 --> 00:05:59,440
python go and sql

00:05:57,199 --> 00:06:00,639
and for ones we have cloud data flow

00:05:59,440 --> 00:06:02,479
flying spark

00:06:00,639 --> 00:06:05,199
and we can even run it in our own

00:06:02,479 --> 00:06:08,240
computer via direct runner

00:06:05,199 --> 00:06:09,360
and the advantage of this is that you

00:06:08,240 --> 00:06:11,759
can use the same

00:06:09,360 --> 00:06:13,039
pipeline that you brought in java python

00:06:11,759 --> 00:06:14,479
whatever sdk

00:06:13,039 --> 00:06:16,000
and you can send it to different run it

00:06:14,479 --> 00:06:16,960
without changing pretty much anything

00:06:16,000 --> 00:06:19,360
from your code

00:06:16,960 --> 00:06:20,720
so you can also you only need to change

00:06:19,360 --> 00:06:23,280
the options

00:06:20,720 --> 00:06:23,840
that are used to to reach this execution

00:06:23,280 --> 00:06:25,520
engine

00:06:23,840 --> 00:06:27,759
for example if you're using cloud

00:06:25,520 --> 00:06:29,919
dataflow you need to add a project

00:06:27,759 --> 00:06:31,600
that is going to be the project in gcp

00:06:29,919 --> 00:06:33,360
that the python is going to be running

00:06:31,600 --> 00:06:35,120
but if you use direct runner you don't

00:06:33,360 --> 00:06:37,520
need to change anything

00:06:35,120 --> 00:06:38,800
with these options in this example we

00:06:37,520 --> 00:06:40,479
have a sampler key

00:06:38,800 --> 00:06:42,639
as you can see that the logic in java

00:06:40,479 --> 00:06:44,400
python ago is pretty much the same

00:06:42,639 --> 00:06:47,039
sql is of course a little bit different

00:06:44,400 --> 00:06:50,240
but this is expected

00:06:47,039 --> 00:06:53,280
so i want to explain you how

00:06:50,240 --> 00:06:54,560
this flexibility came and before doing

00:06:53,280 --> 00:06:56,479
that i want to start

00:06:54,560 --> 00:06:58,880
explaining a bit a bit of the apache in

00:06:56,479 --> 00:06:59,680
history at the very beginning there was

00:06:58,880 --> 00:07:02,240
only java

00:06:59,680 --> 00:07:03,680
and some runners and we need a way to

00:07:02,240 --> 00:07:05,599
translate from java

00:07:03,680 --> 00:07:07,840
to the other one so there was an

00:07:05,599 --> 00:07:10,800
specific way to translate from java

00:07:07,840 --> 00:07:13,759
to data flow another way from java to

00:07:10,800 --> 00:07:16,960
splint another way from java to spark

00:07:13,759 --> 00:07:18,800
and when new sdks came this approach

00:07:16,960 --> 00:07:20,800
wasn't the best of course because we

00:07:18,800 --> 00:07:23,120
will need a way from java

00:07:20,800 --> 00:07:24,960
to data flow another way from python to

00:07:23,120 --> 00:07:27,759
flink another way from

00:07:24,960 --> 00:07:28,720
python to spark and this doesn't scale

00:07:27,759 --> 00:07:31,680
in case that we

00:07:28,720 --> 00:07:33,039
wanted to add a new sdk we will need to

00:07:31,680 --> 00:07:35,759
create a lot of

00:07:33,039 --> 00:07:37,120
translations between the new sdk and the

00:07:35,759 --> 00:07:38,880
and all the runners that we have

00:07:37,120 --> 00:07:41,039
and the same in case we wanted to add a

00:07:38,880 --> 00:07:44,240
new one so the patching

00:07:41,039 --> 00:07:47,280
team came up with this idea so

00:07:44,240 --> 00:07:49,360
on the top we have the sdks java

00:07:47,280 --> 00:07:51,360
python and go and in the middle we have

00:07:49,360 --> 00:07:53,520
the runners and between those

00:07:51,360 --> 00:07:55,360
we have something called the runner api

00:07:53,520 --> 00:07:57,199
this runner api what it does is

00:07:55,360 --> 00:08:00,720
translates from java

00:07:57,199 --> 00:08:01,919
from python or from go to an abstraction

00:08:00,720 --> 00:08:04,879
of those languages

00:08:01,919 --> 00:08:05,680
which we actually use protobuf since it

00:08:04,879 --> 00:08:08,800
can be read

00:08:05,680 --> 00:08:11,039
from pretty much all the languages and

00:08:08,800 --> 00:08:13,840
this abstraction gets sent to

00:08:11,039 --> 00:08:16,000
2.1 which they read this pipeline and

00:08:13,840 --> 00:08:17,680
actually know what they have to do

00:08:16,000 --> 00:08:19,599
and between the runners and the power

00:08:17,680 --> 00:08:22,720
and execution we also add

00:08:19,599 --> 00:08:24,080
one layer this is the fun api this one

00:08:22,720 --> 00:08:26,960
api is also need

00:08:24,080 --> 00:08:28,800
needed because we need a standard

00:08:26,960 --> 00:08:32,959
interface to connect from

00:08:28,800 --> 00:08:34,880
our code to to our execution engine

00:08:32,959 --> 00:08:36,479
and we didn't want to do this for every

00:08:34,880 --> 00:08:39,919
single language for every single

00:08:36,479 --> 00:08:40,640
runner let me show you a bit how this

00:08:39,919 --> 00:08:43,440
works

00:08:40,640 --> 00:08:44,320
what's the schema of all these do we

00:08:43,440 --> 00:08:46,560
have in read

00:08:44,320 --> 00:08:47,360
everything that is language specific and

00:08:46,560 --> 00:08:49,600
in green

00:08:47,360 --> 00:08:51,200
everything that is language agnostic so

00:08:49,600 --> 00:08:54,080
we have our bin application

00:08:51,200 --> 00:08:55,680
in whatever is decay we want it and this

00:08:54,080 --> 00:08:58,880
big application gets sent

00:08:55,680 --> 00:09:01,440
to the job server via the job api then

00:08:58,880 --> 00:09:02,399
this gets sent to our runner using the

00:09:01,440 --> 00:09:04,480
runner api

00:09:02,399 --> 00:09:06,160
in this case we're using flick and flink

00:09:04,480 --> 00:09:08,959
as an example but of course

00:09:06,160 --> 00:09:10,160
the same will apply for data flow or for

00:09:08,959 --> 00:09:13,279
for spark

00:09:10,160 --> 00:09:15,600
flink our runner gets different task

00:09:13,279 --> 00:09:17,600
assign that it needs to execute in this

00:09:15,600 --> 00:09:20,000
case we have an executable states

00:09:17,600 --> 00:09:21,440
a group by key and another executable

00:09:20,000 --> 00:09:24,399
states

00:09:21,440 --> 00:09:26,480
these executable stages connect to an

00:09:24,399 --> 00:09:28,640
sdk harness via define api

00:09:26,480 --> 00:09:30,080
this sdk harness are the pieces of the

00:09:28,640 --> 00:09:33,360
runner that actually execute

00:09:30,080 --> 00:09:35,120
the user code and they tend to be

00:09:33,360 --> 00:09:37,279
docker containers but they don't

00:09:35,120 --> 00:09:39,040
actually have to be they can be

00:09:37,279 --> 00:09:40,640
embedded within the runner itself or

00:09:39,040 --> 00:09:43,920
they can be an extra process

00:09:40,640 --> 00:09:45,680
within within their runner and apart

00:09:43,920 --> 00:09:48,880
from the flexibility that using

00:09:45,680 --> 00:09:51,040
this fan api and this runner api

00:09:48,880 --> 00:09:52,320
pivots when it comes to what in new sdks

00:09:51,040 --> 00:09:55,839
or in new one is

00:09:52,320 --> 00:09:58,560
it also provides one very nice feature

00:09:55,839 --> 00:09:59,360
this feature is called cross language so

00:09:58,560 --> 00:10:01,839
let's say

00:09:59,360 --> 00:10:03,040
that we have our vm application written

00:10:01,839 --> 00:10:06,240
in python

00:10:03,040 --> 00:10:08,560
and we want to use kafkaio calcio is an

00:10:06,240 --> 00:10:09,360
io that has been around in java for a

00:10:08,560 --> 00:10:12,320
long time

00:10:09,360 --> 00:10:13,760
it's very well tested it's very mature

00:10:12,320 --> 00:10:16,959
but it's not

00:10:13,760 --> 00:10:20,160
so much in python so instead

00:10:16,959 --> 00:10:21,040
of having to write the kafka in python

00:10:20,160 --> 00:10:23,200
again

00:10:21,040 --> 00:10:25,519
we can use something called expansion

00:10:23,200 --> 00:10:26,160
service that connects these distributor

00:10:25,519 --> 00:10:30,560
states

00:10:26,160 --> 00:10:33,839
with with a java harness with a fan api

00:10:30,560 --> 00:10:35,279
so we will be able to execute this part

00:10:33,839 --> 00:10:37,440
of the pipeline

00:10:35,279 --> 00:10:39,839
with a java harness and the rest of the

00:10:37,440 --> 00:10:41,839
python can be executed with python

00:10:39,839 --> 00:10:43,760
we can see that the after the first

00:10:41,839 --> 00:10:46,320
exhibitor states with class io

00:10:43,760 --> 00:10:47,120
we have another exhibit executable state

00:10:46,320 --> 00:10:50,720
sorry

00:10:47,120 --> 00:10:51,600
that goes to the python sdk harness via

00:10:50,720 --> 00:10:54,000
the fan api

00:10:51,600 --> 00:10:55,200
and we can see that we have two steps

00:10:54,000 --> 00:10:56,959
there pardo

00:10:55,200 --> 00:10:58,800
and assign window this is another

00:10:56,959 --> 00:11:02,000
optimization that happens

00:10:58,800 --> 00:11:02,880
with this fun api instead of having to

00:11:02,000 --> 00:11:05,200
create

00:11:02,880 --> 00:11:06,399
one container for every single step so

00:11:05,200 --> 00:11:08,959
one for part two

00:11:06,399 --> 00:11:10,160
and one for assigned window we only need

00:11:08,959 --> 00:11:12,480
one of those

00:11:10,160 --> 00:11:14,480
we combine them together we combine all

00:11:12,480 --> 00:11:18,079
the steps and we only need

00:11:14,480 --> 00:11:21,360
this container and finally

00:11:18,079 --> 00:11:21,839
apache beam is very extensible so we can

00:11:21,360 --> 00:11:24,720
create

00:11:21,839 --> 00:11:25,360
new sdks and new runners one example of

00:11:24,720 --> 00:11:28,880
those

00:11:25,360 --> 00:11:31,920
is seo which is an scala sdk

00:11:28,880 --> 00:11:34,000
present by spotify and

00:11:31,920 --> 00:11:36,240
also is very customizable so we can

00:11:34,000 --> 00:11:38,640
create custom sources and custom things

00:11:36,240 --> 00:11:39,839
in case that one of the ios we need is

00:11:38,640 --> 00:11:42,240
not yet available

00:11:39,839 --> 00:11:43,360
in apache beam let's say that you need

00:11:42,240 --> 00:11:45,760
to to read

00:11:43,360 --> 00:11:46,480
an in-house database and we don't have

00:11:45,760 --> 00:11:48,880
any

00:11:46,480 --> 00:11:50,639
i available there you can create your

00:11:48,880 --> 00:11:52,639
own source and things

00:11:50,639 --> 00:11:53,920
and in case you want you can of course

00:11:52,639 --> 00:11:56,880
sell it with the community

00:11:53,920 --> 00:11:58,480
and also apachem is highly customizable

00:11:56,880 --> 00:12:01,120
we have a purdue

00:11:58,480 --> 00:12:01,839
which is a generalization of map and

00:12:01,120 --> 00:12:03,279
flat map

00:12:01,839 --> 00:12:04,959
that allows us to control the elements

00:12:03,279 --> 00:12:07,040
better sending the elements to different

00:12:04,959 --> 00:12:08,639
p collection as we want

00:12:07,040 --> 00:12:10,160
and there are other things that we can

00:12:08,639 --> 00:12:13,279
customize for example

00:12:10,160 --> 00:12:15,680
custom windows with this

00:12:13,279 --> 00:12:16,800
let's head off to the demo and see you

00:12:15,680 --> 00:12:18,880
there

00:12:16,800 --> 00:12:20,399
so here we are in a jupiter lab notebook

00:12:18,880 --> 00:12:21,279
which is the environment we're going to

00:12:20,399 --> 00:12:24,240
be using

00:12:21,279 --> 00:12:26,240
for this demo and the other demo is to

00:12:24,240 --> 00:12:28,000
show you how it is to change from run

00:12:26,240 --> 00:12:28,800
runner to another and how it is to

00:12:28,000 --> 00:12:30,560
change

00:12:28,800 --> 00:12:31,920
from batch to streaming just by adding

00:12:30,560 --> 00:12:32,720
this standard logic that we mentioned

00:12:31,920 --> 00:12:34,399
before like

00:12:32,720 --> 00:12:36,560
adding windows or another way of

00:12:34,399 --> 00:12:37,279
splitting unbounded data into bounded

00:12:36,560 --> 00:12:38,959
data

00:12:37,279 --> 00:12:40,800
the example we're going to be using is a

00:12:38,959 --> 00:12:42,399
word count which is the hello world of

00:12:40,800 --> 00:12:45,279
the etl pipelines

00:12:42,399 --> 00:12:46,480
so let's get into it let me first import

00:12:45,279 --> 00:12:47,279
the things we're going to be using for

00:12:46,480 --> 00:12:48,720
the demo

00:12:47,279 --> 00:12:50,560
and i'm also going to import in some

00:12:48,720 --> 00:12:51,519
variables these various variables are

00:12:50,560 --> 00:12:56,000
going to be used

00:12:51,519 --> 00:12:58,480
to reach my project in in data flow

00:12:56,000 --> 00:12:59,839
so now we have the actual pipeline and

00:12:58,480 --> 00:13:01,519
since we're going to be using the for

00:12:59,839 --> 00:13:03,279
example interactive runner

00:13:01,519 --> 00:13:05,360
we don't need to add any option the same

00:13:03,279 --> 00:13:08,240
way we don't need to add any options

00:13:05,360 --> 00:13:10,240
when using direct1a interactivewire is a

00:13:08,240 --> 00:13:10,959
specific runner made for notebooks that

00:13:10,240 --> 00:13:12,959
provides

00:13:10,959 --> 00:13:14,079
some extra visualization and it's a good

00:13:12,959 --> 00:13:17,519
feed for this

00:13:14,079 --> 00:13:18,240
demo so now it comes the actual business

00:13:17,519 --> 00:13:20,560
logic for

00:13:18,240 --> 00:13:22,560
our pipeline we're going to be reading

00:13:20,560 --> 00:13:25,519
from a public bucket that contains

00:13:22,560 --> 00:13:27,200
a text file which is the playback

00:13:25,519 --> 00:13:30,079
experience layer

00:13:27,200 --> 00:13:31,360
so in order to read from this text file

00:13:30,079 --> 00:13:34,880
we're going to be using

00:13:31,360 --> 00:13:37,279
grid from text with

00:13:34,880 --> 00:13:38,000
from the text and this point to this

00:13:37,279 --> 00:13:40,800
path

00:13:38,000 --> 00:13:42,079
now one would read from text using it

00:13:40,800 --> 00:13:44,320
from text

00:13:42,079 --> 00:13:46,160
the output is going to be every line

00:13:44,320 --> 00:13:49,199
that the file contains

00:13:46,160 --> 00:13:50,399
and we need to split that line into more

00:13:49,199 --> 00:13:53,760
than one word

00:13:50,399 --> 00:13:57,040
so here it comes the actual word count

00:13:53,760 --> 00:13:58,160
so let's do count source and since we're

00:13:57,040 --> 00:14:00,560
outputting more than

00:13:58,160 --> 00:14:03,440
one element form input element we need a

00:14:00,560 --> 00:14:07,199
flat map

00:14:03,440 --> 00:14:07,199
so let's do now split what

00:14:07,920 --> 00:14:12,399
now if you check the split word function

00:14:10,959 --> 00:14:14,000
you can see that we're actually

00:14:12,399 --> 00:14:16,079
outputting

00:14:14,000 --> 00:14:18,399
key values this is because we're going

00:14:16,079 --> 00:14:20,399
to be using a counter key later on

00:14:18,399 --> 00:14:21,440
in this case the key is going to be the

00:14:20,399 --> 00:14:23,120
actual board

00:14:21,440 --> 00:14:25,279
and the value is going to be a one in

00:14:23,120 --> 00:14:28,560
duality this value could be whatever

00:14:25,279 --> 00:14:33,360
but for simplicity let's use one

00:14:28,560 --> 00:14:33,360
and now we can do the counter key count

00:14:34,399 --> 00:14:39,440
now the pattern hasn't run yet since we

00:14:37,600 --> 00:14:41,040
need a specific way of running this

00:14:39,440 --> 00:14:42,800
pipeline

00:14:41,040 --> 00:14:44,320
where using interactive runner an

00:14:42,800 --> 00:14:46,000
interactive provide us

00:14:44,320 --> 00:14:47,760
with a specific way of doing this which

00:14:46,000 --> 00:14:49,600
is iv show

00:14:47,760 --> 00:14:51,839
and it give us some extra visualization

00:14:49,600 --> 00:14:54,240
that you can you will see in some games

00:14:51,839 --> 00:14:56,639
and seconds so if i do

00:14:54,240 --> 00:14:57,360
i need also count which is the variable

00:14:56,639 --> 00:14:59,199
we have here

00:14:57,360 --> 00:15:01,199
we can see that the pipeline is running

00:14:59,199 --> 00:15:03,279
now and in some seconds we will see the

00:15:01,199 --> 00:15:07,120
output of it

00:15:03,279 --> 00:15:09,199
we can see that king appeared 243 times

00:15:07,120 --> 00:15:11,680
and we can even see what was the one

00:15:09,199 --> 00:15:16,000
that appeared the most which is

00:15:11,680 --> 00:15:18,079
not a surprise at all so we have seen

00:15:16,000 --> 00:15:20,399
how to run a pipeline in interactive

00:15:18,079 --> 00:15:24,839
runner let's now move it to data flow

00:15:20,399 --> 00:15:26,160
so we change that runner to data flow

00:15:24,839 --> 00:15:27,680
runner

00:15:26,160 --> 00:15:29,199
and we need to add this option that we

00:15:27,680 --> 00:15:32,800
mentioned before so

00:15:29,199 --> 00:15:32,800
we first need a project

00:15:33,199 --> 00:15:36,399
project is available that is already

00:15:34,639 --> 00:15:38,079
imported here

00:15:36,399 --> 00:15:41,519
we are going to need a region for the

00:15:38,079 --> 00:15:44,800
workers to be in so let's use us

00:15:41,519 --> 00:15:47,600
central one we are going to need a

00:15:44,800 --> 00:15:51,199
temporary location

00:15:47,600 --> 00:15:51,199
which is going to be a bucket

00:15:52,320 --> 00:15:55,839
and a folder called temp

00:15:56,240 --> 00:16:01,600
and i'm going to add a name to

00:15:59,600 --> 00:16:05,040
this pi band which is going to be i

00:16:01,600 --> 00:16:05,040
don't know word count

00:16:06,639 --> 00:16:10,880
so this is everything we need to be its

00:16:09,519 --> 00:16:13,120
data flow

00:16:10,880 --> 00:16:14,800
and now we can see that the business

00:16:13,120 --> 00:16:18,160
logic remains the same

00:16:14,800 --> 00:16:19,920
we don't need to change anything and now

00:16:18,160 --> 00:16:21,360
we need to execute the pipeline before

00:16:19,920 --> 00:16:23,279
we were using ib show

00:16:21,360 --> 00:16:24,880
which again is something specific for

00:16:23,279 --> 00:16:27,600
interactive runner

00:16:24,880 --> 00:16:28,320
so we need to modify this so let's do

00:16:27,600 --> 00:16:31,759
pipeline

00:16:28,320 --> 00:16:32,240
equal p1 p1 is the way of executing pi

00:16:31,759 --> 00:16:34,399
lines

00:16:32,240 --> 00:16:35,360
in data flow running or direct running

00:16:34,399 --> 00:16:38,240
also

00:16:35,360 --> 00:16:39,440
so if we do this the pyramids will start

00:16:38,240 --> 00:16:42,399
reaching gcp

00:16:39,440 --> 00:16:43,759
and if i run this next cell it's going

00:16:42,399 --> 00:16:50,079
to help us

00:16:43,759 --> 00:16:52,720
see the link directly to gcp

00:16:50,079 --> 00:16:54,079
the pipeline is reaching pcp and if i

00:16:52,720 --> 00:16:57,360
click here

00:16:54,079 --> 00:16:57,839
we can see our pipeline running in our

00:16:57,360 --> 00:17:05,839
gsp

00:16:57,839 --> 00:17:05,839
project using data flow runner

00:17:07,919 --> 00:17:11,679
so you can see here that without

00:17:09,839 --> 00:17:14,240
changing our business logic

00:17:11,679 --> 00:17:15,360
just by changing the options we have

00:17:14,240 --> 00:17:17,760
reached data flow

00:17:15,360 --> 00:17:18,720
and the pipeline will work exactly the

00:17:17,760 --> 00:17:20,480
same

00:17:18,720 --> 00:17:22,400
so let's see now how hard it is to

00:17:20,480 --> 00:17:25,039
change from batch to streaming

00:17:22,400 --> 00:17:26,400
the first thing we need to do is to tell

00:17:25,039 --> 00:17:28,400
our options that

00:17:26,400 --> 00:17:29,440
we're using a streaming pipeline so

00:17:28,400 --> 00:17:32,080
let's do a streaming

00:17:29,440 --> 00:17:33,200
equal true and i'm also going to be

00:17:32,080 --> 00:17:36,400
changing the name

00:17:33,200 --> 00:17:36,400
to streaming

00:17:36,960 --> 00:17:40,960
let's run this cell now now before we

00:17:40,240 --> 00:17:44,400
were using

00:17:40,960 --> 00:17:46,720
abundant source which is a public bucket

00:17:44,400 --> 00:17:47,840
since our pilot is streaming now we need

00:17:46,720 --> 00:17:51,679
something unbounded

00:17:47,840 --> 00:17:54,320
so let's use one pops up topic

00:17:51,679 --> 00:17:54,320
so let's do

00:17:54,799 --> 00:18:02,480
sorry grid from

00:17:58,320 --> 00:18:05,760
whatsapp on our topic which is again

00:18:02,480 --> 00:18:06,480
imported in this cell here when we read

00:18:05,760 --> 00:18:10,080
from

00:18:06,480 --> 00:18:12,320
whatsapp the output it's on

00:18:10,080 --> 00:18:14,880
bytes so we need to decode those so

00:18:12,320 --> 00:18:17,039
let's do the code this is the name of

00:18:14,880 --> 00:18:20,480
our step

00:18:17,039 --> 00:18:25,679
and i'm going to be using a map function

00:18:20,480 --> 00:18:25,679
lambda b b code

00:18:26,840 --> 00:18:29,840
utf-8

00:18:30,400 --> 00:18:34,400
puffs up is an unbounded source and

00:18:33,039 --> 00:18:35,600
since we're going to be aggregating

00:18:34,400 --> 00:18:38,000
later with count

00:18:35,600 --> 00:18:41,039
we need to split this unbounded source

00:18:38,000 --> 00:18:44,400
into bounded pieces of data

00:18:41,039 --> 00:18:47,120
in order to do that we create

00:18:44,400 --> 00:18:48,400
one window that is going to split our

00:18:47,120 --> 00:18:52,799
data in

00:18:48,400 --> 00:18:55,360
tanks so window into

00:18:52,799 --> 00:18:57,280
window dot and i'm going to use fix

00:18:55,360 --> 00:19:00,400
window of

00:18:57,280 --> 00:19:03,039
i don't know 60 seconds yeah

00:19:00,400 --> 00:19:05,360
so again the pylon hasn't run yet we

00:19:03,039 --> 00:19:09,120
need to execute the next

00:19:05,360 --> 00:19:09,760
cells so pipeline now should be reaching

00:19:09,120 --> 00:19:11,840
gcp

00:19:09,760 --> 00:19:15,200
and in some seconds we will see the link

00:19:11,840 --> 00:19:15,200
that will take us to gcp

00:19:20,240 --> 00:19:25,840
now let's click here and where again

00:19:26,000 --> 00:19:29,200
going to gcp and we will see our

00:19:28,559 --> 00:19:34,240
pipeline

00:19:29,200 --> 00:19:34,240
running in streaming you can see it here

00:19:34,559 --> 00:19:41,840
and the account is exactly the same

00:19:39,679 --> 00:19:42,720
so you see that the only thing we needed

00:19:41,840 --> 00:19:45,600
to change

00:19:42,720 --> 00:19:47,440
was our source and added this extra

00:19:45,600 --> 00:19:49,039
logic that we mentioned before like a

00:19:47,440 --> 00:19:52,080
window or in the case

00:19:49,039 --> 00:19:54,080
of pubsub decoding the messages but the

00:19:52,080 --> 00:19:55,200
actual word count the actual part that

00:19:54,080 --> 00:19:58,080
we're counting

00:19:55,200 --> 00:20:00,000
remains the same so we did we don't

00:19:58,080 --> 00:20:01,919
didn't need to change the flat map we

00:20:00,000 --> 00:20:04,960
didn't need to change the counter key

00:20:01,919 --> 00:20:07,919
we only needed to change the source

00:20:04,960 --> 00:20:09,360
and adding this extra logic so i hope

00:20:07,919 --> 00:20:10,080
you liked this demo and i hope you got

00:20:09,360 --> 00:20:14,240
the idea

00:20:10,080 --> 00:20:14,240
of the flexibility that apache bin has

00:20:14,640 --> 00:20:18,799
so thanks everyone i hope you liked the

00:20:16,799 --> 00:20:20,960
presentation and i hope you got

00:20:18,799 --> 00:20:22,240
an idea of the flexibility that apache

00:20:20,960 --> 00:20:24,480
ring has

00:20:22,240 --> 00:20:26,240
and hope you like the rest of the bim

00:20:24,480 --> 00:20:35,840
day presentations

00:20:26,240 --> 00:20:35,840
thanks and bye

00:20:40,159 --> 00:20:43,200
thanks inigo this is a great

00:20:41,679 --> 00:20:43,760
introduction to what beam is and what it

00:20:43,200 --> 00:20:46,080
can do

00:20:43,760 --> 00:20:47,919
what do you think about it robert i love

00:20:46,080 --> 00:20:49,919
that with cross language you get to use

00:20:47,919 --> 00:20:52,240
production harder transforms even with

00:20:49,919 --> 00:20:53,760
newer sdks like go

00:20:52,240 --> 00:20:56,480
yeah that's the great thing about it is

00:20:53,760 --> 00:20:59,520
that even if you're writing a new trick

00:20:56,480 --> 00:21:02,080
sorry a new sdk so if you wanted to make

00:20:59,520 --> 00:21:03,520
a javascript or typescript you don't

00:21:02,080 --> 00:21:05,360
have to go and implement all the

00:21:03,520 --> 00:21:07,360
different sources and syncs so

00:21:05,360 --> 00:21:08,799
you can just use the production heart

00:21:07,360 --> 00:21:09,360
and transforms that have been living for

00:21:08,799 --> 00:21:13,200
a while

00:21:09,360 --> 00:21:14,880
in java or python with the new sdk

00:21:13,200 --> 00:21:16,720
so next we'll hear from bojon jong with

00:21:14,880 --> 00:21:17,919
an introduction of building beam io

00:21:16,720 --> 00:21:31,840
using splittable dufund

00:21:17,919 --> 00:21:31,840
take it away

00:21:52,960 --> 00:21:56,880
um hello everyone i'm boyan from google

00:21:55,760 --> 00:21:58,559
cloud dataflow

00:21:56,880 --> 00:22:01,120
today i'm going to give a brief

00:21:58,559 --> 00:22:01,840
introduction about how to build source

00:22:01,120 --> 00:22:04,320
in bing

00:22:01,840 --> 00:22:05,280
with the vadoo phone when we get into

00:22:04,320 --> 00:22:07,520
details

00:22:05,280 --> 00:22:09,039
first i will introduce a little bit more

00:22:07,520 --> 00:22:12,559
on what is being sourced

00:22:09,039 --> 00:22:14,640
and what is bible do for

00:22:12,559 --> 00:22:16,080
most beam pipelines are reading from

00:22:14,640 --> 00:22:18,240
external sources

00:22:16,080 --> 00:22:20,640
and get this output records from the

00:22:18,240 --> 00:22:23,200
source and do certain computations

00:22:20,640 --> 00:22:25,039
and write a computation result into sync

00:22:23,200 --> 00:22:27,200
so take a word called pipeline as

00:22:25,039 --> 00:22:27,919
example i have put a pipeline ships on

00:22:27,200 --> 00:22:29,440
the right

00:22:27,919 --> 00:22:31,280
so you can see that what's called

00:22:29,440 --> 00:22:34,880
pipeline read the

00:22:31,280 --> 00:22:37,760
text file from external source and then

00:22:34,880 --> 00:22:38,080
get to the uh each word from the text

00:22:37,760 --> 00:22:40,240
and

00:22:38,080 --> 00:22:42,400
do a count forward and directly

00:22:40,240 --> 00:22:44,720
resulting to the sink

00:22:42,400 --> 00:22:46,960
so as you can see mostly the beam source

00:22:44,720 --> 00:22:49,280
will be the root nodes of the pipeline

00:22:46,960 --> 00:22:50,720
and before we introduce supportable dew

00:22:49,280 --> 00:22:53,039
phone into bin

00:22:50,720 --> 00:22:54,159
we already have one resource api for

00:22:53,039 --> 00:22:56,320
batch pipeline

00:22:54,159 --> 00:22:57,520
and unbounded source api for streaming

00:22:56,320 --> 00:23:00,159
pipeline

00:22:57,520 --> 00:23:01,440
so for example in this case the text io

00:23:00,159 --> 00:23:04,320
is a boundary source

00:23:01,440 --> 00:23:05,760
and for example kafka is a streaming

00:23:04,320 --> 00:23:08,400
source

00:23:05,760 --> 00:23:09,760
usually when we read from bounty source

00:23:08,400 --> 00:23:12,400
we want to read as

00:23:09,760 --> 00:23:14,320
fast as we can so what we can do is we

00:23:12,400 --> 00:23:17,360
can have multiple parallelisms

00:23:14,320 --> 00:23:18,480
while we do the reading so there are two

00:23:17,360 --> 00:23:20,799
kinds of splits

00:23:18,480 --> 00:23:22,880
the first one is initial split and the

00:23:20,799 --> 00:23:25,200
second one is dynamic split

00:23:22,880 --> 00:23:26,240
initial split will give us initial

00:23:25,200 --> 00:23:28,720
parallelisms

00:23:26,240 --> 00:23:29,520
before we actually reading so still use

00:23:28,720 --> 00:23:32,640
a text l

00:23:29,520 --> 00:23:34,159
example uh we have a text file right and

00:23:32,640 --> 00:23:36,320
we want to read that file

00:23:34,159 --> 00:23:38,559
and if we know that we have two other

00:23:36,320 --> 00:23:40,960
workers before we actually read

00:23:38,559 --> 00:23:43,120
we actually can split the text into two

00:23:40,960 --> 00:23:44,559
charts and we can have these two workers

00:23:43,120 --> 00:23:46,640
read from these two charts

00:23:44,559 --> 00:23:48,960
in parallel then we will have the read

00:23:46,640 --> 00:23:52,000
faster and dynamic split means

00:23:48,960 --> 00:23:53,840
we want to split happens during during

00:23:52,000 --> 00:23:56,480
we are doing the reading

00:23:53,840 --> 00:23:58,880
and that split happens in the future so

00:23:56,480 --> 00:24:01,760
this one will give us more parallelisms

00:23:58,880 --> 00:24:03,679
during we're doing the reading so in

00:24:01,760 --> 00:24:05,840
order to do the dynamic split

00:24:03,679 --> 00:24:07,919
we actually need a boundary source to do

00:24:05,840 --> 00:24:10,240
the initial sizing for the whole source

00:24:07,919 --> 00:24:11,360
this will give a boundary and assess and

00:24:10,240 --> 00:24:13,520
assess information

00:24:11,360 --> 00:24:15,200
of this source to the runners this will

00:24:13,520 --> 00:24:18,240
help the runners to make this

00:24:15,200 --> 00:24:20,159
a split decision and

00:24:18,240 --> 00:24:22,159
during the reading we also wanted the

00:24:20,159 --> 00:24:23,440
bonding source to able to be able to

00:24:22,159 --> 00:24:26,080
report the progress

00:24:23,440 --> 00:24:27,440
this is uh that means we want the uh the

00:24:26,080 --> 00:24:29,360
runners will need to know

00:24:27,440 --> 00:24:30,880
how much the boundary source has read

00:24:29,360 --> 00:24:32,880
and how much work left

00:24:30,880 --> 00:24:34,000
for that for that boundary source it's

00:24:32,880 --> 00:24:36,080
also used for

00:24:34,000 --> 00:24:38,320
signals for the runners to make a split

00:24:36,080 --> 00:24:40,480
decision and finally why is the time

00:24:38,320 --> 00:24:42,240
that runners want to issue a split

00:24:40,480 --> 00:24:44,240
the boundary source should be able to

00:24:42,240 --> 00:24:46,640
perform that split for the source

00:24:44,240 --> 00:24:47,520
and to gain more parallelisms usually

00:24:46,640 --> 00:24:49,440
for streaming

00:24:47,520 --> 00:24:50,799
we actually are able to perform the

00:24:49,440 --> 00:24:52,559
initial splits as well

00:24:50,799 --> 00:24:55,039
but it's not really possible to do the

00:24:52,559 --> 00:24:56,960
dynamic split it's easy to understand

00:24:55,039 --> 00:24:58,640
why we are not able to do so

00:24:56,960 --> 00:25:00,559
it's because first it's on bonding

00:24:58,640 --> 00:25:03,120
source it doesn't have the boundary

00:25:00,559 --> 00:25:04,880
and the second because for the dynamic

00:25:03,120 --> 00:25:05,679
speeds we usually want to split in the

00:25:04,880 --> 00:25:09,039
future

00:25:05,679 --> 00:25:10,480
and it's not very known that's for the

00:25:09,039 --> 00:25:12,559
unbounded source whether we will have

00:25:10,480 --> 00:25:13,039
the future so usually we will not do

00:25:12,559 --> 00:25:14,880
that

00:25:13,039 --> 00:25:16,080
but instead we will do the checkpoint

00:25:14,880 --> 00:25:18,080
for this source and

00:25:16,080 --> 00:25:19,679
the most important things for unbounded

00:25:18,080 --> 00:25:21,919
source is we want

00:25:19,679 --> 00:25:22,880
the body source to check the watermark

00:25:21,919 --> 00:25:24,799
correctly

00:25:22,880 --> 00:25:27,120
especially when there are several

00:25:24,799 --> 00:25:28,240
downstream operations have a complicated

00:25:27,120 --> 00:25:30,159
rendering strategy

00:25:28,240 --> 00:25:32,159
and do the triggers on that so that's

00:25:30,159 --> 00:25:34,080
very important for unbounded source

00:25:32,159 --> 00:25:35,520
so given that we already have the

00:25:34,080 --> 00:25:38,559
boundary source api and

00:25:35,520 --> 00:25:40,480
bonding source apis for the beam source

00:25:38,559 --> 00:25:42,240
why st why we still want to introduce

00:25:40,480 --> 00:25:45,760
the new superpower to fund

00:25:42,240 --> 00:25:47,039
apis in tubing so the most important

00:25:45,760 --> 00:25:49,440
part for here

00:25:47,039 --> 00:25:50,480
is as i mentioned as based on our

00:25:49,440 --> 00:25:52,480
current framework

00:25:50,480 --> 00:25:53,520
we should have the beam source as a root

00:25:52,480 --> 00:25:55,679
node pipeline

00:25:53,520 --> 00:25:57,120
then we will have the use case like for

00:25:55,679 --> 00:25:58,960
example you have if i have two

00:25:57,120 --> 00:26:01,679
external source which is the corey and

00:25:58,960 --> 00:26:02,720
kafka and i store the topic partitions

00:26:01,679 --> 00:26:05,360
on the bigquery

00:26:02,720 --> 00:26:07,360
and first i need to i need to know which

00:26:05,360 --> 00:26:08,400
topic and partition i can read from from

00:26:07,360 --> 00:26:11,200
the bigquery

00:26:08,400 --> 00:26:13,200
and then read from kafka if i want to do

00:26:11,200 --> 00:26:15,360
all of the things inside one pipeline

00:26:13,200 --> 00:26:17,120
then there's no way in current bonding

00:26:15,360 --> 00:26:19,279
source and unbounded source apis

00:26:17,120 --> 00:26:20,240
to do because it has to be the roots

00:26:19,279 --> 00:26:22,880
node but

00:26:20,240 --> 00:26:24,000
then with this before too far we are

00:26:22,880 --> 00:26:26,480
able to do so so

00:26:24,000 --> 00:26:27,840
we can first read from the bigquery get

00:26:26,480 --> 00:26:30,080
a topic partition

00:26:27,840 --> 00:26:31,600
and then let it read from kafka to read

00:26:30,080 --> 00:26:34,400
from that topic partition

00:26:31,600 --> 00:26:35,600
and output records the reason why super

00:26:34,400 --> 00:26:38,000
duper can do so

00:26:35,600 --> 00:26:39,440
is because the bible too a two phone and

00:26:38,000 --> 00:26:40,640
two phone can be any nodes of the

00:26:39,440 --> 00:26:43,440
pipeline

00:26:40,640 --> 00:26:45,919
and the second part is superbowl 25 is a

00:26:43,440 --> 00:26:48,320
unified python streaming source model

00:26:45,919 --> 00:26:50,400
over phone api execution by api

00:26:48,320 --> 00:26:52,080
execution is a new execution model we

00:26:50,400 --> 00:26:54,240
introduced to bim literally

00:26:52,080 --> 00:26:55,360
it's kind of like a complicated concept

00:26:54,240 --> 00:26:57,919
so i will not go

00:26:55,360 --> 00:26:58,960
into details in this slide but i put a

00:26:57,919 --> 00:27:01,840
intro but i'll put

00:26:58,960 --> 00:27:02,799
a uh link related to that at the end of

00:27:01,840 --> 00:27:04,960
the slide

00:27:02,799 --> 00:27:07,039
and the third one is we will have the

00:27:04,960 --> 00:27:08,320
ability to split for the splitable

00:27:07,039 --> 00:27:10,960
difference

00:27:08,320 --> 00:27:12,720
so that means we can provide both

00:27:10,960 --> 00:27:14,720
initial split and dynamic speed for

00:27:12,720 --> 00:27:15,279
bonding source so it's perfect for that

00:27:14,720 --> 00:27:17,440
one

00:27:15,279 --> 00:27:18,720
and for more details about what speed

00:27:17,440 --> 00:27:21,120
means we will

00:27:18,720 --> 00:27:22,640
go into deeper in uh in the following

00:27:21,120 --> 00:27:25,120
slides

00:27:22,640 --> 00:27:25,760
and the last one is seville dufan is a

00:27:25,120 --> 00:27:28,000
dual phone

00:27:25,760 --> 00:27:30,080
right so it will have a simple syntax as

00:27:28,000 --> 00:27:31,679
doofan and the most bin devs are

00:27:30,080 --> 00:27:34,080
familiar with two phones so it will be

00:27:31,679 --> 00:27:36,080
simpler simpler and straightforward for

00:27:34,080 --> 00:27:38,399
them to build their own self sometimes

00:27:36,080 --> 00:27:39,120
top offs will do fun and one more

00:27:38,399 --> 00:27:41,679
interesting things

00:27:39,120 --> 00:27:43,919
around that is if you are uh building

00:27:41,679 --> 00:27:45,679
your own cells on top of super duper

00:27:43,919 --> 00:27:47,840
then all you need to do is have one

00:27:45,679 --> 00:27:49,520
implementation which should be work for

00:27:47,840 --> 00:27:52,240
both streaming and batch

00:27:49,520 --> 00:27:52,960
instead of in the unbounding source and

00:27:52,240 --> 00:27:55,360
on multiple

00:27:52,960 --> 00:27:57,120
source api what you have to do is you

00:27:55,360 --> 00:27:59,039
have to write invitation

00:27:57,120 --> 00:28:00,880
derived from bonding source api for

00:27:59,039 --> 00:28:02,720
batch pipeline and write another

00:28:00,880 --> 00:28:03,520
implementation derived from unbounded

00:28:02,720 --> 00:28:05,200
source api

00:28:03,520 --> 00:28:07,600
for streaming pipeline that's kind of

00:28:05,200 --> 00:28:08,000
like it's burden for most developers to

00:28:07,600 --> 00:28:10,000
do

00:28:08,000 --> 00:28:12,399
to do two implementations for different

00:28:10,000 --> 00:28:14,320
purpose

00:28:12,399 --> 00:28:16,640
so given that we're already know that's

00:28:14,320 --> 00:28:19,120
why we want to have a spiritual phone

00:28:16,640 --> 00:28:20,720
then what is we'll do from so i would as

00:28:19,120 --> 00:28:22,000
i mentioned many times the google

00:28:20,720 --> 00:28:23,360
developer is a dual phone with the

00:28:22,000 --> 00:28:25,600
ability to split

00:28:23,360 --> 00:28:27,440
so first it's a dual phone it will have

00:28:25,600 --> 00:28:28,799
all sync all simple syntax and

00:28:27,440 --> 00:28:30,640
attributes from different

00:28:28,799 --> 00:28:33,200
and the second it have it will have the

00:28:30,640 --> 00:28:35,679
ability to be able to split inside

00:28:33,200 --> 00:28:36,960
one element so what that means split

00:28:35,679 --> 00:28:40,000
inside one element

00:28:36,960 --> 00:28:41,840
let's look into us some more examples so

00:28:40,000 --> 00:28:44,320
let's see we have two elements

00:28:41,840 --> 00:28:45,840
element one element two and we have two

00:28:44,320 --> 00:28:46,720
for instance processing these two

00:28:45,840 --> 00:28:49,120
elements

00:28:46,720 --> 00:28:50,880
and we happen to have the first element

00:28:49,120 --> 00:28:53,120
element one is super huge

00:28:50,880 --> 00:28:55,120
and is heavy and currently we are at

00:28:53,120 --> 00:28:57,120
twenty percent of it

00:28:55,120 --> 00:28:58,240
so let's see for the following eighty

00:28:57,120 --> 00:29:00,240
percent of element

00:28:58,240 --> 00:29:01,360
we still need to take one more day to

00:29:00,240 --> 00:29:03,200
process it

00:29:01,360 --> 00:29:05,520
so if we are using a normal default

00:29:03,200 --> 00:29:06,320
instance what we can do is we have to

00:29:05,520 --> 00:29:08,240
wait for the

00:29:06,320 --> 00:29:09,520
first element to be finished and

00:29:08,240 --> 00:29:12,720
continues to process

00:29:09,520 --> 00:29:13,600
element two so it's it's slow and we

00:29:12,720 --> 00:29:15,840
have to wait

00:29:13,600 --> 00:29:16,880
a lot of time but if we have a support

00:29:15,840 --> 00:29:19,360
upon for that

00:29:16,880 --> 00:29:21,840
what we can do is we can ask the first

00:29:19,360 --> 00:29:24,080
bit uh the first element to split

00:29:21,840 --> 00:29:26,240
so that means we can see i want you

00:29:24,080 --> 00:29:27,200
first element to split at the 50 percent

00:29:26,240 --> 00:29:29,919
of you

00:29:27,200 --> 00:29:30,960
then we will have the uh first half of

00:29:29,919 --> 00:29:33,600
the first element

00:29:30,960 --> 00:29:36,000
on the first vertex for instance and

00:29:33,600 --> 00:29:38,480
then we can have the second half of the

00:29:36,000 --> 00:29:40,159
of the element one and the whole element

00:29:38,480 --> 00:29:42,559
two to be rescheduled on

00:29:40,159 --> 00:29:44,559
another instance at this moment we will

00:29:42,559 --> 00:29:47,039
have two spiritual for instance

00:29:44,559 --> 00:29:49,120
work working on this two out of two i'm

00:29:47,039 --> 00:29:51,520
working on this two chart in parallel

00:29:49,120 --> 00:29:52,159
so that means we we should have the

00:29:51,520 --> 00:29:55,760
pipeline

00:29:52,159 --> 00:29:56,720
to be much faster so as you can see it's

00:29:55,760 --> 00:29:59,360
very

00:29:56,720 --> 00:30:00,320
powerful and it's very useful for batch

00:29:59,360 --> 00:30:02,320
then

00:30:00,320 --> 00:30:03,840
what we should think about when we when

00:30:02,320 --> 00:30:05,679
we're building super duper

00:30:03,840 --> 00:30:07,440
the first one is the element and

00:30:05,679 --> 00:30:09,919
restriction

00:30:07,440 --> 00:30:11,679
usually the uh super duper is a

00:30:09,919 --> 00:30:14,320
different right so it will take a

00:30:11,679 --> 00:30:15,120
input as element and ask for several

00:30:14,320 --> 00:30:18,080
records

00:30:15,120 --> 00:30:20,640
so the element here is the input and the

00:30:18,080 --> 00:30:22,880
restriction usually is a description of

00:30:20,640 --> 00:30:24,399
to describe how much work inside us

00:30:22,880 --> 00:30:26,640
inside that element

00:30:24,399 --> 00:30:28,080
so taking examples through different on

00:30:26,640 --> 00:30:30,399
the right as example

00:30:28,080 --> 00:30:32,399
let's say we have a string of abcd as

00:30:30,399 --> 00:30:35,039
input to that spiritual form

00:30:32,399 --> 00:30:36,000
and what this will differ will do is it

00:30:35,039 --> 00:30:39,200
will output

00:30:36,000 --> 00:30:41,200
characters a b c d separately

00:30:39,200 --> 00:30:42,240
in this case the element will be the

00:30:41,200 --> 00:30:45,440
input string

00:30:42,240 --> 00:30:47,200
which is a b c d and i will if i'm

00:30:45,440 --> 00:30:50,000
writing this kind of sprutifa

00:30:47,200 --> 00:30:52,000
i will say i want to have a restriction

00:30:50,000 --> 00:30:52,559
which can describe how many works in

00:30:52,000 --> 00:30:55,520
this

00:30:52,559 --> 00:30:56,799
string so i will take offset range from

00:30:55,520 --> 00:30:59,440
0 to 4

00:30:56,799 --> 00:31:01,600
to describe that that i have a string

00:30:59,440 --> 00:31:04,799
and the index from for this string

00:31:01,600 --> 00:31:05,760
is from 0 to 4. so that's element and

00:31:04,799 --> 00:31:08,240
restriction

00:31:05,760 --> 00:31:10,559
and we also have a restriction tracker

00:31:08,240 --> 00:31:13,120
to check the progress of the restriction

00:31:10,559 --> 00:31:15,039
and for a restriction tracker you should

00:31:13,120 --> 00:31:17,360
know the current consumer position

00:31:15,039 --> 00:31:18,080
of the restriction and should know the

00:31:17,360 --> 00:31:20,000
progress

00:31:18,080 --> 00:31:21,200
and why it's time to do the split from

00:31:20,000 --> 00:31:22,799
the runners and

00:31:21,200 --> 00:31:25,600
the other restriction tracker should be

00:31:22,799 --> 00:31:27,519
able to do so a simple example can be

00:31:25,600 --> 00:31:29,679
let's say we still have this kind of d

00:31:27,519 --> 00:31:32,320
form and we already output

00:31:29,679 --> 00:31:32,880
character a so the current position is

00:31:32,320 --> 00:31:35,200
one

00:31:32,880 --> 00:31:36,640
and the work has been done is from zero

00:31:35,200 --> 00:31:39,360
to one which i have output

00:31:36,640 --> 00:31:40,000
one characters and the work of the

00:31:39,360 --> 00:31:42,559
remaining

00:31:40,000 --> 00:31:43,919
is from one two four that means i still

00:31:42,559 --> 00:31:48,640
have four characters

00:31:43,919 --> 00:31:48,640
three characters left to output

00:31:49,120 --> 00:31:53,600
combining all the informations about uh

00:31:51,840 --> 00:31:54,960
including what is boundary source and

00:31:53,600 --> 00:31:57,679
bonding source requires

00:31:54,960 --> 00:31:58,880
and what is for dupont can provide now

00:31:57,679 --> 00:32:00,799
we can think about

00:31:58,880 --> 00:32:02,799
when it's time to build a source on top

00:32:00,799 --> 00:32:03,519
of spill dufferin what we should think

00:32:02,799 --> 00:32:05,600
about

00:32:03,519 --> 00:32:07,200
the first question is we should decide

00:32:05,600 --> 00:32:09,440
what what is element and

00:32:07,200 --> 00:32:10,399
what is the restriction it's important

00:32:09,440 --> 00:32:13,279
to pick up the

00:32:10,399 --> 00:32:14,080
suitable element and the restriction for

00:32:13,279 --> 00:32:16,399
your source

00:32:14,080 --> 00:32:19,039
because it will it will affect the

00:32:16,399 --> 00:32:21,840
implementation details of our source

00:32:19,039 --> 00:32:23,600
and the second is we should consider

00:32:21,840 --> 00:32:24,640
what kind of split we want to support

00:32:23,600 --> 00:32:26,720
for our source

00:32:24,640 --> 00:32:28,080
as we know that for bonding source it's

00:32:26,720 --> 00:32:30,240
always good to have

00:32:28,080 --> 00:32:32,080
to have good initial to have initial

00:32:30,240 --> 00:32:34,960
split and dynamic split

00:32:32,080 --> 00:32:36,320
and for streaming it is also always good

00:32:34,960 --> 00:32:39,039
to have initial split

00:32:36,320 --> 00:32:40,399
and to be able to do the check pointing

00:32:39,039 --> 00:32:41,600
and third one is for streaming

00:32:40,399 --> 00:32:44,080
specifically

00:32:41,600 --> 00:32:45,679
is if i'm writing a source for streaming

00:32:44,080 --> 00:32:47,440
i need to make sure whether

00:32:45,679 --> 00:32:49,840
i check under the one's watermark

00:32:47,440 --> 00:32:51,519
correctly and i need to make sure i did

00:32:49,840 --> 00:32:54,000
i don't hold back the watermark

00:32:51,519 --> 00:32:54,320
incorrectly to affect the pipeline sort

00:32:54,000 --> 00:32:57,840
to

00:32:54,320 --> 00:32:57,840
affect the pipeline results

00:32:58,159 --> 00:33:02,720
so the first question what is the

00:33:00,640 --> 00:33:05,360
element and what is the restriction

00:33:02,720 --> 00:33:07,360
so you relay for source the element can

00:33:05,360 --> 00:33:09,840
be the metadata of the source

00:33:07,360 --> 00:33:12,159
and the restriction can be the

00:33:09,840 --> 00:33:12,880
description to describe how much work in

00:33:12,159 --> 00:33:16,159
this

00:33:12,880 --> 00:33:19,360
metadata so taking textile as example

00:33:16,159 --> 00:33:22,000
for textile we can use file name as the

00:33:19,360 --> 00:33:23,600
element for example i want to read from

00:33:22,000 --> 00:33:26,799
file one text write

00:33:23,600 --> 00:33:30,399
and for the restriction we can use

00:33:26,799 --> 00:33:31,360
a offset range from 0 to the total lines

00:33:30,399 --> 00:33:33,519
of the text

00:33:31,360 --> 00:33:36,159
as a restriction so that will describe

00:33:33,519 --> 00:33:38,000
the whole work in such as file

00:33:36,159 --> 00:33:40,640
and another example is reading from

00:33:38,000 --> 00:33:42,000
kafka we know that kafka usually is from

00:33:40,640 --> 00:33:44,559
a topic partitions

00:33:42,000 --> 00:33:46,000
and it will output records with offset

00:33:44,559 --> 00:33:48,799
in that case we can

00:33:46,000 --> 00:33:49,120
use topic partition as element and we

00:33:48,799 --> 00:33:51,200
can

00:33:49,120 --> 00:33:52,240
also use offset range from zero to

00:33:51,200 --> 00:33:54,720
infinity

00:33:52,240 --> 00:33:57,200
to describe the total workload inside

00:33:54,720 --> 00:34:00,159
for that topic and partition

00:33:57,200 --> 00:34:01,120
then it's time to think about whether we

00:34:00,159 --> 00:34:04,880
can do or

00:34:01,120 --> 00:34:08,399
whether we can do for initial split

00:34:04,880 --> 00:34:10,800
so that means uh if if we uh that means

00:34:08,399 --> 00:34:13,040
if we know that our source can dominate

00:34:10,800 --> 00:34:15,119
split it's always good to do so

00:34:13,040 --> 00:34:16,320
if you if we have decided that we want

00:34:15,119 --> 00:34:18,240
to do the initial splits

00:34:16,320 --> 00:34:19,599
the only things we need to do is we can

00:34:18,240 --> 00:34:22,079
create a function

00:34:19,599 --> 00:34:24,159
in the do file and annotate the function

00:34:22,079 --> 00:34:26,960
with bit restriction annotation

00:34:24,159 --> 00:34:28,800
the sdk framework will pick up this

00:34:26,960 --> 00:34:29,760
function to perform initial splits when

00:34:28,800 --> 00:34:32,800
it's time to do so

00:34:29,760 --> 00:34:36,159
automatically so for example

00:34:32,800 --> 00:34:37,679
we still use the textile right so the

00:34:36,159 --> 00:34:40,399
element is a foul name

00:34:37,679 --> 00:34:41,280
and let's say we have timelines for that

00:34:40,399 --> 00:34:44,240
of text

00:34:41,280 --> 00:34:45,599
file and we want to read the text file

00:34:44,240 --> 00:34:47,919
one line by one line

00:34:45,599 --> 00:34:48,720
so in this case restriction will be 0 to

00:34:47,919 --> 00:34:51,919
00:34:48,720 --> 00:34:54,639
and i already know that

00:34:51,919 --> 00:34:56,879
i will have two workers to be able to

00:34:54,639 --> 00:34:59,440
read from two charts in parallel

00:34:56,879 --> 00:35:00,320
from the start so i can split the

00:34:59,440 --> 00:35:04,240
restriction

00:35:00,320 --> 00:35:06,160
from 0 to 5 and 5 to 10 and the runners

00:35:04,240 --> 00:35:08,320
before the reads the runners will

00:35:06,160 --> 00:35:09,280
redistribute these two shards on these

00:35:08,320 --> 00:35:11,520
two workers

00:35:09,280 --> 00:35:16,079
and we will have the rate on these two

00:35:11,520 --> 00:35:18,800
workers in parallel at the same time

00:35:16,079 --> 00:35:19,520
during the actual reading happens uh

00:35:18,800 --> 00:35:21,599
which

00:35:19,520 --> 00:35:23,599
we should think about whether we want to

00:35:21,599 --> 00:35:26,079
provide more information to runners

00:35:23,599 --> 00:35:28,079
that's progress so in terms of progress

00:35:26,079 --> 00:35:30,720
it's always describe how much work

00:35:28,079 --> 00:35:32,800
we have been done and how much work has

00:35:30,720 --> 00:35:35,520
been left for this source

00:35:32,800 --> 00:35:37,119
for batch is a useful signal to make a

00:35:35,520 --> 00:35:39,920
dynamic speed decision

00:35:37,119 --> 00:35:41,280
and for streaming is useful to let the

00:35:39,920 --> 00:35:44,000
runner know the backlog

00:35:41,280 --> 00:35:46,320
especially for data flow data flow will

00:35:44,000 --> 00:35:48,560
use a backlog to determine whether to

00:35:46,320 --> 00:35:50,000
bring up more workers to to work on the

00:35:48,560 --> 00:35:51,920
source so that means

00:35:50,000 --> 00:35:53,359
if we can let the runner knows we still

00:35:51,920 --> 00:35:55,520
have a lot of work to do

00:35:53,359 --> 00:35:56,400
the work uh the data flow can bring up

00:35:55,520 --> 00:35:58,320
more workers

00:35:56,400 --> 00:35:59,760
we will have our streaming pipelines

00:35:58,320 --> 00:36:01,920
faster

00:35:59,760 --> 00:36:03,359
so for the progress let's still use the

00:36:01,920 --> 00:36:04,720
uh textile example

00:36:03,359 --> 00:36:06,880
so let's say we are reading the first

00:36:04,720 --> 00:36:08,320
chart of the text so we have the file

00:36:06,880 --> 00:36:10,320
name as element again

00:36:08,320 --> 00:36:12,160
and it has the first chart which is from

00:36:10,320 --> 00:36:14,320
five zero to five

00:36:12,160 --> 00:36:16,000
and let's say we have output the first

00:36:14,320 --> 00:36:18,160
line so at this moment

00:36:16,000 --> 00:36:20,240
the progress the progress is kind of

00:36:18,160 --> 00:36:22,400
like the work has been done is we have

00:36:20,240 --> 00:36:23,200
output one line which is from zero to

00:36:22,400 --> 00:36:25,920
ten

00:36:23,200 --> 00:36:27,839
always from zero to one and the work

00:36:25,920 --> 00:36:31,440
left is from 1 2

00:36:27,839 --> 00:36:34,960
1 to 4 1 to 5 which means we still have

00:36:31,440 --> 00:36:36,400
four lines to be outputted and when the

00:36:34,960 --> 00:36:38,800
runner's issue splits

00:36:36,400 --> 00:36:40,079
you lay for batch is dynamic split to be

00:36:38,800 --> 00:36:41,440
more parallelisms

00:36:40,079 --> 00:36:43,599
and the first streaming is usually

00:36:41,440 --> 00:36:45,920
checkpointing which means uranus wants

00:36:43,599 --> 00:36:47,680
us to stop reading as soon as possible

00:36:45,920 --> 00:36:49,520
and the return whatever we have to the

00:36:47,680 --> 00:36:52,000
runners so for the

00:36:49,520 --> 00:36:53,520
dynamic split let's say after we give

00:36:52,000 --> 00:36:55,760
the progress to the runners

00:36:53,520 --> 00:36:58,079
and the runner says i want you to split

00:36:55,760 --> 00:37:00,880
at 50 percent of the remaining

00:36:58,079 --> 00:37:03,520
element so that means uh for the current

00:37:00,880 --> 00:37:06,640
remaining work which is from one to four

00:37:03,520 --> 00:37:09,200
one two uh five the runner one has two

00:37:06,640 --> 00:37:11,839
speeds split at fifty percent of it

00:37:09,200 --> 00:37:13,599
so that means uh in the current instance

00:37:11,839 --> 00:37:16,240
we will still output

00:37:13,599 --> 00:37:18,480
uh line two and line three and we will

00:37:16,240 --> 00:37:19,200
return restrictions three to five to the

00:37:18,480 --> 00:37:21,520
runners

00:37:19,200 --> 00:37:22,960
and the runner will have another workers

00:37:21,520 --> 00:37:26,400
to work on this chart

00:37:22,960 --> 00:37:28,560
to continuously to output line 4 to

00:37:26,400 --> 00:37:29,839
length 3 to length 4. so that means at

00:37:28,560 --> 00:37:30,720
this moment we will gain more

00:37:29,839 --> 00:37:34,160
parallelisms

00:37:30,720 --> 00:37:37,040
and our pipeline should be faster and

00:37:34,160 --> 00:37:38,400
for streaming because runner 1 has to

00:37:37,040 --> 00:37:41,119
stop reading immediately

00:37:38,400 --> 00:37:42,480
that means we after we finish processing

00:37:41,119 --> 00:37:44,720
output first line

00:37:42,480 --> 00:37:46,000
we should stop and return the whole

00:37:44,720 --> 00:37:49,280
remaining restriction

00:37:46,000 --> 00:37:51,599
which is from 0 4 1 2

00:37:49,280 --> 00:37:52,880
five to the runners and the runners will

00:37:51,599 --> 00:37:56,720
have another

00:37:52,880 --> 00:37:56,720
uh workers to work on this chart

00:37:57,520 --> 00:38:00,800
and finally we are going to the

00:37:59,520 --> 00:38:03,920
streaming watermark

00:38:00,800 --> 00:38:06,079
uh side as i mentioned the watermark is

00:38:03,920 --> 00:38:08,240
extremely extremely important

00:38:06,079 --> 00:38:09,680
for uh streaming pipelines especially

00:38:08,240 --> 00:38:12,000
when your pipeline has

00:38:09,680 --> 00:38:12,800
complicated a windowing strategy and the

00:38:12,000 --> 00:38:15,520
triggers

00:38:12,800 --> 00:38:17,280
so when we're building our source based

00:38:15,520 --> 00:38:19,119
on spill2fun for streaming

00:38:17,280 --> 00:38:20,560
we should especially care about the

00:38:19,119 --> 00:38:23,520
watermark

00:38:20,560 --> 00:38:26,400
so for watermark uh splurgify offered a

00:38:23,520 --> 00:38:29,440
utility class called watermark estimator

00:38:26,400 --> 00:38:31,440
for you to estimate the watermark

00:38:29,440 --> 00:38:33,680
from this reward you farm there are

00:38:31,440 --> 00:38:34,320
three common tabs for the watermark cast

00:38:33,680 --> 00:38:36,720
meter

00:38:34,320 --> 00:38:38,000
the wall tone monotonically increasing

00:38:36,720 --> 00:38:40,560
and the manual one

00:38:38,000 --> 00:38:41,680
the water and watermark estimator is a

00:38:40,560 --> 00:38:45,280
super straightforward

00:38:41,680 --> 00:38:49,280
it will always use a current processing

00:38:45,280 --> 00:38:51,760
timestamp as the output watermark

00:38:49,280 --> 00:38:54,000
and for monotonically increasing it

00:38:51,760 --> 00:38:56,560
allows you the output tempstamp

00:38:54,000 --> 00:38:57,920
from the records of the output of the

00:38:56,560 --> 00:39:01,040
superduo fund

00:38:57,920 --> 00:39:02,560
and it it expects the timestamp from the

00:39:01,040 --> 00:39:05,280
output of the sql default

00:39:02,560 --> 00:39:06,079
to be always increasing monotonically if

00:39:05,280 --> 00:39:08,480
it doesn't

00:39:06,079 --> 00:39:09,680
it will throw exceptions and errors from

00:39:08,480 --> 00:39:12,560
the sdk

00:39:09,680 --> 00:39:14,240
and says i have errors and now and i

00:39:12,560 --> 00:39:14,720
don't want to output more records

00:39:14,240 --> 00:39:18,400
because

00:39:14,720 --> 00:39:21,760
it has a it has execution errors

00:39:18,400 --> 00:39:24,800
and for manual it it gives the uh

00:39:21,760 --> 00:39:27,200
source author the the freedom to

00:39:24,800 --> 00:39:28,560
set the watermark by themselves instead

00:39:27,200 --> 00:39:31,040
of these four different

00:39:28,560 --> 00:39:31,839
methods you can set the watermark to any

00:39:31,040 --> 00:39:34,880
time stamp that

00:39:31,839 --> 00:39:36,800
makes sense to you as long as you can

00:39:34,880 --> 00:39:40,400
make sure that timestamp is cracked

00:39:36,800 --> 00:39:42,800
and it will not mess up the pipelines

00:39:40,400 --> 00:39:43,680
so basically that's all the things i

00:39:42,800 --> 00:39:47,200
want to

00:39:43,680 --> 00:39:49,040
cover and because uh there are too many

00:39:47,200 --> 00:39:50,640
uh point of views of the support you

00:39:49,040 --> 00:39:51,599
found and the building source with

00:39:50,640 --> 00:39:54,640
bluetooth

00:39:51,599 --> 00:39:55,040
we we don't get into details on each

00:39:54,640 --> 00:39:57,440
point

00:39:55,040 --> 00:39:58,800
but hopefully this overall introduction

00:39:57,440 --> 00:40:02,320
will be helpful

00:39:58,800 --> 00:40:04,000
and finally i will uh

00:40:02,320 --> 00:40:06,319
attach more resources if you are

00:40:04,000 --> 00:40:06,960
interesting here which can help you to

00:40:06,319 --> 00:40:09,280
understand

00:40:06,960 --> 00:40:11,119
more when it's time to build your own

00:40:09,280 --> 00:40:13,440
source and tacos will do fun

00:40:11,119 --> 00:40:15,200
so first if you are interested in

00:40:13,440 --> 00:40:16,400
building your own supervisor phone you

00:40:15,200 --> 00:40:18,480
can look into our

00:40:16,400 --> 00:40:19,520
programming guide so this guide will

00:40:18,480 --> 00:40:23,040
cover

00:40:19,520 --> 00:40:25,119
python java and go sdks

00:40:23,040 --> 00:40:26,319
and if you are using open source runners

00:40:25,119 --> 00:40:29,359
such as spark and

00:40:26,319 --> 00:40:31,040
link and you are uh you are wondering

00:40:29,359 --> 00:40:33,200
what kind of functionalities these

00:40:31,040 --> 00:40:36,319
runners can support forcible dupont

00:40:33,200 --> 00:40:38,480
you can look into the capability matrix

00:40:36,319 --> 00:40:40,160
and as i mentioned for dynamic split

00:40:38,480 --> 00:40:42,880
it's very complicated

00:40:40,160 --> 00:40:45,440
and in for data flow we have a blog post

00:40:42,880 --> 00:40:47,920
to explain how this happens on data flow

00:40:45,440 --> 00:40:48,720
it's very interesting and helpful and if

00:40:47,920 --> 00:40:50,880
you're interested

00:40:48,720 --> 00:40:52,720
you can look into this one for more

00:40:50,880 --> 00:40:56,240
details about the mixed split

00:40:52,720 --> 00:40:57,119
and for bing api we also have a

00:40:56,240 --> 00:40:59,200
centralized

00:40:57,119 --> 00:41:01,680
documentation to see what kind of

00:40:59,200 --> 00:41:02,240
execution models we want to have and all

00:41:01,680 --> 00:41:04,240
the

00:41:02,240 --> 00:41:05,359
different aspects for these models we

00:41:04,240 --> 00:41:07,760
have and

00:41:05,359 --> 00:41:09,839
if you decide to build your own source i

00:41:07,760 --> 00:41:12,800
would recommend you look into our bing

00:41:09,839 --> 00:41:14,160
io guide first to get a sense of how to

00:41:12,800 --> 00:41:17,599
build a handle things

00:41:14,160 --> 00:41:19,760
and the things you need to prepare and

00:41:17,599 --> 00:41:20,880
if you are interested in how to

00:41:19,760 --> 00:41:23,119
interested in knowing

00:41:20,880 --> 00:41:24,000
how these ideas got generated either

00:41:23,119 --> 00:41:26,319
first place

00:41:24,000 --> 00:41:27,920
i would recommend you can look into our

00:41:26,319 --> 00:41:30,160
uh design doc website

00:41:27,920 --> 00:41:31,359
it contains all of their docs we have so

00:41:30,160 --> 00:41:32,800
far and

00:41:31,359 --> 00:41:34,640
there are many interesting reading

00:41:32,800 --> 00:41:36,960
materials there and

00:41:34,640 --> 00:41:38,000
finally if you are decide to contribute

00:41:36,960 --> 00:41:40,880
to the bing

00:41:38,000 --> 00:41:42,720
or or bring any your source or any your

00:41:40,880 --> 00:41:44,640
code into bing

00:41:42,720 --> 00:41:46,640
the first thing i would recommend is to

00:41:44,640 --> 00:41:48,720
look into our contribution guide

00:41:46,640 --> 00:41:50,720
it includes all of the instructions

00:41:48,720 --> 00:41:54,160
ideas on how to contribute

00:41:50,720 --> 00:41:55,440
to our bim community and finally thanks

00:41:54,160 --> 00:41:58,720
for your interest

00:41:55,440 --> 00:42:03,839
and uh thanks for thanks for your time

00:41:58,720 --> 00:42:03,839
and welcome to bing community

00:42:12,720 --> 00:42:16,400
thanks for you on spillable do fun

00:42:14,720 --> 00:42:16,880
sounds like a great improvement on the

00:42:16,400 --> 00:42:18,400
model

00:42:16,880 --> 00:42:20,160
it's neat that if you can define a way

00:42:18,400 --> 00:42:21,920
to restrict and split your data

00:42:20,160 --> 00:42:23,599
beam can make it work dynamically so

00:42:21,920 --> 00:42:24,160
runners can scale your pipeline as

00:42:23,599 --> 00:42:26,800
needed

00:42:24,160 --> 00:42:28,560
what do you like about it sam well i

00:42:26,800 --> 00:42:28,880
think like the main point that i like is

00:42:28,560 --> 00:42:30,960
that

00:42:28,880 --> 00:42:32,960
it unifies all the different sources to

00:42:30,960 --> 00:42:34,800
work under a single abstraction

00:42:32,960 --> 00:42:36,240
um right this will help to increase the

00:42:34,800 --> 00:42:39,359
beam source repertoire

00:42:36,240 --> 00:42:41,520
because like i said previously um

00:42:39,359 --> 00:42:43,520
the the custom source api was different

00:42:41,520 --> 00:42:44,079
for python was different for java and it

00:42:43,520 --> 00:42:46,640
led to

00:42:44,079 --> 00:42:48,079
all sorts of weird interactions and data

00:42:46,640 --> 00:42:50,720
flowing into the back end

00:42:48,079 --> 00:42:52,319
kind of in general so this by using the

00:42:50,720 --> 00:42:55,520
splittable do fund it should

00:42:52,319 --> 00:42:58,640
kind of help to harden the sources and

00:42:55,520 --> 00:43:01,760
sinks and to make them faster overall

00:42:58,640 --> 00:43:02,480
that's a very good point uh for for you

00:43:01,760 --> 00:43:04,319
in the audience

00:43:02,480 --> 00:43:05,680
don't forget you can ask questions in

00:43:04,319 --> 00:43:08,640
the live q a forum

00:43:05,680 --> 00:43:10,640
and get responses from our speakers next

00:43:08,640 --> 00:43:27,839
we'll hear from alex from aqualon

00:43:10,640 --> 00:43:27,839
about how they use dataflow templates

00:43:40,400 --> 00:43:45,839
welcome to data flow templates today

00:43:43,520 --> 00:43:47,680
i'm going to share an example use case

00:43:45,839 --> 00:43:50,640
for data flow templates

00:43:47,680 --> 00:43:51,520
how to create and execute data flow flex

00:43:50,640 --> 00:43:53,839
templates

00:43:51,520 --> 00:43:54,880
and how to get started contributing

00:43:53,839 --> 00:43:57,920
templates to

00:43:54,880 --> 00:44:00,560
open source community my name is

00:43:57,920 --> 00:44:01,520
alex casalabo i'm program manager at

00:44:00,560 --> 00:44:03,839
aqualone

00:44:01,520 --> 00:44:05,440
a custom software engineering company

00:44:03,839 --> 00:44:08,079
google cloud partner

00:44:05,440 --> 00:44:10,240
headquartered in seattle washington with

00:44:08,079 --> 00:44:11,280
engineering offices in europe and north

00:44:10,240 --> 00:44:13,680
america

00:44:11,280 --> 00:44:15,200
over the last 20 years we helped over

00:44:13,680 --> 00:44:18,800
100 clients to build

00:44:15,200 --> 00:44:22,720
software data analytics machine learning

00:44:18,800 --> 00:44:25,440
enterprise and mobile apps

00:44:22,720 --> 00:44:27,920
i will share an example use case where

00:44:25,440 --> 00:44:29,280
data flow was a great solution for data

00:44:27,920 --> 00:44:32,800
transformation

00:44:29,280 --> 00:44:35,760
and custom template helped to integrate

00:44:32,800 --> 00:44:37,200
external data protection service into

00:44:35,760 --> 00:44:39,920
pipeline

00:44:37,200 --> 00:44:41,280
a customer use case was to ingest large

00:44:39,920 --> 00:44:43,680
volumes of data

00:44:41,280 --> 00:44:45,599
that included sensitive data like credit

00:44:43,680 --> 00:44:47,839
card or pii

00:44:45,599 --> 00:44:50,720
and a third-party data protection

00:44:47,839 --> 00:44:53,200
service was used for tokenization

00:44:50,720 --> 00:44:54,240
converting data elements like credit

00:44:53,200 --> 00:44:58,480
card number

00:44:54,240 --> 00:45:01,200
into non-sensitive token representations

00:44:58,480 --> 00:45:03,440
that can be stored safely based on

00:45:01,200 --> 00:45:05,200
access role a user may see a token

00:45:03,440 --> 00:45:07,839
representation of data

00:45:05,200 --> 00:45:08,640
a partially masked data like last four

00:45:07,839 --> 00:45:11,920
digits of

00:45:08,640 --> 00:45:12,480
credit card number or an original credit

00:45:11,920 --> 00:45:15,760
card

00:45:12,480 --> 00:45:18,319
number data flow provides an

00:45:15,760 --> 00:45:20,880
auto scaling fully managed high

00:45:18,319 --> 00:45:23,280
performance data transformation service

00:45:20,880 --> 00:45:26,000
with consumption based pricing that was

00:45:23,280 --> 00:45:27,200
perfect fit for data processing

00:45:26,000 --> 00:45:30,000
challenge was that

00:45:27,200 --> 00:45:31,280
out of box there was no support for

00:45:30,000 --> 00:45:36,480
external calls to

00:45:31,280 --> 00:45:39,520
third-party service to tokenize data

00:45:36,480 --> 00:45:42,720
a custom pipeline has to be built

00:45:39,520 --> 00:45:45,520
to support this and

00:45:42,720 --> 00:45:46,880
different input and output options need

00:45:45,520 --> 00:45:50,240
to be supported

00:45:46,880 --> 00:45:53,359
on google cloud storage we had json csvs

00:45:50,240 --> 00:45:56,000
avro we had data from pubsub

00:45:53,359 --> 00:45:56,560
and also pipeline had to support

00:45:56,000 --> 00:46:00,640
different

00:45:56,560 --> 00:46:04,240
output things bigquery bigtable gcs

00:46:00,640 --> 00:46:04,800
so this variety of options on the i o

00:46:04,240 --> 00:46:08,319
side

00:46:04,800 --> 00:46:09,599
that required a same pipeline to do same

00:46:08,319 --> 00:46:11,680
processing

00:46:09,599 --> 00:46:14,640
templatizing pipeline allowed to

00:46:11,680 --> 00:46:16,640
implement pipeline just once and support

00:46:14,640 --> 00:46:19,760
all required i o options

00:46:16,640 --> 00:46:20,880
using template parameters to implement

00:46:19,760 --> 00:46:23,599
tokenization

00:46:20,880 --> 00:46:25,200
we created data flow pipeline with

00:46:23,599 --> 00:46:27,680
processing logic to call

00:46:25,200 --> 00:46:29,520
external rest service we had several

00:46:27,680 --> 00:46:31,920
challenges to solve

00:46:29,520 --> 00:46:34,400
including optimizing communication with

00:46:31,920 --> 00:46:37,040
rest service for performance

00:46:34,400 --> 00:46:37,920
stateful do fund bim feature helped us

00:46:37,040 --> 00:46:40,880
to buffer

00:46:37,920 --> 00:46:42,000
and batch data into bundles that worker

00:46:40,880 --> 00:46:44,000
can send in one

00:46:42,000 --> 00:46:47,119
request and they will maintain order

00:46:44,000 --> 00:46:50,319
enough data in this patches

00:46:47,119 --> 00:46:52,880
then we package pipeline into template

00:46:50,319 --> 00:46:54,800
and that enables us to support different

00:46:52,880 --> 00:46:57,520
input output sources

00:46:54,800 --> 00:46:58,800
execution parameters and shared template

00:46:57,520 --> 00:47:01,200
easily

00:46:58,800 --> 00:47:02,000
next we will look how to get from a

00:47:01,200 --> 00:47:05,200
pipeline

00:47:02,000 --> 00:47:07,920
to data flow template

00:47:05,200 --> 00:47:09,680
what is a pipeline a pipeline is a flow

00:47:07,920 --> 00:47:13,359
of operations in b

00:47:09,680 --> 00:47:15,920
in simplest way it starts with io read

00:47:13,359 --> 00:47:16,800
data transformations are applied and

00:47:15,920 --> 00:47:20,720
finally

00:47:16,800 --> 00:47:24,720
io write transform is applied to output

00:47:20,720 --> 00:47:27,839
data into output sync

00:47:24,720 --> 00:47:31,920
data flow templates is a way to package

00:47:27,839 --> 00:47:34,800
and stage pipelines it has two phases

00:47:31,920 --> 00:47:35,680
template construction is implementing

00:47:34,800 --> 00:47:38,240
pipeline

00:47:35,680 --> 00:47:38,960
compiling it into execution graph and

00:47:38,240 --> 00:47:42,319
staging

00:47:38,960 --> 00:47:45,040
on google cloud execution phase

00:47:42,319 --> 00:47:46,400
allows to run templates using google

00:47:45,040 --> 00:47:49,680
cloud console

00:47:46,400 --> 00:47:52,160
gcloud tool or rest api without

00:47:49,680 --> 00:47:53,200
development environment without

00:47:52,160 --> 00:47:56,800
configuring

00:47:53,200 --> 00:47:59,680
associated dependencies on your computer

00:47:56,800 --> 00:48:00,720
runtime parameter allows to customize

00:47:59,680 --> 00:48:03,760
execution

00:48:00,720 --> 00:48:06,400
of the pipeline running your pipeline

00:48:03,760 --> 00:48:10,480
does not require you to recompile code

00:48:06,400 --> 00:48:12,319
every time there are two template types

00:48:10,480 --> 00:48:14,880
classic templates are staged as

00:48:12,319 --> 00:48:16,559
permanently fixed execution graphs on

00:48:14,880 --> 00:48:19,359
google cloud storage

00:48:16,559 --> 00:48:21,359
classic templates require bim pipeline

00:48:19,359 --> 00:48:24,559
code modification to implement

00:48:21,359 --> 00:48:26,960
value provider interface to defer

00:48:24,559 --> 00:48:27,680
reading of variables until template is

00:48:26,960 --> 00:48:30,559
run

00:48:27,680 --> 00:48:32,400
for example name of pub sub subscription

00:48:30,559 --> 00:48:35,760
that user might want to pass

00:48:32,400 --> 00:48:38,000
at execution a minor changing option

00:48:35,760 --> 00:48:39,040
will require recompiling template by

00:48:38,000 --> 00:48:41,359
developers

00:48:39,040 --> 00:48:43,680
for example to change to google cloud

00:48:41,359 --> 00:48:46,960
storage instead of pubsub

00:48:43,680 --> 00:48:50,240
flex templates package an existing beam

00:48:46,960 --> 00:48:53,680
pipeline as a docker image on your

00:48:50,240 --> 00:48:56,800
project's container registry and create

00:48:53,680 --> 00:48:59,200
template specification method data on

00:48:56,800 --> 00:49:02,079
google cloud storage bucket

00:48:59,200 --> 00:49:05,040
to execute flex template then just refer

00:49:02,079 --> 00:49:07,599
to the template spec file

00:49:05,040 --> 00:49:09,520
benefits of flex templates are dynamic

00:49:07,599 --> 00:49:13,359
execution graph

00:49:09,520 --> 00:49:16,319
that is constructed and validated at job

00:49:13,359 --> 00:49:18,720
launch time based on final parameters

00:49:16,319 --> 00:49:21,920
that user supplied

00:49:18,720 --> 00:49:23,680
flex templates eliminated need for value

00:49:21,920 --> 00:49:27,680
provider interface

00:49:23,680 --> 00:49:27,680
improving developer experience

00:49:28,079 --> 00:49:32,000
creating data flow flex template has

00:49:30,800 --> 00:49:35,599
three steps

00:49:32,000 --> 00:49:38,559
implement data pipeline create metadata

00:49:35,599 --> 00:49:39,599
and build the template pipeline for flex

00:49:38,559 --> 00:49:43,359
templates

00:49:39,599 --> 00:49:46,559
can be implemented in java or python

00:49:43,359 --> 00:49:50,559
i'm going to switch to my demo

00:49:46,559 --> 00:49:51,760
example that shows our tokenization

00:49:50,559 --> 00:49:54,559
pipeline

00:49:51,760 --> 00:49:56,559
tokenization pipeline was implemented in

00:49:54,559 --> 00:49:59,200
java

00:49:56,559 --> 00:50:00,400
bim pipeline in java has main method as

00:49:59,200 --> 00:50:03,760
an entry point

00:50:00,400 --> 00:50:04,480
for pipeline execution that often

00:50:03,760 --> 00:50:07,680
processes

00:50:04,480 --> 00:50:10,880
options and calls pipeline

00:50:07,680 --> 00:50:13,359
run method that implements

00:50:10,880 --> 00:50:15,119
transforms and other business logic for

00:50:13,359 --> 00:50:18,319
the pipeline

00:50:15,119 --> 00:50:21,359
next beam pipeline

00:50:18,319 --> 00:50:23,920
is compiled into uber file

00:50:21,359 --> 00:50:26,240
that contains all dependencies using

00:50:23,920 --> 00:50:29,520
maven or cradle

00:50:26,240 --> 00:50:33,040
and next step to

00:50:29,520 --> 00:50:36,880
get from pipeline to template

00:50:33,040 --> 00:50:40,079
is using gcloud dataflow flex template

00:50:36,880 --> 00:50:43,520
build command that will package

00:50:40,079 --> 00:50:46,559
that will create docker container

00:50:43,520 --> 00:50:48,880
metadata file and stage them in google

00:50:46,559 --> 00:50:50,960
cloud

00:50:48,880 --> 00:50:52,640
i'm going to switch to my demo

00:50:50,960 --> 00:50:56,240
environment and

00:50:52,640 --> 00:50:57,280
start this process here i'm going to

00:50:56,240 --> 00:51:02,160
start

00:50:57,280 --> 00:51:02,160
maven maven compile process

00:51:02,240 --> 00:51:08,800
and just to verify that it started

00:51:05,520 --> 00:51:12,640
it's going to take a little while to

00:51:08,800 --> 00:51:17,440
complete and while it's compiling

00:51:12,640 --> 00:51:22,800
i'm going to switch to metadata file

00:51:17,440 --> 00:51:25,680
and provide an overview what it is

00:51:22,800 --> 00:51:26,240
template metadata file provides template

00:51:25,680 --> 00:51:29,920
name

00:51:26,240 --> 00:51:33,280
description and parameters

00:51:29,920 --> 00:51:36,400
that user can specify when creating

00:51:33,280 --> 00:51:39,520
job from template in web console

00:51:36,400 --> 00:51:42,640
in gcloud or rest api

00:51:39,520 --> 00:51:46,640
template parameters have name

00:51:42,640 --> 00:51:51,280
label help text ram type properties

00:51:46,640 --> 00:51:54,000
and also can be optional or required

00:51:51,280 --> 00:51:54,800
and support regular expression

00:51:54,000 --> 00:51:58,000
validation

00:51:54,800 --> 00:51:58,960
of user input that comes really user

00:51:58,000 --> 00:52:02,839
friendly

00:51:58,960 --> 00:52:05,359
when a job is created in web

00:52:02,839 --> 00:52:09,200
console maven build

00:52:05,359 --> 00:52:12,800
just completed and we can switch

00:52:09,200 --> 00:52:18,480
to the second step of the

00:52:12,800 --> 00:52:20,720
template build

00:52:18,480 --> 00:52:21,599
the second step is running gcloud

00:52:20,720 --> 00:52:25,359
dataflow

00:52:21,599 --> 00:52:28,720
flex template build i will start it

00:52:25,359 --> 00:52:31,280
right away and will explain what i have

00:52:28,720 --> 00:52:31,280
on the screen

00:52:31,520 --> 00:52:38,000
i use environment variables to help

00:52:34,559 --> 00:52:39,280
organize uh organize my commands and

00:52:38,000 --> 00:52:42,880
scripts

00:52:39,280 --> 00:52:43,920
so i have project id uh storage and

00:52:42,880 --> 00:52:48,160
template

00:52:43,920 --> 00:52:51,839
path um google container registry

00:52:48,160 --> 00:52:54,480
image path and base

00:52:51,839 --> 00:52:56,720
container image as an environment

00:52:54,480 --> 00:52:59,200
variable

00:52:56,720 --> 00:53:00,319
when i start the flex template build

00:52:59,200 --> 00:53:03,359
command

00:53:00,319 --> 00:53:07,280
i provide template path

00:53:03,359 --> 00:53:10,640
where metadata will be stored

00:53:07,280 --> 00:53:14,319
i specify container image path and

00:53:10,640 --> 00:53:18,240
sdk language that that bim pipeline

00:53:14,319 --> 00:53:21,440
is using i also specify

00:53:18,240 --> 00:53:23,359
base container image one of the google

00:53:21,440 --> 00:53:28,160
provided

00:53:23,359 --> 00:53:32,000
images for data flow containers

00:53:28,160 --> 00:53:34,960
lastly i specify some source files

00:53:32,000 --> 00:53:35,760
source for metadata source for uber

00:53:34,960 --> 00:53:38,800
agile

00:53:35,760 --> 00:53:40,319
jar file that we just completed in

00:53:38,800 --> 00:53:44,000
previous step

00:53:40,319 --> 00:53:44,800
and specify main class that will be

00:53:44,000 --> 00:53:47,760
called

00:53:44,800 --> 00:53:50,319
when dataflow creates job from this

00:53:47,760 --> 00:53:50,319
template

00:53:52,559 --> 00:53:59,520
i'm going to pause here

00:53:55,680 --> 00:53:59,520
for this command to complete

00:54:01,760 --> 00:54:08,640
flex template build just completed

00:54:05,119 --> 00:54:11,839
i'm going to navigate to web console

00:54:08,640 --> 00:54:15,359
to verify container registry

00:54:11,839 --> 00:54:15,359
and metadata file

00:54:16,160 --> 00:54:20,240
in my project container registry i just

00:54:19,680 --> 00:54:25,599
go

00:54:20,240 --> 00:54:30,240
inside of the path and

00:54:25,599 --> 00:54:30,240
the image has just been uploaded

00:54:31,760 --> 00:54:40,799
next i'm switching to template path

00:54:35,920 --> 00:54:44,160
that was used for metadata file

00:54:40,799 --> 00:54:47,680
and validating that tokenization demo

00:54:44,160 --> 00:54:50,799
json metadata file has been created

00:54:47,680 --> 00:54:54,400
and completed we can go in and

00:54:50,799 --> 00:54:58,480
actually check the json it has

00:54:54,400 --> 00:55:01,760
a reference to docker image metadata

00:54:58,480 --> 00:55:05,440
and down at the bottom it has

00:55:01,760 --> 00:55:08,319
sdk info that specifies

00:55:05,440 --> 00:55:08,319
template language

00:55:08,960 --> 00:55:13,440
we are ready to run the template in data

00:55:11,760 --> 00:55:17,119
flow

00:55:13,440 --> 00:55:21,839
i'm going to navigate to data flow and

00:55:17,119 --> 00:55:21,839
here create job from template

00:55:26,079 --> 00:55:29,920
in this drop-down i can select different

00:55:29,040 --> 00:55:32,000
templates

00:55:29,920 --> 00:55:36,720
and there's many templates that come

00:55:32,000 --> 00:55:38,799
from google with dataflow

00:55:36,720 --> 00:55:40,240
for this project we created custom

00:55:38,799 --> 00:55:43,440
template

00:55:40,240 --> 00:55:49,839
and i will select custom template

00:55:43,440 --> 00:55:49,839
and navigate to cloud storage

00:55:50,480 --> 00:55:55,520
to select template metadata file

00:55:58,559 --> 00:56:02,880
if you notice that description of the

00:56:01,040 --> 00:56:05,520
template

00:56:02,880 --> 00:56:07,520
that you can see here just changed to

00:56:05,520 --> 00:56:11,359
the template we created

00:56:07,520 --> 00:56:14,720
and now i will provide

00:56:11,359 --> 00:56:16,079
required parameter with json schema with

00:56:14,720 --> 00:56:19,359
data schema

00:56:16,079 --> 00:56:19,359
for my input

00:56:19,440 --> 00:56:22,720
and expand optional parameters to

00:56:21,920 --> 00:56:24,799
specify

00:56:22,720 --> 00:56:27,200
other templates that i will need for the

00:56:24,799 --> 00:56:27,200
demo

00:56:28,720 --> 00:56:32,400
path to input data

00:56:32,480 --> 00:56:36,079
format of the input files and i'm going

00:56:34,960 --> 00:56:39,520
to put

00:56:36,079 --> 00:56:44,480
json with a typo

00:56:39,520 --> 00:56:47,520
just to demonstrate how validation works

00:56:44,480 --> 00:56:51,359
it allows me to easily notice a typo

00:56:47,520 --> 00:56:54,400
go back and fix it

00:56:51,359 --> 00:56:57,040
and continue with other parameters for

00:56:54,400 --> 00:56:57,040
my demo

00:56:57,200 --> 00:57:06,640
uh rest endpoint for tokenization

00:57:01,760 --> 00:57:10,000
a mock service that we use and

00:57:06,640 --> 00:57:13,200
next parameter is payload configuration

00:57:10,000 --> 00:57:16,000
that defines which fields which data

00:57:13,200 --> 00:57:19,920
elements i want to tokenize

00:57:16,000 --> 00:57:19,920
template also supports

00:57:20,000 --> 00:57:26,400
that letter path that we can specify

00:57:23,119 --> 00:57:30,240
to store any any

00:57:26,400 --> 00:57:33,680
errors in data processing

00:57:30,240 --> 00:57:33,680
on google cloud storage

00:57:34,559 --> 00:57:41,200
and lastly i will use

00:57:38,160 --> 00:57:44,880
bigquery table as my output

00:57:41,200 --> 00:57:48,799
and we'll specify table

00:57:44,880 --> 00:57:51,920
here as a parameter when we scroll

00:57:48,799 --> 00:57:54,559
scroll down we see all the

00:57:51,920 --> 00:57:55,760
all the data flow parameters that all

00:57:54,559 --> 00:57:59,920
data flow

00:57:55,760 --> 00:58:02,799
jobs support we can also specify

00:57:59,920 --> 00:58:04,960
any of those parameters with a custom

00:58:02,799 --> 00:58:07,760
template

00:58:04,960 --> 00:58:08,720
i'm going to stick with defaults and now

00:58:07,760 --> 00:58:12,000
we'll click

00:58:08,720 --> 00:58:17,440
run the job to create

00:58:12,000 --> 00:58:17,440
data flow job from this custom template

00:58:18,240 --> 00:58:21,599
it will take a moment to analyze the job

00:58:20,880 --> 00:58:24,559
and create

00:58:21,599 --> 00:58:26,559
execution graph next the job will be

00:58:24,559 --> 00:58:28,799
staged and executed

00:58:26,559 --> 00:58:30,079
it might take few minutes and i will

00:58:28,799 --> 00:58:32,640
demo result

00:58:30,079 --> 00:58:35,119
of running this job that i did before

00:58:32,640 --> 00:58:35,119
the demo

00:58:35,440 --> 00:58:42,640
when job is created from custom template

00:58:39,280 --> 00:58:45,680
job execution graph will be constructed

00:58:42,640 --> 00:58:49,520
based on data processing in the pipeline

00:58:45,680 --> 00:58:52,640
that was implemented uh you can see

00:58:49,520 --> 00:58:53,520
dsg tokenization is one of the uh one of

00:58:52,640 --> 00:58:57,200
the

00:58:53,520 --> 00:59:00,319
transforms here and also

00:58:57,200 --> 00:59:04,079
we're gonna write successful transforms

00:59:00,319 --> 00:59:07,119
into our output and we have error

00:59:04,079 --> 00:59:09,280
handling transforms that will catch any

00:59:07,119 --> 00:59:12,400
errors

00:59:09,280 --> 00:59:16,160
when the job is run the output will be

00:59:12,400 --> 00:59:16,160
stored in bigquery table

00:59:17,200 --> 00:59:22,720
switching to bigquery i used

00:59:22,799 --> 00:59:30,000
demo json gcs to bigquery as a table

00:59:27,040 --> 00:59:30,000
for output

00:59:31,440 --> 00:59:38,960
the table was created by dataflow job

00:59:35,839 --> 00:59:42,319
and table schema matches

00:59:38,960 --> 00:59:44,720
input schema of the input data

00:59:42,319 --> 00:59:48,319
when i click on preview i will see a

00:59:44,720 --> 00:59:51,599
snippet of the output data

00:59:48,319 --> 00:59:52,480
here you can see that ssn and last name

00:59:51,599 --> 00:59:56,240
columns

00:59:52,480 --> 00:59:59,440
have been tokenized and some randomly

00:59:56,240 --> 01:00:02,559
looking data is stored

00:59:59,440 --> 01:00:06,079
in the table

01:00:02,559 --> 01:00:10,079
this is exactly what we wanted to see

01:00:06,079 --> 01:00:14,160
as a result now

01:00:10,079 --> 01:00:15,040
we built andrea dataflow template you

01:00:14,160 --> 01:00:17,599
can build

01:00:15,040 --> 01:00:18,400
custom data flow flex templates for your

01:00:17,599 --> 01:00:21,680
projects

01:00:18,400 --> 01:00:26,319
and easily share them with users

01:00:21,680 --> 01:00:28,640
to create data flow jobs that they need

01:00:26,319 --> 01:00:30,079
you might also be thinking how do i

01:00:28,640 --> 01:00:33,280
share template

01:00:30,079 --> 01:00:35,440
with open source community

01:00:33,280 --> 01:00:37,280
to share templates with community you

01:00:35,440 --> 01:00:39,040
might contribute them to data flow

01:00:37,280 --> 01:00:42,160
templates repo

01:00:39,040 --> 01:00:45,040
it's google cloud repository

01:00:42,160 --> 01:00:46,400
and templates from google and other open

01:00:45,040 --> 01:00:49,119
source contributors

01:00:46,400 --> 01:00:51,119
are stored there it's easy to get

01:00:49,119 --> 01:00:54,240
started with contributions

01:00:51,119 --> 01:00:57,680
fork repository to develop your template

01:00:54,240 --> 01:01:00,079
develop pipeline following style guides

01:00:57,680 --> 01:01:02,799
and best practices

01:01:00,079 --> 01:01:04,160
provided by google sign contributor

01:01:02,799 --> 01:01:06,319
license agreement

01:01:04,160 --> 01:01:07,280
and submit template pull request to

01:01:06,319 --> 01:01:10,000
merge it

01:01:07,280 --> 01:01:10,880
pull request will go through code review

01:01:10,000 --> 01:01:14,880
and will

01:01:10,880 --> 01:01:16,960
get merged with data flow templates

01:01:14,880 --> 01:01:18,160
summarizing today's session key

01:01:16,960 --> 01:01:20,400
takeaways are

01:01:18,160 --> 01:01:21,359
data flow flex templates are

01:01:20,400 --> 01:01:24,240
containerized

01:01:21,359 --> 01:01:25,119
packages with all necessary environment

01:01:24,240 --> 01:01:28,079
to run

01:01:25,119 --> 01:01:28,720
data flow pipeline so users can create

01:01:28,079 --> 01:01:32,079
job from

01:01:28,720 --> 01:01:34,400
template in cloud console gcloud command

01:01:32,079 --> 01:01:37,599
line tool or rest api

01:01:34,400 --> 01:01:38,400
flex templates use dynamic execution

01:01:37,599 --> 01:01:42,000
graph

01:01:38,400 --> 01:01:44,640
creating and validating job

01:01:42,000 --> 01:01:45,599
execution graph with final user

01:01:44,640 --> 01:01:49,119
parameters

01:01:45,599 --> 01:01:49,440
when job is launched flex templates are

01:01:49,119 --> 01:01:52,319
more

01:01:49,440 --> 01:01:52,720
portable easier to support with input

01:01:52,319 --> 01:01:56,880
and

01:01:52,720 --> 01:01:57,839
output options and we also learned how

01:01:56,880 --> 01:02:02,160
to create

01:01:57,839 --> 01:02:04,720
build and run data flow flex template

01:02:02,160 --> 01:02:05,440
thank you for joining data flow template

01:02:04,720 --> 01:02:07,680
session

01:02:05,440 --> 01:02:09,599
reach out to us at aqualon if you are

01:02:07,680 --> 01:02:12,240
interested in building google cloud

01:02:09,599 --> 01:02:21,839
solution with dataflow or apache beam

01:02:12,240 --> 01:02:21,839
for your use case

01:02:25,920 --> 01:02:29,280
thanks alex that was a great overview

01:02:27,760 --> 01:02:31,280
and introduction you know

01:02:29,280 --> 01:02:33,280
before templates it was pretty difficult

01:02:31,280 --> 01:02:33,760
to be able to productionize data flows

01:02:33,280 --> 01:02:36,160
right

01:02:33,760 --> 01:02:38,319
you had to manually have your own

01:02:36,160 --> 01:02:38,960
environments for both python and java

01:02:38,319 --> 01:02:40,960
and then

01:02:38,960 --> 01:02:43,599
kind of compile those independently and

01:02:40,960 --> 01:02:45,760
have some sort of workflow in order to

01:02:43,599 --> 01:02:47,119
work with the data flow api to submit

01:02:45,760 --> 01:02:48,960
them over time

01:02:47,119 --> 01:02:50,880
but with templates everything is pretty

01:02:48,960 --> 01:02:52,640
self-contained within the ui

01:02:50,880 --> 01:02:55,359
where you can just submit your code to

01:02:52,640 --> 01:02:57,520
the service and then it'll automatically

01:02:55,359 --> 01:02:59,440
wrap things up for you into this flex

01:02:57,520 --> 01:03:00,160
template we're even able to schedule

01:02:59,440 --> 01:03:03,200
things

01:03:00,160 --> 01:03:06,079
at some distinct intervals over time

01:03:03,200 --> 01:03:06,960
what did you get out of it robert well

01:03:06,079 --> 01:03:08,240
the uh

01:03:06,960 --> 01:03:09,920
ability to bring your own docker

01:03:08,240 --> 01:03:13,039
containers with the flex templates

01:03:09,920 --> 01:03:16,160
sounds super useful

01:03:13,039 --> 01:03:19,599
yeah that is very true um just

01:03:16,160 --> 01:03:21,920
i know a lot of a lot of customers um

01:03:19,599 --> 01:03:23,680
that we've seen they like to customize

01:03:21,920 --> 01:03:27,119
their own docker containers maybe use

01:03:23,680 --> 01:03:29,119
different operating systems maybe

01:03:27,119 --> 01:03:31,119
don't like necessarily like the update

01:03:29,119 --> 01:03:33,039
cadence for apache beam and want to do

01:03:31,119 --> 01:03:35,599
more frequent updates to the libraries

01:03:33,039 --> 01:03:37,280
so it allows a lot of flexibility

01:03:35,599 --> 01:03:39,280
so next we'll hear from william sykes

01:03:37,280 --> 01:03:45,839
about building beam pipelines with cdc

01:03:39,280 --> 01:03:45,839
and the bcm

01:04:10,799 --> 01:04:15,119
hey beam community will from wiseline

01:04:12,960 --> 01:04:18,960
here to talk about apache beam

01:04:15,119 --> 01:04:20,799
change data capture and p transforms

01:04:18,960 --> 01:04:22,640
over the last two years wiseline has

01:04:20,799 --> 01:04:24,880
been involved in several apache beam

01:04:22,640 --> 01:04:26,960
related initiatives

01:04:24,880 --> 01:04:28,079
during the apache beam user experience

01:04:26,960 --> 01:04:30,400
research project

01:04:28,079 --> 01:04:32,480
wiseline identified pain points learning

01:04:30,400 --> 01:04:33,200
gaps and opportunities to improve the

01:04:32,480 --> 01:04:36,720
adoption

01:04:33,200 --> 01:04:37,520
of the apache beam platform wiseline was

01:04:36,720 --> 01:04:39,680
able to provide

01:04:37,520 --> 01:04:43,359
a comprehensive list of actions to

01:04:39,680 --> 01:04:46,160
improve the apache beam platform

01:04:43,359 --> 01:04:47,359
the cdc and dibysium project this is the

01:04:46,160 --> 01:04:50,400
focus of today's

01:04:47,359 --> 01:04:52,480
topic this project was

01:04:50,400 --> 01:04:55,280
executed in six weeks and the final

01:04:52,480 --> 01:04:58,079
product was merged in the community

01:04:55,280 --> 01:05:00,480
we're also actively involved in beam sdk

01:04:58,079 --> 01:05:02,400
and infrastructure work

01:05:00,480 --> 01:05:04,400
wiseline is working on some enhancements

01:05:02,400 --> 01:05:06,319
to the core beam sdks

01:05:04,400 --> 01:05:07,760
these contributions are crucial in

01:05:06,319 --> 01:05:10,799
easing the adoption

01:05:07,760 --> 01:05:13,599
of apache beam as a solution now a

01:05:10,799 --> 01:05:16,720
little bit about change data capture

01:05:13,599 --> 01:05:18,799
change data capture or cdc is a term

01:05:16,720 --> 01:05:20,400
for a system that monitors and captures

01:05:18,799 --> 01:05:23,520
the changes in data so that other

01:05:20,400 --> 01:05:25,680
software can respond to those changes

01:05:23,520 --> 01:05:26,799
as some organizations have grown they

01:05:25,680 --> 01:05:28,559
have the need to pull

01:05:26,799 --> 01:05:30,799
data from a single source and distribute

01:05:28,559 --> 01:05:32,799
it to many different applications

01:05:30,799 --> 01:05:34,400
for example identifying changes in the

01:05:32,799 --> 01:05:38,000
database and sending them

01:05:34,400 --> 01:05:40,000
to both an erp and crm system

01:05:38,000 --> 01:05:42,480
traditionally data warehouses required

01:05:40,000 --> 01:05:45,359
custom cdc support to manage sync

01:05:42,480 --> 01:05:47,200
with the upstream oltp databases these

01:05:45,359 --> 01:05:49,119
solutions were custom and cumbersome to

01:05:47,200 --> 01:05:51,200
manage

01:05:49,119 --> 01:05:53,680
two primary tribes of cdc include

01:05:51,200 --> 01:05:55,119
trigger and log base

01:05:53,680 --> 01:05:57,280
and there are a variety of commercial

01:05:55,119 --> 01:05:59,280
cdc vendors available in the market

01:05:57,280 --> 01:06:02,160
today

01:05:59,280 --> 01:06:02,480
dibizium the open source offering is a

01:06:02,160 --> 01:06:04,559
is

01:06:02,480 --> 01:06:06,079
essentially a modern distributed change

01:06:04,559 --> 01:06:08,160
data capture platform

01:06:06,079 --> 01:06:09,839
that supports monitoring a variety of

01:06:08,160 --> 01:06:11,920
database systems

01:06:09,839 --> 01:06:12,880
dibizium is an open source project that

01:06:11,920 --> 01:06:15,119
provides a

01:06:12,880 --> 01:06:17,200
low latency data streaming platform for

01:06:15,119 --> 01:06:19,359
change data capture

01:06:17,200 --> 01:06:20,960
you set up and configure dibyzium to

01:06:19,359 --> 01:06:22,559
monitor your databases

01:06:20,960 --> 01:06:24,640
and then your applications consume

01:06:22,559 --> 01:06:26,079
events for each row level change made to

01:06:24,640 --> 01:06:27,920
the database

01:06:26,079 --> 01:06:29,520
only committed changes are visible so

01:06:27,920 --> 01:06:31,280
your application doesn't have to worry

01:06:29,520 --> 01:06:33,039
about transactions or changes that are

01:06:31,280 --> 01:06:35,359
rolled back

01:06:33,039 --> 01:06:37,359
debesium provides a single model for all

01:06:35,359 --> 01:06:39,359
change events so your application does

01:06:37,359 --> 01:06:41,119
not have to worry about the intricacies

01:06:39,359 --> 01:06:42,960
of each kind of database management

01:06:41,119 --> 01:06:45,680
system

01:06:42,960 --> 01:06:47,520
additionally since debesium records the

01:06:45,680 --> 01:06:48,960
history of data changes and durable

01:06:47,520 --> 01:06:50,559
replicated logs

01:06:48,960 --> 01:06:53,039
your application can be stopped and

01:06:50,559 --> 01:06:54,640
restarted at any time

01:06:53,039 --> 01:06:56,319
it will be able to consume all of the

01:06:54,640 --> 01:06:57,119
events it missed while it was not

01:06:56,319 --> 01:06:58,960
running

01:06:57,119 --> 01:07:00,960
ensuring all data are processed

01:06:58,960 --> 01:07:04,400
correctly and completely

01:07:00,960 --> 01:07:07,520
as of now mysql postgresql sql server

01:07:04,400 --> 01:07:08,799
and db2 are all capable of using cdc

01:07:07,520 --> 01:07:10,799
with beam

01:07:08,799 --> 01:07:13,200
now i'm going to talk a bit about the

01:07:10,799 --> 01:07:15,359
motivation for this work

01:07:13,200 --> 01:07:17,760
the primary motivation for this project

01:07:15,359 --> 01:07:18,960
was to add cdc functionality to apache

01:07:17,760 --> 01:07:20,880
beam

01:07:18,960 --> 01:07:22,000
the commercial offerings are expensive

01:07:20,880 --> 01:07:23,520
and are often closed

01:07:22,000 --> 01:07:25,280
restricting the developers and data

01:07:23,520 --> 01:07:29,680
engineers from building creative and

01:07:25,280 --> 01:07:32,079
efficient systems before the debesium io

01:07:29,680 --> 01:07:33,119
beam users that wanted to consume change

01:07:32,079 --> 01:07:35,760
data streams

01:07:33,119 --> 01:07:38,480
from databases needed to take on one of

01:07:35,760 --> 01:07:42,000
the following architectural dependencies

01:07:38,480 --> 01:07:44,079
kafka by deploying to bezium pub sub

01:07:42,000 --> 01:07:45,440
and data catalog by deploying a custom

01:07:44,079 --> 01:07:47,839
connector

01:07:45,440 --> 01:07:48,720
or other commercial connectors by

01:07:47,839 --> 01:07:50,559
developing a p

01:07:48,720 --> 01:07:52,640
transform that provides this

01:07:50,559 --> 01:07:53,680
functionality directly in beam without

01:07:52,640 --> 01:07:55,680
any additional

01:07:53,680 --> 01:07:57,280
architectural requirements like like

01:07:55,680 --> 01:07:59,440
kafka or pub sub

01:07:57,280 --> 01:08:01,760
users can get started consuming change

01:07:59,440 --> 01:08:04,240
data streams from their database

01:08:01,760 --> 01:08:04,799
here you can see how using the debesium

01:08:04,240 --> 01:08:08,559
i o

01:08:04,799 --> 01:08:09,839
reduces complexity in the architecture

01:08:08,559 --> 01:08:11,920
it's important to note that this

01:08:09,839 --> 01:08:15,520
architecture can run in parallel

01:08:11,920 --> 01:08:17,679
with your existing kafka implementations

01:08:15,520 --> 01:08:19,600
here are some details about the solution

01:08:17,679 --> 01:08:22,319
wiseline develop

01:08:19,600 --> 01:08:23,839
dibysium connectors are implemented as a

01:08:22,319 --> 01:08:27,920
kafka source connector

01:08:23,839 --> 01:08:30,159
that splits into a single source task

01:08:27,920 --> 01:08:32,480
wiseline developed a debesium i o

01:08:30,159 --> 01:08:35,279
transformer as a splittable doe fin

01:08:32,480 --> 01:08:35,920
that reads from a variety of sql sources

01:08:35,279 --> 01:08:38,319
using

01:08:35,920 --> 01:08:39,520
kafka source connector and converts the

01:08:38,319 --> 01:08:43,359
read source record

01:08:39,520 --> 01:08:45,199
into a p collection in json format

01:08:43,359 --> 01:08:48,159
using the existing kafka source

01:08:45,199 --> 01:08:48,719
connector dibysium io spreads out cdc

01:08:48,159 --> 01:08:51,440
data

01:08:48,719 --> 01:08:53,040
in a kafka record format or source

01:08:51,440 --> 01:08:54,719
record

01:08:53,040 --> 01:08:57,120
these records will be processed by the

01:08:54,719 --> 01:08:57,520
new splittable dofn transformer to place

01:08:57,120 --> 01:09:00,560
them

01:08:57,520 --> 01:09:03,759
in a beam friendly format json

01:09:00,560 --> 01:09:05,679
inside a p collection this p collection

01:09:03,759 --> 01:09:09,440
will be used in the subsequent p

01:09:05,679 --> 01:09:11,120
transform now a bit about the flow of

01:09:09,440 --> 01:09:13,120
events

01:09:11,120 --> 01:09:15,279
beam lets you easily create data

01:09:13,120 --> 01:09:18,640
pipelines by connecting what they call

01:09:15,279 --> 01:09:20,239
transformations each transformation

01:09:18,640 --> 01:09:21,920
delivers a collection of something

01:09:20,239 --> 01:09:23,759
in this case records to the next

01:09:21,920 --> 01:09:25,359
transformation

01:09:23,759 --> 01:09:27,839
then the last transformation does the

01:09:25,359 --> 01:09:30,880
same and so on

01:09:27,839 --> 01:09:33,759
now let's focus on the transformations

01:09:30,880 --> 01:09:34,799
the pipeline has three important steps

01:09:33,759 --> 01:09:37,920
in step one

01:09:34,799 --> 01:09:40,319
the dibysium io or read transform

01:09:37,920 --> 01:09:42,560
reads and captures the cdc events then

01:09:40,319 --> 01:09:43,040
converts the kafka source record cdc

01:09:42,560 --> 01:09:46,560
event

01:09:43,040 --> 01:09:49,120
into a json element in step two

01:09:46,560 --> 01:09:51,600
the json to table row transformer will

01:09:49,120 --> 01:09:54,320
take that cdc event as json

01:09:51,600 --> 01:09:57,360
and transform it into in this case

01:09:54,320 --> 01:09:59,840
bigquery's table row collection

01:09:57,360 --> 01:10:01,600
finally in step 3 the right transform

01:09:59,840 --> 01:10:02,640
will push out the resultant rows to

01:10:01,600 --> 01:10:05,679
bigquery

01:10:02,640 --> 01:10:07,520
into the specified output table

01:10:05,679 --> 01:10:09,040
here's a bit about wiseline's engagement

01:10:07,520 --> 01:10:10,719
process

01:10:09,040 --> 01:10:12,080
first we gathered input from the apache

01:10:10,719 --> 01:10:13,840
beam community

01:10:12,080 --> 01:10:17,360
we identified that cdc could be

01:10:13,840 --> 01:10:19,440
implemented in beam via dibesium

01:10:17,360 --> 01:10:21,440
artifacts were created in uml to

01:10:19,440 --> 01:10:22,880
validate our understanding

01:10:21,440 --> 01:10:24,320
we then kicked off our six-week

01:10:22,880 --> 01:10:26,960
development phase using agile

01:10:24,320 --> 01:10:28,880
methodology

01:10:26,960 --> 01:10:30,320
we conducted periodic code reviews with

01:10:28,880 --> 01:10:32,400
the community

01:10:30,320 --> 01:10:34,719
and contributed via unit and integration

01:10:32,400 --> 01:10:36,400
testing

01:10:34,719 --> 01:10:38,560
after final review we were able to push

01:10:36,400 --> 01:10:40,000
the code to the community

01:10:38,560 --> 01:10:42,480
now we're going to take a brief look at

01:10:40,000 --> 01:10:44,000
the code we're looking at the latest

01:10:42,480 --> 01:10:48,159
debesium dibysiumio

01:10:44,000 --> 01:10:51,040
dot java file in the beam repository

01:10:48,159 --> 01:10:52,080
here debesium io declares its read t

01:10:51,040 --> 01:10:55,679
class function

01:10:52,080 --> 01:10:57,760
which extends from a p transform

01:10:55,679 --> 01:11:00,159
here we see the overridden expand

01:10:57,760 --> 01:11:00,159
function

01:11:00,239 --> 01:11:06,000
on line 213 a kafka source consumer

01:11:03,679 --> 01:11:07,040
function is instantiated passing the

01:11:06,000 --> 01:11:08,800
configuration

01:11:07,040 --> 01:11:10,080
format function and some other

01:11:08,800 --> 01:11:12,320
restrictions

01:11:10,080 --> 01:11:13,520
the kafka source consumer function is a

01:11:12,320 --> 01:11:15,600
splittable dofin

01:11:13,520 --> 01:11:18,480
used to process records fetched from

01:11:15,600 --> 01:11:21,120
supported dobesian connectors

01:11:18,480 --> 01:11:23,280
now we're going to review kafka source

01:11:21,120 --> 01:11:25,520
consumer process function

01:11:23,280 --> 01:11:27,199
kafka source consumer process function

01:11:25,520 --> 01:11:28,880
passes configuration

01:11:27,199 --> 01:11:30,320
and instantiates the kafka source

01:11:28,880 --> 01:11:33,199
connector

01:11:30,320 --> 01:11:34,960
on line 155 the connector instantiates

01:11:33,199 --> 01:11:36,560
the kafka source task

01:11:34,960 --> 01:11:38,239
which controls the execution of the

01:11:36,560 --> 01:11:42,000
connector

01:11:38,239 --> 01:11:43,920
on line 161 and 162 we initialize and

01:11:42,000 --> 01:11:47,520
start the task

01:11:43,920 --> 01:11:49,920
on line 164 record polling begins

01:11:47,520 --> 01:11:52,239
kafka source record from the connector's

01:11:49,920 --> 01:11:54,080
internal queue

01:11:52,239 --> 01:11:55,760
the kafka source record is then passed

01:11:54,080 --> 01:11:56,800
by the configured map function to

01:11:55,760 --> 01:11:59,199
transform it into

01:11:56,800 --> 01:12:00,480
a json representation which is the

01:11:59,199 --> 01:12:03,199
return type of the p

01:12:00,480 --> 01:12:04,159
collection now i'm going to show you how

01:12:03,199 --> 01:12:07,120
to stand up

01:12:04,159 --> 01:12:08,159
a very simple pipeline using apache beam

01:12:07,120 --> 01:12:12,400
debesium

01:12:08,159 --> 01:12:14,640
mysql and gcp here's a very high level

01:12:12,400 --> 01:12:16,000
of my environment i'm using a

01:12:14,640 --> 01:12:19,199
development workstation

01:12:16,000 --> 01:12:21,199
running debian 10 and maven and some

01:12:19,199 --> 01:12:22,800
other minor dependencies

01:12:21,199 --> 01:12:25,120
for testing make sure this environment

01:12:22,800 --> 01:12:26,800
has correct access to both source and

01:12:25,120 --> 01:12:29,760
target

01:12:26,800 --> 01:12:31,920
i have a mysql database via docker image

01:12:29,760 --> 01:12:33,199
as my source these test dockers

01:12:31,920 --> 01:12:35,360
are available in the division

01:12:33,199 --> 01:12:37,199
repositories

01:12:35,360 --> 01:12:40,800
i'm using bigquery as the destination

01:12:37,199 --> 01:12:42,400
for mysql data after transformation

01:12:40,800 --> 01:12:44,960
the first thing i'll do is configure my

01:12:42,400 --> 01:12:45,840
data source in this case i'm using one

01:12:44,960 --> 01:12:49,280
of the debesium

01:12:45,840 --> 01:12:51,520
example mysql containers version 1.3

01:12:49,280 --> 01:12:53,199
the example mysql docker has a very sim

01:12:51,520 --> 01:12:54,480
simple schema that is perfect for

01:12:53,199 --> 01:12:56,239
testing

01:12:54,480 --> 01:12:58,560
when considering your test setup you

01:12:56,239 --> 01:13:00,080
should make sure that tcp port access is

01:12:58,560 --> 01:13:03,920
available from your source

01:13:00,080 --> 01:13:05,360
to target sync the docker command

01:13:03,920 --> 01:13:06,880
defines the database connection

01:13:05,360 --> 01:13:10,000
parameters which will be used in

01:13:06,880 --> 01:13:13,760
standing up the pipeline in later steps

01:13:10,000 --> 01:13:16,239
now let's set up bigquery log into gcp

01:13:13,760 --> 01:13:17,280
from the bigquery explorer browse to

01:13:16,239 --> 01:13:20,800
your data set

01:13:17,280 --> 01:13:25,679
and create a new table under schema

01:13:20,800 --> 01:13:25,679
click add field and under name enter

01:13:30,840 --> 01:13:33,840
json

01:13:34,560 --> 01:13:38,400
let's prepare the pipeline configuration

01:13:36,480 --> 01:13:40,120
on the development workstation

01:13:38,400 --> 01:13:42,080
we are viewing step one in the

01:13:40,120 --> 01:13:44,080
project.java file

01:13:42,080 --> 01:13:47,040
in this case it's one of the samples

01:13:44,080 --> 01:13:48,960
provided by dibysia

01:13:47,040 --> 01:13:51,440
here step one defines the dibysium

01:13:48,960 --> 01:13:53,360
connector configuration

01:13:51,440 --> 01:13:57,120
the connector configurations contain

01:13:53,360 --> 01:13:58,480
options class and property references

01:13:57,120 --> 01:14:01,679
these are the options that will be

01:13:58,480 --> 01:14:05,199
passed at the command line with maven

01:14:01,679 --> 01:14:08,159
let's take a look at steps two and three

01:14:05,199 --> 01:14:09,840
in step two the i o transforms the json

01:14:08,159 --> 01:14:12,080
into table row

01:14:09,840 --> 01:14:15,440
and in step three the table row data is

01:14:12,080 --> 01:14:17,760
appended to the bigquery output table

01:14:15,440 --> 01:14:19,440
now let's kick off the pipeline in order

01:14:17,760 --> 01:14:20,560
to do so we need to stage our maven

01:14:19,440 --> 01:14:22,719
command

01:14:20,560 --> 01:14:24,320
this command defines the template data

01:14:22,719 --> 01:14:28,080
runner google project

01:14:24,320 --> 01:14:30,000
source jdbc connection info and more

01:14:28,080 --> 01:14:32,080
notice the bigquery temporary directory

01:14:30,000 --> 01:14:34,560
this will need to be configured prior to

01:14:32,080 --> 01:14:34,560
testing

01:14:36,000 --> 01:14:40,480
after the maven command is set up i will

01:14:37,760 --> 01:14:42,000
launch the pipeline

01:14:40,480 --> 01:14:44,800
you will see the pipe building on the

01:14:42,000 --> 01:14:44,800
bottom of the screen

01:14:46,560 --> 01:14:50,960
now i will log into gcp data flow on the

01:14:48,719 --> 01:14:50,960
right

01:14:51,440 --> 01:14:57,840
you will notice the data flow job up and

01:14:54,840 --> 01:14:57,840
running

01:15:02,880 --> 01:15:06,080
let's check bigquery to see if we have

01:15:04,960 --> 01:15:08,800
data

01:15:06,080 --> 01:15:10,480
we do have data if you analyze the logs

01:15:08,800 --> 01:15:12,080
you can see verbose information about

01:15:10,480 --> 01:15:13,120
the pipeline and the transformations

01:15:12,080 --> 01:15:14,719
occurring

01:15:13,120 --> 01:15:16,320
you may have the need to convert strings

01:15:14,719 --> 01:15:17,360
of data in some manner before they land

01:15:16,320 --> 01:15:19,920
on the target

01:15:17,360 --> 01:15:21,600
for example your pipeline might receive

01:15:19,920 --> 01:15:24,159
a date value in pst

01:15:21,600 --> 01:15:25,360
and you would like to convert it to utc

01:15:24,159 --> 01:15:28,239
at your target

01:15:25,360 --> 01:15:29,920
this is possible with this technology

01:15:28,239 --> 01:15:31,679
within the pipeline logging you can see

01:15:29,920 --> 01:15:35,440
the before and after value of the data

01:15:31,679 --> 01:15:35,440
as it transfers the pipeline

01:15:35,840 --> 01:15:39,679
one thing to test is to make an insert

01:15:37,520 --> 01:15:42,320
at the source to see if it populates

01:15:39,679 --> 01:15:42,320
bigquery

01:15:43,199 --> 01:15:47,120
that wraps up my quick demo i hope that

01:15:45,679 --> 01:15:48,239
after going through this with me you

01:15:47,120 --> 01:15:51,040
have a better understanding of

01:15:48,239 --> 01:15:53,199
wiseline's contribution to apache beam

01:15:51,040 --> 01:15:55,199
developing pipelines using apache beam

01:15:53,199 --> 01:15:56,080
allows organizations many benefits

01:15:55,199 --> 01:15:58,080
including

01:15:56,080 --> 01:15:59,600
data portability flexibility and

01:15:58,080 --> 01:16:01,600
scalability

01:15:59,600 --> 01:16:03,440
wisely is actively working on apache

01:16:01,600 --> 01:16:04,880
beam related projects and will continue

01:16:03,440 --> 01:16:06,400
to contribute

01:16:04,880 --> 01:16:08,159
i have added some links at the bottom of

01:16:06,400 --> 01:16:09,199
this document if you would like to learn

01:16:08,159 --> 01:16:11,440
more

01:16:09,199 --> 01:16:21,840
thank you apache beam community and have

01:16:11,440 --> 01:16:21,840
a good day

01:16:27,199 --> 01:16:30,800
thank you so much william avoiding extra

01:16:29,360 --> 01:16:34,480
event infrastructure just to get your

01:16:30,800 --> 01:16:35,280
own data sounds like a huge win so so

01:16:34,480 --> 01:16:38,159
robert i don't

01:16:35,280 --> 01:16:38,880
really fully understand um how cdc can

01:16:38,159 --> 01:16:42,480
be

01:16:38,880 --> 01:16:45,520
useful so i understand that it's

01:16:42,480 --> 01:16:47,840
this way of being able to

01:16:45,520 --> 01:16:48,640
track updates to a database right so you

01:16:47,840 --> 01:16:52,320
can like

01:16:48,640 --> 01:16:53,199
subscribe to updates um like inserts

01:16:52,320 --> 01:16:54,960
deletes but

01:16:53,199 --> 01:16:56,400
how does that how does that really help

01:16:54,960 --> 01:16:58,880
me well

01:16:56,400 --> 01:16:59,920
it's about making sure that your data

01:16:58,880 --> 01:17:02,719
and your analytics

01:16:59,920 --> 01:17:03,199
match up if you don't have if your

01:17:02,719 --> 01:17:05,920
source

01:17:03,199 --> 01:17:06,320
if what you're doing your analysis over

01:17:05,920 --> 01:17:08,480
isn't

01:17:06,320 --> 01:17:10,000
actually based on the source of truth

01:17:08,480 --> 01:17:12,400
your database

01:17:10,000 --> 01:17:13,520
you're going to have drift over time and

01:17:12,400 --> 01:17:15,360
you're going to end up with lots of

01:17:13,520 --> 01:17:16,719
discontinuities in your graph and you'll

01:17:15,360 --> 01:17:20,400
have to spend more compute

01:17:16,719 --> 01:17:24,400
recomputing everything oh okay okay okay

01:17:20,400 --> 01:17:27,600
so it's so basically

01:17:24,400 --> 01:17:29,679
in a normal streaming pipeline

01:17:27,600 --> 01:17:31,760
right you're doing real-time analytics

01:17:29,679 --> 01:17:33,120
but those analytics are streaming in

01:17:31,760 --> 01:17:33,840
directly from the source right they're

01:17:33,120 --> 01:17:36,320
not being

01:17:33,840 --> 01:17:37,679
journaled anywhere that's right well

01:17:36,320 --> 01:17:38,960
they are being journaled they're just

01:17:37,679 --> 01:17:40,880
being journaled and sent out

01:17:38,960 --> 01:17:42,400
by the database itself and that's what

01:17:40,880 --> 01:17:43,760
dbzm gives you

01:17:42,400 --> 01:17:45,840
oh okay so it gives you kind of like

01:17:43,760 --> 01:17:47,520
that extra that extra layer there the

01:17:45,840 --> 01:17:51,040
extra layer of safety

01:17:47,520 --> 01:17:53,360
that's right okay and uh with that

01:17:51,040 --> 01:17:54,880
it's time for the after party uh come

01:17:53,360 --> 01:17:55,679
and join us at the after party by

01:17:54,880 --> 01:17:57,760
clicking the watch

01:17:55,679 --> 01:18:00,640
live button on the events agenda page to

01:17:57,760 --> 01:18:02,719
be redirected to google meet

01:18:00,640 --> 01:18:04,640
so today's speakers will also be joining

01:18:02,719 --> 01:18:07,280
us and we will have some quizzes

01:18:04,640 --> 01:18:07,760
interactive activities surprise guests

01:18:07,280 --> 01:18:11,199
and

01:18:07,760 --> 01:18:13,440
maybe some swag do you think

01:18:11,199 --> 01:18:14,719
do you think there'll be some swag i

01:18:13,440 --> 01:18:15,520
think i know there's going to be some

01:18:14,719 --> 01:18:17,440
swag

01:18:15,520 --> 01:18:27,840
well hope to see you there thank you

01:18:17,440 --> 01:18:27,840
thank you

01:18:30,000 --> 01:18:32,080

YouTube URL: https://www.youtube.com/watch?v=3eIbtW5aR0s


