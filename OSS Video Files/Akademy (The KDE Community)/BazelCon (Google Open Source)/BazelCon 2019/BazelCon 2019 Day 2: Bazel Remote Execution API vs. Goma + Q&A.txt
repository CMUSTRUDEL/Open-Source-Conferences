Title: BazelCon 2019 Day 2: Bazel Remote Execution API vs. Goma + Q&A
Publication date: 2020-01-15
Playlist: BazelCon 2019
Description: 
	Mostyn Bramley-Moore, Vewd Software event: Bazelcon 2019; re_ty: Publish; product: Open Source - General; fullname: Mostyn Bramley-Moore;
Captions: 
	00:00:03,920 --> 00:00:10,440
okay welcome back we got an exciting two

00:00:08,429 --> 00:00:13,230
more sessions and then we are going to

00:00:10,440 --> 00:00:15,179
break for lunch today right now we've

00:00:13,230 --> 00:00:17,039
got mauston from huge software he's

00:00:15,179 --> 00:00:19,380
going to talk about basal remote

00:00:17,039 --> 00:00:26,939
execution API versus Goma so let's

00:00:19,380 --> 00:00:29,269
welcome Austin on the stage so hi

00:00:26,939 --> 00:00:29,269
everybody

00:00:29,660 --> 00:00:35,160
here's my background I work at viewed

00:00:32,610 --> 00:00:37,860
software and our main product is a smart

00:00:35,160 --> 00:00:39,780
ass DK aimed at smart TV and for the

00:00:37,860 --> 00:00:42,690
past five or so years has been based on

00:00:39,780 --> 00:00:44,969
chromium chromium for us is a pretty

00:00:42,690 --> 00:00:47,030
large code base I hear from some of the

00:00:44,969 --> 00:00:49,500
Google guys that it's only medium but

00:00:47,030 --> 00:00:52,160
maybe you got most of the crowd here I'm

00:00:49,500 --> 00:00:54,660
guessing it's pretty big for you too

00:00:52,160 --> 00:00:56,670
we're only interested in the Linux and

00:00:54,660 --> 00:00:59,579
Android part of chromium and only a

00:00:56,670 --> 00:01:03,199
subset of that so that's kind of my bias

00:00:59,579 --> 00:01:06,299
this is the scale that I'm building up

00:01:03,199 --> 00:01:09,060
we use clang in GCC in a single build

00:01:06,299 --> 00:01:10,560
which is a bit unusual if we're

00:01:09,060 --> 00:01:13,320
cross-compiling sometimes there's

00:01:10,560 --> 00:01:15,360
multiple versions of GCC and clang

00:01:13,320 --> 00:01:19,200
because you have to build chromium twice

00:01:15,360 --> 00:01:22,229
to generate snapshots if you're building

00:01:19,200 --> 00:01:24,180
for a different architecture business if

00:01:22,229 --> 00:01:26,790
you're on 64-bit development chain

00:01:24,180 --> 00:01:29,280
building for 32-bit you need to build a

00:01:26,790 --> 00:01:31,049
32-bit version of chromium to create

00:01:29,280 --> 00:01:33,500
this startup data so that's quite

00:01:31,049 --> 00:01:35,909
painful for a build the size of chromium

00:01:33,500 --> 00:01:41,159
and we've got several offices around the

00:01:35,909 --> 00:01:42,899
world so chromium is large and it's

00:01:41,159 --> 00:01:45,689
constantly changing and this is pretty

00:01:42,899 --> 00:01:48,720
pretty bad for a build system puts a lot

00:01:45,689 --> 00:01:51,119
of pressure on us it's painful to build

00:01:48,720 --> 00:01:52,829
on a workstation so much that basically

00:01:51,119 --> 00:01:53,970
nobody ever attempts this anymore even

00:01:52,829 --> 00:01:56,689
if you've got a really powerful

00:01:53,970 --> 00:01:59,329
workstation

00:01:56,689 --> 00:02:01,350
it requires bleeding-edge clang and

00:01:59,329 --> 00:02:02,969
there's not much we can do about that

00:02:01,350 --> 00:02:05,130
because the chromium team at Google is

00:02:02,969 --> 00:02:07,360
interested in taking new C++ features

00:02:05,130 --> 00:02:09,910
all the time

00:02:07,360 --> 00:02:12,360
it's a mostly hermetic build setup which

00:02:09,910 --> 00:02:14,650
is pretty good it allows us to get

00:02:12,360 --> 00:02:19,030
reproducible builds and it helps with

00:02:14,650 --> 00:02:20,950
our case hits and we're stuck using

00:02:19,030 --> 00:02:23,710
whatever chromium decides to use for

00:02:20,950 --> 00:02:25,270
their build system so this is gn+ ninja

00:02:23,710 --> 00:02:27,310
which sort of looks like basil from a

00:02:25,270 --> 00:02:28,360
distance and if you're lucky enough to

00:02:27,310 --> 00:02:38,700
have going around this sort of looks

00:02:28,360 --> 00:02:41,170
like a big basil remote execution API so

00:02:38,700 --> 00:02:43,780
governme and basil have both come from

00:02:41,170 --> 00:02:45,130
google and look very similar the problem

00:02:43,780 --> 00:02:46,930
one is probably an ancestor of the other

00:02:45,130 --> 00:02:48,340
ocean gamers older but I don't really

00:02:46,930 --> 00:02:51,459
know

00:02:48,340 --> 00:02:52,930
I think Gomer is used for some other

00:02:51,459 --> 00:02:55,450
open source Google projects maybe

00:02:52,930 --> 00:02:57,670
Android but I have not played this

00:02:55,450 --> 00:03:01,030
myself and it's injected into the build

00:02:57,670 --> 00:03:05,200
by a simple compiler a program SEC much

00:03:01,030 --> 00:03:08,020
like C cache if you've ever used that so

00:03:05,200 --> 00:03:09,760
how painful is it to build chromium I've

00:03:08,020 --> 00:03:12,580
got some data here that's a few years

00:03:09,760 --> 00:03:13,959
old but the trend has continued the

00:03:12,580 --> 00:03:16,030
interesting part here is that the build

00:03:13,959 --> 00:03:21,040
time increases by about 1% per week

00:03:16,030 --> 00:03:22,300
which is pretty scary if you ask me so

00:03:21,040 --> 00:03:24,970
here's the graph this is a couple of

00:03:22,300 --> 00:03:28,239
years ago from Mac but the trend holds

00:03:24,970 --> 00:03:30,130
for Linux as well to build on a single

00:03:28,239 --> 00:03:31,900
machine you're talking multiple hours

00:03:30,130 --> 00:03:36,610
for a same workstation for most

00:03:31,900 --> 00:03:38,320
companies so just a quick refresher of

00:03:36,610 --> 00:03:40,090
the C++ build model since that's the

00:03:38,320 --> 00:03:42,850
majority of the chromium builds you

00:03:40,090 --> 00:03:45,459
first have to pre-press process source

00:03:42,850 --> 00:03:49,090
plus headers plus macros and you get a

00:03:45,459 --> 00:03:51,160
large pre-processed source then you have

00:03:49,090 --> 00:03:53,650
to compile that source to produce an

00:03:51,160 --> 00:03:54,970
object file which can be slow and you

00:03:53,650 --> 00:03:57,040
have to eventually link that with a

00:03:54,970 --> 00:03:59,950
bunch of other object files when you're

00:03:57,040 --> 00:04:01,989
done and that can be very slow so to

00:03:59,950 --> 00:04:04,750
build premium quickly what the strategy

00:04:01,989 --> 00:04:06,549
that's employed is to reuse as much as

00:04:04,750 --> 00:04:11,980
possible to distribute as much as

00:04:06,549 --> 00:04:14,230
possible until link-local e and this is

00:04:11,980 --> 00:04:15,519
the typical setup for most downstream

00:04:14,230 --> 00:04:17,620
chromium projects

00:04:15,519 --> 00:04:20,139
if you don't have gum you can see cash

00:04:17,620 --> 00:04:22,090
to get the casing unfortunately it's

00:04:20,139 --> 00:04:24,639
local only but when you get a case sheet

00:04:22,090 --> 00:04:27,180
it's very fast and because it's likely

00:04:24,639 --> 00:04:30,759
your cash hit rate is pretty low and

00:04:27,180 --> 00:04:34,180
then to distribute the build people tend

00:04:30,759 --> 00:04:38,110
to use ICC or DCC which sends out the

00:04:34,180 --> 00:04:41,850
pre-process source to your network this

00:04:38,110 --> 00:04:45,550
only works on the LAN and it tends to be

00:04:41,850 --> 00:04:47,289
self managed with ICC people enroll

00:04:45,550 --> 00:04:50,050
their own workstations into the build

00:04:47,289 --> 00:04:55,360
farm and this can be pretty unreliable

00:04:50,050 --> 00:04:56,740
and the other issue is that you get

00:04:55,360 --> 00:04:58,810
slightly different warnings from clang

00:04:56,740 --> 00:05:00,340
when you're building pre-processed

00:04:58,810 --> 00:05:01,960
source to building the original sources

00:05:00,340 --> 00:05:04,539
in theory it shouldn't happen but

00:05:01,960 --> 00:05:07,930
there's always a clang bug that causes

00:05:04,539 --> 00:05:09,880
trouble and chromium is built with W

00:05:07,930 --> 00:05:11,530
error so errors at sorry warnings are

00:05:09,880 --> 00:05:15,550
turned into errors which makes this

00:05:11,530 --> 00:05:18,669
really annoying so back in 2013

00:05:15,550 --> 00:05:20,169
this is kind of what the frequency

00:05:18,669 --> 00:05:23,229
distribution of our build times look

00:05:20,169 --> 00:05:25,479
like at the at the low end we could

00:05:23,229 --> 00:05:28,060
build in under a minute but this is a

00:05:25,479 --> 00:05:29,470
fairly rare occurrence and up to about

00:05:28,060 --> 00:05:33,580
three and a half minutes in the worst

00:05:29,470 --> 00:05:37,570
case first four to 2019 it was more like

00:05:33,580 --> 00:05:39,909
this so there's two interesting parts

00:05:37,570 --> 00:05:42,070
about this made-up graph the first is

00:05:39,909 --> 00:05:45,520
that the builds have grown to about 10

00:05:42,070 --> 00:05:47,349
10 11 12 minutes on average and there's

00:05:45,520 --> 00:05:49,900
the second lump over on the edge which

00:05:47,349 --> 00:05:51,430
is the I just come back from drinking my

00:05:49,900 --> 00:05:53,229
coffee in the build broke because

00:05:51,430 --> 00:05:55,599
somebody's ICC workstation was

00:05:53,229 --> 00:05:59,889
overloaded so this is getting pretty

00:05:55,599 --> 00:06:01,300
frustrating and there's nothing we could

00:05:59,889 --> 00:06:03,849
do because we're sort of hitting

00:06:01,300 --> 00:06:06,070
inherent limits in the ICC protocol and

00:06:03,849 --> 00:06:08,110
C cache is not gonna give us anymore

00:06:06,070 --> 00:06:11,560
okay sheets then we can already get

00:06:08,110 --> 00:06:14,169
locally but then there's this light on

00:06:11,560 --> 00:06:18,430
the horizon early in 2018 the Girma

00:06:14,169 --> 00:06:20,470
client was open sourced it supports

00:06:18,430 --> 00:06:22,030
Linux Windows and Mac clients although

00:06:20,470 --> 00:06:25,029
we've only been testing it with Linux I

00:06:22,030 --> 00:06:25,409
hear that it's experimental on Mac but I

00:06:25,029 --> 00:06:27,269
know

00:06:25,409 --> 00:06:29,459
the chromium hedge team has been using

00:06:27,269 --> 00:06:32,369
their own Gummer back-end so windows

00:06:29,459 --> 00:06:35,099
must work it's kind of like dis city

00:06:32,369 --> 00:06:37,229
pomp mode with a central a/c cache so

00:06:35,099 --> 00:06:41,039
this CC pump mode works by only

00:06:37,229 --> 00:06:46,229
uploading the files once and therefore

00:06:41,039 --> 00:06:48,119
you remove the the the ethernet the

00:06:46,229 --> 00:06:51,599
gigabit ethernet upload sealing that you

00:06:48,119 --> 00:06:53,459
can quite easily hit with icc so there's

00:06:51,599 --> 00:06:55,559
a compiler proxy service that runs on

00:06:53,459 --> 00:06:57,479
the client kind of like the client that

00:06:55,559 --> 00:06:58,679
runs when you start basil or the server

00:06:57,479 --> 00:07:01,229
that runs when you start basil on the

00:06:58,679 --> 00:07:03,089
client machine there's a Germer CC

00:07:01,229 --> 00:07:03,769
compiler wrapper that feeds jobs into

00:07:03,089 --> 00:07:07,169
the

00:07:03,769 --> 00:07:09,179
compiler proxy there's a custom

00:07:07,169 --> 00:07:12,239
preprocessor that it's optimized just to

00:07:09,179 --> 00:07:15,119
find the inputs to a C or C++ compile

00:07:12,239 --> 00:07:18,059
job and lots of caching optimizations

00:07:15,119 --> 00:07:20,129
under the hood at this point there was

00:07:18,059 --> 00:07:21,959
no server available but it was pretty

00:07:20,129 --> 00:07:24,679
buff over HTTP so this is something

00:07:21,959 --> 00:07:27,029
that's quite easy to reverse-engineer

00:07:24,679 --> 00:07:28,619
and this is these are the two most

00:07:27,029 --> 00:07:30,779
interesting messages in there Goma

00:07:28,619 --> 00:07:33,659
protocols it's much simpler than a

00:07:30,779 --> 00:07:36,419
remote execution API you can request an

00:07:33,659 --> 00:07:39,059
execution which uploads the command

00:07:36,419 --> 00:07:42,389
hashes of all the inputs maybe some of

00:07:39,059 --> 00:07:45,389
the inputs are included and the expected

00:07:42,389 --> 00:07:47,610
outputs and the the response you get

00:07:45,389 --> 00:07:49,139
back either says I had these missing

00:07:47,610 --> 00:07:50,759
files with these hashes can you upload

00:07:49,139 --> 00:07:55,739
them please in a second attempt or

00:07:50,759 --> 00:07:58,469
here's your results inside the

00:07:55,739 --> 00:08:00,869
government client these are I think the

00:07:58,469 --> 00:08:02,849
few cases that I found this morning when

00:08:00,869 --> 00:08:04,829
I was double checking my notes there's

00:08:02,849 --> 00:08:06,719
an input file cache so you don't have to

00:08:04,829 --> 00:08:08,669
keep continuously buri read the same

00:08:06,719 --> 00:08:10,769
header files on every compiled unit

00:08:08,669 --> 00:08:14,339
there's a directory listing cache so you

00:08:10,769 --> 00:08:16,019
don't have to hit performance problems

00:08:14,339 --> 00:08:18,179
in your file system particularly on

00:08:16,019 --> 00:08:20,099
Windows I think there's an input

00:08:18,179 --> 00:08:23,219
dependency case which means you don't

00:08:20,099 --> 00:08:26,789
have to rerun the optimized preprocessor

00:08:23,219 --> 00:08:29,939
every time you run a rerun a job there's

00:08:26,789 --> 00:08:31,769
a compiler info cash to quickly return

00:08:29,939 --> 00:08:34,219
details about the compiler which is

00:08:31,769 --> 00:08:36,569
included in the hash that's uploaded and

00:08:34,219 --> 00:08:37,810
there's an output cache which is off by

00:08:36,569 --> 00:08:42,250
default and when I

00:08:37,810 --> 00:08:46,320
no faster than using a network cache on

00:08:42,250 --> 00:08:48,279
the back end there's more caching still

00:08:46,320 --> 00:08:51,640
imprecation is required to function

00:08:48,279 --> 00:08:52,990
because if the gaming client guesses

00:08:51,640 --> 00:08:54,670
wrong and doesn't upload all the

00:08:52,990 --> 00:08:56,440
required inputs you get another

00:08:54,670 --> 00:08:58,330
requested the Goma client will try to

00:08:56,440 --> 00:09:00,040
just upload the inputs that were missing

00:08:58,330 --> 00:09:02,860
the last time

00:09:00,040 --> 00:09:06,100
if the Goma client was to try and upload

00:09:02,860 --> 00:09:07,990
all the inputs and every exit grec then

00:09:06,100 --> 00:09:10,720
you'd have no better than running than

00:09:07,990 --> 00:09:14,080
uploading pre-process source from our

00:09:10,720 --> 00:09:16,360
bandwidth the point of view so

00:09:14,080 --> 00:09:17,830
implication is not an explicit part of

00:09:16,360 --> 00:09:20,740
the protocol but it's required to

00:09:17,830 --> 00:09:22,450
function output caching is completely

00:09:20,740 --> 00:09:29,650
optional but gives you a big speed-up

00:09:22,450 --> 00:09:31,800
and the implication is has different

00:09:29,650 --> 00:09:34,420
access patterns to the application

00:09:31,800 --> 00:09:36,940
during a build common header files can

00:09:34,420 --> 00:09:39,070
be included thousands of times I think

00:09:36,940 --> 00:09:42,160
we have twenty to thirty thousand build

00:09:39,070 --> 00:09:44,380
steps in chromium they're not all C++

00:09:42,160 --> 00:09:47,740
compiler tools but a lot of them are so

00:09:44,380 --> 00:09:50,050
common headers are included a lot and

00:09:47,740 --> 00:09:52,480
whereas the applications basically cased

00:09:50,050 --> 00:09:56,770
once per build and returned and then if

00:09:52,480 --> 00:09:58,870
your make a rebuild each output object

00:09:56,770 --> 00:10:04,150
is returned once during the whole build

00:09:58,870 --> 00:10:07,810
for the whole package chromium so this

00:10:04,150 --> 00:10:10,600
is the design we ended up in pointing in

00:10:07,810 --> 00:10:12,040
there on the private cloud side we've

00:10:10,600 --> 00:10:13,660
got all communication going through a

00:10:12,040 --> 00:10:16,270
scheduler which is basically a simple

00:10:13,660 --> 00:10:18,850
load balancer which sends data since the

00:10:16,270 --> 00:10:24,670
exit wrecks to the least loaded worker

00:10:18,850 --> 00:10:26,170
you know cluster there's a memcache the

00:10:24,670 --> 00:10:28,540
instance that keeps track of all the

00:10:26,170 --> 00:10:30,400
input files there's a tool chain server

00:10:28,540 --> 00:10:32,920
which maintains all the tool chains

00:10:30,400 --> 00:10:35,200
indexed by their hash and there is a

00:10:32,920 --> 00:10:37,450
output cache which is using basil mode

00:10:35,200 --> 00:10:40,390
we're actually kind of abusing the basil

00:10:37,450 --> 00:10:42,279
or my API here storing arbitrary blobs

00:10:40,390 --> 00:10:46,660
via the action cache because there's no

00:10:42,279 --> 00:10:48,360
validation there actually could I just

00:10:46,660 --> 00:10:53,129
go back one slide

00:10:48,360 --> 00:10:54,809
on the office side we have some

00:10:53,129 --> 00:10:57,360
workstations that just correct connect

00:10:54,809 --> 00:10:59,249
directly to the scheduler but also we

00:10:57,360 --> 00:11:01,679
have in some offices we have a pre cache

00:10:59,249 --> 00:11:03,839
so we can do fast cache lookups in the

00:11:01,679 --> 00:11:05,459
office land in the case of a cache miss

00:11:03,839 --> 00:11:08,160
then there precocious forwards to the

00:11:05,459 --> 00:11:10,679
scheduler and continues as normal on

00:11:08,160 --> 00:11:12,600
case you miss when the request goes to

00:11:10,679 --> 00:11:14,699
the scheduler serviced by one of the

00:11:12,600 --> 00:11:16,799
workers and sent back it's cached on the

00:11:14,699 --> 00:11:18,629
on the back end and then sent back to

00:11:16,799 --> 00:11:22,189
the pre cache which cache therefore the

00:11:18,629 --> 00:11:22,189
next is the local client to come along

00:11:22,639 --> 00:11:27,239
so I call our back-end otoi because

00:11:25,350 --> 00:11:31,439
compared to Google scale it really is a

00:11:27,239 --> 00:11:34,499
toy but it scales pretty well I am to

00:11:31,439 --> 00:11:39,660
have at least ninja - je 1000 working

00:11:34,499 --> 00:11:41,220
and keep all their workers busy and this

00:11:39,660 --> 00:11:42,869
works pretty well in practice

00:11:41,220 --> 00:11:45,239
apart from parts of the build graph

00:11:42,869 --> 00:11:49,709
where there isn't enough jobs to to have

00:11:45,239 --> 00:11:51,629
running at once and because the output

00:11:49,709 --> 00:11:53,519
caching because the build is mostly

00:11:51,629 --> 00:11:55,529
hermetic we get mostly caches on rebuild

00:11:53,519 --> 00:11:57,720
which means our CI is basically

00:11:55,529 --> 00:12:00,720
populating the build cache and

00:11:57,720 --> 00:12:03,749
developers getting 99% cash it's quite

00:12:00,720 --> 00:12:04,949
frequently it's usable from offices

00:12:03,749 --> 00:12:07,559
around the world but obviously you're

00:12:04,949 --> 00:12:12,299
dependent on the their connection speed

00:12:07,559 --> 00:12:14,189
to the back end and I think in most

00:12:12,299 --> 00:12:16,470
offices we have up to a gigabyte

00:12:14,189 --> 00:12:20,339
bandwidth but that's it's more likely to

00:12:16,470 --> 00:12:24,149
be half that so here here are the

00:12:20,339 --> 00:12:26,249
results you can see that we are not

00:12:24,149 --> 00:12:29,519
quite as fast as the absolute fastest

00:12:26,249 --> 00:12:33,629
sir builds with see cache we give up

00:12:29,519 --> 00:12:35,819
maybe 1015 seconds but to get a really

00:12:33,629 --> 00:12:37,860
high hit rate is really common

00:12:35,819 --> 00:12:40,489
so our Gummer build times are much less

00:12:37,860 --> 00:12:40,489
distributed

00:12:42,420 --> 00:12:45,960
and the results we see this from this

00:12:44,460 --> 00:12:48,180
are pretty good our developers are more

00:12:45,960 --> 00:12:50,040
willing to experiment with new compiler

00:12:48,180 --> 00:12:52,230
flags new tool chains and there were

00:12:50,040 --> 00:12:55,470
more willing to make the device builds

00:12:52,230 --> 00:12:57,900
and actually test them on device there's

00:12:55,470 --> 00:12:59,640
much lower developer frustration we need

00:12:57,900 --> 00:13:01,590
to find other things to be annoyed by

00:12:59,640 --> 00:13:06,030
and the maintenance has turned out to be

00:13:01,590 --> 00:13:08,100
really low so this is all pretty great

00:13:06,030 --> 00:13:09,660
but this is a basil conference and I

00:13:08,100 --> 00:13:14,310
don't know how interested you guys are

00:13:09,660 --> 00:13:16,620
about Goma but they're actually quite

00:13:14,310 --> 00:13:18,330
closely related related enough that

00:13:16,620 --> 00:13:22,010
there's bugs that are basically

00:13:18,330 --> 00:13:24,270
identical in both backends that I found

00:13:22,010 --> 00:13:27,330
implementation ideas and optimizations

00:13:24,270 --> 00:13:29,810
can be shared and the remote execution

00:13:27,330 --> 00:13:32,130
API working group is really open and

00:13:29,810 --> 00:13:37,220
much friendlier than the Girma team

00:13:32,130 --> 00:13:40,530
which is a bit more hidden within Google

00:13:37,220 --> 00:13:42,860
so then earlier this year the there was

00:13:40,530 --> 00:13:45,000
an open source Germer server released

00:13:42,860 --> 00:13:48,450
but it's not really a server it's a

00:13:45,000 --> 00:13:50,100
proxy to RBE and then the Germer team

00:13:48,450 --> 00:13:51,600
says to me oh yes we're considering

00:13:50,100 --> 00:13:54,210
switching the government client to speak

00:13:51,600 --> 00:13:56,760
re api and that had kind of had me

00:13:54,210 --> 00:13:58,530
worried because we would really like not

00:13:56,760 --> 00:14:00,320
to go back to the bad old days of c

00:13:58,530 --> 00:14:02,280
cache and ICC

00:14:00,320 --> 00:14:06,450
so I started thinking about what we

00:14:02,280 --> 00:14:09,780
could do to avoid that scenario could we

00:14:06,450 --> 00:14:11,430
switch to RBA we're not unable to

00:14:09,780 --> 00:14:13,530
because we need to work with GCC and

00:14:11,430 --> 00:14:17,280
this is not supported by the Gummer

00:14:13,530 --> 00:14:19,380
client the Girma claim has a new feature

00:14:17,280 --> 00:14:21,900
to upload the tool chain as if it was an

00:14:19,380 --> 00:14:24,630
input to a compiled job and this only

00:14:21,900 --> 00:14:25,830
works for clang at the moment and to

00:14:24,630 --> 00:14:27,960
make it worse it's blocked on an

00:14:25,830 --> 00:14:29,610
internal Google bug that I can't see the

00:14:27,960 --> 00:14:33,300
details of so we're kind of stuck there

00:14:29,610 --> 00:14:34,650
but also my boss wouldn't let me do this

00:14:33,300 --> 00:14:38,070
anyway we need to build everything on

00:14:34,650 --> 00:14:40,500
our own hardware but there's other

00:14:38,070 --> 00:14:43,200
open-source RB backends could we use one

00:14:40,500 --> 00:14:45,990
of those well no because they still

00:14:43,200 --> 00:14:48,330
don't support GCC either and it's a bit

00:14:45,990 --> 00:14:49,740
unproven I've heard of other companies

00:14:48,330 --> 00:14:52,260
that work downstream from chromium

00:14:49,740 --> 00:14:54,270
having tried this so far no public

00:14:52,260 --> 00:14:54,860
success stories I'd be really interested

00:14:54,270 --> 00:14:56,570
to hear if

00:14:54,860 --> 00:15:00,170
is anybody here that could prove that

00:14:56,570 --> 00:15:02,960
wrong but we'll see and it's difficult

00:15:00,170 --> 00:15:04,400
to gauge how how performant this would

00:15:02,960 --> 00:15:08,090
be without actually having a full-scale

00:15:04,400 --> 00:15:09,710
test run on the other hand maybe we

00:15:08,090 --> 00:15:11,330
could add re API support to our gomer

00:15:09,710 --> 00:15:13,810
backends it's a similar high-level

00:15:11,330 --> 00:15:16,340
design we're already using basil or mote

00:15:13,810 --> 00:15:19,550
could we basically get there

00:15:16,340 --> 00:15:21,050
piece by piece and test with basil to

00:15:19,550 --> 00:15:24,160
get remote caching up and running and

00:15:21,050 --> 00:15:27,350
rec to test our remote execution

00:15:24,160 --> 00:15:28,700
implementation and this is the design

00:15:27,350 --> 00:15:30,230
that I've come up with so we have all

00:15:28,700 --> 00:15:32,570
the same components the only difference

00:15:30,230 --> 00:15:35,060
here is that we've opened up connection

00:15:32,570 --> 00:15:42,830
directly to basil remote from the client

00:15:35,060 --> 00:15:48,080
side and we have to add re API endpoints

00:15:42,830 --> 00:15:50,840
to our scheduler and to our workers so

00:15:48,080 --> 00:15:54,170
so far I've made progress on the basil

00:15:50,840 --> 00:15:57,380
remote cache API and I can all build

00:15:54,170 --> 00:16:00,650
large projects using remote caching with

00:15:57,380 --> 00:16:03,020
basil mote tested with basil and also

00:16:00,650 --> 00:16:04,400
with rack I've added basic plane support

00:16:03,020 --> 00:16:06,110
direct since we need that to work to

00:16:04,400 --> 00:16:09,200
build chromium which is going to be my

00:16:06,110 --> 00:16:10,370
test case and I can build I haven't

00:16:09,200 --> 00:16:15,290
tested building all of chromium but I

00:16:10,370 --> 00:16:17,960
can build a useful subset so I haven't a

00:16:15,290 --> 00:16:20,600
prototype re API execution service this

00:16:17,960 --> 00:16:22,550
is just enough to successfully build

00:16:20,600 --> 00:16:25,190
with a few bugs but if I rebuild enough

00:16:22,550 --> 00:16:27,290
times I can populate the cache I'm still

00:16:25,190 --> 00:16:28,790
learning this part of the API but it's

00:16:27,290 --> 00:16:31,660
much smaller than the cache API so I'll

00:16:28,790 --> 00:16:34,820
get there soon and I've made rec

00:16:31,660 --> 00:16:38,770
requests inline results so I can then

00:16:34,820 --> 00:16:41,240
move on to an experiment later on I will

00:16:38,770 --> 00:16:46,190
have to integrate all this with our work

00:16:41,240 --> 00:16:48,460
in Goma back-end and optimize and so on

00:16:46,190 --> 00:16:52,070
and there's a few other things like

00:16:48,460 --> 00:16:57,680
figuring out a way how we can use a

00:16:52,070 --> 00:16:59,690
multi tool chain use rec with different

00:16:57,680 --> 00:17:02,840
tool chains in the single build since

00:16:59,690 --> 00:17:04,550
rec is I think you specify some

00:17:02,840 --> 00:17:05,260
environment variables which are related

00:17:04,550 --> 00:17:07,750
to the

00:17:05,260 --> 00:17:09,429
tool chain that you're uploading we need

00:17:07,750 --> 00:17:15,130
to figure out a rapper script to make

00:17:09,429 --> 00:17:17,079
this work so so here's our experiment if

00:17:15,130 --> 00:17:19,149
we can build the chromium and then

00:17:17,079 --> 00:17:21,610
rebuild half us sorry if we can build

00:17:19,149 --> 00:17:22,959
chromium with rec and basil right to

00:17:21,610 --> 00:17:26,500
populate the cache how fast can we

00:17:22,959 --> 00:17:28,029
rebuild it's not really worth

00:17:26,500 --> 00:17:29,860
considering the income incremental

00:17:28,029 --> 00:17:32,230
rebuild time because this is dominated

00:17:29,860 --> 00:17:33,909
by the running ninja which takes a

00:17:32,230 --> 00:17:35,710
couple of seconds and then linking after

00:17:33,909 --> 00:17:38,260
the result so that's not really worth

00:17:35,710 --> 00:17:41,649
measuring and these are our initial

00:17:38,260 --> 00:17:44,440
numbers to do a hot case rebuild with

00:17:41,649 --> 00:17:46,210
rec is a bit under four minutes although

00:17:44,440 --> 00:17:47,820
this seems a bit high so I'm not too

00:17:46,210 --> 00:17:51,809
confident in these numbers just yet

00:17:47,820 --> 00:17:54,159
compared to about a minute with gomu and

00:17:51,809 --> 00:17:55,690
what we're doing this experiment the

00:17:54,159 --> 00:17:58,299
client-side CPU is about a hundred

00:17:55,690 --> 00:17:59,799
percent on the records so something

00:17:58,299 --> 00:18:02,019
seems a bit off here but I haven't been

00:17:59,799 --> 00:18:04,960
able to narrow this down yet and there's

00:18:02,019 --> 00:18:07,480
no speed up above one thread per core on

00:18:04,960 --> 00:18:12,250
my client whereas with C case you can

00:18:07,480 --> 00:18:14,279
run hundreds so digging into things of

00:18:12,250 --> 00:18:16,659
it here's what I've got so far

00:18:14,279 --> 00:18:18,700
for a single execution of rec it's

00:18:16,659 --> 00:18:20,529
actually faster than Girma but it takes

00:18:18,700 --> 00:18:23,590
a lot more user in system time so this

00:18:20,529 --> 00:18:25,779
is time spent in the process doing work

00:18:23,590 --> 00:18:28,149
and time spent in kernel space doing

00:18:25,779 --> 00:18:30,010
work for the task whereas gamma is brief

00:18:28,149 --> 00:18:34,450
idle it must be waiting around for some

00:18:30,010 --> 00:18:38,320
stuff rec has some metrics that are kind

00:18:34,450 --> 00:18:40,149
of deciphered it seems like the bulk is

00:18:38,320 --> 00:18:42,340
in the compiler depths and building a

00:18:40,149 --> 00:18:45,279
muckle tree and calculator digests these

00:18:42,340 --> 00:18:49,600
are all things that are cached in the

00:18:45,279 --> 00:18:52,899
gamma client and looking at gamma we can

00:18:49,600 --> 00:18:56,230
basically see this the RPC white time is

00:18:52,899 --> 00:18:58,179
the bulk of the time here and everything

00:18:56,230 --> 00:19:00,929
else it does likely is a pittance in

00:18:58,179 --> 00:19:00,929
comparison

00:19:01,500 --> 00:19:05,400
so I think this is the problem I'm

00:19:03,450 --> 00:19:10,289
hitting with rec it takes away more

00:19:05,400 --> 00:19:12,919
local CPU resources than Goma and this

00:19:10,289 --> 00:19:17,190
is probably the bottleneck I'm hitting

00:19:12,919 --> 00:19:19,679
and while gomers RPC call is slower it's

00:19:17,190 --> 00:19:21,510
it's not that much slower and it's also

00:19:19,679 --> 00:19:22,289
doing more work on the server side than

00:19:21,510 --> 00:19:25,020
on the client side

00:19:22,289 --> 00:19:27,690
oh yeah there's also differences in the

00:19:25,020 --> 00:19:30,960
product it's using proto 2 versus proto

00:19:27,690 --> 00:19:32,940
3 in re API and it's HTTP 1 instead of

00:19:30,960 --> 00:19:34,830
page should be 2 I haven't dug into this

00:19:32,940 --> 00:19:38,520
yet but we'll see if that makes much of

00:19:34,830 --> 00:19:40,440
a difference later on a couple of other

00:19:38,520 --> 00:19:42,950
things I noticed with rec is that if you

00:19:40,440 --> 00:19:45,720
specify your endpoints by a domain name

00:19:42,950 --> 00:19:48,120
it's bends a lot of time doing just

00:19:45,720 --> 00:19:50,870
doing DNS lookups this is something that

00:19:48,120 --> 00:19:53,429
maybe we could put a warning in rec for

00:19:50,870 --> 00:19:55,890
rec is basically single fire for every

00:19:53,429 --> 00:19:58,500
single compiled job you run rec once it

00:19:55,890 --> 00:20:00,840
does some stuff and then exits so having

00:19:58,500 --> 00:20:03,659
if you're using a domain name here then

00:20:00,840 --> 00:20:06,169
you have one domain then main lookup for

00:20:03,659 --> 00:20:09,809
every compile step in your build graph

00:20:06,169 --> 00:20:11,730
and there's no good way to handle extra

00:20:09,809 --> 00:20:13,289
inputs yet Rick

00:20:11,730 --> 00:20:16,530
figures out the inputs to a job by

00:20:13,289 --> 00:20:18,330
basically running the compiler with em

00:20:16,530 --> 00:20:20,820
to generate a make file rule with all

00:20:18,330 --> 00:20:22,740
the header dependencies but this doesn't

00:20:20,820 --> 00:20:24,840
cover cover things that aren't source

00:20:22,740 --> 00:20:30,419
dependencies like clang plugins or file

00:20:24,840 --> 00:20:33,450
specified on the command line so this

00:20:30,419 --> 00:20:35,370
tells us that at first I was scared that

00:20:33,450 --> 00:20:38,070
the government client would be switched

00:20:35,370 --> 00:20:40,590
to talk ari API but now it sounds really

00:20:38,070 --> 00:20:43,049
good if I can get our back-end to talk

00:20:40,590 --> 00:20:45,690
the required protocols it would have

00:20:43,049 --> 00:20:49,230
really fast cheap pre-processing we have

00:20:45,690 --> 00:20:51,659
really fast cash to look up it's got

00:20:49,230 --> 00:20:54,330
quite a wide language support so besides

00:20:51,659 --> 00:20:57,270
the C and C++ it's got rust jar and dart

00:20:54,330 --> 00:20:58,650
and it's extensible to more languages if

00:20:57,270 --> 00:21:00,960
you're willing to hack the Gomukh line

00:20:58,650 --> 00:21:02,880
itself and it's easy to drop into other

00:21:00,960 --> 00:21:05,030
build systems this is pretty cool in my

00:21:02,880 --> 00:21:05,030
opinion

00:21:06,950 --> 00:21:12,559
so what do I learn along the way well

00:21:11,419 --> 00:21:14,570
first of all I learnt that cache

00:21:12,559 --> 00:21:16,149
optimization is really difficult and the

00:21:14,570 --> 00:21:18,320
Gummer build there's this inherent

00:21:16,149 --> 00:21:21,109
separation between input files and

00:21:18,320 --> 00:21:23,779
output files and since we're only

00:21:21,109 --> 00:21:26,840
dealing with compiler jobs with garma

00:21:23,779 --> 00:21:28,759
this is fine the output of a compiler is

00:21:26,840 --> 00:21:32,330
never uses the input for a compiler if

00:21:28,759 --> 00:21:34,789
you're discounting linking on the other

00:21:32,330 --> 00:21:36,440
hand our API is more general and maybe

00:21:34,789 --> 00:21:38,210
that's more extensible in the future but

00:21:36,440 --> 00:21:42,049
it's not something I can use at the

00:21:38,210 --> 00:21:43,580
moment and since I can't easily

00:21:42,049 --> 00:21:44,779
distinguish between inputs and outputs I

00:21:43,580 --> 00:21:49,159
basically have to put them all in a

00:21:44,779 --> 00:21:50,659
single storage form and due to the size

00:21:49,159 --> 00:21:51,950
of the chromium code base this means I

00:21:50,659 --> 00:21:56,200
have to store them all on disk at the

00:21:51,950 --> 00:21:58,429
moment I have some ideas for possibly

00:21:56,200 --> 00:22:03,830
optimizing that but I haven't had the

00:21:58,429 --> 00:22:06,529
time to check yet one of these things is

00:22:03,830 --> 00:22:08,480
we could try to do different caching

00:22:06,529 --> 00:22:10,759
behavior depending on the source of the

00:22:08,480 --> 00:22:12,259
uploader so if we see if we can tell the

00:22:10,759 --> 00:22:14,629
difference between the client side and

00:22:12,259 --> 00:22:16,460
the backend side maybe we know that

00:22:14,629 --> 00:22:18,409
inputs are only uploaded from the client

00:22:16,460 --> 00:22:20,029
side so we put them in memory we know

00:22:18,409 --> 00:22:22,359
that outputs are only uploaded from the

00:22:20,029 --> 00:22:24,590
backend side so we put them on disk and

00:22:22,359 --> 00:22:27,470
there's a few corner cases that we can

00:22:24,590 --> 00:22:28,940
decide to do either and this could

00:22:27,470 --> 00:22:31,549
potentially work if we're only

00:22:28,940 --> 00:22:33,350
interested in their compiled service if

00:22:31,549 --> 00:22:36,320
our gummer back-end doesn't need to be a

00:22:33,350 --> 00:22:37,820
general or our Gomez /re API back-end

00:22:36,320 --> 00:22:40,850
doesn't need to be a general service

00:22:37,820 --> 00:22:42,019
then maybe this is okay on the other

00:22:40,850 --> 00:22:42,559
hand maybe we could just had a fancier

00:22:42,019 --> 00:22:46,909
cache

00:22:42,559 --> 00:22:49,249
there's we could have tearing we could

00:22:46,909 --> 00:22:51,529
have lfyou cache where you put the

00:22:49,249 --> 00:22:58,009
frequently most frequently used in RAM

00:22:51,529 --> 00:23:00,289
as well as on disk we'll see another

00:22:58,009 --> 00:23:02,659
thing I notice is that inlining is of

00:23:00,289 --> 00:23:05,059
results is optional in our API this

00:23:02,659 --> 00:23:06,739
seems kind of weird to me if you're

00:23:05,059 --> 00:23:08,269
going to do a round trip to check the

00:23:06,739 --> 00:23:10,179
cache results and the case results

00:23:08,269 --> 00:23:13,669
aren't huge I think they should just be

00:23:10,179 --> 00:23:18,710
returned along with the answer if it's

00:23:13,669 --> 00:23:20,120
in the case or not in in terms of Goma

00:23:18,710 --> 00:23:24,409
there's basically a single

00:23:20,120 --> 00:23:26,240
single round trip for a cache hit on our

00:23:24,409 --> 00:23:28,880
API side you basically have to check a

00:23:26,240 --> 00:23:30,140
whole bunch of things even in the case

00:23:28,880 --> 00:23:31,789
of the case sheet you need to check if

00:23:30,140 --> 00:23:33,679
the results are in lined and they're not

00:23:31,789 --> 00:23:36,049
allowed to be inlined unless the client

00:23:33,679 --> 00:23:38,600
requested it in the first place which is

00:23:36,049 --> 00:23:43,760
fine but I think this the default should

00:23:38,600 --> 00:23:46,610
probably be flipped and that's basically

00:23:43,760 --> 00:23:48,350
it I think this is a really cool

00:23:46,610 --> 00:23:50,090
technology that's really easy to drop in

00:23:48,350 --> 00:23:53,330
to build systems without having to

00:23:50,090 --> 00:23:54,980
switch everything to basil I'd like to

00:23:53,330 --> 00:23:57,559
hear from anybody who's attempted this

00:23:54,980 --> 00:24:01,640
with wreck or Goma or in particular the

00:23:57,559 --> 00:24:10,820
dermis server and there you have it any

00:24:01,640 --> 00:24:13,340
questions I have to say I'm particularly

00:24:10,820 --> 00:24:15,320
interested in this talk I think there's

00:24:13,340 --> 00:24:17,210
a lot of great questions in there and I

00:24:15,320 --> 00:24:20,149
think there probably be some questions

00:24:17,210 --> 00:24:24,500
in the audience I was hoping I anybody

00:24:20,149 --> 00:24:27,620
Igor no okay I'll give a few folks a few

00:24:24,500 --> 00:24:29,809
seconds to think about this because I

00:24:27,620 --> 00:24:31,370
work directly with the Goma team okay

00:24:29,809 --> 00:24:34,730
and work directly with flame to you

00:24:31,370 --> 00:24:36,679
afterwards what you just did I work

00:24:34,730 --> 00:24:40,159
directly with the basil team in the

00:24:36,679 --> 00:24:42,380
remote execution team so you know it

00:24:40,159 --> 00:24:45,020
says this is a timely talk and a good

00:24:42,380 --> 00:24:47,210
discussion we have a question up in the

00:24:45,020 --> 00:24:48,919
balcony hello

00:24:47,210 --> 00:24:50,270
thanks for the talk I had write down

00:24:48,919 --> 00:24:52,730
lots of things with stories next to them

00:24:50,270 --> 00:24:55,520
especially I really liked mentioning

00:24:52,730 --> 00:24:56,990
when the coma field starts it isn't what

00:24:55,520 --> 00:24:58,520
necessarily loaded all of the input

00:24:56,990 --> 00:25:00,559
files for compile unless they're needed

00:24:58,520 --> 00:25:02,080
I thought that was an interesting point

00:25:00,559 --> 00:25:05,179
that I like to maybe elaborate more on

00:25:02,080 --> 00:25:07,549
yeah so the Gummer client has heuristics

00:25:05,179 --> 00:25:09,770
for deciding what it should upload I

00:25:07,549 --> 00:25:13,250
don't know all the details except I

00:25:09,770 --> 00:25:14,720
think it because it runs a server that

00:25:13,250 --> 00:25:16,490
runs during the entire bill that can

00:25:14,720 --> 00:25:18,529
remember things header files that it's

00:25:16,490 --> 00:25:20,330
uploaded previously I think the

00:25:18,529 --> 00:25:22,100
heuristics try to pick out oh this is

00:25:20,330 --> 00:25:25,250
the main source file for the builds that

00:25:22,100 --> 00:25:27,710
we should probably upload that and in

00:25:25,250 --> 00:25:30,110
the case of it guesses wrong it's just

00:25:27,710 --> 00:25:32,120
one more round trip with missing results

00:25:30,110 --> 00:25:34,100
I also leaked the comparison of

00:25:32,120 --> 00:25:35,780
whether you know inlining is optional

00:25:34,100 --> 00:25:36,830
with the re API and kind of the little

00:25:35,780 --> 00:25:42,800
steps required for that that was a

00:25:36,830 --> 00:25:52,010
really effective so yeah I'd love to

00:25:42,800 --> 00:25:55,040
talk to you after this great no I was

00:25:52,010 --> 00:25:57,620
trying to get one of the remote

00:25:55,040 --> 00:26:00,760
execution API team members to stand up

00:25:57,620 --> 00:26:03,050
and say something but so far no luck

00:26:00,760 --> 00:26:04,970
I've seen them around yeah yeah

00:26:03,050 --> 00:26:07,310
certainly I haven't seen anyone from the

00:26:04,970 --> 00:26:10,400
government team here you should there

00:26:07,310 --> 00:26:12,710
they're not local so no that's part of

00:26:10,400 --> 00:26:12,950
the reason so we can certainly chat with

00:26:12,710 --> 00:26:14,360
them

00:26:12,950 --> 00:26:16,490
let's talk offline a little bit about

00:26:14,360 --> 00:26:19,210
this and just give a oh we've another

00:26:16,490 --> 00:26:21,140
question okay separate one nice where

00:26:19,210 --> 00:26:23,000
one thing you mention that I thought was

00:26:21,140 --> 00:26:25,790
interesting was for example when you

00:26:23,000 --> 00:26:27,110
build things and C++ you have to expand

00:26:25,790 --> 00:26:29,750
all these you know transitive includes

00:26:27,110 --> 00:26:31,490
um is that something that is cash isn't

00:26:29,750 --> 00:26:33,320
the contents of that process you

00:26:31,490 --> 00:26:34,730
processor expansion and ER you mentioned

00:26:33,320 --> 00:26:36,980
that there's a special like header

00:26:34,730 --> 00:26:38,330
parser for dependencies but is the

00:26:36,980 --> 00:26:40,400
content of those files cache at all or

00:26:38,330 --> 00:26:42,650
is it still there is a separate cache

00:26:40,400 --> 00:26:44,540
for the content of the header files I

00:26:42,650 --> 00:26:47,450
think there's also a separate cache for

00:26:44,540 --> 00:26:48,830
the results of the custom preprocessor

00:26:47,450 --> 00:26:51,740
so sometimes you don't even need to call

00:26:48,830 --> 00:26:53,240
that preprocessor and the preprocessor

00:26:51,740 --> 00:26:55,100
is optimized it's not a real

00:26:53,240 --> 00:26:58,580
preprocessor it just tells you the

00:26:55,100 --> 00:27:00,710
inputs so it doesn't process all the

00:26:58,580 --> 00:27:02,690
file data thank you so much this

00:27:00,710 --> 00:27:05,930
fantastic oh one other thing there's a

00:27:02,690 --> 00:27:07,520
clang project I think clang scanned dips

00:27:05,930 --> 00:27:11,090
which tries to implement some of the

00:27:07,520 --> 00:27:13,070
same optimizations for clang but I'm not

00:27:11,090 --> 00:27:14,960
sure I haven't tried that in practice

00:27:13,070 --> 00:27:16,190
only found out about that yesterday it's

00:27:14,960 --> 00:27:17,810
very interesting because it's something

00:27:16,190 --> 00:27:20,390
I wouldn't have to implement myself for

00:27:17,810 --> 00:27:22,670
pants so yeah look at that thank you

00:27:20,390 --> 00:27:24,560
very much actually the in the Gummer

00:27:22,670 --> 00:27:25,760
client maybe even the preprocessor is a

00:27:24,560 --> 00:27:26,840
separate binary that you could try

00:27:25,760 --> 00:27:28,430
running awesome

00:27:26,840 --> 00:27:31,400
I think it's BSD license code it's quite

00:27:28,430 --> 00:27:34,680
nice wonderful thank you ok

00:27:31,400 --> 00:27:38,600
all right thank Mauston

00:27:34,680 --> 00:27:38,600

YouTube URL: https://www.youtube.com/watch?v=3w_I9ToY04s


