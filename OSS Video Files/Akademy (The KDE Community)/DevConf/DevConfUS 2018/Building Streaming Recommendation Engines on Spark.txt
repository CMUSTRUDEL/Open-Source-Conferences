Title: Building Streaming Recommendation Engines on Spark
Publication date: 2019-02-21
Playlist: DevConfUS 2018
Description: 
	Collaborative filtering is a well known method to implement recommendation engines. Although modern techniques, such as Alternating Least Squares (ALS), allow us to perform rating predictions with large amounts of observations, typically ALS is implemented as a distributed batch algorithm where retraining must be performed with the entirety of the data. However, when dealing with large amounts of data as a stream, batch retraining might be problematic. In this talk Rui will guide us in building a streaming ALS implementation using Apache Spark and based on Stochastic Gradient Descent, where training can be performed using observations as they arrive. The advantages of real-time streaming collaborative filtering will be discussed as well as the scenarios where batch ALS might be preferable.
Captions: 
	00:00:04,330 --> 00:00:08,519
[Music]

00:00:16,260 --> 00:00:32,290
[Music]

00:00:28,840 --> 00:00:33,220
alright ladies and gentlemen next up we

00:00:32,290 --> 00:00:37,180
have with us

00:00:33,220 --> 00:00:39,879
Rivera software engineer at rad

00:00:37,180 --> 00:00:42,340
analytics dot IO and he's gonna be

00:00:39,879 --> 00:00:45,000
talking about building streaming

00:00:42,340 --> 00:00:49,980
recommendation engines on spark I

00:00:45,000 --> 00:00:51,300
request you all to our seat and welcome

00:00:49,980 --> 00:00:55,370
[Music]

00:00:51,300 --> 00:00:56,920
everyone thank you very much for hear me

00:00:55,370 --> 00:00:59,060
[Music]

00:00:56,920 --> 00:01:01,560
hello

00:00:59,060 --> 00:01:05,350
[Music]

00:01:01,560 --> 00:01:07,450
yeah so I'll use this one okay thank you

00:01:05,350 --> 00:01:11,310
so hi everyone thank you very much for

00:01:07,450 --> 00:01:11,310
coming so my name is Floyd

00:01:12,130 --> 00:01:15,239
[Music]

00:01:16,530 --> 00:01:21,540
thank you right okay sorry about that

00:01:18,870 --> 00:01:23,070
thank you so my name is Roy as you just

00:01:21,540 --> 00:01:25,410
heard and I'd like to talk a little bit

00:01:23,070 --> 00:01:27,300
today about building streaming

00:01:25,410 --> 00:01:31,560
distributed streaming recommendation

00:01:27,300 --> 00:01:34,229
engines on spark and I'd like to talk a

00:01:31,560 --> 00:01:36,119
little bit about batch streaming of bats

00:01:34,229 --> 00:01:37,979
recommendation engines with a common

00:01:36,119 --> 00:01:41,460
approach of the viewing recommendation

00:01:37,979 --> 00:01:43,260
engines and also how easy it is on one

00:01:41,460 --> 00:01:45,000
way to build this kind of distributors

00:01:43,260 --> 00:01:47,160
recommendation engines but on the other

00:01:45,000 --> 00:01:49,649
hand our building them in the streaming

00:01:47,160 --> 00:01:51,420
and distributed way can be tricky so we

00:01:49,649 --> 00:01:55,770
have some some road blocks might

00:01:51,420 --> 00:01:57,840
assemble one so I will introduce the

00:01:55,770 --> 00:01:59,520
concept of collaborative filtering which

00:01:57,840 --> 00:02:02,070
is the most common way of producing

00:01:59,520 --> 00:02:04,740
recommendation engines and also cobalt 2

00:02:02,070 --> 00:02:06,660
variants which is the match alternating

00:02:04,740 --> 00:02:09,060
with squares and streaming algebra to

00:02:06,660 --> 00:02:12,420
Express it also introduced a little bit

00:02:09,060 --> 00:02:14,420
about fascist our chemistry some of you

00:02:12,420 --> 00:02:16,709
are familiar with and I'll talk about

00:02:14,420 --> 00:02:19,530
implementation on top of Apache spark of

00:02:16,709 --> 00:02:21,840
distributed streaming AOS and finally

00:02:19,530 --> 00:02:24,780
I'll talk a bit about how to deploy this

00:02:21,840 --> 00:02:27,780
kind of setups on on a module called

00:02:24,780 --> 00:02:30,120
environment such as open shift

00:02:27,780 --> 00:02:32,010
so what is collaborative filtering so I

00:02:30,120 --> 00:02:33,660
mean first let's talk about recommender

00:02:32,010 --> 00:02:36,030
system so recommender systems are

00:02:33,660 --> 00:02:39,990
popular methods of matching historical

00:02:36,030 --> 00:02:41,670
data from users products and the rating

00:02:39,990 --> 00:02:43,890
that you have so the connection between

00:02:41,670 --> 00:02:45,780
those users and products usually you

00:02:43,890 --> 00:02:48,330
have like a unique relation between a

00:02:45,780 --> 00:02:52,170
user products in the rating so say we

00:02:48,330 --> 00:02:54,030
figure to a movie website where you want

00:02:52,170 --> 00:02:56,459
to see any movies or buy a new movie my

00:02:54,030 --> 00:02:58,440
system so movies are you recommend one

00:02:56,459 --> 00:03:00,480
so if you give it five stars you're

00:02:58,440 --> 00:03:02,670
gonna have a unique relation between

00:03:00,480 --> 00:03:05,220
yourself the user the products a movie

00:03:02,670 --> 00:03:06,810
and directly that you just killed and in

00:03:05,220 --> 00:03:08,970
this jargon climate collaborative

00:03:06,810 --> 00:03:11,670
filtering collaborative just means using

00:03:08,970 --> 00:03:13,770
all of the data as you have globally

00:03:11,670 --> 00:03:15,480
from all the users and filtering is

00:03:13,770 --> 00:03:17,520
basically predictive so you're basically

00:03:15,480 --> 00:03:21,720
doing projections on things we already

00:03:17,520 --> 00:03:25,380
have so in a way collaborative filtering

00:03:21,720 --> 00:03:27,270
we use it in our everyday life so and

00:03:25,380 --> 00:03:29,640
it's quite common sense if you think

00:03:27,270 --> 00:03:31,140
about it so let's assume the main idea

00:03:29,640 --> 00:03:33,690
behind it is let's assume you have two

00:03:31,140 --> 00:03:35,430
groups of people so wonderful is Group A

00:03:33,690 --> 00:03:38,370
which is a group of people with which

00:03:35,430 --> 00:03:40,380
she shared a musical tastes so

00:03:38,370 --> 00:03:42,510
everything their wives use alike and you

00:03:40,380 --> 00:03:44,130
have a group B which is people which you

00:03:42,510 --> 00:03:45,870
don't share any kind of disco taste so

00:03:44,130 --> 00:03:48,450
everything they like a musical aids

00:03:45,870 --> 00:03:50,459
so if Group A recommends you an album

00:03:48,450 --> 00:03:53,510
and group V recommends you another album

00:03:50,459 --> 00:03:55,830
so which one are you probably gonna buy

00:03:53,510 --> 00:03:57,870
you're probably gonna buy group a right

00:03:55,830 --> 00:04:00,269
so yeah so let's visit the collaborative

00:03:57,870 --> 00:04:03,600
filtering in a wage so as a bonus

00:04:00,269 --> 00:04:07,049
question this would be says an album is

00:04:03,600 --> 00:04:09,329
really years or citizen up is really bad

00:04:07,049 --> 00:04:11,510
sorry does that mean you're gonna like

00:04:09,329 --> 00:04:11,510
us

00:04:12,240 --> 00:04:16,410
you know because you really don't have

00:04:14,430 --> 00:04:18,600
like a new formative relation between

00:04:16,410 --> 00:04:20,010
that date I had the one before differ if

00:04:18,600 --> 00:04:21,450
you have so it's a different kind of

00:04:20,010 --> 00:04:24,990
relation so you can't really say even

00:04:21,450 --> 00:04:26,280
like that outward so one of the most

00:04:24,990 --> 00:04:28,530
popular methods for collaborative

00:04:26,280 --> 00:04:31,680
filtering is alternative way squares and

00:04:28,530 --> 00:04:34,500
in ALS we assume that we have all of the

00:04:31,680 --> 00:04:36,750
data organized as a sequential ordering

00:04:34,500 --> 00:04:38,910
of users and progress and you can build

00:04:36,750 --> 00:04:39,990
a kind of matrix it's a natural way of

00:04:38,910 --> 00:04:41,820
displaying the states so we have a

00:04:39,990 --> 00:04:44,850
matrix representing all the ratings and

00:04:41,820 --> 00:04:46,560
this is a sparse matrix so obviously you

00:04:44,850 --> 00:04:48,660
can have some ratings missing from there

00:04:46,560 --> 00:04:51,480
so not all of the users raised all of

00:04:48,660 --> 00:04:52,890
the problems and each entry represents a

00:04:51,480 --> 00:04:55,470
unique innovation between the user and

00:04:52,890 --> 00:04:58,050
the products so what are we doing with

00:04:55,470 --> 00:05:00,870
ALS in a nutshell is like we try to

00:04:58,050 --> 00:05:02,640
factorize this big ratings matrix into

00:05:00,870 --> 00:05:04,290
two weighting factor matrix is called

00:05:02,640 --> 00:05:06,660
we're just gonna call them U and P here

00:05:04,290 --> 00:05:08,100
and these two factors when it multiplies

00:05:06,660 --> 00:05:10,620
back together they're gonna give an

00:05:08,100 --> 00:05:11,940
approximation of the ratings matrix and

00:05:10,620 --> 00:05:13,680
that approximation is going to include

00:05:11,940 --> 00:05:15,120
all the ratings that we're missing and

00:05:13,680 --> 00:05:18,780
that's going to be a prediction of the

00:05:15,120 --> 00:05:19,950
way choose another listing so classical

00:05:18,780 --> 00:05:22,500
way of doing this is using a batch

00:05:19,950 --> 00:05:25,380
method right so in a batch method what

00:05:22,500 --> 00:05:27,870
we do is the factorized issues is done

00:05:25,380 --> 00:05:30,390
by defining a loss function so basically

00:05:27,870 --> 00:05:31,710
the final loss function having an error

00:05:30,390 --> 00:05:33,480
term there which is the difference

00:05:31,710 --> 00:05:35,160
between the actual ratings that you have

00:05:33,480 --> 00:05:36,840
and the projects is the prediction of

00:05:35,160 --> 00:05:38,639
the right you can have and you have some

00:05:36,840 --> 00:05:41,490
regularization terms right

00:05:38,639 --> 00:05:42,240
and this was function has to be

00:05:41,490 --> 00:05:44,639
minimized

00:05:42,240 --> 00:05:46,949
unfortunately using ALS this was this

00:05:44,639 --> 00:05:48,689
actual minimization problem as a close

00:05:46,949 --> 00:05:50,669
form the form solution so you can

00:05:48,689 --> 00:05:53,129
actually set the derivatives of loss

00:05:50,669 --> 00:05:55,319
functioning or the review and p20 you

00:05:53,129 --> 00:05:57,780
have a nice set of linear equations

00:05:55,319 --> 00:05:59,789
which you can solve by iterations and so

00:05:57,780 --> 00:06:02,939
that's quite handy right so the way we

00:05:59,789 --> 00:06:05,250
do it is we fix one of the factor matrix

00:06:02,939 --> 00:06:07,319
is we solve the estimator in order the

00:06:05,250 --> 00:06:09,270
other one and then we just iterate the

00:06:07,319 --> 00:06:10,800
process going back to forwards we have

00:06:09,270 --> 00:06:12,180
in one six on the other six so

00:06:10,800 --> 00:06:13,770
eventually this process is going to

00:06:12,180 --> 00:06:16,529
converge and you're gonna have a very

00:06:13,770 --> 00:06:19,800
good approximation of the ratings matrix

00:06:16,529 --> 00:06:21,449
and finally again what you have is

00:06:19,800 --> 00:06:23,550
something like this so you still have

00:06:21,449 --> 00:06:26,099
the data that you actually has and in

00:06:23,550 --> 00:06:27,270
red you have an approximation and what

00:06:26,099 --> 00:06:29,520
is the proximation means mathematically

00:06:27,270 --> 00:06:32,580
is that these values are going to be the

00:06:29,520 --> 00:06:35,400
ones which actually minimize that äôs

00:06:32,580 --> 00:06:36,960
recursion right so it's gonna be you can

00:06:35,400 --> 00:06:38,879
bet there's probably under the each have

00:06:36,960 --> 00:06:42,090
enough states are good enough situations

00:06:38,879 --> 00:06:44,699
are good approximation so to visualize

00:06:42,090 --> 00:06:46,770
this let's imagine a very quirky shop so

00:06:44,699 --> 00:06:48,270
you have a shop where you have 300

00:06:46,770 --> 00:06:50,550
products the only sell stream the

00:06:48,270 --> 00:06:53,400
products and you only have 300 customers

00:06:50,550 --> 00:06:55,889
right and to be even more quirky each

00:06:53,400 --> 00:06:58,199
customer can give a rating in 8 bits so

00:06:55,889 --> 00:07:01,919
you can leave any rating from 1 to 256

00:06:58,199 --> 00:07:04,319
right and as we're humans and we

00:07:01,919 --> 00:07:05,969
visualize that things patterns and

00:07:04,319 --> 00:07:07,409
colors and we're doing numbers

00:07:05,969 --> 00:07:09,719
what's the sender collect to these

00:07:07,409 --> 00:07:12,360
numbers right so I think you know where

00:07:09,719 --> 00:07:14,069
this is going so you can build a ratings

00:07:12,360 --> 00:07:17,310
matrix that looks like something we can

00:07:14,069 --> 00:07:20,979
see right so this is our economy arrays

00:07:17,310 --> 00:07:23,199
so if you use ALS to solve this problem

00:07:20,979 --> 00:07:25,479
we need to go about it so first we fill

00:07:23,199 --> 00:07:27,310
the election structures with random

00:07:25,479 --> 00:07:28,960
numbers so obviously feel some random

00:07:27,310 --> 00:07:30,490
numbers you know the initial

00:07:28,960 --> 00:07:32,710
approximation is going to be a random

00:07:30,490 --> 00:07:34,509
matrix so that makes sense and then you

00:07:32,710 --> 00:07:37,060
start situation and as you go about

00:07:34,509 --> 00:07:38,860
situation you can see that well it's

00:07:37,060 --> 00:07:40,240
actually quite a good job right so after

00:07:38,860 --> 00:07:42,819
a few iterations is actually

00:07:40,240 --> 00:07:46,150
approximating the ratings matrix that we

00:07:42,819 --> 00:07:47,800
had originally so it works but it's

00:07:46,150 --> 00:07:49,449
expected to work in this case so this is

00:07:47,800 --> 00:07:51,639
probably the simplest case where as you

00:07:49,449 --> 00:07:54,430
can imagine so it's a very small dataset

00:07:51,639 --> 00:07:56,469
you have all the ratings you know all

00:07:54,430 --> 00:07:59,560
the ratings and it's not a distributed

00:07:56,469 --> 00:08:02,860
system so no Travis you to fall upon so

00:07:59,560 --> 00:08:05,469
of course is going to work so you might

00:08:02,860 --> 00:08:07,090
be thinking well this is all nice but we

00:08:05,469 --> 00:08:09,580
can do this in the streaming way right

00:08:07,090 --> 00:08:10,659
so if we're feeling with approximating

00:08:09,580 --> 00:08:12,819
that matrix we have four new

00:08:10,659 --> 00:08:14,979
observations new ratings or get new

00:08:12,819 --> 00:08:16,419
users or new processes released we can

00:08:14,979 --> 00:08:19,419
just recalculate the whole thing right

00:08:16,419 --> 00:08:20,949
oh yes you can but in one way that's

00:08:19,419 --> 00:08:23,289
something to be streaming just because

00:08:20,949 --> 00:08:24,789
of you technicalities one of which is

00:08:23,289 --> 00:08:26,949
you have to keep the whole of the data

00:08:24,789 --> 00:08:29,379
so you're not using a stream

00:08:26,949 --> 00:08:31,210
implementation because obviously you get

00:08:29,379 --> 00:08:33,070
a new data that you have to storage the

00:08:31,210 --> 00:08:35,380
entirety of the historical data you

00:08:33,070 --> 00:08:37,390
can't approximately new matrix to just

00:08:35,380 --> 00:08:40,120
set one new rate see that she has and

00:08:37,390 --> 00:08:41,979
secondly if you want to do this in your

00:08:40,120 --> 00:08:44,320
real time it might be a bit problematic

00:08:41,979 --> 00:08:46,839
because you have to imagine if you're a

00:08:44,320 --> 00:08:48,660
shop or a company that has millions of

00:08:46,839 --> 00:08:50,649
users and millions of products obviously

00:08:48,660 --> 00:08:52,800
it's going to be a bit tricky to

00:08:50,649 --> 00:08:56,320
recalculate the whole thing in real time

00:08:52,800 --> 00:08:58,120
so how do we go about it so we want to

00:08:56,320 --> 00:09:00,550
work in a method that allows us to do

00:08:58,120 --> 00:09:01,940
this factorization with just one rating

00:09:00,550 --> 00:09:03,560
or few ratings on

00:09:01,940 --> 00:09:04,880
and fortunately there is a method for

00:09:03,560 --> 00:09:06,470
that it's called the sequester during

00:09:04,880 --> 00:09:09,680
the descent apply to the factorization

00:09:06,470 --> 00:09:11,600
and the specific method we're going to

00:09:09,680 --> 00:09:15,580
use this bias to test integrated descent

00:09:11,600 --> 00:09:18,050
to factorize the ratings matrix so

00:09:15,580 --> 00:09:20,960
what's the difference between successive

00:09:18,050 --> 00:09:23,270
innocence and natural so basically we

00:09:20,960 --> 00:09:25,340
introduced a new concept and this is the

00:09:23,270 --> 00:09:27,950
concept of bias so here we have a bias

00:09:25,340 --> 00:09:31,550
between x and y so X is a user wise

00:09:27,950 --> 00:09:34,130
product and the bias between device

00:09:31,550 --> 00:09:36,200
associated with a browser to the user is

00:09:34,130 --> 00:09:38,540
going to be mu which is the average

00:09:36,200 --> 00:09:40,220
global average of the biases of the

00:09:38,540 --> 00:09:42,440
races I should have sorry and you have

00:09:40,220 --> 00:09:46,090
kind of you have DX which is basically

00:09:42,440 --> 00:09:49,610
how much those rating deviates from the

00:09:46,090 --> 00:09:52,220
average you have for all the users or

00:09:49,610 --> 00:09:54,320
from other products and then the new

00:09:52,220 --> 00:09:56,360
proximities ratings just going to be the

00:09:54,320 --> 00:09:59,990
batch case versus future

00:09:56,360 --> 00:10:02,870
sorry possible biases so if we replace

00:09:59,990 --> 00:10:05,270
this into the the last function so we

00:10:02,870 --> 00:10:09,500
can just define a new boss function for

00:10:05,270 --> 00:10:11,420
the streaming case but now we just don't

00:10:09,500 --> 00:10:13,310
have the new projections in the wall

00:10:11,420 --> 00:10:17,110
function for some regularization terms

00:10:13,310 --> 00:10:19,070
that renouncement go into them so

00:10:17,110 --> 00:10:22,280
calculating the soup gradients is quite

00:10:19,070 --> 00:10:25,520
expensive so we got to calculate them

00:10:22,280 --> 00:10:26,810
for a single observation so we as you

00:10:25,520 --> 00:10:29,060
can see from the other states of the

00:10:26,810 --> 00:10:30,340
biases and the particular factors we can

00:10:29,060 --> 00:10:33,590
do this

00:10:30,340 --> 00:10:35,330
it's just with one observation at a time

00:10:33,590 --> 00:10:38,090
this is actually what we want right so

00:10:35,330 --> 00:10:41,510
provided we has a single rating the

00:10:38,090 --> 00:10:43,130
rating of the user X in products why we

00:10:41,510 --> 00:10:44,300
cannot predict the biases as well as the

00:10:43,130 --> 00:10:46,490
earthen structures and that's one of

00:10:44,300 --> 00:10:48,680
ours to do the factorization movie ocean

00:10:46,490 --> 00:10:51,560
and an interesting phase at this method

00:10:48,680 --> 00:10:55,400
also has a convergence property as a

00:10:51,560 --> 00:10:57,950
fashion item has so the practical

00:10:55,400 --> 00:11:00,920
difference is in terms of the streaming

00:10:57,950 --> 00:11:02,960
nature is obvious now I hope is that in

00:11:00,920 --> 00:11:05,240
both methods the objective is to

00:11:02,960 --> 00:11:07,910
estimate selection structures right but

00:11:05,240 --> 00:11:09,290
in one method so in the batch method

00:11:07,910 --> 00:11:11,480
whenever you get a new observation

00:11:09,290 --> 00:11:13,070
you're going to have to recalculate the

00:11:11,480 --> 00:11:16,760
factors which are easier in nature

00:11:13,070 --> 00:11:18,710
whereas in the streaming version

00:11:16,760 --> 00:11:21,410
whenever you get a new observation you

00:11:18,710 --> 00:11:23,780
just update the great density weights to

00:11:21,410 --> 00:11:26,480
a specific row or a specific comment so

00:11:23,780 --> 00:11:29,980
the user and products out of the that

00:11:26,480 --> 00:11:32,380
specific user and and products

00:11:29,980 --> 00:11:34,450
and it is important to note that under

00:11:32,380 --> 00:11:36,070
under a certain point of view this

00:11:34,450 --> 00:11:38,800
methods aim it's exactly the same thing

00:11:36,070 --> 00:11:40,390
so they both payments calculating the

00:11:38,800 --> 00:11:41,680
weighting factors and from that to make

00:11:40,390 --> 00:11:42,730
the predictions it's just the way they

00:11:41,680 --> 00:11:45,580
used to take that is going to be

00:11:42,730 --> 00:11:47,620
significantly different so I've spoken

00:11:45,580 --> 00:11:50,920
an illustration with the same dataset of

00:11:47,620 --> 00:11:53,560
the streaming case and we're gonna use

00:11:50,920 --> 00:11:55,570
the same manufacturer ratings data and

00:11:53,560 --> 00:11:58,960
as you can see the conversion here seems

00:11:55,570 --> 00:12:00,580
to be happening but slower that is to be

00:11:58,960 --> 00:12:01,960
expected because now we're not be using

00:12:00,580 --> 00:12:04,300
the entirety of the data's just one

00:12:01,960 --> 00:12:06,820
observation at a time but in the end we

00:12:04,300 --> 00:12:07,990
can defend that conversion to a single

00:12:06,820 --> 00:12:10,450
resource we're getting into the

00:12:07,990 --> 00:12:12,490
approximation of your ratings matrix and

00:12:10,450 --> 00:12:15,100
again this is a simple example so this

00:12:12,490 --> 00:12:16,090
is with a small dataset in a local

00:12:15,100 --> 00:12:17,890
machine there's no distribution

00:12:16,090 --> 00:12:19,870
happening but we don't want this right

00:12:17,890 --> 00:12:21,640
we want you to try this with the data

00:12:19,870 --> 00:12:24,370
sets in production so you want to

00:12:21,640 --> 00:12:26,290
actually implement something that works

00:12:24,370 --> 00:12:29,110
at scale so something that might work

00:12:26,290 --> 00:12:31,090
big amounts of nature and to do that

00:12:29,110 --> 00:12:33,730
we're gonna use spark so I'm assuming

00:12:31,090 --> 00:12:36,190
that some of you are some going this

00:12:33,730 --> 00:12:38,640
fall so he who here has worked with

00:12:36,190 --> 00:12:38,640
apache spark

00:12:39,670 --> 00:12:45,790
they're not not as many people so I'm

00:12:43,570 --> 00:12:48,610
just gonna do like the ten-second - or

00:12:45,790 --> 00:12:51,910
introduction of Apache spark I hope IOP

00:12:48,610 --> 00:12:55,810
enhances describes faithful las partes

00:12:51,910 --> 00:12:58,230
so spark the spark is a framework that

00:12:55,810 --> 00:13:01,180
allows you to do distributed

00:12:58,230 --> 00:13:03,760
calculations of scale and it provides

00:13:01,180 --> 00:13:05,890
several core different structures such

00:13:03,760 --> 00:13:09,070
as resilient it's asset distribution

00:13:05,890 --> 00:13:12,040
data sets and data frames and data sets

00:13:09,070 --> 00:13:14,140
and the RTD the resilience your if you

00:13:12,040 --> 00:13:17,080
like to set is an immutable distributed

00:13:14,140 --> 00:13:18,700
tax collection of objects and what does

00:13:17,080 --> 00:13:20,050
that mean it means that when it creates

00:13:18,700 --> 00:13:22,060
one of these rdd's

00:13:20,050 --> 00:13:24,790
they're actually mapped across your

00:13:22,060 --> 00:13:26,920
cluster right and as a removable what

00:13:24,790 --> 00:13:29,530
happens is you can actually map your

00:13:26,920 --> 00:13:31,510
your computations to each of the

00:13:29,530 --> 00:13:33,100
clusters so the calculations are done in

00:13:31,510 --> 00:13:35,110
parallel at the clusters and then it

00:13:33,100 --> 00:13:39,070
just aggravates a results back so this

00:13:35,110 --> 00:13:41,050
allows for for for a very natural way of

00:13:39,070 --> 00:13:43,300
distributing computation if you can

00:13:41,050 --> 00:13:46,090
translate your algorithm in this kind of

00:13:43,300 --> 00:13:48,880
distributed invincible operations and

00:13:46,090 --> 00:13:51,070
for the streaming iOS application we're

00:13:48,880 --> 00:13:52,980
going to use re G's as the core data

00:13:51,070 --> 00:13:57,300
structure

00:13:52,980 --> 00:14:00,270
so Apache spark already provides in its

00:13:57,300 --> 00:14:03,000
ml weave library an implementation of

00:14:00,270 --> 00:14:05,100
ALS adoption to this place but it's a

00:14:03,000 --> 00:14:08,130
badge implementation of alternative

00:14:05,100 --> 00:14:11,130
script but it is a very performant one

00:14:08,130 --> 00:14:13,770
it works very well if you for extremes

00:14:11,130 --> 00:14:16,170
just use it by all means and it has a

00:14:13,770 --> 00:14:18,600
very simple API so basically if you're

00:14:16,170 --> 00:14:22,500
in a model you just use a few just a few

00:14:18,600 --> 00:14:24,750
constants so basically you need what is

00:14:22,500 --> 00:14:26,430
called a rating an erection is just a

00:14:24,750 --> 00:14:28,560
wrapper around the quantities you

00:14:26,430 --> 00:14:31,020
mentioned so the user ID the Product ID

00:14:28,560 --> 00:14:33,930
and the rating and you have the ratings

00:14:31,020 --> 00:14:38,280
which is just your matrix is just an RVT

00:14:33,930 --> 00:14:41,640
of radius you also need a rank which is

00:14:38,280 --> 00:14:44,190
it corresponds to the number of elements

00:14:41,640 --> 00:14:45,720
in each of the columns or rows of the

00:14:44,190 --> 00:14:48,930
lighting structures that you mentioned

00:14:45,720 --> 00:14:51,870
and he also needs iterations which is

00:14:48,930 --> 00:14:54,690
basically the hearts of on how when

00:14:51,870 --> 00:14:56,190
should the feature to process stop so

00:14:54,690 --> 00:14:58,320
this is quite useful because you

00:14:56,190 --> 00:14:59,820
actually can know that the problem is

00:14:58,320 --> 00:15:01,560
computationally bound so you know

00:14:59,820 --> 00:15:04,080
something to last forever you can say

00:15:01,560 --> 00:15:05,550
well after 100 iterations this is gonna

00:15:04,080 --> 00:15:07,270
be a vision else approximation we can

00:15:05,550 --> 00:15:09,480
stop

00:15:07,270 --> 00:15:12,100
it allows you to pass also some

00:15:09,480 --> 00:15:16,890
regularization term as soon as machine

00:15:12,100 --> 00:15:16,890
such as level I'm sorry

00:15:20,370 --> 00:15:25,889
so the way to train a model is quite

00:15:23,449 --> 00:15:28,230
straightforward as I mentioned so

00:15:25,889 --> 00:15:30,600
basically you just pass through the ALS

00:15:28,230 --> 00:15:32,730
class into there's an EOS object you

00:15:30,600 --> 00:15:34,980
just pass the ratings or rank situations

00:15:32,730 --> 00:15:37,680
in the lambda and what you get back is

00:15:34,980 --> 00:15:39,839
basically a class called make you start

00:15:37,680 --> 00:15:43,470
transition model which is just a wrapper

00:15:39,839 --> 00:15:48,180
around that you awaken relations to the

00:15:43,470 --> 00:15:50,110
fracture matrices like that fixing and

00:15:48,180 --> 00:15:52,279
[Music]

00:15:50,110 --> 00:15:55,089
this is to work obviously in the batch

00:15:52,279 --> 00:15:57,890
static right but to actually work in a

00:15:55,089 --> 00:15:59,390
in a streaming implementation we're

00:15:57,890 --> 00:16:00,860
gonna need a streaming data source and

00:15:59,390 --> 00:16:04,279
the streaming data source that we

00:16:00,860 --> 00:16:06,800
decided to use is the sparks disparate

00:16:04,279 --> 00:16:09,560
eyes streams or D streams and they

00:16:06,800 --> 00:16:12,230
basically work as many batches of our

00:16:09,560 --> 00:16:13,820
duties over the over a certain time

00:16:12,230 --> 00:16:16,579
window so basically you're gonna get

00:16:13,820 --> 00:16:18,950
batches of resilience will get the sets

00:16:16,579 --> 00:16:21,440
over a certain frequency and time window

00:16:18,950 --> 00:16:22,730
and then you can process you can apply

00:16:21,440 --> 00:16:27,350
the process to each of these mini

00:16:22,730 --> 00:16:30,310
batches so I mean an important thing I'm

00:16:27,350 --> 00:16:30,310
sorry if I can just scroll back

00:16:31,460 --> 00:16:37,460
so an important thing to notice about or

00:16:34,910 --> 00:16:40,550
advantage of these disparate ice creams

00:16:37,460 --> 00:16:42,800
or these dreams is that we now if we use

00:16:40,550 --> 00:16:44,990
this under stimulus we no longer need to

00:16:42,800 --> 00:16:47,420
keep the entirety of the data in memory

00:16:44,990 --> 00:16:48,649
or at or even access it so basically if

00:16:47,420 --> 00:16:49,910
you can imagine the case that I

00:16:48,649 --> 00:16:51,709
mentioned if you have millions of

00:16:49,910 --> 00:16:53,180
products and millions of users now if

00:16:51,709 --> 00:16:53,750
you get a mini batch with just a few

00:16:53,180 --> 00:16:55,610
Rachel's

00:16:53,750 --> 00:16:58,550
you don't need to say region takes place

00:16:55,610 --> 00:16:59,930
with several hundred million Rachel's to

00:16:58,550 --> 00:17:01,430
review the whole process you can just

00:16:59,930 --> 00:17:06,650
use detective that should having that

00:17:01,430 --> 00:17:08,540
really bad so we wanted to choose the ML

00:17:06,650 --> 00:17:11,329
Web API is quite the straightforward and

00:17:08,540 --> 00:17:14,449
intuitive we wanted to use the same API

00:17:11,329 --> 00:17:16,429
on the streaming on the streaming ALS so

00:17:14,449 --> 00:17:18,650
we're going to use the same type of

00:17:16,429 --> 00:17:20,329
commands and the way we're going to do

00:17:18,650 --> 00:17:22,640
this is initially when we don't have any

00:17:20,329 --> 00:17:25,189
model or data we're gonna create a model

00:17:22,640 --> 00:17:27,050
with the initial ru TV initial media

00:17:25,189 --> 00:17:29,870
patch and then from that model we're

00:17:27,050 --> 00:17:31,010
gonna update it with the mini patches

00:17:29,870 --> 00:17:32,240
the kind of code they're gonna come

00:17:31,010 --> 00:17:34,760
afterwards so you're gonna be

00:17:32,240 --> 00:17:37,850
continuously updating the model has new

00:17:34,760 --> 00:17:40,100
mini batches of data right so what do we

00:17:37,850 --> 00:17:41,929
need to train a model so now I'm just

00:17:40,100 --> 00:17:44,120
gonna give you a few of the steps is

00:17:41,929 --> 00:17:46,370
actually like the algorithmic steps of

00:17:44,120 --> 00:17:48,350
going from that initial mini batch to

00:17:46,370 --> 00:17:50,510
train model it's just going to be a

00:17:48,350 --> 00:17:52,220
couple of steps they're gonna go into

00:17:50,510 --> 00:17:55,520
some detail but hopefully it's gonna

00:17:52,220 --> 00:17:57,530
give you an idea of how it is tricky to

00:17:55,520 --> 00:18:00,050
implement like this kind of operates in

00:17:57,530 --> 00:18:01,340
a distributed way but in a way just the

00:18:00,050 --> 00:18:03,230
flip side of it is actually you get

00:18:01,340 --> 00:18:05,360
obviously a distributed recommendation

00:18:03,230 --> 00:18:06,380
engine which is quite performance so

00:18:05,360 --> 00:18:08,600
what we need to get tomorrow

00:18:06,380 --> 00:18:10,580
so as we seen from if you remember from

00:18:08,600 --> 00:18:12,710
the initial slides from the formulas so

00:18:10,580 --> 00:18:14,179
we need actually these quantities to

00:18:12,710 --> 00:18:16,190
have a train model so once we have them

00:18:14,179 --> 00:18:19,070
we can say we have a train model and we

00:18:16,190 --> 00:18:21,020
can perform predictions so I starts with

00:18:19,070 --> 00:18:23,809
looking at user working factors

00:18:21,020 --> 00:18:26,059
so these operations are going to be

00:18:23,809 --> 00:18:28,400
identical from one image to the other so

00:18:26,059 --> 00:18:29,840
the same set of operations you're gonna

00:18:28,400 --> 00:18:31,260
do on the first mini batch you just

00:18:29,840 --> 00:18:32,760
repeat them with a new data

00:18:31,260 --> 00:18:36,570
and you get to continuously updated

00:18:32,760 --> 00:18:38,990
model so to calculate the user and

00:18:36,570 --> 00:18:42,179
weighting factors so what we need is

00:18:38,990 --> 00:18:45,780
well I can do a bad shape as we get a

00:18:42,179 --> 00:18:47,760
not easy of rattles right and this

00:18:45,780 --> 00:18:53,910
corresponds to the ratings that each

00:18:47,760 --> 00:18:55,320
user gives for products the first thing

00:18:53,910 --> 00:18:58,500
we're going to do is we're going to flip

00:18:55,320 --> 00:19:01,049
this RGV into two parties so one key

00:18:58,500 --> 00:19:04,169
invite user and one key by-product right

00:19:01,049 --> 00:19:05,669
and this will allow us to compute away

00:19:04,169 --> 00:19:07,200
from factories now we're gonna do them

00:19:05,669 --> 00:19:09,270
so you guys just to keep in mind that

00:19:07,200 --> 00:19:11,040
this is the first initial step we have

00:19:09,270 --> 00:19:13,830
no model we have no a constructors who

00:19:11,040 --> 00:19:16,169
don't have anything so the first thing

00:19:13,830 --> 00:19:18,000
that we do is for each of these two

00:19:16,169 --> 00:19:20,040
bodies that you created we're gonna

00:19:18,000 --> 00:19:21,540
generate a random feature venture and

00:19:20,040 --> 00:19:24,299
the way going to do it to just create a

00:19:21,540 --> 00:19:26,250
feature vector of rank R so the rank

00:19:24,299 --> 00:19:28,440
that we decided and we just see we can

00:19:26,250 --> 00:19:30,120
randomly default values and we teach of

00:19:28,440 --> 00:19:32,370
those future ventures with an associate

00:19:30,120 --> 00:19:36,090
to random bias as well right so we can

00:19:32,370 --> 00:19:38,350
do that it's quite easy so the next step

00:19:36,090 --> 00:19:41,919
is to

00:19:38,350 --> 00:19:44,679
because we actually split the RDG into

00:19:41,919 --> 00:19:45,340
users and puzzles keys one by users or

00:19:44,679 --> 00:19:47,019
metros

00:19:45,340 --> 00:19:48,549
we might have some duplicate or uses of

00:19:47,019 --> 00:19:50,919
presence in these are TV so you can

00:19:48,549 --> 00:19:52,600
imagine the case where if you rape the

00:19:50,919 --> 00:19:54,580
two movies obviously you're gonna be on

00:19:52,600 --> 00:19:57,250
queue of entries of these are you right

00:19:54,580 --> 00:20:01,570
so twice in the users and twice any

00:19:57,250 --> 00:20:03,519
progress so what actually we're gonna do

00:20:01,570 --> 00:20:05,529
we're gonna join these ratings which in

00:20:03,519 --> 00:20:08,110
turn will return the data set consisting

00:20:05,529 --> 00:20:12,370
of product IDs user IDs ratings and

00:20:08,110 --> 00:20:14,710
these are factors so we join them and we

00:20:12,370 --> 00:20:17,289
get to return is we have this mappings

00:20:14,710 --> 00:20:22,539
between the user the project's bias and

00:20:17,289 --> 00:20:24,220
the future ventures so finally it's just

00:20:22,539 --> 00:20:26,649
a couple of steps more if you see it's

00:20:24,220 --> 00:20:29,110
actually quite simple where to do is

00:20:26,649 --> 00:20:30,850
swap so rgg keys so instead of having

00:20:29,110 --> 00:20:33,399
one combined user of our product

00:20:30,850 --> 00:20:35,780
features reverse the key and we take

00:20:33,399 --> 00:20:37,520
this intermediate dataset and

00:20:35,780 --> 00:20:39,649
[Music]

00:20:37,520 --> 00:20:42,620
we join in from the other feature vector

00:20:39,649 --> 00:20:46,070
so now we have the complete artist you

00:20:42,620 --> 00:20:48,350
know each each item of it is going to

00:20:46,070 --> 00:20:50,480
include all the biases and all the way

00:20:48,350 --> 00:20:52,730
to know all the future factors for each

00:20:50,480 --> 00:20:57,529
combination of profits and user IDs and

00:20:52,730 --> 00:20:59,390
ratings so now we can calculate school

00:20:57,529 --> 00:21:01,549
bias right so if you remember the global

00:20:59,390 --> 00:21:03,320
bias is simply the average of all the

00:21:01,549 --> 00:21:05,059
ratings that we have so we can do that

00:21:03,320 --> 00:21:06,470
easily miss Bart frets we just calculate

00:21:05,059 --> 00:21:07,120
an average for all the ratings that you

00:21:06,470 --> 00:21:09,590
have

00:21:07,120 --> 00:21:12,380
so finally we just need to now calculate

00:21:09,590 --> 00:21:15,590
the user specific bias and the product

00:21:12,380 --> 00:21:19,090
specific the specific bias and that's

00:21:15,590 --> 00:21:21,009
quite a simple as well so

00:21:19,090 --> 00:21:23,649
so we've seen before the kind of stages

00:21:21,009 --> 00:21:26,019
we can obtain to bias by using just this

00:21:23,649 --> 00:21:27,190
gradient or right so the new bias is

00:21:26,019 --> 00:21:29,639
going to be the old virus positive

00:21:27,190 --> 00:21:31,749
radiation and to calculate the gradient

00:21:29,639 --> 00:21:32,799
we just see these quantities that we

00:21:31,749 --> 00:21:33,999
have here we need the error

00:21:32,799 --> 00:21:36,070
so in essence difference between your

00:21:33,999 --> 00:21:37,720
rating and the projection we need the

00:21:36,070 --> 00:21:41,080
camera and Alonso we have those those

00:21:37,720 --> 00:21:42,700
are model parameters so we have

00:21:41,080 --> 00:21:47,679
everything that we need so I started

00:21:42,700 --> 00:21:50,049
with calculating the gradients so now

00:21:47,679 --> 00:21:52,600
that we have each item of energy for

00:21:50,049 --> 00:21:57,310
each one of them we calculate I'm sorry

00:21:52,600 --> 00:21:59,460
I just so now

00:21:57,310 --> 00:22:01,820
[Music]

00:21:59,460 --> 00:22:03,950
right

00:22:01,820 --> 00:22:05,479
so we calculate the prediction right

00:22:03,950 --> 00:22:08,629
because we have the future pressures and

00:22:05,479 --> 00:22:10,549
now we can calculate the error so the

00:22:08,629 --> 00:22:12,409
error is going to be simply the rating

00:22:10,549 --> 00:22:13,999
Western prediction so you can calculate

00:22:12,409 --> 00:22:16,570
the error so now that we have these

00:22:13,999 --> 00:22:19,429
quantities if you remember just the

00:22:16,570 --> 00:22:20,779
expression of ingredients it's quite

00:22:19,429 --> 00:22:22,809
straightforward to calculate so

00:22:20,779 --> 00:22:25,759
basically what do we do we basically

00:22:22,809 --> 00:22:27,200
take our CDs that you have for the

00:22:25,759 --> 00:22:32,899
lightsome structures for the users in

00:22:27,200 --> 00:22:35,659
the projects and now we just season by

00:22:32,899 --> 00:22:39,080
user and project and we do an aggravated

00:22:35,659 --> 00:22:40,570
son for each sales split sorry these

00:22:39,080 --> 00:22:42,830
actually have right so now we get

00:22:40,570 --> 00:22:44,299
raisins for the users you get the

00:22:42,830 --> 00:22:46,249
gradient for the way to the user weights

00:22:44,299 --> 00:22:48,309
and factors and we get the gradients for

00:22:46,249 --> 00:22:50,929
the products and decorations for the

00:22:48,309 --> 00:22:53,509
profits latent factors so now we have

00:22:50,929 --> 00:22:56,149
all the quantities we need to say we

00:22:53,509 --> 00:22:59,029
have a model so we just sum the

00:22:56,149 --> 00:23:03,060
gradients so that we have one gradient

00:22:59,029 --> 00:23:06,000
for each user in product Vanessa

00:23:03,060 --> 00:23:08,820
so you might say well that seems a lot

00:23:06,000 --> 00:23:10,110
of steps but that's that's what the

00:23:08,820 --> 00:23:13,260
price you have to pay for doing this

00:23:10,110 --> 00:23:14,460
computation in a distributed way so you

00:23:13,260 --> 00:23:17,130
might miss a thing

00:23:14,460 --> 00:23:18,750
well now that we have them we have the

00:23:17,130 --> 00:23:20,790
electron structures you can perform

00:23:18,750 --> 00:23:22,620
easily predictions but what if you get

00:23:20,790 --> 00:23:25,110
new observations or what if you get a

00:23:22,620 --> 00:23:27,150
new data what if you get new data

00:23:25,110 --> 00:23:28,680
through projects we never seen before or

00:23:27,150 --> 00:23:31,500
rather if we've seen before so how do we

00:23:28,680 --> 00:23:35,550
deal with that so you might remember

00:23:31,500 --> 00:23:38,270
that we trained the model with nothing

00:23:35,550 --> 00:23:41,270
in the first initial mini batch right

00:23:38,270 --> 00:23:41,270
sorry

00:23:41,670 --> 00:23:45,780
right so we turned the model with

00:23:43,770 --> 00:23:47,400
nothing for initial mini-batch but now

00:23:45,780 --> 00:23:49,980
we're just carry over the model to the

00:23:47,400 --> 00:23:51,390
next mini window and we just with the

00:23:49,980 --> 00:23:54,110
next mini batch and we just trained it

00:23:51,390 --> 00:23:59,610
to the other things that we have so

00:23:54,110 --> 00:24:01,500
let's look at the case where now assume

00:23:59,610 --> 00:24:03,300
we get a mixture of data from ratings

00:24:01,500 --> 00:24:05,010
we've seen before so imagine you're

00:24:03,300 --> 00:24:07,020
right in your movie you wrecked in the

00:24:05,010 --> 00:24:08,970
last wave directly in your movie but

00:24:07,020 --> 00:24:10,410
some users insides in closing to the

00:24:08,970 --> 00:24:12,180
system is the first time he's trying to

00:24:10,410 --> 00:24:13,830
read something or is writing a movie

00:24:12,180 --> 00:24:15,810
doesn't exist before all the video

00:24:13,830 --> 00:24:18,330
images so as soon here we just assume

00:24:15,810 --> 00:24:19,380
that the cells in red are ratings that

00:24:18,330 --> 00:24:21,960
you haven't seen before

00:24:19,380 --> 00:24:24,090
all right and the others ourselves at

00:24:21,960 --> 00:24:27,510
that for users or products that you've

00:24:24,090 --> 00:24:29,970
seen before so what we do now is instead

00:24:27,510 --> 00:24:32,070
of assigning random future lectures for

00:24:29,970 --> 00:24:34,020
each of the products that we see for all

00:24:32,070 --> 00:24:37,020
the products that we get we basically do

00:24:34,020 --> 00:24:39,150
a full outer join between the rdd's that

00:24:37,020 --> 00:24:40,620
we produce and the weighting factors

00:24:39,150 --> 00:24:42,090
have to do from the tape to our disease

00:24:40,620 --> 00:24:44,580
and the weighting factors that we have

00:24:42,090 --> 00:24:46,470
so that allows us to keep the our

00:24:44,580 --> 00:24:48,960
disease that we already have the future

00:24:46,470 --> 00:24:50,340
in factors that we already have before

00:24:48,960 --> 00:24:53,010
the ones that we've never seen before

00:24:50,340 --> 00:24:54,810
now we can do exactly the same steps as

00:24:53,010 --> 00:24:57,190
before and create a new future ventures

00:24:54,810 --> 00:24:59,739
and random biases as well

00:24:57,190 --> 00:25:03,399
so in this way you can deal any kind of

00:24:59,739 --> 00:25:04,899
situation that arises right so I was

00:25:03,399 --> 00:25:06,549
this behave in the real world so we

00:25:04,899 --> 00:25:08,559
decided to test this with some real data

00:25:06,549 --> 00:25:10,389
and to do that we use the movie line

00:25:08,559 --> 00:25:13,570
this next set which is a quite widely

00:25:10,389 --> 00:25:16,330
used if the set in the recommendation

00:25:13,570 --> 00:25:18,399
engine research field and it actually

00:25:16,330 --> 00:25:20,940
has two variants or small variant which

00:25:18,399 --> 00:25:23,769
is quite big for prototyping so it has

00:25:20,940 --> 00:25:27,039
ratings that users actually gave to

00:25:23,769 --> 00:25:28,749
movies in a small files or 100,000

00:25:27,039 --> 00:25:30,999
ratings of I prefer to do some quick

00:25:28,749 --> 00:25:33,460
person typing of algorithms and has a

00:25:30,999 --> 00:25:34,690
full variant we 26 million ratings which

00:25:33,460 --> 00:25:36,159
is quite clearly if you want to do some

00:25:34,690 --> 00:25:39,009
more in-depth analysis of the

00:25:36,159 --> 00:25:41,950
performance of an algorithm and the data

00:25:39,009 --> 00:25:44,379
actually has several fields of interest

00:25:41,950 --> 00:25:47,229
but we're just gonna use a few ones from

00:25:44,379 --> 00:25:49,899
this dataset which is a user IG the

00:25:47,229 --> 00:25:50,700
movie IG and the ratio test is very word

00:25:49,899 --> 00:25:59,010
of what I'm going to use

00:25:50,700 --> 00:25:59,010
[Music]

00:25:59,060 --> 00:26:05,360
no no I was just going to to go into how

00:26:02,570 --> 00:26:06,860
we set up the whole thing to train the

00:26:05,360 --> 00:26:08,150
screaming face so yeah that's a good

00:26:06,860 --> 00:26:13,280
question but but I'm just going to

00:26:08,150 --> 00:26:16,460
explain your question in a second so how

00:26:13,280 --> 00:26:20,860
we gonna train this so the answer of the

00:26:16,460 --> 00:26:23,180
question so first we train attach the

00:26:20,860 --> 00:26:24,350
model so we can have like a fan of face

00:26:23,180 --> 00:26:27,080
line so you can compare it to the

00:26:24,350 --> 00:26:30,920
streaming version and we just use spark

00:26:27,080 --> 00:26:34,310
and Eliot out of the box batch ALS so we

00:26:30,920 --> 00:26:36,200
split the data into 80% 20% and we

00:26:34,310 --> 00:26:39,530
basically train the data in one of the

00:26:36,200 --> 00:26:42,320
splits and then we just see part of that

00:26:39,530 --> 00:26:44,230
little set to the science so we we know

00:26:42,320 --> 00:26:46,820
that we're going to use exactly the same

00:26:44,230 --> 00:26:50,420
two splits on one method in the other so

00:26:46,820 --> 00:26:52,060
you have a third comparison and we

00:26:50,420 --> 00:26:54,230
calculate some kind of error measure

00:26:52,060 --> 00:26:56,270
which is helpful if you have some kind

00:26:54,230 --> 00:26:58,640
of metric of how well the model is

00:26:56,270 --> 00:27:01,160
performing so in this case we use the

00:26:58,640 --> 00:27:03,260
root mean square error so it's easy to

00:27:01,160 --> 00:27:05,570
calculate since part given to each of

00:27:03,260 --> 00:27:09,460
the positions and and the original

00:27:05,570 --> 00:27:09,460
nature and

00:27:09,780 --> 00:27:15,510
sorry okay so we that we measure the

00:27:14,070 --> 00:27:17,400
rhythms right there and then we're gonna

00:27:15,510 --> 00:27:20,130
compare which means we're there the

00:27:17,400 --> 00:27:24,150
streaming version against this one so

00:27:20,130 --> 00:27:26,520
how do we set up the the actual

00:27:24,150 --> 00:27:29,370
streaming testing case so we use we

00:27:26,520 --> 00:27:33,240
train this on appreciation and the idea

00:27:29,370 --> 00:27:35,430
was to use some kind of message broker

00:27:33,240 --> 00:27:39,180
like Casca and we actually use just to

00:27:35,430 --> 00:27:41,400
simulate the data stream and we use

00:27:39,180 --> 00:27:44,300
stream Z to do that which is the project

00:27:41,400 --> 00:27:48,210
and allows you to deploy capsule

00:27:44,300 --> 00:27:50,610
OpenShift and we use for shrinko which

00:27:48,210 --> 00:27:52,530
is also from retinoic six at i/o which

00:27:50,610 --> 00:27:55,410
allows you to easily deploy spark

00:27:52,530 --> 00:27:57,750
clusters on openshift and basically what

00:27:55,410 --> 00:27:59,610
it just says is we read in answer to

00:27:57,750 --> 00:28:02,370
your questions so we read the entirety

00:27:59,610 --> 00:28:05,160
of the data and then we just basically

00:28:02,370 --> 00:28:10,350
rephrases soozluecke Africa as system

00:28:05,160 --> 00:28:12,060
registry right and we use we use Windows

00:28:10,350 --> 00:28:14,280
of five seconds with a thousand

00:28:12,060 --> 00:28:16,080
observations each but this is just for

00:28:14,280 --> 00:28:17,730
convenience but it's just because it's

00:28:16,080 --> 00:28:20,160
convenient for practical purposes you

00:28:17,730 --> 00:28:21,570
can use whatever you want but I mean

00:28:20,160 --> 00:28:24,210
realistically it feels like one

00:28:21,570 --> 00:28:26,430
observation four minutes or something

00:28:24,210 --> 00:28:27,380
you probably gonna wait several months

00:28:26,430 --> 00:28:28,880
or years

00:28:27,380 --> 00:28:35,419
you wait for this to finish so you just

00:28:28,880 --> 00:28:38,480
see the sexual practical reasons and an

00:28:35,419 --> 00:28:41,000
important note is that the best

00:28:38,480 --> 00:28:42,760
parameters for the batch model are not

00:28:41,000 --> 00:28:44,570
going to be necessarily the best

00:28:42,760 --> 00:28:45,799
parameters for the streaming model

00:28:44,570 --> 00:28:48,890
obviously they're completely different

00:28:45,799 --> 00:28:51,950
or very different algorithms but also

00:28:48,890 --> 00:28:54,320
for for convenience and for practical

00:28:51,950 --> 00:28:57,770
purposes we use the same parameters in

00:28:54,320 --> 00:29:00,110
both models but more more in the next

00:28:57,770 --> 00:29:02,510
slides I'm gonna go into how to estimate

00:29:00,110 --> 00:29:05,750
type of parameters for the first video

00:29:02,510 --> 00:29:10,250
aspiration so this is a result that we

00:29:05,750 --> 00:29:11,990
have so in the horizontal dashed line is

00:29:10,250 --> 00:29:16,520
the root mean square error for the batch

00:29:11,990 --> 00:29:18,080
version and rules for the wine is the

00:29:16,520 --> 00:29:19,460
witness for therefore the streaming

00:29:18,080 --> 00:29:21,770
version you can see well it's despite

00:29:19,460 --> 00:29:24,140
its v fuse in the sense that it is what

00:29:21,770 --> 00:29:25,490
we were expecting so in the beginning

00:29:24,140 --> 00:29:27,260
you don't have much left for the

00:29:25,490 --> 00:29:29,630
streaming version so it's kind of all

00:29:27,260 --> 00:29:31,760
over the place but as time goes by it

00:29:29,630 --> 00:29:34,190
does seem to be converging to a valley

00:29:31,760 --> 00:29:37,130
that which is in the same region as a

00:29:34,190 --> 00:29:39,830
batch vs. so in the end both the batch

00:29:37,130 --> 00:29:41,960
to the streaming process these are the

00:29:39,830 --> 00:29:46,100
same amount situation so it's a

00:29:41,960 --> 00:29:47,780
reasonable result but you might think

00:29:46,100 --> 00:29:49,640
well this is all very goods and

00:29:47,780 --> 00:29:51,530
streaming als is like the silver boo

00:29:49,640 --> 00:29:53,059
it's like you know if magically solves

00:29:51,530 --> 00:29:55,490
every problem might have well that's not

00:29:53,059 --> 00:29:57,710
the case obviously so some things which

00:29:55,490 --> 00:30:01,580
are very important to consider when is

00:29:57,710 --> 00:30:03,169
extremely ours so a problem with but

00:30:01,580 --> 00:30:06,440
then this is not particular of streaming

00:30:03,169 --> 00:30:08,630
ALS is for all a OS and many machine

00:30:06,440 --> 00:30:11,570
learning models is a cold start problem

00:30:08,630 --> 00:30:15,110
so basically the co-star problem refers

00:30:11,570 --> 00:30:17,210
to initial points in your model training

00:30:15,110 --> 00:30:19,830
where you don't have enough data to make

00:30:17,210 --> 00:30:22,740
any kind of insightful in

00:30:19,830 --> 00:30:24,120
or prediction so you can imagine those

00:30:22,740 --> 00:30:25,980
slides our shoulders and we're know is

00:30:24,120 --> 00:30:27,030
in the beginning if you remember the

00:30:25,980 --> 00:30:29,040
weights instructions were completely

00:30:27,030 --> 00:30:30,540
filled with random data so the

00:30:29,040 --> 00:30:32,040
approximation is gonna will completely

00:30:30,540 --> 00:30:33,660
random so if you just have a few

00:30:32,040 --> 00:30:35,370
observations that's not gonna change

00:30:33,660 --> 00:30:38,870
that so it's going to look pretty much

00:30:35,370 --> 00:30:41,160
random so always be careful when

00:30:38,870 --> 00:30:42,660
providing because it might feel tempted

00:30:41,160 --> 00:30:44,880
to since you have a streaming version

00:30:42,660 --> 00:30:45,630
I'm gonna start serving away predictions

00:30:44,880 --> 00:30:48,140
immediately

00:30:45,630 --> 00:30:50,760
so that's might not be the best idea and

00:30:48,140 --> 00:30:52,920
something you might do to mitigate that

00:30:50,760 --> 00:30:54,210
since - so if you have those updates if

00:30:52,920 --> 00:30:56,550
you're a big company and you have loads

00:30:54,210 --> 00:30:59,850
of data first train the streaming model

00:30:56,550 --> 00:31:02,070
on off line with a big chunk of data and

00:30:59,850 --> 00:31:04,020
then start serving in a streaming way

00:31:02,070 --> 00:31:06,240
right so but but bootstrap the model

00:31:04,020 --> 00:31:08,400
where the patient takes don't start from

00:31:06,240 --> 00:31:11,010
zero and immediately start serving

00:31:08,400 --> 00:31:14,580
predictions is like say five rating or

00:31:11,010 --> 00:31:18,390
something like that that might be some

00:31:14,580 --> 00:31:19,710
predictions might be rubbish get back so

00:31:18,390 --> 00:31:23,350
another thing to consider is hyper

00:31:19,710 --> 00:31:26,580
parameter estimation excuse me

00:31:23,350 --> 00:31:30,480
[Music]

00:31:26,580 --> 00:31:34,830
this y parameter estimation in the batch

00:31:30,480 --> 00:31:37,560
POS is quite straightforward because you

00:31:34,830 --> 00:31:41,400
can do like a parameter research for

00:31:37,560 --> 00:31:43,130
parameters for several sets of

00:31:41,400 --> 00:31:46,200
parameters and then you decide well

00:31:43,130 --> 00:31:48,090
parameter 7g is the best for my data and

00:31:46,200 --> 00:31:49,500
then at some point in the future if you

00:31:48,090 --> 00:31:52,440
want to retrain the whole thing you can

00:31:49,500 --> 00:31:53,820
do it like this after two months of

00:31:52,440 --> 00:31:55,650
having this model running you say well

00:31:53,820 --> 00:31:59,100
it's not moving very well if we try to

00:31:55,650 --> 00:32:00,990
retrain it with a rank of double size

00:31:59,100 --> 00:32:02,520
something like that then you can do that

00:32:00,990 --> 00:32:04,140
perfectly you take all of the data you

00:32:02,520 --> 00:32:06,380
rearranged the model that's it that's

00:32:04,140 --> 00:32:09,150
fine because you have all of the data

00:32:06,380 --> 00:32:13,170
but it's a streaming case that's not the

00:32:09,150 --> 00:32:15,270
case because when you get the data you

00:32:13,170 --> 00:32:18,150
discarded the data I mean in theory

00:32:15,270 --> 00:32:19,830
obviously but what I mean is if you if

00:32:18,150 --> 00:32:21,510
you're in a position in the streaming LS

00:32:19,830 --> 00:32:23,430
that you need to refer back to the

00:32:21,510 --> 00:32:25,440
entirety of the data and retrain the

00:32:23,430 --> 00:32:27,810
model then it's not really a streaming

00:32:25,440 --> 00:32:29,850
version you're doing a batch kind of

00:32:27,810 --> 00:32:31,620
match hybrid stream erosion so where did

00:32:29,850 --> 00:32:33,360
you is you have a set of parameters you

00:32:31,620 --> 00:32:34,740
get two data and then if you want to try

00:32:33,360 --> 00:32:36,900
a new set of parameters you can only

00:32:34,740 --> 00:32:38,100
return the model with that new objective

00:32:36,900 --> 00:32:40,470
that you have because you use these

00:32:38,100 --> 00:32:43,050
cards in the previous data so you can't

00:32:40,470 --> 00:32:45,500
really do what are you in fact yes so

00:32:43,050 --> 00:32:48,300
first of all solution around that is to

00:32:45,500 --> 00:32:50,310
perform like a parallel research where

00:32:48,300 --> 00:32:52,110
there is you have a bunch of models in

00:32:50,310 --> 00:32:54,180
the beginning you train them with a set

00:32:52,110 --> 00:32:56,640
of parameters each and then as time goes

00:32:54,180 --> 00:32:58,350
by you see which of these Mazal or which

00:32:56,640 --> 00:33:00,360
of these set of parameters gave me the

00:32:58,350 --> 00:33:02,190
Mosel which is least performant and then

00:33:00,360 --> 00:33:03,750
you prove that model from your search

00:33:02,190 --> 00:33:05,670
you say well I'm just going to do Street

00:33:03,750 --> 00:33:07,530
models and keep doing that this has an

00:33:05,670 --> 00:33:09,510
obvious drawback which is it might be

00:33:07,530 --> 00:33:11,730
computationally expensive to train lots

00:33:09,510 --> 00:33:12,270
of models simultaneously and another

00:33:11,730 --> 00:33:14,790
thing

00:33:12,270 --> 00:33:16,920
there's no theoretical result the result

00:33:14,790 --> 00:33:18,660
that actually guarantees you that a

00:33:16,920 --> 00:33:21,000
multitude scars in the very beginning

00:33:18,660 --> 00:33:22,920
might not be actually the best model in

00:33:21,000 --> 00:33:24,720
the future when you have more data you

00:33:22,920 --> 00:33:26,790
might need the best set of parameters

00:33:24,720 --> 00:33:28,500
for that specific small chunk of data

00:33:26,790 --> 00:33:31,250
that you have problems to train for

00:33:28,500 --> 00:33:33,720
amateurs with the streaming version

00:33:31,250 --> 00:33:35,730
finally there's a consideration of

00:33:33,720 --> 00:33:38,610
performance so in these kind of models

00:33:35,730 --> 00:33:40,650
you're gonna do as if you also joins

00:33:38,610 --> 00:33:42,540
you're gonna have lots of data shuffling

00:33:40,650 --> 00:33:44,730
around the partitions so you have to be

00:33:42,540 --> 00:33:48,270
really careful of optimizing this kind

00:33:44,730 --> 00:33:49,950
of algorithms spark patches parks is

00:33:48,270 --> 00:33:51,690
something very clever who is a batch

00:33:49,950 --> 00:33:54,570
version in which they do something

00:33:51,690 --> 00:33:56,400
called the blocks AOS basically what did

00:33:54,570 --> 00:33:58,260
you is that they pre calculates the

00:33:56,400 --> 00:34:01,170
amount of how to viewing and incoming

00:33:58,260 --> 00:34:04,800
outgoing and in going sorry connections

00:34:01,170 --> 00:34:06,870
between the chunks of our GG's for the

00:34:04,800 --> 00:34:08,580
future awaits instructions so they can

00:34:06,870 --> 00:34:10,290
minimize the amount of data shuffling

00:34:08,580 --> 00:34:13,440
that happens so there's quite square

00:34:10,290 --> 00:34:15,360
algorithm but and I use implementation

00:34:13,440 --> 00:34:17,609
of streaming AOS will give you nothing

00:34:15,360 --> 00:34:19,679
like that so so we have on top of these

00:34:17,609 --> 00:34:22,080
hours you have to think of some clever

00:34:19,679 --> 00:34:25,320
strategies which we minimize - a funny

00:34:22,080 --> 00:34:27,810
and also something that for the more

00:34:25,320 --> 00:34:30,629
seasoned apache spark developer might

00:34:27,810 --> 00:34:33,210
race and you know

00:34:30,629 --> 00:34:37,200
make some alarm bells ring is the fact

00:34:33,210 --> 00:34:39,809
that you might use some agile random

00:34:37,200 --> 00:34:42,419
access fetching of our duties to

00:34:39,809 --> 00:34:43,859
calculate some quantity so say if you're

00:34:42,419 --> 00:34:46,619
if you're now working to find yourself

00:34:43,859 --> 00:34:49,200
calculating the predicted rating for a

00:34:46,619 --> 00:34:51,419
specific user and project many times

00:34:49,200 --> 00:34:54,059
keep in mind that to do that you're

00:34:51,419 --> 00:34:57,180
gonna have to access a specific role

00:34:54,059 --> 00:34:59,369
between commas or column of an RDD and

00:34:57,180 --> 00:35:01,470
that it's not a really this kind of an

00:34:59,369 --> 00:35:03,869
idea type apps or games part so it might

00:35:01,470 --> 00:35:05,220
be the end of engine if you end up your

00:35:03,869 --> 00:35:06,839
code ends up looking like this

00:35:05,220 --> 00:35:09,509
haven't you also look herbs and stuff

00:35:06,839 --> 00:35:12,720
like that so possibly it is you just

00:35:09,509 --> 00:35:17,089
rethink your strategy during the

00:35:12,720 --> 00:35:20,910
projections so so this was basically the

00:35:17,089 --> 00:35:24,420
explanation of a generic sparkly is part

00:35:20,910 --> 00:35:26,999
a OS streaming algorithm so if you want

00:35:24,420 --> 00:35:29,549
to check out more stuff about streaming

00:35:26,999 --> 00:35:32,519
algorithms or access park or OpenShift I

00:35:29,549 --> 00:35:35,009
invite you to take a look at my blog

00:35:32,519 --> 00:35:37,019
this is a specific post on consuming who

00:35:35,009 --> 00:35:38,789
is if you want to see it and if you want

00:35:37,019 --> 00:35:42,420
to play around with distributed

00:35:38,789 --> 00:35:43,559
algorithms on Apache spark on OpenShift

00:35:42,420 --> 00:35:46,140
or on the cloud

00:35:43,559 --> 00:35:49,700
I strongly recommend that you go to the

00:35:46,140 --> 00:35:53,700
right analytics on your website you have

00:35:49,700 --> 00:35:55,859
several use cases for intelligent

00:35:53,700 --> 00:35:58,349
applications machine learning at scale

00:35:55,859 --> 00:36:00,660
which are very good very well-documented

00:35:58,349 --> 00:36:02,969
you can you can actually learn how these

00:36:00,660 --> 00:36:04,769
things work just by reading the source

00:36:02,969 --> 00:36:06,749
code in the documentation and they

00:36:04,769 --> 00:36:08,729
actually have some ready use cases you

00:36:06,749 --> 00:36:12,359
can actually deploy like they have a

00:36:08,729 --> 00:36:14,190
micro service-oriented recognition

00:36:12,359 --> 00:36:15,480
engine builds with a bachelor's version

00:36:14,190 --> 00:36:19,410
of spark which

00:36:15,480 --> 00:36:21,930
very very very interesting so I throw

00:36:19,410 --> 00:36:24,230
the recommendations so that is it for me

00:36:21,930 --> 00:36:27,369
and I thank you very much for a time

00:36:24,230 --> 00:36:27,369
[Music]

00:36:30,160 --> 00:36:40,079
[Music]

00:36:37,180 --> 00:36:40,079
no questions

00:36:40,770 --> 00:36:45,890
he would

00:36:42,920 --> 00:36:47,210
yeah I'm sorry if anyone has any

00:36:45,890 --> 00:36:49,390
questions sorry I thought I was a given

00:36:47,210 --> 00:36:49,390
but

00:36:49,760 --> 00:36:55,150
[Music]

00:36:55,940 --> 00:37:02,190
speaking of you that always went nowhere

00:36:59,000 --> 00:37:02,190

YouTube URL: https://www.youtube.com/watch?v=uRWto60qVG8


