Title: Data Exploration with JupyterHub on OpenShift
Publication date: 2019-02-21
Playlist: DevConfUS 2018
Description: 
	Jupyter Notebooks are very popular among people from Python beginners to Data Scientist. They allow for easy experimentation with Python code and fast iteration on the problem. We decided to provide a service called JupyterHub to our team. This service allows anyone to develop and execute Jupyter Notebooks remotely. We run our JupyterHub on OpenShift, use Ceph as a storage and use plugins to integrate with other services.

In this talk, we will walk through our deployment and use cases. If all goes well, you should have all the information to run your own JupyterHub when you leave.
Captions: 
	00:00:08,519 --> 00:00:17,590
one two okay I think it's it's 9:00 a.m.

00:00:15,700 --> 00:00:20,590
so I don't know if we should start over

00:00:17,590 --> 00:00:23,740
who are streaming so everyone can watch

00:00:20,590 --> 00:00:26,350
the recording after then so welcome the

00:00:23,740 --> 00:00:29,640
big crowd the audience thank you very

00:00:26,350 --> 00:00:32,259
much for getting up so early on Saturday

00:00:29,640 --> 00:00:34,180
and also there is I think Dan Walsh is

00:00:32,259 --> 00:00:37,199
having a presentation at the same time

00:00:34,180 --> 00:00:40,179
so I understand that everyone is there

00:00:37,199 --> 00:00:41,679
so my name is Mark Foley never worked at

00:00:40,179 --> 00:00:42,730
it for six years and I would like to

00:00:41,679 --> 00:00:44,230
tell you something about what we are

00:00:42,730 --> 00:00:46,839
doing in SEO you would do with their

00:00:44,230 --> 00:00:49,809
help on opening shift and when I saw my

00:00:46,839 --> 00:00:51,640
talk on schedule I called it data

00:00:49,809 --> 00:00:53,649
exploration Jupiter up on open shifts

00:00:51,640 --> 00:00:56,920
and then I realized I'm not going to do

00:00:53,649 --> 00:00:58,659
any data exploration so I fixed it and I

00:00:56,920 --> 00:01:02,019
will call the talk enabling data

00:00:58,659 --> 00:01:04,330
exploration on with Jupiter happen

00:01:02,019 --> 00:01:07,060
OpenShift so what I would like to talk

00:01:04,330 --> 00:01:08,950
about is how we deploy to better have

00:01:07,060 --> 00:01:11,110
what are the what are components like

00:01:08,950 --> 00:01:13,689
the technicalities of the actual

00:01:11,110 --> 00:01:16,630
platform and tooling and how we

00:01:13,689 --> 00:01:21,549
integrate it with other parts of the of

00:01:16,630 --> 00:01:23,560
the platform so why do we explore data

00:01:21,549 --> 00:01:28,509
and why do we want to do whatever we do

00:01:23,560 --> 00:01:31,960
in the RAI center of excellence a smart

00:01:28,509 --> 00:01:35,710
person Clive HMMWV from UK said that

00:01:31,960 --> 00:01:37,899
data is the new oil it is very valuable

00:01:35,710 --> 00:01:42,159
as it is if you collect a lot of data

00:01:37,899 --> 00:01:43,840
but it's it needs to be refined same way

00:01:42,159 --> 00:01:48,009
as the oil needs to be refined into

00:01:43,840 --> 00:01:50,740
plastics and and gas and chemicals and

00:01:48,009 --> 00:01:52,539
whatever because without cleaning and

00:01:50,740 --> 00:01:55,329
transformation and stuff it-it's just

00:01:52,539 --> 00:01:56,979
pile of pile of bits which you can't

00:01:55,329 --> 00:02:00,249
really make sense of in most cases

00:01:56,979 --> 00:02:01,840
because it's too much and and and it's

00:02:00,249 --> 00:02:03,580
it needs to be clean and stuff like that

00:02:01,840 --> 00:02:06,610
the gentle and also came up

00:02:03,580 --> 00:02:09,160
with the Tesco Clubcard which might not

00:02:06,610 --> 00:02:11,170
sound like a lot but it is a great

00:02:09,160 --> 00:02:12,700
source of information about customers

00:02:11,170 --> 00:02:19,060
and what they buy and how they work so

00:02:12,700 --> 00:02:20,980
he prolene owes his stuff for being able

00:02:19,060 --> 00:02:24,040
to work with Jupiter have an open shift

00:02:20,980 --> 00:02:26,560
there are some prerequisite ease so let

00:02:24,040 --> 00:02:28,330
me quickly go through that and that's

00:02:26,560 --> 00:02:31,210
one of them it's open if you need to

00:02:28,330 --> 00:02:33,250
have open shift running which is kind of

00:02:31,210 --> 00:02:35,230
obvious if you want to deploy there what

00:02:33,250 --> 00:02:36,670
is open ship who doesn't know what is we

00:02:35,230 --> 00:02:39,310
never heard about open ships or doesn't

00:02:36,670 --> 00:02:41,800
know anything about it which is good but

00:02:39,310 --> 00:02:45,250
quickly for the for the audience of big

00:02:41,800 --> 00:02:47,230
audience and the on the youtube it's an

00:02:45,250 --> 00:02:49,060
enterprise distribution of kubernetes it

00:02:47,230 --> 00:02:51,070
is built on top of kubernetes so there

00:02:49,060 --> 00:02:53,320
are all the it's it's a scalable

00:02:51,070 --> 00:02:55,390
container Orchestrator so if you have

00:02:53,320 --> 00:02:56,620
anything to do with containers and you

00:02:55,390 --> 00:02:57,940
want to run them in production you want

00:02:56,620 --> 00:03:01,440
to use something like Google readies or

00:02:57,940 --> 00:03:04,770
open ship and then it has all the basic

00:03:01,440 --> 00:03:07,330
all the basic concepts as ferny so

00:03:04,770 --> 00:03:09,820
things like port services deployments

00:03:07,330 --> 00:03:12,459
for system volumes but it adds more it

00:03:09,820 --> 00:03:14,080
adds the the development workflow with

00:03:12,459 --> 00:03:16,600
builds and image streams and things like

00:03:14,080 --> 00:03:20,140
that you can go to ok deed of i/o which

00:03:16,600 --> 00:03:21,550
is a new new place where to find

00:03:20,140 --> 00:03:23,230
information about the upstream version

00:03:21,550 --> 00:03:25,420
of open ship which is which it was

00:03:23,230 --> 00:03:28,390
called open shipped origin in the past

00:03:25,420 --> 00:03:30,370
the second thing that we need for the

00:03:28,390 --> 00:03:33,280
work that we are doing with Jupiter hub

00:03:30,370 --> 00:03:39,340
is some object storage you are probably

00:03:33,280 --> 00:03:41,950
familiar with AWS s3 we self and then

00:03:39,340 --> 00:03:43,720
self implements the s3 API so you can

00:03:41,950 --> 00:03:46,720
use the same libraries like both are

00:03:43,720 --> 00:03:50,170
free in Python or the Hadoop as free

00:03:46,720 --> 00:03:55,660
library in spark to access your data

00:03:50,170 --> 00:03:56,739
instead sorry with the s3 API luckily I

00:03:55,660 --> 00:04:00,910
don't have to I didn't have to setup

00:03:56,739 --> 00:04:03,310
either open ships nor nor stuff

00:04:00,910 --> 00:04:06,910
everything that I will use here is

00:04:03,310 --> 00:04:09,560
deployed on MOC Mass open cloud if you

00:04:06,910 --> 00:04:14,459
saw Stevens or sure sure

00:04:09,560 --> 00:04:15,870
talking yesterday about moc we are

00:04:14,459 --> 00:04:18,090
working with them on something called

00:04:15,870 --> 00:04:23,520
open data so this is part of the open

00:04:18,090 --> 00:04:25,380
data and we are deployed there so what

00:04:23,520 --> 00:04:27,509
are the tools that we will be using for

00:04:25,380 --> 00:04:31,440
the data exploration that we are not

00:04:27,509 --> 00:04:32,610
going to do first and they're like the

00:04:31,440 --> 00:04:35,270
core part is Jupiter

00:04:32,610 --> 00:04:38,490
you can run Jupiter or Jupiter notebooks

00:04:35,270 --> 00:04:40,740
on your laptop and you can use that it

00:04:38,490 --> 00:04:44,340
looks like that basically so you have

00:04:40,740 --> 00:04:46,650
some it's split into cells and the cells

00:04:44,340 --> 00:04:49,199
can be either some kind of markdown or

00:04:46,650 --> 00:04:52,410
it can be code or it can be output of

00:04:49,199 --> 00:04:54,330
the code and then you as a user type

00:04:52,410 --> 00:04:57,990
something into your browser it's a web

00:04:54,330 --> 00:04:59,820
application that has a back-end and you

00:04:57,990 --> 00:05:02,280
touch something your web browser and the

00:04:59,820 --> 00:05:04,650
commands the code is sent to the Jupiter

00:05:02,280 --> 00:05:07,169
kernel the kernels there are plenty of

00:05:04,650 --> 00:05:09,800
them I can find them on github and there

00:05:07,169 --> 00:05:13,139
is Titan Scala's yeah I even saw c-sharp

00:05:09,800 --> 00:05:15,210
R is pretty popular and then it sends

00:05:13,139 --> 00:05:20,729
out dog back to the Jupiter UI and you

00:05:15,210 --> 00:05:23,070
see that in your web UI on Jupiter the

00:05:20,729 --> 00:05:25,440
good thing is that the actual file is

00:05:23,070 --> 00:05:28,110
just Jason so if you want to do

00:05:25,440 --> 00:05:30,840
something fancy with the content you can

00:05:28,110 --> 00:05:35,340
take the Chasen and parse it and work

00:05:30,840 --> 00:05:39,120
with that or you can view it and in the

00:05:35,340 --> 00:05:43,409
Jupiter notebook UI and actually run

00:05:39,120 --> 00:05:44,940
code and stuff like that what builds on

00:05:43,409 --> 00:05:48,030
top of that is something called Jupiter

00:05:44,940 --> 00:05:49,979
hub and that basically the change is

00:05:48,030 --> 00:05:52,320
that the two peter notebooks themselves

00:05:49,979 --> 00:05:54,150
are a single user you as a user

00:05:52,320 --> 00:05:56,970
run them on your laptop and you can

00:05:54,150 --> 00:05:58,560
write the code and you can see them see

00:05:56,970 --> 00:06:02,669
the stuff but if you want to provide

00:05:58,560 --> 00:06:04,770
that capability in a distributed way to

00:06:02,669 --> 00:06:06,419
like a team of people in your company or

00:06:04,770 --> 00:06:07,669
at school I think it's like for

00:06:06,419 --> 00:06:09,990
universities it might be super

00:06:07,669 --> 00:06:11,580
interesting or even high schools like if

00:06:09,990 --> 00:06:11,930
you want to start coding in Python you

00:06:11,580 --> 00:06:13,759
can

00:06:11,930 --> 00:06:18,590
by these notebooks you can provide

00:06:13,759 --> 00:06:21,500
Jupiter hub to the to the class and they

00:06:18,590 --> 00:06:23,180
can just go in and they Jupiter happen

00:06:21,500 --> 00:06:26,000
automatically when they login it will

00:06:23,180 --> 00:06:28,550
spawn up the Jupiter notebook for them

00:06:26,000 --> 00:06:31,639
and they can work with that and it

00:06:28,550 --> 00:06:34,340
spawns and manages those notebooks so

00:06:31,639 --> 00:06:35,870
that if a user always gets to his phone

00:06:34,340 --> 00:06:38,600
persistent version of the notebooks that

00:06:35,870 --> 00:06:40,639
he work was working with and the last

00:06:38,600 --> 00:06:43,039
part of the system would be a pretty

00:06:40,639 --> 00:06:45,860
spark I've assumed that you're probably

00:06:43,039 --> 00:06:49,940
all familiar at least slightly with

00:06:45,860 --> 00:06:51,740
apache spark as the website says it's a

00:06:49,940 --> 00:06:53,990
unified analytics engine for large-scale

00:06:51,740 --> 00:06:56,720
data processing so that means that if

00:06:53,990 --> 00:06:58,430
you want to process some data you want

00:06:56,720 --> 00:07:00,080
to do some cleaning and you want to do

00:06:58,430 --> 00:07:03,050
some model training and something you

00:07:00,080 --> 00:07:06,229
will use spark it provides api's and

00:07:03,050 --> 00:07:07,370
libraries like spark SQL and machinima

00:07:06,229 --> 00:07:09,139
melody

00:07:07,370 --> 00:07:12,740
national in library which has

00:07:09,139 --> 00:07:15,080
implemented plenty of algorithms and it

00:07:12,740 --> 00:07:17,449
works in a cluster mode so you have

00:07:15,080 --> 00:07:19,400
master and workers and workers do the

00:07:17,449 --> 00:07:22,580
work and master orchestrate them and we

00:07:19,400 --> 00:07:24,349
will use jupiter notebook to connect to

00:07:22,580 --> 00:07:26,840
spark and do the processing in spark so

00:07:24,349 --> 00:07:29,720
that the notebook or the jupiter server

00:07:26,840 --> 00:07:33,710
doesn't have to be dead beefy and work

00:07:29,720 --> 00:07:35,080
with so much data so I have a quick demo

00:07:33,710 --> 00:07:39,590
I just

00:07:35,080 --> 00:07:42,919
it is nothing fancy but basically just

00:07:39,590 --> 00:07:46,970
walk through of how that works how the

00:07:42,919 --> 00:07:49,849
Jupiter have works so this is I have

00:07:46,970 --> 00:07:53,180
open shipped I have Jupiter have

00:07:49,849 --> 00:07:55,280
deployed so I go to the URL that opens

00:07:53,180 --> 00:07:58,280
you generate it for me and I will sign

00:07:55,280 --> 00:07:59,930
in with my open Chiefs credentials there

00:07:58,280 --> 00:08:04,880
is quite a bit of quite important

00:07:59,930 --> 00:08:06,919
because if I don't I don't want to

00:08:04,880 --> 00:08:09,200
remember another credentials I want to

00:08:06,919 --> 00:08:12,620
use something that I already know

00:08:09,200 --> 00:08:15,710
that later how that is solve in Jupiter

00:08:12,620 --> 00:08:18,470
have now we select from the list of

00:08:15,710 --> 00:08:21,890
images I want to use spark so I will

00:08:18,470 --> 00:08:24,080
select this spark image and these are

00:08:21,890 --> 00:08:27,590
basically just what do you would if you

00:08:24,080 --> 00:08:31,790
if you install Jupiter notebook server

00:08:27,590 --> 00:08:33,950
on your local machine or a laptop these

00:08:31,790 --> 00:08:36,500
images basically represent your laptop

00:08:33,950 --> 00:08:38,630
and install dependencies so if you want

00:08:36,500 --> 00:08:40,370
to use spark and by spark you would need

00:08:38,630 --> 00:08:42,410
some configuration and PI spark

00:08:40,370 --> 00:08:44,720
installed and Java installed on your

00:08:42,410 --> 00:08:47,690
laptop in the same way it works with

00:08:44,720 --> 00:08:49,280
this with these notebooks that the image

00:08:47,690 --> 00:08:53,750
contains all the dependencies that are

00:08:49,280 --> 00:08:56,150
needed so I have these two notebooks one

00:08:53,750 --> 00:08:57,680
I call the bottom because I use both the

00:08:56,150 --> 00:09:01,610
free library which is a library that

00:08:57,680 --> 00:09:03,380
implements s free API so I have my

00:09:01,610 --> 00:09:07,400
credentials in environment variables

00:09:03,380 --> 00:09:12,730
I'll show you how I got them there and I

00:09:07,400 --> 00:09:16,340
will I connect to some endpoint and

00:09:12,730 --> 00:09:20,170
we're gonna run this right so it

00:09:16,340 --> 00:09:24,290
installs the dependency if it's missing

00:09:20,170 --> 00:09:27,380
and I'll connect and then I can list

00:09:24,290 --> 00:09:29,450
buckets if you sort of open data

00:09:27,380 --> 00:09:32,990
presentation yesterday so Steven to

00:09:29,450 --> 00:09:36,620
create the his bucket here so we are on

00:09:32,990 --> 00:09:40,640
the same endpoint so I see his bucket

00:09:36,620 --> 00:09:44,450
and I created mine here so he could see

00:09:40,640 --> 00:09:46,790
my data there and then I have this other

00:09:44,450 --> 00:09:48,680
one which I actually just downloaded

00:09:46,790 --> 00:09:51,470
from internet I looked for PI spark

00:09:48,680 --> 00:09:56,780
Jupiter notebook and I go to this

00:09:51,470 --> 00:10:00,050
repository which someone created I don't

00:09:56,780 --> 00:10:02,870
know the gentleman and he is couple so I

00:10:00,050 --> 00:10:04,340
just took the last one I think because I

00:10:02,870 --> 00:10:07,250
thought that it that is going to be the

00:10:04,340 --> 00:10:08,750
coolest one probably and I had to fix

00:10:07,250 --> 00:10:10,100
some stuff because he was writing it for

00:10:08,750 --> 00:10:12,680
Python too and we are running Python

00:10:10,100 --> 00:10:15,330
free but it was mostly just as syntax

00:10:12,680 --> 00:10:17,580
fixing and what

00:10:15,330 --> 00:10:19,529
does is that it it's again connects to

00:10:17,580 --> 00:10:22,560
it connects is free it connects to spark

00:10:19,529 --> 00:10:25,980
see I have this spark cluster URL in my

00:10:22,560 --> 00:10:29,760
environment and then it's downloads some

00:10:25,980 --> 00:10:32,519
data from from the object storage which

00:10:29,760 --> 00:10:36,839
i pre uploaded there and then it does

00:10:32,519 --> 00:10:42,540
some decision tree building and this is

00:10:36,839 --> 00:10:46,829
between decision tree training and let

00:10:42,540 --> 00:10:48,600
me run that run home below and it

00:10:46,829 --> 00:10:52,230
validates whether the decision tree was

00:10:48,600 --> 00:10:57,029
a good one it uses data data set from KD

00:10:52,230 --> 00:11:00,990
dica which was some network intrusion

00:10:57,029 --> 00:11:03,120
detection competition so build a

00:11:00,990 --> 00:11:08,220
classifier for network intrusion

00:11:03,120 --> 00:11:11,940
detection and so that these are the same

00:11:08,220 --> 00:11:15,510
data set yeah so it's now connecting to

00:11:11,940 --> 00:11:19,380
spark and for the spark we can look here

00:11:15,510 --> 00:11:21,660
into open shape that it's running as

00:11:19,380 --> 00:11:23,130
part of my namespace as part of the

00:11:21,660 --> 00:11:27,570
jupiter have namespace I have two

00:11:23,130 --> 00:11:33,570
workers and I have created a route so

00:11:27,570 --> 00:11:38,459
that we can look into the we can't look

00:11:33,570 --> 00:11:41,149
into that because it doesn't have fix

00:11:38,459 --> 00:11:41,149
that quickly

00:11:47,690 --> 00:11:50,380
come on

00:11:53,930 --> 00:12:04,550
okay so let's go fix that so what do you

00:12:01,760 --> 00:12:10,150
would see here let me try this trend

00:12:04,550 --> 00:12:10,150
thing you start Firefox

00:12:22,790 --> 00:12:25,570
mm-hmm

00:12:26,660 --> 00:12:29,589
what happens

00:12:33,020 --> 00:12:35,860
I'm sorry

00:12:42,170 --> 00:12:51,440
yeah so what we see here is that we have

00:12:45,850 --> 00:12:54,760
to work our notes each each executor

00:12:51,440 --> 00:12:57,200
gets 20 gigabytes of RAM and 8 cores

00:12:54,760 --> 00:13:00,260
well 8 courses together

00:12:57,200 --> 00:13:03,800
so each executor gets 4 and then that's

00:13:00,260 --> 00:13:08,930
where we are actually running the the

00:13:03,800 --> 00:13:12,400
notebook code so it downloaded the data

00:13:08,930 --> 00:13:18,050
and now it's it's processing the data

00:13:12,400 --> 00:13:20,360
splitting the CSV into multiple and then

00:13:18,050 --> 00:13:23,780
it will be then it will be training the

00:13:20,360 --> 00:13:25,550
decision tree what we have in open ship

00:13:23,780 --> 00:13:28,160
as I as I mentioned we have the Jupiter

00:13:25,550 --> 00:13:29,990
hub and we have now my own Jupiter

00:13:28,160 --> 00:13:33,500
server with Jupiter have its routing too

00:13:29,990 --> 00:13:36,650
and we have the spark spark cluster so I

00:13:33,500 --> 00:13:38,720
will let it run it will take some time

00:13:36,650 --> 00:13:41,120
the training it takes like I don't know

00:13:38,720 --> 00:13:48,200
eight minutes so I'll go back to

00:13:41,120 --> 00:13:52,040
presentation and and then we can we can

00:13:48,200 --> 00:13:56,930
revisit that so about the architecture

00:13:52,040 --> 00:13:59,890
of Jupiter have basically the entry

00:13:56,930 --> 00:14:02,900
point where you access you as a user is

00:13:59,890 --> 00:14:06,290
Jupiter proxy which then routes either

00:14:02,900 --> 00:14:08,570
to Jupiter hub API or that your server

00:14:06,290 --> 00:14:10,100
that you started and then there is

00:14:08,570 --> 00:14:11,750
something called spoiler which takes

00:14:10,100 --> 00:14:15,350
care of spawning those notebooks so

00:14:11,750 --> 00:14:18,410
still service per user we use cube

00:14:15,350 --> 00:14:20,270
spoiler as the name suggests it is a

00:14:18,410 --> 00:14:22,640
spoiler that is working with kubernetes

00:14:20,270 --> 00:14:24,770
and it generates the port definition and

00:14:22,640 --> 00:14:27,020
submits it to open ship there is also a

00:14:24,770 --> 00:14:29,270
database which gets its just for

00:14:27,020 --> 00:14:31,310
tracking of users and started notebooks

00:14:29,270 --> 00:14:34,010
and things like that so that it doesn't

00:14:31,310 --> 00:14:36,770
and then the proxy routes and things

00:14:34,010 --> 00:14:40,220
like that so it doesn't disappear when

00:14:36,770 --> 00:14:41,900
there is some restart or something so

00:14:40,220 --> 00:14:45,140
how it works as seesaw

00:14:41,900 --> 00:14:47,630
a user comes to Jupiter hub and then

00:14:45,140 --> 00:14:49,520
it's redirected to some other negation

00:14:47,630 --> 00:14:50,440
there are multiple implementations of

00:14:49,520 --> 00:14:52,090
all the vacation

00:14:50,440 --> 00:14:55,960
for Jupiter health you can have github

00:14:52,090 --> 00:14:58,600
authentication you can have Kerberos you

00:14:55,960 --> 00:15:01,030
can have pre-generated set of users and

00:14:58,600 --> 00:15:04,000
password so if you we were doing some

00:15:01,030 --> 00:15:07,150
demos we just generated 20 users and

00:15:04,000 --> 00:15:12,220
gave gave the attendees for the workshop

00:15:07,150 --> 00:15:14,950
those users and passwords then when you

00:15:12,220 --> 00:15:18,430
when when the server is spawned so the

00:15:14,950 --> 00:15:22,540
user requests the server to be spawned

00:15:18,430 --> 00:15:25,120
Jupiter hub generates generates the

00:15:22,540 --> 00:15:27,610
artifacts for opener ship and if there

00:15:25,120 --> 00:15:29,560
is if it finds that I want to start a

00:15:27,610 --> 00:15:32,410
spark notebook it also generates like

00:15:29,560 --> 00:15:34,210
config map for spark operator I will

00:15:32,410 --> 00:15:38,110
explain explain what spark operator is

00:15:34,210 --> 00:15:42,460
later but basically it takes care of the

00:15:38,110 --> 00:15:45,520
spark clusters so open she starts my

00:15:42,460 --> 00:15:48,360
Jupiter server and notify the operator

00:15:45,520 --> 00:15:50,620
about the requested spark cluster and

00:15:48,360 --> 00:15:53,440
the sparklers they started and then the

00:15:50,620 --> 00:15:56,140
user and then the user accesses his

00:15:53,440 --> 00:15:59,650
notebook and connects the spark and when

00:15:56,140 --> 00:16:05,830
he stops the server it also kills the

00:15:59,650 --> 00:16:08,590
cluster we use APB and several playbook

00:16:05,830 --> 00:16:11,470
bundle you can learn about that from the

00:16:08,590 --> 00:16:15,220
documentation of open ship but basically

00:16:11,470 --> 00:16:16,030
it's just it's just a set of artifacts

00:16:15,220 --> 00:16:18,490
for openshift

00:16:16,030 --> 00:16:22,420
about how to deploy each service and how

00:16:18,490 --> 00:16:24,490
they should work together and you can

00:16:22,420 --> 00:16:26,470
have that in a catalog in openshift and

00:16:24,490 --> 00:16:29,980
nicely deployed by free clicks or

00:16:26,470 --> 00:16:32,620
something like that so that APB source

00:16:29,980 --> 00:16:35,440
code can be found in the open data of

00:16:32,620 --> 00:16:39,790
the io and we have we have a built in

00:16:35,440 --> 00:16:41,410
cryo under organizational data so we can

00:16:39,790 --> 00:16:43,839
go there and you can download an image

00:16:41,410 --> 00:16:48,940
and deploy it to your open shift and try

00:16:43,839 --> 00:16:50,980
it try that yourself so what is special

00:16:48,940 --> 00:16:53,020
about our jupiter hub because this is

00:16:50,980 --> 00:16:54,640
what you could do basically I build my

00:16:53,020 --> 00:16:57,040
work on top of work of

00:16:54,640 --> 00:17:00,250
Graham Templeton who has I have link at

00:16:57,040 --> 00:17:02,110
the end but he is Jupiter huh

00:17:00,250 --> 00:17:04,000
Jupiter upon open shapes or Jupiter have

00:17:02,110 --> 00:17:05,799
quickstart something like that and I

00:17:04,000 --> 00:17:07,540
just took that and build something on

00:17:05,799 --> 00:17:10,260
top of that and what are the differences

00:17:07,540 --> 00:17:13,179
basically are mainly these four things

00:17:10,260 --> 00:17:15,160
so we meet out the discovery single user

00:17:13,179 --> 00:17:17,199
profiles ephemeral clusters and publish

00:17:15,160 --> 00:17:20,589
and share what didn't meet what does it

00:17:17,199 --> 00:17:22,689
mean so you sorted select books for the

00:17:20,589 --> 00:17:26,140
images that is automatically generated

00:17:22,689 --> 00:17:27,970
from the notebooks that are built in

00:17:26,140 --> 00:17:31,299
OpenShift from the images that are built

00:17:27,970 --> 00:17:32,679
in OpenShift it is not very nice right

00:17:31,299 --> 00:17:34,809
now the user experience is not very good

00:17:32,679 --> 00:17:36,850
but I'm planning on improving that with

00:17:34,809 --> 00:17:38,260
some descriptions and like install

00:17:36,850 --> 00:17:40,470
dependencies and things like that so

00:17:38,260 --> 00:17:44,290
it's it's it provides more information

00:17:40,470 --> 00:17:45,880
to the user but it's it's helpful if you

00:17:44,290 --> 00:17:48,190
don't have just know you're going to

00:17:45,880 --> 00:17:50,460
remember you just pick from the from the

00:17:48,190 --> 00:17:53,260
selling books the single user profiles

00:17:50,460 --> 00:17:57,010
we quite quickly when we started to use

00:17:53,260 --> 00:17:59,590
Jupiter have we realize that every sub

00:17:57,010 --> 00:18:02,590
team in our team and every image has to

00:17:59,590 --> 00:18:06,010
have different configuration like if you

00:18:02,590 --> 00:18:07,419
are working with some park a file that

00:18:06,010 --> 00:18:09,490
you download from object storage and you

00:18:07,419 --> 00:18:11,350
don't use part because you just want to

00:18:09,490 --> 00:18:15,490
process it directly in the notebook you

00:18:11,350 --> 00:18:17,410
might need more more memory for the

00:18:15,490 --> 00:18:19,690
notebook if you are working this park

00:18:17,410 --> 00:18:21,669
then you don't need that much memory but

00:18:19,690 --> 00:18:24,370
you need spark deployed if you are

00:18:21,669 --> 00:18:27,309
working with some specific object

00:18:24,370 --> 00:18:28,780
storage endpoint or bucket you might

00:18:27,309 --> 00:18:32,799
need to have that in your environment

00:18:28,780 --> 00:18:35,260
variables so we build this library that

00:18:32,799 --> 00:18:37,890
basically is configured with a config

00:18:35,260 --> 00:18:41,140
map in openshift right now and you can

00:18:37,890 --> 00:18:43,090
mix and match the images that are that

00:18:41,140 --> 00:18:45,340
you are used and the user names and

00:18:43,090 --> 00:18:47,200
users with like what should happen when

00:18:45,340 --> 00:18:52,470
the user selects that image and how it

00:18:47,200 --> 00:18:57,300
should be configured you saw little I

00:18:52,470 --> 00:19:00,190
had that spark cluster inside inside my

00:18:57,300 --> 00:19:01,840
namespace and that works

00:19:00,190 --> 00:19:03,730
it's called spark operator and as

00:19:01,840 --> 00:19:05,350
operators are a concept in kubernetes

00:19:03,730 --> 00:19:08,380
and Oh preciate that there is a service

00:19:05,350 --> 00:19:10,540
the operator which listens on evens and

00:19:08,380 --> 00:19:11,350
if it finds some specific event it will

00:19:10,540 --> 00:19:14,040
react to it

00:19:11,350 --> 00:19:17,800
so here user comes and says please

00:19:14,040 --> 00:19:21,430
operator fairy can I get it

00:19:17,800 --> 00:19:23,350
can I get a spark and says yes sure you

00:19:21,430 --> 00:19:25,090
can get a spark and a deploys part based

00:19:23,350 --> 00:19:27,040
on the configuration and when the user

00:19:25,090 --> 00:19:28,960
leaves and says I don't need a spark

00:19:27,040 --> 00:19:30,910
anymore so it removes the config map or

00:19:28,960 --> 00:19:34,870
the custom resource it will delete the

00:19:30,910 --> 00:19:36,880
spark cluster again so we have that in

00:19:34,870 --> 00:19:40,110
the in the profiles that we say that if

00:19:36,880 --> 00:19:43,630
you select the spark image we want to

00:19:40,110 --> 00:19:46,330
instrument the spark operator about that

00:19:43,630 --> 00:19:50,740
configuration of the spark image and say

00:19:46,330 --> 00:19:54,670
please deploy two workers and one master

00:19:50,740 --> 00:19:56,890
with these resource limits for us

00:19:54,670 --> 00:20:00,430
yeah the last and the last bit is

00:19:56,890 --> 00:20:05,230
basically what we hit is also quite

00:20:00,430 --> 00:20:07,420
early the workflow about sorry the

00:20:05,230 --> 00:20:09,670
workflow about how do you share your

00:20:07,420 --> 00:20:11,620
notebooks because if you have two Patera

00:20:09,670 --> 00:20:15,670
hub and you want to I don't know I want

00:20:11,620 --> 00:20:17,920
to give shared my notebook I have to

00:20:15,670 --> 00:20:19,900
download it I have to send it over email

00:20:17,920 --> 00:20:21,820
or push it to get and then he needs to

00:20:19,900 --> 00:20:24,460
download it and upload it to upload it

00:20:21,820 --> 00:20:27,520
to his jupiter notebook versus jupiter

00:20:24,460 --> 00:20:29,500
have sorry which is not very nice if i

00:20:27,520 --> 00:20:32,080
just want to show him a simple change in

00:20:29,500 --> 00:20:35,830
like line 24 I changed this letter and

00:20:32,080 --> 00:20:39,660
now it works so I build that plugin for

00:20:35,830 --> 00:20:39,660
Jupiter hub where you click a button

00:20:41,970 --> 00:20:51,570
where you click a button you give you

00:20:47,009 --> 00:20:57,779
give it some name no your you give it

00:20:51,570 --> 00:21:00,509
some name and you hit publish and you

00:20:57,779 --> 00:21:03,120
get a URL which you can access and you

00:21:00,509 --> 00:21:05,370
get a and B viewer and we viewer is a

00:21:03,120 --> 00:21:07,139
tool that lets you view the notebooks

00:21:05,370 --> 00:21:09,330
without being able to execute anything

00:21:07,139 --> 00:21:11,669
so it's read-only but it renders the

00:21:09,330 --> 00:21:15,360
notebook in a basically the same way as

00:21:11,669 --> 00:21:17,490
a Jupiter hub and this this URL you can

00:21:15,360 --> 00:21:19,289
share if it's it's public it's not

00:21:17,490 --> 00:21:21,389
behind the order dication so you can

00:21:19,289 --> 00:21:24,330
share it with anyone and then he can

00:21:21,389 --> 00:21:28,830
just view and he can also download the

00:21:24,330 --> 00:21:32,820
notebook so that that I think helped us

00:21:28,830 --> 00:21:37,289
a lot to speed up the speed of the

00:21:32,820 --> 00:21:38,940
process so how is our training going so

00:21:37,289 --> 00:21:44,970
we saw that we see that the decision

00:21:38,940 --> 00:21:47,909
tree classifier got trains this is the

00:21:44,970 --> 00:21:52,409
decision tree so there's a lot of ethan

00:21:47,909 --> 00:21:56,549
else statements and now it's doing

00:21:52,409 --> 00:21:58,590
something else so i didn't really dig

00:21:56,549 --> 00:22:01,919
deep into this notebook i just wanted to

00:21:58,590 --> 00:22:06,929
show it basically with our deployment we

00:22:01,919 --> 00:22:09,629
can directly use spark and and the ml

00:22:06,929 --> 00:22:11,610
libraries without having to do many

00:22:09,629 --> 00:22:13,649
changes to the notebook that i found

00:22:11,610 --> 00:22:22,230
randomly on the internet so that the

00:22:13,649 --> 00:22:26,490
integration is very good so yeah so i

00:22:22,230 --> 00:22:28,470
just wanted to go over a couple ideas

00:22:26,490 --> 00:22:30,330
that i have about like next steps for

00:22:28,470 --> 00:22:32,970
the for the jupiter have that we could

00:22:30,330 --> 00:22:35,820
do so we right now have this spark

00:22:32,970 --> 00:22:37,379
operator integrated but i it seems that

00:22:35,820 --> 00:22:42,509
dusk i don't know if you heard about

00:22:37,379 --> 00:22:44,360
dusk it's a python-based distributed

00:22:42,509 --> 00:22:51,019
analytics engine or

00:22:44,360 --> 00:22:52,610
whatever you would call it provides I've

00:22:51,019 --> 00:22:54,950
never listened for a Linux enabling

00:22:52,610 --> 00:22:57,159
performance at scale for tools laughs so

00:22:54,950 --> 00:22:58,850
it's basically seems like spark

00:22:57,159 --> 00:23:00,740
implemented in Python

00:22:58,850 --> 00:23:04,690
supporting Python better than spark

00:23:00,740 --> 00:23:07,669
maybe so

00:23:04,690 --> 00:23:10,100
we are thinking about like adding that

00:23:07,669 --> 00:23:11,590
next to the spark operator having a desk

00:23:10,100 --> 00:23:16,159
operator which would then spawn

00:23:11,590 --> 00:23:20,090
Dustbuster if users ones that if you if

00:23:16,159 --> 00:23:24,860
you noticed in my notebook I have I have

00:23:20,090 --> 00:23:27,620
these environment variables with with

00:23:24,860 --> 00:23:29,330
credentials they are not there

00:23:27,620 --> 00:23:31,789
automatically but I would like to have

00:23:29,330 --> 00:23:35,710
them they're automatically populated for

00:23:31,789 --> 00:23:40,669
users based on some secret somewhere I

00:23:35,710 --> 00:23:45,980
have to edit them in single user

00:23:40,669 --> 00:23:49,399
profiles config map so I would like to

00:23:45,980 --> 00:23:51,649
have that as a automated way how to get

00:23:49,399 --> 00:23:54,799
those credentials from some source of

00:23:51,649 --> 00:23:56,120
truth and push them into the server

00:23:54,799 --> 00:23:59,090
automatically sort of use them it

00:23:56,120 --> 00:24:00,679
doesn't have to care about that I would

00:23:59,090 --> 00:24:03,230
like to work on get happen give up

00:24:00,679 --> 00:24:05,210
integrations so you can have a button

00:24:03,230 --> 00:24:07,370
same as the published one I would like

00:24:05,210 --> 00:24:09,470
to push this to my git repo or I want I

00:24:07,370 --> 00:24:11,630
want to create a git repo for this

00:24:09,470 --> 00:24:15,049
notebook or something like that

00:24:11,630 --> 00:24:16,820
I've seen some attempts on internet that

00:24:15,049 --> 00:24:21,620
people were doing that but it never

00:24:16,820 --> 00:24:23,539
really worked in a user-friendly way you

00:24:21,620 --> 00:24:26,210
saw that selected books which was pretty

00:24:23,539 --> 00:24:28,820
ugly for the images so I'd like to make

00:24:26,210 --> 00:24:33,649
it more fancy more more user friendly

00:24:28,820 --> 00:24:36,769
and make users make more useful for

00:24:33,649 --> 00:24:38,510
users and also Jupiter have exposes

00:24:36,769 --> 00:24:41,029
metrics so if you want to know how many

00:24:38,510 --> 00:24:43,519
requests how many users and things like

00:24:41,029 --> 00:24:45,080
that and maybe build something on top of

00:24:43,519 --> 00:24:45,590
that like my cluster is getting full

00:24:45,080 --> 00:24:47,180
because I

00:24:45,590 --> 00:24:51,380
to many users using Jupiter have at the

00:24:47,180 --> 00:24:52,970
same time but we need to enable I think

00:24:51,380 --> 00:24:56,090
they are enabled but we I'm not sure

00:24:52,970 --> 00:24:58,040
what is exactly in there and we don't

00:24:56,090 --> 00:25:01,340
have Prometheus watching that so we need

00:24:58,040 --> 00:25:03,770
to set that up also for the Jupiter hub

00:25:01,340 --> 00:25:06,260
ABB and probably extend the metrics

00:25:03,770 --> 00:25:08,420
because as we start using the spark and

00:25:06,260 --> 00:25:10,070
the connection between Jupiter happens

00:25:08,420 --> 00:25:16,070
Park we need to be able to map it

00:25:10,070 --> 00:25:19,190
together in the metrics and and that's

00:25:16,070 --> 00:25:21,650
basically it's basically everything I

00:25:19,190 --> 00:25:26,300
had these are some useful links so it's

00:25:21,650 --> 00:25:27,770
the APB this is the link for the for

00:25:26,300 --> 00:25:30,230
this single user profiles which is a

00:25:27,770 --> 00:25:35,810
quite simple library just for that

00:25:30,230 --> 00:25:38,810
bodies case here are the OpenShift

00:25:35,810 --> 00:25:42,320
configuration for the Jupiter hub which

00:25:38,810 --> 00:25:44,930
is then used in the ADB spark operator a

00:25:42,320 --> 00:25:46,430
colleague from red analytics IO red

00:25:44,930 --> 00:25:49,820
analytics team in redhead with working

00:25:46,430 --> 00:25:52,880
on on the spark operator so that I just

00:25:49,820 --> 00:25:55,190
use it and it worked perfectly and this

00:25:52,880 --> 00:25:57,110
is where we came from

00:25:55,190 --> 00:25:59,150
with the Jupiter have the Jupiter happen

00:25:57,110 --> 00:26:01,700
open ship which your hand above the put

00:25:59,150 --> 00:26:03,470
Graham Templeton put together and you

00:26:01,700 --> 00:26:05,840
can go there and you can try to better

00:26:03,470 --> 00:26:08,090
have without all these sparks and things

00:26:05,840 --> 00:26:11,930
like that just on opal ship in a

00:26:08,090 --> 00:26:16,270
simplest in a simplest form yes so

00:26:11,930 --> 00:26:16,270
that's basically it any questions

00:26:17,799 --> 00:26:21,100
yes sure

00:26:25,850 --> 00:26:30,950
yeah so the question is whether the

00:26:28,519 --> 00:26:33,139
spark operator we get we have one shared

00:26:30,950 --> 00:26:37,009
cluster or if we have two cluster four

00:26:33,139 --> 00:26:38,539
users yeah I didn't mention that so we

00:26:37,009 --> 00:26:41,090
are basically thinking whether we should

00:26:38,539 --> 00:26:42,799
deploy one big beef a cluster or for

00:26:41,090 --> 00:26:45,559
spark and then let us everyone connect

00:26:42,799 --> 00:26:46,789
to it but it has its issues and

00:26:45,559 --> 00:26:48,500
limitations like that you have to

00:26:46,789 --> 00:26:50,299
reserve that capacity on your open

00:26:48,500 --> 00:26:53,539
cheese Buster like if you want to really

00:26:50,299 --> 00:26:55,130
have a hundred users and you would you

00:26:53,539 --> 00:26:57,200
want to allow them all of them at the

00:26:55,130 --> 00:26:59,090
same time go to that cluster then you

00:26:57,200 --> 00:27:00,919
need to have reserved hundreds of

00:26:59,090 --> 00:27:02,870
gigabytes of RAM for those for those

00:27:00,919 --> 00:27:04,820
workers or you can have a ephemeral

00:27:02,870 --> 00:27:07,129
cluster so when the user comes and logs

00:27:04,820 --> 00:27:08,929
in and start the server it will start a

00:27:07,129 --> 00:27:10,970
spark for him and when he goes away it

00:27:08,929 --> 00:27:14,509
will kill the spark Custer for for a

00:27:10,970 --> 00:27:16,490
spark spark cluster so we are doing the

00:27:14,509 --> 00:27:19,700
ephemeral clusters right now so when the

00:27:16,490 --> 00:27:22,460
user comes he gets his own fresh clean

00:27:19,700 --> 00:27:24,500
spark cluster with some resource limits

00:27:22,460 --> 00:27:26,360
which are obviously tighter than if it

00:27:24,500 --> 00:27:30,919
would be one big cluster and you would

00:27:26,360 --> 00:27:33,980
be only one there but we need to do some

00:27:30,919 --> 00:27:35,570
performance testing and and get some

00:27:33,980 --> 00:27:38,120
more data about like how that actually

00:27:35,570 --> 00:27:44,500
works and if it's if it's useful for

00:27:38,120 --> 00:27:44,500
people started on us for water

00:27:46,800 --> 00:27:50,780
the sparkle sir it's quite fast

00:27:50,900 --> 00:27:57,690
basically I can I can promise you soil

00:27:55,200 --> 00:28:04,380
I'll kill my cluster I will stop my

00:27:57,690 --> 00:28:06,570
jupiter server I think it's well it's

00:28:04,380 --> 00:28:10,620
basically couple couple seconds or maybe

00:28:06,570 --> 00:28:15,000
maybe couple of tens of seconds why can

00:28:10,620 --> 00:28:19,740
I close that right so once the Jupiter

00:28:15,000 --> 00:28:22,250
notebook server disappears which should

00:28:19,740 --> 00:28:24,900
be any second now

00:28:22,250 --> 00:28:30,180
but it has to wait for the time out

00:28:24,900 --> 00:28:33,150
because there are no shutdown scripts in

00:28:30,180 --> 00:28:36,420
the in the image it's also one thing

00:28:33,150 --> 00:28:39,420
that we need to fix so when it goes down

00:28:36,420 --> 00:28:41,430
the spark cluster will disappear as well

00:28:39,420 --> 00:28:42,960
and then I can start again so in the

00:28:41,430 --> 00:28:49,400
meantime we can take probably another

00:28:42,960 --> 00:28:53,870
question or two if there are any yes

00:28:49,400 --> 00:28:58,490
where's your data Kappas where is your

00:28:53,870 --> 00:28:59,900
data kept I have an HDFS cluster so we

00:28:58,490 --> 00:29:04,340
have that we have the set of cluster

00:28:59,900 --> 00:29:06,170
making the making the ownership and thus

00:29:04,340 --> 00:29:08,660
if the staff cluster is basically where

00:29:06,170 --> 00:29:11,120
we push and pull data from okay so I

00:29:08,660 --> 00:29:14,060
could just connect my Hadoop cluster

00:29:11,120 --> 00:29:16,810
yeah it doesn't it doesn't really matter

00:29:14,060 --> 00:29:20,630
like what technology you choose for that

00:29:16,810 --> 00:29:23,150
so it's gone so I'll just go here I

00:29:20,630 --> 00:29:26,810
click start my server I will pick this

00:29:23,150 --> 00:29:29,030
Park and I'll go back here and you'll

00:29:26,810 --> 00:29:31,970
see that my jupiter notebook is starting

00:29:29,030 --> 00:29:35,330
and basically immediately i got the two

00:29:31,970 --> 00:29:37,190
workers running and the master node

00:29:35,330 --> 00:29:40,610
takes a bit of time because it needs to

00:29:37,190 --> 00:29:42,740
connect to the workers and figure out it

00:29:40,610 --> 00:29:44,900
depends on like if you are starting it

00:29:42,740 --> 00:29:48,110
for the first time there are some time

00:29:44,900 --> 00:29:51,890
that needs to that images the container

00:29:48,110 --> 00:29:54,200
images takes to download on the node but

00:29:51,890 --> 00:29:56,810
if it's like the second start and the

00:29:54,200 --> 00:29:59,060
images are are same for all every user

00:29:56,810 --> 00:30:04,510
so once they are downloaded on the note

00:29:59,060 --> 00:30:08,330
it is basically instantaneous start I

00:30:04,510 --> 00:30:12,110
don't know what why the master takes so

00:30:08,330 --> 00:30:14,060
long now but I think that it's running

00:30:12,110 --> 00:30:17,300
it's fine it just didn't update the UI

00:30:14,060 --> 00:30:19,810
yeah so it was basically instantaneous

00:30:17,300 --> 00:30:19,810
starter

00:30:23,360 --> 00:30:32,510
yes the notebooks blood spawner doesn't

00:30:29,270 --> 00:30:35,150
scale up to multiple nodes or do you

00:30:32,510 --> 00:30:37,340
have to configure that for just on the

00:30:35,150 --> 00:30:42,080
Jupiter hub yeah so this so the spawner

00:30:37,340 --> 00:30:44,570
the spawner is not doing anything smart

00:30:42,080 --> 00:30:47,420
it just generates the poor definition

00:30:44,570 --> 00:30:50,390
and pushes it to open shift and open

00:30:47,420 --> 00:30:55,190
schedules the bottom so that basically

00:30:50,390 --> 00:30:57,140
means that s is that it's up to the open

00:30:55,190 --> 00:30:58,940
she scheduler to schedule these so it

00:30:57,140 --> 00:31:00,680
would it would distribute them across

00:30:58,940 --> 00:31:03,200
the cluster it wouldn't put them on a

00:31:00,680 --> 00:31:06,260
single note depending on the size and

00:31:03,200 --> 00:31:07,580
and load on the cluster I don't know the

00:31:06,260 --> 00:31:10,820
details of implementation of the open

00:31:07,580 --> 00:31:13,490
shoot scheduler but yeah it is it is it

00:31:10,820 --> 00:31:16,340
is based on the open shift scheduling so

00:31:13,490 --> 00:31:19,100
it would be distributed and the same

00:31:16,340 --> 00:31:21,470
thing goes for for the SPARC it doesn't

00:31:19,100 --> 00:31:23,530
have we could configure it in a way that

00:31:21,470 --> 00:31:26,390
it would have some affinities so like

00:31:23,530 --> 00:31:29,870
get put my Jupiter hub close to the

00:31:26,390 --> 00:31:31,820
SPARC but it doesn't really bring

00:31:29,870 --> 00:31:34,490
anything because we are trying to we are

00:31:31,820 --> 00:31:37,610
trying to pull the data from the cell

00:31:34,490 --> 00:31:40,330
for s3 or something not from not sending

00:31:37,610 --> 00:31:44,350
it from the certain notebook server I

00:31:40,330 --> 00:31:44,350
think there was some other question

00:31:48,129 --> 00:31:52,779
how make sure is this part operator it

00:31:51,039 --> 00:31:56,169
is it is quite new I think it's like a

00:31:52,779 --> 00:31:58,509
couple weeks old but honestly there is

00:31:56,169 --> 00:31:59,919
notice I mean there is missing and I

00:31:58,509 --> 00:32:01,629
still miss that I've already filed

00:31:59,919 --> 00:32:05,019
couple of feature requests I'm missing

00:32:01,629 --> 00:32:06,549
some configuration options like at the

00:32:05,019 --> 00:32:11,349
beginning there was there was no another

00:32:06,549 --> 00:32:12,729
way how to set limits resource limits

00:32:11,349 --> 00:32:14,499
further for the workers and for the

00:32:12,729 --> 00:32:16,809
master and stuff like that so there's

00:32:14,499 --> 00:32:20,079
there now and I have couple more feature

00:32:16,809 --> 00:32:23,319
requests in queue for like I want to be

00:32:20,079 --> 00:32:26,349
able to force update images and I want

00:32:23,319 --> 00:32:27,609
to be able to configure these values and

00:32:26,349 --> 00:32:29,769
stuff like that but generally the

00:32:27,609 --> 00:32:32,919
working of like spawning and killing the

00:32:29,769 --> 00:32:36,249
cluster it's it's working very well I

00:32:32,919 --> 00:32:38,739
haven't I have not an issue that the the

00:32:36,249 --> 00:32:41,339
guy here who works on the spark operator

00:32:38,739 --> 00:32:45,249
he actually built a library in Java

00:32:41,339 --> 00:32:46,899
thing is equals in JVM operators which

00:32:45,249 --> 00:32:50,019
is like a library that you could use to

00:32:46,899 --> 00:32:53,399
build different other operators so he's

00:32:50,019 --> 00:32:55,539
trying to get that very stable and then

00:32:53,399 --> 00:33:01,479
spark operator who directly benefit from

00:32:55,539 --> 00:33:04,209
that okay I think we are out of time

00:33:01,479 --> 00:33:08,489
anyway we have one more minute okay so

00:33:04,209 --> 00:33:08,489
if there is there's a any other question

00:33:08,699 --> 00:33:13,229
no what's here for us thank you very

00:33:10,959 --> 00:33:13,229
much

00:33:15,900 --> 00:33:20,179
and enjoyed the rest of the conference

00:33:17,549 --> 00:33:20,179

YouTube URL: https://www.youtube.com/watch?v=by0l3b55i7g


