Title: Adding Smart Disk Failure Prediction to Ceph
Publication date: 2019-02-21
Playlist: DevConfUS 2018
Description: 
	Ceph is a distributed storage system designed to scale to thousands or tens of thousands of drives. In such systems, failure becomes the norm rather than the exception, and predicting device failures before they happen can significantly improve the overall reliability of the system. This talk will describe a project to make Ceph monitor the storage devices in the cluster, using tools like SMART, so that future failures can be anticipated using a pluggable prediction model, allowing the cluster to reduce the risk of data loss by preemptively warning the operator or migrating data and workload off the failing device. Finally, weâ€™ll discuss the opportunity to build a public data set of device failure data to build a high quality, free and open source model for use it any storage system.
Captions: 
	00:00:03,270 --> 00:00:09,549
safe principal architect at Red Hat and

00:00:06,370 --> 00:00:12,309
Yared hitaka software developer working

00:00:09,549 --> 00:00:15,100
with SAP and they will be talking about

00:00:12,309 --> 00:00:19,860
adding smartdesq failure prediction to

00:00:15,100 --> 00:00:19,860
Seth take it away

00:00:28,370 --> 00:00:44,750
all right hello everyone my name is safe

00:00:30,530 --> 00:00:46,130
while okay so today we're gonna talk

00:00:44,750 --> 00:00:47,809
about editing smart failure prediction

00:00:46,130 --> 00:00:51,110
to Seth we're gonna start with a little

00:00:47,809 --> 00:00:52,489
bit of background what stuff is why

00:00:51,110 --> 00:00:54,770
we're adding smart failure prediction

00:00:52,489 --> 00:00:56,210
we'll talk about the journey we took in

00:00:54,770 --> 00:00:58,670
order to arrive at the architecture and

00:00:56,210 --> 00:01:01,910
that we did the current status of the

00:00:58,670 --> 00:01:03,080
project and some next steps and then

00:01:01,910 --> 00:01:06,619
we'll talk a little bit about average

00:01:03,080 --> 00:01:10,190
just an internship program that led to

00:01:06,619 --> 00:01:12,080
this whole project maybe one of the most

00:01:10,190 --> 00:01:16,700
exciting thing about this project was

00:01:12,080 --> 00:01:20,119
how it brought together the open source

00:01:16,700 --> 00:01:23,600
communities along with the outreach in

00:01:20,119 --> 00:01:28,119
internship of course open source

00:01:23,600 --> 00:01:30,890
community and we getting the new

00:01:28,119 --> 00:01:34,009
industry participants in this project

00:01:30,890 --> 00:01:34,520
which is really excited about this

00:01:34,009 --> 00:01:38,390
project

00:01:34,520 --> 00:01:41,900
rocketstor yeah and this is how open

00:01:38,390 --> 00:01:43,509
source does it's magic which is natural

00:01:41,900 --> 00:01:54,130
[Music]

00:01:43,509 --> 00:01:57,080
sure okay

00:01:54,130 --> 00:02:02,030
so probably all of you here know what

00:01:57,080 --> 00:02:05,690
step is but just a quick recap that is

00:02:02,030 --> 00:02:08,649
an object block and file storage system

00:02:05,690 --> 00:02:12,140
and all in a single cluster it is

00:02:08,649 --> 00:02:14,959
designed so all components scale

00:02:12,140 --> 00:02:15,459
horizontally with no single point of

00:02:14,959 --> 00:02:20,230
failure

00:02:15,459 --> 00:02:24,560
it is Software Defined so it can run on

00:02:20,230 --> 00:02:27,590
commodity hardware and it is self

00:02:24,560 --> 00:02:30,220
managing wherever possible which is

00:02:27,590 --> 00:02:31,990
really relevant for this project

00:02:30,220 --> 00:02:35,740
doesn't want to make it even more

00:02:31,990 --> 00:02:38,650
possible to be self-healing and maybe

00:02:35,740 --> 00:02:41,140
again the most important part is that it

00:02:38,650 --> 00:02:43,450
is completely free and open source you

00:02:41,140 --> 00:02:49,950
can use it you can see all of the code

00:02:43,450 --> 00:02:53,980
which is it's awesome thanks

00:02:49,950 --> 00:02:58,090
so the concept was to teach self how to

00:02:53,980 --> 00:03:02,470
collect health metrics from its devices

00:02:58,090 --> 00:03:04,510
I went to pass it to a pre trained model

00:03:02,470 --> 00:03:08,230
that can predict whether our device is

00:03:04,510 --> 00:03:13,720
about to fail or not or when it's about

00:03:08,230 --> 00:03:16,989
to fail it was important for us to keep

00:03:13,720 --> 00:03:22,000
the design modular so we can either use

00:03:16,989 --> 00:03:26,800
an in the bath model or to send all the

00:03:22,000 --> 00:03:29,680
collected data to service on cloud make

00:03:26,800 --> 00:03:34,329
make it do the prediction and then get

00:03:29,680 --> 00:03:39,459
the prediction back to the cluster and

00:03:34,329 --> 00:03:41,500
then with this estimation of what if the

00:03:39,459 --> 00:03:44,459
device is divided about to fail or or

00:03:41,500 --> 00:03:49,600
when we want to teach staff how to

00:03:44,459 --> 00:03:51,549
respond to an imminent failure before it

00:03:49,600 --> 00:04:01,180
actually happened

00:03:51,549 --> 00:04:04,900
which makes the data even more safe it

00:04:01,180 --> 00:04:09,090
resets to reliability it's a heart

00:04:04,900 --> 00:04:12,450
attack but devices eventually will fail

00:04:09,090 --> 00:04:20,160
does anybody here ever experience

00:04:12,450 --> 00:04:25,630
theta lows terrible losses of photos

00:04:20,160 --> 00:04:28,250
documents still hurt

00:04:25,630 --> 00:04:33,500
on a personal level

00:04:28,250 --> 00:04:35,300
yeah it's not very fun to lose data for

00:04:33,500 --> 00:04:41,900
business businesses it can be

00:04:35,300 --> 00:04:44,449
devastating and so we all use redundancy

00:04:41,900 --> 00:04:48,560
in order to avoid the data loss

00:04:44,449 --> 00:04:52,820
we will replicate a datum by using great

00:04:48,560 --> 00:04:56,479
racial coding so it's playing the

00:04:52,820 --> 00:04:59,900
numbers game just to know how much we

00:04:56,479 --> 00:05:03,139
can invest in that it can be really

00:04:59,900 --> 00:05:09,169
expensive to to be able to replicate

00:05:03,139 --> 00:05:14,330
data with lots of replicas so whenever a

00:05:09,169 --> 00:05:19,280
device fails it changes the statistics

00:05:14,330 --> 00:05:23,289
so the redundancy there yangtze gets

00:05:19,280 --> 00:05:28,760
worse and we will use replicas counts

00:05:23,289 --> 00:05:32,960
and a window opens for even elevated

00:05:28,760 --> 00:05:37,490
risk for data loss larger systems mean

00:05:32,960 --> 00:05:42,229
that we have more devices and it means

00:05:37,490 --> 00:05:45,260
that we have more failure eventually and

00:05:42,229 --> 00:05:48,470
this is a sentence that uses that in

00:05:45,260 --> 00:05:51,110
itself but failure becomes the norm

00:05:48,470 --> 00:05:58,699
rather than the exception again hard

00:05:51,110 --> 00:06:01,580
truth but your effect so if we can

00:05:58,699 --> 00:06:06,770
predict whenever a failure is about to

00:06:01,580 --> 00:06:09,919
happen we can act ahead of time and we

00:06:06,770 --> 00:06:14,330
can preemptively recover

00:06:09,919 --> 00:06:25,099
that device which makes the cluster much

00:06:14,330 --> 00:06:30,169
more reliable yeah and that brings us to

00:06:25,099 --> 00:06:33,879
the other part of the performance so it

00:06:30,169 --> 00:06:40,430
is a natural that the cluster usage has

00:06:33,879 --> 00:06:45,710
a certain pattern of peaks and off-peak

00:06:40,430 --> 00:06:49,340
periods during the day in case we have a

00:06:45,710 --> 00:06:54,919
failure we have to respond with a

00:06:49,340 --> 00:06:58,430
recovery and the priority is very high

00:06:54,919 --> 00:07:01,610
so it might happen on a peak hour which

00:06:58,430 --> 00:07:05,990
is not ideal whatsoever so if we

00:07:01,610 --> 00:07:10,729
preemptively recover we can schedule it

00:07:05,990 --> 00:07:15,409
for a time which is a peak which is

00:07:10,729 --> 00:07:18,620
fantastic yeah and which set in

00:07:15,409 --> 00:07:22,639
particular recovery can be can have

00:07:18,620 --> 00:07:24,409
significant impact on the font maybe say

00:07:22,639 --> 00:07:26,599
if you want to say a couple words about

00:07:24,409 --> 00:07:28,909
that yeah this is sort of a I think in

00:07:26,599 --> 00:07:30,110
any system you're gonna have an extra

00:07:28,909 --> 00:07:31,940
cost that you pay in order physical

00:07:30,110 --> 00:07:34,909
recovery it's a problem that we started

00:07:31,940 --> 00:07:36,500
with incest making that as little as

00:07:34,909 --> 00:07:39,169
possible but it's still something that

00:07:36,500 --> 00:07:41,319
you ideally want to do in an off hour if

00:07:39,169 --> 00:07:41,319
you can

00:07:46,220 --> 00:07:51,020
so when we started this project we're

00:07:49,500 --> 00:07:53,190
sort of blank slate

00:07:51,020 --> 00:07:54,810
our goal was just to make everything

00:07:53,190 --> 00:07:56,700
sort of work in stuff out of the box so

00:07:54,810 --> 00:07:58,890
when you install stuff as a standard

00:07:56,700 --> 00:08:01,320
user do it gather the metrics would be

00:07:58,890 --> 00:08:02,880
the prediction and do everything without

00:08:01,320 --> 00:08:04,980
having to install extra tools or

00:08:02,880 --> 00:08:06,600
external dependencies so we would build

00:08:04,980 --> 00:08:08,400
on the data collection we would build a

00:08:06,600 --> 00:08:09,510
simple prediction model the expectation

00:08:08,400 --> 00:08:11,310
was we would start with something really

00:08:09,510 --> 00:08:13,200
simple it would be open source and we

00:08:11,310 --> 00:08:14,340
would hope that the community or would

00:08:13,200 --> 00:08:17,070
develop something better and it would

00:08:14,340 --> 00:08:22,830
improve over time but it was sort of a

00:08:17,070 --> 00:08:26,370
monolithic approach to the problem about

00:08:22,830 --> 00:08:28,470
halfway through the project company came

00:08:26,370 --> 00:08:31,710
along called profits store and that

00:08:28,470 --> 00:08:33,060
specializes in doing AI enabled data

00:08:31,710 --> 00:08:35,640
center operations and in particular

00:08:33,060 --> 00:08:38,010
doing disk failure prediction and they

00:08:35,640 --> 00:08:40,560
had a SAS based product that collects

00:08:38,010 --> 00:08:42,630
data metrics from their customers and

00:08:40,560 --> 00:08:45,500
has runs a very sophisticated prediction

00:08:42,630 --> 00:08:49,140
model that they claim like 97% accuracy

00:08:45,500 --> 00:08:51,300
and has a dashboard very sophisticated

00:08:49,140 --> 00:08:53,310
and they were very interested in

00:08:51,300 --> 00:08:54,630
integrating the staff so that stuff

00:08:53,310 --> 00:08:57,120
sorts clusters could take advantage of

00:08:54,630 --> 00:08:58,290
their service and and reap the rewards

00:08:57,120 --> 00:08:59,490
and so they wanted to work with

00:08:58,290 --> 00:09:02,850
community to figure out how to integrate

00:08:59,490 --> 00:09:04,500
this i'm with us and their goal

00:09:02,850 --> 00:09:06,240
basically was to provide a free service

00:09:04,500 --> 00:09:08,310
to any set user so that they could share

00:09:06,240 --> 00:09:10,680
the day with data with profit store get

00:09:08,310 --> 00:09:12,480
their predictions back or if they become

00:09:10,680 --> 00:09:13,650
a paying customer for example because

00:09:12,480 --> 00:09:14,880
they had a large cluster they wanted

00:09:13,650 --> 00:09:17,570
more accurate predictions and they could

00:09:14,880 --> 00:09:20,190
use that sort of fee for service as well

00:09:17,570 --> 00:09:21,780
and so and there they wanted to have

00:09:20,190 --> 00:09:23,370
both an on-premise option that would

00:09:21,780 --> 00:09:27,150
work in your data center or be able to

00:09:23,370 --> 00:09:28,740
use their SAS service and so as we this

00:09:27,150 --> 00:09:30,570
conversation involved with with private

00:09:28,740 --> 00:09:32,910
store we realized that the two

00:09:30,570 --> 00:09:34,620
completely different models and ways of

00:09:32,910 --> 00:09:35,820
approaching it and we're sort of

00:09:34,620 --> 00:09:36,540
colliding and we need to figure out how

00:09:35,820 --> 00:09:38,190
to how to

00:09:36,540 --> 00:09:40,620
problem so what we eventually ended up

00:09:38,190 --> 00:09:42,510
with was a modular approach that allows

00:09:40,620 --> 00:09:44,790
you to swap out different components of

00:09:42,510 --> 00:09:46,950
the overall pipeline to enable to

00:09:44,790 --> 00:09:49,080
accommodate both models and the

00:09:46,950 --> 00:09:51,360
self-contained and using a commercial

00:09:49,080 --> 00:09:52,980
service so it basically breaks down into

00:09:51,360 --> 00:09:54,840
three pieces and you have to collect the

00:09:52,980 --> 00:09:56,670
metrics on the devices you have to run

00:09:54,840 --> 00:09:58,050
some prediction on them and then once

00:09:56,670 --> 00:10:00,540
you know what the life expectancy of

00:09:58,050 --> 00:10:02,160
devices are then you have to you want to

00:10:00,540 --> 00:10:06,930
make Stefano matically respond and have

00:10:02,160 --> 00:10:08,310
some mitigation action and so we built

00:10:06,930 --> 00:10:09,750
all these components we want to build

00:10:08,310 --> 00:10:11,010
all these components into SEF so that

00:10:09,750 --> 00:10:12,690
the whole thing can work out of the box

00:10:11,010 --> 00:10:14,490
they can do the collection it can do a

00:10:12,690 --> 00:10:16,380
prediction and it can do the mitigation

00:10:14,490 --> 00:10:17,790
or if you want to use external tools

00:10:16,380 --> 00:10:19,370
because you're already scraping device

00:10:17,790 --> 00:10:21,000
metrics using some other infrastructure

00:10:19,370 --> 00:10:22,380
or if you want to use an external

00:10:21,000 --> 00:10:24,540
service like profit store you can do

00:10:22,380 --> 00:10:25,770
that as well and by building sort of

00:10:24,540 --> 00:10:27,960
breaking it down into these three phases

00:10:25,770 --> 00:10:29,760
we allow you to swap in these different

00:10:27,960 --> 00:10:33,860
components and use it in whatever way

00:10:29,760 --> 00:10:41,160
makes sense so the first part of this is

00:10:33,860 --> 00:10:46,140
gathering device metrics so what is the

00:10:41,160 --> 00:10:52,940
smart smart stands for sales monitoring

00:10:46,140 --> 00:10:57,180
analysis and reporting technology it was

00:10:52,940 --> 00:11:02,270
it was started as an attempt to to give

00:10:57,180 --> 00:11:09,060
access to the devices health brothers

00:11:02,270 --> 00:11:12,960
and it supplies a very very simple

00:11:09,060 --> 00:11:16,430
prediction model whether the health the

00:11:12,960 --> 00:11:20,730
device of the health is okay or not it

00:11:16,430 --> 00:11:24,420
defines several attributes so for

00:11:20,730 --> 00:11:26,370
example the devices temperature the

00:11:24,420 --> 00:11:28,490
number of the hours that the device is

00:11:26,370 --> 00:11:31,930
powered on

00:11:28,490 --> 00:11:35,420
number that sectors so on and so forth

00:11:31,930 --> 00:11:39,320
and each manufacturer you find their own

00:11:35,420 --> 00:11:43,370
thresholds for these pretties attributes

00:11:39,320 --> 00:11:45,410
so in case an attribute process a

00:11:43,370 --> 00:11:51,170
certain threshold the device is

00:11:45,410 --> 00:11:55,190
considered to be failing this is a it's

00:11:51,170 --> 00:11:58,209
a very nice intention but eventually it

00:11:55,190 --> 00:12:03,100
doesn't work many devices will fail

00:11:58,209 --> 00:12:06,560
without crossing certain thresholds or

00:12:03,100 --> 00:12:08,660
the it can be expected that the device

00:12:06,560 --> 00:12:13,149
will sell you will fail but it will not

00:12:08,660 --> 00:12:16,399
show it on the smart simple prediction

00:12:13,149 --> 00:12:21,140
so we decided to collect the Health

00:12:16,399 --> 00:12:24,350
metrics ourselves and to analyze it but

00:12:21,140 --> 00:12:28,520
that came with a few challenges so for

00:12:24,350 --> 00:12:32,779
example the interfaces of setup sauce

00:12:28,520 --> 00:12:35,930
and nvme they present different health

00:12:32,779 --> 00:12:39,860
metrics and of course it was the issue

00:12:35,930 --> 00:12:43,550
of the vendors again that they have

00:12:39,860 --> 00:12:47,089
their own so they don't have to include

00:12:43,550 --> 00:12:50,660
all of the attributes of smart so if you

00:12:47,089 --> 00:12:52,070
buy samsung device it will not

00:12:50,660 --> 00:12:58,089
necessarily have the exact same

00:12:52,070 --> 00:13:07,220
attribute such as IBM device or whatever

00:12:58,089 --> 00:13:11,560
and that that probably that actually put

00:13:07,220 --> 00:13:14,060
some more overhead to normalize the data

00:13:11,560 --> 00:13:17,930
and also they have different scales

00:13:14,060 --> 00:13:20,480
so they've one vendor a little decide on

00:13:17,930 --> 00:13:23,180
the scale of from 0 to 100 and another

00:13:20,480 --> 00:13:28,540
one would be set on a scale from 0 to

00:13:23,180 --> 00:13:28,540
255 so it's not really standardized

00:13:28,590 --> 00:13:38,350
we used smartphone tools smart control

00:13:33,330 --> 00:13:39,730
to collect today sound but if you guys

00:13:38,350 --> 00:13:42,580
have had the chance to work with that

00:13:39,730 --> 00:13:48,310
you know that the output is not the

00:13:42,580 --> 00:13:49,330
ideal for machines so you can see how it

00:13:48,310 --> 00:13:51,160
looks

00:13:49,330 --> 00:13:53,170
first of all it's really important to

00:13:51,160 --> 00:13:55,240
say that the smartphone tools community

00:13:53,170 --> 00:13:59,400
is awesome

00:13:55,240 --> 00:14:02,500
they are super robust they they give

00:13:59,400 --> 00:14:06,610
they support all the devices out there

00:14:02,500 --> 00:14:09,850
they react quickly they're doing a

00:14:06,610 --> 00:14:13,390
really great job however the output is

00:14:09,850 --> 00:14:15,460
aimed towards humans so you can see

00:14:13,390 --> 00:14:18,910
there are all sorts of pretty tables in

00:14:15,460 --> 00:14:23,970
here but if you want to get this data

00:14:18,910 --> 00:14:28,840
and transfer it to some sort of a model

00:14:23,970 --> 00:14:31,630
it's pretty challenging there are out

00:14:28,840 --> 00:14:35,530
there also some rappers that will take

00:14:31,630 --> 00:14:41,220
this data and will process it and parse

00:14:35,530 --> 00:14:45,220
it to sort some sorts of JSON but the

00:14:41,220 --> 00:14:48,160
thing is that they don't always take

00:14:45,220 --> 00:14:53,340
care of all the cases what happens if

00:14:48,160 --> 00:14:58,630
there's a new device out there it's not

00:14:53,340 --> 00:15:02,740
it's not ideal so we decided to do the

00:14:58,630 --> 00:15:10,080
right thing the first stage idea and

00:15:02,740 --> 00:15:16,840
contest smart phone tools for a building

00:15:10,080 --> 00:15:19,960
JSON format output so we prepared a

00:15:16,840 --> 00:15:22,560
patch that was part of outreaches a

00:15:19,960 --> 00:15:26,860
client application process which is

00:15:22,560 --> 00:15:29,059
amazing that it just froze you in the

00:15:26,860 --> 00:15:33,459
water and said okay

00:15:29,059 --> 00:15:38,959
two hands dirty and make a pledge for

00:15:33,459 --> 00:15:42,039
for that specific project so we

00:15:38,959 --> 00:15:46,699
contacted the maintainer of SmartMeters

00:15:42,039 --> 00:15:50,419
pristine Ponte he does a great job in

00:15:46,699 --> 00:15:55,569
responding and helping us we submitted a

00:15:50,419 --> 00:15:59,479
patch as you can guess this is the

00:15:55,569 --> 00:16:02,059
long-standing feature request for

00:15:59,479 --> 00:16:05,959
smartphone tools smart control

00:16:02,059 --> 00:16:08,629
specifically and the good news is that

00:16:05,959 --> 00:16:11,349
it's about to it's expected to be

00:16:08,629 --> 00:16:14,829
released by the end of the year which is

00:16:11,349 --> 00:16:18,349
just in time for the upcoming

00:16:14,829 --> 00:16:23,509
Stephanopoulos release February thank

00:16:18,349 --> 00:16:26,569
you and the second piece of this was

00:16:23,509 --> 00:16:28,759
that the way that Seth had been designed

00:16:26,569 --> 00:16:30,829
and built up until now it can run in any

00:16:28,759 --> 00:16:32,509
hardware but it really didn't deal with

00:16:30,829 --> 00:16:34,459
the details of the underlying devices

00:16:32,509 --> 00:16:35,899
and so we would pay attention to where

00:16:34,459 --> 00:16:37,489
all the ghosties are and what hosts

00:16:35,899 --> 00:16:39,259
there are but it didn't have the

00:16:37,489 --> 00:16:40,699
internal tracking to map that to

00:16:39,259 --> 00:16:42,049
physical devices and so we had to add a

00:16:40,699 --> 00:16:44,329
bunch of instruction to staff in order

00:16:42,049 --> 00:16:46,519
to maintain that metadata and allows to

00:16:44,329 --> 00:16:48,079
store it so the first challenge was

00:16:46,519 --> 00:16:50,179
figuring out how to actually identify a

00:16:48,079 --> 00:16:52,099
physical device it turns out that vendor

00:16:50,179 --> 00:16:54,379
model serial is a somewhat standard way

00:16:52,099 --> 00:16:56,319
to do that it's what the U dev and block

00:16:54,379 --> 00:16:58,519
ID libraries use and so we adopted that

00:16:56,319 --> 00:17:01,669
so we added this additional tracking

00:16:58,519 --> 00:17:03,349
into them this F cluster manager so that

00:17:01,669 --> 00:17:05,089
all the demons are reporting which

00:17:03,349 --> 00:17:06,439
devices they're mapped to we have the

00:17:05,089 --> 00:17:08,419
sort of many to many mapping between

00:17:06,439 --> 00:17:11,809
devices and demons so we know which

00:17:08,419 --> 00:17:13,490
devices depend on which and so on and we

00:17:11,809 --> 00:17:15,350
had added the ability to store a life

00:17:13,490 --> 00:17:18,860
expectancy property with those devices

00:17:15,350 --> 00:17:20,059
so that once we had a prediction we

00:17:18,860 --> 00:17:21,740
could tell the cluster what the life

00:17:20,059 --> 00:17:26,360
expectancy was and then it could respond

00:17:21,740 --> 00:17:28,339
as a result and then we adapted the

00:17:26,360 --> 00:17:30,350
initial prototypes from the cert III

00:17:28,339 --> 00:17:32,509
ending of the project to add a new

00:17:30,350 --> 00:17:34,120
module to set that would first of all

00:17:32,509 --> 00:17:35,410
implement a command

00:17:34,120 --> 00:17:37,000
the object storage demons which are

00:17:35,410 --> 00:17:38,800
actually storing data to scrape the

00:17:37,000 --> 00:17:40,620
spark metrics with smart control and

00:17:38,800 --> 00:17:43,030
pass that back to the central cluster

00:17:40,620 --> 00:17:44,230
the there was a sort of background

00:17:43,030 --> 00:17:45,970
operation that would scrape those on a

00:17:44,230 --> 00:17:48,220
daily basis and store them in a rate of

00:17:45,970 --> 00:17:49,450
spool and so that we had a recent

00:17:48,220 --> 00:17:52,120
history of all these metrics for other

00:17:49,450 --> 00:17:53,590
devices and we had sort of a

00:17:52,120 --> 00:17:55,630
self-contained metrics scraping and

00:17:53,590 --> 00:17:57,520
collection framework the question that

00:17:55,630 --> 00:17:59,260
we frequently got in this project from

00:17:57,520 --> 00:18:01,990
other people was why didn't we rely on

00:17:59,260 --> 00:18:03,670
other tools they are you know Prometheus

00:18:01,990 --> 00:18:04,960
plugins for example that's great metrics

00:18:03,670 --> 00:18:07,179
and there all sorts of external tools to

00:18:04,960 --> 00:18:08,320
do this and then and sort of the balance

00:18:07,179 --> 00:18:10,240
that we're trying to reach is to be able

00:18:08,320 --> 00:18:11,980
to make sure that every user who's using

00:18:10,240 --> 00:18:13,120
stuff can have something that works out

00:18:11,980 --> 00:18:16,870
of the box without having to install

00:18:13,120 --> 00:18:18,250
extra separate stuff but we also because

00:18:16,870 --> 00:18:19,660
we adopted this modular approach we

00:18:18,250 --> 00:18:20,980
still leave the door open so that if you

00:18:19,660 --> 00:18:22,840
already have an external scraping

00:18:20,980 --> 00:18:24,820
framework you can still use that and

00:18:22,840 --> 00:18:27,309
have some other infrastructure that's

00:18:24,820 --> 00:18:28,990
doing scraping prediction and it can

00:18:27,309 --> 00:18:30,250
still feed back into Ceph to tell stuff

00:18:28,990 --> 00:18:33,040
what the life expectancy of those

00:18:30,250 --> 00:18:34,870
devices are and then the same stuff sort

00:18:33,040 --> 00:18:36,730
of automated management logic can say oh

00:18:34,870 --> 00:18:40,000
I know this device is gonna fail and and

00:18:36,730 --> 00:18:41,830
do the right thing as a result which

00:18:40,000 --> 00:18:43,929
brings us to sort of the second phase of

00:18:41,830 --> 00:18:47,590
the architecture which is the failure

00:18:43,929 --> 00:18:49,300
prediction so today we have two

00:18:47,590 --> 00:18:51,220
approaches and two ways to address this

00:18:49,300 --> 00:18:53,679
problem and profit store contributed a

00:18:51,220 --> 00:18:55,780
pre change prediction model to the

00:18:53,679 --> 00:18:59,350
upstream open source project it's a

00:18:55,780 --> 00:19:00,610
bunch of SK learn library module files

00:18:59,350 --> 00:19:02,830
or something actually don't understand

00:19:00,610 --> 00:19:05,110
the today's science at all so it's a

00:19:02,830 --> 00:19:07,600
comparatively simple model but it works

00:19:05,110 --> 00:19:09,820
it runs inside the Ceph manager daemon

00:19:07,600 --> 00:19:11,950
and so you can sort of have an

00:19:09,820 --> 00:19:13,480
out-of-the-box cluster analyzing the

00:19:11,950 --> 00:19:16,450
metrics and doing predictions based on

00:19:13,480 --> 00:19:18,400
that there's also the ability to enable

00:19:16,450 --> 00:19:20,710
the feature where it will call out to an

00:19:18,400 --> 00:19:22,780
external sass API and some service

00:19:20,710 --> 00:19:24,670
either hosted in your data center in the

00:19:22,780 --> 00:19:26,590
cloud and that will feed the metrics to

00:19:24,670 --> 00:19:28,120
an external service and get a prediction

00:19:26,590 --> 00:19:29,830
result back and then store that in a

00:19:28,120 --> 00:19:32,590
cluster so you have both the sort of

00:19:29,830 --> 00:19:35,059
external cloud model or the the online

00:19:32,590 --> 00:19:37,070
model and again prophet stores

00:19:35,059 --> 00:19:39,380
goal is to have sort of a free service

00:19:37,070 --> 00:19:41,809
that you can use or you can pay them to

00:19:39,380 --> 00:19:43,940
get their very accurate predictions to

00:19:41,809 --> 00:19:46,010
do it and because we built this around

00:19:43,940 --> 00:19:47,570
sort of a SAS API there's an opportunity

00:19:46,010 --> 00:19:49,760
for other people to implement that same

00:19:47,570 --> 00:19:51,559
API or one similar and so that you're

00:19:49,760 --> 00:19:52,880
not profit store wouldn't be your only

00:19:51,559 --> 00:19:56,870
choice you can implement your own an

00:19:52,880 --> 00:19:59,360
external prediction service as well the

00:19:56,870 --> 00:20:01,280
natural sort of question that we ask is

00:19:59,360 --> 00:20:03,289
how can we build a better model so we

00:20:01,280 --> 00:20:03,950
have the initial model that that profits

00:20:03,289 --> 00:20:05,750
are donated

00:20:03,950 --> 00:20:07,850
that's the throat it was simple as I

00:20:05,750 --> 00:20:08,900
said but the goal in all this is to

00:20:07,850 --> 00:20:10,190
build you know the most accurate

00:20:08,900 --> 00:20:12,530
prediction models so we can have the

00:20:10,190 --> 00:20:14,090
best data reliability that we can I mean

00:20:12,530 --> 00:20:15,860
they're sort of - key pieces of this and

00:20:14,090 --> 00:20:17,990
the first is that we really need the

00:20:15,860 --> 00:20:19,070
disk failure data and so a lot of

00:20:17,990 --> 00:20:21,650
academic papers have been published

00:20:19,070 --> 00:20:23,720
about just failures and predicting them

00:20:21,650 --> 00:20:26,030
and they tend to rely with private

00:20:23,720 --> 00:20:27,679
datasets that the researchers you know

00:20:26,030 --> 00:20:29,690
got from Yahoo or Google or whatever

00:20:27,679 --> 00:20:31,700
from their big data centers and they do

00:20:29,690 --> 00:20:35,320
their analysis and they publish a paper

00:20:31,700 --> 00:20:37,909
but the data isn't public Backblaze is a

00:20:35,320 --> 00:20:39,320
cloud backup company that and it's very

00:20:37,909 --> 00:20:40,730
generous and that they publish all of

00:20:39,320 --> 00:20:44,150
their biller data and they have a huge

00:20:40,730 --> 00:20:45,740
slet of hard drives that they use and so

00:20:44,150 --> 00:20:46,789
that's that's really the only public

00:20:45,740 --> 00:20:48,440
data set that's out there

00:20:46,789 --> 00:20:50,450
I'm the challenge with sort of both of

00:20:48,440 --> 00:20:52,580
these is that the breadth of the device

00:20:50,450 --> 00:20:54,710
models is limited by what those

00:20:52,580 --> 00:20:57,650
particular cloud vendors or backways

00:20:54,710 --> 00:20:58,669
happens to buy if you look at sort of

00:20:57,650 --> 00:21:00,260
the enterprise world where we have

00:20:58,669 --> 00:21:01,880
companies like EMC and then F or

00:21:00,260 --> 00:21:03,559
whatever deploying their devices there

00:21:01,880 --> 00:21:04,820
of course gathering all the metrics for

00:21:03,559 --> 00:21:06,710
the devices that they buy for their

00:21:04,820 --> 00:21:08,720
customers but again that data set is

00:21:06,710 --> 00:21:10,220
private and so although those particular

00:21:08,720 --> 00:21:12,110
vendors might have failure prediction

00:21:10,220 --> 00:21:14,750
that they built in there's nothing

00:21:12,110 --> 00:21:15,950
really for the rest of us and so the

00:21:14,750 --> 00:21:17,780
bottom line is that we need we need

00:21:15,950 --> 00:21:19,730
failure data a more data in order to

00:21:17,780 --> 00:21:22,880
build build a better model and the other

00:21:19,730 --> 00:21:24,380
thing that's interesting is that there's

00:21:22,880 --> 00:21:25,940
an opportunity to use metrics that

00:21:24,380 --> 00:21:28,700
aren't actually necessarily from the

00:21:25,940 --> 00:21:30,320
device to enhance the quality of the

00:21:28,700 --> 00:21:31,730
prediction so profit source model for

00:21:30,320 --> 00:21:33,559
example looks not just at the device

00:21:31,730 --> 00:21:35,210
metrics that they get from smart but

00:21:33,559 --> 00:21:37,639
also like things like the server load

00:21:35,210 --> 00:21:39,559
the network traffic and

00:21:37,639 --> 00:21:40,999
processes are in the system all this

00:21:39,559 --> 00:21:42,440
other stuff that they scrape about the

00:21:40,999 --> 00:21:43,909
the cluster and the systems that are

00:21:42,440 --> 00:21:45,789
actually consuming the devices and they

00:21:43,909 --> 00:21:47,839
use that to generate a more accurate

00:21:45,789 --> 00:21:49,820
prediction of when things when things

00:21:47,839 --> 00:21:51,709
are gonna fail and so there's a question

00:21:49,820 --> 00:21:52,669
of which which metrics are the important

00:21:51,709 --> 00:21:54,499
ones and are there even other

00:21:52,669 --> 00:21:58,519
opportunities that that we haven't

00:21:54,499 --> 00:22:00,169
thought about and this led us to come up

00:21:58,519 --> 00:22:03,379
with this concept that what we really

00:22:00,169 --> 00:22:06,979
want is an open source public data set

00:22:03,379 --> 00:22:08,479
of disk failure data so that researchers

00:22:06,979 --> 00:22:10,279
and the open source community can build

00:22:08,479 --> 00:22:12,259
build a more accurate prediction model

00:22:10,279 --> 00:22:13,639
and the question is what can we do to

00:22:12,259 --> 00:22:16,070
help help make this happen

00:22:13,639 --> 00:22:18,499
that the concept that we came up with is

00:22:16,070 --> 00:22:20,629
a SAS like service not unlike what

00:22:18,499 --> 00:22:22,399
professore is doing but one that's run

00:22:20,629 --> 00:22:26,869
for the community and a open and

00:22:22,399 --> 00:22:29,419
transparent way where you have systems

00:22:26,869 --> 00:22:31,549
that are basically sharing their device

00:22:29,419 --> 00:22:33,229
health metrics with this public data set

00:22:31,549 --> 00:22:35,419
service so they're publishing their

00:22:33,229 --> 00:22:37,879
smart metrics and in response they get a

00:22:35,419 --> 00:22:40,549
disk failure prediction so it's sort of

00:22:37,879 --> 00:22:42,379
using them providing accurate hopefully

00:22:40,549 --> 00:22:44,239
failure prediction as a carrot in order

00:22:42,379 --> 00:22:48,259
to motivate people to share their data

00:22:44,239 --> 00:22:49,909
and people are obviously and naturally

00:22:48,259 --> 00:22:51,619
skeptical of any instance where you're

00:22:49,909 --> 00:22:53,809
sort of sharing data about your internal

00:22:51,619 --> 00:22:55,339
systems so it's it's very important that

00:22:53,809 --> 00:22:57,559
you make the system transparent and

00:22:55,339 --> 00:22:59,779
protect the privacy and so for example

00:22:57,559 --> 00:23:01,909
uniquely identifying devices with

00:22:59,779 --> 00:23:03,499
randomly generating IDs and hosts

00:23:01,909 --> 00:23:04,789
instead of having without having any

00:23:03,499 --> 00:23:07,999
identifying information like IP

00:23:04,789 --> 00:23:09,619
addresses log and so forth there's sort

00:23:07,999 --> 00:23:11,299
of a question around whether you would

00:23:09,619 --> 00:23:12,979
want to share your serial number that's

00:23:11,299 --> 00:23:14,659
sort of the only really identifying

00:23:12,979 --> 00:23:15,909
information and within the device

00:23:14,659 --> 00:23:17,959
metadata

00:23:15,909 --> 00:23:19,549
Venessa trade off because if you do have

00:23:17,959 --> 00:23:21,259
that you can identify things like you

00:23:19,549 --> 00:23:23,419
know bad batches of devices that come

00:23:21,259 --> 00:23:26,209
off the manufacturing line I might

00:23:23,419 --> 00:23:27,769
correlate with failures but people might

00:23:26,209 --> 00:23:29,959
be more paranoid about it knowing that

00:23:27,769 --> 00:23:32,239
they might have that particular batch so

00:23:29,959 --> 00:23:33,499
hopefully the goal is to motivate people

00:23:32,239 --> 00:23:35,059
to share as much information as possible

00:23:33,499 --> 00:23:37,519
because you get a more accurate more

00:23:35,059 --> 00:23:39,320
accurate prediction and then the result

00:23:37,519 --> 00:23:40,820
would be that you would accumulate this

00:23:39,320 --> 00:23:42,049
big database and they'll generate this

00:23:40,820 --> 00:23:43,549
data set and then share that with

00:23:42,049 --> 00:23:44,779
academic researchers and the open source

00:23:43,549 --> 00:23:46,639
community and people who are trying to

00:23:44,779 --> 00:23:47,520
build better failure models so you sort

00:23:46,639 --> 00:23:50,430
of bypass the

00:23:47,520 --> 00:23:51,180
they have currently where there just is

00:23:50,430 --> 00:23:53,850
no good data

00:23:51,180 --> 00:23:55,350
I'm deterring these things against one

00:23:53,850 --> 00:23:58,560
of the key challenges with this that

00:23:55,350 --> 00:23:59,730
we've identified is identifying the

00:23:58,560 --> 00:24:00,960
failure events because when you're

00:23:59,730 --> 00:24:02,420
training a model you need to know what

00:24:00,960 --> 00:24:04,800
the signal is that you're actually

00:24:02,420 --> 00:24:06,750
trying to predict when did the device

00:24:04,800 --> 00:24:09,300
fail after all the hell of these metrics

00:24:06,750 --> 00:24:12,480
or not and part of that is a definition

00:24:09,300 --> 00:24:14,040
as arriving at whether the definition of

00:24:12,480 --> 00:24:15,600
that actual device failure is and

00:24:14,040 --> 00:24:18,150
because that might never vary between

00:24:15,600 --> 00:24:19,560
different users so is it when the device

00:24:18,150 --> 00:24:21,750
is completely offline and it won't even

00:24:19,560 --> 00:24:23,160
spin up and is it when you have too many

00:24:21,750 --> 00:24:24,270
read errors and you finally decide that

00:24:23,160 --> 00:24:26,010
you're not going to use it anymore is it

00:24:24,270 --> 00:24:27,870
when you get a single read error and you

00:24:26,010 --> 00:24:29,040
decide not to use it different

00:24:27,870 --> 00:24:30,210
environments have sort of different

00:24:29,040 --> 00:24:32,400
thresholds for when they say I'm gonna

00:24:30,210 --> 00:24:36,000
stop using that device and it's it's no

00:24:32,400 --> 00:24:38,040
longer acceptable and the other thing is

00:24:36,000 --> 00:24:40,140
that when a device fails in the real

00:24:38,040 --> 00:24:41,910
world if you imagine in the wild

00:24:40,140 --> 00:24:43,380
somebody might be running any system

00:24:41,910 --> 00:24:46,200
whether it's staff or something else a

00:24:43,380 --> 00:24:47,160
device fails there's some actually not

00:24:46,200 --> 00:24:49,140
they're gonna take you know they're

00:24:47,160 --> 00:24:50,700
gonna their their their raid array might

00:24:49,140 --> 00:24:52,280
use a spare they might replace it is

00:24:50,700 --> 00:24:54,780
they might just leave it failed in place

00:24:52,280 --> 00:24:56,130
and lots of different software stacks

00:24:54,780 --> 00:24:58,440
and human interventions might be

00:24:56,130 --> 00:25:01,050
involved in that and it's I think it's

00:24:58,440 --> 00:25:02,910
unrealistic to say to require that those

00:25:01,050 --> 00:25:05,100
users like take an additional step of

00:25:02,910 --> 00:25:06,300
notifying this cloud service that

00:25:05,100 --> 00:25:07,890
they're sharing their data with that oh

00:25:06,300 --> 00:25:11,240
by the way I decided that this device

00:25:07,890 --> 00:25:13,530
failed is it's if it's not automatic

00:25:11,240 --> 00:25:15,030
then they're not gonna they're not gonna

00:25:13,530 --> 00:25:17,280
share that information so it's hard for

00:25:15,030 --> 00:25:18,810
this public service to sort of get that

00:25:17,280 --> 00:25:22,140
signal know when when the device

00:25:18,810 --> 00:25:24,330
actually failed and the other thing that

00:25:22,140 --> 00:25:25,530
you have to be careful with is if your

00:25:24,330 --> 00:25:27,450
failure prediction is working really

00:25:25,530 --> 00:25:28,950
well then the devices won't actually

00:25:27,450 --> 00:25:30,390
feel right you'll you'll take them out

00:25:28,950 --> 00:25:33,210
of service before they actually crash

00:25:30,390 --> 00:25:35,270
and burn and then you have to be careful

00:25:33,210 --> 00:25:37,500
about making sure that that's not

00:25:35,270 --> 00:25:40,230
polluting the model as it it didn't used

00:25:37,500 --> 00:25:41,540
to be refined and trained and so one

00:25:40,230 --> 00:25:45,350
idea of how to

00:25:41,540 --> 00:25:47,179
is to how to infer failures is to

00:25:45,350 --> 00:25:48,590
associate the device with the hosts that

00:25:47,179 --> 00:25:50,090
contain them so typically you have a

00:25:48,590 --> 00:25:52,160
server that has multiple hard disks in

00:25:50,090 --> 00:25:54,230
it and as long as you have a unique

00:25:52,160 --> 00:25:56,419
identifier for the server then the

00:25:54,230 --> 00:25:57,830
server's can see that there are some

00:25:56,419 --> 00:26:01,370
number of devices that are associated

00:25:57,830 --> 00:26:02,960
with the particular host and over time

00:26:01,370 --> 00:26:05,720
you're going to be receiving a stream of

00:26:02,960 --> 00:26:07,250
metric updates for all those devices but

00:26:05,720 --> 00:26:08,419
then after a failure presumably just

00:26:07,250 --> 00:26:09,440
that one device you're gonna stop

00:26:08,419 --> 00:26:11,510
getting reports for it

00:26:09,440 --> 00:26:14,210
so the idea is to basically infer that a

00:26:11,510 --> 00:26:15,679
device failed if you continue receiving

00:26:14,210 --> 00:26:17,179
reports for all the other devices in the

00:26:15,679 --> 00:26:19,880
system but that one particular device

00:26:17,179 --> 00:26:22,040
failed and perhaps you only do that if

00:26:19,880 --> 00:26:24,559
you see signs that the device is likely

00:26:22,040 --> 00:26:26,690
to fail and then it goes away then you

00:26:24,559 --> 00:26:28,760
can sort of assume that it went away

00:26:26,690 --> 00:26:30,890
because it actually failed or because it

00:26:28,760 --> 00:26:32,540
was taken out of service and I think the

00:26:30,890 --> 00:26:34,490
real question here is as a data science

00:26:32,540 --> 00:26:36,830
question is it is there a sufficient

00:26:34,490 --> 00:26:38,330
signal using that kind of inference in

00:26:36,830 --> 00:26:41,929
order to train an accurate dealer

00:26:38,330 --> 00:26:43,730
prediction model and probably the more

00:26:41,929 --> 00:26:45,770
like specific test that would need to be

00:26:43,730 --> 00:26:47,480
taken would be to validate that type of

00:26:45,770 --> 00:26:49,760
approach where you're you're inferring

00:26:47,480 --> 00:26:51,559
those those data points from an existing

00:26:49,760 --> 00:26:53,510
data set using like the big back place

00:26:51,559 --> 00:26:55,520
data set which actually does have

00:26:53,510 --> 00:26:57,020
failure events because they took that

00:26:55,520 --> 00:27:00,080
time to actually say this device failed

00:26:57,020 --> 00:27:02,090
but to ignore that and try to infer

00:27:00,080 --> 00:27:03,350
failures using this method and then see

00:27:02,090 --> 00:27:05,360
if you can still train an accurate model

00:27:03,350 --> 00:27:07,190
in order to validate whether this in

00:27:05,360 --> 00:27:08,419
fact would work so that's a that's a

00:27:07,190 --> 00:27:12,559
question for a data scientist that

00:27:08,419 --> 00:27:15,420
hopefully someone will will pick up so

00:27:12,559 --> 00:27:21,780
that brings us to the

00:27:15,420 --> 00:27:21,780
sorry that brings us to the third thing

00:27:22,650 --> 00:27:37,540
okay so we had metrics collection we had

00:27:33,430 --> 00:27:41,860
the prediction model and now we we have

00:27:37,540 --> 00:27:44,800
the response phase so it's a question

00:27:41,860 --> 00:27:48,160
what what do we do when one of these

00:27:44,800 --> 00:27:53,740
keys about to fail so the question is

00:27:48,160 --> 00:27:56,740
how much time we have left if we have

00:27:53,740 --> 00:28:00,010
enough time maybe we can just let the

00:27:56,740 --> 00:28:02,050
Ceph operator know that one of the

00:28:00,010 --> 00:28:06,700
devices is about to fail and then they

00:28:02,050 --> 00:28:09,460
can do whatever they decide but if we

00:28:06,700 --> 00:28:14,020
don't have enough time we would like the

00:28:09,460 --> 00:28:17,620
cluster to automatically try to sell

00:28:14,020 --> 00:28:20,860
skilled like I mentioned that self is

00:28:17,620 --> 00:28:27,010
self managing and can be self healing as

00:28:20,860 --> 00:28:31,140
well so what we do is we will mark these

00:28:27,010 --> 00:28:39,040
all these out of the cluster and we will

00:28:31,140 --> 00:28:43,660
migrate that data to other devices yeah

00:28:39,040 --> 00:28:52,450
we also divert the workload away from

00:28:43,660 --> 00:28:59,730
these devices so so yeah we will not

00:28:52,450 --> 00:29:03,730
we're not causing more harm all of these

00:28:59,730 --> 00:29:09,190
so we define the thresholds to do

00:29:03,730 --> 00:29:14,500
the actions so they're all configurable

00:29:09,190 --> 00:29:18,130
I think the default now it's in case a

00:29:14,500 --> 00:29:20,680
device is about to fail less than two

00:29:18,130 --> 00:29:24,880
weeks from now we just automatically

00:29:20,680 --> 00:29:28,000
take action but other than that it's

00:29:24,880 --> 00:29:28,380
like life expectancy is greater than two

00:29:28,000 --> 00:29:33,130
weeks

00:29:28,380 --> 00:29:37,090
the self operator can can decide what to

00:29:33,130 --> 00:29:40,690
do that yeah and then there is a

00:29:37,090 --> 00:29:46,000
question after the device is

00:29:40,690 --> 00:29:46,530
successfully offloaded but why do we do

00:29:46,000 --> 00:29:52,000
that

00:29:46,530 --> 00:29:57,840
should should we drive it to failure to

00:29:52,000 --> 00:29:57,840
prove the model was right or wrong

00:29:58,380 --> 00:30:11,350
yeah some open questions so currently we

00:30:06,010 --> 00:30:16,000
have we have merged the for the persons

00:30:11,350 --> 00:30:19,510
of the device management and the metrics

00:30:16,000 --> 00:30:23,380
collections and respond they automate

00:30:19,510 --> 00:30:27,460
automatic response we still have under

00:30:23,380 --> 00:30:31,290
review the huge pull request of profit

00:30:27,460 --> 00:30:34,980
store hopefully it will be merged soon

00:30:31,290 --> 00:30:41,610
and we're targeting this feature for

00:30:34,980 --> 00:30:47,140
February 19 next next release of Ceph

00:30:41,610 --> 00:30:52,390
and in the future we wish to see an

00:30:47,140 --> 00:30:57,310
orphan SAS service that in an open data

00:30:52,390 --> 00:31:00,010
set just like sage mentioned to have an

00:30:57,310 --> 00:31:04,350
improved free and open source prediction

00:31:00,010 --> 00:31:06,559
model so this is a call for academia for

00:31:04,350 --> 00:31:08,990
professionals for everyone who is

00:31:06,559 --> 00:31:18,139
uh interested in this store and in this

00:31:08,990 --> 00:31:22,159
project everybody can help so this

00:31:18,139 --> 00:31:23,840
project happened in collaboration with

00:31:22,159 --> 00:31:28,730
our treaty organization like we

00:31:23,840 --> 00:31:32,269
mentioned earlier which offers paid

00:31:28,730 --> 00:31:39,619
internships with that promotes diversity

00:31:32,269 --> 00:31:42,200
in fair open source software there are

00:31:39,619 --> 00:31:45,470
many communities which are participating

00:31:42,200 --> 00:31:49,309
this is just a partial list as you can

00:31:45,470 --> 00:31:54,019
see the white works that is that

00:31:49,309 --> 00:31:56,990
applicants look at the projects that are

00:31:54,019 --> 00:31:59,179
participating they pick a project with

00:31:56,990 --> 00:32:01,730
they're very passionate about they

00:31:59,179 --> 00:32:05,119
contact the mentors they see which

00:32:01,730 --> 00:32:08,779
contributions can be made to this to

00:32:05,119 --> 00:32:13,879
that project and again this is hands-on

00:32:08,779 --> 00:32:21,399
so it's not just studying theoretical

00:32:13,879 --> 00:32:26,929
stuff it's just it's real and many times

00:32:21,399 --> 00:32:29,029
there is a certain well maybe a

00:32:26,929 --> 00:32:32,210
threshold you need to be the right board

00:32:29,029 --> 00:32:34,580
here that prevents people to contribute

00:32:32,210 --> 00:32:35,470
to open source because it can be

00:32:34,580 --> 00:32:41,059
intimidating

00:32:35,470 --> 00:32:45,529
so we with this organization you get the

00:32:41,059 --> 00:32:47,720
support of mentors which is fantastic

00:32:45,529 --> 00:32:50,029
the contribution does not have to be

00:32:47,720 --> 00:32:55,279
just developed for developers you can

00:32:50,029 --> 00:32:59,850
contribute for the communications but

00:32:55,279 --> 00:33:01,440
fixes marketing so

00:32:59,850 --> 00:33:04,530
there are a lot of a lot of ways to

00:33:01,440 --> 00:33:10,640
contribute yeah and then you fill out an

00:33:04,530 --> 00:33:14,220
application form object you made many

00:33:10,640 --> 00:33:17,970
projects happen many many contributions

00:33:14,220 --> 00:33:20,190
to open source and that's that's a

00:33:17,970 --> 00:33:23,750
win-win for everybody because the

00:33:20,190 --> 00:33:26,610
interns get experience the interns get

00:33:23,750 --> 00:33:33,300
again the support that is not trivial

00:33:26,610 --> 00:33:35,520
and also mentors potential mentors if

00:33:33,300 --> 00:33:39,660
you have any project that you think

00:33:35,520 --> 00:33:43,550
would be good for newbies in open source

00:33:39,660 --> 00:33:49,310
I encourage you to do that because

00:33:43,550 --> 00:33:49,310
eventually it makes the project happen

00:33:49,700 --> 00:33:57,410
the internships run twice a year a year

00:33:53,010 --> 00:34:00,390
sorry next one starts December and

00:33:57,410 --> 00:34:04,110
applications start they open September

00:34:00,390 --> 00:34:06,200
10 you can see the website I already

00:34:04,110 --> 00:34:12,210
encourage you to go and check it out

00:34:06,200 --> 00:34:16,680
tell everyone about it on a personal

00:34:12,210 --> 00:34:24,510
note I'll share my experience with the

00:34:16,680 --> 00:34:27,330
project so so fitting the project I was

00:34:24,510 --> 00:34:32,670
passionate about I was making the

00:34:27,330 --> 00:34:39,140
contribution with the sage guidance and

00:34:32,670 --> 00:34:42,570
then once it starts a very common

00:34:39,140 --> 00:34:48,300
syndrome for interns is to have the

00:34:42,570 --> 00:34:51,540
impostor syndrome so there is also do

00:34:48,300 --> 00:34:53,500
you realize oh my god maybe I'm not the

00:34:51,540 --> 00:34:57,430
right person to do that

00:34:53,500 --> 00:35:00,490
that the code base is huge where do I

00:34:57,430 --> 00:35:02,680
write the next line of code you're kind

00:35:00,490 --> 00:35:07,990
of freaked out you have some fear and

00:35:02,680 --> 00:35:09,910
out but then yeah we all know that but

00:35:07,990 --> 00:35:14,200
we tend to forget nobody knows

00:35:09,910 --> 00:35:17,670
everything we all learn all the time and

00:35:14,200 --> 00:35:24,869
it's just it's it's good to remember it

00:35:17,670 --> 00:35:27,790
and Seth's community is it's great there

00:35:24,869 --> 00:35:30,430
sincerely happy to help

00:35:27,790 --> 00:35:34,290
they don't criticize you they know that

00:35:30,430 --> 00:35:39,609
it's okay nobody knows it nobody bigger

00:35:34,290 --> 00:35:45,130
not everybody knows the project and at

00:35:39,609 --> 00:35:47,680
the very first stages stage told me that

00:35:45,130 --> 00:35:49,830
there isn't such a thing as a silly

00:35:47,680 --> 00:35:49,830
question

00:35:49,859 --> 00:35:59,589
yeah just think about it sometimes we

00:35:54,369 --> 00:36:02,740
are afraid to ask so yeah so first of

00:35:59,589 --> 00:36:07,020
all I would like to thank outreaches

00:36:02,740 --> 00:36:13,020
organizers States sharp marina and Karen

00:36:07,020 --> 00:36:15,670
and all the team who were very attentive

00:36:13,020 --> 00:36:18,280
responded to everything super quickly

00:36:15,670 --> 00:36:21,930
and really wanted to help all the

00:36:18,280 --> 00:36:26,710
mentors that took part in this project

00:36:21,930 --> 00:36:30,900
and of course stage kids thank you for

00:36:26,710 --> 00:36:36,690
all your patience and all your

00:36:30,900 --> 00:36:36,690
wheelchair health thank you

00:36:37,200 --> 00:36:47,849
so the challenges that with just a quick

00:36:43,599 --> 00:36:55,180
cap for the project so we still have

00:36:47,849 --> 00:36:57,119
smart controls upstream we hope that we

00:36:55,180 --> 00:37:02,890
hope that we're gonna see that release

00:36:57,119 --> 00:37:11,630
around the end of the year we still have

00:37:02,890 --> 00:37:14,430
some changes today architecture okay

00:37:11,630 --> 00:37:18,670
[Music]

00:37:14,430 --> 00:37:21,839
okay yeah I mean we still don't have the

00:37:18,670 --> 00:37:23,979
data to to build our own model hopefully

00:37:21,839 --> 00:37:26,650
we'll they'll have some open data as

00:37:23,979 --> 00:37:29,109
soon and the outcomes that we had from

00:37:26,650 --> 00:37:32,950
the project is we do have a modular

00:37:29,109 --> 00:37:36,640
approach for the metrics collection the

00:37:32,950 --> 00:37:38,349
prediction model and the response we

00:37:36,640 --> 00:37:40,739
have a new participant in the staff

00:37:38,349 --> 00:37:44,249
community profit store were very

00:37:40,739 --> 00:37:44,249
thrilled about this project

00:37:44,279 --> 00:37:54,910
yeah okay sorry we

00:37:50,640 --> 00:37:56,559
what yeah and that's it um thanks to the

00:37:54,910 --> 00:37:58,480
smart modules up stream Christian Franco

00:37:56,559 --> 00:38:00,819
has been great to profit store for

00:37:58,480 --> 00:38:03,339
contributing and that ratio of course

00:38:00,819 --> 00:38:05,380
for setting up the internship so we have

00:38:03,339 --> 00:38:10,109
a kind of at a time but if you have any

00:38:05,380 --> 00:38:10,109
questions please please find us and ask

00:38:17,279 --> 00:38:21,970
so the last time I did you this in

00:38:19,539 --> 00:38:23,440
production was a couple of years ago but

00:38:21,970 --> 00:38:26,849
one of the things that I found was that

00:38:23,440 --> 00:38:30,220
the data supplied by solid state storage

00:38:26,849 --> 00:38:32,220
was you know both completely

00:38:30,220 --> 00:38:37,930
inconsistent between different vendors

00:38:32,220 --> 00:38:39,789
and often extremely sparse has that

00:38:37,930 --> 00:38:50,249
improved it all in the last couple of

00:38:39,789 --> 00:38:50,249
years it's it's quite complex I mean

00:38:53,580 --> 00:39:01,920
we were focusing on collecting the day

00:38:58,180 --> 00:39:01,920
well Brooklyn here the data from

00:39:02,940 --> 00:39:06,820
potentially all the devices but what

00:39:05,020 --> 00:39:13,540
with at the beginning we were focusing

00:39:06,820 --> 00:39:22,300
on seta for hard drive so it did they do

00:39:13,540 --> 00:39:29,130
it oh sorry so I'm not sure if it's if

00:39:22,300 --> 00:39:31,060
it has improved yeah I think mostly were

00:39:29,130 --> 00:39:32,350
relying on smart control to scrape

00:39:31,060 --> 00:39:35,500
everything because they've been pretty

00:39:32,350 --> 00:39:36,970
thorough and I think that as far as what

00:39:35,500 --> 00:39:38,620
data if we're actually getting out and

00:39:36,970 --> 00:39:40,630
the ability to actually predict on that

00:39:38,620 --> 00:39:41,710
is sort of the self contained prediction

00:39:40,630 --> 00:39:43,420
problem at least that's the way I've

00:39:41,710 --> 00:39:45,730
been viewing it and so we're starting

00:39:43,420 --> 00:39:48,660
with something and the hope is that the

00:39:45,730 --> 00:39:48,660
situation will improve over time

00:39:55,930 --> 00:40:01,180
I was wondering how you evaluate the

00:39:58,440 --> 00:40:03,029
accuracy of the prophets Torah

00:40:01,180 --> 00:40:06,819
predictions

00:40:03,029 --> 00:40:09,279
the truth is that we haven't yet they've

00:40:06,819 --> 00:40:13,109
provided a model and we don't we haven't

00:40:09,279 --> 00:40:15,279
take at the time to evaluate it yet so

00:40:13,109 --> 00:40:18,480
we're focusing just on that on the

00:40:15,279 --> 00:40:18,480
system integration problem first

00:40:20,470 --> 00:40:25,000
I guess I had a similar questions so

00:40:22,780 --> 00:40:28,359
about the model right it says like it's

00:40:25,000 --> 00:40:30,430
95 percent 97 percent accurate but it

00:40:28,359 --> 00:40:32,260
doesn't tell a lot about like a more

00:40:30,430 --> 00:40:33,430
better metric would be what's the false

00:40:32,260 --> 00:40:35,349
positive rate and what's a false

00:40:33,430 --> 00:40:37,510
negative rate because if it's a false

00:40:35,349 --> 00:40:39,550
positive then there's a bunch of things

00:40:37,510 --> 00:40:41,800
that you guys do to make sure that there

00:40:39,550 --> 00:40:44,319
are no problems right so how good is the

00:40:41,800 --> 00:40:45,579
model ad in terms of that and how much

00:40:44,319 --> 00:40:48,880
do you care about the false positives

00:40:45,579 --> 00:40:50,859
and yeah that's exactly right there is a

00:40:48,880 --> 00:40:53,230
talk that I saw at Wallops last year

00:40:50,859 --> 00:40:54,339
that and defined it I mean forgetting

00:40:53,230 --> 00:40:56,109
the technical term but if the

00:40:54,339 --> 00:40:58,900
2-dimensional matrix or essentially have

00:40:56,109 --> 00:41:02,290
the probability of positive or negative

00:40:58,900 --> 00:41:04,030
also negatives that's the number that

00:41:02,290 --> 00:41:06,130
they they've given us and again we

00:41:04,030 --> 00:41:09,730
haven't actually analyzed it from a data

00:41:06,130 --> 00:41:11,640
science perspective so I'm clear

00:41:09,730 --> 00:41:15,030
[Music]

00:41:11,640 --> 00:41:16,640
okay thank you saison yeah yeah well we

00:41:15,030 --> 00:41:19,890
have one more question

00:41:16,640 --> 00:41:22,530
so basically about if I were to try

00:41:19,890 --> 00:41:25,170
something out like this what I need to

00:41:22,530 --> 00:41:30,120
set up an agent to collect these logs

00:41:25,170 --> 00:41:33,210
and push it somewhere we're talking

00:41:30,120 --> 00:41:35,040
about the open data set yes or yeah so

00:41:33,210 --> 00:41:36,810
they did I think they're sort of two

00:41:35,040 --> 00:41:38,850
goals one is to make it work for Seth

00:41:36,810 --> 00:41:40,830
out of the box and if we have a public

00:41:38,850 --> 00:41:43,470
data set target you could just turn it

00:41:40,830 --> 00:41:44,910
on but the expectation is that novels

00:41:43,470 --> 00:41:46,380
are just stuff obviously applies whether

00:41:44,910 --> 00:41:48,870
stored in your data center so we'd want

00:41:46,380 --> 00:41:50,760
to build an agent they just install any

00:41:48,870 --> 00:41:52,440
post and turn it on and point it at

00:41:50,760 --> 00:41:55,490
upstream and it would just it would

00:41:52,440 --> 00:41:55,490
share that data

00:42:00,350 --> 00:42:07,550
Thank You Yared Thank You sage we have

00:42:03,800 --> 00:42:12,700
lunch server to exist in lounge we will

00:42:07,550 --> 00:42:12,700
be resuming session at 1:30 thank you

00:42:14,530 --> 00:42:17,789

YouTube URL: https://www.youtube.com/watch?v=iXeH9iE2zU0


