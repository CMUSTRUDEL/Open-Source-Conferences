Title: Building AI with Ceph and OpenShift
Publication date: 2019-02-21
Playlist: DevConfUS 2018
Description: 
	Artificial Intelligence has become quite the hot topic lately. Companies large and small are scrambling to figure out how they can build products that leverage it. In a very fast paced market where new tools are being introduced for data scientists seemingly every week, one thing hasnâ€™t changed: the need for LOTS of quality data.

In this session, we will discuss how to support AI data scientists by building a data lake and machine learning platform in OpenShift using a few common open source products. Ceph will be deployed as the backbone of the data lake. We will show how to use Spark and OpenWhisk in OpenShift to create a pipeline for processing data. We will also enable data scientists to create and execute AI models by connecting all of these technologies in a Jupyter notebook.
Captions: 
	00:00:02,600 --> 00:00:09,379
looking to us about building yeah with

00:00:05,750 --> 00:00:10,929
Saif and OpenShift thank you thanks

00:00:09,379 --> 00:00:16,900
everyone

00:00:10,929 --> 00:00:19,880
so Saturday morning thanks for you know

00:00:16,900 --> 00:00:22,160
not watching cartoons or Netflix or

00:00:19,880 --> 00:00:24,590
anything coming out here to see us speak

00:00:22,160 --> 00:00:26,419
today I'm gonna talk to you about what

00:00:24,590 --> 00:00:28,489
we've been doing in the AI center of

00:00:26,419 --> 00:00:31,009
excellence at Red Hat you guys have seen

00:00:28,489 --> 00:00:32,780
a number of us talked before about our

00:00:31,009 --> 00:00:35,390
strategies with AI and machine learning

00:00:32,780 --> 00:00:38,600
you've seen bas Jack and Steven talk

00:00:35,390 --> 00:00:41,060
about Jupiter hub and set and spark as

00:00:38,600 --> 00:00:43,070
well I'm going to expand on that and

00:00:41,060 --> 00:00:44,990
introduce a concept of open wisk where

00:00:43,070 --> 00:00:47,090
we have serverless actions so you'll see

00:00:44,990 --> 00:00:48,890
a little bit about how we chain all of

00:00:47,090 --> 00:00:53,540
those technologies together to get a

00:00:48,890 --> 00:00:55,670
nice machine learning pipeline first off

00:00:53,540 --> 00:00:57,829
to do some level setting and ground

00:00:55,670 --> 00:01:00,829
rules everyone is familiar about the

00:00:57,829 --> 00:01:03,350
concept of machine learning and AI this

00:01:00,829 --> 00:01:05,930
is to just show an example of a typical

00:01:03,350 --> 00:01:08,330
workflow it starts off with of course

00:01:05,930 --> 00:01:11,210
Daniel mentioned this before the data in

00:01:08,330 --> 00:01:14,330
this case we have a data set live a

00:01:11,210 --> 00:01:17,120
library of data sets and from there we

00:01:14,330 --> 00:01:19,820
want to start to develop a model that's

00:01:17,120 --> 00:01:22,340
where a lot of the brains and the

00:01:19,820 --> 00:01:23,900
intelligence comes into analyzing the

00:01:22,340 --> 00:01:26,540
data and understanding what you want to

00:01:23,900 --> 00:01:28,490
do trial and error from that developing

00:01:26,540 --> 00:01:30,590
of the model you then go into the

00:01:28,490 --> 00:01:33,110
exercise of tuning the model and

00:01:30,590 --> 00:01:35,630
training and using the data that you

00:01:33,110 --> 00:01:38,450
have to then train it once you do that

00:01:35,630 --> 00:01:40,040
of course you know the big excitement is

00:01:38,450 --> 00:01:41,840
after you've trained it and developed

00:01:40,040 --> 00:01:44,360
your model you get to deploy it and

00:01:41,840 --> 00:01:46,790
actually see some data coming in so for

00:01:44,360 --> 00:01:49,220
today's session I'll walk through an

00:01:46,790 --> 00:01:50,600
example of all of those types of steps

00:01:49,220 --> 00:01:52,549
that would go through in a typical

00:01:50,600 --> 00:01:55,850
process so you can see how we can use

00:01:52,549 --> 00:01:59,900
tools in an open shipped world in order

00:01:55,850 --> 00:02:02,540
to achieve that workflow the first part

00:01:59,900 --> 00:02:05,409
of this as we mentioned before was the

00:02:02,540 --> 00:02:07,930
the open ships framework as

00:02:05,409 --> 00:02:10,240
as we start out how do we do this in a

00:02:07,930 --> 00:02:12,610
containerized world well in this case

00:02:10,240 --> 00:02:15,700
we're using open shift open shift as you

00:02:12,610 --> 00:02:18,610
all know by now is a container platform

00:02:15,700 --> 00:02:22,330
that it uses certified kubernetes

00:02:18,610 --> 00:02:24,340
enterprise kubernetes and it also allows

00:02:22,330 --> 00:02:26,010
you to do more hybrid cloud type of

00:02:24,340 --> 00:02:30,310
things you may have some of your

00:02:26,010 --> 00:02:32,709
infrastructure in Google AWS Azure you

00:02:30,310 --> 00:02:34,810
may also have it on Prem open ship gives

00:02:32,709 --> 00:02:39,519
you that ability to seamlessly manage

00:02:34,810 --> 00:02:42,370
all of that in one ecosystem once we

00:02:39,519 --> 00:02:44,950
have our containerized world we need a

00:02:42,370 --> 00:02:47,319
place to store the data for this example

00:02:44,950 --> 00:02:50,260
we'll be using set you can also use

00:02:47,319 --> 00:02:53,170
other technologies the reasoning behind

00:02:50,260 --> 00:02:56,650
step is really because of the growing

00:02:53,170 --> 00:02:58,690
need to be able to separate your data

00:02:56,650 --> 00:03:00,610
from where your performance and your

00:02:58,690 --> 00:03:02,080
compute actually happens and that was

00:03:00,610 --> 00:03:04,630
one of those architectures that came

00:03:02,080 --> 00:03:06,549
from some of the Amazon maybes a lot of

00:03:04,630 --> 00:03:09,190
you may be familiar with Amazon's

00:03:06,549 --> 00:03:11,560
infrastructure you had a lot of the EMR

00:03:09,190 --> 00:03:14,110
type of scenarios we'd have elastic

00:03:11,560 --> 00:03:16,510
MapReduce and Hadoop ecosystem and then

00:03:14,110 --> 00:03:18,910
an environment allows you to spin up a

00:03:16,510 --> 00:03:21,220
Hadoop cluster process your data really

00:03:18,910 --> 00:03:22,540
quickly and then tear down the cluster

00:03:21,220 --> 00:03:24,940
and not have to worry about maintaining

00:03:22,540 --> 00:03:26,470
it obviously if you tend on the cluster

00:03:24,940 --> 00:03:30,150
you have to make sure the storage is

00:03:26,470 --> 00:03:33,519
still there and it's object stores like

00:03:30,150 --> 00:03:36,370
s3 and stuff came about we use set

00:03:33,519 --> 00:03:37,859
because it has s3 capabilities and we're

00:03:36,370 --> 00:03:40,690
able to leverage a lot of the

00:03:37,859 --> 00:03:44,739
technologies that are already built on

00:03:40,690 --> 00:03:46,329
top of s3 of course it has a restful

00:03:44,739 --> 00:03:49,769
gateway which is nice to integrate with

00:03:46,329 --> 00:03:51,910
as well and it's a distributed system

00:03:49,769 --> 00:03:54,910
some of the other software that I'll be

00:03:51,910 --> 00:03:57,489
using in this demonstration will be

00:03:54,910 --> 00:03:59,709
SPARC you've heard a lot about spark and

00:03:57,489 --> 00:04:02,530
a lot of the AI talks and a lot of the

00:03:59,709 --> 00:04:04,989
container talks as well you know SPARC

00:04:02,530 --> 00:04:05,680
is a great engine for processing data

00:04:04,989 --> 00:04:09,459
and allows

00:04:05,680 --> 00:04:11,590
that's and streaming but it also runs on

00:04:09,459 --> 00:04:15,519
Cooper net kubernetes and in this case

00:04:11,590 --> 00:04:17,650
we are using the rad analytics do work

00:04:15,519 --> 00:04:20,229
that they've done in order to move spark

00:04:17,650 --> 00:04:23,229
into a kubernetes framework with their

00:04:20,229 --> 00:04:26,889
of chinko and rad analytics spark

00:04:23,229 --> 00:04:29,169
engines that will be using Jupiter hub

00:04:26,889 --> 00:04:33,639
we've seen several examples of that that

00:04:29,169 --> 00:04:35,800
allows us to have a multi-user kind of

00:04:33,639 --> 00:04:38,260
console to manage Jupiter notebooks

00:04:35,800 --> 00:04:40,930
users can have many different notebooks

00:04:38,260 --> 00:04:43,090
you can also do many different users on

00:04:40,930 --> 00:04:44,949
there and that allows you to do the data

00:04:43,090 --> 00:04:46,600
science work it's designed for data

00:04:44,949 --> 00:04:48,789
science and research and a great tool to

00:04:46,600 --> 00:04:51,400
do that that'll be running on kubernetes

00:04:48,789 --> 00:04:53,020
openshift as well and then the last

00:04:51,400 --> 00:04:55,389
little piece here is some of this may be

00:04:53,020 --> 00:04:57,610
a little bit new to you guys but once

00:04:55,389 --> 00:04:59,710
you actually do the data model work you

00:04:57,610 --> 00:05:02,260
want somewhere to deploy it in this case

00:04:59,710 --> 00:05:04,210
we'll be using open Wisc open Wisc as a

00:05:02,260 --> 00:05:08,800
server less action if you've ever used

00:05:04,210 --> 00:05:11,229
amazon's amazon AWS lambda that's a

00:05:08,800 --> 00:05:14,470
similar concept where it allows you to

00:05:11,229 --> 00:05:15,910
focus more on the delivery of the real

00:05:14,470 --> 00:05:17,349
code and not worry about the

00:05:15,910 --> 00:05:19,690
architecture so i'll show a quick

00:05:17,349 --> 00:05:22,419
example of deploying open whisk and then

00:05:19,690 --> 00:05:23,979
taking that model and the code that uses

00:05:22,419 --> 00:05:25,659
the model and deploying that to open

00:05:23,979 --> 00:05:28,690
whisk as well so that you can have a

00:05:25,659 --> 00:05:33,070
nice rest api wrapper around execution

00:05:28,690 --> 00:05:35,080
of that model so to start off one of the

00:05:33,070 --> 00:05:37,180
first things you want to do is make sure

00:05:35,080 --> 00:05:38,979
you're collecting your data this is just

00:05:37,180 --> 00:05:41,380
a little bit of a diagram that shows you

00:05:38,979 --> 00:05:44,470
a typical workflow that you may have to

00:05:41,380 --> 00:05:45,940
actually ingest your data internally at

00:05:44,470 --> 00:05:47,860
Red Hat we have a lot of different

00:05:45,940 --> 00:05:50,289
systems that send us data this is

00:05:47,860 --> 00:05:53,050
consist some of the examples we do we

00:05:50,289 --> 00:05:56,560
retrieve data from git we also have

00:05:53,050 --> 00:05:58,240
build logs CI logs all that information

00:05:56,560 --> 00:06:01,990
coming in to us but we also have a

00:05:58,240 --> 00:06:04,180
warehouse that houses IT data customer

00:06:01,990 --> 00:06:06,970
data customer feedback support tickets

00:06:04,180 --> 00:06:09,580
and we leverage all of that send that

00:06:06,970 --> 00:06:11,770
data into SAP and then you

00:06:09,580 --> 00:06:14,080
use the machine learning on top of stuff

00:06:11,770 --> 00:06:15,189
in order to process it we have a

00:06:14,080 --> 00:06:18,129
different bunch of different mechanisms

00:06:15,189 --> 00:06:19,840
for getting that data in to set some of

00:06:18,129 --> 00:06:22,389
it is through Jenkins some of this

00:06:19,840 --> 00:06:24,849
through other types of workflow managers

00:06:22,389 --> 00:06:27,610
and it just seamlessly moves that data

00:06:24,849 --> 00:06:32,620
into the object store I'll show some

00:06:27,610 --> 00:06:34,270
examples of that as well in order to

00:06:32,620 --> 00:06:36,069
start out with Seth the first thing

00:06:34,270 --> 00:06:39,300
you'll do is you'll actually configure

00:06:36,069 --> 00:06:42,520
it from an object storage perspective to

00:06:39,300 --> 00:06:43,629
be ready to ingest your data I'm not

00:06:42,520 --> 00:06:45,250
going to go through the exercise of

00:06:43,629 --> 00:06:46,479
installing stuff I think some of you

00:06:45,250 --> 00:06:48,280
have seen that before but I do have a

00:06:46,479 --> 00:06:50,139
link on the slides if you want more

00:06:48,280 --> 00:06:52,479
information about installing it I'll

00:06:50,139 --> 00:06:54,280
just step into the part about setting up

00:06:52,479 --> 00:06:56,319
a user and then actually after you have

00:06:54,280 --> 00:06:57,940
your user set up than just getting at

00:06:56,319 --> 00:07:00,879
and getting your data loaded into the

00:06:57,940 --> 00:07:02,440
system so once you have step object

00:07:00,879 --> 00:07:04,539
stories installed you have to make sure

00:07:02,440 --> 00:07:06,009
the object gateway is installed and then

00:07:04,539 --> 00:07:09,580
once you have that you want to set up

00:07:06,009 --> 00:07:11,620
one of your users with s3 access and in

00:07:09,580 --> 00:07:13,750
this case it's just a quick command that

00:07:11,620 --> 00:07:17,430
you might run where you would create

00:07:13,750 --> 00:07:21,250
your user in the set the Ceph s3

00:07:17,430 --> 00:07:23,979
environment and when you do that what

00:07:21,250 --> 00:07:26,740
you'll get back is an access key and a

00:07:23,979 --> 00:07:29,039
secret key that is just like as if you

00:07:26,740 --> 00:07:32,379
were working in any other s3 environment

00:07:29,039 --> 00:07:34,449
AWS or anything else and that access key

00:07:32,379 --> 00:07:36,339
and that secret key is the important

00:07:34,449 --> 00:07:40,979
part about letting you actually write

00:07:36,339 --> 00:07:40,979
and read data from the set environment

00:07:42,950 --> 00:07:49,370
so from there what I'm going to show

00:07:45,620 --> 00:07:52,070
really quickly is I actually have still

00:07:49,370 --> 00:07:53,870
clear here and hopefully you guys would

00:07:52,070 --> 00:07:55,550
be able to see this okay let's try it

00:07:53,870 --> 00:08:04,460
yeah it looks pretty good

00:07:55,550 --> 00:08:06,560
all right so in here what's that oh whoa

00:08:04,460 --> 00:08:13,610
that's weird hey that's seriously

00:08:06,560 --> 00:08:15,080
cleared it up well that's weird it's

00:08:13,610 --> 00:08:17,620
doing all kinds of funky stuff here

00:08:15,080 --> 00:08:19,790
alright can you see it okay that's good

00:08:17,620 --> 00:08:27,340
so I'm going to show really quickly as I

00:08:19,790 --> 00:08:32,390
have a bucket that I have let's see

00:08:27,340 --> 00:08:35,330
sorry about that in the Massachusetts

00:08:32,390 --> 00:08:37,729
open cloud in that bucket we just called

00:08:35,330 --> 00:08:40,010
it open data hub and since I'm using

00:08:37,729 --> 00:08:42,410
stuff instead of AWS which is see here

00:08:40,010 --> 00:08:44,450
is I'm actually using the AWS command

00:08:42,410 --> 00:08:46,010
line interface and you would go through

00:08:44,450 --> 00:08:47,390
the typical steps of setting up and

00:08:46,010 --> 00:08:49,460
configuring the Intuos command line

00:08:47,390 --> 00:08:51,260
interface actually I'll just tell you

00:08:49,460 --> 00:08:52,910
serious stuff real quick and Steven

00:08:51,260 --> 00:08:55,100
touched up on this before you would

00:08:52,910 --> 00:08:57,260
typically do an AWS configure where you

00:08:55,100 --> 00:08:58,850
take that secret key and that access key

00:08:57,260 --> 00:09:00,800
that showed you in the previous step and

00:08:58,850 --> 00:09:03,440
you would just populate that information

00:09:00,800 --> 00:09:06,830
I'm not going to actually show my secret

00:09:03,440 --> 00:09:08,810
keys here but you get the idea yeah I

00:09:06,830 --> 00:09:12,860
know it has awesome data and though

00:09:08,810 --> 00:09:15,890
trust me so once I do that and have it

00:09:12,860 --> 00:09:19,430
all set up than I do in MV AWS LS you'll

00:09:15,890 --> 00:09:21,470
see here from my training subdirectory I

00:09:19,430 --> 00:09:23,120
actually have no data in there so what I

00:09:21,470 --> 00:09:26,259
want to do really quickly just upload

00:09:23,120 --> 00:09:28,389
some training data that I have here and

00:09:26,259 --> 00:09:30,339
I'm going to just run this command here

00:09:28,389 --> 00:09:32,739
what it's going to do is upload a

00:09:30,339 --> 00:09:35,319
tab-separated value table called

00:09:32,739 --> 00:09:37,269
training data this is actually some data

00:09:35,319 --> 00:09:39,369
that we're using for doing sentiment

00:09:37,269 --> 00:09:42,369
analysis and you'll see that as I work

00:09:39,369 --> 00:09:44,439
through the example however there's all

00:09:42,369 --> 00:09:46,479
kinds of different formats of data I'm

00:09:44,439 --> 00:09:48,479
just doing using tab separated values

00:09:46,479 --> 00:09:51,489
right here you could use snappy

00:09:48,479 --> 00:09:55,239
compression with parquet it can be JSON

00:09:51,489 --> 00:09:56,889
it could be CSV files anything that is

00:09:55,239 --> 00:10:02,589
typically supported by a lot of the

00:09:56,889 --> 00:10:04,449
Hadoop ecosystem now that I've uploaded

00:10:02,589 --> 00:10:06,069
that you'll see I actually have my

00:10:04,449 --> 00:10:09,429
training data there great awesome

00:10:06,069 --> 00:10:11,499
oh yeah uploaded data now what well

00:10:09,429 --> 00:10:12,939
here's where you start to analyze that

00:10:11,499 --> 00:10:15,639
data and a little bit of what inbox

00:10:12,939 --> 00:10:18,209
check and steven is shown before you

00:10:15,639 --> 00:10:22,569
will then start to use Jupiter hub

00:10:18,209 --> 00:10:25,359
Jupiter hub is cool it's very to

00:10:22,569 --> 00:10:27,789
integrate with Steph and actually query

00:10:25,359 --> 00:10:30,819
the data and use tools like spark

00:10:27,789 --> 00:10:32,949
tensorflow so I could learn all of those

00:10:30,819 --> 00:10:34,839
other frameworks in order to process the

00:10:32,949 --> 00:10:37,299
data for this example I'm gonna stick

00:10:34,839 --> 00:10:41,019
with spark and you'll see I'm actually

00:10:37,299 --> 00:10:44,139
accessing the data in set by using the

00:10:41,019 --> 00:10:46,989
s3n library the s3 and libraries and

00:10:44,139 --> 00:10:49,179
jars in order to get access to it so a

00:10:46,989 --> 00:10:52,509
very simple concept there and what I'm

00:10:49,179 --> 00:10:56,199
going to do is show you really quickly

00:10:52,509 --> 00:10:56,740
what we've done as part of the work for

00:10:56,199 --> 00:11:00,640
the

00:10:56,740 --> 00:11:03,850
moc deployment so here I'm logged into

00:11:00,640 --> 00:11:05,590
an open shift instance and this wouldn't

00:11:03,850 --> 00:11:08,110
necessarily be a data science role but

00:11:05,590 --> 00:11:10,000
it'd be more of the DevOps you know

00:11:08,110 --> 00:11:12,340
systems engineer type of role we've

00:11:10,000 --> 00:11:14,530
actually created an APB that allows us

00:11:12,340 --> 00:11:18,280
to go through the service catalog and

00:11:14,530 --> 00:11:19,390
find Jupiter hub down here so I'm gonna

00:11:18,280 --> 00:11:21,720
do is I'm going to create a quick

00:11:19,390 --> 00:11:28,000
project here and I'm just gonna name it

00:11:21,720 --> 00:11:30,280
Jupiter hub and you'll see that project

00:11:28,000 --> 00:11:33,210
there and what I'm going to do is

00:11:30,280 --> 00:11:36,760
actually click on this service broker

00:11:33,210 --> 00:11:38,560
select my project and it will start to

00:11:36,760 --> 00:11:40,510
deploy Jupiter hub now there's a couple

00:11:38,560 --> 00:11:42,460
of options that we have here the

00:11:40,510 --> 00:11:44,200
database memory Jupiter hub memory

00:11:42,460 --> 00:11:46,840
notebook memory I'm gonna increase the

00:11:44,200 --> 00:11:51,160
notebook memory to 2 gig and then start

00:11:46,840 --> 00:11:54,610
to create this cool now it's actually

00:11:51,160 --> 00:11:56,140
starting to deploy my Jupiter hub you'll

00:11:54,610 --> 00:11:57,670
see it's pending and then you'll start

00:11:56,140 --> 00:12:00,250
to see some really interesting things

00:11:57,670 --> 00:12:05,500
happen once the pods start kicking off

00:12:00,250 --> 00:12:07,650
here in a second so we have the let's

00:12:05,500 --> 00:12:09,940
see there we go

00:12:07,650 --> 00:12:12,970
so the pod will start to initialize

00:12:09,940 --> 00:12:15,490
you'll see with Jupiter hub if you saw a

00:12:12,970 --> 00:12:16,740
fostex example earlier today you'll see

00:12:15,490 --> 00:12:19,240
that there was a number of different

00:12:16,740 --> 00:12:20,980
images that you could have we have a

00:12:19,240 --> 00:12:23,350
tensorflow image we have a sidekick

00:12:20,980 --> 00:12:24,970
learn image we have a spark image and so

00:12:23,350 --> 00:12:27,250
this is actually building all of those

00:12:24,970 --> 00:12:30,910
images behind the scenes preparing

00:12:27,250 --> 00:12:34,060
Jupiter hub for you and also creating a

00:12:30,910 --> 00:12:35,830
spark operator that spark operator once

00:12:34,060 --> 00:12:37,690
we actually start a notebook that has

00:12:35,830 --> 00:12:39,250
spark in it at that point each

00:12:37,690 --> 00:12:40,990
individual user will have their own

00:12:39,250 --> 00:12:43,780
spark cluster that spins up behind the

00:12:40,990 --> 00:12:46,690
scenes so as that's running I'm actually

00:12:43,780 --> 00:12:48,550
going to shoot over to a different

00:12:46,690 --> 00:12:49,990
instance of open ship that I have since

00:12:48,550 --> 00:12:52,680
this is still still running and getting

00:12:49,990 --> 00:12:54,420
started I want to show you exact

00:12:52,680 --> 00:12:56,610
what it looks like when you start to get

00:12:54,420 --> 00:13:00,030
into Jupiter hubs so I'm gonna go back

00:12:56,610 --> 00:13:05,520
here in a second one stupid hub actually

00:13:00,030 --> 00:13:07,680
starts up in fact I will because I'm not

00:13:05,520 --> 00:13:09,570
going to I'm going to just continue on

00:13:07,680 --> 00:13:11,760
my server once it starts up it gives you

00:13:09,570 --> 00:13:13,860
the option to select whichever image I'm

00:13:11,760 --> 00:13:15,750
gonna stick skip that step because I've

00:13:13,860 --> 00:13:19,740
already selected a spark image for the

00:13:15,750 --> 00:13:21,600
sake of time and in the spark image I've

00:13:19,740 --> 00:13:23,880
created a note book called sentiment

00:13:21,600 --> 00:13:25,590
analysis training I would love to take

00:13:23,880 --> 00:13:27,480
the credit for all the intelligence on

00:13:25,590 --> 00:13:30,180
here but actually sewage it on our team

00:13:27,480 --> 00:13:33,360
was the the data scientist that helped

00:13:30,180 --> 00:13:36,090
us create this what this code is doing I

00:13:33,360 --> 00:13:38,460
have no idea all I just know is the end

00:13:36,090 --> 00:13:39,720
where it actually trains a model so I'm

00:13:38,460 --> 00:13:41,040
gonna actually run this really quickly

00:13:39,720 --> 00:13:42,480
and just kind of step through the code

00:13:41,040 --> 00:13:48,810
and show you a little bit of magic

00:13:42,480 --> 00:13:51,300
behind the scenes here first thing it's

00:13:48,810 --> 00:13:54,210
doing here is you do have the ability

00:13:51,300 --> 00:13:56,280
with Jupiter hub to add a few more

00:13:54,210 --> 00:13:58,290
libraries that may not be on the image

00:13:56,280 --> 00:14:01,050
that you're using in this case we do

00:13:58,290 --> 00:14:03,480
have spark and we do have scikit-learn

00:14:01,050 --> 00:14:05,190
installed but we do not have tensorflow

00:14:03,480 --> 00:14:06,870
and carrots and some of the other ones

00:14:05,190 --> 00:14:08,700
so what I've done here is I'm going

00:14:06,870 --> 00:14:10,650
through and installing those on the

00:14:08,700 --> 00:14:14,400
system and then it goes down a little

00:14:10,650 --> 00:14:17,130
bit farther and starts to import some of

00:14:14,400 --> 00:14:19,590
those libraries here's where a lot of

00:14:17,130 --> 00:14:22,500
the magic happens with Seth you'll see

00:14:19,590 --> 00:14:25,590
here I'm actually instantiating a PI

00:14:22,500 --> 00:14:27,510
spark instance and what it's doing is

00:14:25,590 --> 00:14:30,660
actually leveraging the spark that's

00:14:27,510 --> 00:14:34,640
already on my Jupiter hub so if I go

00:14:30,660 --> 00:14:34,640
here to my Jupiter hub instance

00:14:38,610 --> 00:14:45,519
you will see that there is a spark

00:14:42,220 --> 00:14:49,360
cluster for sh Griffey that's me that's

00:14:45,519 --> 00:14:51,100
me so I've got my spark cluster and I am

00:14:49,360 --> 00:14:52,930
submitting my job to that but you also

00:14:51,100 --> 00:14:54,910
see there's some other people s fuels

00:14:52,930 --> 00:14:56,529
that Stephen heals and then we have a

00:14:54,910 --> 00:15:00,670
couple of other people that are in the

00:14:56,529 --> 00:15:05,680
system here so what it's done here I've

00:15:00,670 --> 00:15:08,290
actually used the s3 in sorry I said s3

00:15:05,680 --> 00:15:11,230
n plus X at s3 a the newer one I've used

00:15:08,290 --> 00:15:13,360
s3 a to actually connect it to set in

00:15:11,230 --> 00:15:16,600
this case I'm reading a CSV file and

00:15:13,360 --> 00:15:18,610
it's just doing some basic printing some

00:15:16,600 --> 00:15:19,089
validation hey here's what the CSV file

00:15:18,610 --> 00:15:21,100
looks like

00:15:19,089 --> 00:15:24,010
farther down I'm using a little bit of

00:15:21,100 --> 00:15:26,589
pandas and then shows you what some of

00:15:24,010 --> 00:15:28,750
the data looks like that data is stored

00:15:26,589 --> 00:15:31,990
in stuff but now I'm reading it you see

00:15:28,750 --> 00:15:34,899
some of the contents of it and then it

00:15:31,990 --> 00:15:37,000
does all the data sciency stuff and then

00:15:34,899 --> 00:15:39,070
it shows some graphs I like graphs says

00:15:37,000 --> 00:15:41,950
oh that looks cool don't know what it

00:15:39,070 --> 00:15:43,209
tells me but it looks cool and it

00:15:41,950 --> 00:15:46,000
continues down we've got some more

00:15:43,209 --> 00:15:48,520
graphs here the interesting thing is

00:15:46,000 --> 00:15:52,120
when we're doing the sentiment analysis

00:15:48,520 --> 00:15:55,000
training we have to understand whether

00:15:52,120 --> 00:15:56,140
the accuracy of this is it's pretty good

00:15:55,000 --> 00:15:58,029
you know it's just a good model that

00:15:56,140 --> 00:15:59,170
we're working towards or are we having

00:15:58,029 --> 00:16:01,060
trouble with the modeling we need to

00:15:59,170 --> 00:16:02,110
refine it in this case you know the

00:16:01,060 --> 00:16:03,520
example that I'm running through right

00:16:02,110 --> 00:16:06,160
here we don't have much data in the

00:16:03,520 --> 00:16:08,200
system so I'm not gonna get too much

00:16:06,160 --> 00:16:09,670
into it but this will show you some some

00:16:08,200 --> 00:16:11,320
of the cool things we can do with the

00:16:09,670 --> 00:16:12,970
data we're starting a little word cloud

00:16:11,320 --> 00:16:14,860
where we're actually detecting what's

00:16:12,970 --> 00:16:17,649
being talked about in the training data

00:16:14,860 --> 00:16:20,520
but as it goes as it goes a little bit

00:16:17,649 --> 00:16:24,000
farther down you'll see it starts to

00:16:20,520 --> 00:16:27,240
build out the models and

00:16:24,000 --> 00:16:29,069
and somewhere around here where it's

00:16:27,240 --> 00:16:32,430
actually building the model it's gonna

00:16:29,069 --> 00:16:34,019
run the model this is where going back

00:16:32,430 --> 00:16:35,970
to that diagram you would do some tuning

00:16:34,019 --> 00:16:38,129
and manipulation to make sure that the

00:16:35,970 --> 00:16:41,160
model is accurate and then at the bottom

00:16:38,129 --> 00:16:44,459
here once it's actually done it's almost

00:16:41,160 --> 00:16:46,649
there it's that step 83 it's finished

00:16:44,459 --> 00:16:47,430
with that you'll see it the accuracy

00:16:46,649 --> 00:16:50,490
printed out

00:16:47,430 --> 00:16:52,290
accuracy of 69 huh baby that's good

00:16:50,490 --> 00:16:53,639
maybe that's not I would say it's

00:16:52,290 --> 00:16:56,399
probably not good I'm not a data

00:16:53,639 --> 00:16:58,170
scientist but that'll tell you as you

00:16:56,399 --> 00:16:59,959
get more data in there as you train the

00:16:58,170 --> 00:17:04,409
model you can do some cool things there

00:16:59,959 --> 00:17:06,720
so now I've got a great model and I need

00:17:04,409 --> 00:17:09,900
to do something with it what's the next

00:17:06,720 --> 00:17:12,299
step well in this environment what we

00:17:09,900 --> 00:17:14,400
would do is since we already have set

00:17:12,299 --> 00:17:17,459
we're actually gonna store the model and

00:17:14,400 --> 00:17:19,380
set as well and that allows us to get

00:17:17,459 --> 00:17:22,049
access to the model from any number of

00:17:19,380 --> 00:17:25,829
places we can use open whisk we can use

00:17:22,049 --> 00:17:28,919
any basic Python code we can use Java

00:17:25,829 --> 00:17:32,789
code whatever we want and it's agnostic

00:17:28,919 --> 00:17:35,600
at that point so storing the data in the

00:17:32,789 --> 00:17:38,669
model and set what we're doing here is

00:17:35,600 --> 00:17:41,490
pickling the tokenizer x' and the models

00:17:38,669 --> 00:17:46,789
and storing that there and the outcome

00:17:41,490 --> 00:17:50,400
of this if I go back to my s my Amazon

00:17:46,789 --> 00:17:54,900
CLI I'm gonna do an LS on the model

00:17:50,400 --> 00:17:56,280
folder oh yeah sorry about that guys

00:17:54,900 --> 00:17:59,419
let's just shrink this and see if you

00:17:56,280 --> 00:17:59,419
can see a little bit better

00:18:05,100 --> 00:18:10,620
that work better it's a good okay

00:18:07,230 --> 00:18:15,529
awesome all right now a screens all

00:18:10,620 --> 00:18:15,529
jacked up yeah he stills yet okay cool

00:18:15,799 --> 00:18:21,720
alright so what I'm gonna do here is I

00:18:18,600 --> 00:18:23,940
have a little list on this and you'll

00:18:21,720 --> 00:18:27,840
see I've got a number of folders I have

00:18:23,940 --> 00:18:31,340
data sets metrics models and training so

00:18:27,840 --> 00:18:31,340
now I'm gonna look at my models folder

00:18:32,720 --> 00:18:36,509
and I see that I have some sentiment

00:18:35,340 --> 00:18:40,080
data there and I'm gonna go to the

00:18:36,509 --> 00:18:44,850
sentiment folder and I'm sorry

00:18:40,080 --> 00:18:47,039
let that a slash and now you see okay

00:18:44,850 --> 00:18:49,590
great I have my model I have my

00:18:47,039 --> 00:18:53,700
dimensions that have my tokenizer okay

00:18:49,590 --> 00:18:58,889
so what do we do with that so we go back

00:18:53,700 --> 00:19:01,620
to here now we're going to talk about

00:18:58,889 --> 00:19:04,440
how do we deploy the model and make it

00:19:01,620 --> 00:19:06,600
useful in this use case I'm using open

00:19:04,440 --> 00:19:08,429
wisp which is a serverless action that I

00:19:06,600 --> 00:19:09,990
mentioned that earlier but again you can

00:19:08,429 --> 00:19:12,149
use any number of things if you wanted

00:19:09,990 --> 00:19:14,460
to use Argo if you wanted to use knife

00:19:12,149 --> 00:19:15,929
eye or any of those technologies as long

00:19:14,460 --> 00:19:17,909
as you can get access to the data and

00:19:15,929 --> 00:19:20,549
stuff that's great sometimes you may

00:19:17,909 --> 00:19:23,490
want to even catch the the model maybe

00:19:20,549 --> 00:19:26,159
put it into some kind of caching layer

00:19:23,490 --> 00:19:28,860
that you can do that as well so the

00:19:26,159 --> 00:19:30,720
first thing I'm going to do with the the

00:19:28,860 --> 00:19:33,090
open whisk is I don't actually show you

00:19:30,720 --> 00:19:37,049
a quick and easy way to deploy it so I'm

00:19:33,090 --> 00:19:40,019
going to go back to my open shift here

00:19:37,049 --> 00:19:44,170
and I will create a new project called

00:19:40,019 --> 00:19:47,360
SH Griffey open Wis

00:19:44,170 --> 00:19:50,010
[Music]

00:19:47,360 --> 00:19:51,990
and you see that project has been

00:19:50,010 --> 00:19:55,890
created right there so I'm going to go

00:19:51,990 --> 00:19:57,600
back to the command line and it's gonna

00:19:55,890 --> 00:20:03,810
copy this command really quickly and

00:19:57,600 --> 00:20:06,230
I'll explain what it's gonna do and I am

00:20:03,810 --> 00:20:09,330
just going to use the open shipped

00:20:06,230 --> 00:20:12,180
command line interface to actually

00:20:09,330 --> 00:20:14,040
deploy this recently the open whisked

00:20:12,180 --> 00:20:17,520
project that was done on top of open

00:20:14,040 --> 00:20:20,670
shift has moved into incubator at Apache

00:20:17,520 --> 00:20:23,340
so if you actually search for open whisk

00:20:20,670 --> 00:20:25,290
with open shift on Apache then you'll

00:20:23,340 --> 00:20:27,060
it'll come up what I'm doing right here

00:20:25,290 --> 00:20:28,590
is I'm just taking that master template

00:20:27,060 --> 00:20:35,990
and I'm deploying open ship so I'm

00:20:28,590 --> 00:20:42,560
deploying open whisk on top of that and

00:20:35,990 --> 00:20:45,540
let's see it's what you will see here is

00:20:42,560 --> 00:20:48,390
it instantly started creating a lot of

00:20:45,540 --> 00:20:50,100
different deployments you have nginx

00:20:48,390 --> 00:20:53,670
strings II if you're not familiar with

00:20:50,100 --> 00:20:56,730
strings II that is the the kubernetes

00:20:53,670 --> 00:20:58,980
kafka that's also been worked on from a

00:20:56,730 --> 00:21:01,680
lot of the redhead folks and it has

00:20:58,980 --> 00:21:03,150
CouchDB in a number of other things what

00:21:01,680 --> 00:21:05,370
you see in the background is it starts

00:21:03,150 --> 00:21:07,380
to spin up a bunch of different pods but

00:21:05,370 --> 00:21:09,960
again for the sake of time I won't focus

00:21:07,380 --> 00:21:11,400
on waiting for this to be done have

00:21:09,960 --> 00:21:13,130
another instance where this is already

00:21:11,400 --> 00:21:19,050
up and running

00:21:13,130 --> 00:21:21,420
once I have the open whisk environment

00:21:19,050 --> 00:21:24,390
all set up then you actually use the

00:21:21,420 --> 00:21:27,020
open whisks command-line interface to

00:21:24,390 --> 00:21:29,480
take a look and see exactly

00:21:27,020 --> 00:21:33,770
how to deploy the system so what I have

00:21:29,480 --> 00:21:36,500
here is I have some code that is

00:21:33,770 --> 00:21:40,400
actually going to consume that model and

00:21:36,500 --> 00:21:43,550
actually run a sentiment analysis on top

00:21:40,400 --> 00:21:45,470
of that using the model up here you'll

00:21:43,550 --> 00:21:47,660
see some code where it says analyze

00:21:45,470 --> 00:21:50,990
sentiment this is actually where it will

00:21:47,660 --> 00:21:53,030
load the model and start to feed in the

00:21:50,990 --> 00:21:55,070
text that I pass it and then I have some

00:21:53,030 --> 00:21:56,720
additional information here where it's

00:21:55,070 --> 00:21:58,610
just taking some command-line arguments

00:21:56,720 --> 00:22:02,180
nothing fancy just a quick example to

00:21:58,610 --> 00:22:05,780
show you in order to deploy the open

00:22:02,180 --> 00:22:08,510
whisker the open whisk Python code that

00:22:05,780 --> 00:22:10,460
I just showed you you just run a couple

00:22:08,510 --> 00:22:14,510
of commands here so I'm actually going

00:22:10,460 --> 00:22:17,180
to create an action and all kind of step

00:22:14,510 --> 00:22:19,100
through exactly what this is doing in

00:22:17,180 --> 00:22:20,900
open whisk everything is just an action

00:22:19,100 --> 00:22:22,430
so I still do the Python code a very

00:22:20,900 --> 00:22:24,500
simple code I don't want to have to

00:22:22,430 --> 00:22:27,710
worry about spinning up my own nginx my

00:22:24,500 --> 00:22:29,900
own web interface all of the rest api

00:22:27,710 --> 00:22:31,610
that has to go along with it so I can

00:22:29,900 --> 00:22:34,160
take that Python code and you'll see

00:22:31,610 --> 00:22:35,860
here what I'm doing is I have the name

00:22:34,160 --> 00:22:38,630
of the action called sentiment and

00:22:35,860 --> 00:22:41,450
service and then have main dot Python

00:22:38,630 --> 00:22:44,420
and I actually already have a docker

00:22:41,450 --> 00:22:46,130
image that has some of the model

00:22:44,420 --> 00:22:47,870
libraries that has tensorflow it has

00:22:46,130 --> 00:22:50,090
Kerris already installed on that docker

00:22:47,870 --> 00:22:52,460
image so I'm going to do a quick

00:22:50,090 --> 00:22:56,030
deployment of that boom there you go

00:22:52,460 --> 00:22:58,400
it's already deployed so once I want to

00:22:56,030 --> 00:23:01,970
actually test it because open wisk it

00:22:58,400 --> 00:23:03,740
comes with a REST API I am going to use

00:23:01,970 --> 00:23:07,850
postman hopefully you guys can see that

00:23:03,740 --> 00:23:10,790
okay that's a really big postman so I

00:23:07,850 --> 00:23:15,640
have two different postman commands here

00:23:10,790 --> 00:23:19,190
and all I'm doing in postman is a post

00:23:15,640 --> 00:23:21,200
with a to arrest endpoint in this rest

00:23:19,190 --> 00:23:24,050
endpoint you see I'm now pointing to the

00:23:21,200 --> 00:23:25,400
sentiment service and I'm giving it some

00:23:24,050 --> 00:23:28,640
credentials these are my open Wis

00:23:25,400 --> 00:23:29,530
credentials and the body of it is just

00:23:28,640 --> 00:23:30,790
some text that I want

00:23:29,530 --> 00:23:35,070
pasen that i want to send them an

00:23:30,790 --> 00:23:37,810
analysis on I'll send that along the way

00:23:35,070 --> 00:23:40,120
and down here I have an activation ID

00:23:37,810 --> 00:23:42,940
now if you saw you may not have seen it

00:23:40,120 --> 00:23:47,770
but really quickly it's starting to

00:23:42,940 --> 00:23:55,420
build up if I go over here I'm gonna go

00:23:47,770 --> 00:23:57,610
up to the open Wis project it spins up a

00:23:55,420 --> 00:24:00,100
pod automatically and there's my

00:23:57,610 --> 00:24:05,320
sentiment service running in open whisk

00:24:00,100 --> 00:24:07,750
and it's actually doing some analysis

00:24:05,320 --> 00:24:09,130
right here it's loading up some loading

00:24:07,750 --> 00:24:11,920
up the models you see it loaded the

00:24:09,130 --> 00:24:13,540
model and it's actually using tensor

00:24:11,920 --> 00:24:18,310
flow as well to start to process the

00:24:13,540 --> 00:24:20,530
data right as that's running I can then

00:24:18,310 --> 00:24:22,420
hold and see well what's going on with

00:24:20,530 --> 00:24:24,670
the action so I'm going to take that

00:24:22,420 --> 00:24:28,510
activation ID it's basically like an

00:24:24,670 --> 00:24:31,000
execution ID and replace that with just

00:24:28,510 --> 00:24:33,580
the one that I'm copied here and I'm

00:24:31,000 --> 00:24:36,220
going to do again when I do a get to

00:24:33,580 --> 00:24:39,100
open whisk to see what's going on with

00:24:36,220 --> 00:24:41,110
that service it'll all I said it's not

00:24:39,100 --> 00:24:42,990
exist until it's actually done here so

00:24:41,110 --> 00:24:46,600
I'll just keep sending into it completes

00:24:42,990 --> 00:24:50,290
okay we got it 200 there and now what

00:24:46,600 --> 00:24:51,760
you'll see is the result of that action

00:24:50,290 --> 00:24:54,130
there's a little bit of metadata that

00:24:51,760 --> 00:24:56,080
open whisk provides but then at the end

00:24:54,130 --> 00:24:58,930
of the day you see here my results

00:24:56,080 --> 00:25:02,110
sentiment equals positive now the way

00:24:58,930 --> 00:25:04,720
that we've used this in Red Hat is we

00:25:02,110 --> 00:25:06,700
have a sentiment analysis service that

00:25:04,720 --> 00:25:09,010
sits out there and open with and as

00:25:06,700 --> 00:25:10,900
people are submitting their requests

00:25:09,010 --> 00:25:13,780
they're actually calling to our REST API

00:25:10,900 --> 00:25:15,550
and open Wis to get results back we do a

00:25:13,780 --> 00:25:17,470
lot more than just this sentiment

00:25:15,550 --> 00:25:19,420
analysis that I've shown here we do

00:25:17,470 --> 00:25:22,210
entity detection as well so that we

00:25:19,420 --> 00:25:23,590
understand for the given text what are

00:25:22,210 --> 00:25:25,360
people talking about is there a good

00:25:23,590 --> 00:25:26,770
feeling about Red Hat great feeling

00:25:25,360 --> 00:25:28,660
about open ship what do people think

00:25:26,770 --> 00:25:29,560
about DEFCON and it gives you a great

00:25:28,660 --> 00:25:33,780
way to analyze

00:25:29,560 --> 00:25:38,680
the data but to bring it all back home

00:25:33,780 --> 00:25:41,620
now we have an entire process and where

00:25:38,680 --> 00:25:43,630
we build with a model we actually

00:25:41,620 --> 00:25:47,530
uploaded the data built the model and

00:25:43,630 --> 00:25:50,200
deployed it so going back to what we had

00:25:47,530 --> 00:25:52,840
in the original slide gives you the

00:25:50,200 --> 00:25:54,580
pipeline and what we've done here is for

00:25:52,840 --> 00:25:58,240
the storing of the data and the models

00:25:54,580 --> 00:25:59,830
we're using step to develop tune and

00:25:58,240 --> 00:26:01,960
train the models we're using a

00:25:59,830 --> 00:26:05,350
combination of sparking jupiter hub and

00:26:01,960 --> 00:26:06,610
then to deploy the model into the rest

00:26:05,350 --> 00:26:08,710
of the environments actually make it

00:26:06,610 --> 00:26:10,930
usable is open wisk

00:26:08,710 --> 00:26:13,180
obviously for this use case there's a

00:26:10,930 --> 00:26:16,420
number of different users that you might

00:26:13,180 --> 00:26:18,670
have in the system where your engineers

00:26:16,420 --> 00:26:20,590
your data engineers may be more focused

00:26:18,670 --> 00:26:23,380
on the Ceph side and then your data

00:26:20,590 --> 00:26:26,500
scientists will be in the spark and the

00:26:23,380 --> 00:26:28,150
jupiter hub side and then for the DevOps

00:26:26,500 --> 00:26:29,920
and the data engineers as well they'll

00:26:28,150 --> 00:26:31,480
be involved in the open with side so

00:26:29,920 --> 00:26:32,920
you'll haven't done a number of

00:26:31,480 --> 00:26:36,490
different people involved in it but

00:26:32,920 --> 00:26:39,960
that's that's teamwork right so that's

00:26:36,490 --> 00:26:42,850
all I have if you want more information

00:26:39,960 --> 00:26:45,070
we are starting to publish a lot of what

00:26:42,850 --> 00:26:47,350
we're doing inside the data hub to the

00:26:45,070 --> 00:26:49,930
open data hub project and just to let

00:26:47,350 --> 00:26:52,300
you know a lot of what I show what I

00:26:49,930 --> 00:26:54,640
walk through here is part of the open

00:26:52,300 --> 00:26:56,740
data hub project so Steven mentioned it

00:26:54,640 --> 00:26:58,510
before with the MOC and we've seen a lot

00:26:56,740 --> 00:27:00,490
of it as we've gone through the past

00:26:58,510 --> 00:27:03,910
couple of days so you'll see more and

00:27:00,490 --> 00:27:05,950
more information and more code being

00:27:03,910 --> 00:27:07,960
submitted into that repo and of course

00:27:05,950 --> 00:27:10,750
if anyone wants to contribute now feel

00:27:07,960 --> 00:27:12,910
free to join us we also have a lot of

00:27:10,750 --> 00:27:14,020
information about SPARC on Hoover

00:27:12,910 --> 00:27:15,590
Nettie's and openshift

00:27:14,020 --> 00:27:19,700
you can see that in the

00:27:15,590 --> 00:27:23,150
Linux diode IO page you can also take a

00:27:19,700 --> 00:27:25,340
look at the now incubating open whisk on

00:27:23,150 --> 00:27:27,830
openshift project and contribute in that

00:27:25,340 --> 00:27:30,799
way at the end of the day you can always

00:27:27,830 --> 00:27:34,809
just contact me sh Griffey at redhead

00:27:30,799 --> 00:27:34,809
comm any questions

00:27:37,750 --> 00:27:45,570
[Applause]

00:27:37,980 --> 00:27:48,549
[Music]

00:27:45,570 --> 00:27:50,220
that was awesome live demo on the mass

00:27:48,549 --> 00:27:56,350
open club that's the coolest thing ever

00:27:50,220 --> 00:27:58,990
mark I did that thank you you you so

00:27:56,350 --> 00:28:01,840
yeah so we're running data hub one the

00:27:58,990 --> 00:28:04,679
mass open cloud sherrard and I know many

00:28:01,840 --> 00:28:07,059
of you probably don't that the

00:28:04,679 --> 00:28:11,070
infrastructure we're using right now on

00:28:07,059 --> 00:28:13,120
mass open cloud is necessarily ideal for

00:28:11,070 --> 00:28:15,850
machine learning training models that

00:28:13,120 --> 00:28:17,169
kind of thing yeah if you could if you

00:28:15,850 --> 00:28:20,799
could have whatever you want it up there

00:28:17,169 --> 00:28:22,179
within sort of reason what what kind of

00:28:20,799 --> 00:28:25,179
architecture would you like to see what

00:28:22,179 --> 00:28:27,190
is it is it storage is it is it vector

00:28:25,179 --> 00:28:29,049
processing like what where do we need to

00:28:27,190 --> 00:28:31,659
go now you know I think that's an

00:28:29,049 --> 00:28:33,460
interesting one and the reason I'm so

00:28:31,659 --> 00:28:35,679
excited about the MOC is I actually

00:28:33,460 --> 00:28:39,070
would like to see the users tell us that

00:28:35,679 --> 00:28:41,409
I think we'll certainly be doing some

00:28:39,070 --> 00:28:43,390
performance evaluation on the workloads

00:28:41,409 --> 00:28:46,299
that are happening on the MOC I have a

00:28:43,390 --> 00:28:48,039
feeling that we're going to explore more

00:28:46,299 --> 00:28:51,520
about Steph and the storage on stuff

00:28:48,039 --> 00:28:53,649
there's going to be a knowledge sharing

00:28:51,520 --> 00:28:55,240
that has to happen where we have to

00:28:53,649 --> 00:28:57,190
understand the best way to store this

00:28:55,240 --> 00:29:00,100
data I mentioned before you could have a

00:28:57,190 --> 00:29:02,919
tab separated value table or CSV file

00:29:00,100 --> 00:29:04,510
but there's other technologies that will

00:29:02,919 --> 00:29:06,880
allow you to process the data batum

00:29:04,510 --> 00:29:09,549
better more column they're formatted

00:29:06,880 --> 00:29:13,149
data like snap decompression with park'

00:29:09,549 --> 00:29:15,370
things like that but from the visible

00:29:13,149 --> 00:29:18,370
hardware perspective we'll certainly

00:29:15,370 --> 00:29:20,890
want to move more into the GPU

00:29:18,370 --> 00:29:23,260
enablement FPGA type of enablement of

00:29:20,890 --> 00:29:25,779
the system so it'd be great if we start

00:29:23,260 --> 00:29:28,179
to leverage some of those technologies

00:29:25,779 --> 00:29:29,860
as well that will allow everyone to do

00:29:28,179 --> 00:29:33,010
work faster and I think that's gonna be

00:29:29,860 --> 00:29:35,919
the nice blend of having the right

00:29:33,010 --> 00:29:37,809
storage for your data but then also

00:29:35,919 --> 00:29:40,649
having the right compute horsepower in

00:29:37,809 --> 00:29:40,649
order to make it happen

00:29:43,330 --> 00:29:46,960
any other questions

00:29:55,050 --> 00:30:00,630
my question is mostly around data

00:29:57,690 --> 00:30:02,340
acquisition we are currently using I

00:30:00,630 --> 00:30:07,170
mean in your example we are using

00:30:02,340 --> 00:30:10,290
training data so what kind of data

00:30:07,170 --> 00:30:13,440
normalization or cleansing components

00:30:10,290 --> 00:30:16,740
can be used in the openshift environment

00:30:13,440 --> 00:30:20,340
good question the simplest answer to

00:30:16,740 --> 00:30:22,500
that spark is great for ETL we've used

00:30:20,340 --> 00:30:24,540
that internally at Red Hat for a number

00:30:22,500 --> 00:30:25,890
of our products and I think in this use

00:30:24,540 --> 00:30:27,810
case it's great as well

00:30:25,890 --> 00:30:30,030
the nice thing being again you've

00:30:27,810 --> 00:30:31,890
separated the storage from the compute

00:30:30,030 --> 00:30:33,690
so you can have spark you can do your

00:30:31,890 --> 00:30:35,430
cleansing the you manipulation of the

00:30:33,690 --> 00:30:37,530
data and then what you would want from

00:30:35,430 --> 00:30:39,840
that is more of a workflow manager to

00:30:37,530 --> 00:30:41,640
manage those spark jobs as it's flowing

00:30:39,840 --> 00:30:43,740
through the system some of the other

00:30:41,640 --> 00:30:46,350
technologies that we are looking into is

00:30:43,740 --> 00:30:49,530
the ability to do hive on spark so you

00:30:46,350 --> 00:30:51,570
can take a hive job in kubernetes but

00:30:49,530 --> 00:30:53,610
you spark as the framework to do that

00:30:51,570 --> 00:30:54,540
and manipulate the data from there a lot

00:30:53,610 --> 00:30:57,090
of people are just more comfortable

00:30:54,540 --> 00:30:58,290
using using hive as opposed to spark and

00:30:57,090 --> 00:31:00,840
there's some other technologies that

00:30:58,290 --> 00:31:03,740
we're looking at but I gives you a good

00:31:00,840 --> 00:31:03,740
baseline to go off of

00:31:12,600 --> 00:31:20,440
so you mentioned open whisk on open

00:31:17,140 --> 00:31:23,280
shift is kind of an alpha project yes so

00:31:20,440 --> 00:31:23,280
what's missing there

00:31:23,880 --> 00:31:27,940
what major problems am I going to run

00:31:26,350 --> 00:31:30,520
into if I try to use it for something I

00:31:27,940 --> 00:31:32,920
dependent Oh the major issues I've seen

00:31:30,520 --> 00:31:35,410
honestly is how fast is being developed

00:31:32,920 --> 00:31:38,040
I'm actually excited the fact that it's

00:31:35,410 --> 00:31:41,110
moved to incubator because before then

00:31:38,040 --> 00:31:43,090
if I deployed today versus next week

00:31:41,110 --> 00:31:45,610
things may break and there may be some

00:31:43,090 --> 00:31:47,290
inconsistencies there so I think

00:31:45,610 --> 00:31:49,540
incubator will give us a little bit of a

00:31:47,290 --> 00:31:51,640
better chance to version things off it

00:31:49,540 --> 00:31:54,490
has a lot of support for multiple

00:31:51,640 --> 00:31:55,690
languages Java Python you know some of

00:31:54,490 --> 00:31:59,890
the other ones nodejs

00:31:55,690 --> 00:32:01,270
I think the missing thing may be you

00:31:59,890 --> 00:32:03,220
know just putting it through the ringer

00:32:01,270 --> 00:32:05,590
of a real use case and see how it scales

00:32:03,220 --> 00:32:07,720
and then also we're working through the

00:32:05,590 --> 00:32:09,310
exercise of hooking up from EPS to a lot

00:32:07,720 --> 00:32:13,240
of these as well to make sure we can

00:32:09,310 --> 00:32:16,210
monitor the system so I think really it

00:32:13,240 --> 00:32:17,650
seems to be in a final finally in more

00:32:16,210 --> 00:32:19,120
of a stable State this has been the

00:32:17,650 --> 00:32:21,490
development on that's been going on for

00:32:19,120 --> 00:32:23,380
a year now so I think it's starting to

00:32:21,490 --> 00:32:26,070
level off and you get a little bit more

00:32:23,380 --> 00:32:26,070
stability there

00:32:26,170 --> 00:32:33,430
[Music]

00:32:30,280 --> 00:32:35,800
okay thank you Sarah amazing thank you

00:32:33,430 --> 00:32:38,650
thank you

00:32:35,800 --> 00:32:41,880
[Music]

00:32:38,650 --> 00:32:41,880
[Applause]

00:32:44,820 --> 00:32:47,820
yeah

00:32:49,200 --> 00:32:54,660
Thank You mr. reminder to everyone the

00:32:51,930 --> 00:32:57,660
party is tonight it is in the Siskind

00:32:54,660 --> 00:33:03,060
lounge is it is at 7:00 p.m. it is going

00:32:57,660 --> 00:33:05,940
to be the funnest party ever . so please

00:33:03,060 --> 00:33:07,790
do show up we actually have lots of fun

00:33:05,940 --> 00:33:09,930
things I'm not making that up

00:33:07,790 --> 00:33:11,150
Chloe's on the fun committee she could

00:33:09,930 --> 00:33:14,040
tell you more if you want to ask her

00:33:11,150 --> 00:33:16,770
anyway so please do turn up for that and

00:33:14,040 --> 00:33:19,880
also do not forget the keynote tomorrow

00:33:16,770 --> 00:33:24,990
morning which is with Chris Wright and

00:33:19,880 --> 00:33:26,940
Surin what is for Less my mind is jelly

00:33:24,990 --> 00:33:30,050
anyway it's gonna be really good so turn

00:33:26,940 --> 00:33:30,050

YouTube URL: https://www.youtube.com/watch?v=B6E7SyxOB2M


