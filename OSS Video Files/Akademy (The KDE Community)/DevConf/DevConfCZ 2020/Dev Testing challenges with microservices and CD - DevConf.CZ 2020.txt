Title: Dev Testing challenges with microservices and CD - DevConf.CZ 2020
Publication date: 2020-03-27
Playlist: DevConfCZ 2020
Description: 
	Speakers: Mark Turansky, Abhishek Gupta

With a modern-day service, there is ever-increasing pressure to release early and often. The situation is further exacerbated with microservices as these are deployed independently. Traditional quality assurance processes cannot cope with this high degree of agility as there is no time for testing to be “out-of-phase” with development. Given this, developer and QE roles have to be fundamentally reimagined.

In this talk, we’ll go over similar challenges that we faced while building OpenShift Cluster Manager, a microservices based SaaS offering (deployed on top of OpenShift) at Red Hat. We’ll cover the incremental process changes and approaches taken to drive test automation and close collaboration between development and QE teams. The talk does not intend to be prescriptive and instead aims to highlight challenges faced as well as lessons learned along the way that can be helpful for any development or QE team, especially those deploying their applications/services on OpenShift.

[ https://sched.co/YOuN ]

--
Recordings of talks at DevConf are a community effort. Unfortunately not everything works perfectly every time. If you're interested in helping us improve, let us know.
Captions: 
	00:00:00,880 --> 00:00:05,350
hello everyone so introductions I

00:00:03,850 --> 00:00:07,480
believe already have been made hi I'm

00:00:05,350 --> 00:00:09,519
Abbi shake and presenting with me today

00:00:07,480 --> 00:00:12,910
is my colleague mark and we are going to

00:00:09,519 --> 00:00:15,429
talk about the testing challenges that

00:00:12,910 --> 00:00:20,980
we face with micro services and

00:00:15,429 --> 00:00:24,070
continuous deployments so we give a few

00:00:20,980 --> 00:00:27,880
minutes for people to settle in but to

00:00:24,070 --> 00:00:29,679
set some context why are we here so mark

00:00:27,880 --> 00:00:31,869
and I are part of a team at red heart

00:00:29,679 --> 00:00:35,829
that is focused on building deploying

00:00:31,869 --> 00:00:38,860
managing service a cloud-based external

00:00:35,829 --> 00:00:40,360
service and if you guys attended the

00:00:38,860 --> 00:00:43,270
keynote session earlier in the year

00:00:40,360 --> 00:00:46,890
today we talked about the SD org the

00:00:43,270 --> 00:00:49,539
service delivery organization building

00:00:46,890 --> 00:00:50,620
public facing service as part of

00:00:49,539 --> 00:00:52,180
cloud.com

00:00:50,620 --> 00:00:53,980
and we are one of the teams that is

00:00:52,180 --> 00:00:56,800
building one of those services behind

00:00:53,980 --> 00:00:59,230
that and as part of building that

00:00:56,800 --> 00:01:00,370
servers using micro services and you

00:00:59,230 --> 00:01:02,429
know trying to get to continuous

00:01:00,370 --> 00:01:05,080
deployments we faced several challenges

00:01:02,429 --> 00:01:07,689
came across certain hurdles and we are

00:01:05,080 --> 00:01:09,670
here to share some of those challenges

00:01:07,689 --> 00:01:14,140
and share some of our experience over

00:01:09,670 --> 00:01:15,850
with you in terms of the agenda we'll

00:01:14,140 --> 00:01:18,189
start off with looking at some of the

00:01:15,850 --> 00:01:21,220
software development models the testing

00:01:18,189 --> 00:01:23,140
challenges that are inherent to them and

00:01:21,220 --> 00:01:25,060
how micro services and continuous

00:01:23,140 --> 00:01:27,700
deployments play a role and exacerbate

00:01:25,060 --> 00:01:29,740
those situations in some cases we'll

00:01:27,700 --> 00:01:34,390
take a step back and take a broader look

00:01:29,740 --> 00:01:36,280
at the goals around quality assurance

00:01:34,390 --> 00:01:39,460
that we had for our teams for our

00:01:36,280 --> 00:01:41,890
services in particular and then go look

00:01:39,460 --> 00:01:43,570
at some of the dev QE collaboration and

00:01:41,890 --> 00:01:46,899
the process changes that we had to make

00:01:43,570 --> 00:01:49,570
in order to achieve our goals around

00:01:46,899 --> 00:01:52,689
quality finally we will take a look at

00:01:49,570 --> 00:01:54,610
the test automation and the tooling that

00:01:52,689 --> 00:01:56,469
we had to develop in order to achieve

00:01:54,610 --> 00:02:02,500
these goals and we'll wrap up with a

00:01:56,469 --> 00:02:04,240
demo so if you look at the traditional

00:02:02,500 --> 00:02:07,210
software development models one of the

00:02:04,240 --> 00:02:09,550
common ones waterfall over here the

00:02:07,210 --> 00:02:11,590
phases are sequential so we move from

00:02:09,550 --> 00:02:13,900
one page to another in sequence and the

00:02:11,590 --> 00:02:16,840
goal is to make sure that

00:02:13,900 --> 00:02:20,049
the requirements are clearly defined and

00:02:16,840 --> 00:02:22,360
the design is fully kneeled down before

00:02:20,049 --> 00:02:26,040
you move on to your development and

00:02:22,360 --> 00:02:26,040
testing phases and so on

00:02:26,110 --> 00:02:33,970
with prototyping we use development of

00:02:31,360 --> 00:02:36,579
the prototype and evolve the prototype

00:02:33,970 --> 00:02:39,959
in order to help clarify and define and

00:02:36,579 --> 00:02:42,310
evolve the requirements spiral is

00:02:39,959 --> 00:02:44,049
something similar to prototyping with

00:02:42,310 --> 00:02:46,780
some additional phases around risk

00:02:44,049 --> 00:02:48,879
assessment evaluation and so on but the

00:02:46,780 --> 00:02:51,940
general idea is the same in all of these

00:02:48,879 --> 00:02:53,470
models the goal is to get the software

00:02:51,940 --> 00:02:55,420
being developed the functionality being

00:02:53,470 --> 00:02:58,810
produced more aligned with the

00:02:55,420 --> 00:03:01,480
requirements and the focus is not so

00:02:58,810 --> 00:03:06,129
much on getting development and QE more

00:03:01,480 --> 00:03:08,650
aligned with regards to agile it is

00:03:06,129 --> 00:03:10,989
designed by its nature to get the

00:03:08,650 --> 00:03:13,510
development and testing phases a little

00:03:10,989 --> 00:03:16,780
closer to each other in the best-case

00:03:13,510 --> 00:03:19,000
scenario still you can have software

00:03:16,780 --> 00:03:20,799
being released every sprint and in the

00:03:19,000 --> 00:03:23,380
worst case scenario you can expect

00:03:20,799 --> 00:03:26,500
anything from one to three for more

00:03:23,380 --> 00:03:29,859
stabilization code encode Sprint's

00:03:26,500 --> 00:03:33,190
before software is released in practice

00:03:29,859 --> 00:03:34,930
what we saw was even if he had the in

00:03:33,190 --> 00:03:36,609
the best-case scenario

00:03:34,930 --> 00:03:38,440
software being released at the end of

00:03:36,609 --> 00:03:41,440
the Sprint we had three weeks prints and

00:03:38,440 --> 00:03:43,599
for the first seven or so working days

00:03:41,440 --> 00:03:48,280
within the sprint you would have the

00:03:43,599 --> 00:03:49,720
design and development followed by the

00:03:48,280 --> 00:03:53,199
remaining portion of the sprint taken up

00:03:49,720 --> 00:03:56,379
by testing bug fixes stabilization and

00:03:53,199 --> 00:03:58,959
hopefully a release at the end the focus

00:03:56,379 --> 00:04:00,940
of agile typically is around iterative

00:03:58,959 --> 00:04:04,269
deployment development and deployment

00:04:00,940 --> 00:04:06,069
where your requirements cannot be nailed

00:04:04,269 --> 00:04:08,590
down for the next six to eight months a

00:04:06,069 --> 00:04:10,930
year and you want to allow your

00:04:08,590 --> 00:04:15,190
development teams to pivot in case of

00:04:10,930 --> 00:04:18,579
changes in requirements so given this

00:04:15,190 --> 00:04:20,650
out of phase nature of development and

00:04:18,579 --> 00:04:23,890
testing where development goes in first

00:04:20,650 --> 00:04:25,630
followed by testing and hence the outer

00:04:23,890 --> 00:04:27,400
phase nature what are some of the

00:04:25,630 --> 00:04:29,680
challenges that

00:04:27,400 --> 00:04:31,660
from this one there's increased

00:04:29,680 --> 00:04:33,940
turnaround time for testing and

00:04:31,660 --> 00:04:37,570
verification because they're out of

00:04:33,940 --> 00:04:40,870
phase this is especially true in case of

00:04:37,570 --> 00:04:44,500
or other exacerbated in case of globally

00:04:40,870 --> 00:04:47,139
spread out teams where in our case

00:04:44,500 --> 00:04:49,120
specifically we have development as well

00:04:47,139 --> 00:04:52,479
as Kiwi teams spread all across the

00:04:49,120 --> 00:04:56,770
world in North America amia a pack all

00:04:52,479 --> 00:04:59,800
over we have issues around effort

00:04:56,770 --> 00:05:03,970
duplication with test automation so in

00:04:59,800 --> 00:05:06,940
our case development teams did their own

00:05:03,970 --> 00:05:09,729
test automation around unit tests some

00:05:06,940 --> 00:05:11,470
integration testing QE teams build their

00:05:09,729 --> 00:05:14,860
own test cases own test automation

00:05:11,470 --> 00:05:17,260
around integration tests and system

00:05:14,860 --> 00:05:20,020
tests e to e test UI testing anything

00:05:17,260 --> 00:05:23,590
but they both existed in their own

00:05:20,020 --> 00:05:26,770
separate little worlds and completely

00:05:23,590 --> 00:05:29,199
duplicated work all of this results in

00:05:26,770 --> 00:05:30,849
higher turnaround time if we want to for

00:05:29,199 --> 00:05:32,440
software updates and releases if you

00:05:30,849 --> 00:05:36,460
want to wait for full automation and

00:05:32,440 --> 00:05:39,220
full Quality Assurance testing and in

00:05:36,460 --> 00:05:42,370
cases when we are faced with tighter

00:05:39,220 --> 00:05:46,810
deadlines pressure to release early we

00:05:42,370 --> 00:05:48,520
run the risk of reason without proper

00:05:46,810 --> 00:05:55,060
testing and running the risk of

00:05:48,520 --> 00:05:58,659
introducing bugs as a result so now how

00:05:55,060 --> 00:06:01,780
does micro services play a role in all

00:05:58,659 --> 00:06:03,789
of this well micro services can help

00:06:01,780 --> 00:06:05,560
your development teams get more

00:06:03,789 --> 00:06:07,990
productive that is because your

00:06:05,560 --> 00:06:10,300
development teams individuals or smaller

00:06:07,990 --> 00:06:11,699
teams are focused on smaller chunks of

00:06:10,300 --> 00:06:14,560
functionality each micro service

00:06:11,699 --> 00:06:17,199
encompasses you know smaller chunks of

00:06:14,560 --> 00:06:20,949
functionality and it becomes easier to

00:06:17,199 --> 00:06:23,440
get development done to introduce new

00:06:20,949 --> 00:06:25,380
features to make changes and even

00:06:23,440 --> 00:06:28,330
onboard new members into your team

00:06:25,380 --> 00:06:30,099
because the surface area for adoption

00:06:28,330 --> 00:06:34,210
for anybody coming out to the team to

00:06:30,099 --> 00:06:36,250
make changes is small but with each

00:06:34,210 --> 00:06:38,800
micro service you also get more moving

00:06:36,250 --> 00:06:40,600
parts now you don't have one big

00:06:38,800 --> 00:06:42,340
monolith you have ten

00:06:40,600 --> 00:06:43,660
fifteen you know any number of

00:06:42,340 --> 00:06:46,770
microservices and all of these are

00:06:43,660 --> 00:06:49,540
moving parts each of these microservices

00:06:46,770 --> 00:06:52,630
hydrating on their own schedule creating

00:06:49,540 --> 00:06:55,120
more testing overhead so well if there's

00:06:52,630 --> 00:06:57,250
more testing overhead what about

00:06:55,120 --> 00:06:59,020
continuous deployments right I mean the

00:06:57,250 --> 00:07:01,360
whole premise of micro services was

00:06:59,020 --> 00:07:03,790
faster development faster deployments

00:07:01,360 --> 00:07:07,990
and eventually continuous deployments so

00:07:03,790 --> 00:07:11,410
what about that each service in a micro

00:07:07,990 --> 00:07:13,210
service while it needs to be tested

00:07:11,410 --> 00:07:16,360
independently it also needs to be tested

00:07:13,210 --> 00:07:20,110
in an integrated fashion so you can't

00:07:16,360 --> 00:07:22,390
just get away from testing just a single

00:07:20,110 --> 00:07:24,490
micro service because it has touch

00:07:22,390 --> 00:07:27,070
points with every other micro service or

00:07:24,490 --> 00:07:30,700
many other micro services within your

00:07:27,070 --> 00:07:32,260
overall application or service and all

00:07:30,700 --> 00:07:34,870
of those need to be tested in

00:07:32,260 --> 00:07:36,340
conjunction as a unit in your pre prod

00:07:34,870 --> 00:07:39,010
environments and equation staging

00:07:36,340 --> 00:07:40,890
whatnots before you can actually release

00:07:39,010 --> 00:07:43,570
that particular micro service so

00:07:40,890 --> 00:07:46,110
consequently what we see is some of the

00:07:43,570 --> 00:07:51,910
benefits that mean you're promised of

00:07:46,110 --> 00:07:53,910
micro services are not fully realized so

00:07:51,910 --> 00:07:56,980
what are some of the possible options

00:07:53,910 --> 00:08:00,850
can be aim for full automation testing

00:07:56,980 --> 00:08:02,820
well that's a good idea but it takes

00:08:00,850 --> 00:08:05,170
time and doesn't happen on day one

00:08:02,820 --> 00:08:06,910
especially when you're building and

00:08:05,170 --> 00:08:10,660
starting from scratch a new service

00:08:06,910 --> 00:08:13,840
based on micro services the general idea

00:08:10,660 --> 00:08:16,450
is to first build and then focus on

00:08:13,840 --> 00:08:18,070
quality especially in case of when you

00:08:16,450 --> 00:08:19,780
know you're just getting started your

00:08:18,070 --> 00:08:22,150
first target is to have some

00:08:19,780 --> 00:08:23,890
functionality and put in place go for

00:08:22,150 --> 00:08:26,500
alpha testing go for beta testing with

00:08:23,890 --> 00:08:29,350
friendlies with internal users quality

00:08:26,500 --> 00:08:33,490
is not a pressure not a focus from day

00:08:29,350 --> 00:08:35,800
one and even if you specifically make

00:08:33,490 --> 00:08:38,200
quality and full automation a focus on

00:08:35,800 --> 00:08:40,570
day one that will just get you delayed

00:08:38,200 --> 00:08:43,090
releases you know missing deadlines

00:08:40,570 --> 00:08:46,330
risking go to market and so on and so

00:08:43,090 --> 00:08:48,850
forth well so should developers take

00:08:46,330 --> 00:08:51,490
over testing well developers are already

00:08:48,850 --> 00:08:53,740
implementing unit tests and other forms

00:08:51,490 --> 00:08:54,430
of in equation or machine testing can

00:08:53,740 --> 00:08:56,980
they

00:08:54,430 --> 00:09:00,520
you know pick up the full slack and you

00:08:56,980 --> 00:09:04,540
know just combine QE and developers and

00:09:00,520 --> 00:09:07,450
have one single common team well there's

00:09:04,540 --> 00:09:09,399
a reason why developers don't review our

00:09:07,450 --> 00:09:12,580
own code and there's a reason why we are

00:09:09,399 --> 00:09:14,470
not good at finding our own bugs no

00:09:12,580 --> 00:09:16,930
matter how good of a code you write no

00:09:14,470 --> 00:09:19,360
matter how many unit tests and automated

00:09:16,930 --> 00:09:22,060
tests you write you put the code in

00:09:19,360 --> 00:09:24,550
front of QE they will find bugs so that

00:09:22,060 --> 00:09:26,740
is certainly especially true you know

00:09:24,550 --> 00:09:29,589
it's in a shower or to our own QE teams

00:09:26,740 --> 00:09:30,790
you know these folks are awesome you put

00:09:29,589 --> 00:09:33,490
something in front of them they will

00:09:30,790 --> 00:09:36,070
find bugs so the general idea here is

00:09:33,490 --> 00:09:38,140
you shouldn't throw away the years and

00:09:36,070 --> 00:09:40,720
years of QE expertise that various teams

00:09:38,140 --> 00:09:44,230
have built and we should still try to

00:09:40,720 --> 00:09:48,550
leverage that so how do we leverage the

00:09:44,230 --> 00:09:52,500
QA expertise but still get them more in

00:09:48,550 --> 00:09:52,500
phase with the development process so

00:09:54,330 --> 00:09:59,350
when faced with this particular question

00:09:56,740 --> 00:10:01,720
at our end we took a step back and

00:09:59,350 --> 00:10:04,270
looked at some of the broader goals that

00:10:01,720 --> 00:10:07,660
we had around Quality Assurance for our

00:10:04,270 --> 00:10:09,070
own service and we came up with four

00:10:07,660 --> 00:10:12,790
different you know high-level goals

00:10:09,070 --> 00:10:15,520
nothing groundbreaking just common stuff

00:10:12,790 --> 00:10:18,700
one we wanted to improve the quality of

00:10:15,520 --> 00:10:20,740
the code being pushed to we wanted to

00:10:18,700 --> 00:10:22,450
improve the reliability of the service

00:10:20,740 --> 00:10:25,779
that is deployed in production at any

00:10:22,450 --> 00:10:28,089
given point in time 3 we wanted to

00:10:25,779 --> 00:10:30,850
enable Feig frequent and faster

00:10:28,089 --> 00:10:34,900
deployments and lastly we wanted to

00:10:30,850 --> 00:10:39,790
reduce the effort duplication between

00:10:34,900 --> 00:10:41,380
our development and QA teams so we now

00:10:39,790 --> 00:10:44,260
have these goals how do we achieve them

00:10:41,380 --> 00:10:46,360
so we identified four different areas or

00:10:44,260 --> 00:10:48,579
aspects of called assurance as we call

00:10:46,360 --> 00:10:55,660
them where we wanted to focus where we

00:10:48,579 --> 00:10:57,040
wanted to you know well essentially just

00:10:55,660 --> 00:10:58,600
you know where we wanted to focus and

00:10:57,040 --> 00:11:01,029
spend our energy on in order to which

00:10:58,600 --> 00:11:04,420
help us achieve some of these goals

00:11:01,029 --> 00:11:06,910
first off defining code quality

00:11:04,420 --> 00:11:08,410
guidelines and best practices second

00:11:06,910 --> 00:11:10,899
automation testing focusing

00:11:08,410 --> 00:11:14,319
on more pre merged post merge or mission

00:11:10,899 --> 00:11:15,970
testing third production testing thus

00:11:14,319 --> 00:11:17,800
testing the service that we have in

00:11:15,970 --> 00:11:19,839
production and monitoring that and

00:11:17,800 --> 00:11:25,689
lastly buck triage and we'll go over

00:11:19,839 --> 00:11:27,839
each of these areas in detail next so

00:11:25,689 --> 00:11:30,639
what do we mean when we talk about

00:11:27,839 --> 00:11:32,980
coding standards and best practices over

00:11:30,639 --> 00:11:35,740
here the focus is not so much on the

00:11:32,980 --> 00:11:38,439
functionality and the accuracy of the

00:11:35,740 --> 00:11:40,810
code being pushed but more on the

00:11:38,439 --> 00:11:44,079
quality of the code being pushed and

00:11:40,810 --> 00:11:46,240
this talks more about things like code

00:11:44,079 --> 00:11:48,459
format the code structure so in our

00:11:46,240 --> 00:11:51,220
cases our micro services were based on

00:11:48,459 --> 00:11:55,269
golang and you know some of the things

00:11:51,220 --> 00:11:56,920
that we can use to help us with this

00:11:55,269 --> 00:11:59,079
particular area are things like go

00:11:56,920 --> 00:12:02,290
format or goal and you know some of the

00:11:59,079 --> 00:12:05,410
linting tools next we can look at some

00:12:02,290 --> 00:12:08,290
of the language specific guidelines

00:12:05,410 --> 00:12:10,329
you know isms in various languages for

00:12:08,290 --> 00:12:13,959
example goo isms or team project

00:12:10,329 --> 00:12:18,100
standards and best practices for writing

00:12:13,959 --> 00:12:21,430
code and so on and so forth next up is

00:12:18,100 --> 00:12:23,800
around code reviews and test execution

00:12:21,430 --> 00:12:25,389
so we wanted to make sure that we have

00:12:23,800 --> 00:12:27,850
two lengths and we build the tooling

00:12:25,389 --> 00:12:29,439
required in order to conduct reviews

00:12:27,850 --> 00:12:31,810
make sure reviews are happening make

00:12:29,439 --> 00:12:34,500
sure reviews are happening our other

00:12:31,810 --> 00:12:38,920
reviews are being done by the people

00:12:34,500 --> 00:12:40,750
that are that have that domain expertise

00:12:38,920 --> 00:12:43,269
or that functional knowledge in that

00:12:40,750 --> 00:12:45,939
particular area and then the tests are

00:12:43,269 --> 00:12:47,709
being executed via CI and tests are

00:12:45,939 --> 00:12:49,899
passing before a pull request can be

00:12:47,709 --> 00:12:54,459
merged and making sure that that always

00:12:49,899 --> 00:12:56,069
happens and is not violated lastly we

00:12:54,459 --> 00:12:59,079
want to focus on code coverage

00:12:56,069 --> 00:13:00,970
specifically around a measuring code

00:12:59,079 --> 00:13:04,360
coverage and be making sure that with

00:13:00,970 --> 00:13:06,250
each pull request we increase or

00:13:04,360 --> 00:13:10,709
maintain our level of code coverage and

00:13:06,250 --> 00:13:10,709
we don't regress from that perspective

00:13:11,069 --> 00:13:16,779
next up was automation testing and you

00:13:14,559 --> 00:13:20,110
know just divvy it up into two different

00:13:16,779 --> 00:13:22,080
categories pre-emergent post-merge pre

00:13:20,110 --> 00:13:24,180
merge executed against

00:13:22,080 --> 00:13:27,720
each full requests and targeting things

00:13:24,180 --> 00:13:30,180
like unit tests functional in component

00:13:27,720 --> 00:13:33,420
testing API contract testing and

00:13:30,180 --> 00:13:35,580
integration testing pre merge possibly

00:13:33,420 --> 00:13:39,900
with some other dependencies mocked for

00:13:35,580 --> 00:13:42,570
that post merge would be things where

00:13:39,900 --> 00:13:44,850
the testing is done in a pre prod

00:13:42,570 --> 00:13:47,900
environment like in equation or QE or

00:13:44,850 --> 00:13:50,070
staging whatever you might have and

00:13:47,900 --> 00:13:51,830
targeting testing like integration

00:13:50,070 --> 00:13:56,640
testing into n testing system testing

00:13:51,830 --> 00:13:58,740
performance UI testing and so forth the

00:13:56,640 --> 00:14:01,170
third focus area that we had that we

00:13:58,740 --> 00:14:03,570
identified was around production testing

00:14:01,170 --> 00:14:05,880
and monitoring so what we wanted to do

00:14:03,570 --> 00:14:08,310
was make sure that we conduct regular

00:14:05,880 --> 00:14:10,920
testing against the service that we have

00:14:08,310 --> 00:14:12,930
deployed in production and conduct

00:14:10,920 --> 00:14:14,520
things like smoke testing so it doesn't

00:14:12,930 --> 00:14:16,980
need to be full regression testing

00:14:14,520 --> 00:14:20,010
full-blown thorough testing but rather a

00:14:16,980 --> 00:14:23,520
subset of your functionality that you

00:14:20,010 --> 00:14:26,100
have and test we have smoke testing this

00:14:23,520 --> 00:14:27,630
would help ensure the stability and the

00:14:26,100 --> 00:14:30,420
reliability of the service that we have

00:14:27,630 --> 00:14:32,970
deployed in production and can help as

00:14:30,420 --> 00:14:35,190
you can imagine identify and even

00:14:32,970 --> 00:14:39,510
potentially fix some of the bugs before

00:14:35,190 --> 00:14:43,830
users ever see them in some rare cases

00:14:39,510 --> 00:14:46,380
it also helps catch changes to some of

00:14:43,830 --> 00:14:48,150
the functionality in external services

00:14:46,380 --> 00:14:50,220
that you may be integrated with which

00:14:48,150 --> 00:14:51,840
you do not have control over you may not

00:14:50,220 --> 00:14:53,310
have a corresponding staging environment

00:14:51,840 --> 00:14:58,620
for those external vendor managed

00:14:53,310 --> 00:15:01,220
services things like that with regards

00:14:58,620 --> 00:15:04,860
to monitoring we identify the need to

00:15:01,220 --> 00:15:07,170
monitor for errors that any end-user

00:15:04,860 --> 00:15:11,250
interacting with your service we our API

00:15:07,170 --> 00:15:13,950
is CLI UI whichever way if they run into

00:15:11,250 --> 00:15:16,770
some errors accessing your functionality

00:15:13,950 --> 00:15:20,280
those errors need to be logged and be

00:15:16,770 --> 00:15:22,700
accessible to you for analysis and

00:15:20,280 --> 00:15:25,440
helping fix bugs and you identify them

00:15:22,700 --> 00:15:27,240
in this particular case you know there

00:15:25,440 --> 00:15:30,660
are n number of arrow monitoring tools

00:15:27,240 --> 00:15:32,490
that you can use like new lax entry and

00:15:30,660 --> 00:15:35,790
we use something on those lines as well

00:15:32,490 --> 00:15:38,819
and define thresholds

00:15:35,790 --> 00:15:41,309
so that the development team or Sree

00:15:38,819 --> 00:15:45,209
whoever can be alerted in case of errors

00:15:41,309 --> 00:15:49,019
and issues that actual users are hitting

00:15:45,209 --> 00:15:52,199
and we can help debug them and fix them

00:15:49,019 --> 00:15:56,699
before actual end users even report

00:15:52,199 --> 00:15:59,790
those issues on their own last focus

00:15:56,699 --> 00:16:03,389
area was around blog triage bugs can be

00:15:59,790 --> 00:16:05,519
identified by end users by a monitoring

00:16:03,389 --> 00:16:08,720
tools no matter which way bugs are

00:16:05,519 --> 00:16:12,509
identified one thing is fairly common

00:16:08,720 --> 00:16:14,759
we've all come across the random bug

00:16:12,509 --> 00:16:17,129
filed by end users where we'll this is

00:16:14,759 --> 00:16:19,350
not working okay we don't have any

00:16:17,129 --> 00:16:23,579
detail so you know lacking essential key

00:16:19,350 --> 00:16:25,350
details and the objective of barrage is

00:16:23,579 --> 00:16:28,259
to make sure that a we make sure that

00:16:25,350 --> 00:16:31,290
the bug is valid we reproduce it we add

00:16:28,259 --> 00:16:33,389
key details to aid we're debugging and

00:16:31,290 --> 00:16:37,350
fixing the bug and all of this is to

00:16:33,389 --> 00:16:40,079
help ensure that the bugs can be debug

00:16:37,350 --> 00:16:44,660
and fixed as efficiently as possible and

00:16:40,079 --> 00:16:48,480
as quickly as possible so given these

00:16:44,660 --> 00:16:54,299
goals what were some of the changes that

00:16:48,480 --> 00:16:56,220
were required at our end so first off we

00:16:54,299 --> 00:17:00,199
take a look at some of the process

00:16:56,220 --> 00:17:02,490
changes so we had our Devon QE

00:17:00,199 --> 00:17:05,549
collaboration as one of the main focus

00:17:02,490 --> 00:17:06,870
areas for identifying and executing some

00:17:05,549 --> 00:17:09,510
of the process changes and putting in

00:17:06,870 --> 00:17:11,549
some process changes in place to help

00:17:09,510 --> 00:17:16,039
overcome some of that out of phase

00:17:11,549 --> 00:17:18,870
nature of development in testing so

00:17:16,039 --> 00:17:21,809
previously you used to get involved in

00:17:18,870 --> 00:17:24,990
the planning phase in a particular

00:17:21,809 --> 00:17:26,939
sprint with the new process we are

00:17:24,990 --> 00:17:30,960
getting them involved in the grooming

00:17:26,939 --> 00:17:33,480
phase so that before a story or before

00:17:30,960 --> 00:17:36,149
some work is picked up in a strain he

00:17:33,480 --> 00:17:40,409
gets the heads up and you know plans

00:17:36,149 --> 00:17:42,710
accordingly around a understanding what

00:17:40,409 --> 00:17:47,580
the functionality is and being ready for

00:17:42,710 --> 00:17:49,230
building the test automation for that -

00:17:47,580 --> 00:17:52,380
previously QE

00:17:49,230 --> 00:17:54,480
would create the test cases for stories

00:17:52,380 --> 00:17:56,520
that were added to a sprint so that

00:17:54,480 --> 00:17:59,850
remains the same but additionally what

00:17:56,520 --> 00:18:02,040
they do now is break up the stories in

00:17:59,850 --> 00:18:04,830
the new process into pre merge and post

00:18:02,040 --> 00:18:06,750
merge buckets and even for post merge

00:18:04,830 --> 00:18:08,130
things like integration testing system

00:18:06,750 --> 00:18:09,690
testing you are testing performance

00:18:08,130 --> 00:18:12,150
testing you know break up the stories

00:18:09,690 --> 00:18:13,950
into different buckets or the test case

00:18:12,150 --> 00:18:16,440
into different buckets so that they can

00:18:13,950 --> 00:18:20,790
be automated appropriately and

00:18:16,440 --> 00:18:23,520
accordingly developers previously would

00:18:20,790 --> 00:18:26,040
implement unit tests some component

00:18:23,520 --> 00:18:27,840
tests some integration tests but they

00:18:26,040 --> 00:18:29,179
would do so based on test cases that

00:18:27,840 --> 00:18:32,340
they would self-identify

00:18:29,179 --> 00:18:37,710
there was no input being taken from QE

00:18:32,340 --> 00:18:39,360
in the new process while QE is given the

00:18:37,710 --> 00:18:41,760
responsibility of defining the test

00:18:39,360 --> 00:18:45,030
cases and development team collaborates

00:18:41,760 --> 00:18:47,669
with the QA teams and they jointly work

00:18:45,030 --> 00:18:49,860
on automating the same set the same

00:18:47,669 --> 00:18:51,590
repository of test cases and create a

00:18:49,860 --> 00:18:56,160
common repository of test automation

00:18:51,590 --> 00:18:59,400
against those previously QE would start

00:18:56,160 --> 00:19:02,640
their test automation activities whether

00:18:59,400 --> 00:19:05,220
it is end-to-end a system test once the

00:19:02,640 --> 00:19:06,690
feature is developed and it's deployed

00:19:05,220 --> 00:19:10,980
in a pre prod environment

00:19:06,690 --> 00:19:13,710
now given that some of that manual

00:19:10,980 --> 00:19:16,980
testing is taken away from them because

00:19:13,710 --> 00:19:20,250
the ruffman team is helping implement

00:19:16,980 --> 00:19:23,549
some of those automation as well the QE

00:19:20,250 --> 00:19:25,020
team is freed up relatively to pick up

00:19:23,549 --> 00:19:26,700
some of that test automation around

00:19:25,020 --> 00:19:29,640
system testing and - in testing

00:19:26,700 --> 00:19:32,070
alongside feature development especially

00:19:29,640 --> 00:19:33,690
if your functionality is primarily a PR

00:19:32,070 --> 00:19:37,250
driven if your console is making API

00:19:33,690 --> 00:19:40,230
calls rather than not it makes it easier

00:19:37,250 --> 00:19:44,520
once your APS are defined to start

00:19:40,230 --> 00:19:45,929
building automation around that next up

00:19:44,520 --> 00:19:47,970
we took a look at backwards

00:19:45,929 --> 00:19:51,990
compatibility you know this does not

00:19:47,970 --> 00:19:54,510
necessarily tie into your def QE

00:19:51,990 --> 00:19:57,720
collaboration but helps play a major

00:19:54,510 --> 00:20:01,890
role in achieve continuous deployments

00:19:57,720 --> 00:20:03,110
faster deployments so API changes need

00:20:01,890 --> 00:20:07,260
to be backwards compatible

00:20:03,110 --> 00:20:10,260
that's obvious this allows not only your

00:20:07,260 --> 00:20:12,690
UI and your CLI and your clients but

00:20:10,260 --> 00:20:15,120
also your other services within the

00:20:12,690 --> 00:20:17,490
micro services framework that are

00:20:15,120 --> 00:20:19,410
consuming your API and your services to

00:20:17,490 --> 00:20:23,400
catch up to the changes that you are

00:20:19,410 --> 00:20:25,620
pushing so that you can more effectively

00:20:23,400 --> 00:20:28,500
and independently deploy your own

00:20:25,620 --> 00:20:31,980
changes without causing regression and

00:20:28,500 --> 00:20:36,840
issues in other services and essentially

00:20:31,980 --> 00:20:39,600
your application also this avoids

00:20:36,840 --> 00:20:43,170
getting into the whole coordinated

00:20:39,600 --> 00:20:44,970
releases thing where effectively you are

00:20:43,170 --> 00:20:47,010
negating the benefits of micro services

00:20:44,970 --> 00:20:49,890
and getting back to more of a monolith

00:20:47,010 --> 00:20:52,430
by focusing on coordinate releases of

00:20:49,890 --> 00:20:54,870
all your micro services at the same time

00:20:52,430 --> 00:20:57,240
second we also took a look at some of

00:20:54,870 --> 00:21:00,030
the database changes with regards to

00:20:57,240 --> 00:21:02,340
schema changes and so forth and we

00:21:00,030 --> 00:21:07,500
identified the need in some cases to

00:21:02,340 --> 00:21:10,590
adopt a two-phase migration for our

00:21:07,500 --> 00:21:12,590
database changes so for example when

00:21:10,590 --> 00:21:16,680
adding a new column to a database table

00:21:12,590 --> 00:21:18,590
you can in phase one introduce that

00:21:16,680 --> 00:21:21,330
particular column and make that nullable

00:21:18,590 --> 00:21:24,900
optional so that you're consuming

00:21:21,330 --> 00:21:27,270
service can actually continue to you

00:21:24,900 --> 00:21:29,850
know not pass that particular field and

00:21:27,270 --> 00:21:33,030
not fail and then when the code has

00:21:29,850 --> 00:21:35,970
caught up make that particular field

00:21:33,030 --> 00:21:39,570
required one make it you know you know

00:21:35,970 --> 00:21:43,860
phase two migration so what this ensures

00:21:39,570 --> 00:21:46,500
is special.if occur in our case that we

00:21:43,860 --> 00:21:48,420
can leverage in the face of DB

00:21:46,500 --> 00:21:52,260
migrations we can still continue to do

00:21:48,420 --> 00:21:54,360
rolling upgrades rolling releases of our

00:21:52,260 --> 00:21:57,330
service in a scaled-up environment

00:21:54,360 --> 00:21:59,540
without taking downtime lastly we

00:21:57,330 --> 00:22:03,090
reviewed some of our background jobs and

00:21:59,540 --> 00:22:05,100
in our case you know we have quite a few

00:22:03,090 --> 00:22:08,670
background jobs that are scheduled at

00:22:05,100 --> 00:22:12,540
different points in of time of the day

00:22:08,670 --> 00:22:14,640
and it's not feasible really for us to

00:22:12,540 --> 00:22:16,620
make sure that our software releases and

00:22:14,640 --> 00:22:16,830
updates happen at a time when there are

00:22:16,620 --> 00:22:18,690
no

00:22:16,830 --> 00:22:20,430
background jobs running so we need to

00:22:18,690 --> 00:22:23,690
make sure that background jobs can a

00:22:20,430 --> 00:22:26,280
handle the underlying software service

00:22:23,690 --> 00:22:30,060
changing and updating underneath them

00:22:26,280 --> 00:22:35,630
and either fail fast or continue to work

00:22:30,060 --> 00:22:38,850
as expected next up we looked at

00:22:35,630 --> 00:22:42,410
automation around promotion of the

00:22:38,850 --> 00:22:45,150
service from into stage stage to prod

00:22:42,410 --> 00:22:47,640
now even if we don't go there and we

00:22:45,150 --> 00:22:50,460
certainly aren't there yet with regards

00:22:47,640 --> 00:22:53,220
to automated promotion thinking about

00:22:50,460 --> 00:22:54,870
this a particular aspect helps ensure

00:22:53,220 --> 00:22:56,730
that you're you know any changes that

00:22:54,870 --> 00:22:59,220
you're making to a product to your

00:22:56,730 --> 00:23:01,590
process to your tooling is in the right

00:22:59,220 --> 00:23:03,810
direction and definitely helpful

00:23:01,590 --> 00:23:06,780
making sure continuous deployments

00:23:03,810 --> 00:23:08,940
happen more efficiently in this

00:23:06,780 --> 00:23:11,760
particular case integration tests are

00:23:08,940 --> 00:23:14,910
created as part of our feature

00:23:11,760 --> 00:23:17,070
development so we're making you know

00:23:14,910 --> 00:23:18,900
steps in the right direction already and

00:23:17,070 --> 00:23:23,640
passage of these integration tests can

00:23:18,900 --> 00:23:25,830
be used to promote from into stage in

00:23:23,640 --> 00:23:28,410
stage from stage to prod you can have

00:23:25,830 --> 00:23:33,060
additional testing like performance

00:23:28,410 --> 00:23:35,300
testing or some additional testing like

00:23:33,060 --> 00:23:38,910
you are testing be thrown into the mix

00:23:35,300 --> 00:23:42,710
if you want higher levels of reliability

00:23:38,910 --> 00:23:46,170
and quality to go from stage to problem

00:23:42,710 --> 00:23:49,950
the next thing we realized was when

00:23:46,170 --> 00:23:52,230
you're testing if you're if you need to

00:23:49,950 --> 00:23:54,800
build automation testing and your

00:23:52,230 --> 00:23:57,030
functionality is primarily a peer driven

00:23:54,800 --> 00:23:58,650
building all other functionality and

00:23:57,030 --> 00:24:01,380
tasking the development team to do that

00:23:58,650 --> 00:24:04,680
is easy but when you're dealing with the

00:24:01,380 --> 00:24:06,060
UI our team definitely our development

00:24:04,680 --> 00:24:08,820
team does not have the expertise to

00:24:06,060 --> 00:24:10,110
build you a test automation with you

00:24:08,820 --> 00:24:12,330
know they don't have the expertise

00:24:10,110 --> 00:24:14,640
around selenium or any other tools so

00:24:12,330 --> 00:24:17,010
how do we achieve continuous deployments

00:24:14,640 --> 00:24:20,160
and automated promotion when dealing

00:24:17,010 --> 00:24:22,670
with UI changes so one of the things

00:24:20,160 --> 00:24:26,750
that we considered was feature gates

00:24:22,670 --> 00:24:29,400
taking making use of some tools around

00:24:26,750 --> 00:24:30,210
getting features getting access to these

00:24:29,400 --> 00:24:33,150
you are

00:24:30,210 --> 00:24:35,820
components till they get more time to be

00:24:33,150 --> 00:24:39,630
thoroughly tested and you know are ready

00:24:35,820 --> 00:24:41,970
for primetime in this regards we can use

00:24:39,630 --> 00:24:43,800
either feature flags or in some simple

00:24:41,970 --> 00:24:45,630
cases we have also leveraged

00:24:43,800 --> 00:24:48,870
authorization if you're set up for that

00:24:45,630 --> 00:24:50,880
to get access to certain functionality

00:24:48,870 --> 00:24:52,500
and this could be done both for UI

00:24:50,880 --> 00:24:54,810
changes as well as for the backend

00:24:52,500 --> 00:24:57,180
changes allowing certain features more

00:24:54,810 --> 00:24:59,250
time to soak in if either the automation

00:24:57,180 --> 00:25:05,120
has not caught up or they are just

00:24:59,250 --> 00:25:08,280
evolving and being developed still so

00:25:05,120 --> 00:25:09,900
taking a look back at some of the

00:25:08,280 --> 00:25:12,420
aspects of quality assurance that we

00:25:09,900 --> 00:25:15,690
discussed earlier given some of these

00:25:12,420 --> 00:25:17,790
process changes the code quality

00:25:15,690 --> 00:25:19,980
guidelines and best practices this is

00:25:17,790 --> 00:25:23,730
something that the development team in

00:25:19,980 --> 00:25:26,880
our case helps drive the automation

00:25:23,730 --> 00:25:28,590
testing the pre merge testing and the

00:25:26,880 --> 00:25:30,690
automation of that is driven by the

00:25:28,590 --> 00:25:32,340
development team whereas the post merge

00:25:30,690 --> 00:25:35,700
is something that the development team

00:25:32,340 --> 00:25:38,220
and cuing share responsibility what this

00:25:35,700 --> 00:25:39,840
helps drive is you have a common set of

00:25:38,220 --> 00:25:42,270
test cases you have a common set of

00:25:39,840 --> 00:25:45,060
executors test automation and you have a

00:25:42,270 --> 00:25:48,420
common mechanism to execute them monitor

00:25:45,060 --> 00:25:50,880
the results success and failure to guide

00:25:48,420 --> 00:25:53,450
to gauge the reliability and the

00:25:50,880 --> 00:25:56,730
stability of the code being pushed and

00:25:53,450 --> 00:26:00,440
essentially of making sure that you are

00:25:56,730 --> 00:26:02,970
pushing and deploying high quality code

00:26:00,440 --> 00:26:05,220
thirdly with regards to production

00:26:02,970 --> 00:26:07,830
testing and monitoring error monitoring

00:26:05,220 --> 00:26:11,760
is being driven by the development team

00:26:07,830 --> 00:26:15,300
whereas smooth testing is owned by the

00:26:11,760 --> 00:26:17,190
QA team now in a previous you know

00:26:15,300 --> 00:26:18,870
service that we were managing in

00:26:17,190 --> 00:26:21,180
maintaining and developing the

00:26:18,870 --> 00:26:23,430
operations team used to write some

00:26:21,180 --> 00:26:26,010
scripts to test some key functionality

00:26:23,430 --> 00:26:30,060
and you run those scripts in production

00:26:26,010 --> 00:26:32,940
now while that works it was yet another

00:26:30,060 --> 00:26:35,270
team would yet another independently or

00:26:32,940 --> 00:26:39,450
I've got scripts and test automation

00:26:35,270 --> 00:26:42,090
that you have to manage and maintain if

00:26:39,450 --> 00:26:43,389
functionality is added updated you have

00:26:42,090 --> 00:26:46,359
one more surface

00:26:43,389 --> 00:26:48,669
to go coordinate with versus if you are

00:26:46,359 --> 00:26:51,009
already doing automation testing in int

00:26:48,669 --> 00:26:54,219
and stage there is no reason why the

00:26:51,009 --> 00:26:58,809
same testing cannot be conducted in

00:26:54,219 --> 00:27:02,379
production and a save on effort

00:26:58,809 --> 00:27:03,369
duplication and be make sure that the

00:27:02,379 --> 00:27:05,589
test automation

00:27:03,369 --> 00:27:07,599
rying in production is managed and

00:27:05,589 --> 00:27:12,219
maintained and updated alongside future

00:27:07,599 --> 00:27:16,839
development as well lastly bug triage is

00:27:12,219 --> 00:27:18,940
owned by the QA teams because a you know

00:27:16,839 --> 00:27:22,359
they are already set up to create those

00:27:18,940 --> 00:27:25,539
tests and be it also adds valuable

00:27:22,359 --> 00:27:28,239
feedback loop from bugs over back into

00:27:25,539 --> 00:27:34,149
the test cases so that we prevent

00:27:28,239 --> 00:27:35,409
regressions in the future so if we now

00:27:34,149 --> 00:27:37,509
revisit some of the testing challenges

00:27:35,409 --> 00:27:39,969
that we identified earlier with this

00:27:37,509 --> 00:27:41,499
process dev and QE teams even though

00:27:39,969 --> 00:27:43,479
they are spread across the world still

00:27:41,499 --> 00:27:46,989
you know we are not getting them any

00:27:43,479 --> 00:27:48,999
closer but them working on a common set

00:27:46,989 --> 00:27:52,929
of test cases and test automation on the

00:27:48,999 --> 00:27:55,659
same backlog sort of gets them more

00:27:52,929 --> 00:27:57,489
aligned with development teams and then

00:27:55,659 --> 00:27:59,649
you know we can leverage the same

00:27:57,489 --> 00:28:03,369
process that we have in place for just

00:27:59,649 --> 00:28:06,299
working with a remote team and they are

00:28:03,369 --> 00:28:08,769
still in phase rather than out of phase

00:28:06,299 --> 00:28:10,119
the effort duplication is reduced for

00:28:08,769 --> 00:28:13,179
test automation because they are working

00:28:10,119 --> 00:28:16,869
on the same set additional turnaround

00:28:13,179 --> 00:28:20,349
time is reduced and the software updates

00:28:16,869 --> 00:28:22,839
can be you know released faster and more

00:28:20,349 --> 00:28:25,119
efficiently and quality is not

00:28:22,839 --> 00:28:30,459
sacrificed because test automation is

00:28:25,119 --> 00:28:32,379
not or is a priority and not ignored so

00:28:30,459 --> 00:28:33,700
given all of these goals and some of the

00:28:32,379 --> 00:28:36,459
process changes that you've discussed

00:28:33,700 --> 00:28:39,070
how do we leverage tools in order to

00:28:36,459 --> 00:28:42,549
help us achieve some of these changes

00:28:39,070 --> 00:28:45,159
that we just talked about and to discuss

00:28:42,549 --> 00:28:48,249
that and go over that I'll hand it over

00:28:45,159 --> 00:28:51,740
to mark thank you have a shake it

00:28:48,249 --> 00:28:56,030
clicker right just working

00:28:51,740 --> 00:28:58,100
okay what were some of the requirements

00:28:56,030 --> 00:29:01,220
we gathered around this automation tool

00:28:58,100 --> 00:29:03,429
for testing where we would create number

00:29:01,220 --> 00:29:06,110
one we should not recreate the wheel

00:29:03,429 --> 00:29:08,570
every language every platform has got a

00:29:06,110 --> 00:29:10,790
testing library or framework built into

00:29:08,570 --> 00:29:13,460
it or around it there are high quality

00:29:10,790 --> 00:29:14,720
expectations and matchers and so on and

00:29:13,460 --> 00:29:15,260
we should not recreate any of these

00:29:14,720 --> 00:29:17,360
things

00:29:15,260 --> 00:29:19,040
additionally developers are used to

00:29:17,360 --> 00:29:20,750
using these tools and we should make

00:29:19,040 --> 00:29:24,110
them as familiar as possible to reduce

00:29:20,750 --> 00:29:26,090
friction and to increase adoption but we

00:29:24,110 --> 00:29:28,250
did have to add some new features so we

00:29:26,090 --> 00:29:30,170
want to add the ability to label your

00:29:28,250 --> 00:29:32,780
tests arbitrarily because you'll be

00:29:30,170 --> 00:29:34,370
using your tests in many ways such as

00:29:32,780 --> 00:29:36,770
monitoring a smoke testing is

00:29:34,370 --> 00:29:38,660
integration testing etc we want to be

00:29:36,770 --> 00:29:40,670
able to run our tests as performance

00:29:38,660 --> 00:29:43,130
tests and scale which means MATLAB run

00:29:40,670 --> 00:29:44,540
them massively in parallel we wanted the

00:29:43,130 --> 00:29:46,490
ability to have all of our errors

00:29:44,540 --> 00:29:50,360
reported and tracing back to those test

00:29:46,490 --> 00:29:51,710
failures to the same tool we'll get that

00:29:50,360 --> 00:29:55,580
a moment the same tool we use for our

00:29:51,710 --> 00:29:57,370
site reliability engineers all the

00:29:55,580 --> 00:30:00,679
results from all the tests should be

00:29:57,370 --> 00:30:02,390
captured and we should store them so we

00:30:00,679 --> 00:30:07,600
can analyze them over time and analyze

00:30:02,390 --> 00:30:09,800
trends lastly we can use those trends to

00:30:07,600 --> 00:30:11,809
possibly promote and otherwise create

00:30:09,800 --> 00:30:12,950
smart automation tools around how we're

00:30:11,809 --> 00:30:14,870
doing are we getting better are we

00:30:12,950 --> 00:30:20,240
getting worse and all the data can tell

00:30:14,870 --> 00:30:23,230
us that so how do we group with labels

00:30:20,240 --> 00:30:25,400
and why do we group at labels we want

00:30:23,230 --> 00:30:27,110
instead of having a single test like

00:30:25,400 --> 00:30:29,300
we're used to today we'd like to have

00:30:27,110 --> 00:30:30,770
that same test run many times many

00:30:29,300 --> 00:30:32,600
different environments so we want the

00:30:30,770 --> 00:30:33,679
ability to label tests arbitrarily so we

00:30:32,600 --> 00:30:35,990
can slice them and dice them in many

00:30:33,679 --> 00:30:37,220
different ways because we want to use

00:30:35,990 --> 00:30:39,890
them in more than one place we have to

00:30:37,220 --> 00:30:41,960
be able to tag them with more than one

00:30:39,890 --> 00:30:43,700
label and then of course we need the

00:30:41,960 --> 00:30:46,820
ability to filter and execute those

00:30:43,700 --> 00:30:48,440
tests by those labels labels are

00:30:46,820 --> 00:30:50,360
arbitrary and they can be used for

00:30:48,440 --> 00:30:52,190
anything in our case here this is an

00:30:50,360 --> 00:30:53,990
actual test case and we're flagging it

00:30:52,190 --> 00:30:56,150
as a performance test it's a monitoring

00:30:53,990 --> 00:30:58,340
test this particular one is read-only so

00:30:56,150 --> 00:30:59,570
we know it's safe for production and a

00:30:58,340 --> 00:31:02,600
component here this is our telemetry

00:30:59,570 --> 00:31:05,180
component that we're testing and in this

00:31:02,600 --> 00:31:07,520
particular case these are all applicable

00:31:05,180 --> 00:31:10,160
and I'm sure we can think of more labels

00:31:07,520 --> 00:31:11,960
and again they're arbitrary the test

00:31:10,160 --> 00:31:14,150
function here which is currently ellipse

00:31:11,960 --> 00:31:16,100
tout is very familiar go if you're

00:31:14,150 --> 00:31:19,760
familiar with go isms and go testing

00:31:16,100 --> 00:31:24,860
this is it's the same testing definition

00:31:19,760 --> 00:31:27,560
you use in anywhere else and here's the

00:31:24,860 --> 00:31:29,180
body of that test if you're used to go

00:31:27,560 --> 00:31:31,430
testing here's an expectation for

00:31:29,180 --> 00:31:34,130
example and so on it's just an example

00:31:31,430 --> 00:31:39,590
of how we can use the single test in a

00:31:34,130 --> 00:31:42,350
familiar way so how do we get to use

00:31:39,590 --> 00:31:44,540
these same tests for performance if you

00:31:42,350 --> 00:31:46,880
can run a test once you can run it many

00:31:44,540 --> 00:31:48,320
many many times in parallel and if you

00:31:46,880 --> 00:31:49,910
can run it many times in parallel that's

00:31:48,320 --> 00:31:51,680
the width we're talking about and then

00:31:49,910 --> 00:31:54,890
of course there's also depth if you have

00:31:51,680 --> 00:31:57,590
ten actors so to speak running the tests

00:31:54,890 --> 00:31:59,660
in parallel each actor can run it many

00:31:57,590 --> 00:32:03,730
times a piece but hence we added width

00:31:59,660 --> 00:32:07,730
and depth to our test cases all the test

00:32:03,730 --> 00:32:11,320
tests are again measured and stored for

00:32:07,730 --> 00:32:11,320
a lap times and success/failure

00:32:13,870 --> 00:32:18,350
success/failure of course is ultimately

00:32:16,400 --> 00:32:19,940
important for testing but likewise we

00:32:18,350 --> 00:32:21,710
need to include all the elapsed times

00:32:19,940 --> 00:32:23,180
for tests we wanted to have that trend

00:32:21,710 --> 00:32:25,790
analysis to see how we're doing over

00:32:23,180 --> 00:32:27,740
time all of our errors are going to be

00:32:25,790 --> 00:32:29,930
reported to the same error monitoring

00:32:27,740 --> 00:32:34,010
tool that our site reliability engineers

00:32:29,930 --> 00:32:35,690
use in our case it's entry results for

00:32:34,010 --> 00:32:37,610
all these tests should be stored in a

00:32:35,690 --> 00:32:39,950
consumable fashion so that our smart

00:32:37,610 --> 00:32:44,150
automation tools can go ahead and use

00:32:39,950 --> 00:32:47,720
them and they're easy to parse ours is a

00:32:44,150 --> 00:32:49,820
big JSON payload and all of that data

00:32:47,720 --> 00:32:51,260
needs to be stored historically so we

00:32:49,820 --> 00:32:58,220
can analyze it over time and see our

00:32:51,260 --> 00:33:00,620
trends so how do we do it first we

00:32:58,220 --> 00:33:02,300
leverage go testing library to make it

00:33:00,620 --> 00:33:03,730
both familiar and of course use what

00:33:02,300 --> 00:33:06,230
they already have built in for us

00:33:03,730 --> 00:33:09,740
however we need to invoke the tests

00:33:06,230 --> 00:33:11,690
differently you consider that go test is

00:33:09,740 --> 00:33:13,670
just the command-line interface to the

00:33:11,690 --> 00:33:16,970
testing library and go just as our spec

00:33:13,670 --> 00:33:18,509
or j-unit or so on we needed to wrap the

00:33:16,970 --> 00:33:20,940
tests ourselves so we can

00:33:18,509 --> 00:33:22,349
vote them ourselves once we can invoke

00:33:20,940 --> 00:33:24,059
them ourselves we can make therefore

00:33:22,349 --> 00:33:26,729
make a container that would wrap them

00:33:24,059 --> 00:33:28,829
and run those tests in a container once

00:33:26,729 --> 00:33:30,449
we have a container for our tests we can

00:33:28,829 --> 00:33:35,159
then run them on an open shipped cluster

00:33:30,449 --> 00:33:36,839
and because it containerized we well

00:33:35,159 --> 00:33:39,359
we're using it the kubernetes operator

00:33:36,839 --> 00:33:43,139
framework more or less as the control

00:33:39,359 --> 00:33:45,299
plane to run these tests and I think

00:33:43,139 --> 00:34:02,009
we're ready for some videos now how does

00:33:45,299 --> 00:34:05,729
it actually work in practice okay

00:34:02,009 --> 00:34:07,440
so upper left is a definition of what

00:34:05,729 --> 00:34:08,879
one of our tests look like in this

00:34:07,440 --> 00:34:10,679
particular case we're using the config

00:34:08,879 --> 00:34:12,990
map only because of a technical

00:34:10,679 --> 00:34:14,279
implementation detail that at the time

00:34:12,990 --> 00:34:16,259
we couldn't use custom resource

00:34:14,279 --> 00:34:17,730
definitions in the kubernetes operator

00:34:16,259 --> 00:34:19,859
framework if you're familiar with those

00:34:17,730 --> 00:34:22,409
we're using the config Maps which is

00:34:19,859 --> 00:34:25,740
just a grab bag of stuff but it works

00:34:22,409 --> 00:34:27,809
just as well in the bottom left we'll

00:34:25,740 --> 00:34:30,149
show you the pods that are running we're

00:34:27,809 --> 00:34:31,559
on these in a run on an open shift

00:34:30,149 --> 00:34:33,329
cluster in a container therefore we're

00:34:31,559 --> 00:34:35,869
gonna run them in pods and we can run

00:34:33,329 --> 00:34:38,819
many many pods and run them in parallel

00:34:35,869 --> 00:34:39,809
and we'll watch we'll watch as we create

00:34:38,819 --> 00:34:46,559
them the upper right we could watch some

00:34:39,809 --> 00:34:48,359
logs in the lower right this particular

00:34:46,559 --> 00:34:50,579
case we're going to post that upper left

00:34:48,359 --> 00:34:51,450
one posting on the right side and we'll

00:34:50,579 --> 00:34:53,129
see that the containers are being

00:34:51,450 --> 00:34:54,960
created and they're running and on the

00:34:53,129 --> 00:34:56,819
lower right side we'll watch we can tell

00:34:54,960 --> 00:34:58,619
the logs in the pod and watches these

00:34:56,819 --> 00:35:00,359
tests actually just go ahead and do

00:34:58,619 --> 00:35:02,339
their thing and likewise we could watch

00:35:00,359 --> 00:35:03,660
as the testing pods themselves when

00:35:02,339 --> 00:35:08,210
they're done they'll tear themselves

00:35:03,660 --> 00:35:08,210
down and do its thing

00:35:11,670 --> 00:35:15,550
yeah and that's already done it's a

00:35:13,930 --> 00:35:18,130
pretty quick one in this particular case

00:35:15,550 --> 00:35:20,310
we ran things we have width and depth of

00:35:18,130 --> 00:35:22,300
one so we're running just a single

00:35:20,310 --> 00:35:23,530
single pod we're running all the tests

00:35:22,300 --> 00:35:25,270
once it's puppy good

00:35:23,530 --> 00:35:26,920
it's your normal integration test your

00:35:25,270 --> 00:35:30,220
normal unit test your normal whatever

00:35:26,920 --> 00:35:32,590
however you want to run it and we see

00:35:30,220 --> 00:35:41,830
had just one pod was created one pod ran

00:35:32,590 --> 00:35:44,050
torn down and the logs etc yep all right

00:35:41,830 --> 00:35:46,600
all the results themselves is stored on

00:35:44,050 --> 00:35:49,870
the test so the test is both spec and

00:35:46,600 --> 00:35:51,760
status for the config map I should say

00:35:49,870 --> 00:35:53,530
is both spec spec and status for the

00:35:51,760 --> 00:35:57,880
test if you're used to kubernetes and

00:35:53,530 --> 00:35:59,770
open ship you're normally uh you're used

00:35:57,880 --> 00:36:01,570
to spec and Status being on the objects

00:35:59,770 --> 00:36:03,340
themselves so the results themselves is

00:36:01,570 --> 00:36:05,770
stored on the objects if you look at the

00:36:03,340 --> 00:36:08,110
data on the upper right you'll see our

00:36:05,770 --> 00:36:09,850
pods element of the config map will now

00:36:08,110 --> 00:36:11,980
show all the pods that ran in this case

00:36:09,850 --> 00:36:14,380
just one and it contains all the tests I

00:36:11,980 --> 00:36:15,820
ran we have access review access token

00:36:14,380 --> 00:36:18,370
cluster registration these are actual

00:36:15,820 --> 00:36:20,350
pieces of our API that we use and you'll

00:36:18,370 --> 00:36:22,570
see a list of elapsed times for each of

00:36:20,350 --> 00:36:24,580
the tests because it's only once they

00:36:22,570 --> 00:36:26,290
ran all the tests of the labels all all

00:36:24,580 --> 00:36:27,520
of our tests are in there currently and

00:36:26,290 --> 00:36:30,040
all the only random one so you only see

00:36:27,520 --> 00:36:33,480
one elapsed time in our next video well

00:36:30,040 --> 00:36:33,480
show you that we can run them many times

00:36:37,990 --> 00:36:42,670
same setup in the upper left you'll see

00:36:39,730 --> 00:36:44,619
that we have a width and depth set so we

00:36:42,670 --> 00:36:46,060
want to run for paws wide with this and

00:36:44,619 --> 00:36:48,790
though each pot will run the test five

00:36:46,060 --> 00:36:51,160
times a piece just as an example and we

00:36:48,790 --> 00:36:52,990
can see the pods being created and a

00:36:51,160 --> 00:36:56,050
single pot is being tailed on the lower

00:36:52,990 --> 00:36:57,970
right but all the tests all the pods are

00:36:56,050 --> 00:37:01,660
running the same tests and have a

00:36:57,970 --> 00:37:03,850
similar logs if we look at the upper

00:37:01,660 --> 00:37:05,260
right we'll see that the well yeah they

00:37:03,850 --> 00:37:08,140
go there the results already they were

00:37:05,260 --> 00:37:09,910
empty but now they've got all their

00:37:08,140 --> 00:37:11,920
results in them in this particular case

00:37:09,910 --> 00:37:13,810
our label was telemetry we're testing

00:37:11,920 --> 00:37:15,280
one specific component so we're only

00:37:13,810 --> 00:37:17,260
going to label those tests that are

00:37:15,280 --> 00:37:20,740
labeled with that component label and

00:37:17,260 --> 00:37:22,660
you'll see that our pod results all the

00:37:20,740 --> 00:37:25,420
pods are in there and likewise the

00:37:22,660 --> 00:37:28,180
elapsed times we have what five depth so

00:37:25,420 --> 00:37:30,820
each pod random five times we have every

00:37:28,180 --> 00:37:40,960
pod and every test result every run

00:37:30,820 --> 00:37:43,030
stored in that jason blob and lastly

00:37:40,960 --> 00:37:44,590
what errors look like in our particular

00:37:43,030 --> 00:37:46,210
case currently we can have a more

00:37:44,590 --> 00:37:47,980
elaborate data format but currently we

00:37:46,210 --> 00:37:51,520
have just a lapse time stored as the

00:37:47,980 --> 00:37:53,770
results of these tests if the results

00:37:51,520 --> 00:37:55,869
are not a possible duration it means

00:37:53,770 --> 00:37:57,550
it's a failure and that would be an

00:37:55,869 --> 00:37:59,500
error message in its place in this

00:37:57,550 --> 00:38:02,260
particular example our access token

00:37:59,500 --> 00:38:03,700
tests has a missing token error it's

00:38:02,260 --> 00:38:05,530
just an example we can drill into that

00:38:03,700 --> 00:38:07,900
and go see why the tokens missing and so

00:38:05,530 --> 00:38:10,240
on but in the end it either is a

00:38:07,900 --> 00:38:13,980
parsable duration for success and how

00:38:10,240 --> 00:38:13,980
long it took or it's an error message

00:38:30,540 --> 00:38:33,800
no questions

00:38:37,510 --> 00:38:57,190
okay sure the question is what framework

00:38:53,980 --> 00:38:59,280
were using to to solve and Hanan some of

00:38:57,190 --> 00:39:01,690
the challenges that Abhishek first

00:38:59,280 --> 00:39:04,180
presented that's what we're making with

00:39:01,690 --> 00:39:05,380
this tool so if we want to have some

00:39:04,180 --> 00:39:06,550
things that are integration tests

00:39:05,380 --> 00:39:08,890
pre-emerge if we want to have some

00:39:06,550 --> 00:39:11,050
monitoring tests that are post-merge if

00:39:08,890 --> 00:39:12,910
we create this suite of tests and label

00:39:11,050 --> 00:39:14,470
them accordingly some of the tests could

00:39:12,910 --> 00:39:16,180
be used for integration pre merge and

00:39:14,470 --> 00:39:17,490
likewise some can be used for monitoring

00:39:16,180 --> 00:39:19,810
afterwards and smoke testing

00:39:17,490 --> 00:39:22,930
post-production so that way we want to

00:39:19,810 --> 00:39:25,270
use the single test base that we create

00:39:22,930 --> 00:39:28,000
in many ways and that's the slicing and

00:39:25,270 --> 00:39:31,570
dicing by label also not touched upon

00:39:28,000 --> 00:39:33,970
during the demo or the design of it the

00:39:31,570 --> 00:39:37,450
tests themselves live in the code base

00:39:33,970 --> 00:39:39,460
where where the code is so in our case

00:39:37,450 --> 00:39:41,920
our subscription and account management

00:39:39,460 --> 00:39:43,600
services have our tests in our code base

00:39:41,920 --> 00:39:45,280
and likewise our cluster services team

00:39:43,600 --> 00:39:47,380
and so on the queue we can make their

00:39:45,280 --> 00:39:48,430
own repo with their tests the framework

00:39:47,380 --> 00:39:50,200
that we're creating this tool at

00:39:48,430 --> 00:39:52,540
recreating pulls those in as

00:39:50,200 --> 00:39:55,030
dependencies probes each of them for

00:39:52,540 --> 00:39:56,830
their tests and then they're just all in

00:39:55,030 --> 00:39:58,690
there right so we'll have one image

00:39:56,830 --> 00:40:00,369
that's the pod that's running containing

00:39:58,690 --> 00:40:02,290
all the various tests from all those

00:40:00,369 --> 00:40:04,450
components pulled all into one place and

00:40:02,290 --> 00:40:05,470
we can run them all then by label based

00:40:04,450 --> 00:40:09,250
on what we're trying to do in which

00:40:05,470 --> 00:40:12,220
environment so for example just to add a

00:40:09,250 --> 00:40:15,040
little more detail there we are not as

00:40:12,220 --> 00:40:17,560
Mark mentioned reinventing the wheel the

00:40:15,040 --> 00:40:20,470
test cases themselves are the same ones

00:40:17,560 --> 00:40:22,510
that you'll have that will be run by the

00:40:20,470 --> 00:40:24,760
native test runners in any particular

00:40:22,510 --> 00:40:26,890
language what we are adding with this

00:40:24,760 --> 00:40:29,290
particular tool is the ability to slice

00:40:26,890 --> 00:40:31,330
and dice them to run them to get the

00:40:29,290 --> 00:40:33,850
results to get the errors reported and

00:40:31,330 --> 00:40:37,600
do interesting things with regards to

00:40:33,850 --> 00:40:39,369
automated promotions for example what

00:40:37,600 --> 00:40:43,000
was the last what was the result of the

00:40:39,369 --> 00:40:44,800
last three runs you know if those or if

00:40:43,000 --> 00:40:47,109
all of those three past promotes

00:40:44,800 --> 00:40:50,200
something from stage to production what

00:40:47,109 --> 00:40:50,920
was the elapsed time average of all the

00:40:50,200 --> 00:40:53,500
width and

00:40:50,920 --> 00:40:55,480
combined 15 different executions of all

00:40:53,500 --> 00:40:58,059
the last ten ones if you are not

00:40:55,480 --> 00:40:59,920
regressing by more than 10% in terms of

00:40:58,059 --> 00:41:03,069
our scale test load test performance

00:40:59,920 --> 00:41:04,839
test promote something so those are some

00:41:03,069 --> 00:41:07,930
of the things that you can do which were

00:41:04,839 --> 00:41:09,970
missing if you simply run your regular

00:41:07,930 --> 00:41:14,829
tests through something like Jenkins or

00:41:09,970 --> 00:41:18,190
CI all of these the ability to a execute

00:41:14,829 --> 00:41:20,769
them and leverage the results harness

00:41:18,190 --> 00:41:24,069
those results analyze those and take in

00:41:20,769 --> 00:41:26,109
you know some interesting decisions

00:41:24,069 --> 00:41:29,079
based on that is what was missing and

00:41:26,109 --> 00:41:32,589
then also the ability to run the same

00:41:29,079 --> 00:41:35,109
tests in multiple environments and run

00:41:32,589 --> 00:41:37,150
them either as a single test or multiple

00:41:35,109 --> 00:41:38,829
tests for performance and scale so all

00:41:37,150 --> 00:41:41,920
of those things were simple things that

00:41:38,829 --> 00:41:45,099
our tool helps and this tool is also you

00:41:41,920 --> 00:41:46,690
know not something that is replacing any

00:41:45,099 --> 00:41:49,420
of the existing test runners out there

00:41:46,690 --> 00:41:52,539
but just adding some additional metadata

00:41:49,420 --> 00:42:02,519
and the ability to perform these actions

00:41:52,539 --> 00:42:02,519
that we talked about any other questions

00:42:03,220 --> 00:42:21,790
yes the so the question is if you don't

00:42:20,109 --> 00:42:28,810
throw away the Kiwi expertise what if

00:42:21,790 --> 00:42:30,849
you throw away the QE so what exactly do

00:42:28,810 --> 00:42:50,710
you mean the by the management side of

00:42:30,849 --> 00:42:53,470
it okay right so this is more about you

00:42:50,710 --> 00:42:57,339
know process and reporting hierarchies

00:42:53,470 --> 00:43:01,690
and that causing rips around priorities

00:42:57,339 --> 00:43:03,819
and thereby you know adding additional

00:43:01,690 --> 00:43:08,470
overhead by way of difference in

00:43:03,819 --> 00:43:10,450
priorities so in our case yes a droid

00:43:08,470 --> 00:43:12,970
heart development teams and QE teams

00:43:10,450 --> 00:43:16,170
report into their own hierarchies but at

00:43:12,970 --> 00:43:19,089
the end of the day they all report in to

00:43:16,170 --> 00:43:20,530
somebody under on the pnt side on the

00:43:19,089 --> 00:43:22,630
products and technology side on the

00:43:20,530 --> 00:43:28,200
engineering side but because everybody

00:43:22,630 --> 00:43:30,430
as such is an engineer with regards to

00:43:28,200 --> 00:43:32,770
specifically the challenge around

00:43:30,430 --> 00:43:35,079
different priorities two things can

00:43:32,770 --> 00:43:38,290
happen either your QE team is a pool

00:43:35,079 --> 00:43:41,349
that is focused on testing you know

00:43:38,290 --> 00:43:44,530
multiple more than one product service

00:43:41,349 --> 00:43:47,170
whatnots and hence is a single pool of

00:43:44,530 --> 00:43:48,400
QA engineers helping out for different

00:43:47,170 --> 00:43:49,990
products and services different

00:43:48,400 --> 00:43:51,520
development teams and then competing

00:43:49,990 --> 00:43:54,520
with priorities or you can have your own

00:43:51,520 --> 00:43:57,400
dedicated one in our case we've had over

00:43:54,520 --> 00:43:59,560
the past years experimented with both we

00:43:57,400 --> 00:44:02,650
I've worked on development teams that

00:43:59,560 --> 00:44:05,230
have a pool of cue engineers and I

00:44:02,650 --> 00:44:09,760
worked on developing teams which have

00:44:05,230 --> 00:44:11,800
their own dedicated QE in the end you

00:44:09,760 --> 00:44:14,079
know anything and everything that you do

00:44:11,800 --> 00:44:16,390
over here is not going to help with that

00:44:14,079 --> 00:44:18,279
particular problem

00:44:16,390 --> 00:44:20,259
the end of the day you just have to work

00:44:18,279 --> 00:44:23,499
through that offline and you know

00:44:20,259 --> 00:44:25,509
nothing we do in terms of some of the

00:44:23,499 --> 00:44:29,079
things we discuss today will help with

00:44:25,509 --> 00:44:31,809
that but what does help is when they

00:44:29,079 --> 00:44:34,930
have time in the very least if QE can

00:44:31,809 --> 00:44:37,299
get you and these process changes can

00:44:34,930 --> 00:44:40,059
get you to a point where you have a

00:44:37,299 --> 00:44:42,190
single set of test cases that you are

00:44:40,059 --> 00:44:44,529
automating and a single set of test

00:44:42,190 --> 00:44:47,769
automation that you're building then

00:44:44,529 --> 00:44:51,789
qe in some cycles in some releases some

00:44:47,769 --> 00:44:53,980
updates can take up 70% of the load and

00:44:51,789 --> 00:44:56,529
in other cycles can take a 30% of the

00:44:53,980 --> 00:45:01,180
load but if you're working on a common

00:44:56,529 --> 00:45:04,569
set of automation you can have better

00:45:01,180 --> 00:45:06,609
luck with these problems than if both of

00:45:04,569 --> 00:45:08,319
them were doing their own test

00:45:06,609 --> 00:45:11,309
automation and working completely

00:45:08,319 --> 00:45:11,309
separately

00:45:21,990 --> 00:45:39,830
yes I'm sorry can you repeat that

00:45:36,240 --> 00:45:39,830
the door is really distracting

00:45:55,999 --> 00:46:01,109
sure that's a great question how do we

00:45:59,099 --> 00:46:02,489
fail fast and not containerize a test

00:46:01,109 --> 00:46:04,349
not run them on this big cluster how do

00:46:02,489 --> 00:46:06,269
we avoid all that if the test can fail

00:46:04,349 --> 00:46:08,579
quickly yeah that's a great question

00:46:06,269 --> 00:46:12,150
the way we're reusing the go testing

00:46:08,579 --> 00:46:15,679
library is which is repackaging how that

00:46:12,150 --> 00:46:18,029
go test function is is packaged and

00:46:15,679 --> 00:46:20,279
because we take over the execution of

00:46:18,029 --> 00:46:22,709
that runtime we can run them as a unit

00:46:20,279 --> 00:46:24,779
test right in my own local IDE on my

00:46:22,709 --> 00:46:27,059
local box so I can run that as a unit

00:46:24,779 --> 00:46:28,949
test like I did before and I can

00:46:27,059 --> 00:46:31,109
repackage it to put it into a container

00:46:28,949 --> 00:46:33,449
to run it many times in parallel we want

00:46:31,109 --> 00:46:36,029
to use the same test many times for many

00:46:33,449 --> 00:46:37,619
purposes so yes we can do it fail fast

00:46:36,029 --> 00:46:40,289
on your local development for a unit

00:46:37,619 --> 00:46:44,299
test and we can do it much more in

00:46:40,289 --> 00:46:44,299
parallel for more performance and so on

00:46:52,420 --> 00:46:55,200
anyone else

00:46:57,980 --> 00:47:07,639
okay thank you

00:47:00,550 --> 00:47:07,639

YouTube URL: https://www.youtube.com/watch?v=cFFl8I7BI4o


