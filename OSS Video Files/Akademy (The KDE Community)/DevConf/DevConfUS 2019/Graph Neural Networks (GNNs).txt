Title: Graph Neural Networks (GNNs)
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Francesco Murdaca

The human brain has many capabilities thanks to its network structure that allows transferring information among neurons in order to perform a specific action or multiple actions. Using Machine Learning (ML) we are focusing on the learning capability, but the brain uses its network also to store information in the memory.

Nowadays many fields (e.g. medicine, biology, security, space) rely on graph structures to store data with semantic and context specific to that domain. 
Graph neural networks (GNNs) allow machines to learn from this kind of structure, taking a little step closer to mimic the behavior and architecture of the human brain.

In this talk, we will explore this type of neural networks that take graphs as input, showing their capabilities, issues and applications.
Captions: 
	00:00:03,120 --> 00:00:09,910
so welcome everyone my name is Francesco

00:00:06,370 --> 00:00:13,629
and from Red Hat and I work in is your

00:00:09,910 --> 00:00:15,969
team and I'm specifically part of which

00:00:13,629 --> 00:00:19,420
is a project focus on recommender system

00:00:15,969 --> 00:00:22,170
for AI software today I will talk about

00:00:19,420 --> 00:00:24,580
the graph neural networks so this is a

00:00:22,170 --> 00:00:26,050
different type from the traditional one

00:00:24,580 --> 00:00:27,400
because they focus on the specific

00:00:26,050 --> 00:00:30,220
constructs of data which are on the

00:00:27,400 --> 00:00:32,500
graph and you will see why this is so

00:00:30,220 --> 00:00:35,079
important today especially because you

00:00:32,500 --> 00:00:37,660
have many types of graph everywhere so

00:00:35,079 --> 00:00:40,059
social networks knowledge graphs and

00:00:37,660 --> 00:00:58,659
many types of graph which contain many

00:00:40,059 --> 00:01:02,019
useful information if we want to briefly

00:00:58,659 --> 00:01:04,479
introduce so the basic components for

00:01:02,019 --> 00:01:09,189
neural networks and then we go a little

00:01:04,479 --> 00:01:12,159
bit more into details so

00:01:09,189 --> 00:01:15,930
we will try to answer this simple

00:01:12,159 --> 00:01:15,930
question so what is our neural networks

00:01:17,130 --> 00:01:32,049
so graph yes networks a neural networks

00:01:23,189 --> 00:01:35,789
so narrow networks is a very broad but I

00:01:32,049 --> 00:01:39,640
will try to do that in two slides so

00:01:35,789 --> 00:01:42,340
first of all from a general overview we

00:01:39,640 --> 00:01:45,189
have artificial intelligence domain so

00:01:42,340 --> 00:01:46,599
you have robotics and fields then we

00:01:45,189 --> 00:01:48,610
have a subdomain which is the machine

00:01:46,599 --> 00:01:51,580
learning one so in machine learning we

00:01:48,610 --> 00:01:54,549
try to teach the Machine a specific task

00:01:51,580 --> 00:01:57,539
using different class of metals we have

00:01:54,549 --> 00:02:02,950
a supervised one and supervised one and

00:01:57,539 --> 00:02:05,459
reinforcement learning then we have the

00:02:02,950 --> 00:02:07,630
neural networks which can be adopted

00:02:05,459 --> 00:02:09,910
different classes or you can use them in

00:02:07,630 --> 00:02:12,400
supervised and supervised also informal

00:02:09,910 --> 00:02:15,040
learning and this type of a neural

00:02:12,400 --> 00:02:17,799
networks one type of approach that you

00:02:15,040 --> 00:02:20,290
can use in machine learning

00:02:17,799 --> 00:02:22,900
the last leg is the deep learning so the

00:02:20,290 --> 00:02:34,569
learning is when we basically use this

00:02:22,900 --> 00:02:38,410
neural network many many layers so let's

00:02:34,569 --> 00:02:41,049
start with an analogy so first of all we

00:02:38,410 --> 00:02:43,660
know that neural network is by far the

00:02:41,049 --> 00:02:46,660
most efficient and powerful machine

00:02:43,660 --> 00:02:48,800
existing which is the brain here we see

00:02:46,660 --> 00:02:51,980
on the right corner on the

00:02:48,800 --> 00:02:54,710
right on the right corner the treatise

00:02:51,980 --> 00:02:57,350
which is basically receiving some

00:02:54,710 --> 00:03:00,290
signals and this goes into the cell body

00:02:57,350 --> 00:03:03,410
which is processing the signals and the

00:03:00,290 --> 00:03:06,890
output is sent to the other neurons

00:03:03,410 --> 00:03:09,200
as an analogy we have the artificial

00:03:06,890 --> 00:03:11,480
neurons which is receive basically

00:03:09,200 --> 00:03:14,690
different inputs as numbers and this um

00:03:11,480 --> 00:03:16,670
this will be processed somehow and with

00:03:14,690 --> 00:03:19,100
some weights and biases in order to

00:03:16,670 --> 00:03:23,600
produce some output so this is the

00:03:19,100 --> 00:03:30,160
general way has been the application

00:03:23,600 --> 00:03:34,790
never inspire but of course this output

00:03:30,160 --> 00:03:36,950
takes and it needs to be bound somehow

00:03:34,790 --> 00:03:40,010
otherwise the output could be between

00:03:36,950 --> 00:03:44,140
minus infinity and plus infinity so this

00:03:40,010 --> 00:03:47,180
is why we how do we decide if the is

00:03:44,140 --> 00:03:49,280
activated or is fired we use activation

00:03:47,180 --> 00:03:53,300
function so this activation function

00:03:49,280 --> 00:03:57,020
gives you a range in which the Narron

00:03:53,300 --> 00:03:59,480
can be fired or not and several types of

00:03:57,020 --> 00:04:01,430
activation function and its activation

00:03:59,480 --> 00:04:04,510
function are different depending on the

00:04:01,430 --> 00:04:07,910
architecture and for computational

00:04:04,510 --> 00:04:12,590
purposes and performances these are also

00:04:07,910 --> 00:04:17,410
different types so we know how the

00:04:12,590 --> 00:04:20,690
single neurone works so now we go to the

00:04:17,410 --> 00:04:23,750
never networks we go together many types

00:04:20,690 --> 00:04:26,270
of mirrors and typically this is from

00:04:23,750 --> 00:04:30,530
the input to the output you go from left

00:04:26,270 --> 00:04:33,890
to right so they flow in something

00:04:30,530 --> 00:04:43,100
called forward propagation because you

00:04:33,890 --> 00:04:48,340
go for each input layers hidden layers

00:04:43,100 --> 00:04:51,200
and output layer of hidden layers and

00:04:48,340 --> 00:04:54,350
this is how the general structure of a

00:04:51,200 --> 00:04:58,700
neural network so how does it actually

00:04:54,350 --> 00:05:05,870
learn so in order to let's say strain a

00:04:58,700 --> 00:05:09,220
model we need some of the fundamental

00:05:05,870 --> 00:05:13,970
part in machine learning and how

00:05:09,220 --> 00:05:16,430
basically trained model in order to for

00:05:13,970 --> 00:05:24,380
some specific application then we need

00:05:16,430 --> 00:05:28,460
an algorithm a stand the biases and how

00:05:24,380 --> 00:05:30,320
do we quantify if the it is getting to

00:05:28,460 --> 00:05:31,970
the right performances we define a cost

00:05:30,320 --> 00:05:33,890
function and we minimize this cost

00:05:31,970 --> 00:05:37,280
function using an algorithm which is

00:05:33,890 --> 00:05:40,280
typically their descendants this is a

00:05:37,280 --> 00:05:41,930
very simple way to explain how the works

00:05:40,280 --> 00:05:45,020
of course in practice there are more

00:05:41,930 --> 00:05:47,300
complex thing to consider there are

00:05:45,020 --> 00:05:50,870
either parameter series learning rates

00:05:47,300 --> 00:05:53,360
and other issue that you need to deal

00:05:50,870 --> 00:05:55,490
with when you train a model but this is

00:05:53,360 --> 00:05:57,680
for the sake of this talk this is just

00:05:55,490 --> 00:06:01,550
what we need to know at the moment

00:05:57,680 --> 00:06:04,009
and just to give you a little comparison

00:06:01,550 --> 00:06:07,100
again with the with the brain we know

00:06:04,009 --> 00:06:09,860
that official neural networks are

00:06:07,100 --> 00:06:13,699
different from the brain in the sense

00:06:09,860 --> 00:06:17,690
that in terms of size it said the brain

00:06:13,699 --> 00:06:21,949
is around thirty thousand three hundred

00:06:17,690 --> 00:06:24,789
thousand trillion neurons artificial

00:06:21,949 --> 00:06:28,699
neural networks has much less because of

00:06:24,789 --> 00:06:31,760
computational purposes and limitation of

00:06:28,699 --> 00:06:35,750
the machines in terms of speed the

00:06:31,760 --> 00:06:40,840
neural networks the artificial one of

00:06:35,750 --> 00:06:43,970
course while the one of the brain

00:06:40,840 --> 00:06:46,430
depends on the age of the gender or many

00:06:43,970 --> 00:06:48,410
other factors the training Calgary for

00:06:46,430 --> 00:06:50,810
the neural networks for the artificial

00:06:48,410 --> 00:06:52,340
evidence are the current descent while

00:06:50,810 --> 00:07:00,229
for the brain we still don't know

00:06:52,340 --> 00:07:02,539
actually for the brain is actually

00:07:00,229 --> 00:07:05,630
depends you can have training models

00:07:02,539 --> 00:07:07,849
using many views and many machines so

00:07:05,630 --> 00:07:11,539
this is quite demanding in terms of

00:07:07,849 --> 00:07:13,659
power field application the artificial

00:07:11,539 --> 00:07:16,370
neural networks are for specific tasks

00:07:13,659 --> 00:07:18,430
while the brain can actually deal with

00:07:16,370 --> 00:07:21,590
many tasks and can learn very easily

00:07:18,430 --> 00:07:23,539
other things and regarding the signals

00:07:21,590 --> 00:07:26,210
as we saw the beginning the comparison

00:07:23,539 --> 00:07:28,909
of the analogy between the two they are

00:07:26,210 --> 00:07:31,250
more or less in the in a similar way but

00:07:28,909 --> 00:07:34,860
this is just to show that we are still

00:07:31,250 --> 00:07:39,360
far from the

00:07:34,860 --> 00:07:44,790
the brain we're closing the gap in the

00:07:39,360 --> 00:07:48,210
future so I show just a simple neural

00:07:44,790 --> 00:07:50,310
architecture until now but nowadays with

00:07:48,210 --> 00:07:53,490
many set of architectures I will not go

00:07:50,310 --> 00:07:56,970
through all of them I will show you some

00:07:53,490 --> 00:07:58,800
very films and most used one for many

00:07:56,970 --> 00:08:01,200
application like humans recognition or

00:07:58,800 --> 00:08:04,200
language processing this is commercial

00:08:01,200 --> 00:08:16,200
networks commercial networks are used to

00:08:04,200 --> 00:08:19,860
extract features and typically they work

00:08:16,200 --> 00:08:25,140
very well for different tasks or

00:08:19,860 --> 00:08:29,580
sometimes they can they can also say for

00:08:25,140 --> 00:08:31,320
example in this case the other type of

00:08:29,580 --> 00:08:34,350
architecture like recurrent neural

00:08:31,320 --> 00:08:38,880
networks which are used for basically

00:08:34,350 --> 00:08:45,720
for when we need to have loops in the

00:08:38,880 --> 00:08:47,820
network so that will mean out so to

00:08:45,720 --> 00:08:49,470
remember some context this is important

00:08:47,820 --> 00:08:54,420
especially for application with the

00:08:49,470 --> 00:08:59,290
machine translation and text in general

00:08:54,420 --> 00:09:02,800
so I could go I will stop here regarding

00:08:59,290 --> 00:09:04,900
the neural networks go ahead but for

00:09:02,800 --> 00:09:10,980
this is what we need to know until now

00:09:04,900 --> 00:09:14,260
so now we will move to the so the graph

00:09:10,980 --> 00:09:16,450
basically spread in many or almost all

00:09:14,260 --> 00:09:19,980
sectors we can find some example with

00:09:16,450 --> 00:09:23,290
graphs in general a graph graph is a

00:09:19,980 --> 00:09:26,320
structure which is made by nodes and

00:09:23,290 --> 00:09:30,150
edges this node could contain entities

00:09:26,320 --> 00:09:32,860
or some specific object and edges

00:09:30,150 --> 00:09:37,120
instead the connection between these

00:09:32,860 --> 00:09:40,780
nodes or these entities there are many

00:09:37,120 --> 00:09:43,920
types of graphs that we can classify in

00:09:40,780 --> 00:09:48,460
different ways for example we have

00:09:43,920 --> 00:09:51,070
directed graphs so and the graph can be

00:09:48,460 --> 00:09:53,080
basically Traverse in many directions

00:09:51,070 --> 00:09:57,070
while the data graph will follow up a

00:09:53,080 --> 00:10:00,010
size path the graph can be weighted or

00:09:57,070 --> 00:10:03,070
labeled so you can have several numbers

00:10:00,010 --> 00:10:06,640
or you can have strings but in general

00:10:03,070 --> 00:10:09,390
they can be very different and the

00:10:06,640 --> 00:10:12,220
approach to work on this graph can be

00:10:09,390 --> 00:10:17,080
quite complex and different respected

00:10:12,220 --> 00:10:20,050
simple ones they could be if we sell a

00:10:17,080 --> 00:10:21,700
graph on the on the plane to B space you

00:10:20,050 --> 00:10:25,440
can see that so the edges are not

00:10:21,700 --> 00:10:25,440
crossing then the case is a plan

00:10:41,250 --> 00:11:12,820
and this is very simple math so we

00:11:10,300 --> 00:11:14,740
consider three notes for example and we

00:11:12,820 --> 00:11:18,610
come we can construct these matrices in

00:11:14,740 --> 00:11:20,500
this matrix basically how the nodes are

00:11:18,610 --> 00:11:22,630
connected between each other you can

00:11:20,500 --> 00:11:25,800
easily identify how many edges and which

00:11:22,630 --> 00:11:31,240
edges are connected between the nodes

00:11:25,800 --> 00:11:34,960
the degree matrix is so here is a single

00:11:31,240 --> 00:11:38,470
vector but you can see that basically it

00:11:34,960 --> 00:11:43,110
tells you how many edges are for each

00:11:38,470 --> 00:11:46,540
nodes and there is another matrix we'll

00:11:43,110 --> 00:11:48,970
use especially in dealing with graphs

00:11:46,540 --> 00:11:51,250
and this is basically difference between

00:11:48,970 --> 00:11:54,990
the diagonal matrix and other the

00:11:51,250 --> 00:11:54,990
antigen semantics it's a show before

00:11:55,160 --> 00:12:01,350
and this is a list of examples so graphs

00:11:58,890 --> 00:12:04,290
we can find graphs basically everywhere

00:12:01,350 --> 00:12:06,960
and language processing social network

00:12:04,290 --> 00:12:10,010
you can see that they cover almost every

00:12:06,960 --> 00:12:12,930
type of sector and this type of data are

00:12:10,010 --> 00:12:15,180
produced in many companies so this is

00:12:12,930 --> 00:12:26,940
very important and this is a problem

00:12:15,180 --> 00:12:29,640
that we need to deal with now we should

00:12:26,940 --> 00:12:31,740
have the basic background let's say to

00:12:29,640 --> 00:12:36,570
understand how the governor a network

00:12:31,740 --> 00:12:45,950
works so first of all let's focus on the

00:12:36,570 --> 00:12:48,480
motivation why so first of all this from

00:12:45,950 --> 00:12:52,560
so most of the traditional neural

00:12:48,480 --> 00:12:56,310
networks will deal with the data and so

00:12:52,560 --> 00:12:58,860
we find images numbers text audio this

00:12:56,310 --> 00:13:01,350
is something that we are dealing today

00:12:58,860 --> 00:13:03,750
with most of the traditional neural

00:13:01,350 --> 00:13:05,700
networks but there is another class of

00:13:03,750 --> 00:13:08,520
data which are flows which are called

00:13:05,700 --> 00:13:10,680
data and this type of data requires are

00:13:08,520 --> 00:13:13,950
different types of approach in order to

00:13:10,680 --> 00:13:17,210
be the deal to it so you find basically

00:13:13,950 --> 00:13:19,160
all the graph structures trees networks

00:13:17,210 --> 00:13:24,440
manifolds

00:13:19,160 --> 00:13:27,830
non-avian Dada is actually set of the

00:13:24,440 --> 00:13:30,590
graphs so the graphs are part of this

00:13:27,830 --> 00:13:33,050
data but there are approaches which are

00:13:30,590 --> 00:13:41,660
called geometric learning so this is the

00:13:33,050 --> 00:13:43,580
set above the neural networks the second

00:13:41,660 --> 00:13:46,280
reason is related to the graph

00:13:43,580 --> 00:13:48,440
embeddings so there are typically three

00:13:46,280 --> 00:13:51,260
approaches that you can use in order to

00:13:48,440 --> 00:13:55,130
embed the network so embedding networks

00:13:51,260 --> 00:13:57,080
means to move from the graph basically

00:13:55,130 --> 00:13:59,690
from the nodes from the edges to a

00:13:57,080 --> 00:14:01,220
two-dimensional vector and once you move

00:13:59,690 --> 00:14:02,890
to the two-dimensional vector you can

00:14:01,220 --> 00:14:08,720
basically apply all the other

00:14:02,890 --> 00:14:10,730
conventional neural networks the graph

00:14:08,720 --> 00:14:13,970
to do is to basically use this

00:14:10,730 --> 00:14:18,020
representation of networks and most of

00:14:13,970 --> 00:14:20,930
the talks are basically based on the

00:14:18,020 --> 00:14:23,420
graph and ethics and the third reason is

00:14:20,930 --> 00:14:26,990
basically related to the human range so

00:14:23,420 --> 00:14:29,240
we want to cost we want to get a step

00:14:26,990 --> 00:14:31,550
closer to mimic what the brain is gonna

00:14:29,240 --> 00:14:35,380
do so we need to start to deal with

00:14:31,550 --> 00:14:35,380
these types of data

00:14:38,400 --> 00:14:43,710
description of the so governor electors

00:14:41,580 --> 00:14:46,290
are one type of symmetric if you learn

00:14:43,710 --> 00:14:49,920
as I said geometric learning is the

00:14:46,290 --> 00:14:53,510
domain that deal with all the non

00:14:49,920 --> 00:14:57,900
written data graph is one subset of this

00:14:53,510 --> 00:15:00,750
particular approaches and what we can

00:14:57,900 --> 00:15:03,510
actually learn with graph so these are

00:15:00,750 --> 00:15:06,420
the typical type of problems that you

00:15:03,510 --> 00:15:08,930
can ask when you want to use graphs so

00:15:06,420 --> 00:15:11,550
you want to predict not classification

00:15:08,930 --> 00:15:13,920
which basically means that you have a

00:15:11,550 --> 00:15:15,690
graph with some nodes which are level

00:15:13,920 --> 00:15:18,300
and some nodes which are allowable and

00:15:15,690 --> 00:15:21,660
you want to predict what is the label

00:15:18,300 --> 00:15:23,640
related to these nodes and this is

00:15:21,660 --> 00:15:26,670
important for many fields as you can see

00:15:23,640 --> 00:15:31,320
there is predicting preferences for user

00:15:26,670 --> 00:15:33,720
and it's a social network or to predict

00:15:31,320 --> 00:15:35,839
customer if they should get a loan or

00:15:33,720 --> 00:15:39,480
not

00:15:35,839 --> 00:15:41,580
the second type of problems that you can

00:15:39,480 --> 00:15:45,380
have is a link prediction so you want to

00:15:41,580 --> 00:15:48,420
find basically hidden connection between

00:15:45,380 --> 00:15:52,820
practices and this is very important for

00:15:48,420 --> 00:15:55,770
the commando systems in different fields

00:15:52,820 --> 00:15:58,350
community detection or they also called

00:15:55,770 --> 00:16:01,020
cluster detection so if you want to

00:15:58,350 --> 00:16:05,760
identify some cluster inside the graph

00:16:01,020 --> 00:16:07,680
and that can be basically clustered

00:16:05,760 --> 00:16:09,690
together so if they are similarities for

00:16:07,680 --> 00:16:14,100
certain nodes and you want to discover

00:16:09,690 --> 00:16:15,750
some specific cluster inside and this is

00:16:14,100 --> 00:16:17,119
very important if you have a very huge

00:16:15,750 --> 00:16:20,719
graphs

00:16:17,119 --> 00:16:27,079
and this is also used in social network

00:16:20,719 --> 00:16:31,099
for website specification so if you want

00:16:27,079 --> 00:16:34,099
to classify graph this can be important

00:16:31,099 --> 00:16:39,259
for many application as you see there is

00:16:34,099 --> 00:16:42,559
also images because the graph are also

00:16:39,259 --> 00:16:45,919
the Euclidian data so you can map also

00:16:42,559 --> 00:16:51,039
images to graph so this is an extension

00:16:45,919 --> 00:16:51,039
of what you can do with with graph

00:16:51,759 --> 00:16:58,399
regarding methods and application so the

00:16:54,589 --> 00:16:59,989
core problem of the of the graph neural

00:16:58,399 --> 00:17:02,269
network is how to deal with the

00:16:59,989 --> 00:17:04,399
representation of the graph in a way

00:17:02,269 --> 00:17:07,639
that they can be used in a machine

00:17:04,399 --> 00:17:09,379
learning model basically and here is the

00:17:07,639 --> 00:17:11,119
main idea so as I said before we're

00:17:09,379 --> 00:17:13,369
talking about graph embeddings if we

00:17:11,119 --> 00:17:16,789
focus on one of the problem I said this

00:17:13,369 --> 00:17:19,429
which is an odd classification this is

00:17:16,789 --> 00:17:21,529
what is what is done in another bad dick

00:17:19,429 --> 00:17:28,159
so typically you have for each node you

00:17:21,529 --> 00:17:29,330
can build some Nazi for each of these

00:17:28,159 --> 00:17:34,249
nodes you can build a computational

00:17:29,330 --> 00:17:36,169
graph you can aggregate this this graph

00:17:34,249 --> 00:17:38,210
for each of the node and basically you

00:17:36,169 --> 00:17:42,889
can have a simple representation of all

00:17:38,210 --> 00:17:46,129
the graphs for one targeted node so the

00:17:42,889 --> 00:17:48,120
idea is similar and what actually was

00:17:46,129 --> 00:17:52,149
done is to

00:17:48,120 --> 00:17:55,210
start to consider aggregation function

00:17:52,149 --> 00:17:57,850
for all the notes then to define a loss

00:17:55,210 --> 00:18:00,970
function as I show at the beginning how

00:17:57,850 --> 00:18:03,010
a neural network works I basically there

00:18:00,970 --> 00:18:05,110
in the middle and neural networks so

00:18:03,010 --> 00:18:06,669
then you train on a set of nodes so you

00:18:05,110 --> 00:18:08,500
decide which note you will use for your

00:18:06,669 --> 00:18:11,409
training model and then you will test

00:18:08,500 --> 00:18:13,149
them with the other nodes and finally

00:18:11,409 --> 00:18:19,299
you can generate embeddings for the new

00:18:13,149 --> 00:18:24,210
nodes so the problem on how to define

00:18:19,299 --> 00:18:30,909
the governor network is how you define

00:18:24,210 --> 00:18:33,330
the aggregation function so regarding a

00:18:30,909 --> 00:18:35,860
history the graph entire networks are

00:18:33,330 --> 00:18:38,020
quite new I would say with respect to

00:18:35,860 --> 00:18:40,720
the other type of called additional

00:18:38,020 --> 00:18:44,980
neural networks and they were born

00:18:40,720 --> 00:18:47,919
basically in 2005 this is just a little

00:18:44,980 --> 00:18:50,529
steps that were using initializing a

00:18:47,919 --> 00:18:54,549
random embedding for each node and they

00:18:50,529 --> 00:18:57,580
were using some constraint algorithm to

00:18:54,549 --> 00:18:59,980
iterate on all these nodes but this has

00:18:57,580 --> 00:19:02,060
some limitation that tends to be

00:18:59,980 --> 00:19:04,130
overcome

00:19:02,060 --> 00:19:07,670
later on and you see that there is a gap

00:19:04,130 --> 00:19:09,620
of quite some many years before moving

00:19:07,670 --> 00:19:14,960
forward because they were mostly

00:19:09,620 --> 00:19:17,900
focusing on the Freeman data and so the

00:19:14,960 --> 00:19:25,270
solution was to use one of the type of

00:19:17,900 --> 00:19:25,270
current and this was a huge improvement

00:19:25,540 --> 00:19:31,460
the other at the same time there was

00:19:28,870 --> 00:19:33,980
starting to move from the convolutional

00:19:31,460 --> 00:19:36,080
net start to be applied on the graph so

00:19:33,980 --> 00:19:38,930
what I thought the beginning for the

00:19:36,080 --> 00:19:41,780
images which are in 2d can be also it

00:19:38,930 --> 00:19:45,920
was extended to the use of graphs and

00:19:41,780 --> 00:19:47,540
this led to the creation of new types of

00:19:45,920 --> 00:19:50,900
neural network for graph which were

00:19:47,540 --> 00:19:53,810
based on the commotion networks and this

00:19:50,900 --> 00:19:56,630
was basically from now on there will be

00:19:53,810 --> 00:19:59,120
like twenty or thirty papers every year

00:19:56,630 --> 00:20:01,520
since 2017 which are for some new type

00:19:59,120 --> 00:20:03,680
of approaches and most of them comes

00:20:01,520 --> 00:20:06,560
from applying what we learn in

00:20:03,680 --> 00:20:09,410
traditional neural networks and we now

00:20:06,560 --> 00:20:13,790
applying them in the governor burrell

00:20:09,410 --> 00:20:16,280
networks so how do we distinguish most

00:20:13,790 --> 00:20:19,010
of the neural network as I said is

00:20:16,280 --> 00:20:22,070
mainly related to the how do we activate

00:20:19,010 --> 00:20:25,160
the neighborhood function and this is

00:20:22,070 --> 00:20:31,060
basically all the graph that you can

00:20:25,160 --> 00:20:31,060
find nowadays so most of them

00:20:31,270 --> 00:20:35,960
distinguishes because of the aggregation

00:20:33,470 --> 00:20:38,570
staff and the detail and then you have

00:20:35,960 --> 00:20:40,850
maybe you have some update function

00:20:38,570 --> 00:20:42,440
which are specific for each of the of

00:20:40,850 --> 00:20:45,559
the metals but as you can see there are

00:20:42,440 --> 00:20:50,929
all the traditional one so you can find

00:20:45,559 --> 00:20:59,480
conclusion at work all of them take some

00:20:50,929 --> 00:21:01,669
ideas from the traditional one they can

00:20:59,480 --> 00:21:03,350
be applied almost in all the domain that

00:21:01,669 --> 00:21:05,210
we saw for the traditional one the

00:21:03,350 --> 00:21:08,419
advantage is that we have the use of

00:21:05,210 --> 00:21:13,520
graph which have semantic relationship

00:21:08,419 --> 00:21:15,770
between the nodes and can be much more

00:21:13,520 --> 00:21:18,409
information and in the in the

00:21:15,770 --> 00:21:21,380
representation so you can see here for

00:21:18,409 --> 00:21:26,900
example how you can deal with images in

00:21:21,380 --> 00:21:30,230
the in graphs the text in almost every

00:21:26,900 --> 00:21:31,820
field in lab never machine translation

00:21:30,230 --> 00:21:34,370
they even they start to be applied

00:21:31,820 --> 00:21:39,020
almost in all the domain that you have

00:21:34,370 --> 00:21:41,150
in traditional ones physics so here is

00:21:39,020 --> 00:21:44,030
very important especially for the type

00:21:41,150 --> 00:21:47,420
of because here the data are especially

00:21:44,030 --> 00:21:51,510
graphs for molecules and

00:21:47,420 --> 00:21:54,090
or that is related with biology and also

00:21:51,510 --> 00:21:56,670
in combinatorial organization they were

00:21:54,090 --> 00:21:59,580
used with the salesman problem they are

00:21:56,670 --> 00:22:02,610
started tried to use also the neural

00:21:59,580 --> 00:22:07,140
networks and or knowledge graph and

00:22:02,610 --> 00:22:09,750
enough generation related to the this is

00:22:07,140 --> 00:22:13,890
the last point so what are they issue at

00:22:09,750 --> 00:22:16,200
the moment and you can we can see that

00:22:13,890 --> 00:22:19,620
there are some related to the structure

00:22:16,200 --> 00:22:22,710
so not we cannot use the same number of

00:22:19,620 --> 00:22:25,800
layers for example for the convolution

00:22:22,710 --> 00:22:30,450
and apply to the neural networks because

00:22:25,800 --> 00:22:33,180
there are some problem with the obvious

00:22:30,450 --> 00:22:34,910
amusing and this needs to be deal with

00:22:33,180 --> 00:22:37,850
the other type of neural networks

00:22:34,910 --> 00:22:41,700
scalability there is no just a

00:22:37,850 --> 00:22:46,710
generalization for the in the in the

00:22:41,700 --> 00:22:51,170
scale of the new type of a graph which

00:22:46,710 --> 00:22:54,120
deal with very big sizes

00:22:51,170 --> 00:22:56,850
so most of the graph neural networks are

00:22:54,120 --> 00:22:59,070
based on the assumption that the graphs

00:22:56,850 --> 00:23:03,600
are homogeneous so that you have similar

00:22:59,070 --> 00:23:05,960
type of note similar type of edges this

00:23:03,600 --> 00:23:08,029
still big open

00:23:05,960 --> 00:23:10,730
point in research is the dynamic growth

00:23:08,029 --> 00:23:13,279
how to deal with graphs which changing

00:23:10,730 --> 00:23:15,200
time interpretability which is also

00:23:13,279 --> 00:23:17,690
something that we are dealing with the

00:23:15,200 --> 00:23:19,279
traditional neural network and there is

00:23:17,690 --> 00:23:22,190
still not a consistent or it come

00:23:19,279 --> 00:23:24,320
framework so there is not a precise way

00:23:22,190 --> 00:23:26,539
to say that we have to use this kind of

00:23:24,320 --> 00:23:32,570
neural networks on some specific

00:23:26,539 --> 00:23:34,690
application so conclusion graphs are

00:23:32,570 --> 00:23:38,600
basically spread everywhere we can have

00:23:34,690 --> 00:23:41,750
you can find a down based on graph IV

00:23:38,600 --> 00:23:43,399
application we can use graph also in

00:23:41,750 --> 00:23:45,740
with machine learning and they can be

00:23:43,399 --> 00:23:48,020
applied in many tasks we are taking a

00:23:45,740 --> 00:23:50,240
little step closer to me mix the human

00:23:48,020 --> 00:23:54,620
brain and there are still some open

00:23:50,240 --> 00:23:57,010
point which is which are open in the

00:23:54,620 --> 00:24:00,079
moment

00:23:57,010 --> 00:24:00,079
[Music]

00:24:17,150 --> 00:24:21,630
yeah we're really nice tall but I had a

00:24:19,710 --> 00:24:23,910
question regarding training the neural

00:24:21,630 --> 00:24:29,730
net so I'm assuming this is frame using

00:24:23,910 --> 00:24:32,549
backdrop so is there any K taking that

00:24:29,730 --> 00:24:36,150
there no loops in the graph so that how

00:24:32,549 --> 00:24:38,730
does backdrop even work doesn't it go in

00:24:36,150 --> 00:24:39,900
loops and the gradient blowing up and

00:24:38,730 --> 00:24:44,120
things like that does it

00:24:39,900 --> 00:24:48,360
how do training this actually work so

00:24:44,120 --> 00:24:51,750
the idea is to use the the core problem

00:24:48,360 --> 00:24:53,970
is how to represent the graphs in a way

00:24:51,750 --> 00:24:57,630
that we can insert in a machine learning

00:24:53,970 --> 00:24:59,610
problem so what I show is how to define

00:24:57,630 --> 00:25:02,700
this aggregation function in order to

00:24:59,610 --> 00:25:04,830
move from the space of the graph to this

00:25:02,700 --> 00:25:08,280
place can be applied in traditional

00:25:04,830 --> 00:25:10,290
neural networks and most of the

00:25:08,280 --> 00:25:12,780
initially the algorithm was different

00:25:10,290 --> 00:25:14,190
from the propagation but then thanks to

00:25:12,780 --> 00:25:16,919
the new architecture you can apply

00:25:14,190 --> 00:25:18,809
basically the traditional gradient

00:25:16,919 --> 00:25:21,380
descent problem is also in the graph

00:25:18,809 --> 00:25:21,380
neural networks

00:25:33,180 --> 00:25:42,520
by your entire network is there any loop

00:25:39,730 --> 00:25:46,830
inside the like it's a graph right you

00:25:42,520 --> 00:25:50,230
do is ensure that no cycles in the graph

00:25:46,830 --> 00:25:53,350
well there are of course some of the

00:25:50,230 --> 00:25:57,910
graph some of the assumption is that the

00:25:53,350 --> 00:26:00,430
graph can be like homogeneous and there

00:25:57,910 --> 00:26:02,050
are no cycle inside the graph so there

00:26:00,430 --> 00:26:05,440
are still some limitation of course for

00:26:02,050 --> 00:26:09,510
some type of graph so there is this

00:26:05,440 --> 00:26:09,510
assumption still needs to be

00:26:24,890 --> 00:26:28,360
we've got another question

00:26:29,970 --> 00:26:32,030

YouTube URL: https://www.youtube.com/watch?v=oFWaLHaY9Uw


