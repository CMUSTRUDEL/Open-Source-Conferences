Title: Handling chaos in containerized environments
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Alex Corvin

Unexpected things always happen in production. Robust applications must account for inevitable chaos. In this talk we'll explore methods for detecting, expecting, and automatically handling chaos in applications deployed on Kubernetes and OpenShift. We'll look at this through the lens of the Open Data Hub project, which is a machine-learning-as-a-service platform for running AI/ML workloads on Kubernetes.

This talk will focus on how Prometheus is used to monitor the Open Data Hub, some common failure scenarios that we've detected, how we take advantage of kubernetes features like pod anti-affinity and auto scaling to build resilient applications, and how we can use tools like kube-monkey to create a culture of building resilient applications.
Captions: 
	00:00:02,520 --> 00:00:08,850
I don't want to be on the about me side

00:00:06,720 --> 00:00:10,139
yeah I want to be on the title slide all

00:00:08,850 --> 00:00:14,879
right thank you everybody for coming

00:00:10,139 --> 00:00:17,670
today I am Alex and I'm here to talk to

00:00:14,879 --> 00:00:19,740
you a little bit about handling chaos in

00:00:17,670 --> 00:00:22,350
containerized environments specifically

00:00:19,740 --> 00:00:30,060
I'm gonna focus really on kubernetes or

00:00:22,350 --> 00:00:33,510
OpenShift containerized environments I'm

00:00:30,060 --> 00:00:38,489
a software engineer I work right now on

00:00:33,510 --> 00:00:40,530
the open data hub project if you project

00:00:38,489 --> 00:00:45,390
some of the other talks we're giving

00:00:40,530 --> 00:00:47,940
several this this weekend and I focused

00:00:45,390 --> 00:00:50,160
mainly on the internal Red Hat instance

00:00:47,940 --> 00:00:52,860
of the open data hub so we create a

00:00:50,160 --> 00:00:54,719
platform for other teams at Red Hat to

00:00:52,860 --> 00:00:56,610
be able to like perform the data science

00:00:54,719 --> 00:00:58,020
and analytics experiments and work and

00:00:56,610 --> 00:01:00,020
give them a platform for storing all

00:00:58,020 --> 00:01:01,790
their data right

00:01:00,020 --> 00:01:04,460
and one of the things we really focus on

00:01:01,790 --> 00:01:06,020
is like enabling teams at Red Hat to

00:01:04,460 --> 00:01:08,000
become what we say like more data

00:01:06,020 --> 00:01:09,890
centric so teach them to do data

00:01:08,000 --> 00:01:12,860
analysis to teach them to you know work

00:01:09,890 --> 00:01:16,370
with big data that kind of thing and my

00:01:12,860 --> 00:01:18,320
job primarily is to like I'm mostly

00:01:16,370 --> 00:01:20,810
tasked with keeping the internal data

00:01:18,320 --> 00:01:22,730
hub stable making sure it's you know

00:01:20,810 --> 00:01:24,800
highly available making sure we have

00:01:22,730 --> 00:01:27,620
good uptime so that the teams that rely

00:01:24,800 --> 00:01:29,810
on us can actually have a data platform

00:01:27,620 --> 00:01:33,440
that they can they can rely on right in

00:01:29,810 --> 00:01:35,120
use so you know I mean gentleman I'm

00:01:33,440 --> 00:01:37,400
super passionate about like Cyril is

00:01:35,120 --> 00:01:39,260
super passionate about some site

00:01:37,400 --> 00:01:41,780
liability engineering DevOps that kind

00:01:39,260 --> 00:01:43,790
of thing don't really think of myself as

00:01:41,780 --> 00:01:46,100
a developer and kind of a systems guy

00:01:43,790 --> 00:01:48,800
but I like to code so so that's what I

00:01:46,100 --> 00:01:50,900
do and then just a fun thing about me

00:01:48,800 --> 00:01:52,010
I'm a beekeeper so I get really bored if

00:01:50,900 --> 00:01:54,830
I don't have a lot of projects

00:01:52,010 --> 00:01:56,420
beekeeping is one of them it's usually

00:01:54,830 --> 00:01:59,610
an interesting thing to talk about so if

00:01:56,420 --> 00:02:02,760
you want to talk about beekeeping it

00:01:59,610 --> 00:02:05,070
so today I decided in we're going to be

00:02:02,760 --> 00:02:08,880
talking about today I'm gonna cover

00:02:05,070 --> 00:02:11,670
first what do I mean baked by chaos go

00:02:08,880 --> 00:02:13,440
on that on the next slide you know kind

00:02:11,670 --> 00:02:15,540
of like how we would have handled that

00:02:13,440 --> 00:02:18,120
chaos in more traditional environments

00:02:15,540 --> 00:02:19,920
that weren't containerized and then I'll

00:02:18,120 --> 00:02:21,390
talk about how open shift can help you

00:02:19,920 --> 00:02:23,670
cope with this chaos I think it has a

00:02:21,390 --> 00:02:25,500
lot of ways that can help you manage it

00:02:23,670 --> 00:02:27,900
that are a lot better than than what it

00:02:25,500 --> 00:02:30,720
worked like in the traditional ways and

00:02:27,900 --> 00:02:33,300
I'll do I think three kind of hands-on

00:02:30,720 --> 00:02:37,330
demos with that and talk about two other

00:02:33,300 --> 00:02:38,980
ways that I won't be able to do demo

00:02:37,330 --> 00:02:41,560
and finally I'll make some

00:02:38,980 --> 00:02:43,270
recommendations on just like way as I

00:02:41,560 --> 00:02:46,690
think that you can build cultures and

00:02:43,270 --> 00:02:48,310
teams that plan for this chaos and

00:02:46,690 --> 00:02:50,290
expect the chaos and learn to kind of

00:02:48,310 --> 00:02:52,870
manage and control it

00:02:50,290 --> 00:02:56,860
and I'm hoping that I'll have some time

00:02:52,870 --> 00:02:59,680
at the end for questions so we'll see so

00:02:56,860 --> 00:03:02,020
first of all what do I mean by chaos

00:02:59,680 --> 00:03:04,630
I made this kind of cool word map with

00:03:02,020 --> 00:03:07,120
with some things I had fun putting

00:03:04,630 --> 00:03:09,010
gremlins on there but so so basically

00:03:07,120 --> 00:03:11,410
what I mean by chaos is that if you've

00:03:09,010 --> 00:03:14,500
ever run a production system at scale

00:03:11,410 --> 00:03:17,920
like you probably know that eventually

00:03:14,500 --> 00:03:20,470
kind of just weird things happen right

00:03:17,920 --> 00:03:22,330
you may maybe know somebody or know a

00:03:20,470 --> 00:03:24,010
story about somebody who accidentally

00:03:22,330 --> 00:03:29,860
dropped like your production database

00:03:24,010 --> 00:03:32,260
table you own a software patch and it

00:03:29,860 --> 00:03:34,240
turned out that that software patch just

00:03:32,260 --> 00:03:38,590
let anybody log in with any blank

00:03:34,240 --> 00:03:41,010
password somebody thought they were in

00:03:38,590 --> 00:03:44,580
the staging pre-production environment

00:03:41,010 --> 00:03:46,409
like you know scheduled like a fell over

00:03:44,580 --> 00:03:47,670
of your edge load balancer and it turns

00:03:46,409 --> 00:03:50,909
out they were in production it just

00:03:47,670 --> 00:03:52,590
killed all your connections you know

00:03:50,909 --> 00:03:54,239
weird things happen those three examples

00:03:52,590 --> 00:03:56,159
are all things that have happened on

00:03:54,239 --> 00:03:59,690
teams I'm on and said I've been on not

00:03:56,159 --> 00:04:02,430
none anymore it wasn't an itch I promise

00:03:59,690 --> 00:04:04,080
like we all know that we should you know

00:04:02,430 --> 00:04:05,519
things happen right we all know that we

00:04:04,080 --> 00:04:08,329
should follow the best practices for

00:04:05,519 --> 00:04:10,200
testing our code right but you know

00:04:08,329 --> 00:04:12,000
deadlines are

00:04:10,200 --> 00:04:13,470
and you've got a rush to get software

00:04:12,000 --> 00:04:15,060
out and maybe you don't test all the

00:04:13,470 --> 00:04:17,340
edge cases and then it turns out that

00:04:15,060 --> 00:04:20,389
some like weird non Unicode character

00:04:17,340 --> 00:04:26,970
comes in it just breaks everything right

00:04:20,389 --> 00:04:29,280
maybe it's not human at all maybe I was

00:04:26,970 --> 00:04:30,510
trying to scroll go back this is where I

00:04:29,280 --> 00:04:35,639
wanna be hung where's my mouse I gotta

00:04:30,510 --> 00:04:37,860
find my mouse ok I found my mouse maybe

00:04:35,639 --> 00:04:40,800
maybe your network is a little flaky

00:04:37,860 --> 00:04:42,360
maybe maybe an AWS region goes down

00:04:40,800 --> 00:04:43,590
right and takes down half the internet

00:04:42,360 --> 00:04:46,050
and you can't watch Netflix anymore

00:04:43,590 --> 00:04:48,210
maybe your hardware fails maybe Russian

00:04:46,050 --> 00:04:51,780
hackers das you right cuz you know they

00:04:48,210 --> 00:04:54,930
want to steal your money right and so

00:04:51,780 --> 00:04:58,020
like maybe the chaos is not a bad thing

00:04:54,930 --> 00:05:00,710
maybe your system has like grown and

00:04:58,020 --> 00:05:02,910
become really popular and now you're

00:05:00,710 --> 00:05:04,560
maybe your billionaire right and you

00:05:02,910 --> 00:05:06,120
bought an island and instead you're just

00:05:04,560 --> 00:05:08,160
hanging out on your island and your

00:05:06,120 --> 00:05:10,680
swimming pool filled with gold and then

00:05:08,160 --> 00:05:12,479
you just forget to scale your system

00:05:10,680 --> 00:05:13,830
right so you're so busy counting your

00:05:12,479 --> 00:05:16,350
money that you forget to plan for

00:05:13,830 --> 00:05:18,570
capacity planning and then you know you

00:05:16,350 --> 00:05:20,280
have a spike in traffic and it brings

00:05:18,570 --> 00:05:26,100
down your system right so so all this

00:05:20,280 --> 00:05:27,870
stuff can happen and like I think like

00:05:26,100 --> 00:05:30,810
you can do one of two things right maybe

00:05:27,870 --> 00:05:33,539
you hire a team of like 500 full-time

00:05:30,810 --> 00:05:35,699
engineers and you know they maintain

00:05:33,539 --> 00:05:37,710
this system night and day and everybody

00:05:35,699 --> 00:05:38,100
is really busy and you know it kind of

00:05:37,710 --> 00:05:39,720
sucks

00:05:38,100 --> 00:05:41,639
or maybe you're a team of one and your

00:05:39,720 --> 00:05:43,169
phone is constantly buzzing and like

00:05:41,639 --> 00:05:44,520
you're on every night and weekend you

00:05:43,169 --> 00:05:47,160
don't get to sleep at all in your family

00:05:44,520 --> 00:05:49,470
hates you and like you're just miserable

00:05:47,160 --> 00:05:51,300
right and neither of these solutions is

00:05:49,470 --> 00:05:53,940
really very good long-term right neither

00:05:51,300 --> 00:05:56,670
one of these is gonna scale but

00:05:53,940 --> 00:05:58,890
if you don't do anything that you just

00:05:56,670 --> 00:06:00,450
use app is going to come to the house

00:05:58,890 --> 00:06:02,580
right and it's gonna fail and you're

00:06:00,450 --> 00:06:06,000
gonna go out of business so so what do

00:06:02,580 --> 00:06:07,740
we do about this right I think in the

00:06:06,000 --> 00:06:09,120
kind of traditional world I'm gonna I'm

00:06:07,740 --> 00:06:10,710
gonna tell story of how this works we

00:06:09,120 --> 00:06:12,540
kind of before containers and before

00:06:10,710 --> 00:06:14,370
open ship let's say you have an app

00:06:12,540 --> 00:06:15,480
right you're cool new startup you

00:06:14,370 --> 00:06:17,070
developing you know the cool new app

00:06:15,480 --> 00:06:18,780
maybe your Instagram or you know

00:06:17,070 --> 00:06:23,220
whatever is gonna replace Instagram so

00:06:18,780 --> 00:06:26,070
you deploy this app on a server it turns

00:06:23,220 --> 00:06:28,110
out that you're trying to get up and

00:06:26,070 --> 00:06:30,290
running really fast you deploy this app

00:06:28,110 --> 00:06:32,370
you don't really take time for

00:06:30,290 --> 00:06:33,840
configuration management or deployment

00:06:32,370 --> 00:06:35,220
automation they're like tests because

00:06:33,840 --> 00:06:36,570
those are for losers and you know who

00:06:35,220 --> 00:06:40,050
wants to monitor monitoring's for

00:06:36,570 --> 00:06:42,630
dummies right so you just it turns out

00:06:40,050 --> 00:06:45,780
people like your app so you decide you

00:06:42,630 --> 00:06:47,490
need to scale your app so you deploy a

00:06:45,780 --> 00:06:49,260
few more instances of you you spin up a

00:06:47,490 --> 00:06:50,090
few more AWS servers or whatever and now

00:06:49,260 --> 00:06:52,440
you're

00:06:50,090 --> 00:06:55,590
right because you've got four instances

00:06:52,440 --> 00:06:57,300
and maybe maybe you buy that the load

00:06:55,590 --> 00:06:59,220
balancer to put in front of it right so

00:06:57,300 --> 00:07:02,460
now you're redundant right and that's

00:06:59,220 --> 00:07:12,990
cool and then you know it turns out that

00:07:02,460 --> 00:07:15,120
people like your app so you're in like

00:07:12,990 --> 00:07:16,310
London and China and Australia and

00:07:15,120 --> 00:07:18,600
everywhere and you're really awesome

00:07:16,310 --> 00:07:19,920
remember we still haven't taken the time

00:07:18,600 --> 00:07:21,510
for configuration management so we're

00:07:19,920 --> 00:07:25,320
just we're super busy having to do all

00:07:21,510 --> 00:07:27,660
this right now you're right so maybe

00:07:25,320 --> 00:07:30,060
gdpr comes out or CCP is coming up if

00:07:27,660 --> 00:07:31,950
you're in California and so now you've

00:07:30,060 --> 00:07:33,450
got to start worrying about where is my

00:07:31,950 --> 00:07:35,700
data located and how can we get access

00:07:33,450 --> 00:07:37,560
to it it's just piling on more normal

00:07:35,700 --> 00:07:38,790
processes that how do we how do we cope

00:07:37,560 --> 00:07:40,290
with all this right and you still don't

00:07:38,790 --> 00:07:41,520
have any like configuration but you're

00:07:40,290 --> 00:07:43,980
still doing everything manually like

00:07:41,520 --> 00:07:46,710
it's just really hard to do right and so

00:07:43,980 --> 00:07:48,060
you think oh man I that is not scaling I

00:07:46,710 --> 00:07:48,940
need more people I can't do this all

00:07:48,060 --> 00:07:50,860
alone so now

00:07:48,940 --> 00:07:52,420
you like start filling where people are

00:07:50,860 --> 00:07:55,420
the problem right you hire hire a couple

00:07:52,420 --> 00:07:56,680
of Engineers and and now you have more

00:07:55,420 --> 00:07:58,180
and more people working on the system

00:07:56,680 --> 00:07:59,830
right and so they're starting to like

00:07:58,180 --> 00:08:02,110
commit conflicting changes and like

00:07:59,830 --> 00:08:03,580
they're starting to different things

00:08:02,110 --> 00:08:05,260
sometimes you can't keep track of

00:08:03,580 --> 00:08:07,930
everything and it's not working so you

00:08:05,260 --> 00:08:09,220
think our men this I'm not good maybe I

00:08:07,930 --> 00:08:10,270
should that maybe I should like mine

00:08:09,220 --> 00:08:11,350
they did by that's or something I'm

00:08:10,270 --> 00:08:12,580
running everything locally in my own

00:08:11,350 --> 00:08:14,560
data center right now maybe it'll help

00:08:12,580 --> 00:08:16,000
if I if I switch to the end so I do do

00:08:14,560 --> 00:08:17,500
that right and so like it's taking more

00:08:16,000 --> 00:08:18,730
time to manage is taking more money to

00:08:17,500 --> 00:08:23,620
manage it's taking more time more money

00:08:18,730 --> 00:08:25,870
right and the cast continues so if

00:08:23,620 --> 00:08:28,780
anybody's ever ever dealt with this I

00:08:25,870 --> 00:08:30,580
think that it sucks and I think that

00:08:28,780 --> 00:08:32,800
luckily there is a better way and I

00:08:30,580 --> 00:08:34,930
think the open shift can help so that's

00:08:32,800 --> 00:08:36,520
my little OpenShift hero guy swooping in

00:08:34,930 --> 00:08:38,500
I was looking for a year over the Cape

00:08:36,520 --> 00:08:40,599
but my icon library didn't have one so

00:08:38,500 --> 00:08:45,010
if you names right I like a library give

00:08:40,599 --> 00:08:46,930
me a user with a cape openshift I'd say

00:08:45,010 --> 00:08:48,520
has has some really cool features just

00:08:46,930 --> 00:08:51,940
out of the box that you can use to help

00:08:48,520 --> 00:08:55,150
with this chaos as I said we're going to

00:08:51,940 --> 00:08:57,790
explore five of them today and I take

00:08:55,150 --> 00:08:59,890
three kind of in hands-on so the first

00:08:57,790 --> 00:09:01,550
one I want to talk about is monitoring

00:08:59,890 --> 00:09:03,830
so

00:09:01,550 --> 00:09:06,050
you know in the past we had things like

00:09:03,830 --> 00:09:07,730
like zabbix and Nagios and all this

00:09:06,050 --> 00:09:10,490
other stuff that that like worked

00:09:07,730 --> 00:09:11,480
reasonably well but there's a new player

00:09:10,490 --> 00:09:12,410
that's come on the scene called

00:09:11,480 --> 00:09:14,390
prometheus

00:09:12,410 --> 00:09:16,970
if anybody's used Prometheus it's really

00:09:14,390 --> 00:09:18,290
cool it's really swanky I would say it's

00:09:16,970 --> 00:09:21,260
kind of become the new standard for

00:09:18,290 --> 00:09:23,270
monitoring applications and what's

00:09:21,260 --> 00:09:26,180
really nice is the Prometheus integrates

00:09:23,270 --> 00:09:27,170
just beautifully with kubernetes and and

00:09:26,180 --> 00:09:29,660
therefore OpenShift

00:09:27,170 --> 00:09:31,970
and you can configure it any way that

00:09:29,660 --> 00:09:33,920
you just get automatic monitoring

00:09:31,970 --> 00:09:35,360
scraping of your services you can you

00:09:33,920 --> 00:09:37,850
can really easily if you have a custom

00:09:35,360 --> 00:09:40,990
like flask app or whatever you can very

00:09:37,850 --> 00:09:42,699
easily add custom metrics into it

00:09:40,990 --> 00:09:44,980
if you're running like my sequel

00:09:42,699 --> 00:09:47,079
database or whatever there's like plenty

00:09:44,980 --> 00:09:48,939
of options for telling for me these two

00:09:47,079 --> 00:09:50,220
just spring clear my secret server you

00:09:48,939 --> 00:09:52,240
know and you don't have to worry about

00:09:50,220 --> 00:09:55,839
custom metrics or anything like it's

00:09:52,240 --> 00:09:58,029
just out there you pull them right and

00:09:55,839 --> 00:09:59,470
and so like Prometheus has those native

00:09:58,029 --> 00:10:01,990
operations and all sorts of other things

00:09:59,470 --> 00:10:03,730
and there's a library or a tool called

00:10:01,990 --> 00:10:05,170
gamma that you may or may not know about

00:10:03,730 --> 00:10:07,480
it that's been around a bit longer than

00:10:05,170 --> 00:10:09,490
Prometheus but it's a really nice

00:10:07,480 --> 00:10:11,619
visualization tool on top of Prometheus

00:10:09,490 --> 00:10:15,040
so you can get a really smoky monitoring

00:10:11,619 --> 00:10:16,660
suite and then Prometheus has an alert

00:10:15,040 --> 00:10:18,819
manager component as well that you can

00:10:16,660 --> 00:10:21,670
configure to generate alerts based on

00:10:18,819 --> 00:10:23,740
the metrics that it scrapes and you can

00:10:21,670 --> 00:10:25,240
you know it can send you an email like

00:10:23,740 --> 00:10:26,769
and you integrate with it slack or

00:10:25,240 --> 00:10:28,240
whatever you can you can have it send a

00:10:26,769 --> 00:10:31,089
pager duty if you're using that like it

00:10:28,240 --> 00:10:34,089
works really well right and so real

00:10:31,089 --> 00:10:38,060
quick I'm going to try and demo sort of

00:10:34,089 --> 00:10:41,440
just this so bear with me

00:10:38,060 --> 00:10:43,970
I'm going to see what happens if I drag

00:10:41,440 --> 00:10:47,860
this over okay I have to escape out of

00:10:43,970 --> 00:10:47,860
this I think this gate

00:10:50,090 --> 00:10:55,200
so I'm going to do everything over here

00:10:52,860 --> 00:10:59,180
later

00:10:55,200 --> 00:10:59,180
is that big enough can you guys see that

00:11:02,510 --> 00:11:08,670
I'm gonna do this I don't want to do

00:11:06,180 --> 00:11:10,350
that I'm gonna switch to screen

00:11:08,670 --> 00:11:12,240
mirroring because I feel like that'll be

00:11:10,350 --> 00:11:14,420
easier it's anybody seen my mouse there

00:11:12,240 --> 00:11:14,420
it is

00:11:16,240 --> 00:11:19,170
displays

00:11:20,620 --> 00:11:28,240
I'm here alright so here's what we're

00:11:24,550 --> 00:11:30,820
gonna do this is my command line right

00:11:28,240 --> 00:11:33,340
grano see Who am I and I am logged into

00:11:30,820 --> 00:11:38,410
an open shift server that open ship

00:11:33,340 --> 00:11:40,570
server is running in a few VMs in AWS so

00:11:38,410 --> 00:11:44,080
I've been ship for provision just a

00:11:40,570 --> 00:11:45,900
couple days ago it should just work fine

00:11:44,080 --> 00:11:47,400
and so the first thing I'm going to do

00:11:45,900 --> 00:11:49,560
remember I mentioned I work on the open

00:11:47,400 --> 00:11:52,680
data hub team and they've been doing

00:11:49,560 --> 00:11:55,350
abdi you open a project cool things we

00:11:52,680 --> 00:11:56,940
can do is deploy Prometheus and grow

00:11:55,350 --> 00:11:58,710
fauna for you and configure it all to

00:11:56,940 --> 00:12:00,660
scrape metrics from any services to

00:11:58,710 --> 00:12:02,640
played in your open ship project and

00:12:00,660 --> 00:12:05,070
display it in graph on so it's kind of

00:12:02,640 --> 00:12:08,570
cool I'm gonna open up a team box

00:12:05,070 --> 00:12:12,300
because I like Tmax I'm going to go to

00:12:08,570 --> 00:12:14,370
the agent so this is this is a this is

00:12:12,300 --> 00:12:15,839
not yet a public repo but something like

00:12:14,370 --> 00:12:18,170
it will probably be at some point but

00:12:15,839 --> 00:12:18,170
it's a

00:12:19,660 --> 00:12:23,460
deploy odh

00:12:24,450 --> 00:12:27,810
so if this is going to do is learn as

00:12:26,130 --> 00:12:30,750
well playbook that deploys the open up

00:12:27,810 --> 00:12:32,130
operator to my own ship things so this

00:12:30,750 --> 00:12:33,600
is gonna it's actually gonna create the

00:12:32,130 --> 00:12:35,339
open ship project for me which is nice

00:12:33,600 --> 00:12:37,470
it's just called death camp demo and

00:12:35,339 --> 00:12:39,899
then it applies a few open ship it has

00:12:37,470 --> 00:12:42,630
some resources to it so if I open up

00:12:39,899 --> 00:12:44,250
this guy over here open shift and I go

00:12:42,630 --> 00:12:47,430
to projects of the community in-depth

00:12:44,250 --> 00:12:49,380
demo that's cool oh man I didn't delete

00:12:47,430 --> 00:12:51,360
the namespace that's okay that's kind of

00:12:49,380 --> 00:12:53,470
work out alright now I'm going to leave

00:12:51,360 --> 00:12:56,229
the namespace

00:12:53,470 --> 00:12:59,339
Oh see delete I didn't clean up after

00:12:56,229 --> 00:12:59,339
myself last night sorry guys

00:13:00,730 --> 00:13:05,130
I want you to get the full experience

00:13:03,480 --> 00:13:06,360
it's gonna take a couple seconds so

00:13:05,130 --> 00:13:09,410
anyways who's gonna talk through this

00:13:06,360 --> 00:13:13,620
what's gonna happen is I'm going to

00:13:09,410 --> 00:13:15,810
reinstall that operator which is going

00:13:13,620 --> 00:13:17,370
to give you a pod that's running the

00:13:15,810 --> 00:13:19,260
operator and then I'll run another

00:13:17,370 --> 00:13:20,700
command that's going to create an open

00:13:19,260 --> 00:13:21,870
chef customer it yeah an open ship

00:13:20,700 --> 00:13:24,060
customer resource for the open data

00:13:21,870 --> 00:13:26,340
evaporator and tell it that I want to

00:13:24,060 --> 00:13:28,700
enable monitoring so I can show you that

00:13:26,340 --> 00:13:28,700
real quick

00:13:33,610 --> 00:13:39,110
so if I look at openshift object

00:13:36,860 --> 00:13:42,020
templates Oh dhcr

00:13:39,110 --> 00:13:43,640
I can see that down here I'm telling it

00:13:42,020 --> 00:13:45,770
monitoring truce that means I want

00:13:43,640 --> 00:13:49,400
monitoring notice I have disabled

00:13:45,770 --> 00:13:50,990
jupiter hub and FARC and Sullivan and I

00:13:49,400 --> 00:13:52,190
guess you could up again on that best

00:13:50,990 --> 00:13:54,230
line and what the difference between a

00:13:52,190 --> 00:13:56,840
ICO a Jupiter up and Jupiter are bond

00:13:54,230 --> 00:13:58,220
open ship it is but if you want a stock

00:13:56,840 --> 00:14:00,710
deployment or educated deployment or

00:13:58,220 --> 00:14:02,840
coming soon like cough that and then

00:14:00,710 --> 00:14:04,030
sell them and then ultimately the server

00:14:02,840 --> 00:14:06,140
spark and all that kind of stuff

00:14:04,030 --> 00:14:07,190
Explorer open it up because it's really

00:14:06,140 --> 00:14:09,170
cool and it can give you all that stuff

00:14:07,190 --> 00:14:11,630
really easily today we're just gonna do

00:14:09,170 --> 00:14:14,450
the monitoring aspect let's see what's

00:14:11,630 --> 00:14:19,820
happening with my project deletion

00:14:14,450 --> 00:14:19,950
I think that's done cool let's try that

00:14:19,820 --> 00:14:23,099
again

00:14:19,950 --> 00:14:23,099
[Applause]

00:14:27,990 --> 00:14:33,330
now we can see all this from scratch so

00:14:30,030 --> 00:14:38,640
my demo project I created again this

00:14:33,330 --> 00:14:41,850
time and so the next thing I'm going to

00:14:38,640 --> 00:14:45,180
do is deploy that custom resource that

00:14:41,850 --> 00:14:47,100
we were talking about so full disclosure

00:14:45,180 --> 00:14:49,140
this is gonna fail first because it

00:14:47,100 --> 00:14:51,270
waits for it doesn't properly wait for

00:14:49,140 --> 00:14:55,830
the like graph on a deployment to get

00:14:51,270 --> 00:14:59,700
created so the operators going to deploy

00:14:55,830 --> 00:15:01,140
prometheus where does correctly wait for

00:14:59,700 --> 00:15:03,030
Prometheus happen because what happens

00:15:01,140 --> 00:15:05,760
is Prometheus starts up then it

00:15:03,030 --> 00:15:08,010
configures the fauna to connect to per

00:15:05,760 --> 00:15:09,570
medias as a data source obviously for

00:15:08,010 --> 00:15:11,490
that to work you have to media is

00:15:09,570 --> 00:15:14,490
working first so the open data operator

00:15:11,490 --> 00:15:16,110
actually does wait my crappy firm

00:15:14,490 --> 00:15:17,970
together ansible does not and I didn't

00:15:16,110 --> 00:15:20,160
take the time to fix it but at some

00:15:17,970 --> 00:15:22,460
point this is going to become ready yeah

00:15:20,160 --> 00:15:25,950
we'll see you get pods

00:15:22,460 --> 00:15:28,160
I'm gonna wait for griffons to switch to

00:15:25,950 --> 00:15:28,160
ready

00:15:34,230 --> 00:15:43,890
this takes a few seconds I don't know so

00:15:40,410 --> 00:15:45,060
now I should so just one out of two is

00:15:43,890 --> 00:15:49,110
ready so I'm gonna wait a little bit

00:15:45,060 --> 00:15:51,030
longer for two out of two

00:15:49,110 --> 00:15:54,690
there we go all right so I'm gonna run

00:15:51,030 --> 00:15:57,720
that script again and it is all an

00:15:54,690 --> 00:16:00,150
important some stuff and do it changes

00:15:57,720 --> 00:16:04,020
with some stuff but then what it will do

00:16:00,150 --> 00:16:07,350
is create a really basic frost gap and

00:16:04,020 --> 00:16:09,000
then a Prometheus black box exporter

00:16:07,350 --> 00:16:11,160
which we can use to run availability

00:16:09,000 --> 00:16:13,740
checks against you know any arbitrary HT

00:16:11,160 --> 00:16:16,620
to the end point and I think that's it

00:16:13,740 --> 00:16:21,080
so what I do now is if I go into open

00:16:16,620 --> 00:16:21,080
shift and go to my graph

00:16:22,200 --> 00:16:26,639
oh the other thing my handsome beau does

00:16:23,760 --> 00:16:29,639
is it gives me a graph on a dashboard

00:16:26,639 --> 00:16:31,980
which I just importantly yamo so this

00:16:29,639 --> 00:16:34,260
takes a little while to set up but

00:16:31,980 --> 00:16:37,440
what's gonna happen is I have this other

00:16:34,260 --> 00:16:48,209
deployment which is a demo app which

00:16:37,440 --> 00:16:51,660
give me just one thing that's gonna be

00:16:48,209 --> 00:16:53,040
interesting in a second is this this

00:16:51,660 --> 00:16:56,760
hostname it's gonna be the name of the

00:16:53,040 --> 00:17:02,070
pilot I'd curl this over here it's that

00:16:56,760 --> 00:17:04,410
same thing so now you can see that this

00:17:02,070 --> 00:17:06,929
service is now green it's available so

00:17:04,410 --> 00:17:10,439
what's happening here is prometheus is

00:17:06,929 --> 00:17:13,020
configured to run a regular checks

00:17:10,439 --> 00:17:14,819
against that HTTP endpoint it's green

00:17:13,020 --> 00:17:16,770
that means is up its up and then over

00:17:14,819 --> 00:17:19,650
here this is the number of page cap or

00:17:16,770 --> 00:17:21,630
page hits on it rather the index rep

00:17:19,650 --> 00:17:24,120
increments they Prometheus counter every

00:17:21,630 --> 00:17:25,500
time it gets old so these availability

00:17:24,120 --> 00:17:28,530
checks are running every five seconds

00:17:25,500 --> 00:17:32,910
and it's going up what's nice though is

00:17:28,530 --> 00:17:33,670
that like so these ability checks those

00:17:32,910 --> 00:17:35,500
little apart

00:17:33,670 --> 00:17:36,850
for these rats I'm sure you could do it

00:17:35,500 --> 00:17:39,670
dynamically I didn't take the time to

00:17:36,850 --> 00:17:42,010
but these metric pulling up here for

00:17:39,670 --> 00:17:44,080
counts are just like I told Prometheus

00:17:42,010 --> 00:17:48,430
to scrape any kubernetes service that

00:17:44,080 --> 00:17:49,930
exists actually you can enable that and

00:17:48,430 --> 00:17:51,010
disable that really easily I'm gonna

00:17:49,930 --> 00:17:56,290
show that because it's cool though so

00:17:51,010 --> 00:18:01,980
you get service this guy

00:17:56,290 --> 00:18:01,980
those he described service

00:18:05,810 --> 00:18:09,080
I'm sorry

00:18:09,490 --> 00:18:14,010
standby there's supposed to be a

00:18:12,750 --> 00:18:16,350
it's an annotation I was looking in

00:18:14,010 --> 00:18:17,640
their labels is under annotations by

00:18:16,350 --> 00:18:19,559
sending an annotation of per media

00:18:17,640 --> 00:18:22,380
status straight besides true I'm time

00:18:19,559 --> 00:18:24,120
for medias to scrape this so if you will

00:18:22,380 --> 00:18:25,470
spin up a real application and you put

00:18:24,120 --> 00:18:27,690
an open Shepherd or kubernetes and

00:18:25,470 --> 00:18:29,400
service in front of it and you set this

00:18:27,690 --> 00:18:31,020
annotation you automatically get

00:18:29,400 --> 00:18:33,600
Prometheus cribbing and it's really nice

00:18:31,020 --> 00:18:39,809
and I think that before Prometheus if

00:18:33,600 --> 00:18:41,820
you had like nog user now it's just like

00:18:39,809 --> 00:18:43,860
you deploy Prometheus once you configure

00:18:41,820 --> 00:18:45,240
it correctly and it's really easy to get

00:18:43,860 --> 00:18:47,130
monitoring and really easy to start

00:18:45,240 --> 00:18:48,480
playing with with chronographs and get

00:18:47,130 --> 00:18:53,010
metrics for your service and just

00:18:48,480 --> 00:18:56,789
awesome so the next thing I'm gonna go

00:18:53,010 --> 00:19:01,910
back to my presentation and I'm going to

00:18:56,789 --> 00:19:04,080
step forward to availability so

00:19:01,910 --> 00:19:06,990
I'm not gonna spend much time in the

00:19:04,080 --> 00:19:09,810
slides because I'm a dentist again so so

00:19:06,990 --> 00:19:11,910
let me let me do this I'm going to make

00:19:09,810 --> 00:19:13,380
this smaller and we're going to watch

00:19:11,910 --> 00:19:18,000
cutting in the background that's green

00:19:13,380 --> 00:19:19,890
right now I'm new OC get pods so this is

00:19:18,000 --> 00:19:23,450
this is the one instance of my API

00:19:19,890 --> 00:19:25,920
running right it's in this pod here and

00:19:23,450 --> 00:19:27,720
let's say it like some sort of chaos

00:19:25,920 --> 00:19:35,070
happens and your pod dies so I'm just

00:19:27,720 --> 00:19:38,430
going to delete this and what we should

00:19:35,070 --> 00:19:41,610
see is that we're red and if I run a

00:19:38,430 --> 00:19:43,290
curl command like I get some gibberish

00:19:41,610 --> 00:19:46,640
now instead of that myself the world so

00:19:43,290 --> 00:19:46,640
like my API died right

00:19:46,800 --> 00:19:52,080
before committees before openshift if

00:19:49,800 --> 00:19:53,700
you wanted to avoid this like what's

00:19:52,080 --> 00:19:54,630
happening here I don't have any

00:19:53,700 --> 00:19:57,030
redundancy I don't have any high

00:19:54,630 --> 00:19:58,470
availability remember we talked about

00:19:57,030 --> 00:20:00,150
like if you want to fix this you've got

00:19:58,470 --> 00:20:01,770
a few more servers you got a load

00:20:00,150 --> 00:20:08,400
balancer if you can figure out that it's

00:20:01,770 --> 00:20:10,350
a lot of work this kind of becomes

00:20:08,400 --> 00:20:13,320
second nature and so if you use open

00:20:10,350 --> 00:20:14,880
shift this is like not an exciting thing

00:20:13,320 --> 00:20:18,690
to talk about in the top but if you have

00:20:14,880 --> 00:20:21,360
this is like if I want to fix this I can

00:20:18,690 --> 00:20:22,950
run a scale command I would yeah in

00:20:21,360 --> 00:20:24,570
production I would do this in yamo and

00:20:22,950 --> 00:20:27,420
put it in get but this is just easier

00:20:24,570 --> 00:20:30,810
for this I can OSC scale command and

00:20:27,420 --> 00:20:33,320
then I some do OC get pods I'm gonna

00:20:30,810 --> 00:20:33,320
watch that

00:20:34,530 --> 00:20:39,570
what's happening is my demo app where

00:20:38,190 --> 00:20:41,820
there was previously just one of these

00:20:39,570 --> 00:20:44,930
there's now two of these and there

00:20:41,820 --> 00:20:44,930
should be two

00:20:45,330 --> 00:20:49,380
so I scaled up to two it and now I have

00:20:47,370 --> 00:20:52,080
two pods and what I can do now is I can

00:20:49,380 --> 00:20:52,500
delete this pod do the same thing I did

00:20:52,080 --> 00:20:57,810
before

00:20:52,500 --> 00:21:00,060
oh so you delete that my dies but this

00:20:57,810 --> 00:21:01,200
is never gonna be read I can pair it and

00:21:00,060 --> 00:21:03,900
it's just gonna keep working

00:21:01,200 --> 00:21:04,770
and once that pod dies I'm gonna run my

00:21:03,900 --> 00:21:07,740
command

00:21:04,770 --> 00:21:09,390
I do this a bunch of times you'll see

00:21:07,740 --> 00:21:15,030
like I'm always getting that part I'm

00:21:09,390 --> 00:21:17,220
gonna watch this actually watch but we

00:21:15,030 --> 00:21:18,750
should see eventually is this hostname

00:21:17,220 --> 00:21:20,790
should switch to a different part there

00:21:18,750 --> 00:21:23,670
that goes so what happened was I deleted

00:21:20,790 --> 00:21:25,560
name pad and the requests were going to

00:21:23,670 --> 00:21:27,810
the remaining pod at some point open

00:21:25,560 --> 00:21:28,980
ship realized there's any one party or

00:21:27,810 --> 00:21:30,030
there's supposed to be two and it's not

00:21:28,980 --> 00:21:31,530
a different one and now all these

00:21:30,030 --> 00:21:34,020
shifters round-robin eating me between

00:21:31,530 --> 00:21:35,910
them so with like one commands getting

00:21:34,020 --> 00:21:38,130
out from one to two you have more

00:21:35,910 --> 00:21:39,470
availability usually one at least three

00:21:38,130 --> 00:21:41,389
like

00:21:39,470 --> 00:21:43,250
it's a super easy and open chef this is

00:21:41,389 --> 00:21:44,750
super easy and kubernetes it was really

00:21:43,250 --> 00:21:47,149
hard you know in a traditional

00:21:44,750 --> 00:21:52,100
environment so this is a really cool

00:21:47,149 --> 00:21:55,490
thing back to my presentation I think

00:21:52,100 --> 00:21:58,070
the next thing is on hardware placement

00:21:55,490 --> 00:22:00,820
so I'm going to do this I'm gonna switch

00:21:58,070 --> 00:22:00,820
back to

00:22:02,710 --> 00:22:06,870
I don't know where my thing went

00:22:05,250 --> 00:22:08,970
our husband says gonna switch back to

00:22:06,870 --> 00:22:10,680
extended but I guess I'm not gonna

00:22:08,970 --> 00:22:12,060
switch back to extended I can just talk

00:22:10,680 --> 00:22:14,490
about this for a second so this is one

00:22:12,060 --> 00:22:16,620
that have a demo for it and this is one

00:22:14,490 --> 00:22:19,710
that I want to talk about like where

00:22:16,620 --> 00:22:22,020
your pods run so again the traditional

00:22:19,710 --> 00:22:24,960
applications you want high availability

00:22:22,020 --> 00:22:26,370
you want to avoid chaos like network

00:22:24,960 --> 00:22:28,560
failure failure

00:22:26,370 --> 00:22:31,800
so you deploy an instance of your app in

00:22:28,560 --> 00:22:34,290
like keeping in mind which rackets on

00:22:31,800 --> 00:22:36,600
which type of topic type of rack servers

00:22:34,290 --> 00:22:38,070
on which row it's on which data center

00:22:36,600 --> 00:22:40,020
River song which availability zone it's

00:22:38,070 --> 00:22:42,990
in and like it's a lot to keep track of

00:22:40,020 --> 00:22:45,600
it and then just keeping track of it is

00:22:42,990 --> 00:22:46,920
a big job and then managing where your

00:22:45,600 --> 00:22:48,780
applications are deployed to them

00:22:46,920 --> 00:22:50,130
they're spread across them so if an AWS

00:22:48,780 --> 00:22:52,080
region goes down you're still running

00:22:50,130 --> 00:22:55,830
the other one like there's a lot of work

00:22:52,080 --> 00:22:57,390
and it's hard to do openshift you still

00:22:55,830 --> 00:22:59,760
gonna have to keep track of that

00:22:57,390 --> 00:23:02,310
underlying hybrid topology but the

00:22:59,760 --> 00:23:03,570
placement is really easy so there's two

00:23:02,310 --> 00:23:05,640
features I want to talk about I think

00:23:03,570 --> 00:23:08,810
it's - I'm not looking at my notes I'm

00:23:05,640 --> 00:23:10,790
doing this we're doing alive

00:23:08,810 --> 00:23:14,270
so so one feature that you can take a

00:23:10,790 --> 00:23:17,000
take advantage of is pod affinity or

00:23:14,270 --> 00:23:19,880
anti affinities so let's say you have in

00:23:17,000 --> 00:23:22,730
our API example we had two pods right

00:23:19,880 --> 00:23:24,980
and if one note goes down you want your

00:23:22,730 --> 00:23:28,400
API to stay up so you can do is to open

00:23:24,980 --> 00:23:31,250
ship to apply an entity rule on these

00:23:28,400 --> 00:23:42,370
paths so that they never get placed on

00:23:31,250 --> 00:23:42,370
the same reasons another one you're good

00:23:43,330 --> 00:23:48,470
and you want like a latency between them

00:23:46,340 --> 00:23:50,330
so maybe you want your paths around the

00:23:48,470 --> 00:23:53,960
same node as your database you do that

00:23:50,330 --> 00:23:56,750
do it with an affinity rule so until the

00:23:53,960 --> 00:24:05,420
pod always run with the MIT so that's

00:23:56,750 --> 00:24:08,420
one thing placements opens it gives you

00:24:05,420 --> 00:24:09,710
the ability to assign arbitrary labels

00:24:08,420 --> 00:24:11,900
to your underlying

00:24:09,710 --> 00:24:13,040
openshift hardware nodes right so you

00:24:11,900 --> 00:24:15,000
can do even do another one you can

00:24:13,040 --> 00:24:17,610
specify a label for your veil

00:24:15,000 --> 00:24:19,020
you can specify level for the like

00:24:17,610 --> 00:24:20,970
hardware first and you can specify the

00:24:19,020 --> 00:24:23,760
type of rack server like you just

00:24:20,970 --> 00:24:26,670
whatever you want and then you tell open

00:24:23,760 --> 00:24:28,230
ships to deploy to use those labels when

00:24:26,670 --> 00:24:31,140
deploying your pots so again you can do

00:24:28,230 --> 00:24:33,270
Finiti to know labels so you can say

00:24:31,140 --> 00:24:35,100
that like don't run any of the two pods

00:24:33,270 --> 00:24:36,480
on the same underlying node with these

00:24:35,100 --> 00:24:38,160
labels or whatever you do whatever you

00:24:36,480 --> 00:24:41,340
want so it becomes really easy to manage

00:24:38,160 --> 00:24:43,350
like the placement of applications

00:24:41,340 --> 00:24:49,230
relative to your underlying hardware and

00:24:43,350 --> 00:24:54,840
topology and again doing that without a

00:24:49,230 --> 00:24:57,030
lot of work it's just hard it was if it

00:24:54,840 --> 00:24:58,920
was hardware like had a VM you had to

00:24:57,030 --> 00:25:00,540
you know pick the Machine right and you

00:24:58,920 --> 00:25:01,830
have these massive max of your data

00:25:00,540 --> 00:25:04,050
centers and know where everything is

00:25:01,830 --> 00:25:06,270
deployed when you have thousands of

00:25:04,050 --> 00:25:07,830
servers that's hard if they're the ends

00:25:06,270 --> 00:25:09,330
you have let's just like another layer

00:25:07,830 --> 00:25:11,890
of complexity of to keep track of where

00:25:09,330 --> 00:25:14,260
the hypervisor is and like

00:25:11,890 --> 00:25:17,440
it's a lot of work and kÃ¼bra is no

00:25:14,260 --> 00:25:19,810
it makes it really easy I couldn't

00:25:17,440 --> 00:25:22,240
really do that because it's hard to get

00:25:19,810 --> 00:25:26,260
a test environment with multiple with

00:25:22,240 --> 00:25:29,560
multiple nodes is something that I can

00:25:26,260 --> 00:25:31,810
demonstrate so specifically what I want

00:25:29,560 --> 00:25:33,220
to demonstrate is some of the kubernetes

00:25:31,810 --> 00:25:38,740
or OpenShift

00:25:33,220 --> 00:25:40,210
other scaling features on this slide but

00:25:38,740 --> 00:25:41,320
I think the exciting thing here is going

00:25:40,210 --> 00:25:44,110
to be the demo so that's what I'm going

00:25:41,320 --> 00:25:45,850
to do so what I'm gonna do is I have

00:25:44,110 --> 00:25:49,810
another script here called deploy

00:25:45,850 --> 00:25:51,520
autoscaler so openshift has the ability

00:25:49,810 --> 00:25:54,310
to automatically scale your applications

00:25:51,520 --> 00:25:56,440
up or down based on memory consumption

00:25:54,310 --> 00:25:58,330
or CPU consumption you have to have the

00:25:56,440 --> 00:25:59,470
metrics module or whatever enabled in

00:25:58,330 --> 00:26:00,580
Europe and ship server but I think

00:25:59,470 --> 00:26:10,930
that's becoming pretty standard with

00:26:00,580 --> 00:26:12,580
open chip for so it's on in mine I think

00:26:10,930 --> 00:26:14,740
I'm using the same basic deployment

00:26:12,580 --> 00:26:16,540
basic class gap before there's also

00:26:14,740 --> 00:26:18,910
going to do what's called a horizontal

00:26:16,540 --> 00:26:20,160
autoscaler so my new around the OSI get

00:26:18,910 --> 00:26:21,900
HPA

00:26:20,160 --> 00:26:25,169
you can see I have this Def Con demo

00:26:21,900 --> 00:26:27,780
autoscale and it's going to watch my

00:26:25,169 --> 00:26:30,840
DEFCON 4 demo app deployment config and

00:26:27,780 --> 00:26:35,700
say minimum of three pods maximum of ten

00:26:30,840 --> 00:26:41,789
pods so if I do OC describe deployment

00:26:35,700 --> 00:26:43,530
config dev demo what we should see yeah

00:26:41,789 --> 00:26:46,160
and I just something I can do describe

00:26:43,530 --> 00:26:46,160
I'm gonna do get

00:26:46,800 --> 00:26:51,920
Percy get DC dev cops demo

00:26:53,470 --> 00:26:58,899
so notice that desired is three now so

00:26:57,250 --> 00:27:01,690
remember it Loomis Kevin up to two

00:26:58,899 --> 00:27:03,639
before the horizontals gather and tired

00:27:01,690 --> 00:27:06,159
to keep a minimum of three pods of that

00:27:03,639 --> 00:27:07,259
and it just did it so that's kind of

00:27:06,159 --> 00:27:11,080
cool

00:27:07,259 --> 00:27:14,230
one thing I can do over an open shift to

00:27:11,080 --> 00:27:17,649
this monitoring tab and CSUN dashboards

00:27:14,230 --> 00:27:23,519
and I can try and get the memory

00:27:17,649 --> 00:27:23,519
consumption for my app so she's probably

00:27:23,570 --> 00:27:27,859
if I can scroll down I got a select my

00:27:25,820 --> 00:27:29,570
name's face if you haven't played with

00:27:27,859 --> 00:27:31,700
this it's really cool so this is this is

00:27:29,570 --> 00:27:33,200
again on top of Prometheus built-in

00:27:31,700 --> 00:27:35,119
tuner which if cluster and automatically

00:27:33,200 --> 00:27:36,710
pulling metrics where you're positive

00:27:35,119 --> 00:27:39,249
play with it is really cool but I want

00:27:36,710 --> 00:27:43,210
to find that dev time demo namespace

00:27:39,249 --> 00:27:43,210
stop looking in scroll out

00:27:45,400 --> 00:27:51,210
alright so in here I can find my memory

00:27:50,230 --> 00:27:54,970
usage

00:27:51,210 --> 00:27:56,590
see deaf company has a request of 500

00:27:54,970 --> 00:27:58,809
Meg's limit of 500 Meg's it was only

00:27:56,590 --> 00:28:03,370
using like 35 or 37 Meg's right now so

00:27:58,809 --> 00:28:07,750
not very much what I'm going to do is go

00:28:03,370 --> 00:28:12,790
to the load endpoint of my really fancy

00:28:07,750 --> 00:28:16,180
API what this does is it generates like

00:28:12,790 --> 00:28:18,429
a hundred Meg's of random binary data in

00:28:16,180 --> 00:28:20,950
Python and stores it in a flash global

00:28:18,429 --> 00:28:22,330
variable list don't never do that it's

00:28:20,950 --> 00:28:25,170
terrible but it was the easiest way to

00:28:22,330 --> 00:28:27,520
come up with the simulated memories it's

00:28:25,170 --> 00:28:30,429
we're gonna see if I run this a few

00:28:27,520 --> 00:28:33,520
times this memory usage here is going to

00:28:30,429 --> 00:28:36,130
start to trickle up it should spread

00:28:33,520 --> 00:28:38,730
this across them at once I'm going

00:28:36,130 --> 00:28:38,730
around a couple times

00:28:42,050 --> 00:28:49,170
your pods would run out of memory they

00:28:46,640 --> 00:28:51,630
would run and kill your pods and like

00:28:49,170 --> 00:28:53,490
you dad just like things can be hard to

00:28:51,630 --> 00:28:57,720
debug whatever but like you'd have

00:28:53,490 --> 00:29:01,380
instability without downtime right so

00:28:57,720 --> 00:29:02,820
let's see yeah so this is 30 bags and my

00:29:01,380 --> 00:29:04,860
there's some latency here so it's like

00:29:02,820 --> 00:29:10,230
some time to catch up but what I can do

00:29:04,860 --> 00:29:12,180
is Oh see get HPA again so now let's say

00:29:10,230 --> 00:29:14,130
you're like I'm using 13% out of my

00:29:12,180 --> 00:29:17,280
target 25% so what I'm telling this

00:29:14,130 --> 00:29:19,200
horizontal Potter's gotta do is if this

00:29:17,280 --> 00:29:21,900
deployment configs start using more than

00:29:19,200 --> 00:29:23,970
25% of its memory targets and start

00:29:21,900 --> 00:29:27,300
scaling that up to a max of 10 so if I

00:29:23,970 --> 00:29:29,100
incur this guy a few more you know I

00:29:27,300 --> 00:29:32,720
should be running this in coal and in

00:29:29,100 --> 00:29:32,720
the browser

00:29:34,580 --> 00:29:40,610
because of sticky sessions what's

00:29:38,960 --> 00:29:42,380
happening a second ago is all the

00:29:40,610 --> 00:29:46,190
requests were probably going to one

00:29:42,380 --> 00:29:51,140
endpoint I want them to evenly gets

00:29:46,190 --> 00:29:53,210
killed so 32% so in a second

00:29:51,140 --> 00:29:55,610
this should def and three to a higher

00:29:53,210 --> 00:29:57,670
number I'm gonna do this a couple more

00:29:55,610 --> 00:29:57,670
times

00:30:01,690 --> 00:30:09,020
so describe no not just yet so again you

00:30:07,580 --> 00:30:10,760
can see now there's four pots and that

00:30:09,020 --> 00:30:13,670
will just happen on mag Laden up to ten

00:30:10,760 --> 00:30:17,540
pots and so back to comparing this to

00:30:13,670 --> 00:30:19,220
before open ship before kubernetes like

00:30:17,540 --> 00:30:21,080
I ran a production environment that

00:30:19,220 --> 00:30:24,220
running on a bunch of the ends and this

00:30:21,080 --> 00:30:27,500
was my dream to get to write we had

00:30:24,220 --> 00:30:28,970
really common like usage cycles I'm sure

00:30:27,500 --> 00:30:31,880
a lot of you are familiar with this like

00:30:28,970 --> 00:30:34,160
during the day traffic picks up and it

00:30:31,880 --> 00:30:37,340
goes down maybe it follows like a global

00:30:34,160 --> 00:30:39,050
pattern of time zones and kind of lazy

00:30:37,340 --> 00:30:41,240
way to do this and I think a lot of us

00:30:39,050 --> 00:30:43,880
do is you just scare replication up

00:30:41,240 --> 00:30:45,350
really high and you just you account for

00:30:43,880 --> 00:30:47,390
the peak and then maybe you double your

00:30:45,350 --> 00:30:49,070
peak right so you just that's capacity

00:30:47,390 --> 00:30:52,160
planning right I'm just I'm super over

00:30:49,070 --> 00:30:58,790
over-allocated of us budget gets like

00:30:52,160 --> 00:31:00,680
massive it takes into account how much

00:30:58,790 --> 00:31:05,060
usage you're using and what your road is

00:31:00,680 --> 00:31:07,640
and like that's a demon it is hard to do

00:31:05,060 --> 00:31:09,650
with VMs and it is really easy to do

00:31:07,640 --> 00:31:11,870
with open ship and with kubernetes like

00:31:09,650 --> 00:31:14,180
that took two minutes like that was

00:31:11,870 --> 00:31:16,910
super easy and now if you remember you

00:31:14,180 --> 00:31:18,380
so the limitation of this is it's

00:31:16,910 --> 00:31:21,050
currently at least as far as I'm aware

00:31:18,380 --> 00:31:23,630
it's only based on CPU usage or memory

00:31:21,050 --> 00:31:26,660
usage but if your workload like if

00:31:23,630 --> 00:31:28,370
that's if you fit within that box this

00:31:26,660 --> 00:31:31,090
is so easy to do and you should play

00:31:28,370 --> 00:31:33,140
with it is pada pada pada auto-scaling

00:31:31,090 --> 00:31:35,630
so play with it

00:31:33,140 --> 00:31:38,059
all right we got 14 more minutes I'm

00:31:35,630 --> 00:31:40,429
gonna go back to the slides now bear

00:31:38,059 --> 00:31:42,970
with me because I do want my notes for

00:31:40,429 --> 00:31:42,970
the rest of this

00:31:44,200 --> 00:31:48,009
when I'm caught for a second

00:31:54,760 --> 00:31:57,760
extended

00:32:09,180 --> 00:32:13,170
alright we're back action so that was

00:32:11,040 --> 00:32:15,060
that was scaling and I think I'd talked

00:32:13,170 --> 00:32:19,490
about everything I wanted to do

00:32:15,060 --> 00:32:19,490
all right come on Mouse where I

00:32:21,060 --> 00:32:26,130
like my screen so I want to do all right

00:32:24,150 --> 00:32:27,900
I'm good now right so the last thing

00:32:26,130 --> 00:32:29,430
that's like specific weapon ship

00:32:27,900 --> 00:32:31,470
features that one talk about is complex

00:32:29,430 --> 00:32:36,000
rollout strategies so this is really

00:32:31,470 --> 00:32:37,680
designed to address something that like

00:32:36,000 --> 00:32:39,810
example of a cast before I remember I

00:32:37,680 --> 00:32:41,790
talked about like you upgraded your

00:32:39,810 --> 00:32:44,520
software and maybe introduced abroad or

00:32:41,790 --> 00:32:46,890
whatever right and like a really common

00:32:44,520 --> 00:32:50,270
way of addressing that is to leverage

00:32:46,890 --> 00:32:53,640
something called a Bluegreen Department

00:32:50,270 --> 00:32:55,380
is you maintain two instances of your

00:32:53,640 --> 00:32:56,730
application right you have the old

00:32:55,380 --> 00:32:57,780
application that you know works and

00:32:56,730 --> 00:33:02,400
traffic's going to it today and

00:32:57,780 --> 00:33:03,930
everything's good again kind of the easy

00:33:02,400 --> 00:33:05,880
way to do this is just sending all of

00:33:03,930 --> 00:33:07,890
your traffic now to the new version but

00:33:05,880 --> 00:33:09,360
that can be dangerous because what if

00:33:07,890 --> 00:33:11,550
you have a bug and then like you break

00:33:09,360 --> 00:33:14,580
all your users and it's no fun so the

00:33:11,550 --> 00:33:16,890
green deployment you should do is run

00:33:14,580 --> 00:33:20,850
both instances in parallel and try and

00:33:16,890 --> 00:33:23,550
migrate some subset of your users to it

00:33:20,850 --> 00:33:25,890
so with the OpenShift what you can do is

00:33:23,550 --> 00:33:28,380
just like deploy multiple instances of

00:33:25,890 --> 00:33:30,120
your deployment config put your service

00:33:28,380 --> 00:33:31,860
in front of both of it you have your man

00:33:30,120 --> 00:33:33,480
now maybe you create a second route and

00:33:31,860 --> 00:33:35,460
you deploy like your dev team or your

00:33:33,480 --> 00:33:37,380
internal users or something at this new

00:33:35,460 --> 00:33:50,490
route test out the application there

00:33:37,380 --> 00:33:52,650
fireworks have time to talk about like

00:33:50,490 --> 00:33:53,970
actually demo here but SEO has this

00:33:52,650 --> 00:33:57,780
thing called a service master it works

00:33:53,970 --> 00:34:00,780
with kubernetes and you can like so the

00:33:57,780 --> 00:34:02,400
bishop is time manually manually with

00:34:00,780 --> 00:34:04,290
SEO it's just like built-in and so you

00:34:02,400 --> 00:34:05,820
can do really cool things like to send

00:34:04,290 --> 00:34:09,270
up your new application and then send a

00:34:05,820 --> 00:34:11,550
specific percentage of your traffic to

00:34:09,270 --> 00:34:13,050
your new deployment like if you have

00:34:11,550 --> 00:34:14,460
different global regions you can say

00:34:13,050 --> 00:34:15,929
send this region over here in this

00:34:14,460 --> 00:34:17,490
region over there like you can do really

00:34:15,929 --> 00:34:20,650
cool things you can orchestrate the flow

00:34:17,490 --> 00:34:23,290
of traffic like to make sure you

00:34:20,650 --> 00:34:27,060
rising your resources like you need to

00:34:23,290 --> 00:34:29,470
really cool stuff so putting aside like

00:34:27,060 --> 00:34:30,640
consider doing Bluegreen deployments if

00:34:29,470 --> 00:34:32,680
you're not because they're kind of group

00:34:30,640 --> 00:34:34,480
and like you know they're they're a good

00:34:32,680 --> 00:34:36,340
way to make sure you're safely upgrading

00:34:34,480 --> 00:34:38,620
and things like that if you really want

00:34:36,340 --> 00:34:39,790
to be cool play with this do cuz it's

00:34:38,620 --> 00:34:43,300
kind of cool and I want to play with it

00:34:39,790 --> 00:34:49,000
more so so that's that's what open ship

00:34:43,300 --> 00:34:50,680
can do to help I do think is like those

00:34:49,000 --> 00:34:53,770
are tools that can really help that a

00:34:50,680 --> 00:34:58,210
lot of this starts with the team and so

00:34:53,770 --> 00:35:00,700
I think in order to really like take

00:34:58,210 --> 00:35:02,830
this chaos head-on and address it like

00:35:00,700 --> 00:35:07,060
you need to change change your team

00:35:02,830 --> 00:35:09,460
culture things I want to specifically

00:35:07,060 --> 00:35:11,370
Mac recommend one is I think you have to

00:35:09,460 --> 00:35:17,020
change what it means for work to be done

00:35:11,370 --> 00:35:17,830
people on my team like I harp on this is

00:35:17,020 --> 00:35:20,020
not production

00:35:17,830 --> 00:35:21,880
unless it's monitored and unless you

00:35:20,020 --> 00:35:24,190
have a plan for scaling and unless you

00:35:21,880 --> 00:35:26,080
can like deploy repeatedly and easily

00:35:24,190 --> 00:35:27,520
and it's documented and like you have

00:35:26,080 --> 00:35:30,089
monitoring and already and the team

00:35:27,520 --> 00:35:32,740
knows what all that is

00:35:30,089 --> 00:35:33,940
my team is probably really tired with me

00:35:32,740 --> 00:35:37,059
saying that but I think the alternative

00:35:33,940 --> 00:35:38,710
is you do prototype you get it working

00:35:37,059 --> 00:35:40,480
on somebody's lap top and then you say

00:35:38,710 --> 00:35:43,270
okay let's deploy this per to production

00:35:40,480 --> 00:35:45,069
and then let's just do that it doesn't

00:35:43,270 --> 00:35:46,150
work right if there's obvious extra

00:35:45,069 --> 00:35:47,230
stuff that goes in you're running in

00:35:46,150 --> 00:35:50,140
production you have to have a plan for

00:35:47,230 --> 00:35:53,079
it and I think as teams we have to

00:35:50,140 --> 00:35:55,180
really adopt that mindset and like hold

00:35:53,079 --> 00:35:56,920
ourselves to a higher standard and this

00:35:55,180 --> 00:35:59,380
can be a hard way like you have you have

00:35:56,920 --> 00:36:00,849
deadlines you have like management says

00:35:59,380 --> 00:36:02,589
this is really easy to your laptop why

00:36:00,849 --> 00:36:04,150
is it not working production yet right I

00:36:02,589 --> 00:36:05,619
think this takes buy-in from from

00:36:04,150 --> 00:36:06,970
multiple levels it takes line from the

00:36:05,619 --> 00:36:09,369
team takes work from the team takes

00:36:06,970 --> 00:36:11,650
fly-in from leadership and I think it's

00:36:09,369 --> 00:36:13,329
a culture of it like we just have to fix

00:36:11,650 --> 00:36:15,940
right so so that's the first thing I

00:36:13,329 --> 00:36:18,549
think we have to do the second thing I

00:36:15,940 --> 00:36:21,980
think we have to do is embrace the

00:36:18,549 --> 00:36:24,119
DevOps model right so

00:36:21,980 --> 00:36:25,650
developer would like the code and just

00:36:24,119 --> 00:36:26,970
just one to write the code and didn't

00:36:25,650 --> 00:36:28,200
want to care about what happens in

00:36:26,970 --> 00:36:29,849
production they kind of just throw it

00:36:28,200 --> 00:36:31,349
over the raff to the ops team is right

00:36:29,849 --> 00:36:33,900
and this is DevOps right if you've read

00:36:31,349 --> 00:36:36,720
you read any DevOps book the PHP project

00:36:33,900 --> 00:36:38,280
is a good one by the way right this is

00:36:36,720 --> 00:36:40,170
DevOps right you have to get the people

00:36:38,280 --> 00:36:41,670
right in the code to at least know

00:36:40,170 --> 00:36:43,319
what's going on in production and then

00:36:41,670 --> 00:36:45,059
they can start like thinking about

00:36:43,319 --> 00:36:46,500
monitoring and maybe instead of writing

00:36:45,059 --> 00:36:48,089
some of the he make application that's

00:36:46,500 --> 00:36:50,339
like impossible monitor they start

00:36:48,089 --> 00:36:52,670
thinking how do I build in prometheus

00:36:50,339 --> 00:36:54,720
metrics into my app or just like head

00:36:52,670 --> 00:36:57,000
architect this in the way that it's

00:36:54,720 --> 00:36:58,559
really easy to scale or monitor and like

00:36:57,000 --> 00:37:00,390
at least get the OP like you still have

00:36:58,559 --> 00:37:05,280
Ops teams you set dev teams to get them

00:37:00,390 --> 00:37:23,339
together and talking and production as a

00:37:05,280 --> 00:37:24,960
result if you simulate this chaos in

00:37:23,339 --> 00:37:26,369
your world you get really good at

00:37:24,960 --> 00:37:30,210
handling you know what to do every

00:37:26,369 --> 00:37:31,829
machine knows what to do and the areas

00:37:30,210 --> 00:37:36,410
of your applications that are not really

00:37:31,829 --> 00:37:36,410
yet robust are resilient right

00:37:38,080 --> 00:38:24,500
so like every 15 seconds a normally you

00:38:22,340 --> 00:38:26,630
configure cue monkey to run at a set

00:38:24,500 --> 00:38:28,460
time everyday and you give it a window

00:38:26,630 --> 00:38:32,150
of time during which it's okay to kill

00:38:28,460 --> 00:38:33,800
pods and you give it parameters for like

00:38:32,150 --> 00:38:36,320
how many pods in a deployment or

00:38:33,800 --> 00:38:38,720
whatever to kill in debug mode you tell

00:38:36,320 --> 00:38:40,490
to run like a regular interval and every

00:38:38,720 --> 00:38:42,620
time it runs to kill pods or moves like

00:38:40,490 --> 00:38:43,970
a rating thing so it's kind of designed

00:38:42,620 --> 00:38:47,210
to deploy in your production environment

00:38:43,970 --> 00:38:49,160
and just turn on and applications can

00:38:47,210 --> 00:38:51,200
opt into it so like it doesn't kill a

00:38:49,160 --> 00:38:54,020
pod unless a an application specifically

00:38:51,200 --> 00:38:55,400
opted into it and so like I think you

00:38:54,020 --> 00:38:57,980
can really leverage this on your team's

00:38:55,400 --> 00:38:59,660
deploy that every team use it and tell

00:38:57,980 --> 00:39:01,370
teams that they can opt in on this

00:38:59,660 --> 00:39:02,450
and like they set the parameters that

00:39:01,370 --> 00:39:03,920
say you have a really important app

00:39:02,450 --> 00:39:06,410
that's like not ready to be brought down

00:39:03,920 --> 00:39:08,420
on my team this is probably why 6h like

00:39:06,410 --> 00:39:16,760
I don't want that those pauses to

00:39:08,420 --> 00:39:18,830
randomly get killed if you just randomly

00:39:16,760 --> 00:39:20,210
kill and you can test your money you

00:39:18,830 --> 00:39:21,800
would test your rating you can test what

00:39:20,210 --> 00:39:24,260
happens to users and it but it just

00:39:21,800 --> 00:39:25,910
gives it to you for free and like it

00:39:24,260 --> 00:39:27,440
will become the norm in your team's will

00:39:25,910 --> 00:39:28,940
just accept it right so what this is

00:39:27,440 --> 00:39:31,760
gonna do is it's gonna point you monkey

00:39:28,940 --> 00:39:34,220
configured in the debug mode it's going

00:39:31,760 --> 00:39:40,180
to deploy another instance of my basic

00:39:34,220 --> 00:39:42,710
class gap that we've been showing off my

00:39:40,180 --> 00:39:44,840
have so far using open ship deployment

00:39:42,710 --> 00:39:47,120
config from what I could tell coop

00:39:44,840 --> 00:39:48,320
monkey requires a kubernetes deployment

00:39:47,120 --> 00:39:50,870
which is slightly different from a

00:39:48,320 --> 00:39:52,790
deployment config but if I go over here

00:39:50,870 --> 00:39:54,770
you'll see I have this cute monkey pod

00:39:52,790 --> 00:39:58,580
that's getting stirred up but I have

00:39:54,770 --> 00:40:02,560
this kid monkey victim pod I think I can

00:39:58,580 --> 00:40:02,560
do this in the yes I have these labels I

00:40:02,650 --> 00:40:07,970
can expand the window this is like my

00:40:05,900 --> 00:40:11,000
first time using over to four so all

00:40:07,970 --> 00:40:14,270
right mr. lucky monkey with any growth

00:40:11,000 --> 00:40:17,420
that means I'm opting in you give it an

00:40:14,270 --> 00:40:19,370
identifier you tell it like a mode which

00:40:17,420 --> 00:40:21,230
is like fixed or random so specific like

00:40:19,370 --> 00:40:22,490
how many of the pods should keep monkey

00:40:21,230 --> 00:40:25,940
kill I'm telling you they're just one at

00:40:22,490 --> 00:40:27,110
a time in time between failures like how

00:40:25,940 --> 00:40:29,170
often they should get killed but in

00:40:27,110 --> 00:40:33,560
debug mode that's kind of an a ignored

00:40:29,170 --> 00:40:35,950
anyways that if I go over to my cube

00:40:33,560 --> 00:40:35,950
monkey

00:40:41,570 --> 00:40:48,410
I could do this in the command line but

00:40:43,310 --> 00:40:51,170
Amari here like you can see it's every

00:40:48,410 --> 00:40:53,600
15 seconds and it's identifying this

00:40:51,170 --> 00:40:55,430
cute monkey victim deployment and it's

00:40:53,600 --> 00:40:57,980
gonna kill it right so every 15 seconds

00:40:55,430 --> 00:40:59,480
my pods getting killed again in

00:40:57,980 --> 00:41:02,660
production you would probably not run

00:40:59,480 --> 00:41:04,160
every 15 seconds but you could in this

00:41:02,660 --> 00:41:21,920
cute monkey victim actually I don't want

00:41:04,160 --> 00:41:24,560
to do that like what kind of things you

00:41:21,920 --> 00:41:27,740
can put in place to watch this is so

00:41:24,560 --> 00:41:29,510
this other line is green now prometheus

00:41:27,740 --> 00:41:31,970
is doing availability checks on my app

00:41:29,510 --> 00:41:34,820
and every 15 seconds or whatever is can

00:41:31,970 --> 00:41:36,230
be read so I can watch that and see my

00:41:34,820 --> 00:41:38,329
service is intermittently

00:41:36,230 --> 00:41:40,220
I should fix that and what can I do oh

00:41:38,329 --> 00:41:41,960
maybe I should do it I can scale it up

00:41:40,220 --> 00:41:43,790
there's just one pot now we already know

00:41:41,960 --> 00:41:45,890
how to scale up to to do that kind of

00:41:43,790 --> 00:41:50,030
thing and so that this this is a really

00:41:45,890 --> 00:41:51,980
easy way to like nobody like it's kind

00:41:50,030 --> 00:41:53,900
of boring to do like availability

00:41:51,980 --> 00:41:55,810
testing on your service right this this

00:41:53,900 --> 00:41:58,339
kind of just does it for you in a way

00:41:55,810 --> 00:42:01,570
and I think like it can be a really good

00:41:58,339 --> 00:42:03,920
tool in your tool chest of building more

00:42:01,570 --> 00:42:06,619
resilient apps more cats resilient apps

00:42:03,920 --> 00:42:09,320
right so I think that that is all I have

00:42:06,619 --> 00:42:11,390
and as promised there's time for

00:42:09,320 --> 00:42:14,300
questions I'm gonna go to the resources

00:42:11,390 --> 00:42:23,839
slide though I have to find the

00:42:14,300 --> 00:42:25,430
resources slide I guess I can open up to

00:42:23,839 --> 00:42:27,010
questions if anybody has questions for

00:42:25,430 --> 00:42:28,300
me

00:42:27,010 --> 00:42:31,160
and I'm all yours yeah here the

00:42:28,300 --> 00:42:34,940
resources so geez

00:42:31,160 --> 00:42:37,220
this has links to the code that I use

00:42:34,940 --> 00:42:38,870
for this to the open beta project and to

00:42:37,220 --> 00:42:41,170
keep monkey in SEO that I talked about a

00:42:38,870 --> 00:42:41,170
little bit

00:42:56,660 --> 00:43:00,500
then so the question was when he died I

00:42:58,880 --> 00:43:02,600
showed the leaving the pod right the

00:43:00,500 --> 00:43:05,020
question was what happens kind of

00:43:02,600 --> 00:43:08,150
undercover is is it a kill is it like a

00:43:05,020 --> 00:43:11,900
like is it a hard or a graceful kill

00:43:08,150 --> 00:43:12,980
that kind of thing I think I don't know

00:43:11,900 --> 00:43:14,450
the answer to that I think that's

00:43:12,980 --> 00:43:17,660
something you can configure with

00:43:14,450 --> 00:43:18,950
OpenShift and you can do like startup

00:43:17,660 --> 00:43:20,150
and shutdown commands and that kind of

00:43:18,950 --> 00:43:21,980
thing and you can kind of build it into

00:43:20,150 --> 00:43:23,270
the app but I'm really glad you asked

00:43:21,980 --> 00:43:25,640
that question because it reminded me of

00:43:23,270 --> 00:43:27,830
something I want to talk about there are

00:43:25,640 --> 00:43:29,840
multiple deployment modes you can

00:43:27,830 --> 00:43:31,310
specify and you can specify recreate

00:43:29,840 --> 00:43:34,250
versus rolling and the difference there

00:43:31,310 --> 00:43:36,500
is I think with recreate it will send

00:43:34,250 --> 00:43:38,570
down your app and then spit up a new pod

00:43:36,500 --> 00:43:39,980
and so like

00:43:38,570 --> 00:43:41,750
database or something like that that's

00:43:39,980 --> 00:43:43,280
you know you can't really run multiple

00:43:41,750 --> 00:43:44,900
instances of is talking to the same

00:43:43,280 --> 00:43:46,880
underlying files you'd want to delete

00:43:44,900 --> 00:43:47,750
the pod before you spit up the new ones

00:43:46,880 --> 00:43:51,260
so there's a little bit of downtime

00:43:47,750 --> 00:43:53,630
right with rolling like spins up a new

00:43:51,260 --> 00:43:55,580
pod then spins down the old pod so you

00:43:53,630 --> 00:43:58,070
don't have that downtime so that's

00:43:55,580 --> 00:44:01,190
another thing you can tweak to handle

00:43:58,070 --> 00:44:03,950
this kind of stuff yeah there are ways

00:44:01,190 --> 00:44:06,380
to specify that kill mode okay and I

00:44:03,950 --> 00:44:08,690
guess the same thing with the cube

00:44:06,380 --> 00:44:11,210
monkey where I would assume that should

00:44:08,690 --> 00:44:12,920
be like a catastrophic right just turn

00:44:11,210 --> 00:44:17,060
off the lights the pods gone right

00:44:12,920 --> 00:44:18,290
that's that's my assumption I don't have

00:44:17,060 --> 00:44:19,310
to look into it more to figure out

00:44:18,290 --> 00:44:22,370
exactly what's happening cuz I agree

00:44:19,310 --> 00:44:26,060
that's what you want maybe there are

00:44:22,370 --> 00:44:27,950
different ways that be an interesting

00:44:26,060 --> 00:44:29,540
thing to contribute is like a way to

00:44:27,950 --> 00:44:31,790
specify how it kills it because then you

00:44:29,540 --> 00:44:33,170
could test like how do your apps like is

00:44:31,790 --> 00:44:35,060
your app configured to die gracefully

00:44:33,170 --> 00:44:38,210
that kind of thing and I'm really glad

00:44:35,060 --> 00:44:40,940
you introduced us because I I wasn't

00:44:38,210 --> 00:44:44,820
aware of a few monkey kubernetes but

00:44:40,940 --> 00:44:46,670
this is massive because this

00:44:44,820 --> 00:44:49,710
anybody's is moving no Cody Cooper nazar

00:44:46,670 --> 00:44:52,290
OpenShift environment to start to adopt

00:44:49,710 --> 00:44:54,990
some of the things that our commonplace

00:44:52,290 --> 00:44:57,120
at Netflix that nobody else that I know

00:44:54,990 --> 00:44:59,970
is pretty doing it I've run into a few

00:44:57,120 --> 00:45:06,440
companies that are supposedly doing you

00:44:59,970 --> 00:45:06,440
know the simian army stuff nobody else

00:45:10,280 --> 00:45:15,720
it wouldn't work finally changes your

00:45:12,960 --> 00:45:17,790
deployment and it did work like once

00:45:15,720 --> 00:45:19,200
you're running it it was so easy to opt

00:45:17,790 --> 00:45:20,910
in you just set those labels on your

00:45:19,200 --> 00:45:22,800
service and now that service is going to

00:45:20,910 --> 00:45:25,530
get killed regularly right it's super

00:45:22,800 --> 00:45:27,450
easy so yeah like we use these tools for

00:45:25,530 --> 00:45:29,280
the traditional deployments we should be

00:45:27,450 --> 00:45:31,550
using them for open ship and kubernetes

00:45:29,280 --> 00:45:31,550
to

00:45:36,880 --> 00:45:43,570
introducing

00:45:38,780 --> 00:45:46,260
cube monkey so I have a question

00:45:43,570 --> 00:45:50,140
so do you think it's also a good way to

00:45:46,260 --> 00:45:51,940
introduce your monkey if you want to if

00:45:50,140 --> 00:45:54,340
you if you want a clear service based on

00:45:51,940 --> 00:45:56,200
a certain condition like I regard I'm

00:45:54,340 --> 00:45:58,900
alive from the fact that you I know you

00:45:56,200 --> 00:46:00,670
said that if you kill anything for 15

00:45:58,900 --> 00:46:02,440
minutes or something but apart from the

00:46:00,670 --> 00:46:04,090
time condition like it's probably a

00:46:02,440 --> 00:46:05,890
health check or anything like that do

00:46:04,090 --> 00:46:10,210
you think it's it's a good way to do it

00:46:05,890 --> 00:46:12,700
yes I know monkey can do right now is

00:46:10,210 --> 00:46:14,830
you can specify like the mean time

00:46:12,700 --> 00:46:17,080
between failure and maybe minimum I

00:46:14,830 --> 00:46:20,470
don't know on your pod deployment

00:46:17,080 --> 00:46:22,690
configuration right as you say like kill

00:46:20,470 --> 00:46:26,230
this everyone day-to-day whatever you

00:46:22,690 --> 00:46:29,410
want right and when cute monkey runs but

00:46:26,230 --> 00:46:31,720
you're not in debug mode it will decide

00:46:29,410 --> 00:46:34,240
like is this eligible for being killed

00:46:31,720 --> 00:46:35,830
and then there's like a random coin flip

00:46:34,240 --> 00:46:39,400
thing it does decide whether or not to

00:46:35,830 --> 00:46:42,580
flip it now so there's like I don't know

00:46:39,400 --> 00:46:44,230
if you raising like more than that like

00:46:42,580 --> 00:46:45,940
you described but again I think that'd

00:46:44,230 --> 00:46:47,080
be like I think you monkeys relatively

00:46:45,940 --> 00:46:50,490
new I think that'd be really cool thing

00:46:47,080 --> 00:46:50,490
to contribute to it

00:46:54,460 --> 00:46:58,940
Alex

00:46:57,240 --> 00:47:01,700
do you know what the plans are for

00:46:58,940 --> 00:47:05,369
extending the auto scaling capabilities

00:47:01,700 --> 00:47:07,160
is operators the answer to that so the

00:47:05,369 --> 00:47:09,329
question is like what are the plans for

00:47:07,160 --> 00:47:10,950
extending the the auto scaling

00:47:09,329 --> 00:47:12,810
capabilities if they plan use like

00:47:10,950 --> 00:47:13,920
operators or whatever for that again I

00:47:12,810 --> 00:47:15,380
don't really know the answer for that

00:47:13,920 --> 00:47:17,099
like I don't

00:47:15,380 --> 00:47:20,250
development of this I'm just somebody

00:47:17,099 --> 00:47:22,770
who's excited to use it I mentioned that

00:47:20,250 --> 00:47:24,869
the auto scaling right now is limited to

00:47:22,770 --> 00:47:26,849
only being able to scale based on CPU

00:47:24,869 --> 00:47:29,099
usage or memory usage I think like I

00:47:26,849 --> 00:47:31,410
really have to be able to do like a

00:47:29,099 --> 00:47:32,730
custom script that decides whether or

00:47:31,410 --> 00:47:34,890
not to scale up and I could do it based

00:47:32,730 --> 00:47:37,980
on like message queue numbers or

00:47:34,890 --> 00:47:39,720
something like that I don't know what

00:47:37,980 --> 00:47:43,040
the plans are for that but I hope that

00:47:39,720 --> 00:47:43,040
that becomes a thing because

00:47:51,099 --> 00:48:05,710
retailers over thank you Alex

00:48:01,430 --> 00:48:05,710

YouTube URL: https://www.youtube.com/watch?v=J_CnE3VZnCg


