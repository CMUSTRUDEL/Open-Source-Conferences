Title: Automatic Configuration of Complex Hardware
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Han Dong

SEUSS is custom operating system we spliced into the backend of a high throughput distributed Serverless platform, Apache OpenWhisk. SEUSS uses an alternative isolation mechanism to containers, Library Operating Systems (LibOSs). LibOSs enable a lightweight snapshotting technique. Snapshotting LibOSs enables two counterintuitive results: 1) although LibOSs inherently replicate system state, SEUSS can cache multiplicatively more functions on a node; 2) although LibOSs can suffer bad "first run" performance, SEUSS is able to reduce cold start times by orders of magnitude. By increasing sharing and decreasing deterministic bringup, SEUSS radically reduces the amount of hardware and cycles required to run a FaaS platform.
Captions: 
	00:00:02,550 --> 00:00:09,600
all right you guys hear me all right so

00:00:06,990 --> 00:00:12,289
my name is Han I'm a grad student at BU

00:00:09,600 --> 00:00:14,850
and this is kind of collaboratory work

00:00:12,289 --> 00:00:19,650
over there somewhere I read had with my

00:00:14,850 --> 00:00:21,750
inter my mentor Sanjay he gave a talk

00:00:19,650 --> 00:00:26,490
earlier about like deep learning stuff

00:00:21,750 --> 00:00:29,100
and here we're kind of trying to kind of

00:00:26,490 --> 00:00:32,940
try to apply that to something that's

00:00:29,100 --> 00:00:35,760
more in the system's inside so so in

00:00:32,940 --> 00:00:38,579
terms of operating systems and physical

00:00:35,760 --> 00:00:40,829
hardware itself so actually before I

00:00:38,579 --> 00:00:43,200
kind of talk about why we want to do

00:00:40,829 --> 00:00:45,120
something called configuration of these

00:00:43,200 --> 00:00:47,840
Hardware kind of start off with some

00:00:45,120 --> 00:00:51,600
motivation about why we want to do this

00:00:47,840 --> 00:00:55,559
so many just talking about a little bit

00:00:51,600 --> 00:00:59,010
of a kind of the evolution of CPU

00:00:55,559 --> 00:01:00,540
architecture history so hopefully all of

00:00:59,010 --> 00:01:03,660
you are familiar with kind of Moore's

00:01:00,540 --> 00:01:05,640
law for the longest time we've had the

00:01:03,660 --> 00:01:09,420
benefit of you know doubling the amount

00:01:05,640 --> 00:01:11,999
of transistors on the actual processor

00:01:09,420 --> 00:01:13,920
and it's it's been on a very nice

00:01:11,999 --> 00:01:15,479
trajectory until about a decade ago

00:01:13,920 --> 00:01:18,950
where we cannot actually physically

00:01:15,479 --> 00:01:22,439
shrink those transistors anymore and and

00:01:18,950 --> 00:01:24,630
one of the one of the things that

00:01:22,439 --> 00:01:27,869
impacted that will was actually the

00:01:24,630 --> 00:01:29,459
gainer scaling so the Dennis scaling

00:01:27,869 --> 00:01:31,529
actually made Moore's law useful because

00:01:29,459 --> 00:01:34,159
it basically said that as you double the

00:01:31,529 --> 00:01:36,810
amount of transistors the amount of like

00:01:34,159 --> 00:01:38,189
thermal dissipation to actually cool the

00:01:36,810 --> 00:01:40,799
increased transistors account actually

00:01:38,189 --> 00:01:42,389
stayed constant so that effective means

00:01:40,799 --> 00:01:44,459
that by doubling a transistor account

00:01:42,389 --> 00:01:45,509
you're using about the same power to run

00:01:44,459 --> 00:01:47,450
your application so you're kind of

00:01:45,509 --> 00:01:50,549
doubling your performance in a way and

00:01:47,450 --> 00:01:52,770
roughly about a decade ago is where this

00:01:50,549 --> 00:01:55,259
trajectory is no longer possible right

00:01:52,770 --> 00:01:57,149
and that's kind of where we are in the

00:01:55,259 --> 00:01:58,529
modern era leading on where we cannot

00:01:57,149 --> 00:02:00,989
actually shrink the transistor die

00:01:58,529 --> 00:02:02,429
anymore without causing tremendous

00:02:00,989 --> 00:02:03,840
heating costs and also impacting

00:02:02,429 --> 00:02:06,630
performance because you can actually

00:02:03,840 --> 00:02:10,140
keep all the other logic operating at

00:02:06,630 --> 00:02:12,300
the same time and to kind of dress this

00:02:10,140 --> 00:02:14,340
from both the software's perspective so

00:02:12,300 --> 00:02:15,690
like systems research and also hardware

00:02:14,340 --> 00:02:16,240
perspective there's been multiple

00:02:15,690 --> 00:02:18,580
approaches

00:02:16,240 --> 00:02:20,500
right because because for the longest

00:02:18,580 --> 00:02:23,110
time systems research we've always

00:02:20,500 --> 00:02:24,670
relied on the fact that every four years

00:02:23,110 --> 00:02:27,460
or two years we can double our

00:02:24,670 --> 00:02:31,030
performance all right by just switching

00:02:27,460 --> 00:02:32,860
to a newer hardware and and that has

00:02:31,030 --> 00:02:35,230
impact in terms of like the design of

00:02:32,860 --> 00:02:39,310
the hardware design of the software etc

00:02:35,230 --> 00:02:41,860
and to kind of tackle this the fact that

00:02:39,310 --> 00:02:45,640
we cannot rely on having faster clock

00:02:41,860 --> 00:02:47,740
cycles a lot of the there's been a lot

00:02:45,640 --> 00:02:50,830
of user level software kind of like this

00:02:47,740 --> 00:02:53,910
to take advantage of off of the hardware

00:02:50,830 --> 00:02:56,230
better right so DB DK is kind of like a

00:02:53,910 --> 00:02:58,120
kind of like a user level library that

00:02:56,230 --> 00:03:00,490
lets you gain raw access to the hardware

00:02:58,120 --> 00:03:02,200
to process a packets and there's also

00:03:00,490 --> 00:03:04,390
added these other libraries to you know

00:03:02,200 --> 00:03:06,790
take advantage of these accelerators

00:03:04,390 --> 00:03:09,370
vector accelerators they're increasingly

00:03:06,790 --> 00:03:12,550
they're very common in almost all kinds

00:03:09,370 --> 00:03:14,890
of data centers and there's also been

00:03:12,550 --> 00:03:17,320
kind of like a kind of explosion

00:03:14,890 --> 00:03:18,520
recently in terms of unit kernels right

00:03:17,320 --> 00:03:20,740
there's all these different unit kernel

00:03:18,520 --> 00:03:23,440
projects where the idea is that you can

00:03:20,740 --> 00:03:25,060
use a high-level language to build a

00:03:23,440 --> 00:03:27,190
kernel that's kind of specialized for

00:03:25,060 --> 00:03:30,600
your application and it's a single

00:03:27,190 --> 00:03:33,160
address for a single process right and

00:03:30,600 --> 00:03:35,350
and the whole point is that a lot of the

00:03:33,160 --> 00:03:37,480
overhead of our traditional operating

00:03:35,350 --> 00:03:39,910
system where we kind of always written

00:03:37,480 --> 00:03:42,810
them because we wanted them to be

00:03:39,910 --> 00:03:45,100
adapted different kinds of workloads etc

00:03:42,810 --> 00:03:46,750
if you really want to shooting for

00:03:45,100 --> 00:03:48,520
performance for like a single set of

00:03:46,750 --> 00:03:51,640
workloads for example it might you know

00:03:48,520 --> 00:03:53,740
kernel might be better for that and at

00:03:51,640 --> 00:03:54,880
the same time there's been kind of a try

00:03:53,740 --> 00:03:56,620
can to keep up different kinds of

00:03:54,880 --> 00:03:57,760
hardware that's been added into a system

00:03:56,620 --> 00:03:59,380
to write for different kinds of

00:03:57,760 --> 00:04:01,690
accelerators

00:03:59,380 --> 00:04:03,160
there's also programmable stuff they've

00:04:01,690 --> 00:04:04,960
added in 2d so this is like a

00:04:03,160 --> 00:04:07,300
programmable SSD you can actually write

00:04:04,960 --> 00:04:09,370
logic in there like code and it will

00:04:07,300 --> 00:04:11,680
actually operate on data as it's being

00:04:09,370 --> 00:04:14,140
written to the SSD there's also

00:04:11,680 --> 00:04:17,350
programmable memory also and then you

00:04:14,140 --> 00:04:19,600
know there's FPGA etc it is also a lot

00:04:17,350 --> 00:04:21,609
more different kinds of like devices to

00:04:19,600 --> 00:04:24,400
talk and there's also different layers

00:04:21,609 --> 00:04:26,410
of memory network memory layers now

00:04:24,400 --> 00:04:28,680
right I think it's about seven or eight

00:04:26,410 --> 00:04:30,720
if you consider like the

00:04:28,680 --> 00:04:34,860
whatever the newest memory the

00:04:30,720 --> 00:04:37,800
non-volatile memory stuff and and with

00:04:34,860 --> 00:04:40,919
these different hardware a question we

00:04:37,800 --> 00:04:43,139
can ask kinda is like how could we so if

00:04:40,919 --> 00:04:45,330
we so we if we are running kind of like

00:04:43,139 --> 00:04:47,940
a customized software for a single

00:04:45,330 --> 00:04:49,919
application where we even customize the

00:04:47,940 --> 00:04:52,560
system itself could we actually

00:04:49,919 --> 00:04:54,210
customize the hardware for a single

00:04:52,560 --> 00:04:55,949
application and what would that mean

00:04:54,210 --> 00:04:57,389
right customizing the hardware the

00:04:55,949 --> 00:05:00,270
hardware is already customized enough

00:04:57,389 --> 00:05:02,400
how much more could you customize it and

00:05:00,270 --> 00:05:05,160
in this case the hardware we're kind of

00:05:02,400 --> 00:05:08,789
focusing on is a network card right so a

00:05:05,160 --> 00:05:10,530
network card exists in all systems you

00:05:08,789 --> 00:05:11,940
know every time you send a packet the

00:05:10,530 --> 00:05:13,740
packet goes through the network card and

00:05:11,940 --> 00:05:15,030
goes up through your operating systems

00:05:13,740 --> 00:05:19,820
that can actually expect you to you the

00:05:15,030 --> 00:05:22,320
data so at a high level this is how the

00:05:19,820 --> 00:05:24,180
the toolchain I guess in a way of how

00:05:22,320 --> 00:05:26,520
how packet receives work in like a

00:05:24,180 --> 00:05:29,039
modern system right you get your packet

00:05:26,520 --> 00:05:32,910
onto the neck and on the neck itself

00:05:29,039 --> 00:05:34,979
there's a set of cues like receive cues

00:05:32,910 --> 00:05:36,570
where the packets are basically every

00:05:34,979 --> 00:05:39,780
packet receives it puts it in this queue

00:05:36,570 --> 00:05:41,130
and the packets here are basically DMA

00:05:39,780 --> 00:05:43,590
to some region of memory that your

00:05:41,130 --> 00:05:45,300
system allocates and with that packet

00:05:43,590 --> 00:05:47,580
data eventually when it's ready to

00:05:45,300 --> 00:05:48,979
inform the software or the device driver

00:05:47,580 --> 00:05:51,419
that you're ready to handle it

00:05:48,979 --> 00:05:53,430
interrupts gets fired and there's like a

00:05:51,419 --> 00:05:57,000
software interrupt handler that then

00:05:53,430 --> 00:05:58,590
processes the data and then passes it up

00:05:57,000 --> 00:06:01,380
to the network stack to your application

00:05:58,590 --> 00:06:03,180
right so transmitting kind of the same

00:06:01,380 --> 00:06:05,729
in Reverse you're just kind of adding

00:06:03,180 --> 00:06:07,169
your payload with the show headers on

00:06:05,729 --> 00:06:10,289
top putting on the transmit queue and

00:06:07,169 --> 00:06:12,120
sending it out right so this pipeline is

00:06:10,289 --> 00:06:17,000
the same for almost all network cards

00:06:12,120 --> 00:06:20,250
and the kind of interesting thing about

00:06:17,000 --> 00:06:22,199
the network card we are specifically

00:06:20,250 --> 00:06:25,949
looking at here is this 10 gig in

00:06:22,199 --> 00:06:29,280
network card from Intel um I think was

00:06:25,949 --> 00:06:31,289
released about maybe eight years ago so

00:06:29,280 --> 00:06:33,120
thinking gee is still kind of being used

00:06:31,289 --> 00:06:35,220
it's not that bad yet but the

00:06:33,120 --> 00:06:38,820
interesting thing about it is that if

00:06:35,220 --> 00:06:42,720
you look at the actual data sheet for

00:06:38,820 --> 00:06:44,640
this network card there's about 5,000

00:06:42,720 --> 00:06:46,260
600 of these registers that exist down

00:06:44,640 --> 00:06:48,960
there that you can actually write into

00:06:46,260 --> 00:06:50,940
and these values are our 32-bit

00:06:48,960 --> 00:06:53,700
registers and they have different impact

00:06:50,940 --> 00:06:56,580
for for different for the way the card

00:06:53,700 --> 00:06:58,200
behaves and a lot of them you know it's

00:06:56,580 --> 00:07:01,320
kind of like a pie graph of different

00:06:58,200 --> 00:07:04,620
different parts of the card and so there

00:07:01,320 --> 00:07:07,650
are a lot of these values and part of my

00:07:04,620 --> 00:07:11,790
job was to kind of see which of these

00:07:07,650 --> 00:07:13,710
values actually make a difference in

00:07:11,790 --> 00:07:17,160
terms of the operation of the device

00:07:13,710 --> 00:07:20,610
right so so here's just a table of some

00:07:17,160 --> 00:07:22,140
of them and and and the configuration

00:07:20,610 --> 00:07:24,660
I'm kind of talking about it's basically

00:07:22,140 --> 00:07:27,240
this I'm just kind of looking at this

00:07:24,660 --> 00:07:31,500
entry right now which is the which is

00:07:27,240 --> 00:07:33,510
the hardware interrupts delay value so

00:07:31,500 --> 00:07:36,960
so this hardware interrupts delay value

00:07:33,510 --> 00:07:39,060
is basically a value that you can set on

00:07:36,960 --> 00:07:42,960
the card where after its received a

00:07:39,060 --> 00:07:44,730
product a packet it delays for a certain

00:07:42,960 --> 00:07:46,320
set of microseconds before firing the

00:07:44,730 --> 00:07:48,450
inter up and letting the software know

00:07:46,320 --> 00:07:50,520
that it's time to process the packet and

00:07:48,450 --> 00:07:52,650
when the pass and when the processor

00:07:50,520 --> 00:07:55,350
gets that inner fired it will basically

00:07:52,650 --> 00:07:57,930
pull as much as they can either through

00:07:55,350 --> 00:07:59,520
some budget or pull until some time but

00:07:57,930 --> 00:08:01,620
and basically try to process as many

00:07:59,520 --> 00:08:04,979
packets as possible that exists on the

00:08:01,620 --> 00:08:06,660
card and the simple math of just

00:08:04,979 --> 00:08:08,940
multiplying these possible values

00:08:06,660 --> 00:08:12,300
together it's a huge configuration state

00:08:08,940 --> 00:08:14,880
right by 10 to 20 different possible

00:08:12,300 --> 00:08:19,650
states that you can set just a very just

00:08:14,880 --> 00:08:21,240
a set of subsets of these and and and

00:08:19,650 --> 00:08:22,470
for the rest of talk were kind of like I

00:08:21,240 --> 00:08:26,669
mentioned we're kind of just interested

00:08:22,470 --> 00:08:28,650
in investigating what like how how could

00:08:26,669 --> 00:08:32,280
we configure this value for your

00:08:28,650 --> 00:08:35,880
application like delaying the delay into

00:08:32,280 --> 00:08:37,500
hardware interrupt value and then the

00:08:35,880 --> 00:08:41,099
kind of general question of kind of this

00:08:37,500 --> 00:08:42,839
project is like can you actually do in

00:08:41,099 --> 00:08:47,810
an automated way where you don't have to

00:08:42,839 --> 00:08:50,370
manually either manually run your

00:08:47,810 --> 00:08:52,650
benchmark to kind of figure out how how

00:08:50,370 --> 00:08:55,740
to how to how to change this complexity

00:08:52,650 --> 00:08:59,279
of setting all these values

00:08:55,740 --> 00:09:01,110
and the and the the error were kind of

00:08:59,279 --> 00:09:03,029
senses they're like like a machine

00:09:01,110 --> 00:09:04,800
learning section we're kind of trying to

00:09:03,029 --> 00:09:08,040
use machine learning techniques to kind

00:09:04,800 --> 00:09:12,510
of learn about the best way to configure

00:09:08,040 --> 00:09:15,240
this device and I'll get on to that kind

00:09:12,510 --> 00:09:16,500
of towards the end of this talk the

00:09:15,240 --> 00:09:18,899
first part of this talk is kind of just

00:09:16,500 --> 00:09:20,399
investigating whether where they're

00:09:18,899 --> 00:09:23,270
actually setting this value makes a

00:09:20,399 --> 00:09:25,950
difference for a set of applications and

00:09:23,270 --> 00:09:27,570
what would that mean when we actually

00:09:25,950 --> 00:09:32,750
apply it using machine learning

00:09:27,570 --> 00:09:37,440
techniques to learn about these okay

00:09:32,750 --> 00:09:40,140
okay I just give that slide yes so so

00:09:37,440 --> 00:09:41,940
the kind of question is first question

00:09:40,140 --> 00:09:43,800
is if we just change these hardware

00:09:41,940 --> 00:09:46,290
configuration parameters from whatever

00:09:43,800 --> 00:09:48,120
the default values are being set by

00:09:46,290 --> 00:09:52,050
Linux could we get better performance

00:09:48,120 --> 00:09:54,149
for an application in this case we kind

00:09:52,050 --> 00:09:55,980
of use a very simple toy problem just to

00:09:54,149 --> 00:09:57,870
understand end to end because we don't

00:09:55,980 --> 00:09:59,790
want something complicated where you're

00:09:57,870 --> 00:10:01,500
running a high end like benchmarking

00:09:59,790 --> 00:10:04,380
tool where packets are coming in some

00:10:01,500 --> 00:10:05,940
like randomly sampled distribution right

00:10:04,380 --> 00:10:08,310
we want a simple thing where we can

00:10:05,940 --> 00:10:09,839
reason about like okay just how many

00:10:08,310 --> 00:10:11,160
packets I should be getting just how

00:10:09,839 --> 00:10:13,230
many interrupts I think that should be

00:10:11,160 --> 00:10:15,390
getting etc so in this case we kind of

00:10:13,230 --> 00:10:17,279
have like a application called net pipe

00:10:15,390 --> 00:10:21,420
where you kind of just send between two

00:10:17,279 --> 00:10:22,500
machines that are under VLAN and then so

00:10:21,420 --> 00:10:24,029
there's mostly quiet

00:10:22,500 --> 00:10:25,920
they're basically connected through a

00:10:24,029 --> 00:10:28,709
switch and they just ping-pong back back

00:10:25,920 --> 00:10:31,680
and forth each other with with a packet

00:10:28,709 --> 00:10:34,140
of size M for some n iterations and it's

00:10:31,680 --> 00:10:36,089
a synchronous benchmark where a guy

00:10:34,140 --> 00:10:37,860
sends it and then the other guy receives

00:10:36,089 --> 00:10:40,649
it it sends a response back which is the

00:10:37,860 --> 00:10:42,810
same packet and maybe some backpack etc

00:10:40,649 --> 00:10:44,430
and we're just measuring like what is

00:10:42,810 --> 00:10:47,520
the throughput to do this ping-pong back

00:10:44,430 --> 00:10:49,079
and forth right and and the and the knob

00:10:47,520 --> 00:10:52,010
we're turning here is this interrupts

00:10:49,079 --> 00:10:55,230
delay value so this interval delay value

00:10:52,010 --> 00:10:57,300
just affects the single machine where

00:10:55,230 --> 00:10:59,339
the single machine is is the is the

00:10:57,300 --> 00:11:01,380
server machine where we're saying we're

00:10:59,339 --> 00:11:02,910
gonna keep everything else as in kind of

00:11:01,380 --> 00:11:05,040
like a black box we don't really care

00:11:02,910 --> 00:11:07,080
how to configure eight how they're

00:11:05,040 --> 00:11:09,540
configured outside of the single machine

00:11:07,080 --> 00:11:17,310
we just want the server to be configured

00:11:09,540 --> 00:11:19,350
where we tune the area of delay so so so

00:11:17,310 --> 00:11:21,510
initially we kind of do like a very

00:11:19,350 --> 00:11:23,010
static search so and that's what kind of

00:11:21,510 --> 00:11:26,370
all these graphs are showing where on

00:11:23,010 --> 00:11:29,010
the x-axis we're kind of just tuning or

00:11:26,370 --> 00:11:31,200
changing the area of delay values and on

00:11:29,010 --> 00:11:33,420
the y-axis is the measure throughput and

00:11:31,200 --> 00:11:34,890
then all these different bars are kind

00:11:33,420 --> 00:11:36,900
of for like different different message

00:11:34,890 --> 00:11:38,970
sizes that we test it with and we kind

00:11:36,900 --> 00:11:41,340
of generated like a 3d surface of them

00:11:38,970 --> 00:11:44,160
that we've been just adding to it

00:11:41,340 --> 00:11:45,570
because it's actually it would take a

00:11:44,160 --> 00:11:46,950
very long time to run off hospital

00:11:45,570 --> 00:11:48,930
message size it's all possible to lay

00:11:46,950 --> 00:11:51,000
values right so we're kind of just took

00:11:48,930 --> 00:11:52,650
a kind of like a random sweep through

00:11:51,000 --> 00:11:56,430
some exercise to see whether there was

00:11:52,650 --> 00:11:59,280
anything and the kind of interesting we

00:11:56,430 --> 00:12:02,160
found was at a message size of 10

00:11:59,280 --> 00:12:05,400
kilobytes or roughly in this range if

00:12:02,160 --> 00:12:07,710
you set the interrupt delay value to

00:12:05,400 --> 00:12:10,290
about 40 microsecond you get about like

00:12:07,710 --> 00:12:13,020
a 50% increase in your throughput versus

00:12:10,290 --> 00:12:14,700
compared to 20 micro second obviously

00:12:13,020 --> 00:12:16,530
there was a bunch more but I'm just

00:12:14,700 --> 00:12:19,500
showing a couple ones to show you and

00:12:16,530 --> 00:12:21,810
the default one is actually the policy

00:12:19,500 --> 00:12:23,990
that exists currently which is a kind of

00:12:21,810 --> 00:12:26,790
more dynamic policy inside Linux where

00:12:23,990 --> 00:12:28,710
every time you get an interrupt it uses

00:12:26,790 --> 00:12:30,510
some statistics of how many packets

00:12:28,710 --> 00:12:33,450
receive it's saying so far and it

00:12:30,510 --> 00:12:36,180
computes like a new interval value that

00:12:33,450 --> 00:12:38,310
is within some range for example so it's

00:12:36,180 --> 00:12:41,010
kind of more dynamic policy which will

00:12:38,310 --> 00:12:43,740
make sense because Linux is kind of like

00:12:41,010 --> 00:12:45,120
the entire yeah which kind of makes

00:12:43,740 --> 00:12:46,620
sense because it's hopefully more

00:12:45,120 --> 00:12:48,330
adaptive to different kinds of workloads

00:12:46,620 --> 00:12:50,580
but in this case since you're just

00:12:48,330 --> 00:12:52,020
measuring one we're hoping that maybe it

00:12:50,580 --> 00:12:54,480
could adapt to it too because it's just

00:12:52,020 --> 00:12:56,640
sending one packet back and forth but

00:12:54,480 --> 00:12:59,520
it's kind of interesting that at a very

00:12:56,640 --> 00:13:01,650
specific era of delay value a very

00:12:59,520 --> 00:13:03,540
static point so in this case we're doing

00:13:01,650 --> 00:13:05,130
this statically so we just set up so we

00:13:03,540 --> 00:13:06,630
just set the Internet delay value once

00:13:05,130 --> 00:13:09,870
and we never touch it and we just leave

00:13:06,630 --> 00:13:11,610
it right and and it's kind of

00:13:09,870 --> 00:13:13,320
interesting to figure out what is going

00:13:11,610 --> 00:13:15,870
out here how are you able to get like a

00:13:13,320 --> 00:13:17,280
50% boost in your throughput by setting

00:13:15,870 --> 00:13:20,010
your era of delay value to 40

00:13:17,280 --> 00:13:23,130
microseconds at a very specific minute

00:13:20,010 --> 00:13:26,000
right so one of the things that we tried

00:13:23,130 --> 00:13:28,710
looking at is basically adding

00:13:26,000 --> 00:13:31,890
instrumenting code to kind of log every

00:13:28,710 --> 00:13:35,720
single packet it's seen when when the

00:13:31,890 --> 00:13:37,650
interweb gets fired so so so in Linux

00:13:35,720 --> 00:13:41,070
packet processing there's something

00:13:37,650 --> 00:13:44,070
called new api which is basically every

00:13:41,070 --> 00:13:46,770
time a interrupt gets fired and your

00:13:44,070 --> 00:13:49,230
inner arkanar is called you're basically

00:13:46,770 --> 00:13:50,670
at that point you basically you have a

00:13:49,230 --> 00:13:54,150
budget for how many packets you can

00:13:50,670 --> 00:13:55,980
process before you before you before you

00:13:54,150 --> 00:13:58,560
get contact stretched out or your stop

00:13:55,980 --> 00:13:59,970
processing right so so so that budget is

00:13:58,560 --> 00:14:02,370
a limit for how many packets you can

00:13:59,970 --> 00:14:04,980
process at once once the area of get

00:14:02,370 --> 00:14:07,590
fired so if there aren't enough packets

00:14:04,980 --> 00:14:09,900
to be processed then then you just

00:14:07,590 --> 00:14:12,200
process as much as possible and you stop

00:14:09,900 --> 00:14:23,310
otherwise you have a budget for how many

00:14:12,200 --> 00:14:25,820
it yeah yeah but there but when you're

00:14:23,310 --> 00:14:28,740
sending a packet it could get us

00:14:25,820 --> 00:14:30,210
depending on how TCP does it right there

00:14:28,740 --> 00:14:33,210
could be multiple packets in which case

00:14:30,210 --> 00:14:34,920
yeah so yeah so that's kind of the

00:14:33,210 --> 00:14:37,350
effect we were trying to see which is

00:14:34,920 --> 00:14:39,000
where like how and and and this

00:14:37,350 --> 00:14:41,010
measurement here is basically other than

00:14:39,000 --> 00:14:43,740
the payload packet how many other

00:14:41,010 --> 00:14:46,200
packets are knowledgeable PAC is right

00:14:43,740 --> 00:14:48,840
so this means like on average for every

00:14:46,200 --> 00:14:52,160
single receive or every single interrupt

00:14:48,840 --> 00:14:54,870
that gets fired this is this is how many

00:14:52,160 --> 00:14:56,910
acknowledgment packets were appended to

00:14:54,870 --> 00:15:00,150
the to the actual payload packet right

00:14:56,910 --> 00:15:01,710
because in TCP they'd do something like

00:15:00,150 --> 00:15:05,190
piggybacking where you piggyback your

00:15:01,710 --> 00:15:07,110
Nora packets right so so what this kind

00:15:05,190 --> 00:15:10,820
of show these things in this example is

00:15:07,110 --> 00:15:13,470
that at a 40 micro second delay we are

00:15:10,820 --> 00:15:17,280
we are more I guess efficient in terms

00:15:13,470 --> 00:15:19,320
of processing apps as in we we hit some

00:15:17,280 --> 00:15:21,390
threshold and some delay value where

00:15:19,320 --> 00:15:23,670
we're being more efficient in terms of

00:15:21,390 --> 00:15:25,710
how we process you know acts because

00:15:23,670 --> 00:15:30,210
we're processing more acts than these

00:15:25,710 --> 00:15:32,189
other examples so I mean that doesn't

00:15:30,210 --> 00:15:35,410
necessarily explain

00:15:32,189 --> 00:15:36,999
everything is just a metric that we

00:15:35,410 --> 00:15:40,179
tried to measure to try to understand

00:15:36,999 --> 00:15:42,959
this for a nominal that we saw a couple

00:15:40,179 --> 00:15:46,689
other metrics that we measured was a

00:15:42,959 --> 00:15:48,730
demerit interrupts I got fired total for

00:15:46,689 --> 00:15:51,579
this workload also the how much

00:15:48,730 --> 00:15:56,049
instructions were run throughout this

00:15:51,579 --> 00:15:58,149
benchmark right so so for example we see

00:15:56,049 --> 00:16:00,879
so one interesting thing is we actually

00:15:58,149 --> 00:16:02,829
ran way less instructions here compared

00:16:00,879 --> 00:16:05,980
to everything else we also have way less

00:16:02,829 --> 00:16:07,689
interrupts everything else and and and

00:16:05,980 --> 00:16:09,129
the worker is the same across all of

00:16:07,689 --> 00:16:11,049
these right so we're just sending em

00:16:09,129 --> 00:16:12,279
packets and times and it's the same

00:16:11,049 --> 00:16:15,429
across all of these and the only

00:16:12,279 --> 00:16:18,249
hardware parameter we change was this so

00:16:15,429 --> 00:16:20,199
so you may think about so kind of our

00:16:18,249 --> 00:16:22,420
intuition for what is going on here is

00:16:20,199 --> 00:16:24,279
the fact that we are we are more

00:16:22,420 --> 00:16:27,100
efficient in terms of polling in a way

00:16:24,279 --> 00:16:28,809
because because because because every

00:16:27,100 --> 00:16:30,730
time a Tanner gets fired your card is

00:16:28,809 --> 00:16:33,189
doing a sort of polling to pull as much

00:16:30,730 --> 00:16:34,989
back as if possible but if there if but

00:16:33,189 --> 00:16:36,670
if your error upgrade is not correct in

00:16:34,989 --> 00:16:38,619
terms of optimizing the amount of

00:16:36,670 --> 00:16:40,420
packets coming in so you're polling in a

00:16:38,619 --> 00:16:42,999
very much more efficient rate then

00:16:40,420 --> 00:16:44,589
you're just wasting CPU cycles right so

00:16:42,999 --> 00:16:46,509
what this kind of shows us is that at

00:16:44,589 --> 00:16:48,189
this forty microcircuit second delay we

00:16:46,509 --> 00:16:50,259
hit some threshold with the other app

00:16:48,189 --> 00:16:52,779
with with the message size and the other

00:16:50,259 --> 00:16:56,889
client that we are able to be much more

00:16:52,779 --> 00:17:00,790
efficient in terms of processing packets

00:16:56,889 --> 00:17:03,670
basically and and a lot of this is this

00:17:00,790 --> 00:17:05,260
kind of trying to look at look at

00:17:03,670 --> 00:17:06,939
measurements that exist in two systems

00:17:05,260 --> 00:17:09,610
as are measuring how how it's running

00:17:06,939 --> 00:17:14,679
right when when we're kind of tuning

00:17:09,610 --> 00:17:20,559
these Hardware parameters yeah no that's

00:17:14,679 --> 00:17:23,949
fine yeah this is the yeah yeah yeah

00:17:20,559 --> 00:17:26,459
which is not necessary 5,000 because for

00:17:23,949 --> 00:17:29,649
this run we're sending 10,000 kilobytes

00:17:26,459 --> 00:17:31,690
5,000 times so that kind of implies that

00:17:29,649 --> 00:17:33,639
the TCP stack probably split the

00:17:31,690 --> 00:17:35,200
packages to multiple short sizes and

00:17:33,639 --> 00:17:40,029
sent those instead and we're getting a

00:17:35,200 --> 00:17:44,049
bunch of interrupts there yeah it's yeah

00:17:40,029 --> 00:17:45,910
mt is 1,500 it's just a default yeah but

00:17:44,049 --> 00:17:48,160
the other unintuitive

00:17:45,910 --> 00:17:50,470
might be is that the fact that for the

00:17:48,160 --> 00:17:51,910
for the default one which is the Linux

00:17:50,470 --> 00:17:54,130
default policy it has way less

00:17:51,910 --> 00:17:58,600
interrupts than anything we've seen

00:17:54,130 --> 00:18:00,070
before and it's it's the destructions is

00:17:58,600 --> 00:18:02,920
slightly lower than the other ones but

00:18:00,070 --> 00:18:05,890
not not as much right it's also able to

00:18:02,920 --> 00:18:08,050
process the axe very efficiently and for

00:18:05,890 --> 00:18:10,090
some reason it's stupid is the same as

00:18:08,050 --> 00:18:13,240
the others right so that's kind of weird

00:18:10,090 --> 00:18:15,100
and and to kind of try to investigate

00:18:13,240 --> 00:18:17,920
what's what's going on that in that in

00:18:15,100 --> 00:18:20,970
the dynamic policy we basically logged

00:18:17,920 --> 00:18:23,920
throughout the whole run all the

00:18:20,970 --> 00:18:25,870
everytime Linux updated is interrupt we

00:18:23,920 --> 00:18:28,330
just stored it and this is a dump of it

00:18:25,870 --> 00:18:30,430
right so this is what the actual dynamic

00:18:28,330 --> 00:18:32,770
policy is doing throughout the run it's

00:18:30,430 --> 00:18:35,350
I think what this PC shows is that for

00:18:32,770 --> 00:18:37,090
this benchmark it probably not it's it's

00:18:35,350 --> 00:18:38,860
not doing a good job of adapting to it

00:18:37,090 --> 00:18:40,600
right it's doing like a large search

00:18:38,860 --> 00:18:43,480
throughout this space and the

00:18:40,600 --> 00:18:45,490
interesting thing about this is that by

00:18:43,480 --> 00:18:47,410
default it only sets its interrupt

00:18:45,490 --> 00:18:48,850
values from zero to about a hundred and

00:18:47,410 --> 00:18:51,520
twenty something so that's what this

00:18:48,850 --> 00:18:53,650
peak here is but but you can actually

00:18:51,520 --> 00:18:57,100
set it way higher up to about over a

00:18:53,650 --> 00:18:58,420
thousand microsecond if you want so so

00:18:57,100 --> 00:19:00,160
so what this kind of this kind of

00:18:58,420 --> 00:19:02,080
explains that maybe at some point it's

00:19:00,160 --> 00:19:03,430
like delaying too long right so it's

00:19:02,080 --> 00:19:05,350
delaying too long it's not responding

00:19:03,430 --> 00:19:07,900
back fast enough in which case you're

00:19:05,350 --> 00:19:09,760
kind of suffering the since you're not

00:19:07,900 --> 00:19:11,260
responding fast enough it's the sender's

00:19:09,760 --> 00:19:12,670
not responding fast enough either and

00:19:11,260 --> 00:19:16,660
you're kind of stalling yourself in a

00:19:12,670 --> 00:19:18,760
way so this could be yeah so I'm not

00:19:16,660 --> 00:19:20,230
saying the policy is bad in any way it

00:19:18,760 --> 00:19:22,930
is for this workload it's probably not

00:19:20,230 --> 00:19:24,670
doing as well as it could another

00:19:22,930 --> 00:19:26,350
workload that we ran that could be

00:19:24,670 --> 00:19:29,230
probably more realistic than just paying

00:19:26,350 --> 00:19:31,030
a package back-and-forth is actually

00:19:29,230 --> 00:19:32,560
memcache key right so memcache key is

00:19:31,030 --> 00:19:33,850
commonly used when you're dedicated

00:19:32,560 --> 00:19:36,250
whole system

00:19:33,850 --> 00:19:38,380
it's a America value store and most of

00:19:36,250 --> 00:19:40,690
the time you run it under some SLA right

00:19:38,380 --> 00:19:42,490
you're saying I want 99 percentile of my

00:19:40,690 --> 00:19:45,880
requests under like a thousand

00:19:42,490 --> 00:19:48,100
microseconds for example right and and

00:19:45,880 --> 00:19:51,100
and and our goal here is can you

00:19:48,100 --> 00:19:53,320
maximize is there a way by tuning like

00:19:51,100 --> 00:19:55,120
to interrupt delay to maximize your

00:19:53,320 --> 00:19:57,700
queries per second while while

00:19:55,120 --> 00:19:59,559
maintaining this SLA right which is kind

00:19:57,700 --> 00:20:01,389
of unintuitive because you

00:19:59,559 --> 00:20:02,830
talking about latency-sensitive then

00:20:01,389 --> 00:20:04,480
shouldn't you just be polling all the

00:20:02,830 --> 00:20:06,370
time right you never want to be Delaney

00:20:04,480 --> 00:20:08,080
in Iraq because you want to handle that

00:20:06,370 --> 00:20:11,320
in Arab immediately

00:20:08,080 --> 00:20:15,759
Wow well while maintaining your your

00:20:11,320 --> 00:20:19,480
tailor agency and the interesting that

00:20:15,759 --> 00:20:21,970
we found is we ran this so basically the

00:20:19,480 --> 00:20:23,740
way we read this is we ran the same

00:20:21,970 --> 00:20:25,799
server and client except this is

00:20:23,740 --> 00:20:28,960
multi-core so eight cores on both sides

00:20:25,799 --> 00:20:31,690
and we and we basically measure the

00:20:28,960 --> 00:20:33,519
d-max QPS it could hit or curries per

00:20:31,690 --> 00:20:36,100
second it could hit while maintaining a

00:20:33,519 --> 00:20:37,929
thousand microsecond to latency under a

00:20:36,100 --> 00:20:40,990
99% a latency under a thousand

00:20:37,929 --> 00:20:43,059
microsecond and we just increase we did

00:20:40,990 --> 00:20:45,820
like a linear stand through this delay

00:20:43,059 --> 00:20:49,059
value delaying the interrupts and we

00:20:45,820 --> 00:20:51,850
just kind of plotted this right so it's

00:20:49,059 --> 00:20:55,779
relative an interesting that basically

00:20:51,850 --> 00:20:57,519
as you increase your delay your queries

00:20:55,779 --> 00:21:00,730
per second doesn't really increase at

00:20:57,519 --> 00:21:02,679
all at all nor does it really decrease

00:21:00,730 --> 00:21:04,269
value until you hit these later kind of

00:21:02,679 --> 00:21:05,970
weird cases where you really shouldn't

00:21:04,269 --> 00:21:08,529
delay this too much right

00:21:05,970 --> 00:21:09,909
but the interesting thing if you think

00:21:08,529 --> 00:21:12,220
about what

00:21:09,909 --> 00:21:13,840
what does delay interrupt we love you if

00:21:12,220 --> 00:21:16,269
your delay interrupt that means that

00:21:13,840 --> 00:21:18,549
you're delaying the handler code for the

00:21:16,269 --> 00:21:21,759
interrupts right so if you're able to

00:21:18,549 --> 00:21:23,470
delay as long as possible and in the

00:21:21,759 --> 00:21:25,360
moment that you want to handle it you're

00:21:23,470 --> 00:21:27,519
much more efficient at processing those

00:21:25,360 --> 00:21:30,610
packets what you're actually saving is

00:21:27,519 --> 00:21:32,740
actually power usage right so if you and

00:21:30,610 --> 00:21:35,529
this is the same plot except we measure

00:21:32,740 --> 00:21:38,980
that the amount of power it uses over

00:21:35,529 --> 00:21:42,460
time so so basically what this means is

00:21:38,980 --> 00:21:44,950
that you can actually delay your you

00:21:42,460 --> 00:21:48,129
can't delay the interrupts while

00:21:44,950 --> 00:21:50,230
maintaining the the throughput that you

00:21:48,129 --> 00:21:53,169
got the critters per second but doing it

00:21:50,230 --> 00:21:57,850
at a way doing at lower power to put

00:21:53,169 --> 00:21:59,679
right which is over here and and we and

00:21:57,850 --> 00:22:04,149
we did kind of did a comparison between

00:21:59,679 --> 00:22:07,990
what Linux the default had along with

00:22:04,149 --> 00:22:10,360
Linux with busy people doing so busy poe

00:22:07,990 --> 00:22:12,050
is kind of like a setting you can enable

00:22:10,360 --> 00:22:14,420
on the device where you

00:22:12,050 --> 00:22:15,500
say here's busy I turned out be zero and

00:22:14,420 --> 00:22:17,810
I give you a budget

00:22:15,500 --> 00:22:19,250
well busy PO does is after the interweb

00:22:17,810 --> 00:22:21,440
gets fired and it starts processing

00:22:19,250 --> 00:22:22,910
packets it will actually pull a bit

00:22:21,440 --> 00:22:25,160
longer because it's trying to anticipate

00:22:22,910 --> 00:22:27,530
that you might want to pull a bit

00:22:25,160 --> 00:22:29,840
because your latency bound right and if

00:22:27,530 --> 00:22:32,720
you and then we we calculate the perk or

00:22:29,840 --> 00:22:36,020
interrupt because this is a 1/4 odd off

00:22:32,720 --> 00:22:38,570
and and we compare them right so you can

00:22:36,020 --> 00:22:41,600
see that by doing a static delay of this

00:22:38,570 --> 00:22:45,100
value we actually have a slightly lower

00:22:41,600 --> 00:22:49,220
in error rate than even doing busy PO

00:22:45,100 --> 00:22:52,160
right so I guess the kind of hypothesis

00:22:49,220 --> 00:22:54,920
here is the fact that by doing this kind

00:22:52,160 --> 00:22:57,020
of sad delay you're able to kind of do a

00:22:54,920 --> 00:22:58,700
I guess a smarter version of busy

00:22:57,020 --> 00:23:01,520
polling where where the moment your

00:22:58,700 --> 00:23:03,320
inner up gets fired is when you're when

00:23:01,520 --> 00:23:04,910
you're processing packets at a much more

00:23:03,320 --> 00:23:09,230
optimal rate then you're just randomly

00:23:04,910 --> 00:23:12,560
either through some heuristics or or do

00:23:09,230 --> 00:23:15,170
some actual busy polling and that's kind

00:23:12,560 --> 00:23:18,860
of and and if you actually want a really

00:23:15,170 --> 00:23:19,940
decrease latency which is kind of a

00:23:18,860 --> 00:23:21,980
separate thing that would be more

00:23:19,940 --> 00:23:23,600
systemic changes to the way your system

00:23:21,980 --> 00:23:25,370
stack works right and the way like

00:23:23,600 --> 00:23:27,980
packets are caught copy the user space

00:23:25,370 --> 00:23:30,770
etc in this case we're kind of showing

00:23:27,980 --> 00:23:33,920
that the technique of just delaying your

00:23:30,770 --> 00:23:36,110
inter up or delay when to packet when

00:23:33,920 --> 00:23:38,660
when to do packet processing you're able

00:23:36,110 --> 00:23:43,130
to actually get benefits that may not

00:23:38,660 --> 00:23:45,320
seem obvious and another interesting

00:23:43,130 --> 00:23:49,430
example that we did was instead of worry

00:23:45,320 --> 00:23:51,110
about tail latency for example we can

00:23:49,430 --> 00:23:52,640
just think what would if what if we just

00:23:51,110 --> 00:23:54,350
care about pushing the queries per

00:23:52,640 --> 00:23:56,630
second by increasing the pipeline so

00:23:54,350 --> 00:23:58,160
pipelining here just means that we're

00:23:56,630 --> 00:23:59,810
gonna pipeline sixteen request at a time

00:23:58,160 --> 00:24:02,180
versus earlier where we weren't doing

00:23:59,810 --> 00:24:03,860
any pipelining so if I'm a basically

00:24:02,180 --> 00:24:05,840
destroy your tail latency but you can

00:24:03,860 --> 00:24:10,190
increase your cruise per second for the

00:24:05,840 --> 00:24:12,200
same memcache the workload and here this

00:24:10,190 --> 00:24:15,470
red bar is kind of the the throughput

00:24:12,200 --> 00:24:17,450
where Linux is and here with this we

00:24:15,470 --> 00:24:19,970
actually found that if you increase your

00:24:17,450 --> 00:24:21,920
delay to a completely unintuitive level

00:24:19,970 --> 00:24:23,510
of eight hundred microseconds you

00:24:21,920 --> 00:24:26,590
actually do the best in terms of

00:24:23,510 --> 00:24:29,150
maximizing your off per sec

00:24:26,590 --> 00:24:32,750
so we still do spend some time to kind

00:24:29,150 --> 00:24:36,140
of understand this value because almost

00:24:32,750 --> 00:24:39,640
no network cards set your delay value

00:24:36,140 --> 00:24:39,640
this high yeah

00:24:48,500 --> 00:24:54,289
yeah when the first packet comes it will

00:24:53,570 --> 00:24:57,320
wait

00:24:54,289 --> 00:25:02,510
the amount of time before firing so

00:24:57,320 --> 00:25:04,039
after the first the controller will like

00:25:02,510 --> 00:25:05,870
the I'm talking about the the actual

00:25:04,039 --> 00:25:08,419
hardware the fires the interrupt

00:25:05,870 --> 00:25:13,159
Trudy device driver that that then

00:25:08,419 --> 00:25:16,010
handles it through the network stack yes

00:25:13,159 --> 00:25:17,690
because the the hardware will fire a irq

00:25:16,010 --> 00:25:21,580
that you have to register with the

00:25:17,690 --> 00:25:31,760
function and yeah yeah I stopped one

00:25:21,580 --> 00:25:35,480
yeah yep yeah that's the hardware limit

00:25:31,760 --> 00:25:38,299
for that one yeah it's yeah until

00:25:35,480 --> 00:25:39,770
basically a millisecond on that specific

00:25:38,299 --> 00:25:46,010
device that's that's how long you can

00:25:39,770 --> 00:25:49,390
wait max basically so so most of this so

00:25:46,010 --> 00:25:51,559
far is kind of just understanding what

00:25:49,390 --> 00:25:53,960
does what is changing one of these

00:25:51,559 --> 00:25:55,309
hardware parameters mean right the other

00:25:53,960 --> 00:25:57,980
thing we're kind of interested is to

00:25:55,309 --> 00:26:00,559
kind of instead of doing this manual

00:25:57,980 --> 00:26:02,360
search could we do it more smarter by

00:26:00,559 --> 00:26:04,340
kind of feeding it to some sort of

00:26:02,360 --> 00:26:07,429
machine learning algorithm in this case

00:26:04,340 --> 00:26:09,470
reinforcement learning where you know

00:26:07,429 --> 00:26:12,200
you can think of the agent as maybe the

00:26:09,470 --> 00:26:14,720
software or device driver and the

00:26:12,200 --> 00:26:16,520
environment is like the oh no the agents

00:26:14,720 --> 00:26:18,380
may be probably the network card because

00:26:16,520 --> 00:26:20,809
the agent is doing some sort of action

00:26:18,380 --> 00:26:22,400
and the action itself is it's is tuning

00:26:20,809 --> 00:26:24,650
the device is setting the device reserve

00:26:22,400 --> 00:26:26,690
al you and the environment is basically

00:26:24,650 --> 00:26:29,210
your your system software right or your

00:26:26,690 --> 00:26:32,330
application and it's gauges and given

00:26:29,210 --> 00:26:34,309
given that action are you able to get

00:26:32,330 --> 00:26:36,049
some reward in this case like doing

00:26:34,309 --> 00:26:38,630
better in terms of throughput or lower

00:26:36,049 --> 00:26:40,970
latency and and feeding doing this and

00:26:38,630 --> 00:26:45,770
doing this feedback loop of Ave self

00:26:40,970 --> 00:26:47,720
improvement system in this example in

00:26:45,770 --> 00:26:50,960
this case is something that we're kind

00:26:47,720 --> 00:26:53,570
of still working on because it's kind of

00:26:50,960 --> 00:26:55,340
a lot of challenges with the learning

00:26:53,570 --> 00:26:58,880
aspect of this that we're still figuring

00:26:55,340 --> 00:27:00,450
out so I'll kind of talk about some of

00:26:58,880 --> 00:27:01,799
these challenge

00:27:00,450 --> 00:27:04,320
because we're still trying to understand

00:27:01,799 --> 00:27:05,940
both from a like systematic cinematic

00:27:04,320 --> 00:27:08,370
sense of what does changing the hardware

00:27:05,940 --> 00:27:10,140
mean to the system and also how can we

00:27:08,370 --> 00:27:13,200
present this as a problem to evening a

00:27:10,140 --> 00:27:15,390
learning algorithm to learn right so so

00:27:13,200 --> 00:27:17,309
like general you know should we be using

00:27:15,390 --> 00:27:19,890
kind of like supervised for something

00:27:17,309 --> 00:27:21,630
like this unsupervised like where where

00:27:19,890 --> 00:27:24,179
we kind of give it like a like a

00:27:21,630 --> 00:27:25,740
reinforcement learning approach or kind

00:27:24,179 --> 00:27:28,529
of like a more supervised approach where

00:27:25,740 --> 00:27:30,960
we basically run a bunch of experiments

00:27:28,529 --> 00:27:33,450
real experiments and gather raw numbers

00:27:30,960 --> 00:27:34,770
and just feed it to like a like SVM or

00:27:33,450 --> 00:27:37,770
some sort of classifier right with

00:27:34,770 --> 00:27:39,299
actual labeled data or some or or or

00:27:37,770 --> 00:27:41,429
should we stick to some sort of like

00:27:39,299 --> 00:27:43,409
statistical method or some heuristics

00:27:41,429 --> 00:27:44,820
right that is that is more that is

00:27:43,409 --> 00:27:48,960
different than what would hatch right

00:27:44,820 --> 00:27:51,720
now there's also I guess I'm challenges

00:27:48,960 --> 00:27:54,570
of measuring like of goodness because

00:27:51,720 --> 00:27:57,210
because we consider traditional a

00:27:54,570 --> 00:27:58,620
ice-like reinforcement learning stuff

00:27:57,210 --> 00:28:02,130
right it's being applied in stuff like

00:27:58,620 --> 00:28:03,990
robotics an image image processing right

00:28:02,130 --> 00:28:05,820
where you have a clear go of whether

00:28:03,990 --> 00:28:09,299
either it works or it doesn't

00:28:05,820 --> 00:28:11,460
in this case it's more it's not that

00:28:09,299 --> 00:28:13,649
clear-cut whether something is better it

00:28:11,460 --> 00:28:15,630
could be better because for that

00:28:13,649 --> 00:28:17,850
specific message size or task for that

00:28:15,630 --> 00:28:20,279
specific workload the goodness doesn't

00:28:17,850 --> 00:28:24,029
translate as well across different kinds

00:28:20,279 --> 00:28:26,549
of workloads and and and I guess it's

00:28:24,029 --> 00:28:27,779
important to kind of that and something

00:28:26,549 --> 00:28:29,309
that we're still trying to understand

00:28:27,779 --> 00:28:32,340
which is how do we how do we measure

00:28:29,309 --> 00:28:37,730
this to kind of understand whether it is

00:28:32,340 --> 00:28:41,309
doing good at all I guess and also

00:28:37,730 --> 00:28:44,659
applying this to actual hardware is kind

00:28:41,309 --> 00:28:44,659
of it on no error right yep

00:29:25,970 --> 00:29:33,060
yeah yeah yeah that's yeah that's a like

00:29:30,870 --> 00:29:37,440
another box of questions to be asked

00:29:33,060 --> 00:29:39,900
yeah I think yeah yeah I'm not I I

00:29:37,440 --> 00:29:42,990
honestly I don't know what the answer to

00:29:39,900 --> 00:29:45,510
that solution is weather yeah weather is

00:29:42,990 --> 00:29:48,600
possible but if we just focus on making

00:29:45,510 --> 00:29:50,730
like one application work in a way I

00:29:48,600 --> 00:29:54,090
think that's for me that's like a

00:29:50,730 --> 00:29:55,680
progress because like like the the issue

00:29:54,090 --> 00:29:58,890
here also is the fact that we are

00:29:55,680 --> 00:30:00,540
actually learning from the physical

00:29:58,890 --> 00:30:03,030
hardware itself and how it operates

00:30:00,540 --> 00:30:05,070
right you'd like how can you actually

00:30:03,030 --> 00:30:06,810
even simulate this cause like to

00:30:05,070 --> 00:30:08,040
actually run each of these experiments

00:30:06,810 --> 00:30:10,170
is very expensive because you have to

00:30:08,040 --> 00:30:11,820
run through the entire software stack

00:30:10,170 --> 00:30:14,220
and your hardware itself and there's all

00:30:11,820 --> 00:30:16,560
these interactions that you you do not

00:30:14,220 --> 00:30:18,660
have control over versus maybe a more

00:30:16,560 --> 00:30:21,000
simulated environment right maybe that's

00:30:18,660 --> 00:30:23,370
also part of the challenge which is like

00:30:21,000 --> 00:30:30,030
how do we extract meaningful data from

00:30:23,370 --> 00:30:32,490
this and yeah so yeah that's kind of

00:30:30,030 --> 00:30:34,350
rough end because the learning aspect

00:30:32,490 --> 00:30:36,690
we're kind of working we're still

00:30:34,350 --> 00:30:47,750
working on hopefully maybe I'll have

00:30:36,690 --> 00:30:47,750
much better start to show ya

00:30:55,909 --> 00:31:01,729
well if we would so the policy of

00:30:59,449 --> 00:31:04,159
setting to interrupt away is in the

00:31:01,729 --> 00:31:05,509
device driver itself it's indeed yet

00:31:04,159 --> 00:31:19,159
yeah it's in the device driver code

00:31:05,509 --> 00:31:22,219
itself it is yeah yeah yeah yeah it's uh

00:31:19,159 --> 00:31:25,099
yeah its base yeah I have it's it's like

00:31:22,219 --> 00:31:27,649
you like a forty line switch statement

00:31:25,099 --> 00:31:29,659
based on certain assumptions about like

00:31:27,649 --> 00:31:31,219
the peak optimal way this should be

00:31:29,659 --> 00:31:36,199
running and it's like stretches between

00:31:31,219 --> 00:31:38,089
certain values and and that's it yeah so

00:31:36,199 --> 00:31:44,899
that could be adapted that that could be

00:31:38,089 --> 00:31:46,459
much better I guess Ben yeah and yeah

00:31:44,899 --> 00:31:47,869
and and you know it doesn't have much

00:31:46,459 --> 00:31:49,459
application state because literally

00:31:47,869 --> 00:31:51,229
looking at how many packets resistor we

00:31:49,459 --> 00:31:54,139
receive and how many packets is sent and

00:31:51,229 --> 00:31:55,819
using that as a metric and perhaps like

00:31:54,139 --> 00:31:56,989
some of these other measurements like

00:31:55,819 --> 00:31:58,879
some of these other metrics that we're

00:31:56,989 --> 00:32:03,019
measure might might be more interesting

00:31:58,879 --> 00:32:06,139
to to you to use it for also right and

00:32:03,019 --> 00:32:07,999
also I guess here it's a very simple

00:32:06,139 --> 00:32:09,829
painful application to write so maybe

00:32:07,999 --> 00:32:11,419
it's not it doesn't do well in that case

00:32:09,829 --> 00:32:13,519
maybe it does well in this other case

00:32:11,419 --> 00:32:15,409
where like they're multiple network

00:32:13,519 --> 00:32:17,179
multiple application has different

00:32:15,409 --> 00:32:23,319
demands and it maybe can adapt to that

00:32:17,179 --> 00:32:25,909
better also yeah yep so this is the

00:32:23,319 --> 00:32:28,249
thank you for attending my talk if you

00:32:25,909 --> 00:32:36,069
guys have any questions be happy to

00:32:28,249 --> 00:32:36,069
answer them yep

00:32:50,770 --> 00:32:58,070
yeah yeah yeah I think so too

00:32:54,790 --> 00:33:00,230
yeah because the the pipeline here is 16

00:32:58,070 --> 00:33:04,760
so he can for every core can send 16

00:33:00,230 --> 00:33:06,350
requests why after you gather so so that

00:33:04,760 --> 00:33:08,000
means when you're getting a packet where

00:33:06,350 --> 00:33:11,630
you get an interrupt your pop processing

00:33:08,000 --> 00:33:13,910
them did I think delaying longer is much

00:33:11,630 --> 00:33:16,490
better in this case right like the

00:33:13,910 --> 00:33:18,559
default policy can never go beyond this

00:33:16,490 --> 00:33:20,000
much delay basically because it's this

00:33:18,559 --> 00:33:22,610
hard-coded to be about a hundred

00:33:20,000 --> 00:33:24,169
something whereas the physical card

00:33:22,610 --> 00:33:25,790
itself you can go up to a limit of a

00:33:24,169 --> 00:33:30,130
thousand microsecond right so it's

00:33:25,790 --> 00:33:30,130
basically a space I can actually explore

00:33:34,000 --> 00:33:43,030
yeah yeah

00:34:00,270 --> 00:34:06,030
yeah yeah yeah so I think that

00:34:03,900 --> 00:34:07,650
interestingly here is that maybe there

00:34:06,030 --> 00:34:09,149
is some very nice threshold you can hit

00:34:07,650 --> 00:34:22,790
where you're doing that much more

00:34:09,149 --> 00:34:25,889
efficiently than some other for this one

00:34:22,790 --> 00:34:28,139
yeah this one is actually interesting so

00:34:25,889 --> 00:34:31,280
the Tilly agency in this case was about

00:34:28,139 --> 00:34:34,440
six milliseconds for the request and

00:34:31,280 --> 00:34:36,000
actually at this point at this 800

00:34:34,440 --> 00:34:38,700
something microsecond delay it had the

00:34:36,000 --> 00:34:42,119
lowest latency about five hundred five

00:34:38,700 --> 00:34:45,600
five milliseconds yeah no idea why

00:34:42,119 --> 00:34:47,940
that's the case versus the which is

00:34:45,600 --> 00:34:48,990
Linux one I mean the yeah Tilly's I

00:34:47,940 --> 00:34:50,520
don't know if you want to care about

00:34:48,990 --> 00:34:54,350
Heroes yeah like five five five

00:34:50,520 --> 00:34:54,350
milliseconds maybe you do

00:35:18,559 --> 00:35:29,069
yeah that would be hmm

00:35:22,519 --> 00:35:31,829
well yeah yeah because the device the

00:35:29,069 --> 00:35:33,900
device itself has about 500 kilobytes of

00:35:31,829 --> 00:35:36,089
storage of packets but once they get to

00:35:33,900 --> 00:35:38,249
it will let DNA is a memory region in

00:35:36,089 --> 00:35:39,839
your system because you know when you

00:35:38,249 --> 00:35:41,279
inside the device you tell it here's my

00:35:39,839 --> 00:35:44,309
memory and you can write your data to it

00:35:41,279 --> 00:35:45,769
with this DMA yeah I don't think I've

00:35:44,309 --> 00:35:49,670
seen the case where you actually

00:35:45,769 --> 00:35:49,670
overflowed the device itself

00:35:57,610 --> 00:36:03,659

YouTube URL: https://www.youtube.com/watch?v=8UQTlNQTKtQ


