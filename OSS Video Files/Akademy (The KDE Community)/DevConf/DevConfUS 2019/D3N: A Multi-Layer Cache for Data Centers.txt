Title: D3N: A Multi-Layer Cache for Data Centers
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Emine Ugar Kaynar

Current caching methods for improving the performance of big-data jobs assume abundant (e.g., full bi-section) bandwidth to cache nodes. However, many enterprise data centers and co-location facilities exhibit significant network imbalances due to over-subscription and incremental networking upgrades. In this talk, we describe D3N, a novel multi-layer cooperative caching architecture that mitigates network imbalances by caching data on the access side of each layer of hierarchical network topology.

Our prototype of D3N, which incorporates a two-layer cache, is highly-performant (can read cached data at 5GB/s, the maximum speed of our SSDs) and significantly improves big-data jobs' performance. To fully utilize bandwidth within each layer under dynamic conditions, we present an algorithm that adaptively adjusts cache sizes of each layer based on observed workload patterns and network congestion.
Captions: 
	00:00:03,170 --> 00:00:09,150
so hello everyone thank you very much

00:00:06,270 --> 00:00:12,719
for coming to the last session on last

00:00:09,150 --> 00:00:15,779
day today I'm gonna give you a talk

00:00:12,719 --> 00:00:20,400
about the v3 and the project that I have

00:00:15,779 --> 00:00:23,490
been working for for a long time this is

00:00:20,400 --> 00:00:26,010
a joint project between two universities

00:00:23,490 --> 00:00:28,440
Boston inertia and northeastern it's

00:00:26,010 --> 00:00:30,840
actually start as a research project but

00:00:28,440 --> 00:00:34,020
later on thanks to the res are they

00:00:30,840 --> 00:00:37,470
really interested with the idea and now

00:00:34,020 --> 00:00:40,950
they are collaborating and our coats

00:00:37,470 --> 00:00:43,980
become part of upstream so the tree is

00:00:40,950 --> 00:00:46,829
like touch infrastructure for data

00:00:43,980 --> 00:00:50,239
centers and its call is to speed up the

00:00:46,829 --> 00:00:50,239
performance of big data analytics

00:00:53,930 --> 00:00:56,620
sorry

00:01:06,530 --> 00:01:13,180
so here we are seeing a typical private

00:01:09,740 --> 00:01:15,650
data center and enterprise data center

00:01:13,180 --> 00:01:18,920
it's very important today for many

00:01:15,650 --> 00:01:21,910
organizations because they wanna collect

00:01:18,920 --> 00:01:25,570
data analyze the data data basically

00:01:21,910 --> 00:01:28,970
value to their business and our

00:01:25,570 --> 00:01:31,340
organization because of that for storage

00:01:28,970 --> 00:01:34,580
repository is called data Lake to store

00:01:31,340 --> 00:01:38,600
the huge amount of data and data leaks

00:01:34,580 --> 00:01:40,760
are typically deployed as object storage

00:01:38,600 --> 00:01:44,210
immutable object storage they're running

00:01:40,760 --> 00:01:47,690
on cheap commodity hardware and at the

00:01:44,210 --> 00:01:49,490
same time these organization create a

00:01:47,690 --> 00:01:52,550
lot of life computer cluster or another

00:01:49,490 --> 00:01:55,700
cluster through analyze the data sets

00:01:52,550 --> 00:01:58,369
and it could be a one big cluster or it

00:01:55,700 --> 00:01:59,930
could be multiple costs or used by

00:01:58,369 --> 00:02:01,400
different organs the different groups

00:01:59,930 --> 00:02:03,710
within the organization like your

00:02:01,400 --> 00:02:06,020
performance team has a cluster or your

00:02:03,710 --> 00:02:07,759
marketing team as a cluster and what I

00:02:06,020 --> 00:02:09,590
mean by elastic clusters I'm talking

00:02:07,759 --> 00:02:11,600
about big the atomic frameworks like

00:02:09,590 --> 00:02:14,350
Hadoop spark which run in a distributed

00:02:11,600 --> 00:02:17,690
manner and the businesses for these

00:02:14,350 --> 00:02:19,760
another clusters are stored in the data

00:02:17,690 --> 00:02:23,050
like their input data sets of the data

00:02:19,760 --> 00:02:25,240
sometimes even the intermediate data set

00:02:23,050 --> 00:02:28,000
and as you see here in able to access

00:02:25,240 --> 00:02:29,630
these data says this cluster has to go

00:02:28,000 --> 00:02:32,030
over the network

00:02:29,630 --> 00:02:34,130
so today even we talk about food

00:02:32,030 --> 00:02:36,560
bisection bandwidth may be flat data

00:02:34,130 --> 00:02:39,650
centers but in reality most of the data

00:02:36,560 --> 00:02:42,110
center has a high over subscription and

00:02:39,650 --> 00:02:44,180
organic URLs and what I mean by organic

00:02:42,110 --> 00:02:46,130
growth is you upgrade some part of your

00:02:44,180 --> 00:02:48,890
switches or part of the data center but

00:02:46,130 --> 00:02:50,990
not not the other part so it's something

00:02:48,890 --> 00:02:54,320
basically I'm trying to say is because

00:02:50,990 --> 00:02:56,930
of the over subscription organic growth

00:02:54,320 --> 00:02:59,000
we see Network imbalances we observe a

00:02:56,930 --> 00:03:01,640
lot of network congestion and as a

00:02:59,000 --> 00:03:06,430
result accessing the data sets or in the

00:03:01,640 --> 00:03:09,410
data lake so a lot of like high latency

00:03:06,430 --> 00:03:11,439
so that's a function we have in here for

00:03:09,410 --> 00:03:15,890
this project

00:03:11,439 --> 00:03:17,450
to tackle down it first we look at how

00:03:15,890 --> 00:03:19,069
these applications are running what are

00:03:17,450 --> 00:03:21,530
the typical characteristic of these

00:03:19,069 --> 00:03:23,959
running application so there are a lot

00:03:21,530 --> 00:03:26,120
of literature's and also we have some

00:03:23,959 --> 00:03:30,319
our own data sets from our industrial

00:03:26,120 --> 00:03:32,599
partner like to Sigma we have public

00:03:30,319 --> 00:03:35,480
available traces from Facebook for their

00:03:32,599 --> 00:03:38,120
like Hadoop clusters and among these

00:03:35,480 --> 00:03:40,519
data sets what we observe is a high data

00:03:38,120 --> 00:03:43,129
input through your source data are

00:03:40,519 --> 00:03:44,780
repeatedly accesses the other thing we

00:03:43,129 --> 00:03:46,970
observe is there is an uneven data

00:03:44,780 --> 00:03:49,760
popularity so certain data sets are

00:03:46,970 --> 00:03:52,280
absolutely much frequently access and

00:03:49,760 --> 00:03:54,409
others and we also observe file

00:03:52,280 --> 00:03:56,959
popularities changes over time so if

00:03:54,409 --> 00:03:58,849
file is popular today maybe in a week

00:03:56,959 --> 00:04:03,099
but after that it's not become hot

00:03:58,849 --> 00:04:06,950
anymore so for these three observation

00:04:03,099 --> 00:04:09,469
caching is seems to us obvious solution

00:04:06,950 --> 00:04:11,510
the other thing we observe is usually

00:04:09,469 --> 00:04:14,810
the data says access sequentially and

00:04:11,510 --> 00:04:17,470
with their entire is so user usually

00:04:14,810 --> 00:04:21,130
process entire data sets

00:04:17,470 --> 00:04:23,560
and prefetching definitely help us

00:04:21,130 --> 00:04:26,040
improve the performance for those type

00:04:23,560 --> 00:04:29,200
of accesses so based on this observation

00:04:26,040 --> 00:04:31,540
we've come up with the main idea for dgn

00:04:29,200 --> 00:04:33,430
and the main idea is here is we are

00:04:31,540 --> 00:04:35,530
catching the data on the access side of

00:04:33,430 --> 00:04:39,130
the bottlenecks

00:04:35,530 --> 00:04:41,020
so here I'm gonna overview what is the

00:04:39,130 --> 00:04:43,810
DTN architecture and how does it works

00:04:41,020 --> 00:04:46,800
so in the figure you see a stuffed

00:04:43,810 --> 00:04:49,570
cluster which usually run on hard disks

00:04:46,800 --> 00:04:51,850
we have a hierarchical network topology

00:04:49,570 --> 00:04:54,760
like a factory right we have a bunch of

00:04:51,850 --> 00:04:58,440
switches and then we have racks with top

00:04:54,760 --> 00:04:58,440
of racks which and in each rack

00:04:59,949 --> 00:05:06,409
and in interact you see computes not

00:05:04,340 --> 00:05:10,400
this would be bare metal we are

00:05:06,409 --> 00:05:15,139
containers problem great I don't know

00:05:10,400 --> 00:05:17,330
you can see so in our design the initial

00:05:15,139 --> 00:05:19,849
thing we did is we placed strategically

00:05:17,330 --> 00:05:22,849
rack local cache servers and these cache

00:05:19,849 --> 00:05:26,130
servers are acute with high speed SSDs

00:05:22,849 --> 00:05:29,130
and we play some pro rack

00:05:26,130 --> 00:05:33,990
and then we run cast services on top of

00:05:29,130 --> 00:05:36,210
each each cat server and these cast

00:05:33,990 --> 00:05:39,210
services we call a Ron Allen and they

00:05:36,210 --> 00:05:42,300
act as a local cache to the entire rack

00:05:39,210 --> 00:05:46,980
so any requests going to the backhand

00:05:42,300 --> 00:05:49,530
sub cluster from any note has to go

00:05:46,980 --> 00:05:51,090
through the layer 1 cache so you have to

00:05:49,530 --> 00:05:53,670
first check the dates off whether it's

00:05:51,090 --> 00:05:55,920
existing layer 1 or not this may be

00:05:53,670 --> 00:05:59,010
trying to prevent any traffic going out

00:05:55,920 --> 00:06:01,470
of the top of racks which basically

00:05:59,010 --> 00:06:05,250
so if you have more sharing happening

00:06:01,470 --> 00:06:08,370
among your clusters then what we did is

00:06:05,250 --> 00:06:10,710
we create more care services and call

00:06:08,370 --> 00:06:13,620
them lay your soul and basically what we

00:06:10,710 --> 00:06:16,920
did is we group those care services

00:06:13,620 --> 00:06:19,230
inside a pool and create layers now as

00:06:16,920 --> 00:06:22,320
you see may you become a one big

00:06:19,230 --> 00:06:25,170
distributed cash for this one cluster so

00:06:22,320 --> 00:06:27,870
any miss from like your local layer one

00:06:25,170 --> 00:06:29,970
cash is now gonna go and check the layer

00:06:27,870 --> 00:06:35,100
two cash so this way we are trying to

00:06:29,970 --> 00:06:38,840
prevent traffic going outside to your

00:06:35,100 --> 00:06:41,850
cluster basically so in your

00:06:38,840 --> 00:06:43,320
organization data sensor if you have

00:06:41,850 --> 00:06:45,300
more sharing happening among your

00:06:43,320 --> 00:06:47,760
clusters you can you can even create a

00:06:45,300 --> 00:06:49,650
higher level which is layer three in

00:06:47,760 --> 00:06:52,740
this case which is going to be shared

00:06:49,650 --> 00:06:55,320
among multiple cluster and it's kind of

00:06:52,740 --> 00:06:57,870
again prevents any accesses going to the

00:06:55,320 --> 00:07:01,260
backyards so the main design goal of the

00:06:57,870 --> 00:07:02,940
tree and in here is the minimize access

00:07:01,260 --> 00:07:07,630
to the back hands

00:07:02,940 --> 00:07:10,860
and maximize the storage servers

00:07:07,630 --> 00:07:13,660
so later on we implement this in the set

00:07:10,860 --> 00:07:16,180
in here I'm showing you how the self

00:07:13,660 --> 00:07:19,660
object store works so we have a self

00:07:16,180 --> 00:07:22,260
cluster running and the safe provides a

00:07:19,660 --> 00:07:26,170
gateway Colorado's gateway which

00:07:22,260 --> 00:07:29,160
provides user to access their object

00:07:26,170 --> 00:07:32,650
store and typically clients in here

00:07:29,160 --> 00:07:36,400
clients are like MapReduce jobs your

00:07:32,650 --> 00:07:38,080
hive job spark jobs 40 requests a load

00:07:36,400 --> 00:07:40,690
balancer and then load balancer

00:07:38,080 --> 00:07:43,260
distribute the requests across rather

00:07:40,690 --> 00:07:43,260
scale face

00:07:43,500 --> 00:07:50,730
and reduce a tree is also provide as

00:07:46,280 --> 00:07:54,120
Swift's and s3 api's and today most of

00:07:50,730 --> 00:07:56,750
the applications are also support these

00:07:54,120 --> 00:07:56,750
API

00:07:59,630 --> 00:08:07,070
so what we did is we implements the dgn

00:08:03,080 --> 00:08:09,980
logic inside the Rado CFA and if it has

00:08:07,070 --> 00:08:14,480
actually three layers the front ends

00:08:09,980 --> 00:08:17,300
which provides Webster restful api

00:08:14,480 --> 00:08:19,400
things and then we have the rod escape a

00:08:17,300 --> 00:08:22,030
layer and other needs we have the

00:08:19,400 --> 00:08:24,830
Liberals in the level which provides the

00:08:22,030 --> 00:08:28,410
reduce protocol for step cluster and we

00:08:24,830 --> 00:08:33,830
did our modification to this slide

00:08:28,410 --> 00:08:33,830
for middle layer the wrath of ETA

00:08:34,120 --> 00:08:38,199
so now I'm going to show you how this

00:08:36,009 --> 00:08:40,180
architecture changed so instead of like

00:08:38,199 --> 00:08:42,189
clients going to the Longfellow 3rd and

00:08:40,180 --> 00:08:44,470
the load balancer distributors refers to

00:08:42,189 --> 00:08:48,370
the role of Gateway in our design

00:08:44,470 --> 00:08:50,709
clients go to the nearest reduce gateway

00:08:48,370 --> 00:08:53,470
we call the Giotto Skippy and how do

00:08:50,709 --> 00:08:55,689
they do that they use any cat DNS any

00:08:53,470 --> 00:08:58,809
cash solution so with any case they go

00:08:55,689 --> 00:09:02,620
to the near nearby DNS server that we

00:08:58,809 --> 00:09:06,250
provide and the DNS basically going to

00:09:02,620 --> 00:09:07,900
the client to the closest rather scary

00:09:06,250 --> 00:09:10,480
and if you wanna look a little bit more

00:09:07,900 --> 00:09:12,610
details now clients basically gonna

00:09:10,480 --> 00:09:15,130
forward a decree so the nearby rattles

00:09:12,610 --> 00:09:18,520
gateway which from the local a Ron cash

00:09:15,130 --> 00:09:20,980
and if the data is not there then layer

00:09:18,520 --> 00:09:24,910
one cache is going to run a consistent

00:09:20,980 --> 00:09:26,560
hashing algorithm to locate object in

00:09:24,910 --> 00:09:28,840
the second layer and request it from

00:09:26,560 --> 00:09:30,490
there and if the object is not in the

00:09:28,840 --> 00:09:33,010
second layer then the second layer is

00:09:30,490 --> 00:09:36,220
going to go and bring the object back to

00:09:33,010 --> 00:09:40,000
the client so we did our modification to

00:09:36,220 --> 00:09:43,870
the sefirot of gateway we also upstream

00:09:40,000 --> 00:09:45,910
the courts since s3 is since reduce kata

00:09:43,870 --> 00:09:47,830
provides a threesome flexible object

00:09:45,910 --> 00:09:50,410
interface automatically DT and also

00:09:47,830 --> 00:09:54,340
provides them so any application we can

00:09:50,410 --> 00:09:56,650
able to talk three or swift's can use

00:09:54,340 --> 00:09:58,720
the cache with our current

00:09:56,650 --> 00:10:00,700
implementation we only implement the

00:09:58,720 --> 00:10:02,860
first two layer layer 1 and layer 2 that

00:10:00,700 --> 00:10:05,020
because that was what we need in in our

00:10:02,860 --> 00:10:07,090
initial implementation and as I

00:10:05,020 --> 00:10:09,490
mentioned ago previously we are using

00:10:07,090 --> 00:10:12,610
consistent hashing algorithm and we are

00:10:09,490 --> 00:10:14,350
catching the data across nvme SSDs we

00:10:12,610 --> 00:10:16,780
are using a me and me because we are in

00:10:14,350 --> 00:10:19,450
here talking about the big data sets so

00:10:16,780 --> 00:10:21,460
nvm is our performance and at the same

00:10:19,450 --> 00:10:24,529
time cost-efficient comparing to other

00:10:21,460 --> 00:10:27,459
like faster solutions like Ram

00:10:24,529 --> 00:10:29,989
and the cash is layer 1 and layer 2

00:10:27,459 --> 00:10:32,059
logically separated but they're sharing

00:10:29,989 --> 00:10:35,449
the same physical hardware so sort of

00:10:32,059 --> 00:10:37,099
they're running on the same server they

00:10:35,449 --> 00:10:41,449
are sharing the same SSDs space

00:10:37,099 --> 00:10:45,439
basically next what we look at is ok if

00:10:41,449 --> 00:10:47,569
the layer 1 and layer 2 are running on

00:10:45,439 --> 00:10:51,439
the same machine and if they're using

00:10:47,569 --> 00:10:53,839
the same cascade how should we split

00:10:51,439 --> 00:10:55,669
this cache space right what is the way

00:10:53,839 --> 00:10:59,179
to do and this is what we have

00:10:55,669 --> 00:11:01,250
implemented we basically split the cash

00:10:59,179 --> 00:11:04,880
5050 M

00:11:01,250 --> 00:11:07,370
and however later on me realizes this

00:11:04,880 --> 00:11:08,750
might be not the best solution so then

00:11:07,370 --> 00:11:12,920
we come up with the algorithm which

00:11:08,750 --> 00:11:15,290
basically observed access pattern and

00:11:12,920 --> 00:11:18,260
network congestion and design the

00:11:15,290 --> 00:11:20,450
allocated space per layer this is

00:11:18,260 --> 00:11:22,850
important because in your cluster you

00:11:20,450 --> 00:11:26,330
may have a very high cluster locality or

00:11:22,850 --> 00:11:27,980
you may have a very conscious congested

00:11:26,330 --> 00:11:29,900
network to look back and storage right

00:11:27,980 --> 00:11:32,990
and in this case you want to store more

00:11:29,900 --> 00:11:36,380
data on the layer so it could be the

00:11:32,990 --> 00:11:38,360
opposite you may have a lot of clients

00:11:36,380 --> 00:11:40,220
running on the same rack accessing the

00:11:38,360 --> 00:11:42,530
same data or maybe you have a high

00:11:40,220 --> 00:11:44,390
congestion within the cluster network

00:11:42,530 --> 00:11:48,050
among your racks then you want to store

00:11:44,390 --> 00:11:51,500
more data on layer one so we propose an

00:11:48,050 --> 00:11:53,540
algorithm which is adaptive capsize

00:11:51,500 --> 00:11:56,090
management algorithm and our algorithm

00:11:53,540 --> 00:11:58,280
measure the reuse distance istagram and

00:11:56,090 --> 00:12:00,470
the mean miss latency and then by using

00:11:58,280 --> 00:12:05,120
these two metrics we find optimal cache

00:12:00,470 --> 00:12:06,830
size so the algorithms are on the paper

00:12:05,120 --> 00:12:10,389
I'm not going to go in details but I'm

00:12:06,830 --> 00:12:14,660
happy to discuss offline later on

00:12:10,389 --> 00:12:17,740
also then we run a lot of evaluation to

00:12:14,660 --> 00:12:21,579
look at the performance of the cash and

00:12:17,740 --> 00:12:24,199
here on micro benchmarks and we also

00:12:21,579 --> 00:12:26,720
create a simple numerical model to show

00:12:24,199 --> 00:12:30,079
the value of multi-level cache against

00:12:26,720 --> 00:12:33,850
only local layer one cache only one

00:12:30,079 --> 00:12:33,850
layer so distributed layer 2 cache

00:12:34,040 --> 00:12:40,560
so micro benchmarks what we saw with the

00:12:37,980 --> 00:12:43,890
results the tree implementation is super

00:12:40,560 --> 00:12:46,830
fast for the reads hits we can able to

00:12:43,890 --> 00:12:49,140
saturate the maximum speed of SSDs and

00:12:46,830 --> 00:12:52,700
at the same time the Nick so read

00:12:49,140 --> 00:12:56,340
throughput is increased by five times

00:12:52,700 --> 00:12:58,770
for the write-back cache we can able to

00:12:56,340 --> 00:13:01,200
saturate the maximum write bandwidth and

00:12:58,770 --> 00:13:04,020
the performance improves again and the

00:13:01,200 --> 00:13:10,200
write through policy has a small

00:13:04,020 --> 00:13:13,260
overheads up to 10% and also we look at

00:13:10,200 --> 00:13:17,520
the value of multi-level cache by using

00:13:13,260 --> 00:13:20,040
our model and we Ramar model against the

00:13:17,520 --> 00:13:22,530
public variable Facebook trace and 2

00:13:20,040 --> 00:13:25,470
Sigma trace that we have our results

00:13:22,530 --> 00:13:27,420
show of that smart layer has provides

00:13:25,470 --> 00:13:29,540
more truth than any single layer

00:13:27,420 --> 00:13:32,960
solution

00:13:29,540 --> 00:13:35,120
we also look at the algorithm these

00:13:32,960 --> 00:13:36,620
experiments the previous experiments

00:13:35,120 --> 00:13:39,560
that is the micro benchmarks we were

00:13:36,620 --> 00:13:43,040
running on the real environment about

00:13:39,560 --> 00:13:45,050
the simulation we did not actually for

00:13:43,040 --> 00:13:46,070
the dynamic cash management algorithm

00:13:45,050 --> 00:13:48,110
you didn't actually implement that

00:13:46,070 --> 00:13:50,780
algorithm yet but we run it on the

00:13:48,110 --> 00:13:52,970
simulation and in this experiment we are

00:13:50,780 --> 00:13:56,810
showing the algorithm adapts the changes

00:13:52,970 --> 00:13:59,960
in the access pattern from the right

00:13:56,810 --> 00:14:01,700
graph you see the on the x-axis of the

00:13:59,960 --> 00:14:06,020
algorithm run time basically the time

00:14:01,700 --> 00:14:09,140
and the y axis showing the local layer1

00:14:06,020 --> 00:14:11,390
capacity so what we did is from one rack

00:14:09,140 --> 00:14:19,250
and in this experiments we have Cyrax

00:14:11,390 --> 00:14:21,320
actually I'm just so one rack which is

00:14:19,250 --> 00:14:25,700
rack one makes a lot of aggressive

00:14:21,320 --> 00:14:31,280
requests keep requesting different files

00:14:25,700 --> 00:14:35,420
and what we see is until the minute 36

00:14:31,280 --> 00:14:40,040
its increase its local one layer cache

00:14:35,420 --> 00:14:41,520
capacity and after time 36 36 minutes

00:14:40,040 --> 00:14:43,649
mark

00:14:41,520 --> 00:14:45,480
we the wreck one stopped making requests

00:14:43,649 --> 00:14:48,330
and at that time react before start

00:14:45,480 --> 00:14:50,370
making a request and then after 36

00:14:48,330 --> 00:14:53,040
minutes month we observe the right rack

00:14:50,370 --> 00:14:54,810
for increases local layer 1 capacities

00:14:53,040 --> 00:14:57,060
so this car we are trying to show the

00:14:54,810 --> 00:15:01,320
algorithm adapts when your access

00:14:57,060 --> 00:15:03,390
pattern change and we also show the

00:15:01,320 --> 00:15:04,740
total run time completion so we compare

00:15:03,390 --> 00:15:08,100
the dynamic algorithm with static

00:15:04,740 --> 00:15:11,130
allocation which has like 50% l1 and 50%

00:15:08,100 --> 00:15:12,990
l2 and we also compare the results with

00:15:11,130 --> 00:15:16,320
only layer on which is the distributed

00:15:12,990 --> 00:15:19,260
cache and we show that our dynamic

00:15:16,320 --> 00:15:23,899
results of dynamic allocation in

00:15:19,260 --> 00:15:27,120
produced around side we also look at the

00:15:23,899 --> 00:15:30,870
network law changes so in this

00:15:27,120 --> 00:15:34,800
experiment we put a congestion to the

00:15:30,870 --> 00:15:38,760
network on one of the racks which kind

00:15:34,800 --> 00:15:41,610
of side other which kind of other

00:15:38,760 --> 00:15:44,010
requests hide the other requests and

00:15:41,610 --> 00:15:46,470
because that rack has a limited or

00:15:44,010 --> 00:15:48,839
congested network the other racks also

00:15:46,470 --> 00:15:51,959
get impacted so during that congestion

00:15:48,839 --> 00:15:54,660
window because accessing two layers who

00:15:51,959 --> 00:15:56,940
becomes expensive for at least one rack

00:15:54,660 --> 00:15:59,490
we observe that all the racks are

00:15:56,940 --> 00:16:03,210
increasing their layer round capacity so

00:15:59,490 --> 00:16:07,350
that was the goal of these experiments

00:16:03,210 --> 00:16:12,060
we also run we also use a Facebook trace

00:16:07,350 --> 00:16:14,520
with different locality with different

00:16:12,060 --> 00:16:16,290
local level so the Facebook trace

00:16:14,520 --> 00:16:17,720
actually doesn't have any Rockville

00:16:16,290 --> 00:16:20,850
country but we synthetically generated

00:16:17,720 --> 00:16:23,220
so hundred percent means in here all the

00:16:20,850 --> 00:16:24,990
requests are rack local versus zero

00:16:23,220 --> 00:16:27,000
first that means requests are randomly

00:16:24,990 --> 00:16:28,500
distributed among different tracks and

00:16:27,000 --> 00:16:31,170
for all the cases we observe that

00:16:28,500 --> 00:16:33,540
dynamic allocation outperform the static

00:16:31,170 --> 00:16:36,670
allocation

00:16:33,540 --> 00:16:38,470
this is one of the cool experiment we

00:16:36,670 --> 00:16:42,880
have this actually running in our

00:16:38,470 --> 00:16:46,660
production in our real environment so we

00:16:42,880 --> 00:16:50,470
again play the Facebook trace in here we

00:16:46,660 --> 00:16:52,900
are just running to read running the

00:16:50,470 --> 00:16:57,190
Facebook trace which which has 75 we use

00:16:52,900 --> 00:17:00,220
around 850 jobs and we are comparing to

00:16:57,190 --> 00:17:02,110
the channel implementation with original

00:17:00,220 --> 00:17:04,290
vanilla reduce cafe which has more

00:17:02,110 --> 00:17:06,970
caching and we are running it under

00:17:04,290 --> 00:17:08,589
network scenario in one of them we have

00:17:06,970 --> 00:17:09,939
a high connectivity between our cat

00:17:08,589 --> 00:17:15,490
services with black hands which is

00:17:09,939 --> 00:17:17,470
around 40 gigabits per second and we

00:17:15,490 --> 00:17:19,180
have two racks here so the aggregated

00:17:17,470 --> 00:17:21,160
pam it was 80 gigabytes the other

00:17:19,180 --> 00:17:24,100
scenario which has applicability is more

00:17:21,160 --> 00:17:27,750
realistic and for both of the

00:17:24,100 --> 00:17:30,790
experiments we showed that d3 and

00:17:27,750 --> 00:17:33,190
improve the performance a lot and it's

00:17:30,790 --> 00:17:35,920
also on the right left side graph you

00:17:33,190 --> 00:17:39,010
see the traffic go into the black hands

00:17:35,920 --> 00:17:41,590
and as you see in here we decrease the

00:17:39,010 --> 00:17:43,400
total amount of network going to the

00:17:41,590 --> 00:17:46,309
back end

00:17:43,400 --> 00:17:48,370
so the treants summarize the d3 on is

00:17:46,309 --> 00:17:50,690
right now four ports to reach cash and

00:17:48,370 --> 00:17:52,580
we also propose a dynamic change

00:17:50,690 --> 00:17:55,490
partitioning algorithm but we didn't

00:17:52,580 --> 00:17:57,650
actually implement this algorithm yet we

00:17:55,490 --> 00:18:00,590
recently also implement a prefetching

00:17:57,650 --> 00:18:03,230
mechanism this has been done actually by

00:18:00,590 --> 00:18:05,210
a group of students so the tree and

00:18:03,230 --> 00:18:08,090
implementation right now support read

00:18:05,210 --> 00:18:10,790
read ahead prefetching or user can

00:18:08,090 --> 00:18:12,260
define basically commands and say I

00:18:10,790 --> 00:18:15,800
wanna prefetch lab data and the data

00:18:12,260 --> 00:18:17,900
will be available to in the cache so the

00:18:15,800 --> 00:18:19,670
tree and also supports write cache but

00:18:17,900 --> 00:18:21,590
we don't have any redundancy mechanism

00:18:19,670 --> 00:18:26,390
today we support right back and right

00:18:21,590 --> 00:18:28,490
through so the status of the project we

00:18:26,390 --> 00:18:33,380
upstream the code the read cash code is

00:18:28,490 --> 00:18:36,410
upstream next step is upstream or at the

00:18:33,380 --> 00:18:39,500
prefetching mechanism there and the

00:18:36,410 --> 00:18:41,990
feature works for thinking some of them

00:18:39,500 --> 00:18:43,760
in in progress we are trying to make

00:18:41,990 --> 00:18:48,540
sure the write back cache has some

00:18:43,760 --> 00:18:51,040
redundancy because we wanna

00:18:48,540 --> 00:18:52,960
tolerate the failures kind of scenario

00:18:51,040 --> 00:18:55,630
and we wanna look at the cash management

00:18:52,960 --> 00:18:59,260
policies right now ro the captions are

00:18:55,630 --> 00:19:01,750
using basically LRU so we want to use

00:18:59,260 --> 00:19:03,430
some like do some smart caching using

00:19:01,750 --> 00:19:06,490
some machine learning techniques you

00:19:03,430 --> 00:19:07,900
want to predict the feature accesses we

00:19:06,490 --> 00:19:09,610
want to predict the access patterns to

00:19:07,900 --> 00:19:12,910
prefetch the data before client even

00:19:09,610 --> 00:19:14,770
access we also want to support multiple

00:19:12,910 --> 00:19:17,440
backhands right now current design

00:19:14,770 --> 00:19:20,920
supports only single backhands and we

00:19:17,440 --> 00:19:22,870
also want to use the cache level events

00:19:20,920 --> 00:19:24,730
to him to application what I mean by

00:19:22,870 --> 00:19:27,880
that the applications are not aware of

00:19:24,730 --> 00:19:29,710
what is in the cache so if we if we

00:19:27,880 --> 00:19:31,929
provide some hints the application that

00:19:29,710 --> 00:19:34,410
application can be arranged Aspire or

00:19:31,929 --> 00:19:34,410
jobs

00:19:35,120 --> 00:19:39,410
there are also some limitation that I

00:19:38,000 --> 00:19:41,330
want to talk about with the current

00:19:39,410 --> 00:19:44,720
designs so one of the thing is right now

00:19:41,330 --> 00:19:45,620
all decisions are making locally so when

00:19:44,720 --> 00:19:48,260
a cat saravana

00:19:45,620 --> 00:19:51,020
Avik something it has no more global

00:19:48,260 --> 00:19:53,180
view or when it's want to make a request

00:19:51,020 --> 00:19:55,220
it's just running the consistent hashing

00:19:53,180 --> 00:19:57,110
and forwards its request so we are

00:19:55,220 --> 00:19:59,930
coupling to policy and mechanism here

00:19:57,110 --> 00:20:02,930
and we are catching everything in block

00:19:59,930 --> 00:20:06,710
granularity right now so we don't have

00:20:02,930 --> 00:20:11,900
any idea of or we don't have any mapping

00:20:06,710 --> 00:20:14,420
between the objects and the blocks and

00:20:11,900 --> 00:20:16,760
we also have a lot of redundancy because

00:20:14,420 --> 00:20:19,130
multiple cache servers only one can

00:20:16,760 --> 00:20:21,260
store the same object how your level

00:20:19,130 --> 00:20:23,330
layers you can store the same object so

00:20:21,260 --> 00:20:28,760
is that redundancy really necessary in

00:20:23,330 --> 00:20:31,100
SS required so we are we wanna trying to

00:20:28,760 --> 00:20:33,500
eliminate that redundancy

00:20:31,100 --> 00:20:36,350
and also if you want to do some more

00:20:33,500 --> 00:20:38,690
smart caching kind of stuff the current

00:20:36,350 --> 00:20:40,910
implementation is a little bit like

00:20:38,690 --> 00:20:42,500
limiting us on that sense and also we

00:20:40,910 --> 00:20:46,070
are not able to do why they're in it for

00:20:42,500 --> 00:20:48,410
caching so because of that we are

00:20:46,070 --> 00:20:50,480
changing with a bit the current

00:20:48,410 --> 00:20:53,660
architecture that's what I have been

00:20:50,480 --> 00:20:55,789
working over the summer so we are trying

00:20:53,660 --> 00:20:58,309
to make sure in sort of like using

00:20:55,789 --> 00:21:01,250
consistent hashing can we use more

00:20:58,309 --> 00:21:04,640
flexible solution consistent hashing is

00:21:01,250 --> 00:21:06,049
great it's easy easy to implement of the

00:21:04,640 --> 00:21:08,620
upstream we don't have to change

00:21:06,049 --> 00:21:12,140
anything but we want to live with more

00:21:08,620 --> 00:21:14,419
complex cache management's because of

00:21:12,140 --> 00:21:16,130
that we are moving forward to directory

00:21:14,419 --> 00:21:18,289
paths based cast solution and what I

00:21:16,130 --> 00:21:21,380
mean by that so each cache is going to

00:21:18,289 --> 00:21:23,809
sort a directory for each object about

00:21:21,380 --> 00:21:25,880
their location maybe more information

00:21:23,809 --> 00:21:29,000
about like the file information user

00:21:25,880 --> 00:21:31,039
information who is accessing and we also

00:21:29,000 --> 00:21:33,260
want to make sure the management

00:21:31,039 --> 00:21:34,760
policies can be tuned if you are using

00:21:33,260 --> 00:21:37,010
wide area network if you are using

00:21:34,760 --> 00:21:39,440
hybrid class if your or if you're just

00:21:37,010 --> 00:21:40,940
in a single data center so that's one of

00:21:39,440 --> 00:21:43,760
the direction that we are going forwards

00:21:40,940 --> 00:21:45,710
right now the second one is we are also

00:21:43,760 --> 00:21:48,380
working on the right track caching with

00:21:45,710 --> 00:21:51,740
redundancy so the goal is in here

00:21:48,380 --> 00:21:54,980
implementing a persistence layer in the

00:21:51,740 --> 00:21:58,039
Rado state way that we can use and we

00:21:54,980 --> 00:22:00,440
want to ensure the redundancy happens

00:21:58,039 --> 00:22:02,590
before the acknowledgement the right you

00:22:00,440 --> 00:22:05,470
want to make sure

00:22:02,590 --> 00:22:06,850
the failure happens we recover it we

00:22:05,470 --> 00:22:09,659
want to make sure we are implementing

00:22:06,850 --> 00:22:13,809
the right code for the storage service

00:22:09,659 --> 00:22:18,880
so we have some meetings with the staff

00:22:13,809 --> 00:22:22,059
folks we come up with this idea is so

00:22:18,880 --> 00:22:23,260
I'm gonna share with you today and so

00:22:22,059 --> 00:22:26,320
instead of point gun you know

00:22:23,260 --> 00:22:28,419
reinventing the veil we can only use the

00:22:26,320 --> 00:22:31,059
existing redundant storage system for

00:22:28,419 --> 00:22:34,929
write back cache which is already access

00:22:31,059 --> 00:22:37,690
incest so the basic idea here is we

00:22:34,929 --> 00:22:40,539
basically going to run another OS across

00:22:37,690 --> 00:22:42,640
there as a write back cache so as you

00:22:40,539 --> 00:22:44,770
see in this figure if you have a read

00:22:42,640 --> 00:22:47,320
request they're gonna be served by the

00:22:44,770 --> 00:22:49,419
rgw cache but if you have any write

00:22:47,320 --> 00:22:53,200
requests if they're right through the

00:22:49,419 --> 00:22:55,539
rgw cache gonna flash going to write the

00:22:53,200 --> 00:22:57,460
right through to the backhand OS these

00:22:55,539 --> 00:22:59,230
however if you if your clients would

00:22:57,460 --> 00:23:01,299
like to use writes back mechanism than

00:22:59,230 --> 00:23:06,669
the rgw is gonna write the data back to

00:23:01,299 --> 00:23:09,340
the OSD cluster so we're color planning

00:23:06,669 --> 00:23:13,750
like infinitives or run this

00:23:09,340 --> 00:23:16,929
architecture of course the current self

00:23:13,750 --> 00:23:18,700
has a lot of medical necessity stuffs

00:23:16,929 --> 00:23:20,980
that we may not need in this cache

00:23:18,700 --> 00:23:23,169
design so we are planning to tune that

00:23:20,980 --> 00:23:25,090
catch OSD cluster and make sure it's

00:23:23,169 --> 00:23:26,559
performance you know we don't know we

00:23:25,090 --> 00:23:28,330
are not going to sure it's gonna work or

00:23:26,559 --> 00:23:30,820
not right now but we are going to see

00:23:28,330 --> 00:23:33,010
that so again

00:23:30,820 --> 00:23:35,050
we will have toys because there's one

00:23:33,010 --> 00:23:37,990
for caching the data for right back and

00:23:35,050 --> 00:23:40,000
one for for the back hands which is

00:23:37,990 --> 00:23:47,020
going to sort the persistent data for

00:23:40,000 --> 00:23:50,830
long-term storage and then rgw cache in

00:23:47,020 --> 00:23:52,990
here is gonna provide us mechanism and

00:23:50,830 --> 00:23:55,060
then once we have this right back

00:23:52,990 --> 00:23:58,210
mechanic right back cashing I believe we

00:23:55,060 --> 00:24:00,100
can build on top of its are this is

00:23:58,210 --> 00:24:02,020
gonna be like the core and then we're

00:24:00,100 --> 00:24:07,030
gonna build our own research on top of

00:24:02,020 --> 00:24:10,120
this so that's that's also we have the

00:24:07,030 --> 00:24:12,330
web page for the projects you can find

00:24:10,120 --> 00:24:15,310
them either at the mass of a cloud or

00:24:12,330 --> 00:24:19,030
collaborative page and we have the

00:24:15,310 --> 00:24:22,060
github repo for the corpse and the

00:24:19,030 --> 00:24:24,010
simulator that we have for dynamic ash

00:24:22,060 --> 00:24:27,750
management's and if you have any

00:24:24,010 --> 00:24:27,750
questions feel free to shoot me an email

00:24:32,350 --> 00:24:35,450
[Music]

00:24:42,420 --> 00:24:47,080
so not asking about the application

00:24:44,680 --> 00:24:49,060
hinting so you're actually mentioning

00:24:47,080 --> 00:24:51,280
that the cash could send hints to the

00:24:49,060 --> 00:24:52,300
application have you thought about it

00:24:51,280 --> 00:24:54,160
the other way around where the

00:24:52,300 --> 00:24:57,430
application could provide hints about

00:24:54,160 --> 00:24:59,710
the data caching were efficient yeah we

00:24:57,430 --> 00:25:02,830
think both you're right so the question

00:24:59,710 --> 00:25:04,120
is you think about caching the

00:25:02,830 --> 00:25:04,570
application did you think all the way

00:25:04,120 --> 00:25:06,610
around

00:25:04,570 --> 00:25:07,990
yes we did that's why we implemented

00:25:06,610 --> 00:25:11,760
prefetching mechanism so in the

00:25:07,990 --> 00:25:14,730
prefetching we specifically we are

00:25:11,760 --> 00:25:17,050
providing through way one of them is

00:25:14,730 --> 00:25:18,670
client can say ok I want to prefetch

00:25:17,050 --> 00:25:20,470
that data because I'm going to access it

00:25:18,670 --> 00:25:25,600
that's where we can bring the data of

00:25:20,470 --> 00:25:27,220
beforehand or right now the prefetch

00:25:25,600 --> 00:25:29,920
mechanism is work like that for the

00:25:27,220 --> 00:25:33,070
reader hat at least you access one block

00:25:29,920 --> 00:25:36,490
and then next and block is going to be

00:25:33,070 --> 00:25:38,020
prefetch what we want to do is for

00:25:36,490 --> 00:25:39,700
example if we realize if the

00:25:38,020 --> 00:25:41,950
applications tell us okay I'm going to

00:25:39,700 --> 00:25:43,660
read that one in the near future we can

00:25:41,950 --> 00:25:45,700
also prefetch those as well so we think

00:25:43,660 --> 00:25:47,860
about it I think there will be a

00:25:45,700 --> 00:25:51,810
research direction gonna go to that way

00:25:47,860 --> 00:25:54,950
as well but that's very critical for

00:25:51,810 --> 00:25:54,950
you're right

00:26:00,830 --> 00:26:06,020
hi sorry I'm pretty new to like disk

00:26:04,250 --> 00:26:08,900
caching and professional Ike terms like

00:26:06,020 --> 00:26:12,160
I sometimes used interchangeably I can

00:26:08,900 --> 00:26:15,500
use like explain the difference so

00:26:12,160 --> 00:26:17,780
prefetching means so caching is improved

00:26:15,500 --> 00:26:19,760
performance not on the first time so

00:26:17,780 --> 00:26:23,090
when you request the file at the first

00:26:19,760 --> 00:26:25,100
time it won't exist in your memory or

00:26:23,090 --> 00:26:27,770
the cache it's going to be the Miss so

00:26:25,100 --> 00:26:30,920
you have to go read it from the original

00:26:27,770 --> 00:26:33,380
place back end and bring it back so in

00:26:30,920 --> 00:26:34,820
the cache however the second access is

00:26:33,380 --> 00:26:36,590
going to be in the heat in the cache

00:26:34,820 --> 00:26:38,810
which means you you're not you don't

00:26:36,590 --> 00:26:41,450
have to go to the back hand however the

00:26:38,810 --> 00:26:43,970
first is going to be the Miss what's the

00:26:41,450 --> 00:26:47,150
prefetching tour before you even access

00:26:43,970 --> 00:26:48,770
the data before you issue the i/o the

00:26:47,150 --> 00:26:50,990
cache is going to go to the back hand

00:26:48,770 --> 00:26:53,660
and bring that data and store it so your

00:26:50,990 --> 00:26:55,650
first request is going to be also served

00:26:53,660 --> 00:26:59,000
by the cache

00:26:55,650 --> 00:26:59,000
that's the difference

00:27:04,810 --> 00:27:07,890
I believe there's a

00:27:11,890 --> 00:27:18,039
I think you mentioned that you were

00:27:13,659 --> 00:27:20,140
using LRU cache algorithm yeah can you

00:27:18,039 --> 00:27:25,240
talk to the eviction policy is there

00:27:20,140 --> 00:27:28,870
like a limit on the cache one and out

00:27:25,240 --> 00:27:31,029
sue they're sharing one big LRU and we

00:27:28,870 --> 00:27:33,190
didn't prioritize a long in the

00:27:31,029 --> 00:27:35,799
implementation as lawmakers overall to

00:27:33,190 --> 00:27:39,460
request that's something we are wanna

00:27:35,799 --> 00:27:41,440
look at it and we are using LRU because

00:27:39,460 --> 00:27:44,940
it was simple to implement first we

00:27:41,440 --> 00:27:48,669
wanna make sure the system works working

00:27:44,940 --> 00:27:50,110
what we wanna do is actually like vivan

00:27:48,669 --> 00:27:54,100
off for example you some machine

00:27:50,110 --> 00:27:57,639
learning techniques instead of like LRU

00:27:54,100 --> 00:28:00,070
or I don't 5'4 or LF you whatever and is

00:27:57,639 --> 00:28:01,389
gonna give us any benefit it's I don't

00:28:00,070 --> 00:28:03,399
know it's going to be performance enough

00:28:01,389 --> 00:28:07,570
I don't know is it gonna be give us a

00:28:03,399 --> 00:28:11,350
benefit but we wanna look at that but as

00:28:07,570 --> 00:28:14,019
I said right now each cash give their

00:28:11,350 --> 00:28:15,850
own independence decision so one cast

00:28:14,019 --> 00:28:18,130
servers has one al are you second cast

00:28:15,850 --> 00:28:19,510
server has their own LRU and then week

00:28:18,130 --> 00:28:21,909
something we don't know anything about

00:28:19,510 --> 00:28:24,519
other cache servers so we have one is

00:28:21,909 --> 00:28:27,130
ruq and whatever the leased access we

00:28:24,519 --> 00:28:29,350
just evicted and again as I said there's

00:28:27,130 --> 00:28:34,120
no prior situation I think this is also

00:28:29,350 --> 00:28:37,179
important because we have treat or each

00:28:34,120 --> 00:28:39,610
object same or should we prioritize some

00:28:37,179 --> 00:28:40,840
objects and also for read and writes

00:28:39,610 --> 00:28:42,730
requests we don't have any

00:28:40,840 --> 00:28:44,590
prioritization right now so should we

00:28:42,730 --> 00:28:49,150
have to

00:28:44,590 --> 00:28:51,280
maybe not epic from writing cash or we

00:28:49,150 --> 00:28:53,080
shouldn't epic from which I'd like those

00:28:51,280 --> 00:28:54,610
are the trade-offs we have to look at it

00:28:53,080 --> 00:28:58,380
and this is where the research I think

00:28:54,610 --> 00:28:58,380
the part of this project

00:29:02,370 --> 00:29:05,120

YouTube URL: https://www.youtube.com/watch?v=troLFFM6btc


