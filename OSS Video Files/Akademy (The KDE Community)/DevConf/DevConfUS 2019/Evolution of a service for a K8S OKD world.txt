Title: Evolution of a service for a K8S OKD world
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speakers: John Mulligan and Raghavendra Talur

Heketi is a storage volume manager service for Gluster in environments like Kubernetes/Openshift. In the course of the last few years, as maintainers of this project, we have learned about the difference in usage patterns in the container orchestrator world as compared to traditional unix server. The events a traditional system sees in a year, software running in orchestration can see in a week. The volume of activity and the volatility of the environment are all amplified. In this talk, we will walk through the challenges encountered and the solutions we developed to stabilize Heketi running within an orchestration environment and how many of these lessons can be applied to other software.
Captions: 
	00:00:02,850 --> 00:00:08,800
thank you very much as she said my name

00:00:06,370 --> 00:00:11,469
is John Mulligan I work with my

00:00:08,800 --> 00:00:14,169
colleague here rock vendor tour we work

00:00:11,469 --> 00:00:19,620
in storage right hat I've been working

00:00:14,169 --> 00:00:23,080
on this project for about two years and

00:00:19,620 --> 00:00:25,420
so one of the main issues that I've been

00:00:23,080 --> 00:00:29,170
dealing with during that time is

00:00:25,420 --> 00:00:35,410
stabilizing and making the software that

00:00:29,170 --> 00:00:38,499
we work on more robust that's why I have

00:00:35,410 --> 00:00:39,999
my alternate tile here teaching a

00:00:38,499 --> 00:00:42,129
stateful application how to better

00:00:39,999 --> 00:00:45,480
survive in rough turf I don't think this

00:00:42,129 --> 00:00:45,480
mic is picking me up at all

00:00:47,930 --> 00:00:52,130
so we have an alternate title which is a

00:00:50,120 --> 00:00:57,670
little bit more accurate but has fewer

00:00:52,130 --> 00:01:01,010
buzzwords here's my graphical

00:00:57,670 --> 00:01:05,680
representation of our system when things

00:01:01,010 --> 00:01:08,570
go wrong you've got our poor application

00:01:05,680 --> 00:01:12,350
not really surviving well in the hostile

00:01:08,570 --> 00:01:14,510
environment and hopefully after this

00:01:12,350 --> 00:01:16,630
you'll have learned a little bit from

00:01:14,510 --> 00:01:16,630
our

00:01:52,440 --> 00:01:59,420
I know I'm not getting picked up switch

00:02:06,680 --> 00:02:11,950
okay that's a little butter I'm sorry

00:02:09,840 --> 00:02:16,150
about that

00:02:11,950 --> 00:02:17,500
okay so specifically the project that

00:02:16,150 --> 00:02:20,620
we've been working on is called a caddy

00:02:17,500 --> 00:02:23,910
it's a bridge between the storage

00:02:20,620 --> 00:02:25,319
back-end and front-end

00:02:23,910 --> 00:02:28,060
[Music]

00:02:25,319 --> 00:02:34,510
as I said it's a stateful application

00:02:28,060 --> 00:02:36,370
and the hopefully you'll be learning a

00:02:34,510 --> 00:02:41,890
little bit about how we made this offer

00:02:36,370 --> 00:02:43,209
a bit more robust quick introduction to

00:02:41,890 --> 00:02:46,390
Ketty itself

00:02:43,209 --> 00:02:50,860
McKenney has written and go it has a

00:02:46,390 --> 00:02:53,140
front end with a rest style API the API

00:02:50,860 --> 00:02:56,799
is accessed by go clients or Python

00:02:53,140 --> 00:02:58,480
clients on the backend Kitty is actually

00:02:56,799 --> 00:03:00,880
reaching out in configuring storage

00:02:58,480 --> 00:03:03,940
services these are either running

00:03:00,880 --> 00:03:06,549
natively inside of kubernetes itself or

00:03:03,940 --> 00:03:09,010
on dedicated storage nodes and you katie

00:03:06,549 --> 00:03:14,230
has to control them via commands running

00:03:09,010 --> 00:03:17,709
over SSH the the system was originally

00:03:14,230 --> 00:03:20,799
designed for use managing the cluster

00:03:17,709 --> 00:03:23,950
system under OpenStack that was its very

00:03:20,799 --> 00:03:29,170
early origins and it quickly became

00:03:23,950 --> 00:03:33,870
adapted to being used for open share the

00:03:29,170 --> 00:03:36,490
system was originally designed for use

00:03:33,870 --> 00:03:39,130
single node

00:03:36,490 --> 00:03:42,190
and it didn't really have any sort of

00:03:39,130 --> 00:03:44,430
built-in HJ capabilities if the server

00:03:42,190 --> 00:03:48,650
went down it stayed down

00:03:44,430 --> 00:03:48,650
[Music]

00:03:50,990 --> 00:03:56,270
so we've dealt with a lot of different

00:03:53,740 --> 00:04:00,260
engagements both on the community and

00:03:56,270 --> 00:04:02,450
many on the commercial side a lot of

00:04:00,260 --> 00:04:08,840
what we did is face directly on those

00:04:02,450 --> 00:04:10,430
experiences so again I'm kind of

00:04:08,840 --> 00:04:12,380
repeating myself but one of the

00:04:10,430 --> 00:04:14,750
hopefully the takeaway here isn't

00:04:12,380 --> 00:04:18,080
necessarily what exactly we did to kitty

00:04:14,750 --> 00:04:21,640
but lessons that may be applicable to

00:04:18,080 --> 00:04:23,450
other applications as well

00:04:21,640 --> 00:04:27,500
[Music]

00:04:23,450 --> 00:04:31,340
so what's so unusual about running a

00:04:27,500 --> 00:04:33,680
service like this under kubernetes it

00:04:31,340 --> 00:04:36,350
turns out about a few months after

00:04:33,680 --> 00:04:40,130
starting working on the project I said

00:04:36,350 --> 00:04:42,380
to a co-worker well you know seeing how

00:04:40,130 --> 00:04:43,780
this environment works it's a lot like

00:04:42,380 --> 00:04:47,090
what you'd see on a more traditional

00:04:43,780 --> 00:04:49,670
system only compressed down you know

00:04:47,090 --> 00:04:51,080
running for a week in this system is

00:04:49,670 --> 00:04:55,010
like running a year in your typical

00:04:51,080 --> 00:04:57,230
storage data center the important

00:04:55,010 --> 00:05:01,280
takeaways are that the environment is

00:04:57,230 --> 00:05:03,290
very dynamic you have nodes coming up

00:05:01,280 --> 00:05:08,660
and down the services are expected to

00:05:03,290 --> 00:05:10,640
move on there and there are certain

00:05:08,660 --> 00:05:14,750
aspects to kubernetes that you can see

00:05:10,640 --> 00:05:17,180
harken back to its origin as a system

00:05:14,750 --> 00:05:21,530
for managing mainly for managing

00:05:17,180 --> 00:05:25,100
stateless micro services there's some

00:05:21,530 --> 00:05:29,540
complexity around how the networking and

00:05:25,100 --> 00:05:32,300
that the storage server wants to use

00:05:29,540 --> 00:05:35,840
what we call hosted by one of the

00:05:32,300 --> 00:05:38,150
applications more oriented around the

00:05:35,840 --> 00:05:40,460
actual native networking inside of the

00:05:38,150 --> 00:05:44,180
system and that can add some additional

00:05:40,460 --> 00:05:47,360
complexity as well last by the most

00:05:44,180 --> 00:05:51,980
important is the user expectation users

00:05:47,360 --> 00:05:53,720
really expect to have rated deployments

00:05:51,980 --> 00:05:55,360
whereas and the traditional data center

00:05:53,720 --> 00:05:57,789
was storage

00:05:55,360 --> 00:06:00,219
you know you be you know talking to your

00:05:57,789 --> 00:06:04,000
coworker or making a ticket in a

00:06:00,219 --> 00:06:06,460
ticketing system versus kubernetes PV

00:06:04,000 --> 00:06:08,349
and PVC mechanism where the application

00:06:06,460 --> 00:06:10,449
developer is actually asking the system

00:06:08,349 --> 00:06:13,439
for storage itself and people just

00:06:10,449 --> 00:06:13,439
expect this to work

00:06:15,340 --> 00:06:20,680
so early on because of the simplicity of

00:06:18,640 --> 00:06:24,610
the service it was fairly easy to get it

00:06:20,680 --> 00:06:28,630
running in Currys it was very easy to

00:06:24,610 --> 00:06:33,580
convert to a pod it's a single binary it

00:06:28,630 --> 00:06:36,880
has logging to standard out already it

00:06:33,580 --> 00:06:39,700
didn't have any complex demonization it

00:06:36,880 --> 00:06:42,160
just forked and ran for the parent

00:06:39,700 --> 00:06:44,070
forked and ran it so it was very easy to

00:06:42,160 --> 00:06:47,560
get it running as a you know

00:06:44,070 --> 00:06:49,780
containerized service one of the nice

00:06:47,560 --> 00:06:52,720
things about doing that is it gave the

00:06:49,780 --> 00:06:57,430
system some fairly simple h.a properties

00:06:52,720 --> 00:06:59,470
right off the bat the database that the

00:06:57,430 --> 00:07:04,090
kiddie system uses was placed directly

00:06:59,470 --> 00:07:06,670
on a volume manager and because cluster

00:07:04,090 --> 00:07:10,060
has a network file system the database

00:07:06,670 --> 00:07:12,160
could move so if the kitty pod or the

00:07:10,060 --> 00:07:14,370
node it was running on went down the

00:07:12,160 --> 00:07:17,110
system could simply run on another node

00:07:14,370 --> 00:07:19,330
unfortunately it's not multi master

00:07:17,110 --> 00:07:21,260
because the database format is very

00:07:19,330 --> 00:07:24,110
simplistic but

00:07:21,260 --> 00:07:28,370
it at least has some basic a chat

00:07:24,110 --> 00:07:30,140
capability and then finally one of the

00:07:28,370 --> 00:07:34,550
interesting aspects of the system is

00:07:30,140 --> 00:07:37,340
that as it manages sorry Gloucester pods

00:07:34,550 --> 00:07:40,730
are containerize cluster it uses

00:07:37,340 --> 00:07:43,310
kubernetes own native command execution

00:07:40,730 --> 00:07:45,260
framework this is the same thing that

00:07:43,310 --> 00:07:53,090
you end up using if you run a cube

00:07:45,260 --> 00:07:56,630
control exec okay so now getting right

00:07:53,090 --> 00:07:57,950
to the meat of it when we kind of

00:07:56,630 --> 00:08:01,100
started looking at some of the

00:07:57,950 --> 00:08:03,830
reliability issues on the system we

00:08:01,100 --> 00:08:07,190
started analyzing what was there we

00:08:03,830 --> 00:08:11,270
quickly realized that what the system

00:08:07,190 --> 00:08:13,880
was trying to do was use language

00:08:11,270 --> 00:08:17,890
mechanisms that are there for error

00:08:13,880 --> 00:08:21,020
handling but that don't really work

00:08:17,890 --> 00:08:26,570
outside the scope of a single execution

00:08:21,020 --> 00:08:28,780
of the process specifically talking

00:08:26,570 --> 00:08:31,100
about the defer mechanism in bell rang

00:08:28,780 --> 00:08:33,560
so what the system was doing is

00:08:31,100 --> 00:08:36,650
expecting that you know if there was an

00:08:33,560 --> 00:08:39,050
error handling condition X that the

00:08:36,650 --> 00:08:41,900
defer statement would be able to revert

00:08:39,050 --> 00:08:43,940
that and clean it up the problem is in

00:08:41,900 --> 00:08:47,890
this very dynamic environment the

00:08:43,940 --> 00:08:50,649
application could crash could be evicted

00:08:47,890 --> 00:08:54,370
and so the process was being terminated

00:08:50,649 --> 00:08:58,899
at points along the execution chain so

00:08:54,370 --> 00:09:01,450
it might be creating some movies or

00:08:58,899 --> 00:09:05,880
gloucester breaks the next thing you

00:09:01,450 --> 00:09:09,459
know you've been deleted or evicted and

00:09:05,880 --> 00:09:15,310
then you come back up the state of the

00:09:09,459 --> 00:09:17,110
system is no longer consistent so one of

00:09:15,310 --> 00:09:19,600
the things we realized is that we had to

00:09:17,110 --> 00:09:21,670
stop relying just on the mechanisms that

00:09:19,600 --> 00:09:24,700
the language provided us and actually

00:09:21,670 --> 00:09:28,470
work on some other design such that the

00:09:24,700 --> 00:09:31,810
system would be able to survive being

00:09:28,470 --> 00:09:37,420
terminated come back up and do sensible

00:09:31,810 --> 00:09:39,700
things so this brings us to the

00:09:37,420 --> 00:09:41,740
operations layer you could call it all

00:09:39,700 --> 00:09:44,550
sorts of different things I've seen

00:09:41,740 --> 00:09:47,500
other tools do different similar things

00:09:44,550 --> 00:09:52,209
but ultimately the point is to bake

00:09:47,500 --> 00:09:55,300
reliability into the design so what we

00:09:52,209 --> 00:09:58,449
chose was kind of record what you do

00:09:55,300 --> 00:10:02,199
before you do it approach this design

00:09:58,449 --> 00:10:04,540
was created in order to allow us to roll

00:10:02,199 --> 00:10:07,720
back anything that we had started at any

00:10:04,540 --> 00:10:11,949
point so no matter where you crash along

00:10:07,720 --> 00:10:14,470
the way or get terminated a long way the

00:10:11,949 --> 00:10:20,470
system will be able to come back up see

00:10:14,470 --> 00:10:22,480
what state was and then undo that things

00:10:20,470 --> 00:10:25,149
we thought about is that perhaps we

00:10:22,480 --> 00:10:27,820
could add resumed ability we haven't

00:10:25,149 --> 00:10:29,450
done that some of the operations aren't

00:10:27,820 --> 00:10:32,360
naturally resumable

00:10:29,450 --> 00:10:39,890
how's that work could have considered

00:10:32,360 --> 00:10:42,140
doing that so one of the aspects of this

00:10:39,890 --> 00:10:47,510
system was that we designed it such that

00:10:42,140 --> 00:10:51,850
the the state that was recorded into the

00:10:47,510 --> 00:10:55,720
database would allow us to analyze this

00:10:51,850 --> 00:10:55,720
repeating myself sorry about that

00:10:59,710 --> 00:11:06,320
anyway long story short the initial

00:11:03,470 --> 00:11:09,380
version of the system had creation and

00:11:06,320 --> 00:11:11,510
rollback we really wanted was you know a

00:11:09,380 --> 00:11:15,050
full robust approach that would allow us

00:11:11,510 --> 00:11:17,480
to do anything after crash unfortunately

00:11:15,050 --> 00:11:20,000
we had a lot of other deliverables at

00:11:17,480 --> 00:11:21,920
the same time so what we ended up doing

00:11:20,000 --> 00:11:25,070
is making sure that the design and

00:11:21,920 --> 00:11:27,050
covered what we wanted to do but that

00:11:25,070 --> 00:11:34,220
would have to come back and implement

00:11:27,050 --> 00:11:37,430
the cleanup later as we were working on

00:11:34,220 --> 00:11:41,810
other features the operations framework

00:11:37,430 --> 00:11:44,390
out into the field we had to build some

00:11:41,810 --> 00:11:46,220
stopgap tools this end actually ended up

00:11:44,390 --> 00:11:48,980
being a really good experience for us

00:11:46,220 --> 00:11:53,420
because we were building tools that were

00:11:48,980 --> 00:11:55,910
very generic simple to use that over

00:11:53,420 --> 00:12:01,220
time we were able to you know share with

00:11:55,910 --> 00:12:03,230
other teams and it helped us actually

00:12:01,220 --> 00:12:06,230
implement to clean up stuff when we

00:12:03,230 --> 00:12:08,720
eventually got around to doing it so we

00:12:06,230 --> 00:12:09,680
had this framework in place we had this

00:12:08,720 --> 00:12:13,490
metadata

00:12:09,680 --> 00:12:15,949
database and then the tools that were

00:12:13,490 --> 00:12:18,379
external to the process could help us

00:12:15,949 --> 00:12:22,040
clean things up in the meantime and

00:12:18,379 --> 00:12:25,040
somewhat a semi manual approach and then

00:12:22,040 --> 00:12:29,480
we were able to work on the fully

00:12:25,040 --> 00:12:31,040
automatically enough over time one of

00:12:29,480 --> 00:12:33,290
the things that kept us from doing

00:12:31,040 --> 00:12:36,470
everything right away is that the system

00:12:33,290 --> 00:12:42,410
had a fairly nice test framework but it

00:12:36,470 --> 00:12:45,170
was really only testing so we had to

00:12:42,410 --> 00:12:48,470
take a side road and spend a good amount

00:12:45,170 --> 00:12:50,720
of time developing an error error

00:12:48,470 --> 00:12:53,119
testing framework this included building

00:12:50,720 --> 00:12:56,269
error injection into the system itself

00:12:53,119 --> 00:12:58,639
so by setting up the configuration in a

00:12:56,269 --> 00:13:01,610
certain way we could actually induce

00:12:58,639 --> 00:13:03,619
errors at any particular step along the

00:13:01,610 --> 00:13:06,439
chain of actions the system has to

00:13:03,619 --> 00:13:10,009
perform to set up the storage and then

00:13:06,439 --> 00:13:12,800
at that point we could test the various

00:13:10,009 --> 00:13:17,179
failure scenarios that the cleanup code

00:13:12,800 --> 00:13:19,490
was supposed to handle and then

00:13:17,179 --> 00:13:21,800
eventually about I'm gonna say six

00:13:19,490 --> 00:13:24,740
months after we'd shipped the first

00:13:21,800 --> 00:13:26,430
version with operations we had developed

00:13:24,740 --> 00:13:27,990
our clean up code

00:13:26,430 --> 00:13:34,260
and we're able to provide that to our

00:13:27,990 --> 00:13:38,040
users and our customers as part of the

00:13:34,260 --> 00:13:41,339
experience working on these problems we

00:13:38,040 --> 00:13:43,500
learned that other important aspects to

00:13:41,339 --> 00:13:47,490
keeping the system reliable just to

00:13:43,500 --> 00:13:51,270
build good robust diagnostic tools tools

00:13:47,490 --> 00:13:54,959
that help the user get to the room as

00:13:51,270 --> 00:13:58,649
fast as possible you know we wanted to

00:13:54,959 --> 00:14:04,560
keep the tools simple and evolve them as

00:13:58,649 --> 00:14:08,970
we worked on cases to you know learn

00:14:04,560 --> 00:14:15,209
along the way and building tools based

00:14:08,970 --> 00:14:16,860
on experiences so a lot of stuff that

00:14:15,209 --> 00:14:20,220
did get built in the server is very

00:14:16,860 --> 00:14:21,390
useful it's in the field now but so some

00:14:20,220 --> 00:14:23,850
of the tools that we had built

00:14:21,390 --> 00:14:27,540
externally we're still using fairly

00:14:23,850 --> 00:14:29,760
frequently one of the ones that I like

00:14:27,540 --> 00:14:33,120
to use I use it fairly frequently as a

00:14:29,760 --> 00:14:36,270
tool that allows me to compare the state

00:14:33,120 --> 00:14:38,810
of a KT database along with the state of

00:14:36,270 --> 00:14:42,420
the cluster system and kubernetes itself

00:14:38,810 --> 00:14:46,440
it will show us any discrepancies and we

00:14:42,420 --> 00:14:50,910
can use that to either debug or even fix

00:14:46,440 --> 00:14:53,279
the problem sometime lastly I want to

00:14:50,910 --> 00:14:56,010
mention that we have built in some

00:14:53,279 --> 00:15:00,150
metrics into the system this is useful

00:14:56,010 --> 00:15:02,370
for both admins who are you know trying

00:15:00,150 --> 00:15:04,440
to monitor the state of the system but

00:15:02,370 --> 00:15:08,130
by building in metrics around the

00:15:04,440 --> 00:15:08,620
operations themselves you could have an

00:15:08,130 --> 00:15:11,290
idea

00:15:08,620 --> 00:15:13,750
the overall health of the system if if

00:15:11,290 --> 00:15:16,150
the operations are failing they're not

00:15:13,750 --> 00:15:18,570
cleaning automatically cleaned up that's

00:15:16,150 --> 00:15:21,940
a time for actually a human to intervene

00:15:18,570 --> 00:15:25,650
versus you know the automatically not

00:15:21,940 --> 00:15:25,650
pull just to work in a while

00:15:26,740 --> 00:15:36,180
okay so now as I was kind of joking at

00:15:31,120 --> 00:15:39,250
the bottom as my do as I say not as I do

00:15:36,180 --> 00:15:41,230
it turns out that there are many things

00:15:39,250 --> 00:15:45,490
I would love to do with the system and

00:15:41,230 --> 00:15:48,760
that are generally a good idea we don't

00:15:45,490 --> 00:15:50,950
not fully there yet so I just want to

00:15:48,760 --> 00:15:52,840
talk about this briefly one of the

00:15:50,950 --> 00:15:55,300
issues with what we're doing in the

00:15:52,840 --> 00:15:57,490
system is that we have duplicate states

00:15:55,300 --> 00:16:00,880
we have state both in lustre and in the

00:15:57,490 --> 00:16:03,580
Cadi database itself ideally what we'd

00:16:00,880 --> 00:16:06,520
be doing is minimizing that taking away

00:16:03,580 --> 00:16:08,830
as much unique state from McKenney now

00:16:06,520 --> 00:16:11,770
that could also lead to some performance

00:16:08,830 --> 00:16:14,170
problems so the other aspect is to use

00:16:11,770 --> 00:16:17,560
the state in the database more as a

00:16:14,170 --> 00:16:21,700
cache and we've done that a little bit

00:16:17,560 --> 00:16:23,980
by adding a device recent command so

00:16:21,700 --> 00:16:25,720
Katie needs to know about the sizes of

00:16:23,980 --> 00:16:29,530
the devices and the amount of things

00:16:25,720 --> 00:16:32,230
being stored on them to make allocation

00:16:29,530 --> 00:16:34,600
decisions however if something has

00:16:32,230 --> 00:16:36,700
changed on the system by the admin or if

00:16:34,600 --> 00:16:39,910
there is a blog or something we have a

00:16:36,700 --> 00:16:42,580
tool that allows us to invalidate what's

00:16:39,910 --> 00:16:45,130
in the DB and replace that with what's

00:16:42,580 --> 00:16:49,770
live on the system so we get the benefit

00:16:45,130 --> 00:16:53,790
of actually having fast local data

00:16:49,770 --> 00:16:56,670
but having the ability to reconcile that

00:16:53,790 --> 00:16:58,590
with what's actually on the system would

00:16:56,670 --> 00:17:00,750
be nice if we were actually able to do

00:16:58,590 --> 00:17:05,310
that more for some other storage objects

00:17:00,750 --> 00:17:14,420
something we made were made in the

00:17:05,310 --> 00:17:23,250
short-term but it would be nice okay so

00:17:14,420 --> 00:17:25,640
summary slide so long story short one of

00:17:23,250 --> 00:17:29,070
the issues we encountered was that the

00:17:25,640 --> 00:17:30,990
code was written in a way to just try

00:17:29,070 --> 00:17:34,290
and naturally take care of itself

00:17:30,990 --> 00:17:38,730
but that kind of organic growth doesn't

00:17:34,290 --> 00:17:52,110
really pay off so nothing it's designed

00:17:38,730 --> 00:17:54,540
for making software reliable it's worth

00:17:52,110 --> 00:17:58,370
taking some thought into it and making

00:17:54,540 --> 00:18:04,440
it so that like we did with cleanup

00:17:58,370 --> 00:18:11,130
implement the core data structures and

00:18:04,440 --> 00:18:12,690
then do the other parts later on it's

00:18:11,130 --> 00:18:14,980
important to blow out the track what's

00:18:12,690 --> 00:18:18,399
your name versus what you don't

00:18:14,980 --> 00:18:20,999
and learn from our errors was very

00:18:18,399 --> 00:18:20,999
important

00:18:23,419 --> 00:18:28,399
and that's it from the track thank you

00:18:26,539 --> 00:18:31,399
everybody for coming if you have any

00:18:28,399 --> 00:18:34,639
questions it's a small crowd and I know

00:18:31,399 --> 00:18:40,070
other parties coming up and that mic

00:18:34,639 --> 00:18:41,950
troubles the whole time so appreciate if

00:18:40,070 --> 00:18:49,779
you have any questions but

00:18:41,950 --> 00:18:49,779
[Music]

00:19:00,739 --> 00:19:04,759
so I was wondering about your operations

00:19:02,840 --> 00:19:07,219
layer that sounds like a kind of

00:19:04,759 --> 00:19:09,169
journaling based solution did you guys

00:19:07,219 --> 00:19:10,549
use a replicated state machine for that

00:19:09,169 --> 00:19:12,320
or do you care about the high

00:19:10,549 --> 00:19:13,609
availability of that log or worried

00:19:12,320 --> 00:19:14,419
about that or how do you worry about

00:19:13,609 --> 00:19:15,710
like gap

00:19:14,419 --> 00:19:20,269
what if log gets corrupted basically

00:19:15,710 --> 00:19:23,119
it's my time actually does happen so one

00:19:20,269 --> 00:19:27,259
of the issues is but again from the

00:19:23,119 --> 00:19:29,440
evolution of the system that we have

00:19:27,259 --> 00:19:34,969
it's called bolt DB it's a native golang

00:19:29,440 --> 00:19:36,499
database unfortunately it does have some

00:19:34,969 --> 00:19:39,799
drawbacks and that's a single file

00:19:36,499 --> 00:19:41,719
database if it goes away you're kind of

00:19:39,799 --> 00:19:44,450
toast and that goes back to what I was

00:19:41,719 --> 00:19:46,789
saying about the cache ideally we were

00:19:44,450 --> 00:19:49,190
able to derive all that information from

00:19:46,789 --> 00:19:51,379
the system unfortunately there are some

00:19:49,190 --> 00:19:57,499
unique pieces of data that are only kept

00:19:51,379 --> 00:19:59,659
in the Academy at this time in the

00:19:57,499 --> 00:20:02,419
meantime trying to store more metadata

00:19:59,659 --> 00:20:03,950
in bruster itself unfortunately Gluster

00:20:02,419 --> 00:20:07,549
wasn't designed to store arbitrary

00:20:03,950 --> 00:20:10,460
metadata on all the volumes so it's a

00:20:07,549 --> 00:20:12,229
trade-off one of the things about the

00:20:10,460 --> 00:20:14,779
framework but we also tried to do that I

00:20:12,229 --> 00:20:18,589
I meant to mention that I must I think I

00:20:14,779 --> 00:20:20,869
skipped was that we're also trying to

00:20:18,589 --> 00:20:24,309
retain backwards compatibility with the

00:20:20,869 --> 00:20:26,960
existing systems out there in the field

00:20:24,309 --> 00:20:29,690
so we didn't want to disrupt our users

00:20:26,960 --> 00:20:31,879
very much a very small team so it was

00:20:29,690 --> 00:20:33,769
you know we could have gone off for two

00:20:31,879 --> 00:20:36,739
years and tried to redesign everything

00:20:33,769 --> 00:20:39,000
to use at CD or whatever you know there

00:20:36,739 --> 00:20:41,970
are times where I wish we had done that

00:20:39,000 --> 00:20:43,100
to get paid off a lot of our users in

00:20:41,970 --> 00:20:45,480
the short-term

00:20:43,100 --> 00:20:48,560
okay very cool also glad to hear about

00:20:45,480 --> 00:20:48,560
ball TV that's cool sue

00:20:51,450 --> 00:20:54,579
[Music]

00:20:58,770 --> 00:21:03,899
[Music]

00:21:00,850 --> 00:21:03,899

YouTube URL: https://www.youtube.com/watch?v=6-ox-QZ-k40


