Title: AIOps: Anomaly detection with Prometheus and Istio
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Marcel Hild

As IT operations become more agile and complex, at the same time the need to enhance operational efficiency and intelligence grows. Monitoring applications and kubernetes clusters with Prometheus has become quite common. Yet identifying relevant metrics and thresholds for your setup is getting harder.

In this talk, Marcel will show the tooling used to collect and store metrics gathered by Prometheus for the long term. Then analyze those on a large scale for extracting trends and seasonality but also forecasting of expected values for a given metric. Finally, he will integrate the predicted metrics back into the Prometheus monitoring and alerting stack to enable dynamic thresholding and anomaly detection.
Captions: 
	00:00:02,580 --> 00:00:09,500
can everybody hear me okay yes awesome

00:00:05,460 --> 00:00:13,049
so thanks for coming to this very first

00:00:09,500 --> 00:00:16,350
talk about AI ops and I think the first

00:00:13,049 --> 00:00:19,650
thing that I like to do before diving

00:00:16,350 --> 00:00:22,920
into the details is to set the stage

00:00:19,650 --> 00:00:27,600
what AI ops means because nowadays is

00:00:22,920 --> 00:00:29,430
often confused with ml ops so where ml

00:00:27,600 --> 00:00:32,789
ops which is machine learning

00:00:29,430 --> 00:00:36,500
operations focuses on how we operate

00:00:32,789 --> 00:00:39,450
machine learning workloads with

00:00:36,500 --> 00:00:41,460
operations and in the cloud native works

00:00:39,450 --> 00:00:45,840
the word world

00:00:41,460 --> 00:00:49,140
AI ops more focuses on how we can use a

00:00:45,840 --> 00:00:52,410
I to improve and augment operations

00:00:49,140 --> 00:00:56,870
so although AI and ML are like similar

00:00:52,410 --> 00:01:00,930
and synonym and used those two terms

00:00:56,870 --> 00:01:03,180
should be different differentiated so

00:01:00,930 --> 00:01:05,820
this is how I look like on the Internet

00:01:03,180 --> 00:01:08,299
I'm very close to Denmark and there's

00:01:05,820 --> 00:01:14,520
still the old logo unfortunately oops

00:01:08,299 --> 00:01:18,029
this pig nose avatar that's me close to

00:01:14,520 --> 00:01:21,590
Denmark close to the Baltic Sea usually

00:01:18,029 --> 00:01:21,590
we only get like 20

00:01:26,189 --> 00:01:32,189
we only ship passengers no containers

00:01:29,670 --> 00:01:37,649
yet but out native unfortunately but

00:01:32,189 --> 00:01:40,140
we're working on it this is the logo I'm

00:01:37,649 --> 00:01:44,310
I'm working for Red Hat it's a stealth

00:01:40,140 --> 00:01:49,709
startup to be to doing its exit soon oh

00:01:44,310 --> 00:01:51,659
no we already done our exit and I'm

00:01:49,709 --> 00:01:54,329
working in the office of the CTO also

00:01:51,659 --> 00:01:56,280
known as octo and we do a lot of funny

00:01:54,329 --> 00:01:58,079
things making sure that things don't

00:01:56,280 --> 00:02:01,560
implode play around with new

00:01:58,079 --> 00:02:06,000
technologies and it looks dangerous but

00:02:01,560 --> 00:02:07,770
actually it's a lot of fun also like to

00:02:06,000 --> 00:02:09,959
level set how Red Hat cease AI because

00:02:07,770 --> 00:02:12,090
red hats usually an infrastructure

00:02:09,959 --> 00:02:15,540
provider what the heck are we doing with

00:02:12,090 --> 00:02:18,810
AI so first and foremost we also want to

00:02:15,540 --> 00:02:21,480
make sure that I workloads run really

00:02:18,810 --> 00:02:30,120
well on top of the infrastructure and

00:02:21,480 --> 00:02:34,769
the platform that we provide and yeah so

00:02:30,120 --> 00:02:38,849
one of the projects here is project top

00:02:34,769 --> 00:02:45,870
and that colleague of mine is working on

00:02:38,849 --> 00:02:48,560
where we look and one of the cool things

00:02:45,870 --> 00:02:50,430
that he did is that they are doing is

00:02:48,560 --> 00:02:53,250
recompiling tensorflow

00:02:50,430 --> 00:02:55,290
just for your individual machine and by

00:02:53,250 --> 00:03:01,849
the just recombining with the correct

00:02:55,290 --> 00:03:03,750
flex we can squeeze out 10 to 15% more

00:03:01,849 --> 00:03:04,849
performance just by recompiling

00:03:03,750 --> 00:03:10,799
tensorflow

00:03:04,849 --> 00:03:14,359
and another side note I lost talks talks

00:03:10,799 --> 00:03:17,430
where you get some pointers where to

00:03:14,359 --> 00:03:20,280
follow up after the talk so I put these

00:03:17,430 --> 00:03:22,590
sticky notes on top of the slides you

00:03:20,280 --> 00:03:24,419
can take pictures whenever you see them

00:03:22,590 --> 00:03:27,859
or at the end of the presentation I'm

00:03:24,419 --> 00:03:31,919
gonna show up all those sticky notes

00:03:27,859 --> 00:03:37,590
then another thing that's our team works

00:03:31,919 --> 00:03:44,239
honest but I'm proudly wearing a shirt

00:03:37,590 --> 00:03:47,040
this is an reference architecture of a

00:03:44,239 --> 00:03:49,380
platform to manage AI and machine

00:03:47,040 --> 00:03:53,069
learning workloads on top of openshift

00:03:49,380 --> 00:03:57,410
or kubernetes so it's not a product per

00:03:53,069 --> 00:03:57,410
se that you can buy it's but it's in

00:04:14,959 --> 00:04:42,139
and this talk is about that are using

00:04:39,919 --> 00:04:45,139
some AI technology to be more

00:04:42,139 --> 00:04:48,470
intelligent but also at the same time we

00:04:45,139 --> 00:04:52,580
are trying to make our own products more

00:04:48,470 --> 00:04:57,080
intelligent and augment some AI

00:04:52,580 --> 00:05:00,320
capabilities in those products and if

00:04:57,080 --> 00:05:02,210
we're talking about open shift and

00:05:00,320 --> 00:05:07,310
kubernetes most of the time we're

00:05:02,210 --> 00:05:10,400
dealing with time series data which is

00:05:07,310 --> 00:05:13,060
metrics and if you're operating you

00:05:10,400 --> 00:05:16,760
consume all these time series metrics

00:05:13,060 --> 00:05:19,400
and you have no clue what to do with

00:05:16,760 --> 00:05:22,550
them so we use some AI to make you a bit

00:05:19,400 --> 00:05:25,340
smarter of this with this so I'm going

00:05:22,550 --> 00:05:27,710
to talk about Prometheus what it is then

00:05:25,340 --> 00:05:32,270
how to store it for long term because as

00:05:27,710 --> 00:05:36,700
you know without data nothing that we

00:05:32,270 --> 00:05:39,470
look at the anatomy of an anomaly and

00:05:36,700 --> 00:05:43,729
finally how to integrate all that into

00:05:39,470 --> 00:05:47,360
your monitoring setup this talk is not

00:05:43,729 --> 00:05:48,800
about a shiny product and the holy grail

00:05:47,360 --> 00:05:51,590
of monitoring so that's not what I'm

00:05:48,800 --> 00:05:54,830
going to give you and I'm not gonna show

00:05:51,590 --> 00:05:57,979
you how we turned our Messi Messi

00:05:54,830 --> 00:06:03,140
monitoring solution into this old-school

00:05:57,979 --> 00:06:07,400
spider demon and I'm also not gonna show

00:06:03,140 --> 00:06:10,060
you a success story how we applied some

00:06:07,400 --> 00:06:14,090
of these things it's more like we've

00:06:10,060 --> 00:06:18,229
investigated how we can use AI and

00:06:14,090 --> 00:06:22,070
machine learning on top of Prometheus

00:06:18,229 --> 00:06:25,060
data and I'd point you to some tools and

00:06:22,070 --> 00:06:26,900
scripts to get started on that journey

00:06:25,060 --> 00:06:28,610
I'll have some

00:06:26,900 --> 00:06:30,889
questions that you might ask yourself

00:06:28,610 --> 00:06:33,590
and maybe so also some answers to those

00:06:30,889 --> 00:06:38,449
questions and the good thing it's all

00:06:33,590 --> 00:06:41,060
open source so what is Prometheus maybe

00:06:38,449 --> 00:06:45,919
some of you folks can raise your hands

00:06:41,060 --> 00:06:48,289
who know what Prometheus is it's great

00:06:45,919 --> 00:06:52,280
so the the knowledgeable folks are in

00:06:48,289 --> 00:06:54,169
the front and to those folks in the

00:06:52,280 --> 00:06:56,479
background I have this great prometheus

00:06:54,169 --> 00:06:59,449
architecture slide right so everybody

00:06:56,479 --> 00:07:02,900
loves architecture slide so now you know

00:06:59,449 --> 00:07:05,330
what prometheus is right and we can go

00:07:02,900 --> 00:07:09,050
to the bottom of it no let's back up a

00:07:05,330 --> 00:07:13,520
little bit so Prometheus in a simplistic

00:07:09,050 --> 00:07:15,320
worldview is a Greek guy and elsewhere

00:07:13,520 --> 00:07:19,490
in the kubernetes world everything has

00:07:15,320 --> 00:07:22,639
to be named after Creek people that did

00:07:19,490 --> 00:07:24,620
some stuff and for me this was what was

00:07:22,639 --> 00:07:27,229
the guy that then returned fire back

00:07:24,620 --> 00:07:29,479
from the gods to the humans and hence

00:07:27,229 --> 00:07:34,340
the torch so now you know the story

00:07:29,479 --> 00:07:39,560
why Prometheus has this torch symbol and

00:07:34,340 --> 00:07:43,820
prometheus looks at targets that's how

00:07:39,560 --> 00:07:47,260
they call them and as we want to monitor

00:07:43,820 --> 00:07:50,659
things prometheus monitors those targets

00:07:47,260 --> 00:07:53,240
by pulling data from those targets and

00:07:50,659 --> 00:07:55,610
that's really important so every target

00:07:53,240 --> 00:07:58,810
like you have a web application you have

00:07:55,610 --> 00:08:03,199
a database service or a part or anything

00:07:58,810 --> 00:08:06,470
exposes its metrics we are normal HTTP

00:08:03,199 --> 00:08:10,250
route slash metrics and Prometheus is

00:08:06,470 --> 00:08:15,650
the one that pulls the current state of

00:08:10,250 --> 00:08:20,659
the target and stores it well in this

00:08:15,650 --> 00:08:23,180
database it's a very optimized time

00:08:20,659 --> 00:08:29,770
serious database within go especially

00:08:23,180 --> 00:08:34,279
for these kind of operations and without

00:08:29,770 --> 00:08:36,469
any alerting you probably can't do

00:08:34,279 --> 00:08:40,130
monitoring so it also has the capability

00:08:36,469 --> 00:08:42,020
to write some rules how it went to

00:08:40,130 --> 00:08:45,590
trigger alert and then it pushes out

00:08:42,020 --> 00:08:50,630
these alerts to an alert manager to get

00:08:45,590 --> 00:08:53,450
you notified so in essence and its core

00:08:50,630 --> 00:08:56,750
prometheus is made for monitoring and

00:08:53,450 --> 00:08:59,500
alerting based on a very capable time

00:08:56,750 --> 00:08:59,500
series dB

00:08:59,620 --> 00:09:06,860
so the question what do we need for

00:09:01,970 --> 00:09:12,590
machine learning any idea any from you

00:09:06,860 --> 00:09:14,390
guys just say one word exactly so what's

00:09:12,590 --> 00:09:20,990
the data and Promethea is gonna look

00:09:14,390 --> 00:09:24,830
like but before I go into how the data

00:09:20,990 --> 00:09:28,100
actually looks like it's confused of my

00:09:24,830 --> 00:09:33,100
slides here so I'm gonna talk about how

00:09:28,100 --> 00:09:33,100
to store the data for long term because

00:09:33,790 --> 00:09:40,760
if you just have a short time window you

00:09:38,870 --> 00:09:43,000
probably can't do any long-term

00:09:40,760 --> 00:09:46,250
predictions and as I said earlier

00:09:43,000 --> 00:09:48,110
Prometheus is made for monitoring and

00:09:46,250 --> 00:09:51,110
alerting it doesn't give you

00:09:48,110 --> 00:09:54,350
capabilities to store your metrics for a

00:09:51,110 --> 00:09:56,810
very long time it usually has a

00:09:54,350 --> 00:09:59,960
retention of like two days or something

00:09:56,810 --> 00:10:02,600
like this but it's not for giving you

00:09:59,960 --> 00:10:06,620
the data back from your last Black

00:10:02,600 --> 00:10:09,770
Friday sales from last year so we needed

00:10:06,620 --> 00:10:12,410
to look for some solutions how to store

00:10:09,770 --> 00:10:14,060
it for long term and that's time when we

00:10:12,410 --> 00:10:18,280
started this project like one year ago

00:10:14,060 --> 00:10:21,590
and we looked at a project called Panos

00:10:18,280 --> 00:10:24,230
which i thought was also a Kree code but

00:10:21,590 --> 00:10:34,700
apparently it's just some Marvel

00:10:24,230 --> 00:10:37,490
character it's based on a large part of

00:10:34,700 --> 00:10:42,290
the Prometheus code base what it's does

00:10:37,490 --> 00:10:44,770
is takes the time series blop that we

00:10:42,290 --> 00:10:49,480
saw earlier that is stored on disk and

00:10:44,770 --> 00:10:52,100
puts it into some object storage and

00:10:49,480 --> 00:10:53,020
does some optimization like down scaling

00:10:52,100 --> 00:10:55,810
and

00:10:53,020 --> 00:10:59,370
pre-processing of the internal queries

00:10:55,810 --> 00:11:04,300
internal metrics for some so that your

00:10:59,370 --> 00:11:08,620
queries can run faster and then provides

00:11:04,300 --> 00:11:11,649
a global view on your installed

00:11:08,620 --> 00:11:14,020
Prometheus instance plus the metrics

00:11:11,649 --> 00:11:20,490
that you uploaded to your object storage

00:11:14,020 --> 00:11:23,490
and in essence it gives you unlimited

00:11:20,490 --> 00:11:26,770
storage for your Prometheus metrics data

00:11:23,490 --> 00:11:30,570
but at that time we had some problems

00:11:26,770 --> 00:11:33,640
installing it so we looked at some other

00:11:30,570 --> 00:11:36,670
solutions to that and one thing for time

00:11:33,640 --> 00:11:38,740
series it always also pops up to your

00:11:36,670 --> 00:11:44,010
mind as in flux DB which is also written

00:11:38,740 --> 00:11:47,920
in go which is integrates very nice to

00:11:44,010 --> 00:11:52,990
to Prometheus because you can just point

00:11:47,920 --> 00:11:56,560
your Prometheus instance to write with

00:11:52,990 --> 00:11:59,020
the remote write API right it's time

00:11:56,560 --> 00:12:02,079
serious metrics to another database

00:11:59,020 --> 00:12:03,790
which happens to be in flux and it

00:12:02,079 --> 00:12:06,670
integrates very nice with Prometheus

00:12:03,790 --> 00:12:08,709
just just install in flux to one

00:12:06,670 --> 00:12:13,750
configuration setting in Prometheus and

00:12:08,709 --> 00:12:17,410
off you go but unfortunately in flux

00:12:13,750 --> 00:12:25,209
tries to hold all the information in its

00:12:17,410 --> 00:12:28,779
memory so if we look at to two months or

00:12:25,209 --> 00:12:32,250
four months of data the memory spikes

00:12:28,779 --> 00:12:36,010
really really high and the solution that

00:12:32,250 --> 00:12:37,870
that in flux gives us you go to a

00:12:36,010 --> 00:12:40,200
clustered environments you spin up a

00:12:37,870 --> 00:12:42,550
cluster of influx nodes

00:12:40,200 --> 00:12:45,490
unfortunately this solution didn't work

00:12:42,550 --> 00:12:48,880
out for us because it's it's the paid

00:12:45,490 --> 00:12:51,550
model of influx so if you are running

00:12:48,880 --> 00:12:54,310
it's in your data center and you already

00:12:51,550 --> 00:12:57,010
have influx it's a very good product and

00:12:54,310 --> 00:13:02,550
you're happy to use it but for for

00:12:57,010 --> 00:13:05,370
assets just in work because yeah Ram

00:13:02,550 --> 00:13:09,930
so what we did we created the Prometheus

00:13:05,370 --> 00:13:13,740
scraper part this thing still exists out

00:13:09,930 --> 00:13:15,990
there we scrape prometheus the

00:13:13,740 --> 00:13:19,890
Prometheus API and the returns JSON

00:13:15,990 --> 00:13:22,380
metrics which we store on our self

00:13:19,890 --> 00:13:24,840
object storage which is just an s3

00:13:22,380 --> 00:13:27,600
compatible object storage so we had

00:13:24,840 --> 00:13:31,710
these Chaisson flops lying there for

00:13:27,600 --> 00:13:33,810
days or hours worth of data the good

00:13:31,710 --> 00:13:36,390
thing about this is you don't need to

00:13:33,810 --> 00:13:38,190
talk to your ops people so they don't

00:13:36,390 --> 00:13:41,070
need to reconfigure their Prometheus

00:13:38,190 --> 00:13:43,380
instances I only need access to that

00:13:41,070 --> 00:13:46,140
instance which is something that you get

00:13:43,380 --> 00:13:49,920
easier than a new configuration setting

00:13:46,140 --> 00:13:51,870
that writes the TSD be plopped somewhere

00:13:49,920 --> 00:13:55,950
or something else so it's less intrusive

00:13:51,870 --> 00:14:00,990
and the good thing is that it can be

00:13:55,950 --> 00:14:04,980
queried by spark SQL so spark as some of

00:14:00,990 --> 00:14:07,890
you might know is a good for MapReduce

00:14:04,980 --> 00:14:10,790
kind of workload batch processing etc so

00:14:07,890 --> 00:14:14,340
if you have large data sets that spend

00:14:10,790 --> 00:14:18,600
terabytes of data and you want to query

00:14:14,340 --> 00:14:23,780
them just like a database then spark

00:14:18,600 --> 00:14:28,920
spark ql it's called spark SQL no SPARQL

00:14:23,780 --> 00:14:30,270
to the rescue you pointed at the s3 back

00:14:28,920 --> 00:14:34,680
end where you're chasing plops are

00:14:30,270 --> 00:14:37,590
stores and they load into memory or into

00:14:34,680 --> 00:14:43,050
distributed memory and you can do some

00:14:37,590 --> 00:14:48,840
simple analysis like the median of other

00:14:43,050 --> 00:14:52,040
variants or whatever of these metrics of

00:14:48,840 --> 00:14:52,040
these of these data sets

00:14:52,270 --> 00:14:56,890
and we'll also add here we have some

00:14:54,670 --> 00:15:01,540
notebooks that connected to this kind of

00:14:56,890 --> 00:15:05,220
data button till then things have

00:15:01,540 --> 00:15:08,080
changed tano's is now very good

00:15:05,220 --> 00:15:12,370
integrated solution and it's running in

00:15:08,080 --> 00:15:15,630
production in our team and we're

00:15:12,370 --> 00:15:19,209
collecting from the OpenShift for

00:15:15,630 --> 00:15:24,130
clusters thousands of metrics in

00:15:19,209 --> 00:15:32,410
production to a tano's cluster running

00:15:24,130 --> 00:15:35,200
at Alice a.m. in our service so nowadays

00:15:32,410 --> 00:15:38,200
if you want to store data prometheus

00:15:35,200 --> 00:15:40,690
data for the long term calluses I think

00:15:38,200 --> 00:15:45,070
the right way to go and the integration

00:15:40,690 --> 00:15:47,620
is really really really easy and

00:15:45,070 --> 00:15:49,660
straightforward because Squier ain't a

00:15:47,620 --> 00:15:54,399
nose feels exactly like query in

00:15:49,660 --> 00:15:59,079
Prometheus so what do we really need for

00:15:54,399 --> 00:16:04,079
machine learning consistent data going

00:15:59,079 --> 00:16:04,079
back to how our data looks like

00:16:05,880 --> 00:16:13,390
Prometheus metrics metrics types can be

00:16:09,070 --> 00:16:16,000
of four types first is a gauge which is

00:16:13,390 --> 00:16:18,310
basically a time series and we have

00:16:16,000 --> 00:16:22,630
counters which are special time

00:16:18,310 --> 00:16:25,870
seriousness they are monotonically

00:16:22,630 --> 00:16:29,380
increasing so gauge can go up and down

00:16:25,870 --> 00:16:31,120
and they counter probably hopefully just

00:16:29,380 --> 00:16:34,209
goes up until something is being reset

00:16:31,120 --> 00:16:37,589
then we have histograms which are

00:16:34,209 --> 00:16:41,380
cumulative histograms of values and

00:16:37,589 --> 00:16:44,770
summaries which a snapshot of values in

00:16:41,380 --> 00:16:47,050
a certain time value and the to cross

00:16:44,770 --> 00:16:49,540
the difference between an histogram and

00:16:47,050 --> 00:16:51,459
a summary you probably need to reach the

00:16:49,540 --> 00:16:54,579
documentation a couple of times but

00:16:51,459 --> 00:16:57,100
basically if you only want to know the

00:16:54,579 --> 00:16:59,290
distribution of your values in buckets

00:16:57,100 --> 00:17:02,100
then using histogram but if he wants to

00:16:59,290 --> 00:17:06,160
know some actual values like a sample

00:17:02,100 --> 00:17:09,820
how long your longest theory took

00:17:06,160 --> 00:17:12,520
you probably wanted to use a summary so

00:17:09,820 --> 00:17:18,240
here picture again gage goes up and down

00:17:12,520 --> 00:17:24,760
counter goes up and a histogram and a

00:17:18,240 --> 00:17:28,750
summary and a metric so metrics is

00:17:24,760 --> 00:17:30,460
easier to say and okay I know then to

00:17:28,750 --> 00:17:33,370
actually understand so if I'm saying a

00:17:30,460 --> 00:17:37,180
metric to you you think of a oh it's a

00:17:33,370 --> 00:17:41,770
time series but in Prometheus having a

00:17:37,180 --> 00:17:46,500
metric called like load one or or some

00:17:41,770 --> 00:17:50,590
like latency of your of your web service

00:17:46,500 --> 00:17:55,900
that's just the name of the metric but

00:17:50,590 --> 00:18:01,120
an actual time series is composed of the

00:17:55,900 --> 00:18:04,000
metric name and its labels so every

00:18:01,120 --> 00:18:09,310
unique combination of the metric name

00:18:04,000 --> 00:18:13,270
plus its labels and the values for those

00:18:09,310 --> 00:18:16,110
labels make up a time series so choose

00:18:13,270 --> 00:18:19,030
very wisely what you put on those labels

00:18:16,110 --> 00:18:21,880
because if you have an infinite amount

00:18:19,030 --> 00:18:24,340
of values for those labels you probably

00:18:21,880 --> 00:18:27,990
have an infinite amount of metrics which

00:18:24,340 --> 00:18:27,990
is not so good

00:18:30,920 --> 00:18:39,110
so monitoring is hot if you remember

00:18:36,470 --> 00:18:45,020
that Prometheus just pulls from those

00:18:39,110 --> 00:18:50,860
targets we see that slash metrics the

00:18:45,020 --> 00:18:54,920
targets can expose anything to you right

00:18:50,860 --> 00:18:57,800
so I'm not in control what metrics I'm

00:18:54,920 --> 00:19:03,220
getting if I'm installing OpenShift

00:18:57,800 --> 00:19:06,950
or kubernetes I get 1,000 metric names

00:19:03,220 --> 00:19:09,290
which is not 1,000 metrics because you

00:19:06,950 --> 00:19:13,130
have this combination with the labels so

00:19:09,290 --> 00:19:15,230
you have a lot of time series being

00:19:13,130 --> 00:19:19,340
thrown at you and with every iteration

00:19:15,230 --> 00:19:21,590
of that this web service the pots that

00:19:19,340 --> 00:19:24,110
are being installed in your cluster you

00:19:21,590 --> 00:19:27,700
can get other metrics so we don't have a

00:19:24,110 --> 00:19:31,250
schema that is being enforced which is

00:19:27,700 --> 00:19:32,900
hard for data scientists and as we know

00:19:31,250 --> 00:19:38,270
it's 80 percent of your time is

00:19:32,900 --> 00:19:40,730
understanding those those names and

00:19:38,270 --> 00:19:44,000
throwing away the bad stuff we're just

00:19:40,730 --> 00:19:49,340
some developer changed some metric name

00:19:44,000 --> 00:19:51,800
and you don't understand you don't know

00:19:49,340 --> 00:19:54,650
what it means so I think the first thing

00:19:51,800 --> 00:19:56,870
that he want to do is some analysis of

00:19:54,650 --> 00:19:59,090
the metric of the metadata and those

00:19:56,870 --> 00:20:01,940
metrics and that's what we did we came

00:19:59,090 --> 00:20:05,150
up with some notebooks looking at the

00:20:01,940 --> 00:20:09,770
distribution of the labels here we say

00:20:05,150 --> 00:20:12,530
do we see for I don't know what metric

00:20:09,770 --> 00:20:15,950
that is but basically we're plotting the

00:20:12,530 --> 00:20:20,420
label values over some time and we see

00:20:15,950 --> 00:20:24,350
that at one point the amount of values

00:20:20,420 --> 00:20:27,320
just doubled another analysis that we

00:20:24,350 --> 00:20:29,840
did is called some T distributed

00:20:27,320 --> 00:20:32,150
stochastic neighbor embedding I can

00:20:29,840 --> 00:20:35,590
barely pronounce that name so we get an

00:20:32,150 --> 00:20:38,930
approval for it it's called TSN II I

00:20:35,590 --> 00:20:40,820
have no clue how that works but I fight

00:20:38,930 --> 00:20:42,590
check that notebook and pointed at my

00:20:40,820 --> 00:20:46,460
data even

00:20:42,590 --> 00:20:48,260
me as as a simple guy can see that there

00:20:46,460 --> 00:20:50,539
are some classes in there and there are

00:20:48,260 --> 00:20:52,520
some classes that are smaller than

00:20:50,539 --> 00:20:54,529
others so maybe we have a problem here

00:20:52,520 --> 00:20:56,390
and maybe we can identify with the

00:20:54,529 --> 00:20:59,950
monitoring guys why are there some

00:20:56,390 --> 00:20:59,950
smaller classes of these labels

00:21:01,179 --> 00:21:09,260
so using these notebooks to talk with

00:21:05,720 --> 00:21:13,070
the monitoring world or understanding

00:21:09,260 --> 00:21:15,860
the initial nature of the metrics that

00:21:13,070 --> 00:21:21,490
you're looking at can kick start you in

00:21:15,860 --> 00:21:24,500
your discovery with Prometheus metrics

00:21:21,490 --> 00:21:26,270
now going to now many d types because in

00:21:24,500 --> 00:21:28,669
the end we want to detect something

00:21:26,270 --> 00:21:32,299
anomalous in our system once we

00:21:28,669 --> 00:21:34,159
understood how what metrics are good and

00:21:32,299 --> 00:21:36,860
what mad metrics we wanted to focus on

00:21:34,159 --> 00:21:41,179
we want to see if there are some

00:21:36,860 --> 00:21:44,390
anomalies in mine no metrics if we

00:21:41,179 --> 00:21:46,039
understand what an anomaly is we need to

00:21:44,390 --> 00:21:49,820
understand the components of a time

00:21:46,039 --> 00:21:54,350
series so a time series can have a trend

00:21:49,820 --> 00:21:56,149
it can go up or down and it can have

00:21:54,350 --> 00:21:58,789
some inner trance which we would call a

00:21:56,149 --> 00:22:00,830
seasonality like in the morning you have

00:21:58,789 --> 00:22:03,230
a lot of people powering up their

00:22:00,830 --> 00:22:04,940
computers so you will have a spike there

00:22:03,230 --> 00:22:06,740
and then in the evening when everybody

00:22:04,940 --> 00:22:09,950
leaves the office it will go down and

00:22:06,740 --> 00:22:12,010
this happens every day until we have a

00:22:09,950 --> 00:22:15,890
weekend and then the seasonality changes

00:22:12,010 --> 00:22:20,899
right so this is also some nature of

00:22:15,890 --> 00:22:24,350
this time series and if we're looking at

00:22:20,899 --> 00:22:28,760
anomaly types it's basically something

00:22:24,350 --> 00:22:31,720
that doesn't happen as expected

00:22:28,760 --> 00:22:34,850
so if usually the trend is going up and

00:22:31,720 --> 00:22:37,820
suddenly it goes down I would call it an

00:22:34,850 --> 00:22:41,210
anomaly if the seasonality always is

00:22:37,820 --> 00:22:44,539
like really cyclic but then it somehow

00:22:41,210 --> 00:22:49,549
differs from what I'm expecting it's an

00:22:44,539 --> 00:22:52,039
anomaly if usually I'm at a threshold of

00:22:49,549 --> 00:22:55,580
two and suddenly I'm seeing values at

00:22:52,039 --> 00:23:01,220
five six I would call it an anomaly

00:22:55,580 --> 00:23:02,960
and to specify that anomaly a bit more

00:23:01,220 --> 00:23:07,000
precise I would call it a point wise

00:23:02,960 --> 00:23:09,890
anomaly a seasonal anomaly maybe a trend

00:23:07,000 --> 00:23:12,770
so plotting these graphs here is very

00:23:09,890 --> 00:23:14,660
important for you to plot them over a

00:23:12,770 --> 00:23:17,150
long time together feeling for it and

00:23:14,660 --> 00:23:21,260
then you can use a tool like profits

00:23:17,150 --> 00:23:25,250
which we embedded in our systems in our

00:23:21,260 --> 00:23:27,440
container this is a library from

00:23:25,250 --> 00:23:30,530
Facebook it still actively maintained

00:23:27,440 --> 00:23:35,630
which and it's pretty cool you just give

00:23:30,530 --> 00:23:41,630
it some time serious and it spits out an

00:23:35,630 --> 00:23:45,260
upper window upper upper whitehead upper

00:23:41,630 --> 00:23:47,690
and upper pans and a lower band for that

00:23:45,260 --> 00:23:50,090
for that window and a predictive

00:23:47,690 --> 00:23:55,430
prediction of the time series so the

00:23:50,090 --> 00:23:58,870
black dots my observed values and the

00:23:55,430 --> 00:24:01,820
blue line is what profit will predict

00:23:58,870 --> 00:24:05,480
and it also extracts a trend of your

00:24:01,820 --> 00:24:08,900
data so here we see that it's goes a

00:24:05,480 --> 00:24:13,250
little bit up on the right and it will

00:24:08,900 --> 00:24:16,520
extract the seasonality of your data so

00:24:13,250 --> 00:24:19,610
what you probably just do is would do is

00:24:16,520 --> 00:24:22,540
use prophets predict the value that you

00:24:19,610 --> 00:24:26,650
are observing that you would observe at

00:24:22,540 --> 00:24:29,390
time and plus X compared with your

00:24:26,650 --> 00:24:33,140
actually observed value and if it's

00:24:29,390 --> 00:24:35,660
differs thrown alert call everybody on

00:24:33,140 --> 00:24:38,300
duty hey we have a different money

00:24:35,660 --> 00:24:42,950
toward well you then we were expecting

00:24:38,300 --> 00:24:44,630
right yeah no because then then you

00:24:42,950 --> 00:24:46,940
would always call your folks because you

00:24:44,630 --> 00:24:50,090
just seeing one anomaly which is

00:24:46,940 --> 00:24:53,570
probably okay for distributed system so

00:24:50,090 --> 00:24:54,950
you would also want to find out when you

00:24:53,570 --> 00:24:59,600
are actually calling something in an

00:24:54,950 --> 00:25:03,560
anomaly and wants to call somebody on

00:24:59,600 --> 00:25:06,710
page HUD and here are also some clever

00:25:03,560 --> 00:25:08,710
things this is just one example one

00:25:06,710 --> 00:25:11,749
example how you would

00:25:08,710 --> 00:25:14,720
define an actual anomaly in the

00:25:11,749 --> 00:25:17,269
accumulator example you would just

00:25:14,720 --> 00:25:21,259
happen counter when you haven't value

00:25:17,269 --> 00:25:24,619
that this is an anomaly type you would

00:25:21,259 --> 00:25:28,460
increase that counter and if your the

00:25:24,619 --> 00:25:30,889
next value that you're seeing it's not

00:25:28,460 --> 00:25:33,340
an anomaly you would dig decrease that

00:25:30,889 --> 00:25:35,809
counter but with a higher number and

00:25:33,340 --> 00:25:39,129
then you know the only thing that he was

00:25:35,809 --> 00:25:41,869
set up is at what point of that

00:25:39,129 --> 00:25:45,259
increased counter you would actually

00:25:41,869 --> 00:25:48,789
call it an anomaly and you can

00:25:45,259 --> 00:25:54,349
experiment with different kinds of these

00:25:48,789 --> 00:25:56,359
these filters for anomaly types until

00:25:54,349 --> 00:25:59,179
you're happy and work with your

00:25:56,359 --> 00:26:02,119
monitoring folks to actually bring value

00:25:59,179 --> 00:26:05,090
to the table so the architecture so far

00:26:02,119 --> 00:26:07,399
architecture setup so far we have some

00:26:05,090 --> 00:26:11,090
application running on top of OpenShift

00:26:07,399 --> 00:26:13,759
kubernetes which is reporting its values

00:26:11,090 --> 00:26:18,409
it's metrics to Prometheus we store

00:26:13,759 --> 00:26:20,720
those values in self or in tano's then

00:26:18,409 --> 00:26:23,149
we have some Jupiter notebooks where you

00:26:20,720 --> 00:26:25,519
do some initial research some data

00:26:23,149 --> 00:26:27,859
science exploration part to understand

00:26:25,519 --> 00:26:34,009
the values and the nature of your

00:26:27,859 --> 00:26:36,409
metrics and you're also using spark to

00:26:34,009 --> 00:26:38,629
process that larger bit of data or maybe

00:26:36,409 --> 00:26:41,389
not so if you're using tunnels anyway

00:26:38,629 --> 00:26:45,080
now you want to get your hands on

00:26:41,389 --> 00:26:46,909
something and you don't want to open up

00:26:45,080 --> 00:26:49,099
those all those notebooks no we're

00:26:46,909 --> 00:26:51,279
living in a container world and as we

00:26:49,099 --> 00:26:53,570
saw in the keynote it's just a matter of

00:26:51,279 --> 00:26:56,450
reverse searching in my history and

00:26:53,570 --> 00:26:58,999
firing off that cube control command and

00:26:56,450 --> 00:27:01,970
oh boom you have 10,000 containers

00:26:58,999 --> 00:27:05,259
running and that's what we created sort

00:27:01,970 --> 00:27:05,259
of for you so we

00:27:12,590 --> 00:27:21,660
or it's forecasted values in its

00:27:17,000 --> 00:27:24,600
attached storage somehow and then that's

00:27:21,660 --> 00:27:27,530
I think is the nice thing about this set

00:27:24,600 --> 00:27:27,530
up expose

00:27:34,890 --> 00:27:42,360
i forecaster becomes just another target

00:27:39,750 --> 00:27:44,700
for Prometheus so I the only thing that

00:27:42,360 --> 00:27:47,340
I need to set up with my monitoring

00:27:44,700 --> 00:27:49,920
folks this give me access to your

00:27:47,340 --> 00:27:52,950
Prometheus environment and also straight

00:27:49,920 --> 00:27:56,429
myself and this container is really easy

00:27:52,950 --> 00:28:01,220
to install in your cluster to experiment

00:27:56,429 --> 00:28:01,220
with because it's just a container

00:28:15,800 --> 00:28:21,800
there it has some configuration and s we

00:28:19,430 --> 00:28:23,810
always configure our stuff with

00:28:21,800 --> 00:28:26,260
environment variables the only thing

00:28:23,810 --> 00:28:26,260
that you would

00:28:34,520 --> 00:28:39,240
it's couplets talker

00:28:36,540 --> 00:28:43,679
operations blah blah blah why heads up a

00:28:39,240 --> 00:28:46,010
number why had burst of wires for real

00:28:43,679 --> 00:28:49,350
no profit anomaly

00:28:46,010 --> 00:28:53,059
so these metrics are being created out

00:28:49,350 --> 00:28:55,380
of the configuration that you give it

00:28:53,059 --> 00:28:59,250
and you can set up some learning rules

00:28:55,380 --> 00:29:08,100
to be alerted if you seen application

00:28:59,250 --> 00:29:13,290
and as everybody loves demos my window

00:29:08,100 --> 00:29:16,440
over here so I have prepared something

00:29:13,290 --> 00:29:19,710
for you I have on my laptop a mini shift

00:29:16,440 --> 00:29:22,190
cluster running where I have a

00:29:19,710 --> 00:29:27,679
Prometheus and where I have this

00:29:22,190 --> 00:29:33,000
training application so this guy here is

00:29:27,679 --> 00:29:38,760
scraping my Prometheus and I've

00:29:33,000 --> 00:29:44,940
configured it with a metric and as we

00:29:38,760 --> 00:29:48,809
see it's exposing these predicted node

00:29:44,940 --> 00:29:51,809
loads one career Y hat upper so I'm

00:29:48,809 --> 00:29:56,210
predicting the metric name node load one

00:29:51,809 --> 00:30:01,200
which is just the load of this nodes

00:29:56,210 --> 00:30:02,850
obviously and I'm not using profits for

00:30:01,200 --> 00:30:07,490
this one but I'm using Korea which is

00:30:02,850 --> 00:30:11,960
another way to forecast time series and

00:30:07,490 --> 00:30:19,710
it's giving me the upper boundary of

00:30:11,960 --> 00:30:22,350
zero which is a number okay so I can

00:30:19,710 --> 00:30:26,429
also look at this data in Prometheus

00:30:22,350 --> 00:30:29,280
which is very nice to start with to

00:30:26,429 --> 00:30:32,179
begin with but everybody loves real

00:30:29,280 --> 00:30:39,120
dashboards and crafts so we also have a

00:30:32,179 --> 00:30:42,720
chronograph and in the upper graph I'm

00:30:39,120 --> 00:30:44,370
seeing the actual for actual value like

00:30:42,720 --> 00:30:47,270
here the red one

00:30:44,370 --> 00:30:47,270
versus the

00:31:46,610 --> 00:31:53,370
till value versus the furrier prediction

00:31:49,769 --> 00:31:56,360
and as you can see here the blue line

00:31:53,370 --> 00:31:56,360
matches

00:32:17,150 --> 00:32:24,730
and here we see families being predicted

00:32:21,020 --> 00:32:27,800
and unfortunately prophets didn't

00:32:24,730 --> 00:32:30,350
predict an anomaly here why I don't know

00:32:27,800 --> 00:32:33,380
maybe the accumulator wasn't assumed and

00:32:30,350 --> 00:32:36,590
so far this demo data yes a year old so

00:32:33,380 --> 00:32:41,830
I don't remember it completely but

00:32:36,590 --> 00:32:41,830
Fourier found an anomaly so great so

00:33:18,299 --> 00:33:21,139
it is

00:33:26,610 --> 00:33:33,659
so you see URLs that you might want to

00:33:31,620 --> 00:33:42,409
take pictures of unless you work in my

00:33:33,659 --> 00:33:42,409
team then you should probably questions

00:33:45,220 --> 00:33:48,380
[Applause]

00:33:53,750 --> 00:34:01,409
did you have any issues using FB profit

00:33:57,240 --> 00:34:04,320
on second data or were using seconds or

00:34:01,409 --> 00:34:05,850
did you use a larger time series amount

00:34:04,320 --> 00:34:07,529
because I know that it has trouble

00:34:05,850 --> 00:34:09,210
working on very small increments of time

00:34:07,529 --> 00:34:11,550
because it was designed for business

00:34:09,210 --> 00:34:13,919
data like sales or something like that

00:34:11,550 --> 00:34:15,540
day to day kind of stuff that's an

00:34:13,919 --> 00:34:18,240
interesting question question so you

00:34:15,540 --> 00:34:20,700
mean from the precision of seconds I

00:34:18,240 --> 00:34:22,800
think on the time position or a human

00:34:20,700 --> 00:34:25,740
secondary data yeah we are timestamps

00:34:22,800 --> 00:34:28,320
like seconds like literal seconds or

00:34:25,740 --> 00:34:34,200
were they minutes or hours yeah I think

00:34:28,320 --> 00:34:36,570
we are using a second sample data there

00:34:34,200 --> 00:34:40,399
so okay I'm not sure if did we have any

00:34:36,570 --> 00:34:44,179
problems with the with the cranial

00:34:40,399 --> 00:34:56,250
granularity of the data I'm seeing knows

00:34:44,179 --> 00:35:02,220
apparently not much better doing daily

00:34:56,250 --> 00:35:05,369
and it's an interesting thought so you

00:35:02,220 --> 00:35:07,680
maybe make might connect with Hema and

00:35:05,369 --> 00:35:12,200
Anand who did the actual work of that so

00:35:07,680 --> 00:35:12,200
I'm just showcasing stuff that's

00:35:21,000 --> 00:35:23,990
oh yes

00:35:33,420 --> 00:35:39,730
no crow fauna is a separate tool

00:35:36,750 --> 00:35:44,520
different from Prometheus but it's sort

00:35:39,730 --> 00:35:48,339
of the most used graphing tool in the

00:35:44,520 --> 00:35:50,559
cloud native world because it is really

00:35:48,339 --> 00:35:52,480
easy to build your custom dashboards and

00:35:50,559 --> 00:35:55,420
graphs and it's really well-suited for

00:35:52,480 --> 00:36:00,660
time series data and it's now also

00:35:55,420 --> 00:36:00,660
dipping into the space of locked data

00:36:06,030 --> 00:36:13,730
No and thank you for your valuable time

00:36:10,200 --> 00:36:13,730
and listening to me thank you

00:36:14,350 --> 00:36:18,760

YouTube URL: https://www.youtube.com/watch?v=5lT-GajT_Wo


