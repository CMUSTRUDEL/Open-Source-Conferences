Title: Responsible AI: Ethics in Software Development - DevConf.CZ 2021
Publication date: 2021-03-17
Playlist: DevConfCZ 2021
Description: 
	Speaker: Rishabh Gaur


Responsible AI: Building ethical practices in your software development lifecycle
Advancements in AI are different than other technologies because of the pace of innovation, and its proximity to human intelligence - impacting us at a personal and societal level. The industry today is optimistic about the incredible potential for AI and other advanced technologies to empower people, widely benefiting current and future generations, thereby working for the common good.

However, nearly 9 out of 10 organizations across countries have encountered ethical issues resulting from the use of AI. As Microsoft, we recognize that these same technologies also raise important challenges that we need to address clearly, thoughtfully, and affirmatively. In this talk, I will present Microsoft's approach towards Responsible AI, talking about the six principles to develop technology responsibly.
This will be a focussed session where I will start with a demo of how, if not used properly, AI can lead to biases. Post that we will go into Microsoft's principles and how they lead to an efficient system - showcasing how the same demo will look after incorporating the Responsible AI Principles. 


Schedule: https://sched.co/gmN8
Captions: 
	00:00:01,920 --> 00:00:05,200
my name is rishabha i

00:00:03,280 --> 00:00:07,200
work as a technical architect in

00:00:05,200 --> 00:00:10,080
microsoft let me get my system ready

00:00:07,200 --> 00:00:11,440
sharing the screen perfect okay so uh

00:00:10,080 --> 00:00:13,519
today we are here for a very interesting

00:00:11,440 --> 00:00:15,599
topic called responsibility

00:00:13,519 --> 00:00:17,279
now every organization out there they

00:00:15,599 --> 00:00:19,359
are committed to the advancement of ai

00:00:17,279 --> 00:00:22,080
uh driven by ethical principles

00:00:19,359 --> 00:00:23,600
uh that put people first uh and today we

00:00:22,080 --> 00:00:24,160
will look at the implications of this

00:00:23,600 --> 00:00:26,240
and

00:00:24,160 --> 00:00:27,519
what are the possible repercussions for

00:00:26,240 --> 00:00:30,400
the same as

00:00:27,519 --> 00:00:31,199
uh a little bit about me uh wait a

00:00:30,400 --> 00:00:34,239
second

00:00:31,199 --> 00:00:35,920
yeah so a little bit about me uh i work

00:00:34,239 --> 00:00:37,520
as a technical architect at microsoft uh

00:00:35,920 --> 00:00:39,360
i'm privileged to be a part of a very

00:00:37,520 --> 00:00:40,079
small and interesting group in microsoft

00:00:39,360 --> 00:00:42,000
called the

00:00:40,079 --> 00:00:43,520
microsoft technology center internally

00:00:42,000 --> 00:00:45,680
we call it mtc

00:00:43,520 --> 00:00:46,640
uh we are very small like i work in the

00:00:45,680 --> 00:00:48,559
indian npc

00:00:46,640 --> 00:00:50,320
uh the india is a continent and we have

00:00:48,559 --> 00:00:51,360
a very small team of four technical

00:00:50,320 --> 00:00:53,360
architects and

00:00:51,360 --> 00:00:55,600
one director uh for the entire indian

00:00:53,360 --> 00:00:57,920
subcontinent so all of us have to be a

00:00:55,600 --> 00:00:58,879
cross domain uh specializing crosstalk

00:00:57,920 --> 00:01:00,719
in solutions

00:00:58,879 --> 00:01:02,320
uh let me myself i look after all of our

00:01:00,719 --> 00:01:03,760
iot service offerings the entire

00:01:02,320 --> 00:01:04,320
application development scenarios on

00:01:03,760 --> 00:01:06,000
azure

00:01:04,320 --> 00:01:08,400
and the low code notebook offering in

00:01:06,000 --> 00:01:10,080
power platform so in my day to day job

00:01:08,400 --> 00:01:11,680
uh we host customers uh

00:01:10,080 --> 00:01:13,119
day in day out on deep technical

00:01:11,680 --> 00:01:15,520
discussions so

00:01:13,119 --> 00:01:16,400
uh we have a specialist in microsoft who

00:01:15,520 --> 00:01:17,920
specialize in

00:01:16,400 --> 00:01:19,119
particular technologies but whenever

00:01:17,920 --> 00:01:20,240
there's a requirement of a deep

00:01:19,119 --> 00:01:21,600
technical discussion

00:01:20,240 --> 00:01:22,960
and this could be around architecture

00:01:21,600 --> 00:01:24,560
this could be around designing this

00:01:22,960 --> 00:01:26,880
could be about rapid prototyping

00:01:24,560 --> 00:01:28,240
that's where we come in as mtc uh we

00:01:26,880 --> 00:01:30,640
show to our customers the

00:01:28,240 --> 00:01:31,920
so-called uh art of the possible when it

00:01:30,640 --> 00:01:33,840
comes to technology

00:01:31,920 --> 00:01:35,119
uh you can find me on linkedin uh at god

00:01:33,840 --> 00:01:35,759
ratio this is my last thing and the

00:01:35,119 --> 00:01:38,000
first thing

00:01:35,759 --> 00:01:40,000
so and i know it's like pretty early day

00:01:38,000 --> 00:01:42,960
one of the devconf and

00:01:40,000 --> 00:01:44,479
i know it's devconf but my call my

00:01:42,960 --> 00:01:47,040
entire talk is not going to be

00:01:44,479 --> 00:01:48,320
code heavy uh too very frank i won't be

00:01:47,040 --> 00:01:50,880
showing any code at all

00:01:48,320 --> 00:01:52,399
so this is this talk is just to make you

00:01:50,880 --> 00:01:54,159
uh meant to get your thought process

00:01:52,399 --> 00:01:55,040
started to get your brain juices flowing

00:01:54,159 --> 00:01:57,280
if i made

00:01:55,040 --> 00:02:00,399
so sit back get a cup of coffee and just

00:01:57,280 --> 00:02:03,360
let's go like we'll get started now

00:02:00,399 --> 00:02:05,280
so uh if you think about any solution

00:02:03,360 --> 00:02:05,920
any big data iot solution out there

00:02:05,280 --> 00:02:08,560
today

00:02:05,920 --> 00:02:09,200
uh it's all about three components there

00:02:08,560 --> 00:02:10,959
it's

00:02:09,200 --> 00:02:12,400
things that this could be your sensors

00:02:10,959 --> 00:02:14,319
and microcontroller units

00:02:12,400 --> 00:02:16,160
this could be anything okay it's things

00:02:14,319 --> 00:02:18,400
they are generating vast amount of

00:02:16,160 --> 00:02:20,080
data now what you need to do is you need

00:02:18,400 --> 00:02:21,599
to actually work on top of the data

00:02:20,080 --> 00:02:23,200
analyze that data to get

00:02:21,599 --> 00:02:24,640
some insights that's the second stage

00:02:23,200 --> 00:02:25,120
you need to get some insights out of the

00:02:24,640 --> 00:02:27,120
data

00:02:25,120 --> 00:02:28,319
by doing some sort of analysis there now

00:02:27,120 --> 00:02:29,840
once you're done with the analysis

00:02:28,319 --> 00:02:31,920
that's when the next stage comes in

00:02:29,840 --> 00:02:33,599
you need to turn those insights into

00:02:31,920 --> 00:02:36,000
action labels so that's stage

00:02:33,599 --> 00:02:37,680
three of the process now pretty

00:02:36,000 --> 00:02:39,519
straightforward piece because now what

00:02:37,680 --> 00:02:40,640
will happen is your actions will in turn

00:02:39,519 --> 00:02:42,560
generate more data

00:02:40,640 --> 00:02:44,800
thereby completing a digital feedback

00:02:42,560 --> 00:02:46,879
loop now what's really interesting about

00:02:44,800 --> 00:02:48,239
this entire slide is this is a very very

00:02:46,879 --> 00:02:50,000
easy concept to understand

00:02:48,239 --> 00:02:51,760
but if you take it one step further if

00:02:50,000 --> 00:02:53,760
you really think about implementation

00:02:51,760 --> 00:02:55,680
that's where the entire uh sort of

00:02:53,760 --> 00:02:57,120
complications comes into play

00:02:55,680 --> 00:02:58,879
that's where you really started to think

00:02:57,120 --> 00:03:00,159
about how can i go about really

00:02:58,879 --> 00:03:02,560
implementing this solution

00:03:00,159 --> 00:03:04,000
at scale it's a very complex piece in

00:03:02,560 --> 00:03:06,560
fact before going into

00:03:04,000 --> 00:03:08,400
how to do this uh a general question

00:03:06,560 --> 00:03:11,120
first we need to talk uh we need to ask

00:03:08,400 --> 00:03:13,680
ourselves what makes ai different from

00:03:11,120 --> 00:03:14,879
any other technology and basically why

00:03:13,680 --> 00:03:17,120
responsibility

00:03:14,879 --> 00:03:18,800
so there are multiple reasons here first

00:03:17,120 --> 00:03:19,840
it's the pace of innovation around this

00:03:18,800 --> 00:03:21,599
technology itself

00:03:19,840 --> 00:03:23,920
it's absolutely incredible for example

00:03:21,599 --> 00:03:25,280
in 2018 we achieve human parity when it

00:03:23,920 --> 00:03:27,120
comes to machine translation

00:03:25,280 --> 00:03:28,080
in 2019 we achieve human parity when it

00:03:27,120 --> 00:03:28,799
comes to gender and language

00:03:28,080 --> 00:03:30,799
understanding

00:03:28,799 --> 00:03:32,720
so let's think about how rapidly it has

00:03:30,799 --> 00:03:34,480
transformed this is just the last five

00:03:32,720 --> 00:03:37,200
years that i am showing on this line

00:03:34,480 --> 00:03:38,480
uh secondly it's the proximity of these

00:03:37,200 --> 00:03:40,959
ai technologies

00:03:38,480 --> 00:03:43,200
to human intelligence itself and about

00:03:40,959 --> 00:03:43,920
how we personally experience the world

00:03:43,200 --> 00:03:46,560
around us

00:03:43,920 --> 00:03:48,640
so the whole concept of sensing uh and

00:03:46,560 --> 00:03:51,599
lastly the biggest difference from

00:03:48,640 --> 00:03:52,080
any other technology is ai's power to

00:03:51,599 --> 00:03:55,120
both

00:03:52,080 --> 00:03:56,080
uh hard and good that is driving today's

00:03:55,120 --> 00:03:58,560
conversation

00:03:56,080 --> 00:03:59,280
in fact let me take you 30 years back

00:03:58,560 --> 00:04:02,239
today

00:03:59,280 --> 00:04:03,120
30 years back every business out there

00:04:02,239 --> 00:04:06,159
was looking at

00:04:03,120 --> 00:04:07,120
software as a way to redefine how it ran

00:04:06,159 --> 00:04:09,200
its operations

00:04:07,120 --> 00:04:10,480
they were systems of record they were

00:04:09,200 --> 00:04:11,040
getting created which were able to

00:04:10,480 --> 00:04:13,200
manage

00:04:11,040 --> 00:04:14,560
every whole process in the enterprise so

00:04:13,200 --> 00:04:16,639
this could be from accounting

00:04:14,560 --> 00:04:17,759
to payroll to resource planning to

00:04:16,639 --> 00:04:19,840
customer management

00:04:17,759 --> 00:04:20,959
now this change it was foundational to

00:04:19,840 --> 00:04:22,079
the digital transformation of an

00:04:20,959 --> 00:04:25,280
organization

00:04:22,079 --> 00:04:27,199
but as big a change as it was

00:04:25,280 --> 00:04:29,520
the entire digitization of the core

00:04:27,199 --> 00:04:30,560
processes it didn't alter the primary

00:04:29,520 --> 00:04:32,960
business of a company

00:04:30,560 --> 00:04:34,240
it just made it much more efficient now

00:04:32,960 --> 00:04:36,880
in the past decade

00:04:34,240 --> 00:04:37,600
however systems of record they have been

00:04:36,880 --> 00:04:39,600
extended

00:04:37,600 --> 00:04:41,280
to the so-called systems of engagement

00:04:39,600 --> 00:04:43,120
now what i mean by that is now it's

00:04:41,280 --> 00:04:44,080
redefining how companies engage with

00:04:43,120 --> 00:04:45,840
their customers

00:04:44,080 --> 00:04:47,759
how the customers themselves use the

00:04:45,840 --> 00:04:48,560
products or buy their products and

00:04:47,759 --> 00:04:51,040
eventually

00:04:48,560 --> 00:04:52,400
how actually even what those actual

00:04:51,040 --> 00:04:54,479
products themselves are

00:04:52,400 --> 00:04:55,840
so along the way software evolved from

00:04:54,479 --> 00:04:58,800
being focused on

00:04:55,840 --> 00:05:00,560
efficiencies to bring becoming a core

00:04:58,800 --> 00:05:02,400
aspect of every business out there

00:05:00,560 --> 00:05:04,560
actually if i really think about it uh

00:05:02,400 --> 00:05:06,320
it quickly became the part of business

00:05:04,560 --> 00:05:07,680
where differentiation happens between

00:05:06,320 --> 00:05:09,280
your competitors

00:05:07,680 --> 00:05:11,199
so for example let me give you a simple

00:05:09,280 --> 00:05:14,639
example now think of netflix

00:05:11,199 --> 00:05:16,720
airbnb uber or amazon now even referring

00:05:14,639 --> 00:05:18,320
to these companies as media company or a

00:05:16,720 --> 00:05:18,880
real estate company or a transportation

00:05:18,320 --> 00:05:21,120
company

00:05:18,880 --> 00:05:22,479
or retail company in our own head it

00:05:21,120 --> 00:05:24,479
sounds pretty weird

00:05:22,479 --> 00:05:26,160
because they are in a sense truly

00:05:24,479 --> 00:05:26,720
software driven companies because they

00:05:26,160 --> 00:05:28,400
understand

00:05:26,720 --> 00:05:30,560
that software is the primary function of

00:05:28,400 --> 00:05:32,560
their organization so modern companies

00:05:30,560 --> 00:05:36,240
are modern organizations out there

00:05:32,560 --> 00:05:38,240
they really need to actually look at

00:05:36,240 --> 00:05:40,720
software as something that's infused

00:05:38,240 --> 00:05:42,080
through every aspect of their business

00:05:40,720 --> 00:05:44,639
a critical component of their

00:05:42,080 --> 00:05:46,560
operational efficiencies so

00:05:44,639 --> 00:05:47,600
what has happened is okay this was

00:05:46,560 --> 00:05:48,720
something that we have been seeing for

00:05:47,600 --> 00:05:50,880
the past 10 years

00:05:48,720 --> 00:05:51,840
but the full digitization of companies

00:05:50,880 --> 00:05:53,840
it has led to

00:05:51,840 --> 00:05:55,840
a very interesting secondary effect

00:05:53,840 --> 00:05:58,000
which is the proliferation of data

00:05:55,840 --> 00:05:59,360
now systems of record they transformed

00:05:58,000 --> 00:06:02,400
what traditionally were

00:05:59,360 --> 00:06:04,000
uh paper files into digital stores where

00:06:02,400 --> 00:06:05,600
all the businesses data

00:06:04,000 --> 00:06:08,000
reside behind the company's code

00:06:05,600 --> 00:06:09,440
processes uh then systems of engagement

00:06:08,000 --> 00:06:10,160
they added a vast amount of data with

00:06:09,440 --> 00:06:11,680
respect to

00:06:10,160 --> 00:06:13,039
your product usage your customer

00:06:11,680 --> 00:06:14,560
outcomes all of those things customer

00:06:13,039 --> 00:06:17,199
interactions with your products

00:06:14,560 --> 00:06:19,440
so this in turn created the perfect

00:06:17,199 --> 00:06:20,639
environment for systems of intelligence

00:06:19,440 --> 00:06:22,800
now systems of intelligence they

00:06:20,639 --> 00:06:24,560
leverage vast amounts of data generated

00:06:22,800 --> 00:06:26,160
in an enterprise to create those expert

00:06:24,560 --> 00:06:27,840
systems now

00:06:26,160 --> 00:06:29,520
what has changed now because

00:06:27,840 --> 00:06:30,400
traditionally the term artificial

00:06:29,520 --> 00:06:33,520
intelligence

00:06:30,400 --> 00:06:35,199
uh it was reserved for describing very

00:06:33,520 --> 00:06:36,080
special occasions when a machine was

00:06:35,199 --> 00:06:38,720
able to perform

00:06:36,080 --> 00:06:40,000
tasks uh that are normally associated

00:06:38,720 --> 00:06:41,600
with human intelligence

00:06:40,000 --> 00:06:43,360
now as powerful as reporting or

00:06:41,600 --> 00:06:44,800
analytics can be they are definitely not

00:06:43,360 --> 00:06:47,039
at human intelligence level

00:06:44,800 --> 00:06:48,479
so what happened why is everybody using

00:06:47,039 --> 00:06:50,639
the term ai

00:06:48,479 --> 00:06:51,840
day in day out now i think the primary

00:06:50,639 --> 00:06:53,120
reason for this is the growing

00:06:51,840 --> 00:06:53,919
sophistication of the techniques

00:06:53,120 --> 00:06:55,280
available

00:06:53,919 --> 00:06:57,039
uh in particularly in the area of

00:06:55,280 --> 00:06:57,919
machine learning as technology improves

00:06:57,039 --> 00:06:59,520
exponentially

00:06:57,919 --> 00:07:00,960
it allows us to compute more and more

00:06:59,520 --> 00:07:03,199
data together so

00:07:00,960 --> 00:07:05,039
and interesting pieces and new systems

00:07:03,199 --> 00:07:07,199
of information can actually

00:07:05,039 --> 00:07:08,240
join my existing system of intelligence

00:07:07,199 --> 00:07:10,479
now for example

00:07:08,240 --> 00:07:12,319
i can connect any of my iot enabled

00:07:10,479 --> 00:07:13,840
devices directly to the cloud today

00:07:12,319 --> 00:07:15,520
so those things are already there and

00:07:13,840 --> 00:07:17,360
because of this

00:07:15,520 --> 00:07:19,440
this entire change that we are seeing

00:07:17,360 --> 00:07:20,319
today it becomes very imperative that we

00:07:19,440 --> 00:07:22,720
look deeply

00:07:20,319 --> 00:07:24,720
at the responsible development and views

00:07:22,720 --> 00:07:25,520
of ai that prioritizes people at the

00:07:24,720 --> 00:07:27,520
center of

00:07:25,520 --> 00:07:28,800
uh to accelerate the positive outcomes

00:07:27,520 --> 00:07:31,919
now let's look at

00:07:28,800 --> 00:07:32,800
what possibly can happen if it's not

00:07:31,919 --> 00:07:35,039
taken care of

00:07:32,800 --> 00:07:36,720
so on a weekly basis there are local

00:07:35,039 --> 00:07:38,400
headlines everywhere in the world

00:07:36,720 --> 00:07:40,319
related to the concerns regarding the

00:07:38,400 --> 00:07:41,280
use of ai so let's look at some of the

00:07:40,319 --> 00:07:43,759
repercussions

00:07:41,280 --> 00:07:44,400
uh so for example this was one article

00:07:43,759 --> 00:07:46,400
that i

00:07:44,400 --> 00:07:47,759
was able to find uh focused around

00:07:46,400 --> 00:07:49,360
secure fairness so this

00:07:47,759 --> 00:07:50,879
is concerned given that the

00:07:49,360 --> 00:07:52,720
african-american community

00:07:50,879 --> 00:07:54,000
they are already over scrutinized by law

00:07:52,720 --> 00:07:55,680
enforcement in the u.s

00:07:54,000 --> 00:07:57,360
now this suggests that patient

00:07:55,680 --> 00:08:00,479
recognition technology

00:07:57,360 --> 00:08:02,240
it's likely to be overused on the

00:08:00,479 --> 00:08:03,280
segment of the population on which it

00:08:02,240 --> 00:08:05,759
underperforms

00:08:03,280 --> 00:08:07,039
so think about it it's a very negative

00:08:05,759 --> 00:08:08,560
paradox here

00:08:07,039 --> 00:08:10,720
because facial recognition algorithm

00:08:08,560 --> 00:08:12,319
they are not tested for racial bias

00:08:10,720 --> 00:08:14,400
so this is something that can creep into

00:08:12,319 --> 00:08:17,520
your system another interesting piece

00:08:14,400 --> 00:08:19,120
uh this one so basically uh this was a

00:08:17,520 --> 00:08:20,800
new york times article just two years

00:08:19,120 --> 00:08:22,400
back about how facial recognition

00:08:20,800 --> 00:08:24,560
is actually accumulate if you are a

00:08:22,400 --> 00:08:26,639
white guy now even if you are

00:08:24,560 --> 00:08:27,759
of a different gender even then it was

00:08:26,639 --> 00:08:30,240
causing issues

00:08:27,759 --> 00:08:30,800
uh another very interesting one is this

00:08:30,240 --> 00:08:33,039
one

00:08:30,800 --> 00:08:34,640
now we see examples of this is more of a

00:08:33,039 --> 00:08:35,360
diversity or ethnicity issues so

00:08:34,640 --> 00:08:36,800
basically

00:08:35,360 --> 00:08:38,880
in this picture this asian man he is

00:08:36,800 --> 00:08:40,880
trying to put his picture onto a system

00:08:38,880 --> 00:08:43,200
uh in this case it was a passport uh

00:08:40,880 --> 00:08:44,800
software and software was repeatedly

00:08:43,200 --> 00:08:45,120
declining his application saying that

00:08:44,800 --> 00:08:47,680
his

00:08:45,120 --> 00:08:48,800
uh eyes were closed now a very important

00:08:47,680 --> 00:08:50,959
point here is

00:08:48,800 --> 00:08:51,839
this is not somebody doing intentionally

00:08:50,959 --> 00:08:54,160
something okay

00:08:51,839 --> 00:08:55,760
this is not uh like nobody is actually

00:08:54,160 --> 00:08:57,200
intentionally putting these issues

00:08:55,760 --> 00:08:59,360
into the software that they are actually

00:08:57,200 --> 00:09:01,040
building it's just that the software

00:08:59,360 --> 00:09:02,080
that was developed to identify whether a

00:09:01,040 --> 00:09:04,320
person is uploading

00:09:02,080 --> 00:09:06,640
a valid image or not on their passport

00:09:04,320 --> 00:09:08,800
uh or for their passport application

00:09:06,640 --> 00:09:10,480
the data set that was used for that

00:09:08,800 --> 00:09:12,160
identification scenario

00:09:10,480 --> 00:09:14,320
it did not have a representation from

00:09:12,160 --> 00:09:16,480
every uh every ethnicity

00:09:14,320 --> 00:09:18,000
and that was causing this issue uh

00:09:16,480 --> 00:09:18,800
another example and again this is

00:09:18,000 --> 00:09:20,080
something that you

00:09:18,800 --> 00:09:23,040
actually will face so this is more of an

00:09:20,080 --> 00:09:24,880
energy example of turkish language

00:09:23,040 --> 00:09:26,240
now turkish is a very special structure

00:09:24,880 --> 00:09:29,120
when it comes to their language

00:09:26,240 --> 00:09:29,920
that is their third pronoun it does not

00:09:29,120 --> 00:09:31,519
have a gender

00:09:29,920 --> 00:09:33,360
so when we input a sentence to the third

00:09:31,519 --> 00:09:34,880
pronoun to a machine translation system

00:09:33,360 --> 00:09:36,800
so this could be being translated this

00:09:34,880 --> 00:09:38,480
could be google translate

00:09:36,800 --> 00:09:40,320
so in this case the translating software

00:09:38,480 --> 00:09:43,360
it needs to map the third pronoun

00:09:40,320 --> 00:09:44,800
to either a he or a she now what we see

00:09:43,360 --> 00:09:46,320
is that the algorithms they are trying

00:09:44,800 --> 00:09:47,120
to put their own biases in these

00:09:46,320 --> 00:09:49,040
translations

00:09:47,120 --> 00:09:50,880
for example as you can see on the screen

00:09:49,040 --> 00:09:53,200
a doctor or engineer was a he

00:09:50,880 --> 00:09:54,640
but a nurse was a sheep again nobody

00:09:53,200 --> 00:09:56,640
deployed this but it just

00:09:54,640 --> 00:09:58,000
happened because the data set that was

00:09:56,640 --> 00:09:59,920
used to train this

00:09:58,000 --> 00:10:02,079
it had more representation from debt

00:09:59,920 --> 00:10:04,720
sector uh the last one

00:10:02,079 --> 00:10:06,399
again this is more about accountability

00:10:04,720 --> 00:10:07,680
very very crucial like for example when

00:10:06,399 --> 00:10:09,600
a driverless car

00:10:07,680 --> 00:10:11,120
crashes who gets the blame and who's

00:10:09,600 --> 00:10:13,120
going to pay for the damage

00:10:11,120 --> 00:10:15,360
so as you can see from all these

00:10:13,120 --> 00:10:16,480
examples if ethical checks and balances

00:10:15,360 --> 00:10:18,959
they are not in place

00:10:16,480 --> 00:10:20,880
it can lead to a lot of issues with our

00:10:18,959 --> 00:10:21,600
uh without us ever even realizing that

00:10:20,880 --> 00:10:23,600
we are doing it

00:10:21,600 --> 00:10:24,640
and we can be the developers who are

00:10:23,600 --> 00:10:26,959
going to develop this

00:10:24,640 --> 00:10:28,880
and in fact that is why responsible and

00:10:26,959 --> 00:10:32,480
ethical practices they are crucial

00:10:28,880 --> 00:10:33,839
from the grounds up so in the next

00:10:32,480 --> 00:10:35,120
uh i think we're done with 10 minutes i

00:10:33,839 --> 00:10:36,800
have around 20 minutes and then we'll

00:10:35,120 --> 00:10:38,399
take a 10 minute q a

00:10:36,800 --> 00:10:39,680
so in the next 20 minutes what i'll do

00:10:38,399 --> 00:10:40,959
is i'll show you the approach that

00:10:39,680 --> 00:10:42,640
microsoft suggests

00:10:40,959 --> 00:10:44,160
and what i have personally learned from

00:10:42,640 --> 00:10:45,920
my interactions with customer

00:10:44,160 --> 00:10:47,839
in the indian subcontinent day in day

00:10:45,920 --> 00:10:50,160
out uh and again uh

00:10:47,839 --> 00:10:51,680
whatever you'll see uh i work very

00:10:50,160 --> 00:10:52,240
closely with the engineering team in

00:10:51,680 --> 00:10:54,959
redmond

00:10:52,240 --> 00:10:55,519
uh and it's something that we have been

00:10:54,959 --> 00:10:57,760
doing

00:10:55,519 --> 00:10:59,040
from the grounds up globally across our

00:10:57,760 --> 00:11:01,440
different subsidiaries

00:10:59,040 --> 00:11:03,600
so when you talk about responsible ai

00:11:01,440 --> 00:11:05,120
from a microsoft perspective it's uh

00:11:03,600 --> 00:11:06,720
built down into four different pieces

00:11:05,120 --> 00:11:09,040
i'll talk about three of them the first

00:11:06,720 --> 00:11:10,079
one is to have an ai strategy the top

00:11:09,040 --> 00:11:12,160
down approach

00:11:10,079 --> 00:11:13,200
second one is to enable an ai ready

00:11:12,160 --> 00:11:14,880
culture this is where

00:11:13,200 --> 00:11:17,519
we talk about having that culture from

00:11:14,880 --> 00:11:19,200
the grounds up the third one is around

00:11:17,519 --> 00:11:20,399
having technology which is able to do

00:11:19,200 --> 00:11:21,680
that which is something that i will not

00:11:20,399 --> 00:11:23,600
be covering today because

00:11:21,680 --> 00:11:24,800
again technology we all understand that

00:11:23,600 --> 00:11:26,959
we all understand how

00:11:24,800 --> 00:11:28,560
rest networks or how image networks but

00:11:26,959 --> 00:11:30,000
the fourth aspect which is very crucial

00:11:28,560 --> 00:11:31,680
which is where i'll talk about like i'll

00:11:30,000 --> 00:11:32,320
spend significant time there and i'll

00:11:31,680 --> 00:11:35,440
talk about

00:11:32,320 --> 00:11:37,120
uh the six principles that we have we

00:11:35,440 --> 00:11:39,200
have announced as microsoft so that's

00:11:37,120 --> 00:11:41,440
around responsible interest for the ai

00:11:39,200 --> 00:11:42,560
so let's go to the first one which is

00:11:41,440 --> 00:11:46,399
about defining

00:11:42,560 --> 00:11:48,160
an ai strategy so again the whole idea

00:11:46,399 --> 00:11:49,040
here is very simple very straightforward

00:11:48,160 --> 00:11:51,440
to enable

00:11:49,040 --> 00:11:53,120
true ai transformation organization as

00:11:51,440 --> 00:11:54,959
microsoft we believe that

00:11:53,120 --> 00:11:56,800
you need to bring ai to three different

00:11:54,959 --> 00:11:59,120
pieces of your organization

00:11:56,800 --> 00:12:00,880
so the first one is you need to bring ai

00:11:59,120 --> 00:12:02,240
to every application out there now a

00:12:00,880 --> 00:12:03,440
simple example that i can give you so

00:12:02,240 --> 00:12:06,160
the organization

00:12:03,440 --> 00:12:07,040
can add a simple chat bot on top of your

00:12:06,160 --> 00:12:09,279
existing website

00:12:07,040 --> 00:12:11,040
or your application that interacts with

00:12:09,279 --> 00:12:13,200
users in more natural ways

00:12:11,040 --> 00:12:14,560
enabling you to so-called to create a

00:12:13,200 --> 00:12:16,800
very uh

00:12:14,560 --> 00:12:18,639
involved customer experience and help

00:12:16,800 --> 00:12:19,360
your employees maximize their time as

00:12:18,639 --> 00:12:22,000
well

00:12:19,360 --> 00:12:23,760
or it could be it's as simple as adding

00:12:22,000 --> 00:12:25,680
a predictive analytics to

00:12:23,760 --> 00:12:28,240
a range of different applications making

00:12:25,680 --> 00:12:29,760
it easier to plan ahead or decrease the

00:12:28,240 --> 00:12:32,959
operating cost

00:12:29,760 --> 00:12:35,120
now while this is great but for true ai

00:12:32,959 --> 00:12:37,680
transformation you can't limit the

00:12:35,120 --> 00:12:39,600
impact of aidr applications only you

00:12:37,680 --> 00:12:42,240
have to bring ai to every process

00:12:39,600 --> 00:12:43,839
be it internal paid extern a logical

00:12:42,240 --> 00:12:45,519
place to start on this is

00:12:43,839 --> 00:12:47,600
to empower your technical development

00:12:45,519 --> 00:12:49,360
needs by giving them the tools they need

00:12:47,600 --> 00:12:51,360
to quickly and easily leverage

00:12:49,360 --> 00:12:52,480
uh they can help bring ai to your

00:12:51,360 --> 00:12:53,839
processes or

00:12:52,480 --> 00:12:55,839
and really image your results for

00:12:53,839 --> 00:12:57,680
example enterprises uh today

00:12:55,839 --> 00:12:59,279
and i have talked to them i have talked

00:12:57,680 --> 00:13:00,720
to the ceos of multiple organizations

00:12:59,279 --> 00:13:02,160
who are already doing this they are

00:13:00,720 --> 00:13:02,720
leveraging intelligent solutions to

00:13:02,160 --> 00:13:04,720
let's say

00:13:02,720 --> 00:13:06,800
uh help their marketing teams monitor

00:13:04,720 --> 00:13:08,880
their brand by tracking user feedback

00:13:06,800 --> 00:13:10,399
or improve their seller efficiency by

00:13:08,880 --> 00:13:12,880
prioritizing the so-called

00:13:10,399 --> 00:13:13,600
lead generation or help their finance or

00:13:12,880 --> 00:13:15,519
operations

00:13:13,600 --> 00:13:17,200
to reduce cost and optimize their

00:13:15,519 --> 00:13:17,839
operations by data driven insights as

00:13:17,200 --> 00:13:20,800
well

00:13:17,839 --> 00:13:21,920
so having ai in every single process

00:13:20,800 --> 00:13:24,399
that's very important

00:13:21,920 --> 00:13:26,720
and lastly you can't transform by

00:13:24,399 --> 00:13:28,480
bringing ai to trust your developers

00:13:26,720 --> 00:13:30,320
every department needs to partner with

00:13:28,480 --> 00:13:30,959
your developers in fact if you think uh

00:13:30,320 --> 00:13:33,040
if you read

00:13:30,959 --> 00:13:34,240
any of the gartner's latest hype trends

00:13:33,040 --> 00:13:36,639
for the next decade

00:13:34,240 --> 00:13:38,560
citizen movement is very key there so

00:13:36,639 --> 00:13:40,000
the whole idea here is how can i

00:13:38,560 --> 00:13:41,760
democratize ai

00:13:40,000 --> 00:13:43,839
to every single employee how can i bring

00:13:41,760 --> 00:13:45,760
ai to my business user

00:13:43,839 --> 00:13:48,000
who does not understand computer

00:13:45,760 --> 00:13:50,959
programming how can i enable him to use

00:13:48,000 --> 00:13:52,959
a sophisticated trained model and again

00:13:50,959 --> 00:13:54,399
uh the entire premise of transfer

00:13:52,959 --> 00:13:56,639
learning actually from sale

00:13:54,399 --> 00:13:58,079
or giving our team explainable ai models

00:13:56,639 --> 00:14:01,600
it comes here

00:13:58,079 --> 00:14:02,160
so all of this is on the verge of how to

00:14:01,600 --> 00:14:04,480
enable

00:14:02,160 --> 00:14:06,079
a ai strategy organization so from

00:14:04,480 --> 00:14:07,360
technical employees to non-technical

00:14:06,079 --> 00:14:09,760
users we believe

00:14:07,360 --> 00:14:10,800
ai will give them the power to transform

00:14:09,760 --> 00:14:12,560
how they work

00:14:10,800 --> 00:14:14,480
and think in more innovative ways than

00:14:12,560 --> 00:14:16,959
before so

00:14:14,480 --> 00:14:17,680
next let's look at uh next we want to

00:14:16,959 --> 00:14:19,760
discuss the

00:14:17,680 --> 00:14:21,680
qualities that characterize in ai ready

00:14:19,760 --> 00:14:23,440
culture and demonstrate how

00:14:21,680 --> 00:14:25,120
change management can make this culture

00:14:23,440 --> 00:14:27,440
transformation a reality

00:14:25,120 --> 00:14:28,480
so you might ask okay what exactly makes

00:14:27,440 --> 00:14:30,959
an organization's

00:14:28,480 --> 00:14:30,959
culture

00:14:32,079 --> 00:14:35,120
so the whole idea here is from our

00:14:33,440 --> 00:14:36,320
perspective uh fostering an airbnb

00:14:35,120 --> 00:14:36,959
culture it requires three different

00:14:36,320 --> 00:14:38,959
pieces

00:14:36,959 --> 00:14:40,000
first it requires being a data driven

00:14:38,959 --> 00:14:42,880
organization

00:14:40,000 --> 00:14:44,079
and and it's it's not only about

00:14:42,880 --> 00:14:45,920
accessing the entire

00:14:44,079 --> 00:14:47,839
data state it's not about creating those

00:14:45,920 --> 00:14:49,199
data lakes and then dividing them into

00:14:47,839 --> 00:14:50,560
different data warehouses and then you

00:14:49,199 --> 00:14:52,560
go into data marks no

00:14:50,560 --> 00:14:54,000
it's about ensuring that the data in

00:14:52,560 --> 00:14:56,399
that data warehouse is

00:14:54,000 --> 00:14:57,440
of absolute highest quality it's about

00:14:56,399 --> 00:14:59,279
ensuring that

00:14:57,440 --> 00:15:01,519
you have the best and the most complete

00:14:59,279 --> 00:15:02,160
data as the foundation for your ai

00:15:01,519 --> 00:15:04,480
systems

00:15:02,160 --> 00:15:06,399
which because of paramount importance

00:15:04,480 --> 00:15:07,839
second piece is to empower your

00:15:06,399 --> 00:15:09,120
people to participate in the air

00:15:07,839 --> 00:15:10,720
transformation the so-called

00:15:09,120 --> 00:15:12,959
democratization of ai

00:15:10,720 --> 00:15:15,320
and to create an inclusive environment

00:15:12,959 --> 00:15:18,000
that follows cross-functional

00:15:15,320 --> 00:15:19,519
multi-disciplinary collaboration i mean

00:15:18,000 --> 00:15:21,120
the whole premise of the citizen

00:15:19,519 --> 00:15:23,680
movement that gartner has said

00:15:21,120 --> 00:15:25,440
is around the fact that your business

00:15:23,680 --> 00:15:27,519
user should not be

00:15:25,440 --> 00:15:29,440
dependent on your developer it should be

00:15:27,519 --> 00:15:31,519
a team sport it should be a fuse

00:15:29,440 --> 00:15:33,519
team so that's what it is so fundamental

00:15:31,519 --> 00:15:36,000
to empowerment is enablement

00:15:33,519 --> 00:15:37,120
giving people the space the resources

00:15:36,000 --> 00:15:39,360
the security

00:15:37,120 --> 00:15:40,800
and the support to improve on uh what

00:15:39,360 --> 00:15:42,800
they are able to do with ai

00:15:40,800 --> 00:15:44,560
now empowerment also requires uh

00:15:42,800 --> 00:15:46,160
allowing room for errors

00:15:44,560 --> 00:15:48,000
encouraging experimentation and that's

00:15:46,160 --> 00:15:48,480
something uh that we are doing and in

00:15:48,000 --> 00:15:50,000
fact

00:15:48,480 --> 00:15:52,240
just after this slide we'll go into how

00:15:50,000 --> 00:15:54,240
exactly i personally your house if

00:15:52,240 --> 00:15:55,440
we have approached customers in the

00:15:54,240 --> 00:15:58,000
indian subcontinent or

00:15:55,440 --> 00:15:59,600
in the asian territories uh when it

00:15:58,000 --> 00:16:00,560
comes to driving these changes

00:15:59,600 --> 00:16:02,800
and you will see what are the

00:16:00,560 --> 00:16:04,079
repercussions of those as well so i'll

00:16:02,800 --> 00:16:04,639
show you some interesting case studies

00:16:04,079 --> 00:16:06,399
there

00:16:04,639 --> 00:16:08,240
so lastly it's about creating a

00:16:06,399 --> 00:16:10,079
responsible approach to air

00:16:08,240 --> 00:16:11,600
so that uh you address the challenges

00:16:10,079 --> 00:16:12,800
the questions are

00:16:11,600 --> 00:16:14,880
here it is the challenging questions

00:16:12,800 --> 00:16:16,720
that ai presents in front of you so from

00:16:14,880 --> 00:16:18,160
our perspective uh this third key

00:16:16,720 --> 00:16:20,880
element of an ai ready culture

00:16:18,160 --> 00:16:21,440
is to foster a responsible approach to

00:16:20,880 --> 00:16:23,600
ai

00:16:21,440 --> 00:16:26,000
now as the eye continues to evolve it

00:16:23,600 --> 00:16:27,440
has the potential to drive considerable

00:16:26,000 --> 00:16:29,360
changes to our lives

00:16:27,440 --> 00:16:31,279
raising complex uh and challenging

00:16:29,360 --> 00:16:32,079
questions about what future we want to

00:16:31,279 --> 00:16:33,680
see

00:16:32,079 --> 00:16:35,279
so these are questions that deserve a

00:16:33,680 --> 00:16:35,759
dedicated discussion so let's go ahead

00:16:35,279 --> 00:16:37,600
and

00:16:35,759 --> 00:16:38,959
explore them more deeply before that i

00:16:37,600 --> 00:16:40,720
just look at

00:16:38,959 --> 00:16:42,000
the chat window in case there are some

00:16:40,720 --> 00:16:44,240
questions

00:16:42,000 --> 00:16:45,519
if they are not it's absolutely fine i

00:16:44,240 --> 00:16:49,279
see there are no questions it's

00:16:45,519 --> 00:16:51,040
good yeah you can post your questions

00:16:49,279 --> 00:16:52,560
since heyman has already mentioned uh i

00:16:51,040 --> 00:16:54,320
will respond to them but we will have a

00:16:52,560 --> 00:16:55,680
dedicated q a at the end as well

00:16:54,320 --> 00:16:57,120
uh again this is a very interesting

00:16:55,680 --> 00:16:58,480
discussion you can your questions could

00:16:57,120 --> 00:16:59,839
be around anything to be very friendly

00:16:58,480 --> 00:17:00,800
it could be around app dev could be

00:16:59,839 --> 00:17:03,199
around github

00:17:00,800 --> 00:17:05,039
i'll answer all of those don't worry so

00:17:03,199 --> 00:17:08,400
with that let's move forward so

00:17:05,039 --> 00:17:09,760
yeah so the last piece and again this is

00:17:08,400 --> 00:17:10,959
the most central piece and this is where

00:17:09,760 --> 00:17:11,520
i'll show you something very interesting

00:17:10,959 --> 00:17:13,839
now

00:17:11,520 --> 00:17:14,640
so uh i got a confirmation about my

00:17:13,839 --> 00:17:18,240
speech uh

00:17:14,640 --> 00:17:20,559
at the event uh sometime in uh i think

00:17:18,240 --> 00:17:21,439
in december uh and uh at that point of

00:17:20,559 --> 00:17:24,000
time i started

00:17:21,439 --> 00:17:25,679
preparing for it so i went through three

00:17:24,000 --> 00:17:27,280
i went through multiple articles in

00:17:25,679 --> 00:17:28,799
harvard business review and

00:17:27,280 --> 00:17:31,520
this is a very interesting piece now

00:17:28,799 --> 00:17:34,720
look at these three different articles

00:17:31,520 --> 00:17:36,160
all three of them uh they are actually

00:17:34,720 --> 00:17:37,919
talking about the repercussions of

00:17:36,160 --> 00:17:39,679
responsibility or what if ethical

00:17:37,919 --> 00:17:42,880
practices are not there in your area

00:17:39,679 --> 00:17:45,679
uh all three of them they are actually

00:17:42,880 --> 00:17:46,400
posted within like two months i'll ask

00:17:45,679 --> 00:17:48,080
you once

00:17:46,400 --> 00:17:50,320
and look at this interesting piece all

00:17:48,080 --> 00:17:51,600
three of them talk to different aspects

00:17:50,320 --> 00:17:53,520
of your organization

00:17:51,600 --> 00:17:55,039
is ethical principle getting ethical

00:17:53,520 --> 00:17:56,080
principles in your system is it a social

00:17:55,039 --> 00:17:58,000
responsibility

00:17:56,080 --> 00:17:59,039
is it a strategical decision or is it

00:17:58,000 --> 00:18:00,559
something that should be done with

00:17:59,039 --> 00:18:02,480
technology again

00:18:00,559 --> 00:18:04,000
the entire premises people don't even

00:18:02,480 --> 00:18:06,559
understand today today

00:18:04,000 --> 00:18:08,880
when we are working on any ai model our

00:18:06,559 --> 00:18:11,679
approach is to get the maximum accuracy

00:18:08,880 --> 00:18:12,559
get the best results but we don't care

00:18:11,679 --> 00:18:14,080
about these things

00:18:12,559 --> 00:18:16,080
and in fact i will show you what

00:18:14,080 --> 00:18:18,960
actually happens when you don't do that

00:18:16,080 --> 00:18:20,720
so uh as microsoft our ai journey so in

00:18:18,960 --> 00:18:22,320
light of this entire responsibility

00:18:20,720 --> 00:18:24,559
uh we have seen that organizations they

00:18:22,320 --> 00:18:26,320
are finding the need to create processes

00:18:24,559 --> 00:18:28,000
and structures to guide their

00:18:26,320 --> 00:18:29,760
internal ai efforts whether they are

00:18:28,000 --> 00:18:31,360
deploying third party ai solutions or

00:18:29,760 --> 00:18:33,520
developing their own solutions as well

00:18:31,360 --> 00:18:35,120
now we also recognize that every

00:18:33,520 --> 00:18:38,000
organization will have their own

00:18:35,120 --> 00:18:39,360
beliefs and standards in their hrd so

00:18:38,000 --> 00:18:39,840
what i will do in the next 10 minutes is

00:18:39,360 --> 00:18:41,200
share

00:18:39,840 --> 00:18:42,880
our principles and then we'll

00:18:41,200 --> 00:18:43,440
straightaway open the fourth question

00:18:42,880 --> 00:18:46,640
announces

00:18:43,440 --> 00:18:49,120
okay so uh microsoft uh

00:18:46,640 --> 00:18:50,320
our journey started uh in 2016 as some

00:18:49,120 --> 00:18:52,160
of you might be familiar

00:18:50,320 --> 00:18:54,160
we put forward six principles that guide

00:18:52,160 --> 00:18:56,960
our development and use of ai these were

00:18:54,160 --> 00:18:59,039
fairness uh reliability and safety

00:18:56,960 --> 00:19:01,200
privacy and security inclusiveness

00:18:59,039 --> 00:19:03,200
accountability and transparency

00:19:01,200 --> 00:19:04,480
so next we put these principles into

00:19:03,200 --> 00:19:05,919
practice uh

00:19:04,480 --> 00:19:07,520
and then finally we use tools and

00:19:05,919 --> 00:19:08,400
resources that makes it easier for

00:19:07,520 --> 00:19:10,000
developers

00:19:08,400 --> 00:19:12,000
and data scientists to identify and

00:19:10,000 --> 00:19:14,400
mitigate potentially harmful resources

00:19:12,000 --> 00:19:15,840
or issues in their process in their

00:19:14,400 --> 00:19:16,720
building of the data science life cycle

00:19:15,840 --> 00:19:19,039
process

00:19:16,720 --> 00:19:20,000
so let's talk about these kind of

00:19:19,039 --> 00:19:21,360
principles

00:19:20,000 --> 00:19:23,039
okay so first principle is around

00:19:21,360 --> 00:19:25,280
fairness so for ai

00:19:23,039 --> 00:19:26,559
this means that ai systems should treat

00:19:25,280 --> 00:19:28,640
everyone fairly

00:19:26,559 --> 00:19:30,880
and avoid affecting similarly situated

00:19:28,640 --> 00:19:32,320
groups of people in different areas

00:19:30,880 --> 00:19:34,240
so basically the idea here is for

00:19:32,320 --> 00:19:36,799
example when ai systems can

00:19:34,240 --> 00:19:38,000
are required to provide guidance on

00:19:36,799 --> 00:19:41,120
let's say medical treatment

00:19:38,000 --> 00:19:42,960
or loan applications or employment

00:19:41,120 --> 00:19:45,280
they should make the same recommendation

00:19:42,960 --> 00:19:47,120
to everyone with similar symptoms or

00:19:45,280 --> 00:19:49,120
similar financial circumstances

00:19:47,120 --> 00:19:51,039
or similar professional qualifications

00:19:49,120 --> 00:19:52,720
but unfortunately because ai as i

00:19:51,039 --> 00:19:53,039
mentioned earlier because ai is designed

00:19:52,720 --> 00:19:55,520
by

00:19:53,039 --> 00:19:56,400
humans and it's trained using data that

00:19:55,520 --> 00:19:58,559
reflects

00:19:56,400 --> 00:20:00,000
our imperfect world in the way uh the

00:19:58,559 --> 00:20:02,480
world in which we live in

00:20:00,000 --> 00:20:03,679
they may reinforce those biases so as

00:20:02,480 --> 00:20:05,280
microsoft i mean we

00:20:03,679 --> 00:20:07,520
encountered this uh when we were talking

00:20:05,280 --> 00:20:08,559
with a large financial learning

00:20:07,520 --> 00:20:11,039
institution in india

00:20:08,559 --> 00:20:11,760
and they were developing a risk scoring

00:20:11,039 --> 00:20:14,640
system for

00:20:11,760 --> 00:20:16,960
loan approvals okay and what happened

00:20:14,640 --> 00:20:17,919
was uh we trained an existing industry

00:20:16,960 --> 00:20:20,080
algorithm

00:20:17,919 --> 00:20:21,840
using the customer's data set and then

00:20:20,080 --> 00:20:24,080
what we did was we actually tried

00:20:21,840 --> 00:20:25,600
the system in a pilot program uh with

00:20:24,080 --> 00:20:27,679
the customer and in fact what we did was

00:20:25,600 --> 00:20:29,520
we had a side by side proof of concept

00:20:27,679 --> 00:20:30,720
with human loan approvals as well so

00:20:29,520 --> 00:20:33,200
that our

00:20:30,720 --> 00:20:34,640
pilot program can be validated as well

00:20:33,200 --> 00:20:36,559
now uh

00:20:34,640 --> 00:20:38,559
when we conducted the initial audit of

00:20:36,559 --> 00:20:40,400
the system we discovered that yes the

00:20:38,559 --> 00:20:43,760
system was very well because it was

00:20:40,400 --> 00:20:45,840
giving loans to only low risk uh

00:20:43,760 --> 00:20:47,280
so basically only approving low risk

00:20:45,840 --> 00:20:48,960
loans basically

00:20:47,280 --> 00:20:51,280
but interestingly what we were able to

00:20:48,960 --> 00:20:54,400
find was all the approved loans

00:20:51,280 --> 00:20:56,720
were for male borrowers there was not

00:20:54,400 --> 00:20:57,679
a single female borrower there and why

00:20:56,720 --> 00:20:59,520
was that happening

00:20:57,679 --> 00:21:01,360
because the train the training data for

00:20:59,520 --> 00:21:02,080
this it reflected the fact that loan

00:21:01,360 --> 00:21:04,480
officers

00:21:02,080 --> 00:21:06,159
they historically favor male borrowers

00:21:04,480 --> 00:21:07,840
and again the idea was this data set was

00:21:06,159 --> 00:21:08,799
like 200 years old the company was

00:21:07,840 --> 00:21:11,120
established

00:21:08,799 --> 00:21:12,559
since the 1800s and inspecting the

00:21:11,120 --> 00:21:13,520
system it allowed us to actually

00:21:12,559 --> 00:21:15,360
identify this

00:21:13,520 --> 00:21:16,799
and connect that bias before the system

00:21:15,360 --> 00:21:17,840
was actually gone into production or

00:21:16,799 --> 00:21:20,559
into deployment

00:21:17,840 --> 00:21:21,200
so okay what are the recommendations

00:21:20,559 --> 00:21:22,720
from our end

00:21:21,200 --> 00:21:24,559
to make sure that fairness is taken care

00:21:22,720 --> 00:21:26,960
of first just we recommend that you

00:21:24,559 --> 00:21:29,520
understand the scope the spirit

00:21:26,960 --> 00:21:30,000
and the potential uses of any ai system

00:21:29,520 --> 00:21:32,080
uh

00:21:30,000 --> 00:21:33,760
next is you ensure that the design teams

00:21:32,080 --> 00:21:35,280
they reflect the diversity in the world

00:21:33,760 --> 00:21:36,320
that was one of the major issues which

00:21:35,280 --> 00:21:38,720
caused this one

00:21:36,320 --> 00:21:40,559
uh third you can identify and mitigate

00:21:38,720 --> 00:21:42,240
biasing data sets by evaluating where

00:21:40,559 --> 00:21:43,600
your data is coming from

00:21:42,240 --> 00:21:45,520
understanding how it's organized and

00:21:43,600 --> 00:21:46,880
testing it to show its true

00:21:45,520 --> 00:21:50,000
representation

00:21:46,880 --> 00:21:51,440
uh next you can also identify and

00:21:50,000 --> 00:21:53,360
mitigate bias in machine learning

00:21:51,440 --> 00:21:54,880
algorithms as well it's not just that

00:21:53,360 --> 00:21:57,200
data will always give you those biases

00:21:54,880 --> 00:21:58,799
and this is where uh

00:21:57,200 --> 00:22:00,559
intelligibility and this is something i

00:21:58,799 --> 00:22:02,720
uh covered in the latest slide as well

00:22:00,559 --> 00:22:04,159
intelligibility of a particular training

00:22:02,720 --> 00:22:05,840
model is also very important

00:22:04,159 --> 00:22:07,120
and lastly we should leverage human

00:22:05,840 --> 00:22:08,080
review which is something we did in this

00:22:07,120 --> 00:22:09,919
scenario as well

00:22:08,080 --> 00:22:10,960
so humans reviewed by trained employees

00:22:09,919 --> 00:22:12,400
to understand the meaning and

00:22:10,960 --> 00:22:14,240
implications of the ai results

00:22:12,400 --> 00:22:15,679
it becomes extremely crucial when you

00:22:14,240 --> 00:22:18,080
have to use

00:22:15,679 --> 00:22:19,840
ai to inform consequential decisions

00:22:18,080 --> 00:22:21,840
about people or

00:22:19,840 --> 00:22:23,520
whenever human beings are involved so

00:22:21,840 --> 00:22:24,400
again very important concept around

00:22:23,520 --> 00:22:26,720
fairness and

00:22:24,400 --> 00:22:28,799
what can happen if it goes wrong the

00:22:26,720 --> 00:22:29,520
next principle is around reliability and

00:22:28,799 --> 00:22:32,080
safety

00:22:29,520 --> 00:22:33,679
uh to build trust it's important that ai

00:22:32,080 --> 00:22:36,799
systems they operate reliably

00:22:33,679 --> 00:22:38,880
safely and consistently under

00:22:36,799 --> 00:22:40,000
under the normal circumstances and

00:22:38,880 --> 00:22:42,400
unexpected conditions

00:22:40,000 --> 00:22:43,600
as well how they behave and the variety

00:22:42,400 --> 00:22:44,480
of conditions they can handle

00:22:43,600 --> 00:22:46,480
reliability

00:22:44,480 --> 00:22:47,919
it largely reflects the range of

00:22:46,480 --> 00:22:49,520
situations or circumstances

00:22:47,919 --> 00:22:51,200
that developers can anticipate so

00:22:49,520 --> 00:22:52,400
basically that's where the bias comes in

00:22:51,200 --> 00:22:53,360
if you are not able to even think of a

00:22:52,400 --> 00:22:56,320
particular scenario

00:22:53,360 --> 00:22:58,320
it won't be there in the model itself so

00:22:56,320 --> 00:23:00,960
again the example here was uh

00:22:58,320 --> 00:23:02,159
this was with the south east asian uh

00:23:00,960 --> 00:23:03,360
mining company

00:23:02,159 --> 00:23:05,200
uh we were designing it they were

00:23:03,360 --> 00:23:06,960
designing an ai system to identify if

00:23:05,200 --> 00:23:09,520
their drivers were alert

00:23:06,960 --> 00:23:10,240
when operating heavy mining machinery

00:23:09,520 --> 00:23:11,440
and

00:23:10,240 --> 00:23:13,440
that's where we actually had to

00:23:11,440 --> 00:23:14,000
implement a very staged implementation

00:23:13,440 --> 00:23:15,760
process

00:23:14,000 --> 00:23:18,080
so the system can be thoroughly tested

00:23:15,760 --> 00:23:20,080
before the large scale implementation

00:23:18,080 --> 00:23:21,360
again what you need to take care of here

00:23:20,080 --> 00:23:23,760
is one you need to understand your

00:23:21,360 --> 00:23:26,799
organization's ai maturity

00:23:23,760 --> 00:23:28,080
second you need to make sure that uh

00:23:26,799 --> 00:23:29,760
because as microsoft we recommend

00:23:28,080 --> 00:23:30,400
developing practices for auditing ai

00:23:29,760 --> 00:23:32,960
systems

00:23:30,400 --> 00:23:34,559
uh if you can use that and again i just

00:23:32,960 --> 00:23:36,240
i'm talking about microsoft but again

00:23:34,559 --> 00:23:37,440
amazon is doing the same as well google

00:23:36,240 --> 00:23:39,039
is doing the same as well

00:23:37,440 --> 00:23:40,559
so at the end of the day the industry

00:23:39,039 --> 00:23:41,679
leads me as industry leaders we are

00:23:40,559 --> 00:23:43,520
doing all of these things

00:23:41,679 --> 00:23:44,799
so that any organization out there can

00:23:43,520 --> 00:23:48,080
use this uh

00:23:44,799 --> 00:23:50,720
third you should evaluate when and how

00:23:48,080 --> 00:23:52,240
an ai system should seek human input it

00:23:50,720 --> 00:23:55,520
becomes extremely crucial

00:23:52,240 --> 00:23:58,000
in almost every scenario it's not a

00:23:55,520 --> 00:23:59,279
actual replacement it's a collaboration

00:23:58,000 --> 00:24:00,799
so that's very crucial

00:23:59,279 --> 00:24:03,039
and finally they recommend developing a

00:24:00,799 --> 00:24:05,679
robust feedback mechanism for users

00:24:03,039 --> 00:24:06,720
to report performance issues and again

00:24:05,679 --> 00:24:07,919
if you think about it in a very

00:24:06,720 --> 00:24:10,080
traditional sense

00:24:07,919 --> 00:24:11,919
feedback should be eventually reaching

00:24:10,080 --> 00:24:13,440
the training algorithms themselves so

00:24:11,919 --> 00:24:14,159
that it can retrain the model and go

00:24:13,440 --> 00:24:16,320
back

00:24:14,159 --> 00:24:18,640
so that's our reliability and safety

00:24:16,320 --> 00:24:19,679
next one a very very crucial piece uh

00:24:18,640 --> 00:24:21,600
it's a

00:24:19,679 --> 00:24:23,039
crucial to develop air systems that can

00:24:21,600 --> 00:24:25,200
protect private information

00:24:23,039 --> 00:24:26,640
and resist attacks and again i'll show

00:24:25,200 --> 00:24:28,480
you a very i'll tell you a very

00:24:26,640 --> 00:24:30,720
interesting uh case study here

00:24:28,480 --> 00:24:31,520
so the whole idea here is as ai becomes

00:24:30,720 --> 00:24:33,600
more

00:24:31,520 --> 00:24:35,520
prevalent in the industry today uh

00:24:33,600 --> 00:24:37,120
securing important personal and business

00:24:35,520 --> 00:24:37,919
information is becoming more critical

00:24:37,120 --> 00:24:40,000
and complex

00:24:37,919 --> 00:24:41,840
the privacy and data security issues

00:24:40,000 --> 00:24:43,200
they require especially close attention

00:24:41,840 --> 00:24:45,039
for ai because

00:24:43,200 --> 00:24:46,400
access to data is sort of essential to

00:24:45,039 --> 00:24:47,200
air systems and hence it becomes very

00:24:46,400 --> 00:24:50,240
crucial

00:24:47,200 --> 00:24:51,520
so uh the interesting example here is uh

00:24:50,240 --> 00:24:54,159
this was microsoft's

00:24:51,520 --> 00:24:55,039
own uh so we released a chit chat bought

00:24:54,159 --> 00:24:57,840
on twitter

00:24:55,039 --> 00:25:00,080
uh it was called day uh so basically we

00:24:57,840 --> 00:25:02,320
taught the bot the chatbot to learn from

00:25:00,080 --> 00:25:03,600
online interactions so that the bot

00:25:02,320 --> 00:25:05,600
could better replicate human

00:25:03,600 --> 00:25:06,799
communication and personality traits

00:25:05,600 --> 00:25:08,000
but what happened was and this is

00:25:06,799 --> 00:25:08,640
something this was something we did two

00:25:08,000 --> 00:25:11,200
years ago

00:25:08,640 --> 00:25:12,720
but what happened was uh within 24 years

00:25:11,200 --> 00:25:13,360
of the deployment of that particular

00:25:12,720 --> 00:25:15,360
board

00:25:13,360 --> 00:25:17,039
users on twitter they realize that the

00:25:15,360 --> 00:25:18,880
bot can actually learn

00:25:17,039 --> 00:25:21,679
and they begin to feed the bot with

00:25:18,880 --> 00:25:23,760
tectonic uh so turning that entire broad

00:25:21,679 --> 00:25:25,440
form a very polite board to a vehicle

00:25:23,760 --> 00:25:26,159
for hate speech and we have to remove

00:25:25,440 --> 00:25:28,480
that

00:25:26,159 --> 00:25:30,080
so again here the important pieces are

00:25:28,480 --> 00:25:32,320
you need to comply with

00:25:30,080 --> 00:25:33,120
relevant data protection data privacy

00:25:32,320 --> 00:25:35,440
and data

00:25:33,120 --> 00:25:37,600
uh transparency laws this could be gdpr

00:25:35,440 --> 00:25:38,720
this could be the california privacy act

00:25:37,600 --> 00:25:40,080
and all of those things

00:25:38,720 --> 00:25:42,320
uh second you need to make sure you

00:25:40,080 --> 00:25:43,200
design ai systems that maintain one the

00:25:42,320 --> 00:25:45,279
anonymity

00:25:43,200 --> 00:25:47,840
and second the integrity of personal

00:25:45,279 --> 00:25:48,480
data and lastly to protect the ai system

00:25:47,840 --> 00:25:50,880
from bad

00:25:48,480 --> 00:25:51,679
actors uh you need to make sure that

00:25:50,880 --> 00:25:54,400
they are designed

00:25:51,679 --> 00:25:56,400
in accordance with secure deployment and

00:25:54,400 --> 00:25:58,400
moreover they should be uh designed to

00:25:56,400 --> 00:26:00,000
identify abnormal behaviors

00:25:58,400 --> 00:26:01,679
or to prevent uh to prevent the

00:26:00,000 --> 00:26:02,480
manipulation and malicious attacks that

00:26:01,679 --> 00:26:04,880
can happen

00:26:02,480 --> 00:26:06,960
on that particular pace in this case the

00:26:04,880 --> 00:26:09,440
chatbot that we built

00:26:06,960 --> 00:26:10,000
the next principle is inclusiveness and

00:26:09,440 --> 00:26:12,080
again

00:26:10,000 --> 00:26:13,440
there are already i mean we know that

00:26:12,080 --> 00:26:15,039
for a fact that there are around one

00:26:13,440 --> 00:26:15,600
billion people with disabilities around

00:26:15,039 --> 00:26:17,360
the world

00:26:15,600 --> 00:26:19,520
and for them ai technologies can be

00:26:17,360 --> 00:26:21,440
truly game changer ai can

00:26:19,520 --> 00:26:23,360
improve access to education the

00:26:21,440 --> 00:26:25,360
government services employment

00:26:23,360 --> 00:26:26,640
information all of those things for them

00:26:25,360 --> 00:26:28,400
so intelligent solutions like for

00:26:26,640 --> 00:26:30,720
example a simple solution for

00:26:28,400 --> 00:26:32,960
a real-time speech to text transcription

00:26:30,720 --> 00:26:34,559
or visual recognition services or

00:26:32,960 --> 00:26:37,360
predictive text functionality

00:26:34,559 --> 00:26:38,320
they're already improving or empowering

00:26:37,360 --> 00:26:40,880
those with

00:26:38,320 --> 00:26:41,919
hearing visual or other impairments so

00:26:40,880 --> 00:26:43,279
uh

00:26:41,919 --> 00:26:44,799
in this case we actually were working

00:26:43,279 --> 00:26:45,360
with the australian department for human

00:26:44,799 --> 00:26:47,679
services

00:26:45,360 --> 00:26:49,679
and the dhs there uh to build an ai

00:26:47,679 --> 00:26:51,840
system that could augment the

00:26:49,679 --> 00:26:52,960
so-called overlaid uh overloaded call

00:26:51,840 --> 00:26:54,960
center operators

00:26:52,960 --> 00:26:57,200
now with inclusion in mind we conducted

00:26:54,960 --> 00:26:58,880
user research and we discovered that

00:26:57,200 --> 00:27:00,880
some citizens they did not have the

00:26:58,880 --> 00:27:03,360
ability to call the dhs

00:27:00,880 --> 00:27:04,720
uh due to either because of their

00:27:03,360 --> 00:27:05,520
disability or because of the lack of

00:27:04,720 --> 00:27:07,760
phone service

00:27:05,520 --> 00:27:09,200
meaning that they were they could only

00:27:07,760 --> 00:27:11,039
provide uh

00:27:09,200 --> 00:27:12,240
they could only access services by

00:27:11,039 --> 00:27:14,320
in-person appointment

00:27:12,240 --> 00:27:16,640
so what we did there was again making

00:27:14,320 --> 00:27:18,720
sure that we created a multi-modal

00:27:16,640 --> 00:27:20,799
multi-channel intelligent chatbot that

00:27:18,720 --> 00:27:22,480
can not only improve uh

00:27:20,799 --> 00:27:24,399
accessibility for those groups but also

00:27:22,480 --> 00:27:27,919
provide a more convenient experience

00:27:24,399 --> 00:27:29,600
for every single individual so

00:27:27,919 --> 00:27:31,279
these are the four principles now

00:27:29,600 --> 00:27:32,720
underlying these principles are two

00:27:31,279 --> 00:27:34,159
foundational elements

00:27:32,720 --> 00:27:35,919
which are essential for ensuring that

00:27:34,159 --> 00:27:36,480
effectiveness of these four are taken

00:27:35,919 --> 00:27:38,960
care of

00:27:36,480 --> 00:27:40,880
which is transparency and accountability

00:27:38,960 --> 00:27:42,640
uh when we talk about transparency

00:27:40,880 --> 00:27:44,399
uh whenever a systems they are used to

00:27:42,640 --> 00:27:46,640
help inform decisions that help

00:27:44,399 --> 00:27:48,480
uh they have tremendous impact of people

00:27:46,640 --> 00:27:49,600
like like people's life for example in

00:27:48,480 --> 00:27:51,360
the healthcare sector

00:27:49,600 --> 00:27:53,679
uh it's critical that people understand

00:27:51,360 --> 00:27:56,799
how these decisions were made

00:27:53,679 --> 00:27:58,080
uh so a crucial part of transparency as

00:27:56,799 --> 00:28:00,480
i mentioned earlier

00:27:58,080 --> 00:28:01,600
is what we refer to as intelligibility

00:28:00,480 --> 00:28:04,240
or the so-called

00:28:01,600 --> 00:28:04,960
useful explanation of the behavior of ai

00:28:04,240 --> 00:28:07,520
systems

00:28:04,960 --> 00:28:09,200
and their confidence so uh again in this

00:28:07,520 --> 00:28:12,240
case we actually partnered with

00:28:09,200 --> 00:28:14,399
a large healthcare provider in india

00:28:12,240 --> 00:28:16,159
uh to develop a more accurate risk model

00:28:14,399 --> 00:28:19,279
for detecting a cardiovascular

00:28:16,159 --> 00:28:21,200
disease or cardiac diseases and we had

00:28:19,279 --> 00:28:23,200
around 32 000 patients

00:28:21,200 --> 00:28:24,640
records and we were able to build a risk

00:28:23,200 --> 00:28:25,360
model that was significantly more

00:28:24,640 --> 00:28:27,279
accurate

00:28:25,360 --> 00:28:29,120
than the order that was used previously

00:28:27,279 --> 00:28:30,559
but again what was important that we

00:28:29,120 --> 00:28:32,720
wanted to actually

00:28:30,559 --> 00:28:33,760
ensure that the providers they could

00:28:32,720 --> 00:28:36,559
understand

00:28:33,760 --> 00:28:37,600
how the system scores patients won uh so

00:28:36,559 --> 00:28:39,360
basically what we did was

00:28:37,600 --> 00:28:40,720
work together to create an interface

00:28:39,360 --> 00:28:41,360
that explains the result or the

00:28:40,720 --> 00:28:43,520
so-called

00:28:41,360 --> 00:28:45,360
explainability of a particular model so

00:28:43,520 --> 00:28:48,159
okay so the interface uses three

00:28:45,360 --> 00:28:50,159
categories dietary medical and activity

00:28:48,159 --> 00:28:51,840
again uh not to go about to go too much

00:28:50,159 --> 00:28:53,360
deeper here but the whole idea is

00:28:51,840 --> 00:28:55,360
it showcases the healthcare providers

00:28:53,360 --> 00:28:57,039
how it's called patients uh enabling

00:28:55,360 --> 00:28:58,320
providers to develop the most effective

00:28:57,039 --> 00:29:00,880
treatment plans for their

00:28:58,320 --> 00:29:02,240
patients specifically the last principle

00:29:00,880 --> 00:29:02,640
again this is the second last slide that

00:29:02,240 --> 00:29:04,320
i have

00:29:02,640 --> 00:29:06,000
and then we'll open up for question

00:29:04,320 --> 00:29:08,399
analysis so the last principle

00:29:06,000 --> 00:29:09,679
is accountability we believe that people

00:29:08,399 --> 00:29:11,919
who design and deploy

00:29:09,679 --> 00:29:13,600
ai systems they must be accountable for

00:29:11,919 --> 00:29:15,279
how their systems operate

00:29:13,600 --> 00:29:17,279
uh the need for accountability is

00:29:15,279 --> 00:29:19,360
particularly crucial with

00:29:17,279 --> 00:29:20,399
sensitive use cases for example facial

00:29:19,360 --> 00:29:22,559
recognition

00:29:20,399 --> 00:29:24,399
now it's a very interesting piece

00:29:22,559 --> 00:29:24,880
because recently we have seen there has

00:29:24,399 --> 00:29:27,120
been

00:29:24,880 --> 00:29:28,960
a growing demand for facial recognition

00:29:27,120 --> 00:29:29,760
technology especially by the law

00:29:28,960 --> 00:29:32,240
enforcement

00:29:29,760 --> 00:29:33,840
organizations who see a lot of potential

00:29:32,240 --> 00:29:36,480
use cases of this technology like

00:29:33,840 --> 00:29:37,600
for example finding missing children now

00:29:36,480 --> 00:29:39,520
however as

00:29:37,600 --> 00:29:40,640
like as microsoft recognized that these

00:29:39,520 --> 00:29:42,559
technologies

00:29:40,640 --> 00:29:44,640
they can potentially be used by a

00:29:42,559 --> 00:29:45,840
government to put fundamental freedoms

00:29:44,640 --> 00:29:47,520
of people at risk

00:29:45,840 --> 00:29:49,520
for example enabling continuous

00:29:47,520 --> 00:29:51,760
surveillance of specific individuals

00:29:49,520 --> 00:29:54,720
so to that end we have publicly called

00:29:51,760 --> 00:29:57,279
for regulations on this and in fact

00:29:54,720 --> 00:29:58,480
we have we have we are like very vocal

00:29:57,279 --> 00:29:59,760
advocates of this that

00:29:58,480 --> 00:30:01,600
this should be regulations when it comes

00:29:59,760 --> 00:30:02,799
to any technology out there so it's

00:30:01,600 --> 00:30:04,000
important to recognize that facial

00:30:02,799 --> 00:30:05,279
recognition technology

00:30:04,000 --> 00:30:08,080
is not like it's not going to be the

00:30:05,279 --> 00:30:09,200
last technology with sensitive use cases

00:30:08,080 --> 00:30:10,880
or corner cases

00:30:09,200 --> 00:30:12,240
uh it only serves to highlight the

00:30:10,880 --> 00:30:14,399
importance of

00:30:12,240 --> 00:30:15,600
one remaining vigilant and second

00:30:14,399 --> 00:30:18,159
accountable for

00:30:15,600 --> 00:30:19,279
faults uh and harmful uses in all the

00:30:18,159 --> 00:30:21,200
emerging ai cases

00:30:19,279 --> 00:30:23,200
use cases that we'll see in future as

00:30:21,200 --> 00:30:25,200
well so uh

00:30:23,200 --> 00:30:27,360
at microsoft we have developed these six

00:30:25,200 --> 00:30:29,760
principles to guide our use of ai

00:30:27,360 --> 00:30:30,960
with the aim of respecting uh collective

00:30:29,760 --> 00:30:32,320
values when

00:30:30,960 --> 00:30:34,159
helping society realize the full

00:30:32,320 --> 00:30:36,159
potential of air uh we encourage

00:30:34,159 --> 00:30:38,000
organizations to do the same as well

00:30:36,159 --> 00:30:39,760
uh from holistic uh like from

00:30:38,000 --> 00:30:40,399
holistically transforming businesses or

00:30:39,760 --> 00:30:42,960
industries

00:30:40,399 --> 00:30:44,960
to addressing critical use cases or

00:30:42,960 --> 00:30:47,200
critical issues that are facing humanity

00:30:44,960 --> 00:30:48,799
uh ai is already solving some of the

00:30:47,200 --> 00:30:49,360
most complex challenges that are out

00:30:48,799 --> 00:30:51,360
there

00:30:49,360 --> 00:30:52,640
and redefining how humans and technology

00:30:51,360 --> 00:30:55,760
interact with each other

00:30:52,640 --> 00:30:57,600
so with that today uh what i did was i

00:30:55,760 --> 00:30:59,760
think in a very brief 32 minutes i'm

00:30:57,600 --> 00:31:01,200
outlining some of the steps that we

00:30:59,760 --> 00:31:04,000
have taken or we are taking to

00:31:01,200 --> 00:31:05,840
prioritize responsible ai

00:31:04,000 --> 00:31:07,919
and in hopes that our experience can

00:31:05,840 --> 00:31:09,919
help others as well however we recognize

00:31:07,919 --> 00:31:12,000
that we do not have all the answers

00:31:09,919 --> 00:31:13,760
it's very true for example the twitter

00:31:12,000 --> 00:31:15,840
chatbot that we created

00:31:13,760 --> 00:31:16,960
and and every organization out there it

00:31:15,840 --> 00:31:19,200
has to

00:31:16,960 --> 00:31:20,159
has it has to have its own beliefs and

00:31:19,200 --> 00:31:22,880
standards as

00:31:20,159 --> 00:31:24,559
organizations and as a society we uh our

00:31:22,880 --> 00:31:27,279
steps to a responsible ai

00:31:24,559 --> 00:31:28,880
it will need to continually evolve to

00:31:27,279 --> 00:31:30,720
reflect new innovations

00:31:28,880 --> 00:31:32,000
and lessons from both our mistakes and

00:31:30,720 --> 00:31:33,840
our accomplishments by

00:31:32,000 --> 00:31:35,279
engaging with ai in responsible manner

00:31:33,840 --> 00:31:37,360
we can ensure that it fulfills the

00:31:35,279 --> 00:31:39,600
promise to create a better future

00:31:37,360 --> 00:31:41,600
so thank you all uh for attending this

00:31:39,600 --> 00:31:43,360
session today the next step for all of

00:31:41,600 --> 00:31:45,519
you is to share what you learned today

00:31:43,360 --> 00:31:47,039
with your within your organization so as

00:31:45,519 --> 00:31:48,080
to discover what opportunities ai

00:31:47,039 --> 00:31:50,080
presents and

00:31:48,080 --> 00:31:51,679
for you yourself and for your business

00:31:50,080 --> 00:31:54,080
and now with that we'll open up four

00:31:51,679 --> 00:31:54,080
questions

00:31:58,000 --> 00:32:02,559
i see there are a few questions

00:32:04,720 --> 00:32:09,760
uh none in the q a one in

00:32:10,080 --> 00:32:15,840
uh okay one in chat window

00:32:17,840 --> 00:32:22,720
even if there are any questions maybe

00:32:19,440 --> 00:32:25,600
you can ask me those it's okay

00:32:22,720 --> 00:32:26,880
uh well rishabh i could see one of our

00:32:25,600 --> 00:32:30,000
attendee has

00:32:26,880 --> 00:32:32,320
raised one question are there some

00:32:30,000 --> 00:32:34,159
open source projects following the

00:32:32,320 --> 00:32:36,399
guidelines you are talking about

00:32:34,159 --> 00:32:37,600
i mean if you have any idea about any

00:32:36,399 --> 00:32:39,440
open source project

00:32:37,600 --> 00:32:40,960
which is also following the guidelines

00:32:39,440 --> 00:32:43,679
that you have described

00:32:40,960 --> 00:32:44,480
during your session it would be great uh

00:32:43,679 --> 00:32:48,000
information

00:32:44,480 --> 00:32:48,000
for the person who is looking for

00:32:48,880 --> 00:32:52,960
okay uh i like it from the top of my

00:32:51,440 --> 00:32:53,519
mind i don't have an answer for that

00:32:52,960 --> 00:32:55,039
question

00:32:53,519 --> 00:32:56,720
but i will be there in the discussion

00:32:55,039 --> 00:32:57,840
i'll make sure that if i find some link

00:32:56,720 --> 00:32:59,840
i'll share those

00:32:57,840 --> 00:33:01,360
uh whatever i discuss today it's more or

00:32:59,840 --> 00:33:02,000
less around case studies we did with our

00:33:01,360 --> 00:33:03,760
customers

00:33:02,000 --> 00:33:05,279
again because i personally work with

00:33:03,760 --> 00:33:05,679
customers day in day out so that's what

00:33:05,279 --> 00:33:07,039
i had

00:33:05,679 --> 00:33:08,640
but yeah if i am able to find some

00:33:07,039 --> 00:33:10,399
answers i'll make sure i share those

00:33:08,640 --> 00:33:12,880
things with you yes

00:33:10,399 --> 00:33:14,320
hey monty as we wait for uh another

00:33:12,880 --> 00:33:15,360
question from our audience i i have one

00:33:14,320 --> 00:33:17,200
question you brought up a really

00:33:15,360 --> 00:33:20,240
interesting case study where

00:33:17,200 --> 00:33:24,080
uh the

00:33:20,240 --> 00:33:27,120
loans in india your training data was

00:33:24,080 --> 00:33:29,679
uh male oriented and therefore your ai

00:33:27,120 --> 00:33:32,240
models were male oriented as well

00:33:29,679 --> 00:33:33,919
thankfully you looked at the data and

00:33:32,240 --> 00:33:37,279
and saw that it was lopsided

00:33:33,919 --> 00:33:39,120
and it wasn't approving women but

00:33:37,279 --> 00:33:42,399
what are techniques that people can use

00:33:39,120 --> 00:33:45,120
to understand is their bias

00:33:42,399 --> 00:33:45,919
in their models uh whether it's sex or

00:33:45,120 --> 00:33:50,159
age

00:33:45,919 --> 00:33:52,880
you know or you know or whatever

00:33:50,159 --> 00:33:55,039
yeah so a pretty good question eric uh

00:33:52,880 --> 00:33:56,240
basically one key aspect is around

00:33:55,039 --> 00:33:58,080
explainability

00:33:56,240 --> 00:33:59,919
and again every big organization is

00:33:58,080 --> 00:34:01,919
actually working on this all the

00:33:59,919 --> 00:34:02,960
trained models are out there they have

00:34:01,919 --> 00:34:04,960
that component they are

00:34:02,960 --> 00:34:06,159
okay if my model is giving some

00:34:04,960 --> 00:34:08,079
particular result why

00:34:06,159 --> 00:34:09,839
exactly is that result coming in that's

00:34:08,079 --> 00:34:10,480
one beautiful way to actually think

00:34:09,839 --> 00:34:12,159
about

00:34:10,480 --> 00:34:13,679
actually learning it from machines the

00:34:12,159 --> 00:34:14,480
second pieces i mentioned in my talk as

00:34:13,679 --> 00:34:16,639
well uh

00:34:14,480 --> 00:34:18,000
is about having checks and balances

00:34:16,639 --> 00:34:20,079
having some humans

00:34:18,000 --> 00:34:21,520
in the loop who can make it who can make

00:34:20,079 --> 00:34:22,800
sure that okay when my system is going

00:34:21,520 --> 00:34:25,040
into pilot when it's the

00:34:22,800 --> 00:34:26,159
it's at a proof of concept stage at that

00:34:25,040 --> 00:34:26,800
time if you have those checks and

00:34:26,159 --> 00:34:28,639
balances

00:34:26,800 --> 00:34:31,359
before going into production because i

00:34:28,639 --> 00:34:33,599
mean our chatbot it went into production

00:34:31,359 --> 00:34:34,800
and then we faced those issues it was

00:34:33,599 --> 00:34:35,440
actually on twitter and that's when we

00:34:34,800 --> 00:34:37,359
realized that

00:34:35,440 --> 00:34:39,119
people can also learn that it is

00:34:37,359 --> 00:34:41,599
learning uh it can

00:34:39,119 --> 00:34:42,639
be taught to speak in a different way

00:34:41,599 --> 00:34:44,240
and it became a

00:34:42,639 --> 00:34:46,480
it became a disaster for us as an

00:34:44,240 --> 00:34:48,720
organization so

00:34:46,480 --> 00:34:50,560
one technique being make sure that that

00:34:48,720 --> 00:34:51,839
those human checks are part of your test

00:34:50,560 --> 00:34:55,440
plan

00:34:51,839 --> 00:34:59,280
and you validate absolutely yeah i think

00:34:55,440 --> 00:35:00,560
absolutely but it but the scale i mean

00:34:59,280 --> 00:35:01,760
at a massive skill it's just not

00:35:00,560 --> 00:35:02,320
possible to have human checks and

00:35:01,760 --> 00:35:04,560
balances

00:35:02,320 --> 00:35:06,000
when it comes to machine learning if it

00:35:04,560 --> 00:35:06,880
was a typical software developer life

00:35:06,000 --> 00:35:08,160
cycle yes

00:35:06,880 --> 00:35:10,480
but when it comes to machine learning

00:35:08,160 --> 00:35:12,079
models it's not possible to have human

00:35:10,480 --> 00:35:13,280
checks and balances in every stage

00:35:12,079 --> 00:35:15,359
and that's where it becomes important

00:35:13,280 --> 00:35:16,079
that you reevaluate all the decisions

00:35:15,359 --> 00:35:18,320
that your

00:35:16,079 --> 00:35:19,680
air algorithm is taking and that's where

00:35:18,320 --> 00:35:25,119
explainability becomes

00:35:19,680 --> 00:35:25,119

YouTube URL: https://www.youtube.com/watch?v=uwlNfyc5N24


