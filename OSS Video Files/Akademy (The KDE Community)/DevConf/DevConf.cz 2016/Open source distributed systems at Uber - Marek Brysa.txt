Title: Open source distributed systems at Uber - Marek Brysa
Publication date: 2016-02-11
Playlist: DevConf.cz 2016
Description: 
	Overview of the key open source projects created at Uber to enable it's global scale and rapid growth.

Scalable, fault-tolerant application-layer sharding:
https://github.com/uber/ringpop-node
https://github.com/uber/ringpop-go

Network multiplexing and framing protocol for RPC: 
https://github.com/uber/tchannel

Service discovery and routing for large scale microservice operations:
https://github.com/uber/hyperbahn

Presentation: http://bit.ly/20QmbKW
Captions: 
	00:00:00,000 --> 00:00:07,309
it's mostly focused on being available

00:00:04,890 --> 00:00:10,380
we can deal with some inconsistencies

00:00:07,309 --> 00:00:14,670
later even if it means like manual

00:00:10,380 --> 00:00:18,330
intervention but we never want our we

00:00:14,670 --> 00:00:21,990
never want to be down we want the app to

00:00:18,330 --> 00:00:27,980
be working every time every day every

00:00:21,990 --> 00:00:30,000
minute and that's that's challenging so

00:00:27,980 --> 00:00:33,239
but you what you're seeing on the screen

00:00:30,000 --> 00:00:36,239
is is the view of Amsterdam the city

00:00:33,239 --> 00:00:38,989
that I live in now from uber point of

00:00:36,239 --> 00:00:41,250
view you can see like the s like the

00:00:38,989 --> 00:00:43,710
brightness of the color represent

00:00:41,250 --> 00:00:49,039
represents how often and uber cars car

00:00:43,710 --> 00:00:52,320
drives there so here and so our

00:00:49,039 --> 00:00:55,050
architecture we have my servers in

00:00:52,320 --> 00:00:57,750
geographically distributed data centers

00:00:55,050 --> 00:01:01,170
and we have a service-oriented

00:00:57,750 --> 00:01:05,339
architecture with hundreds of micro

00:01:01,170 --> 00:01:10,979
services and it's not only about people

00:01:05,339 --> 00:01:14,930
actually we have no tech talk is good

00:01:10,979 --> 00:01:20,759
enough without a picture of some kitten

00:01:14,930 --> 00:01:24,330
so uber is actually going into the like

00:01:20,759 --> 00:01:27,960
the last mile delivery business we can

00:01:24,330 --> 00:01:31,350
deliver packages different cities that's

00:01:27,960 --> 00:01:34,439
part of the brush service we can deliver

00:01:31,350 --> 00:01:37,439
food very quickly within minutes as part

00:01:34,439 --> 00:01:41,420
of the uber eats service and we have

00:01:37,439 --> 00:01:45,090
some like fun promotions where you can

00:01:41,420 --> 00:01:48,780
like we agree with some animal shelter

00:01:45,090 --> 00:01:52,619
and we like take some kitten from them

00:01:48,780 --> 00:01:55,380
and and people can can order an uber car

00:01:52,619 --> 00:01:57,420
with kitten and like the money that they

00:01:55,380 --> 00:01:59,189
give them goes goes to the animal

00:01:57,420 --> 00:02:02,750
shelter and then they also can keep the

00:01:59,189 --> 00:02:02,750
cat so yeah

00:02:02,890 --> 00:02:13,959
some fun right so about open source our

00:02:10,899 --> 00:02:17,709
platforms are built on open source

00:02:13,959 --> 00:02:22,020
technologies everywhere we are using

00:02:17,709 --> 00:02:25,420
ubuntu and debian linux on our servers

00:02:22,020 --> 00:02:27,459
we are using a lot of docker dr. was a

00:02:25,420 --> 00:02:29,880
big buzzword last year I don't know if

00:02:27,459 --> 00:02:34,630
it still is this year but I guess it is

00:02:29,880 --> 00:02:36,450
and the main languages our back-end

00:02:34,630 --> 00:02:40,239
services are written in our Python

00:02:36,450 --> 00:02:45,610
nodejs and recently go has been gaining

00:02:40,239 --> 00:02:48,070
a lot of speed our storage or like

00:02:45,610 --> 00:02:52,030
further back end services include Kafka

00:02:48,070 --> 00:02:56,519
for logging radius for caching cassandra

00:02:52,030 --> 00:03:00,489
for a key value storage basically and

00:02:56,519 --> 00:03:02,140
hadoop for data analytics and the list

00:03:00,489 --> 00:03:04,480
goes on and on we use open source

00:03:02,140 --> 00:03:07,500
technologies everywhere but we are not

00:03:04,480 --> 00:03:10,930
only using them we are also contributing

00:03:07,500 --> 00:03:13,060
every project every service that is

00:03:10,930 --> 00:03:16,750
created at uber is considered for open

00:03:13,060 --> 00:03:18,549
sourcing of course that can can happen

00:03:16,750 --> 00:03:21,010
every time you know there are some like

00:03:18,549 --> 00:03:24,340
business critical things that must be

00:03:21,010 --> 00:03:26,500
kept secret sometimes it doesn't make a

00:03:24,340 --> 00:03:29,709
lot of sense to open source something

00:03:26,500 --> 00:03:33,820
but if if we can do it v.v open source

00:03:29,709 --> 00:03:38,290
things and we have about well more than

00:03:33,820 --> 00:03:42,640
eight eighty original public github

00:03:38,290 --> 00:03:46,720
repositories also more than 40 forked

00:03:42,640 --> 00:03:49,320
repositories and our our software

00:03:46,720 --> 00:03:53,200
engineers are constantly contributing to

00:03:49,320 --> 00:03:56,140
two projects in different ecosystems i

00:03:53,200 --> 00:03:58,840
would say mostly in in ojs at this point

00:03:56,140 --> 00:04:05,070
but like contributions from our people

00:03:58,840 --> 00:04:05,070
are ya very often our people contribute

00:04:05,910 --> 00:04:12,170
ok so

00:04:09,710 --> 00:04:14,550
this graph shows you how many

00:04:12,170 --> 00:04:20,640
microservices there are at goober this

00:04:14,550 --> 00:04:22,800
is like historical graph so to eat two

00:04:20,640 --> 00:04:25,700
years ago there were no micro cell vs

00:04:22,800 --> 00:04:29,640
everything was served by a monolithic

00:04:25,700 --> 00:04:32,340
API service and but since then since

00:04:29,640 --> 00:04:34,200
then with like the number of micro

00:04:32,340 --> 00:04:37,620
celebrity sex has exploded we now have

00:04:34,200 --> 00:04:41,760
over 700 microservices running at Bieber

00:04:37,620 --> 00:04:45,480
and even keeping them running you know

00:04:41,760 --> 00:04:47,490
it's a challenging thing and so I'm

00:04:45,480 --> 00:04:50,460
going to talk about three things that

00:04:47,490 --> 00:04:53,730
support this micro service-oriented

00:04:50,460 --> 00:04:55,680
architecture the first ring pop which is

00:04:53,730 --> 00:04:58,230
scalable fault tolerant

00:04:55,680 --> 00:05:01,110
application-level sharding you'll see

00:04:58,230 --> 00:05:05,430
what it is in a minute t channel high

00:05:01,110 --> 00:05:08,580
performance RPC and hyper bond service

00:05:05,430 --> 00:05:10,590
discovery and routing for our large

00:05:08,580 --> 00:05:16,770
scares large-scale service oriented

00:05:10,590 --> 00:05:19,530
architecture operations alright so ring

00:05:16,770 --> 00:05:24,690
pop ring pop is actually the project

00:05:19,530 --> 00:05:27,060
that we are developing in Amsterdam so

00:05:24,690 --> 00:05:30,300
ring tompkins consists of two main

00:05:27,060 --> 00:05:32,510
things it's it's a consistent hashing

00:05:30,300 --> 00:05:35,580
ring and a membership protocol

00:05:32,510 --> 00:05:40,050
consistent hashing allows us to to

00:05:35,580 --> 00:05:43,980
consistently how chart requests to to

00:05:40,050 --> 00:05:49,650
the workers that actually do do the

00:05:43,980 --> 00:05:52,320
business logic so imagine you have a you

00:05:49,650 --> 00:05:55,230
have a trip that that that i started the

00:05:52,320 --> 00:05:56,820
tribez and ID and because the trip is

00:05:55,230 --> 00:05:58,980
full of state you know and you want

00:05:56,820 --> 00:06:01,260
you're doing some caching you want the

00:05:58,980 --> 00:06:04,530
cash to be fresh so you want that one

00:06:01,260 --> 00:06:06,150
request be routed to the to the one host

00:06:04,530 --> 00:06:10,290
to the one instance and that one

00:06:06,150 --> 00:06:11,669
instance is is going to handle the the

00:06:10,290 --> 00:06:13,620
whole trip and you can imagine that the

00:06:11,669 --> 00:06:16,560
supply service for example or the demand

00:06:13,620 --> 00:06:18,750
service that we have are being served on

00:06:16,560 --> 00:06:20,789
hundreds of hosts there are hundreds of

00:06:18,750 --> 00:06:23,039
instances and we need to you need to

00:06:20,789 --> 00:06:23,520
route this request consistently to one

00:06:23,039 --> 00:06:25,530
machine that

00:06:23,520 --> 00:06:27,840
is going to handle that one trip and

00:06:25,530 --> 00:06:31,710
then you know it can it can do other

00:06:27,840 --> 00:06:34,560
things and if of course we want to be

00:06:31,710 --> 00:06:38,060
available so if that machine fails we

00:06:34,560 --> 00:06:40,800
want this failure to be non-disruptive

00:06:38,060 --> 00:06:42,840
we won the trip to to still continue

00:06:40,800 --> 00:06:47,069
right if the machine crashes we cannot

00:06:42,840 --> 00:06:51,020
afford to lose the trip and a membership

00:06:47,069 --> 00:06:55,560
protocol membership protocol gives us

00:06:51,020 --> 00:06:57,900
fall detection in a decentralized way in

00:06:55,560 --> 00:07:01,020
a scalable way so we will see how these

00:06:57,900 --> 00:07:03,720
two things come together so I will dig

00:07:01,020 --> 00:07:09,810
dig a little deep into the underlying

00:07:03,720 --> 00:07:12,979
technology so hashing ring is actually

00:07:09,810 --> 00:07:21,539
like a like a continuous or like

00:07:12,979 --> 00:07:23,940
basically a 4-byte integer so the first

00:07:21,539 --> 00:07:26,610
thing we need to do we are we are

00:07:23,940 --> 00:07:29,520
basically placing workers on on this

00:07:26,610 --> 00:07:32,699
hashing ring and or instances as we call

00:07:29,520 --> 00:07:37,919
them and then as the requests coming we

00:07:32,699 --> 00:07:40,289
route them to these instances based on

00:07:37,919 --> 00:07:44,340
based on hashing so first thing we need

00:07:40,289 --> 00:07:47,300
to do we need to place these these

00:07:44,340 --> 00:07:51,210
instances on the hashing ring so here

00:07:47,300 --> 00:07:53,759
these via three instances a B and C and

00:07:51,210 --> 00:07:55,110
we've run them through a hashing

00:07:53,759 --> 00:07:59,130
function that is going to be just

00:07:55,110 --> 00:08:01,529
returned a 4-byte integer and that

00:07:59,130 --> 00:08:08,460
places places them on on the hashing

00:08:01,529 --> 00:08:13,050
ring this this determines the key space

00:08:08,460 --> 00:08:15,719
division so every instance is

00:08:13,050 --> 00:08:17,880
responsible for for the keys for dashing

00:08:15,719 --> 00:08:23,310
keys that our counter clockwise until

00:08:17,880 --> 00:08:26,060
the next next instance so in this case

00:08:23,310 --> 00:08:29,159
instance B is is responsible for the

00:08:26,060 --> 00:08:31,319
left half of the key space and instant

00:08:29,159 --> 00:08:34,130
is B and C have split the right bar

00:08:31,319 --> 00:08:36,089
right path right part of the key space

00:08:34,130 --> 00:08:39,060
in reality it's a little more

00:08:36,089 --> 00:08:40,800
complicated than this it it actually

00:08:39,060 --> 00:08:43,860
never happens that like this would be it

00:08:40,800 --> 00:08:46,709
is imbalanced but for the sake of

00:08:43,860 --> 00:08:49,320
simplicity let's let's assume it's kind

00:08:46,709 --> 00:08:55,860
of this balance but it in a rarity it's

00:08:49,320 --> 00:08:58,529
not okay and now if you want to if we

00:08:55,860 --> 00:09:00,870
want to determine which instance is

00:08:58,529 --> 00:09:03,510
responsible for a given user for example

00:09:00,870 --> 00:09:06,890
or given trip we hash the user or the

00:09:03,510 --> 00:09:10,529
trip and it falls somewhere on the ring

00:09:06,890 --> 00:09:14,790
in this case users 1 and 5 a fallen onto

00:09:10,529 --> 00:09:19,589
our our being will be handled by by

00:09:14,790 --> 00:09:21,329
instance be and the same way users user

00:09:19,589 --> 00:09:23,250
a it is going to be handled by instant

00:09:21,329 --> 00:09:29,550
see and user for is going to be handled

00:09:23,250 --> 00:09:33,089
by instance a and now let's say instant

00:09:29,550 --> 00:09:36,180
c has has caught fire you know like it's

00:09:33,089 --> 00:09:39,029
down somebody has cut the cable and we

00:09:36,180 --> 00:09:42,449
need this this thing to cause as few

00:09:39,029 --> 00:09:44,430
disruptions as possible so our

00:09:42,449 --> 00:09:47,040
membership protocol determines that that

00:09:44,430 --> 00:09:50,970
instant c is down it removes it from the

00:09:47,040 --> 00:09:55,709
ring and now instance a is responsible

00:09:50,970 --> 00:09:59,399
also for user user 8 because he is no

00:09:55,709 --> 00:10:03,149
longer part of the ring and but as you

00:09:59,399 --> 00:10:06,500
can see users one and five are still

00:10:03,149 --> 00:10:12,360
being handled by instance be so unless

00:10:06,500 --> 00:10:14,279
the instance goes down no the request is

00:10:12,360 --> 00:10:18,350
going to be routed to the same instance

00:10:14,279 --> 00:10:21,269
every time and in the other case if we

00:10:18,350 --> 00:10:24,630
put another instance in during in this

00:10:21,269 --> 00:10:27,779
case d it's going to take over some of a

00:10:24,630 --> 00:10:31,050
ski space and user for it's now being

00:10:27,779 --> 00:10:33,660
handled by instance d so the important

00:10:31,050 --> 00:10:37,350
thing to remember again unless something

00:10:33,660 --> 00:10:41,160
goes wrong the request is always being

00:10:37,350 --> 00:10:43,680
handled by one instance from start to

00:10:41,160 --> 00:10:47,120
beginning a super from from beginning to

00:10:43,680 --> 00:10:47,120
the end all right

00:10:47,630 --> 00:10:55,590
and now how we how we do the membership

00:10:51,090 --> 00:10:59,040
protocol so our membership protocol is

00:10:55,590 --> 00:11:03,420
based on swim if you've never heard of

00:10:59,040 --> 00:11:06,630
it go read the spin paper it's it's an

00:11:03,420 --> 00:11:09,540
excellent paper i really recommend it

00:11:06,630 --> 00:11:16,140
it's it really yeah it's a great read do

00:11:09,540 --> 00:11:18,089
it so this is this is very assuming we

00:11:16,140 --> 00:11:20,760
have three instances and we somehow want

00:11:18,089 --> 00:11:25,020
to keep the membership and we want to

00:11:20,760 --> 00:11:31,320
keep it in a decentralized scalable way

00:11:25,020 --> 00:11:36,360
and swim allows us to do this so in the

00:11:31,320 --> 00:11:38,390
steady state these instances cycle

00:11:36,360 --> 00:11:43,529
through their membership lists and

00:11:38,390 --> 00:11:47,820
randomly in given intervals being their

00:11:43,529 --> 00:11:49,910
neighbor neighbors and if nothing

00:11:47,820 --> 00:11:54,320
happens they just happily being and

00:11:49,910 --> 00:12:00,270
everything is fine you know but if

00:11:54,320 --> 00:12:03,210
instance be goes down and instance a

00:12:00,270 --> 00:12:09,480
decides like tries to ping it it doesn't

00:12:03,210 --> 00:12:11,700
get a response back so but we don't know

00:12:09,480 --> 00:12:13,770
you know like sometimes packets get get

00:12:11,700 --> 00:12:15,660
lost on the network it might be just a

00:12:13,770 --> 00:12:18,150
temporary thing you know we don't want

00:12:15,660 --> 00:12:20,940
to jump to conclusion that instance be

00:12:18,150 --> 00:12:24,740
is down so we want to be you want to be

00:12:20,940 --> 00:12:28,320
sure so what happens is that instance a

00:12:24,740 --> 00:12:30,810
asks instance asks another instance in

00:12:28,320 --> 00:12:34,200
this instant in this case instant see to

00:12:30,810 --> 00:12:36,270
do what is called an indirect ping it

00:12:34,200 --> 00:12:40,830
sends a ping request to instance see and

00:12:36,270 --> 00:12:47,520
instant see Pink's instance be on ace

00:12:40,830 --> 00:12:49,830
behalf and so this this is for the case

00:12:47,520 --> 00:12:52,560
where you know like link between a and B

00:12:49,830 --> 00:12:55,260
has been broken but still the link

00:12:52,560 --> 00:12:58,860
between C and B is fine so we don't want

00:12:55,260 --> 00:13:00,400
to mark be as faulty because it's still

00:12:58,860 --> 00:13:02,380
there just just a

00:13:00,400 --> 00:13:09,490
cannot reach be but that's not not a not

00:13:02,380 --> 00:13:12,550
a big problem so the indirect thing yeah

00:13:09,490 --> 00:13:18,670
the indirect being fails and now a

00:13:12,550 --> 00:13:20,680
declares be a suspect that means it's

00:13:18,670 --> 00:13:22,240
not yet declared faulty you know it

00:13:20,680 --> 00:13:23,860
could be just a temporary failure we

00:13:22,240 --> 00:13:25,960
don't want to be causing many

00:13:23,860 --> 00:13:28,510
destruction and unless that disruptions

00:13:25,960 --> 00:13:31,090
unless it's really necessary so it's

00:13:28,510 --> 00:13:37,710
just been declared suspect and but after

00:13:31,090 --> 00:13:41,110
some time yeah yeah forgot something so

00:13:37,710 --> 00:13:43,420
what's important here is that this

00:13:41,110 --> 00:13:48,690
protocol is based on what we call

00:13:43,420 --> 00:13:51,520
gossiping that is in this indistinct we

00:13:48,690 --> 00:13:54,820
gossip or like we piggyback what we call

00:13:51,520 --> 00:13:57,960
it information about the membership to

00:13:54,820 --> 00:14:02,350
other instances so the next time a

00:13:57,960 --> 00:14:04,180
pink's see it also the ping carries the

00:14:02,350 --> 00:14:11,320
information that be has been declared

00:14:04,180 --> 00:14:13,480
suspect and so in the next ping the

00:14:11,320 --> 00:14:16,230
information that that be is declared

00:14:13,480 --> 00:14:18,910
suspect goes to goes to instant see and

00:14:16,230 --> 00:14:21,610
after some after a little while you know

00:14:18,910 --> 00:14:23,230
when be doesn't doesn't respond back

00:14:21,610 --> 00:14:33,130
it's declared this actually declared

00:14:23,230 --> 00:14:35,800
faulty so the the best thing about this

00:14:33,130 --> 00:14:40,210
approach is that it's it's it's very

00:14:35,800 --> 00:14:42,610
scalable think it's doing what we call

00:14:40,210 --> 00:14:45,880
infection style dissemination it it

00:14:42,610 --> 00:14:47,770
doesn't it goes randomly it randomly

00:14:45,880 --> 00:14:51,910
disseminate information about membership

00:14:47,770 --> 00:14:55,590
and but this this this this random

00:14:51,910 --> 00:14:59,620
infection style dissemination eventually

00:14:55,590 --> 00:15:01,900
gets to all the notes but the beautiful

00:14:59,620 --> 00:15:04,690
thing about this is that the traffic is

00:15:01,900 --> 00:15:07,380
constant per note so that when we grow

00:15:04,690 --> 00:15:07,380
the cluster

00:15:09,699 --> 00:15:16,449
notes don't need to don't need to keep

00:15:13,720 --> 00:15:18,850
connections to every other node there is

00:15:16,449 --> 00:15:21,459
they just they just they just they are

00:15:18,850 --> 00:15:24,699
just pinging some you know some subset

00:15:21,459 --> 00:15:30,279
like in constant intervals so it's very

00:15:24,699 --> 00:15:32,829
scalable and we have we have we have

00:15:30,279 --> 00:15:35,889
clusters with 1000 instances in

00:15:32,829 --> 00:15:39,419
production we have tested it to two

00:15:35,889 --> 00:15:41,829
thousand and five hundred instances and

00:15:39,419 --> 00:15:45,129
we just don't have enough hardware at

00:15:41,829 --> 00:15:47,829
this point to to to go further but we

00:15:45,129 --> 00:15:50,649
think is going to be fine up to up to

00:15:47,829 --> 00:15:54,040
10,000 instances of this running so

00:15:50,649 --> 00:15:57,160
that's that's that's that's something

00:15:54,040 --> 00:16:01,829
about the scale uber we have service

00:15:57,160 --> 00:16:05,199
that as 1000 instances in production

00:16:01,829 --> 00:16:08,529
okay so how does have that swing pop

00:16:05,199 --> 00:16:10,899
actually how does one use fring pop so

00:16:08,529 --> 00:16:16,689
ring pop is an application level meter

00:16:10,899 --> 00:16:19,809
middleware requests come come into the

00:16:16,689 --> 00:16:22,119
service Ring Pop based on the hashing

00:16:19,809 --> 00:16:24,609
ring and the membership decides whether

00:16:22,119 --> 00:16:29,139
it should handle the request or forward

00:16:24,609 --> 00:16:37,359
it to impart to some to some other

00:16:29,139 --> 00:16:39,399
instance and so you can imagine

00:16:37,359 --> 00:16:42,069
something like this this is just

00:16:39,399 --> 00:16:46,239
pseudocode and like look too closely at

00:16:42,069 --> 00:16:48,819
it so if you look at the diagram in the

00:16:46,239 --> 00:16:51,039
bottom you know these are our three

00:16:48,819 --> 00:16:54,069
instances a request for user one comes

00:16:51,039 --> 00:16:58,480
to instance a instance a is not the

00:16:54,069 --> 00:17:01,720
owner of of the of the of the you of the

00:16:58,480 --> 00:17:03,910
user based on the consistent hashing so

00:17:01,720 --> 00:17:05,980
it forwards it to buddy knows where it

00:17:03,910 --> 00:17:09,519
should go immediately so it forwards it

00:17:05,980 --> 00:17:14,110
to two instance c and instant see

00:17:09,519 --> 00:17:18,309
handles it and replies through instance

00:17:14,110 --> 00:17:20,799
a sense the response so this is what the

00:17:18,309 --> 00:17:22,280
code could could look like for some kind

00:17:20,799 --> 00:17:30,160
of business logic on

00:17:22,280 --> 00:17:32,660
on users yeah so you have seen that

00:17:30,160 --> 00:17:37,670
there's a lot of RPC happening you know

00:17:32,660 --> 00:17:41,690
many services many many things happening

00:17:37,670 --> 00:17:46,790
many many requests and responses flying

00:17:41,690 --> 00:17:53,270
or responses Frank flying around and the

00:17:46,790 --> 00:17:56,720
f we have we used to use HTTP but there

00:17:53,270 --> 00:18:01,460
were many issues with it https complex

00:17:56,720 --> 00:18:06,730
protocol it's kind of slow performing in

00:18:01,460 --> 00:18:11,060
some cases so we came up with this hour

00:18:06,730 --> 00:18:14,950
the yeah there was a need to to make

00:18:11,060 --> 00:18:22,070
something something more reliable faster

00:18:14,950 --> 00:18:24,170
and so he created ditional t channel was

00:18:22,070 --> 00:18:27,080
created with the service-oriented

00:18:24,170 --> 00:18:29,480
architecture in mind from from the start

00:18:27,080 --> 00:18:34,220
so you are not calling hosts you are

00:18:29,480 --> 00:18:37,220
calling services we can do we wanted to

00:18:34,220 --> 00:18:39,800
be able to trace any requests requests

00:18:37,220 --> 00:18:42,830
in our service oriented architecture so

00:18:39,800 --> 00:18:45,800
that we can do we can / we can do

00:18:42,830 --> 00:18:52,040
diagnostics we can do performance

00:18:45,800 --> 00:18:54,050
testing and tracing of the requests we

00:18:52,040 --> 00:18:57,830
want it to be easy to implement in the

00:18:54,050 --> 00:19:00,170
languages that we use also we wanted it

00:18:57,830 --> 00:19:02,450
to support multiplexing that is you can

00:19:00,170 --> 00:19:06,100
you can have one connection open and

00:19:02,450 --> 00:19:08,810
send many requests you don't have to

00:19:06,100 --> 00:19:12,290
wait for the response as you as you

00:19:08,810 --> 00:19:14,000
would need with HTTP HTTP you can work

00:19:12,290 --> 00:19:16,730
around it by using multiple connections

00:19:14,000 --> 00:19:21,680
but then you're using more connections

00:19:16,730 --> 00:19:26,140
and it's not scalable we wanted it to

00:19:21,680 --> 00:19:26,140
support arbitrary several serialization

00:19:29,680 --> 00:19:36,169
they still use a lot of Jason but

00:19:33,370 --> 00:19:39,860
the probability of Jason is that is it

00:19:36,169 --> 00:19:42,830
slow to parse and produce we have

00:19:39,860 --> 00:19:46,400
founded our many of our services where

00:19:42,830 --> 00:19:48,950
they're basically doing nothing all day

00:19:46,400 --> 00:19:52,299
and that is like parsing and and

00:19:48,950 --> 00:19:55,720
producing Jason so so we actually

00:19:52,299 --> 00:20:03,740
started using thrift which is a

00:19:55,720 --> 00:20:05,290
different serialization approach and we

00:20:03,740 --> 00:20:08,059
wanted of course it to support

00:20:05,290 --> 00:20:10,070
high-performance forwarding because

00:20:08,059 --> 00:20:14,360
thats that's what we do what do you do a

00:20:10,070 --> 00:20:19,429
lot as you have seen so t channel is a

00:20:14,360 --> 00:20:23,780
binary protocol and it is binary because

00:20:19,429 --> 00:20:26,390
based we can be envied now fixed there

00:20:23,780 --> 00:20:28,220
are fixed points in its in the t channel

00:20:26,390 --> 00:20:30,410
header that we can access directly

00:20:28,220 --> 00:20:32,240
without needing of an a difficult or

00:20:30,410 --> 00:20:35,270
complicated parsing and based on these

00:20:32,240 --> 00:20:37,640
headers in fixed positions in the byte

00:20:35,270 --> 00:20:39,530
stream we can determine the the

00:20:37,640 --> 00:20:41,929
forwarding we don't need to parse the

00:20:39,530 --> 00:20:44,450
whole request it requests in any way so

00:20:41,929 --> 00:20:49,910
that allows that allows the very fast

00:20:44,450 --> 00:20:52,160
forwarding and yeah there's a there's a

00:20:49,910 --> 00:20:54,559
request ID which actually allows us to

00:20:52,160 --> 00:20:56,720
do do the tracing i described we can

00:20:54,559 --> 00:21:00,980
just trace any request by its idea

00:20:56,720 --> 00:21:03,290
throughout the whole network yeah so

00:21:00,980 --> 00:21:06,049
again t channel is open source we have

00:21:03,290 --> 00:21:09,679
four implementations in node.js python

00:21:06,049 --> 00:21:14,090
go java and we've also built tools to

00:21:09,679 --> 00:21:18,110
support it tickle does yeah it is just

00:21:14,090 --> 00:21:24,410
curl 40 channel and TCAP allows us to to

00:21:18,110 --> 00:21:30,260
trace and diagnose t channel requests as

00:21:24,410 --> 00:21:32,600
they fly through the network yeah and of

00:21:30,260 --> 00:21:35,360
course if you have a large service

00:21:32,600 --> 00:21:38,600
oriented architecture operation you need

00:21:35,360 --> 00:21:41,090
some way to to do service discovery and

00:21:38,600 --> 00:21:42,950
routing on this large scale so that it's

00:21:41,090 --> 00:21:45,590
one thing to have one service that is

00:21:42,950 --> 00:21:46,610
like nicely working and it's reliable

00:21:45,590 --> 00:21:48,110
and it uses

00:21:46,610 --> 00:21:50,990
initiating and everything is working

00:21:48,110 --> 00:21:53,720
fine but you need a way to to

00:21:50,990 --> 00:21:56,920
communicate between these these services

00:21:53,720 --> 00:22:01,900
so that's where hyper brawn comes in

00:21:56,920 --> 00:22:01,900
hyper bond gives us service discovery

00:22:02,260 --> 00:22:09,770
requests forwarding it clients have to

00:22:06,290 --> 00:22:12,830
do almost zero configuration you just

00:22:09,770 --> 00:22:16,370
need to give it one hyper bound instance

00:22:12,830 --> 00:22:18,020
and it will bootstrap automatically IP

00:22:16,370 --> 00:22:24,410
address of one hyper bound instance in

00:22:18,020 --> 00:22:26,929
the booth automatically and also hyper

00:22:24,410 --> 00:22:29,000
bond allows us to do circuit breaking

00:22:26,929 --> 00:22:33,200
that means if one service is misbehaving

00:22:29,000 --> 00:22:36,250
it doesn't respond we can we can cut

00:22:33,200 --> 00:22:38,650
traffic to it so that it doesn't disrupt

00:22:36,250 --> 00:22:42,350
services that are downstream to it

00:22:38,650 --> 00:22:45,380
upstream and also allows us to rate

00:22:42,350 --> 00:22:49,010
limiting if again one service is

00:22:45,380 --> 00:22:51,770
misbehaving is producing many requests

00:22:49,010 --> 00:22:54,100
we want to limit it dynamically so that

00:22:51,770 --> 00:23:02,049
again it doesn't disrupt more of our

00:22:54,100 --> 00:23:05,510
architecture so ring pop hyper ban is is

00:23:02,049 --> 00:23:10,850
actually based on ring pop the hyper ban

00:23:05,510 --> 00:23:14,600
routers form a ring and services connect

00:23:10,850 --> 00:23:17,299
to this ring services do not connect to

00:23:14,600 --> 00:23:19,700
every note in the hyper boundaries they

00:23:17,299 --> 00:23:27,530
just have a subset of them which we call

00:23:19,700 --> 00:23:29,900
an affinity group and so so you can

00:23:27,530 --> 00:23:33,169
imagine service when the service starts

00:23:29,900 --> 00:23:37,100
it contacts one of the hyperbole nodes

00:23:33,169 --> 00:23:38,570
and the hyper brown note tells it where

00:23:37,100 --> 00:23:41,660
it should connect where its affinity

00:23:38,570 --> 00:23:47,210
group is and it connects there and then

00:23:41,660 --> 00:23:52,549
it's it's ready to do to to respond to

00:23:47,210 --> 00:23:57,380
requests so this is what a request flow

00:23:52,549 --> 00:23:59,740
could look like in hyper bond so service

00:23:57,380 --> 00:24:04,420
a connects to

00:23:59,740 --> 00:24:06,490
hyper ban hyper ban em and basically

00:24:04,420 --> 00:24:08,320
service a bonds to once wants to do a

00:24:06,490 --> 00:24:11,590
request to service be yeah that's des

00:24:08,320 --> 00:24:15,040
ago so service a sense sends the request

00:24:11,590 --> 00:24:18,670
to hyperbole hyper ban this determines

00:24:15,040 --> 00:24:22,330
where the bear service bees affinity

00:24:18,670 --> 00:24:25,210
group is forward to request to do one of

00:24:22,330 --> 00:24:27,880
their routers in in the Beast affinity

00:24:25,210 --> 00:24:31,630
group and then forward say to service be

00:24:27,880 --> 00:24:32,890
serviced be again determines where where

00:24:31,630 --> 00:24:35,020
it should actually handle the request

00:24:32,890 --> 00:24:37,510
forwards it to the correct instance and

00:24:35,020 --> 00:24:38,590
then the whole thing goes back so you

00:24:37,510 --> 00:24:41,710
can see there's a lot of routing

00:24:38,590 --> 00:24:44,050
happening and it's it's an forwarding

00:24:41,710 --> 00:24:46,470
happening and that's only things to

00:24:44,050 --> 00:24:55,179
teach channel that we can do this in a

00:24:46,470 --> 00:24:59,070
high-performance way so conclusion uber

00:24:55,179 --> 00:25:02,080
is both open source user and contributor

00:24:59,070 --> 00:25:04,809
we have large service-oriented

00:25:02,080 --> 00:25:07,600
architecture based on our open source

00:25:04,809 --> 00:25:09,610
projects you can just there's good

00:25:07,600 --> 00:25:14,920
documentation you can really go to

00:25:09,610 --> 00:25:16,900
github take a look play with it these

00:25:14,920 --> 00:25:20,190
projects are ring pop digital and hyper

00:25:16,900 --> 00:25:23,050
ban there's plenty plenty more there and

00:25:20,190 --> 00:25:25,300
we still but we still have a lot of

00:25:23,050 --> 00:25:31,210
interesting challenges ahead you know

00:25:25,300 --> 00:25:33,190
we've been around for like four or five

00:25:31,210 --> 00:25:37,179
years something like that our

00:25:33,190 --> 00:25:39,940
architecture is still evolving and here

00:25:37,179 --> 00:25:43,000
so things will change things do break

00:25:39,940 --> 00:25:50,620
but a lot of interesting stuff is

00:25:43,000 --> 00:25:54,040
happening so thank you yeah very tiring

00:25:50,620 --> 00:25:58,630
if you want to work on this and more

00:25:54,040 --> 00:26:02,770
cool stuff get in touch with me also if

00:25:58,630 --> 00:26:04,630
you want to if you want to take an uber

00:26:02,770 --> 00:26:08,260
you still don't have an account you can

00:26:04,630 --> 00:26:12,960
use a promo code mark for you to get 250

00:26:08,260 --> 00:26:16,750
cards of your first ride yeah so

00:26:12,960 --> 00:26:20,880
that's all from me I'm ready to answer

00:26:16,750 --> 00:26:44,940
any questions you may have all right

00:26:20,880 --> 00:26:50,500
yeah I think you were first yeah yeah

00:26:44,940 --> 00:26:52,960
okay yeah the first question was do we

00:26:50,500 --> 00:27:03,490
use some kind of raft or Pyxis

00:26:52,960 --> 00:27:08,529
implementation answer is answer is kinda

00:27:03,490 --> 00:27:11,200
like we use Cassandra and we used react

00:27:08,529 --> 00:27:14,169
which are both based on these things but

00:27:11,200 --> 00:27:20,110
we don't have anything in house based on

00:27:14,169 --> 00:27:21,759
on these two so kinda and the second

00:27:20,110 --> 00:27:23,799
question was we collect a lot of data

00:27:21,759 --> 00:27:25,720
about bar what are we doing the data are

00:27:23,799 --> 00:27:28,570
we selling it or not so we are

00:27:25,720 --> 00:27:34,120
definitely not selling to data as you

00:27:28,570 --> 00:27:36,789
like actually yeah we have a big head

00:27:34,120 --> 00:27:40,600
loop operation you're a la analyzing a

00:27:36,789 --> 00:27:43,169
lot of data but we are definitely not

00:27:40,600 --> 00:27:43,169
selling the data

00:27:46,500 --> 00:27:57,280
so you can imagine that okay I'm not

00:27:55,120 --> 00:28:08,500
sure I can actually like talk about it a

00:27:57,280 --> 00:28:10,930
lot but part yeah like there's there's a

00:28:08,500 --> 00:28:12,910
lot of space for for optimization in

00:28:10,930 --> 00:28:18,760
routing for example you know like of the

00:28:12,910 --> 00:28:22,990
cars and yeah that's that's about

00:28:18,760 --> 00:28:34,080
everything I can probably say here yeah

00:28:22,990 --> 00:28:34,080
I'm sorry oh yeah I think yeah yeah yeah

00:28:37,800 --> 00:28:46,470
yeah yeah so so we work with you you IDs

00:28:42,510 --> 00:28:49,900
throughout yeah sure the question was

00:28:46,470 --> 00:28:52,120
how do we determine the identity of the

00:28:49,900 --> 00:28:54,990
user basically yes so we don't use user

00:28:52,120 --> 00:29:01,030
names we every user is associated with

00:28:54,990 --> 00:29:07,410
uuid so we use these new ideas as the

00:29:01,030 --> 00:29:07,410
source for the hashing right yeah

00:29:09,500 --> 00:29:16,620
yeah question was about security and

00:29:12,840 --> 00:29:20,390
encryption 50 channel this is all

00:29:16,620 --> 00:29:24,480
happening internally we don't do any

00:29:20,390 --> 00:29:28,440
encryption on the application layer I

00:29:24,480 --> 00:29:31,289
believe there are there there are ipsec

00:29:28,440 --> 00:29:37,350
things happening you know like in the

00:29:31,289 --> 00:29:39,000
lower layers of the network but yeah you

00:29:37,350 --> 00:29:42,000
wouldn't use t channel to connect from

00:29:39,000 --> 00:29:44,460
your cell phone to to to our

00:29:42,000 --> 00:29:48,600
infrastructure we use we still use HTTP

00:29:44,460 --> 00:30:14,730
and rest based approach to get to the

00:29:48,600 --> 00:30:17,580
end device yeah yeah so good question

00:30:14,730 --> 00:30:20,279
was what happens if the die if death if

00:30:17,580 --> 00:30:22,140
the node actually dies in bit in the

00:30:20,279 --> 00:30:26,940
middle of the trip how do we like load

00:30:22,140 --> 00:30:33,120
the state so there's a couple of things

00:30:26,940 --> 00:30:35,640
we do one we use vuze readies for

00:30:33,120 --> 00:30:39,270
caching so you would stay the transient

00:30:35,640 --> 00:30:45,059
state in in Redis as one place or we

00:30:39,270 --> 00:30:47,700
have a couple of storage services based

00:30:45,059 --> 00:30:50,730
on Cassandra based on react that where

00:30:47,700 --> 00:30:59,029
we store the data so that it can be

00:30:50,730 --> 00:31:02,220
loaded in case the note dies yeah yeah

00:30:59,029 --> 00:31:06,809
yes yes so the advantage there is that

00:31:02,220 --> 00:31:08,549
like your your caches are fresh and also

00:31:06,809 --> 00:31:13,700
there are there are other services with

00:31:08,549 --> 00:31:16,380
which only store really transient data

00:31:13,700 --> 00:31:21,090
that get that get refreshed every couple

00:31:16,380 --> 00:31:21,840
of seconds so if if the the note if one

00:31:21,090 --> 00:31:24,029
of the instances

00:31:21,840 --> 00:31:29,279
dies the data gets gets refreshed from

00:31:24,029 --> 00:32:22,740
from the source very quickly so or

00:31:29,279 --> 00:32:26,669
something like that what so the question

00:32:22,740 --> 00:32:29,909
was is there any guarantee that the

00:32:26,669 --> 00:32:39,630
requests get where they are supposed to

00:32:29,909 --> 00:32:42,260
go the answer is no you know like you

00:32:39,630 --> 00:32:45,779
can you can always lose the packets

00:32:42,260 --> 00:32:49,890
somewhere in the network but what you

00:32:45,779 --> 00:32:54,149
can do is do you can do speculative

00:32:49,890 --> 00:32:57,840
execution that is you actually ask two

00:32:54,149 --> 00:33:01,740
instances for some some requests and you

00:32:57,840 --> 00:33:03,570
use the first response that comes in so

00:33:01,740 --> 00:33:05,340
that if one of the packets is some some

00:33:03,570 --> 00:33:09,539
lost somewhere you still get the second

00:33:05,340 --> 00:33:11,250
one you can also wait for your responses

00:33:09,539 --> 00:33:13,890
if it doesn't doesn't come in you repeat

00:33:11,250 --> 00:33:18,929
it but then of course you are faced with

00:33:13,890 --> 00:33:21,000
some latency issues yeah so yeah there

00:33:18,929 --> 00:33:22,860
are ways to to work around this but of

00:33:21,000 --> 00:33:26,340
course if you know if a packet is lost

00:33:22,860 --> 00:33:27,899
somewhere in in mid like in the cable

00:33:26,340 --> 00:33:32,700
you know it get just get stuck in the

00:33:27,899 --> 00:33:37,690
cable so yeah you need

00:33:32,700 --> 00:33:41,560
yeah yeah so thanks thanks to thanks to

00:33:37,690 --> 00:33:46,420
the fact that we have request IDs we can

00:33:41,560 --> 00:33:48,640
we can we know that we have like we sent

00:33:46,420 --> 00:33:52,030
a request with an ID and we are waiting

00:33:48,640 --> 00:33:53,640
for a response with an ID so if the

00:33:52,030 --> 00:33:56,400
response doesn't come in we can safely

00:33:53,640 --> 00:33:58,990
resend the request because the

00:33:56,400 --> 00:34:00,730
destination service knows that it's

00:33:58,990 --> 00:34:03,460
already served it so it can like say

00:34:00,730 --> 00:34:06,100
send the same response again you know so

00:34:03,460 --> 00:34:09,880
we can guarantee that it's going to

00:34:06,100 --> 00:34:12,910
handle the request only once and as well

00:34:09,880 --> 00:34:15,340
we can we can we can guarantee that the

00:34:12,910 --> 00:34:19,440
source service is going to handle the

00:34:15,340 --> 00:34:22,440
response only ones thanks to the IDs

00:34:19,440 --> 00:34:22,440
yeah

00:34:35,899 --> 00:34:50,339
so ya well what what used to happen is

00:34:43,710 --> 00:34:53,190
that devices would basically ping our

00:34:50,339 --> 00:34:56,879
data center every couple of seconds and

00:34:53,190 --> 00:34:59,400
they would get the whole state of the

00:34:56,879 --> 00:35:01,589
application back and it will just

00:34:59,400 --> 00:35:04,680
display the state but we are we are kind

00:35:01,589 --> 00:35:07,920
of moving away from this approach so the

00:35:04,680 --> 00:35:10,849
mobile application knows what what data

00:35:07,920 --> 00:35:13,499
it should request and it just asks the

00:35:10,849 --> 00:35:15,839
our back-end services for the data and

00:35:13,499 --> 00:35:17,789
if the connection gets broken your

00:35:15,839 --> 00:35:19,499
application bond update for a couple of

00:35:17,789 --> 00:35:22,200
seconds but when the collection

00:35:19,499 --> 00:35:30,619
connection comes back and we'll just

00:35:22,200 --> 00:35:37,200
load the data that is necessary so so

00:35:30,619 --> 00:35:40,289
there well I don't think there's a big

00:35:37,200 --> 00:35:42,960
room for conflict really the application

00:35:40,289 --> 00:35:48,480
is fairly simple actually so you just

00:35:42,960 --> 00:35:53,089
when you make a request to to to get a

00:35:48,480 --> 00:35:55,890
car you you just like you know commit

00:35:53,089 --> 00:35:57,420
with pressing the button the request a

00:35:55,890 --> 00:36:00,450
drug gets to the server or it doesn't

00:35:57,420 --> 00:36:02,279
and if it gets there it's get it gets

00:36:00,450 --> 00:36:06,029
processed on the server and you will

00:36:02,279 --> 00:36:08,970
never like you wouldn't send the request

00:36:06,029 --> 00:36:10,950
twice so you know like there's I don't

00:36:08,970 --> 00:36:15,170
feel I don't think there's a lot of room

00:36:10,950 --> 00:36:15,170
for conflicts actually yeah

00:36:25,500 --> 00:36:33,760
java second question is a few other

00:36:29,110 --> 00:36:35,970
times making it stable because support

00:36:33,760 --> 00:36:35,970
role

00:36:39,780 --> 00:36:45,250
yes so the first question is hyper bond

00:36:42,550 --> 00:36:53,110
is written in nodejs why why didn't we

00:36:45,250 --> 00:36:57,370
choose go basically no Jess at that time

00:36:53,110 --> 00:37:01,030
was the language to to write things in a

00:36:57,370 --> 00:37:05,470
tuber there were the software engineers

00:37:01,030 --> 00:37:10,080
able to write things in nodejs so that's

00:37:05,470 --> 00:37:12,250
the basic reason and the second question

00:37:10,080 --> 00:37:19,440
yeah yeah did we have problems with

00:37:12,250 --> 00:37:19,440
nodejs yes so yes we we've had problems

00:37:20,550 --> 00:37:36,850
yeah that's why actually go is gaining a

00:37:23,770 --> 00:37:40,580
lot of speed at uber now we are

00:37:36,850 --> 00:37:43,220
considering all up

00:37:40,580 --> 00:37:47,620
so unfortunately we are really out of

00:37:43,220 --> 00:37:47,620
time so thanks mark for the great road

00:38:02,290 --> 00:38:05,290
okay

00:38:11,160 --> 00:38:15,930
do the bloating he's yeah

00:38:23,750 --> 00:38:26,750
funny

00:38:42,890 --> 00:38:51,240
so sorry Peter questioning the question

00:38:48,270 --> 00:38:55,609
goes by the dried out robot might even

00:38:51,240 --> 00:38:55,609

YouTube URL: https://www.youtube.com/watch?v=pUQQ8PbSPFY


