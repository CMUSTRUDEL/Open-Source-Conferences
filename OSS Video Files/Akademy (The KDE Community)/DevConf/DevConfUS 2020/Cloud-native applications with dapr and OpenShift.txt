Title: Cloud-native applications with dapr and OpenShift
Publication date: 2020-10-07
Playlist: DevConfUS 2020
Description: 
	Speaker: Ip Sam

Dapr is Microsoft's latest new open source project. Dapr is an event driven, portable distributed application runtime. It supports many programming languages and it is a natural fit for Kubernetes and OpenShift. Dapr provides consistency and portability via standard open APIs. Dapr's building blocks enable easy, event-driven, stateful, microservices development. Dapr handles state, resource bindings and pub/sub messaging, which enable event-driven, resilient architectures that scale. In this presentation, we will cover how engineering teams can leverage Dapr with OpenShift for the application development, writing high-performance, highly-scalable applications using OpenShift. We will start with a high level Dapr architecture overview. Then go through an example how to develop Dapr microservices, and look at the deployment process to get it up to OpenShift. We will use OpenShift to fine tune the performance.
Captions: 
	00:00:00,960 --> 00:00:04,560
uh we're going to start the pre-recorded

00:00:02,639 --> 00:00:05,520
talk which is entitled cloud native

00:00:04,560 --> 00:00:09,840
application

00:00:05,520 --> 00:00:12,799
with day pair and open shift by ipsa

00:00:09,840 --> 00:00:14,480
hello everybody uh welcome to defcons

00:00:12,799 --> 00:00:16,320
2020.

00:00:14,480 --> 00:00:18,400
we will talk about the cloud native

00:00:16,320 --> 00:00:21,760
application with

00:00:18,400 --> 00:00:21,760
the apr data

00:00:22,960 --> 00:00:27,519
uh what exactly is data label is the

00:00:26,640 --> 00:00:30,480
microsoft

00:00:27,519 --> 00:00:31,359
latest open source project it is an

00:00:30,480 --> 00:00:33,280
event-driven

00:00:31,359 --> 00:00:34,640
portable one-time for building

00:00:33,280 --> 00:00:37,840
distributed microsoft

00:00:34,640 --> 00:00:39,120
microservices for stainless and stateful

00:00:37,840 --> 00:00:42,480
application on the cloud

00:00:39,120 --> 00:00:44,320
and also on the edge it embeds the

00:00:42,480 --> 00:00:45,840
diversity of languages and development

00:00:44,320 --> 00:00:47,840
framework

00:00:45,840 --> 00:00:50,239
it's upon many programming languages and

00:00:47,840 --> 00:00:52,800
is a natural fit for kubernetes and

00:00:50,239 --> 00:00:52,800
openshift

00:00:53,360 --> 00:00:58,480
in the deeper framework we have the

00:00:56,879 --> 00:01:02,239
labor concept

00:00:58,480 --> 00:01:05,519
paper concept contain two fundamental

00:01:02,239 --> 00:01:07,200
concepts building blocks the building

00:01:05,519 --> 00:01:08,720
blocks implement distributed system

00:01:07,200 --> 00:01:11,360
capability

00:01:08,720 --> 00:01:12,720
they include for example publication and

00:01:11,360 --> 00:01:15,520
subscription

00:01:12,720 --> 00:01:16,640
day management resource binding and

00:01:15,520 --> 00:01:20,159
distributed

00:01:16,640 --> 00:01:23,360
tracing components encapsulate the

00:01:20,159 --> 00:01:25,840
implementation of a building block api

00:01:23,360 --> 00:01:26,560
so that would include staff postgres

00:01:25,840 --> 00:01:29,840
sequel

00:01:26,560 --> 00:01:32,880
mysql radius mongodb

00:01:29,840 --> 00:01:34,640
many of the components also is plugable

00:01:32,880 --> 00:01:37,840
so that you the implementation can be

00:01:34,640 --> 00:01:37,840
swapped out and swapping

00:01:38,479 --> 00:01:44,560
so inside a building block you have a

00:01:42,000 --> 00:01:46,079
building block api right so it expose a

00:01:44,560 --> 00:01:49,200
public api

00:01:46,079 --> 00:01:51,119
that you can call from your code using

00:01:49,200 --> 00:01:51,840
the component to implement the building

00:01:51,119 --> 00:01:54,640
block

00:01:51,840 --> 00:01:56,560
capability so each building block have

00:01:54,640 --> 00:01:59,920
multiple components as you see from the

00:01:56,560 --> 00:01:59,920
architecture diagram here

00:02:00,479 --> 00:02:05,119
so daver have the following building

00:02:03,680 --> 00:02:07,840
block components

00:02:05,119 --> 00:02:10,000
that were built in right for example

00:02:07,840 --> 00:02:14,160
surface-to-surface invocation

00:02:10,000 --> 00:02:17,599
day management publish and subscribe

00:02:14,160 --> 00:02:21,040
resource binding and trigger actor

00:02:17,599 --> 00:02:24,160
observability and secret

00:02:21,040 --> 00:02:25,599
so these are all built-in building

00:02:24,160 --> 00:02:27,200
blocks and you could also

00:02:25,599 --> 00:02:31,520
do your own customization building

00:02:27,200 --> 00:02:31,520
blocks uh depends on what you need

00:02:31,760 --> 00:02:38,160
the data architecture you can see

00:02:34,800 --> 00:02:40,400
at the application code at the top

00:02:38,160 --> 00:02:41,680
right you could support any any type of

00:02:40,400 --> 00:02:45,760
framework you could have a

00:02:41,680 --> 00:02:48,640
golang or nodejs python java

00:02:45,760 --> 00:02:49,920
ruby c-sharp those are all different

00:02:48,640 --> 00:02:51,280
type of application code that's

00:02:49,920 --> 00:02:53,519
supported

00:02:51,280 --> 00:02:58,959
then you talk to the middle tier data

00:02:53,519 --> 00:02:58,959
using some sort of http api or grpc api

00:02:59,120 --> 00:03:04,400
and then and then you talk interact with

00:03:01,760 --> 00:03:07,200
all these different components in data

00:03:04,400 --> 00:03:08,080
and then deeper will talk to the lower

00:03:07,200 --> 00:03:11,360
layer

00:03:08,080 --> 00:03:18,720
on the cloud surfaces using aws

00:03:11,360 --> 00:03:21,920
google cloud microsoft and so on

00:03:18,720 --> 00:03:22,800
um the component um basically is a

00:03:21,920 --> 00:03:26,080
functionality

00:03:22,800 --> 00:03:27,840
uh deliver as a component

00:03:26,080 --> 00:03:30,080
each component has a interface

00:03:27,840 --> 00:03:31,440
definition

00:03:30,080 --> 00:03:34,080
all these different components are

00:03:31,440 --> 00:03:38,319
plug-enabled so we can

00:03:34,080 --> 00:03:38,319
swap them in and swap them out easily

00:03:39,040 --> 00:03:44,080
the component type so we have seen

00:03:41,760 --> 00:03:47,440
earlier we have surface and location

00:03:44,080 --> 00:03:50,480
resource binding date management

00:03:47,440 --> 00:03:52,959
distributed tracing vision subscribe

00:03:50,480 --> 00:03:52,959
and act

00:03:55,680 --> 00:04:01,840
so table on open shift so

00:03:59,599 --> 00:04:03,360
what you need to do is to set up um the

00:04:01,840 --> 00:04:06,319
different um

00:04:03,360 --> 00:04:06,959
ports using the apus and each part

00:04:06,319 --> 00:04:10,480
contain

00:04:06,959 --> 00:04:11,200
a diaper container right so for example

00:04:10,480 --> 00:04:14,560
you have a

00:04:11,200 --> 00:04:15,680
day per actor on one port and then you

00:04:14,560 --> 00:04:18,880
have a data

00:04:15,680 --> 00:04:20,799
paper cycle injector in one part it

00:04:18,880 --> 00:04:24,560
operates in one part all these different

00:04:20,799 --> 00:04:28,160
paper part interact with the

00:04:24,560 --> 00:04:31,280
paper die cut

00:04:28,160 --> 00:04:34,960
so um paper paper also have an api

00:04:31,280 --> 00:04:36,720
that support http or grpc that talk to

00:04:34,960 --> 00:04:40,000
your application code

00:04:36,720 --> 00:04:44,400
right so all these got encapsulated in

00:04:40,000 --> 00:04:44,400
in openshift on the left side

00:04:44,560 --> 00:04:49,360
and and that could interact with other

00:04:46,960 --> 00:04:52,639
components on the right hand side right

00:04:49,360 --> 00:04:53,600
using the resource binding the state or

00:04:52,639 --> 00:04:55,280
store

00:04:53,600 --> 00:04:58,479
the perfection subscribe and the

00:04:55,280 --> 00:04:58,479
distributed tracing

00:05:00,720 --> 00:05:04,400
so in order to use dava in openshift

00:05:03,600 --> 00:05:07,520
first

00:05:04,400 --> 00:05:10,560
we would install ham right so download

00:05:07,520 --> 00:05:13,759
your latest hem from github.com

00:05:10,560 --> 00:05:17,360
and unpack your ham and

00:05:13,759 --> 00:05:17,759
and then find the hem binary and move it

00:05:17,360 --> 00:05:23,120
into

00:05:17,759 --> 00:05:23,120
the user pin use a local pin hem

00:05:23,199 --> 00:05:26,880
and once you have installed hem you can

00:05:24,880 --> 00:05:29,840
go ahead and install docker

00:05:26,880 --> 00:05:30,479
using the yummy install docker then you

00:05:29,840 --> 00:05:32,240
could use

00:05:30,479 --> 00:05:35,120
the system control to start the start

00:05:32,240 --> 00:05:35,120
the docker surface

00:05:37,280 --> 00:05:40,800
at this time you are available to

00:05:39,199 --> 00:05:44,479
install data

00:05:40,800 --> 00:05:45,840
you can use the w get uh dot dash q and

00:05:44,479 --> 00:05:48,639
get the data

00:05:45,840 --> 00:05:51,039
get the installation script from from

00:05:48,639 --> 00:05:54,479
github

00:05:51,039 --> 00:05:55,840
so once you get the shell script

00:05:54,479 --> 00:05:59,360
you can you need to log into your

00:05:55,840 --> 00:06:01,759
openshift customer as an administrator

00:05:59,360 --> 00:06:04,400
make sure that you check in check your

00:06:01,759 --> 00:06:06,479
login status with the oc hmi

00:06:04,400 --> 00:06:09,199
and you should be an admin administrator

00:06:06,479 --> 00:06:09,199
for your customer

00:06:10,880 --> 00:06:14,800
so since we are using a ham repository

00:06:13,280 --> 00:06:17,360
to install deeper

00:06:14,800 --> 00:06:19,199
you need to add the table url into the

00:06:17,360 --> 00:06:21,919
hem repository

00:06:19,199 --> 00:06:24,080
using the hand repo app data and passing

00:06:21,919 --> 00:06:28,000
the url

00:06:24,080 --> 00:06:28,000
you could add the hand repository

00:06:29,759 --> 00:06:33,600
and then next step you need to do a hem

00:06:32,000 --> 00:06:37,440
repo update

00:06:33,600 --> 00:06:37,440
making sure that you get the latest code

00:06:38,240 --> 00:06:44,880
so now you have data

00:06:41,680 --> 00:06:47,520
and ham and docker installed

00:06:44,880 --> 00:06:49,840
you can go ahead and create a namespace

00:06:47,520 --> 00:06:51,840
in openshift

00:06:49,840 --> 00:06:53,840
so in this case you can use our oc

00:06:51,840 --> 00:06:58,000
create namespace

00:06:53,840 --> 00:06:58,000
i'm giving it a name called data system

00:06:59,680 --> 00:07:04,960
once you have your namespace you can

00:07:02,479 --> 00:07:09,280
start installing data

00:07:04,960 --> 00:07:11,280
into the namespace using hem install

00:07:09,280 --> 00:07:14,479
paper and then pass in your namespace

00:07:11,280 --> 00:07:14,479
name data system

00:07:14,800 --> 00:07:18,319
so that would install the um the

00:07:17,360 --> 00:07:20,720
namespace

00:07:18,319 --> 00:07:20,720
for you

00:07:23,520 --> 00:07:29,199
so once the name the data is installed

00:07:27,120 --> 00:07:31,919
uh double check and make sure that you

00:07:29,199 --> 00:07:34,400
have the data operator

00:07:31,919 --> 00:07:37,520
paper sidecut injector the vapor

00:07:34,400 --> 00:07:41,520
placement and the data sentry

00:07:37,520 --> 00:07:44,639
so these four objects should be

00:07:41,520 --> 00:07:49,759
available as four different parts

00:07:44,639 --> 00:07:52,400
in your paper system name space

00:07:49,759 --> 00:07:54,240
so uh the data operator manage the

00:07:52,400 --> 00:07:57,199
component and service

00:07:54,240 --> 00:07:58,879
services endpoint for data right so

00:07:57,199 --> 00:08:02,160
that's pretty clear

00:07:58,879 --> 00:08:03,280
uh the taper in sidecar injector injects

00:08:02,160 --> 00:08:06,319
deeper into the

00:08:03,280 --> 00:08:09,360
uh into your pots

00:08:06,319 --> 00:08:11,440
the vapor placement used for

00:08:09,360 --> 00:08:12,639
for actor where it creates a mapping

00:08:11,440 --> 00:08:15,280
table that map

00:08:12,639 --> 00:08:17,680
actor instance to port and then the

00:08:15,280 --> 00:08:19,120
paper sentry manage the transportation

00:08:17,680 --> 00:08:23,360
layer security

00:08:19,120 --> 00:08:23,360
basically as an act as a certificate

00:08:24,840 --> 00:08:27,759
authority

00:08:26,000 --> 00:08:29,919
now you can check your port status and

00:08:27,759 --> 00:08:30,800
make sure that they are in the running

00:08:29,919 --> 00:08:34,159
state

00:08:30,800 --> 00:08:36,000
so when you call oc get port plus in the

00:08:34,159 --> 00:08:37,760
namespace table system

00:08:36,000 --> 00:08:41,839
you should see the four different part

00:08:37,760 --> 00:08:41,839
of warning

00:08:42,800 --> 00:08:46,160
so in the sample code we're going to

00:08:44,320 --> 00:08:48,240
demo how to get the table running in

00:08:46,160 --> 00:08:51,360
openshift

00:08:48,240 --> 00:08:53,040
and then deploy the node.js app let's

00:08:51,360 --> 00:08:56,800
subscribe to the order

00:08:53,040 --> 00:08:59,040
message and per system so in this

00:08:56,800 --> 00:08:59,839
diagram here we have the data one time

00:08:59,040 --> 00:09:03,440
that talk to

00:08:59,839 --> 00:09:07,680
the node code using the data api

00:09:03,440 --> 00:09:09,680
we are using the state management using

00:09:07,680 --> 00:09:11,279
state store including some databases

00:09:09,680 --> 00:09:14,160
such as redis

00:09:11,279 --> 00:09:15,600
so on the left side we have the user

00:09:14,160 --> 00:09:17,680
they were interacting with this

00:09:15,600 --> 00:09:19,200
application using a get and post

00:09:17,680 --> 00:09:21,760
endpoint

00:09:19,200 --> 00:09:22,560
so the get endpoint will get a list of

00:09:21,760 --> 00:09:24,320
orders

00:09:22,560 --> 00:09:26,720
the post endpoint will be creating a new

00:09:24,320 --> 00:09:26,720
order

00:09:27,839 --> 00:09:31,680
so in this python app we generate

00:09:30,399 --> 00:09:35,519
messages

00:09:31,680 --> 00:09:37,440
with a no app consume and per system

00:09:35,519 --> 00:09:39,760
so this architecture diagram will show

00:09:37,440 --> 00:09:41,680
you the day per component

00:09:39,760 --> 00:09:43,440
so the data one time we'll talk to the

00:09:41,680 --> 00:09:45,760
python code

00:09:43,440 --> 00:09:46,480
and then um so you have one day put and

00:09:45,760 --> 00:09:48,320
then uh

00:09:46,480 --> 00:09:51,279
you have another data one time that

00:09:48,320 --> 00:09:54,880
doctor the no code so these two

00:09:51,279 --> 00:09:56,560
papers one time talk to each other

00:09:54,880 --> 00:09:58,720
and then at the end if the table one

00:09:56,560 --> 00:10:01,519
time will talk to the stage store

00:09:58,720 --> 00:10:01,519
with the redis

00:10:02,240 --> 00:10:06,480
so first get the latest code from

00:10:04,240 --> 00:10:10,560
davepad this is a hello world project

00:10:06,480 --> 00:10:14,000
if you go to github.com.keep

00:10:10,560 --> 00:10:17,680
then you cd into the sample hello folder

00:10:14,000 --> 00:10:19,279
you should be able to see the project

00:10:17,680 --> 00:10:21,839
the sample code has a dependency on

00:10:19,279 --> 00:10:24,399
radius as a state store

00:10:21,839 --> 00:10:27,600
for data persistency so therefore you

00:10:24,399 --> 00:10:27,600
need to install redis

00:10:28,800 --> 00:10:32,399
so the new order pulse endpoint so when

00:10:31,680 --> 00:10:34,880
we need to

00:10:32,399 --> 00:10:36,399
create a new order right so we call

00:10:34,880 --> 00:10:39,600
app.pose and

00:10:36,399 --> 00:10:41,600
basically pass in new order as a as an

00:10:39,600 --> 00:10:43,200
endpoint api

00:10:41,600 --> 00:10:46,079
so you have the request and response

00:10:43,200 --> 00:10:46,800
object so in this sample code here right

00:10:46,079 --> 00:10:49,279
it would

00:10:46,800 --> 00:10:53,519
create it would call where the state

00:10:49,279 --> 00:10:53,519
store to persist the order information

00:10:55,519 --> 00:10:59,440
and then to get endpoints similarly you

00:10:58,079 --> 00:11:02,880
can call app.get

00:10:59,440 --> 00:11:05,920
and slash order and then pass in the

00:11:02,880 --> 00:11:07,760
request that contains the order id

00:11:05,920 --> 00:11:09,839
so in this case it will call the red

00:11:07,760 --> 00:11:12,399
state store and receive the latest order

00:11:09,839 --> 00:11:12,399
information

00:11:13,519 --> 00:11:17,600
so now you know we need the dependency

00:11:16,480 --> 00:11:20,720
on reddit

00:11:17,600 --> 00:11:21,279
to install vedas um you can call him

00:11:20,720 --> 00:11:25,200
repo

00:11:21,279 --> 00:11:28,480
act and using chat dot

00:11:25,200 --> 00:11:31,760
binary you can install that

00:11:28,480 --> 00:11:32,560
uh you can add that repository and then

00:11:31,760 --> 00:11:34,880
after that you can

00:11:32,560 --> 00:11:36,560
call him install where this and install

00:11:34,880 --> 00:11:40,079
where it is into

00:11:36,560 --> 00:11:42,959
your namespace uh where does also

00:11:40,079 --> 00:11:44,399
uh have a dependency on the secret so

00:11:42,959 --> 00:11:46,240
extract the secret from the default

00:11:44,399 --> 00:11:49,279
namespace for reddit

00:11:46,240 --> 00:11:52,240
so you use the oc guest secret

00:11:49,279 --> 00:11:53,120
specify the namespace specify the

00:11:52,240 --> 00:11:56,079
jsonpath

00:11:53,120 --> 00:11:57,680
using data.writer's password once you

00:11:56,079 --> 00:11:59,279
get the password you can update the

00:11:57,680 --> 00:12:02,720
weather cmo file

00:11:59,279 --> 00:12:04,079
in the deploy directory and update a

00:12:02,720 --> 00:12:07,440
redis host to

00:12:04,079 --> 00:12:11,839
use the redis master 6379 and then the

00:12:07,440 --> 00:12:11,839
password from the last step

00:12:12,320 --> 00:12:17,519
now you have the redis yemo file updated

00:12:15,120 --> 00:12:19,279
you can call ocr pi dash f

00:12:17,519 --> 00:12:22,399
on the ml file and make sure that the

00:12:19,279 --> 00:12:22,399
component get created

00:12:22,560 --> 00:12:27,040
now you can create the uh no and python

00:12:25,519 --> 00:12:30,560
application

00:12:27,040 --> 00:12:33,040
using ocr dash f you can create a nodemo

00:12:30,560 --> 00:12:35,440
and then ocrpi dash f create a python

00:12:33,040 --> 00:12:35,440
mode

00:12:36,240 --> 00:12:39,519
so at this time you can do our oc get

00:12:38,079 --> 00:12:42,880
port on the

00:12:39,519 --> 00:12:46,480
daypa system you can see that the

00:12:42,880 --> 00:12:50,160
taper operator label placement

00:12:46,480 --> 00:12:53,440
the sentry the data sidecar no app

00:12:50,160 --> 00:12:56,079
python app radius were all set up

00:12:53,440 --> 00:12:56,079
and running

00:12:58,399 --> 00:13:02,880
so observe that the message you can look

00:13:01,839 --> 00:13:04,720
at the message and

00:13:02,880 --> 00:13:06,240
by looking at the logs and coming out

00:13:04,720 --> 00:13:08,560
from the pod

00:13:06,240 --> 00:13:09,279
so all you need you to do is to do an oc

00:13:08,560 --> 00:13:13,120
log and

00:13:09,279 --> 00:13:15,839
port with the no no no app port name

00:13:13,120 --> 00:13:17,680
and then making sure that you can you

00:13:15,839 --> 00:13:21,839
got the order id

00:13:17,680 --> 00:13:21,839
coming out from the logs

00:13:22,079 --> 00:13:25,760
and then at the end you can expose the

00:13:23,600 --> 00:13:30,240
wow from the no app

00:13:25,760 --> 00:13:30,240
so you can do oc expo surface no app

00:13:30,880 --> 00:13:34,800
you can call the no app endpoint and

00:13:33,360 --> 00:13:39,200
confirm

00:13:34,800 --> 00:13:39,200
that the order id is being persist

00:13:39,839 --> 00:13:44,160
in this case i'll order id42

00:13:46,160 --> 00:13:50,480
so now you have just uh finished the

00:13:48,000 --> 00:13:52,720
deployment of ava app

00:13:50,480 --> 00:13:54,639
you can go ahead and update the sample

00:13:52,720 --> 00:14:00,480
code and fitness scenario

00:13:54,639 --> 00:14:03,680
right so in conclusion paper

00:14:00,480 --> 00:14:06,000
work well with cloud native openshift

00:14:03,680 --> 00:14:09,519
enable easy event driven stay for

00:14:06,000 --> 00:14:12,639
microservices deployment and development

00:14:09,519 --> 00:14:16,079
data provide consistent and portability

00:14:12,639 --> 00:14:19,920
using standard api including http

00:14:16,079 --> 00:14:20,320
and grpc this architecture is also open

00:14:19,920 --> 00:14:21,920
source

00:14:20,320 --> 00:14:25,199
and work well with any programming

00:14:21,920 --> 00:14:27,360
languages and development framework

00:14:25,199 --> 00:14:28,480
that's at the end of the presentation

00:14:27,360 --> 00:14:29,120
please let me know if you have any

00:14:28,480 --> 00:14:31,839
question

00:14:29,120 --> 00:14:31,839
thank you

00:14:41,680 --> 00:14:49,440
hi so let me check again to see if yip

00:14:46,079 --> 00:14:51,199
is here if you're listening yet

00:14:49,440 --> 00:14:54,240
this is a picture taken on the fifth

00:14:51,199 --> 00:14:57,600
avenue in new york

00:14:54,240 --> 00:14:59,360
uh if you're here yep uh we need to have

00:14:57,600 --> 00:15:03,279
you

00:14:59,360 --> 00:15:03,279
try to share your audio and video

00:15:11,360 --> 00:15:18,399
uh okay so i guess yip is not here

00:15:14,959 --> 00:15:21,760
um for the q a um you can

00:15:18,399 --> 00:15:24,079
try and follow up with him in person um

00:15:21,760 --> 00:15:25,519
or you can also try going to the

00:15:24,079 --> 00:15:29,120
breakout booth later on

00:15:25,519 --> 00:15:30,480
um he may be there people who want to

00:15:29,120 --> 00:15:33,040
have discussions about things that

00:15:30,480 --> 00:15:35,120
happened in evolving technology

00:15:33,040 --> 00:15:36,160
the breakout booth is the place to do it

00:15:35,120 --> 00:15:39,279
it's the

00:15:36,160 --> 00:15:41,440
new edition room and i'll put that link

00:15:39,279 --> 00:15:47,199
in chat

00:15:41,440 --> 00:15:47,199

YouTube URL: https://www.youtube.com/watch?v=OGnllj80bHk


