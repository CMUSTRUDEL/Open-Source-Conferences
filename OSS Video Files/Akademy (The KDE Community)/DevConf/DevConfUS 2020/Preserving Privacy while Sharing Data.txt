Title: Preserving Privacy while Sharing Data
Publication date: 2020-10-06
Playlist: DevConfUS 2020
Description: 
	Speaker: Gordon Haff

Deep learning and machine learning more broadly depend on large quantities of data to develop accurate predictive models. In areas such as medical research, sharing data among institutions can lead to even greater value. However, data often includes personally identifiable information that we may not want to (or even be legally allowed to) share with others. Traditional anonymization techniques only help to a degree.

In this talk, Red Hat's Gordon Haff will share with you the active activity taking place in academia, open source communities, and elsewhere into techniques such as differential privacy and secure multi-party computation. The goal of this research and ongoing work is to help individuals and organizations work collaboratively while preserving the anonymity of individual data points.
Captions: 
	00:00:00,399 --> 00:00:04,880
hi everyone this is akanksha and i'll be

00:00:03,040 --> 00:00:07,680
moderating this

00:00:04,880 --> 00:00:10,080
session which is preserving privacy

00:00:07,680 --> 00:00:13,519
while sharing data by gordon half

00:00:10,080 --> 00:00:15,280
and we'll be starting in another

00:00:13,519 --> 00:00:16,880
two three minutes i think or we

00:00:15,280 --> 00:00:19,119
shouldn't wait for more people to join

00:00:16,880 --> 00:00:19,119
in

00:00:19,520 --> 00:00:22,640
all right then i think we are good to

00:00:21,520 --> 00:00:25,599
start now

00:00:22,640 --> 00:00:26,720
i'm gonna play the let me know if you

00:00:25,599 --> 00:00:28,840
are unable to hear

00:00:26,720 --> 00:00:30,400
of or if there's some technical

00:00:28,840 --> 00:00:33,279
difficulty

00:00:30,400 --> 00:00:33,279
so here we go

00:00:34,719 --> 00:00:38,559
emerging technology evangelist red hat

00:00:37,440 --> 00:00:41,600
and today

00:00:38,559 --> 00:00:45,280
i'd like to talk with you about

00:00:41,600 --> 00:00:48,079
preserving privacy while sharing data

00:00:45,280 --> 00:00:49,600
sharing data can accelerate innovation

00:00:48,079 --> 00:00:52,879
in a lot of different fields

00:00:49,600 --> 00:00:56,079
from telecoms to healthcare

00:00:52,879 --> 00:00:58,800
governments to energy and so forth

00:00:56,079 --> 00:01:01,280
and this is particularly true with all

00:00:58,800 --> 00:01:02,640
these open data sets out there that we

00:01:01,280 --> 00:01:06,080
just combined together

00:01:02,640 --> 00:01:08,560
in various ways and this whole process

00:01:06,080 --> 00:01:09,680
is being accelerated as well by the fact

00:01:08,560 --> 00:01:11,280
that some of the best

00:01:09,680 --> 00:01:13,360
techniques that we've come up with

00:01:11,280 --> 00:01:16,240
recently such as

00:01:13,360 --> 00:01:17,119
deep learning many types of machine

00:01:16,240 --> 00:01:20,400
learning

00:01:17,119 --> 00:01:23,040
really require vast amounts of data

00:01:20,400 --> 00:01:25,520
in order to be effective but there's a

00:01:23,040 --> 00:01:29,200
problem

00:01:25,520 --> 00:01:32,000
data can be sensitive data can be

00:01:29,200 --> 00:01:34,479
private so it becomes important to think

00:01:32,000 --> 00:01:37,600
about ways that we can share

00:01:34,479 --> 00:01:40,000
and act on data without revealing

00:01:37,600 --> 00:01:41,520
information that people and

00:01:40,000 --> 00:01:44,079
organizations

00:01:41,520 --> 00:01:44,880
don't want to be revealed and that's the

00:01:44,079 --> 00:01:48,479
subject

00:01:44,880 --> 00:01:49,840
of today's talk before we dive down into

00:01:48,479 --> 00:01:52,240
the details

00:01:49,840 --> 00:01:53,200
let's look at the map of the territory

00:01:52,240 --> 00:02:09,840
so to speak

00:01:53,200 --> 00:02:09,840
this comes from andrew kraske

00:02:16,959 --> 00:02:24,000
as it is for example being computed upon

00:02:20,640 --> 00:02:24,480
and aggregated and you see here things

00:02:24,000 --> 00:02:27,120
like

00:02:24,480 --> 00:02:28,560
homomorphic encryption fully homomorphic

00:02:27,120 --> 00:02:31,680
encryption

00:02:28,560 --> 00:02:32,160
zero knowledge proofs i'm not going to

00:02:31,680 --> 00:02:35,360
spend

00:02:32,160 --> 00:02:37,519
time down there today

00:02:35,360 --> 00:02:39,840
because fully homomorphic encryption

00:02:37,519 --> 00:02:41,120
while it's very interesting technique

00:02:39,840 --> 00:02:44,720
because it allows you

00:02:41,120 --> 00:02:47,360
to have computation done

00:02:44,720 --> 00:02:50,000
for example on a public cloud provider

00:02:47,360 --> 00:02:52,400
where the data that is being worked on

00:02:50,000 --> 00:02:53,440
is all encrypted so there is no

00:02:52,400 --> 00:02:59,120
opportunity

00:02:53,440 --> 00:03:01,519
no real need for trust in organizations

00:02:59,120 --> 00:03:02,720
in order to protect the confidentiality

00:03:01,519 --> 00:03:05,360
of that data

00:03:02,720 --> 00:03:06,239
but it's very compute intensive although

00:03:05,360 --> 00:03:08,959
it's an area of

00:03:06,239 --> 00:03:11,920
uh significant research today is

00:03:08,959 --> 00:03:14,239
interesting but really isn't practical

00:03:11,920 --> 00:03:15,519
in a any kind of a production sense

00:03:14,239 --> 00:03:18,319
today

00:03:15,519 --> 00:03:20,959
i will spend some time however talking

00:03:18,319 --> 00:03:23,360
about multi-party compensation

00:03:20,959 --> 00:03:25,440
uh which as you see overlapped with the

00:03:23,360 --> 00:03:28,400
policy enforcement circle

00:03:25,440 --> 00:03:30,959
up there because the protocol that's

00:03:28,400 --> 00:03:34,480
used for multi-query communication

00:03:30,959 --> 00:03:36,599
does in a sense provide a policy

00:03:34,480 --> 00:03:37,920
around the protection of data

00:03:36,599 --> 00:03:40,879
confidentiality

00:03:37,920 --> 00:03:41,840
but at the same time it is also a way to

00:03:40,879 --> 00:03:43,920
get input

00:03:41,840 --> 00:03:44,959
privacy but i'll be spending more time

00:03:43,920 --> 00:03:47,599
in that

00:03:44,959 --> 00:03:49,040
trusted execution environments are also

00:03:47,599 --> 00:03:52,239
in that same

00:03:49,040 --> 00:03:53,760
general category this is also an area of

00:03:52,239 --> 00:03:55,840
active research

00:03:53,760 --> 00:03:57,040
with things like confidential computing

00:03:55,840 --> 00:04:00,159
consortium

00:03:57,040 --> 00:04:04,400
uh project anarchs in one

00:04:00,159 --> 00:04:08,720
of the uh projects they're associated

00:04:04,400 --> 00:04:12,080
with peds and again other ways that

00:04:08,720 --> 00:04:15,360
you can for example execute

00:04:12,080 --> 00:04:18,400
operations on a cloud provider

00:04:15,360 --> 00:04:20,560
and you don't need to trust much of the

00:04:18,400 --> 00:04:22,960
underlying software stack

00:04:20,560 --> 00:04:25,120
because the tee provides certain types

00:04:22,960 --> 00:04:29,600
of attestation and so forth

00:04:25,120 --> 00:04:32,880
that makes that trust unnecessary

00:04:29,600 --> 00:04:33,520
in the lower right hand corner and this

00:04:32,880 --> 00:04:36,639
is where

00:04:33,520 --> 00:04:37,680
i'm going to spend all of my time here

00:04:36,639 --> 00:04:41,600
today

00:04:37,680 --> 00:04:44,560
uh is is how do you protect

00:04:41,600 --> 00:04:45,440
the privacy of data that's being shared

00:04:44,560 --> 00:04:48,759
and that's being

00:04:45,440 --> 00:04:50,000
aggregated and a lot of traditional

00:04:48,759 --> 00:04:52,320
anonymization

00:04:50,000 --> 00:04:55,040
techniques go into here though some of

00:04:52,320 --> 00:04:56,000
those arguably apply to input privacy as

00:04:55,040 --> 00:04:58,800
well

00:04:56,000 --> 00:04:59,520
but a particular technique i'm going to

00:04:58,800 --> 00:05:03,120
talk about

00:04:59,520 --> 00:05:06,400
in this area is differential privacy

00:05:03,120 --> 00:05:10,080
um which essentially brings

00:05:06,400 --> 00:05:13,680
formal methods to aggregating

00:05:10,080 --> 00:05:15,280
uh collections of data so

00:05:13,680 --> 00:05:18,000
those are the areas that we're going to

00:05:15,280 --> 00:05:18,000
look at today

00:05:18,320 --> 00:05:21,840
let's start by talking about

00:05:20,000 --> 00:05:25,039
anonymization work

00:05:21,840 --> 00:05:27,039
generally though and very closely

00:05:25,039 --> 00:05:28,960
related to this and we can probably

00:05:27,039 --> 00:05:30,960
just take them as being the same thing

00:05:28,960 --> 00:05:34,400
for our purposes today

00:05:30,960 --> 00:05:34,960
is pseudo anonymization uh which is

00:05:34,400 --> 00:05:38,320
rather

00:05:34,960 --> 00:05:41,360
stripping out data

00:05:38,320 --> 00:05:44,800
sensitive data from a record

00:05:41,360 --> 00:05:47,919
is replacing the sense to data with

00:05:44,800 --> 00:05:51,520
some sort of token or pseudonymous

00:05:47,919 --> 00:05:54,800
wood in those particular fields

00:05:51,520 --> 00:05:58,639
and certainly the the

00:05:54,800 --> 00:05:59,919
naive or the simple way of thinking

00:05:58,639 --> 00:06:03,199
about anonymization

00:05:59,919 --> 00:06:06,880
is simply that we're removing

00:06:03,199 --> 00:06:09,919
things like names addresses

00:06:06,880 --> 00:06:13,120
social security numbers those

00:06:09,919 --> 00:06:16,479
kind of classic identifying

00:06:13,120 --> 00:06:18,560
uh fields that would go

00:06:16,479 --> 00:06:20,960
be attacked through some sort of record

00:06:18,560 --> 00:06:24,560
of things like health care information

00:06:20,960 --> 00:06:28,160
or financial records and so forth

00:06:24,560 --> 00:06:31,440
um they also encrypt or transform

00:06:28,160 --> 00:06:32,880
personal data fields to make them harder

00:06:31,440 --> 00:06:36,080
to de-anonymize

00:06:32,880 --> 00:06:37,440
so for example in a healthcare record

00:06:36,080 --> 00:06:41,199
you might have

00:06:37,440 --> 00:06:43,680
a birth and

00:06:41,199 --> 00:06:44,479
when you that you certainly have a day

00:06:43,680 --> 00:06:47,360
of birth

00:06:44,479 --> 00:06:48,000
however when you share that data maybe

00:06:47,360 --> 00:06:51,759
the exact

00:06:48,000 --> 00:06:55,039
date of birth isn't very important maybe

00:06:51,759 --> 00:06:56,319
it's fine just to say somebody is 30

00:06:55,039 --> 00:06:59,919
years old or they were

00:06:56,319 --> 00:07:02,400
born in a particular year or

00:06:59,919 --> 00:07:03,919
maybe even they were born in a

00:07:02,400 --> 00:07:05,759
particular decade and

00:07:03,919 --> 00:07:07,280
obviously there was a certain part of

00:07:05,759 --> 00:07:10,800
figuring out

00:07:07,280 --> 00:07:13,680
how much you can

00:07:10,800 --> 00:07:16,479
transform data before it becomes less

00:07:13,680 --> 00:07:19,919
useful but nonetheless it is a technique

00:07:16,479 --> 00:07:23,120
that can be used to help protect

00:07:19,919 --> 00:07:26,960
the um sensitive information

00:07:23,120 --> 00:07:30,319
in set data and then

00:07:26,960 --> 00:07:33,360
finally you can aggregate by a

00:07:30,319 --> 00:07:35,840
trusted agency and this still

00:07:33,360 --> 00:07:37,599
doesn't provide perfect anonymization

00:07:35,840 --> 00:07:40,639
for reasons that i'll get

00:07:37,599 --> 00:07:44,560
into but i is certainly a very

00:07:40,639 --> 00:07:46,560
common way of protecting

00:07:44,560 --> 00:07:48,560
individual data points while still

00:07:46,560 --> 00:07:51,599
having a meaningful

00:07:48,560 --> 00:07:54,879
set of data that is uh

00:07:51,599 --> 00:07:57,520
aggregated up across a population and

00:07:54,879 --> 00:07:58,160
a very good example of this that's very

00:07:57,520 --> 00:08:01,280
familiar

00:07:58,160 --> 00:08:04,479
to a lot of people is census

00:08:01,280 --> 00:08:05,039
demographic etc types of data that the

00:08:04,479 --> 00:08:09,199
u.s

00:08:05,039 --> 00:08:11,440
census aggregates uh in various ways

00:08:09,199 --> 00:08:14,919
that they

00:08:11,440 --> 00:08:18,479
that tried to preserve anonymity of the

00:08:14,919 --> 00:08:19,840
individuals who have taken a survey

00:08:18,479 --> 00:08:23,840
and i'm going to talk about that in a

00:08:19,840 --> 00:08:26,800
lot more detail in a few slides

00:08:23,840 --> 00:08:27,520
and the question is does all of this

00:08:26,800 --> 00:08:30,560
work

00:08:27,520 --> 00:08:33,839
and the answer is

00:08:30,560 --> 00:08:36,959
yes kinda sorta

00:08:33,839 --> 00:08:38,719
a lot of time it depends upon what your

00:08:36,959 --> 00:08:42,159
assumptions are

00:08:38,719 --> 00:08:44,880
because there's quite a few challenges

00:08:42,159 --> 00:08:46,399
so the first is even defining what

00:08:44,880 --> 00:08:49,760
constitutes personal

00:08:46,399 --> 00:08:52,080
data in the first place and this

00:08:49,760 --> 00:08:54,800
doesn't have a straightforward answer

00:08:52,080 --> 00:08:57,760
because for example something like

00:08:54,800 --> 00:08:58,640
salary data for example might be

00:08:57,760 --> 00:09:01,920
considered

00:08:58,640 --> 00:09:05,040
by many people to be

00:09:01,920 --> 00:09:08,399
personal sensitive types of data

00:09:05,040 --> 00:09:11,200
and yet it's required to be disclosed

00:09:08,399 --> 00:09:14,480
in some situations uh for instance

00:09:11,200 --> 00:09:16,959
government employees in much of the us

00:09:14,480 --> 00:09:17,600
uh and in some countries it's a norm

00:09:16,959 --> 00:09:20,720
that

00:09:17,600 --> 00:09:22,880
that salary data is public

00:09:20,720 --> 00:09:25,120
so you first of all have to determine

00:09:22,880 --> 00:09:28,880
what actually constitutes

00:09:25,120 --> 00:09:31,120
sensitive data and this can also be

00:09:28,880 --> 00:09:32,080
complicated for reasons i'll get to the

00:09:31,120 --> 00:09:36,000
moment

00:09:32,080 --> 00:09:38,560
uh sorry for the interruption i'm

00:09:36,000 --> 00:09:41,920
gonna try to reshare my screen so that

00:09:38,560 --> 00:09:41,920
you can hear him properly

00:09:42,160 --> 00:09:47,680
situationally uh government employees in

00:09:46,080 --> 00:09:50,720
much of the u.s

00:09:47,680 --> 00:09:53,839
uh and in some countries it's a norm

00:09:50,720 --> 00:09:56,000
that salary data is public

00:09:53,839 --> 00:09:58,240
so you first of all have to determine

00:09:56,000 --> 00:10:02,079
what actually constitutes

00:09:58,240 --> 00:10:04,320
sensitive data and this can also be

00:10:02,079 --> 00:10:05,200
complicated for reasons i'll get to in a

00:10:04,320 --> 00:10:08,959
moment

00:10:05,200 --> 00:10:12,240
uh i'm trusted aggregator

00:10:08,959 --> 00:10:15,360
in the last slide but you know who can

00:10:12,240 --> 00:10:18,560
you really trust uh a lot of

00:10:15,360 --> 00:10:20,160
private organizations sell data for

00:10:18,560 --> 00:10:23,440
profit these days

00:10:20,160 --> 00:10:26,560
and also you know there's data breaches

00:10:23,440 --> 00:10:30,000
uh do you trust a company to keep your

00:10:26,560 --> 00:10:30,000
personal records safe

00:10:30,240 --> 00:10:35,680
there is also some specific technical

00:10:33,040 --> 00:10:37,440
issues that relate to aggregating data

00:10:35,680 --> 00:10:38,959
and one of them is the lack of data

00:10:37,440 --> 00:10:41,440
diversity and k

00:10:38,959 --> 00:10:42,000
anonymity failures is one term that's

00:10:41,440 --> 00:10:45,519
used

00:10:42,000 --> 00:10:46,160
in this uh in this area uh and basically

00:10:45,519 --> 00:10:49,519
what this

00:10:46,160 --> 00:10:54,720
means is that if it's known

00:10:49,519 --> 00:10:54,720
that you are part of a data set

00:10:55,200 --> 00:10:58,880
that by itself can reveal something

00:10:57,920 --> 00:11:01,920
about you

00:10:58,880 --> 00:11:04,160
now the fact that you're a us

00:11:01,920 --> 00:11:06,000
citizen for example doesn't reveal an

00:11:04,160 --> 00:11:07,839
awful lot other than the fact of course

00:11:06,000 --> 00:11:10,640
that you're a u.s citizen

00:11:07,839 --> 00:11:12,800
but if you're in some particular health

00:11:10,640 --> 00:11:16,320
database for example

00:11:12,800 --> 00:11:18,399
that may reveal that um

00:11:16,320 --> 00:11:20,320
you were tested for some particular

00:11:18,399 --> 00:11:22,480
disease which presumably

00:11:20,320 --> 00:11:24,959
increases the probability that you

00:11:22,480 --> 00:11:28,079
actually have that disease

00:11:24,959 --> 00:11:30,560
and then um finally and again

00:11:28,079 --> 00:11:32,320
this is a whole field of study and into

00:11:30,560 --> 00:11:35,440
a lot more detail here

00:11:32,320 --> 00:11:38,480
but uh there's a general susceptibility

00:11:35,440 --> 00:11:41,600
to certain types of attacks

00:11:38,480 --> 00:11:45,839
um and it actually turns out

00:11:41,600 --> 00:11:47,519
that when researchers study this kind of

00:11:45,839 --> 00:11:51,600
thing

00:11:47,519 --> 00:11:52,320
a lot of aggregated data and anonymized

00:11:51,600 --> 00:11:55,680
data

00:11:52,320 --> 00:11:58,399
that i think the average individual

00:11:55,680 --> 00:11:59,000
would say well yeah that seems like it's

00:11:58,399 --> 00:12:02,160
pretty

00:11:59,000 --> 00:12:04,560
anonymized and that ought to be okay but

00:12:02,160 --> 00:12:05,839
researchers have come up with ways some

00:12:04,560 --> 00:12:08,560
of which are admittedly

00:12:05,839 --> 00:12:10,480
probably a bit artificial in real world

00:12:08,560 --> 00:12:14,000
data sets uh

00:12:10,480 --> 00:12:17,839
where it is at least prince in principle

00:12:14,000 --> 00:12:20,560
possible to re-identify um

00:12:17,839 --> 00:12:22,720
individuals in those data sets in a

00:12:20,560 --> 00:12:25,839
number of different ways and even if

00:12:22,720 --> 00:12:26,160
they can't do it perfectly they can do

00:12:25,839 --> 00:12:30,320
it

00:12:26,160 --> 00:12:33,200
in a statistical way so we can say well

00:12:30,320 --> 00:12:33,600
i don't know that this data point is

00:12:33,200 --> 00:12:36,720
this

00:12:33,600 --> 00:12:40,720
person but there's a 30

00:12:36,720 --> 00:12:43,680
chance a 50 chance and in many cases

00:12:40,720 --> 00:12:46,240
that's all that you need some uh in some

00:12:43,680 --> 00:12:49,519
cases say if somebody is trying to track

00:12:46,240 --> 00:12:52,320
down that individual

00:12:49,519 --> 00:12:52,560
one of the particular attacks and this

00:12:52,320 --> 00:12:55,760
is

00:12:52,560 --> 00:12:58,000
from the u.s census bureau uh slide the

00:12:55,760 --> 00:12:58,399
u.s census bureau actually has a lot of

00:12:58,000 --> 00:13:00,560
good

00:12:58,399 --> 00:13:03,040
information if you won't dig into this

00:13:00,560 --> 00:13:05,519
kind of thing in more detail

00:13:03,040 --> 00:13:08,000
and on the right hand side you would

00:13:05,519 --> 00:13:09,920
have certain types of aggregation with

00:13:08,000 --> 00:13:12,399
with statistical measures

00:13:09,920 --> 00:13:14,560
against those aggregates so the number

00:13:12,399 --> 00:13:15,600
of females the number of males the

00:13:14,560 --> 00:13:18,839
numbers

00:13:15,600 --> 00:13:22,320
uh numbers of uh white black

00:13:18,839 --> 00:13:25,920
married black females etc

00:13:22,320 --> 00:13:28,880
and uh you know though none of those

00:13:25,920 --> 00:13:29,360
are one uh you know they're they're all

00:13:28,880 --> 00:13:31,839
are an

00:13:29,360 --> 00:13:33,360
aggregate of some number of individuals

00:13:31,839 --> 00:13:35,040
and you know we could increase the

00:13:33,360 --> 00:13:38,240
numbers here

00:13:35,040 --> 00:13:39,199
too but really you can think of that as

00:13:38,240 --> 00:13:42,399
almost a set

00:13:39,199 --> 00:13:45,320
of simultaneous equations which

00:13:42,399 --> 00:13:48,160
can be solved for and again can be

00:13:45,320 --> 00:13:49,680
transformed then by solving that set of

00:13:48,160 --> 00:13:52,959
equations

00:13:49,680 --> 00:13:55,199
into some very specific data points that

00:13:52,959 --> 00:13:58,560
don't have a name attached to them

00:13:55,199 --> 00:14:01,600
but our unique fingerprints which

00:13:58,560 --> 00:14:02,959
at least in smaller data sets may make

00:14:01,600 --> 00:14:06,240
it possible

00:14:02,959 --> 00:14:07,360
to again statistically perhaps figure

00:14:06,240 --> 00:14:10,800
out

00:14:07,360 --> 00:14:12,560
what is the entire set of data

00:14:10,800 --> 00:14:15,199
that applies to this particular

00:14:12,560 --> 00:14:18,399
individual

00:14:15,199 --> 00:14:22,480
we can also identify patterns this is

00:14:18,399 --> 00:14:24,959
uh an online fitness tracker

00:14:22,480 --> 00:14:26,000
data point set of data points from some

00:14:24,959 --> 00:14:29,440
individual

00:14:26,000 --> 00:14:32,560
and if you look at these patterns

00:14:29,440 --> 00:14:35,440
you can look at that and go hmm

00:14:32,560 --> 00:14:37,199
this individual that this belongs to i

00:14:35,440 --> 00:14:38,320
don't know their name i don't know their

00:14:37,199 --> 00:14:40,800
address i don't know

00:14:38,320 --> 00:14:41,600
anything else about them but they've

00:14:40,800 --> 00:14:44,639
spent

00:14:41,600 --> 00:14:48,079
all they seem to concentrate their time

00:14:44,639 --> 00:14:48,720
in a fairly limited area which is where

00:14:48,079 --> 00:14:51,839
you know

00:14:48,720 --> 00:14:53,279
the bright white is in that in that

00:14:51,839 --> 00:14:56,720
screenshot

00:14:53,279 --> 00:14:59,120
and we might infer

00:14:56,720 --> 00:15:00,800
the person lives right around there

00:14:59,120 --> 00:15:02,320
somewhere now

00:15:00,800 --> 00:15:04,399
maybe they get in their car and they

00:15:02,320 --> 00:15:05,440
drive to a park and that's where they do

00:15:04,399 --> 00:15:08,880
their running

00:15:05,440 --> 00:15:11,920
um maybe uh but

00:15:08,880 --> 00:15:12,399
if if again if somebody is trying to

00:15:11,920 --> 00:15:16,000
track

00:15:12,399 --> 00:15:18,079
somebody down for example uh

00:15:16,000 --> 00:15:20,160
something like this would give them a

00:15:18,079 --> 00:15:22,560
pretty good idea

00:15:20,160 --> 00:15:24,480
and uh you know certainly similar if

00:15:22,560 --> 00:15:27,360
this was done with gps

00:15:24,480 --> 00:15:28,800
uh gps tracks from automobiles or

00:15:27,360 --> 00:15:31,920
anything like that

00:15:28,800 --> 00:15:35,600
uh yeah you know if somebody is

00:15:31,920 --> 00:15:37,920
always coming and going to this house

00:15:35,600 --> 00:15:40,560
that tells us something about that's

00:15:37,920 --> 00:15:42,079
probably where they live and if they're

00:15:40,560 --> 00:15:44,399
going a lot to an

00:15:42,079 --> 00:15:46,800
other person another house somewhere

00:15:44,399 --> 00:15:49,920
well that tells us something as well

00:15:46,800 --> 00:15:49,920
about their patterns

00:15:50,399 --> 00:15:54,079
particularly interesting area and

00:15:52,079 --> 00:15:56,320
probably particularly relevant

00:15:54,079 --> 00:15:57,199
today and will be very relevant when we

00:15:56,320 --> 00:15:59,920
talk about

00:15:57,199 --> 00:16:02,000
differential privacy is this idea of

00:15:59,920 --> 00:16:03,360
linkage attacks so what we have here in

00:16:02,000 --> 00:16:07,759
the left

00:16:03,360 --> 00:16:11,279
is hospital visits for an individual

00:16:07,759 --> 00:16:12,399
this is an anonymized in principle

00:16:11,279 --> 00:16:14,800
anonymized

00:16:12,399 --> 00:16:17,519
record there's no one's name here

00:16:14,800 --> 00:16:20,510
there's no social security number here

00:16:17,519 --> 00:16:21,839
um but it does include

00:16:20,510 --> 00:16:24,720
[Music]

00:16:21,839 --> 00:16:25,680
relevant identifying information a date

00:16:24,720 --> 00:16:29,440
of birth

00:16:25,680 --> 00:16:31,360
gender and a zip code give birth

00:16:29,440 --> 00:16:33,600
we could fuzz that a bit by just

00:16:31,360 --> 00:16:35,680
changing that to a year as i mentioned

00:16:33,600 --> 00:16:39,600
earlier

00:16:35,680 --> 00:16:42,560
zip code is presumably

00:16:39,600 --> 00:16:43,600
a five digit zip code not a nine digit

00:16:42,560 --> 00:16:46,320
zip code

00:16:43,600 --> 00:16:48,560
although even then zip codes can be

00:16:46,320 --> 00:16:49,279
pretty small when you get into more

00:16:48,560 --> 00:16:52,240
rural

00:16:49,279 --> 00:16:54,800
uh rural areas for example but now let's

00:16:52,240 --> 00:16:55,279
assume for purposes of argument that we

00:16:54,800 --> 00:16:58,160
have

00:16:55,279 --> 00:16:59,680
a record that taken by itself is pretty

00:16:58,160 --> 00:17:02,880
anonymous

00:16:59,680 --> 00:17:06,240
well the same individual has

00:17:02,880 --> 00:17:09,679
other records that are out there

00:17:06,240 --> 00:17:12,880
in open data sets or even that may be

00:17:09,679 --> 00:17:15,600
legally required to be

00:17:12,880 --> 00:17:18,079
public for various reasons so voter

00:17:15,600 --> 00:17:21,439
registration in many places

00:17:18,079 --> 00:17:24,640
will have information like name address

00:17:21,439 --> 00:17:27,520
phone number as well as maybe

00:17:24,640 --> 00:17:28,559
date of birth gender almost certainly

00:17:27,520 --> 00:17:30,880
zip code

00:17:28,559 --> 00:17:33,120
um and again you can argue you know how

00:17:30,880 --> 00:17:33,679
much of this is public in a given case

00:17:33,120 --> 00:17:36,080
but

00:17:33,679 --> 00:17:37,120
for purposes of our discussion here you

00:17:36,080 --> 00:17:40,160
know let's assume

00:17:37,120 --> 00:17:43,280
that there is some

00:17:40,160 --> 00:17:46,160
number of fields that might overlap

00:17:43,280 --> 00:17:47,200
with a sen with a piece of sensitive

00:17:46,160 --> 00:17:50,799
data

00:17:47,200 --> 00:17:52,480
and we can start to do correlations and

00:17:50,799 --> 00:17:53,840
of course there's not just one public

00:17:52,480 --> 00:17:57,039
record there's probably

00:17:53,840 --> 00:17:59,440
many public records

00:17:57,039 --> 00:18:00,960
and to illustrate this a particularly

00:17:59,440 --> 00:18:04,160
interesting example

00:18:00,960 --> 00:18:06,240
uh comes from a number of years back

00:18:04,160 --> 00:18:08,000
when many of you may remember there was

00:18:06,240 --> 00:18:10,880
something called the netflix

00:18:08,000 --> 00:18:12,160
prize and the idea with netflix prize

00:18:10,880 --> 00:18:15,200
was netflix

00:18:12,160 --> 00:18:16,799
released a whole bunch of records that

00:18:15,200 --> 00:18:20,400
were anonymized so no

00:18:16,799 --> 00:18:24,799
names no user names no

00:18:20,400 --> 00:18:24,799
no zip codes no um

00:18:25,440 --> 00:18:28,880
other information ip addresses that

00:18:28,400 --> 00:18:31,440
maybe

00:18:28,880 --> 00:18:33,919
could start to give a fingerprint for a

00:18:31,440 --> 00:18:37,039
user even if their name wasn't attached

00:18:33,919 --> 00:18:40,640
and these records were basically

00:18:37,039 --> 00:18:42,640
you know alice uh really liked these

00:18:40,640 --> 00:18:45,679
three movies hated these four

00:18:42,640 --> 00:18:48,720
movies bob loved

00:18:45,679 --> 00:18:49,679
this one movie hated everything else and

00:18:48,720 --> 00:18:52,720
so forth

00:18:49,679 --> 00:18:56,559
um and it was

00:18:52,720 --> 00:18:58,400
anonymized supposedly uh now what these

00:18:56,559 --> 00:19:02,000
researchers did

00:18:58,400 --> 00:19:04,559
was they took that netflix data

00:19:02,000 --> 00:19:06,640
which was intended basically for

00:19:04,559 --> 00:19:08,400
researchers to develop machine learning

00:19:06,640 --> 00:19:10,640
algorithms to improve

00:19:08,400 --> 00:19:13,200
netflix's recommendation engine which

00:19:10,640 --> 00:19:15,280
didn't really work out all that well

00:19:13,200 --> 00:19:17,360
for other reasons that aren't relevant

00:19:15,280 --> 00:19:18,880
here but in any case there was this

00:19:17,360 --> 00:19:21,360
large public data set

00:19:18,880 --> 00:19:22,160
and what these researchers did was they

00:19:21,360 --> 00:19:25,200
looked at an

00:19:22,160 --> 00:19:28,320
other public data set which is

00:19:25,200 --> 00:19:30,880
the in which is the rating

00:19:28,320 --> 00:19:32,080
information in the internet movie

00:19:30,880 --> 00:19:36,840
database

00:19:32,080 --> 00:19:40,400
um and again you have users

00:19:36,840 --> 00:19:41,200
who like movies don't like movies and so

00:19:40,400 --> 00:19:45,360
forth

00:19:41,200 --> 00:19:47,280
um and in those cases

00:19:45,360 --> 00:19:48,559
at least some of those users are

00:19:47,280 --> 00:19:52,320
probably

00:19:48,559 --> 00:19:54,720
able to be identified because

00:19:52,320 --> 00:19:56,880
they may have a name that they use

00:19:54,720 --> 00:19:59,760
across different logins they may

00:19:56,880 --> 00:20:02,480
use something that is their actual

00:19:59,760 --> 00:20:06,159
real-life name their real true name

00:20:02,480 --> 00:20:08,480
um and by combining those two different

00:20:06,159 --> 00:20:09,280
data sets what the researchers were able

00:20:08,480 --> 00:20:12,240
to do

00:20:09,280 --> 00:20:13,280
was essentially look at everybody's

00:20:12,240 --> 00:20:16,080
fingerprint

00:20:13,280 --> 00:20:17,520
in the netflix data and everybody's

00:20:16,080 --> 00:20:20,799
fingerprint

00:20:17,520 --> 00:20:25,120
in the imdb data and

00:20:20,799 --> 00:20:27,600
discover that you know if somebody

00:20:25,120 --> 00:20:28,880
liked these three movies and hit these

00:20:27,600 --> 00:20:32,880
four movies and

00:20:28,880 --> 00:20:36,080
maybe no one hates hardly anybody hates

00:20:32,880 --> 00:20:37,440
one of those four movies and yet there's

00:20:36,080 --> 00:20:40,720
a very similar

00:20:37,440 --> 00:20:43,440
record in the imdb database that has a

00:20:40,720 --> 00:20:46,880
very similar type of fingerprint

00:20:43,440 --> 00:20:47,360
you start going you know that might very

00:20:46,880 --> 00:20:50,799
well

00:20:47,360 --> 00:20:53,200
be the same person and

00:20:50,799 --> 00:20:54,480
i'm not going to keep repeating this

00:20:53,200 --> 00:20:57,280
this may be

00:20:54,480 --> 00:20:59,039
a it's there may be a statistical

00:20:57,280 --> 00:21:01,039
inference we may not be

00:20:59,039 --> 00:21:02,559
a hundred percent sure it's not a

00:21:01,039 --> 00:21:06,000
smoking gun

00:21:02,559 --> 00:21:06,880
we can take the court but it starts to

00:21:06,000 --> 00:21:09,039
be a pretty

00:21:06,880 --> 00:21:10,480
good indicator and maybe you can

00:21:09,039 --> 00:21:14,720
correlate them with yet

00:21:10,480 --> 00:21:17,039
a third database of some sort which

00:21:14,720 --> 00:21:18,080
has a similar type of correlation

00:21:17,039 --> 00:21:20,880
pattern and we can

00:21:18,080 --> 00:21:21,760
start to de-anonymize the data now

00:21:20,880 --> 00:21:24,000
nothing here

00:21:21,760 --> 00:21:25,200
it's you know it's probably it's

00:21:24,000 --> 00:21:29,039
probably not

00:21:25,200 --> 00:21:32,320
a um a serious matter

00:21:29,039 --> 00:21:35,200
if someone uncovers that aha

00:21:32,320 --> 00:21:35,919
he didn't like the star wars prequels

00:21:35,200 --> 00:21:38,720
well

00:21:35,919 --> 00:21:40,159
did anyone like star wars prequels but

00:21:38,720 --> 00:21:42,720
that's a different matter

00:21:40,159 --> 00:21:43,280
but certainly you can imagine this being

00:21:42,720 --> 00:21:45,200
a more

00:21:43,280 --> 00:21:48,799
serious case if we were talking

00:21:45,200 --> 00:21:48,799
healthcare records for example

00:21:49,760 --> 00:21:54,640
this is not a new problem and solutions

00:21:53,840 --> 00:21:58,080
to this

00:21:54,640 --> 00:21:58,559
are not new uh going back to 1930s the

00:21:58,080 --> 00:22:01,280
u.s

00:21:58,559 --> 00:22:01,840
census for example stopped publishing

00:22:01,280 --> 00:22:04,320
small

00:22:01,840 --> 00:22:06,159
area data and this is the third problem

00:22:04,320 --> 00:22:10,000
i was talking about earlier you know

00:22:06,159 --> 00:22:13,360
if you have a census tract that

00:22:10,000 --> 00:22:17,360
one family or two families lived in

00:22:13,360 --> 00:22:21,200
and you publish that aggregated data

00:22:17,360 --> 00:22:23,280
well you're not really anonymizing it

00:22:21,200 --> 00:22:26,799
because there aren't that many people

00:22:23,280 --> 00:22:29,840
that could be in that small

00:22:26,799 --> 00:22:32,080
area this by the way is a is a similar

00:22:29,840 --> 00:22:35,120
type of thing to what say red hat does

00:22:32,080 --> 00:22:38,000
with employee surveys where that's

00:22:35,120 --> 00:22:39,840
you ask responds a whole bunch of

00:22:38,000 --> 00:22:43,039
questions including questions

00:22:39,840 --> 00:22:46,320
about their manager and then

00:22:43,039 --> 00:22:49,679
the aggregated data is published

00:22:46,320 --> 00:22:52,840
uh within the company and

00:22:49,679 --> 00:22:57,039
data is also shared directly

00:22:52,840 --> 00:23:01,120
with the individual's

00:22:57,039 --> 00:23:03,120
managers but is only shared with them

00:23:01,120 --> 00:23:05,280
if they have a certain number of direct

00:23:03,120 --> 00:23:07,600
reports because after all if they only

00:23:05,280 --> 00:23:10,480
have one direct report

00:23:07,600 --> 00:23:11,760
the aggregation of the data of their

00:23:10,480 --> 00:23:14,240
direct reports

00:23:11,760 --> 00:23:15,120
is the same thing as how that individual

00:23:14,240 --> 00:23:18,400
answered

00:23:15,120 --> 00:23:19,760
and even if you had two or three direct

00:23:18,400 --> 00:23:23,200
reports

00:23:19,760 --> 00:23:23,600
um you know the manager in aggregate

00:23:23,200 --> 00:23:27,360
gets

00:23:23,600 --> 00:23:29,280
a bad rating and they have a good

00:23:27,360 --> 00:23:30,320
working relationship with two of their

00:23:29,280 --> 00:23:33,520
employees

00:23:30,320 --> 00:23:36,720
and a bad relationship with their third

00:23:33,520 --> 00:23:38,799
employee um you know

00:23:36,720 --> 00:23:41,039
they can make pretty good inferences

00:23:38,799 --> 00:23:44,640
from that

00:23:41,039 --> 00:23:48,080
but let's fast forward to today and

00:23:44,640 --> 00:23:51,480
start and talk about formal privacy

00:23:48,080 --> 00:23:55,600
which the u.s census bureau adopted in

00:23:51,480 --> 00:23:58,480
2020 and specifically

00:23:55,600 --> 00:23:58,880
if the technique of differential privacy

00:23:58,480 --> 00:24:01,919
which

00:23:58,880 --> 00:24:05,360
comes from a 2006

00:24:01,919 --> 00:24:07,200
paper so it's fairly recent uh

00:24:05,360 --> 00:24:10,000
about something called epsilon

00:24:07,200 --> 00:24:14,880
differential privacy specifically

00:24:10,000 --> 00:24:19,120
and the impetus of differential privacy

00:24:14,880 --> 00:24:22,720
is the fact that as you have all these

00:24:19,120 --> 00:24:25,200
very large data sets as machine learning

00:24:22,720 --> 00:24:28,720
has gone extremely powerful

00:24:25,200 --> 00:24:29,919
a lot of these traditional making some

00:24:28,720 --> 00:24:32,159
more ad-hoc

00:24:29,919 --> 00:24:33,200
statistical disclosure limitation

00:24:32,159 --> 00:24:36,400
techniques

00:24:33,200 --> 00:24:40,400
were really starting to be ineffective

00:24:36,400 --> 00:24:44,080
and intuition and intuitions

00:24:40,400 --> 00:24:47,520
about what was sufficient anonymization

00:24:44,080 --> 00:24:50,559
where becoming less and less useful

00:24:47,520 --> 00:24:53,760
and the basic idea here is

00:24:50,559 --> 00:24:54,480
widely shared statistics over a set of

00:24:53,760 --> 00:24:56,320
data

00:24:54,480 --> 00:24:58,840
without revealing anything about the

00:24:56,320 --> 00:25:01,520
individuals that's the real objective

00:24:58,840 --> 00:25:04,559
here some of the requirements

00:25:01,520 --> 00:25:07,440
what be a formal model um

00:25:04,559 --> 00:25:08,640
so again not ad hoc but having math

00:25:07,440 --> 00:25:10,799
against it

00:25:08,640 --> 00:25:12,640
resist both these kind of linkage

00:25:10,799 --> 00:25:15,600
attacks i've talked to

00:25:12,640 --> 00:25:17,039
and i again and hopefully resist future

00:25:15,600 --> 00:25:20,400
attacks that we might not

00:25:17,039 --> 00:25:20,960
know about today and importantly and

00:25:20,400 --> 00:25:24,000
again

00:25:20,960 --> 00:25:26,720
one main impetus behind it is to be

00:25:24,000 --> 00:25:29,039
effective in places where

00:25:26,720 --> 00:25:30,000
there may be a lot of these external

00:25:29,039 --> 00:25:33,520
data sets

00:25:30,000 --> 00:25:34,240
available the basic way differential

00:25:33,520 --> 00:25:37,440
privacy

00:25:34,240 --> 00:25:38,320
works is injecting random data into a

00:25:37,440 --> 00:25:40,320
data set

00:25:38,320 --> 00:25:41,440
in a mathematical rigorous way to

00:25:40,320 --> 00:25:45,679
protect individual

00:25:41,440 --> 00:25:48,720
privacy and the value of the randomness

00:25:45,679 --> 00:25:50,720
trades off privacy and utility and

00:25:48,720 --> 00:25:53,679
accuracy so

00:25:50,720 --> 00:25:55,600
if you look at the right-hand side here

00:25:53,679 --> 00:25:59,279
you the idea is that you have

00:25:55,600 --> 00:26:03,039
data is aggregated by a trusted curator

00:25:59,279 --> 00:26:07,039
alice you have a querier

00:26:03,039 --> 00:26:10,159
who uh you can't trust not to

00:26:07,039 --> 00:26:13,360
try and get at sensitive data

00:26:10,159 --> 00:26:14,320
that querier uh puts query in to the

00:26:13,360 --> 00:26:17,279
curator

00:26:14,320 --> 00:26:18,880
the curator comes up with an answer and

00:26:17,279 --> 00:26:22,400
then the curator

00:26:18,880 --> 00:26:25,679
adds some noise that and that noise

00:26:22,400 --> 00:26:27,200
is mathematically guaranteed to be

00:26:25,679 --> 00:26:30,200
sufficient

00:26:27,200 --> 00:26:31,679
to keep the querier from being able to

00:26:30,200 --> 00:26:35,840
de-anonymize

00:26:31,679 --> 00:26:35,840
uh the subject of their query

00:26:36,159 --> 00:26:41,760
and the other way i think about this

00:26:39,840 --> 00:26:43,679
is that you have this real world

00:26:41,760 --> 00:26:47,679
computation so you've got

00:26:43,679 --> 00:26:51,840
a query a request do some computation

00:26:47,679 --> 00:26:56,400
get output you have a different

00:26:51,840 --> 00:26:59,440
input that that doesn't include

00:26:56,400 --> 00:27:03,200
the data from somebody uh an

00:26:59,440 --> 00:27:04,000
individual you computation analysis of

00:27:03,200 --> 00:27:07,120
that

00:27:04,000 --> 00:27:07,760
output it the difference between those

00:27:07,120 --> 00:27:11,919
two

00:27:07,760 --> 00:27:15,760
is at most a value of epsilon

00:27:11,919 --> 00:27:19,279
um this is so in other words

00:27:15,760 --> 00:27:21,760
you can have a data set and you

00:27:19,279 --> 00:27:23,200
you don't know mathematically whether an

00:27:21,760 --> 00:27:25,760
individual person is

00:27:23,200 --> 00:27:27,440
in that data set or not there are some

00:27:25,760 --> 00:27:30,960
limitations here

00:27:27,440 --> 00:27:34,640
um base rate so basically

00:27:30,960 --> 00:27:38,320
if i know certain public

00:27:34,640 --> 00:27:43,360
uh characteristics about you such as sex

00:27:38,320 --> 00:27:46,640
age and so forth i can infer

00:27:43,360 --> 00:27:49,360
certain things about you such as

00:27:46,640 --> 00:27:50,880
your likelihood to come down come down

00:27:49,360 --> 00:27:53,120
with some disease

00:27:50,880 --> 00:27:54,720
without knowing any private information

00:27:53,120 --> 00:27:58,320
about you um

00:27:54,720 --> 00:28:01,679
having anonymizing data doesn't change

00:27:58,320 --> 00:28:04,880
that fact the noise is

00:28:01,679 --> 00:28:07,840
something of concern uh in that there

00:28:04,880 --> 00:28:10,240
is this idea that you have that you are

00:28:07,840 --> 00:28:11,440
injecting noise into data so at some

00:28:10,240 --> 00:28:14,399
level the

00:28:11,440 --> 00:28:15,200
result isn't as good as it could be

00:28:14,399 --> 00:28:18,080
otherwise

00:28:15,200 --> 00:28:19,520
and this was a concern by a number of

00:28:18,080 --> 00:28:22,799
researchers with the u.s

00:28:19,520 --> 00:28:25,600
census use of technique for example

00:28:22,799 --> 00:28:26,960
subsequent research suggests that in

00:28:25,600 --> 00:28:29,279
general

00:28:26,960 --> 00:28:31,039
you can you can strike a pretty good

00:28:29,279 --> 00:28:34,559
balance here between

00:28:31,039 --> 00:28:35,520
i get between protection of sensitive

00:28:34,559 --> 00:28:39,600
data

00:28:35,520 --> 00:28:43,440
and the accuracy

00:28:39,600 --> 00:28:45,600
of the over all aggregated data set

00:28:43,440 --> 00:28:47,919
the final and probably the most

00:28:45,600 --> 00:28:49,440
difficult problem here is the idea of

00:28:47,919 --> 00:28:52,640
repeated queries

00:28:49,440 --> 00:28:55,679
i mentioned this epsilon uh as

00:28:52,640 --> 00:28:58,159
epsilon value earlier and it's

00:28:55,679 --> 00:28:58,799
probably a better way just to talk about

00:28:58,159 --> 00:29:01,840
is

00:28:58,799 --> 00:29:04,000
you can set a privacy budget the thing

00:29:01,840 --> 00:29:06,799
is you use up that budget

00:29:04,000 --> 00:29:08,880
every time you do a query against the

00:29:06,799 --> 00:29:12,399
sav data which is a particular

00:29:08,880 --> 00:29:13,520
problem in a lot of modern systems where

00:29:12,399 --> 00:29:16,720
you're doing

00:29:13,520 --> 00:29:19,200
digital queries rather than just having

00:29:16,720 --> 00:29:20,880
some aggregated tables that you can uh

00:29:19,200 --> 00:29:23,279
that you can access

00:29:20,880 --> 00:29:24,720
to and this there are ways to deal with

00:29:23,279 --> 00:29:27,600
this so for example

00:29:24,720 --> 00:29:28,960
a randomized subset of data can be can

00:29:27,600 --> 00:29:31,919
be used

00:29:28,960 --> 00:29:33,600
uh and and once you re the privacy

00:29:31,919 --> 00:29:35,840
budget is exceeded you can have a

00:29:33,600 --> 00:29:37,520
different randomized subset of data so

00:29:35,840 --> 00:29:40,799
there are techniques there

00:29:37,520 --> 00:29:42,880
but nonetheless it's a limitation

00:29:40,799 --> 00:29:45,200
now all of this is assumed that we have

00:29:42,880 --> 00:29:46,480
a trusted third party but what if we

00:29:45,200 --> 00:29:48,080
don't

00:29:46,480 --> 00:29:50,240
and that's where the multi-party

00:29:48,080 --> 00:29:51,200
computation i mentioned at the beginning

00:29:50,240 --> 00:29:52,880
comes in

00:29:51,200 --> 00:29:55,600
this is essentially collaborative

00:29:52,880 --> 00:29:58,640
analysis of silo data sets without

00:29:55,600 --> 00:29:59,039
trusting a third party essentially you

00:29:58,640 --> 00:30:02,480
have a

00:29:59,039 --> 00:30:05,600
protocol that is equivalent to

00:30:02,480 --> 00:30:09,039
an incorruptable trusted party and

00:30:05,600 --> 00:30:10,039
conceptually some of this is similar to

00:30:09,039 --> 00:30:12,640
how

00:30:10,039 --> 00:30:13,600
enterprise uh distributed ledger

00:30:12,640 --> 00:30:16,640
technology

00:30:13,600 --> 00:30:19,919
blockchains work and

00:30:16,640 --> 00:30:23,840
the basic mechanism here is that

00:30:19,919 --> 00:30:27,200
parties can jointly compute a function

00:30:23,840 --> 00:30:30,640
on their own an input a share of

00:30:27,200 --> 00:30:33,200
inputs using a protocol without

00:30:30,640 --> 00:30:35,600
information about those inputs being

00:30:33,200 --> 00:30:38,720
revealed

00:30:35,600 --> 00:30:42,240
um the objective here is to preserve

00:30:38,720 --> 00:30:43,279
privacy and correctness uh at the same

00:30:42,240 --> 00:30:45,919
time there there is an

00:30:43,279 --> 00:30:46,320
assumption and there is an assumption

00:30:45,919 --> 00:30:49,520
that

00:30:46,320 --> 00:30:52,640
some number of participants will

00:30:49,520 --> 00:30:57,440
be trying to break the protocol

00:30:52,640 --> 00:31:00,080
uh will collude with each other and

00:30:57,440 --> 00:31:01,200
exactly how you implement multi-party

00:31:00,080 --> 00:31:03,279
computation

00:31:01,200 --> 00:31:05,600
does somewhat depend on the threat model

00:31:03,279 --> 00:31:08,960
that you're assuming so if you're

00:31:05,600 --> 00:31:13,360
assuming that one party

00:31:08,960 --> 00:31:17,120
may by design or by accident

00:31:13,360 --> 00:31:18,960
reveal data if they could

00:31:17,120 --> 00:31:21,200
that is a different threat model than if

00:31:18,960 --> 00:31:22,399
you assume fifty percent of the parties

00:31:21,200 --> 00:31:25,600
may collude

00:31:22,399 --> 00:31:26,399
in a way to sort of pierce the veil so

00:31:25,600 --> 00:31:27,679
to speak

00:31:26,399 --> 00:31:30,559
and then you also need to think about

00:31:27,679 --> 00:31:34,000
how much overhead is involved

00:31:30,559 --> 00:31:37,039
uh it works by you of a protocol

00:31:34,000 --> 00:31:37,840
distributing encrypted uh specifically

00:31:37,039 --> 00:31:41,120
aes

00:31:37,840 --> 00:31:43,279
shares of data uh as i said the

00:31:41,120 --> 00:31:45,039
implementation and efficiency depends on

00:31:43,279 --> 00:31:48,320
threat assumptions

00:31:45,039 --> 00:31:50,080
and although there's different overheads

00:31:48,320 --> 00:31:52,080
with different threat models and so

00:31:50,080 --> 00:31:54,960
forth in general we can say that

00:31:52,080 --> 00:31:57,279
unlike safely homomorphic encryption the

00:31:54,960 --> 00:31:59,039
compute overhead is fairly low

00:31:57,279 --> 00:32:00,679
but there's a law of communications

00:31:59,039 --> 00:32:02,000
overhead you could call these

00:32:00,679 --> 00:32:04,559
cryptographic

00:32:02,000 --> 00:32:05,360
uh communic encrypted communications

00:32:04,559 --> 00:32:08,320
taking

00:32:05,360 --> 00:32:09,519
place between parties uh one specific

00:32:08,320 --> 00:32:12,240
use of this

00:32:09,519 --> 00:32:13,679
i came out of boston university in the

00:32:12,240 --> 00:32:17,039
city of boston

00:32:13,679 --> 00:32:20,640
where uh participating companies

00:32:17,039 --> 00:32:23,679
shared their individual wage data

00:32:20,640 --> 00:32:24,799
but in a way that was protected by

00:32:23,679 --> 00:32:27,760
cryptography so

00:32:24,799 --> 00:32:28,480
they weren't laying any third party

00:32:27,760 --> 00:32:32,880
actually

00:32:28,480 --> 00:32:32,880
see the unencrypted data shares

00:32:34,320 --> 00:32:40,960
if you are interested in this topic

00:32:37,760 --> 00:32:42,159
um we've actually written uh and had

00:32:40,960 --> 00:32:45,600
some interviews

00:32:42,159 --> 00:32:47,039
on red hat research quarterly and uh

00:32:45,600 --> 00:32:49,039
give you a plug this is one of the

00:32:47,039 --> 00:32:50,000
organizations at red hat i do a lot of

00:32:49,039 --> 00:32:53,200
work with

00:32:50,000 --> 00:32:56,399
i suggest you subscribe to our

00:32:53,200 --> 00:32:58,320
newsletter uh there's also a

00:32:56,399 --> 00:33:00,240
related website that has a lot of

00:32:58,320 --> 00:33:02,799
information about ongoing

00:33:00,240 --> 00:33:05,360
uh research happening at boston

00:33:02,799 --> 00:33:08,480
university and other universities

00:33:05,360 --> 00:33:09,600
that are associated uh with with red hat

00:33:08,480 --> 00:33:11,640
research

00:33:09,600 --> 00:33:14,080
um boston university red hat

00:33:11,640 --> 00:33:16,720
collaboratory is where some of that's

00:33:14,080 --> 00:33:17,679
taking place and i mentioned at the very

00:33:16,720 --> 00:33:20,880
beginning

00:33:17,679 --> 00:33:22,399
uh andrew trask an open mind

00:33:20,880 --> 00:33:24,559
uh if you're a sort that likes to get

00:33:22,399 --> 00:33:27,440
their hands dirty in this stuff

00:33:24,559 --> 00:33:28,720
um there is a python set python

00:33:27,440 --> 00:33:30,240
libraries there

00:33:28,720 --> 00:33:32,399
that let you play around with

00:33:30,240 --> 00:33:34,159
differential privacy and multi-party

00:33:32,399 --> 00:33:37,120
computation

00:33:34,159 --> 00:33:40,480
so with that thank you and we have some

00:33:37,120 --> 00:33:40,480
time for some questions

00:33:49,440 --> 00:33:53,840
does anyone out there have any questions

00:34:02,799 --> 00:34:09,280
as i did drop in the chat um

00:34:06,080 --> 00:34:12,480
we the privacy was actually the topic

00:34:09,280 --> 00:34:15,079
at um this week's we we split up

00:34:12,480 --> 00:34:16,320
uh red hat research day into some

00:34:15,079 --> 00:34:19,679
topic-oriented

00:34:16,320 --> 00:34:23,599
shorter sessions this month and

00:34:19,679 --> 00:34:24,960
we um we had a cessna in privacy earlier

00:34:23,599 --> 00:34:28,000
this week and we'll

00:34:24,960 --> 00:34:30,079
have video up from that uh hopefully

00:34:28,000 --> 00:34:31,040
fairly soon so if you're interested in

00:34:30,079 --> 00:34:32,879
this topic

00:34:31,040 --> 00:34:35,839
that'd be a great place to go for some

00:34:32,879 --> 00:34:35,839
more

00:34:40,480 --> 00:34:45,919
well if nobody has any questions

00:34:46,399 --> 00:34:49,440
i will sign off you can always uh reach

00:34:49,119 --> 00:34:53,359
me

00:34:49,440 --> 00:34:56,720
at ghaf uh at redhat.com

00:34:53,359 --> 00:34:58,880
if you have any questions and i'm also

00:34:56,720 --> 00:34:59,920
on twitter with the same handle assuming

00:34:58,880 --> 00:35:02,000
i can get my

00:34:59,920 --> 00:35:03,040
twitter working properly again one of

00:35:02,000 --> 00:35:07,520
these days

00:35:03,040 --> 00:35:07,520

YouTube URL: https://www.youtube.com/watch?v=VrliEn9ktmE


