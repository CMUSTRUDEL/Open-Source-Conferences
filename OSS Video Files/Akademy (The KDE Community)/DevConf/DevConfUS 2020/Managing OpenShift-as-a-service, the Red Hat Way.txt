Title: Managing OpenShift-as-a-service, the Red Hat Way
Publication date: 2020-10-06
Playlist: DevConfUS 2020
Description: 
	Speaker: Jeremy Eder

In this session, Red Hatâ€™s Service Delivery Architects will walk through how cloud.redhat.com can be used to deploy a managed OpenShift 4.x cluster across multiple clouds. We will show how our SRE team manages these clusters at scale, and how cluster telemetry is used to drive engineering and feature priorities. Finally, we will show a worked example of how we evolved an existing application to onboard onto OSD.
Captions: 
	00:00:01,439 --> 00:00:07,759
so hello everybody um

00:00:04,480 --> 00:00:10,240
we will now be showing you

00:00:07,759 --> 00:00:11,920
jeremy eder's talk which is on managing

00:00:10,240 --> 00:00:15,120
openshift as a service

00:00:11,920 --> 00:00:18,640
the red hat way and

00:00:15,120 --> 00:00:23,439
i will be sharing the talk on

00:00:18,640 --> 00:00:27,119
a separate hop-in window you could also

00:00:23,439 --> 00:00:29,920
use the pre-recorded youtube link which

00:00:27,119 --> 00:00:31,840
is available in the chat in case there

00:00:29,920 --> 00:00:32,960
are technical difficulties in the live

00:00:31,840 --> 00:00:35,760
streaming

00:00:32,960 --> 00:00:36,160
and please also keep your hop in window

00:00:35,760 --> 00:00:39,440
open

00:00:36,160 --> 00:00:42,960
in another tab for any live q a

00:00:39,440 --> 00:00:46,160
um and jeremy is a part of the call

00:00:42,960 --> 00:00:46,800
to answer any of your questions again

00:00:46,160 --> 00:00:48,960
there

00:00:46,800 --> 00:00:50,960
i sure wish we were able to be together

00:00:48,960 --> 00:00:53,520
in boston or brno

00:00:50,960 --> 00:00:54,239
for that matter uh to see each other in

00:00:53,520 --> 00:00:57,360
person

00:00:54,239 --> 00:01:00,079
um but i guess this will just have to do

00:00:57,360 --> 00:01:00,879
welcome to this talk managing openshift

00:01:00,079 --> 00:01:03,520
as a service

00:01:00,879 --> 00:01:05,439
the red hat way my name is jeremy eater

00:01:03,520 --> 00:01:07,520
and i work on the

00:01:05,439 --> 00:01:09,280
service delivery team at red hat which

00:01:07,520 --> 00:01:11,280
is the team responsible for

00:01:09,280 --> 00:01:14,240
building and operating red hat's managed

00:01:11,280 --> 00:01:14,240
services products

00:01:17,360 --> 00:01:23,439
red hat sells openshift as a service in

00:01:20,880 --> 00:01:27,520
a variety of ways we sell it on

00:01:23,439 --> 00:01:30,320
we sell it and operate it on

00:01:27,520 --> 00:01:31,920
azure in the form of azure reddit

00:01:30,320 --> 00:01:35,280
openshift

00:01:31,920 --> 00:01:36,560
on aws and gcp in the form of openshift

00:01:35,280 --> 00:01:39,680
dedicated

00:01:36,560 --> 00:01:41,759
on ibm cloud in the form of red hat

00:01:39,680 --> 00:01:43,280
openshift on ibm cloud

00:01:41,759 --> 00:01:46,399
and of course you can manage a

00:01:43,280 --> 00:01:46,399
self-managed openshift

00:01:47,520 --> 00:01:50,880
this talk is going to cover three main

00:01:49,600 --> 00:01:54,399
topics

00:01:50,880 --> 00:01:57,600
how we provision clusters how we manage

00:01:54,399 --> 00:01:59,200
openshift at scale and how we run

00:01:57,600 --> 00:02:02,799
production applications

00:01:59,200 --> 00:02:06,399
on top of one of these managed clusters

00:02:02,799 --> 00:02:09,440
clusters so let's talk about

00:02:06,399 --> 00:02:13,200
how we provision clusters first

00:02:09,440 --> 00:02:14,720
this is uh a diagram of our

00:02:13,200 --> 00:02:16,239
what we call a service delivery

00:02:14,720 --> 00:02:19,440
management plan

00:02:16,239 --> 00:02:24,000
and this is the system by which we

00:02:19,440 --> 00:02:28,959
generate open shift clusters via api

00:02:24,000 --> 00:02:31,760
it starts at the front door which is

00:02:28,959 --> 00:02:32,879
a microservice known as the cluster

00:02:31,760 --> 00:02:36,000
service

00:02:32,879 --> 00:02:38,160
and that guy is

00:02:36,000 --> 00:02:40,400
built and operated by us and when

00:02:38,160 --> 00:02:43,040
coupled by the next component hive

00:02:40,400 --> 00:02:44,560
we can provision in life cycle clusters

00:02:43,040 --> 00:02:47,680
so the cluster services job

00:02:44,560 --> 00:02:50,560
is to life cycle openshift clusters it

00:02:47,680 --> 00:02:52,800
does that by telling hive what to do

00:02:50,560 --> 00:02:55,120
any crud operation against a managed

00:02:52,800 --> 00:02:58,239
openshift cluster starts and ends

00:02:55,120 --> 00:02:58,640
with the cluster service cluster service

00:02:58,239 --> 00:03:00,720
has

00:02:58,640 --> 00:03:02,720
a friend called the account management

00:03:00,720 --> 00:03:04,400
service and that's where we keep our

00:03:02,720 --> 00:03:07,120
business logic

00:03:04,400 --> 00:03:09,200
subscription management feature gates

00:03:07,120 --> 00:03:11,920
terms and conditions

00:03:09,200 --> 00:03:12,640
the clusters change log are all

00:03:11,920 --> 00:03:15,920
implemented

00:03:12,640 --> 00:03:15,920
in ams

00:03:16,080 --> 00:03:19,280
the last major component of the

00:03:17,760 --> 00:03:22,800
management control plane is

00:03:19,280 --> 00:03:25,519
hive and hive has one responsibility

00:03:22,800 --> 00:03:27,040
to actuate crud operations on one or

00:03:25,519 --> 00:03:30,400
more clusters

00:03:27,040 --> 00:03:33,120
um hive is where the openshift install

00:03:30,400 --> 00:03:33,680
itself runs when you create a managed

00:03:33,120 --> 00:03:37,040
cluster

00:03:33,680 --> 00:03:38,959
it's also responsible for adding nodes

00:03:37,040 --> 00:03:40,879
adding an identity provider

00:03:38,959 --> 00:03:44,239
deprovisioning

00:03:40,879 --> 00:03:47,120
and so forth incidentally hive is

00:03:44,239 --> 00:03:48,959
hive also has that same role in red

00:03:47,120 --> 00:04:01,840
hat's advanced cluster

00:03:48,959 --> 00:04:01,840
manager product or acm

00:04:02,560 --> 00:04:06,319
the service delivery team has also built

00:04:04,720 --> 00:04:09,360
a an sdk

00:04:06,319 --> 00:04:12,080
and cli utilities to manage your

00:04:09,360 --> 00:04:13,439
openshift dedicated or some upcoming

00:04:12,080 --> 00:04:15,840
managed products

00:04:13,439 --> 00:04:18,400
and i'll show you that now um let's see

00:04:15,840 --> 00:04:22,400
if i hopefully you can see my terminal

00:04:18,400 --> 00:04:23,360
and i will uh let's just try and fire it

00:04:22,400 --> 00:04:28,160
up

00:04:23,360 --> 00:04:31,199
so the command line utility for uh

00:04:28,160 --> 00:04:32,639
for managing openshift dedicated

00:04:31,199 --> 00:04:33,600
clusters which is where we'll start

00:04:32,639 --> 00:04:36,639
today

00:04:33,600 --> 00:04:39,440
it's called ocm i'm already logged in

00:04:36,639 --> 00:04:40,639
to ocm um i went to cloud.reddit.com i

00:04:39,440 --> 00:04:42,960
created an account

00:04:40,639 --> 00:04:44,639
i got a token for that account i have

00:04:42,960 --> 00:04:48,000
the token in my home directory

00:04:44,639 --> 00:04:50,400
and i use that to log in to uh to log

00:04:48,000 --> 00:04:53,440
into api.openshift.com which is

00:04:50,400 --> 00:04:55,280
where i'm calling it ocm um so yeah

00:04:53,440 --> 00:04:56,960
hopefully you can see that text output

00:04:55,280 --> 00:04:58,080
it's pretty familiar hopefully pretty

00:04:56,960 --> 00:04:58,960
familiar to anyone who's used the

00:04:58,080 --> 00:05:02,320
command line

00:04:58,960 --> 00:05:04,479
um any every crud operation uh is is

00:05:02,320 --> 00:05:08,840
available here or the majority of them

00:05:04,479 --> 00:05:12,240
and let's try and look at creating a

00:05:08,840 --> 00:05:14,960
cluster so first i would go ocm

00:05:12,240 --> 00:05:16,720
cluster create and see some of the

00:05:14,960 --> 00:05:18,800
options that are there for us

00:05:16,720 --> 00:05:20,400
and so we can set an expiration which

00:05:18,800 --> 00:05:22,080
would mean the cluster would be tied

00:05:20,400 --> 00:05:24,880
off at a certain time deleted at a

00:05:22,080 --> 00:05:28,320
certain time use that for testing

00:05:24,880 --> 00:05:30,720
and we can do things like choose aws or

00:05:28,320 --> 00:05:31,199
gcp we can choose which region to run it

00:05:30,720 --> 00:05:33,759
in

00:05:31,199 --> 00:05:35,280
whether it's spread across multiple

00:05:33,759 --> 00:05:37,280
availability zones which would be a

00:05:35,280 --> 00:05:39,280
recommendation for any production system

00:05:37,280 --> 00:05:40,400
uh in that scenario and i'll go through

00:05:39,280 --> 00:05:43,440
a diagram in a second

00:05:40,400 --> 00:05:44,560
but in that in that scenario um you know

00:05:43,440 --> 00:05:47,840
master nodes are

00:05:44,560 --> 00:05:50,080
and compute nodes are split across uh

00:05:47,840 --> 00:05:52,400
three availability zones or regions

00:05:50,080 --> 00:05:55,199
depending on which provider

00:05:52,400 --> 00:05:57,840
vernacular here you're using so in this

00:05:55,199 --> 00:06:00,960
case let's try and create a

00:05:57,840 --> 00:06:06,960
an openshift dedicated cluster on

00:06:00,960 --> 00:06:10,639
uh on gcp so they call their regions

00:06:06,960 --> 00:06:15,280
usc 1 would be fine and i am going to do

00:06:10,639 --> 00:06:19,039
provider gcp

00:06:15,280 --> 00:06:24,479
and let's call it i don't know def conf

00:06:19,039 --> 00:06:24,479
uh j heater uh let's fire that up

00:06:27,520 --> 00:06:31,199
and uh that is the attributes of the

00:06:30,560 --> 00:06:34,319
cluster

00:06:31,199 --> 00:06:35,039
as they are initially created so i've

00:06:34,319 --> 00:06:38,560
got my name

00:06:35,039 --> 00:06:40,800
of the cluster um there's a

00:06:38,560 --> 00:06:41,759
randomly generated sub domain so you get

00:06:40,800 --> 00:06:43,759
a unique url

00:06:41,759 --> 00:06:45,039
we also um and i'll go through this in a

00:06:43,759 --> 00:06:45,759
little more detail in a second but there

00:06:45,039 --> 00:06:49,520
will be a

00:06:45,759 --> 00:06:51,520
let's encrypt certificate that ties back

00:06:49,520 --> 00:06:53,759
to that domain we generate and lifecycle

00:06:51,520 --> 00:06:54,880
that certificate for every cluster

00:06:53,759 --> 00:06:56,319
in this case you know the default

00:06:54,880 --> 00:06:57,919
topology of this cluster is going to be

00:06:56,319 --> 00:07:00,080
three masters three infra

00:06:57,919 --> 00:07:01,280
and infra nodes and uh four compute

00:07:00,080 --> 00:07:03,199
nodes um and you can

00:07:01,280 --> 00:07:05,039
you can obviously change the the number

00:07:03,199 --> 00:07:07,599
of computes so

00:07:05,039 --> 00:07:09,199
um another option i skipped there is you

00:07:07,599 --> 00:07:11,440
could choose the instance type for

00:07:09,199 --> 00:07:13,599
example if you wanted larger instances

00:07:11,440 --> 00:07:15,120
um and you know depending on what your

00:07:13,599 --> 00:07:16,080
workload is and obviously you can change

00:07:15,120 --> 00:07:18,080
the quantity

00:07:16,080 --> 00:07:19,520
of them some of these other options

00:07:18,080 --> 00:07:23,599
aren't too important for this demo

00:07:19,520 --> 00:07:26,319
but um you know there it says multi-az

00:07:23,599 --> 00:07:27,520
is false so yeah because i didn't select

00:07:26,319 --> 00:07:31,039
that option when i was creating the

00:07:27,520 --> 00:07:34,960
cluster so if i do ocm now describe

00:07:31,039 --> 00:07:38,720
uh cluster i could type

00:07:34,960 --> 00:07:40,880
uh dev conf j eater

00:07:38,720 --> 00:07:42,319
you'll see a similar output and this

00:07:40,880 --> 00:07:46,160
will update as the

00:07:42,319 --> 00:07:48,400
cluster begins installing um

00:07:46,160 --> 00:07:50,720
what's happening now and i'll bring up

00:07:48,400 --> 00:07:52,800
bring bring up that diagram again

00:07:50,720 --> 00:07:54,479
what's happening now is my command line

00:07:52,800 --> 00:07:56,080
client talked to

00:07:54,479 --> 00:08:00,000
cluster service which is which is a

00:07:56,080 --> 00:08:03,840
microservice behind api.openshift.com

00:08:00,000 --> 00:08:06,000
and it cluster service took my options

00:08:03,840 --> 00:08:08,800
and turned them into an install config

00:08:06,000 --> 00:08:11,520
for openshift

00:08:08,800 --> 00:08:12,560
from there cluster service generates a

00:08:11,520 --> 00:08:14,879
cr

00:08:12,560 --> 00:08:15,919
so hive has a crd called cluster

00:08:14,879 --> 00:08:17,759
deployment

00:08:15,919 --> 00:08:20,639
the install config gets put into that

00:08:17,759 --> 00:08:24,000
cluster deployment cr

00:08:20,639 --> 00:08:27,039
as a cr and

00:08:24,000 --> 00:08:28,319
it gets sent to hive so that's what's

00:08:27,039 --> 00:08:31,599
happened so far

00:08:28,319 --> 00:08:34,640
um hive watches those crs

00:08:31,599 --> 00:08:37,360
and begins um you know taking action

00:08:34,640 --> 00:08:38,959
once it sees them so in this case it's a

00:08:37,360 --> 00:08:40,880
new cluster create

00:08:38,959 --> 00:08:42,000
using the attributes that i specified on

00:08:40,880 --> 00:08:44,159
the command line

00:08:42,000 --> 00:08:46,160
that are inside the cluster deployment

00:08:44,159 --> 00:08:47,760
hive then takes the installer of the

00:08:46,160 --> 00:08:49,040
version that i chose in this case i

00:08:47,760 --> 00:08:52,080
didn't specify a version so it's

00:08:49,040 --> 00:08:52,080
whatever the default is

00:08:52,399 --> 00:08:56,240
and begins the install with that version

00:08:54,640 --> 00:08:57,360
using the install config that was

00:08:56,240 --> 00:09:00,160
generated by

00:08:57,360 --> 00:09:00,640
cluster service so somewhere in the

00:09:00,160 --> 00:09:02,720
middle

00:09:00,640 --> 00:09:04,000
account the ams i mentioned earlier

00:09:02,720 --> 00:09:06,080
account management service

00:09:04,000 --> 00:09:07,680
i was consulted for subscriptions was

00:09:06,080 --> 00:09:09,360
consulted for the fact that i

00:09:07,680 --> 00:09:10,720
accepted terms and conditions these are

00:09:09,360 --> 00:09:13,279
all you know just standard

00:09:10,720 --> 00:09:14,959
contractual stuff um just verified that

00:09:13,279 --> 00:09:18,480
my account is valid essentially the

00:09:14,959 --> 00:09:22,399
account that i logged into ocm with

00:09:18,480 --> 00:09:24,080
um and yeah and so now hive owns this

00:09:22,399 --> 00:09:25,920
hive is now like i said spinning up an

00:09:24,080 --> 00:09:27,279
install and uh

00:09:25,920 --> 00:09:29,760
uh usually takes a couple minutes to

00:09:27,279 --> 00:09:30,959
begin but after that the normal open

00:09:29,760 --> 00:09:32,959
shift install period

00:09:30,959 --> 00:09:34,720
and um i'll show you that in a minute

00:09:32,959 --> 00:09:36,399
once this cluster is up i've also

00:09:34,720 --> 00:09:37,920
uh created some clusters in advance as

00:09:36,399 --> 00:09:39,040
you can imagine because we just don't

00:09:37,920 --> 00:09:42,000
have enough time

00:09:39,040 --> 00:09:43,200
so we provide an sdk and cli utilities

00:09:42,000 --> 00:09:43,839
to manage these clusters you've seen

00:09:43,200 --> 00:09:46,880
that

00:09:43,839 --> 00:09:47,839
now i mentioned we've got three masters

00:09:46,880 --> 00:09:50,880
three infra

00:09:47,839 --> 00:09:52,399
and uh four computes so let's look at

00:09:50,880 --> 00:09:56,240
what one of these clusters actually

00:09:52,399 --> 00:09:58,800
is um starting with the master nodes

00:09:56,240 --> 00:10:00,560
uh in this case this diagram represents

00:09:58,800 --> 00:10:01,760
a multi-az cluster i did not create a

00:10:00,560 --> 00:10:03,839
multi-az cluster

00:10:01,760 --> 00:10:05,440
but this is what a real production

00:10:03,839 --> 00:10:07,519
openshift dedicated cluster

00:10:05,440 --> 00:10:09,600
um we recommended topology for this is

00:10:07,519 --> 00:10:10,880
the default topology incidentally

00:10:09,600 --> 00:10:13,200
so you have to do anything to get all of

00:10:10,880 --> 00:10:14,000
this uh all of this goodness so zone one

00:10:13,200 --> 00:10:16,480
has the master

00:10:14,000 --> 00:10:17,600
zones two and three um these are within

00:10:16,480 --> 00:10:20,640
the cloud provider

00:10:17,600 --> 00:10:22,000
um are where each other master lives so

00:10:20,640 --> 00:10:25,680
if there's an az failure

00:10:22,000 --> 00:10:28,720
your uh master api for openshift itself

00:10:25,680 --> 00:10:31,200
would remain would remain up there's an

00:10:28,720 --> 00:10:33,120
elb in front of that so yeah load

00:10:31,200 --> 00:10:36,720
balance between those three masters

00:10:33,120 --> 00:10:39,519
um the infra nodes which host uh

00:10:36,720 --> 00:10:41,360
things like prometheus and uh some some

00:10:39,519 --> 00:10:44,480
other support operators

00:10:41,360 --> 00:10:45,839
um the registry several other

00:10:44,480 --> 00:10:48,079
other kind of infrastructure related

00:10:45,839 --> 00:10:49,600
components go there and uh there's also

00:10:48,079 --> 00:10:50,399
three of those nodes and those are also

00:10:49,600 --> 00:10:53,200
spread amongst the

00:10:50,399 --> 00:10:54,480
availability zones um the router of

00:10:53,200 --> 00:10:55,920
course probably the most important part

00:10:54,480 --> 00:10:57,040
of openshift what's the point if there's

00:10:55,920 --> 00:10:58,560
no router

00:10:57,040 --> 00:11:00,800
also lives on those infra nodes in this

00:10:58,560 --> 00:11:02,480
case so

00:11:00,800 --> 00:11:04,720
that's the control plane then the

00:11:02,480 --> 00:11:06,320
compute nodes the the worker pool so to

00:11:04,720 --> 00:11:10,079
say can be of any size

00:11:06,320 --> 00:11:12,800
um up to several hundred nodes and

00:11:10,079 --> 00:11:14,000
uh those are also spread equally amongst

00:11:12,800 --> 00:11:17,040
the availability zones that

00:11:14,000 --> 00:11:20,800
are uh that the cloud provider

00:11:17,040 --> 00:11:23,680
has provided so there's

00:11:20,800 --> 00:11:25,200
yield several different elbs but um one

00:11:23,680 --> 00:11:26,880
in front of the masters and then one in

00:11:25,200 --> 00:11:28,399
front of the infranods which hosts the

00:11:26,880 --> 00:11:31,600
routers as well

00:11:28,399 --> 00:11:34,320
access to these clusters could be via

00:11:31,600 --> 00:11:35,760
public internet which is the default and

00:11:34,320 --> 00:11:37,839
another option i did not specify would

00:11:35,760 --> 00:11:40,880
be to make the cluster private

00:11:37,839 --> 00:11:43,360
um which means that it cannot be

00:11:40,880 --> 00:11:46,480
accessed over the internet

00:11:43,360 --> 00:11:48,160
um it can be accessed only by vpn

00:11:46,480 --> 00:11:50,240
so you would set up a connection between

00:11:48,160 --> 00:11:50,959
your you know your workstation or your

00:11:50,240 --> 00:11:54,320
data center

00:11:50,959 --> 00:11:57,120
more likely and or office and

00:11:54,320 --> 00:11:58,560
the cloud provider so that you could

00:11:57,120 --> 00:12:00,240
connect to these clusters with other

00:11:58,560 --> 00:12:01,519
development clusters or just have some

00:12:00,240 --> 00:12:04,000
some application that needs more

00:12:01,519 --> 00:12:07,760
security um privacy is available to you

00:12:04,000 --> 00:12:12,240
it's represented here by that vpn icon

00:12:07,760 --> 00:12:14,959
so yeah um

00:12:12,240 --> 00:12:17,839
now we've got this cluster spinning up

00:12:14,959 --> 00:12:20,720
you've seen the topology of it

00:12:17,839 --> 00:12:22,399
red hat um service delivery has over the

00:12:20,720 --> 00:12:25,440
last couple of years

00:12:22,399 --> 00:12:26,399
taken on significant amounts of of uh

00:12:25,440 --> 00:12:28,720
the openshift

00:12:26,399 --> 00:12:29,920
product infrastructure to run on top of

00:12:28,720 --> 00:12:32,240
openshift dedicated

00:12:29,920 --> 00:12:33,680
and that's to keep ourselves you know as

00:12:32,240 --> 00:12:35,440
sharp as possible we want the highest

00:12:33,680 --> 00:12:37,200
possible slas for our own stuff we want

00:12:35,440 --> 00:12:38,880
to make sure we have the right

00:12:37,200 --> 00:12:40,560
operational experience with the most

00:12:38,880 --> 00:12:41,920
critical applications it just makes

00:12:40,560 --> 00:12:44,880
sense to do this

00:12:41,920 --> 00:12:46,320
some of the applications that our teams

00:12:44,880 --> 00:12:49,279
run

00:12:46,320 --> 00:12:50,639
are something called telemeter so every

00:12:49,279 --> 00:12:52,639
openshift cluster

00:12:50,639 --> 00:12:54,480
will uh transmit some telemetry data

00:12:52,639 --> 00:12:55,760
about its health about its version

00:12:54,480 --> 00:12:58,399
and so forth that's that's all

00:12:55,760 --> 00:13:01,680
documented in the open shift docs

00:12:58,399 --> 00:13:05,680
back to red hat and we make product

00:13:01,680 --> 00:13:07,839
decisions as well as bug severity kind

00:13:05,680 --> 00:13:08,480
of priority decisions based on data that

00:13:07,839 --> 00:13:10,560
we collect

00:13:08,480 --> 00:13:12,959
from the fleet at large it's fantastic

00:13:10,560 --> 00:13:15,279
from an engineering perspective

00:13:12,959 --> 00:13:17,120
i prefer obviously strongly to be data

00:13:15,279 --> 00:13:19,519
driven and that gives us

00:13:17,120 --> 00:13:20,160
the backing data to prioritize certain

00:13:19,519 --> 00:13:22,720
bugs

00:13:20,160 --> 00:13:24,240
over other ones it also gives us the

00:13:22,720 --> 00:13:26,240
ability to see what's happening in

00:13:24,240 --> 00:13:29,360
pre-release versions of openshift

00:13:26,240 --> 00:13:31,360
in the field at large and to prevent the

00:13:29,360 --> 00:13:32,079
release of things if we see them before

00:13:31,360 --> 00:13:35,600
they're

00:13:32,079 --> 00:13:37,519
stable so anyone testing in um candidate

00:13:35,600 --> 00:13:38,480
channels or we offer nightly channels

00:13:37,519 --> 00:13:40,880
actually through

00:13:38,480 --> 00:13:42,399
uh openshift itself anyone testing those

00:13:40,880 --> 00:13:44,000
versions is still transmitting data back

00:13:42,399 --> 00:13:46,880
to us and we use that data

00:13:44,000 --> 00:13:47,519
to decide as one of the inputs to decide

00:13:46,880 --> 00:13:49,680
um

00:13:47,519 --> 00:13:50,800
whether a particular release is ready to

00:13:49,680 --> 00:13:53,440
go

00:13:50,800 --> 00:13:53,920
so that's telemeter fantastic love it uh

00:13:53,440 --> 00:13:55,199
quay

00:13:53,920 --> 00:13:57,519
you may have heard of quay as the

00:13:55,199 --> 00:13:58,800
registry uh free internet register of

00:13:57,519 --> 00:14:00,880
container registry

00:13:58,800 --> 00:14:02,000
um massive massive amount of data

00:14:00,880 --> 00:14:04,880
transmit there i think

00:14:02,000 --> 00:14:06,639
several petabytes a month um just a

00:14:04,880 --> 00:14:09,120
gargantuan amount of data

00:14:06,639 --> 00:14:10,959
and they use the cdn to serve most of it

00:14:09,120 --> 00:14:12,000
because it's mostly reads obviously but

00:14:10,959 --> 00:14:14,880
that way

00:14:12,000 --> 00:14:16,720
um clusters also run on openshift

00:14:14,880 --> 00:14:21,120
dedicated there's several of them

00:14:16,720 --> 00:14:23,120
and they serve the container images for

00:14:21,120 --> 00:14:26,959
every installation of openshift

00:14:23,120 --> 00:14:28,639
um for some of the operator

00:14:26,959 --> 00:14:30,240
downloads depending on whether it's part

00:14:28,639 --> 00:14:32,320
of openshift or not

00:14:30,240 --> 00:14:34,000
and uh so so it's critical to us being

00:14:32,320 --> 00:14:35,519
able to deliver open shift

00:14:34,000 --> 00:14:37,519
clusters regardless of whether they're

00:14:35,519 --> 00:14:38,240
managed by us or or not you end up

00:14:37,519 --> 00:14:41,120
hitting

00:14:38,240 --> 00:14:42,560
uh koi at some point or another once the

00:14:41,120 --> 00:14:45,120
cluster's up and running

00:14:42,560 --> 00:14:46,079
there's a service called the openshift

00:14:45,120 --> 00:14:47,199
update service

00:14:46,079 --> 00:14:49,360
you may have heard a code name of

00:14:47,199 --> 00:14:49,920
cincinnati it's called openshift update

00:14:49,360 --> 00:14:51,440
service

00:14:49,920 --> 00:14:53,120
i'll talk a little bit more about that

00:14:51,440 --> 00:14:54,639
guy towards the end of the presentation

00:14:53,120 --> 00:14:56,800
but

00:14:54,639 --> 00:14:58,480
essentially every cluster also checks in

00:14:56,800 --> 00:15:00,880
for updates occasionally

00:14:58,480 --> 00:15:02,720
to check in whether or not uh new

00:15:00,880 --> 00:15:05,839
versions are available

00:15:02,720 --> 00:15:07,279
and that service is where we would say

00:15:05,839 --> 00:15:08,560
oh you're running this version

00:15:07,279 --> 00:15:10,000
you know you you would tell us what

00:15:08,560 --> 00:15:10,959
version you're running or your cluster

00:15:10,000 --> 00:15:14,160
would tell us

00:15:10,959 --> 00:15:16,959
and uh and we would give you the

00:15:14,160 --> 00:15:19,440
set of acceptable upgrade paths for you

00:15:16,959 --> 00:15:21,920
based on the version that you're running

00:15:19,440 --> 00:15:23,680
um so that runs also on on openshift

00:15:21,920 --> 00:15:24,959
dedicated and i've got again

00:15:23,680 --> 00:15:26,959
some a little bit more deeper analysis

00:15:24,959 --> 00:15:29,279
of that one later i mentioned ams

00:15:26,959 --> 00:15:30,560
earlier um and cluster service

00:15:29,279 --> 00:15:35,360
incidentally and ocm

00:15:30,560 --> 00:15:35,360
in general the red bar on this slide um

00:15:35,600 --> 00:15:39,600
also runs on openshift dedicated so you

00:15:37,839 --> 00:15:41,519
can see kind of a trend here

00:15:39,600 --> 00:15:43,279
in our entitlement services as well so

00:15:41,519 --> 00:15:46,800
all of that stuff running on

00:15:43,279 --> 00:15:50,000
the same clusters that we uh sell

00:15:46,800 --> 00:15:52,880
same topology you know same sre team

00:15:50,000 --> 00:15:53,519
uh in most of the cases same sra team

00:15:52,880 --> 00:15:55,279
and

00:15:53,519 --> 00:15:56,959
um yeah so we take a lot of pride in

00:15:55,279 --> 00:15:58,800
being able to offer uh

00:15:56,959 --> 00:16:00,560
being kind of the front line so

00:15:58,800 --> 00:16:02,320
delivering open shift goes through

00:16:00,560 --> 00:16:05,040
open shift dedicated kind of the point

00:16:02,320 --> 00:16:07,360
of the point of the conversation

00:16:05,040 --> 00:16:08,720
um so pretty cool and i would consider

00:16:07,360 --> 00:16:12,720
that the red hat way as well

00:16:08,720 --> 00:16:15,199
um that we

00:16:12,720 --> 00:16:15,839
put all of our eggs in our own

00:16:15,199 --> 00:16:18,480
software's

00:16:15,839 --> 00:16:19,279
basket because we trust it and uh that

00:16:18,480 --> 00:16:22,880
gives us the

00:16:19,279 --> 00:16:26,720
best possible signal best possible

00:16:22,880 --> 00:16:30,320
feedback loop so i've gone through

00:16:26,720 --> 00:16:32,079
the cli overview and i've deployed a

00:16:30,320 --> 00:16:34,160
cluster on gcp

00:16:32,079 --> 00:16:35,600
i highly doubt we've got many updates to

00:16:34,160 --> 00:16:37,839
talk about yet

00:16:35,600 --> 00:16:38,959
yeah no updates to it just yet it's

00:16:37,839 --> 00:16:42,880
installing

00:16:38,959 --> 00:16:46,639
um so let's keep moving

00:16:42,880 --> 00:16:46,639
i wanted to show you uh

00:16:46,880 --> 00:16:50,240
this product isn't released yet but it's

00:16:49,440 --> 00:16:52,880
it's called

00:16:50,240 --> 00:16:54,639
amazon red hat open shift um announced

00:16:52,880 --> 00:16:55,759
it back in may i believe it's a

00:16:54,639 --> 00:16:59,680
collaboration

00:16:55,759 --> 00:16:59,680
between red hat and aws to

00:17:00,959 --> 00:17:04,240
my daughter uh it's a collaboration

00:17:03,040 --> 00:17:08,079
between aws

00:17:04,240 --> 00:17:11,199
and red hat to build a first party aws

00:17:08,079 --> 00:17:14,160
service to deliver openshift through

00:17:11,199 --> 00:17:16,480
excuse me through aws's console

00:17:14,160 --> 00:17:18,559
absolutely fantastic been a tremendous

00:17:16,480 --> 00:17:19,679
amount of effort to build and we're

00:17:18,559 --> 00:17:21,280
getting there we're getting there for

00:17:19,679 --> 00:17:23,919
sure

00:17:21,280 --> 00:17:25,039
so the surface is working internally

00:17:23,919 --> 00:17:28,000
working internally here

00:17:25,039 --> 00:17:32,480
um sachi a as i mentioned but let's try

00:17:28,000 --> 00:17:35,360
and deploy one of those clusters

00:17:32,480 --> 00:17:36,559
uh where did we go so that uses slightly

00:17:35,360 --> 00:17:39,280
different command line utility

00:17:36,559 --> 00:17:39,280
incidentally

00:17:39,520 --> 00:17:43,919
but it is still talking to ocm

00:17:44,640 --> 00:17:47,679
octl actually let me show you some of

00:17:46,240 --> 00:17:49,440
the options here they look very familiar

00:17:47,679 --> 00:17:52,480
because they are built on the same

00:17:49,440 --> 00:17:55,120
sdk in the back end the same ocm sdk

00:17:52,480 --> 00:17:55,120
in the back end

00:17:55,520 --> 00:18:01,120
there's some additional features um that

00:17:57,200 --> 00:18:01,120
we've added to moa ctl

00:19:23,039 --> 00:19:26,480
and we can stream those logs back to

00:19:25,360 --> 00:19:28,080
your cli using

00:19:26,480 --> 00:19:29,919
using that watch option there's a bunch

00:19:28,080 --> 00:19:31,760
of stuff a bunch of cool stuff to

00:19:29,919 --> 00:19:34,240
kind of get a better status of what's

00:19:31,760 --> 00:19:35,440
happening of your installation

00:19:34,240 --> 00:19:37,360
uh this is all available on

00:19:35,440 --> 00:19:39,600
cloud.reddit.com so i'll show you

00:19:37,360 --> 00:19:41,600
the web side of this in a minute or two

00:19:39,600 --> 00:19:42,640
but uh let's let's do something a little

00:19:41,600 --> 00:19:46,080
bit different here

00:19:42,640 --> 00:19:48,000
um let's create a cluster in interactive

00:19:46,080 --> 00:19:49,440
mode so i'm going to pass the flag

00:19:48,000 --> 00:19:52,799
double dash

00:19:49,440 --> 00:19:54,160
directive there i'm going to give it uh

00:19:52,799 --> 00:19:56,080
since i specified the name on the

00:19:54,160 --> 00:19:57,440
command line didn't have to um it's

00:19:56,080 --> 00:20:00,000
defaulting to that so

00:19:57,440 --> 00:20:01,440
devconf amro which stands for amazon red

00:20:00,000 --> 00:20:04,640
hat open shift

00:20:01,440 --> 00:20:06,159
j eater that's my cluster name multiple

00:20:04,640 --> 00:20:09,679
availability zones sure

00:20:06,159 --> 00:20:13,200
let's do multiple azs in this one and

00:20:09,679 --> 00:20:14,559
it is now validating my aws credentials

00:20:13,200 --> 00:20:17,679
which are inside

00:20:14,559 --> 00:20:18,400
so i've got the aws cli uh installed and

00:20:17,679 --> 00:20:22,080
configured

00:20:18,400 --> 00:20:24,320
on my workstation and

00:20:22,080 --> 00:20:25,679
the most ctl utility is using those

00:20:24,320 --> 00:20:28,799
credentials

00:20:25,679 --> 00:20:32,640
to to

00:20:28,799 --> 00:20:35,679
authenticate against aws's api

00:20:32,640 --> 00:20:37,440
so that it can create resources within

00:20:35,679 --> 00:20:40,240
that account resources being the

00:20:37,440 --> 00:20:41,200
the openshift cluster itself nodes elbs

00:20:40,240 --> 00:20:43,280
everything else

00:20:41,200 --> 00:20:44,880
um i get a choice of which region to do

00:20:43,280 --> 00:20:49,600
it i prefer

00:20:44,880 --> 00:20:52,240
u.s we west and i can choose a version

00:20:49,600 --> 00:20:54,720
um in this case i don't know four four

00:20:52,240 --> 00:20:56,640
sixteen let's go with that one

00:20:54,720 --> 00:20:58,159
i can choose which type of instances

00:20:56,640 --> 00:21:00,400
that i want um

00:20:58,159 --> 00:21:02,240
probably leave this as well let's change

00:21:00,400 --> 00:21:06,320
it let's go with the r5

00:21:02,240 --> 00:21:08,320
x large instance type um

00:21:06,320 --> 00:21:10,000
yeah that should work and it you'll

00:21:08,320 --> 00:21:11,760
notice here the default number of

00:21:10,000 --> 00:21:14,960
compute nodes is nine

00:21:11,760 --> 00:21:17,120
and the way that ends up is uh

00:21:14,960 --> 00:21:19,039
there are three in each availability

00:21:17,120 --> 00:21:20,640
zone so it spreads them out

00:21:19,039 --> 00:21:23,200
so if there were an easy failure you

00:21:20,640 --> 00:21:24,000
have a certain amount of compute left

00:21:23,200 --> 00:21:27,280
over

00:21:24,000 --> 00:21:28,400
on all the other ac's okay so we'll go

00:21:27,280 --> 00:21:30,480
with nine

00:21:28,400 --> 00:21:31,919
i don't have a reason to customize the

00:21:30,480 --> 00:21:33,760
networking on this cluster so i'm going

00:21:31,919 --> 00:21:36,400
to skip those three

00:21:33,760 --> 00:21:38,159
uh sorry four options uh i'm not gonna

00:21:36,400 --> 00:21:40,880
make the cluster private because i can't

00:21:38,159 --> 00:21:43,600
peer into it from my house very easily

00:21:40,880 --> 00:21:45,280
um but you can also toggle this after

00:21:43,600 --> 00:21:46,960
installation no problem so you can go

00:21:45,280 --> 00:21:49,200
from public to private

00:21:46,960 --> 00:21:50,480
and from private back to public for both

00:21:49,200 --> 00:21:52,799
the api

00:21:50,480 --> 00:21:54,559
uh and the ingress so the router itself

00:21:52,799 --> 00:21:57,600
you can set those independently

00:21:54,559 --> 00:22:00,880
um at any time during

00:21:57,600 --> 00:22:02,480
the cluster's life so

00:22:00,880 --> 00:22:05,280
that's actually uh but there's actually

00:22:02,480 --> 00:22:07,039
a major project for service delivery uh

00:22:05,280 --> 00:22:08,400
a it's something we add on top of

00:22:07,039 --> 00:22:10,000
openshift by

00:22:08,400 --> 00:22:11,679
popular demand from customers saying

00:22:10,000 --> 00:22:13,760
look we're

00:22:11,679 --> 00:22:15,440
we have to have private clusters so

00:22:13,760 --> 00:22:17,120
wasn't a feature of openshift

00:22:15,440 --> 00:22:18,880
at the time when we were building this

00:22:17,120 --> 00:22:21,919
service and

00:22:18,880 --> 00:22:23,600
again because of the popular demand our

00:22:21,919 --> 00:22:26,799
sre team has implemented private

00:22:23,600 --> 00:22:29,520
clusters and exposed it through the ui

00:22:26,799 --> 00:22:31,120
as well so i'm going to choose no here

00:22:29,520 --> 00:22:34,159
which is the default

00:22:31,120 --> 00:22:37,679
and now it is

00:22:34,159 --> 00:22:39,679
creating my amazon red hat open shift

00:22:37,679 --> 00:22:41,039
cluster again not a ga service can't do

00:22:39,679 --> 00:22:45,039
this yourself today

00:22:41,039 --> 00:22:46,400
but it's doing it so i can now type most

00:22:45,039 --> 00:22:48,880
utl

00:22:46,400 --> 00:22:49,679
uh list clusters in this case you can

00:22:48,880 --> 00:22:52,720
see

00:22:49,679 --> 00:22:55,039
before i do that a bunch of output here

00:22:52,720 --> 00:22:57,280
just a little bit of helper uh text most

00:22:55,039 --> 00:22:59,840
etl list clusters is there

00:22:57,280 --> 00:23:01,039
um it asks you if you want to create an

00:22:59,840 --> 00:23:03,039
idp

00:23:01,039 --> 00:23:04,159
i will um probably skip that step for

00:23:03,039 --> 00:23:07,280
this demo but you can

00:23:04,159 --> 00:23:08,480
you can um authenticate to the cluster

00:23:07,280 --> 00:23:11,200
via

00:23:08,480 --> 00:23:12,960
um github is probably the most popular

00:23:11,200 --> 00:23:16,159
one but you can use google or

00:23:12,960 --> 00:23:17,919
ldap um anything that's supported by

00:23:16,159 --> 00:23:20,240
openshift actually

00:23:17,919 --> 00:23:21,039
it tells me my aws account number it

00:23:20,240 --> 00:23:22,880
tells me the

00:23:21,039 --> 00:23:24,720
topology of the cluster if you'll notice

00:23:22,880 --> 00:23:28,799
there's nine compute nodes

00:23:24,720 --> 00:23:31,840
um and it status pending

00:23:28,799 --> 00:23:34,080
so right now pending just means that the

00:23:31,840 --> 00:23:35,919
cluster service has transmitted the

00:23:34,080 --> 00:23:38,000
cluster deployment over to hive and

00:23:35,919 --> 00:23:38,320
we're either waiting on a slot on hive

00:23:38,000 --> 00:23:39,679
or

00:23:38,320 --> 00:23:41,520
it's uh it's going to pick it up you

00:23:39,679 --> 00:23:44,799
know usually picks it up within

00:23:41,520 --> 00:23:47,039
um just a minute a couple minutes so

00:23:44,799 --> 00:23:48,640
if i go to uh oh the last thing i'll say

00:23:47,039 --> 00:23:52,080
and i'll run this in a second

00:23:48,640 --> 00:23:54,000
is um yeah you can get the install

00:23:52,080 --> 00:23:56,559
logs and i'll just show you that briefly

00:23:54,000 --> 00:23:58,240
so most ctl lists clusters i should see

00:23:56,559 --> 00:24:00,240
uh at least one yeah i've got two

00:23:58,240 --> 00:24:02,080
because i tested a test of this

00:24:00,240 --> 00:24:03,679
and delivered one cluster which is

00:24:02,080 --> 00:24:04,799
already running and i'll show you that

00:24:03,679 --> 00:24:06,799
in the ui in a minute

00:24:04,799 --> 00:24:08,000
this one now says state is installing so

00:24:06,799 --> 00:24:09,520
it went from pending

00:24:08,000 --> 00:24:11,600
uh to installing fairly quickly in that

00:24:09,520 --> 00:24:12,960
length of time so now installing means

00:24:11,600 --> 00:24:15,440
there is a pod

00:24:12,960 --> 00:24:17,440
on the hive cluster that is running the

00:24:15,440 --> 00:24:20,240
openshift install

00:24:17,440 --> 00:24:23,120
for the version that i chose which was

00:24:20,240 --> 00:24:26,080
00:24:23,120 --> 00:24:26,720
with the install configuration that i

00:24:26,080 --> 00:24:29,760
passed

00:24:26,720 --> 00:24:31,440
uh that i also passed so it was the r5x

00:24:29,760 --> 00:24:32,240
large it was multi-az it was the number

00:24:31,440 --> 00:24:35,520
of compute nodes

00:24:32,240 --> 00:24:37,120
and so forth um that was all passed

00:24:35,520 --> 00:24:37,760
through to hive and it is and it is it

00:24:37,120 --> 00:24:39,520
is now running

00:24:37,760 --> 00:24:40,880
it says it's installing so that takes a

00:24:39,520 --> 00:24:43,279
bit and

00:24:40,880 --> 00:24:46,080
if i wanted to know what it's doing i

00:24:43,279 --> 00:24:50,400
can just simply copy this command

00:24:46,080 --> 00:24:52,880
and run most ctl logs install

00:24:50,400 --> 00:24:54,320
so there's either install or uninstall

00:24:52,880 --> 00:24:55,679
the name of the cluster and then watch

00:24:54,320 --> 00:24:56,480
and then i mentioned earlier that we're

00:24:55,679 --> 00:25:01,039
streaming the

00:24:56,480 --> 00:25:03,520
hive of the install logs from the hive

00:25:01,039 --> 00:25:05,200
cluster back through cluster service and

00:25:03,520 --> 00:25:05,840
eventually to your command line client

00:25:05,200 --> 00:25:09,360
here

00:25:05,840 --> 00:25:11,279
so we can watch the installation logs

00:25:09,360 --> 00:25:12,720
and these are just regular open shift

00:25:11,279 --> 00:25:14,000
install logs you can see right now

00:25:12,720 --> 00:25:17,120
because it's uh

00:25:14,000 --> 00:25:20,320
an aws cluster it's creating a whole ton

00:25:17,120 --> 00:25:22,559
of aws resources in here for us and this

00:25:20,320 --> 00:25:26,640
will continue for a bit

00:25:22,559 --> 00:25:28,400
while the cluster is being installed

00:25:26,640 --> 00:25:30,640
okay so while this cluster is installing

00:25:28,400 --> 00:25:32,080
i can safely control c out of that

00:25:30,640 --> 00:25:33,760
uh the install will still progress in

00:25:32,080 --> 00:25:37,600
the background

00:25:33,760 --> 00:25:37,600
it should still be an installing state

00:25:38,000 --> 00:25:42,159
i wanted to show you uh i mentioned i

00:25:40,880 --> 00:25:44,320
stood up another uh

00:25:42,159 --> 00:25:45,279
cluster as part of this demo i wanted to

00:25:44,320 --> 00:25:47,679
show you how

00:25:45,279 --> 00:25:48,720
you can edit this cluster um edit an

00:25:47,679 --> 00:25:51,039
existing cluster

00:25:48,720 --> 00:25:52,880
uh the attributes of it after it's

00:25:51,039 --> 00:25:56,080
running

00:25:52,880 --> 00:26:00,320
so let's try uh most ctl edit

00:25:56,080 --> 00:26:00,320
cluster oh boy

00:26:04,159 --> 00:26:08,080
this is the one um that's already been

00:26:06,159 --> 00:26:11,520
installed

00:26:08,080 --> 00:26:13,520
and i will also edit this

00:26:11,520 --> 00:26:14,799
in interactive mode you could just as

00:26:13,520 --> 00:26:17,840
easily

00:26:14,799 --> 00:26:20,880
um pass

00:26:17,840 --> 00:26:23,520
whatever flags you wanted to to the uh

00:26:20,880 --> 00:26:24,880
the edit function right now but i'll do

00:26:23,520 --> 00:26:27,679
an i'll do

00:26:24,880 --> 00:26:27,679
interactive mode

00:26:28,400 --> 00:26:30,960
so the first thing it'll ask me in

00:26:29,600 --> 00:26:32,080
interactive mode is whether i want to

00:26:30,960 --> 00:26:34,080
flip the cluster to

00:26:32,080 --> 00:26:35,760
private mode i mentioned don't want to

00:26:34,080 --> 00:26:39,679
do that

00:26:35,760 --> 00:26:42,159
for myself enable cluster admins that's

00:26:39,679 --> 00:26:43,120
defaulted to yes which means i can

00:26:42,159 --> 00:26:47,039
assign

00:26:43,120 --> 00:26:49,520
that role to any user that's in my idp

00:26:47,039 --> 00:26:50,640
for me for example my github user could

00:26:49,520 --> 00:26:54,559
be cluster admin

00:26:50,640 --> 00:26:56,799
within this cluster i'll leave that

00:26:54,559 --> 00:26:59,760
changing the number of compute nodes by

00:26:56,799 --> 00:27:02,960
the way this demo cluster that i created

00:26:59,760 --> 00:27:06,000
prior to this yesterday

00:27:02,960 --> 00:27:07,919
was not multi-az so that's why

00:27:06,000 --> 00:27:10,320
it has five compute notes on it now

00:27:07,919 --> 00:27:14,400
otherwise it would be a minimum of nine

00:27:10,320 --> 00:27:16,240
but in this case let's make it um

00:27:14,400 --> 00:27:18,400
you know six compute nodes so we'll add

00:27:16,240 --> 00:27:18,400
one

00:27:18,720 --> 00:27:24,559
and it's going to do that so now let's

00:27:21,679 --> 00:27:27,840
see if i can bring up

00:27:24,559 --> 00:27:30,880
um a

00:27:27,840 --> 00:27:30,880
web browser

00:27:32,960 --> 00:27:38,960
and show you what it looks like

00:27:36,640 --> 00:27:38,960
there

00:27:40,080 --> 00:27:43,760
okay so this is cloudvredit.com i

00:27:42,240 --> 00:27:47,120
mentioned um ocm

00:27:43,760 --> 00:27:49,440
and this is uh this

00:27:47,120 --> 00:27:50,399
is the list of clusters that i

00:27:49,440 --> 00:27:52,720
personally have

00:27:50,399 --> 00:27:53,600
installed in this environment um so i've

00:27:52,720 --> 00:27:56,799
got a couple that are

00:27:53,600 --> 00:28:00,080
already installed the uh the gcp one and

00:27:56,799 --> 00:28:01,760
the um amra one are

00:28:00,080 --> 00:28:04,240
uh you can currently see there in

00:28:01,760 --> 00:28:05,679
installing mode in the locations or

00:28:04,240 --> 00:28:08,880
regions that i chose so

00:28:05,679 --> 00:28:11,120
for example eu west 3 translates to to

00:28:08,880 --> 00:28:13,919
paris

00:28:11,120 --> 00:28:15,200
um and u.s east 1 where i put the gcp

00:28:13,919 --> 00:28:18,960
cluster is

00:28:15,200 --> 00:28:20,799
monk's corner south carolina us

00:28:18,960 --> 00:28:22,000
so those are both installing i've got my

00:28:20,799 --> 00:28:23,120
other clusters that are already

00:28:22,000 --> 00:28:26,080
installed

00:28:23,120 --> 00:28:27,039
um let's pull up the cluster that i just

00:28:26,080 --> 00:28:30,320
scaled

00:28:27,039 --> 00:28:33,440
which is a j eater amro 45

00:28:30,320 --> 00:28:35,679
10. and if i dig into this cluster

00:28:33,440 --> 00:28:38,320
uh you'll see it's ready and that it has

00:28:35,679 --> 00:28:40,559
now uh the number six compute nodes

00:28:38,320 --> 00:28:41,919
um you can see this is desired nodes

00:28:40,559 --> 00:28:43,840
versus actual

00:28:41,919 --> 00:28:45,679
so when you scale a cluster it takes a

00:28:43,840 --> 00:28:47,200
couple minutes to provision it and to

00:28:45,679 --> 00:28:48,399
join it to the openshift cluster which

00:28:47,200 --> 00:28:49,200
is what's happening in the background

00:28:48,399 --> 00:28:52,000
here

00:28:49,200 --> 00:28:53,120
um and so it actual nodes is still five

00:28:52,000 --> 00:28:54,640
and then a couple of minutes from now

00:28:53,120 --> 00:28:56,640
this will update itself and there'll be

00:28:54,640 --> 00:28:58,080
there'll be six nodes um in that cluster

00:28:56,640 --> 00:29:00,320
and you would see that represented in

00:28:58,080 --> 00:29:03,039
the number of vcpus and the number

00:29:00,320 --> 00:29:05,679
uh the amount of memory available so

00:29:03,039 --> 00:29:08,000
this is what ocm the web ui looks like

00:29:05,679 --> 00:29:10,840
i mentioned earlier the uh that the

00:29:08,000 --> 00:29:13,600
account management service

00:29:10,840 --> 00:29:14,480
um how to change log for a cluster we

00:29:13,600 --> 00:29:17,840
call it the

00:29:14,480 --> 00:29:20,559
um service log internally but

00:29:17,840 --> 00:29:22,480
you can see here that the history of

00:29:20,559 --> 00:29:26,159
this cluster so for example

00:29:22,480 --> 00:29:29,520
uh it was born on the 18th of september

00:29:26,159 --> 00:29:30,559
and uh we changed the number of compute

00:29:29,520 --> 00:29:34,640
nodes

00:29:30,559 --> 00:29:34,640
we added an identity provider i did

00:29:34,880 --> 00:29:39,360
um i added my user to cluster admin so

00:29:38,720 --> 00:29:42,240
you can see here

00:29:39,360 --> 00:29:43,120
this this jeremy eater that's my github

00:29:42,240 --> 00:29:45,600
handle

00:29:43,120 --> 00:29:46,720
and uh it's been added to cluster admin

00:29:45,600 --> 00:29:48,640
so

00:29:46,720 --> 00:29:50,399
when i log in to openshift so i'll do it

00:29:48,640 --> 00:29:52,000
in just a sec i will use it will

00:29:50,399 --> 00:29:54,720
authenticate against github

00:29:52,000 --> 00:29:56,080
and i will have full administrative

00:29:54,720 --> 00:29:59,120
rights on this cluster

00:29:56,080 --> 00:30:00,640
and then finally um adding more uh

00:29:59,120 --> 00:30:03,039
more compute nodes you can see here this

00:30:00,640 --> 00:30:04,240
is the action that we just took

00:30:03,039 --> 00:30:06,399
where the compute nodes have been

00:30:04,240 --> 00:30:08,320
updated to six

00:30:06,399 --> 00:30:09,840
so i'll just quickly show you it's not

00:30:08,320 --> 00:30:10,799
super important but i will quickly show

00:30:09,840 --> 00:30:14,159
you how

00:30:10,799 --> 00:30:16,240
uh to um

00:30:14,159 --> 00:30:17,600
log into the cluster uh incidentally so

00:30:16,240 --> 00:30:19,120
this isn't really an ocm demo but

00:30:17,600 --> 00:30:21,120
there's a bunch of other features here

00:30:19,120 --> 00:30:22,720
for example connecting your vpc or

00:30:21,120 --> 00:30:24,640
making things public or private

00:30:22,720 --> 00:30:26,000
is really just as simple as clicking

00:30:24,640 --> 00:30:28,480
this uh

00:30:26,000 --> 00:30:29,679
ticking this box and clicking change you

00:30:28,480 --> 00:30:30,240
could do that via the command line as

00:30:29,679 --> 00:30:31,679
well

00:30:30,240 --> 00:30:33,520
if you remember earlier there was a

00:30:31,679 --> 00:30:36,080
prompt for private cluster in the

00:30:33,520 --> 00:30:37,600
interactive mode of most etl edit it's

00:30:36,080 --> 00:30:39,520
the same thing as this website it's all

00:30:37,600 --> 00:30:41,679
talking back to the same apis

00:30:39,520 --> 00:30:43,520
so if i click excuse me so if i click um

00:30:41,679 --> 00:30:44,880
open console

00:30:43,520 --> 00:30:47,039
i will be presented with a normal

00:30:44,880 --> 00:30:49,360
openshift login screen

00:30:47,039 --> 00:30:50,159
you'll notice that there was no ssl

00:30:49,360 --> 00:30:52,559
pop-up

00:30:50,159 --> 00:30:54,320
that is because we um when we provision

00:30:52,559 --> 00:30:56,640
clusters we go as i mentioned earlier we

00:30:54,320 --> 00:30:57,279
give you a dns zone a route 53 dns zone

00:30:56,640 --> 00:31:00,720
or

00:30:57,279 --> 00:31:03,679
or google their dns provider and

00:31:00,720 --> 00:31:04,559
we um we go and request a certificate

00:31:03,679 --> 00:31:06,399
for you

00:31:04,559 --> 00:31:08,559
and again we life cycle that certificate

00:31:06,399 --> 00:31:11,919
so it won't expire it's all automated

00:31:08,559 --> 00:31:12,720
um here you see a list of the idps that

00:31:11,919 --> 00:31:14,480
are configured

00:31:12,720 --> 00:31:15,840
in my case i named it github one so i'll

00:31:14,480 --> 00:31:19,840
click there

00:31:15,840 --> 00:31:19,840
um and i will log in

00:31:24,159 --> 00:31:29,200
and i believe i will need my token sorry

00:31:27,440 --> 00:31:32,240
about this

00:31:29,200 --> 00:31:35,440
let's get a token going you should all

00:31:32,240 --> 00:31:38,880
be running two-factor

00:31:35,440 --> 00:31:38,880
let's see github

00:31:42,960 --> 00:31:47,440
cool we are in and so yeah here's the

00:31:46,720 --> 00:31:49,919
openshift

00:31:47,440 --> 00:31:51,440
console this is what every openshift

00:31:49,919 --> 00:31:53,200
cluster looks like that's what

00:31:51,440 --> 00:31:55,039
an openshift dedicated cluster looks

00:31:53,200 --> 00:31:58,159
like that's what a uh

00:31:55,039 --> 00:31:59,919
in this case this is an amro cluster um

00:31:58,159 --> 00:32:01,600
such as a default screen and i'm in as

00:31:59,919 --> 00:32:04,799
cluster admin through that uh

00:32:01,600 --> 00:32:06,720
idp integration cool

00:32:04,799 --> 00:32:08,240
so here you could look at things like if

00:32:06,720 --> 00:32:10,240
i looked at events just quickly

00:32:08,240 --> 00:32:12,320
just to show you what's happening um in

00:32:10,240 --> 00:32:16,159
the background you can see that

00:32:12,320 --> 00:32:19,279
uh my cluster is

00:32:16,159 --> 00:32:21,200
adding that node so

00:32:19,279 --> 00:32:23,519
it's all coming together and i believe

00:32:21,200 --> 00:32:26,799
if i went to compute and then

00:32:23,519 --> 00:32:28,799
um machine sets

00:32:26,799 --> 00:32:30,240
you'll see i have six of six machines

00:32:28,799 --> 00:32:32,159
here i don't know if you can

00:32:30,240 --> 00:32:34,000
be a little bit tough to read but i

00:32:32,159 --> 00:32:35,279
thought it has finally added that node

00:32:34,000 --> 00:32:36,480
to the cluster there's my point we

00:32:35,279 --> 00:32:39,840
started with five

00:32:36,480 --> 00:32:43,760
um and now we've got six so cool

00:32:39,840 --> 00:32:45,919
let's move on so back to the

00:32:43,760 --> 00:32:47,600
terminal and let's get the slides going

00:32:45,919 --> 00:32:51,120
again

00:32:47,600 --> 00:32:51,120
so how do we manage

00:32:52,159 --> 00:32:57,120
how do we manage these clusters let's

00:32:53,919 --> 00:32:59,600
talk about that

00:32:57,120 --> 00:33:01,679
so first of all openshift itself

00:32:59,600 --> 00:33:04,799
openshift4 is a drastic change

00:33:01,679 --> 00:33:07,200
in operability from openshift 3.

00:33:04,799 --> 00:33:08,240
we through the acquisition of coreos

00:33:07,200 --> 00:33:10,000
have a much

00:33:08,240 --> 00:33:11,760
more automated self-driving self-healing

00:33:10,000 --> 00:33:13,440
platform than we had with openshift 3.

00:33:11,760 --> 00:33:15,200
any users of openshift 3 should be able

00:33:13,440 --> 00:33:16,880
to vouch for that

00:33:15,200 --> 00:33:19,679
day 2 operations such as i mentioned

00:33:16,880 --> 00:33:22,480
life cycling certificates

00:33:19,679 --> 00:33:23,600
excuse me monitoring and so forth are

00:33:22,480 --> 00:33:26,080
things that we

00:33:23,600 --> 00:33:27,279
add onto the platform and of course a

00:33:26,080 --> 00:33:30,480
deeper understanding of the

00:33:27,279 --> 00:33:32,880
cloud provider internals and

00:33:30,480 --> 00:33:33,840
you know the intersection of openshift

00:33:32,880 --> 00:33:37,519
and the cloud

00:33:33,840 --> 00:33:39,840
provider is of you know is of interest

00:33:37,519 --> 00:33:42,240
to any platform team

00:33:39,840 --> 00:33:43,039
so some of the day two stuff that we add

00:33:42,240 --> 00:33:46,480
to

00:33:43,039 --> 00:33:48,000
uh uh to help build this service and to

00:33:46,480 --> 00:33:48,880
manage these clusters we have so the

00:33:48,000 --> 00:33:51,679
left side of this

00:33:48,880 --> 00:33:52,000
uh of this chart is what we have running

00:33:51,679 --> 00:33:54,640
on

00:33:52,000 --> 00:33:57,519
our management plane clusters so we have

00:33:54,640 --> 00:34:00,159
the hive service obviously

00:33:57,519 --> 00:34:02,480
we also have a pagerduty operator and so

00:34:00,159 --> 00:34:06,399
our sre team uses pagerduty for

00:34:02,480 --> 00:34:10,320
managing alerts and so forth so we um

00:34:06,399 --> 00:34:13,280
wrote an operator that um

00:34:10,320 --> 00:34:15,040
that configures pagerduty and then we'll

00:34:13,280 --> 00:34:18,800
uh lay that secret down

00:34:15,040 --> 00:34:19,839
inside the uh the actual cluster we also

00:34:18,800 --> 00:34:22,000
you know have a let's encrypt

00:34:19,839 --> 00:34:25,280
certificate service and

00:34:22,000 --> 00:34:26,240
um the dns integration that i mentioned

00:34:25,280 --> 00:34:28,159
earlier

00:34:26,240 --> 00:34:30,000
so that runs inside our management plan

00:34:28,159 --> 00:34:31,839
what's running on the cluster is we lay

00:34:30,000 --> 00:34:33,919
down certain permissions

00:34:31,839 --> 00:34:36,240
that allow us to manage the cluster that

00:34:33,919 --> 00:34:36,240
allow

00:34:36,800 --> 00:34:40,480
different tiers of roles to be you know

00:34:39,599 --> 00:34:42,879
configured by

00:34:40,480 --> 00:34:44,639
uh the owner of the cluster so i

00:34:42,879 --> 00:34:45,359
mentioned cluster admins earlier there's

00:34:44,639 --> 00:34:46,960
also a

00:34:45,359 --> 00:34:49,040
sort of a mid tier called dedicated

00:34:46,960 --> 00:34:50,879
admins can do can install operators can

00:34:49,040 --> 00:34:54,560
do quite a bit but not

00:34:50,879 --> 00:34:54,560
the full kind of cluster admin stuff

00:34:54,800 --> 00:35:00,480
we also do obviously backups of fcd for

00:34:58,000 --> 00:35:01,520
recovery of the cluster itself not the

00:35:00,480 --> 00:35:03,119
application data but

00:35:01,520 --> 00:35:04,560
recovery of the cluster itself and we

00:35:03,119 --> 00:35:07,119
have a set of alerts

00:35:04,560 --> 00:35:08,720
um that we also add on top of what the

00:35:07,119 --> 00:35:12,160
openshift product

00:35:08,720 --> 00:35:13,760
adds so just wanted to talk a little bit

00:35:12,160 --> 00:35:16,000
more deeply about monitoring

00:35:13,760 --> 00:35:18,800
observability is key for any sre team

00:35:16,000 --> 00:35:20,320
it's near and dear to my heart as well

00:35:18,800 --> 00:35:22,560
but this is what the monitoring stack

00:35:20,320 --> 00:35:24,160
looks like on every open shift cluster

00:35:22,560 --> 00:35:25,440
whether it's managed or not looks like

00:35:24,160 --> 00:35:27,760
this

00:35:25,440 --> 00:35:28,960
so the openshift team develops all of

00:35:27,760 --> 00:35:30,960
these components

00:35:28,960 --> 00:35:32,240
and uh yeah they're controlled by an

00:35:30,960 --> 00:35:33,839
operator called the cluster monitoring

00:35:32,240 --> 00:35:35,920
operator

00:35:33,839 --> 00:35:38,160
all of this data or i should say it's

00:35:35,920 --> 00:35:39,680
all of this data is available in cluster

00:35:38,160 --> 00:35:41,359
um tremendous amounts of data are

00:35:39,680 --> 00:35:44,000
available in clusters and they're

00:35:41,359 --> 00:35:47,280
visualized in the web ui as well

00:35:44,000 --> 00:35:48,800
um and they also call back to

00:35:47,280 --> 00:35:50,720
our telemetry service that i mentioned

00:35:48,800 --> 00:35:52,320
earlier so a subset of this data just

00:35:50,720 --> 00:35:54,800
kind of the

00:35:52,320 --> 00:35:56,400
key performance indicators that you know

00:35:54,800 --> 00:35:58,000
we as a software vendor are interested

00:35:56,400 --> 00:35:59,280
in learning across our fleet

00:35:58,000 --> 00:36:01,359
such as the health of the cluster

00:35:59,280 --> 00:36:02,400
whether there's uh operators are

00:36:01,359 --> 00:36:04,000
malfunctioning

00:36:02,400 --> 00:36:05,680
whether an upgrade succeeded that kind

00:36:04,000 --> 00:36:07,119
of stuff

00:36:05,680 --> 00:36:08,800
but this is what it looks like so we run

00:36:07,119 --> 00:36:11,520
other components under that operator

00:36:08,800 --> 00:36:14,640
like the prometheus node exporter

00:36:11,520 --> 00:36:16,079
prometheus itself obviously keep state

00:36:14,640 --> 00:36:17,200
metrics which gives us some additional

00:36:16,079 --> 00:36:21,760
data collection

00:36:17,200 --> 00:36:26,160
uh grafana is on each cluster and um

00:36:21,760 --> 00:36:28,320
and alert manager so

00:36:26,160 --> 00:36:29,280
the this product also configures alert

00:36:28,320 --> 00:36:31,520
manager with

00:36:29,280 --> 00:36:32,400
um that's where the pagerduty

00:36:31,520 --> 00:36:35,280
integration

00:36:32,400 --> 00:36:38,000
um lives and uh yeah i mentioned the

00:36:35,280 --> 00:36:39,680
telemeter client also which is what's uh

00:36:38,000 --> 00:36:41,359
running on each cluster that you know

00:36:39,680 --> 00:36:42,560
transmits those uh key performance

00:36:41,359 --> 00:36:46,000
indicators

00:36:42,560 --> 00:36:46,720
of uh the cluster's health so that's how

00:36:46,000 --> 00:36:50,240
the cluster

00:36:46,720 --> 00:36:52,560
monitoring operator works and our team

00:36:50,240 --> 00:36:54,400
um yes here's a snapshot of the the

00:36:52,560 --> 00:36:56,240
telemetry data that comes back

00:36:54,400 --> 00:36:58,720
uh i've redacted anything sensitive

00:36:56,240 --> 00:37:00,560
obviously but this cluster is running on

00:36:58,720 --> 00:37:03,599
aws it's version four four

00:37:00,560 --> 00:37:05,440
its availability is currently 100 um

00:37:03,599 --> 00:37:06,800
within the time frame obviously of this

00:37:05,440 --> 00:37:09,440
of this snapshot

00:37:06,800 --> 00:37:10,000
and uh there's 42 nodes in this cluster

00:37:09,440 --> 00:37:11,520
so

00:37:10,000 --> 00:37:13,200
kind of a and it's running version 4

00:37:11,520 --> 00:37:14,079
416. so this is the kind of data that we

00:37:13,200 --> 00:37:17,280
get back

00:37:14,079 --> 00:37:18,560
we're looking at the number of uh

00:37:17,280 --> 00:37:20,160
nodes in the cluster to see kind of

00:37:18,560 --> 00:37:21,280
where's the sweet spot for what most

00:37:20,160 --> 00:37:24,480
people are

00:37:21,280 --> 00:37:26,320
doing with these clusters you know

00:37:24,480 --> 00:37:27,520
how often updates are performed you can

00:37:26,320 --> 00:37:30,640
see this one had an upgrade

00:37:27,520 --> 00:37:32,240
that was done um about a week ago

00:37:30,640 --> 00:37:34,160
so that would be it would be pretty

00:37:32,240 --> 00:37:36,800
close to the latest version

00:37:34,160 --> 00:37:38,720
um and then the number of scd objects we

00:37:36,800 --> 00:37:40,640
also pull that back because

00:37:38,720 --> 00:37:42,240
you know if that goes sideways um that's

00:37:40,640 --> 00:37:45,119
that's a pretty

00:37:42,240 --> 00:37:46,400
i should say uh easy to detect issue

00:37:45,119 --> 00:37:48,880
that you can prevent

00:37:46,400 --> 00:37:50,000
um you can prevent issues with your

00:37:48,880 --> 00:37:51,680
cluster if you're if you're tracking

00:37:50,000 --> 00:37:54,480
that sort of thing

00:37:51,680 --> 00:37:56,079
cool so moving on i mentioned the pager

00:37:54,480 --> 00:37:58,000
duty integration earlier so we've got

00:37:56,079 --> 00:38:00,160
that whole monitoring stack

00:37:58,000 --> 00:38:01,359
across every cluster that is under

00:38:00,160 --> 00:38:04,160
management

00:38:01,359 --> 00:38:05,680
they all feed back to pagerduty and uh

00:38:04,160 --> 00:38:08,400
yeah it's a 24x7

00:38:05,680 --> 00:38:09,280
sre team as you might imagine so people

00:38:08,400 --> 00:38:13,200
all over the place

00:38:09,280 --> 00:38:15,040
um um you know again using pagerduty to

00:38:13,200 --> 00:38:17,200
consolidate the alerts and take action

00:38:15,040 --> 00:38:21,520
upon them

00:38:17,200 --> 00:38:24,000
okay cool so let's finally talk about

00:38:21,520 --> 00:38:25,040
um how a production application that

00:38:24,000 --> 00:38:27,760
we're running

00:38:25,040 --> 00:38:28,640
i mentioned earlier the openshift update

00:38:27,760 --> 00:38:32,160
service

00:38:28,640 --> 00:38:34,720
um i called it cincinnati and uh

00:38:32,160 --> 00:38:36,400
it's just a code name i wanted to use

00:38:34,720 --> 00:38:39,760
that as the example today

00:38:36,400 --> 00:38:42,400
um you by the time you're seeing this

00:38:39,760 --> 00:38:43,599
um you already missed unfortunately or

00:38:42,400 --> 00:38:47,200
maybe some didn't

00:38:43,599 --> 00:38:47,920
uh a deep dive into the cincinnati

00:38:47,200 --> 00:38:51,760
onboarding

00:38:47,920 --> 00:38:56,560
into um our managed environment

00:38:51,760 --> 00:38:58,400
uh by aditya canardi and vadim rodkowski

00:38:56,560 --> 00:39:00,160
the recording will be available it was a

00:38:58,400 --> 00:39:01,760
couple hours ago i think

00:39:00,160 --> 00:39:03,040
but they did a deep dive into this and

00:39:01,760 --> 00:39:03,920
i'll cover it a little bit of a higher

00:39:03,040 --> 00:39:06,079
level but this is

00:39:03,920 --> 00:39:07,760
a an application that runs on our

00:39:06,079 --> 00:39:09,680
managed fleet all of the stuff that i

00:39:07,760 --> 00:39:11,599
mentioned earlier is true and

00:39:09,680 --> 00:39:14,320
there's there's some really cool stuff

00:39:11,599 --> 00:39:15,839
that this team has done together uh

00:39:14,320 --> 00:39:17,760
with so the development team has done

00:39:15,839 --> 00:39:20,000
along with the sra team to

00:39:17,760 --> 00:39:21,280
um improve the applications performance

00:39:20,000 --> 00:39:22,480
and improve the applications

00:39:21,280 --> 00:39:24,160
availability

00:39:22,480 --> 00:39:26,240
uh improve their development processes

00:39:24,160 --> 00:39:29,359
to kind of suit um

00:39:26,240 --> 00:39:30,800
what the ops teams the sr8 teams need

00:39:29,359 --> 00:39:32,640
which is a tremendous example highly

00:39:30,800 --> 00:39:35,599
recommend watching their video

00:39:32,640 --> 00:39:37,200
um okay so anyway uh the openshift

00:39:35,599 --> 00:39:39,599
update services job is

00:39:37,200 --> 00:39:40,400
simply to return the available they call

00:39:39,599 --> 00:39:42,560
them edges

00:39:40,400 --> 00:39:44,079
the available upgrade paths for the

00:39:42,560 --> 00:39:45,680
version that you're on right now so i'm

00:39:44,079 --> 00:39:48,160
on four four sixteen

00:39:45,680 --> 00:39:49,040
what is my path to four or five uh this

00:39:48,160 --> 00:39:52,160
service will

00:39:49,040 --> 00:39:54,960
will calculate and uh and just

00:39:52,160 --> 00:39:57,280
send that back to you so stateless uh

00:39:54,960 --> 00:39:59,599
stateless application i should mention

00:39:57,280 --> 00:40:01,520
it also ties back to uh here's called

00:39:59,599 --> 00:40:03,440
tool booth that's the code name for

00:40:01,520 --> 00:40:04,640
uh the account management service but

00:40:03,440 --> 00:40:07,839
when you call in

00:40:04,640 --> 00:40:12,240
you know we verify obviously who uh

00:40:07,839 --> 00:40:12,240
entitlements and so forth at that time

00:40:12,880 --> 00:40:15,920
so that is the overview of the openshift

00:40:15,200 --> 00:40:18,400
update

00:40:15,920 --> 00:40:18,400
service

00:40:19,359 --> 00:40:22,720
yeah kind of went through most of this

00:40:20,800 --> 00:40:23,599
earlier there's different channels i

00:40:22,720 --> 00:40:24,880
don't know how much

00:40:23,599 --> 00:40:27,200
we need to get into here but you can

00:40:24,880 --> 00:40:28,480
have a candidate channel fast channel

00:40:27,200 --> 00:40:31,119
stable channel

00:40:28,480 --> 00:40:32,720
um and these are all different levels of

00:40:31,119 --> 00:40:33,839
risk associated which kind of came from

00:40:32,720 --> 00:40:36,640
core os's

00:40:33,839 --> 00:40:38,319
approach to in tectonic where there's

00:40:36,640 --> 00:40:41,440
kind of different levels of

00:40:38,319 --> 00:40:43,599
risk for your cluster so

00:40:41,440 --> 00:40:45,839
i think the intent is that folks will

00:40:43,599 --> 00:40:45,839
run

00:40:47,200 --> 00:40:53,599
some portion of their clusters on

00:40:50,880 --> 00:40:54,480
the fast channel um some portion of

00:40:53,599 --> 00:40:58,160
their clusters

00:40:54,480 --> 00:41:00,960
on um even on candidate channel if they

00:40:58,160 --> 00:41:02,960
have the flexibility to do that

00:41:00,960 --> 00:41:04,240
and use that to make sure that their

00:41:02,960 --> 00:41:04,800
applications are ready for the next

00:41:04,240 --> 00:41:07,839
version

00:41:04,800 --> 00:41:09,520
that's all just like beta testing so

00:41:07,839 --> 00:41:12,560
yeah that's what cincinnati does

00:41:09,520 --> 00:41:13,920
um cool so

00:41:12,560 --> 00:41:15,839
some of the stuff that this team came up

00:41:13,920 --> 00:41:17,599
with together is really just gold mining

00:41:15,839 --> 00:41:19,040
uh bringing this application on and i

00:41:17,599 --> 00:41:20,880
know a lot of users of kubernetes

00:41:19,040 --> 00:41:24,160
struggle with these areas i

00:41:20,880 --> 00:41:25,680
uh so

00:41:24,160 --> 00:41:27,680
let's let's look at what the perfect

00:41:25,680 --> 00:41:30,400
deployment might be um

00:41:27,680 --> 00:41:32,319
there's it's a partnership and you have

00:41:30,400 --> 00:41:33,440
to understand what kubernetes is capable

00:41:32,319 --> 00:41:35,599
of how it behaves

00:41:33,440 --> 00:41:36,560
as well as how openshift applies updates

00:41:35,599 --> 00:41:38,079
to a cluster

00:41:36,560 --> 00:41:40,000
in order to make sure your application

00:41:38,079 --> 00:41:41,599
takes advantage of the right features

00:41:40,000 --> 00:41:43,040
and your application is designed in such

00:41:41,599 --> 00:41:44,640
a way to to

00:41:43,040 --> 00:41:48,240
leverage those features in the in the

00:41:44,640 --> 00:41:49,040
most pragmatic way quite honestly um so

00:41:48,240 --> 00:41:50,800
so

00:41:49,040 --> 00:41:52,160
for example in the stateless application

00:41:50,800 --> 00:41:54,400
we don't have to worry about storage

00:41:52,160 --> 00:41:55,599
um so which this application is again in

00:41:54,400 --> 00:41:58,319
memory database

00:41:55,599 --> 00:41:58,960
um it's it's okay to run more than one

00:41:58,319 --> 00:42:02,079
replica

00:41:58,960 --> 00:42:03,920
and have that load balanced behind

00:42:02,079 --> 00:42:06,640
aha proxy which is what openshift uses

00:42:03,920 --> 00:42:06,640
to load balance

00:42:07,839 --> 00:42:12,560
sorry applications and you know our

00:42:10,800 --> 00:42:14,400
request and limit set

00:42:12,560 --> 00:42:16,880
what is your upgrade strategy so when

00:42:14,400 --> 00:42:18,880
you roll out a new version

00:42:16,880 --> 00:42:21,520
does it drain connections and slowly

00:42:18,880 --> 00:42:25,119
roll slowly roll out that change

00:42:21,520 --> 00:42:27,440
across your replica set um

00:42:25,119 --> 00:42:30,079
liveness and readiness probes are very

00:42:27,440 --> 00:42:31,760
important so if a pot is malfunctioning

00:42:30,079 --> 00:42:33,440
the router will eventually not send

00:42:31,760 --> 00:42:35,839
traffic to those pods

00:42:33,440 --> 00:42:37,280
uh pod disruption budgets also important

00:42:35,839 --> 00:42:39,920
so that the application stays

00:42:37,280 --> 00:42:42,640
up um if nodes are you know coming and

00:42:39,920 --> 00:42:45,680
going and so forth uh it will maintain

00:42:42,640 --> 00:42:47,040
um availability of the application and

00:42:45,680 --> 00:42:49,119
that can actually interrupt

00:42:47,040 --> 00:42:50,800
or delay i should say upgrades if

00:42:49,119 --> 00:42:52,319
there's only so many nodes on a cluster

00:42:50,800 --> 00:42:54,240
and depending on how your

00:42:52,319 --> 00:42:55,520
disruption budget is configured

00:42:54,240 --> 00:42:57,040
anti-affinity just to

00:42:55,520 --> 00:42:58,720
make sure the pods are not running on

00:42:57,040 --> 00:43:00,880
the same node so

00:42:58,720 --> 00:43:01,839
you know let's say you have uh 10

00:43:00,880 --> 00:43:03,680
replicas

00:43:01,839 --> 00:43:04,960
if six or seven of them end up on one

00:43:03,680 --> 00:43:07,200
node and that node

00:43:04,960 --> 00:43:08,000
um runs in two issues then that capacity

00:43:07,200 --> 00:43:09,920
would be

00:43:08,000 --> 00:43:11,440
um you know you would be down that that

00:43:09,920 --> 00:43:12,800
percent of capacity which is maybe not

00:43:11,440 --> 00:43:15,040
maybe not what you want

00:43:12,800 --> 00:43:16,560
so using pod anti-affinity will spread

00:43:15,040 --> 00:43:18,720
them around

00:43:16,560 --> 00:43:20,160
um kind of yeah hopefully

00:43:18,720 --> 00:43:22,000
self-explanatory

00:43:20,160 --> 00:43:24,079
and then things like using deprecated

00:43:22,000 --> 00:43:26,000
apis over the last couple of releases

00:43:24,079 --> 00:43:28,960
the kubernetes upstream has been

00:43:26,000 --> 00:43:30,000
um you know rotating out beta apis and

00:43:28,960 --> 00:43:31,920
alpha apis and

00:43:30,000 --> 00:43:33,040
that that may actually be in use in your

00:43:31,920 --> 00:43:36,480
applications

00:43:33,040 --> 00:43:38,400
um and being able to know that those

00:43:36,480 --> 00:43:40,640
are used by an application is important

00:43:38,400 --> 00:43:43,040
as well so

00:43:40,640 --> 00:43:44,480
that's the kubernetes side the

00:43:43,040 --> 00:43:46,400
application also has to make

00:43:44,480 --> 00:43:47,760
some changes or at least be designed in

00:43:46,400 --> 00:43:49,359
a

00:43:47,760 --> 00:43:51,920
in a way that can take advantage of them

00:43:49,359 --> 00:43:53,200
so the application needs to export loads

00:43:51,920 --> 00:43:55,520
of metrics

00:43:53,200 --> 00:43:57,359
hopefully based on slis and slos that

00:43:55,520 --> 00:43:58,880
have business agreement with them

00:43:57,359 --> 00:44:00,640
um and that the application has been

00:43:58,880 --> 00:44:03,119
tested to be able to sustain

00:44:00,640 --> 00:44:04,240
its own slos during a cluster upgrade so

00:44:03,119 --> 00:44:05,839
for example

00:44:04,240 --> 00:44:08,079
um i mentioned you know you've got 10

00:44:05,839 --> 00:44:10,560
replicas if two or three of them go

00:44:08,079 --> 00:44:12,160
away because they're being rescheduled

00:44:10,560 --> 00:44:15,040
or a node fails

00:44:12,160 --> 00:44:15,680
you know d can you maintain those slos

00:44:15,040 --> 00:44:17,839
while

00:44:15,680 --> 00:44:18,720
your while the cluster is either under

00:44:17,839 --> 00:44:20,839
stress

00:44:18,720 --> 00:44:22,560
or while upgrades are occurring for

00:44:20,839 --> 00:44:25,520
example

00:44:22,560 --> 00:44:25,839
so how can we possibly make it easier to

00:44:25,520 --> 00:44:28,880
do

00:44:25,839 --> 00:44:31,599
all of that stuff one of the

00:44:28,880 --> 00:44:32,400
sre teams in service delivery has put

00:44:31,599 --> 00:44:35,440
together

00:44:32,400 --> 00:44:37,280
a prototype we're just simply calling it

00:44:35,440 --> 00:44:38,800
deployment validation operator

00:44:37,280 --> 00:44:40,400
uh just in the last couple of weeks have

00:44:38,800 --> 00:44:42,240
put it in operator hub

00:44:40,400 --> 00:44:43,839
and it is going to so we have some

00:44:42,240 --> 00:44:46,480
internal projects that already do

00:44:43,839 --> 00:44:47,599
this but our idea is to put that all out

00:44:46,480 --> 00:44:49,280
in the community and then become

00:44:47,599 --> 00:44:51,520
consumers of that community code

00:44:49,280 --> 00:44:53,440
just uh you know the right way quite

00:44:51,520 --> 00:44:55,119
honestly it's a good example of that

00:44:53,440 --> 00:44:57,440
so we had some kind of early starts in

00:44:55,119 --> 00:44:58,160
this area when we onboard new products

00:44:57,440 --> 00:44:59,760
into

00:44:58,160 --> 00:45:01,200
our managed services fleet we want them

00:44:59,760 --> 00:45:02,720
to take advantage of

00:45:01,200 --> 00:45:03,839
all of these things so that their uh

00:45:02,720 --> 00:45:05,200
their application has the highest

00:45:03,839 --> 00:45:07,119
possible availability

00:45:05,200 --> 00:45:09,040
and and you know we're not receiving

00:45:07,119 --> 00:45:10,400
pages for application downtime when

00:45:09,040 --> 00:45:13,280
uh we could have easily avoided it by

00:45:10,400 --> 00:45:13,760
config so we want a way to be able to

00:45:13,280 --> 00:45:16,480
tell

00:45:13,760 --> 00:45:17,040
development teams uh programmatically

00:45:16,480 --> 00:45:19,520
what

00:45:17,040 --> 00:45:21,119
uh what their current state of uh you

00:45:19,520 --> 00:45:22,319
know what their current state is and

00:45:21,119 --> 00:45:25,200
capabilities

00:45:22,319 --> 00:45:25,839
and continuously validate that on on a

00:45:25,200 --> 00:45:28,720
cluster

00:45:25,839 --> 00:45:30,640
so the deployment validation operator is

00:45:28,720 --> 00:45:32,079
an early start

00:45:30,640 --> 00:45:33,359
um and i would encourage you to take a

00:45:32,079 --> 00:45:35,680
look at it if you just go to operator

00:45:33,359 --> 00:45:37,599
hub and search for it you'll find it

00:45:35,680 --> 00:45:40,800
um an early start in this area it's only

00:45:37,599 --> 00:45:42,960
got a handful of checks in it right now

00:45:40,800 --> 00:45:44,720
but the teams are slowly moving um the

00:45:42,960 --> 00:45:49,280
majority of those checks

00:45:44,720 --> 00:45:49,280
into the into the dvo operator

00:45:58,960 --> 00:46:03,520
yeah so i hope the dvo uh gets some more

00:46:01,760 --> 00:46:04,480
eyeballs on it to kind of make it useful

00:46:03,520 --> 00:46:07,760
for more

00:46:04,480 --> 00:46:09,359
more workloads and scenarios um

00:46:07,760 --> 00:46:11,520
so our clusters are ready you can see

00:46:09,359 --> 00:46:12,640
them in the console here uh they've gone

00:46:11,520 --> 00:46:14,640
from installing to

00:46:12,640 --> 00:46:17,040
ready and if i were to look on the

00:46:14,640 --> 00:46:18,800
command line and my logs have

00:46:17,040 --> 00:46:20,480
ended with an install completed

00:46:18,800 --> 00:46:23,520
successfully message

00:46:20,480 --> 00:46:24,160
so in this talk we've uh you know we've

00:46:23,520 --> 00:46:25,920
covered

00:46:24,160 --> 00:46:27,920
how we provision clusters we talked

00:46:25,920 --> 00:46:29,119
about the microservices behind our

00:46:27,920 --> 00:46:31,520
service

00:46:29,119 --> 00:46:34,079
delivery management plan i've showed you

00:46:31,520 --> 00:46:34,079
how to

00:46:34,480 --> 00:46:37,599
showed you how to provision clusters on

00:46:36,960 --> 00:46:39,760
gcp

00:46:37,599 --> 00:46:41,280
and talked about an upcoming amazon red

00:46:39,760 --> 00:46:43,440
hat openshift product

00:46:41,280 --> 00:46:44,880
and talk about the monitoring and

00:46:43,440 --> 00:46:46,560
observability that we have built into

00:46:44,880 --> 00:46:48,000
openshift and the pagerduty integration

00:46:46,560 --> 00:46:49,119
that our sra team used that's how we

00:46:48,000 --> 00:46:52,079
manage the fleet of scale

00:46:49,119 --> 00:46:53,920
and then finally we talked about the

00:46:52,079 --> 00:46:54,640
openshift update service and how we've

00:46:53,920 --> 00:46:57,520
tried to

00:46:54,640 --> 00:46:58,480
help that team deliver the the perfect

00:46:57,520 --> 00:47:00,079
deployment

00:46:58,480 --> 00:47:02,319
so that's what i have for you today

00:47:00,079 --> 00:47:02,800
thanks for joining um i appreciate your

00:47:02,319 --> 00:47:05,200
time

00:47:02,800 --> 00:47:07,520
and if you like what you've seen

00:47:05,200 --> 00:47:08,319
consider subscribing and smash that like

00:47:07,520 --> 00:47:11,839
button

00:47:08,319 --> 00:47:11,839
happy devconf

00:47:16,880 --> 00:47:19,280

YouTube URL: https://www.youtube.com/watch?v=eljzty4Ca4I


