Title: An Introduction to hpxMP... - Tianyi Zhang - CppCon 2019
Publication date: 2019-10-23
Playlist: CppCon 2019 Lightning Talks
Description: 
	http://CppCon.org
—
Discussion & Comments: https://www.reddit.com/r/cpp/
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2019
—
Lightning Talk
— 
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:09,150 --> 00:00:14,520
hello everyone I'm tae and I'm from

00:00:12,190 --> 00:00:14,520
Lucia

00:00:15,250 --> 00:00:22,480
and also a volunteer STP become 19 and

00:00:19,290 --> 00:00:25,510
today I want to share a project called

00:00:22,480 --> 00:00:28,869
HP XMP which is a modern OpenMP

00:00:25,510 --> 00:00:31,720
implementation using HP X and where HP x

00:00:28,869 --> 00:00:33,880
is an asynchronous many tasks system so

00:00:31,720 --> 00:00:37,570
hard won't give a talk about HP X this

00:00:33,880 --> 00:00:40,000
morning I hope you all enjoyed it and so

00:00:37,570 --> 00:00:43,300
first let's see a very simple Oh clampy

00:00:40,000 --> 00:00:46,720
program so here's a PI calculation

00:00:43,300 --> 00:00:49,450
program and we just use a for loop to

00:00:46,720 --> 00:00:52,750
add all the areas together so that we

00:00:49,450 --> 00:00:55,300
can get the result of hi and this is the

00:00:52,750 --> 00:00:56,830
code for for the using for loops and

00:00:55,300 --> 00:00:59,350
which is not running in parallel

00:00:56,830 --> 00:01:04,839
however this open peeve is a very simple

00:00:59,350 --> 00:01:07,960
line of OMP parallel for we can tell the

00:01:04,839 --> 00:01:11,560
compilers how we want to make the loop

00:01:07,960 --> 00:01:14,470
running in parallel so it will utilize

00:01:11,560 --> 00:01:16,930
the as many courses you have or you how

00:01:14,470 --> 00:01:22,300
many cores you want to use which makes

00:01:16,930 --> 00:01:24,130
things much faster and GCC and clan all

00:01:22,300 --> 00:01:26,950
have their own implementation of the

00:01:24,130 --> 00:01:30,729
automobile including an inhale so why we

00:01:26,950 --> 00:01:33,490
need hv XMP the reason is many

00:01:30,729 --> 00:01:36,340
asynchronous men it has six systems are

00:01:33,490 --> 00:01:39,670
fighting this open piece so for example

00:01:36,340 --> 00:01:41,380
if we want to use HP X and open IP in

00:01:39,670 --> 00:01:43,930
the same scenario they will start

00:01:41,380 --> 00:01:47,229
fighting each other which call which is

00:01:43,930 --> 00:01:51,370
causing the performance disaster so we

00:01:47,229 --> 00:01:55,090
have some ideas too so here is the

00:01:51,370 --> 00:01:58,240
structure of the OMP like libraries

00:01:55,090 --> 00:02:00,520
which they expose the directives to the

00:01:58,240 --> 00:02:03,490
users and some open V libraries to the

00:02:00,520 --> 00:02:06,180
users and in the system later the

00:02:03,490 --> 00:02:10,720
open-pit runtime will handle the

00:02:06,180 --> 00:02:13,689
threading and other things so you think

00:02:10,720 --> 00:02:16,540
it using HP XMP we just simply replace

00:02:13,689 --> 00:02:18,939
the open p runtime library using HP XMP

00:02:16,540 --> 00:02:21,510
runtime library where HP x will handle

00:02:18,939 --> 00:02:25,390
the synchronization and threading stuff

00:02:21,510 --> 00:02:28,330
so for users there will be no no changes

00:02:25,390 --> 00:02:29,230
for them and we started the project on

00:02:28,330 --> 00:02:32,200
github and

00:02:29,230 --> 00:02:33,790
we added some pragmas supported which is

00:02:32,200 --> 00:02:38,560
open P parallel forint

00:02:33,790 --> 00:02:40,239
of OMB task and so open p5 was released

00:02:38,560 --> 00:02:42,610
last year and we are trying to keep up

00:02:40,239 --> 00:02:45,700
with the most recent standard so we add

00:02:42,610 --> 00:02:50,470
the task groups tough reduction into HP

00:02:45,700 --> 00:02:53,620
XMP as well and this introduces how to

00:02:50,470 --> 00:02:56,500
use HD XMP so if you have a program

00:02:53,620 --> 00:02:58,959
written in bit written with open P you

00:02:56,500 --> 00:03:02,410
simply compile with your compiler

00:02:58,959 --> 00:03:04,150
program with clay or GCC with no changes

00:03:02,410 --> 00:03:06,970
and when you run your program you need

00:03:04,150 --> 00:03:10,620
to LT preload the open HP XMP shared

00:03:06,970 --> 00:03:13,950
library so that HP XMP will handle the

00:03:10,620 --> 00:03:17,920
HP X and it should XMP will handle the

00:03:13,950 --> 00:03:21,760
threading stuff for you so for example

00:03:17,920 --> 00:03:24,849
we have two cores so HP x wheel starts

00:03:21,760 --> 00:03:27,459
to operation scissors thread and when it

00:03:24,849 --> 00:03:31,390
sees the oprima on be parallel it will

00:03:27,459 --> 00:03:34,510
starts it will create as many as the

00:03:31,390 --> 00:03:36,579
threads you want it to create so HP axis

00:03:34,510 --> 00:03:39,430
read will be created and if you have

00:03:36,579 --> 00:03:41,530
task HP x will also create HP x read and

00:03:39,430 --> 00:03:44,169
schedules of thread on to the operating

00:03:41,530 --> 00:03:49,030
system threat so here are some examples

00:03:44,169 --> 00:03:51,250
and till we have we have a parallel for

00:03:49,030 --> 00:03:53,769
loop which print the all the hollow word

00:03:51,250 --> 00:03:56,709
from different thread since I'm telling

00:03:53,769 --> 00:03:59,280
the telling the system we are using two

00:03:56,709 --> 00:04:01,840
threads so half of the loops are

00:03:59,280 --> 00:04:05,260
performed by thread 1 and half of them

00:04:01,840 --> 00:04:07,510
are performed by third 0 so difference

00:04:05,260 --> 00:04:10,389
red is sailing how to work to the is

00:04:07,510 --> 00:04:12,160
printing hello world and because OpenMP

00:04:10,389 --> 00:04:16,120
is about parallel computing so the

00:04:12,160 --> 00:04:19,630
performance is pretty important and we

00:04:16,120 --> 00:04:22,560
tested it using blaze mark and some

00:04:19,630 --> 00:04:25,240
opener but Aruna program beat us cute so

00:04:22,560 --> 00:04:27,190
we discovered that for small green size

00:04:25,240 --> 00:04:28,960
we have something we have really have

00:04:27,190 --> 00:04:31,240
some work to do to improve the

00:04:28,960 --> 00:04:34,330
performance but for larger grain size we

00:04:31,240 --> 00:04:36,120
are pretty good at it and we we are

00:04:34,330 --> 00:04:38,500
actually faster than the GCC

00:04:36,120 --> 00:04:41,110
implementation and but still slower than

00:04:38,500 --> 00:04:43,240
the client limitation

00:04:41,110 --> 00:04:45,250
so here's the acknowledgment and my

00:04:43,240 --> 00:04:46,570
contact information thank you all for

00:04:45,250 --> 00:04:50,779
coming

00:04:46,570 --> 00:04:50,779

YouTube URL: https://www.youtube.com/watch?v=Ct7dT0o3wJc


