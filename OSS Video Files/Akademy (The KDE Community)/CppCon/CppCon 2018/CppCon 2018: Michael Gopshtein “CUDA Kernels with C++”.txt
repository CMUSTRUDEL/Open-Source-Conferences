Title: CppCon 2018: Michael Gopshtein “CUDA Kernels with C++”
Publication date: 2018-10-11
Playlist: CppCon 2018
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2018
—
CUDA is a parallel computing platform for a generic computing on Nvidia GPUs. We will focus on CUDA C/C++ programming, covering in detail various types of GPU memory (Shared/Global/Texture/ConstShared/Pinned), optimization of the work distribution between CUDA “threads”, precompiled CUDA code VS Runtime Compilation. 
Often the CUDA code is written in C style, missing the opportunity to use safer and more readable C++ constructs. In the spirit of the conference a special focus will be given to the use of modern C++ features in the CUDA applications inside the device (“kernel”) code, host code, and more importantly – which of the C++ paradigms can cross the bridge between the two worlds. 
— 
Michael Gopshtein, Eyeway Vision
Framework TL

Michael is a software engineer with more that 15 years of C++ experience. He worked on various performance-bound projects, like load generation on servers, network sniffing and packet analysis. In the recent years Michael is focused on Augmented Reality challenges in Eyeway Vision, where he got interested in accelerating Computer Vision algorithms on GPU, in particular in profiling and optimizing CUDA code. Michael is a big C++ enthusiast, a co-organizer of a local “Core C++” user group and occasional blogger.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,879
my name is Michael Westen I work for IVA

00:00:03,419 --> 00:00:08,490
vision will recreate augmented reality

00:00:05,879 --> 00:00:10,050
glasses the next generation of the

00:00:08,490 --> 00:00:13,139
glasses and we believe that eventually

00:00:10,050 --> 00:00:16,920
AR will replace all other devices or

00:00:13,139 --> 00:00:19,260
other displays and we use CUDA to

00:00:16,920 --> 00:00:20,760
accelerate Albert was on on GPU devices

00:00:19,260 --> 00:00:24,600
especially for computer vision

00:00:20,760 --> 00:00:27,420
algorithms now basically what is CUDA

00:00:24,600 --> 00:00:29,880
it's a platform by Nvidia which allows

00:00:27,420 --> 00:00:32,219
you to create applications and

00:00:29,880 --> 00:00:36,020
algorithms which run on NVIDIA GPU

00:00:32,219 --> 00:00:38,910
devices and it's heavily used for

00:00:36,020 --> 00:00:40,760
algorithms the chrysella computer vision

00:00:38,910 --> 00:00:44,219
but also for machine learning

00:00:40,760 --> 00:00:48,960
simulations and other read amending

00:00:44,219 --> 00:00:53,550
algorithms and it contains support for

00:00:48,960 --> 00:00:56,309
different languages it comes with a set

00:00:53,550 --> 00:00:59,250
of tools like the web the compilers

00:00:56,309 --> 00:01:01,199
debuggers profilers and also with a set

00:00:59,250 --> 00:01:04,409
of libraries which you can which can

00:01:01,199 --> 00:01:06,750
start using out of the box and we will

00:01:04,409 --> 00:01:08,670
be focusing on on CUDA C++

00:01:06,750 --> 00:01:11,970
I'm not going to do a different

00:01:08,670 --> 00:01:13,110
reduction into the CUDA C++ but even if

00:01:11,970 --> 00:01:16,170
you are not familiar with it

00:01:13,110 --> 00:01:19,229
I will explain all examples so it should

00:01:16,170 --> 00:01:21,869
be easy to follow the session so we'll

00:01:19,229 --> 00:01:24,659
use like the hello world example for

00:01:21,869 --> 00:01:27,119
CUDA algorithms it's an addition of two

00:01:24,659 --> 00:01:30,990
vectors we have vector a and vector B

00:01:27,119 --> 00:01:32,939
and we want to create an vector C which

00:01:30,990 --> 00:01:35,549
is an element wise some of the vectors

00:01:32,939 --> 00:01:38,159
and the separation it has a nice

00:01:35,549 --> 00:01:40,860
property but we can calculate each cell

00:01:38,159 --> 00:01:44,520
in C independently of other cells and

00:01:40,860 --> 00:01:47,909
then with CUDA we'll assign each each

00:01:44,520 --> 00:01:50,159
cell in C a separate CUDA thread which

00:01:47,909 --> 00:01:52,110
will handle the calculation and then

00:01:50,159 --> 00:01:54,570
we'll span out many many such CUDA

00:01:52,110 --> 00:01:55,409
threads and hopefully we'll complete the

00:01:54,570 --> 00:01:59,640
calculation

00:01:55,409 --> 00:02:03,000
faster now storm could have thread it's

00:01:59,640 --> 00:02:06,140
not exactly like I said that you may be

00:02:03,000 --> 00:02:08,759
used from from students and from

00:02:06,140 --> 00:02:10,830
operating systems but for the sake of

00:02:08,759 --> 00:02:12,049
the session the knowledge is is good

00:02:10,830 --> 00:02:14,849
enough

00:02:12,049 --> 00:02:18,120
so the way you express it with CUDA who

00:02:14,849 --> 00:02:20,250
is called a c c++ is that first you

00:02:18,120 --> 00:02:23,879
create a function in this case it's an

00:02:20,250 --> 00:02:25,560
kernel which runs on a GPU and you have

00:02:23,879 --> 00:02:29,459
to start it with special keyboard global

00:02:25,560 --> 00:02:32,310
but otherwise it looks like a regular

00:02:29,459 --> 00:02:36,030
function it has all the parameters right

00:02:32,310 --> 00:02:38,400
the vectors C and B and the first line

00:02:36,030 --> 00:02:40,739
in the body it calculates the index

00:02:38,400 --> 00:02:42,689
which it should operate on and will not

00:02:40,739 --> 00:02:45,390
talk about this block indexes and thread

00:02:42,689 --> 00:02:47,519
indexes what's important is that each

00:02:45,390 --> 00:02:50,549
could thread will get a unique index

00:02:47,519 --> 00:02:53,760
into the data and then in second line we

00:02:50,549 --> 00:02:57,420
do a regular calculation of some of the

00:02:53,760 --> 00:03:00,150
vectors and this part of the program

00:02:57,420 --> 00:03:04,409
which runs on the GPU is is the farthest

00:03:00,150 --> 00:03:06,780
device code or kernel and again just not

00:03:04,409 --> 00:03:10,620
to be confused with kernels it may be

00:03:06,780 --> 00:03:13,049
used on Linux and so on just means that

00:03:10,620 --> 00:03:17,519
that's a function which runs on the GPU

00:03:13,049 --> 00:03:19,530
and it's launched from a CPU code and in

00:03:17,519 --> 00:03:22,079
second part of this example we launched

00:03:19,530 --> 00:03:24,480
the kernel using the special triple

00:03:22,079 --> 00:03:26,579
angle bracket syntax and we're not going

00:03:24,480 --> 00:03:29,069
to talk about the parameters inside but

00:03:26,579 --> 00:03:32,690
otherwise it looks like a curricular

00:03:29,069 --> 00:03:35,879
function call we pass the parameters in

00:03:32,690 --> 00:03:38,609
and the function starts running on a GPU

00:03:35,879 --> 00:03:43,169
and this part is called a host or a CPU

00:03:38,609 --> 00:03:46,079
code and also can all the tags on the

00:03:43,169 --> 00:03:48,750
left side which mark which part of the

00:03:46,079 --> 00:03:52,079
code runs from device and and which runs

00:03:48,750 --> 00:03:54,239
on host I will use the same color tags

00:03:52,079 --> 00:03:58,560
in this presentation make it easier to

00:03:54,239 --> 00:04:00,810
see which parts run well but all the

00:03:58,560 --> 00:04:03,419
question is this example is a pure C

00:04:00,810 --> 00:04:07,560
example and where are all the pluses of

00:04:03,419 --> 00:04:09,419
a c++ and what i found out that in most

00:04:07,560 --> 00:04:11,639
of the examples defined on internet and

00:04:09,419 --> 00:04:14,639
even in books especially the basic

00:04:11,639 --> 00:04:17,039
examples they all pure C code and what I

00:04:14,639 --> 00:04:19,799
want to show is how you can use standard

00:04:17,039 --> 00:04:23,789
C++ features in CUDA and make your code

00:04:19,799 --> 00:04:26,169
safer more readable and maybe even

00:04:23,789 --> 00:04:28,970
faster

00:04:26,169 --> 00:04:31,550
so on would like to start out George

00:04:28,970 --> 00:04:36,110
news from CUDA programming and and

00:04:31,550 --> 00:04:39,979
hopefully find a path to do better code

00:04:36,110 --> 00:04:42,289
I want to mention that the different

00:04:39,979 --> 00:04:45,949
ways to use the C++ with CUDA

00:04:42,289 --> 00:04:48,530
so one host is frost it ships as part of

00:04:45,949 --> 00:04:52,069
the Kula package and on top of OpenGL

00:04:48,530 --> 00:04:55,150
can use sicko and they're also open ACC

00:04:52,069 --> 00:05:00,800
openmp extensions which are basically

00:04:55,150 --> 00:05:03,110
notation based extensions to C++ so I'm

00:05:00,800 --> 00:05:05,120
not going to talk about but how I decide

00:05:03,110 --> 00:05:08,389
which one you want to use but given that

00:05:05,120 --> 00:05:11,150
use included C C++ and you're creating

00:05:08,389 --> 00:05:13,580
your own kernels I want to show which

00:05:11,150 --> 00:05:16,789
which features of C++ you may want to

00:05:13,580 --> 00:05:20,360
use going back to definition of the

00:05:16,789 --> 00:05:23,780
function in the current form it operates

00:05:20,360 --> 00:05:28,130
on integer vectors but what if we would

00:05:23,780 --> 00:05:30,110
like to sum up float arrays then the

00:05:28,130 --> 00:05:32,060
solution is very simple we create

00:05:30,110 --> 00:05:35,210
another function with a different name

00:05:32,060 --> 00:05:37,820
let's say F column F and now the

00:05:35,210 --> 00:05:39,880
parameters of floating points but

00:05:37,820 --> 00:05:44,150
otherwise the body is exactly the same

00:05:39,880 --> 00:05:45,590
this best resolution is much simpler we

00:05:44,150 --> 00:05:50,419
defined the column function its template

00:05:45,590 --> 00:05:52,460
and we use T stars arguments in the

00:05:50,419 --> 00:05:54,219
function signature and then when we

00:05:52,460 --> 00:05:57,979
launch the function we can either use

00:05:54,219 --> 00:06:00,259
implicit syntax of its intention to int

00:05:57,979 --> 00:06:01,849
or we could skip it and the template

00:06:00,259 --> 00:06:04,340
parameters will be deduced automatically

00:06:01,849 --> 00:06:06,409
it works like in the regular CPU code I

00:06:04,340 --> 00:06:08,960
mean it's a very basic feature of sleep

00:06:06,409 --> 00:06:11,030
as fast it will not be very exciting but

00:06:08,960 --> 00:06:14,360
just shows that you can use it with CUDA

00:06:11,030 --> 00:06:17,270
kernels as well so the second aspect of

00:06:14,360 --> 00:06:19,669
this function signature what you want to

00:06:17,270 --> 00:06:22,190
address our other pointers so what

00:06:19,669 --> 00:06:27,380
exactly the memory is pointers are

00:06:22,190 --> 00:06:31,580
pointed to in the color system basically

00:06:27,380 --> 00:06:34,580
the CPU with CPU memory or host memory

00:06:31,580 --> 00:06:37,520
which is which is usual memory that we

00:06:34,580 --> 00:06:38,720
use in all our C++ programs and then

00:06:37,520 --> 00:06:41,420
each GPU device

00:06:38,720 --> 00:06:41,960
comes with its own memory and the

00:06:41,420 --> 00:06:45,710
colonel's

00:06:41,960 --> 00:06:48,170
Serrano GPU may only access directly the

00:06:45,710 --> 00:06:50,390
memory of the device so what we usually

00:06:48,170 --> 00:06:52,520
do what we usually do in CUDA programs

00:06:50,390 --> 00:06:55,220
the first allocate the memory on the

00:06:52,520 --> 00:06:59,060
device with some cudamalloc a call and

00:06:55,220 --> 00:07:03,370
then copy the data we scoop this kudamon

00:06:59,060 --> 00:07:08,150
copy so so in this example we would copy

00:07:03,370 --> 00:07:10,580
vectors a and B from CPU to GPU then

00:07:08,150 --> 00:07:15,290
would launch the kernel and ready to

00:07:10,580 --> 00:07:18,920
copy results back from GPU tcpo now this

00:07:15,290 --> 00:07:21,620
is example very simple code which

00:07:18,920 --> 00:07:26,060
launches our original but in this case

00:07:21,620 --> 00:07:28,940
we send it a pointers to local in local

00:07:26,060 --> 00:07:31,670
interests and the main problem with that

00:07:28,940 --> 00:07:35,840
code is that it compiles but it fails in

00:07:31,670 --> 00:07:37,760
on time and of course in C++ we would

00:07:35,840 --> 00:07:39,800
like to capture all the arrows as always

00:07:37,760 --> 00:07:42,680
possible and we want to leverage the

00:07:39,800 --> 00:07:45,650
type system to avoid these errors in

00:07:42,680 --> 00:07:48,470
compile time so what we may like to do

00:07:45,650 --> 00:07:51,080
is to replace roll pointers with special

00:07:48,470 --> 00:07:52,880
type of device pointers which basically

00:07:51,080 --> 00:07:56,420
explicitly indicate that this memory

00:07:52,880 --> 00:08:01,940
resides on the GPU and it's ended safe

00:07:56,420 --> 00:08:04,130
to access it from current code and so we

00:08:01,940 --> 00:08:10,220
want to create a helper class like

00:08:04,130 --> 00:08:12,170
device PTR which basically it stores it

00:08:10,220 --> 00:08:15,290
stores the row pointer inside but then

00:08:12,170 --> 00:08:20,210
constructor from the raw pointer is

00:08:15,290 --> 00:08:22,610
marked as explicit and then I should the

00:08:20,210 --> 00:08:24,820
preset could say but you know that this

00:08:22,610 --> 00:08:27,890
pointer is safe to be used in GPU code

00:08:24,820 --> 00:08:30,770
but it also note the special keywords

00:08:27,890 --> 00:08:34,250
like on scroll device which means that

00:08:30,770 --> 00:08:36,620
this function can run in GPU context and

00:08:34,250 --> 00:08:39,200
basically all the functions

00:08:36,620 --> 00:08:42,410
three functions and number functions

00:08:39,200 --> 00:08:45,380
which invoked from other methods on the

00:08:42,410 --> 00:08:47,710
GPU should have this device device

00:08:45,380 --> 00:08:47,710
keyboard

00:08:48,850 --> 00:08:53,269
can also add the cutter function a

00:08:51,410 --> 00:08:55,959
convenience function should take eighty

00:08:53,269 --> 00:09:01,610
pointer and would make a device PTR

00:08:55,959 --> 00:09:04,490
table pointer then and then you know the

00:09:01,610 --> 00:09:06,380
classes like this example we can have a

00:09:04,490 --> 00:09:09,440
process function which gets the pointers

00:09:06,380 --> 00:09:11,329
to the input data in the pointer to the

00:09:09,440 --> 00:09:15,050
place where I want to store the result

00:09:11,329 --> 00:09:18,200
the C and then we start from a allocate

00:09:15,050 --> 00:09:22,190
a memory on the GPU with cudamalloc then

00:09:18,200 --> 00:09:24,860
we copy the input memory of a to the GPU

00:09:22,190 --> 00:09:26,959
Wizkid mmm copy and you may note +

00:09:24,860 --> 00:09:31,910
parameter to the function it's called a

00:09:26,959 --> 00:09:34,490
mem copy host to device so to specify to

00:09:31,910 --> 00:09:38,240
specify which which direction the copy

00:09:34,490 --> 00:09:40,880
is either it's from host to device or

00:09:38,240 --> 00:09:43,130
from device to host and then we'll do

00:09:40,880 --> 00:09:45,980
the same for vector B and we also

00:09:43,130 --> 00:09:49,100
allocate a memory for vector C and now

00:09:45,980 --> 00:09:51,529
we can launch our kernel and instead of

00:09:49,100 --> 00:09:55,250
using our there are pointers you would

00:09:51,529 --> 00:09:59,510
use to make device PTR call and now what

00:09:55,250 --> 00:10:01,970
the call is safe and once finished you

00:09:59,510 --> 00:10:04,399
want to copy the memory back of the

00:10:01,970 --> 00:10:09,110
resulting vector C and three all all the

00:10:04,399 --> 00:10:13,160
race in the GPU maybe even a simple way

00:10:09,110 --> 00:10:15,470
of doing this code you can do this so we

00:10:13,160 --> 00:10:16,279
assume that to have some device some

00:10:15,470 --> 00:10:19,250
device in memory

00:10:16,279 --> 00:10:21,380
helper class which can allocate a number

00:10:19,250 --> 00:10:24,709
for us and also release the memory in

00:10:21,380 --> 00:10:27,829
the destructor and then a special cop

00:10:24,709 --> 00:10:31,519
element function no doesn't need to get

00:10:27,829 --> 00:10:33,649
a special parameter of the copy

00:10:31,519 --> 00:10:35,449
direction as a parameter it can be just

00:10:33,649 --> 00:10:38,180
reduced from the type of the arguments

00:10:35,449 --> 00:10:40,220
right if no copying from the pointer in

00:10:38,180 --> 00:10:42,410
the CPU to the pointer and GPU then

00:10:40,220 --> 00:10:46,760
introduced that the direction is host to

00:10:42,410 --> 00:10:51,170
device so can create the best memory

00:10:46,760 --> 00:10:54,980
class also in a very similar way in this

00:10:51,170 --> 00:10:58,040
case we can we can make the row start of

00:10:54,980 --> 00:11:00,660
private and add static functions which

00:10:58,040 --> 00:11:03,480
is basically define whether Velcade

00:11:00,660 --> 00:11:06,449
elements or bytes I think just worried

00:11:03,480 --> 00:11:08,940
ability if you have the best memory cost

00:11:06,449 --> 00:11:11,759
of type T we just have a constructor

00:11:08,940 --> 00:11:13,290
which gets sizes parameter it's not

00:11:11,759 --> 00:11:15,569
clear if you're talking about bytes or

00:11:13,290 --> 00:11:18,240
number of elements so I would prefer to

00:11:15,569 --> 00:11:20,250
make it explicit and in the destructor

00:11:18,240 --> 00:11:22,860
of this class we would free the memory

00:11:20,250 --> 00:11:27,870
and we can also add conversion rate or

00:11:22,860 --> 00:11:30,000
to any wise pointer okay now going back

00:11:27,870 --> 00:11:32,399
to the to device pointer there are

00:11:30,000 --> 00:11:35,189
certain properties of the pointer which

00:11:32,399 --> 00:11:37,199
which one which we want to have for

00:11:35,189 --> 00:11:39,629
example won't want to allow the

00:11:37,199 --> 00:11:42,810
conversion of int pointer to a constant

00:11:39,629 --> 00:11:46,470
pointer but not vice versa and also we

00:11:42,810 --> 00:11:48,720
don't want to allow copying of pointers

00:11:46,470 --> 00:11:52,019
of different types so basically what we

00:11:48,720 --> 00:11:54,329
say we want to allow a copy on the West

00:11:52,019 --> 00:11:58,889
pointer of type T had to the west point

00:11:54,329 --> 00:12:01,709
of type T or if and only if the T one

00:11:58,889 --> 00:12:05,790
pointer is natively convertible duty

00:12:01,709 --> 00:12:10,500
pointer and we can easily express it in

00:12:05,790 --> 00:12:12,689
C++ so we create the constructor the

00:12:10,500 --> 00:12:15,360
device pointer which takes the West

00:12:12,689 --> 00:12:17,850
pointer of type t1 but we only enable it

00:12:15,360 --> 00:12:21,149
if the types of the pointer convertible

00:12:17,850 --> 00:12:24,149
and again it's not very exciting piece

00:12:21,149 --> 00:12:26,819
of office Busta's code but it just shows

00:12:24,149 --> 00:12:29,339
that you can use all this Phinney and

00:12:26,819 --> 00:12:32,810
all the tax rates in the GPU code in

00:12:29,339 --> 00:12:35,910
same way as you do in the city of code

00:12:32,810 --> 00:12:39,870
and we can also also use it for static

00:12:35,910 --> 00:12:43,019
polymorphism so this example create a

00:12:39,870 --> 00:12:44,910
fan toklas the binary of Plus which

00:12:43,019 --> 00:12:48,809
basically which defines which operation

00:12:44,910 --> 00:12:51,300
we want to run on our vectors in this

00:12:48,809 --> 00:12:54,480
case implements a function cooperator

00:12:51,300 --> 00:12:58,050
and just performs the sum of two

00:12:54,480 --> 00:13:00,120
elements and a kernel function can be

00:12:58,050 --> 00:13:02,879
made template by the type of the

00:13:00,120 --> 00:13:05,610
operation you want to invoke and when we

00:13:02,879 --> 00:13:10,319
launch the kernel you specify the name

00:13:05,610 --> 00:13:13,290
of the abstract we should love to create

00:13:10,319 --> 00:13:14,130
on the GPU and then we can customize

00:13:13,290 --> 00:13:18,320
perform

00:13:14,130 --> 00:13:20,640
of our own function okay so I've seen

00:13:18,320 --> 00:13:23,250
the wave doing static polymorphism is

00:13:20,640 --> 00:13:28,260
templates the next stop is the lung

00:13:23,250 --> 00:13:30,770
polymorphism so now have an interface a

00:13:28,260 --> 00:13:34,050
binary or interface which defines an

00:13:30,770 --> 00:13:35,720
abstract function called patrol which

00:13:34,050 --> 00:13:40,350
operates up to elements of type T and

00:13:35,720 --> 00:13:43,830
again want to untie the implementation

00:13:40,350 --> 00:13:45,930
of this interface by name or + which

00:13:43,830 --> 00:13:51,090
does the sum of the elements as we

00:13:45,930 --> 00:13:54,420
expected to do now on the Y side we can

00:13:51,090 --> 00:13:56,520
have some worker function which

00:13:54,420 --> 00:13:59,130
basically gets all the vectors a B and C

00:13:56,520 --> 00:14:02,280
and also gets a reference to an

00:13:59,130 --> 00:14:05,370
interface and inside it will use the

00:14:02,280 --> 00:14:07,830
interface via the virtual function and

00:14:05,370 --> 00:14:11,880
we'll call the correct operation of the

00:14:07,830 --> 00:14:14,790
provided implementation and now at

00:14:11,880 --> 00:14:17,310
column function can create an instance

00:14:14,790 --> 00:14:21,980
of binary or plus inside and just invoke

00:14:17,310 --> 00:14:25,590
the work function and that's fine but

00:14:21,980 --> 00:14:28,110
you may note that both the at Cornell

00:14:25,590 --> 00:14:31,940
and worker functions they they all run

00:14:28,110 --> 00:14:35,100
in the context of the GPU basically

00:14:31,940 --> 00:14:38,850
you're not allowed to pass a parameter

00:14:35,100 --> 00:14:42,120
with virtual functions when you launch

00:14:38,850 --> 00:14:44,820
the new chicano and make sense right as

00:14:42,120 --> 00:14:46,740
the classes with virtual functions they

00:14:44,820 --> 00:14:49,200
probably have some pointer to virtual

00:14:46,740 --> 00:14:51,330
table which is located in a sip in the

00:14:49,200 --> 00:14:55,170
CPU memory and then it's not accessible

00:14:51,330 --> 00:14:56,990
from a typical memory but not the

00:14:55,170 --> 00:14:59,580
business logic which wants to decide

00:14:56,990 --> 00:15:03,540
which of the which of the pressures will

00:14:59,580 --> 00:15:11,670
invoke it use their sights on the CPU so

00:15:03,540 --> 00:15:14,460
won't somehow to be able to send okay so

00:15:11,670 --> 00:15:17,180
one way one way you can solve it is as

00:15:14,460 --> 00:15:20,930
we did in previous example we can just

00:15:17,180 --> 00:15:24,270
add a template parameter to a function

00:15:20,930 --> 00:15:26,910
associated the binary up class and then

00:15:24,270 --> 00:15:28,080
our column function will create the

00:15:26,910 --> 00:15:31,100
object of the correct

00:15:28,080 --> 00:15:36,029
which implements the interface and and

00:15:31,100 --> 00:15:38,040
the Durant worker that's fine but there

00:15:36,029 --> 00:15:39,839
is a design problem as if they already

00:15:38,040 --> 00:15:43,820
have this parameter as a template

00:15:39,839 --> 00:15:47,970
parameter then at no point

00:15:43,820 --> 00:15:51,390
converted later in processing to the

00:15:47,970 --> 00:15:52,320
virtual function so instead would like

00:15:51,390 --> 00:15:55,230
to do something like that

00:15:52,320 --> 00:15:59,160
would like our at kernel to have a

00:15:55,230 --> 00:16:02,910
parameter which is of the type of

00:15:59,160 --> 00:16:05,670
pointer to an interface or binary op in

00:16:02,910 --> 00:16:07,709
somehow in our CPU code you would like

00:16:05,670 --> 00:16:10,980
to create this object in the GPU

00:16:07,709 --> 00:16:14,940
namespace so strategy to solve that

00:16:10,980 --> 00:16:17,660
issue would this first we would allocate

00:16:14,940 --> 00:16:22,410
the memory on the GPU as we usually do

00:16:17,660 --> 00:16:24,899
then on the GPU side we would initialize

00:16:22,410 --> 00:16:30,180
the memory to cook the correct type of

00:16:24,899 --> 00:16:32,430
object in and now on CPU we have valid T

00:16:30,180 --> 00:16:36,300
pointer in hand and we can use it to

00:16:32,430 --> 00:16:39,480
watch the kernel so can build this dev

00:16:36,300 --> 00:16:39,959
object after class which will do the

00:16:39,480 --> 00:16:44,700
work for us

00:16:39,959 --> 00:16:48,240
so it has the memory will help deaf

00:16:44,700 --> 00:16:50,520
number a class which you've seen you

00:16:48,240 --> 00:16:53,940
seen before in the constructor it was

00:16:50,520 --> 00:16:58,829
first arcade one element of type type T

00:16:53,940 --> 00:17:00,920
which means just the raw memory of the

00:16:58,829 --> 00:17:03,750
proprietary

00:17:00,920 --> 00:17:06,900
now we want to instruct the without the

00:17:03,750 --> 00:17:09,689
object so so what we'd like to do is

00:17:06,900 --> 00:17:12,929
basically is to run a placement new

00:17:09,689 --> 00:17:16,309
operator in the located memory and we

00:17:12,929 --> 00:17:18,990
must do it on the device

00:17:16,309 --> 00:17:21,929
so then create this allocate object

00:17:18,990 --> 00:17:24,990
function above which basically it's a

00:17:21,929 --> 00:17:26,970
calling function it gets the parameters

00:17:24,990 --> 00:17:29,790
the pointer to the T it is to construct

00:17:26,970 --> 00:17:32,190
and all the arguments and then it just

00:17:29,790 --> 00:17:35,460
calls a placement operator new and you

00:17:32,190 --> 00:17:38,370
know may also note that arguments are

00:17:35,460 --> 00:17:41,100
passed by value as these arguments are

00:17:38,370 --> 00:17:41,370
being passed from a CPU to GPU and you

00:17:41,100 --> 00:17:45,750
can

00:17:41,370 --> 00:17:47,130
use any references in this case and we

00:17:45,750 --> 00:17:48,780
also have to make this function a free

00:17:47,130 --> 00:17:54,030
function it can be a member function

00:17:48,780 --> 00:17:55,760
it's a limitation of CUDA but the global

00:17:54,030 --> 00:17:59,480
functions must be freestanding functions

00:17:55,760 --> 00:18:02,100
but but you can at least use some names

00:17:59,480 --> 00:18:04,890
use the namespace to make the

00:18:02,100 --> 00:18:07,710
constellation more clear and then in the

00:18:04,890 --> 00:18:10,620
constructor of our deaf object root

00:18:07,710 --> 00:18:13,350
launches its kernel but only one

00:18:10,620 --> 00:18:18,330
instance only one could offered and that

00:18:13,350 --> 00:18:19,830
will do the location for us enough of

00:18:18,330 --> 00:18:22,790
convenience you can also add a

00:18:19,830 --> 00:18:26,520
conversion operator from deaf memory

00:18:22,790 --> 00:18:28,980
from the dev object of type T to the

00:18:26,520 --> 00:18:32,400
point of type T and we can use it in our

00:18:28,980 --> 00:18:34,110
functions and then similar way we can

00:18:32,400 --> 00:18:37,770
also implement the destruction of the

00:18:34,110 --> 00:18:40,830
object so create another kernel function

00:18:37,770 --> 00:18:44,100
treat object which basically calls the

00:18:40,830 --> 00:18:46,290
destructor explicitly and and then the

00:18:44,100 --> 00:18:52,740
DEF memory will just release the memory

00:18:46,290 --> 00:18:56,450
on the GPU for us we've seen use dynamic

00:18:52,740 --> 00:18:59,250
polymorphism next stop is new delete

00:18:56,450 --> 00:19:00,210
basically it's a shortest section in

00:18:59,250 --> 00:19:02,340
stock

00:19:00,210 --> 00:19:04,590
you didn't know that you can use malloc

00:19:02,340 --> 00:19:07,460
free and you delete in the kernel in

00:19:04,590 --> 00:19:10,160
column code it works just as expected a

00:19:07,460 --> 00:19:12,480
daughter command the text should do it

00:19:10,160 --> 00:19:16,320
because of the performance but what is

00:19:12,480 --> 00:19:18,620
possible and works and once we talk

00:19:16,320 --> 00:19:23,550
about the memory you may want to look a

00:19:18,620 --> 00:19:26,190
bit deeper so it's not so related

00:19:23,550 --> 00:19:28,470
directly to C to C plus pass aspects of

00:19:26,190 --> 00:19:30,750
CUDA but I suppose was special

00:19:28,470 --> 00:19:33,210
developers would like to talk about

00:19:30,750 --> 00:19:37,140
memory so let's see what's happening in

00:19:33,210 --> 00:19:40,200
this case so this is a snapshot of video

00:19:37,140 --> 00:19:41,670
visual profiler I'm not sure I'm not

00:19:40,200 --> 00:19:45,320
sure that you can read all the other

00:19:41,670 --> 00:19:49,700
names but basically the blue bar on top

00:19:45,320 --> 00:19:53,530
is the CPU side of the processing that

00:19:49,700 --> 00:19:56,650
these are the

00:19:53,530 --> 00:19:57,940
recalls and the launches that we do in

00:19:56,650 --> 00:20:00,280
our main function

00:19:57,940 --> 00:20:04,540
and we can measure it on one CPU side

00:20:00,280 --> 00:20:07,840
and now the green ones are basically the

00:20:04,540 --> 00:20:11,350
copying of the vectors a and B from host

00:20:07,840 --> 00:20:14,740
to the GPU and then the thread part

00:20:11,350 --> 00:20:15,370
small one edie at Safin is our is at

00:20:14,740 --> 00:20:17,470
colonel

00:20:15,370 --> 00:20:21,220
it's the actual work we should perform

00:20:17,470 --> 00:20:24,790
and and then on the right side we copy

00:20:21,220 --> 00:20:26,350
the result back from the GPU to a cpu so

00:20:24,790 --> 00:20:30,760
in this case the portion of the actual

00:20:26,350 --> 00:20:34,120
processing is very small as I use a very

00:20:30,760 --> 00:20:37,110
simple algorithm of course the ratio can

00:20:34,120 --> 00:20:40,360
be different for different algorithms

00:20:37,110 --> 00:20:42,730
but we still want to get better than

00:20:40,360 --> 00:20:47,230
that and in this example the copy and

00:20:42,730 --> 00:20:49,540
the processing are synchronous with CUDA

00:20:47,230 --> 00:20:52,480
I believe that in most of the devices

00:20:49,540 --> 00:20:57,010
you can do the copy and to compute

00:20:52,480 --> 00:20:58,120
operations in the same time the still

00:20:57,010 --> 00:21:00,640
sounds is a problem

00:20:58,120 --> 00:21:04,860
as before we do any computation of a and

00:21:00,640 --> 00:21:07,690
B we need to copy the first two to GPU

00:21:04,860 --> 00:21:11,290
so in order to solve this problem we can

00:21:07,690 --> 00:21:14,790
take a and B race and break them to

00:21:11,290 --> 00:21:17,710
Chuck's and then we'll process them all

00:21:14,790 --> 00:21:20,110
in some for loop and for example in

00:21:17,710 --> 00:21:24,040
initiation of the loop will first copy

00:21:20,110 --> 00:21:29,160
child K of a to the GPU next the copy

00:21:24,040 --> 00:21:32,140
junk K of vector B to GPU knocking sound

00:21:29,160 --> 00:21:35,200
started parallel both to process the

00:21:32,140 --> 00:21:39,480
chunk K on the GPU and also to copy next

00:21:35,200 --> 00:21:42,340
chunk to the memory and then achieve

00:21:39,480 --> 00:21:46,240
basically the best performance which

00:21:42,340 --> 00:21:50,980
would be the max of of copy and compute

00:21:46,240 --> 00:21:54,670
times and for that we need to use the

00:21:50,980 --> 00:21:58,840
kudamon copy sync and the important note

00:21:54,670 --> 00:22:03,550
it requires the memory on host to be a

00:21:58,840 --> 00:22:05,860
piant memory and also make sense as the

00:22:03,550 --> 00:22:06,970
cooperation is a secre knows and it

00:22:05,860 --> 00:22:09,460
needs to make sure the

00:22:06,970 --> 00:22:12,250
is not swapped out from the active

00:22:09,460 --> 00:22:14,530
memory and we can pin the memory either

00:22:12,250 --> 00:22:17,380
we could host a log which basically

00:22:14,530 --> 00:22:19,600
would allocate the memory on the CPU as

00:22:17,380 --> 00:22:22,030
a pimp memory or we can take an existing

00:22:19,600 --> 00:22:26,470
memory and pin it we could a host

00:22:22,030 --> 00:22:30,960
register another magic feature in the

00:22:26,470 --> 00:22:35,080
encoder package is the zero copy memory

00:22:30,960 --> 00:22:37,780
basically what you do first you also

00:22:35,080 --> 00:22:41,620
need to start from the pinned memory and

00:22:37,780 --> 00:22:44,650
and you can use the same functions host

00:22:41,620 --> 00:22:48,310
host register but now you also need to

00:22:44,650 --> 00:22:53,080
provide a special parameter to register

00:22:48,310 --> 00:22:55,090
it as as a mapped memory then we need to

00:22:53,080 --> 00:22:57,610
call this gate device pointer function

00:22:55,090 --> 00:22:59,950
which basically takes the pointer to the

00:22:57,610 --> 00:23:02,770
CPU memory and returns as a point of to

00:22:59,950 --> 00:23:07,510
me GPU memory and now we can launch our

00:23:02,770 --> 00:23:09,850
own or safely and in the most recent I

00:23:07,510 --> 00:23:13,060
believe that a screen not existing

00:23:09,850 --> 00:23:15,610
versions of the of the GPUs or the CUDA

00:23:13,060 --> 00:23:17,680
pills it also supports a unified virtual

00:23:15,610 --> 00:23:20,200
other sink which means basically you

00:23:17,680 --> 00:23:22,570
don't need to call this device pointer

00:23:20,200 --> 00:23:26,440
function the best pointer is exactly the

00:23:22,570 --> 00:23:30,550
same address as Hospital so it sounds

00:23:26,440 --> 00:23:32,440
very easy to use but if you look at the

00:23:30,550 --> 00:23:35,020
performance as you see that in the

00:23:32,440 --> 00:23:37,570
previous example of the silk copy took

00:23:35,020 --> 00:23:42,010
us two milliseconds that now we're up to

00:23:37,570 --> 00:23:46,540
70 milliseconds so the zero copy comes

00:23:42,010 --> 00:23:49,270
with shorter cost as we can see the next

00:23:46,540 --> 00:23:53,370
feature basically its most recent from

00:23:49,270 --> 00:23:56,080
CUDA six is a unified memory just not

00:23:53,370 --> 00:23:58,140
don't be confused between a unified

00:23:56,080 --> 00:24:00,600
memory and unified virtual other things

00:23:58,140 --> 00:24:03,160
the total different features

00:24:00,600 --> 00:24:06,100
so unified memory you allocated

00:24:03,160 --> 00:24:08,350
cudamalloc managed and you have to

00:24:06,100 --> 00:24:10,570
allocate the memory as such you can't

00:24:08,350 --> 00:24:14,880
take an existing memory on the CPU and

00:24:10,570 --> 00:24:16,580
somehow make it managed and then

00:24:14,880 --> 00:24:20,149
describe the

00:24:16,580 --> 00:24:23,919
in very very high-level example of the

00:24:20,149 --> 00:24:27,590
data in host memory like our a a vector

00:24:23,919 --> 00:24:29,899
and now when the device code our kernel

00:24:27,590 --> 00:24:32,960
you try to read the memory but you'll

00:24:29,899 --> 00:24:36,640
get a page fault and then the page will

00:24:32,960 --> 00:24:40,010
be migrated which is copied from device

00:24:36,640 --> 00:24:43,360
on host to device and it will become

00:24:40,010 --> 00:24:45,830
unavailable on the device so all the

00:24:43,360 --> 00:24:49,669
migrations of the data are based on the

00:24:45,830 --> 00:24:51,559
page faults on both sides but how we

00:24:49,669 --> 00:24:58,960
measure it then promise is even worse

00:24:51,559 --> 00:25:02,090
and but you must know that

00:24:58,960 --> 00:25:04,909
so unified memory the watchmen control

00:25:02,090 --> 00:25:09,350
there's not special functions like

00:25:04,909 --> 00:25:12,080
prefetch is sync basically tell tell -

00:25:09,350 --> 00:25:16,039
could to start copy in the memory in

00:25:12,080 --> 00:25:20,299
advance on my advice which advises the

00:25:16,039 --> 00:25:24,380
driver were were memory located

00:25:20,299 --> 00:25:27,440
initially I think that this functions a

00:25:24,380 --> 00:25:30,169
bit for cheating right as we wanted to

00:25:27,440 --> 00:25:34,340
unified memory I thought we went back to

00:25:30,169 --> 00:25:37,730
control on the performance and also I

00:25:34,340 --> 00:25:39,440
want to say that I tried it on sqm

00:25:37,730 --> 00:25:42,679
desktop computers on this laptop and

00:25:39,440 --> 00:25:46,580
also awesome gaming machines but not

00:25:42,679 --> 00:25:49,700
only high performance machines like IBM

00:25:46,580 --> 00:25:54,080
Power PC so the results may be different

00:25:49,700 --> 00:25:57,769
on different cards well I'll just

00:25:54,080 --> 00:26:00,279
summarize it performance we sort of

00:25:57,769 --> 00:26:04,090
expect is best when we do the hard work

00:26:00,279 --> 00:26:06,440
or the copying by ourselves and

00:26:04,090 --> 00:26:09,830
simplicity would come at the price of

00:26:06,440 --> 00:26:15,260
performance the interpretation to this

00:26:09,830 --> 00:26:17,750
last column of our success it's a it's

00:26:15,260 --> 00:26:19,850
an interesting use case when you only

00:26:17,750 --> 00:26:21,649
access a small amount of the data or the

00:26:19,850 --> 00:26:24,169
input data and you don't know in advance

00:26:21,649 --> 00:26:26,899
which data the algorithm will access

00:26:24,169 --> 00:26:29,930
then the clear advantages to zero copy

00:26:26,899 --> 00:26:32,600
of unified memory approaches

00:26:29,930 --> 00:26:34,420
they will copy much less memory then you

00:26:32,600 --> 00:26:39,710
would do otherwise

00:26:34,420 --> 00:26:43,580
so memory's hard okay so going back to

00:26:39,710 --> 00:26:45,800
typical features next feature which

00:26:43,580 --> 00:26:51,110
they're useful and very important is a

00:26:45,800 --> 00:26:53,690
lambda so start from very basic very

00:26:51,110 --> 00:26:56,150
simple lambda in a kernel called a

00:26:53,690 --> 00:26:59,180
kernel you create a statist lambda op

00:26:56,150 --> 00:27:01,540
which indicates the operation and then

00:26:59,180 --> 00:27:05,150
walk it in place

00:27:01,540 --> 00:27:07,220
and we can also add a caption parameters

00:27:05,150 --> 00:27:10,430
to lambda in this case we're capturing

00:27:07,220 --> 00:27:14,030
both vectors and B and also the index

00:27:10,430 --> 00:27:17,210
and and the game works exactly as it

00:27:14,030 --> 00:27:19,370
would expect in a CUDA code but I think

00:27:17,210 --> 00:27:21,920
that the most exciting news of lambdas

00:27:19,370 --> 00:27:26,300
is the fact that you can pass the lambda

00:27:21,920 --> 00:27:27,740
from as a parameter from a CPU to GPU so

00:27:26,300 --> 00:27:32,330
in this case in the main function we

00:27:27,740 --> 00:27:34,520
create a lambda the connote a special

00:27:32,330 --> 00:27:38,150
device keyword which is a good extension

00:27:34,520 --> 00:27:40,670
to a standard lambda syntax then lambda

00:27:38,150 --> 00:27:43,460
indicates an operation of some a plus B

00:27:40,670 --> 00:27:47,570
and when we launch the kernel we just

00:27:43,460 --> 00:27:50,330
pass it as a parameter to a kernel if we

00:27:47,570 --> 00:27:54,020
were to work you also need to specify a

00:27:50,330 --> 00:27:55,490
a special operation flag we basically

00:27:54,020 --> 00:27:59,300
add the support to the extent that

00:27:55,490 --> 00:28:01,160
lambda syntax so now have a slammed

00:27:59,300 --> 00:28:03,290
object which passes the border between

00:28:01,160 --> 00:28:05,060
the CPU and the GPU and the question is

00:28:03,290 --> 00:28:10,640
what can be captured inside the mount

00:28:05,060 --> 00:28:12,530
object so the first thing you can

00:28:10,640 --> 00:28:14,510
capture is we can capture object by

00:28:12,530 --> 00:28:16,910
reference and again it makes sense

00:28:14,510 --> 00:28:20,630
we know that the memory in CPU and GPU

00:28:16,910 --> 00:28:24,130
is distinct and we can't can take any

00:28:20,630 --> 00:28:28,730
any pointers from CPU and move it to GPU

00:28:24,130 --> 00:28:32,560
and that will you fail it in the compile

00:28:28,730 --> 00:28:35,600
time if you try to do it the more

00:28:32,560 --> 00:28:38,740
interesting case may be is the capture

00:28:35,600 --> 00:28:42,710
of this pointer so let's see an example

00:28:38,740 --> 00:28:43,790
so now our house or some operation is a

00:28:42,710 --> 00:28:46,220
bit more complex

00:28:43,790 --> 00:28:50,120
so instead of making the sum of a and B

00:28:46,220 --> 00:28:53,600
on don't also add some wasn't value a I

00:28:50,120 --> 00:28:56,140
to all the elements so create the

00:28:53,600 --> 00:28:58,670
structure up which holds basically the I

00:28:56,140 --> 00:29:03,560
we should which would like to add to all

00:28:58,670 --> 00:29:06,260
the at all the cells in C and the create

00:29:03,560 --> 00:29:09,280
the lambda which does a plus B plus I

00:29:06,260 --> 00:29:12,710
and we launch the kernel is at lambda

00:29:09,280 --> 00:29:14,840
the problem is this web code is that it

00:29:12,710 --> 00:29:18,100
it compiles but it fails in run time and

00:29:14,840 --> 00:29:23,510
the reason is again that this pointer

00:29:18,100 --> 00:29:25,870
was captured by the pointer I this is a

00:29:23,510 --> 00:29:29,120
pointer and to solve it we can just use

00:29:25,870 --> 00:29:33,740
there's no syntax to capture this by

00:29:29,120 --> 00:29:36,650
value now it works fine so have obstruct

00:29:33,740 --> 00:29:40,220
it creates a lamp inside and I can

00:29:36,650 --> 00:29:42,320
launch the corner with the lambda still

00:29:40,220 --> 00:29:44,840
looks like a problem from from design

00:29:42,320 --> 00:29:47,330
perspective right all we wanted to do is

00:29:44,840 --> 00:29:50,000
to create a struct which represents a

00:29:47,330 --> 00:29:53,390
simple operation of a plus B plus I but

00:29:50,000 --> 00:29:56,570
instead it has the supply function which

00:29:53,390 --> 00:29:59,150
knows the name of the kernel and it has

00:29:56,570 --> 00:30:01,730
all these template parameters instead

00:29:59,150 --> 00:30:05,750
would like to do something like that in

00:30:01,730 --> 00:30:09,050
like abstract ways could just create the

00:30:05,750 --> 00:30:13,120
operation as a lambda and then somehow

00:30:09,050 --> 00:30:16,250
use it in when we launched the kernel

00:30:13,120 --> 00:30:19,910
let's try to do it straight forward to

00:30:16,250 --> 00:30:23,080
take the to take the lambda from a

00:30:19,910 --> 00:30:26,150
struct and send it to a kernel function

00:30:23,080 --> 00:30:30,290
and the problem is that fails in the

00:30:26,150 --> 00:30:35,540
compile time because we can't turn an

00:30:30,290 --> 00:30:40,550
auto can to turn a lambda in extended

00:30:35,540 --> 00:30:43,100
lambda as auto from the function another

00:30:40,550 --> 00:30:45,830
limitation also in the CUDA compiler for

00:30:43,100 --> 00:30:48,560
every CPU code it works fine but what is

00:30:45,830 --> 00:30:52,250
CUDA we need to do something some tricks

00:30:48,560 --> 00:30:54,080
to make it work okay so next attempt is

00:30:52,250 --> 00:30:56,070
instead of returning auto from a cop

00:30:54,080 --> 00:30:58,649
what to return

00:30:56,070 --> 00:31:01,440
specific type so we can try to return

00:30:58,649 --> 00:31:07,129
STD function which takes two ins and

00:31:01,440 --> 00:31:10,169
returns an int and that it also fails

00:31:07,129 --> 00:31:13,350
because we can't use STD function in the

00:31:10,169 --> 00:31:15,840
kernel code it makes sense as we know

00:31:13,350 --> 00:31:18,359
that all the functions which invoked in

00:31:15,840 --> 00:31:20,789
the CUDA code should have this special

00:31:18,359 --> 00:31:24,600
device keyboard and and there's no

00:31:20,789 --> 00:31:28,049
reason why a student will contain this

00:31:24,600 --> 00:31:31,320
device keywords inside happily we can

00:31:28,049 --> 00:31:35,159
use instead a special special version of

00:31:31,320 --> 00:31:36,570
the function from a video package so we

00:31:35,159 --> 00:31:38,759
include and we functional instead

00:31:36,570 --> 00:31:43,409
functional and the return and withstood

00:31:38,759 --> 00:31:46,889
function but now the code compiles but

00:31:43,409 --> 00:31:50,759
again it fails in first in runtime and

00:31:46,889 --> 00:31:52,830
the reason is that you can't pass and

00:31:50,759 --> 00:31:55,259
env stood function between the CPU and

00:31:52,830 --> 00:31:58,169
GPU it also makes sense

00:31:55,259 --> 00:32:00,989
as probably the initial function is

00:31:58,169 --> 00:32:03,599
implemented in terms of virtual

00:32:00,989 --> 00:32:08,429
functions and we know that you can pass

00:32:03,599 --> 00:32:11,190
this between the host and the device so

00:32:08,429 --> 00:32:14,009
the only solution that I found the best

00:32:11,190 --> 00:32:17,970
way to send a whole structure up to the

00:32:14,009 --> 00:32:21,299
kernel and then said the colonel it can

00:32:17,970 --> 00:32:24,059
call the make up operation get back and

00:32:21,299 --> 00:32:28,950
we stood function invoke it invoke it

00:32:24,059 --> 00:32:31,320
and now it works it's a bit uglier than

00:32:28,950 --> 00:32:35,989
what we wanted it to be but what it

00:32:31,320 --> 00:32:35,989
works and may be useful in some cases

00:32:36,080 --> 00:32:42,149
okay so we've seen the use of lambda on

00:32:38,820 --> 00:32:45,330
the invested function but the question

00:32:42,149 --> 00:32:47,700
is now are all this zero of head

00:32:45,330 --> 00:32:50,849
obstructions as we like it to be in C++

00:32:47,700 --> 00:32:53,700
so one approach might be to do some

00:32:50,849 --> 00:32:56,489
profile ink on the code just hard to

00:32:53,700 --> 00:33:00,570
profile the code to this granularity of

00:32:56,489 --> 00:33:03,779
the results so instead we can use tools

00:33:00,570 --> 00:33:06,109
the compile Explorer and starting from

00:33:03,779 --> 00:33:10,789
from recent versions of compile Explorer

00:33:06,109 --> 00:33:10,789
supports good color compilation

00:33:10,860 --> 00:33:17,950
basically we can write our algorithm as

00:33:14,370 --> 00:33:22,150
a simple c function or use all the super

00:33:17,950 --> 00:33:24,930
smart features and we can compare on the

00:33:22,150 --> 00:33:30,390
right side the result of the computation

00:33:24,930 --> 00:33:33,730
we can and in the ultimate obstructions

00:33:30,390 --> 00:33:37,840
basically we expect to see exactly the

00:33:33,730 --> 00:33:40,750
same binary code between hand random 8

00:33:37,840 --> 00:33:43,900
simple C and and the false pass pass

00:33:40,750 --> 00:33:47,260
code and another tool which comes as

00:33:43,900 --> 00:33:51,250
part of the CUDA package is cool Bob

00:33:47,260 --> 00:33:55,410
damp in this case it works on on an

00:33:51,250 --> 00:33:59,110
object file and you should use a special

00:33:55,410 --> 00:34:02,309
versa flag si SS which means that you

00:33:59,110 --> 00:34:06,520
want to see the binary code and again

00:34:02,309 --> 00:34:08,350
can compare different versions of the

00:34:06,520 --> 00:34:11,350
functions C version and a specific

00:34:08,350 --> 00:34:15,159
version and make sure that basically the

00:34:11,350 --> 00:34:17,020
binary is exactly the same so the tests

00:34:15,159 --> 00:34:20,440
that I did have seen a static

00:34:17,020 --> 00:34:23,590
polymorphism is easier overhead as we

00:34:20,440 --> 00:34:25,480
expect same goes for lambda and even in

00:34:23,590 --> 00:34:30,669
this example the use of the analyst

00:34:25,480 --> 00:34:36,250
function also produced exactly the same

00:34:30,669 --> 00:34:39,190
binary as as a legacy code I believe

00:34:36,250 --> 00:34:42,010
that in some cases like back where we

00:34:39,190 --> 00:34:44,740
were still function it may add some

00:34:42,010 --> 00:34:47,590
overhead but at least in C the simple

00:34:44,740 --> 00:34:49,659
example the optimizer managed to read

00:34:47,590 --> 00:34:57,370
you to remove all the oil all the

00:34:49,659 --> 00:34:58,990
overhead ok so now just to mention other

00:34:57,370 --> 00:35:01,780
features of SIP as pass which you can

00:34:58,990 --> 00:35:04,660
use in the CUDA code so we've seen the

00:35:01,780 --> 00:35:07,960
use of auto you can use context / you

00:35:04,660 --> 00:35:11,010
can use a range based for loop and can

00:35:07,960 --> 00:35:14,260
use on the are r-value semantics move

00:35:11,010 --> 00:35:18,330
its talk works and CUDA code in same way

00:35:14,260 --> 00:35:20,920
as in the regular service pass code

00:35:18,330 --> 00:35:23,340
don't exception to this rule are the

00:35:20,920 --> 00:35:26,610
exceptions and also it should not

00:35:23,340 --> 00:35:30,240
prices it's very similar to what happens

00:35:26,610 --> 00:35:32,970
with embedded C++ code metrically used

00:35:30,240 --> 00:35:35,310
Maus it can use most of the features but

00:35:32,970 --> 00:35:37,770
not exceptions at least not in their

00:35:35,310 --> 00:35:39,840
current form it might be that in the

00:35:37,770 --> 00:35:41,610
standards are - from now you'll get a

00:35:39,840 --> 00:35:43,380
new support for exceptions which are

00:35:41,610 --> 00:35:48,030
more applicable to embedded code and to

00:35:43,380 --> 00:35:50,520
good as well let's now do a couple of

00:35:48,030 --> 00:35:54,690
features which are specific to CUDA C++

00:35:50,520 --> 00:35:58,050
support let's look at the first example

00:35:54,690 --> 00:36:02,460
we have this kernel which needs to loop

00:35:58,050 --> 00:36:05,190
over certain range of data and to make

00:36:02,460 --> 00:36:08,550
it run faster like to use the pragma no

00:36:05,190 --> 00:36:11,670
statement and in some of the tests we

00:36:08,550 --> 00:36:14,700
shaved which have done it helps to boost

00:36:11,670 --> 00:36:17,790
the performance on the CUDA kernels and

00:36:14,700 --> 00:36:20,880
until the recent versions of CUDA had to

00:36:17,790 --> 00:36:25,110
do specify hard-coded value to the

00:36:20,880 --> 00:36:27,960
Paragon role in the recent versions to

00:36:25,110 --> 00:36:30,030
basically use any value which which can

00:36:27,960 --> 00:36:32,940
be deduced in compiled time so in this

00:36:30,030 --> 00:36:35,850
case the value that was supplied to

00:36:32,940 --> 00:36:39,570
pragma enroll is the result of a

00:36:35,850 --> 00:36:41,820
constant x / function which separates on

00:36:39,570 --> 00:36:44,520
tepid parameter in some predefined value

00:36:41,820 --> 00:36:47,040
so basically any value which can be

00:36:44,520 --> 00:36:52,200
deduced in compiled time can be used as

00:36:47,040 --> 00:36:55,410
pérignon or parameter the other feature

00:36:52,200 --> 00:36:58,290
of CUDA is good rank incorporation and

00:36:55,410 --> 00:37:01,500
do and if you need to previous talk I

00:36:58,290 --> 00:37:04,470
talked about about ranking population of

00:37:01,500 --> 00:37:07,730
C++ so we could it's part of the package

00:37:04,470 --> 00:37:10,800
it's supported out of the box and it has

00:37:07,730 --> 00:37:14,130
the number of advantages like a clear

00:37:10,800 --> 00:37:17,670
advantage but you can tune a compilation

00:37:14,130 --> 00:37:20,430
flags based on the real environment so

00:37:17,670 --> 00:37:23,790
in runtime we can check for example the

00:37:20,430 --> 00:37:26,010
GPU our system is using and then to no

00:37:23,790 --> 00:37:30,150
compression flags to use appropriate

00:37:26,010 --> 00:37:33,090
architecture and other flags but the

00:37:30,150 --> 00:37:37,080
most prosperous aspect another

00:37:33,090 --> 00:37:42,150
cooperation is the ability to select

00:37:37,080 --> 00:37:45,610
they fit parameters in runtime let's see

00:37:42,150 --> 00:37:47,440
another example so you have this process

00:37:45,610 --> 00:37:51,430
function and we know that you want to

00:37:47,440 --> 00:37:54,370
use parole and we get the under the

00:37:51,430 --> 00:37:56,710
enroll factor as a template parameter to

00:37:54,370 --> 00:37:58,990
the colon function but the problem is

00:37:56,710 --> 00:38:02,830
that in the main function we only know

00:37:58,990 --> 00:38:05,980
the value for the number of layers you

00:38:02,830 --> 00:38:09,160
want to run in runtime we can only know

00:38:05,980 --> 00:38:14,080
the dynamically so how can we invoke

00:38:09,160 --> 00:38:15,940
this scoring function one solution may

00:38:14,080 --> 00:38:17,770
be to add the proxy function we should

00:38:15,940 --> 00:38:20,410
basically switch on all the possible

00:38:17,770 --> 00:38:23,380
values of number of layers and would

00:38:20,410 --> 00:38:29,110
invoke an appropriate template version

00:38:23,380 --> 00:38:31,840
of a function it works but but first

00:38:29,110 --> 00:38:34,420
it's much more code it requires you to

00:38:31,840 --> 00:38:37,000
know all the basically all the values

00:38:34,420 --> 00:38:38,800
for the layers in advance it also means

00:38:37,000 --> 00:38:41,350
that all the versions of the process

00:38:38,800 --> 00:38:48,130
function have been compiled and added to

00:38:41,350 --> 00:38:50,070
a binary so instead want to use this

00:38:48,130 --> 00:38:55,030
rant incorporation feature and let's see

00:38:50,070 --> 00:38:57,610
how we can do it so basically it gives a

00:38:55,030 --> 00:38:59,290
brief overview of the process and all

00:38:57,610 --> 00:39:03,720
the functions are very well documented

00:38:59,290 --> 00:39:03,720
you can look at the the code reference

00:39:04,020 --> 00:39:10,540
but in general first step is to load our

00:39:08,260 --> 00:39:13,990
source code and all the headers into

00:39:10,540 --> 00:39:15,970
buffers and this is a simple simple

00:39:13,990 --> 00:39:17,770
operation it does not require any

00:39:15,970 --> 00:39:20,050
special support from Kuta it could just

00:39:17,770 --> 00:39:24,280
load your files or can even build a

00:39:20,050 --> 00:39:28,120
program on the fly next we use that and

00:39:24,280 --> 00:39:31,960
the RTC gate program it basically takes

00:39:28,120 --> 00:39:36,640
the buffers and registers them as a CUDA

00:39:31,960 --> 00:39:39,490
program now next step before we can

00:39:36,640 --> 00:39:41,370
compile our program is to specify all

00:39:39,490 --> 00:39:45,400
the stations of the template parameters

00:39:41,370 --> 00:39:49,119
in order for our compiler to know which

00:39:45,400 --> 00:39:51,640
versions of template it needs to compile

00:39:49,119 --> 00:39:55,240
and for simple cases you can just build

00:39:51,640 --> 00:39:58,599
this thing on by ourselves the process

00:39:55,240 --> 00:40:00,519
10 int it's a very simple use case but

00:39:58,599 --> 00:40:04,779
what if if we have some more complex

00:40:00,519 --> 00:40:08,069
type like a thrust vector so this thing

00:40:04,779 --> 00:40:13,000
that we had to build is the one below

00:40:08,069 --> 00:40:16,420
and there's some even more complicated

00:40:13,000 --> 00:40:18,789
cases so to help us you can use a

00:40:16,420 --> 00:40:22,240
function get type name which basically

00:40:18,789 --> 00:40:25,839
is templated by the type which you want

00:40:22,240 --> 00:40:28,119
to add to a function expression and it

00:40:25,839 --> 00:40:31,109
builds the name for us so we can use it

00:40:28,119 --> 00:40:37,750
to build the template installation named

00:40:31,109 --> 00:40:41,500
it as needed for our use case ok so now

00:40:37,750 --> 00:40:43,630
we have all the template type functions

00:40:41,500 --> 00:40:45,579
registered and we can go on and compile

00:40:43,630 --> 00:40:49,150
the program the output of the

00:40:45,579 --> 00:40:53,019
compilation is basically a batter of the

00:40:49,150 --> 00:40:58,650
binary of the binary code in a good

00:40:53,019 --> 00:41:04,150
internal BTX format so next should load

00:40:58,650 --> 00:41:09,880
these bits to the GPU and we need to

00:41:04,150 --> 00:41:12,519
find the function inside set the binary

00:41:09,880 --> 00:41:15,150
which we to invoke and that's easy for

00:41:12,519 --> 00:41:18,150
the C functions that was not functions

00:41:15,150 --> 00:41:21,160
we need to do the mangling of the names

00:41:18,150 --> 00:41:23,079
but also it's very easy it is part of

00:41:21,160 --> 00:41:25,839
the package we can use a special

00:41:23,079 --> 00:41:28,480
function get lowered name which takes

00:41:25,839 --> 00:41:31,960
the name of the function as user would

00:41:28,480 --> 00:41:34,059
see it and we need to use the same name

00:41:31,960 --> 00:41:36,099
of the function as we have Trib

00:41:34,059 --> 00:41:39,849
as we have previously registered in the

00:41:36,099 --> 00:41:42,039
ad name and now we can then get the

00:41:39,849 --> 00:41:44,859
mangled name and use it for the model

00:41:42,039 --> 00:41:48,819
great function and finally we can launch

00:41:44,859 --> 00:41:52,150
our kernel just to note that some of

00:41:48,819 --> 00:41:55,210
functions start from the env RT C which

00:41:52,150 --> 00:41:57,490
is NVIDIA parent incorporation and some

00:41:55,210 --> 00:42:00,490
a part of see you which is a more

00:41:57,490 --> 00:42:02,890
general of the package but again in the

00:42:00,490 --> 00:42:06,660
code of documentation you can find the

00:42:02,890 --> 00:42:09,910
all the examples and see how it's done

00:42:06,660 --> 00:42:15,400
it's a bit of writing but it's not very

00:42:09,910 --> 00:42:19,990
complex still just to say a word about

00:42:15,400 --> 00:42:22,210
thrust it's part of the CUDA package

00:42:19,990 --> 00:42:25,480
it's a template library and sort of

00:42:22,210 --> 00:42:27,700
inspired by steel it has classes like

00:42:25,480 --> 00:42:29,859
device vector host vector which

00:42:27,700 --> 00:42:32,890
represent the data as vector as

00:42:29,859 --> 00:42:36,220
continuous stream of bytes and it has

00:42:32,890 --> 00:42:38,890
many many algorithms which also similar

00:42:36,220 --> 00:42:43,450
to all good in this tail that can run

00:42:38,890 --> 00:42:46,150
the processor in parallel on the GPU and

00:42:43,450 --> 00:42:48,250
also has a device pointer class which is

00:42:46,150 --> 00:42:51,609
very similar to the Vice pointer as we

00:42:48,250 --> 00:42:55,119
discussed but of course it's much more

00:42:51,609 --> 00:42:57,069
complete and and you can just take it

00:42:55,119 --> 00:42:59,289
out of thrust and start using it instead

00:42:57,069 --> 00:43:03,579
of the royalty pointers in the kernel

00:42:59,289 --> 00:43:10,059
code okay so finally bridge start start

00:43:03,579 --> 00:43:12,730
using C++ in a CUDA kernel code you can

00:43:10,059 --> 00:43:15,069
do couple of references there was a

00:43:12,730 --> 00:43:17,529
github paper in which you can find all

00:43:15,069 --> 00:43:21,250
the examples from the stock and some

00:43:17,529 --> 00:43:25,119
more examples I so want to plug a meetup

00:43:21,250 --> 00:43:28,930
group and in this link we can find out

00:43:25,119 --> 00:43:31,809
on the talks and heavy a youtube channel

00:43:28,930 --> 00:43:34,150
that you can go and watch and some

00:43:31,809 --> 00:43:37,630
relevant links so one is a description

00:43:34,150 --> 00:43:39,400
of the new features in CUDA 8 and it

00:43:37,630 --> 00:43:42,430
includes a detailed description of

00:43:39,400 --> 00:43:46,960
different c++ features that were added

00:43:42,430 --> 00:43:49,930
to the CUDA support and also you might

00:43:46,960 --> 00:43:53,829
be interested in the Coco's library I'm

00:43:49,930 --> 00:43:58,000
not an expert directly to to describe it

00:43:53,829 --> 00:44:00,759
but basically it's a C++ supposes

00:43:58,000 --> 00:44:03,039
library to to create a multi-platform

00:44:00,759 --> 00:44:05,410
code which will run on different types

00:44:03,039 --> 00:44:09,060
of GPUs and also all of the GI devices

00:44:05,410 --> 00:44:17,319
and so on so thanks

00:44:09,060 --> 00:44:19,279
[Applause]

00:44:17,319 --> 00:44:22,299
just two quick questions

00:44:19,279 --> 00:44:25,130
what is 32 meaning that kernel function

00:44:22,299 --> 00:44:32,539
like at Colonel there's a like a

00:44:25,130 --> 00:44:34,579
template parameter 32 which one whenever

00:44:32,539 --> 00:44:37,789
you call that CUDA function there is

00:44:34,579 --> 00:44:40,369
that re brackets and there's 32 yeah so

00:44:37,789 --> 00:44:44,779
we launch a kernel function basically it

00:44:40,369 --> 00:44:46,910
always returns void and the call is as

00:44:44,779 --> 00:44:49,789
the Chronos it launches the colon

00:44:46,910 --> 00:44:52,460
function on a GPU but then you need to

00:44:49,789 --> 00:44:54,619
use the special functions to wait for

00:44:52,460 --> 00:44:56,390
the column function to complete and at

00:44:54,619 --> 00:44:58,789
all and always returns void you can't

00:44:56,390 --> 00:45:02,539
return any value out of it you need to

00:44:58,789 --> 00:45:05,329
use I can our case the output was the C

00:45:02,539 --> 00:45:07,460
vector I so need to pre allocate the

00:45:05,329 --> 00:45:10,009
memory for the C vector on the GPU and

00:45:07,460 --> 00:45:13,039
then gets filled and you get to copy it

00:45:10,009 --> 00:45:15,170
back where it finishes okay and if you

00:45:13,039 --> 00:45:17,630
could go to the slide number 19 I think

00:45:15,170 --> 00:45:20,150
there's just a small issue just wanted

00:45:17,630 --> 00:45:37,039
to confirm this one nine one nine

00:45:20,150 --> 00:45:40,369
nineteen almost there yeah yeah I think

00:45:37,039 --> 00:45:44,359
the names allocate elements and bytes

00:45:40,369 --> 00:45:49,249
are kind of other this thing so elements

00:45:44,359 --> 00:45:52,190
returns the number of bytes right so

00:45:49,249 --> 00:45:55,249
this case just to make the allocation of

00:45:52,190 --> 00:45:57,079
the memory box please it so if you use

00:45:55,249 --> 00:45:59,599
alkyl bytes it will call the constructor

00:45:57,079 --> 00:46:02,089
the number of bytes as you provided and

00:45:59,599 --> 00:46:07,219
the allocate elements which call it this

00:46:02,089 --> 00:46:13,910
and okay number of L got it okay thank

00:46:07,219 --> 00:46:17,690
you hey so when you decide to return an

00:46:13,910 --> 00:46:20,690
and vsd function wouldn't be possible to

00:46:17,690 --> 00:46:23,130
return just es d function by adding also

00:46:20,690 --> 00:46:26,370
the keywords that specified that

00:46:23,130 --> 00:46:27,990
to run in the GPU side and separate

00:46:26,370 --> 00:46:30,240
earning years and VSE function just

00:46:27,990 --> 00:46:35,550
return DSC function just what does it do

00:46:30,240 --> 00:46:37,830
differently internally well I wanted the

00:46:35,550 --> 00:46:40,380
original original attempt was was to

00:46:37,830 --> 00:46:43,170
create the lambda on the CPU and then

00:46:40,380 --> 00:46:48,690
just send it as an argument to GP

00:46:43,170 --> 00:46:52,770
function so like we had even in this

00:46:48,690 --> 00:46:53,430
example in the just find it quicker in

00:46:52,770 --> 00:46:56,510
the presentation

00:46:53,430 --> 00:46:56,510
[Music]

00:47:04,690 --> 00:47:10,420
like this example locate a lambda and

00:47:08,030 --> 00:47:12,800
send it as a parameter to Colonel

00:47:10,420 --> 00:47:16,640
basically wanted to use exactly the same

00:47:12,800 --> 00:47:18,380
column function the ED kernel is but to

00:47:16,640 --> 00:47:24,500
create the lambda in different way on

00:47:18,380 --> 00:47:32,390
the CPU before we send it to Japan sir

00:47:24,500 --> 00:47:33,830
what you asked if we would add device in

00:47:32,390 --> 00:47:35,510
from the sed function when we already

00:47:33,830 --> 00:47:48,890
returned the SD a function if that would

00:47:35,510 --> 00:47:50,990
work well and we stood on it I believe

00:47:48,890 --> 00:47:55,360
that that should contain the device

00:47:50,990 --> 00:47:58,550
keyboard in its in separate or

00:47:55,360 --> 00:48:00,770
instruction call crater but you can't

00:47:58,550 --> 00:48:07,370
edit from outside somehow it should be

00:48:00,770 --> 00:48:12,080
in the in the cooperator inside hi

00:48:07,370 --> 00:48:15,680
just one question regarding one key or

00:48:12,080 --> 00:48:18,110
that I noticed in one of your slides I

00:48:15,680 --> 00:48:20,870
don't remember exactly which number it

00:48:18,110 --> 00:48:22,970
was so you used underscore underscore in

00:48:20,870 --> 00:48:27,380
line underscore underscore rather than

00:48:22,970 --> 00:48:32,480
in line so I'm not sure which is a

00:48:27,380 --> 00:48:34,400
difference between this one yes which is

00:48:32,480 --> 00:48:38,560
the difference between this one and a

00:48:34,400 --> 00:48:42,830
normal one without so this one's called

00:48:38,560 --> 00:48:47,450
so this one is part o of the CUDA

00:48:42,830 --> 00:48:52,250
special keyboards and what you could you

00:48:47,450 --> 00:48:55,850
could have used the normal one first of

00:48:52,250 --> 00:48:57,590
all you can just keep at keyboard right

00:48:55,850 --> 00:48:59,840
as for the functions which are defined

00:48:57,590 --> 00:49:04,160
in the body of your class you don't even

00:48:59,840 --> 00:49:06,530
need to specify it as in line a griever

00:49:04,160 --> 00:49:08,990
for cuda compiler want to make sure that

00:49:06,530 --> 00:49:11,690
it's in mind before to add the even

00:49:08,990 --> 00:49:13,670
keyboard but I can tell you if if it's

00:49:11,690 --> 00:49:16,070
required or it will do it automatically

00:49:13,670 --> 00:49:19,820
without explicit specifying yeah

00:49:16,070 --> 00:49:21,950
because sometimes say I mean I'm sure

00:49:19,820 --> 00:49:25,130
that this code would compile without the

00:49:21,950 --> 00:49:28,850
underscore as well compile yes I'm not

00:49:25,130 --> 00:49:33,530
sure about I'm not sure about the the

00:49:28,850 --> 00:49:36,770
impact or there is any difference if you

00:49:33,530 --> 00:49:39,290
compile but my own question is the

00:49:36,770 --> 00:49:45,260
optimization whether the others code

00:49:39,290 --> 00:49:48,590
will get in line so don't know I just

00:49:45,260 --> 00:49:56,840
put it in all be on the safe side maybe

00:49:48,590 --> 00:50:03,200
but okay thanks hi regarding the memory

00:49:56,840 --> 00:50:04,720
types for so GPU access like pinned

00:50:03,200 --> 00:50:09,640
[Music]

00:50:04,720 --> 00:50:13,190
share it as I understood from your

00:50:09,640 --> 00:50:17,810
presentation they didn't perform that

00:50:13,190 --> 00:50:19,850
well as shown and zero copy and the

00:50:17,810 --> 00:50:23,960
unified yes shared is totally different

00:50:19,850 --> 00:50:26,660
okay type of them but could you explain

00:50:23,960 --> 00:50:32,600
like the classical use cases where it

00:50:26,660 --> 00:50:35,030
would be necessary to use these I think

00:50:32,600 --> 00:50:37,970
that it just was simplicity if you get a

00:50:35,030 --> 00:50:41,960
multi-gpu system it may be much harder

00:50:37,970 --> 00:50:45,980
to trace all the memory location of all

00:50:41,960 --> 00:50:49,280
the objects so on neck top on the

00:50:45,980 --> 00:50:51,380
desktop and have a single GPU I don't

00:50:49,280 --> 00:50:55,280
believe that as much use case to do it

00:50:51,380 --> 00:50:59,570
as after all we use CUDA to make the

00:50:55,280 --> 00:51:01,130
performance better right and but can I

00:50:59,570 --> 00:51:04,190
mention that in much more complex

00:51:01,130 --> 00:51:06,410
systems you know with tens of of GPUs

00:51:04,190 --> 00:51:09,830
it's much harder to do it manually so

00:51:06,410 --> 00:51:12,860
just want the driver to handle all the

00:51:09,830 --> 00:51:17,000
memory on demand and fade the texture

00:51:12,860 --> 00:51:20,960
performance maybe yeah well just to be

00:51:17,000 --> 00:51:23,859
clear I we work on the MX net library

00:51:20,960 --> 00:51:27,039
it's open source uses CUDA a lot

00:51:23,859 --> 00:51:30,190
we have this different like memory

00:51:27,039 --> 00:51:34,029
storages and one of in some of those are

00:51:30,190 --> 00:51:37,359
pinned or whatever and I never

00:51:34,029 --> 00:51:40,170
understood the purpose of them and

00:51:37,359 --> 00:51:42,789
that's why I'm asking what I have for so

00:51:40,170 --> 00:51:47,499
so usability is for sure

00:51:42,789 --> 00:51:49,630
I mean Fox's perspective so that there's

00:51:47,499 --> 00:51:52,059
the two case as I mentioned of a sparse

00:51:49,630 --> 00:51:54,640
access right if you only access a small

00:51:52,059 --> 00:51:57,279
portion of the data and you don't know

00:51:54,640 --> 00:51:59,349
in advance which one you will do so that

00:51:57,279 --> 00:52:02,200
instead of copying all the data to GPU

00:51:59,349 --> 00:52:05,130
only access a small part of it it was

00:52:02,200 --> 00:52:07,749
somehow automatically copied less data

00:52:05,130 --> 00:52:10,059
but I think that's the most of the

00:52:07,749 --> 00:52:13,720
examples you want to process all the

00:52:10,059 --> 00:52:16,210
data at the copy to GPU and then I would

00:52:13,720 --> 00:52:19,480
not use not the zero copy and not the

00:52:16,210 --> 00:52:22,660
not the unified memory like if the code

00:52:19,480 --> 00:52:24,910
is not too too complicated and you

00:52:22,660 --> 00:52:27,970
managed to do it without then I believe

00:52:24,910 --> 00:52:36,130
that it's best performance to do all the

00:52:27,970 --> 00:52:37,680
manual coding operations this line is

00:52:36,130 --> 00:52:40,210
never going to end

00:52:37,680 --> 00:52:44,410
question you mentioned the thrust

00:52:40,210 --> 00:52:48,880
library provides shield like containers

00:52:44,410 --> 00:52:51,960
like lecture so the hood compiler by

00:52:48,880 --> 00:52:55,450
itself doesn't provide containers and

00:52:51,960 --> 00:53:00,759
what other containers we might be able

00:52:55,450 --> 00:53:04,509
to use so CUDA actually adjust that's

00:53:00,759 --> 00:53:07,799
the way to write to launch a code on GP

00:53:04,509 --> 00:53:11,680
right they could itself the basic CUDA

00:53:07,799 --> 00:53:13,980
does not include any any containers and

00:53:11,680 --> 00:53:17,289
the frost library on top of CUDA

00:53:13,980 --> 00:53:19,170
supports only the vector type so it has

00:53:17,289 --> 00:53:23,739
the device vector and the host vector

00:53:19,170 --> 00:53:26,410
but not other containers still for many

00:53:23,739 --> 00:53:30,220
algorithms and surely for this one but

00:53:26,410 --> 00:53:34,089
how many others still very useful as to

00:53:30,220 --> 00:53:36,910
the vector so the performance I believe

00:53:34,089 --> 00:53:41,470
it will be not the same as you can

00:53:36,910 --> 00:53:42,640
with the manual code and especially if

00:53:41,470 --> 00:53:45,040
you take care for all the memory

00:53:42,640 --> 00:53:47,860
alignment and so on then often the

00:53:45,040 --> 00:53:49,300
memory will not be continuous aligned in

00:53:47,860 --> 00:53:56,680
the buffer like in the case of the

00:53:49,300 --> 00:53:59,710
vector so when you do like work manually

00:53:56,680 --> 00:54:02,830
then you achieve the best performance

00:53:59,710 --> 00:54:04,840
and when you go up you get usability and

00:54:02,830 --> 00:54:10,180
you lose some of the performance thank

00:54:04,840 --> 00:54:10,590
you okay thanks to the people remaining

00:54:10,180 --> 00:54:13,820
in the room

00:54:10,590 --> 00:54:13,820

YouTube URL: https://www.youtube.com/watch?v=HIJTRrm9nzY


