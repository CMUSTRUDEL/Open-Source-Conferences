Title: CppCon 2018: Olivier Giroux “High-Radix Concurrent C++”
Publication date: 2018-10-18
Playlist: CppCon 2018
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2018
—
In this talk we will share the joy of seeing ordinary C++ concurrent code, doing ordinary concurrent things in 100000 concurrent threads. I’ll analyze how the code is behaving and point out what is similar and different about the execution of that code at this scale. This talk is the code-heavy continuation of last year’s English-heavy “Designing C++ Hardware” about the Volta architecture.

Featuring:

Multiple compilers.
Godbolting.
C++20 concurrency predictions.
Live demo (attempt).
— 
Olivier Giroux, NVIDIA
Distinguished Architect

Olivier Giroux has worked on nine GPU and five SM architecture generations released by NVIDIA. Lately, he works to clarify the forms and semantics of valid GPU programs, present and future. He was the programming model lead for the NVIDIA Volta architecture. He is the chair of SG1, the Concurrency study group of the ISO C++ committee, and is a passionate contributor to C++'s forward progress guarantees and memory model.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,910
good afternoon everybody my name is le

00:00:02,490 --> 00:00:10,290
Bisou who I am an architect and NVIDIA I

00:00:05,910 --> 00:00:13,139
designed GPUs I'm but do what do I have

00:00:10,290 --> 00:00:16,740
to do with C++ well I'm also on the C++

00:00:13,139 --> 00:00:18,449
committee I'm the chair of study group

00:00:16,740 --> 00:00:23,939
one which is a concurrency and

00:00:18,449 --> 00:00:27,510
parallelism study group okay so I chose

00:00:23,939 --> 00:00:29,220
this title of high radix concurrent C++

00:00:27,510 --> 00:00:31,679
because I was thinking about two things

00:00:29,220 --> 00:00:34,320
I was thinking about high radix data

00:00:31,679 --> 00:00:36,780
structures you know trees with large fan

00:00:34,320 --> 00:00:38,520
outs and I was thinking about high radix

00:00:36,780 --> 00:00:40,309
threading models you know groups of

00:00:38,520 --> 00:00:43,860
threads and groups of groups of threads

00:00:40,309 --> 00:00:47,940
that themselves involve large numbers

00:00:43,860 --> 00:00:51,899
you know groups of 256 groups of 256

00:00:47,940 --> 00:00:54,059
threads is a perfectly normal thing in

00:00:51,899 --> 00:00:59,789
my world so I was thinking about these

00:00:54,059 --> 00:01:00,719
two things together and I wanted to show

00:00:59,789 --> 00:01:02,789
you something that I think would

00:01:00,719 --> 00:01:05,180
surprise you all right so the big

00:01:02,789 --> 00:01:10,080
spoilers is we're gonna talk about abuse

00:01:05,180 --> 00:01:14,119
you're not surprised ok so last year I

00:01:10,080 --> 00:01:17,369
give a talk about the design of Volta

00:01:14,119 --> 00:01:18,930
Volta was our big supercomputing

00:01:17,369 --> 00:01:23,070
architecture that we launched last year

00:01:18,930 --> 00:01:26,009
a data center architecture that we we

00:01:23,070 --> 00:01:30,030
designed with C++ in mind from the

00:01:26,009 --> 00:01:33,450
get-go there are large parts of the

00:01:30,030 --> 00:01:36,810
design that we completely replaced and

00:01:33,450 --> 00:01:39,360
as we were designing the replacements we

00:01:36,810 --> 00:01:41,430
would occasionally quote the C++

00:01:39,360 --> 00:01:44,579
standard at one another so that's how

00:01:41,430 --> 00:01:47,729
much of an impact our thought principal

00:01:44,579 --> 00:01:50,220
suppose had okay so we refer to our

00:01:47,729 --> 00:01:53,549
architectures for compute as having a

00:01:50,220 --> 00:01:56,909
capability level here I'm saying that

00:01:53,549 --> 00:01:59,430
Volta has a compute 7.0 compute seven

00:01:56,909 --> 00:02:01,909
zero capability level and that

00:01:59,430 --> 00:02:04,799
introduced a number of cool things that

00:02:01,909 --> 00:02:06,390
you may or may not have heard heard

00:02:04,799 --> 00:02:07,920
about okay so the first one I'm pretty

00:02:06,390 --> 00:02:11,430
sure you've heard about and that's our

00:02:07,920 --> 00:02:13,590
fused Mme operations it used to be that

00:02:11,430 --> 00:02:17,910
fuse FMA was a big thing

00:02:13,590 --> 00:02:21,000
is your code thank you for FMA nowadays

00:02:17,910 --> 00:02:23,160
we're really looking at fusing matrix

00:02:21,000 --> 00:02:25,290
multiply operations to accelerate matrix

00:02:23,160 --> 00:02:28,560
multiply much more okay so that was

00:02:25,290 --> 00:02:30,569
definitely the biggest headliner of our

00:02:28,560 --> 00:02:33,870
launch Pro volta was our four by four by

00:02:30,569 --> 00:02:35,160
four matrix multiply hardware okay but

00:02:33,870 --> 00:02:38,940
there were other cool things in there

00:02:35,160 --> 00:02:43,140
that are that are particularly of

00:02:38,940 --> 00:02:45,000
interest to C++ developers and so two

00:02:43,140 --> 00:02:46,860
examples of that is that we added

00:02:45,000 --> 00:02:48,390
support for starvation free algorithms

00:02:46,860 --> 00:02:51,209
which are going to make a showing in

00:02:48,390 --> 00:02:53,640
this talk and we redesigned the memory

00:02:51,209 --> 00:02:56,970
model for C++ C lab okay but last year

00:02:53,640 --> 00:02:59,760
that may have felt pretty distant and

00:02:56,970 --> 00:03:02,340
academic for for a lot of you you know

00:02:59,760 --> 00:03:04,349
Volta GPUs although they're available to

00:03:02,340 --> 00:03:07,349
the mass market and they're fairly

00:03:04,349 --> 00:03:09,180
expensive and a lot of them went to data

00:03:07,349 --> 00:03:12,540
centers or supercomputers for example

00:03:09,180 --> 00:03:14,489
this is a picture of summit the world's

00:03:12,540 --> 00:03:18,569
fastest supercomputer which is built on

00:03:14,489 --> 00:03:20,040
Volta there was a big news that was

00:03:18,569 --> 00:03:21,959
coming this year that you know I've

00:03:20,040 --> 00:03:23,970
known all along obviously was coming

00:03:21,959 --> 00:03:27,389
this year the big news is is that this

00:03:23,970 --> 00:03:29,340
year we followed up on Volta with a new

00:03:27,389 --> 00:03:31,859
architecture called Turing you have

00:03:29,340 --> 00:03:34,709
probably heard possibly heard about

00:03:31,859 --> 00:03:37,109
Turing you've possibly heard that Turing

00:03:34,709 --> 00:03:41,099
includes ray tracing acceleration which

00:03:37,109 --> 00:03:42,630
is super cool and I'm stoked about but

00:03:41,099 --> 00:03:44,700
there's something you may not have heard

00:03:42,630 --> 00:03:46,500
what you may not have heard actually is

00:03:44,700 --> 00:03:49,440
that Turing includes all of the

00:03:46,500 --> 00:03:53,280
improvements from Volta as well so

00:03:49,440 --> 00:03:55,739
Turing is compute 7/5 and compute 7/5 is

00:03:53,280 --> 00:03:58,980
a strict superset of compute seven zero

00:03:55,739 --> 00:04:02,310
now that starts to be a really big deal

00:03:58,980 --> 00:04:04,530
for all GPU for all C++ developers

00:04:02,310 --> 00:04:06,510
because a lot of you will will come to

00:04:04,530 --> 00:04:08,419
own one of these like you might buy one

00:04:06,510 --> 00:04:10,620
because you're building a gaming rig or

00:04:08,419 --> 00:04:11,910
you know eventually down the line

00:04:10,620 --> 00:04:14,430
they'll be available in many more

00:04:11,910 --> 00:04:15,859
systems and so you it's possible that

00:04:14,430 --> 00:04:20,039
you'll just come to own one of these

00:04:15,859 --> 00:04:23,130
it's created C++ you're a C++ developer

00:04:20,039 --> 00:04:24,570
you should run C++ on this alright so

00:04:23,130 --> 00:04:26,980
I'm going to tell you what you can

00:04:24,570 --> 00:04:29,550
expect and you

00:04:26,980 --> 00:04:34,480
how much fun that's going to be actually

00:04:29,550 --> 00:04:38,050
okay so I was thinking about this much

00:04:34,480 --> 00:04:40,270
earlier in the year I knew Turing was

00:04:38,050 --> 00:04:44,740
about to hit shelves around city peak on

00:04:40,270 --> 00:04:47,980
and so I was thinking what would make a

00:04:44,740 --> 00:04:49,930
completely unexpected demo something I

00:04:47,980 --> 00:04:52,300
would I could show you and he would be

00:04:49,930 --> 00:04:56,590
like wow I didn't expect to be used to

00:04:52,300 --> 00:04:59,320
be good at this okay so what do people

00:04:56,590 --> 00:05:02,800
think GPUs are good for by the way this

00:04:59,320 --> 00:05:04,420
is not this is me making a guess as to

00:05:02,800 --> 00:05:05,920
what people think or talking to people

00:05:04,420 --> 00:05:08,500
do you know what they think GPUs are

00:05:05,920 --> 00:05:10,060
good for I think GPUs are good for more

00:05:08,500 --> 00:05:12,840
than for many more things all right so

00:05:10,060 --> 00:05:14,890
what the people think GPUs are good for

00:05:12,840 --> 00:05:16,960
people think you win the point people

00:05:14,890 --> 00:05:20,380
think GPUs they think floats little

00:05:16,960 --> 00:05:22,180
floats big floats double doubles you

00:05:20,380 --> 00:05:25,990
know double double the scientific

00:05:22,180 --> 00:05:28,510
computation complex doubles people are

00:05:25,990 --> 00:05:31,840
thinking arrays like they're thinking I

00:05:28,510 --> 00:05:33,700
know when gpgpu came out you would

00:05:31,840 --> 00:05:35,350
allocate large buffers and you would

00:05:33,700 --> 00:05:37,750
move these large buffers and

00:05:35,350 --> 00:05:40,360
coarse-grained data movements okay so

00:05:37,750 --> 00:05:42,460
I'm thinking floating-point data I'm

00:05:40,360 --> 00:05:45,100
thinking large arrays I'm thinking

00:05:42,460 --> 00:05:47,800
highly regular memory access you know

00:05:45,100 --> 00:05:51,100
adjacent threads accessing adjacent

00:05:47,800 --> 00:05:55,690
memory locations and I'm thinking I'm

00:05:51,100 --> 00:05:58,240
thinking when I put my c++ is g1 head on

00:05:55,690 --> 00:06:01,110
I'm thinking lock-free algorithms I

00:05:58,240 --> 00:06:05,400
can't actually write arbitrary

00:06:01,110 --> 00:06:09,640
synchronizing code on GPUs because if I

00:06:05,400 --> 00:06:12,760
write a lock for example and you could

00:06:09,640 --> 00:06:15,670
search for that on the web if I write a

00:06:12,760 --> 00:06:19,030
lock for example I might expect GPUs to

00:06:15,670 --> 00:06:21,610
have problems with that what do people

00:06:19,030 --> 00:06:23,080
think GPUs are bad for well the have you

00:06:21,610 --> 00:06:26,950
ever heard of anybody doing string

00:06:23,080 --> 00:06:29,370
processing on a GPU probably not what

00:06:26,950 --> 00:06:35,950
about node base data structures you know

00:06:29,370 --> 00:06:37,870
so trees large trees what about random

00:06:35,950 --> 00:06:39,640
memory walks so I go to memory and it's

00:06:37,870 --> 00:06:40,020
just a total scatter shot there's like

00:06:39,640 --> 00:06:42,430
no

00:06:40,020 --> 00:06:45,130
not going for adjacent locations do you

00:06:42,430 --> 00:06:50,350
have a question oh you thought of

00:06:45,130 --> 00:06:52,300
strings oh good good good good

00:06:50,350 --> 00:06:55,750
how about starvation free algorithms you

00:06:52,300 --> 00:06:58,770
know algorithms that for example polling

00:06:55,750 --> 00:07:01,030
on a memory location with potentially

00:06:58,770 --> 00:07:02,170
contention with other threads you're

00:07:01,030 --> 00:07:03,910
waiting for a thread to set this

00:07:02,170 --> 00:07:05,350
location but you don't really know which

00:07:03,910 --> 00:07:08,410
other thread is setting this location

00:07:05,350 --> 00:07:13,470
you know it might be in the same group

00:07:08,410 --> 00:07:16,120
as you we're in a different group okay I

00:07:13,470 --> 00:07:22,060
think most people think GPUs are bad for

00:07:16,120 --> 00:07:24,460
this thanks to the peanut gallery so I'm

00:07:22,060 --> 00:07:26,410
thinking to myself let's build a try

00:07:24,460 --> 00:07:29,710
let's go build a try all right let's do

00:07:26,410 --> 00:07:34,180
that so what's a try so a try is a node

00:07:29,710 --> 00:07:36,100
base of the structure and I most often

00:07:34,180 --> 00:07:41,680
refer to it as a radix Treaty so it's

00:07:36,100 --> 00:07:45,100
it's a tree with a large fan-out not you

00:07:41,680 --> 00:07:48,160
know not a binary search tree that

00:07:45,100 --> 00:07:51,250
encodes the the you know that is

00:07:48,160 --> 00:07:54,210
essentially a map where the keys are not

00:07:51,250 --> 00:07:57,120
stored in the tree the keys are the

00:07:54,210 --> 00:07:59,980
location in the tree okay so for example

00:07:57,120 --> 00:08:03,970
if I were to insert if I wanted to make

00:07:59,980 --> 00:08:05,620
a map of strings to something I would

00:08:03,970 --> 00:08:08,680
have a root node which is an empty

00:08:05,620 --> 00:08:12,070
string then I would have a first level

00:08:08,680 --> 00:08:13,810
of nodes which is that in this index by

00:08:12,070 --> 00:08:15,430
the alphabet of the first letter in a

00:08:13,810 --> 00:08:17,320
word and then after that there'd be

00:08:15,430 --> 00:08:18,910
another node with the second letter of

00:08:17,320 --> 00:08:21,340
the alphabet and then another node with

00:08:18,910 --> 00:08:26,230
the third letter of the alphabet and so

00:08:21,340 --> 00:08:29,680
on okay so radix 26 that's high hence I

00:08:26,230 --> 00:08:32,589
get it got a top title and like I just

00:08:29,680 --> 00:08:34,630
said it you can implement you know a map

00:08:32,589 --> 00:08:36,490
conceptually a map of string to

00:08:34,630 --> 00:08:39,700
something using this data structure and

00:08:36,490 --> 00:08:40,990
in this demo in this talk that's

00:08:39,700 --> 00:08:43,810
something that's gonna be a count it's

00:08:40,990 --> 00:08:47,230
gonna be a we're going to compute a word

00:08:43,810 --> 00:08:49,150
frequency over a text corpus by

00:08:47,230 --> 00:08:50,920
inserting into a tri data structure and

00:08:49,150 --> 00:08:52,520
incrementing the count as we find or

00:08:50,920 --> 00:08:59,900
insert

00:08:52,520 --> 00:09:01,460
to the try okay so this is this is sort

00:08:59,900 --> 00:09:03,650
of an interview question type of problem

00:09:01,460 --> 00:09:06,380
what might what might our code look like

00:09:03,650 --> 00:09:08,900
so here's a summary of what my data

00:09:06,380 --> 00:09:10,880
structure will look like and how I index

00:09:08,900 --> 00:09:13,100
it and after this we're gonna look at

00:09:10,880 --> 00:09:14,660
the algorithm this isn't supposed to be

00:09:13,100 --> 00:09:16,780
a trick question it's not supposed to be

00:09:14,660 --> 00:09:19,370
complicated this is just a sequential

00:09:16,780 --> 00:09:21,980
sequentially building a try so here I've

00:09:19,370 --> 00:09:25,780
got I've got one type of structure try

00:09:21,980 --> 00:09:30,590
each note of a try is itself a try I

00:09:25,780 --> 00:09:32,720
have 26 down level references that are

00:09:30,590 --> 00:09:36,800
each one a pointer to another try and

00:09:32,720 --> 00:09:38,510
each node gets one of these counts this

00:09:36,800 --> 00:09:40,760
helper function here is going to take

00:09:38,510 --> 00:09:41,990
some character not a Unicode character

00:09:40,760 --> 00:09:44,000
in this case but it's gonna take some

00:09:41,990 --> 00:09:48,650
character and it will compute the index

00:09:44,000 --> 00:09:52,010
into into the fan out of the tree and

00:09:48,650 --> 00:09:54,740
I'll return minus one when it's not an

00:09:52,010 --> 00:09:57,560
alphabetical character it's not very

00:09:54,740 --> 00:10:00,440
complicated I am NOT looking to make a

00:09:57,560 --> 00:10:03,560
thing of beauty of this it's just a

00:10:00,440 --> 00:10:05,420
simple version of this okay how do we

00:10:03,560 --> 00:10:07,520
make a try well this is going to be some

00:10:05,420 --> 00:10:10,990
of the heart of my code base here make

00:10:07,520 --> 00:10:13,370
try function takes a try to insert into

00:10:10,990 --> 00:10:15,500
for this example it's going to start out

00:10:13,370 --> 00:10:16,940
empty but actually for the Oliver name

00:10:15,500 --> 00:10:18,320
it doesn't need to start out empty it

00:10:16,940 --> 00:10:22,160
could be you could think of it as a pen

00:10:18,320 --> 00:10:24,970
to try I'm going to make a bump

00:10:22,160 --> 00:10:27,860
allocator which is the pointer into some

00:10:24,970 --> 00:10:30,080
pre-allocated range of tri nodes I'm

00:10:27,860 --> 00:10:32,180
taking allocation out of this problem

00:10:30,080 --> 00:10:34,490
we're only going to look at the the main

00:10:32,180 --> 00:10:37,490
algorithm because that's gonna be my

00:10:34,490 --> 00:10:40,370
bump alligator a pointer into a range of

00:10:37,490 --> 00:10:42,200
free nodes and then a begin and end into

00:10:40,370 --> 00:10:44,270
a character sequence which is going to

00:10:42,200 --> 00:10:50,390
be the text corpus that will find words

00:10:44,270 --> 00:10:53,990
from okay the old rhythm itself is not

00:10:50,390 --> 00:10:56,660
super complicated basically we start at

00:10:53,990 --> 00:11:00,140
the root node and we walk characters

00:10:56,660 --> 00:11:02,740
until we so for each character we pick a

00:11:00,140 --> 00:11:05,930
sub node of the node we're looking at

00:11:02,740 --> 00:11:09,170
we're going to keep going

00:11:05,930 --> 00:11:11,839
like this as long as we don't find a

00:11:09,170 --> 00:11:14,270
space or a non alphabetical character

00:11:11,839 --> 00:11:16,490
where the index will be minus one at

00:11:14,270 --> 00:11:19,149
that point we say wherever we are in the

00:11:16,490 --> 00:11:22,970
tree we increment the count there and

00:11:19,149 --> 00:11:25,610
and reset to the root if there is no

00:11:22,970 --> 00:11:28,370
node for the node we're looking for

00:11:25,610 --> 00:11:29,720
then it'll be null and we'll just bump

00:11:28,370 --> 00:11:34,149
the bump allocator and assign that

00:11:29,720 --> 00:11:37,730
pointer to that place in the tree okay

00:11:34,149 --> 00:11:38,899
not too complicated I want to I want to

00:11:37,730 --> 00:11:40,220
point out that all of the code in the

00:11:38,899 --> 00:11:42,080
stock is going to be available in github

00:11:40,220 --> 00:11:42,410
at the end so you know don't worry about

00:11:42,080 --> 00:11:45,890
it

00:11:42,410 --> 00:11:48,830
you can you can check it out after okay

00:11:45,890 --> 00:11:53,270
so let's go run this and this is the

00:11:48,830 --> 00:11:55,640
part where we attempt live demos um so I

00:11:53,270 --> 00:11:58,180
have three systems here that are

00:11:55,640 --> 00:12:02,720
standing by to run this demo for me I

00:11:58,180 --> 00:12:04,160
here I've so I've compiled in in the

00:12:02,720 --> 00:12:06,950
sources that are available with the

00:12:04,160 --> 00:12:09,230
stock I have compiled try St the Tri

00:12:06,950 --> 00:12:11,779
single-threaded which is exactly the

00:12:09,230 --> 00:12:14,630
code I just showed you and I can

00:12:11,779 --> 00:12:16,130
confront this over a number of books

00:12:14,630 --> 00:12:19,430
that I got from the Gutenberg Project

00:12:16,130 --> 00:12:22,730
this is these text files they come by

00:12:19,430 --> 00:12:24,890
just you know W getting books from the

00:12:22,730 --> 00:12:26,990
Gutenberg Project so here I just did

00:12:24,890 --> 00:12:28,779
this on my laptop right now I have

00:12:26,990 --> 00:12:32,690
another I have another system over here

00:12:28,779 --> 00:12:34,430
run this it's a lower clock system but

00:12:32,690 --> 00:12:38,800
it has many more cores and that's gonna

00:12:34,430 --> 00:12:38,800
become important in the next demo okay

00:12:38,830 --> 00:12:44,690
alright so on the on the system that has

00:12:43,070 --> 00:12:47,600
the many more cores that we're going to

00:12:44,690 --> 00:12:50,150
use for the other demos which is the

00:12:47,600 --> 00:12:53,660
specs are down here it takes 68

00:12:50,150 --> 00:12:58,279
milliseconds to Newton's ones right okay

00:12:53,660 --> 00:13:00,500
well obviously we're gonna multi thread

00:12:58,279 --> 00:13:02,839
this we're gonna we're gonna go to the

00:13:00,500 --> 00:13:04,510
sg-1 version of this okay so how do we

00:13:02,839 --> 00:13:06,589
how are we going to convert our

00:13:04,510 --> 00:13:07,730
sequential version into a concurrent

00:13:06,589 --> 00:13:10,160
version there's a couple of things that

00:13:07,730 --> 00:13:12,680
we have to do one of the first things

00:13:10,160 --> 00:13:14,089
that we have to do is that we have to

00:13:12,680 --> 00:13:16,250
realize that we're now going to become

00:13:14,089 --> 00:13:18,260
currently accessing the try and actually

00:13:16,250 --> 00:13:19,430
there's no member of the try that we

00:13:18,260 --> 00:13:22,670
will not access

00:13:19,430 --> 00:13:24,740
from one thread or with interlock so

00:13:22,670 --> 00:13:26,570
we're actually going to make the count

00:13:24,740 --> 00:13:29,830
atomic and we're going to make the

00:13:26,570 --> 00:13:33,440
pointers atomic and we're going to

00:13:29,830 --> 00:13:35,420
eliminate insertion races by using an

00:13:33,440 --> 00:13:37,480
atomic flag and the algorithm is going

00:13:35,420 --> 00:13:41,390
to be on on the next on the next slide

00:13:37,480 --> 00:13:42,770
there's two more things we need to do we

00:13:41,390 --> 00:13:44,450
need to realize that the bump allocator

00:13:42,770 --> 00:13:45,980
will also be used concurrently and so

00:13:44,450 --> 00:13:49,970
we're gonna make that one at a Tomic as

00:13:45,980 --> 00:13:51,470
well and then finally there's something

00:13:49,970 --> 00:13:53,870
pretty fundamental about making the

00:13:51,470 --> 00:13:55,610
multi-threaded version where i'm going

00:13:53,870 --> 00:13:57,560
to invoke this make tri function in

00:13:55,610 --> 00:14:00,470
multiple threads they need to know which

00:13:57,560 --> 00:14:03,470
thread they are so each one is going to

00:14:00,470 --> 00:14:07,010
receive an index that says which thread

00:14:03,470 --> 00:14:08,480
ID it is within domain which is the

00:14:07,010 --> 00:14:11,000
total number of threads that are going

00:14:08,480 --> 00:14:15,410
to be running this algorithm okay

00:14:11,000 --> 00:14:17,510
this part so far is pretty simple there

00:14:15,410 --> 00:14:21,020
is more complexity in how we go about

00:14:17,510 --> 00:14:22,510
implementing the algorithm okay so we're

00:14:21,020 --> 00:14:25,339
gonna do it in three easy steps

00:14:22,510 --> 00:14:28,339
first step is there's going to be a part

00:14:25,339 --> 00:14:29,870
of our program that is going to split

00:14:28,339 --> 00:14:32,089
the input range to a bunch of strips

00:14:29,870 --> 00:14:34,459
we're going to do this relatively

00:14:32,089 --> 00:14:36,620
crudely let's say we're gonna take the

00:14:34,459 --> 00:14:38,510
whole input the whole text corpus and

00:14:36,620 --> 00:14:40,760
we're gonna slice it into domain number

00:14:38,510 --> 00:14:44,089
of slices and then each thread will go

00:14:40,760 --> 00:14:47,510
access the index numbered slice within

00:14:44,089 --> 00:14:49,040
the domain number of slices okay so

00:14:47,510 --> 00:14:51,020
we're gonna do that but there is a trick

00:14:49,040 --> 00:14:53,209
there's a trick when you do this now the

00:14:51,020 --> 00:14:54,470
boundaries of these slices for the input

00:14:53,209 --> 00:14:57,709
actually some of them fall into the

00:14:54,470 --> 00:15:01,250
middle of a word so what we're gonna do

00:14:57,709 --> 00:15:04,339
is either as a pre adjustment step or as

00:15:01,250 --> 00:15:06,680
part of how the algorithm runs we're

00:15:04,339 --> 00:15:09,920
going to adjust the strips start and end

00:15:06,680 --> 00:15:13,130
to start at the next word and end at the

00:15:09,920 --> 00:15:14,180
next word with treatment of the boundary

00:15:13,130 --> 00:15:16,760
conditions where you start at the

00:15:14,180 --> 00:15:20,810
beginning in you end at the end okay not

00:15:16,760 --> 00:15:24,320
too complicated alright finally we need

00:15:20,810 --> 00:15:26,959
to we can run the same code as a

00:15:24,320 --> 00:15:29,000
sequential version except that

00:15:26,959 --> 00:15:32,390
insertions now are concurrent which is

00:15:29,000 --> 00:15:33,350
why we made the of the members of the

00:15:32,390 --> 00:15:36,110
try structure

00:15:33,350 --> 00:15:40,280
atomic and so we're gonna run code like

00:15:36,110 --> 00:15:44,240
this okay so this is just replacing the

00:15:40,280 --> 00:15:47,270
node insertion part we're going to check

00:15:44,240 --> 00:15:49,370
if the pointer is null and if it's not

00:15:47,270 --> 00:15:51,200
then we're good to go we can just use

00:15:49,370 --> 00:15:53,600
that pointer you just read it at the

00:15:51,200 --> 00:15:56,840
bottom if it is no when we get there

00:15:53,600 --> 00:15:58,940
then one of the threads has to be

00:15:56,840 --> 00:16:02,510
elected to go do the insertion and

00:15:58,940 --> 00:16:05,140
that's the test and set that is that is

00:16:02,510 --> 00:16:07,820
right here so this test and set

00:16:05,140 --> 00:16:10,850
determines if you're the first person to

00:16:07,820 --> 00:16:13,970
arrive from thread to the first led to

00:16:10,850 --> 00:16:17,540
arrive to insert the node and that tells

00:16:13,970 --> 00:16:19,070
you to go to go do it okay that sends

00:16:17,540 --> 00:16:21,410
you over here so that's on the false

00:16:19,070 --> 00:16:23,600
right if the previous value of the flag

00:16:21,410 --> 00:16:26,030
is false then you take this branch and

00:16:23,600 --> 00:16:27,950
you're the one who's the first one and

00:16:26,030 --> 00:16:29,990
then you do essentially the same thing

00:16:27,950 --> 00:16:33,350
as the other slide except just atomic if

00:16:29,990 --> 00:16:35,180
I you know we used to increment and then

00:16:33,350 --> 00:16:40,340
assign so we do this again we fetch add

00:16:35,180 --> 00:16:44,480
and then we store okay now when you

00:16:40,340 --> 00:16:47,450
restore to the pointer that will tell

00:16:44,480 --> 00:16:50,530
the other threads the one that lost the

00:16:47,450 --> 00:16:53,690
test and set and went into a spin loop

00:16:50,530 --> 00:16:55,850
waiting for it to become non-law that

00:16:53,690 --> 00:16:59,660
releases that permits those threads to

00:16:55,850 --> 00:17:03,920
make more progress now okay this is the

00:16:59,660 --> 00:17:06,770
this is where this becomes not a lock

00:17:03,920 --> 00:17:09,230
tree algorithm but a starvation tree

00:17:06,770 --> 00:17:12,079
algorithm and that's pretty important

00:17:09,230 --> 00:17:14,120
because it was very natural for me to

00:17:12,079 --> 00:17:16,790
make this step here it was very net it

00:17:14,120 --> 00:17:20,959
was very easy for me to deal with the

00:17:16,790 --> 00:17:23,570
RACI insertions into the tribe by using

00:17:20,959 --> 00:17:25,310
this spin lock approach this spin lock

00:17:23,570 --> 00:17:28,220
approach is not a major concern because

00:17:25,310 --> 00:17:30,320
we know that first of all the odds of

00:17:28,220 --> 00:17:32,240
reaching the spin lock are very low and

00:17:30,320 --> 00:17:34,100
then when you reach the spin lock you

00:17:32,240 --> 00:17:36,590
know for sure there's another thread

00:17:34,100 --> 00:17:40,970
that's going to set this pointer to non

00:17:36,590 --> 00:17:44,760
null very soon so it's not really a

00:17:40,970 --> 00:17:47,730
question of performance or a question of

00:17:44,760 --> 00:17:51,410
mode I'm not very concerned about the

00:17:47,730 --> 00:17:56,910
presence of the spin lock on that basis

00:17:51,410 --> 00:18:00,720
but I am concerned both as a GPU

00:17:56,910 --> 00:18:03,230
architect and as a member of sg-1 I am

00:18:00,720 --> 00:18:07,020
concerned that now this algorithm is

00:18:03,230 --> 00:18:08,580
fundamentally different from from other

00:18:07,020 --> 00:18:10,590
algorithms I could have chosen here now

00:18:08,580 --> 00:18:13,620
it's a starvation free algorithm it

00:18:10,590 --> 00:18:16,110
requires a benevolent scheduler for this

00:18:13,620 --> 00:18:17,550
application to make progress and you can

00:18:16,110 --> 00:18:21,450
learn all about that by searching for on

00:18:17,550 --> 00:18:23,040
the nature of Argos ok we're going to

00:18:21,450 --> 00:18:26,970
talk about this again a little bit later

00:18:23,040 --> 00:18:30,240
in the context of GPUs ok so let's just

00:18:26,970 --> 00:18:33,000
go and run this well I can run this on

00:18:30,240 --> 00:18:36,090
my laptop right here there's a couple of

00:18:33,000 --> 00:18:37,850
things that just happened Orange oh my

00:18:36,090 --> 00:18:40,830
laptop does not have 40 cores we're

00:18:37,850 --> 00:18:42,270
we're logged in somewhere now um so I

00:18:40,830 --> 00:18:46,140
can run this on the same system where I

00:18:42,270 --> 00:18:47,940
ran the single threaded version there's

00:18:46,140 --> 00:18:50,220
two things that you can notice well one

00:18:47,940 --> 00:18:51,720
is that actually if we run the

00:18:50,220 --> 00:18:55,430
multi-threaded version in single thread

00:18:51,720 --> 00:18:57,920
we slowed down and that's to be expected

00:18:55,430 --> 00:19:01,650
it's to be expected for a few reasons

00:18:57,920 --> 00:19:03,690
one the code generation is not as good

00:19:01,650 --> 00:19:05,400
as the original code generation because

00:19:03,690 --> 00:19:06,780
now we have Atomics and we have releases

00:19:05,400 --> 00:19:08,430
and acquires and there are fewer

00:19:06,780 --> 00:19:13,590
optimizations that the compiler could do

00:19:08,430 --> 00:19:15,930
it still can do many but not all and in

00:19:13,590 --> 00:19:18,360
this example here actually we did spin

00:19:15,930 --> 00:19:21,060
it we did spin a thread we spun up one

00:19:18,360 --> 00:19:23,850
and then we joined it but we did spin up

00:19:21,060 --> 00:19:26,790
a thread so we slowed down in single

00:19:23,850 --> 00:19:28,230
threaded execution mode but we did meta

00:19:26,790 --> 00:19:29,910
speed up so that's pretty cool

00:19:28,230 --> 00:19:32,220
that's pretty cool and you know the

00:19:29,910 --> 00:19:34,320
results are not completely repeatable

00:19:32,220 --> 00:19:37,710
you know you can see that it bounces

00:19:34,320 --> 00:19:40,080
around and I guess here we could try it

00:19:37,710 --> 00:19:41,880
on my lap thought you know so

00:19:40,080 --> 00:19:43,740
interestingly my laptop in that machine

00:19:41,880 --> 00:19:47,190
sort of solved the problem in about the

00:19:43,740 --> 00:19:48,570
same time well I mean no but there's a

00:19:47,190 --> 00:19:51,870
reason I mean that other machine that

00:19:48,570 --> 00:19:54,120
other machine is a is a large the on

00:19:51,870 --> 00:19:55,680
plots of course it's clocked lower of

00:19:54,120 --> 00:19:58,300
course it's got a beefier memory system

00:19:55,680 --> 00:19:59,980
but it's clocked lower my laptop here

00:19:58,300 --> 00:20:02,370
probably just you know boosted to its

00:19:59,980 --> 00:20:08,920
max clocks for a short amount of time

00:20:02,370 --> 00:20:10,480
okay so in some run the stability of the

00:20:08,920 --> 00:20:13,840
run is little bit annoying but I'm in

00:20:10,480 --> 00:20:15,910
summer and I got these numbers and we

00:20:13,840 --> 00:20:18,340
didn't that is speed up you saw the

00:20:15,910 --> 00:20:20,680
other numbers so you can see what the

00:20:18,340 --> 00:20:22,840
nature of the speed-up looks like so we

00:20:20,680 --> 00:20:25,900
did Metis speed up I mean we didn't

00:20:22,840 --> 00:20:28,090
quite strong scale here but uh we got a

00:20:25,900 --> 00:20:29,920
speed-up so that's good you know I feel

00:20:28,090 --> 00:20:34,900
like the small amount of effort I put

00:20:29,920 --> 00:20:38,260
into this paid off you know okay cool

00:20:34,900 --> 00:20:41,140
cool alright so we check you check tens

00:20:38,260 --> 00:20:43,600
of threads how do I get to a hundred

00:20:41,140 --> 00:20:50,470
thousand threads well what do I have to

00:20:43,600 --> 00:20:52,000
do to run at that scale well yes yes you

00:20:50,470 --> 00:20:55,810
answered that question GPU is the answer

00:20:52,000 --> 00:20:57,460
yes um okay but concretely in code what

00:20:55,810 --> 00:21:01,480
do I have to do in the previous couple

00:20:57,460 --> 00:21:03,640
of slides I showed you some code yes we

00:21:01,480 --> 00:21:07,390
could have spent more time looking at it

00:21:03,640 --> 00:21:12,250
but I need to press on okay so so what's

00:21:07,390 --> 00:21:14,800
the CUDA code here do you all know CUDA

00:21:12,250 --> 00:21:17,230
right now I don't know oh well that guy

00:21:14,800 --> 00:21:21,690
does that guy does oh no you don't need

00:21:17,230 --> 00:21:24,280
to own tell me many of you know CUDA but

00:21:21,690 --> 00:21:26,410
but I'm up here and I'm thinking oh

00:21:24,280 --> 00:21:30,640
actually you know it's really not a

00:21:26,410 --> 00:21:32,080
given so here's a crash course here's a

00:21:30,640 --> 00:21:34,930
crash course Tucker you're going you

00:21:32,080 --> 00:21:36,970
right now we're going to take a 10 or

00:21:34,930 --> 00:21:40,030
15-minute crash course in CUDA are you

00:21:36,970 --> 00:21:43,270
ready let's do it all right so what's

00:21:40,030 --> 00:21:44,950
CUDA who does an integrated

00:21:43,270 --> 00:21:47,770
heterogeneous parallel programming

00:21:44,950 --> 00:21:51,040
system for C++ it's really important to

00:21:47,770 --> 00:21:53,950
know that it is not C++ it's a

00:21:51,040 --> 00:21:55,540
programming system that includes C++ as

00:21:53,950 --> 00:21:58,380
one of the elements and it certainly is

00:21:55,540 --> 00:22:00,280
the central element that is built around

00:21:58,380 --> 00:22:02,080
what else is important to know well I

00:22:00,280 --> 00:22:04,630
said heritage header Oh genius well that

00:22:02,080 --> 00:22:07,350
means that it accounts for both the host

00:22:04,630 --> 00:22:10,510
CPU and the accelerator the device and

00:22:07,350 --> 00:22:12,299
what do I mean by integrated I mean that

00:22:10,510 --> 00:22:15,929
it you write

00:22:12,299 --> 00:22:20,019
source code you don't write a host

00:22:15,929 --> 00:22:21,820
program and a accelerator program and

00:22:20,019 --> 00:22:22,450
then pipe the inputs and outputs from

00:22:21,820 --> 00:22:25,000
one to the other

00:22:22,450 --> 00:22:28,360
that's not how could it works in CUDA

00:22:25,000 --> 00:22:33,370
you write essentially one code base and

00:22:28,360 --> 00:22:35,019
it's all it's all in C++ and all you

00:22:33,370 --> 00:22:37,029
know that the the functionality

00:22:35,019 --> 00:22:38,440
interacts in a tightly integrated way

00:22:37,029 --> 00:22:40,899
okay so what's the programming

00:22:38,440 --> 00:22:45,549
abstraction to this like what what what

00:22:40,899 --> 00:22:47,950
does CUDA look and feel like in short so

00:22:45,549 --> 00:22:50,019
the model is that there are some

00:22:47,950 --> 00:22:52,870
functions in your program that you're

00:22:50,019 --> 00:22:55,269
going to call with very large numbers of

00:22:52,870 --> 00:22:58,029
threads and when I say practically

00:22:55,269 --> 00:23:00,669
unlimited what I mean is that when you

00:22:58,029 --> 00:23:03,370
launch threads on CUDA you will give it

00:23:00,669 --> 00:23:04,720
sort of a an index space that could be

00:23:03,370 --> 00:23:06,639
one-dimensional two-dimensional or

00:23:04,720 --> 00:23:08,860
three-dimensional you know and in the

00:23:06,639 --> 00:23:11,919
case of three-dimensional it's it's like

00:23:08,860 --> 00:23:14,289
64 bits three-dimensional so the number

00:23:11,919 --> 00:23:17,620
of threads you can express is extremely

00:23:14,289 --> 00:23:19,649
large obviously you know the machine

00:23:17,620 --> 00:23:22,539
cannot run a 64 bit number of threads

00:23:19,649 --> 00:23:25,059
you know you can't do that but I'm you

00:23:22,539 --> 00:23:28,210
can launch yeah you could for all

00:23:25,059 --> 00:23:32,409
practical purposes map the index space

00:23:28,210 --> 00:23:34,090
of your problem to CUDA you do not net

00:23:32,409 --> 00:23:35,769
you do not need to be in charge of

00:23:34,090 --> 00:23:37,659
dicing up the different threads and

00:23:35,769 --> 00:23:40,510
manually managing the threads if you

00:23:37,659 --> 00:23:43,120
have a problem and your problem measures

00:23:40,510 --> 00:23:45,519
a hundred thousand by ten thousand you

00:23:43,120 --> 00:23:49,750
can express that to CUDA and then CUDA

00:23:45,519 --> 00:23:51,190
will launch the tasks and they run for

00:23:49,750 --> 00:23:54,130
the amount of time that it takes for the

00:23:51,190 --> 00:23:56,110
computation to finish and obviously GPU

00:23:54,130 --> 00:23:57,970
is being used in HPC some of these

00:23:56,110 --> 00:24:02,830
computations can take a long time to

00:23:57,970 --> 00:24:06,610
finish okay so select functions not all

00:24:02,830 --> 00:24:09,399
the functions select functions that you

00:24:06,610 --> 00:24:11,889
select that can be called by practically

00:24:09,399 --> 00:24:14,049
unlimited number of threads in groups of

00:24:11,889 --> 00:24:15,940
up to 1024 you decide how they're

00:24:14,049 --> 00:24:18,730
grouped but the maximum group size is

00:24:15,940 --> 00:24:20,799
1024 with some constraints which I'm

00:24:18,730 --> 00:24:23,289
going to replace by details with some

00:24:20,799 --> 00:24:24,600
constraints and if you if your program

00:24:23,289 --> 00:24:27,540
stays within

00:24:24,600 --> 00:24:31,560
the constraints then your program will

00:24:27,540 --> 00:24:33,810
see consistent memory at all times okay

00:24:31,560 --> 00:24:37,500
so what are those constraints so in CUDA

00:24:33,810 --> 00:24:39,270
one dot o which is the model you know

00:24:37,500 --> 00:24:42,270
that a lot of people initially heard

00:24:39,270 --> 00:24:43,950
about that you you cudamalloc and then

00:24:42,270 --> 00:24:45,540
you get a different the separate pointer

00:24:43,950 --> 00:24:47,280
to CPU can't touch that and you could a

00:24:45,540 --> 00:24:51,060
mem copy and you have to mem copy in and

00:24:47,280 --> 00:24:52,920
mem copy out and all that that's back to

00:24:51,060 --> 00:24:58,410
CUDA one dot oh but from a model

00:24:52,920 --> 00:25:00,270
perspective it's parallelism which is

00:24:58,410 --> 00:25:02,370
the absence of inter dependencies

00:25:00,270 --> 00:25:04,890
between tasks okay so you express a

00:25:02,370 --> 00:25:07,260
number of tasks and you vouch that they

00:25:04,890 --> 00:25:10,760
are independent because that's

00:25:07,260 --> 00:25:14,400
parallelism and asynchrony where you

00:25:10,760 --> 00:25:17,160
express these tasks not only are they

00:25:14,400 --> 00:25:19,500
independent between one another but a

00:25:17,160 --> 00:25:23,160
dependency on these tasks need to be

00:25:19,500 --> 00:25:26,820
expressed explicitly so therefore they

00:25:23,160 --> 00:25:30,450
can begin executing concurrently with

00:25:26,820 --> 00:25:33,510
your CPU thread ok that's between groups

00:25:30,450 --> 00:25:35,850
and then within a group that can be up

00:25:33,510 --> 00:25:37,590
to 1024 threads you get a guarantee of

00:25:35,850 --> 00:25:39,630
concurrency that is these threads in

00:25:37,590 --> 00:25:41,610
fact can interdependencies the threads

00:25:39,630 --> 00:25:45,330
within a group can communicate with each

00:25:41,610 --> 00:25:46,920
other okay so you launch groups that are

00:25:45,330 --> 00:25:49,350
tightly knit and can communicate and

00:25:46,920 --> 00:25:50,910
these groups between themselves are

00:25:49,350 --> 00:25:54,750
loosely knit and they cannot communicate

00:25:50,910 --> 00:25:57,150
so that's the CUDA mono model they could

00:25:54,750 --> 00:26:02,040
attend Oh model you know ten years later

00:25:57,150 --> 00:26:04,860
basically is that it's that model from

00:26:02,040 --> 00:26:07,890
one oh but we've also added the ability

00:26:04,860 --> 00:26:11,850
for these groups of threads to loosely

00:26:07,890 --> 00:26:13,560
interdependence well since you know that

00:26:11,850 --> 00:26:15,030
we got a lot of mileage out of this

00:26:13,560 --> 00:26:18,470
model because we didn't completely

00:26:15,030 --> 00:26:24,260
change it right we added we clarified

00:26:18,470 --> 00:26:27,870
that they can also communicate with a

00:26:24,260 --> 00:26:29,430
clarified progress guarantee okay so why

00:26:27,870 --> 00:26:31,110
parallelism well it's because you get

00:26:29,430 --> 00:26:33,630
fewer bugs and higher performance just

00:26:31,110 --> 00:26:36,480
by default of course when you vouch that

00:26:33,630 --> 00:26:37,970
your tasks are independent and you it

00:26:36,480 --> 00:26:40,820
wasn't quite true and you wish

00:26:37,970 --> 00:26:43,400
they would you know communicate then

00:26:40,820 --> 00:26:47,030
then you have to do a lot more work of

00:26:43,400 --> 00:26:48,770
course but as a baseline you get fewer

00:26:47,030 --> 00:26:50,240
bugs and higher performance and then why

00:26:48,770 --> 00:26:52,790
did we reintroduce concurrency well it's

00:26:50,240 --> 00:26:54,170
because actually if you get dirty a

00:26:52,790 --> 00:26:58,340
little bit then you can get higher

00:26:54,170 --> 00:27:00,820
performance and so just like C++ CUDA

00:26:58,340 --> 00:27:04,010
C++ it doesn't judge if you mix paradox

00:27:00,820 --> 00:27:06,590
its parallelism and asynchrony and

00:27:04,010 --> 00:27:09,950
concurrency and consistency and the

00:27:06,590 --> 00:27:14,960
progress all in some measure all

00:27:09,950 --> 00:27:16,730
together okay now one thing I want to

00:27:14,960 --> 00:27:21,290
emphasize is that CUDA C++ is always a

00:27:16,730 --> 00:27:23,980
superset of C bus bus it's it it's

00:27:21,290 --> 00:27:27,590
always a superset of C bus bus because

00:27:23,980 --> 00:27:30,470
the host CPU never loses anything when

00:27:27,590 --> 00:27:33,770
you go to CUDA you never need to give up

00:27:30,470 --> 00:27:35,570
anything your all of your CPU code still

00:27:33,770 --> 00:27:37,070
can do everything your CPU code used to

00:27:35,570 --> 00:27:38,720
do it has access to all the libraries it

00:27:37,070 --> 00:27:41,570
wants it has access to all the features

00:27:38,720 --> 00:27:44,270
of the language okay some of the

00:27:41,570 --> 00:27:46,520
features then also extend to other

00:27:44,270 --> 00:27:49,670
processors and this is sort of how

00:27:46,520 --> 00:27:51,020
things are divided up so the host so

00:27:49,670 --> 00:27:53,650
there are things that only the host

00:27:51,020 --> 00:27:55,430
processor can do the things on the left

00:27:53,650 --> 00:27:57,590
tomorrow and anything to do with

00:27:55,430 --> 00:28:00,590
exceptions anything to do with RT TI and

00:27:57,590 --> 00:28:02,720
TLS there are things that both

00:28:00,590 --> 00:28:06,350
processors can do but the two but the

00:28:02,720 --> 00:28:07,820
processors need to do it in isolation

00:28:06,350 --> 00:28:09,860
what I mean by that is you can make

00:28:07,820 --> 00:28:12,500
polymorphic objects you can construct

00:28:09,860 --> 00:28:13,790
them on the CPU and the CPU can use

00:28:12,500 --> 00:28:15,260
those and you can construct them on the

00:28:13,790 --> 00:28:16,790
GPU and the GPU can use those but you

00:28:15,260 --> 00:28:19,610
can't pass a polymorphic object from the

00:28:16,790 --> 00:28:21,650
CPU to the GPU and all three of these

00:28:19,610 --> 00:28:24,200
are essentially because there's some

00:28:21,650 --> 00:28:28,220
notion of there being a pointer to the

00:28:24,200 --> 00:28:31,010
isit code and coded in these things and

00:28:28,220 --> 00:28:32,900
obviously the CPU can't execute the GPU

00:28:31,010 --> 00:28:35,720
code and the GPU can't execute the CPU

00:28:32,900 --> 00:28:37,730
go okay so beware and then finally in

00:28:35,720 --> 00:28:39,980
the third column you have what all

00:28:37,730 --> 00:28:42,170
processors can use and they can use

00:28:39,980 --> 00:28:44,150
together at the same time that is the

00:28:42,170 --> 00:28:45,860
CPU can construct an object with certain

00:28:44,150 --> 00:28:47,540
features and just pass it to the GPU and

00:28:45,860 --> 00:28:49,430
the do people understand what that

00:28:47,540 --> 00:28:51,010
object is and implement those features

00:28:49,430 --> 00:28:53,770
faithfully and that's base

00:28:51,010 --> 00:28:57,000
the rest of C++ you know and then like

00:28:53,770 --> 00:28:57,000
the point details are at this link

00:28:57,630 --> 00:29:03,580
okay there are nine extensions we're

00:29:01,450 --> 00:29:06,400
going to need nine things that are not

00:29:03,580 --> 00:29:09,669
in C++ that we're going to need to be

00:29:06,400 --> 00:29:14,260
able to read the demo code today okay

00:29:09,669 --> 00:29:16,120
nine things those nine things are we're

00:29:14,260 --> 00:29:19,000
going to have some memory management

00:29:16,120 --> 00:29:24,370
api's there are a lot simpler if you if

00:29:19,000 --> 00:29:27,040
you if you dabbled in CUDA before these

00:29:24,370 --> 00:29:29,169
joint these were added to CUDA these are

00:29:27,040 --> 00:29:32,770
much simpler than what we used to have

00:29:29,169 --> 00:29:35,559
okay so could a malloc manage and CUDA

00:29:32,770 --> 00:29:38,830
free they'll implement a symmetric heap

00:29:35,559 --> 00:29:42,130
what I mean by that is you allocate

00:29:38,830 --> 00:29:44,049
memory into that heap and the CUDA

00:29:42,130 --> 00:29:47,520
driver will make sure that the GPU and

00:29:44,049 --> 00:29:49,929
CPU have a consistent view of these

00:29:47,520 --> 00:29:51,190
without you doing anything else special

00:29:49,929 --> 00:29:52,210
you don't need to do mem copies you

00:29:51,190 --> 00:29:54,700
don't need to do create a mem copies

00:29:52,210 --> 00:29:56,200
with that you just know what you do what

00:29:54,700 --> 00:29:59,280
you do in the CPU you do what you do in

00:29:56,200 --> 00:30:05,140
the GPU it works

00:29:59,280 --> 00:30:08,590
we'll use cudamemcpy rememory like a

00:30:05,140 --> 00:30:10,870
hero there are there Wow it's because

00:30:08,590 --> 00:30:12,880
there's fixed function Hardware on GPUs

00:30:10,870 --> 00:30:14,950
that's exceptionally good

00:30:12,880 --> 00:30:16,270
that's copying memory yes but we're not

00:30:14,950 --> 00:30:19,570
going to use that feature but it's also

00:30:16,270 --> 00:30:21,100
exceptionally good at zeroing memory so

00:30:19,570 --> 00:30:23,679
we're actually gonna use this in this

00:30:21,100 --> 00:30:25,179
demo there are some decorations we're

00:30:23,679 --> 00:30:26,620
gonna need to put on our functions and

00:30:25,179 --> 00:30:29,380
I'm gonna show you what to do with that

00:30:26,620 --> 00:30:30,820
in the next slide curio device

00:30:29,380 --> 00:30:33,280
synchronizes how you resolve the

00:30:30,820 --> 00:30:34,900
asynchrony of executing cuda threads i

00:30:33,280 --> 00:30:37,990
said that in cuda you can invoke

00:30:34,900 --> 00:30:39,700
functions in many threads asynchronously

00:30:37,990 --> 00:30:41,049
well the cuda device synchronizes the

00:30:39,700 --> 00:30:45,280
function you call when you need to get

00:30:41,049 --> 00:30:48,190
the result there's the infamous triple

00:30:45,280 --> 00:30:50,799
chevron call that when we call a

00:30:48,190 --> 00:30:53,020
function and put the triple brackets

00:30:50,799 --> 00:30:55,570
that's going to call that function on

00:30:53,020 --> 00:30:57,910
the GPU in that mini blocks and that

00:30:55,570 --> 00:31:02,409
main threads and then here we're gonna

00:30:57,910 --> 00:31:04,160
have our indices which play the same

00:31:02,409 --> 00:31:07,580
role as in

00:31:04,160 --> 00:31:11,420
our cpu example we had index and domain

00:31:07,580 --> 00:31:13,520
these will help us work this out okay so

00:31:11,420 --> 00:31:16,430
here's an eye chart and we're gonna dice

00:31:13,520 --> 00:31:18,710
it into little pieces one at a time okay

00:31:16,430 --> 00:31:20,840
so host device is the decoration you put

00:31:18,710 --> 00:31:23,480
on ordinary functions there is a lot of

00:31:20,840 --> 00:31:25,310
code in the world a lot of my code a lot

00:31:23,480 --> 00:31:26,960
of your code there's a lot of code in

00:31:25,310 --> 00:31:28,360
the world that can just be annotated in

00:31:26,960 --> 00:31:30,860
post device

00:31:28,360 --> 00:31:34,070
it just has to not use our TTI

00:31:30,860 --> 00:31:36,320
and not throw and carefully use function

00:31:34,070 --> 00:31:39,890
pointers assuming that you meet these

00:31:36,320 --> 00:31:44,060
that can be host device okay so this is

00:31:39,890 --> 00:31:48,920
my ordinary function that does nothing

00:31:44,060 --> 00:31:51,710
interesting entry points into the GPU

00:31:48,920 --> 00:31:53,780
have to be marked with global those are

00:31:51,710 --> 00:31:55,690
just entry points though global

00:31:53,780 --> 00:31:57,920
functions can't call global functions

00:31:55,690 --> 00:32:00,020
the only thing that can call a global

00:31:57,920 --> 00:32:03,470
function is the CPU and it can only call

00:32:00,020 --> 00:32:04,940
it with triple brackets okay so our

00:32:03,470 --> 00:32:08,030
entry point is gonna be marked with

00:32:04,940 --> 00:32:10,700
global there is a gotcha you got to be

00:32:08,030 --> 00:32:13,790
aware of you guys are sophisticated C++

00:32:10,700 --> 00:32:15,800
people you need to know that when you

00:32:13,790 --> 00:32:17,240
pass operands to a global function they

00:32:15,800 --> 00:32:21,830
are assumed to be trivially copyable

00:32:17,240 --> 00:32:22,430
whether it's true or not so be careful

00:32:21,830 --> 00:32:24,740
all right

00:32:22,430 --> 00:32:26,960
we're gonna use the managed heap in this

00:32:24,740 --> 00:32:28,580
demo so in this demo what I did is I

00:32:26,960 --> 00:32:30,140
made an allocator this is just an

00:32:28,580 --> 00:32:31,880
alligator and basically there's only two

00:32:30,140 --> 00:32:33,800
lines of it that are interesting

00:32:31,880 --> 00:32:35,750
could it free and could a mount manage

00:32:33,800 --> 00:32:38,000
the rest is just filling out the

00:32:35,750 --> 00:32:41,450
alligator boy door plate and that

00:32:38,000 --> 00:32:43,430
eliminates a lot of tedium we can use

00:32:41,450 --> 00:32:44,990
triple bracket on a global function in

00:32:43,430 --> 00:32:46,250
fact we must use triple bracket on a

00:32:44,990 --> 00:32:49,940
global function here I'm just going to

00:32:46,250 --> 00:32:51,440
launch it do you know one thread call su

00:32:49,940 --> 00:32:52,640
triple bracket or asynchronous so we

00:32:51,440 --> 00:32:55,670
need to call device synchronized when

00:32:52,640 --> 00:32:57,290
they're done because my ordinary

00:32:55,670 --> 00:33:00,140
function is marked host device the host

00:32:57,290 --> 00:33:01,760
would call it two and so I can for

00:33:00,140 --> 00:33:03,620
example just call ordinary function on

00:33:01,760 --> 00:33:05,360
the CPU side that that can be really

00:33:03,620 --> 00:33:07,580
convenient yep you know different people

00:33:05,360 --> 00:33:10,280
have different workflows my personal

00:33:07,580 --> 00:33:12,950
workflow is I tend to write all of my

00:33:10,280 --> 00:33:15,950
solver or my model or whatever it is I'm

00:33:12,950 --> 00:33:17,640
working on in host device code and then

00:33:15,950 --> 00:33:20,070
I get a lot of mileage

00:33:17,640 --> 00:33:23,070
of running on the CPU for a while and

00:33:20,070 --> 00:33:25,110
then I take it to the GPU knowing that

00:33:23,070 --> 00:33:26,880
it's already kind of pretty good you

00:33:25,110 --> 00:33:29,490
know and then I do then I do that by

00:33:26,880 --> 00:33:32,010
that next step of finishing my GPU

00:33:29,490 --> 00:33:35,610
tuning for example that way it's a very

00:33:32,010 --> 00:33:38,580
still way to work so Q okay

00:33:35,610 --> 00:33:41,940
so that's the cheat sheet using each of

00:33:38,580 --> 00:33:43,440
the nine things once okay but that's not

00:33:41,940 --> 00:33:47,130
actually a try solver that's not what it

00:33:43,440 --> 00:33:50,790
does it's just the cheechee we can take

00:33:47,130 --> 00:33:53,880
a look at CUDA on GOG bolt for example

00:33:50,790 --> 00:33:57,720
if I put my cheat sheet program into the

00:33:53,880 --> 00:34:00,450
compiler Explorer I get access to cuda

00:33:57,720 --> 00:34:05,700
now on the cuda Explorer on the compiler

00:34:00,450 --> 00:34:08,730
explorer yes and here I have it running

00:34:05,700 --> 00:34:12,570
with NBCC 9.2 I can also run it with

00:34:08,730 --> 00:34:17,340
clang and what I see here as the

00:34:12,570 --> 00:34:20,129
assembly is Nvidia speed X assembly so

00:34:17,340 --> 00:34:22,560
you can take CUDA code and put it on the

00:34:20,129 --> 00:34:25,169
compiler Explorer and take a look at

00:34:22,560 --> 00:34:28,020
what the generated Nvidia intermediate

00:34:25,169 --> 00:34:30,990
assembly looks like this assembly is the

00:34:28,020 --> 00:34:33,810
same one that's documented at the PX

00:34:30,990 --> 00:34:36,540
manual available on our website you can

00:34:33,810 --> 00:34:37,770
download it I and you know special

00:34:36,540 --> 00:34:39,050
shout-out to the memory consistency

00:34:37,770 --> 00:34:45,359
model which I love

00:34:39,050 --> 00:34:49,139
okay you'll need coffee to read it

00:34:45,359 --> 00:34:52,679
though okay so all right now that we've

00:34:49,139 --> 00:34:56,730
done this now we we know CUDA now cool

00:34:52,679 --> 00:34:59,520
now that we know CUDA what's MA what's

00:34:56,730 --> 00:35:01,740
the Tri demo what does the tried and

00:34:59,520 --> 00:35:04,950
will look like now um I did all the

00:35:01,740 --> 00:35:06,660
decorations so am I good no IDE there's

00:35:04,950 --> 00:35:10,050
more decorations just they all over them

00:35:06,660 --> 00:35:12,510
and my good well actually I have some

00:35:10,050 --> 00:35:14,760
more details to give you the more

00:35:12,510 --> 00:35:17,220
details is that I didn't tell you what

00:35:14,760 --> 00:35:20,010
happened with the library so if I revise

00:35:17,220 --> 00:35:22,080
my previous slide what I should have

00:35:20,010 --> 00:35:25,200
pointed out is that everything in stood

00:35:22,080 --> 00:35:29,040
can be used by the CPU alone

00:35:25,200 --> 00:35:30,780
and what can be used by both processors

00:35:29,040 --> 00:35:32,030
all the time is the rest of the C++

00:35:30,780 --> 00:35:36,110
language

00:35:32,030 --> 00:35:39,540
okay what we need for this demo is

00:35:36,110 --> 00:35:41,430
access to a free-standing library if

00:35:39,540 --> 00:35:43,980
freestanding the standard recognizes two

00:35:41,430 --> 00:35:45,960
subsets of C++ one is hosted which means

00:35:43,980 --> 00:35:49,260
everything the subset meaning everything

00:35:45,960 --> 00:35:51,390
and the second subset is freestanding

00:35:49,260 --> 00:35:53,640
which is an implementation defined set

00:35:51,390 --> 00:35:56,730
of headers but at least the ones in

00:35:53,640 --> 00:35:58,770
table 16 okay and the ones in table 16

00:35:56,730 --> 00:36:02,730
are very basic things but that includes

00:35:58,770 --> 00:36:05,520
atomic which is what we need okay so

00:36:02,730 --> 00:36:07,740
what we're going to do when to to run

00:36:05,520 --> 00:36:11,040
this demo on the GPU is that we're going

00:36:07,740 --> 00:36:14,700
to use my free-standing library I put it

00:36:11,040 --> 00:36:16,290
on github and it put it live today you

00:36:14,700 --> 00:36:21,210
can try it out you should read the

00:36:16,290 --> 00:36:24,210
readme though okay alright so now now we

00:36:21,210 --> 00:36:26,940
have a free-standing library here which

00:36:24,210 --> 00:36:30,540
both which all all processors can use

00:36:26,940 --> 00:36:34,740
and it puts standard definitions of

00:36:30,540 --> 00:36:37,020
symbols in the semicolon Conan stood : :

00:36:34,740 --> 00:36:39,300
namespace alright so now we're gonna

00:36:37,020 --> 00:36:42,120
look at our CUDA C++ version what do we

00:36:39,300 --> 00:36:44,040
do well we do the nine things we do the

00:36:42,120 --> 00:36:45,660
decoration so we put host device on our

00:36:44,040 --> 00:36:47,130
solver and we do the other things with

00:36:45,660 --> 00:36:50,220
the managed alligator to allocate the

00:36:47,130 --> 00:36:52,760
memory and whatnot okay we convert our

00:36:50,220 --> 00:36:58,170
studio mix to synthy studio tommix and

00:36:52,760 --> 00:37:00,540
also that one and we're done now we've

00:36:58,170 --> 00:37:03,380
ported it okay so let's go run that and

00:37:00,540 --> 00:37:06,000
obviously I cannot run down on my laptop

00:37:03,380 --> 00:37:08,100
but we can run that on this system over

00:37:06,000 --> 00:37:10,920
here and that will speed up the cuda

00:37:08,100 --> 00:37:15,390
driver and this will run thank you uhm

00:37:10,920 --> 00:37:18,510
and boom there we go well that's pretty

00:37:15,390 --> 00:37:20,190
cool that app now came up it ran in a

00:37:18,510 --> 00:37:24,990
hundred and sixty-three thousand threads

00:37:20,190 --> 00:37:27,000
and it ran on the system when cold about

00:37:24,990 --> 00:37:29,400
five times faster and when the system

00:37:27,000 --> 00:37:32,940
went high about ten times faster than

00:37:29,400 --> 00:37:35,280
what the 40 threads were able to do on

00:37:32,940 --> 00:37:40,980
the xeon before we can also do the same

00:37:35,280 --> 00:37:42,960
thing on a on a on a different system so

00:37:40,980 --> 00:37:44,160
the other one was a big beefy server

00:37:42,960 --> 00:37:46,829
type of machine

00:37:44,160 --> 00:37:52,799
this one here is just my desktop at the

00:37:46,829 --> 00:37:54,660
office works just as well all right so

00:37:52,799 --> 00:37:56,760
let's get back to the show all right so

00:37:54,660 --> 00:37:59,839
I'm gonna I have rigid all of the run

00:37:56,760 --> 00:38:02,940
numbers so let's say three milliseconds

00:37:59,839 --> 00:38:05,160
that's pretty good you know that was

00:38:02,940 --> 00:38:06,809
pretty low effort once we got a

00:38:05,160 --> 00:38:12,660
free-standing library that was pretty

00:38:06,809 --> 00:38:15,480
low effort yeah right all right so a

00:38:12,660 --> 00:38:18,809
hundred thousand threads check okay

00:38:15,480 --> 00:38:21,619
but what what just happened you know I I

00:38:18,809 --> 00:38:23,609
thought GP was we're terrible with this

00:38:21,619 --> 00:38:26,400
okay so a little bit more details about

00:38:23,609 --> 00:38:29,099
the performance actually the computation

00:38:26,400 --> 00:38:30,780
took even less time than you think it

00:38:29,099 --> 00:38:33,809
took about six hundred and fifty

00:38:30,780 --> 00:38:36,660
microseconds for the GPU to build the

00:38:33,809 --> 00:38:37,950
try and then the rest is the overhead of

00:38:36,660 --> 00:38:40,319
getting the data over there and back

00:38:37,950 --> 00:38:43,500
over the PCI bus and some control

00:38:40,319 --> 00:38:45,210
overhead it the the ratio of overhead

00:38:43,500 --> 00:38:47,309
the computation gets worse with smaller

00:38:45,210 --> 00:38:52,079
problem tile sizes and gets better with

00:38:47,309 --> 00:38:54,270
larger program sizes problem sizes you

00:38:52,079 --> 00:38:55,740
could commonly for example if you're

00:38:54,270 --> 00:38:57,420
doing a lot of processing you would

00:38:55,740 --> 00:39:00,450
commonly overlap transfers with

00:38:57,420 --> 00:39:02,039
computation so that it amortizes out to

00:39:00,450 --> 00:39:05,910
just paying the six hundred and fifty

00:39:02,039 --> 00:39:08,130
microseconds but again he was supposed

00:39:05,910 --> 00:39:11,640
to be terrible why it wasn't it terrible

00:39:08,130 --> 00:39:14,069
well the reason is this applications

00:39:11,640 --> 00:39:16,500
probably memory latency bound pretty

00:39:14,069 --> 00:39:17,940
much everywhere it runs on CPUs it's

00:39:16,500 --> 00:39:22,829
chasing over memory it's being pretty

00:39:17,940 --> 00:39:24,420
chaotic it's not very predictable but if

00:39:22,829 --> 00:39:27,270
you throw a hundred thousand threads at

00:39:24,420 --> 00:39:30,450
a problem the exposed latency pretty

00:39:27,270 --> 00:39:32,549
much disappears and at that point when

00:39:30,450 --> 00:39:34,920
the exposed latency disappears you just

00:39:32,549 --> 00:39:37,140
become bandwidth bound and there's a lot

00:39:34,920 --> 00:39:39,020
of bandwidth on GPUs you know the memory

00:39:37,140 --> 00:39:41,640
bandwidth of the GPU we ran on is

00:39:39,020 --> 00:39:45,660
something like 900 gigabytes per second

00:39:41,640 --> 00:39:48,000
ok very expensive CPU will get you I

00:39:45,660 --> 00:39:49,710
suppose right now is like 50 or 60

00:39:48,000 --> 00:39:53,430
gigabytes per second something it's a

00:39:49,710 --> 00:39:56,640
lot more memory bandwidth okay

00:39:53,430 --> 00:39:57,870
now kidney GPU run this demo no they

00:39:56,640 --> 00:40:01,410
cannot and actually that's

00:39:57,870 --> 00:40:03,540
kind of why I'm here that's what's cool

00:40:01,410 --> 00:40:06,810
about the launch of the new Turing GPUs

00:40:03,540 --> 00:40:09,660
is that there's only a few GPUs that can

00:40:06,810 --> 00:40:10,560
run this there is our large data center

00:40:09,660 --> 00:40:14,760
voltage chips

00:40:10,560 --> 00:40:16,380
there's our embedded AI driving platform

00:40:14,760 --> 00:40:19,580
Xavier that can run this but it hasn't

00:40:16,380 --> 00:40:24,660
it's not available in your hands yet and

00:40:19,580 --> 00:40:26,190
Turing pretty cool okay so only these

00:40:24,660 --> 00:40:28,080
will run this because we have a

00:40:26,190 --> 00:40:29,490
starvation free algorithm and if you

00:40:28,080 --> 00:40:31,670
were to take this all rhythm and run it

00:40:29,490 --> 00:40:36,480
on another GPU then what would happen is

00:40:31,670 --> 00:40:37,920
it would hang now maybe we could dig

00:40:36,480 --> 00:40:39,540
ourselves out of this hole by throwing

00:40:37,920 --> 00:40:41,160
more complexity at it and imagining a

00:40:39,540 --> 00:40:42,300
different algorithm that is potentially

00:40:41,160 --> 00:40:45,240
lock-free and all that but you know what

00:40:42,300 --> 00:40:46,830
that would be more effort and and a big

00:40:45,240 --> 00:40:49,020
part of my point here is that actually

00:40:46,830 --> 00:40:51,690
that was fairly straightforward with a

00:40:49,020 --> 00:40:54,180
free-standing library okay

00:40:51,690 --> 00:40:56,100
so pretty quickly um we're gonna I

00:40:54,180 --> 00:41:00,930
wanted to tell you guys something about

00:40:56,100 --> 00:41:03,750
C++ 20 because that's one of the things

00:41:00,930 --> 00:41:06,560
I'm trying to deliver to you guys that

00:41:03,750 --> 00:41:08,970
would be great with the rest of sg-1

00:41:06,560 --> 00:41:11,090
okay so there's this part which wasn't

00:41:08,970 --> 00:41:13,530
great we did have a spin loop in there

00:41:11,090 --> 00:41:16,200
again I explained that that spin loop is

00:41:13,530 --> 00:41:17,790
probably okay but in general you know

00:41:16,200 --> 00:41:19,560
good polling loops are very difficult to

00:41:17,790 --> 00:41:21,300
write and so we end up with only

00:41:19,560 --> 00:41:26,280
terrible polling loops and everyone's

00:41:21,300 --> 00:41:28,080
code bases so in C++ 20 we have a new

00:41:26,280 --> 00:41:31,080
facility that's going to make this much

00:41:28,080 --> 00:41:32,790
easier we're going to have essentially

00:41:31,080 --> 00:41:35,940
an abstraction for polling loops called

00:41:32,790 --> 00:41:37,290
atomic weight and here is the atomic

00:41:35,940 --> 00:41:40,080
weight explicit because it takes a

00:41:37,290 --> 00:41:43,440
memory order so it replaces the polling

00:41:40,080 --> 00:41:46,230
loop with this polling abstraction it is

00:41:43,440 --> 00:41:48,330
paired with a notify which is somewhat

00:41:46,230 --> 00:41:49,650
similar in concept to waiting on a

00:41:48,330 --> 00:41:51,060
condition variable and notifying a

00:41:49,650 --> 00:41:53,640
condition variable and the reason that

00:41:51,060 --> 00:41:55,560
we need a notify is because the idea is

00:41:53,640 --> 00:41:59,630
that this is going to probably be

00:41:55,560 --> 00:42:01,890
implemented using say on Linux futex

00:41:59,630 --> 00:42:03,720
ultimately you know initially polling a

00:42:01,890 --> 00:42:05,400
bit and then eventually maybe yielding

00:42:03,720 --> 00:42:07,110
or sleeping a bit and then after you've

00:42:05,400 --> 00:42:08,910
failed for a while you should probably

00:42:07,110 --> 00:42:11,360
come to full rest into the kernel you

00:42:08,910 --> 00:42:13,490
call futex but

00:42:11,360 --> 00:42:16,040
if inside of atomic weight there's a

00:42:13,490 --> 00:42:20,870
path that leads to futex then you also

00:42:16,040 --> 00:42:23,050
need if you tax week and that's what's

00:42:20,870 --> 00:42:26,120
going to go into the atomic down file

00:42:23,050 --> 00:42:28,610
okay so that's one thing there's a

00:42:26,120 --> 00:42:31,100
number of other things I'm I probably

00:42:28,610 --> 00:42:33,350
don't really have time to to talk about

00:42:31,100 --> 00:42:36,470
them in great detail but I'll tell you

00:42:33,350 --> 00:42:38,000
one thing about each one we're going to

00:42:36,470 --> 00:42:39,980
have an auto joining thread which

00:42:38,000 --> 00:42:41,630
interruptible waiting that's cool we

00:42:39,980 --> 00:42:43,220
have the weighting functions that I just

00:42:41,630 --> 00:42:45,250
talked about we'll finally have

00:42:43,220 --> 00:42:48,890
semaphores

00:42:45,250 --> 00:42:50,690
finally we'll have latches and barriers

00:42:48,890 --> 00:42:52,970
which are cooperative synchronization

00:42:50,690 --> 00:42:55,280
primitives latches are one-time use fan

00:42:52,970 --> 00:42:57,260
in from multiple threads barriers are

00:42:55,280 --> 00:42:59,690
multiple use fan-in and fan-out between

00:42:57,260 --> 00:43:01,520
multiple threads we have atomic ref

00:42:59,690 --> 00:43:04,490
which is going to allow you to do atomic

00:43:01,520 --> 00:43:06,230
operations on non atomic variables one

00:43:04,490 --> 00:43:11,120
of the cool things about that is it will

00:43:06,230 --> 00:43:13,510
actually help port code that has crusty

00:43:11,120 --> 00:43:17,090
old C code in it that just has you know

00:43:13,510 --> 00:43:19,430
volatile and pointers or something you

00:43:17,090 --> 00:43:20,990
know it's dirty I don't like it I don't

00:43:19,430 --> 00:43:22,670
like both Island pointers for

00:43:20,990 --> 00:43:24,290
synchronization it's in people's code

00:43:22,670 --> 00:43:26,240
it's a reality atomic graph actually

00:43:24,290 --> 00:43:30,050
will make these code bases be easier to

00:43:26,240 --> 00:43:32,360
be brought into the modern present okay

00:43:30,050 --> 00:43:34,430
uncie is we're going to have a simple

00:43:32,360 --> 00:43:36,290
and effective vectorization policy we

00:43:34,430 --> 00:43:38,960
have a bunch of improvements to atomic

00:43:36,290 --> 00:43:41,870
phagon whatnot and we have some repairs

00:43:38,960 --> 00:43:46,610
to the memory model orange light all

00:43:41,870 --> 00:43:52,930
right so to wrap this up Turing's are

00:43:46,610 --> 00:43:56,300
shipping by now you all of all people

00:43:52,930 --> 00:43:57,290
should try them you should try c++ on

00:43:56,300 --> 00:44:02,300
them

00:43:57,290 --> 00:44:05,660
and if you try them we'd like to hear

00:44:02,300 --> 00:44:09,220
back from you definitely okay and

00:44:05,660 --> 00:44:12,800
today's code is available at this thing

00:44:09,220 --> 00:44:13,790
and that's it I'll take

00:44:12,800 --> 00:44:22,100
I think we have a little bit of time for

00:44:13,790 --> 00:44:23,630
questions thank you well we'll stretch

00:44:22,100 --> 00:44:25,460
into new Don the next five minutes I

00:44:23,630 --> 00:44:40,550
guess the next thought probably starts

00:44:25,460 --> 00:44:42,500
that 15 hi Billy hi yes it it's

00:44:40,550 --> 00:44:44,600
perfectly valid for this pinning side to

00:44:42,500 --> 00:44:46,310
have experienced spurious weights in

00:44:44,600 --> 00:44:47,600
fact you know it doesn't promise to go

00:44:46,310 --> 00:44:56,030
to sleep and it doesn't promise to not

00:44:47,600 --> 00:44:58,580
go to sleep so it it does nothing bad to

00:44:56,030 --> 00:45:01,580
my try demo the atomic weight does not

00:44:58,580 --> 00:45:03,410
return early so it can wake internally

00:45:01,580 --> 00:45:05,270
but it means to retest the memory

00:45:03,410 --> 00:45:07,090
location and if the memory location has

00:45:05,270 --> 00:45:50,350
not been updated it needs to sleep again

00:45:07,090 --> 00:45:52,580
yeah we have 15 right right so there's

00:45:50,350 --> 00:45:55,190
there's a lot of different reasons why

00:45:52,580 --> 00:45:56,990
various versions of DirectX in the past

00:45:55,190 --> 00:45:59,060
experience I overhead like for example

00:45:56,990 --> 00:46:01,520
there are many generations where almost

00:45:59,060 --> 00:46:04,220
every call was a kernel call and not

00:46:01,520 --> 00:46:06,860
just a user mode dispatch there is quite

00:46:04,220 --> 00:46:09,500
a lot in CUDA that is simply done in

00:46:06,860 --> 00:46:13,070
user mode and doesn't need a kernel

00:46:09,500 --> 00:46:15,350
transition so the efficiency is you know

00:46:13,070 --> 00:46:18,650
it approaches basically just the

00:46:15,350 --> 00:46:24,260
efficiency of signaling over the PCI bus

00:46:18,650 --> 00:46:26,299
and then then the dominant factor in how

00:46:24,260 --> 00:46:28,429
expensive it is for you to go

00:46:26,299 --> 00:46:31,939
to the GPU and back is really how much

00:46:28,429 --> 00:46:34,239
data you're referencing so if you need

00:46:31,939 --> 00:46:38,630
to reference a lot of data on both sides

00:46:34,239 --> 00:46:40,969
then that data has to move ideally what

00:46:38,630 --> 00:46:43,279
would happen is you would move your data

00:46:40,969 --> 00:46:45,079
to the GPU and try to keep it there as

00:46:43,279 --> 00:46:46,880
long as possible if you have multiple

00:46:45,079 --> 00:46:49,219
computations you try to keep your data

00:46:46,880 --> 00:46:50,959
there even if the control comes back and

00:46:49,219 --> 00:46:54,579
forth you try to keep the data there so

00:46:50,959 --> 00:46:54,579
it doesn't take time on the PCI bus

00:46:59,100 --> 00:47:08,059
[Music]

00:47:01,209 --> 00:47:09,859
right right so so so the code so you're

00:47:08,059 --> 00:47:12,679
in control of how the code moves back

00:47:09,859 --> 00:47:15,739
and forth definitely if you use the

00:47:12,679 --> 00:47:17,839
managed memory allocator then CUDA is in

00:47:15,739 --> 00:47:21,559
control of how the move data is moved

00:47:17,839 --> 00:47:24,890
but you can if we were to get into a

00:47:21,559 --> 00:47:27,890
tuning discussion there's a lot of hints

00:47:24,890 --> 00:47:28,309
and nods you can use to say well this

00:47:27,890 --> 00:47:30,949
data

00:47:28,309 --> 00:47:33,109
everyone can touch but but but usually

00:47:30,949 --> 00:47:35,179
the GPU touches it and then CUDA takes

00:47:33,109 --> 00:47:37,670
that into account when it decides where

00:47:35,179 --> 00:47:40,160
to put it and then in the extreme end is

00:47:37,670 --> 00:47:42,679
you you can still write CUDA Wando

00:47:40,160 --> 00:47:45,109
like you you still can go back and say

00:47:42,679 --> 00:47:47,059
well for most of my data

00:47:45,109 --> 00:47:49,459
I let CUDA decide what to do but for

00:47:47,059 --> 00:47:52,309
this piece of data just this one I'm

00:47:49,459 --> 00:47:56,390
gonna control it and so at the sort of

00:47:52,309 --> 00:48:18,099
high end of performance tuning you have

00:47:56,390 --> 00:48:18,099
full control no

00:48:26,980 --> 00:48:32,270
and okay alright alright um how do you

00:48:30,590 --> 00:48:36,190
do bug this your first question to your

00:48:32,270 --> 00:48:36,190
first question how do you do beg this

00:48:36,280 --> 00:48:41,420
actually you debug this you did bug this

00:48:39,050 --> 00:48:44,060
is the way you would normally do bug

00:48:41,420 --> 00:48:47,540
this which which could mean different

00:48:44,060 --> 00:48:49,730
things if you're a printf debugger we

00:48:47,540 --> 00:48:51,350
have printf so you can do that of course

00:48:49,730 --> 00:48:55,130
printf from hundreds of thousands of

00:48:51,350 --> 00:49:00,620
threads could be very impressive okay um

00:48:55,130 --> 00:49:03,950
okay but more seriously now um CUDA

00:49:00,620 --> 00:49:07,370
comes with gdb it comes with CUDA gdb

00:49:03,950 --> 00:49:10,340
it comes with a a gdb that does

00:49:07,370 --> 00:49:12,350
everything your normal gdb does but in

00:49:10,340 --> 00:49:15,410
addition to what it normally does you

00:49:12,350 --> 00:49:18,020
can back-trace and set context and print

00:49:15,410 --> 00:49:19,820
variables and jump to frames and jump to

00:49:18,020 --> 00:49:22,910
arbitrary threads that are currently

00:49:19,820 --> 00:49:24,320
running on the GPU side it looks and

00:49:22,910 --> 00:49:27,710
feels exactly the same as usual

00:49:24,320 --> 00:49:30,020
and that gdb also works with DDD so you

00:49:27,710 --> 00:49:32,660
could use DDD visually to do it okay so

00:49:30,020 --> 00:49:34,430
on linux you that's how you do it how

00:49:32,660 --> 00:49:36,710
would you do it on Windows well how do

00:49:34,430 --> 00:49:37,250
you develop it obviously you use Visual

00:49:36,710 --> 00:49:40,970
Studio

00:49:37,250 --> 00:49:43,400
now what like obviously um so we're

00:49:40,970 --> 00:49:44,900
integrated into Visual Studio as well so

00:49:43,400 --> 00:49:47,200
you use Visual Studio to the bug on

00:49:44,900 --> 00:49:50,060
Windows okay so you the bug as normal

00:49:47,200 --> 00:49:56,180
basically okay now for the memory model

00:49:50,060 --> 00:49:59,630
annotations um and so first off and now

00:49:56,180 --> 00:50:03,250
I'm having a really channel my as you

00:49:59,630 --> 00:50:07,850
want chair hat I'm you you it is not

00:50:03,250 --> 00:50:11,510
salient what the Machine does the C++

00:50:07,850 --> 00:50:14,390
memory model is is the memory model of

00:50:11,510 --> 00:50:15,970
the C++ abstract machine and you shall

00:50:14,390 --> 00:50:18,650
use it correctly

00:50:15,970 --> 00:50:22,450
even if using it incorrectly does not

00:50:18,650 --> 00:50:22,450
result in failure on the box you're on

00:50:30,660 --> 00:50:40,000
so not so not quite I mean it does

00:50:35,230 --> 00:50:42,490
generate it it has to generate em fence

00:50:40,000 --> 00:50:44,680
in certain cases the compiler mappings

00:50:42,490 --> 00:50:48,040
are available on a website from the

00:50:44,680 --> 00:50:49,990
University of Cambridge it generates

00:50:48,040 --> 00:50:54,910
mostly the same code you are correct

00:50:49,990 --> 00:50:59,740
it's not exactly the same code that's

00:50:54,910 --> 00:51:16,810
right on this atomic library does the

00:50:59,740 --> 00:51:18,310
right thing right right but um but I'd

00:51:16,810 --> 00:51:21,430
like to point out that the optimizer on

00:51:18,310 --> 00:51:23,410
x86 will exploit the freedom that you

00:51:21,430 --> 00:51:25,570
give it when you use a the correct

00:51:23,410 --> 00:51:28,840
memory order if you use memory order

00:51:25,570 --> 00:51:30,670
acquire there's an optimization known in

00:51:28,840 --> 00:51:34,240
the memory model community as the roach

00:51:30,670 --> 00:51:36,190
motel optimization which is which is

00:51:34,240 --> 00:51:39,700
operations can check in but they can't

00:51:36,190 --> 00:51:41,290
check out so so if you have a critical

00:51:39,700 --> 00:51:43,810
section and it starts with an acquire

00:51:41,290 --> 00:51:45,400
and it ends with the release operations

00:51:43,810 --> 00:51:46,420
before the critical section that can

00:51:45,400 --> 00:51:48,550
actually be moved into the critical

00:51:46,420 --> 00:51:51,010
section and operations out below can be

00:51:48,550 --> 00:51:53,740
moved up but not the reverse right so

00:51:51,010 --> 00:51:56,550
that optimization is a valid and

00:51:53,740 --> 00:52:01,150
interesting optimization even on x86

00:51:56,550 --> 00:52:03,790
right and and so using the proper memory

00:52:01,150 --> 00:52:06,400
order always rewards you with the best

00:52:03,790 --> 00:52:08,800
performance on every machine there is so

00:52:06,400 --> 00:52:11,470
yes the step is a little bit bigger when

00:52:08,800 --> 00:52:13,870
you go to arm and power and definitely

00:52:11,470 --> 00:52:16,720
nvidia gpus so yes you have to use the

00:52:13,870 --> 00:52:19,240
right one our memory model and I'd you

00:52:16,720 --> 00:52:21,100
know it would take hours to really get

00:52:19,240 --> 00:52:32,550
into it but our memory model is in the

00:52:21,100 --> 00:52:32,550
vein of IBM Power yes

00:52:33,250 --> 00:52:38,970
yes they take more things actually

00:52:35,500 --> 00:52:38,970
they're sort of very etic like there's

00:52:41,160 --> 00:52:46,420
they they don't have to be concept for

00:52:43,750 --> 00:52:47,950
no there they're as good as operands to

00:52:46,420 --> 00:52:49,780
the function itself so you could you

00:52:47,950 --> 00:52:52,480
could have just computed some value and

00:52:49,780 --> 00:52:59,430
pass it in yeah what do you mean what

00:52:52,480 --> 00:53:05,140
are they they're they're unsigned oh oh

00:52:59,430 --> 00:53:07,990
this is the high radix part so so we

00:53:05,140 --> 00:53:09,310
don't have we don't present a model of

00:53:07,990 --> 00:53:10,540
one hundred and sixty-three thousand

00:53:09,310 --> 00:53:13,119
eight hundred and forty flat threads

00:53:10,540 --> 00:53:16,930
that's not the model threads are

00:53:13,119 --> 00:53:18,430
collected into blocks and blocks can

00:53:16,930 --> 00:53:24,369
have between one and ten twenty four

00:53:18,430 --> 00:53:27,460
threads and then and then we have waves

00:53:24,369 --> 00:53:30,730
of blocks or called grids but they're

00:53:27,460 --> 00:53:34,119
invocations of a function okay so so

00:53:30,730 --> 00:53:36,880
when you say so you you decide do I want

00:53:34,119 --> 00:53:38,890
lots of small blocks or fewer bigger

00:53:36,880 --> 00:53:42,580
blocks to express the problem I'm

00:53:38,890 --> 00:53:44,619
working on the trade-off is that bigger

00:53:42,580 --> 00:53:47,800
blocks consume more register file space

00:53:44,619 --> 00:53:49,510
and so you you might want a smaller

00:53:47,800 --> 00:53:50,920
block so that your threads can have more

00:53:49,510 --> 00:53:54,430
registers because we have a variable

00:53:50,920 --> 00:53:56,380
register architecture it's one dimension

00:53:54,430 --> 00:53:58,390
the other dimension is that threads

00:53:56,380 --> 00:53:59,589
within a block or guaranteed concurrent

00:53:58,390 --> 00:54:01,450
and they're guaranteed to be able to

00:53:59,589 --> 00:54:04,930
cooperate on a problem so if you wanted

00:54:01,450 --> 00:54:08,320
many threads to work together to compute

00:54:04,930 --> 00:54:10,839
a result then you want those threads to

00:54:08,320 --> 00:54:13,030
be together in a block so as you think

00:54:10,839 --> 00:54:15,160
about how you decompose your problem you

00:54:13,030 --> 00:54:16,510
know you might say I want a hundred and

00:54:15,160 --> 00:54:18,790
twenty eight threads per block with

00:54:16,510 --> 00:54:20,170
however many blocks that means because I

00:54:18,790 --> 00:54:21,910
want more registers per thread or I

00:54:20,170 --> 00:54:23,500
might say I want 1024 threads per block

00:54:21,910 --> 00:54:24,940
because the cooperation between these

00:54:23,500 --> 00:54:27,339
threads is where the real value comes

00:54:24,940 --> 00:54:42,190
from and but they'll get they'll get

00:54:27,339 --> 00:54:45,440
fewer registers thank you hi yeah

00:54:42,190 --> 00:54:45,440
[Music]

00:54:50,100 --> 00:54:56,920
right so it's some it is not generally

00:54:55,060 --> 00:54:59,070
the case that lock-free algorithms are

00:54:56,920 --> 00:55:01,360
faster than starvation free algorithms

00:54:59,070 --> 00:55:03,150
it's an intuition that a lot of people

00:55:01,360 --> 00:55:06,340
have but it's an incorrect one

00:55:03,150 --> 00:55:08,770
sometimes the lock-free expression of an

00:55:06,340 --> 00:55:11,350
algorithm has many more steps with

00:55:08,770 --> 00:55:14,860
memory barriers in it that you don't

00:55:11,350 --> 00:55:18,460
need to have there if you use sort of a

00:55:14,860 --> 00:55:21,220
a low contention mutual exclusion scheme

00:55:18,460 --> 00:55:23,500
in this case here the reason we really

00:55:21,220 --> 00:55:26,590
got away with having mutual exclusion is

00:55:23,500 --> 00:55:27,520
because a radix 26:3 reduces contention

00:55:26,590 --> 00:55:30,220
real fast

00:55:27,520 --> 00:55:34,840
you know log 26 is a fantastic

00:55:30,220 --> 00:55:37,450
amortization strategy so so so we had

00:55:34,840 --> 00:55:40,600
mutexes here and we survived because

00:55:37,450 --> 00:55:42,310
contention was low okay there are many

00:55:40,600 --> 00:55:43,720
other there are other cases I can think

00:55:42,310 --> 00:55:45,250
of that have this attribute where I

00:55:43,720 --> 00:55:46,240
could write the lock free one but the

00:55:45,250 --> 00:55:49,210
lock free one would have a lot more

00:55:46,240 --> 00:55:53,380
overhead okay so how would you choose I

00:55:49,210 --> 00:55:55,680
think you you would choose the same this

00:55:53,380 --> 00:55:58,900
is one of the things that I originally

00:55:55,680 --> 00:56:01,420
imagined it would not be true but turns

00:55:58,900 --> 00:56:03,250
out is true is you bring to it sort of

00:56:01,420 --> 00:56:07,890
the same thinking you would on the cpu

00:56:03,250 --> 00:56:07,890
and it's all about contention thank you

00:56:10,440 --> 00:56:15,520
any more questions

00:56:13,440 --> 00:56:22,409
thank you

00:56:15,520 --> 00:56:22,409

YouTube URL: https://www.youtube.com/watch?v=75LcDvlEIYw


