Title: CppCon 2018: Steven Simpson “Source Instrumentation for Monitoring C++ in Production”
Publication date: 2018-10-12
Playlist: CppCon 2018
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2018
—
It is essential to discuss how modern C++ code can be effectively instrumented, in order to effectively monitor it after deployment. This talk will focus on portable source instrumentation techniques such as logging, tracing and metrics. Straightforward, but well designed code additions can drastically ease the troubleshooting of functional issues, and identification of performance bottlenecks, in production. 

Of course when dealing with C++ performance is often critical, and so minimizing the cost of any instrumentation is also critical. Key to this is understanding the trade-off between the detail of information collected, and the overheads of exposing that information. It is also important to understand how best to benefit from advances in contemporary monitoring infrastructure, popularised by "cloud" environments. 

This talk will open with a brief summary of monitoring goals, infrastructure, benefits, and existing practise. It will then detail practicalities of building a set of C++ source instrumentation primitives, based on proven principles employed in demanding production software.
— 
Steven Simpson, Graphcore
Senior Software Engineer

Hardware verification engineer, turned software engineer, turned infrastructure engineer. Most recently involved with developing monitoring systems for supercomputers at a well known university, but with over ten years experience spanning a wide range of software engineering problems using C++, and occasionally other languages. This has ranged from developing firmware for high-performance network switches, building distributed SQL databases, and deploying HPC infrastructure. 

Working as an ASIC verification engineer has led to an unhealthy paranoia when it comes testing software. Having lost many hours debugging deadlocks and memory corruptions, is most content when able to solve complex problems with the intelligible, uncomplicated code which is easily understood by others. 

Currently working on real-time video transcoding software, manipulating data streams at over 10Gbps, with strict performance and uptime requirements. Previously a speaker at ACCU, NDC Oslo, FOSDEM, PGDay Paris, PGDay Brussels conferences on various topics around software development, databases and infrastructure. Now living in Norway having moved from the UK, primarily for the better weather. 
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,850
good morning thank you for turning up so

00:00:02,399 --> 00:00:09,649
early very excited and surprised to see

00:00:05,850 --> 00:00:09,649
that you were managed to get out of bed

00:00:09,650 --> 00:00:19,160
I'm going to start by admitting a little

00:00:13,320 --> 00:00:24,680
secret to one of my very first jobs I

00:00:19,160 --> 00:00:28,619
got earlier in my career was as a junior

00:00:24,680 --> 00:00:32,540
verification and modeling engineer I had

00:00:28,619 --> 00:00:36,809
no idea what this is I knew two things I

00:00:32,540 --> 00:00:40,879
knew it was programming and I knew they

00:00:36,809 --> 00:00:45,180
were gonna pay me I knew absolutely

00:00:40,879 --> 00:00:46,739
nothing else about this job would anyone

00:00:45,180 --> 00:00:48,800
else like to admit that they've done the

00:00:46,739 --> 00:00:52,170
same thing

00:00:48,800 --> 00:00:56,160
fantastic that makes me feel a lot

00:00:52,170 --> 00:01:01,920
better so what this job was actually

00:00:56,160 --> 00:01:03,780
about was building silicon chips not

00:01:01,920 --> 00:01:09,000
processors they were actually for

00:01:03,780 --> 00:01:13,770
networking but what's really interesting

00:01:09,000 --> 00:01:17,030
about this process is these designs

00:01:13,770 --> 00:01:21,180
start off looking a lot like software

00:01:17,030 --> 00:01:24,630
you write some syntax in some text files

00:01:21,180 --> 00:01:28,460
and through a very long and drawn-out

00:01:24,630 --> 00:01:32,700
process they get manufactured into these

00:01:28,460 --> 00:01:37,470
silicon wafers which then become your

00:01:32,700 --> 00:01:41,369
chip this process is extremely expensive

00:01:37,470 --> 00:01:44,520
a conservative estimate might put it up

00:01:41,369 --> 00:01:51,390
five or ten million dollars and that's

00:01:44,520 --> 00:01:55,590
really being conservative about it so

00:01:51,390 --> 00:01:58,670
this is where my job came in my job as

00:01:55,590 --> 00:02:04,320
it turned out was to write tests for

00:01:58,670 --> 00:02:08,729
these designs in C++ because you want to

00:02:04,320 --> 00:02:11,950
get the design right first time that

00:02:08,729 --> 00:02:14,590
rarely happens but that's the idea

00:02:11,950 --> 00:02:18,430
and so we would incorporate simulations

00:02:14,590 --> 00:02:23,379
of these designs into C++ code and write

00:02:18,430 --> 00:02:25,750
tests for them because this process of

00:02:23,379 --> 00:02:29,380
making these chips is so expensive and

00:02:25,750 --> 00:02:34,500
suck and so drawn out when you get a bug

00:02:29,380 --> 00:02:37,840
in the chip you really want to make sure

00:02:34,500 --> 00:02:40,360
that you can fix it and work out what's

00:02:37,840 --> 00:02:44,410
going on without having to change the

00:02:40,360 --> 00:02:45,940
chip and so you lose you don't have a

00:02:44,410 --> 00:02:48,209
lot of the niceties that you have when

00:02:45,940 --> 00:02:51,610
you're building software you don't have

00:02:48,209 --> 00:02:55,030
any debugger in the same sense there's

00:02:51,610 --> 00:02:59,049
no running a debug build there's no just

00:02:55,030 --> 00:03:03,220
adding some extra logs what you do have

00:02:59,049 --> 00:03:06,910
though is a huge amount of statistics in

00:03:03,220 --> 00:03:09,730
the chip huge amount little registers

00:03:06,910 --> 00:03:13,630
that you can read numbers out of and

00:03:09,730 --> 00:03:15,430
these numbers are accumulating various

00:03:13,630 --> 00:03:20,590
bits of information about what's going

00:03:15,430 --> 00:03:23,859
on inside and this is all you have to do

00:03:20,590 --> 00:03:26,410
bug anything that might go wrong in the

00:03:23,859 --> 00:03:29,079
chip and you have to make sure it's in

00:03:26,410 --> 00:03:32,769
the chip design from the start there's

00:03:29,079 --> 00:03:37,840
no adding it in afterwards this is not

00:03:32,769 --> 00:03:44,069
possible so let's compare this to how we

00:03:37,840 --> 00:03:51,459
build C++ we write a similar set of code

00:03:44,069 --> 00:03:53,980
and the equivalent to manufacturing I

00:03:51,459 --> 00:03:57,010
guess would be putting it into

00:03:53,980 --> 00:04:03,519
production so compiling it testing it

00:03:57,010 --> 00:04:05,530
deploying it shipping it but how often

00:04:03,519 --> 00:04:10,389
do we think about how much this actually

00:04:05,530 --> 00:04:15,010
costs it cost a very small amount of

00:04:10,389 --> 00:04:18,190
money is it actually quite expensive how

00:04:15,010 --> 00:04:21,820
easy is it to actually change code that

00:04:18,190 --> 00:04:24,280
we've put in to production this is going

00:04:21,820 --> 00:04:25,010
to depend heavily on what domain you're

00:04:24,280 --> 00:04:28,190
in

00:04:25,010 --> 00:04:30,380
but is worth thinking about because I

00:04:28,190 --> 00:04:32,540
think even if it's really easy for you

00:04:30,380 --> 00:04:42,470
to update your code in production you

00:04:32,540 --> 00:04:45,320
want to do it as least as possible so

00:04:42,470 --> 00:04:47,990
what we mean by production ready we

00:04:45,320 --> 00:04:50,450
talked about this a lot especially in

00:04:47,990 --> 00:04:53,960
code reviews for example is this code

00:04:50,450 --> 00:04:58,040
production ready I wouldn't do this in

00:04:53,960 --> 00:05:02,390
production things like that maybe if it

00:04:58,040 --> 00:05:03,860
compiles its production ready I think

00:05:02,390 --> 00:05:08,960
we've all worked at places where this is

00:05:03,860 --> 00:05:15,020
true if it's tested is it production

00:05:08,960 --> 00:05:18,830
ready well one thing that production

00:05:15,020 --> 00:05:21,230
ready does not mean is bug-free you

00:05:18,830 --> 00:05:27,800
always have to account that something

00:05:21,230 --> 00:05:33,920
might go wrong in production so how

00:05:27,800 --> 00:05:36,200
about this observable think back to that

00:05:33,920 --> 00:05:39,770
chip we made sure when we were designing

00:05:36,200 --> 00:05:43,250
the chip that it was observable so we

00:05:39,770 --> 00:05:46,610
could work out what goes wrong when it

00:05:43,250 --> 00:05:49,880
has been manufactured so this is what I

00:05:46,610 --> 00:05:53,090
want to talk about how can we make our

00:05:49,880 --> 00:05:55,910
software observable so that when

00:05:53,090 --> 00:05:58,790
something goes wrong in production we

00:05:55,910 --> 00:06:04,030
can easily work out what went wrong and

00:05:58,790 --> 00:06:06,710
how we can fix it so let's talk about

00:06:04,030 --> 00:06:11,390
instrumentation it's in the title of a

00:06:06,710 --> 00:06:14,360
talk let's get a definition should

00:06:11,390 --> 00:06:18,830
always start with a definition right so

00:06:14,360 --> 00:06:20,060
wikipedia says instrumentation is a

00:06:18,830 --> 00:06:22,280
collective term for measuring

00:06:20,060 --> 00:06:23,510
instruments used for indicating

00:06:22,280 --> 00:06:27,320
measuring and recording physical

00:06:23,510 --> 00:06:32,360
quantities fantastic it's quite useful

00:06:27,320 --> 00:06:34,480
to know and even better it says in the

00:06:32,360 --> 00:06:37,310
context of computer programming

00:06:34,480 --> 00:06:38,479
instrumentation refers to an ability to

00:06:37,310 --> 00:06:40,879
monitor or

00:06:38,479 --> 00:06:43,370
the level of a products performance to

00:06:40,879 --> 00:06:47,949
diagnose errors and write trace

00:06:43,370 --> 00:06:51,199
information this is exactly what it is

00:06:47,949 --> 00:06:53,800
but we should get a second opinion so

00:06:51,199 --> 00:06:57,639
let's ask urban dictionary

00:06:53,800 --> 00:07:00,889
great source of all technical knowledge

00:06:57,639 --> 00:07:03,979
the Urban Dictionary says has its little

00:07:00,889 --> 00:07:08,870
example at the bottom hey John look at

00:07:03,979 --> 00:07:10,309
that expensive instrumentation and what

00:07:08,870 --> 00:07:13,159
sue is actually talking about here is

00:07:10,309 --> 00:07:16,099
the fact that if you add instrumentation

00:07:13,159 --> 00:07:20,599
to your software it may have overheads

00:07:16,099 --> 00:07:21,949
so it is actually expensive so what I'm

00:07:20,599 --> 00:07:30,399
telling you is urban dictionary is a

00:07:21,949 --> 00:07:32,240
great source for C++ programmers so

00:07:30,399 --> 00:07:35,689
instrumentation allows us to observe

00:07:32,240 --> 00:07:38,629
deployed software we can monitor that

00:07:35,689 --> 00:07:41,870
it's working we can measure the

00:07:38,629 --> 00:07:45,020
performance of it as it runs and we can

00:07:41,870 --> 00:07:49,810
diagnose errors when they occur and

00:07:45,020 --> 00:07:49,810
trace them back to what caused them

00:07:51,580 --> 00:07:58,099
specifically in this talk we're going to

00:07:55,399 --> 00:08:01,240
talk about source instrumentation so

00:07:58,099 --> 00:08:05,149
this is adding code to your source

00:08:01,240 --> 00:08:07,729
adding features to your code to

00:08:05,149 --> 00:08:11,419
instrument it and this is built-in to

00:08:07,729 --> 00:08:13,699
your production releases I really want

00:08:11,419 --> 00:08:16,009
to emphasize this the instrumentation is

00:08:13,699 --> 00:08:17,539
not something you just do in your debug

00:08:16,009 --> 00:08:21,169
build it's something that's in your

00:08:17,539 --> 00:08:23,089
product in your software your software

00:08:21,169 --> 00:08:28,580
becomes instrumented and runs in that

00:08:23,089 --> 00:08:31,789
form and there are some alternatives to

00:08:28,580 --> 00:08:37,529
doing this but this is not what we're

00:08:31,789 --> 00:08:40,469
going to talk about today the best for

00:08:37,529 --> 00:08:45,600
of instrumentation printf I mean

00:08:40,469 --> 00:08:48,629
blogging I always get that wrong so this

00:08:45,600 --> 00:08:52,920
is what logging is for at some point

00:08:48,629 --> 00:08:55,069
early in the morning you will get a

00:08:52,920 --> 00:08:59,009
message saying your software crashed

00:08:55,069 --> 00:09:02,639
this will happen prepare for it and you

00:08:59,009 --> 00:09:07,259
will ask what's in the log and what were

00:09:02,639 --> 00:09:13,800
they tell you is it stood bad Alec and

00:09:07,259 --> 00:09:15,779
you then have to work out why and you

00:09:13,800 --> 00:09:18,689
might then question why you became a

00:09:15,779 --> 00:09:20,759
software developer maybe you start

00:09:18,689 --> 00:09:24,420
looking on jobs on Stack Overflow for

00:09:20,759 --> 00:09:25,769
new jobs but a few hours later you'll

00:09:24,420 --> 00:09:32,790
probably ask them to reboot the machine

00:09:25,769 --> 00:09:35,910
and maybe it will fix everything ok

00:09:32,790 --> 00:09:38,430
let's get a bit technical now so I have

00:09:35,910 --> 00:09:41,910
this little example program they'll be

00:09:38,430 --> 00:09:45,930
going to work through in the talk it's

00:09:41,910 --> 00:09:48,449
just a basic shell of a C++ program does

00:09:45,930 --> 00:09:50,610
some processing and any exception that

00:09:48,449 --> 00:09:53,850
occurs in the processing is printed out

00:09:50,610 --> 00:09:56,639
we've probably all wrote little snippets

00:09:53,850 --> 00:09:59,879
of code like this but what we're going

00:09:56,639 --> 00:10:03,600
to focus on is this error that's coming

00:09:59,879 --> 00:10:06,589
out of the code and we can assume that

00:10:03,600 --> 00:10:09,329
it's coming out of this process file

00:10:06,589 --> 00:10:11,579
function that we've read maybe it reads

00:10:09,329 --> 00:10:19,559
a file and processes the contents of the

00:10:11,579 --> 00:10:22,589
file so what we really need is a lot

00:10:19,559 --> 00:10:25,709
more information about this error this

00:10:22,589 --> 00:10:27,240
permission denied messages not nearly

00:10:25,709 --> 00:10:33,000
enough information for us to work out

00:10:27,240 --> 00:10:36,899
what's gone wrong so we could add some

00:10:33,000 --> 00:10:41,749
context and this is where we start

00:10:36,899 --> 00:10:44,399
talking about logging maybe we use C out

00:10:41,749 --> 00:10:47,370
just to print some extra information out

00:10:44,399 --> 00:10:50,980
and in this case wouldn't it be useful

00:10:47,370 --> 00:10:57,370
to know which file you are accessing

00:10:50,980 --> 00:11:00,279
where the permission was denied maybe in

00:10:57,370 --> 00:11:03,010
addition we want some context about why

00:11:00,279 --> 00:11:06,699
this operation is even occurring what

00:11:03,010 --> 00:11:07,660
user caused the operation what

00:11:06,699 --> 00:11:11,170
connection

00:11:07,660 --> 00:11:13,240
the request came from thought this will

00:11:11,170 --> 00:11:20,139
vary depending on their sort of software

00:11:13,240 --> 00:11:22,690
you're working on and then on top of

00:11:20,139 --> 00:11:24,760
this we can clean up this error message

00:11:22,690 --> 00:11:27,010
that we're sending to the user through

00:11:24,760 --> 00:11:30,279
the exception because the user doesn't

00:11:27,010 --> 00:11:31,500
care but permission for something has

00:11:30,279 --> 00:11:33,610
been denied

00:11:31,500 --> 00:11:36,399
well they care about is that they

00:11:33,610 --> 00:11:38,800
request failed and maybe we tell them a

00:11:36,399 --> 00:11:40,779
little bit of information we don't

00:11:38,800 --> 00:11:46,570
expose too much of the internals of our

00:11:40,779 --> 00:11:49,240
application so this is a good start now

00:11:46,570 --> 00:11:53,430
we have a lot more information and maybe

00:11:49,240 --> 00:11:53,430
we can work out what went wrong

00:11:58,270 --> 00:12:06,640
so these little bits of writing stuff to

00:12:03,650 --> 00:12:09,890
the screen at some point we'll think

00:12:06,640 --> 00:12:12,800
okay we're definitely logging now so we

00:12:09,890 --> 00:12:17,630
should use a logging library be

00:12:12,800 --> 00:12:20,600
professional about it so maybe we use

00:12:17,630 --> 00:12:22,010
something of Internet maybe we get

00:12:20,600 --> 00:12:24,500
something come up package manager

00:12:22,010 --> 00:12:27,100
maybe we write one ourselves maybe the

00:12:24,500 --> 00:12:30,320
company has their own logging library

00:12:27,100 --> 00:12:36,170
whatever it is you know it's going to

00:12:30,320 --> 00:12:39,320
look a bit like this maybe you get some

00:12:36,170 --> 00:12:42,350
ability to use format strings you can

00:12:39,320 --> 00:12:46,340
specify severity levels and they'll have

00:12:42,350 --> 00:12:51,220
lots of useful features for writing log

00:12:46,340 --> 00:12:57,800
files and maintaining them but this

00:12:51,220 --> 00:13:03,020
really isn't what's interesting I think

00:12:57,800 --> 00:13:09,560
about logging the problem with logging

00:13:03,020 --> 00:13:11,660
is us the humans we've made log files to

00:13:09,560 --> 00:13:15,830
be human readable so that we can read

00:13:11,660 --> 00:13:19,520
them and understand quickly how to fix a

00:13:15,830 --> 00:13:22,990
problem and this is fine if you've got a

00:13:19,520 --> 00:13:27,400
little log file or a couple of files to

00:13:22,990 --> 00:13:31,280
read the problem gets increasingly worse

00:13:27,400 --> 00:13:34,550
when your software grows or more people

00:13:31,280 --> 00:13:39,200
are using your software you can actually

00:13:34,550 --> 00:13:42,950
become less and less happy because we

00:13:39,200 --> 00:13:48,680
don't scale we can't read thousands of

00:13:42,950 --> 00:13:52,550
log files it's not possible and so we

00:13:48,680 --> 00:13:55,700
started to imagine was this extra layer

00:13:52,550 --> 00:13:59,830
between our logs which for the purposes

00:13:55,700 --> 00:14:01,660
of this talk we'll just call magic and

00:13:59,830 --> 00:14:03,460
this

00:14:01,660 --> 00:14:06,430
processes all this log data we've got

00:14:03,460 --> 00:14:10,720
and puts it into a nicer form for us to

00:14:06,430 --> 00:14:12,760
be able to read I'm not going to go into

00:14:10,720 --> 00:14:15,610
too much detail about this magic but

00:14:12,760 --> 00:14:18,430
roughly speaking what it refers to is a

00:14:15,610 --> 00:14:21,670
growing ecosystem of software and

00:14:18,430 --> 00:14:25,590
services that let you process and

00:14:21,670 --> 00:14:28,420
understand your logs better and this

00:14:25,590 --> 00:14:30,070
it's a huge amount of software that does

00:14:28,420 --> 00:14:33,160
this for you and if you're running your

00:14:30,070 --> 00:14:34,750
applications on a cloud then you can

00:14:33,160 --> 00:14:37,000
probably just pipe all of your logs into

00:14:34,750 --> 00:14:43,510
some service and it will sort them out

00:14:37,000 --> 00:14:45,850
for you so what this gives you typically

00:14:43,510 --> 00:14:48,220
is some ability to search through your

00:14:45,850 --> 00:14:51,640
lobs for particular errors or a

00:14:48,220 --> 00:14:55,170
particular time it might do some

00:14:51,640 --> 00:14:58,780
reporting for you produce some metrics

00:14:55,170 --> 00:15:02,890
tell you how many errors occurred on a

00:14:58,780 --> 00:15:04,660
particular host for example and maybe

00:15:02,890 --> 00:15:07,150
it'll give you some alerting so you can

00:15:04,660 --> 00:15:09,580
only be notified when really important

00:15:07,150 --> 00:15:15,010
things happen you're not constantly

00:15:09,580 --> 00:15:21,610
reading through these laws so this is a

00:15:15,010 --> 00:15:26,440
very rough overview of this systems the

00:15:21,610 --> 00:15:30,810
problem comes if we want to use these

00:15:26,440 --> 00:15:34,030
systems all we have to feed them is this

00:15:30,810 --> 00:15:38,230
human readable text that we munched

00:15:34,030 --> 00:15:40,570
together from all this information so

00:15:38,230 --> 00:15:45,820
the first thing a lot of these systems

00:15:40,570 --> 00:15:50,970
do is process this data into a much

00:15:45,820 --> 00:15:50,970
nicer format give some structure to it

00:15:51,630 --> 00:15:57,880
fill out for example the username and

00:15:54,760 --> 00:16:00,040
the IP address so we can search on these

00:15:57,880 --> 00:16:02,740
things and we can index all of our logs

00:16:00,040 --> 00:16:07,770
and all of our errors by user and say

00:16:02,740 --> 00:16:07,770
which errors occurred for this user

00:16:09,010 --> 00:16:14,800
and this is typically implemented in

00:16:11,410 --> 00:16:18,190
various ways but the most common is this

00:16:14,800 --> 00:16:22,300
mess of regular expressions you end up

00:16:18,190 --> 00:16:27,160
writing to parse out the data from your

00:16:22,300 --> 00:16:31,180
lobs but this is insane

00:16:27,160 --> 00:16:33,430
we already had those bits of data in a

00:16:31,180 --> 00:16:37,180
nicely structured form in our

00:16:33,430 --> 00:16:39,490
application and we merged them into a

00:16:37,180 --> 00:16:45,430
text format which we then passed back

00:16:39,490 --> 00:16:50,079
into a structured format what we really

00:16:45,430 --> 00:16:53,949
want to do is just output the data that

00:16:50,079 --> 00:16:56,260
we had in a structured form it's still

00:16:53,949 --> 00:16:59,829
human readable if we need to but it

00:16:56,260 --> 00:17:02,320
makes it so much more easy or machines

00:16:59,829 --> 00:17:04,480
to process it and this is what we want

00:17:02,320 --> 00:17:07,209
to do we want to automate the processing

00:17:04,480 --> 00:17:09,880
of this huge mess of log data that we've

00:17:07,209 --> 00:17:14,980
become required and this is becoming

00:17:09,880 --> 00:17:18,339
very popular in other languages doesn't

00:17:14,980 --> 00:17:20,350
seem to have caught on so much in C++

00:17:18,339 --> 00:17:24,790
yet well I think it's something that is

00:17:20,350 --> 00:17:30,660
worth mentioning because it allows us to

00:17:24,790 --> 00:17:30,660
eliminate all of this unnecessary work

00:17:31,799 --> 00:17:40,419
and this idea of structuring our data

00:17:37,169 --> 00:17:44,790
brings us on to the more interesting

00:17:40,419 --> 00:17:44,790
topic in my opinion which is tracing

00:17:45,720 --> 00:17:52,799
tracing is basically logging but it

00:17:49,780 --> 00:17:57,990
gives a bit more information about what

00:17:52,799 --> 00:18:01,480
your logs mean so a trace is typically

00:17:57,990 --> 00:18:03,940
something that has a start and something

00:18:01,480 --> 00:18:07,270
that has an end so an operation that

00:18:03,940 --> 00:18:08,980
takes some amount of time and the

00:18:07,270 --> 00:18:13,780
questions we're interested in arcs King

00:18:08,980 --> 00:18:17,710
in answering with tracing is what caused

00:18:13,780 --> 00:18:21,400
the error we want to build up a history

00:18:17,710 --> 00:18:22,930
of what happened in our system so that

00:18:21,400 --> 00:18:27,460
we can trace back

00:18:22,930 --> 00:18:28,840
the source of the error and the other

00:18:27,460 --> 00:18:30,850
thing we're interested in doing is

00:18:28,840 --> 00:18:35,880
looking at performance so how long did

00:18:30,850 --> 00:18:44,940
something take a good example of this is

00:18:35,880 --> 00:18:50,050
s Trace brilliant little utility which

00:18:44,940 --> 00:18:53,230
instruments some program and logs out

00:18:50,050 --> 00:18:58,650
the system calls that are used by the

00:18:53,230 --> 00:19:02,590
program so if we look at this example of

00:18:58,650 --> 00:19:08,980
catting some file out it will tell us

00:19:02,590 --> 00:19:13,150
that open was called because we need to

00:19:08,980 --> 00:19:15,429
read the file and later on we call

00:19:13,150 --> 00:19:21,309
another system call to read the data out

00:19:15,429 --> 00:19:24,910
of the file the first thing this tells

00:19:21,309 --> 00:19:29,260
us is what actually happened the system

00:19:24,910 --> 00:19:34,570
call the arguments and even the error

00:19:29,260 --> 00:19:39,610
message the error code sorry which is

00:19:34,570 --> 00:19:45,700
very useful in itself but we also learn

00:19:39,610 --> 00:19:49,000
the time that this occurred and how long

00:19:45,700 --> 00:19:53,910
it took so we can start looking for

00:19:49,000 --> 00:19:53,910
bottlenecks in the code

00:19:58,110 --> 00:20:03,429
so the way this tends to evolve is you

00:20:00,639 --> 00:20:06,580
start with having some logging in your

00:20:03,429 --> 00:20:08,399
application maybe you have this process

00:20:06,580 --> 00:20:11,860
file function that we touched on earlier

00:20:08,399 --> 00:20:14,169
it reads a file opens a file and then

00:20:11,860 --> 00:20:20,830
reads the data and processes each line

00:20:14,169 --> 00:20:22,720
of the file in some way and we log it

00:20:20,830 --> 00:20:27,460
and we log which file we read and we log

00:20:22,720 --> 00:20:30,730
which user did it useful wouldn't it be

00:20:27,460 --> 00:20:33,220
nice if we knew when it ended because

00:20:30,730 --> 00:20:36,279
then we know how long it takes we know

00:20:33,220 --> 00:20:43,090
when our code is finished with a file

00:20:36,279 --> 00:20:45,460
and it closes it off so eventually we

00:20:43,090 --> 00:20:47,710
realize okay now we're doing tracing

00:20:45,460 --> 00:20:52,450
we're not doing logging anymore so we

00:20:47,710 --> 00:20:55,029
use a library for that and typically you

00:20:52,450 --> 00:20:59,590
get these little utilities they look a

00:20:55,029 --> 00:21:02,350
bit like your log code but they'll

00:20:59,590 --> 00:21:06,460
produce some instance of some trace

00:21:02,350 --> 00:21:09,999
object for you and then you can end it

00:21:06,460 --> 00:21:11,830
when your operations finished and many

00:21:09,999 --> 00:21:13,690
of these tools will if you don't do it

00:21:11,830 --> 00:21:16,110
explicitly the destructor will do it for

00:21:13,690 --> 00:21:21,879
you so if your function finishes then

00:21:16,110 --> 00:21:26,350
the end trace gets written and you can

00:21:21,879 --> 00:21:28,210
add the same information as we did with

00:21:26,350 --> 00:21:29,230
the log any sort of arbitrary

00:21:28,210 --> 00:21:34,450
information you think might be

00:21:29,230 --> 00:21:39,159
interesting file name user IP address we

00:21:34,450 --> 00:21:41,169
can add that as well so let's look at

00:21:39,159 --> 00:21:43,240
the an example of how this might be

00:21:41,169 --> 00:21:45,879
output because this data doesn't have to

00:21:43,240 --> 00:21:51,129
be output as logs it's quite common to

00:21:45,879 --> 00:21:54,669
do so maybe we get these file read

00:21:51,129 --> 00:21:58,749
events with some metadata and then we

00:21:54,669 --> 00:22:00,730
get the end maybe in our application it

00:21:58,749 --> 00:22:02,909
opens up a couple of files and processes

00:22:00,730 --> 00:22:02,909
them

00:22:04,950 --> 00:22:10,380
what it's common to do kind of like we

00:22:06,990 --> 00:22:12,300
looked at with the s trace example is to

00:22:10,380 --> 00:22:18,240
only admit one event for these two

00:22:12,300 --> 00:22:21,630
events so we admit the event when the

00:22:18,240 --> 00:22:25,020
operation ends and we admit all of the

00:22:21,630 --> 00:22:27,870
metadata and then instead of the start

00:22:25,020 --> 00:22:30,870
we just record the time that the event

00:22:27,870 --> 00:22:34,980
started or alternatively we could do

00:22:30,870 --> 00:22:37,380
record the duration you get the same

00:22:34,980 --> 00:22:42,360
information regardless of what you put

00:22:37,380 --> 00:22:45,780
out we're tracing becomes really

00:22:42,360 --> 00:22:49,500
interesting is how you build up

00:22:45,780 --> 00:22:54,090
relationships between operations so if

00:22:49,500 --> 00:22:58,140
we have some connect operation some user

00:22:54,090 --> 00:23:02,580
starts a connection to our service we

00:22:58,140 --> 00:23:04,830
can trace that but then inside that

00:23:02,580 --> 00:23:08,280
connection maybe we have to read some

00:23:04,830 --> 00:23:13,410
files so we have another operation

00:23:08,280 --> 00:23:16,580
within an operation and so what we can

00:23:13,410 --> 00:23:20,940
do is actually link these together

00:23:16,580 --> 00:23:23,280
through some scheme or another so I'm

00:23:20,940 --> 00:23:29,430
incrementing integer or some sort of

00:23:23,280 --> 00:23:32,580
UUID and we can tag the inner trace with

00:23:29,430 --> 00:23:37,890
the ID of the outer trace so now we get

00:23:32,580 --> 00:23:39,270
this relationship between the traces you

00:23:37,890 --> 00:23:42,450
typically don't have to worry about this

00:23:39,270 --> 00:23:46,110
because if you're using a sufficient

00:23:42,450 --> 00:23:48,300
library then it will have support for

00:23:46,110 --> 00:23:50,400
building these relationships so if you

00:23:48,300 --> 00:23:52,860
create your trace and you create your

00:23:50,400 --> 00:23:54,600
inner trace from the first trace then

00:23:52,860 --> 00:24:01,080
they will get linked together through

00:23:54,600 --> 00:24:05,970
some identifier what's even more

00:24:01,080 --> 00:24:11,030
interesting is that you can often write

00:24:05,970 --> 00:24:14,630
logs to traces so now you can associate

00:24:11,030 --> 00:24:16,500
errors with particular operations and

00:24:14,630 --> 00:24:18,599
this Cascades

00:24:16,500 --> 00:24:21,139
so you have an error

00:24:18,599 --> 00:24:25,859
the very bottom of your application you

00:24:21,139 --> 00:24:28,589
failed to write a file you can then

00:24:25,859 --> 00:24:31,799
trace it all the way up to where the

00:24:28,589 --> 00:24:36,449
client invoked that operation that

00:24:31,799 --> 00:24:38,369
ultimately failed and presumably this is

00:24:36,449 --> 00:24:42,109
where the name tracing comes from as you

00:24:38,369 --> 00:24:42,109
get to trace the origin of your errors

00:24:43,399 --> 00:24:49,679
let's look a little bit more of these

00:24:45,269 --> 00:24:52,529
relationships we can trace errors from

00:24:49,679 --> 00:24:57,019
client to root cause but we can also see

00:24:52,529 --> 00:25:00,149
within operation where bottlenecks are

00:24:57,019 --> 00:25:06,559
is one operation taking longer than

00:25:00,149 --> 00:25:09,209
another and this is actually essential

00:25:06,559 --> 00:25:14,549
for systems with any sort of concurrency

00:25:09,209 --> 00:25:17,459
in because if we exact if we imagine two

00:25:14,549 --> 00:25:21,359
concurrent streams of processing going

00:25:17,459 --> 00:25:25,139
on here they're interlinked so they're

00:25:21,359 --> 00:25:26,759
interleaved bits of one happen then bits

00:25:25,139 --> 00:25:29,579
of the other happen then bits of the

00:25:26,759 --> 00:25:32,729
next one happen so by building these

00:25:29,579 --> 00:25:34,739
relationships we actually know the

00:25:32,729 --> 00:25:36,809
correct flow and we don't have to infer

00:25:34,739 --> 00:25:41,279
it from just reading through the log

00:25:36,809 --> 00:25:42,779
file because log file isn't ordered it's

00:25:41,279 --> 00:25:48,329
just a complete mess of all the things

00:25:42,779 --> 00:25:51,419
that are concurrently happening so

00:25:48,329 --> 00:25:53,759
pretty pictures time a really nice way

00:25:51,419 --> 00:25:59,809
to visualize these tracers is with

00:25:53,759 --> 00:26:02,309
little timelines we can for example

00:25:59,809 --> 00:26:07,889
display all of the traces for a

00:26:02,309 --> 00:26:12,169
particular user every time a user causes

00:26:07,889 --> 00:26:12,169
a file to be read we can display that

00:26:12,409 --> 00:26:20,609
well what we can also do is then display

00:26:16,289 --> 00:26:24,389
the connection for that user and we can

00:26:20,609 --> 00:26:26,249
draw the links between them so we start

00:26:24,389 --> 00:26:28,229
getting this sort of it looks a little

00:26:26,249 --> 00:26:32,539
bit like a call graph like the

00:26:28,229 --> 00:26:32,539
connection triggers the file read

00:26:33,120 --> 00:26:42,790
and we can add the other data in the

00:26:39,910 --> 00:26:44,950
file alongside it as well if we're

00:26:42,790 --> 00:26:51,010
interested in correlating the two

00:26:44,950 --> 00:26:54,270
together this becomes even more

00:26:51,010 --> 00:26:58,090
interesting when you think about

00:26:54,270 --> 00:27:01,090
software that is broken up into multiple

00:26:58,090 --> 00:27:03,700
address spaces for example multiple

00:27:01,090 --> 00:27:05,500
processes or if you have a distributed

00:27:03,700 --> 00:27:09,160
application where parts video running on

00:27:05,500 --> 00:27:11,470
different nodes you can then that link

00:27:09,160 --> 00:27:13,690
operations which occur in completely

00:27:11,470 --> 00:27:17,200
different processes and nodes together

00:27:13,690 --> 00:27:18,780
and you can say the cause of this was

00:27:17,200 --> 00:27:23,790
actually something that happened

00:27:18,780 --> 00:27:28,480
somewhere else not in my address space

00:27:23,790 --> 00:27:31,360
so this is a lot of benefits for doing

00:27:28,480 --> 00:27:33,670
just by adding taking our logs but just

00:27:31,360 --> 00:27:42,600
adding a little bit more structure to

00:27:33,670 --> 00:27:44,970
them that's what we're doing so in a C++

00:27:42,600 --> 00:27:48,360
application we care a lot about

00:27:44,970 --> 00:27:51,640
performance so we need to be very valued

00:27:48,360 --> 00:27:54,070
careful about the overheads which we

00:27:51,640 --> 00:27:56,590
incur one thing we're particularly

00:27:54,070 --> 00:28:00,670
interested in is being able to disable

00:27:56,590 --> 00:28:03,040
our tray cing and this is a similar

00:28:00,670 --> 00:28:04,930
concept to what you might do for your

00:28:03,040 --> 00:28:08,980
logs you want to be able to turn them

00:28:04,930 --> 00:28:13,500
off and when you turn them off you

00:28:08,980 --> 00:28:20,290
ideally want them to have no overhead

00:28:13,500 --> 00:28:26,010
ideally you can't always get this low

00:28:20,290 --> 00:28:26,010
but there's some systems you can

00:28:27,200 --> 00:28:32,360
and then what we care about is of course

00:28:29,960 --> 00:28:34,460
the overhead when the tracing is turned

00:28:32,360 --> 00:28:38,929
on because when it's turned on is when

00:28:34,460 --> 00:28:42,380
it's valuable and there's a few things

00:28:38,929 --> 00:28:45,460
we can do a very common technique is to

00:28:42,380 --> 00:28:49,100
sample your traces so only actually emit

00:28:45,460 --> 00:28:53,389
information for every tenth operation or

00:28:49,100 --> 00:28:56,450
every hundredth operation we could

00:28:53,389 --> 00:28:59,210
aggregate our trace data so instead of

00:28:56,450 --> 00:29:03,019
outputting all the details of every

00:28:59,210 --> 00:29:05,080
trace we just output the count of things

00:29:03,019 --> 00:29:08,720
that happened and we just output the

00:29:05,080 --> 00:29:11,059
total time that all of the operations

00:29:08,720 --> 00:29:14,899
took this still has a lot of value

00:29:11,059 --> 00:29:20,690
because we can then see for example the

00:29:14,899 --> 00:29:23,149
average time that our operation took and

00:29:20,690 --> 00:29:25,850
if you're thinking this sounds a lot

00:29:23,149 --> 00:29:29,120
like what I get from my profiler and

00:29:25,850 --> 00:29:31,840
you'd be right in a sense a profiler

00:29:29,120 --> 00:29:36,080
that you might use a development time

00:29:31,840 --> 00:29:40,220
it's like a very specialized type of

00:29:36,080 --> 00:29:44,559
tracing for finding performance

00:29:40,220 --> 00:29:47,210
bottlenecks and then we can also build

00:29:44,559 --> 00:29:50,450
tracing systems but don't actually use

00:29:47,210 --> 00:29:53,029
logging that are much more efficient

00:29:50,450 --> 00:29:56,080
than formatting and writing out huge

00:29:53,029 --> 00:30:01,549
blobs of text to files we can imagine

00:29:56,080 --> 00:30:05,750
much better optimized binary formats if

00:30:01,549 --> 00:30:07,299
you think about the tool TCP dump this

00:30:05,750 --> 00:30:10,460
is a tracing tool

00:30:07,299 --> 00:30:14,659
fundamentally it's tracing your network

00:30:10,460 --> 00:30:17,840
activity in and out of a node and it has

00:30:14,659 --> 00:30:22,570
a very specific format for storing the

00:30:17,840 --> 00:30:22,570
traces the network packets in a file and

00:30:22,840 --> 00:30:29,500
if we go to look at the Linux kernel

00:30:25,929 --> 00:30:32,990
they have a tracing file format as well

00:30:29,500 --> 00:30:34,909
Linux has this very elaborate mechanism

00:30:32,990 --> 00:30:39,879
of adding trace points into the kernel

00:30:34,909 --> 00:30:39,879
so you can see what's happening

00:30:40,120 --> 00:30:46,010
but there's the big problem with any

00:30:44,540 --> 00:30:49,580
sort of tracing in any sort of logging

00:30:46,010 --> 00:30:53,810
is that the overhead grows as your

00:30:49,580 --> 00:30:56,930
application does more things if your

00:30:53,810 --> 00:31:00,140
application does a thousand things then

00:30:56,930 --> 00:31:02,120
you have to trace a thousand times if it

00:31:00,140 --> 00:31:04,190
does ten thousand if it traced ten

00:31:02,120 --> 00:31:06,770
thousand times unless you're doing some

00:31:04,190 --> 00:31:09,140
sampling of course so the overhead

00:31:06,770 --> 00:31:10,820
always grows as your application starts

00:31:09,140 --> 00:31:14,840
to do more things or you want to trace

00:31:10,820 --> 00:31:17,200
more parts of your application which

00:31:14,840 --> 00:31:25,880
leads us on to why we want to talk about

00:31:17,200 --> 00:31:29,930
metrics what a matrix they're just

00:31:25,880 --> 00:31:35,180
numbers interesting numbers hold

00:31:29,930 --> 00:31:38,390
periodically and a really good example

00:31:35,180 --> 00:31:42,380
of this is top have you ever seen a

00:31:38,390 --> 00:31:43,940
screen like this little utility you can

00:31:42,380 --> 00:31:46,490
run and get it on most operating systems

00:31:43,940 --> 00:31:49,550
this is H top in particular but

00:31:46,490 --> 00:31:51,170
fundamentally they're the same you get a

00:31:49,550 --> 00:31:56,960
lot of really useful information about

00:31:51,170 --> 00:31:57,980
your system you get your CPU usage you

00:31:56,960 --> 00:32:00,710
get the amount of memory that you're

00:31:57,980 --> 00:32:04,780
using number of processes that are

00:32:00,710 --> 00:32:10,660
running the uptime of your server

00:32:04,780 --> 00:32:14,200
extremely useful but notice the wide

00:32:10,660 --> 00:32:16,130
array of different types of numbers

00:32:14,200 --> 00:32:18,770
we've got duration

00:32:16,130 --> 00:32:21,170
we've got counters we've got absolute

00:32:18,770 --> 00:32:26,840
values like memory well your relative

00:32:21,170 --> 00:32:31,660
values like CPU usage this is really

00:32:26,840 --> 00:32:35,560
useful and typically what we want a

00:32:31,660 --> 00:32:40,570
metric for is the history of it

00:32:35,560 --> 00:32:45,200
so if we have memory usage of a node

00:32:40,570 --> 00:32:48,920
when our bad alloc occurs in our

00:32:45,200 --> 00:32:51,530
production system we're able to look at

00:32:48,920 --> 00:32:53,720
this memory metric and see

00:32:51,530 --> 00:32:57,580
well something something happened here

00:32:53,720 --> 00:32:57,580
we should probably take a look at that

00:32:58,480 --> 00:33:06,020
and then we can build alerts on top of

00:33:02,330 --> 00:33:11,800
this to say if our memory increases over

00:33:06,020 --> 00:33:15,170
a certain threshold then tell us and

00:33:11,800 --> 00:33:17,900
maybe if we're clever enough we can find

00:33:15,170 --> 00:33:20,200
out why it broke and everything goes

00:33:17,900 --> 00:33:20,200
back to normal

00:33:21,850 --> 00:33:29,830
the typical workflow for system metrics

00:33:26,690 --> 00:33:32,540
at least this is very commonplace

00:33:29,830 --> 00:33:36,470
there's been the technique that's been

00:33:32,540 --> 00:33:39,470
around long long long time you collect

00:33:36,470 --> 00:33:42,670
the metrics from your servers you store

00:33:39,470 --> 00:33:46,190
them somewhere and then you analyze them

00:33:42,670 --> 00:33:48,710
and there were a huge number of systems

00:33:46,190 --> 00:33:50,180
that will do this for you and again if

00:33:48,710 --> 00:33:51,920
you're using some sort of cloud provider

00:33:50,180 --> 00:33:55,100
well they will also have a metrics

00:33:51,920 --> 00:33:57,530
collection system you can use what we

00:33:55,100 --> 00:34:00,440
want to do when we're developing

00:33:57,530 --> 00:34:03,740
software is hook into this we want to

00:34:00,440 --> 00:34:05,300
expose our own metrics and have them

00:34:03,740 --> 00:34:08,330
collected and have them analyzed and

00:34:05,300 --> 00:34:11,060
alert on them and we want to get all the

00:34:08,330 --> 00:34:13,700
same benefits that we get when we're

00:34:11,060 --> 00:34:16,659
monitoring our infrastructure and our

00:34:13,700 --> 00:34:23,540
service our temperature isn't CPUs and

00:34:16,659 --> 00:34:31,879
less let's look at what a metric is made

00:34:23,540 --> 00:34:36,790
of we give the name temperature some

00:34:31,879 --> 00:34:40,280
sort of count number of things happened

00:34:36,790 --> 00:34:43,280
and we tag it if we have multiple

00:34:40,280 --> 00:34:45,800
versions of that metric we have multiple

00:34:43,280 --> 00:34:47,750
hosts then each of which have some

00:34:45,800 --> 00:34:50,210
temperature sensor then we can tag it

00:34:47,750 --> 00:34:55,899
and say specifically which one this is a

00:34:50,210 --> 00:34:59,540
measurement for and of course the value

00:34:55,899 --> 00:35:02,110
and then the timestamp which we took the

00:34:59,540 --> 00:35:02,110
measurement for

00:35:02,770 --> 00:35:10,810
this example happens to be open metrics

00:35:06,290 --> 00:35:16,210
it's a evolving an open standard for

00:35:10,810 --> 00:35:16,210
passing metric data between systems

00:35:17,530 --> 00:35:21,460
let's get back to some code why we're

00:35:20,930 --> 00:35:24,560
here

00:35:21,460 --> 00:35:30,590
same example was earlier little process

00:35:24,560 --> 00:35:33,320
file function within a metrics library

00:35:30,590 --> 00:35:38,030
and this library it will be best to use

00:35:33,320 --> 00:35:40,490
the same library that the library that

00:35:38,030 --> 00:35:42,770
your infrastructure recommends you use

00:35:40,490 --> 00:35:44,360
so if you're using a particular type of

00:35:42,770 --> 00:35:46,550
monitoring software for your

00:35:44,360 --> 00:35:50,930
infrastructure then they will probably

00:35:46,550 --> 00:35:56,630
have a C++ client that you can use to

00:35:50,930 --> 00:36:02,030
expose your own metrics what we can do

00:35:56,630 --> 00:36:05,090
is start building metrics in for example

00:36:02,030 --> 00:36:06,800
maybe we want to count something maybe

00:36:05,090 --> 00:36:13,040
we want to count the number of times we

00:36:06,800 --> 00:36:14,990
read a file so we can add a little

00:36:13,040 --> 00:36:16,820
counter we can add some tags and some

00:36:14,990 --> 00:36:20,410
metadata to do it and then we can

00:36:16,820 --> 00:36:20,410
increment it every time we read a file

00:36:21,460 --> 00:36:29,330
the count of itself will typically look

00:36:25,910 --> 00:36:34,630
like this it will have some integer

00:36:29,330 --> 00:36:38,510
inside it probably an atomic integer and

00:36:34,630 --> 00:36:43,480
some function to increment the count and

00:36:38,510 --> 00:36:48,560
a function to obtain the count not very

00:36:43,480 --> 00:36:51,670
complicated the idea behind this counter

00:36:48,560 --> 00:36:55,130
though is to keep that increment as

00:36:51,670 --> 00:36:58,070
lightweight as possible we want it to do

00:36:55,130 --> 00:37:00,140
as little as possible so when we add it

00:36:58,070 --> 00:37:04,130
in to our code so every time we read a

00:37:00,140 --> 00:37:10,940
file the overhead we're adding in to our

00:37:04,130 --> 00:37:14,090
application is negligible and a lot

00:37:10,940 --> 00:37:16,780
cheaper than if we were to put a log in

00:37:14,090 --> 00:37:16,780
the same place

00:37:21,099 --> 00:37:26,299
typically this will vary depending on

00:37:24,650 --> 00:37:30,650
the library in the infrastructure using

00:37:26,299 --> 00:37:32,779
but how you collect these counters will

00:37:30,650 --> 00:37:37,099
through be through some thread that's

00:37:32,779 --> 00:37:41,630
running periodically and picking up each

00:37:37,099 --> 00:37:44,239
value from each counter you have some

00:37:41,630 --> 00:37:46,339
registry somewhere in the library of all

00:37:44,239 --> 00:37:47,930
the counters and all the metrics you run

00:37:46,339 --> 00:37:55,279
through each of them pull out the value

00:37:47,930 --> 00:37:56,329
and publish them this is quite

00:37:55,279 --> 00:37:59,509
heavyweight work

00:37:56,329 --> 00:38:00,969
maybe we're formatting the metrics into

00:37:59,509 --> 00:38:03,049
text

00:38:00,969 --> 00:38:05,170
maybe we're sending them over some

00:38:03,049 --> 00:38:07,519
network socket or writing into a file

00:38:05,170 --> 00:38:10,299
but it doesn't matter because we're only

00:38:07,519 --> 00:38:13,189
doing it every time we poll the data

00:38:10,299 --> 00:38:16,189
we're only doing every five seconds or

00:38:13,189 --> 00:38:17,689
every 10 seconds that increment the

00:38:16,189 --> 00:38:20,979
thing that we actually put in the

00:38:17,689 --> 00:38:25,819
critical path of our code is still

00:38:20,979 --> 00:38:30,699
extremely cheap they're adding these

00:38:25,819 --> 00:38:34,299
counters doesn't incur too much overhead

00:38:30,699 --> 00:38:37,509
I'm sure some of you are thinking they I

00:38:34,299 --> 00:38:41,150
can definitely do better than this I

00:38:37,509 --> 00:38:42,650
know you're thinking it it sounds like a

00:38:41,150 --> 00:38:45,469
really interesting problem to just

00:38:42,650 --> 00:38:48,979
really optimize that increment get as

00:38:45,469 --> 00:38:50,749
fast as possible well yeah other people

00:38:48,979 --> 00:38:52,339
have thought about it too and they

00:38:50,749 --> 00:38:53,779
thought it was really interesting and

00:38:52,339 --> 00:38:55,880
they wrote papers on it and they tried

00:38:53,779 --> 00:38:58,249
to standardize it so if you're really

00:38:55,880 --> 00:39:01,429
interested in how you can write really

00:38:58,249 --> 00:39:06,140
efficient counters there's a good paper

00:39:01,429 --> 00:39:10,729
for you to go and read let's look at a

00:39:06,140 --> 00:39:14,660
different example what about if we

00:39:10,729 --> 00:39:17,569
wanted to count the number of times we

00:39:14,660 --> 00:39:19,759
read a line from a file this is a much

00:39:17,569 --> 00:39:23,650
more frequent occurrence than just

00:39:19,759 --> 00:39:23,650
reading a file in its entirety

00:39:25,490 --> 00:39:30,440
so this loop is critical performance

00:39:27,830 --> 00:39:33,520
wise but it's still fairly heavyweight

00:39:30,440 --> 00:39:38,420
they're pulling out of line from a file

00:39:33,520 --> 00:39:40,869
processing it some way pausing here so

00:39:38,420 --> 00:39:40,869
on and so on

00:39:41,530 --> 00:39:47,480
but it's still quite a good this makes

00:39:44,720 --> 00:39:50,660
it a good candidate for adding a metric

00:39:47,480 --> 00:39:52,339
to because the increment relative to

00:39:50,660 --> 00:40:00,260
what you're doing is still very

00:39:52,339 --> 00:40:02,540
lightweight however it's still a non

00:40:00,260 --> 00:40:06,020
zero overhead and there will be

00:40:02,540 --> 00:40:09,230
situations where the cost of

00:40:06,020 --> 00:40:13,400
incrementing a counter is still an

00:40:09,230 --> 00:40:15,830
overhead to your operation so we have to

00:40:13,400 --> 00:40:21,020
think about is the information we're

00:40:15,830 --> 00:40:26,200
getting valuable enough to woven slowing

00:40:21,020 --> 00:40:31,250
down the code let's look at the data

00:40:26,200 --> 00:40:33,080
that we might get out of this counter so

00:40:31,250 --> 00:40:36,380
we're pulling it every five seconds as

00:40:33,080 --> 00:40:39,440
our application starts up count to zero

00:40:36,380 --> 00:40:41,330
nothing's happening and then something

00:40:39,440 --> 00:40:47,410
starts happening and we start processing

00:40:41,330 --> 00:40:51,230
data the numbers start going up this is

00:40:47,410 --> 00:40:54,200
meaningless you can look at that and

00:40:51,230 --> 00:40:57,440
really infer nothing other than the

00:40:54,200 --> 00:41:02,990
number went up a bit what we really want

00:40:57,440 --> 00:41:05,690
to do is visualize it of course and now

00:41:02,990 --> 00:41:10,390
we get to see some really interesting

00:41:05,690 --> 00:41:13,940
things we can see roughly when

00:41:10,390 --> 00:41:17,720
processing starts when the processing

00:41:13,940 --> 00:41:21,740
finishes these flat areas are where

00:41:17,720 --> 00:41:23,930
nothing's going on and we can see

00:41:21,740 --> 00:41:28,460
roughly the number of lines that we've

00:41:23,930 --> 00:41:31,599
read through each file when this

00:41:28,460 --> 00:41:31,599
leveling off occurs

00:41:33,259 --> 00:41:39,660
what's even more interesting is when we

00:41:35,729 --> 00:41:43,979
host process the output from this metric

00:41:39,660 --> 00:41:46,680
and for example graph the rate that the

00:41:43,979 --> 00:41:49,440
counter is increasing at now it becomes

00:41:46,680 --> 00:41:52,140
even more obvious when our operation is

00:41:49,440 --> 00:41:54,269
starting and stopping we can see it

00:41:52,140 --> 00:41:57,809
roughly takes the first one roughly

00:41:54,269 --> 00:42:03,319
takes 40 seconds and we can see very

00:41:57,809 --> 00:42:07,890
clearly that two operations occurred and

00:42:03,319 --> 00:42:09,900
we can see the throughput so we can

00:42:07,890 --> 00:42:16,229
actually see the performance of our

00:42:09,900 --> 00:42:18,900
processing loop on the graph and even

00:42:16,229 --> 00:42:22,380
better what we can do is we can see if

00:42:18,900 --> 00:42:24,779
the performance changes throughout the

00:42:22,380 --> 00:42:26,609
processing and this is something you

00:42:24,779 --> 00:42:30,150
wouldn't typically see if all you do is

00:42:26,609 --> 00:42:32,789
collect the time your operation took and

00:42:30,150 --> 00:42:34,829
the number of lines you processed you

00:42:32,789 --> 00:42:38,369
would get an average over the whole

00:42:34,829 --> 00:42:43,819
operation what you see with this counter

00:42:38,369 --> 00:42:50,690
is you see if the rate changes that this

00:42:43,819 --> 00:42:55,160
excites me a lot if we tagged our

00:42:50,690 --> 00:42:58,380
metrics in a nice way we can correlate

00:42:55,160 --> 00:43:00,239
what's happening depending on different

00:42:58,380 --> 00:43:04,499
dimensions in our system if we have

00:43:00,239 --> 00:43:07,589
multiple users we can see that one of

00:43:04,499 --> 00:43:10,079
the users in our system when they begin

00:43:07,589 --> 00:43:14,910
requesting affects the performance of

00:43:10,079 --> 00:43:17,249
the other one and we can do other

00:43:14,910 --> 00:43:22,410
processing to these numbers as well we

00:43:17,249 --> 00:43:25,859
can graph the sum of the rates for all

00:43:22,410 --> 00:43:28,079
the users in our system and now we learn

00:43:25,859 --> 00:43:31,559
even more we learned that there's some

00:43:28,079 --> 00:43:34,469
sort of startup slower performance and

00:43:31,559 --> 00:43:36,749
then after some time we see the

00:43:34,469 --> 00:43:39,960
performance increase so maybe this is an

00:43:36,749 --> 00:43:41,519
effect of file caching once you read the

00:43:39,960 --> 00:43:43,630
file once it gets stored in memory and

00:43:41,519 --> 00:43:45,970
so it's faster to process it

00:43:43,630 --> 00:43:48,370
time and you can see that there's some

00:43:45,970 --> 00:43:51,120
sort of limit so even though we have two

00:43:48,370 --> 00:43:53,440
users for running requests in parallel

00:43:51,120 --> 00:43:57,750
there's some sort of sealing to our

00:43:53,440 --> 00:44:03,430
performance and all of this information

00:43:57,750 --> 00:44:06,330
comes from adding just one counter to

00:44:03,430 --> 00:44:13,860
your loop that's doing your interesting

00:44:06,330 --> 00:44:18,190
processing I think this is pretty cool

00:44:13,860 --> 00:44:22,990
and we can go one step further with our

00:44:18,190 --> 00:44:27,270
metrics and are pretty graph we can put

00:44:22,990 --> 00:44:33,070
this side by side with our tracing data

00:44:27,270 --> 00:44:35,710
that we found earlier and now we start

00:44:33,070 --> 00:44:39,580
to fill in even more gaps in our

00:44:35,710 --> 00:44:42,100
knowledge just looking at the metric we

00:44:39,580 --> 00:44:45,010
don't necessarily know whether this is

00:44:42,100 --> 00:44:47,110
two distinct operations or whether it's

00:44:45,010 --> 00:44:51,490
one operation that happened to dip in

00:44:47,110 --> 00:44:54,760
performance very drastically but if we

00:44:51,490 --> 00:44:57,460
look at the tracing data alongside it we

00:44:54,760 --> 00:45:02,110
can verify that in fact it was two

00:44:57,460 --> 00:45:05,680
distinct operations and from the metrics

00:45:02,110 --> 00:45:11,800
we can see the performance within the

00:45:05,680 --> 00:45:15,220
operations I think this is really

00:45:11,800 --> 00:45:18,670
valuable lesson to take away just very

00:45:15,220 --> 00:45:22,240
simple additions to your code can tell

00:45:18,670 --> 00:45:27,000
you so much information but you do have

00:45:22,240 --> 00:45:34,900
to put a bit of thought into it it's not

00:45:27,000 --> 00:45:37,300
effortless so we're at the end nearly

00:45:34,900 --> 00:45:41,400
time for some coffee I'm desperate for

00:45:37,300 --> 00:45:45,970
some coffee what am I trying to say

00:45:41,400 --> 00:45:47,820
develop observable software think back

00:45:45,970 --> 00:45:51,190
to that chip I talked about at the start

00:45:47,820 --> 00:45:55,000
we had to make sure that chip was

00:45:51,190 --> 00:45:56,359
observable good work out what happens

00:45:55,000 --> 00:46:00,589
when it goes wrong

00:45:56,359 --> 00:46:04,009
and I think there is huge advantage in

00:46:00,589 --> 00:46:05,720
doing this with software as well try to

00:46:04,009 --> 00:46:08,509
debug in development as you would in

00:46:05,720 --> 00:46:11,109
production so when something goes wrong

00:46:08,509 --> 00:46:13,970
in production you don't need to install

00:46:11,109 --> 00:46:17,329
your debugger on your production server

00:46:13,970 --> 00:46:21,049
you don't need to install tracing tools

00:46:17,329 --> 00:46:26,119
or some other form of instrumentation

00:46:21,049 --> 00:46:29,809
your software is monitor Ville and

00:46:26,119 --> 00:46:34,849
observable your software becomes its own

00:46:29,809 --> 00:46:39,470
debugger but we have to take into

00:46:34,849 --> 00:46:40,700
account that while there is a lot of

00:46:39,470 --> 00:46:44,239
information to be had there

00:46:40,700 --> 00:46:46,700
there is overheads in doing it but it

00:46:44,239 --> 00:46:50,059
doesn't have to be expensive as long as

00:46:46,700 --> 00:46:54,019
you're mindful of where you use

00:46:50,059 --> 00:46:56,749
different techniques we can log and we

00:46:54,019 --> 00:46:59,359
can log errors because we have to and we

00:46:56,749 --> 00:47:01,789
should we can trace things at a very

00:46:59,359 --> 00:47:05,779
coarse granularity that happened

00:47:01,789 --> 00:47:08,809
infrequently and then in our more hot

00:47:05,779 --> 00:47:11,900
loops we can think about adding some

00:47:08,809 --> 00:47:18,769
metrics instead so that there is less

00:47:11,900 --> 00:47:22,220
overhead incurred there are always

00:47:18,769 --> 00:47:24,799
trade-offs and the techniques here are

00:47:22,220 --> 00:47:27,499
very complementary you used them

00:47:24,799 --> 00:47:29,690
together and this includes other types

00:47:27,499 --> 00:47:32,749
of instrumentation just because you're

00:47:29,690 --> 00:47:35,329
adding some instrumentation to your code

00:47:32,749 --> 00:47:39,410
to your software doesn't mean you can't

00:47:35,329 --> 00:47:42,410
still use your debugger or tools like s

00:47:39,410 --> 00:47:43,940
trace or other instrumentation tools it

00:47:42,410 --> 00:47:47,140
doesn't mean that you can't use your

00:47:43,940 --> 00:47:50,980
compiler to add instrumentation as well

00:47:47,140 --> 00:47:54,489
but with source instrumentation you can

00:47:50,980 --> 00:47:58,220
choose what information to expose that

00:47:54,489 --> 00:48:00,890
is useful to your particular domain so

00:47:58,220 --> 00:48:03,019
if you're writing some sort of video

00:48:00,890 --> 00:48:04,609
processing framework maybe you're

00:48:03,019 --> 00:48:09,170
interested in counting things like the

00:48:04,609 --> 00:48:10,550
number of frames processed if you're

00:48:09,170 --> 00:48:12,830
building a data

00:48:10,550 --> 00:48:15,230
maybe you're interested in the number of

00:48:12,830 --> 00:48:18,460
times a table is accessed or a row in a

00:48:15,230 --> 00:48:20,710
table is accessed this is all possible

00:48:18,460 --> 00:48:22,760
when you actually think about what

00:48:20,710 --> 00:48:26,900
instrumentation and what information you

00:48:22,760 --> 00:48:29,840
want out of your code so with that I'm

00:48:26,900 --> 00:48:40,130
going to thank you for coming and I hope

00:48:29,840 --> 00:48:41,900
you enjoy the rest of your conference so

00:48:40,130 --> 00:48:44,570
we have ten minutes for questions and

00:48:41,900 --> 00:48:49,040
they were microphones if anyone would

00:48:44,570 --> 00:48:52,390
like or I am around until Friday so

00:48:49,040 --> 00:48:57,230
please feel free to come and talk to me

00:48:52,390 --> 00:49:02,300
yes and so I've had a look into tracing

00:48:57,230 --> 00:49:05,330
quite a bit myself and the availability

00:49:02,300 --> 00:49:07,400
of C++ libraries is poor I think because

00:49:05,330 --> 00:49:10,550
I could use a little bit closer by the

00:49:07,400 --> 00:49:14,090
availability of C++ libraries for open

00:49:10,550 --> 00:49:16,790
tracing these poor at best I think and

00:49:14,090 --> 00:49:19,250
I'm not really aware of any metrics

00:49:16,790 --> 00:49:25,130
libraries how what's your experience

00:49:19,250 --> 00:49:29,170
have you got any suggestions so the

00:49:25,130 --> 00:49:31,910
question was are there any specific

00:49:29,170 --> 00:49:35,210
examples of metrics libraries and

00:49:31,910 --> 00:49:38,180
tracing libraries for C++ and I left

00:49:35,210 --> 00:49:40,700
this out of the presentation sort of on

00:49:38,180 --> 00:49:44,140
purpose because as the gentleman says

00:49:40,700 --> 00:49:47,270
the choice is while there is choice

00:49:44,140 --> 00:49:49,790
there's no sort of de facto standard

00:49:47,270 --> 00:49:52,160
there's no and this is even true for

00:49:49,790 --> 00:49:55,099
logging libraries there have thousands

00:49:52,160 --> 00:49:56,930
of logging libraries every framework has

00:49:55,099 --> 00:49:58,820
its own logging library every company

00:49:56,930 --> 00:50:01,030
I've worked in has their own internal

00:49:58,820 --> 00:50:04,040
logging lobby there's no standard for it

00:50:01,030 --> 00:50:06,950
but to directly answer your question the

00:50:04,040 --> 00:50:08,540
library I've had most so the in the

00:50:06,950 --> 00:50:10,450
infrastructure I've used the most is a

00:50:08,540 --> 00:50:14,990
piece of software called Prometheus and

00:50:10,450 --> 00:50:18,290
it has a number of C++ clients which let

00:50:14,990 --> 00:50:21,109
you expose metrics to Prometheus so it's

00:50:18,290 --> 00:50:22,609
very the problem is that these libraries

00:50:21,109 --> 00:50:24,020
aren't standard and they're often

00:50:22,609 --> 00:50:25,910
specific to the IMP

00:50:24,020 --> 00:50:30,430
the tool you're using which is

00:50:25,910 --> 00:50:33,320
unfortunate and for tracing there is a

00:50:30,430 --> 00:50:36,260
evolving open-source project called open

00:50:33,320 --> 00:50:38,480
tracing and that links into a piece of

00:50:36,260 --> 00:50:40,869
software called Jaeger which is a

00:50:38,480 --> 00:50:43,640
distributed tracing system and that has

00:50:40,869 --> 00:50:45,830
C++ clients so those are two things you

00:50:43,640 --> 00:50:48,260
could look at but this is an interesting

00:50:45,830 --> 00:50:52,700
point I think we could do a lot better

00:50:48,260 --> 00:50:55,280
to try and evolve some libraries maybe

00:50:52,700 --> 00:50:58,070
not necessary in a standard context but

00:50:55,280 --> 00:51:00,260
at least as a community where there are

00:50:58,070 --> 00:51:02,660
tools which are become the de facto

00:51:00,260 --> 00:51:03,860
standard so if we use a library from

00:51:02,660 --> 00:51:07,790
here and a light be from here we write

00:51:03,860 --> 00:51:15,680
some code we can use a common library to

00:51:07,790 --> 00:51:17,930
introduce metrics and traces yes sir

00:51:15,680 --> 00:51:20,540
yeah I guess all three of us had the

00:51:17,930 --> 00:51:24,470
same question can you say a little more

00:51:20,540 --> 00:51:27,380
about the CGA girl library does it work

00:51:24,470 --> 00:51:29,600
with just other processes across C++ or

00:51:27,380 --> 00:51:34,160
does it work across language as well as

00:51:29,600 --> 00:51:36,619
cross process so my company we use a

00:51:34,160 --> 00:51:39,320
scripting language in addition to

00:51:36,619 --> 00:51:41,630
JavaScript in addition to C++ and having

00:51:39,320 --> 00:51:46,430
visibility across all three would be

00:51:41,630 --> 00:51:50,630
great so the question was specific to

00:51:46,430 --> 00:51:53,540
Jaeger and I guess perhaps generally

00:51:50,630 --> 00:51:56,300
with tracing clients is there any cross

00:51:53,540 --> 00:51:58,900
language clients we could use so you can

00:51:56,300 --> 00:52:02,510
collect tracing information from

00:51:58,900 --> 00:52:04,609
different parts of your stack so the

00:52:02,510 --> 00:52:07,940
answer to that is yes Jaeger

00:52:04,609 --> 00:52:08,570
specifically supports pretty much every

00:52:07,940 --> 00:52:11,119
language

00:52:08,570 --> 00:52:15,350
so this Jaeger is an implementation of

00:52:11,119 --> 00:52:19,190
the open tracing standard and the open

00:52:15,350 --> 00:52:20,720
tracing standard is open tracing has

00:52:19,190 --> 00:52:23,510
lots of clients for all different

00:52:20,720 --> 00:52:27,800
languages so you could put some traces

00:52:23,510 --> 00:52:29,869
in your C++ clue C++ code that actually

00:52:27,800 --> 00:52:33,740
call operations in a different language

00:52:29,869 --> 00:52:36,140
and still link those traces together so

00:52:33,740 --> 00:52:37,670
I think you mentioned JavaScript I'm not

00:52:36,140 --> 00:52:39,650
100% sure but I'm

00:52:37,670 --> 00:52:41,450
would be surprised if there wasn't the

00:52:39,650 --> 00:52:48,470
JavaScript client there's definitely

00:52:41,450 --> 00:52:56,150
things like Python and Ruby and anything

00:52:48,470 --> 00:52:58,700
like that okay all right I guess just as

00:52:56,150 --> 00:53:02,119
a comment we effectively had to hand

00:52:58,700 --> 00:53:05,420
roll our own instrumentation library and

00:53:02,119 --> 00:53:08,539
for those who might use Intel's TVB they

00:53:05,420 --> 00:53:11,150
do have an innumerable thread storage

00:53:08,539 --> 00:53:12,829
counter that kind of does the atomic

00:53:11,150 --> 00:53:16,460
thing you were talking about yeah yeah

00:53:12,829 --> 00:53:22,549
so just as an FYI yeah all right so

00:53:16,460 --> 00:53:25,039
thank you any other any other questions

00:53:22,549 --> 00:53:27,039
yes sir you just spelled prometheus how

00:53:25,039 --> 00:53:30,470
did spelled so that I can look it up

00:53:27,039 --> 00:53:35,029
so the Prometheus / meteors what did it

00:53:30,470 --> 00:53:38,920
what is it the I just spoke to me pyaare

00:53:35,029 --> 00:53:42,200
om e theist

00:53:38,920 --> 00:53:44,589
it's the same as the film the alien film

00:53:42,200 --> 00:53:47,210
Prometheus yeah it's a it's a great

00:53:44,589 --> 00:53:47,720
metrics collection software I quite like

00:53:47,210 --> 00:53:50,750
it myself

00:53:47,720 --> 00:53:54,160
yes sir hey I think I heard you say that

00:53:50,750 --> 00:53:58,720
you should be careful about not putting

00:53:54,160 --> 00:54:01,130
instrumentation in everywhere yeah and

00:53:58,720 --> 00:54:03,430
well it's easy to agree with that I

00:54:01,130 --> 00:54:06,259
think the recommendation should be

00:54:03,430 --> 00:54:07,880
instrumentation is a key functionality

00:54:06,259 --> 00:54:09,079
of your software you cannot operate the

00:54:07,880 --> 00:54:11,059
software without instruments

00:54:09,079 --> 00:54:13,150
instrumentation and you should put it

00:54:11,059 --> 00:54:16,579
everywhere where it's needed just like

00:54:13,150 --> 00:54:18,140
you know basic functionality of the

00:54:16,579 --> 00:54:19,700
thing right if your if your thing is

00:54:18,140 --> 00:54:21,890
supposed to compute a square root you

00:54:19,700 --> 00:54:24,049
compute square root right if you're

00:54:21,890 --> 00:54:27,579
supposed to run the software you

00:54:24,049 --> 00:54:30,529
instrument it so the so the comment was

00:54:27,579 --> 00:54:33,740
maybe the advice shouldn't be should be

00:54:30,529 --> 00:54:35,319
you instrument as much as you can and

00:54:33,740 --> 00:54:42,319
especially things that are important

00:54:35,319 --> 00:54:45,410
right so I in a former life I was an SRE

00:54:42,319 --> 00:54:48,410
and and they the advice I would give any

00:54:45,410 --> 00:54:50,150
developer is you define the SLS and SL

00:54:48,410 --> 00:54:51,230
A's for the software that you're

00:54:50,150 --> 00:54:52,520
building and then

00:54:51,230 --> 00:54:55,160
you build the instrumentation that you

00:54:52,520 --> 00:54:59,450
need to monitor for those s ellos and s

00:54:55,160 --> 00:55:01,940
LA's period right and you don't instead

00:54:59,450 --> 00:55:03,530
you want to instrument a little bit more

00:55:01,940 --> 00:55:05,240
than that so you can troubleshoot and

00:55:03,530 --> 00:55:08,480
find out why you're in violation of your

00:55:05,240 --> 00:55:10,280
s ellos but you start from what are the

00:55:08,480 --> 00:55:14,060
properties of the software that you want

00:55:10,280 --> 00:55:16,550
to maintain or the system and then you

00:55:14,060 --> 00:55:18,590
put all the instrumentation need to be

00:55:16,550 --> 00:55:22,160
able to maintain that those properties

00:55:18,590 --> 00:55:27,410
right you its if that means you have to

00:55:22,160 --> 00:55:30,080
buy an extra CPU an AWS or Google

00:55:27,410 --> 00:55:33,380
compute engine in my case then go ahead

00:55:30,080 --> 00:55:36,920
and do it that's so this is a extremely

00:55:33,380 --> 00:55:40,160
good point in that sometimes the

00:55:36,920 --> 00:55:44,060
instrumentation is critical or even a

00:55:40,160 --> 00:55:46,970
requirement of your software and so any

00:55:44,060 --> 00:55:48,980
overhead has to be acceptable and you

00:55:46,970 --> 00:55:51,050
just have to snail overhead right we're

00:55:48,980 --> 00:55:53,840
ahead is stuff that you don't need in

00:55:51,050 --> 00:55:56,119
this case instrumentation is something

00:55:53,840 --> 00:55:59,660
you need anymore it's part of the

00:55:56,119 --> 00:56:01,990
functionality of the system so this this

00:55:59,660 --> 00:56:05,540
talk is actually about half the size

00:56:01,990 --> 00:56:09,950
that it started out and I had a section

00:56:05,540 --> 00:56:11,810
about exactly this the trade offs went

00:56:09,950 --> 00:56:13,490
into much more detail about the

00:56:11,810 --> 00:56:16,750
trade-offs between different types of

00:56:13,490 --> 00:56:20,600
instrumentation how much overhead is

00:56:16,750 --> 00:56:25,220
good enough for how little overhead and

00:56:20,600 --> 00:56:27,680
when you might use different things when

00:56:25,220 --> 00:56:29,510
you need to use instrumentation because

00:56:27,680 --> 00:56:34,790
there's a requirement from a customer or

00:56:29,510 --> 00:56:37,310
you have for example an SLA then how can

00:56:34,790 --> 00:56:39,740
you make a trace as efficient as

00:56:37,310 --> 00:56:43,400
possible and unfortunately that turned

00:56:39,740 --> 00:56:46,640
out to be far too much to cover in one

00:56:43,400 --> 00:56:49,359
game thank you the question yes sir I

00:56:46,640 --> 00:56:52,430
was going to basically agree and I think

00:56:49,359 --> 00:56:56,240
but with logging it's interesting I

00:56:52,430 --> 00:56:58,910
think is that often at least my

00:56:56,240 --> 00:57:03,109
experiences that you put logging where

00:56:58,910 --> 00:57:05,390
you need it to solve a problem

00:57:03,109 --> 00:57:07,460
you don't put it for the future problems

00:57:05,390 --> 00:57:09,739
you're going to have so it's usually too

00:57:07,460 --> 00:57:11,660
late and I wonder whether tracing I

00:57:09,739 --> 00:57:15,170
suppose your comment is you put it

00:57:11,660 --> 00:57:16,910
everywhere or for the SLA or however

00:57:15,170 --> 00:57:20,299
they say it's a very difficult problem

00:57:16,910 --> 00:57:20,779
knowing exactly before beforehand where

00:57:20,299 --> 00:57:23,869
to put it

00:57:20,779 --> 00:57:26,150
I think this it's quite a it's an

00:57:23,869 --> 00:57:27,619
interesting topic for people to do so

00:57:26,150 --> 00:57:29,839
you could do it at kind of a business

00:57:27,619 --> 00:57:33,730
level so you say right okay well we'll

00:57:29,839 --> 00:57:36,829
put logging tracing and metrics on

00:57:33,730 --> 00:57:38,869
business terms if you like and not do it

00:57:36,829 --> 00:57:40,789
any lower and this also depends on what

00:57:38,869 --> 00:57:44,960
you're using them for are you using them

00:57:40,789 --> 00:57:47,119
to monitor or for debugging tool logging

00:57:44,960 --> 00:57:49,640
is often more a debugging tool whereas

00:57:47,119 --> 00:57:53,359
tracing is more monitoring and metrics I

00:57:49,640 --> 00:57:56,690
think so the summary of that comment I

00:57:53,359 --> 00:57:58,910
think is that logging is often more

00:57:56,690 --> 00:58:03,170
geared towards things do you absolutely

00:57:58,910 --> 00:58:07,430
need to record like errors and tracing

00:58:03,170 --> 00:58:09,349
and specifically gentleman said errors

00:58:07,430 --> 00:58:10,369
are blogs or for things that you know

00:58:09,349 --> 00:58:13,279
you need to record

00:58:10,369 --> 00:58:14,989
whereas traces are typically more for

00:58:13,279 --> 00:58:17,210
things you think you might need to

00:58:14,989 --> 00:58:22,269
record for future issues that you don't

00:58:17,210 --> 00:58:25,839
know about and yeah I completely agree I

00:58:22,269 --> 00:58:28,279
think the more you start to look at

00:58:25,839 --> 00:58:31,999
tracing is something distinct from

00:58:28,279 --> 00:58:34,789
logging you realize that logging is

00:58:31,999 --> 00:58:39,730
really just for errors it's about adding

00:58:34,789 --> 00:58:41,599
context to errors maybe warnings as well

00:58:39,730 --> 00:58:44,420
but that's really all you should be

00:58:41,599 --> 00:58:46,009
using logging for and if you have some

00:58:44,420 --> 00:58:47,569
customer requirement that you produce a

00:58:46,009 --> 00:58:50,869
log file in the format that of course

00:58:47,569 --> 00:58:52,999
you need to do some logging but I think

00:58:50,869 --> 00:58:57,140
if you generally all this sort of info

00:58:52,999 --> 00:58:58,999
logging and the debug logging we should

00:58:57,140 --> 00:59:01,700
be thinking about in a more structured

00:58:58,999 --> 00:59:06,049
way perhaps looking at using some

00:59:01,700 --> 00:59:08,059
tracing is dead and one of the ideas of

00:59:06,049 --> 00:59:10,160
this talk was to try and start at its

00:59:08,059 --> 00:59:14,029
discussion because I think in the C++

00:59:10,160 --> 00:59:16,950
community we overlook a lot of the

00:59:14,029 --> 00:59:20,910
problems of actually running C++ in

00:59:16,950 --> 00:59:23,810
reduction it's quite hard work and we

00:59:20,910 --> 00:59:28,820
worry too much about angle brackets and

00:59:23,810 --> 00:59:33,270
syntax and the newest feature of lambdas

00:59:28,820 --> 00:59:35,380
so with that my time is up so thank you

00:59:33,270 --> 00:59:40,809
for coming see you around

00:59:35,380 --> 00:59:40,809

YouTube URL: https://www.youtube.com/watch?v=0WgC5jnrRx8


