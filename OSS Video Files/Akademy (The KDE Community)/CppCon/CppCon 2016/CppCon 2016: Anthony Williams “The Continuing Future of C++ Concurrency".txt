Title: CppCon 2016: Anthony Williams “The Continuing Future of C++ Concurrency"
Publication date: 2016-10-02
Playlist: CppCon 2016
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2016
—
An overview of the additions to the standard C++ concurrency libraries in the Technical Specifications for Concurrency and Parallelism and the C++14 and C++17 standards. These additions include: continuations, latches, barriers, atomic smart pointers, shared ownership mutexes, executors, concurrent queues, distributed counters, coroutines, parallel algorithms and more.
— 
Anthony Williams
Just Software Solutions Ltd
Anthony Williams is the author of C++ Concurrency in Action.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,020 --> 00:00:05,400
laughing folks I hope you've been

00:00:02,520 --> 00:00:07,170
joining conference so far I got to talk

00:00:05,400 --> 00:00:12,139
to you about the continuing future of

00:00:07,170 --> 00:00:15,839
C++ concurrency I'm gonna start with

00:00:12,139 --> 00:00:17,910
what's new in C++ 14 since the C++ 11

00:00:15,839 --> 00:00:19,289
standard obviously we add we add the

00:00:17,910 --> 00:00:20,939
first standard that I can knowledge the

00:00:19,289 --> 00:00:24,240
existence of concurrency receive bus

00:00:20,939 --> 00:00:26,930
plus 11 so C++ 14 the first time we

00:00:24,240 --> 00:00:30,150
actually added new stuff to that and

00:00:26,930 --> 00:00:33,960
we've got new stuff coming in C++ 17 as

00:00:30,150 --> 00:00:36,860
well and then of course a bunch of

00:00:33,960 --> 00:00:40,050
technical specifications on concurrency

00:00:36,860 --> 00:00:41,850
parallelism and transactional memory so

00:00:40,050 --> 00:00:43,640
I'm going to give a brief description of

00:00:41,850 --> 00:00:46,710
some of the contents of those as well so

00:00:43,640 --> 00:00:49,890
yeah there is a continuing future for

00:00:46,710 --> 00:00:55,739
concurrency in C++ and so that's what

00:00:49,890 --> 00:00:57,930
we're gonna talk about in C++ 14 we

00:00:55,739 --> 00:01:00,629
actually only added one new concurrency

00:00:57,930 --> 00:01:04,320
feature and that was a shared timed

00:01:00,629 --> 00:01:08,430
mutex and then the corresponding Rai

00:01:04,320 --> 00:01:11,010
lock type shared lock so those of you

00:01:08,430 --> 00:01:13,470
coming from a POSIX background might

00:01:11,010 --> 00:01:17,520
know about the pthread read wider

00:01:13,470 --> 00:01:21,180
mutexes or on Windows the slim reader

00:01:17,520 --> 00:01:26,880
rider locks and that's essentially what

00:01:21,180 --> 00:01:30,119
this this wraps you multiple threads can

00:01:26,880 --> 00:01:35,040
hold a shared lock alternatively one

00:01:30,119 --> 00:01:38,640
thread may hold an exclusive lock so if

00:01:35,040 --> 00:01:41,119
you've got a it's typically an

00:01:38,640 --> 00:01:45,000
optimization for a data structure that's

00:01:41,119 --> 00:01:49,110
read most often and then updated rarely

00:01:45,000 --> 00:01:51,240
so you can now have multiple threads

00:01:49,110 --> 00:01:53,460
that are reading concurrently and it's

00:01:51,240 --> 00:01:55,290
all safe but then when you want to

00:01:53,460 --> 00:01:57,960
update everything then you say hang on a

00:01:55,290 --> 00:01:59,340
minute all the reading none of you can

00:01:57,960 --> 00:02:02,659
read whilst they just change this and

00:01:59,340 --> 00:02:02,659
then you go again

00:02:05,060 --> 00:02:10,880
it's got timed in the title because some

00:02:09,380 --> 00:02:13,160
of the functions have timeout so you can

00:02:10,880 --> 00:02:16,940
say try and acquire a lock for a certain

00:02:13,160 --> 00:02:18,680
period of time no this is in the in C++

00:02:16,940 --> 00:02:20,780
7 we've got standard time to meet X and

00:02:18,680 --> 00:02:23,900
standard recursive time to mute X so

00:02:20,780 --> 00:02:28,040
this is not a new thing the concept of

00:02:23,900 --> 00:02:30,260
it being a time mutex so yeah those and

00:02:28,040 --> 00:02:34,930
those functions with the timeouts are

00:02:30,260 --> 00:02:37,670
therefore both the exclusive lock

00:02:34,930 --> 00:02:39,170
fraction functions and then for the

00:02:37,670 --> 00:02:40,670
shared lock functions so you can try to

00:02:39,170 --> 00:02:48,370
say try and acquire a shared lock for a

00:02:40,670 --> 00:02:51,980
certain period of time you use it

00:02:48,370 --> 00:02:54,290
okay libraries as an example so we've

00:02:51,980 --> 00:02:56,390
got some some some data structure that

00:02:54,290 --> 00:02:58,489
we that we're trying to deal with you

00:02:56,390 --> 00:03:00,640
know we've got a you know in this case a

00:02:58,489 --> 00:03:03,350
table which a map of strings to strings

00:03:00,640 --> 00:03:06,680
we've got a we have our shared time

00:03:03,350 --> 00:03:11,870
mutex at the top and with implementing

00:03:06,680 --> 00:03:14,390
some function to find an entry so with

00:03:11,870 --> 00:03:15,739
this is a read operation with finding so

00:03:14,390 --> 00:03:18,350
we're gonna take your shared lock and

00:03:15,739 --> 00:03:20,870
we're going to use that nice new Rai

00:03:18,350 --> 00:03:24,400
type shared lock on our shared time

00:03:20,870 --> 00:03:27,140
mutex construct a card we're going to

00:03:24,400 --> 00:03:29,030
find the entry in the table we can just

00:03:27,140 --> 00:03:30,830
throw if we didn't find what we're after

00:03:29,030 --> 00:03:32,030
and if we did find it then we're gonna

00:03:30,830 --> 00:03:34,880
return the sink maybe I mean that's just

00:03:32,030 --> 00:03:38,269
the standard map poker but the whole

00:03:34,880 --> 00:03:39,950
point no the the the bit that we the

00:03:38,269 --> 00:03:45,290
specific to this is we've got the shared

00:03:39,950 --> 00:03:47,420
lock on the shared time mutex on the

00:03:45,290 --> 00:03:51,410
flip side then we know if you're adding

00:03:47,420 --> 00:03:54,049
an entry to this table then again we're

00:03:51,410 --> 00:03:55,850
going to want to know protect it this

00:03:54,049 --> 00:03:57,620
time we want the exclusive lock so we've

00:03:55,850 --> 00:03:58,610
got our lock guard know the same lock

00:03:57,620 --> 00:04:00,590
guard that we would use it with a

00:03:58,610 --> 00:04:02,540
standard mutex and now it works with a

00:04:00,590 --> 00:04:08,870
shared time mutex because it implements

00:04:02,540 --> 00:04:10,310
is the lockable content so it just works

00:04:08,870 --> 00:04:12,010
he's just sticking in there and this

00:04:10,310 --> 00:04:14,660
this then acquires the exclusive lock

00:04:12,010 --> 00:04:16,250
once the exclusive Rockies acquired none

00:04:14,660 --> 00:04:18,380
of those none of the readers will know

00:04:16,250 --> 00:04:19,970
it will wait until any pet

00:04:18,380 --> 00:04:25,700
readers have released their shared locks

00:04:19,970 --> 00:04:27,470
before you proceed and then no news no

00:04:25,700 --> 00:04:31,360
new readers can acquire shared locks

00:04:27,470 --> 00:04:34,370
whilst this the exclusive lock is held

00:04:31,360 --> 00:04:35,630
so our adding of an entry to the table

00:04:34,370 --> 00:04:38,030
is perfectly safe we don't have to worry

00:04:35,630 --> 00:04:45,470
about know the concurrent readers on on

00:04:38,030 --> 00:04:51,710
my table like I said there's a timed

00:04:45,470 --> 00:04:53,240
part so the shared lock Aria has allows

00:04:51,710 --> 00:04:56,330
you to put that spot of the constructor

00:04:53,240 --> 00:04:58,400
just like unique lock does so you can

00:04:56,330 --> 00:05:00,020
say I'm going to try and acquire my

00:04:58,400 --> 00:05:02,390
shared lock in this case for one second

00:05:00,020 --> 00:05:03,740
if I'd if you don't get it in one second

00:05:02,390 --> 00:05:05,660
then the share dog just doesn't own the

00:05:03,740 --> 00:05:07,790
lock and so then you if you use it if

00:05:05,660 --> 00:05:09,440
you use this tri tri lock and

00:05:07,790 --> 00:05:11,750
constructor you then get after query it

00:05:09,440 --> 00:05:15,470
did I get the lock does my share lock

00:05:11,750 --> 00:05:18,320
own no L got owns lock and if I didn't

00:05:15,470 --> 00:05:21,470
then I can't do whatever it was I was

00:05:18,320 --> 00:05:25,130
trying to do and so you have to give up

00:05:21,470 --> 00:05:26,720
and and do something else and again and

00:05:25,130 --> 00:05:27,950
typically you want to use this where

00:05:26,720 --> 00:05:30,230
you've got some sort of well I have to

00:05:27,950 --> 00:05:31,880
I've got a certain amount of time in

00:05:30,230 --> 00:05:33,650
which I can do this bit processing

00:05:31,880 --> 00:05:35,000
before I need to do something else with

00:05:33,650 --> 00:05:41,120
its update the screen respond to an

00:05:35,000 --> 00:05:42,710
event or no do something but the if I

00:05:41,120 --> 00:05:43,820
can't get my lock within that period of

00:05:42,710 --> 00:05:45,760
time um typically it's not going to be

00:05:43,820 --> 00:05:48,380
seconds it's gonna be nine milliseconds

00:05:45,760 --> 00:05:49,880
but anyway so if you can't get them up

00:05:48,380 --> 00:05:50,780
within a certain period of time then I'm

00:05:49,880 --> 00:05:52,430
going to give up and I'm gonna do

00:05:50,780 --> 00:05:55,790
something else and typically that's

00:05:52,430 --> 00:05:57,590
where I need to do this eventually but

00:05:55,790 --> 00:05:59,300
if I if I can't do it right now then

00:05:57,590 --> 00:06:01,100
that's not that's okay that's not a big

00:05:59,300 --> 00:06:03,080
problem I'll do it next time because I'm

00:06:01,100 --> 00:06:04,370
no I'm going to call it unction in the

00:06:03,080 --> 00:06:05,450
loop and every time we're going to try

00:06:04,370 --> 00:06:07,400
and acquire the lock and if I succeed

00:06:05,450 --> 00:06:10,940
then I can do it and if I don't know

00:06:07,400 --> 00:06:13,100
nevermind and I mean that's just the

00:06:10,940 --> 00:06:14,330
general principle whenever that's why we

00:06:13,100 --> 00:06:16,400
have these time functions in the

00:06:14,330 --> 00:06:18,950
standard for this standard mutex anyway

00:06:16,400 --> 00:06:21,920
no for obviously for time to me text

00:06:18,950 --> 00:06:25,190
rather than just plain centimeters so

00:06:21,920 --> 00:06:27,830
that's it's the same applies with shared

00:06:25,190 --> 00:06:30,340
time to text so it's not not a new

00:06:27,830 --> 00:06:30,340
concept there

00:06:33,609 --> 00:06:52,189
there is a question already yes so okay

00:06:49,729 --> 00:06:53,899
so the question is are we going to use

00:06:52,189 --> 00:06:55,879
shared lock for reading purposes and

00:06:53,899 --> 00:06:58,039
lock guard for writing purposes and yes

00:06:55,879 --> 00:06:59,419
that's in general true because for for

00:06:58,039 --> 00:07:01,549
writing purposes you want an exclusive

00:06:59,419 --> 00:07:03,769
lock and so we'll lock garden and unique

00:07:01,549 --> 00:07:06,709
locker the other types that give us that

00:07:03,769 --> 00:07:09,319
and the reading purposes you want the

00:07:06,709 --> 00:07:12,019
shared lock and so the standard shared

00:07:09,319 --> 00:07:16,969
lock guard is what gives us they gives

00:07:12,019 --> 00:07:19,179
us that with the better name of being

00:07:16,969 --> 00:07:21,319
read lock and write lock

00:07:19,179 --> 00:07:28,159
who knows that's water under the bridge

00:07:21,319 --> 00:07:29,839
this is what we've got so on this side

00:07:28,159 --> 00:07:32,509
say profile promo profile

00:07:29,839 --> 00:07:35,149
it's all about timings now fundamentally

00:07:32,509 --> 00:07:37,549
the reason why you would want to use a

00:07:35,149 --> 00:07:39,289
shared mutex is because you're trying to

00:07:37,549 --> 00:07:41,479
optimize for the case where you've got

00:07:39,289 --> 00:07:43,939
lots of readers and an occasional update

00:07:41,479 --> 00:07:48,429
and it's you're trying to do that this

00:07:43,939 --> 00:07:50,899
isn't trying to be an optimization but

00:07:48,429 --> 00:07:52,999
in order to keep track of all your

00:07:50,899 --> 00:07:54,559
readers every time you require a read

00:07:52,999 --> 00:07:56,659
lock you've actually got to update the

00:07:54,559 --> 00:07:58,789
mutex itself in some fashion to say I've

00:07:56,659 --> 00:08:00,109
got another reader and then when I'm

00:07:58,789 --> 00:08:03,109
done you've got to say that read has

00:08:00,109 --> 00:08:07,299
done now so any exclusive lot of people

00:08:03,109 --> 00:08:10,129
who are waiting can now take the lock so

00:08:07,299 --> 00:08:13,369
there's actually still the contention on

00:08:10,129 --> 00:08:15,499
the lock itself so you need to profile

00:08:13,369 --> 00:08:17,119
to make sure that you are getting the

00:08:15,499 --> 00:08:20,299
optimization and the performance benefit

00:08:17,119 --> 00:08:22,759
you hoped you would know like any

00:08:20,299 --> 00:08:24,469
optimization profile beforehand profile

00:08:22,759 --> 00:08:25,699
afterwards you might find in your case

00:08:24,469 --> 00:08:30,439
that's actually better just to use a

00:08:25,699 --> 00:08:32,779
plain standard mutex and this will

00:08:30,439 --> 00:08:34,639
probably vary between platforms so and

00:08:32,779 --> 00:08:36,379
possibly even with standard library

00:08:34,639 --> 00:08:38,269
implementations on the same platform so

00:08:36,379 --> 00:08:40,129
you need to if you're porting across

00:08:38,269 --> 00:08:43,179
multiple platforms you need to profile

00:08:40,129 --> 00:08:43,179
on all the platforms as well

00:08:45,360 --> 00:08:51,640
so C++ 14

00:08:47,920 --> 00:08:57,690
that was all there was to it concurrency

00:08:51,640 --> 00:09:01,060
in C++ 17 we've got a little bit more so

00:08:57,690 --> 00:09:04,480
we've got shared mutexes non-shared time

00:09:01,060 --> 00:09:08,560
mutex is non timed Brummer standard shed

00:09:04,480 --> 00:09:12,190
mutex we've got a new very Adak version

00:09:08,560 --> 00:09:14,980
of standard lock guard and then a nifty

00:09:12,190 --> 00:09:17,080
c++ 17 feature which it's not a

00:09:14,980 --> 00:09:19,570
concurrency feature but it works nicely

00:09:17,080 --> 00:09:21,910
and that is that you can deduce your

00:09:19,570 --> 00:09:24,190
template arguments for class templates

00:09:21,910 --> 00:09:27,010
from their initializer so standard lock

00:09:24,190 --> 00:09:30,040
guard doesn't require you to specify the

00:09:27,010 --> 00:09:31,840
mutex type anymore and know that a

00:09:30,040 --> 00:09:36,840
standard unique lock or standard shared

00:09:31,840 --> 00:09:40,390
lock parameter on top of that we've got

00:09:36,840 --> 00:09:43,270
parallelism the parallelism TS version 1

00:09:40,390 --> 00:09:46,120
has been merged into the c++ 17 working

00:09:43,270 --> 00:09:55,300
draft so there are now parallel versions

00:09:46,120 --> 00:09:57,130
of most of the STL algorithms and also

00:09:55,300 --> 00:09:59,620
say for anyone who's been following the

00:09:57,130 --> 00:10:02,650
standardization process and was hoping

00:09:59,620 --> 00:10:05,260
for joining thread you're not gonna get

00:10:02,650 --> 00:10:06,460
it unless somebody resubmit to the

00:10:05,260 --> 00:10:06,790
proposal and the committee changed their

00:10:06,460 --> 00:10:09,820
mind

00:10:06,790 --> 00:10:12,730
no it it got voted down at the last

00:10:09,820 --> 00:10:13,780
meeting so sorry we're not gonna doesn't

00:10:12,730 --> 00:10:20,860
look like we're gonna have standard

00:10:13,780 --> 00:10:22,570
joining thread after all so we got

00:10:20,860 --> 00:10:25,200
shared time mutex why do we need shared

00:10:22,570 --> 00:10:29,260
mutex and the answer is of course

00:10:25,200 --> 00:10:33,100
performance on some platforms notably

00:10:29,260 --> 00:10:35,910
windows I think then if you don't have

00:10:33,100 --> 00:10:38,500
to support they'll try lock with timeout

00:10:35,910 --> 00:10:41,380
then you can actually make the

00:10:38,500 --> 00:10:44,560
implementation cheaper overall and so

00:10:41,380 --> 00:10:48,070
you can get better performance if you're

00:10:44,560 --> 00:10:49,720
never doing timed operations and given

00:10:48,070 --> 00:10:52,769
that that's probably most of the uses

00:10:49,720 --> 00:10:55,499
never use the timed operations then

00:10:52,769 --> 00:10:57,029
that's a potential and and yeah and

00:10:55,499 --> 00:11:00,360
you're using this for performance in the

00:10:57,029 --> 00:11:01,949
first place so having your know making

00:11:00,360 --> 00:11:03,119
sure that it is as fast as it can be for

00:11:01,949 --> 00:11:06,899
your use case is probably quite

00:11:03,119 --> 00:11:08,670
important and the reason that we didn't

00:11:06,899 --> 00:11:10,829
get it in the first place is just a

00:11:08,670 --> 00:11:20,369
matter of known history and how the

00:11:10,829 --> 00:11:23,879
committee works so as far as baryonic

00:11:20,369 --> 00:11:26,129
what guard goes this is great if you if

00:11:23,879 --> 00:11:29,249
you're trying to do an operation that is

00:11:26,129 --> 00:11:34,199
going to be needed to acquire multiple

00:11:29,249 --> 00:11:37,619
mutexes then if you specify specific ly

00:11:34,199 --> 00:11:40,379
lock them one at a time then your own

00:11:37,619 --> 00:11:42,540
that's a classic deadlock no deadlock

00:11:40,379 --> 00:11:45,179
case is no you you try and lock commute

00:11:42,540 --> 00:11:47,160
XA and then you lock mutex B and in one

00:11:45,179 --> 00:11:51,029
thread and somehow another thread locks

00:11:47,160 --> 00:11:53,239
B and then a and all know everything

00:11:51,029 --> 00:11:58,290
goes wrong your program grinds to a halt

00:11:53,239 --> 00:12:00,899
so in C++ 11 we had this standard lock

00:11:58,290 --> 00:12:04,319
function so you could give it a couple

00:12:00,899 --> 00:12:06,600
of function no to mutex III mutexes biog

00:12:04,319 --> 00:12:08,189
mutexes and it would work out a way of

00:12:06,600 --> 00:12:11,879
locking them all in a deadlock free

00:12:08,189 --> 00:12:14,519
fashion but in order to use that with

00:12:11,879 --> 00:12:16,709
your RA how I guards like standard lock

00:12:14,519 --> 00:12:19,889
garden standard unique lock then you

00:12:16,709 --> 00:12:22,350
then had to either do the lock first and

00:12:19,889 --> 00:12:25,410
then adopt everything afterwards no you

00:12:22,350 --> 00:12:28,649
split over multiple lines and it's it's

00:12:25,410 --> 00:12:30,569
easy to get wrong and making mistake so

00:12:28,649 --> 00:12:32,670
with the new very avec la guard you can

00:12:30,569 --> 00:12:35,069
do that all in one line standard lock

00:12:32,670 --> 00:12:40,919
guard you take the type 1 mutex type 2

00:12:35,069 --> 00:12:45,209
and then your parameters and this uses

00:12:40,919 --> 00:12:46,709
the same underlying acquire all the

00:12:45,209 --> 00:12:50,790
locks without deadlock mechanism the

00:12:46,709 --> 00:12:53,639
standard lock does but know is it's all

00:12:50,790 --> 00:12:55,709
already owned work by the by the lock

00:12:53,639 --> 00:12:57,179
guard so you don't then need to worry

00:12:55,709 --> 00:12:59,309
about making sure that they things

00:12:57,179 --> 00:13:01,579
release when you get to the end of the

00:12:59,309 --> 00:13:01,579
function

00:13:03,540 --> 00:13:11,350
and like I said we also C++ 17 also

00:13:08,020 --> 00:13:13,240
gives us this implicit deduction so you

00:13:11,350 --> 00:13:17,860
can specify still standard lock guard

00:13:13,240 --> 00:13:20,200
off standard mutex no and it just works

00:13:17,860 --> 00:13:22,720
but that's one but you can also emit

00:13:20,200 --> 00:13:23,860
that now the commands template type

00:13:22,720 --> 00:13:25,690
deduction means you can just say

00:13:23,860 --> 00:13:30,880
standard lock guard no template

00:13:25,690 --> 00:13:36,910
parameters and that that is great if

00:13:30,880 --> 00:13:39,130
you're no it simplifies typing it means

00:13:36,910 --> 00:13:42,370
if you change the type of the mutex for

00:13:39,130 --> 00:13:43,900
from no standard mutex to standard time

00:13:42,370 --> 00:13:45,310
mutex because somewhere in your code you

00:13:43,900 --> 00:13:46,960
realize that you need a timeout

00:13:45,310 --> 00:13:48,460
then that's now just going to work and

00:13:46,960 --> 00:14:05,740
all the uses of lock guys you don't need

00:13:48,460 --> 00:14:07,690
to go through and update them or see and

00:14:05,740 --> 00:14:09,430
that obviously applies I mean either

00:14:07,690 --> 00:14:13,540
it's a it's a general feature from C++

00:14:09,430 --> 00:14:14,860
17 so it applies to all class templates

00:14:13,540 --> 00:14:16,210
that can be deducing the initializer so

00:14:14,860 --> 00:14:24,450
it applies to standard unique lock and

00:14:16,210 --> 00:14:30,130
standard shared lock as well okay so

00:14:24,450 --> 00:14:34,090
that C plus plus 17 either way so let's

00:14:30,130 --> 00:14:35,950
move on to some of these TSS concurrency

00:14:34,090 --> 00:14:37,510
TS version one now this has actually

00:14:35,950 --> 00:14:41,640
been released as version one it is a

00:14:37,510 --> 00:14:45,690
published iso/ts I give us a few things

00:14:41,640 --> 00:14:49,650
gives us continuations on our futures so

00:14:45,690 --> 00:14:52,840
now you can say here's my future and

00:14:49,650 --> 00:14:58,630
when it is ready then I want to do some

00:14:52,840 --> 00:15:02,200
other asynchronous tasks you can say

00:14:58,630 --> 00:15:04,690
I've got a bunch of futures and when

00:15:02,200 --> 00:15:05,980
they're all ready then I want to do

00:15:04,690 --> 00:15:09,310
something I'm going to wait for all of

00:15:05,980 --> 00:15:11,050
them to be ready or for the first one

00:15:09,310 --> 00:15:13,330
out of a bunch to be ready so this

00:15:11,050 --> 00:15:16,310
function is to give you that no there's

00:15:13,330 --> 00:15:19,430
sort of no set joining for so

00:15:16,310 --> 00:15:22,149
for gathering futures together which is

00:15:19,430 --> 00:15:24,889
incredibly powerful - and actually

00:15:22,149 --> 00:15:26,899
combined with the continuations that is

00:15:24,889 --> 00:15:28,790
a great combination because you can say

00:15:26,899 --> 00:15:31,579
when everything is ready then schedule

00:15:28,790 --> 00:15:32,990
this new a synchronous task and that

00:15:31,579 --> 00:15:34,430
task will only start when all the

00:15:32,990 --> 00:15:39,439
previous one when all the features are

00:15:34,430 --> 00:15:43,819
ready and then we've got latches and

00:15:39,439 --> 00:15:46,249
barriers for know providing scheduling

00:15:43,819 --> 00:15:47,720
points within our code so when a whole

00:15:46,249 --> 00:15:49,339
bunch of threads have reached a given

00:15:47,720 --> 00:15:52,910
point in the code then we can move on

00:15:49,339 --> 00:15:58,550
and now we the latches and barriers give

00:15:52,910 --> 00:16:00,290
us that those increments and then

00:15:58,550 --> 00:16:02,059
finally we've got atomic shot smart

00:16:00,290 --> 00:16:05,209
pointers in the form of atomic shared

00:16:02,059 --> 00:16:09,019
pointer and atomic weak pointer so where

00:16:05,209 --> 00:16:12,290
as ace in the C++ 11 a single shared

00:16:09,019 --> 00:16:13,699
pointer instance no cannot you cannot

00:16:12,290 --> 00:16:14,959
have two threads accessing map because

00:16:13,699 --> 00:16:16,309
otherwise it's a database unless you

00:16:14,959 --> 00:16:22,100
obviously wrap it with a mutex or some

00:16:16,309 --> 00:16:24,500
other synchronization in that if you use

00:16:22,100 --> 00:16:27,199
the concurrency TS then you can have

00:16:24,500 --> 00:16:28,730
atomic shared pointer so that you can

00:16:27,199 --> 00:16:39,559
then have multiple threads access that

00:16:28,730 --> 00:16:41,259
without external synchronization but

00:16:39,559 --> 00:16:43,759
there's also a whole bunch of things

00:16:41,259 --> 00:16:45,110
coming up for the concurrency TS version

00:16:43,759 --> 00:16:47,300
- now these are all proposals under

00:16:45,110 --> 00:16:49,430
consideration the working draft for

00:16:47,300 --> 00:16:54,309
concurrency but TS version two is

00:16:49,430 --> 00:16:58,009
somewhat scant at the moment we've got

00:16:54,309 --> 00:16:59,689
the big big thing really from my

00:16:58,009 --> 00:17:05,120
perspective is the executives and

00:16:59,689 --> 00:17:06,949
schedulers now this is the the the

00:17:05,120 --> 00:17:08,659
fundamental building blocks which are

00:17:06,949 --> 00:17:10,309
going to get us thread pools thread

00:17:08,659 --> 00:17:13,100
pools and and things like that are

00:17:10,309 --> 00:17:16,490
things that we have been wanting as a

00:17:13,100 --> 00:17:18,770
concurrency group for concurrency

00:17:16,490 --> 00:17:22,220
support in standard TS since we were

00:17:18,770 --> 00:17:25,610
working on C++ 11 but we ran out of time

00:17:22,220 --> 00:17:28,510
to specify them properly and so all we

00:17:25,610 --> 00:17:28,510
got the standard async

00:17:28,590 --> 00:17:35,370
and as a group people have been working

00:17:32,820 --> 00:17:38,249
on ever since there have been many many

00:17:35,370 --> 00:17:40,070
proposals and changes to the way the API

00:17:38,249 --> 00:17:43,679
works around executives and schedulers

00:17:40,070 --> 00:17:47,850
but we're really really keen to get them

00:17:43,679 --> 00:17:50,610
and so hope we're gonna get them

00:17:47,850 --> 00:17:54,139
somewhat sometimes see actually in an

00:17:50,610 --> 00:17:54,139
API that we can all agree on

00:17:54,350 --> 00:18:01,289
there's also proposals for distributed

00:17:56,700 --> 00:18:02,549
counters so obviously if you've got

00:18:01,289 --> 00:18:04,799
hundreds of threads and they were all

00:18:02,549 --> 00:18:06,779
trying to update a single count then

00:18:04,799 --> 00:18:08,159
that's an epic point of contention and

00:18:06,779 --> 00:18:11,129
that's going to cause everything to slow

00:18:08,159 --> 00:18:13,259
down so you don't want to do that so if

00:18:11,129 --> 00:18:14,700
you don't need everything to really

00:18:13,259 --> 00:18:16,830
really really be up-to-date and you can

00:18:14,700 --> 00:18:18,720
cope with batching things then you can

00:18:16,830 --> 00:18:20,220
have your distributed counter and you

00:18:18,720 --> 00:18:23,399
say well here's this counter that we

00:18:20,220 --> 00:18:25,769
eventually going to update and this this

00:18:23,399 --> 00:18:27,690
thread is got is going to it updates it

00:18:25,769 --> 00:18:29,429
to the local count and then the

00:18:27,690 --> 00:18:31,710
propagation to the central count then

00:18:29,429 --> 00:18:33,480
happens automatically in some fashion

00:18:31,710 --> 00:18:35,610
and obviously there's ways that you can

00:18:33,480 --> 00:18:37,980
say update it now please because I

00:18:35,610 --> 00:18:39,899
really care but it means that it

00:18:37,980 --> 00:18:42,600
provides you the eventual consistency on

00:18:39,899 --> 00:18:49,590
the count without the direct contention

00:18:42,600 --> 00:18:51,330
that you might otherwise get also we've

00:18:49,590 --> 00:18:55,019
got proposals for concurrent unordered

00:18:51,330 --> 00:18:57,029
containers so I mean in the standard at

00:18:55,019 --> 00:19:00,899
the moment we've got no unordered map

00:18:57,029 --> 00:19:02,610
and unordered set and there are

00:19:00,899 --> 00:19:04,409
proposals to provide concurrent versions

00:19:02,610 --> 00:19:06,840
of those so that you can do your table

00:19:04,409 --> 00:19:12,509
lookups so the example that I had with

00:19:06,840 --> 00:19:15,059
the shared mutex then it used a standard

00:19:12,509 --> 00:19:16,830
map so it's not quite know but if you if

00:19:15,059 --> 00:19:19,230
you use the unordered version and the

00:19:16,830 --> 00:19:21,119
the new the proposed concurrent on order

00:19:19,230 --> 00:19:22,139
version you wouldn't need to put mutex

00:19:21,119 --> 00:19:25,590
lock in because that would deal with

00:19:22,139 --> 00:19:27,629
internally in the container it is

00:19:25,590 --> 00:19:28,950
potentially possible to for some

00:19:27,629 --> 00:19:31,740
implementations for that to be locked

00:19:28,950 --> 00:19:35,490
free so it depends on quite on the API

00:19:31,740 --> 00:19:37,529
and whether whether the the

00:19:35,490 --> 00:19:40,589
implementations then provide those but

00:19:37,529 --> 00:19:42,729
there is potential for that

00:19:40,589 --> 00:19:46,179
similarly we've got concurrent queues

00:19:42,729 --> 00:19:47,679
now seeking know so can come on an order

00:19:46,179 --> 00:19:50,109
containers and concurrent queues give us

00:19:47,679 --> 00:19:53,649
basic buildings structures for lots of

00:19:50,109 --> 00:19:56,499
cross straight communication so again

00:19:53,649 --> 00:19:58,749
it's all about you don't not needing to

00:19:56,499 --> 00:20:00,249
use a mutex yourself because the

00:19:58,749 --> 00:20:02,830
containers and data structures take care

00:20:00,249 --> 00:20:04,269
of it for you and queue is a great

00:20:02,830 --> 00:20:06,519
before you can use them for message

00:20:04,269 --> 00:20:09,399
queues so you can have no communicating

00:20:06,519 --> 00:20:11,679
state machines they horse communicating

00:20:09,399 --> 00:20:13,659
sequential processes so you gives you

00:20:11,679 --> 00:20:15,190
can have each each thread essentially

00:20:13,659 --> 00:20:16,899
communicates only with the outside world

00:20:15,190 --> 00:20:19,749
through through a queue of some

00:20:16,899 --> 00:20:22,119
description so once you've got that then

00:20:19,749 --> 00:20:28,709
you don't then need to worry about lots

00:20:22,119 --> 00:20:31,389
of synchronization problems and simply

00:20:28,709 --> 00:20:33,609
so and then we've got no safe concurrent

00:20:31,389 --> 00:20:36,009
stream access is coming we hope and

00:20:33,609 --> 00:20:40,779
there's certainly proposals for it so

00:20:36,009 --> 00:20:42,070
that you can say here's this bunch of I

00:20:40,779 --> 00:20:44,049
know I'm running on a thread it I'm

00:20:42,070 --> 00:20:45,399
writing stuff out to a stream I know

00:20:44,049 --> 00:20:47,649
that another thread is also going to be

00:20:45,399 --> 00:20:49,359
biting stuff happens to a stream here is

00:20:47,649 --> 00:20:52,359
a bunch of stuff that I want to make

00:20:49,359 --> 00:20:54,579
sure it appears together but I've one I

00:20:52,359 --> 00:20:57,669
can't do it as a single insert I'm like

00:20:54,579 --> 00:21:01,539
and I want and so we're providing them

00:20:57,669 --> 00:21:03,489
some mechanism for ensuring that a write

00:21:01,539 --> 00:21:09,519
is then atomic from the point of view of

00:21:03,489 --> 00:21:10,749
the outside world resumable functions

00:21:09,519 --> 00:21:16,690
and care routines there's a whole bunch

00:21:10,749 --> 00:21:20,200
of talks on that today guys I spoke to

00:21:16,690 --> 00:21:24,369
yesterday how they put up a slide and

00:21:20,200 --> 00:21:27,399
said that it was no like-a a mini cooker

00:21:24,369 --> 00:21:31,570
routine calm so yeah there's a whole

00:21:27,399 --> 00:21:35,559
bunch of stuff on that that that allows

00:21:31,570 --> 00:21:38,709
you to simplify code again with

00:21:35,559 --> 00:21:40,419
asynchronous stuff if you Hartmut

00:21:38,709 --> 00:21:45,429
haven't had a slide up yesterday about

00:21:40,419 --> 00:21:48,639
the making a synchronous parallel

00:21:45,429 --> 00:21:51,039
algorithms and how him no sticking : you

00:21:48,639 --> 00:21:51,940
got two two parallel algorithms that

00:21:51,039 --> 00:21:53,979
you've got with futures that you're

00:21:51,940 --> 00:21:54,620
waiting on so that you could then and

00:21:53,979 --> 00:21:58,080
that

00:21:54,620 --> 00:21:59,550
the outer function then to also return

00:21:58,080 --> 00:22:01,190
the future without even having to worry

00:21:59,550 --> 00:22:04,800
about making sure they were both ready

00:22:01,190 --> 00:22:09,330
in that particular case he could have

00:22:04,800 --> 00:22:11,490
used when all and-and-and continuations

00:22:09,330 --> 00:22:13,350
as well but it's it co-routines one

00:22:11,490 --> 00:22:14,670
alternative in some sense they provide

00:22:13,350 --> 00:22:17,340
you concurrency on a single thread

00:22:14,670 --> 00:22:18,740
because you can no switch between your

00:22:17,340 --> 00:22:21,210
different functions

00:22:18,740 --> 00:22:22,200
now when you haven't got work to do on

00:22:21,210 --> 00:22:23,100
this function you're waiting for

00:22:22,200 --> 00:22:29,010
something then you can run something

00:22:23,100 --> 00:22:31,650
else and then pipe lines so we've got

00:22:29,010 --> 00:22:33,030
sequence of data no data coming in and

00:22:31,650 --> 00:22:35,130
we've got a sequence of operations we

00:22:33,030 --> 00:22:38,490
need to do to it so we'd be able to

00:22:35,130 --> 00:22:39,990
pipeline and we say they do this and

00:22:38,490 --> 00:22:41,340
then do that and then pass a result of

00:22:39,990 --> 00:22:42,870
this other thing and then split it out

00:22:41,340 --> 00:22:44,160
and pass the results of these two other

00:22:42,870 --> 00:22:47,850
functions that running parallel and then

00:22:44,160 --> 00:22:50,040
combine the results both of you do

00:22:47,850 --> 00:22:55,680
command line processing in UNIX the UNIX

00:22:50,040 --> 00:22:58,980
pipes no but but actually in your code

00:22:55,680 --> 00:23:00,150
in your C++ program solver see it's

00:22:58,980 --> 00:23:01,320
great if you've got a whole bunch of

00:23:00,150 --> 00:23:04,590
data coming in and you do the same

00:23:01,320 --> 00:23:11,190
sequence of operations on every bit of

00:23:04,590 --> 00:23:13,590
data that comes through okay so I'm

00:23:11,190 --> 00:23:14,930
gonna have some actual more concrete

00:23:13,590 --> 00:23:17,310
examples of some of these things

00:23:14,930 --> 00:23:20,280
everything in the TS is is in the

00:23:17,310 --> 00:23:21,810
standard experimental namespace that

00:23:20,280 --> 00:23:23,460
doesn't mean that it's poor code and

00:23:21,810 --> 00:23:26,790
that you shouldn't use it it means that

00:23:23,460 --> 00:23:28,170
it's an experimental interface the C++

00:23:26,790 --> 00:23:29,400
standards committee aren't committing

00:23:28,170 --> 00:23:32,130
that this is what it's going to be like

00:23:29,400 --> 00:23:36,620
when it gets into the final standard so

00:23:32,130 --> 00:23:38,880
I don't let that put you off I know that

00:23:36,620 --> 00:23:40,950
some workplaces have an experimental

00:23:38,880 --> 00:23:42,990
main space for their own bits of code

00:23:40,950 --> 00:23:45,300
where don't use this in production may

00:23:42,990 --> 00:23:47,370
say well just because it's in standard

00:23:45,300 --> 00:23:49,740
experimental from the point of view of

00:23:47,370 --> 00:23:51,870
the standard doesn't mean that obviously

00:23:49,740 --> 00:23:52,950
you hope that if you use whichever

00:23:51,870 --> 00:23:54,300
implementer you've got your

00:23:52,950 --> 00:23:55,800
implementation from you hope that it's

00:23:54,300 --> 00:23:59,670
as good quality as the rest of the stuff

00:23:55,800 --> 00:24:02,250
they provide you in the slides I'm going

00:23:59,670 --> 00:24:04,710
to use HDD exp instead because standard

00:24:02,250 --> 00:24:07,160
experimental is a bit long for on slide

00:24:04,710 --> 00:24:07,160
examples

00:24:07,289 --> 00:24:14,799
okay so continuations continuations give

00:24:13,330 --> 00:24:19,599
us a new task to run when a future

00:24:14,799 --> 00:24:21,039
becomes ready and conceptually you say

00:24:19,599 --> 00:24:22,720
when the future is ready you then do

00:24:21,039 --> 00:24:27,249
this and so sure enough the function is

00:24:22,720 --> 00:24:29,109
called then if you've got if you start

00:24:27,249 --> 00:24:30,700
with a standard future well if you start

00:24:29,109 --> 00:24:34,239
with a standard experimental future

00:24:30,700 --> 00:24:36,070
because they only work on that then your

00:24:34,239 --> 00:24:37,419
function then must take a standard

00:24:36,070 --> 00:24:40,960
experimental future as the only

00:24:37,419 --> 00:24:43,059
parameter and the source of futures are

00:24:40,960 --> 00:24:44,679
one-shot things so if you normally if

00:24:43,059 --> 00:24:46,210
you get the value from a future then

00:24:44,679 --> 00:24:47,649
that future is no longer valid so

00:24:46,210 --> 00:24:49,720
similarly if you change a continuation

00:24:47,649 --> 00:24:51,220
on a future the source feature is no

00:24:49,720 --> 00:24:53,379
longer valid because it is going to be

00:24:51,220 --> 00:24:54,789
passed into the continuation and instead

00:24:53,379 --> 00:24:58,389
you've got a new future to hold the

00:24:54,789 --> 00:24:59,859
result from the continuation and all

00:24:58,389 --> 00:25:01,720
this means that on any given future and

00:24:59,859 --> 00:25:05,470
you want continuation can be added but

00:25:01,720 --> 00:25:08,320
of course you can chain them so here's

00:25:05,470 --> 00:25:09,489
some code we're going to try and find

00:25:08,320 --> 00:25:11,859
the answer we all know it's going to be

00:25:09,489 --> 00:25:15,009
42 but we're going to assume that we

00:25:11,859 --> 00:25:15,999
haven't got there yet and when we do get

00:25:15,009 --> 00:25:20,169
the answer we're going to process the

00:25:15,999 --> 00:25:21,999
result so we use standard experimental I

00:25:20,169 --> 00:25:24,509
think to find the answer and that gives

00:25:21,999 --> 00:25:27,879
us back a standard experimental future

00:25:24,509 --> 00:25:31,710
we can then say then process the result

00:25:27,879 --> 00:25:33,609
and that gives us a new future f2 which

00:25:31,710 --> 00:25:37,629
processing the result is returning a

00:25:33,609 --> 00:25:42,549
string so f2 will be a standard

00:25:37,629 --> 00:25:44,200
experimental future for a string and at

00:25:42,549 --> 00:25:48,039
this point F is no longer valid because

00:25:44,200 --> 00:25:49,679
the know it's it's been swallowed into

00:25:48,039 --> 00:25:52,229
the continuation

00:25:49,679 --> 00:25:57,909
when the original async task returns

00:25:52,229 --> 00:26:00,309
then it will pass the the resulting

00:25:57,909 --> 00:26:02,379
future holding the result as that

00:26:00,309 --> 00:26:04,059
parameter to process result so we can

00:26:02,379 --> 00:26:05,919
see it takes span to the exponent of the

00:26:04,059 --> 00:26:09,070
future of an int as it's one and only

00:26:05,919 --> 00:26:10,450
parameter so that's where know that is

00:26:09,070 --> 00:26:13,539
going to be that is conceptually where

00:26:10,450 --> 00:26:16,529
our where a future F goes it becomes the

00:26:13,539 --> 00:26:16,529
parameter to that function

00:26:18,670 --> 00:26:38,380
yes okay so the question is can you add

00:26:36,280 --> 00:26:40,480
parallel veins to a single future so you

00:26:38,380 --> 00:26:41,890
can say F dot then to get process result

00:26:40,480 --> 00:26:43,600
and then also F dot then something else

00:26:41,890 --> 00:26:45,820
and the answer is no you can't because

00:26:43,600 --> 00:26:48,220
after your first F dot then your F has

00:26:45,820 --> 00:26:50,560
no has no future in it is it's invalid

00:26:48,220 --> 00:26:52,930
so you can't then do that you can chain

00:26:50,560 --> 00:26:55,030
on to the result of F two but you can't

00:26:52,930 --> 00:26:57,190
have multiple parallel ends on a stand

00:26:55,030 --> 00:27:02,080
an experimental future if you want to do

00:26:57,190 --> 00:27:05,250
that you can with shared futures but

00:27:02,080 --> 00:27:05,250
we'll look at that in just a second

00:27:05,820 --> 00:27:12,300
continuations also allow you to process

00:27:07,960 --> 00:27:15,430
exceptions so if your first first

00:27:12,300 --> 00:27:16,780
function throws then that gets stored in

00:27:15,430 --> 00:27:18,610
the future like it normally would and

00:27:16,780 --> 00:27:22,390
this is it's stored in the future that

00:27:18,610 --> 00:27:25,330
is passed to the continuation so we have

00:27:22,390 --> 00:27:27,940
stand experimental async or fail fail is

00:27:25,330 --> 00:27:30,250
going to throw a runtime error we then

00:27:27,940 --> 00:27:35,080
chain on the continuation next next

00:27:30,250 --> 00:27:37,320
takes the future that that will be a

00:27:35,080 --> 00:27:39,700
future that is holding the exception of

00:27:37,320 --> 00:27:42,730
the standard runtime error that was

00:27:39,700 --> 00:27:46,240
thrown and so when you call get F gets

00:27:42,730 --> 00:27:48,760
hit inside next then that's going to

00:27:46,240 --> 00:27:52,270
throw because F got get throws if the

00:27:48,760 --> 00:27:54,820
future holds an exception and then

00:27:52,270 --> 00:27:57,670
obviously in food then the final future

00:27:54,820 --> 00:27:59,320
is the the one returned codes a result

00:27:57,670 --> 00:28:01,360
from next next through because it

00:27:59,320 --> 00:28:03,160
because and so you actually end up the

00:28:01,360 --> 00:28:06,580
exception propagates all the way out so

00:28:03,160 --> 00:28:09,970
the final get info will also throw an

00:28:06,580 --> 00:28:11,530
exception obviously if you catch the

00:28:09,970 --> 00:28:12,880
exception in your continuation then you

00:28:11,530 --> 00:28:15,570
can do whatever processing you like with

00:28:12,880 --> 00:28:17,950
it which is why we pass in the future

00:28:15,570 --> 00:28:19,690
rather than passing in the values so

00:28:17,950 --> 00:28:21,370
that these exception gets propagated to

00:28:19,690 --> 00:28:24,250
and you can then process it or all allow

00:28:21,370 --> 00:28:27,690
allow it to propagate out and pass on to

00:28:24,250 --> 00:28:27,690
the next continuation in the chain

00:28:31,090 --> 00:28:38,179
if you've got some function that doesn't

00:28:35,299 --> 00:28:41,720
take a future so your protests result

00:28:38,179 --> 00:28:44,389
wants a warrant well you can just wrap

00:28:41,720 --> 00:28:46,899
it wrap it in the lambda know so your

00:28:44,389 --> 00:28:49,700
lambda takes a future if you use the new

00:28:46,899 --> 00:28:53,989
generic lambda so that could say square

00:28:49,700 --> 00:28:56,720
brackets Auto F so to take the future

00:28:53,989 --> 00:29:04,399
and then obviously you can process the

00:28:56,720 --> 00:29:07,690
result a unpack call F get it's a little

00:29:04,399 --> 00:29:07,690
bit of overhead but it's not too tricky

00:29:09,429 --> 00:29:14,859
okay so continuations work we shared

00:29:13,190 --> 00:29:18,919
features as well someone said can you

00:29:14,859 --> 00:29:22,809
have multiple parallel veins on the same

00:29:18,919 --> 00:29:24,859
future well with shared future you can

00:29:22,809 --> 00:29:26,450
obviously if you started with a shared

00:29:24,859 --> 00:29:31,340
future then the continuation must also

00:29:26,450 --> 00:29:34,879
take a shared future and shared futures

00:29:31,340 --> 00:29:36,919
are reusable so and calling get on the

00:29:34,879 --> 00:29:38,239
shared future it's still usable same

00:29:36,919 --> 00:29:41,470
applies do you have a continuation may

00:29:38,239 --> 00:29:41,470
remains valid so you can have multiple

00:29:42,549 --> 00:29:48,889
so same example here we've got a sink to

00:29:46,879 --> 00:29:51,529
find the answer we're going to call dot

00:29:48,889 --> 00:29:54,700
share so that Fi is a span of the

00:29:51,529 --> 00:29:57,889
experimental shared future of an int and

00:29:54,700 --> 00:30:01,029
then we are training to continuations

00:29:57,889 --> 00:30:05,049
these will run in parallel so as soon as

00:30:01,029 --> 00:30:07,879
the original a sink has finished then

00:30:05,049 --> 00:30:13,609
the tasks for next one and next two

00:30:07,879 --> 00:30:17,649
we'll both be scheduled I will say at

00:30:13,609 --> 00:30:20,649
this point that the scheduling is

00:30:17,649 --> 00:30:20,649
unspecified

00:30:24,480 --> 00:30:29,130
the Tia says that continuation tasks are

00:30:26,160 --> 00:30:33,179
gone on unspecified threat the idea is

00:30:29,130 --> 00:30:36,260
that by the time we've got as far as

00:30:33,179 --> 00:30:39,330
integrating into the C++ standard then

00:30:36,260 --> 00:30:41,730
we will have sorted our problems with

00:30:39,330 --> 00:30:45,809
executives and thread pools and so we

00:30:41,730 --> 00:30:48,030
will add to then a means of specifying

00:30:45,809 --> 00:30:50,190
where you're going to run your run your

00:30:48,030 --> 00:30:51,690
continuation but for the meantime it

00:30:50,190 --> 00:30:55,290
will run on some threads somewhere in

00:30:51,690 --> 00:30:59,160
your system yes there is a question in

00:30:55,290 --> 00:31:01,440
the middle what happens if one of them

00:30:59,160 --> 00:31:03,090
throws well it's just a continuation

00:31:01,440 --> 00:31:06,120
which is amazing tasks wrapped in the

00:31:03,090 --> 00:31:09,179
future so what the each continuation

00:31:06,120 --> 00:31:10,919
that throat if it throws then the future

00:31:09,179 --> 00:31:12,210
associated with that continuation will

00:31:10,919 --> 00:31:14,580
swallow the it will capture the

00:31:12,210 --> 00:31:17,070
exception so in this case we've got two

00:31:14,580 --> 00:31:21,419
continuations which has stored futures F

00:31:17,070 --> 00:31:23,549
2 and F 3 although actually on the slide

00:31:21,419 --> 00:31:27,620
I've called them F 2 and F 2 so that's

00:31:23,549 --> 00:31:33,090
just confusion but each of those will

00:31:27,620 --> 00:31:35,190
know if next one throws an exception

00:31:33,090 --> 00:31:37,410
then the next two tasks is still going

00:31:35,190 --> 00:31:39,030
to run and but the future from that

00:31:37,410 --> 00:31:41,070
associated with the next one

00:31:39,030 --> 00:31:43,679
continuation will hold the hold the

00:31:41,070 --> 00:31:46,679
exception ready to throw when you call

00:31:43,679 --> 00:31:48,780
get and obviously if you've been chained

00:31:46,679 --> 00:31:50,850
continuations on on those then the

00:31:48,780 --> 00:31:53,370
exceptions propagate down the one path

00:31:50,850 --> 00:32:04,950
where where that exception went and not

00:31:53,370 --> 00:32:07,740
down the other one okay so what if you

00:32:04,950 --> 00:32:09,299
wanted to wait for your futures you can

00:32:07,740 --> 00:32:11,940
wait for any of them to be ready you've

00:32:09,299 --> 00:32:14,429
got a whole bunch of futures and you

00:32:11,940 --> 00:32:16,230
want to wait for just one well that's

00:32:14,429 --> 00:32:18,150
right that's we've got to overload when

00:32:16,230 --> 00:32:19,830
any I've got two overloads these are

00:32:18,150 --> 00:32:21,809
great big long signatures but actually

00:32:19,830 --> 00:32:25,559
it's really quite straightforward the

00:32:21,809 --> 00:32:28,049
top top overload is a very addict it pot

00:32:25,559 --> 00:32:30,980
when any and then you pass it any number

00:32:28,049 --> 00:32:34,020
of futures and it returns you a result

00:32:30,980 --> 00:32:36,059
now the result is a B is a package that

00:32:34,020 --> 00:32:38,330
holds a tuple with all the futures that

00:32:36,059 --> 00:32:39,830
you gave it and then a flag to say which

00:32:38,330 --> 00:32:41,779
one of those it was that was ready when

00:32:39,830 --> 00:32:42,980
it checked obviously by the time you get

00:32:41,779 --> 00:32:45,590
around to processing it they might all

00:32:42,980 --> 00:32:47,450
be ready or some no three out of four or

00:32:45,590 --> 00:32:51,409
whatever but there will be one that

00:32:47,450 --> 00:32:53,450
triggered the the when any result the

00:32:51,409 --> 00:32:56,570
the future that's returned from when any

00:32:53,450 --> 00:32:59,029
to be ready and the when any result has

00:32:56,570 --> 00:33:01,549
a frank to say it was this one the third

00:32:59,029 --> 00:33:05,899
one in the list and by the way here is

00:33:01,549 --> 00:33:07,789
the list so it gives you the list back

00:33:05,899 --> 00:33:10,039
so that you can not only get to the one

00:33:07,789 --> 00:33:11,659
that you've triggered it but also to get

00:33:10,039 --> 00:33:14,240
all the others and you can then process

00:33:11,659 --> 00:33:14,929
them no wait for them in turn if they're

00:33:14,240 --> 00:33:18,370
not ready yet

00:33:14,929 --> 00:33:18,370
or do some others form and processing

00:33:18,700 --> 00:33:25,730
the bottom overload also has a great big

00:33:23,059 --> 00:33:32,840
long return type but actually it's it

00:33:25,730 --> 00:33:36,169
just when any on an iterator range the

00:33:32,840 --> 00:33:40,190
so you pass in a range any iterator

00:33:36,169 --> 00:33:42,710
range and it will iterate through and

00:33:40,190 --> 00:33:44,809
build your set the result is always a

00:33:42,710 --> 00:33:47,000
vector that holds the futures that were

00:33:44,809 --> 00:33:50,510
passed in and regardless of where your

00:33:47,000 --> 00:33:57,440
Ritter ages came from have a question at

00:33:50,510 --> 00:34:00,110
the front yes if a top so the question

00:33:57,440 --> 00:34:01,309
is if a task associated with a future

00:34:00,110 --> 00:34:03,740
throws an exception does it make it

00:34:01,309 --> 00:34:05,179
ready and yes with futures if they if

00:34:03,740 --> 00:34:07,730
they capture an exception they are then

00:34:05,179 --> 00:34:09,980
ready and so then the when any will

00:34:07,730 --> 00:34:11,690
indeed trigger and so if you've got

00:34:09,980 --> 00:34:13,280
three tasks and one of them throws and

00:34:11,690 --> 00:34:15,859
then and that throws before the others

00:34:13,280 --> 00:34:17,389
have got the results when any if you can

00:34:15,859 --> 00:34:19,099
you pass those to win any the when any

00:34:17,389 --> 00:34:22,010
result will be ready and it will say

00:34:19,099 --> 00:34:29,450
that one that through is the one that's

00:34:22,010 --> 00:34:31,849
ready the one that triggered so this is

00:34:29,450 --> 00:34:35,359
great if you've got speculative tasks if

00:34:31,849 --> 00:34:38,839
you want to say well I've got plenty of

00:34:35,359 --> 00:34:40,669
course to spare and I've got two

00:34:38,839 --> 00:34:44,450
possible ways of calculating this value

00:34:40,669 --> 00:34:45,800
three possible ways maybe and I don't

00:34:44,450 --> 00:34:47,839
know which one's going to be fastest

00:34:45,800 --> 00:34:49,550
today so let's launch all of them and

00:34:47,839 --> 00:34:52,379
whichever one gets back quickest I'll

00:34:49,550 --> 00:34:56,609
take the result and then process it

00:34:52,379 --> 00:34:57,930
so that's good alternatively if you've

00:34:56,609 --> 00:35:01,559
got a whole bunch of stuff that you want

00:34:57,930 --> 00:35:03,150
to do and when the first things ready

00:35:01,559 --> 00:35:07,769
then I want to do some more processing

00:35:03,150 --> 00:35:09,869
and then when the other bit has become

00:35:07,769 --> 00:35:11,549
ready to but I want to process which I

00:35:09,869 --> 00:35:13,739
don't know which of my three things is

00:35:11,549 --> 00:35:15,359
gonna be ready first and so I want to

00:35:13,739 --> 00:35:17,369
process the one that comes back first

00:35:15,359 --> 00:35:19,019
and I want to do that processing first

00:35:17,369 --> 00:35:21,390
so I could chain a continuation on each

00:35:19,019 --> 00:35:23,549
one individually but actually I only

00:35:21,390 --> 00:35:26,099
what I want to only process one of the

00:35:23,549 --> 00:35:29,759
results at a time so I would say when

00:35:26,099 --> 00:35:31,529
any the first one comes back I do the

00:35:29,759 --> 00:35:35,009
processing and then wait for the second

00:35:31,529 --> 00:35:37,319
and third in turn and it allows you to

00:35:35,009 --> 00:35:39,960
not fuss about which order so we

00:35:37,319 --> 00:35:42,950
launched two things with async f1 and f2

00:35:39,960 --> 00:35:45,410
give us our tasks and then when any

00:35:42,950 --> 00:35:47,759
these are standard futures or

00:35:45,410 --> 00:35:50,039
experimental futures and so we have to

00:35:47,759 --> 00:35:52,710
move them we move them into our when any

00:35:50,039 --> 00:35:55,940
call and then we get back a future that

00:35:52,710 --> 00:35:58,319
holds the potential that when any result

00:35:55,940 --> 00:35:59,910
we can have got no it's an experimental

00:35:58,319 --> 00:36:01,829
future so it can of course train it with

00:35:59,910 --> 00:36:12,529
continuations so we can say F 3 dot then

00:36:01,829 --> 00:36:16,019
some some do some processing ok when all

00:36:12,529 --> 00:36:19,289
is the counterpart you wait for all your

00:36:16,019 --> 00:36:20,910
futures to be ready and again it's got

00:36:19,289 --> 00:36:24,029
two overloads first one just takes a

00:36:20,910 --> 00:36:25,650
bunch of futures as parameters and the

00:36:24,029 --> 00:36:28,289
second one takes an iterative range and

00:36:25,650 --> 00:36:30,809
again the result holds a vector this

00:36:28,289 --> 00:36:32,549
time because the results are all deaf

00:36:30,809 --> 00:36:33,839
definitely all going to be ready you

00:36:32,549 --> 00:36:36,150
don't have to there's not this when any

00:36:33,839 --> 00:36:38,039
result complicatedness the first one is

00:36:36,150 --> 00:36:39,420
just a future holding a tuple and the

00:36:38,039 --> 00:36:46,979
second one is a future holding a vector

00:36:39,420 --> 00:36:48,509
or futures and this time it's great if

00:36:46,979 --> 00:36:49,529
you've got a bunch of subtasks you want

00:36:48,509 --> 00:36:50,940
to launch the off and then when they're

00:36:49,529 --> 00:36:54,509
all ready then you've got some more

00:36:50,940 --> 00:36:57,390
processing to do so you do exactly that

00:36:54,509 --> 00:37:02,099
now here's a three task subtask 1 2 3

00:36:57,390 --> 00:37:03,700
with async and then when all now you get

00:37:02,099 --> 00:37:08,350
the result

00:37:03,700 --> 00:37:08,350
or chain your continuation or whatever

00:37:12,010 --> 00:37:16,070
okay it's not the only changes to this

00:37:14,480 --> 00:37:17,810
so the futures interface as a whole

00:37:16,070 --> 00:37:20,530
there's no a collection of little

00:37:17,810 --> 00:37:22,700
changes we've got first up are two

00:37:20,530 --> 00:37:27,800
functions to create futures that are

00:37:22,700 --> 00:37:31,190
already ready standard no make ready

00:37:27,800 --> 00:37:35,060
future that gives us a future that holds

00:37:31,190 --> 00:37:36,830
a value so rather than actually spawning

00:37:35,060 --> 00:37:40,490
a task you say well I already know the

00:37:36,830 --> 00:37:41,780
results gonna be so here we are and then

00:37:40,490 --> 00:37:43,940
the counter parts for that which is

00:37:41,780 --> 00:37:46,730
making exceptional future which is no

00:37:43,940 --> 00:37:48,050
I'm going to holding a creative future

00:37:46,730 --> 00:37:50,150
that's holding an exceptional ready to

00:37:48,050 --> 00:37:56,980
throw so if anyone dares call get on it

00:37:50,150 --> 00:38:01,010
then it will throw up in their face and

00:37:56,980 --> 00:38:03,050
both and again these cases where as a

00:38:01,010 --> 00:38:05,390
general thing you're built you've got an

00:38:03,050 --> 00:38:07,280
API and the library structure that you

00:38:05,390 --> 00:38:09,200
use these futures for communication and

00:38:07,280 --> 00:38:10,940
in this particular code branch you

00:38:09,200 --> 00:38:12,650
realize you know the answers already you

00:38:10,940 --> 00:38:14,840
don't need to actually spawn any tasks

00:38:12,650 --> 00:38:16,280
to do it so rather than having to do a

00:38:14,840 --> 00:38:17,510
little dance of creating a promise in

00:38:16,280 --> 00:38:19,400
getting a future and setting the value

00:38:17,510 --> 00:38:20,720
on the promise or selling the exception

00:38:19,400 --> 00:38:26,840
on the promise then you can just call

00:38:20,720 --> 00:38:28,580
these functions instead and then we've

00:38:26,840 --> 00:38:30,770
also got this nice little nifty function

00:38:28,580 --> 00:38:32,240
is ready we can ask a future are you

00:38:30,770 --> 00:38:36,140
ready we can pull it rather than happen

00:38:32,240 --> 00:38:38,150
to wait for it to be ready or do a nifty

00:38:36,140 --> 00:38:40,940
little dance with saying wait for no

00:38:38,150 --> 00:38:44,230
seconds to see whether you're ready then

00:38:40,940 --> 00:38:44,230
we can just worry are you ready

00:38:59,370 --> 00:39:03,880
okay next on the list is laches and

00:39:01,390 --> 00:39:06,940
barriers laches a single-use countdown

00:39:03,880 --> 00:39:09,540
you say well I've got this bunch of

00:39:06,940 --> 00:39:12,310
events that I need to happen and then

00:39:09,540 --> 00:39:16,180
when all the events have happened I can

00:39:12,310 --> 00:39:18,250
do some more processing and so know it

00:39:16,180 --> 00:39:20,620
counts down and they count and as each

00:39:18,250 --> 00:39:22,330
as each straight does that does the

00:39:20,620 --> 00:39:25,030
things they it says count down the hatch

00:39:22,330 --> 00:39:27,100
count down the latch and then you and

00:39:25,030 --> 00:39:28,420
then you can have another thread or some

00:39:27,100 --> 00:39:30,880
number of threads waiting on this latch

00:39:28,420 --> 00:39:33,630
and then when as soon as the counting is

00:39:30,880 --> 00:39:39,940
zero all the waiters are notified and

00:39:33,630 --> 00:39:41,590
everyone can proceed its it latches it's

00:39:39,940 --> 00:39:46,120
permanently that so it's a one-shot

00:39:41,590 --> 00:39:48,280
thing whereas barriers on the other hand

00:39:46,120 --> 00:39:50,200
are a reusable mechanism and this time

00:39:48,280 --> 00:39:52,930
it's where you've got a bunch of threads

00:39:50,200 --> 00:39:56,350
that are waiting for each other in some

00:39:52,930 --> 00:39:58,900
sense so if you if you've got a

00:39:56,350 --> 00:40:00,700
multi-stage process and all the threads

00:39:58,900 --> 00:40:03,280
need to have completed stage a before

00:40:00,700 --> 00:40:08,170
they can all move and do stage P then

00:40:03,280 --> 00:40:11,050
you have create a barrier and no at the

00:40:08,170 --> 00:40:12,850
end of it so you say here's a barrier

00:40:11,050 --> 00:40:14,680
for five threads here are my five

00:40:12,850 --> 00:40:16,540
threads they each of them will arrive at

00:40:14,680 --> 00:40:18,910
the barrier when all of them arrived

00:40:16,540 --> 00:40:22,090
then they all can proceed and then they

00:40:18,910 --> 00:40:23,200
and if often if this is a cycling

00:40:22,090 --> 00:40:24,940
process because of the way you're

00:40:23,200 --> 00:40:26,890
processing your data so then they've all

00:40:24,940 --> 00:40:28,930
proceeded and they do the next no so

00:40:26,890 --> 00:40:30,880
you've got packet of data coming in so

00:40:28,930 --> 00:40:33,310
five threads all do their parts on on

00:40:30,880 --> 00:40:34,690
packet a when they've all process pakad

00:40:33,310 --> 00:40:36,640
a they all arrive at the barrier and

00:40:34,690 --> 00:40:39,820
then they could all then move on to

00:40:36,640 --> 00:40:42,250
process packet B and then again when

00:40:39,820 --> 00:40:43,780
they've all done through packet B then

00:40:42,250 --> 00:40:47,280
they will arrive at the barrier and then

00:40:43,780 --> 00:40:47,280
again they're all released to move on

00:40:50,820 --> 00:40:58,300
final thing in the concurrency TS is

00:40:53,670 --> 00:41:00,940
atomic smart pointers now shared pointer

00:40:58,300 --> 00:41:02,380
and weak pointer are not bitwise

00:41:00,940 --> 00:41:02,700
copyable because they've got to deal

00:41:02,380 --> 00:41:04,619
with

00:41:02,700 --> 00:41:07,589
France counts so you're not allowed to

00:41:04,619 --> 00:41:10,380
say standard atomic of standards shared

00:41:07,589 --> 00:41:14,910
pointer or standard atomic of standards

00:41:10,380 --> 00:41:18,180
weak pointer but it would be really nice

00:41:14,910 --> 00:41:20,400
if you could and so in the TS we've got

00:41:18,180 --> 00:41:22,650
standard atomic well standard

00:41:20,400 --> 00:41:25,230
experimental atomic underscore shared

00:41:22,650 --> 00:41:28,500
under supporter and atomic underscore

00:41:25,230 --> 00:41:30,540
weak underscore pointer and these are

00:41:28,500 --> 00:41:32,420
special types that provide the same

00:41:30,540 --> 00:41:36,420
interface as the standard atomic

00:41:32,420 --> 00:41:37,710
interface but they actually work on

00:41:36,420 --> 00:41:40,380
share pointers whereas the standard

00:41:37,710 --> 00:41:45,150
atomic interface can't because it has to

00:41:40,380 --> 00:41:46,950
deal with the reference counting and

00:41:45,150 --> 00:41:49,050
this you would use where you've got a

00:41:46,950 --> 00:41:50,640
single shared pointer object which you

00:41:49,050 --> 00:41:53,250
want to have multiple threads obviously

00:41:50,640 --> 00:41:54,690
each thread can quite happily have

00:41:53,250 --> 00:41:56,609
shared point or objects of their own and

00:41:54,690 --> 00:41:58,560
they can even refer to the same thing

00:41:56,609 --> 00:42:00,300
and the reference count will be dealt

00:41:58,560 --> 00:42:03,540
with accordingly but if you've got a

00:42:00,300 --> 00:42:05,640
single object light in a linked list if

00:42:03,540 --> 00:42:08,579
no is that the next point is in a linked

00:42:05,640 --> 00:42:14,040
list that's being traversed by multiple

00:42:08,579 --> 00:42:16,079
threads then if you try and update those

00:42:14,040 --> 00:42:17,220
concurrently from multiple threads and

00:42:16,079 --> 00:42:18,510
that's not going to work if you've got

00:42:17,220 --> 00:42:19,920
another thread traversing and you need

00:42:18,510 --> 00:42:22,109
to use external synchronization like a

00:42:19,920 --> 00:42:24,060
mutex if you use the atomic shared

00:42:22,109 --> 00:42:27,060
pointer the synchronization is moved

00:42:24,060 --> 00:42:29,280
internally and again depending on your

00:42:27,060 --> 00:42:32,750
implementation does it may use a mutex

00:42:29,280 --> 00:42:32,750
or it may manage to be locked free

00:42:37,740 --> 00:42:43,110
okay so what's coming up proposals under

00:42:40,770 --> 00:42:44,730
consideration so yeah all these things

00:42:43,110 --> 00:42:46,950
that I've just done the examples for are

00:42:44,730 --> 00:42:49,320
in concurrent CTS version one so that's

00:42:46,950 --> 00:42:51,960
actually an official ISO document these

00:42:49,320 --> 00:42:53,369
things some of them there's I've been

00:42:51,960 --> 00:42:54,690
put in the working draft but most of

00:42:53,369 --> 00:42:59,480
them haven't they're just proposals for

00:42:54,690 --> 00:42:59,480
discussion so everything could change

00:43:00,530 --> 00:43:07,700
executives and schedulers these are the

00:43:05,400 --> 00:43:10,350
building blocks our thread pools an

00:43:07,700 --> 00:43:14,250
executor schedules tasks for execution

00:43:10,350 --> 00:43:16,619
and you might have different properties

00:43:14,250 --> 00:43:18,450
you might have three pools might have

00:43:16,619 --> 00:43:20,810
serial executives that run things on a

00:43:18,450 --> 00:43:23,100
particular thread one in after another

00:43:20,810 --> 00:43:26,130
partner took yesterday about the

00:43:23,100 --> 00:43:28,650
potential for GPU executives so you can

00:43:26,130 --> 00:43:32,070
say no his bunch of stuff go run it on

00:43:28,650 --> 00:43:33,960
the GPU or numerous acute errs so here's

00:43:32,070 --> 00:43:36,360
a bunch of stuff go run it over on that

00:43:33,960 --> 00:43:40,940
particular node that has best access to

00:43:36,360 --> 00:43:43,380
this chunk of memory so we're working on

00:43:40,940 --> 00:43:48,510
an interface that's going to work all of

00:43:43,380 --> 00:43:50,100
these I think there's a talk running at

00:43:48,510 --> 00:43:51,720
the moment that's talking more about

00:43:50,100 --> 00:43:52,710
that so go check if you're actually

00:43:51,720 --> 00:43:58,230
interested go and watch the video

00:43:52,710 --> 00:44:00,960
afterwards okay I said what about

00:43:58,230 --> 00:44:02,310
distributed counters we can attempt to

00:44:00,960 --> 00:44:04,830
improve performance by reducing

00:44:02,310 --> 00:44:07,290
contention on a global counter by

00:44:04,830 --> 00:44:09,570
buffering the change changes locally and

00:44:07,290 --> 00:44:12,270
then only to provide know providing

00:44:09,570 --> 00:44:14,030
eventual consistency to make sure that

00:44:12,270 --> 00:44:19,160
the global count is eventually correct

00:44:14,030 --> 00:44:19,160
but doesn't have to be correct right now

00:44:24,350 --> 00:44:29,460
and all those maps I mean proposal is

00:44:27,390 --> 00:44:31,560
concurrent unordered value map which is

00:44:29,460 --> 00:44:34,560
a bit of a mouthful but it does tell you

00:44:31,560 --> 00:44:36,690
essentially the essential properties its

00:44:34,560 --> 00:44:39,330
concurrent for concurrent accesses it's

00:44:36,690 --> 00:44:41,100
on orders so now like the unordered

00:44:39,330 --> 00:44:45,960
containers it's Pro uses a hash

00:44:41,100 --> 00:44:48,540
internally its doors values rather

00:44:45,960 --> 00:44:50,310
because all the standard containers that

00:44:48,540 --> 00:44:52,890
we have at the moment then if you if you

00:44:50,310 --> 00:44:54,750
you can get iterators that and you can

00:44:52,890 --> 00:44:56,790
get references to the contained values

00:44:54,750 --> 00:45:01,050
with the concurrent or normal value map

00:44:56,790 --> 00:45:02,280
you can't get those references and

00:45:01,050 --> 00:45:03,780
everything in all the functions that

00:45:02,280 --> 00:45:06,690
would return a reference instead return

00:45:03,780 --> 00:45:10,800
a value and some of them return an

00:45:06,690 --> 00:45:13,740
optional value because the container

00:45:10,800 --> 00:45:16,440
might have changed under no so the value

00:45:13,740 --> 00:45:21,150
that could have come back comm isn't

00:45:16,440 --> 00:45:22,680
there and it all does simplify things

00:45:21,150 --> 00:45:24,210
because they don't they're not going to

00:45:22,680 --> 00:45:25,770
work with the span of algorithms then

00:45:24,210 --> 00:45:29,100
they could contain some algorithms

00:45:25,770 --> 00:45:31,080
themselves so reduce and for each for

00:45:29,100 --> 00:45:33,840
example as member functions the same way

00:45:31,080 --> 00:45:37,370
that standard list provides sort because

00:45:33,840 --> 00:45:37,370
it doesn't work with the standard sort

00:45:42,320 --> 00:45:47,580
as I said queues are a vital means of

00:45:45,780 --> 00:45:50,610
Internet communication a building block

00:45:47,580 --> 00:45:53,820
for doing communicate Tings sequential

00:45:50,610 --> 00:45:56,130
processes as well as all sorts of other

00:45:53,820 --> 00:46:00,300
just general no inter thread

00:45:56,130 --> 00:46:01,770
communication they may or may not be

00:46:00,300 --> 00:46:03,930
knocked free they might be fixed size

00:46:01,770 --> 00:46:06,090
they might be unbounded now there's a

00:46:03,930 --> 00:46:08,820
whole bunch of properties the proposals

00:46:06,090 --> 00:46:10,050
give you lots of ways of tweaking that

00:46:08,820 --> 00:46:11,070
so you can have lots of different cue

00:46:10,050 --> 00:46:15,690
types with all sorts of different

00:46:11,070 --> 00:46:17,280
properties one nifty feature is the

00:46:15,690 --> 00:46:18,900
ability to close queues because

00:46:17,280 --> 00:46:20,220
obviously knows you've got a bunch of

00:46:18,900 --> 00:46:22,190
work coming through and you can say okay

00:46:20,220 --> 00:46:24,540
I'm done now done sending so then the

00:46:22,190 --> 00:46:27,300
processing thread at the other end could

00:46:24,540 --> 00:46:28,320
say yeah okay if I got more work to do

00:46:27,300 --> 00:46:30,750
no no more work

00:46:28,320 --> 00:46:33,540
yeah okay I can I can finish now and it

00:46:30,750 --> 00:46:36,610
helps you be cleaning up

00:46:33,540 --> 00:46:38,410
the proposal allows offer other than

00:46:36,610 --> 00:46:41,680
that we have at the moment allows you to

00:46:38,410 --> 00:46:43,300
get a handle to just one end of a queue

00:46:41,680 --> 00:46:45,670
so you can have a push handle where you

00:46:43,300 --> 00:46:49,710
can ship stick more stuff and a pop

00:46:45,670 --> 00:46:52,240
handle for taking stuff that's ready

00:46:49,710 --> 00:46:54,010
so rather than passing the cue round

00:46:52,240 --> 00:46:55,600
then you can pass around these and that

00:46:54,010 --> 00:46:56,860
sort of a means of abstracting it so it

00:46:55,600 --> 00:46:59,260
doesn't matter what the concrete type of

00:46:56,860 --> 00:47:03,670
the queue is you can you can pass round

00:46:59,260 --> 00:47:05,350
a a pop handle and you don't have to

00:47:03,670 --> 00:47:08,380
worry about whether that was a pop

00:47:05,350 --> 00:47:10,390
handle to a bound EQ or an unbound EQ or

00:47:08,380 --> 00:47:12,040
a lock for EQ or a lot bass queue it

00:47:10,390 --> 00:47:15,480
doesn't matter from the point of the

00:47:12,040 --> 00:47:17,650
code that's doing the processing and

00:47:15,480 --> 00:47:21,310
then they work with these iterators as

00:47:17,650 --> 00:47:28,660
well so that's there's quite a lot of

00:47:21,310 --> 00:47:30,370
flexibility in that proposal safe

00:47:28,660 --> 00:47:32,920
concurrent stream access at the moment

00:47:30,370 --> 00:47:34,600
if you use the standard output streams

00:47:32,920 --> 00:47:38,200
then it is guaranteed that each

00:47:34,600 --> 00:47:40,540
individual insert is atomic from the

00:47:38,200 --> 00:47:43,090
point of view of the final output but if

00:47:40,540 --> 00:47:45,130
you're doing multiple inserts no insert

00:47:43,090 --> 00:47:47,440
ten twenty thirty years three separate

00:47:45,130 --> 00:47:49,300
inserts and another thread does 40 50 60

00:47:47,440 --> 00:47:51,580
is three separate inserts then the two

00:47:49,300 --> 00:47:55,260
threads can now be interleaved so you

00:47:51,580 --> 00:47:58,690
could get no 10 40 50 20 60 30 and

00:47:55,260 --> 00:48:01,210
obviously the the the 10 is before the

00:47:58,690 --> 00:48:02,740
20 is before 30 and the 40s before the

00:48:01,210 --> 00:48:04,930
50s before the 60 but they've all

00:48:02,740 --> 00:48:08,410
interleaved across the across the two

00:48:04,930 --> 00:48:13,420
threads so it would be nice if we could

00:48:08,410 --> 00:48:15,280
avoid that and those proposals around

00:48:13,420 --> 00:48:19,540
that one of the proposals has basic go

00:48:15,280 --> 00:48:20,830
stream buffer the name of this type and

00:48:19,540 --> 00:48:24,850
the way it's specified keeps changing

00:48:20,830 --> 00:48:26,110
through the proposals so quite how what

00:48:24,850 --> 00:48:27,990
it's going to finally look like by the

00:48:26,110 --> 00:48:30,310
time it gets as far as RTS I'm not sure

00:48:27,990 --> 00:48:32,500
but the whole but the principle is that

00:48:30,310 --> 00:48:36,400
you create some some buffer for that for

00:48:32,500 --> 00:48:38,020
what you're you're saying I want to

00:48:36,400 --> 00:48:40,600
gather the output I'm going to output it

00:48:38,020 --> 00:48:43,180
to standard standard C out and now I can

00:48:40,600 --> 00:48:44,740
do a bunch of inserts and it's open and

00:48:43,180 --> 00:48:47,670
the whole thing is then atomic from the

00:48:44,740 --> 00:48:47,670
point of view of the final output

00:48:53,910 --> 00:48:57,820
it's to be used in conjunction with is

00:48:56,230 --> 00:48:59,710
that does it replace basic Oh stream but

00:48:57,820 --> 00:49:03,810
no of course it doesn't is to be used it

00:48:59,710 --> 00:49:07,120
it will wrap another basic Oh stream so

00:49:03,810 --> 00:49:08,170
so then yeah it's exclusively for this

00:49:07,120 --> 00:49:15,760
purpose rather than a general

00:49:08,170 --> 00:49:17,380
replacement co-routines co-routines

00:49:15,760 --> 00:49:20,650
great they expose a pull interface for

00:49:17,380 --> 00:49:23,470
call back style implementations and and

00:49:20,650 --> 00:49:25,720
reusable function resumable functions is

00:49:23,470 --> 00:49:28,410
a way of generating async calls from

00:49:25,720 --> 00:49:32,620
codes no weak things like the KOA weight

00:49:28,410 --> 00:49:34,510
interface keyword so you stick koa

00:49:32,620 --> 00:49:36,970
weight in and it says this and this

00:49:34,510 --> 00:49:41,490
function is it's going to resume when

00:49:36,970 --> 00:49:43,660
it's when this other task is done and

00:49:41,490 --> 00:49:45,040
there were alternative ways of

00:49:43,660 --> 00:49:47,650
structuring code that does asynchronous

00:49:45,040 --> 00:49:51,580
operations like I said this whole bunch

00:49:47,650 --> 00:49:54,220
of talks on that it has the potential

00:49:51,580 --> 00:49:55,630
for greatly simplifying things if you do

00:49:54,220 --> 00:49:58,300
it wrong it's also got the potential for

00:49:55,630 --> 00:50:02,350
greatly confusing people so make sure

00:49:58,300 --> 00:50:04,300
you don't confuse your your users but

00:50:02,350 --> 00:50:08,320
yeah it it has there's a great great

00:50:04,300 --> 00:50:10,690
potential there and can consider FICO

00:50:08,320 --> 00:50:12,370
quite a lot the boilerplate if you look

00:50:10,690 --> 00:50:15,550
at the boilerplate that's equivalent to

00:50:12,370 --> 00:50:17,470
lots of the the know like the using the

00:50:15,550 --> 00:50:19,180
KOA weight keyword then you can then you

00:50:17,470 --> 00:50:21,010
could say I'm glad I didn't have to

00:50:19,180 --> 00:50:24,640
write all that myself and the compilers

00:50:21,010 --> 00:50:26,800
doing it for me but you could in most

00:50:24,640 --> 00:50:32,710
cases then things are equivalent to no

00:50:26,800 --> 00:50:36,130
span of async tasks and things okay so

00:50:32,710 --> 00:50:39,120
pipelines again like like unix pipelines

00:50:36,130 --> 00:50:43,990
and on the command line and things so

00:50:39,120 --> 00:50:45,730
you have some inputs from a source you

00:50:43,990 --> 00:50:48,330
have a sink where you're sending stuff

00:50:45,730 --> 00:50:51,160
and then you create a pipeline which

00:50:48,330 --> 00:50:53,500
take theta from a source and then does a

00:50:51,160 --> 00:50:57,240
series of processing then dumps it out

00:50:53,500 --> 00:50:57,240
to the final sink

00:50:57,800 --> 00:51:02,870
again listen no these are all proposals

00:51:00,050 --> 00:51:06,520
so the precise syntax and function names

00:51:02,870 --> 00:51:06,520
might change but that's the general idea

00:51:10,960 --> 00:51:16,040
another thing that that's coming up and

00:51:13,340 --> 00:51:18,920
I really really hope he's actually going

00:51:16,040 --> 00:51:23,360
to make it into into the next es is

00:51:18,920 --> 00:51:25,370
hazard pointers and they fundamentally

00:51:23,360 --> 00:51:27,290
help us provide an answer to the

00:51:25,370 --> 00:51:30,920
question when is it safe to delete this

00:51:27,290 --> 00:51:33,320
object if you've got a data structure

00:51:30,920 --> 00:51:36,910
which as is accessed from multiple

00:51:33,320 --> 00:51:39,350
threads and you want know you've got a a

00:51:36,910 --> 00:51:40,970
map or a list or something and you've

00:51:39,350 --> 00:51:42,920
got some some node that you want to

00:51:40,970 --> 00:51:46,490
delete some part of the data structure

00:51:42,920 --> 00:51:47,750
that you want to want to remove then if

00:51:46,490 --> 00:51:49,520
you're running things in a lock free

00:51:47,750 --> 00:51:51,860
fashion then how do you know that

00:51:49,520 --> 00:51:53,960
somebody else isn't accessing it so it's

00:51:51,860 --> 00:51:55,210
safe to delete and has a pointers help

00:51:53,960 --> 00:51:58,910
give us an answer to that question

00:51:55,210 --> 00:52:01,430
because you declare I got a hazard

00:51:58,910 --> 00:52:06,230
pointer pointing to this object before

00:52:01,430 --> 00:52:08,240
you try and use it and then if you want

00:52:06,230 --> 00:52:09,470
to delete it you say has anybody got any

00:52:08,240 --> 00:52:11,000
hazard pointers pointing to this object

00:52:09,470 --> 00:52:13,010
and if the answer is yes then you stick

00:52:11,000 --> 00:52:14,210
it on a deferred reclamation list and if

00:52:13,010 --> 00:52:17,480
the answer is no then you know you can

00:52:14,210 --> 00:52:19,280
delete it right now it's an alternative

00:52:17,480 --> 00:52:21,020
to garbage collection and reference

00:52:19,280 --> 00:52:23,030
counting so rather than using atomic

00:52:21,020 --> 00:52:26,090
shared pointer you can use hazard

00:52:23,030 --> 00:52:27,080
pointers at the point of use it's

00:52:26,090 --> 00:52:28,910
potentially a little bit more

00:52:27,080 --> 00:52:32,960
complicated but the potential for

00:52:28,910 --> 00:52:38,720
performance is much greater so it's a

00:52:32,960 --> 00:52:40,250
trade-off like most things in life say

00:52:38,720 --> 00:52:42,440
yes and this is how it works you've got

00:52:40,250 --> 00:52:46,760
on the using thread you obtain a pointer

00:52:42,440 --> 00:52:49,790
to an object you mark that you're

00:52:46,760 --> 00:52:51,130
accessing ax Xing accessing that object

00:52:49,790 --> 00:52:54,680
X with a hazard pointer

00:52:51,130 --> 00:52:55,910
you check that it that at that point

00:52:54,680 --> 00:52:59,270
you've still got is still valid before

00:52:55,910 --> 00:53:00,770
you actually dereference it you then use

00:52:59,270 --> 00:53:01,940
the point in some fashion you

00:53:00,770 --> 00:53:04,790
dereference it you pass it to a function

00:53:01,940 --> 00:53:08,150
you do whatever then when you're done

00:53:04,790 --> 00:53:11,180
you release the has a pointer and if you

00:53:08,150 --> 00:53:16,160
actually will and

00:53:11,180 --> 00:53:18,950
you if X is marked as marked for

00:53:16,160 --> 00:53:21,319
reclamation and you're on the

00:53:18,950 --> 00:53:23,059
reclamation processing list then you

00:53:21,319 --> 00:53:25,000
delete that object if it's been marked

00:53:23,059 --> 00:53:27,559
so and there are no other has a pointers

00:53:25,000 --> 00:53:29,750
if a thread is actually trying to do the

00:53:27,559 --> 00:53:32,150
deleting deliberately trying to remove

00:53:29,750 --> 00:53:34,099
it from from a data structure then again

00:53:32,150 --> 00:53:36,369
you start by obtaining the pointer you

00:53:34,099 --> 00:53:39,910
mark that it is now to be deleted and

00:53:36,369 --> 00:53:43,250
then you check the hazard pointers and

00:53:39,910 --> 00:53:45,920
if somebody holds a has a pointer then

00:53:43,250 --> 00:53:49,900
you have to put it for reclamation later

00:53:45,920 --> 00:53:49,900
otherwise you can delete it right now

00:53:50,890 --> 00:53:55,579
there are many more proposals not coming

00:53:53,660 --> 00:53:57,140
here if you actually if you want to have

00:53:55,579 --> 00:54:00,079
a look look on the committee website or

00:53:57,140 --> 00:54:02,210
go to the ISO CPP foundation because

00:54:00,079 --> 00:54:04,309
lots of the papers getting published on

00:54:02,210 --> 00:54:06,140
the links get published on there as they

00:54:04,309 --> 00:54:13,579
get released and you can look through

00:54:06,140 --> 00:54:15,619
the archives to - the final papers so

00:54:13,579 --> 00:54:16,309
there's also per a specification for

00:54:15,619 --> 00:54:18,890
parallelism

00:54:16,309 --> 00:54:22,190
we've got ts1 which is now merged into

00:54:18,890 --> 00:54:23,780
c++ 17 got parallel algorithms including

00:54:22,190 --> 00:54:25,400
MapReduce alright well or something

00:54:23,780 --> 00:54:28,040
similar it's called transform reduce

00:54:25,400 --> 00:54:29,540
it's not quite the same spec as no

00:54:28,040 --> 00:54:32,089
people might expect from MapReduce but

00:54:29,540 --> 00:54:34,160
it does essentially the same thing we

00:54:32,089 --> 00:54:36,319
have like way execution agents and the

00:54:34,160 --> 00:54:40,099
potential for Cindy and vector

00:54:36,319 --> 00:54:41,510
algorithms for parallelism TS version -

00:54:40,099 --> 00:54:42,980
the only thing that's concretely added

00:54:41,510 --> 00:54:45,500
to the working paper at the moment is

00:54:42,980 --> 00:54:52,940
task blocks but there's no more work

00:54:45,500 --> 00:54:55,640
being done the c++ 17 library

00:54:52,940 --> 00:54:57,319
incorporates the version 1 TS and that

00:54:55,640 --> 00:54:59,780
essentially provides us with a whole new

00:54:57,319 --> 00:55:01,609
overrides for all this but for most

00:54:59,780 --> 00:55:03,589
let's times YB albums they all have this

00:55:01,609 --> 00:55:07,960
execution policy as the first

00:55:03,589 --> 00:55:10,900
first parameter that execution policy

00:55:07,960 --> 00:55:14,150
can be one of three things at the moment

00:55:10,900 --> 00:55:17,980
sequential which is just the same as a

00:55:14,150 --> 00:55:20,420
normal algorithm standard path which is

00:55:17,980 --> 00:55:22,609
and that was it to be parallel long from

00:55:20,420 --> 00:55:24,740
multiple threads and then par Veck which

00:55:22,609 --> 00:55:25,400
says we'll also take make use of

00:55:24,740 --> 00:55:31,220
vectorize a

00:55:25,400 --> 00:55:32,750
if you can and I said the vast majority

00:55:31,220 --> 00:55:34,430
it's a great big long list I've

00:55:32,750 --> 00:55:37,280
highlighted in bold a couple of ones

00:55:34,430 --> 00:55:39,830
that are relevant to people and for each

00:55:37,280 --> 00:55:42,170
is good general one but also merge for

00:55:39,830 --> 00:55:43,880
urging things we'll find and copy and

00:55:42,170 --> 00:55:47,420
counts transform reduce down at the

00:55:43,880 --> 00:55:54,080
bottom now sort most of the algorithms

00:55:47,420 --> 00:55:57,890
of that task blocks from Powell ISM to

00:55:54,080 --> 00:56:04,150
you t2 are about managing hierarchies of

00:55:57,890 --> 00:56:06,440
tasks so you can nest task box ND and

00:56:04,150 --> 00:56:08,770
nested task box can wrap in parallel

00:56:06,440 --> 00:56:10,850
it's sort of fork/join parallelism so

00:56:08,770 --> 00:56:12,680
because all the nested task blocks

00:56:10,850 --> 00:56:15,080
within task region are complete when it

00:56:12,680 --> 00:56:16,880
exits so that's no so you you cradle

00:56:15,080 --> 00:56:18,620
your nested task box and then when you

00:56:16,880 --> 00:56:21,770
exit the block then all those parallel

00:56:18,620 --> 00:56:25,190
tasks are joined we have that fourth

00:56:21,770 --> 00:56:26,750
line fork/join parallelism which is

00:56:25,190 --> 00:56:28,280
great for something so if a recursive

00:56:26,750 --> 00:56:30,230
divide and conquer through four out

00:56:28,280 --> 00:56:36,140
through us through a system then then

00:56:30,230 --> 00:56:40,100
that's great there's also the

00:56:36,140 --> 00:56:41,990
transactional memory TS two basic types

00:56:40,100 --> 00:56:45,590
of transaction blocks synchronized

00:56:41,990 --> 00:56:47,120
blocks and atomic clocks and the

00:56:45,590 --> 00:56:50,780
synchronized blocks are sort of

00:56:47,120 --> 00:56:54,710
equivalent to a know as if there was a

00:56:50,780 --> 00:56:57,890
mutex around it but it sort of makes it

00:56:54,710 --> 00:56:59,600
work so there isn't atomic blocks are

00:56:57,890 --> 00:57:00,770
lower-level potentially and then you

00:56:59,600 --> 00:57:01,700
have to deal with Possible's of

00:57:00,770 --> 00:57:03,200
exceptions and whether they

00:57:01,700 --> 00:57:09,200
automatically commit or automatically

00:57:03,200 --> 00:57:11,060
cancel so you introduced them you just

00:57:09,200 --> 00:57:12,320
put the key appropriate keyword before a

00:57:11,060 --> 00:57:15,890
block and mady's now part of the

00:57:12,320 --> 00:57:17,600
transaction it behaves as if it's a

00:57:15,890 --> 00:57:23,990
global if there's a global mutex on the

00:57:17,600 --> 00:57:25,460
synchronized block atomic box executes

00:57:23,990 --> 00:57:27,620
atomically and not concurrently with

00:57:25,460 --> 00:57:29,450
synchronized blocks again so you stick

00:57:27,620 --> 00:57:31,460
your neck your keyword on and declare

00:57:29,450 --> 00:57:33,580
the block is very very straightforward

00:57:31,460 --> 00:57:33,580
to use

00:57:35,220 --> 00:57:39,359
but atomic box a lower level so they can

00:57:37,859 --> 00:57:42,330
potentially execute concurrently with

00:57:39,359 --> 00:57:45,540
each other if they don't share this I

00:57:42,330 --> 00:57:47,010
use the same variables and they differ

00:57:45,540 --> 00:57:48,660
in their behavior with exceptions so

00:57:47,010 --> 00:57:50,880
obviously no except you're saying there

00:57:48,660 --> 00:57:53,250
won't be any exceptions so it's

00:57:50,880 --> 00:57:54,840
undefined behavior and then you can

00:57:53,250 --> 00:57:58,080
either commit or cancel when the

00:57:54,840 --> 00:58:01,109
exception it escapes by the even on

00:57:58,080 --> 00:58:02,580
those cases if you're trying to if the

00:58:01,109 --> 00:58:04,950
exception is get is allowed to escape

00:58:02,580 --> 00:58:06,810
for a cancel then it must be a special

00:58:04,950 --> 00:58:08,640
type of exception that transaction safe

00:58:06,810 --> 00:58:09,750
because otherwise the cancel transaction

00:58:08,640 --> 00:58:11,420
will roll back the creation of the

00:58:09,750 --> 00:58:18,090
exception that was going to be thrown so

00:58:11,420 --> 00:58:20,150
you don't want that okay so I reckon

00:58:18,090 --> 00:58:24,980
there's about two minutes for questions

00:58:20,150 --> 00:58:24,980
anybody got any yes

00:58:53,650 --> 00:58:59,799
okay so so the question is in obviously

00:58:58,359 --> 00:59:00,940
the operating systems provide there's a

00:58:59,799 --> 00:59:02,740
whole bunch of things that we could

00:59:00,940 --> 00:59:03,999
potentially wait for so we could wait

00:59:02,740 --> 00:59:06,039
for things coming from a socket we can

00:59:03,999 --> 00:59:07,900
wait for things coming from from a file

00:59:06,039 --> 00:59:10,960
and all sorts of stuff and so is there

00:59:07,900 --> 00:59:14,980
any means of in standard C++ of

00:59:10,960 --> 00:59:17,230
capturing that and I think I have to say

00:59:14,980 --> 00:59:18,940
no I think you look at the various TS is

00:59:17,230 --> 00:59:19,930
so like if you're looking at waiting for

00:59:18,940 --> 00:59:21,789
the network then you look at the

00:59:19,930 --> 00:59:24,160
networking TS and use their facilities

00:59:21,789 --> 00:59:27,339
which will wrap the OS and likewise if

00:59:24,160 --> 00:59:30,640
you'll know but there isn't a general

00:59:27,339 --> 00:59:41,349
general framework for capturing OS wait

00:59:30,640 --> 00:59:43,210
of all things would you be able to use

00:59:41,349 --> 00:59:45,880
networking with the futures mmm

00:59:43,210 --> 00:59:48,569
probably at the moment no but but but I

00:59:45,880 --> 00:59:50,799
I imagine that that sort of integration

00:59:48,569 --> 00:59:53,289
when things all get everything gets

00:59:50,799 --> 00:59:54,549
moved into the C++ standard then that

00:59:53,289 --> 00:59:56,200
would be something that we were looking

00:59:54,549 --> 00:59:58,989
as the integration between the different

00:59:56,200 --> 01:00:02,369
parts but not a moment they're different

00:59:58,989 --> 01:00:02,369
yeses so they don't talk to each other

01:00:03,630 --> 01:00:06,849
yes

01:00:05,019 --> 01:00:09,039
first I would like to point out that

01:00:06,849 --> 01:00:12,210
make exceptional future is probably the

01:00:09,039 --> 01:00:15,519
coolest name in STL so please keep it

01:00:12,210 --> 01:00:19,119
and my question is regarding shared time

01:00:15,519 --> 01:00:20,980
mutex okay I'm not sure I get it

01:00:19,119 --> 01:00:22,839
correctly but so when you request a

01:00:20,980 --> 01:00:24,819
write lock you still don't get it

01:00:22,839 --> 01:00:27,249
because there are some read locks on it

01:00:24,819 --> 01:00:29,410
but you're requesting it so you're

01:00:27,249 --> 01:00:33,400
waiting for for the read locks to to

01:00:29,410 --> 01:00:35,440
finish does this this mere fact prevent

01:00:33,400 --> 01:00:38,559
newer read locks to be created already

01:00:35,440 --> 01:00:41,549
or or can it be a starvation that you

01:00:38,559 --> 01:00:44,039
know you just keep waiting and waiting

01:00:41,549 --> 01:00:49,150
that depends on the implementation and

01:00:44,039 --> 01:00:50,559
obviously avoiding starvation is a key

01:00:49,150 --> 01:00:54,099
feature that implementers are going to

01:00:50,559 --> 01:00:55,749
want to do there are techniques to do to

01:00:54,099 --> 01:00:58,539
avoid exactly what you you said so that

01:00:55,749 --> 01:01:00,970
if a waiter is waiting then further

01:00:58,539 --> 01:01:05,319
readers cannot acquire the lock and they

01:01:00,970 --> 01:01:06,410
have to wait too so you can implement it

01:01:05,319 --> 01:01:08,060
like that but

01:01:06,410 --> 01:01:10,640
sometimes there are reasons why you

01:01:08,060 --> 01:01:12,500
might not want to you might want your

01:01:10,640 --> 01:01:15,020
readers to starve or your waiters to

01:01:12,500 --> 01:01:16,370
starve because of the way that your

01:01:15,020 --> 01:01:18,730
system is structured and you know that

01:01:16,370 --> 01:01:23,120
eventually it will get to the point and

01:01:18,730 --> 01:01:25,250
no so there is no wings measure to do

01:01:23,120 --> 01:01:27,470
specify there is no there is there's no

01:01:25,250 --> 01:01:30,380
no way to specify you have to look at

01:01:27,470 --> 01:01:32,360
what you're implemented us and if you've

01:01:30,380 --> 01:01:33,350
got a choice of implementations you can

01:01:32,360 --> 01:01:35,780
use choose the one that has the

01:01:33,350 --> 01:01:36,860
properties you want otherwise you have

01:01:35,780 --> 01:01:41,380
to hope its quality of implementation

01:01:36,860 --> 01:01:41,380
and not specified Thanks

01:01:42,770 --> 01:01:47,330
so when an exception gets propagated

01:01:45,320 --> 01:01:49,700
from threat to threat when you're using

01:01:47,330 --> 01:01:52,700
futures is it exactly the same exception

01:01:49,700 --> 01:01:55,310
with the same type or is it some sort of

01:01:52,700 --> 01:01:57,770
a wrapping object okay it will

01:01:55,310 --> 01:02:01,220
definitely have the same type I will say

01:01:57,770 --> 01:02:04,070
that and on some platforms it will be

01:02:01,220 --> 01:02:07,370
really really the same object that is

01:02:04,070 --> 01:02:09,650
wreath Rohn you know I think GCC does

01:02:07,370 --> 01:02:12,080
that on Linux it references counts the

01:02:09,650 --> 01:02:14,060
exceptions so you catch an exception you

01:02:12,080 --> 01:02:17,120
stick it in the future you then call get

01:02:14,060 --> 01:02:19,580
and it wreath rose the same object on

01:02:17,120 --> 01:02:24,350
other platforms like say Visual Studio

01:02:19,580 --> 01:02:27,080
on Windows then it will use internal

01:02:24,350 --> 01:02:28,670
stuff to copy the exception so when you

01:02:27,080 --> 01:02:32,450
so when the exception is wreath thrown

01:02:28,670 --> 01:02:36,920
it is a copy of the same type but it's

01:02:32,450 --> 01:02:40,100
it will always have the same type though

01:02:36,920 --> 01:02:42,320
when we're when any return the thread

01:02:40,100 --> 01:02:44,720
that triggered it or does it just return

01:02:42,320 --> 01:02:50,390
a thread that's ready at the time that

01:02:44,720 --> 01:02:53,900
after it was triggered okay so yeah so

01:02:50,390 --> 01:02:55,790
when any it will yes the the result will

01:02:53,900 --> 01:02:57,110
have a flag that says which of the

01:02:55,790 --> 01:02:59,060
futures it was that triggered it and

01:02:57,110 --> 01:03:01,820
then it returns all of them to you so

01:02:59,060 --> 01:03:03,290
you you know which one was the one that

01:03:01,820 --> 01:03:04,820
triggered but then more than that might

01:03:03,290 --> 01:03:09,700
be ready but by the time you look at the

01:03:04,820 --> 01:03:09,700
result stop aren't you a question yes

01:03:11,610 --> 01:03:16,860
have a really quick question about the

01:03:13,200 --> 01:03:21,570
when any also is there any expectation

01:03:16,860 --> 01:03:25,140
of fairness in terms of like if there's

01:03:21,570 --> 01:03:27,150
multiple things waiting is there some

01:03:25,140 --> 01:03:33,180
expectation that these things will be

01:03:27,150 --> 01:03:35,340
kind of generally fair not over and

01:03:33,180 --> 01:03:37,980
above anything that your RS provides so

01:03:35,340 --> 01:03:40,680
it's probably going to Europe use the

01:03:37,980 --> 01:03:42,900
underlying OS weighting facilities so no

01:03:40,680 --> 01:03:44,310
on Windows for example your problem

01:03:42,900 --> 01:03:47,640
might end up using the equivalent of

01:03:44,310 --> 01:03:48,990
wait for multiple objects and so it

01:03:47,640 --> 01:03:55,920
doesn't provide you any more fairness

01:03:48,990 --> 01:03:57,870
than that would okay thanks yes when you

01:03:55,920 --> 01:03:58,290
called get and an exception is wreath

01:03:57,870 --> 01:04:00,270
rowned

01:03:58,290 --> 01:04:02,190
do you still have the original back

01:04:00,270 --> 01:04:07,170
trace from the original throw point or

01:04:02,190 --> 01:04:09,270
is it wreath rone from get I would I

01:04:07,170 --> 01:04:12,630
would say that probably depends entirely

01:04:09,270 --> 01:04:14,520
on your implementation with GCC it's

01:04:12,630 --> 01:04:16,500
possible because it's the same object it

01:04:14,520 --> 01:04:19,290
might still have the same back trace

01:04:16,500 --> 01:04:20,940
available I haven't checked but if it's

01:04:19,290 --> 01:04:22,740
a if it's a fresh object because it's a

01:04:20,940 --> 01:04:24,540
copy but then it definitely won't have

01:04:22,740 --> 01:04:30,780
unless you did something to capture it

01:04:24,540 --> 01:04:32,490
in your exception yourself manually very

01:04:30,780 --> 01:04:36,140
simple question how come nobody

01:04:32,490 --> 01:04:36,140
implemented or proposed a semaphore

01:04:38,060 --> 01:04:48,240
because they didn't yeah and the the the

01:04:45,120 --> 01:04:51,930
C++ c11 library was based on boost which

01:04:48,240 --> 01:04:53,490
didn't have a semaphore and nobody's

01:04:51,930 --> 01:04:55,830
bothered write proposal for to have one

01:04:53,490 --> 01:05:03,240
since so that's the fundamental reason

01:04:55,830 --> 01:05:05,300
there's no proposal for it okay so it

01:05:03,240 --> 01:05:07,310
looks like we have no more questions

01:05:05,300 --> 01:05:09,370
yeah

01:05:07,310 --> 01:05:09,370

YouTube URL: https://www.youtube.com/watch?v=FaHJOkOrfNo


