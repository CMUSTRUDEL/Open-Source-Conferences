Title: CppCon 2016: Gordon Brown & Michael Wong “Towards Heterogeneous Programming in C++"
Publication date: 2016-10-01
Playlist: CppCon 2016
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2016
—
Current semiconductor trends show a major shift in computer system architectures towards heterogeneous systems that combine a CPU with other processors such as GPUs, DSPs and FPGAs; that work together, performing different tasks in parallel. This shift has brought a dramatic change in programming paradigms in the pursuit of a common language that will provide efficiency and performance portability across these systems. A wide range of programming languages and models has emerged over the last decade, all with this goal in their sights, and the trend is converging on C++ as the language of choice.

This new world of heterogeneous systems brings with it many new challenges for the C++ community and for the language itself, and if these were to be overcome then C++ would have the potential to standardize the future of heterogeneous programming. In fact this work has already begun in SG14 in form of a mandate to support massive parallel dispatch for heterogeneous devices in the C++ standard using recent models such as SYCL and HPX. A recent approach to solving these challenges comes in the form of SYCL, a shared-source C++ programming model for OpenCL. SYCL takes a very different approach from many before it, in that it is specifically designed to be standard C++ without any extensions to the language.

This talk will present the recent rise in demand for heterogeneous programming in C++ and the new challenges that this brings to the C++ community. It will then take a look at some of the different programming languages and models that have arrived in the heterogeneous landscape, the motivations behind them, how they have attempted to solve those challenges and what we can learn from them. Finally it will take a look at the future of heterogeneous programming in C++ and what that might look like.
— 
Gordon Brown
Staff Software Engineer, SYCL, Codeplay Software
Gordon Brown is a software engineer specializing in SYCL technologies and has been involved in the standardization of the Khronos standard and the development of Codeplay's implementation from it's inception.

Michael Wong
Codeplay Software/ISOCPP
VP of R&D/Director
Michael Wong is the CEO of OpenMP. He is the Canadian representative to the C++ Standard and OpenMP Committee. He is also a Director of ISOCPP.org and a VP, Vice-Chair of Programming Languages for Canada’s Standard Council. He has so many titles, it’s a wonder he can get anything done.
He chairs the WG21 SG5 Transactional Memory and SG14 Games Development/Low Latency, and is the co-author of a number C++11/OpenMP/Transactional Memory features including generalized attributes, user-defined literals, inheriting constructors, weakly ordered memory models, and explicit conversion operators. Having been the past C++ team lead to IBM’s XL C++ compiler means he has been messing around with designing C++ compilers for twenty years. His current research interest, i.e. what he would like to do if he had time is in the area of parallel programming, transactional memory, C++ benchmark performance, object model, generic programming and template metaprogramming. He holds a B.Sc from University of Toronto, and a Masters in Mathematics from University of Waterloo.

He has been asked to speak at ACCU, C++Now, Meeting C++, ADC++, CASCON, Bloomberg, CERN, and many Universities, research centers and companies.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,000 --> 00:00:04,170
hey hey hey everyone yeah thanks thanks

00:00:01,680 --> 00:00:06,509
for coming my name is Gordon Brennan I'm

00:00:04,170 --> 00:00:09,870
a software engineer at Coldplay software

00:00:06,509 --> 00:00:11,490
hi I'm here with our VP of R&D Michael

00:00:09,870 --> 00:00:15,929
Wong and we're going to talk about

00:00:11,490 --> 00:00:18,210
heterogeneous programming in C++ so

00:00:15,929 --> 00:00:20,460
first of all the acknowledgment

00:00:18,210 --> 00:00:22,380
disclaimer oh the content for these

00:00:20,460 --> 00:00:24,630
slides has been contributed by other

00:00:22,380 --> 00:00:27,230
members of play as well as the standards

00:00:24,630 --> 00:00:29,490
groups like the C++ Crippen and Chronos

00:00:27,230 --> 00:00:31,489
obviously any any mistakes there they're

00:00:29,490 --> 00:00:34,200
obviously mine

00:00:31,489 --> 00:00:36,210
now obviously legal disclaimer that

00:00:34,200 --> 00:00:37,950
these are these are the views of myself

00:00:36,210 --> 00:00:43,649
not necessarily of called play in

00:00:37,950 --> 00:00:45,930
general so so who are who are we who are

00:00:43,649 --> 00:00:48,870
could place who could play our primarily

00:00:45,930 --> 00:00:51,480
involved in developing solutions for

00:00:48,870 --> 00:00:53,610
heterogeneous systems or we're involved

00:00:51,480 --> 00:00:57,090
in in many different standards bodies

00:00:53,610 --> 00:00:58,920
including Kronos HSA and the C post

00:00:57,090 --> 00:01:02,190
finest work we're also recently we've

00:00:58,920 --> 00:01:04,320
also been involved in a standards for

00:01:02,190 --> 00:01:12,960
safety critical applications such as the

00:01:04,320 --> 00:01:15,600
ISO 26262 and EDS we also have a suite

00:01:12,960 --> 00:01:17,850
of for for developing heterogeneous

00:01:15,600 --> 00:01:20,340
solutions called compute suite so this

00:01:17,850 --> 00:01:22,950
has a computer which is our sort of core

00:01:20,340 --> 00:01:24,930
technology which targets are a wide

00:01:22,950 --> 00:01:27,000
range of of different heterogeneous

00:01:24,930 --> 00:01:29,070
systems and can support our many

00:01:27,000 --> 00:01:31,860
different standards and we have compute

00:01:29,070 --> 00:01:33,360
CPP which is our implementation of the

00:01:31,860 --> 00:01:38,070
the seco standard which I'm going to

00:01:33,360 --> 00:01:39,869
talk about but later on let's talk so so

00:01:38,070 --> 00:01:40,320
first of all why am I here to talking to

00:01:39,869 --> 00:01:43,229
you today

00:01:40,320 --> 00:01:45,689
so as of the the CEO so standards

00:01:43,229 --> 00:01:47,970
meeting in Jacksonville February this

00:01:45,689 --> 00:01:52,380
year we we now have a mandate to bring

00:01:47,970 --> 00:01:54,810
head range computing to C++ so what is

00:01:52,380 --> 00:01:56,640
hedging is computing so hedge computing

00:01:54,810 --> 00:01:58,350
involves gaining performance through the

00:01:56,640 --> 00:01:59,939
utilisation of systems which make use of

00:01:58,350 --> 00:02:01,619
more than one kind of processor each

00:01:59,939 --> 00:02:03,810
with specialized processing capabilities

00:02:01,619 --> 00:02:06,540
to handle particular tasks so it's all

00:02:03,810 --> 00:02:08,819
about gaining parallelism over Britain

00:02:06,540 --> 00:02:12,030
tasks on hardware this is the same

00:02:08,819 --> 00:02:13,550
specifically for so why should you be

00:02:12,030 --> 00:02:16,670
interested so

00:02:13,550 --> 00:02:19,330
Hedges computing is is everywhere it's

00:02:16,670 --> 00:02:22,490
driving the technology of the future yes

00:02:19,330 --> 00:02:24,500
it's been used for a lot of things such

00:02:22,490 --> 00:02:27,440
as image processing machine learning you

00:02:24,500 --> 00:02:31,220
can see in self-driving cars in drones

00:02:27,440 --> 00:02:33,320
and speech recognition animation medical

00:02:31,220 --> 00:02:36,680
imaging and telecommunications and many

00:02:33,320 --> 00:02:39,680
others so what I'd like you to take away

00:02:36,680 --> 00:02:41,510
from today's talk is a sort of a vision

00:02:39,680 --> 00:02:45,200
of what the future of Hedges can be an

00:02:41,510 --> 00:02:47,330
NC plus course is going to look like so

00:02:45,200 --> 00:02:49,220
an overview of what I'm going to talk

00:02:47,330 --> 00:02:51,020
about today so I'm gonna start off with

00:02:49,220 --> 00:02:53,660
a sort of a brief histories or where we

00:02:51,020 --> 00:02:55,180
are and how we got here then I'm gonna

00:02:53,660 --> 00:02:56,840
start I talked about C++ as a

00:02:55,180 --> 00:02:59,000
programming language for a change

00:02:56,840 --> 00:03:01,130
computing and then gonna sort of look at

00:02:59,000 --> 00:03:02,480
some of the the major challenges we face

00:03:01,130 --> 00:03:04,310
in hedging use computing and some of the

00:03:02,480 --> 00:03:07,810
solutions for them and then I'm gonna

00:03:04,310 --> 00:03:11,000
look at cycle so a new standard for a

00:03:07,810 --> 00:03:13,870
programming heterogeneous aim for hedgy

00:03:11,000 --> 00:03:16,120
news programming in C++ and some of the

00:03:13,870 --> 00:03:18,280
approaches that takes then I'm gonna

00:03:16,120 --> 00:03:20,480
omit Michaels going to close off with

00:03:18,280 --> 00:03:22,100
top of the looking at the future of

00:03:20,480 --> 00:03:35,390
heterogeneous programming in C++ and

00:03:22,100 --> 00:03:40,880
where the standard is going so to start

00:03:35,390 --> 00:03:42,920
with who we got here so back in in 1989

00:03:40,880 --> 00:03:45,560
we were at this point called so if you

00:03:42,920 --> 00:03:47,570
could sort of you can refer to is there

00:03:45,560 --> 00:03:49,220
the single Cordillera so this was a

00:03:47,570 --> 00:03:51,590
point where we were gaining steady

00:03:49,220 --> 00:03:55,100
performance schemes and in CPUs based on

00:03:51,590 --> 00:03:57,800
on miroslaw as a principle that if so if

00:03:55,100 --> 00:04:00,170
every year the the CPUs would have more

00:03:57,800 --> 00:04:03,050
and more transistors which would give us

00:04:00,170 --> 00:04:06,350
a V performance gains this is often

00:04:03,050 --> 00:04:08,330
referred to as the the free lunch the

00:04:06,350 --> 00:04:10,910
earliest sign of a heterogeneous

00:04:08,330 --> 00:04:12,500
computing was is back in 2001 where the

00:04:10,910 --> 00:04:15,050
the introduction of the first user

00:04:12,500 --> 00:04:20,959
programmable shaders allowing you to do

00:04:15,050 --> 00:04:23,810
general-purpose computer on GPUs in 2005

00:04:20,959 --> 00:04:25,880
the the the free lunch was over the sort

00:04:23,810 --> 00:04:29,300
of the the performance game

00:04:25,880 --> 00:04:32,720
from from miroslaw stop a wall where she

00:04:29,300 --> 00:04:34,010
performed CPU performance stopped CPUs

00:04:32,720 --> 00:04:36,290
stopped getting faster and this

00:04:34,010 --> 00:04:38,470
performance game stopped so where to

00:04:36,290 --> 00:04:41,000
look to new ways to gain performance

00:04:38,470 --> 00:04:43,790
shortly after this Intel announced the

00:04:41,000 --> 00:04:45,170
first range of dual-core CPU so CPUs

00:04:43,790 --> 00:04:47,270
that would gain performance through

00:04:45,170 --> 00:04:50,300
having multiple cores on on the same

00:04:47,270 --> 00:04:52,970
chip that would run in parallel and

00:04:50,300 --> 00:04:55,700
around her in 2007 this is quite hard to

00:04:52,970 --> 00:04:58,520
have an exact time for this but there's

00:04:55,700 --> 00:05:01,340
there's a point where I came evident

00:04:58,520 --> 00:05:03,110
that the performance gains from adding

00:05:01,340 --> 00:05:04,880
adding multiple cores to a CPU adding

00:05:03,110 --> 00:05:06,770
more cores wasn't giving us the same

00:05:04,880 --> 00:05:09,290
performance gains as we did with Moore's

00:05:06,770 --> 00:05:11,180
Law and although there are some

00:05:09,290 --> 00:05:13,340
applications that take that they take

00:05:11,180 --> 00:05:15,200
advantage of having more coarse and as

00:05:13,340 --> 00:05:18,770
there's architectures that have a large

00:05:15,200 --> 00:05:20,810
number of course they generally the for

00:05:18,770 --> 00:05:21,920
course was the optimal and beyond that

00:05:20,810 --> 00:05:26,420
we're starting to see our diminishing

00:05:21,920 --> 00:05:29,590
returns shortly after this a Kudo was

00:05:26,420 --> 00:05:33,920
released so initially as a CC API for

00:05:29,590 --> 00:05:36,740
offloading code to NVIDIA GPUs and then

00:05:33,920 --> 00:05:39,830
2008 we had open CL which is very very

00:05:36,740 --> 00:05:41,450
similar but it was open to a CPUs and

00:05:39,830 --> 00:05:44,870
GPUs and it was wasn't restricted to

00:05:41,450 --> 00:05:47,900
just in video this was open standard and

00:05:44,870 --> 00:05:50,020
then in 2009 with DirectX 11 was

00:05:47,900 --> 00:05:51,980
released with Derek computes this was

00:05:50,020 --> 00:05:52,730
shaders that she could do

00:05:51,980 --> 00:05:54,860
general-purpose

00:05:52,730 --> 00:05:59,630
computations this is this is generally

00:05:54,860 --> 00:06:01,520
used for four games in 2011 in DNS the

00:05:59,630 --> 00:06:03,620
first range of their ap use so these

00:06:01,520 --> 00:06:06,560
were devices where rather than having a

00:06:03,620 --> 00:06:08,900
CPU in a discrete GPU you would have the

00:06:06,560 --> 00:06:11,060
CPU and GPU on the same work with shared

00:06:08,900 --> 00:06:15,950
memory has changed the way that you have

00:06:11,060 --> 00:06:17,780
to program them clear in 2011 open ECC

00:06:15,950 --> 00:06:20,090
was released so opening SEC was

00:06:17,780 --> 00:06:21,470
originally branched off from openmp in

00:06:20,090 --> 00:06:23,600
order to support offloading to

00:06:21,470 --> 00:06:26,980
accelerators so the time openmp only

00:06:23,600 --> 00:06:30,410
supported a multi-core CPU parallelism

00:06:26,980 --> 00:06:33,490
and then in in 2010 I all Tara and I'm

00:06:30,410 --> 00:06:37,250
just open CL support for their FPGAs and

00:06:33,490 --> 00:06:39,740
2013 open MP also announced support for

00:06:37,250 --> 00:06:42,380
offloading to accelerators

00:06:39,740 --> 00:06:44,180
2015 Texas Instruments and Oh support of

00:06:42,380 --> 00:06:46,370
open sale support for their DSPs and

00:06:44,180 --> 00:06:48,590
then in 2016

00:06:46,370 --> 00:06:51,470
recently each the HSC standard was

00:06:48,590 --> 00:06:54,350
ratified so HSC sign is quite similar to

00:06:51,470 --> 00:06:55,880
open clas a lot more lower level it's

00:06:54,350 --> 00:07:00,220
more focused on defining the

00:06:55,880 --> 00:07:00,220
requirements for heterogeneous devices

00:07:07,030 --> 00:07:16,220
so so so the question so what is the

00:07:14,480 --> 00:07:18,860
most popular language for at Eugene is

00:07:16,220 --> 00:07:21,320
computing and there so I tried to answer

00:07:18,860 --> 00:07:23,750
this question and so to do this sort of

00:07:21,320 --> 00:07:26,000
my myself and some some college came up

00:07:23,750 --> 00:07:29,210
with a list of heterogeneous programming

00:07:26,000 --> 00:07:30,440
languages and models and there so the

00:07:29,210 --> 00:07:32,870
sort of the criteria for this would be

00:07:30,440 --> 00:07:34,790
any any language or moral as I like a

00:07:32,870 --> 00:07:38,080
language in itself is not a wrapper over

00:07:34,790 --> 00:07:40,850
another language and it would have to

00:07:38,080 --> 00:07:43,490
support more aim heterogenous devices

00:07:40,850 --> 00:07:46,880
not just multi-core CPUs so we took

00:07:43,490 --> 00:07:48,020
these and we the release the dates that

00:07:46,880 --> 00:07:49,640
these were released and we map them out

00:07:48,020 --> 00:07:52,970
over so a year by year how many

00:07:49,640 --> 00:07:54,980
languages were available we came up with

00:07:52,970 --> 00:07:57,860
this graph and this is this isn't a

00:07:54,980 --> 00:08:00,140
definitive representation it's a cross

00:07:57,860 --> 00:08:02,870
sample of the languages and models that

00:08:00,140 --> 00:08:03,890
we that we could think of so there's I'm

00:08:02,870 --> 00:08:06,100
sure there's there some that aren't in

00:08:03,890 --> 00:08:09,590
here but this is a representation of of

00:08:06,100 --> 00:08:12,020
the trend of programming models and

00:08:09,590 --> 00:08:14,030
languages for a teams computer the thing

00:08:12,020 --> 00:08:15,500
we see here is that from our in 2003

00:08:14,030 --> 00:08:16,970
onwards there's been a steady gain and

00:08:15,500 --> 00:08:18,740
in programming languages for

00:08:16,970 --> 00:08:20,920
heterogeneous computing particularly in

00:08:18,740 --> 00:08:23,420
C and C++

00:08:20,920 --> 00:08:25,910
this is this another side with you with

00:08:23,420 --> 00:08:29,780
the same data the thing we see here is

00:08:25,910 --> 00:08:32,660
that even as far back as 1999 the C and

00:08:29,780 --> 00:08:34,850
C++ where the dominant languages for

00:08:32,660 --> 00:08:37,130
heterogeneous computing so for a long

00:08:34,850 --> 00:08:39,530
time C was the most dominant language

00:08:37,130 --> 00:08:41,660
harbor in recent years C++ has rapidly

00:08:39,530 --> 00:08:43,850
overtaken it the other thing to notice

00:08:41,660 --> 00:08:47,000
is that in the last few years while C++

00:08:43,850 --> 00:08:49,070
has been so wrapped or taking the other

00:08:47,000 --> 00:08:51,020
languages other languages have become

00:08:49,070 --> 00:08:54,710
sort of plateaued and if they've not

00:08:51,020 --> 00:08:58,380
been increasing as much

00:08:54,710 --> 00:09:00,300
so the next thing I'd like to do is look

00:08:58,380 --> 00:09:03,680
at some of the the major challenges we

00:09:00,300 --> 00:09:06,750
face and head to G's computer and so

00:09:03,680 --> 00:09:08,220
these are the the four four majors those

00:09:06,750 --> 00:09:10,100
challenges I want to focus on there's

00:09:08,220 --> 00:09:11,880
there are other other things that are

00:09:10,100 --> 00:09:13,220
important for a genius computing but

00:09:11,880 --> 00:09:16,260
these are the ones I want to focus on

00:09:13,220 --> 00:09:19,500
and so first of all performance

00:09:16,260 --> 00:09:21,990
portability so we think of performance

00:09:19,500 --> 00:09:24,990
portability we think of writing code

00:09:21,990 --> 00:09:26,430
once and then running it everywhere so

00:09:24,990 --> 00:09:29,730
it's performance portability across

00:09:26,430 --> 00:09:31,620
Hetal use devices really possible and

00:09:29,730 --> 00:09:35,520
the answer is it depends on how you look

00:09:31,620 --> 00:09:37,470
at it so the heterogeneous landscape has

00:09:35,520 --> 00:09:41,310
a wide range of different devices now

00:09:37,470 --> 00:09:45,150
there's accelerator CPUs GPUs DSPs AP

00:09:41,310 --> 00:09:47,370
use FPGAs and there's many others and so

00:09:45,150 --> 00:09:51,320
to look at some of these all closer

00:09:47,370 --> 00:09:53,610
so a typical CPU architecture you have

00:09:51,320 --> 00:09:55,440
so you have a small a small number of

00:09:53,610 --> 00:09:56,760
cores that are running separate

00:09:55,440 --> 00:09:59,010
instructions on each each core

00:09:56,760 --> 00:10:00,600
independently and it's this low

00:09:59,010 --> 00:10:02,370
bandwidth memory ranked random access

00:10:00,600 --> 00:10:05,760
and then you have is this is generally

00:10:02,370 --> 00:10:06,990
suggested for task parallelism and then

00:10:05,760 --> 00:10:09,720
if you look at a typical GPU

00:10:06,990 --> 00:10:12,270
architecture this is uh you have a large

00:10:09,720 --> 00:10:15,450
number of of execution units with a

00:10:12,270 --> 00:10:16,860
single instruction running on a number

00:10:15,450 --> 00:10:20,040
of execution units and this is this is

00:10:16,860 --> 00:10:21,630
generally in lockstep GPUs have a more

00:10:20,040 --> 00:10:23,610
hierarchical memory structure so you

00:10:21,630 --> 00:10:24,930
have multiple different regions of

00:10:23,610 --> 00:10:28,140
memory with different affinity to the

00:10:24,930 --> 00:10:30,090
computation and then you have the GPS of

00:10:28,140 --> 00:10:33,570
a higher bandwidth bandwidth memory and

00:10:30,090 --> 00:10:35,220
have a more predictable memory so and

00:10:33,570 --> 00:10:38,640
then and these GPUs are generally

00:10:35,220 --> 00:10:40,350
suggested for data parallelism if we

00:10:38,640 --> 00:10:42,000
look at FPGA is a typical FPGA

00:10:40,350 --> 00:10:45,180
architectures and if FPGAs are very

00:10:42,000 --> 00:10:47,760
different from from CPUs or GPUs the

00:10:45,180 --> 00:10:49,620
main the main difference is that whereas

00:10:47,760 --> 00:10:51,330
whether CPU a GPU would copy your code

00:10:49,620 --> 00:10:51,690
over to the device in order to execute

00:10:51,330 --> 00:10:53,790
it

00:10:51,690 --> 00:10:55,530
when FPGAs are a reprogrammable at

00:10:53,790 --> 00:10:58,050
runtime so they could the code that you

00:10:55,530 --> 00:11:00,900
execute is actually synthesized onto the

00:10:58,050 --> 00:11:02,940
hardware itself and this is done through

00:11:00,900 --> 00:11:05,070
having these configurable logic blocks

00:11:02,940 --> 00:11:06,630
that are used for both memory and

00:11:05,070 --> 00:11:08,190
computation and these are

00:11:06,630 --> 00:11:09,390
connected through configurable ratings

00:11:08,190 --> 00:11:12,420
that provide different levels of

00:11:09,390 --> 00:11:15,150
bandwidth fpg is a very low power

00:11:12,420 --> 00:11:18,180
consumption and they have a sort of a

00:11:15,150 --> 00:11:21,060
stream execution model where rather than

00:11:18,180 --> 00:11:22,380
running a function function once you

00:11:21,060 --> 00:11:24,180
have got something that's run

00:11:22,380 --> 00:11:29,100
continuously with with data streaming

00:11:24,180 --> 00:11:31,350
and you know as DSP as well so the DSPs

00:11:29,100 --> 00:11:34,890
are also very different from a CPU or

00:11:31,350 --> 00:11:36,240
GPU these are these are very sort of

00:11:34,890 --> 00:11:37,530
purpose-built compute engines for

00:11:36,240 --> 00:11:40,200
performing things like add subtract

00:11:37,530 --> 00:11:42,270
divide multiply very very quickly and

00:11:40,200 --> 00:11:46,980
they're generally used for a digital

00:11:42,270 --> 00:11:49,320
analog audio data stream processing so

00:11:46,980 --> 00:11:52,200
to answer the the question is

00:11:49,320 --> 00:11:54,750
performance portability possible the the

00:11:52,200 --> 00:11:57,420
short answer is is no architectures by

00:11:54,750 --> 00:12:01,710
designer are so different that there is

00:11:57,420 --> 00:12:03,630
no one solution fits all in reality you

00:12:01,710 --> 00:12:06,800
have you have to find a balance between

00:12:03,630 --> 00:12:10,260
performance Portability and productivity

00:12:06,800 --> 00:12:12,210
so performance being the the scalability

00:12:10,260 --> 00:12:14,340
and efficiency of your your application

00:12:12,210 --> 00:12:15,810
portability being the the range of

00:12:14,340 --> 00:12:18,870
hardware that your application can

00:12:15,810 --> 00:12:21,270
target and productivity being the range

00:12:18,870 --> 00:12:23,220
of tasks your application can perform so

00:12:21,270 --> 00:12:25,530
generally if you have if you have

00:12:23,220 --> 00:12:27,180
performance and portability if you are

00:12:25,530 --> 00:12:28,800
able to achieve sort of a good amount of

00:12:27,180 --> 00:12:33,780
performance portability this usually

00:12:28,800 --> 00:12:35,070
comes at the cost of productivity so so

00:12:33,780 --> 00:12:37,920
the long answer to this question is

00:12:35,070 --> 00:12:39,720
performs portability possible it the

00:12:37,920 --> 00:12:42,030
answer is yes the with the right

00:12:39,720 --> 00:12:43,830
programming model you can write you can

00:12:42,030 --> 00:12:46,950
represent your problem in a way which

00:12:43,830 --> 00:12:48,180
can adapt to different execution models

00:12:46,950 --> 00:12:59,519
and different memory models of different

00:12:48,180 --> 00:13:01,170
kind of a heterogeneous devices so

00:12:59,519 --> 00:13:03,959
the next thing I'd like to look at is

00:13:01,170 --> 00:13:06,470
heterogeneous offloading so how do you

00:13:03,959 --> 00:13:08,999
offload code to a heterogeneous debase

00:13:06,470 --> 00:13:10,619
so so this the best way to answer this

00:13:08,999 --> 00:13:14,579
question is just to start with the the

00:13:10,619 --> 00:13:16,920
C++ accomplish model so it's obviously

00:13:14,579 --> 00:13:18,269
yet you have your C++ source file you

00:13:16,920 --> 00:13:20,339
have a pastor suite compiler and you

00:13:18,269 --> 00:13:22,290
have an object file and you pass it

00:13:20,339 --> 00:13:24,420
through a linker and have the executable

00:13:22,290 --> 00:13:29,009
and then you execute all runs on on the

00:13:24,420 --> 00:13:30,600
CPU but what happens if you want to also

00:13:29,009 --> 00:13:32,490
target a GPU I know that had Gelinas

00:13:30,600 --> 00:13:33,929
device how how do you adapt this

00:13:32,490 --> 00:13:36,839
completion model in order to support

00:13:33,929 --> 00:13:39,569
that so there's there's generally two

00:13:36,839 --> 00:13:42,629
two approaches to this and one one

00:13:39,569 --> 00:13:45,089
separate source in a single source so

00:13:42,629 --> 00:13:47,610
separate source compilation as an

00:13:45,089 --> 00:13:49,439
example we're going to use OpenCL so a

00:13:47,610 --> 00:13:51,689
separate source completion alongside

00:13:49,439 --> 00:13:54,569
your C++ source file you have a device

00:13:51,689 --> 00:13:57,449
source so a separate source in this

00:13:54,569 --> 00:13:59,220
let's compile separately this is this is

00:13:57,449 --> 00:14:02,579
then compiled by an online compiler that

00:13:59,220 --> 00:14:03,569
is linked to your executable so the the

00:14:02,579 --> 00:14:06,209
main thing to notice here that this

00:14:03,569 --> 00:14:07,410
means that because this is is compiled

00:14:06,209 --> 00:14:10,139
by an online compilers compiled at

00:14:07,410 --> 00:14:11,579
runtime which means that you your device

00:14:10,139 --> 00:14:15,420
source has to be shipped with your

00:14:11,579 --> 00:14:18,839
executable so to go back in and look at

00:14:15,420 --> 00:14:21,389
a single source this so you see possible

00:14:18,839 --> 00:14:22,889
sample as an example so the thing to

00:14:21,389 --> 00:14:24,569
notice here is rather than having the

00:14:22,889 --> 00:14:28,589
device code as a separate source file

00:14:24,569 --> 00:14:31,740
it's all in embedded inside the same C++

00:14:28,589 --> 00:14:34,559
source file then alongside your CPU

00:14:31,740 --> 00:14:36,360
compiler you compile your the same

00:14:34,559 --> 00:14:37,920
source file with a device compiler in

00:14:36,360 --> 00:14:39,929
this little generate alongside this CP

00:14:37,920 --> 00:14:43,110
object some form of device intermediate

00:14:39,929 --> 00:14:44,670
representation or object file this is

00:14:43,110 --> 00:14:47,129
then linked together with the CP object

00:14:44,670 --> 00:14:48,809
and then you end up with an executable

00:14:47,129 --> 00:14:50,519
executable with some embedded

00:14:48,809 --> 00:14:54,240
intermediate representation or object

00:14:50,519 --> 00:14:55,949
that will execute on your device so some

00:14:54,240 --> 00:14:57,839
of the some of the main benefits of the

00:14:55,949 --> 00:14:59,819
the single source model over the

00:14:57,839 --> 00:15:01,610
separate source is that there the host

00:14:59,819 --> 00:15:05,670
CPU code and the device code are are

00:15:01,610 --> 00:15:07,499
same same c++ source file unless hello

00:15:05,670 --> 00:15:09,240
is compile time evaluation of you device

00:15:07,499 --> 00:15:12,360
code which gives you type safety across

00:15:09,240 --> 00:15:13,290
the host cpu interface and the supports

00:15:12,360 --> 00:15:15,450
generic programming

00:15:13,290 --> 00:15:27,180
I also removes the need to distribute

00:15:15,450 --> 00:15:28,650
your source code so the next next thing

00:15:27,180 --> 00:15:30,360
or two look is how to describe

00:15:28,650 --> 00:15:32,160
parallelism so there's many different

00:15:30,360 --> 00:15:33,750
forms of parallelism there's different

00:15:32,160 --> 00:15:36,480
ways of describing and you're in your

00:15:33,750 --> 00:15:38,870
source code so these are the three

00:15:36,480 --> 00:15:41,850
things we'll look at so the first is

00:15:38,870 --> 00:15:43,560
directive versus explicit parallelism so

00:15:41,850 --> 00:15:45,630
on the left here we have an example of

00:15:43,560 --> 00:15:47,460
directive parallelism with OpenMP and

00:15:45,630 --> 00:15:50,370
the right we have an example of explicit

00:15:47,460 --> 00:15:52,050
parallelism with c++ amp so the way that

00:15:50,370 --> 00:15:54,330
director parallelism works is you have

00:15:52,050 --> 00:15:57,360
some sequential C++ code like some form

00:15:54,330 --> 00:16:00,210
of loop and then you have a pragma that

00:15:57,360 --> 00:16:02,630
you attach to it there is a hint to the

00:16:00,210 --> 00:16:05,550
compiler and the compiler then uses to

00:16:02,630 --> 00:16:07,050
paralyze it either bias or transforming

00:16:05,550 --> 00:16:09,650
the code to run multiple threads or

00:16:07,050 --> 00:16:11,940
transforming it to some D operations

00:16:09,650 --> 00:16:14,670
which with the explicit parallelism you

00:16:11,940 --> 00:16:16,590
have some from if API where you you

00:16:14,670 --> 00:16:19,230
define a function and then a range of

00:16:16,590 --> 00:16:21,270
execution and then that's this function

00:16:19,230 --> 00:16:25,530
is executed on the hardware across for

00:16:21,270 --> 00:16:27,300
that number of vexation so next through

00:16:25,530 --> 00:16:29,880
a look at task parallelism versus data

00:16:27,300 --> 00:16:32,520
parallelism so on the left you have an

00:16:29,880 --> 00:16:34,590
example of task parallelism TBB an

00:16:32,520 --> 00:16:37,410
irregular an example of idea piles and

00:16:34,590 --> 00:16:39,360
with CUDA so task parallelism is where

00:16:37,410 --> 00:16:43,550
you have multiple potentially different

00:16:39,360 --> 00:16:46,680
tasks they're running in in parallel and

00:16:43,550 --> 00:16:48,990
with the data parallelism you have the

00:16:46,680 --> 00:16:53,310
same task being performed across a large

00:16:48,990 --> 00:16:55,350
data set in parallel so next next

00:16:53,310 --> 00:16:57,450
there's a cube queue execution versus

00:16:55,350 --> 00:16:59,160
stream execution so on the Left you of

00:16:57,450 --> 00:17:01,050
an example of queue execution with CUDA

00:16:59,160 --> 00:17:04,949
on the right we're an example of stream

00:17:01,050 --> 00:17:07,110
execution with brook GPU so with qu

00:17:04,949 --> 00:17:08,550
execution you have a function that's

00:17:07,110 --> 00:17:11,130
placed on a queue and then that's

00:17:08,550 --> 00:17:13,079
executed and then that returns whereas

00:17:11,130 --> 00:17:14,880
the stream execution your function is

00:17:13,079 --> 00:17:19,530
executing a continuous loop with data

00:17:14,880 --> 00:17:20,699
streaming and and out of it so finally

00:17:19,530 --> 00:17:23,070
the last thing I want to look at is data

00:17:20,699 --> 00:17:24,060
locality and movement so one of the

00:17:23,070 --> 00:17:26,410
biggest limiting factors in

00:17:24,060 --> 00:17:29,500
heterogeneous computing is

00:17:26,410 --> 00:17:33,130
the the cost of data movement and both

00:17:29,500 --> 00:17:35,290
time and power consumption so it can it

00:17:33,130 --> 00:17:37,420
can be considerable I might attain to to

00:17:35,290 --> 00:17:39,100
move the data from a whole CPU to

00:17:37,420 --> 00:17:41,460
hydrogens to basically want to execute

00:17:39,100 --> 00:17:43,600
on this this depends on the architecture

00:17:41,460 --> 00:17:45,640
the the bandwidth of a device can also

00:17:43,600 --> 00:17:47,700
impose bottlenecks which can affect the

00:17:45,640 --> 00:17:49,660
throughput of your device as well

00:17:47,700 --> 00:17:51,760
additionally it's very important to

00:17:49,660 --> 00:17:54,070
ensure that the performance gain you

00:17:51,760 --> 00:17:55,540
achieve from performing the computation

00:17:54,070 --> 00:17:58,270
on the heterogeneous face as opposed to

00:17:55,540 --> 00:17:59,560
the host CPU is larger than the cost of

00:17:58,270 --> 00:18:01,840
moving the data to the device otherwise

00:17:59,560 --> 00:18:04,000
it's not it's not worth the performance

00:18:01,840 --> 00:18:05,680
on the device the other thing to

00:18:04,000 --> 00:18:07,630
consider is that many devices such as

00:18:05,680 --> 00:18:09,910
GPUs have a hierarchy of different

00:18:07,630 --> 00:18:12,460
memory regions in each of these region

00:18:09,910 --> 00:18:15,910
has a different memory size affinity and

00:18:12,460 --> 00:18:19,060
access latency so for example global

00:18:15,910 --> 00:18:22,260
data's generally is DDR and has sort of

00:18:19,060 --> 00:18:24,430
a very large amount of data but has a

00:18:22,260 --> 00:18:27,070
sort of a low affinity so very high

00:18:24,430 --> 00:18:30,280
access latency whereas private memories

00:18:27,070 --> 00:18:33,610
generally sort of registers or on chip

00:18:30,280 --> 00:18:35,200
statigram where you have them so it was

00:18:33,610 --> 00:18:36,550
a much smaller size but has a much

00:18:35,200 --> 00:18:39,670
higher affinity to the computation so

00:18:36,550 --> 00:18:41,050
much lower access latency so no one

00:18:39,670 --> 00:18:42,940
having your data as close to the

00:18:41,050 --> 00:18:47,320
computation as possible can conversely

00:18:42,940 --> 00:18:48,430
reduce the cost of video movement so a

00:18:47,320 --> 00:18:50,170
look looking at the cost of data

00:18:48,430 --> 00:18:52,750
movement in in terms of power

00:18:50,170 --> 00:18:55,330
consumption this is a slide taken from a

00:18:52,750 --> 00:18:58,990
talk by Bill dally and 2010 from in

00:18:55,330 --> 00:19:01,750
video this is this is the measuring the

00:18:58,990 --> 00:19:04,780
cost of performing computations Earth

00:19:01,750 --> 00:19:07,360
meta computations and moving data on an

00:19:04,780 --> 00:19:09,280
NVIDIA GPU so the thing to notice here

00:19:07,360 --> 00:19:12,090
is that in the top left corner you have

00:19:09,280 --> 00:19:14,320
a 64-bit double-precision operation

00:19:12,090 --> 00:19:17,260
that's cost in 20 packaged jewels and

00:19:14,320 --> 00:19:22,110
then just below that there's the the

00:19:17,260 --> 00:19:24,910
blue dot is a read of 464 64-bit and

00:19:22,110 --> 00:19:28,060
operands for that operation and that

00:19:24,910 --> 00:19:31,390
costs 50 packages then so if you like to

00:19:28,060 --> 00:19:32,620
move these for 64-bit operands one

00:19:31,390 --> 00:19:34,900
millimeter across the chip that would

00:19:32,620 --> 00:19:36,460
cost you 26 package jewels over here to

00:19:34,900 --> 00:19:38,920
move it across the entire chip that'll

00:19:36,460 --> 00:19:40,240
cost you a nano jewel and then if you

00:19:38,920 --> 00:19:44,830
were to move it off onto d around

00:19:40,240 --> 00:19:46,840
sixteen annuals so the the Causton in in

00:19:44,830 --> 00:19:49,720
power consumption from moving the data

00:19:46,840 --> 00:19:54,010
can can can have a dramatic impact on

00:19:49,720 --> 00:19:58,690
the performance of the Vera execute on

00:19:54,010 --> 00:20:00,370
the device so this is how do you how do

00:19:58,690 --> 00:20:02,710
you move data from the whole CPU to

00:20:00,370 --> 00:20:03,640
advise so there's there's a couple

00:20:02,710 --> 00:20:05,470
different ways of doing it there's

00:20:03,640 --> 00:20:08,740
explicit data movement and implicit data

00:20:05,470 --> 00:20:11,230
movement so on the left is an example of

00:20:08,740 --> 00:20:12,970
implicit data movement the c++ amp and

00:20:11,230 --> 00:20:15,880
the right is an example of explicit data

00:20:12,970 --> 00:20:19,570
movement with cuda so implicit data

00:20:15,880 --> 00:20:22,270
movement works by having data structures

00:20:19,570 --> 00:20:25,179
that are cross core CPU and device that

00:20:22,270 --> 00:20:27,330
can be used so on this this is this has

00:20:25,179 --> 00:20:30,400
to be used with a single source

00:20:27,330 --> 00:20:31,780
programming model so you have a single

00:20:30,400 --> 00:20:33,250
structure that you use on both the host

00:20:31,780 --> 00:20:34,809
and the device code and then this is

00:20:33,250 --> 00:20:37,090
this implicitly handles the data

00:20:34,809 --> 00:20:39,010
movement for you whereas with explicit

00:20:37,090 --> 00:20:41,320
data movement you have these explicit

00:20:39,010 --> 00:20:42,670
API calls that you have to explicitly

00:20:41,320 --> 00:20:47,950
pass pointers and say I want to move

00:20:42,670 --> 00:20:49,690
this day on over to the device so the

00:20:47,950 --> 00:20:51,429
other thing is how do you how do you

00:20:49,690 --> 00:20:53,679
address memory across the host CPU in

00:20:51,429 --> 00:20:57,010
the device and there's a few different

00:20:53,679 --> 00:20:58,270
ways of doing this so in comparison

00:20:57,010 --> 00:21:00,330
there's the first of all there's there's

00:20:58,270 --> 00:21:03,610
multi address space although where

00:21:00,330 --> 00:21:06,400
pointers have attributes or structures

00:21:03,610 --> 00:21:09,280
which specify where what sort of memory

00:21:06,400 --> 00:21:11,200
region that data should be stored as all

00:21:09,280 --> 00:21:13,720
those fine are clean control over where

00:21:11,200 --> 00:21:14,740
your memories allocated or moved to but

00:21:13,720 --> 00:21:17,440
it does mean you have to define

00:21:14,740 --> 00:21:20,050
explicitly and there's noncoherent

00:21:17,440 --> 00:21:22,510
single address space this this works by

00:21:20,050 --> 00:21:24,610
you have pointers that address the

00:21:22,510 --> 00:21:27,340
shared address space across the host CPU

00:21:24,610 --> 00:21:30,460
in the device and they do this by having

00:21:27,340 --> 00:21:32,679
map operations a API switch allow you to

00:21:30,460 --> 00:21:36,309
so say you're accessing a pointer on the

00:21:32,679 --> 00:21:38,110
host you then have an API which says I

00:21:36,309 --> 00:21:39,700
want to map this data across to the

00:21:38,110 --> 00:21:42,280
device and then you could use the same

00:21:39,700 --> 00:21:45,730
pointer to access the same address in on

00:21:42,280 --> 00:21:47,140
the device and there is with cache

00:21:45,730 --> 00:21:48,550
coherence single address space this is

00:21:47,140 --> 00:21:51,340
similar to the the noncoherent single

00:21:48,550 --> 00:21:53,460
address space the difference is this is

00:21:51,340 --> 00:21:55,289
the way you when you address

00:21:53,460 --> 00:21:57,149
a pointer on the horse and device you're

00:21:55,289 --> 00:22:00,179
generally either addressing the same

00:21:57,149 --> 00:22:01,799
physical memory and hardware or you're

00:22:00,179 --> 00:22:04,049
accessing a cache coherent Hrunting

00:22:01,799 --> 00:22:06,390
which performs the synchronization at

00:22:04,049 --> 00:22:09,390
the cache level so let's allows you to

00:22:06,390 --> 00:22:11,720
access the horse CPU and the same

00:22:09,390 --> 00:22:12,899
pointers on the host CPU and the device

00:22:11,720 --> 00:22:15,179
concurrently

00:22:12,899 --> 00:22:18,029
however we are given up a lot of control

00:22:15,179 --> 00:22:30,299
over when the date has moved so so this

00:22:18,029 --> 00:22:31,500
can be an efficient in some cases so

00:22:30,299 --> 00:22:34,830
next I'm going to look at talk about

00:22:31,500 --> 00:22:38,429
cycles this is a new standard that for

00:22:34,830 --> 00:22:41,700
heterogeneous computing C++ so first of

00:22:38,429 --> 00:22:44,070
all open CL is a standard from the

00:22:41,700 --> 00:22:45,840
kernel script that gives praise a C API

00:22:44,070 --> 00:22:48,630
and icy language that allows you to

00:22:45,840 --> 00:22:51,570
offload code to many different

00:22:48,630 --> 00:22:53,970
heterogenous devices and circle is a

00:22:51,570 --> 00:22:57,510
relatively new standard that aims to

00:22:53,970 --> 00:22:59,190
provide our provider a single source C++

00:22:57,510 --> 00:23:01,620
programming model that allows you to

00:22:59,190 --> 00:23:03,120
vary standard C++ and target this range

00:23:01,620 --> 00:23:07,230
of this large range of heterogeneous

00:23:03,120 --> 00:23:10,110
devices through open CL so the way the

00:23:07,230 --> 00:23:12,659
ecosystem for cycle works is so you have

00:23:10,110 --> 00:23:15,090
your C++ application and you have some

00:23:12,659 --> 00:23:16,440
template libraries and then some of

00:23:15,090 --> 00:23:19,020
these simple average may implement an

00:23:16,440 --> 00:23:22,409
algorithm using sickled this is then

00:23:19,020 --> 00:23:27,450
compiled and executed via OpenCL on a

00:23:22,409 --> 00:23:29,460
wide range of devices so first of all

00:23:27,450 --> 00:23:31,110
how do cycle improve the hedging is

00:23:29,460 --> 00:23:33,960
offload and portability performance

00:23:31,110 --> 00:23:36,809
porkbelly so the first thing is cycle

00:23:33,960 --> 00:23:39,390
isn't entirely standard C++ cycle allows

00:23:36,809 --> 00:23:41,309
you to compile to spear and cycle to

00:23:39,390 --> 00:23:44,700
support some a multi compilation single

00:23:41,309 --> 00:23:46,440
source module so in order to explain the

00:23:44,700 --> 00:23:47,940
the multi they said the multi

00:23:46,440 --> 00:23:49,590
compilation single source model I'll

00:23:47,940 --> 00:23:53,250
first explain the single compilation

00:23:49,590 --> 00:23:54,630
model so the way this works is you you

00:23:53,250 --> 00:23:57,169
wrap around your so this is the same

00:23:54,630 --> 00:23:59,399
single source model from from earlier so

00:23:57,169 --> 00:24:01,620
the way single compilation works is you

00:23:59,399 --> 00:24:03,330
wrap around the CPU compile and the

00:24:01,620 --> 00:24:05,100
device compiler so you have a single

00:24:03,330 --> 00:24:07,230
single source host and device compiler

00:24:05,100 --> 00:24:09,330
this is quite quite simple

00:24:07,230 --> 00:24:10,470
you have a single compiler that you pass

00:24:09,330 --> 00:24:11,610
here you compile with your source file

00:24:10,470 --> 00:24:14,220
and you end up with an executable that

00:24:11,610 --> 00:24:15,510
you can run on the CPU and a GPU the

00:24:14,220 --> 00:24:19,290
problem with that is that this is tight

00:24:15,510 --> 00:24:20,790
to a particular compiler chain so say

00:24:19,290 --> 00:24:23,429
for example you want it to have an

00:24:20,790 --> 00:24:27,419
application that supported a AMD GPU

00:24:23,429 --> 00:24:28,590
nvidia GPU and some DCP description so i

00:24:27,419 --> 00:24:29,850
were to do that you would have to write

00:24:28,590 --> 00:24:32,010
first of all you could you could write

00:24:29,850 --> 00:24:33,750
some C+ or some code and compile that

00:24:32,010 --> 00:24:34,919
with a suppose for some compiler you end

00:24:33,750 --> 00:24:38,520
up with an extra cute bowl that can run

00:24:34,919 --> 00:24:41,160
CPU and execute on an AMD GPU and then

00:24:38,520 --> 00:24:42,480
you write some critical source compile

00:24:41,160 --> 00:24:43,830
it with a cuda compiler you end up with

00:24:42,480 --> 00:24:46,710
an executable that can run on the CPU

00:24:43,830 --> 00:24:49,230
and execute on an NVIDIA GPU and same

00:24:46,710 --> 00:24:50,490
again openmp source you would compile it

00:24:49,230 --> 00:24:53,190
would open MP compiler and you have an

00:24:50,490 --> 00:24:55,799
executable around CPU and run in 70

00:24:53,190 --> 00:24:57,540
operations the problem with this is that

00:24:55,799 --> 00:24:59,130
you have three different compilers

00:24:57,540 --> 00:25:00,419
generating three different executables

00:24:59,130 --> 00:25:02,460
with three different sets of language

00:25:00,419 --> 00:25:03,870
extensions and these are generally not

00:25:02,460 --> 00:25:05,190
interoperable with each other which

00:25:03,870 --> 00:25:07,049
means that if you want to have a single

00:25:05,190 --> 00:25:08,610
application that can target all of these

00:25:07,049 --> 00:25:10,410
different hardware it makes it very

00:25:08,610 --> 00:25:11,910
difficult to build this generally it has

00:25:10,410 --> 00:25:13,799
to be the device you want to write

00:25:11,910 --> 00:25:17,280
execute on has to be decided at compile

00:25:13,799 --> 00:25:19,410
time so some of the things that sickles

00:25:17,280 --> 00:25:21,000
tried to do to resolve this the first

00:25:19,410 --> 00:25:23,910
thing is cycle is entirely standard C++

00:25:21,000 --> 00:25:27,360
so there's there's no no attributes

00:25:23,910 --> 00:25:29,280
no additional syntax no pragmas and no

00:25:27,360 --> 00:25:31,140
keywords it's just entirely standard C++

00:25:29,280 --> 00:25:34,770
that can be compiled with any C++

00:25:31,140 --> 00:25:36,870
compiler the next thing is that cyclical

00:25:34,770 --> 00:25:38,280
can target through spear this appears

00:25:36,870 --> 00:25:40,080
another standard from the Chronos script

00:25:38,280 --> 00:25:42,900
Center this a standard portable

00:25:40,080 --> 00:25:45,240
intermediate representation this allows

00:25:42,900 --> 00:25:47,820
other circle to target compiled to a

00:25:45,240 --> 00:25:51,510
single binary and target a large number

00:25:47,820 --> 00:25:53,270
of hedging devices so to look at the the

00:25:51,510 --> 00:25:56,100
multi-core the multi compilation model

00:25:53,270 --> 00:25:57,419
and say this works so first of all here

00:25:56,100 --> 00:26:00,380
you see that the device compiler is the

00:25:57,419 --> 00:26:03,900
second compiler and let's generate spear

00:26:00,380 --> 00:26:05,220
now the way the multi compilation model

00:26:03,900 --> 00:26:07,650
works is we separate the host compiler

00:26:05,220 --> 00:26:08,910
and the device compiler the first thing

00:26:07,650 --> 00:26:11,700
this means is the host compiler can be

00:26:08,910 --> 00:26:14,400
any standard C++ compiler so GCC clang

00:26:11,700 --> 00:26:16,860
Visual C++ in Tosa goes soakin fitting

00:26:14,400 --> 00:26:19,290
with any existing tow chain the next

00:26:16,860 --> 00:26:21,059
thing is that you generally want a

00:26:19,290 --> 00:26:23,549
single executable with

00:26:21,059 --> 00:26:25,469
embedded spear and that can then execute

00:26:23,549 --> 00:26:28,379
across a wide range of heterogeneous

00:26:25,469 --> 00:26:30,569
devices and this can be to the device

00:26:28,379 --> 00:26:31,559
can be selected at runtime because you

00:26:30,569 --> 00:26:33,839
have the standard intermediate

00:26:31,559 --> 00:26:37,079
representation you this adds a lot of

00:26:33,839 --> 00:26:38,519
performance portability however circle

00:26:37,079 --> 00:26:40,349
specification doesn't mandate you have

00:26:38,519 --> 00:26:42,269
to use spear you can use any

00:26:40,349 --> 00:26:43,769
intermediate representation or binary

00:26:42,269 --> 00:26:45,839
format is supported by an open CL

00:26:43,769 --> 00:26:49,799
implementation so for example if you

00:26:45,839 --> 00:26:52,889
want it to support PTX you could have

00:26:49,799 --> 00:26:54,239
you have the same cycle source code you

00:26:52,889 --> 00:26:55,829
pass it through protection of the same

00:26:54,239 --> 00:26:57,899
compiler or another compiled compiler

00:26:55,829 --> 00:26:59,639
and then generate PTX and that can be

00:26:57,899 --> 00:27:04,619
linked into the same executable and then

00:26:59,639 --> 00:27:05,909
you can select that or run thing so so

00:27:04,619 --> 00:27:07,169
the next thing is I'd just cycle support

00:27:05,909 --> 00:27:10,469
different ways of representing

00:27:07,169 --> 00:27:12,629
parallelism so cycle is an explicit

00:27:10,469 --> 00:27:15,209
parallelism model and it has queue

00:27:12,629 --> 00:27:17,219
execution mode on later versions of

00:27:15,209 --> 00:27:21,929
cycle made me try to do a different

00:27:17,219 --> 00:27:23,219
additional forms of parallelism a cycle

00:27:21,929 --> 00:27:26,669
also supports both tasks and data

00:27:23,219 --> 00:27:28,979
parallelism so these are two of the API

00:27:26,669 --> 00:27:32,549
sin in cycle the first one is a single

00:27:28,979 --> 00:27:33,779
task this performs a pass a lambda or a

00:27:32,549 --> 00:27:37,349
functor object here and has executed

00:27:33,779 --> 00:27:40,259
once and then you have another parallel

00:27:37,349 --> 00:27:41,969
for which you pass a functor lambda

00:27:40,259 --> 00:27:45,389
object again but they also pass a range

00:27:41,969 --> 00:27:46,559
let's remain specifies the the the work

00:27:45,389 --> 00:27:50,579
work range that you want to execute

00:27:46,559 --> 00:27:52,849
across the next thing is how the SEC

00:27:50,579 --> 00:27:55,259
will make the a movement more efficient

00:27:52,849 --> 00:27:58,409
first of all the cycle separates the

00:27:55,259 --> 00:27:59,969
storage and access of there and another

00:27:58,409 --> 00:28:02,399
cycle allows you to specify where you

00:27:59,969 --> 00:28:03,779
want your data to go on the device in

00:28:02,399 --> 00:28:07,619
cycle also allows you to create

00:28:03,779 --> 00:28:09,509
automatic data dependency graphs so the

00:28:07,619 --> 00:28:11,549
first thing is cycle separates the

00:28:09,509 --> 00:28:12,779
storage and access of data through this

00:28:11,549 --> 00:28:15,419
relationship between buffers and

00:28:12,779 --> 00:28:18,419
accessors so buffer is an object which

00:28:15,419 --> 00:28:20,749
maintains data across off the host CPU

00:28:18,419 --> 00:28:23,999
and one or more heterogeneous devices

00:28:20,749 --> 00:28:25,469
you then have an accessor which is used

00:28:23,999 --> 00:28:27,299
to describe the access on a particular

00:28:25,469 --> 00:28:29,279
device so here we have an access which

00:28:27,299 --> 00:28:32,309
our CPU then you can create another

00:28:29,279 --> 00:28:33,119
access or to a GPU and buffers an access

00:28:32,309 --> 00:28:34,830
there's a booth

00:28:33,119 --> 00:28:36,090
template there typesafe across

00:28:34,830 --> 00:28:39,659
Austin this is what allows the tape

00:28:36,090 --> 00:28:40,950
safety across Austin device so one of

00:28:39,659 --> 00:28:42,899
the things access letters allow you to

00:28:40,950 --> 00:28:45,240
do is access allow you to specify what

00:28:42,899 --> 00:28:48,630
where you want the data to be stored or

00:28:45,240 --> 00:28:49,529
allocated so you have a kernel function

00:28:48,630 --> 00:28:52,230
you want to execute and you have a

00:28:49,529 --> 00:28:54,840
buffer you can create a global access so

00:28:52,230 --> 00:28:57,059
this will store the memory in the global

00:28:54,840 --> 00:28:59,159
memory region so say you have a constant

00:28:57,059 --> 00:29:02,250
access this will store the memory and

00:28:59,159 --> 00:29:03,600
the read-only memory region well if you

00:29:02,250 --> 00:29:06,440
can have a local access or the switch

00:29:03,600 --> 00:29:08,610
store memory and the group memory region

00:29:06,440 --> 00:29:11,039
so you can have quite fingering control

00:29:08,610 --> 00:29:12,570
over where your your data stored the

00:29:11,039 --> 00:29:14,639
next thing is access is allow you to do

00:29:12,570 --> 00:29:16,139
is access it specify how you want to

00:29:14,639 --> 00:29:18,960
access video so whether we want it to be

00:29:16,139 --> 00:29:22,470
read write or read write or anything

00:29:18,960 --> 00:29:23,970
else and then that using this this the

00:29:22,470 --> 00:29:26,340
the runtime can create these data

00:29:23,970 --> 00:29:29,669
dependency graphs so as an example of

00:29:26,340 --> 00:29:32,429
this if say you have 4 buffers and 3

00:29:29,669 --> 00:29:35,010
kernels so the kernel a reads from

00:29:32,429 --> 00:29:37,200
buffer a and race to buffer B can all be

00:29:35,010 --> 00:29:39,570
reached from buffer a erase to buffer C

00:29:37,200 --> 00:29:43,320
and then kernel see you reach from

00:29:39,570 --> 00:29:44,549
buffer B and C reads writes back to D so

00:29:43,320 --> 00:29:46,380
the runtime is able to automatically

00:29:44,549 --> 00:29:48,419
determine the colonel and colonel B can

00:29:46,380 --> 00:29:51,090
run in parallel because there's no

00:29:48,419 --> 00:29:52,919
dependencies there but kernel C has a

00:29:51,090 --> 00:29:54,210
dependency on both a and B completing so

00:29:52,919 --> 00:29:57,179
then it has to wait for them to finish

00:29:54,210 --> 00:29:59,130
before it can execute so some of the

00:29:57,179 --> 00:30:01,500
benefits of this is their dependency

00:29:59,130 --> 00:30:04,230
graphs is you you allows you to specify

00:30:01,500 --> 00:30:05,190
your problems in terms of relationship

00:30:04,230 --> 00:30:08,279
so you don't need to perform any

00:30:05,190 --> 00:30:11,419
explicit data copies also removes the

00:30:08,279 --> 00:30:14,279
need for complex event handling between

00:30:11,419 --> 00:30:17,130
executions because it's automatically

00:30:14,279 --> 00:30:18,840
constructed via the accessors finally

00:30:17,130 --> 00:30:20,370
allows the run team to perform data

00:30:18,840 --> 00:30:22,620
movement optimizations so it can

00:30:20,370 --> 00:30:24,149
preemptively copy data to a device where

00:30:22,620 --> 00:30:25,889
you're going to need it it can avoid

00:30:24,149 --> 00:30:27,539
unnecessarily copy the data back to the

00:30:25,889 --> 00:30:29,700
host if you need to use again on that

00:30:27,539 --> 00:30:31,440
same device layer and I can avoid copies

00:30:29,700 --> 00:30:32,610
to and from the device if you don't if

00:30:31,440 --> 00:30:37,710
you're not interested in the original

00:30:32,610 --> 00:30:39,419
values of the idea so let's finish off

00:30:37,710 --> 00:30:42,720
what does cycle look like so I'm going

00:30:39,419 --> 00:30:44,850
to go through a simple example of cycle

00:30:42,720 --> 00:30:46,980
application so we're going to try to

00:30:44,850 --> 00:30:48,720
implement a parallel ad function that

00:30:46,980 --> 00:30:51,360
takes two input vectors and then I

00:30:48,720 --> 00:30:53,190
vector so the first thing we do is

00:30:51,360 --> 00:30:57,539
include the header file so this includes

00:30:53,190 --> 00:30:58,740
in the entire runtime api so the first

00:30:57,539 --> 00:31:00,150
thing the first thing we have to do in

00:30:58,740 --> 00:31:05,549
the function is create both the buffers

00:31:00,150 --> 00:31:07,890
so buffers handle a maintain a pointer

00:31:05,549 --> 00:31:09,870
across a host and multiple devices I

00:31:07,890 --> 00:31:11,970
also performed synchronization so using

00:31:09,870 --> 00:31:13,789
Raia when buffers are destroyed it

00:31:11,970 --> 00:31:16,950
synchronizes the data back to the host

00:31:13,789 --> 00:31:18,480
so the the buffers lifetime was this

00:31:16,950 --> 00:31:19,530
function so when the buffers are

00:31:18,480 --> 00:31:21,270
destroyed it into this function will

00:31:19,530 --> 00:31:24,990
synchronize the data back to the vectors

00:31:21,270 --> 00:31:26,340
that you passed in so the next thing you

00:31:24,990 --> 00:31:28,830
do is create a queue in a queue for

00:31:26,340 --> 00:31:30,240
executing work and from the queue you

00:31:28,830 --> 00:31:33,360
can create what's called a command group

00:31:30,240 --> 00:31:35,940
so a command group defines the device

00:31:33,360 --> 00:31:38,130
code you want to execute as well as the

00:31:35,940 --> 00:31:43,559
data dependencies you need for that

00:31:38,130 --> 00:31:44,700
via that function so the first the first

00:31:43,559 --> 00:31:46,980
thing you do inside the command group is

00:31:44,700 --> 00:31:48,840
you create these accessors so create

00:31:46,980 --> 00:31:52,140
three accessors for the three buffers to

00:31:48,840 --> 00:31:54,450
two input input buffers have read access

00:31:52,140 --> 00:31:58,799
and I hope the buffer has as write

00:31:54,450 --> 00:32:01,289
access then you use a parallel for so

00:31:58,799 --> 00:32:05,460
earlier in order to create a device

00:32:01,289 --> 00:32:07,640
function so this takes two parameters

00:32:05,460 --> 00:32:09,870
this is first arrange to describe the

00:32:07,640 --> 00:32:11,640
the range of execution you wanna do so

00:32:09,870 --> 00:32:13,860
here we take the size of the access the

00:32:11,640 --> 00:32:15,809
vector in order to determine the number

00:32:13,860 --> 00:32:17,400
of work items want to execute and the

00:32:15,809 --> 00:32:20,730
second parameter is a lambda function

00:32:17,400 --> 00:32:22,289
describing the device code so the the

00:32:20,730 --> 00:32:24,090
lambdas lambda is the code that's

00:32:22,289 --> 00:32:25,230
compiled by the listicle device compiler

00:32:24,090 --> 00:32:28,470
in order to execute on its what's

00:32:25,230 --> 00:32:30,419
executed on the on the device so this

00:32:28,470 --> 00:32:32,250
takes a parameter as well takes an ID so

00:32:30,419 --> 00:32:34,350
for each execution of this function on

00:32:32,250 --> 00:32:38,760
the device the ID represents that space

00:32:34,350 --> 00:32:40,919
and the and the range of execution so

00:32:38,760 --> 00:32:43,080
the thing to notice here is that the

00:32:40,919 --> 00:32:45,840
parallel four takes a template parameter

00:32:43,080 --> 00:32:49,620
and the reason for this is that because

00:32:45,840 --> 00:32:52,530
circle has this a supports any standard

00:32:49,620 --> 00:32:54,230
C++ compiler and any device compiler

00:32:52,530 --> 00:32:57,450
into our intro operation between them

00:32:54,230 --> 00:32:59,610
the there's no standard way in C++ to

00:32:57,450 --> 00:33:00,780
name lambdas so every every see photos

00:32:59,610 --> 00:33:02,400
compiler has a different way of naming

00:33:00,780 --> 00:33:05,580
lambdas

00:33:02,400 --> 00:33:06,870
so we need a wait for the host compiler

00:33:05,580 --> 00:33:09,030
and the device compilers to be able to

00:33:06,870 --> 00:33:10,830
communicate regarding the device

00:33:09,030 --> 00:33:12,420
function so we we add this template

00:33:10,830 --> 00:33:14,070
parameter in order to name the to

00:33:12,420 --> 00:33:17,510
excellent so this is this kernel T here

00:33:14,070 --> 00:33:20,700
gives names this this function so

00:33:17,510 --> 00:33:22,559
finally we add the body of the function

00:33:20,700 --> 00:33:24,870
so we use the subject operator of the

00:33:22,559 --> 00:33:26,550
accessors to read from the input

00:33:24,870 --> 00:33:29,700
pointers add them together and assign it

00:33:26,550 --> 00:33:32,190
to their pointer and that's it and

00:33:29,700 --> 00:33:33,870
finally you you initialize your vectors

00:33:32,190 --> 00:33:36,030
and then you call the parallel add and

00:33:33,870 --> 00:33:37,590
because the buffer synchronizes the data

00:33:36,030 --> 00:33:42,360
when this function returns you have your

00:33:37,590 --> 00:33:45,360
result in a and that's them so how we

00:33:42,360 --> 00:34:02,280
lookin for time yep so so I'm gonna pass

00:33:45,360 --> 00:34:04,440
over to my code to talk about the okay

00:34:02,280 --> 00:34:07,140
thanks everybody that was great I mean

00:34:04,440 --> 00:34:08,940
when I saw I've been involved in open MP

00:34:07,140 --> 00:34:10,290
accelerator design as well as a lot of

00:34:08,940 --> 00:34:11,429
things when I first saw sicko I was

00:34:10,290 --> 00:34:13,440
really impressed with how it does

00:34:11,429 --> 00:34:16,620
everything at a very high abstract level

00:34:13,440 --> 00:34:18,600
very much like what C++ needs that

00:34:16,620 --> 00:34:20,100
ultimately I think is the big benefit of

00:34:18,600 --> 00:34:22,679
sicko you notice that it has it

00:34:20,100 --> 00:34:24,120
implicitly allows you to do data

00:34:22,679 --> 00:34:26,550
movement and supposed to exclusive I'll

00:34:24,120 --> 00:34:29,100
say explicit isn't as bad some cases you

00:34:26,550 --> 00:34:32,100
do need it but in other case in this for

00:34:29,100 --> 00:34:34,020
C++ it's a lot better model in that way

00:34:32,100 --> 00:34:38,790
so one other thing I want to talk about

00:34:34,020 --> 00:34:40,320
is about where C++ is going so what does

00:34:38,790 --> 00:34:42,120
what is the future of program uc+

00:34:40,320 --> 00:34:43,740
supposed to look like given the fact

00:34:42,120 --> 00:34:46,260
that we have all these possible models

00:34:43,740 --> 00:34:49,889
to learn from and it's time to add that

00:34:46,260 --> 00:34:51,210
to C++ so couple of things is that of

00:34:49,889 --> 00:34:53,970
course this is still being talked about

00:34:51,210 --> 00:34:55,800
in the committee and we anticipate that

00:34:53,970 --> 00:34:58,380
it will be three or four years before we

00:34:55,800 --> 00:34:59,790
can get to a technical specification the

00:34:58,380 --> 00:35:01,830
thing that will make that happen are

00:34:59,790 --> 00:35:04,320
these things we actually already almost

00:35:01,830 --> 00:35:07,350
60 80 percent of the way there but

00:35:04,320 --> 00:35:09,420
believe what all of this so we we so

00:35:07,350 --> 00:35:11,550
just to recap where we are with c++ 17

00:35:09,420 --> 00:35:13,410
we got two parallel algorithms and some

00:35:11,550 --> 00:35:14,640
progress guarantees the basic guarantees

00:35:13,410 --> 00:35:16,109
and the parallel forward progress

00:35:14,640 --> 00:35:18,239
guarantees the blue

00:35:16,109 --> 00:35:20,579
stuff is what's now in the queue to go

00:35:18,239 --> 00:35:22,529
into C++ 20 you got things from

00:35:20,579 --> 00:35:25,289
parallelism called database parallelism

00:35:22,529 --> 00:35:27,269
task based parallelism execution agents

00:35:25,289 --> 00:35:28,739
on the right hand side the concurrency

00:35:27,269 --> 00:35:30,839
stuff we've already got the future plus

00:35:28,739 --> 00:35:33,119
for us with the when the wait when all

00:35:30,839 --> 00:35:36,480
executives people out there talking

00:35:33,119 --> 00:35:39,390
about co-routines I'd lead the group on

00:35:36,480 --> 00:35:41,249
transactional memory synchronic atomic

00:35:39,390 --> 00:35:45,869
views things like that latches and

00:35:41,249 --> 00:35:46,619
barriers none of this is for GPUs you

00:35:45,869 --> 00:35:52,039
know that right

00:35:46,619 --> 00:35:56,549
all of this stuff is only for CPUs okay

00:35:52,039 --> 00:35:59,489
so how do we get from there to the to to

00:35:56,549 --> 00:36:01,890
GPU computing indeed we have a mountain

00:35:59,489 --> 00:36:03,869
to climb but it's actually not as bad as

00:36:01,890 --> 00:36:09,509
it seems okay

00:36:03,869 --> 00:36:11,849
so what we can do is that if we add the

00:36:09,509 --> 00:36:13,739
the futures and continuation model gives

00:36:11,849 --> 00:36:16,710
us an interesting starting point by

00:36:13,739 --> 00:36:19,259
extension extending C++ 11 features with

00:36:16,710 --> 00:36:21,329
this Microsoft style dot n continuations

00:36:19,259 --> 00:36:23,730
which adds the sequential and parallel

00:36:21,329 --> 00:36:26,910
composition capabilities when you have

00:36:23,730 --> 00:36:29,430
when all when all when all the futures

00:36:26,910 --> 00:36:31,319
comes back that's great so join or when

00:36:29,430 --> 00:36:32,910
any futures comes back then you can do

00:36:31,319 --> 00:36:35,640
something with it that is called a

00:36:32,910 --> 00:36:37,109
choice okay so these are compositions we

00:36:35,640 --> 00:36:39,920
also have these additional utilities

00:36:37,109 --> 00:36:41,670
which I don't really want to talk about

00:36:39,920 --> 00:36:43,410
because they don't really add any

00:36:41,670 --> 00:36:44,069
semantics Anthony Williams is talking

00:36:43,410 --> 00:36:46,499
about them right now

00:36:44,069 --> 00:36:49,470
downstairs and he can do a great job on

00:36:46,499 --> 00:36:52,799
this stuff so heterogeneous computing

00:36:49,470 --> 00:36:56,160
for sick hole has couple of choices that

00:36:52,799 --> 00:36:58,529
are already built up okay I don't want

00:36:56,160 --> 00:37:00,749
to use CUDA CUDA is a great example

00:36:58,529 --> 00:37:02,549
because they've done a lot of pioneering

00:37:00,749 --> 00:37:03,930
work and in fact everything looks kind

00:37:02,549 --> 00:37:06,589
of like CUDA but there's one problem

00:37:03,930 --> 00:37:10,559
with CUDA what is that

00:37:06,589 --> 00:37:13,559
it's C it looks like C it looks like

00:37:10,559 --> 00:37:16,589
functions procedures line by line this

00:37:13,559 --> 00:37:19,589
is not looks nothing like C++ we don't

00:37:16,589 --> 00:37:22,410
want that we want something that is C++

00:37:19,589 --> 00:37:24,989
like that's template like that enables

00:37:22,410 --> 00:37:27,779
you to do static and and and and and

00:37:24,989 --> 00:37:30,029
dynamic polymorphism that can make it

00:37:27,779 --> 00:37:31,499
work with templates with concepts

00:37:30,029 --> 00:37:34,979
and all the other nice stuff that's

00:37:31,499 --> 00:37:37,439
coming so the peel of the groups that we

00:37:34,979 --> 00:37:39,479
we have sicko is actually a really

00:37:37,439 --> 00:37:42,149
really good example at a very high level

00:37:39,479 --> 00:37:46,439
of allowing implicit data movement on

00:37:42,149 --> 00:37:49,409
the one hand okay as well as automatic

00:37:46,439 --> 00:37:51,179
data scoping it's ideal from things like

00:37:49,409 --> 00:37:53,159
heterogeneous computing in for deep

00:37:51,179 --> 00:37:57,059
learning neural networks machine vision

00:37:53,159 --> 00:37:59,279
self-driving cars okay but we also can

00:37:57,059 --> 00:38:01,589
ignore the distributor computing aspect

00:37:59,279 --> 00:38:05,009
that's offered by LSU's hardwoods talk

00:38:01,589 --> 00:38:08,069
on using HP X they do a terrific job as

00:38:05,009 --> 00:38:11,039
well - of doing this in a very very C

00:38:08,069 --> 00:38:12,959
plus in a very C++ way but they do they

00:38:11,039 --> 00:38:15,329
use an explicit memory movement the room

00:38:12,959 --> 00:38:18,749
directives ok nothing wrong with that in

00:38:15,329 --> 00:38:20,489
some cases you do need those things so

00:38:18,749 --> 00:38:22,229
what else do we need we also need what

00:38:20,489 --> 00:38:23,399
NVIDIA has and really it had these ways

00:38:22,229 --> 00:38:25,349
of create these things called bulk

00:38:23,399 --> 00:38:27,029
dispatch once you get to the CPU

00:38:25,349 --> 00:38:29,129
by the way Nvidia doesn't care how you

00:38:27,029 --> 00:38:30,329
get to the GPU because why should they

00:38:29,129 --> 00:38:32,009
okay

00:38:30,329 --> 00:38:33,899
they just think they just know that once

00:38:32,009 --> 00:38:36,509
you get to the GPU they know how to

00:38:33,899 --> 00:38:38,639
blast everything in a bog dispatch fire

00:38:36,509 --> 00:38:40,889
off a million threads and make it all

00:38:38,639 --> 00:38:42,599
run in high throughput manner how you

00:38:40,889 --> 00:38:44,099
get the data that you know what remember

00:38:42,599 --> 00:38:45,689
coordinates talk about the data move and

00:38:44,099 --> 00:38:47,309
how you get the data that's your problem

00:38:45,689 --> 00:38:48,599
thank you very much

00:38:47,309 --> 00:38:53,249
just get it there and we're gonna make

00:38:48,599 --> 00:38:55,979
it run really fast we need that the

00:38:53,249 --> 00:38:58,439
other thing of course is I I repeated

00:38:55,979 --> 00:38:59,069
hpx cuz I love it so much but that was

00:38:58,439 --> 00:39:00,929
an accident

00:38:59,069 --> 00:39:03,479
but the heterogeneous computing compiler

00:39:00,929 --> 00:39:06,749
from HSA why do we care about that the

00:39:03,479 --> 00:39:10,139
AMD guys too initiated the designs for

00:39:06,749 --> 00:39:12,809
for what they call APU integrating the

00:39:10,139 --> 00:39:14,579
CPU and the GPU onto a single chip what

00:39:12,809 --> 00:39:16,559
that gives you what that offers you is

00:39:14,579 --> 00:39:18,479
tremendous low latency one of the

00:39:16,559 --> 00:39:20,309
biggest problem you'll remember is this

00:39:18,479 --> 00:39:23,129
data movement you know the chart that

00:39:20,309 --> 00:39:25,439
the golden was talking about apu solves

00:39:23,129 --> 00:39:28,769
that problem in a unique way using user

00:39:25,439 --> 00:39:30,929
cues okay and we need to learn from that

00:39:28,769 --> 00:39:33,359
if you if you know I share this SG 14

00:39:30,929 --> 00:39:35,099
study group and in order for C++ to work

00:39:33,359 --> 00:39:38,519
at different architectures whether it's

00:39:35,099 --> 00:39:40,349
a discrete CP GPU or an integrated GPU

00:39:38,519 --> 00:39:41,819
we need to be able to learn from that

00:39:40,349 --> 00:39:44,299
and that's why I'm taking a look at

00:39:41,819 --> 00:39:46,440
those four models okay

00:39:44,299 --> 00:39:47,729
ultimately what's gonna come out is that

00:39:46,440 --> 00:39:49,920
we're gonna be able to hopefully support

00:39:47,729 --> 00:39:53,069
massive parallelism on multiple

00:39:49,920 --> 00:39:54,989
distributor notes which CPU or GPU and

00:39:53,069 --> 00:39:56,549
baseman head starts in games and

00:39:54,989 --> 00:39:58,529
graphics I won't go over to this too

00:39:56,549 --> 00:40:00,930
much because this is essentially about

00:39:58,529 --> 00:40:03,180
with how it all came or it can't come

00:40:00,930 --> 00:40:06,779
from but what I do want to talk about is

00:40:03,180 --> 00:40:09,469
how we're gonna get there in order to

00:40:06,779 --> 00:40:14,989
get to GPU computing we need executives

00:40:09,469 --> 00:40:17,640
executives are these unique creatures

00:40:14,989 --> 00:40:21,380
are essentially to function execution

00:40:17,640 --> 00:40:24,809
what alligators are to memory allocation

00:40:21,380 --> 00:40:28,140
they're also what iterators are to STL

00:40:24,809 --> 00:40:30,989
they marry iterators marry between

00:40:28,140 --> 00:40:32,789
containers and algorithms executives

00:40:30,989 --> 00:40:35,549
marry between the constructs of

00:40:32,789 --> 00:40:37,380
parallelism like for loops parallel

00:40:35,549 --> 00:40:39,269
regions with the resources that you're

00:40:37,380 --> 00:40:41,729
gonna use to execute that maybe they

00:40:39,269 --> 00:40:44,999
might be a GPU node here an open MP

00:40:41,729 --> 00:40:47,130
cluster okay a bunch of threads a thread

00:40:44,999 --> 00:40:48,799
pool if you don't have executives we're

00:40:47,130 --> 00:40:51,479
gonna have what's called an end-to-end

00:40:48,799 --> 00:40:53,219
relationship where you're gonna have you

00:40:51,479 --> 00:40:54,989
know every for each we'll have to have

00:40:53,219 --> 00:40:57,210
something that says this for each is

00:40:54,989 --> 00:40:59,099
used to access a thread pool then you

00:40:57,210 --> 00:41:01,380
have another for each for openmp runtime

00:40:59,099 --> 00:41:02,729
then you have a sink for fibers you

00:41:01,380 --> 00:41:05,190
don't really want a world where you have

00:41:02,729 --> 00:41:07,229
that end to end between the control

00:41:05,190 --> 00:41:08,999
structures and the diverse execution

00:41:07,229 --> 00:41:11,039
resource if it can all go through an

00:41:08,999 --> 00:41:16,200
executor the executor can effectively

00:41:11,039 --> 00:41:19,440
select the execution resource based on

00:41:16,200 --> 00:41:21,869
you your policy parameters you can now

00:41:19,440 --> 00:41:26,279
select where to run this stuff when who

00:41:21,869 --> 00:41:27,660
run it and how to run it okay the how is

00:41:26,279 --> 00:41:30,359
already supplied essentially by the

00:41:27,660 --> 00:41:32,729
controls control architectures C++ now

00:41:30,359 --> 00:41:35,519
already have quite a few house we can do

00:41:32,729 --> 00:41:37,739
async we can do package tasks we can do

00:41:35,519 --> 00:41:39,359
basic threads we can do futures we do a

00:41:37,739 --> 00:41:41,700
couple of other things okay

00:41:39,359 --> 00:41:43,019
but right now C++ has none of the

00:41:41,700 --> 00:41:44,969
concepts at the bottom we don't have

00:41:43,019 --> 00:41:51,400
thread pools we don't have ideas about C

00:41:44,969 --> 00:41:56,499
PGP we only know about CPUs all right

00:41:51,400 --> 00:41:57,880
so there are several computing competing

00:41:56,499 --> 00:41:59,589
proposals and they be running around

00:41:57,880 --> 00:42:01,809
each other for the last three years we

00:41:59,589 --> 00:42:03,160
last this last summer we started talking

00:42:01,809 --> 00:42:05,200
to them and making sure we've been

00:42:03,160 --> 00:42:07,240
having you know I almost bi-weekly

00:42:05,200 --> 00:42:09,730
telecon calls and we're making really

00:42:07,240 --> 00:42:11,170
good progress and right now this is what

00:42:09,730 --> 00:42:12,940
it's coming down to looking like it's

00:42:11,170 --> 00:42:14,440
gonna be look like a minimal proposal

00:42:12,940 --> 00:42:17,019
that supports this kind of chart

00:42:14,440 --> 00:42:18,430
hierarchy where it's a foundation for

00:42:17,019 --> 00:42:20,559
later proposals for heterogeneous

00:42:18,430 --> 00:42:21,400
computing this is only glimpse of some

00:42:20,559 --> 00:42:23,200
of the work we're doing we're gonna

00:42:21,400 --> 00:42:24,460
submit a proposed a paper for Issaquah

00:42:23,200 --> 00:42:26,230
for this so I don't want to go too

00:42:24,460 --> 00:42:29,019
deeply but you'll notice that in this

00:42:26,230 --> 00:42:31,960
diagram you essentially have an executor

00:42:29,019 --> 00:42:34,450
which has one or more of execution

00:42:31,960 --> 00:42:36,490
functions that would create lightweight

00:42:34,450 --> 00:42:39,279
execution agents which then execute

00:42:36,490 --> 00:42:40,839
execution these instruction streams on

00:42:39,279 --> 00:42:43,150
the other hand at the very top year

00:42:40,839 --> 00:42:45,190
these contacts which runs these

00:42:43,150 --> 00:42:47,230
execution agents and these contacts

00:42:45,190 --> 00:42:50,619
would manage these execution resources

00:42:47,230 --> 00:42:53,079
aka like a thread pool and the and they

00:42:50,619 --> 00:42:55,839
would have execute them on instances of

00:42:53,079 --> 00:42:59,769
execution platforms I just said all that

00:42:55,839 --> 00:43:01,539
sorry just a quick recap sigh my got

00:42:59,769 --> 00:43:03,489
about ten minutes left now essentially

00:43:01,539 --> 00:43:06,489
there is also simply pair ilysm that's

00:43:03,489 --> 00:43:08,739
required one of the key markers of GPU

00:43:06,489 --> 00:43:11,259
computing is that it's it's almost all a

00:43:08,739 --> 00:43:12,970
lot of it is Cindy in nature you can't

00:43:11,259 --> 00:43:15,460
do it without it so that was the other

00:43:12,970 --> 00:43:18,220
component that was missing okay without

00:43:15,460 --> 00:43:19,839
it we're not gonna go anywhere we don't

00:43:18,220 --> 00:43:22,210
have a standard for Cindy computing

00:43:19,839 --> 00:43:24,910
there's boost's md every vendor has

00:43:22,210 --> 00:43:26,680
invented some sort of sim D capabilities

00:43:24,910 --> 00:43:29,289
but everybody programs that using

00:43:26,680 --> 00:43:31,390
built-in languages dopin of function

00:43:29,289 --> 00:43:32,950
calls that supplied by the vendor it's

00:43:31,390 --> 00:43:35,650
terrible every you know you see you

00:43:32,950 --> 00:43:36,489
should you switch from sse2 to a BX or

00:43:35,650 --> 00:43:37,749
to IBM's

00:43:36,489 --> 00:43:41,079
I can know what they are even now used

00:43:37,749 --> 00:43:43,089
to work for um you know you would have

00:43:41,079 --> 00:43:46,539
to be right there all your color that we

00:43:43,089 --> 00:43:48,160
love doing this own stuff so there's

00:43:46,539 --> 00:43:50,049
been two proposals against here there's

00:43:48,160 --> 00:43:52,599
a competition there's been two proposals

00:43:50,049 --> 00:43:54,130
put out one is based on boo Cindy Jo Val

00:43:52,599 --> 00:43:55,809
who I think is gonna talk about all has

00:43:54,130 --> 00:43:58,569
already talked about it's by somebody

00:43:55,809 --> 00:44:00,099
named Matias Kanade okay and then a

00:43:58,569 --> 00:44:02,380
competing proposal came out based on the

00:44:00,099 --> 00:44:04,990
vc library also by someone whose name is

00:44:02,380 --> 00:44:08,230
very similar to my peers in this case

00:44:04,990 --> 00:44:10,450
here's Kratz so it gets very confusing

00:44:08,230 --> 00:44:15,850
to say whose proposal this is it's it's

00:44:10,450 --> 00:44:18,190
Matias is proposal okay and what it is

00:44:15,850 --> 00:44:19,330
is now it's settling to the point that

00:44:18,190 --> 00:44:22,810
it's gonna look like something like this

00:44:19,330 --> 00:44:25,930
it's gonna be a Dana paw that has a that

00:44:22,810 --> 00:44:28,570
holds elements and elements of type T

00:44:25,930 --> 00:44:30,070
that's written particularly for Cindy

00:44:28,570 --> 00:44:31,600
ratchet register okay

00:44:30,070 --> 00:44:33,190
now this morning a gentleman asked me

00:44:31,600 --> 00:44:35,080
what if I have a different ABI will day

00:44:33,190 --> 00:44:37,480
is an a B there's a default that ABI

00:44:35,080 --> 00:44:40,480
here assume but you can there's an ABI

00:44:37,480 --> 00:44:42,490
parameter that you can add in if you

00:44:40,480 --> 00:44:44,560
have a different ABI and this is really

00:44:42,490 --> 00:44:46,390
important because you know that this is

00:44:44,560 --> 00:44:48,040
a fundamental thing with Cindy I'm not

00:44:46,390 --> 00:44:50,080
gonna go too deeply into this there are

00:44:48,040 --> 00:44:52,210
built-in operators there's no promotions

00:44:50,080 --> 00:44:54,280
but some of this stuff you can get from

00:44:52,210 --> 00:44:56,320
the slides I do want to get to the final

00:44:54,280 --> 00:44:59,560
thing this is that with this we're gonna

00:44:56,320 --> 00:45:01,510
be able to have the ability to with the

00:44:59,560 --> 00:45:04,840
minimal proposals for executors we're

00:45:01,510 --> 00:45:08,760
going to be able to get to a space with

00:45:04,840 --> 00:45:11,680
sim D to a point where we can eventually

00:45:08,760 --> 00:45:13,420
get to this this this space now we do

00:45:11,680 --> 00:45:15,040
have to do other things we do the things

00:45:13,420 --> 00:45:17,109
that golden talked about how do what do

00:45:15,040 --> 00:45:18,790
we do about the address space c++

00:45:17,109 --> 00:45:21,460
assumes a single address space single

00:45:18,790 --> 00:45:23,170
flat address space we have to either do

00:45:21,460 --> 00:45:24,609
move to some sort of either multiple

00:45:23,170 --> 00:45:27,280
address space that's either cache

00:45:24,609 --> 00:45:29,280
coherent or cache noncoherent we have to

00:45:27,280 --> 00:45:32,410
make decisions as to whether we should

00:45:29,280 --> 00:45:34,480
support either implicit data movement or

00:45:32,410 --> 00:45:37,720
explicit data movement I kind of think

00:45:34,480 --> 00:45:40,180
we need both we have to get to a few

00:45:37,720 --> 00:45:43,260
other stage about should we support

00:45:40,180 --> 00:45:46,030
legacy GPU devices they screed or or or

00:45:43,260 --> 00:45:48,040
just integrate it we probably need to

00:45:46,030 --> 00:45:48,940
need almost all of them to some extent

00:45:48,040 --> 00:45:51,910
because we don't know whether we're

00:45:48,940 --> 00:45:53,410
doing on an FPGA okay or just a pure

00:45:51,910 --> 00:45:55,000
accelerator so there are a couple of

00:45:53,410 --> 00:45:56,260
things but ultimately I want to take the

00:45:55,000 --> 00:45:57,540
thing I want to take you you guys want

00:45:56,260 --> 00:45:59,440
to take away from this is that

00:45:57,540 --> 00:46:00,280
heterogeneous computing has been coming

00:45:59,440 --> 00:46:03,940
for a long time

00:46:00,280 --> 00:46:07,090
and that sickle right now today and a

00:46:03,940 --> 00:46:09,550
few other candidates like HP X like HTTP

00:46:07,090 --> 00:46:14,140
is able to supply heterogeneous

00:46:09,550 --> 00:46:17,470
computing on C++ okay but depending on

00:46:14,140 --> 00:46:18,270
the on the type of devices they might

00:46:17,470 --> 00:46:21,180
the

00:46:18,270 --> 00:46:23,040
support my very and finally that we're

00:46:21,180 --> 00:46:25,740
also trying to add this to the C++

00:46:23,040 --> 00:46:27,510
standard okay and that's pretty much

00:46:25,740 --> 00:46:28,560
closed as the talk and I think the only

00:46:27,510 --> 00:46:30,690
thing that I want to mention of course

00:46:28,560 --> 00:46:31,950
is that now we just last week the

00:46:30,690 --> 00:46:33,810
company has been working to code play

00:46:31,950 --> 00:46:36,360
company has been working really hard at

00:46:33,810 --> 00:46:38,550
releasing compute cpp as a community

00:46:36,360 --> 00:46:40,350
edition which is now a free for download

00:46:38,550 --> 00:46:42,540
and you can just go to this address it

00:46:40,350 --> 00:46:45,150
essentially is an open source sickle

00:46:42,540 --> 00:46:47,010
project that has the compute CPP SDK

00:46:45,150 --> 00:46:49,740
which is a collection of sample code and

00:46:47,010 --> 00:46:51,840
integration tools it also is the one of

00:46:49,740 --> 00:46:54,030
the only one of the few implementations

00:46:51,840 --> 00:46:58,260
of parallel STL the thing that's already

00:46:54,030 --> 00:47:00,660
in C++ 17 okay and it's a sickle based

00:46:58,260 --> 00:47:03,180
implementation the difference is that it

00:47:00,660 --> 00:47:05,610
actually impacts on this GPU these these

00:47:03,180 --> 00:47:08,670
these these parallel SCO teen runs on

00:47:05,610 --> 00:47:10,620
the runs on the GPU not just on the CPU

00:47:08,670 --> 00:47:12,450
can also run on the CPU as well to most

00:47:10,620 --> 00:47:13,950
implementation would only run on the CPU

00:47:12,450 --> 00:47:17,340
because that's all the standard actually

00:47:13,950 --> 00:47:20,070
mandates it has something called vision

00:47:17,340 --> 00:47:22,080
CPP for vision Processing's for

00:47:20,070 --> 00:47:26,160
self-driving car this is unbelievably

00:47:22,080 --> 00:47:28,560
useful okay it's the basis of why things

00:47:26,160 --> 00:47:31,710
crashed and why they don't crash okay

00:47:28,560 --> 00:47:34,110
and because we're using C++ algorithms

00:47:31,710 --> 00:47:36,360
to process images at a high enough speed

00:47:34,110 --> 00:47:38,520
such that they can be responsive enough

00:47:36,360 --> 00:47:40,170
for safety critical demands you know

00:47:38,520 --> 00:47:42,060
your brake fails you have to have that

00:47:40,170 --> 00:47:45,000
light show up on your dashboard within

00:47:42,060 --> 00:47:47,550
less than 5 nano milliseconds ok

00:47:45,000 --> 00:47:49,500
otherwise you're not you're not safe the

00:47:47,550 --> 00:47:51,840
other thing of course is the eigen C++

00:47:49,500 --> 00:47:53,490
library something that is that allows so

00:47:51,840 --> 00:47:54,960
this is all this is demonstrating what

00:47:53,490 --> 00:47:55,770
you can do with sickle you can build on

00:47:54,960 --> 00:47:58,050
top of sickle

00:47:55,770 --> 00:48:00,540
such that you can execute it for deep

00:47:58,050 --> 00:48:02,250
learning neural networks why is this

00:48:00,540 --> 00:48:03,870
important if you haven't heard about

00:48:02,250 --> 00:48:05,970
what's going on in the in the world now

00:48:03,870 --> 00:48:08,090
almost everything you're using right now

00:48:05,970 --> 00:48:12,450
uses some sort of deep learning when you

00:48:08,090 --> 00:48:14,100
do a Google Translation it's doing some

00:48:12,450 --> 00:48:16,350
sort of deep learning going back to the

00:48:14,100 --> 00:48:18,390
central server and responding to you

00:48:16,350 --> 00:48:20,820
when you're doing image recognition that

00:48:18,390 --> 00:48:22,740
is all C++ deep learning using something

00:48:20,820 --> 00:48:24,750
called tensorflow and you can now

00:48:22,740 --> 00:48:26,520
potentially do and in my future talks

00:48:24,750 --> 00:48:28,410
I'm gonna talk a lot more about what

00:48:26,520 --> 00:48:31,170
these things can do and that's the

00:48:28,410 --> 00:48:32,130
potential that we can get to in this

00:48:31,170 --> 00:48:34,500
future world

00:48:32,130 --> 00:48:35,820
that we're envisioning right now so

00:48:34,500 --> 00:48:37,560
hopefully you guys can join us and

00:48:35,820 --> 00:48:40,740
download some of this some of this stuff

00:48:37,560 --> 00:48:42,900
our team is ready to support the sickle

00:48:40,740 --> 00:48:44,850
but in the meantime my job isn't

00:48:42,900 --> 00:48:46,560
necessarily to my company's horn but is

00:48:44,850 --> 00:48:49,110
to try to bring this stuff to the

00:48:46,560 --> 00:48:50,610
standard not necessarily sickle but

00:48:49,110 --> 00:48:52,470
it'll be something that combines with

00:48:50,610 --> 00:48:54,360
sickle and other things like what we

00:48:52,470 --> 00:48:57,480
know about from HP X what we know about

00:48:54,360 --> 00:48:59,910
from Nvidia what we know about from from

00:48:57,480 --> 00:49:01,650
HTC I know Google already also has a

00:48:59,910 --> 00:49:03,720
cuda implementation we're trying to take

00:49:01,650 --> 00:49:05,820
all that learning and add that to the

00:49:03,720 --> 00:49:08,220
C++ standard so the ultimate result is

00:49:05,820 --> 00:49:09,630
something that works really well for C++

00:49:08,220 --> 00:49:13,830
thank you very much

00:49:09,630 --> 00:49:16,500
questions I don't know if I have any

00:49:13,830 --> 00:49:20,210
other slides ah this has been conclusion

00:49:16,500 --> 00:49:23,100
I'm also leave that up here okay

00:49:20,210 --> 00:49:25,380
discussions questions oh the one thing I

00:49:23,100 --> 00:49:27,660
do I I'll take one question but there if

00:49:25,380 --> 00:49:30,330
you're doing a fpga with sickle there's

00:49:27,660 --> 00:49:32,190
an open-source one called tricycle that

00:49:30,330 --> 00:49:45,550
is done by silence that is doing it go

00:49:32,190 --> 00:49:50,260
ahead please yes I have yes yes I know

00:49:45,550 --> 00:49:52,900
I totally understand what you're saying

00:49:50,260 --> 00:49:53,980
um the question is could it in the last

00:49:52,900 --> 00:49:56,590
couple of years have improved

00:49:53,980 --> 00:50:01,300
significantly for C++ and they have they

00:49:56,590 --> 00:50:03,670
have implemented most of C++ so I would

00:50:01,300 --> 00:50:05,500
be saying that secretly it's rude has

00:50:03,670 --> 00:50:07,510
always been seen okay

00:50:05,500 --> 00:50:09,160
of that I'm I don't think people would

00:50:07,510 --> 00:50:10,510
disagree in the last couple of years

00:50:09,160 --> 00:50:12,820
they've done tremendous progress

00:50:10,510 --> 00:50:15,070
- what C++ in particular we admire how

00:50:12,820 --> 00:50:18,010
they've been able to do passing

00:50:15,070 --> 00:50:19,630
functions from the host to the to the

00:50:18,010 --> 00:50:23,290
device but they are essentially the

00:50:19,630 --> 00:50:24,790
entire compilation a compilation unit so

00:50:23,290 --> 00:50:26,950
yes they still so that's why I wanted to

00:50:24,790 --> 00:50:28,870
mention CUDA and I know Google's effort

00:50:26,950 --> 00:50:31,300
on could CUDA that we're still also

00:50:28,870 --> 00:50:36,130
learning from them as well - so no we're

00:50:31,300 --> 00:50:37,660
definitely not ignoring CUDA the one

00:50:36,130 --> 00:50:41,920
thing though CUDA is driving - what is

00:50:37,660 --> 00:50:44,950
more F a unified unified virtual memory

00:50:41,920 --> 00:50:47,440
okay and that's the latest thing so and

00:50:44,950 --> 00:50:48,940
that's the latest thing that they are

00:50:47,440 --> 00:50:51,460
trying to do so that's one model we're

00:50:48,940 --> 00:50:53,110
also at apt adapting as well to be using

00:50:51,460 --> 00:50:57,160
that stuff that unifies which will never

00:50:53,110 --> 00:50:58,690
system okay they do have good stuff they

00:50:57,160 --> 00:51:00,250
let the industry for quite some time and

00:50:58,690 --> 00:51:02,310
they probably will continue yeah go

00:51:00,250 --> 00:51:02,310
ahead

00:51:15,180 --> 00:51:19,829
that's it so the question is the sicko

00:51:16,920 --> 00:51:28,050
offer a notion of time control like real

00:51:19,829 --> 00:51:30,030
time I assume this is for real yeah so

00:51:28,050 --> 00:51:32,220
in the current cycle specifications is

00:51:30,030 --> 00:51:34,980
this there's no M there's no API for

00:51:32,220 --> 00:51:38,329
measuring time one on spaces so cycle is

00:51:34,980 --> 00:51:41,460
based on the feature support of OpenCL

00:51:38,329 --> 00:51:43,500
so cycle can only provide the right sort

00:51:41,460 --> 00:52:08,430
of hardware support for open sale

00:51:43,500 --> 00:52:10,770
devices can support so the so the

00:52:08,430 --> 00:52:12,839
question is can you provide sort of

00:52:10,770 --> 00:52:15,420
device specializations off of certain

00:52:12,839 --> 00:52:17,520
functions yes so that is possible in

00:52:15,420 --> 00:52:19,140
cycle year there's there's M so there's

00:52:17,520 --> 00:52:20,730
macros you can use to see how I want

00:52:19,140 --> 00:52:23,310
this this code to be defined only for

00:52:20,730 --> 00:52:25,470
the GPU they're only for the the device

00:52:23,310 --> 00:52:28,790
basically so you can specialize host

00:52:25,470 --> 00:52:28,790
host a database a good

00:52:38,119 --> 00:52:43,230
so that's the question if you don't

00:52:40,260 --> 00:52:45,210
specify the device and does it choose an

00:52:43,230 --> 00:52:47,070
automatic device so there's so the

00:52:45,210 --> 00:52:49,859
example I showed has just showed the

00:52:47,070 --> 00:52:51,420
queue and because I saw the huge ass or

00:52:49,859 --> 00:52:53,609
a default selection picks a device for

00:52:51,420 --> 00:52:55,590
you but there is sort of a range of

00:52:53,609 --> 00:52:56,970
different options you can do and you can

00:52:55,590 --> 00:52:59,160
configure it to pick a very specific

00:52:56,970 --> 00:53:01,109
device where the device selector object

00:52:59,160 --> 00:53:02,369
which is basically like a a functor

00:53:01,109 --> 00:53:04,920
object where you provide a heuristic

00:53:02,369 --> 00:53:07,140
that allows you to see sort of fine to

00:53:04,920 --> 00:53:09,150
me pick a a device that's a GPU from

00:53:07,140 --> 00:53:11,130
this vendor or supports this number of

00:53:09,150 --> 00:53:13,170
execution units so you can you can be

00:53:11,130 --> 00:53:15,150
quite specific about it the the other

00:53:13,170 --> 00:53:17,550
thing that the cycle provides is you

00:53:15,150 --> 00:53:19,710
don't actually need a an open sale

00:53:17,550 --> 00:53:22,500
device in order to takes you so every

00:53:19,710 --> 00:53:24,840
every cycle implementation requires what

00:53:22,500 --> 00:53:27,480
could we call host device so everything

00:53:24,840 --> 00:53:28,950
that can be run on a device in cycle can

00:53:27,480 --> 00:53:32,130
also be run on the host with the same

00:53:28,950 --> 00:53:33,630
execution and memory expectations so you

00:53:32,130 --> 00:53:54,660
can do debug on any standard C++

00:53:33,630 --> 00:53:57,359
compiler so how do you have you sort of

00:53:54,660 --> 00:54:00,990
program differently for FPGAs in terms

00:53:57,359 --> 00:54:03,119
of terms of bits so the moment the LED

00:54:00,990 --> 00:54:06,420
so the open sale standard is still still

00:54:03,119 --> 00:54:08,790
working towards supporting FPGA and this

00:54:06,420 --> 00:54:10,500
there's a lot of fpga vendors in opencl

00:54:08,790 --> 00:54:11,880
so there's there's a lot of active work

00:54:10,500 --> 00:54:13,520
in the standards just now for trying to

00:54:11,880 --> 00:54:15,450
provide sort of a better

00:54:13,520 --> 00:54:18,630
standardization to cover the different

00:54:15,450 --> 00:54:19,500
sort of memory model execution model of

00:54:18,630 --> 00:54:21,180
every GS

00:54:19,500 --> 00:54:23,220
so there's there's still some work to be

00:54:21,180 --> 00:54:25,980
done there and sickled will continue to

00:54:23,220 --> 00:54:27,900
support all the versions of OpenCL so

00:54:25,980 --> 00:54:30,630
the moment cycle one point two supports

00:54:27,900 --> 00:54:31,890
opencl 1.2 hardware the layer layer or

00:54:30,630 --> 00:54:33,990
cycle will continue to support layer

00:54:31,890 --> 00:54:36,480
versions of open sale so all advances in

00:54:33,990 --> 00:54:38,970
an open CL and hardware and software

00:54:36,480 --> 00:54:41,750
capabilities will be represented in

00:54:38,970 --> 00:54:41,750
cycle as well

00:54:50,670 --> 00:54:56,410
so so so the question was so they're the

00:54:53,590 --> 00:54:59,200
example that showed at the end so so

00:54:56,410 --> 00:55:00,730
that was so that example you could you

00:54:59,200 --> 00:55:03,460
could do yourself if you if you don't

00:55:00,730 --> 00:55:05,170
load it compute CPP so that would work

00:55:03,460 --> 00:55:09,490
work now with the current version of

00:55:05,170 --> 00:55:12,130
computer view so at the moment computes

00:55:09,490 --> 00:55:14,170
equation also yeah so the question is

00:55:12,130 --> 00:55:16,210
there any particular so what target

00:55:14,170 --> 00:55:18,280
platforms to be support and at the

00:55:16,210 --> 00:55:21,609
moment compute CPP the Community Edition

00:55:18,280 --> 00:55:25,150
supports you been to 1404 and CentOS

00:55:21,609 --> 00:55:27,609
operating systems and we for Intel CPU

00:55:25,150 --> 00:55:30,010
and AMD GPU so we've tested we are

00:55:27,609 --> 00:55:32,050
certain number of different OpenGL

00:55:30,010 --> 00:55:34,960
drivers but the if when you download

00:55:32,050 --> 00:55:36,760
went down the compute PP the integration

00:55:34,960 --> 00:55:40,680
guide and Abel will give you some

00:55:36,760 --> 00:55:40,680

YouTube URL: https://www.youtube.com/watch?v=0Thv72yhxxw


