Title: CppCon 2016: D. Dechev & D. Zhang  “High Performance C++ Concurrent Transactional Data Structures"
Publication date: 2016-09-30
Playlist: CppCon 2016
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2016
—
In the session, we will discuss two strategies for implementing scalable transactional data structures using both locks and lock-free synchronizations. The locking strategy employs MRLock, which is a novel shared-memory resource allocation lock for multi-core processors. It uses a lock-free FIFO queue to manage locking requests in batches, which minimizes memory contention among threads. It is fast and designed as a drop-in replacement for the two-phase locking methods in C++11 and Boost library (std::lock() and boost::lock()). When combined with existing lock-based transaction synchronization techniques such as semantic locking and transaction boosting, it can be used by programmers who prefer lock-based code to implement high-performance transactional data structures on familiar grounds. The lock-free strategy is based on lock-free transactional transformation (LFTT), which uses transaction descriptor object to announce the transaction globally so that delayed threads can be helped. It is applicable to linked data structures such as linked lists and skip lists. The logical status of any node in the data structure depends on the status of the transaction descriptor that was embedded in it. Conflict transactions do not need to revert their operations as required in some of the existing methodologies. We will demonstrate the application of this strategy to existing lock-free lists and skip lists. 

We will also introduce a lock-free logarithmic search data structure based on multi-dimensional linked list. This brand new data structure is designed from ground up to achieve full potential for concurrent accesses. It has a distributed memory layout which alleviates contention. Write operations modify at most two nodes so interference among operations are brought down to a minimum. This data structure implements the collection/dictionary abstract data type, which is ubiquitous in modern applications. When combined with the above mentioned transactional strategies, this work could greatly benefit application developers who deal with data intensive scenarios such as in memory databases.


Preliminary version of source code can be accessed from https://ucf-cs.github.io/tlds/. The research work mentioned in this presentation can be accessed from http://cse.eecs.ucf.edu/bios/delizhang.html.
— 
Damian Dechev
Associate Professor, University of Central Florida
Dr. Damian Dechev is an Assistant Professor at the EECS Department at the University of Central Florida and the founder of the Computer Software Engineering - Scalable and Secure Systems Lab at UCF. He specializes in the design of scalable multiprocessor data structures and algorithms and has applied them in the design of real-time embedded space systems at NASA JPL and the HPC data-intensive applications at Sandia National Labs. 

Deli Zhang
Software Development Engineer, Microsoft
Deli Zhang received his Ph.D. in Computer Science at the University of Central Florida. He worked as a research assistant under the guidance of Dr. Damian Dechev. Deli specializes in developing non-blocking data structures and algorithms, applying non-blocking synchronization in existing performance critical applications. He has also collaborated with Sandia National Laboratory on large scale performance monitoring and simulation analysis.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,060 --> 00:00:03,750
good afternoon everyone so people are

00:00:02,100 --> 00:00:06,450
still coming so I'm a as well as the

00:00:03,750 --> 00:00:08,849
skillet started I'm going to talk about

00:00:06,450 --> 00:00:10,740
transactional data structures which is a

00:00:08,849 --> 00:00:12,870
step beyond non-blocking

00:00:10,740 --> 00:00:15,089
or concurrent data structures I'm Kelly

00:00:12,870 --> 00:00:16,890
and I recently graduated University of

00:00:15,089 --> 00:00:19,140
Central Florida and joined Microsoft and

00:00:16,890 --> 00:00:21,390
most of the talk is based on research

00:00:19,140 --> 00:00:23,880
projects that's been done in UCF

00:00:21,390 --> 00:00:27,210
together with my advisor dr. demian that

00:00:23,880 --> 00:00:29,699
you so in this talk I'm going to first

00:00:27,210 --> 00:00:31,980
this discuss what transactional data

00:00:29,699 --> 00:00:34,469
structures are and why we need them in

00:00:31,980 --> 00:00:36,420
concurrent programming and I'll explain

00:00:34,469 --> 00:00:38,210
three piece of methodologies that will

00:00:36,420 --> 00:00:40,860
enable us to build high-performance

00:00:38,210 --> 00:00:43,559
transactional data structures in c plus

00:00:40,860 --> 00:00:45,539
plus the multi resource lock lock free

00:00:43,559 --> 00:00:48,840
transactional transformation and the

00:00:45,539 --> 00:00:50,489
multi-dimensional list now after that I

00:00:48,840 --> 00:00:51,870
will show you some experiment result

00:00:50,489 --> 00:00:53,730
with the components and conclude the

00:00:51,870 --> 00:00:57,989
talk with a summary and discussion on

00:00:53,730 --> 00:00:59,760
future work now at the beginning of the

00:00:57,989 --> 00:01:02,850
mantiqueira we know we have to exploit

00:00:59,760 --> 00:01:04,229
parallelism and now we are already at a

00:01:02,850 --> 00:01:06,840
doorway of Manu korero

00:01:04,229 --> 00:01:10,530
here is a picture of the Intel Xeon Phi

00:01:06,840 --> 00:01:12,380
processors with 72 full-fledged X 80

00:01:10,530 --> 00:01:16,380
score as eighty six cores

00:01:12,380 --> 00:01:19,830
now this processor is still on the

00:01:16,380 --> 00:01:22,439
market in the following month is and in

00:01:19,830 --> 00:01:25,080
the same time we have this year's number

00:01:22,439 --> 00:01:27,659
one top 500 supercomputers goes to a new

00:01:25,080 --> 00:01:30,750
system built in China which has 260 risk

00:01:27,659 --> 00:01:33,420
of course the main core chips are here

00:01:30,750 --> 00:01:36,090
do we have admin software to take

00:01:33,420 --> 00:01:39,450
advantage of them for the past two

00:01:36,090 --> 00:01:40,860
decades researchers has been proposing a

00:01:39,450 --> 00:01:43,430
large name of non-block in data

00:01:40,860 --> 00:01:46,979
structures so these data structures

00:01:43,430 --> 00:01:48,990
besides the great scalability the most

00:01:46,979 --> 00:01:51,659
appealing aspect of them is that they

00:01:48,990 --> 00:01:53,850
support strong progress guarantees

00:01:51,659 --> 00:01:55,890
such as weight freedom log freedom and

00:01:53,850 --> 00:01:58,619
abstraction freedom with freedom

00:01:55,890 --> 00:02:00,299
requires that all thread in the system

00:01:58,619 --> 00:02:04,110
must make progress in finite number of

00:02:00,299 --> 00:02:05,939
steps so that no one will starve and the

00:02:04,110 --> 00:02:08,009
weaker log freedom requires that at

00:02:05,939 --> 00:02:09,899
least one thread is making progress so

00:02:08,009 --> 00:02:11,760
the whole system will not starve and

00:02:09,899 --> 00:02:12,989
obstruction freed and only requires a

00:02:11,760 --> 00:02:14,280
thread to make a progress without

00:02:12,989 --> 00:02:16,080
competing threads

00:02:14,280 --> 00:02:18,720
we'll be focusing on logarithm because

00:02:16,080 --> 00:02:21,240
it provides just enough progress

00:02:18,720 --> 00:02:23,580
guarantee without the overhead of weight

00:02:21,240 --> 00:02:27,270
free synchronization now the non block

00:02:23,580 --> 00:02:29,220
and data structures they have new seller

00:02:27,270 --> 00:02:31,290
for correctness properties the

00:02:29,220 --> 00:02:36,810
invisibility is the most widely adopted

00:02:31,290 --> 00:02:39,030
one inner eyes ability is compositional

00:02:36,810 --> 00:02:41,900
meaning that we can put multiple

00:02:39,030 --> 00:02:44,220
linearizable objects into a container

00:02:41,900 --> 00:02:46,860
library so that the library as a whole

00:02:44,220 --> 00:02:49,310
is still in the risible as a result we

00:02:46,860 --> 00:02:51,480
already have many of these libraries

00:02:49,310 --> 00:02:54,360
available on the internet such as lips

00:02:51,480 --> 00:02:57,510
EDS turbo and interest thread building

00:02:54,360 --> 00:02:59,760
blocks here is the graph showing the

00:02:57,510 --> 00:03:01,890
linearizable operations we have to

00:02:59,760 --> 00:03:04,380
thread access into linearizable objects

00:03:01,890 --> 00:03:06,330
a and b so by identifying the

00:03:04,380 --> 00:03:09,150
linearization point indicated by the

00:03:06,330 --> 00:03:11,730
arrows we could obtain a valid

00:03:09,150 --> 00:03:14,270
sequential history at the button so that

00:03:11,730 --> 00:03:17,640
the whole system is still in a riceball

00:03:14,270 --> 00:03:19,980
now all looks good and it seems we have

00:03:17,640 --> 00:03:22,049
been doing a lot work with this

00:03:19,980 --> 00:03:23,450
non-blocking data structure so can we

00:03:22,049 --> 00:03:26,459
stop researcher and put them to work

00:03:23,450 --> 00:03:28,200
unfortunately all existing concurrent

00:03:26,459 --> 00:03:30,360
data structures they miss a very

00:03:28,200 --> 00:03:32,220
important functionality which is allow

00:03:30,360 --> 00:03:34,799
users to compose multiple objects

00:03:32,220 --> 00:03:36,660
together correctly this is something we

00:03:34,799 --> 00:03:38,310
do effortlessly all the time with a

00:03:36,660 --> 00:03:40,680
conventional sequential data structures

00:03:38,310 --> 00:03:43,140
so here is a very simple example of a

00:03:40,680 --> 00:03:46,019
move function that tries to delete a key

00:03:43,140 --> 00:03:47,940
from set a and insert it into set B in

00:03:46,019 --> 00:03:50,220
sequential scenario this code will be

00:03:47,940 --> 00:03:53,700
trivially correct but in concurrent

00:03:50,220 --> 00:03:55,739
execution the operation might get into

00:03:53,700 --> 00:03:58,200
our interrupted right after deleting key

00:03:55,739 --> 00:04:03,030
from set a thus leaving the two set in

00:03:58,200 --> 00:04:06,030
an inconsistent state now what if we

00:04:03,030 --> 00:04:06,450
only have one container that's not going

00:04:06,030 --> 00:04:08,940
to help

00:04:06,450 --> 00:04:10,829
we still need additional synchronization

00:04:08,940 --> 00:04:13,170
in a second example we try to complete

00:04:10,829 --> 00:04:15,269
some value and put the new key value

00:04:13,170 --> 00:04:17,340
pair into the map only if the map

00:04:15,269 --> 00:04:19,169
doesn't already contain the key the

00:04:17,340 --> 00:04:21,359
thread might get interrupted after the

00:04:19,169 --> 00:04:23,610
condition check so the operation might

00:04:21,359 --> 00:04:26,610
wrongly overwrite an existing value in

00:04:23,610 --> 00:04:27,780
the map a more complicated example

00:04:26,610 --> 00:04:30,510
involves two

00:04:27,780 --> 00:04:32,880
hash tables consider program tries to

00:04:30,510 --> 00:04:35,250
maintain a forward map from a contact

00:04:32,880 --> 00:04:38,910
name to phone number and The Weavers map

00:04:35,250 --> 00:04:41,430
from the phone number to names if you

00:04:38,910 --> 00:04:43,350
want to update a phone number then what

00:04:41,430 --> 00:04:45,540
you do is you update entry in the

00:04:43,350 --> 00:04:48,240
contact map and you delete old entry

00:04:45,540 --> 00:04:50,760
from the full map and insert a new entry

00:04:48,240 --> 00:04:53,130
all this have to happen in one atomic

00:04:50,760 --> 00:04:54,690
steps but all concurrent there are

00:04:53,130 --> 00:04:57,990
structures they don't readily support

00:04:54,690 --> 00:05:00,840
this kind of transaction the problem is

00:04:57,990 --> 00:05:03,600
that linearizable operations are still

00:05:00,840 --> 00:05:06,090
not composable and the main programs are

00:05:03,600 --> 00:05:08,520
not aware of this issue so given the

00:05:06,090 --> 00:05:10,650
above example if the two hash tables

00:05:08,520 --> 00:05:12,870
were using logs then maybe the user

00:05:10,650 --> 00:05:15,360
can't get it to work by acquiring all

00:05:12,870 --> 00:05:17,640
logs at the same time but this require

00:05:15,360 --> 00:05:19,470
the maps to expose the internal logs to

00:05:17,640 --> 00:05:22,229
the user which breaks encapsulation or

00:05:19,470 --> 00:05:24,570
if the maps are constructing a lock free

00:05:22,229 --> 00:05:27,150
manner then there is nothing much user

00:05:24,570 --> 00:05:29,400
can do besides ask the library either to

00:05:27,150 --> 00:05:30,810
provide a new method that's manual

00:05:29,400 --> 00:05:34,910
composition which leads to state

00:05:30,810 --> 00:05:37,440
explosion so in order to support

00:05:34,910 --> 00:05:40,850
reusable software design in modular

00:05:37,440 --> 00:05:42,870
fashion we need to have a transactional

00:05:40,850 --> 00:05:45,479
transactional execution support in

00:05:42,870 --> 00:05:48,930
concurrent data structures now similar

00:05:45,479 --> 00:05:50,700
to a database transaction we say a data

00:05:48,930 --> 00:05:52,770
structure support transactional

00:05:50,700 --> 00:05:55,560
execution if we can execute a sequence

00:05:52,770 --> 00:05:58,380
of operations atomically and any

00:05:55,560 --> 00:06:00,390
isolation atomicity requires that if one

00:05:58,380 --> 00:06:02,940
operation fails the entire transaction

00:06:00,390 --> 00:06:05,700
should abort and isolation requires that

00:06:02,940 --> 00:06:07,140
the concurrent execution of transaction

00:06:05,700 --> 00:06:08,789
appears to take effect in some

00:06:07,140 --> 00:06:10,979
sequential order that respect the

00:06:08,789 --> 00:06:12,840
real-time ordering now if the data

00:06:10,979 --> 00:06:15,450
structures satisfy these two property we

00:06:12,840 --> 00:06:17,430
say L is strictly serializable this is

00:06:15,450 --> 00:06:20,370
analog of linearise ability for

00:06:17,430 --> 00:06:22,410
transactions now let's review some of

00:06:20,370 --> 00:06:24,270
the current applicable approaches that

00:06:22,410 --> 00:06:27,390
we can use to build some preliminary

00:06:24,270 --> 00:06:29,039
transactional data structures given a

00:06:27,390 --> 00:06:30,419
sequential data structure we can of

00:06:29,039 --> 00:06:33,240
course easily apply software

00:06:30,419 --> 00:06:35,370
transactional memory any code that's

00:06:33,240 --> 00:06:37,320
been wrapped between the TM Begay and TM

00:06:35,370 --> 00:06:40,380
and will be executed in the atomic

00:06:37,320 --> 00:06:41,849
fashion so here I show a simple example

00:06:40,380 --> 00:06:43,830
of in so

00:06:41,849 --> 00:06:46,919
function taking from a sequential order

00:06:43,830 --> 00:06:49,409
list what we need to do is to notify the

00:06:46,919 --> 00:06:51,869
STM when we read and write a shared

00:06:49,409 --> 00:06:55,199
memory location by using the TM read and

00:06:51,869 --> 00:06:58,249
the TM write macro we also need to like

00:06:55,199 --> 00:07:02,129
the STM take over the memory management

00:06:58,249 --> 00:07:04,830
so in principle an STM instruments read

00:07:02,129 --> 00:07:06,899
memory accesses which were calls the

00:07:04,830 --> 00:07:08,849
location of thread weed in reset and the

00:07:06,899 --> 00:07:11,219
rights in the right set conflicts are

00:07:08,849 --> 00:07:13,259
detected among the read white set among

00:07:11,219 --> 00:07:16,169
different thread so in the presence of

00:07:13,259 --> 00:07:18,059
conflict only one transaction is allowed

00:07:16,169 --> 00:07:22,740
to commit and the others will be aborted

00:07:18,059 --> 00:07:24,659
and restarted so because STM instrument

00:07:22,740 --> 00:07:27,360
slowly mow a low-level memory accesses

00:07:24,659 --> 00:07:29,610
it has large runtime overhead and this

00:07:27,360 --> 00:07:32,309
is one of the reason that STM hasn't

00:07:29,610 --> 00:07:34,709
been so widely used yet and a more

00:07:32,309 --> 00:07:36,990
important aspect is that the low-level

00:07:34,709 --> 00:07:39,479
memory access conflict they do not

00:07:36,990 --> 00:07:42,179
necessarily correspond to data structure

00:07:39,479 --> 00:07:45,079
level semantic conflicts so this way STM

00:07:42,179 --> 00:07:47,939
causes way more ports than these two

00:07:45,079 --> 00:07:50,129
here is an example consider we have a

00:07:47,939 --> 00:07:52,349
linked list and thread wire and thread

00:07:50,129 --> 00:07:55,800
two tries to insert key 4 and 1

00:07:52,349 --> 00:07:57,419
respectively now any good concurrent

00:07:55,800 --> 00:07:59,039
implementation of linked list should

00:07:57,419 --> 00:08:01,199
allow this to operation to proceed

00:07:59,039 --> 00:08:04,110
concurrently because they actually write

00:08:01,199 --> 00:08:06,779
to this joint memory locations but in an

00:08:04,110 --> 00:08:10,550
STM they do have read write conflict on

00:08:06,779 --> 00:08:14,149
node 0 so STM has to abort one of them

00:08:10,550 --> 00:08:16,409
to address the semantic conflict issue

00:08:14,149 --> 00:08:18,269
hurricane koskinen they propose the

00:08:16,409 --> 00:08:20,749
transactional boosting which is actually

00:08:18,269 --> 00:08:23,189
a very cool technique to obtain

00:08:20,749 --> 00:08:25,619
transactional data structure from base

00:08:23,189 --> 00:08:29,550
in res body or a structure in

00:08:25,619 --> 00:08:31,379
transactional posting all we need is

00:08:29,550 --> 00:08:33,180
linearizable data structure to start

00:08:31,379 --> 00:08:35,759
with and each operation will need to

00:08:33,180 --> 00:08:38,250
acquire an abstract lock before actually

00:08:35,759 --> 00:08:40,620
executed operation so the abstract locks

00:08:38,250 --> 00:08:42,269
are designed based on the knowledge of

00:08:40,620 --> 00:08:45,420
commutativity of the data structure

00:08:42,269 --> 00:08:47,639
operations so that they ensure that non

00:08:45,420 --> 00:08:49,769
commutative operations will never occur

00:08:47,639 --> 00:08:52,350
concurrently and in case of a

00:08:49,769 --> 00:08:54,450
transaction fell the transaction will

00:08:52,350 --> 00:08:56,550
invoke the inverses of already

00:08:54,450 --> 00:09:00,360
operations to restore the abstract state

00:08:56,550 --> 00:09:02,639
of the data structure now here is the

00:09:00,360 --> 00:09:05,130
specific specification of a Scylla data

00:09:02,639 --> 00:09:07,200
type we can see that insert and remove

00:09:05,130 --> 00:09:10,769
function they are actually committed

00:09:07,200 --> 00:09:14,850
with each other as long as the key the

00:09:10,769 --> 00:09:16,560
access are different so the abstract log

00:09:14,850 --> 00:09:19,800
of a cell Adel type will be associated

00:09:16,560 --> 00:09:22,170
with each day lucky and for example if a

00:09:19,800 --> 00:09:23,940
thread tries to insert key two and three

00:09:22,170 --> 00:09:26,639
at the same time in the transaction it

00:09:23,940 --> 00:09:28,380
needs to acquire actually two locks one

00:09:26,639 --> 00:09:32,910
associated with key two and the other

00:09:28,380 --> 00:09:34,649
associated with the key three now the

00:09:32,910 --> 00:09:36,870
problem with transactional boosting is

00:09:34,649 --> 00:09:39,329
that although it provides significant

00:09:36,870 --> 00:09:41,100
impaired performance the ASTM the use of

00:09:39,329 --> 00:09:43,139
locks degrades the progress guaranteed

00:09:41,100 --> 00:09:48,529
when you supply to log three data

00:09:43,139 --> 00:09:51,060
structures and as this graph shows when

00:09:48,529 --> 00:09:52,470
transaction failed to acquire a lock it

00:09:51,060 --> 00:09:55,050
has to roll back already

00:09:52,470 --> 00:09:57,089
a transaction in this case it needs to

00:09:55,050 --> 00:09:58,829
insert the node nine back into the data

00:09:57,089 --> 00:10:00,660
structure which causes additional

00:09:58,829 --> 00:10:02,100
overhead because this rollback process

00:10:00,660 --> 00:10:06,360
does not contribute to the overall

00:10:02,100 --> 00:10:08,430
throughput so I'm going to introduce

00:10:06,360 --> 00:10:09,750
some of the research what we did in

00:10:08,430 --> 00:10:12,930
order to build path performing

00:10:09,750 --> 00:10:15,510
transactional data structure in C++ the

00:10:12,930 --> 00:10:17,610
first piece is multi resource log which

00:10:15,510 --> 00:10:19,500
is a centralized lock manager that

00:10:17,610 --> 00:10:22,620
allows you to acquire multiple locks at

00:10:19,500 --> 00:10:24,779
once and this can work together with the

00:10:22,620 --> 00:10:26,220
transaction boosting technique for

00:10:24,779 --> 00:10:29,459
legacy log based data structure to

00:10:26,220 --> 00:10:31,410
convert them into law lock based

00:10:29,459 --> 00:10:32,699
transactional data structure and the

00:10:31,410 --> 00:10:34,800
second piece is the log for a

00:10:32,699 --> 00:10:37,670
transactional transformation this is

00:10:34,800 --> 00:10:39,510
actually a methodology to obtain

00:10:37,670 --> 00:10:41,940
high-performance transactional data

00:10:39,510 --> 00:10:43,860
structure from lock free based data

00:10:41,940 --> 00:10:46,319
structures so after the transform

00:10:43,860 --> 00:10:47,910
formation the data structure will still

00:10:46,319 --> 00:10:50,430
be log free while supporting

00:10:47,910 --> 00:10:52,800
transactions now the third piece is a

00:10:50,430 --> 00:10:54,630
multi-dimensional linked list which is a

00:10:52,800 --> 00:10:56,579
brand new search data structure designed

00:10:54,630 --> 00:11:00,500
from ground up that is optimized for

00:10:56,579 --> 00:11:02,699
concurrency and transaction support so

00:11:00,500 --> 00:11:05,310
lastly we need to put all these pieces

00:11:02,699 --> 00:11:07,640
together so that we can have a framework

00:11:05,310 --> 00:11:09,580
to support cross container

00:11:07,640 --> 00:11:12,050
actions and the user can compose

00:11:09,580 --> 00:11:13,840
transactions among multiple instance of

00:11:12,050 --> 00:11:16,460
data structures

00:11:13,840 --> 00:11:18,380
now before diving into details our

00:11:16,460 --> 00:11:20,180
briefly review some of the techniques

00:11:18,380 --> 00:11:21,980
that we used in the log free programming

00:11:20,180 --> 00:11:25,610
so the compare-and-swap is a atomic

00:11:21,980 --> 00:11:27,860
primitive that only updates a memory

00:11:25,610 --> 00:11:30,770
location if the current value matches

00:11:27,860 --> 00:11:32,660
our expectation and most clog-free

00:11:30,770 --> 00:11:34,820
programs are based on caste based with

00:11:32,660 --> 00:11:36,890
high loops in which the operation

00:11:34,820 --> 00:11:40,970
continuously to try to update a memory

00:11:36,890 --> 00:11:43,550
location using cache and until it a

00:11:40,970 --> 00:11:45,860
succeeds now the descriptive object is a

00:11:43,550 --> 00:11:48,560
small memory structure that will store

00:11:45,860 --> 00:11:51,530
the exclusion contacts of the operation

00:11:48,560 --> 00:11:54,710
in case the execution of the operation

00:11:51,530 --> 00:11:56,180
is delayed or interrupted another thread

00:11:54,710 --> 00:11:58,550
can pick this up and continue the

00:11:56,180 --> 00:12:00,740
execution the cooperative execution is a

00:11:58,550 --> 00:12:02,960
mechanism for thread to help enjoy the

00:12:00,740 --> 00:12:05,930
finish unfinished pending operations

00:12:02,960 --> 00:12:09,100
so a typical log free code will look

00:12:05,930 --> 00:12:11,840
like this operation starts with a new

00:12:09,100 --> 00:12:14,090
operation and creates a new descriptor

00:12:11,840 --> 00:12:16,610
for this current operation and it reads

00:12:14,090 --> 00:12:19,220
the current note and see if there is any

00:12:16,610 --> 00:12:21,550
pending operation if so it will sell out

00:12:19,220 --> 00:12:23,750
to help finish the pending operation and

00:12:21,550 --> 00:12:26,980
finally what tries to update the

00:12:23,750 --> 00:12:29,990
description current note using the cache

00:12:26,980 --> 00:12:32,600
now remember that transaction the

00:12:29,990 --> 00:12:35,660
boosting require an operation to acquire

00:12:32,600 --> 00:12:37,430
a abstract lock before execution so for

00:12:35,660 --> 00:12:39,170
multiple operations in the transaction

00:12:37,430 --> 00:12:41,660
the transaction poustinik

00:12:39,170 --> 00:12:44,210
technique requires a data structure to

00:12:41,660 --> 00:12:47,120
acquire magical locks so if we treat

00:12:44,210 --> 00:12:48,740
each day item as a resource then in this

00:12:47,120 --> 00:12:50,600
case we can actually reduce the

00:12:48,740 --> 00:12:52,970
transactional synchronization problem to

00:12:50,600 --> 00:12:55,070
resource relocation problem which tests

00:12:52,970 --> 00:12:57,560
that given pool of key resources that

00:12:55,070 --> 00:13:00,560
would fire exclusive access each thread

00:12:57,560 --> 00:13:01,790
we may request one to K resources and

00:13:00,560 --> 00:13:05,300
the thread shouldn't remain blocked

00:13:01,790 --> 00:13:07,580
until all the resources available now

00:13:05,300 --> 00:13:08,810
this can also seen as a generalization

00:13:07,580 --> 00:13:13,490
of the famous dining philosophers

00:13:08,810 --> 00:13:16,790
problem generally there are two strategy

00:13:13,490 --> 00:13:19,940
to approach this problem we can use

00:13:16,790 --> 00:13:20,899
locking protocols and assign a lock to

00:13:19,940 --> 00:13:23,600
individually

00:13:20,899 --> 00:13:26,240
this and acquired the locks one by one

00:13:23,600 --> 00:13:27,920
so this locking protocol has long been

00:13:26,240 --> 00:13:30,290
adapted by the database concurrency

00:13:27,920 --> 00:13:32,029
control such that we have the two phase

00:13:30,290 --> 00:13:34,610
locking resource hierarchy into this

00:13:32,029 --> 00:13:36,350
locking we're only allowed to acquire

00:13:34,610 --> 00:13:38,629
locks in one phase and release them in

00:13:36,350 --> 00:13:40,670
another and we suppose hierarchy we

00:13:38,629 --> 00:13:42,170
assigned Toto order to other locks and

00:13:40,670 --> 00:13:44,300
all threats should acquire them in the

00:13:42,170 --> 00:13:46,009
same order now the problem with these

00:13:44,300 --> 00:13:47,929
locking protocols is that especially

00:13:46,009 --> 00:13:49,790
with two phase locking is that when

00:13:47,929 --> 00:13:52,129
running on the shared memory system they

00:13:49,790 --> 00:13:55,300
are prone to conflict in a retry if the

00:13:52,129 --> 00:13:57,920
memory contention is level is high so

00:13:55,300 --> 00:13:59,779
another strategy is to actually acquire

00:13:57,920 --> 00:14:04,009
multiple locks in a batch and spatch

00:13:59,779 --> 00:14:06,249
walking Emma lock is such an example but

00:14:04,009 --> 00:14:09,139
before explaining it in details I will

00:14:06,249 --> 00:14:11,990
briefly show you the extended ta-tas

00:14:09,139 --> 00:14:16,389
lock which is a simplified version of

00:14:11,990 --> 00:14:19,939
Emma Lock now in the extended EITS lock

00:14:16,389 --> 00:14:22,879
each thread while we encode it Swiss was

00:14:19,939 --> 00:14:25,279
we passed in a bit set and each piece

00:14:22,879 --> 00:14:27,019
will represent one resource so the

00:14:25,279 --> 00:14:29,319
conflict among resource requests are

00:14:27,019 --> 00:14:33,050
detected using bit wide and operation

00:14:29,319 --> 00:14:35,029
between the value P and R and if there

00:14:33,050 --> 00:14:37,699
is no conflict the lock value is updated

00:14:35,029 --> 00:14:41,990
using the bitwise operation and the

00:14:37,699 --> 00:14:44,629
cache so this way the lock can handle

00:14:41,990 --> 00:14:47,509
requests in batch but there are a few

00:14:44,629 --> 00:14:47,990
drawbacks it provides no fairness

00:14:47,509 --> 00:14:50,179
guarantee

00:14:47,990 --> 00:14:52,220
so all words are contingent ending for

00:14:50,179 --> 00:14:54,620
the same memory location so whoever wins

00:14:52,220 --> 00:14:58,459
the cash wins the access to the request

00:14:54,620 --> 00:15:00,889
and it causes heavy contention that's

00:14:58,459 --> 00:15:04,519
the same reason because it has only one

00:15:00,889 --> 00:15:06,230
memory location for you to contend most

00:15:04,519 --> 00:15:09,439
importantly the only support a limited

00:15:06,230 --> 00:15:12,439
number of resources because on a 64-bit

00:15:09,439 --> 00:15:17,869
system you can only cast six four PS at

00:15:12,439 --> 00:15:21,319
the same time now in the Emma log we

00:15:17,869 --> 00:15:25,009
introduce a array based lock three 5oq

00:15:21,319 --> 00:15:27,259
to replace the actual memory location so

00:15:25,009 --> 00:15:30,740
that each thread has now allocated a

00:15:27,259 --> 00:15:33,320
cell to write its resource request this

00:15:30,740 --> 00:15:35,630
way we can guarantee five of Veronese am

00:15:33,320 --> 00:15:37,910
contenders and it can support unbounded

00:15:35,630 --> 00:15:40,750
number of resources and can handle

00:15:37,910 --> 00:15:44,000
contention value so how does Mr Locke

00:15:40,750 --> 00:15:46,280
works the graph on the Left shows that a

00:15:44,000 --> 00:15:49,040
thread well in Q is resourceful passing

00:15:46,280 --> 00:15:51,590
to the Q and it now scans from the hell

00:15:49,040 --> 00:15:54,860
of the queue for resource conflict with

00:15:51,590 --> 00:15:58,190
any outstanding resource requests for

00:15:54,860 --> 00:16:01,250
example in cell 5 a thread is requesting

00:15:58,190 --> 00:16:03,140
to access the last resource so when it

00:16:01,250 --> 00:16:06,530
scans from the head is this no conflicts

00:16:03,140 --> 00:16:08,960
now it's good to go it gains access to

00:16:06,530 --> 00:16:12,140
that resource but when another thread in

00:16:08,960 --> 00:16:14,660
queue is request in cell 6 it says

00:16:12,140 --> 00:16:17,030
conflicts with the outstanding requests

00:16:14,660 --> 00:16:20,360
in cell number 3 so it now it has to

00:16:17,030 --> 00:16:25,670
wait either is ping or whatever then

00:16:20,360 --> 00:16:27,470
when the cell when we win a thread

00:16:25,670 --> 00:16:29,900
holding the lock on cell number 3 we

00:16:27,470 --> 00:16:33,410
listed by selling the beasts to 0 the

00:16:29,900 --> 00:16:35,960
the thread now well scan on cell number

00:16:33,410 --> 00:16:39,650
4 because there is still one outstanding

00:16:35,960 --> 00:16:42,020
resource conflict now the data structure

00:16:39,650 --> 00:16:44,540
of the analog for each cell with torn

00:16:42,020 --> 00:16:45,950
bits that request as well as a sequence

00:16:44,540 --> 00:16:48,740
number which will be used as a

00:16:45,950 --> 00:16:50,330
synchronization signal and the analog

00:16:48,740 --> 00:16:53,570
data structure itself contains only

00:16:50,330 --> 00:16:57,200
atomic head and its help index values as

00:16:53,570 --> 00:16:59,750
well as the fixed size battery now

00:16:57,200 --> 00:17:02,720
during the initialization we allocate

00:16:59,750 --> 00:17:06,230
the buffer and store all the beads to

00:17:02,720 --> 00:17:08,510
ones and set the value of all the

00:17:06,230 --> 00:17:13,760
sequence number to equal to their cell

00:17:08,510 --> 00:17:16,579
index the acquire function contains two

00:17:13,760 --> 00:17:18,829
parts the first part is a cash based

00:17:16,579 --> 00:17:23,240
loop where we try to use cast you

00:17:18,829 --> 00:17:25,280
increment the tail position by 1 and if

00:17:23,240 --> 00:17:27,140
we succeed at this it means that we

00:17:25,280 --> 00:17:29,600
allocate a new cell for this request and

00:17:27,140 --> 00:17:31,340
we can write the request value into the

00:17:29,600 --> 00:17:33,290
bits set and increment the sequence

00:17:31,340 --> 00:17:36,110
number to indicate the cell has been

00:17:33,290 --> 00:17:38,390
taken now in the second part is a while

00:17:36,110 --> 00:17:40,350
better spin loop the spins from the head

00:17:38,390 --> 00:17:42,210
of the queue to detect

00:17:40,350 --> 00:17:46,169
and you outstanding resource conflict

00:17:42,210 --> 00:17:49,080
until reach the position allocated to

00:17:46,169 --> 00:17:51,779
this grad now once the local choir

00:17:49,080 --> 00:17:54,809
function returns it returns to the user

00:17:51,779 --> 00:17:59,279
a spin position which will serve it as a

00:17:54,809 --> 00:18:02,149
lock handle for four to be used in the

00:17:59,279 --> 00:18:05,090
release function so to release a lock

00:18:02,149 --> 00:18:08,970
the use of has a single lock handle and

00:18:05,090 --> 00:18:12,840
the cell the bit set in that associated

00:18:08,970 --> 00:18:15,000
cell will be set to zero and next we

00:18:12,840 --> 00:18:17,460
need to do one thing we need to clear

00:18:15,000 --> 00:18:19,860
from the head to advance the head index

00:18:17,460 --> 00:18:22,470
but we only do this if the bits that in

00:18:19,860 --> 00:18:25,950
the head cell has been already cleared

00:18:22,470 --> 00:18:28,740
and if so while we set the beats in the

00:18:25,950 --> 00:18:30,659
cell to whines and we well increment the

00:18:28,740 --> 00:18:32,850
sequence number in the bits that are in

00:18:30,659 --> 00:18:39,899
that cell to indicate the cell is free

00:18:32,850 --> 00:18:41,639
to be used again so here is how the

00:18:39,899 --> 00:18:44,879
sequence number changes throughout the

00:18:41,639 --> 00:18:47,789
lifecycle of the ring buffer initially

00:18:44,879 --> 00:18:49,409
we use the sequence number two to do

00:18:47,789 --> 00:18:53,309
balance checking and initially they were

00:18:49,409 --> 00:18:55,500
set to the index of the each cell with

00:18:53,309 --> 00:18:57,389
each in queue the sequence number

00:18:55,500 --> 00:18:59,250
increased by one so when the tail wraps

00:18:57,389 --> 00:19:01,740
around the deceased that the current

00:18:59,250 --> 00:19:04,379
tell index does not equal to the cell

00:19:01,740 --> 00:19:06,779
index so the cells taken it won't

00:19:04,379 --> 00:19:09,570
overrun the overwrite an existing value

00:19:06,779 --> 00:19:11,700
when the tell when the head advances and

00:19:09,570 --> 00:19:15,179
release a cell it will increase the

00:19:11,700 --> 00:19:18,360
sequence number by size minus one so

00:19:15,179 --> 00:19:20,789
that the cell index now equals to the

00:19:18,360 --> 00:19:25,200
tail index this indicates the cell is

00:19:20,789 --> 00:19:26,820
ready for reuse again the concurrent

00:19:25,200 --> 00:19:29,340
update of the ring buffer is safe

00:19:26,820 --> 00:19:30,840
because through the use of the sequence

00:19:29,340 --> 00:19:32,730
number will guarantee that head always

00:19:30,840 --> 00:19:35,669
precedes the tail and the tail is always

00:19:32,730 --> 00:19:39,090
larger than head by at most n where n is

00:19:35,669 --> 00:19:40,500
the size of the ring buffer now the

00:19:39,090 --> 00:19:42,629
important thing is that we don't need to

00:19:40,500 --> 00:19:44,309
use atomic operations to update this set

00:19:42,629 --> 00:19:46,529
anymore as we did with the extended

00:19:44,309 --> 00:19:48,929
t-80s log because the bit sets are

00:19:46,529 --> 00:19:51,240
initialized to ones and there are only

00:19:48,929 --> 00:19:53,429
there will be only one writer each time

00:19:51,240 --> 00:19:53,730
to update a bit set so in the presence

00:19:53,429 --> 00:19:56,400
of

00:19:53,730 --> 00:19:57,600
go writer intermediate value of the bits

00:19:56,400 --> 00:19:59,310
that you're in the right operation

00:19:57,600 --> 00:20:01,530
represents some super sets of the

00:19:59,310 --> 00:20:06,690
request resources that means we always

00:20:01,530 --> 00:20:09,390
block enough resources okay here's the

00:20:06,690 --> 00:20:12,240
very code quick code example to show you

00:20:09,390 --> 00:20:14,850
how to use a Murloc on existing

00:20:12,240 --> 00:20:16,380
concurrent data structure suppose we

00:20:14,850 --> 00:20:18,060
have a concurrent set which is

00:20:16,380 --> 00:20:22,470
implemented as a fine-grained locking

00:20:18,060 --> 00:20:24,840
list and we want to access key to seven

00:20:22,470 --> 00:20:27,000
five eight at the same time all we need

00:20:24,840 --> 00:20:30,120
to do is we create a resource request

00:20:27,000 --> 00:20:33,570
and pass the ITM a lock once the lock

00:20:30,120 --> 00:20:36,090
returns we can operate on the container

00:20:33,570 --> 00:20:39,330
as normal knowing that these kids won't

00:20:36,090 --> 00:20:43,620
be accessed by any other thread until we

00:20:39,330 --> 00:20:45,330
unlock so let's take a look at the

00:20:43,620 --> 00:20:49,680
problem of supporting lock free

00:20:45,330 --> 00:20:51,210
transactions given a lock free data

00:20:49,680 --> 00:20:53,700
structure we want to support

00:20:51,210 --> 00:20:56,280
transactions while maintaining the log

00:20:53,700 --> 00:20:58,440
free progress guarantee so we cannot use

00:20:56,280 --> 00:21:00,990
locks anymore or we cannot block any

00:20:58,440 --> 00:21:05,040
operations anymore so the illustration

00:21:00,990 --> 00:21:07,020
here shows you the challenge giving a

00:21:05,040 --> 00:21:10,560
lock for a linked list let's say we want

00:21:07,020 --> 00:21:13,230
to insert key for one and seven in one

00:21:10,560 --> 00:21:15,420
patch but at the same time there might

00:21:13,230 --> 00:21:18,150
be another threat concurrently deleting

00:21:15,420 --> 00:21:20,190
node one from the data structure before

00:21:18,150 --> 00:21:22,130
you finish the transaction so this

00:21:20,190 --> 00:21:24,510
actually leaves the first transaction in

00:21:22,130 --> 00:21:27,000
inconsistent States which breaks the

00:21:24,510 --> 00:21:29,850
isolation property so the challenge for

00:21:27,000 --> 00:21:32,670
lock free transaction synchronization is

00:21:29,850 --> 00:21:34,500
that we need to effectively buffer the

00:21:32,670 --> 00:21:36,420
write operations so that they are not

00:21:34,500 --> 00:21:38,730
visible to operations outside the scope

00:21:36,420 --> 00:21:40,710
of the transaction and we also need to

00:21:38,730 --> 00:21:43,260
efficiently row backfield transactions

00:21:40,710 --> 00:21:48,030
in order to restore the abstract data

00:21:43,260 --> 00:21:50,820
structure state now I'm introducing log

00:21:48,030 --> 00:21:53,400
three transactional transformation this

00:21:50,820 --> 00:21:55,980
is actually a methodology and to

00:21:53,400 --> 00:21:58,620
transform existing lock free data

00:21:55,980 --> 00:22:01,890
structure code into log free

00:21:58,620 --> 00:22:04,680
transactional data structure when you

00:22:01,890 --> 00:22:06,720
apply this methodology what we can do is

00:22:04,680 --> 00:22:07,419
we obtain a lock for each transaction or

00:22:06,720 --> 00:22:09,190
a structure it

00:22:07,419 --> 00:22:13,659
supports lock-free semantic conflict

00:22:09,190 --> 00:22:15,669
detection and it was supported well has

00:22:13,659 --> 00:22:18,070
a functionality to do logical status

00:22:15,669 --> 00:22:20,649
interpretation to limit actually it

00:22:18,070 --> 00:22:22,509
remains row backs and it has cooperative

00:22:20,649 --> 00:22:25,359
transaction execution to minimize the

00:22:22,509 --> 00:22:27,070
world so all these combined it provides

00:22:25,359 --> 00:22:29,409
confusing bad performance than the

00:22:27,070 --> 00:22:31,659
transaction boosting or STM or an

00:22:29,409 --> 00:22:35,559
existing data structure transaction

00:22:31,659 --> 00:22:37,779
approaches so currently this methodology

00:22:35,559 --> 00:22:40,359
can be applied to set and map data types

00:22:37,779 --> 00:22:42,940
and node based linked data structures

00:22:40,359 --> 00:22:48,700
such as linked lists binary search trees

00:22:42,940 --> 00:22:52,139
skip lists and under lists so in an oft

00:22:48,700 --> 00:22:54,999
T we use node based conflict detection

00:22:52,139 --> 00:22:57,070
for each node in the data structure we

00:22:54,999 --> 00:22:58,959
embed a node infrastructure which

00:22:57,070 --> 00:23:00,759
contains the operation ID and the

00:22:58,959 --> 00:23:03,489
reference to a transaction descriptor

00:23:00,759 --> 00:23:05,950
and in that transaction descriptor it is

00:23:03,489 --> 00:23:08,169
shared among all those participating in

00:23:05,950 --> 00:23:10,809
the same transaction so in it will all

00:23:08,169 --> 00:23:14,219
store array of operation types and

00:23:10,809 --> 00:23:16,509
operands as well as a transaction status

00:23:14,219 --> 00:23:19,690
the transaction status can be either

00:23:16,509 --> 00:23:22,209
committed active or boarded the concept

00:23:19,690 --> 00:23:24,549
here is that whenever a thread tries to

00:23:22,209 --> 00:23:26,709
modify a node it has to first read the

00:23:24,549 --> 00:23:29,259
note infrastructure and hence the

00:23:26,709 --> 00:23:30,969
transaction descriptor to see to make

00:23:29,259 --> 00:23:33,639
sure that the previous transaction is

00:23:30,969 --> 00:23:35,589
not active this way we prevent

00:23:33,639 --> 00:23:37,629
concurrent modification to the same node

00:23:35,589 --> 00:23:41,229
which is equivalent to prevent

00:23:37,629 --> 00:23:46,749
noncommutative operations from executing

00:23:41,229 --> 00:23:49,629
on the same key in a set now in the

00:23:46,749 --> 00:23:51,849
transform the insert function here is

00:23:49,629 --> 00:23:53,950
the workflow the great blocks are

00:23:51,849 --> 00:23:57,549
extracted from the base data structure

00:23:53,950 --> 00:24:00,309
so we keep those code we use the base

00:23:57,549 --> 00:24:02,649
data structures no Travis algorithm to

00:24:00,309 --> 00:24:05,440
locate the node content in the key K and

00:24:02,649 --> 00:24:08,229
if the node actually does not exist

00:24:05,440 --> 00:24:10,629
well well insert it as usual but if the

00:24:08,229 --> 00:24:13,029
node does exist we will introduce a new

00:24:10,629 --> 00:24:15,849
code pad to update an old infrastructure

00:24:13,029 --> 00:24:18,190
under a node if any of these two

00:24:15,849 --> 00:24:20,290
operations returned need to reach high

00:24:18,190 --> 00:24:26,320
while we start the whole process again

00:24:20,290 --> 00:24:28,510
so in the newly introduced update node

00:24:26,320 --> 00:24:30,820
info function what we do is that we

00:24:28,510 --> 00:24:32,950
first check if the node info has been

00:24:30,820 --> 00:24:35,320
pretty marked if so it means that the

00:24:32,950 --> 00:24:37,480
node has been logically deleted and we

00:24:35,320 --> 00:24:39,460
need to invoke the base data structures

00:24:37,480 --> 00:24:42,460
delete function to physically remove the

00:24:39,460 --> 00:24:44,350
node from the data structure and then we

00:24:42,460 --> 00:24:46,600
test if the previous transaction is

00:24:44,350 --> 00:24:49,180
still active and if so we will help

00:24:46,600 --> 00:24:51,100
finish that transaction this way we

00:24:49,180 --> 00:24:53,680
enforce the civilization at this point

00:24:51,100 --> 00:24:56,140
so that current transaction will always

00:24:53,680 --> 00:24:59,050
read the last final result of the

00:24:56,140 --> 00:25:02,140
previous transaction after that we can

00:24:59,050 --> 00:25:05,500
test if the key is logically present in

00:25:02,140 --> 00:25:08,440
a set or not I'll explain this in the

00:25:05,500 --> 00:25:11,050
next slide but for an insert function if

00:25:08,440 --> 00:25:13,540
the key does not exist it means that we

00:25:11,050 --> 00:25:18,700
can actually use cast to update the know

00:25:13,540 --> 00:25:20,830
the infrastructure on this node now here

00:25:18,700 --> 00:25:25,150
is the truth table for interpreting the

00:25:20,830 --> 00:25:28,120
logical status of actual key so the

00:25:25,150 --> 00:25:30,480
intuition behind this is that we don't

00:25:28,120 --> 00:25:33,220
actually need to execute the inverse

00:25:30,480 --> 00:25:35,140
operation like we did for a transaction

00:25:33,220 --> 00:25:36,610
the boosting technique in order to

00:25:35,140 --> 00:25:39,970
restore the data structure abstract

00:25:36,610 --> 00:25:42,340
state all we need to do is that one when

00:25:39,970 --> 00:25:44,200
a transaction fails the subsequent

00:25:42,340 --> 00:25:46,990
operation only needs to inversely

00:25:44,200 --> 00:25:49,900
interpret the logical status of the key

00:25:46,990 --> 00:25:52,720
in order to recover the correct abstract

00:25:49,900 --> 00:25:55,600
state so for an insert function if the

00:25:52,720 --> 00:25:59,580
previous transaction is committed we

00:25:55,600 --> 00:26:01,870
think the key is present in the set an

00:25:59,580 --> 00:26:05,740
aborted transaction means the key does

00:26:01,870 --> 00:26:07,960
not exist and for an active transactions

00:26:05,740 --> 00:26:11,410
we only consider the key to be present

00:26:07,960 --> 00:26:14,140
in the set on for this photo operations

00:26:11,410 --> 00:26:16,690
in the same transaction so the

00:26:14,140 --> 00:26:19,750
interpreter for a delete operation is

00:26:16,690 --> 00:26:21,820
exactly the opposite and the inspiration

00:26:19,750 --> 00:26:23,350
for fine operation is that we think the

00:26:21,820 --> 00:26:25,270
key always present because this is a

00:26:23,350 --> 00:26:30,580
read-only operation will not change the

00:26:25,270 --> 00:26:32,890
sellers of the key now once we done with

00:26:30,580 --> 00:26:34,169
the transformation of the base data

00:26:32,890 --> 00:26:37,320
structure code and up

00:26:34,169 --> 00:26:39,509
in transactional data structure what we

00:26:37,320 --> 00:26:43,399
do to execute operation is that we

00:26:39,509 --> 00:26:45,869
create a transaction descriptor and we

00:26:43,399 --> 00:26:48,509
specify the operations in descriptor and

00:26:45,869 --> 00:26:50,820
in that execution function it-- well

00:26:48,509 --> 00:26:52,529
execute the operations one by one it

00:26:50,820 --> 00:26:54,389
will continue to the next one only if

00:26:52,529 --> 00:26:56,669
the previous one has successfully

00:26:54,389 --> 00:26:59,100
returned and after all that it will

00:26:56,669 --> 00:27:02,249
atomically update the transaction status

00:26:59,100 --> 00:27:04,499
using the cache to either committed or

00:27:02,249 --> 00:27:07,739
aborted depends on if all proceedings

00:27:04,499 --> 00:27:09,929
are successful so there is one

00:27:07,739 --> 00:27:11,489
additional step is that we need to beat

00:27:09,929 --> 00:27:14,340
mark all the know the infrastructure

00:27:11,489 --> 00:27:16,200
unsuccessfully didn't note this way we

00:27:14,340 --> 00:27:18,450
can physically remove them afterwards

00:27:16,200 --> 00:27:21,480
this is optional but sometimes this

00:27:18,450 --> 00:27:25,169
helps performance the low the notes will

00:27:21,480 --> 00:27:26,909
not interfere with the correctness of

00:27:25,169 --> 00:27:29,159
the algorithm is just improve the

00:27:26,909 --> 00:27:31,950
performance if we remove the extra nodes

00:27:29,159 --> 00:27:35,639
from the data structure now there are a

00:27:31,950 --> 00:27:40,470
few salaries here very rarely a thread

00:27:35,639 --> 00:27:42,450
may stuck in endless open and this may

00:27:40,470 --> 00:27:44,639
because there are cyclic dependencies

00:27:42,450 --> 00:27:46,590
among transactions themselves so we

00:27:44,639 --> 00:27:49,200
detect and recover from this by using a

00:27:46,590 --> 00:27:52,320
help stack so before setup to help the

00:27:49,200 --> 00:27:54,389
other thread thread records the current

00:27:52,320 --> 00:27:57,419
transaction descriptor in the local help

00:27:54,389 --> 00:27:59,789
stack and cyclic dependencies are

00:27:57,419 --> 00:28:01,919
detected as a duplicate entry in the hop

00:27:59,789 --> 00:28:05,519
stack and then we will abort one of the

00:28:01,919 --> 00:28:07,529
transaction to break the cycle now if we

00:28:05,519 --> 00:28:11,399
disable the helping mechanism at all

00:28:07,529 --> 00:28:16,440
then it means that we have a abstraction

00:28:11,399 --> 00:28:21,359
free transaction execution the upside of

00:28:16,440 --> 00:28:23,789
this is that we can now keep the

00:28:21,359 --> 00:28:25,739
transact the execution contacts in the

00:28:23,789 --> 00:28:27,179
transaction descriptor and we can use

00:28:25,739 --> 00:28:29,369
many of the existing contention

00:28:27,179 --> 00:28:33,600
management schemes such as aggressive

00:28:29,369 --> 00:28:37,409
polite and camera so here is a graph

00:28:33,600 --> 00:28:40,019
showing you how oft to works with a very

00:28:37,409 --> 00:28:42,059
simple example three threads

00:28:40,019 --> 00:28:44,489
trying to execute three transactions

00:28:42,059 --> 00:28:47,580
thread one finish the first transaction

00:28:44,489 --> 00:28:48,120
by inserting node three and wine into

00:28:47,580 --> 00:28:52,140
the link

00:28:48,120 --> 00:28:54,600
and three to insert in note 4 but it's

00:28:52,140 --> 00:28:57,090
currently working on note 2 in the

00:28:54,600 --> 00:28:58,860
meantime through a three star deleted no

00:28:57,090 --> 00:29:01,470
three by updating the note

00:28:58,860 --> 00:29:03,960
infrastructure on that note when it

00:29:01,470 --> 00:29:05,760
tries to access note 4 it actually sees

00:29:03,960 --> 00:29:08,280
that this note is being accessed by

00:29:05,760 --> 00:29:11,070
active the transaction so it has to sell

00:29:08,280 --> 00:29:15,210
out help finish in t2 before resuming

00:29:11,070 --> 00:29:18,990
delete in old port so wunst 3 3 3

00:29:15,210 --> 00:29:22,980
completes its operation both t2 and t3

00:29:18,990 --> 00:29:25,350
should all be committed and the note at

00:29:22,980 --> 00:29:27,420
the key 3 & 4 will be interpreted as

00:29:25,350 --> 00:29:32,460
absent from the set by subsequent

00:29:27,420 --> 00:29:35,930
operations now this is a code symbol

00:29:32,460 --> 00:29:39,210
showing you how to actually use the

00:29:35,930 --> 00:29:42,330
transformed data structure given three

00:29:39,210 --> 00:29:44,430
sets instances and all we need to do is

00:29:42,330 --> 00:29:45,750
create a transaction descriptor and fill

00:29:44,430 --> 00:29:52,940
in the descriptor with a desired

00:29:45,750 --> 00:29:58,230
operations for each set and execute it

00:29:52,940 --> 00:30:00,720
now we're ready to talk about trans are

00:29:58,230 --> 00:30:03,990
two ways to implement transactional data

00:30:00,720 --> 00:30:06,270
structures and some data structures they

00:30:03,990 --> 00:30:08,430
can benefit from log 3 or the concurrent

00:30:06,270 --> 00:30:11,070
execution but some cannot for example a

00:30:08,430 --> 00:30:15,090
stack because of these are inherent

00:30:11,070 --> 00:30:17,040
sequential semantics we it cannot obtain

00:30:15,090 --> 00:30:21,480
much performance benefits even if we

00:30:17,040 --> 00:30:23,430
come early to log free but concrete Maps

00:30:21,480 --> 00:30:26,130
is one of those data structures that can

00:30:23,430 --> 00:30:28,650
benefit a lot from lock free and

00:30:26,130 --> 00:30:32,910
transactional institution because of its

00:30:28,650 --> 00:30:36,510
semantics is inherent parallel so in

00:30:32,910 --> 00:30:39,060
sequential scenario these maps are

00:30:36,510 --> 00:30:42,930
usually implemented in by binary search

00:30:39,060 --> 00:30:45,000
tree skip lists and hash trees now we

00:30:42,930 --> 00:30:48,090
definitely want to a better performing

00:30:45,000 --> 00:30:50,760
concurrent map this will improve the

00:30:48,090 --> 00:30:52,440
baseline performance because which have

00:30:50,760 --> 00:30:54,780
Earth's transaction synchronization

00:30:52,440 --> 00:30:57,650
scheme you choose this will bring down

00:30:54,780 --> 00:30:58,770
additional overhead and all existing

00:30:57,650 --> 00:31:00,890
implementations

00:30:58,770 --> 00:31:04,160
they all have concurrency

00:31:00,890 --> 00:31:06,980
patience so let's take a look at the

00:31:04,160 --> 00:31:09,370
binary search trees one problem with

00:31:06,980 --> 00:31:12,320
that is that is the binary search trees

00:31:09,370 --> 00:31:15,170
the logical other wing of the nodes of

00:31:12,320 --> 00:31:17,120
the keys does not necessarily correspond

00:31:15,170 --> 00:31:20,720
to the physical layout of the tree this

00:31:17,120 --> 00:31:24,410
creates a very traumatic for fine

00:31:20,720 --> 00:31:28,310
operation in lock-free execution so

00:31:24,410 --> 00:31:31,010
consider of where tries to find node 7

00:31:28,310 --> 00:31:33,710
on the BST as shown on the bottom and

00:31:31,010 --> 00:31:36,460
when which node 9 it gets interrupted

00:31:33,710 --> 00:31:39,950
and in the meantime another threat

00:31:36,460 --> 00:31:43,340
deletes the node 3 from the tree and

00:31:39,950 --> 00:31:46,610
change the layout so when threatened

00:31:43,340 --> 00:31:49,070
with zooms it does not find node 7 so

00:31:46,610 --> 00:31:52,010
now it has to decide if node 7 does not

00:31:49,070 --> 00:31:58,490
exist in the tree or it has to start all

00:31:52,010 --> 00:32:00,950
over again this problem to solve this

00:31:58,490 --> 00:32:03,650
problem researchers has been proposing

00:32:00,950 --> 00:32:06,200
embed logic authoring into the denote

00:32:03,650 --> 00:32:08,510
themselves or use external trees to

00:32:06,200 --> 00:32:10,340
store all the values at a very button so

00:32:08,510 --> 00:32:12,620
that they have a fixed position but all

00:32:10,340 --> 00:32:14,660
those introduce additional overheads and

00:32:12,620 --> 00:32:16,340
the more troubling issue is that the

00:32:14,660 --> 00:32:19,610
sequential bottleneck introduced by the

00:32:16,340 --> 00:32:22,400
rebalancing process required by a BST so

00:32:19,610 --> 00:32:25,010
BST needs to be balanced to maintain the

00:32:22,400 --> 00:32:26,960
optimal search time but this process

00:32:25,010 --> 00:32:30,620
involves modification to a large number

00:32:26,960 --> 00:32:32,750
of nodes and this will likely store

00:32:30,620 --> 00:32:34,580
other concurrent operations to the best

00:32:32,750 --> 00:32:37,190
of my knowledge there is no log free

00:32:34,580 --> 00:32:39,530
implementation of rebalancing algorithm

00:32:37,190 --> 00:32:41,750
of a BST but people have been proposing

00:32:39,530 --> 00:32:48,140
relaxed dancing and background passing

00:32:41,750 --> 00:32:50,300
to alleviate this issue so a Paller

00:32:48,140 --> 00:32:52,490
candidate for building concurrent maps

00:32:50,300 --> 00:32:54,800
are using is using the Skip list which

00:32:52,490 --> 00:32:58,550
is a probabilistic balanced alternative

00:32:54,800 --> 00:33:01,730
to BST in a skip list and contains nodes

00:32:58,550 --> 00:33:04,250
with multiple links and at the very

00:33:01,730 --> 00:33:06,920
bottom the nodes form linked lists and

00:33:04,250 --> 00:33:10,070
the upper level links are created with

00:33:06,920 --> 00:33:12,200
exponentially lower probability so that

00:33:10,070 --> 00:33:14,570
the guarantee logarithm search time and

00:33:12,200 --> 00:33:16,220
also the upper level links they are

00:33:14,570 --> 00:33:21,049
are actually redundant shortcut links

00:33:16,220 --> 00:33:24,799
into distant nodes so one problem with

00:33:21,049 --> 00:33:26,870
skip list is that the insert and delete

00:33:24,799 --> 00:33:30,529
operation there were modified multiple

00:33:26,870 --> 00:33:33,110
nodes well I mean how do I mean by this

00:33:30,529 --> 00:33:37,730
is that consider we are inserting node

00:33:33,110 --> 00:33:40,519
four and six in the Skip list insert

00:33:37,730 --> 00:33:43,460
node four requires us to modify links on

00:33:40,519 --> 00:33:46,159
node one and three in certain node six

00:33:43,460 --> 00:33:51,110
well requires to modify links unknown

00:33:46,159 --> 00:33:52,940
one three and five if this is a if this

00:33:51,110 --> 00:33:55,509
is a simple linked list this to

00:33:52,940 --> 00:33:59,600
insertion should have no dependency or

00:33:55,509 --> 00:34:01,940
interference but in the in the Skip list

00:33:59,600 --> 00:34:04,549
they all need to modify node one three

00:34:01,940 --> 00:34:08,139
when on one of them will succeed and the

00:34:04,549 --> 00:34:11,329
other one we try so skip list introduce

00:34:08,139 --> 00:34:16,250
additional access conflicts among

00:34:11,329 --> 00:34:18,560
supposedly commutable operations now

00:34:16,250 --> 00:34:20,929
what makes an efficient lock free map

00:34:18,560 --> 00:34:23,419
based on these observations we want to

00:34:20,929 --> 00:34:26,750
maximize the disjoint access parallelism

00:34:23,419 --> 00:34:29,030
and the data structure preferably should

00:34:26,750 --> 00:34:31,159
have low branching factor so that we can

00:34:29,030 --> 00:34:33,679
avoid hotspot and I should have

00:34:31,159 --> 00:34:36,560
localized updates to avoid access

00:34:33,679 --> 00:34:39,649
conflict and fixed key position to

00:34:36,560 --> 00:34:42,829
simplify the process query now in the

00:34:39,649 --> 00:34:45,200
proposed multi-dimensional list the

00:34:42,829 --> 00:34:47,119
lower level nodes will have smaller

00:34:45,200 --> 00:34:49,010
branching factor which brings down the

00:34:47,119 --> 00:34:51,770
average branching factor of the data

00:34:49,010 --> 00:34:55,030
structure and insert delete function

00:34:51,770 --> 00:34:58,730
only modifies two adjacent nodes at most

00:34:55,030 --> 00:35:01,280
it has a fixed layout for a fixed set of

00:34:58,730 --> 00:35:05,960
keys and does not require balancing or

00:35:01,280 --> 00:35:08,510
randomization now the intuition behind a

00:35:05,960 --> 00:35:11,540
model is this simple here is the link

00:35:08,510 --> 00:35:13,099
list and the insert and deletion

00:35:11,540 --> 00:35:14,780
function on a linked list is very

00:35:13,099 --> 00:35:18,800
efficient requiring only one cast

00:35:14,780 --> 00:35:20,960
updates on the pointer but the search is

00:35:18,800 --> 00:35:25,520
linear and which is not ideal for large

00:35:20,960 --> 00:35:27,470
key space now what we can do is we can

00:35:25,520 --> 00:35:28,170
rearrange the linked list into shorter

00:35:27,470 --> 00:35:30,869
sub lists

00:35:28,170 --> 00:35:33,690
and to search within each sub list in

00:35:30,869 --> 00:35:36,690
this case for this example we have 16

00:35:33,690 --> 00:35:39,299
nodes so we arrange the linked list into

00:35:36,690 --> 00:35:41,520
four columns well if we could first

00:35:39,299 --> 00:35:43,020
determine which key which column a key

00:35:41,520 --> 00:35:46,369
belongs to and do a linear search within

00:35:43,020 --> 00:35:48,930
a column that would be much faster now

00:35:46,369 --> 00:35:53,130
based on this observation we came up

00:35:48,930 --> 00:35:55,260
with the 2d list we replace the links

00:35:53,130 --> 00:35:57,750
from the button nose to top notes with

00:35:55,260 --> 00:36:00,420
links among the top notes and we

00:35:57,750 --> 00:36:02,339
introduce a two-dimensional vector which

00:36:00,420 --> 00:36:03,990
serves as a coordinate for each node the

00:36:02,339 --> 00:36:07,260
nodes are arranged by the lexicographic

00:36:03,990 --> 00:36:09,329
ordering of the coordinate and in this

00:36:07,260 --> 00:36:13,200
case the worst case search time becomes

00:36:09,329 --> 00:36:16,410
the square root of n if we could further

00:36:13,200 --> 00:36:19,859
generalize this into higher dimension we

00:36:16,410 --> 00:36:22,140
could further reduce the search time now

00:36:19,859 --> 00:36:23,700
here's just the definition of the

00:36:22,140 --> 00:36:26,490
multi-dimensional linked list for your

00:36:23,700 --> 00:36:28,980
reference afterwards I'm going to skip

00:36:26,490 --> 00:36:33,299
into the example of a 3d list which will

00:36:28,980 --> 00:36:36,569
make things easier now in this example

00:36:33,299 --> 00:36:39,510
3d list we have three dimension D 0 D 1

00:36:36,569 --> 00:36:43,890
and D 2 along each dimension the nodes

00:36:39,510 --> 00:36:46,710
form a sub list so for dimension 0 we

00:36:43,890 --> 00:36:49,109
have one sub list and for dimension one

00:36:46,710 --> 00:36:51,930
we have four sub lists which are those

00:36:49,109 --> 00:36:53,819
vertical columns and we for dimension D

00:36:51,930 --> 00:36:55,859
- we have several sub lists which are

00:36:53,819 --> 00:37:01,670
those sub lists pointing backwards into

00:36:55,859 --> 00:37:04,049
the slides so in order to support

00:37:01,670 --> 00:37:06,540
lexicographic ordering of the keys we

00:37:04,049 --> 00:37:09,059
need to first map the scalar keys into a

00:37:06,540 --> 00:37:10,470
high dimensional vector this mapping

00:37:09,059 --> 00:37:12,839
function should be injective and

00:37:10,470 --> 00:37:16,200
monotonic so that we can preserve the

00:37:12,839 --> 00:37:18,990
other wing of the keys now the map

00:37:16,200 --> 00:37:20,970
function should also be preferable it

00:37:18,990 --> 00:37:22,559
should be uniform so that we reduce the

00:37:20,970 --> 00:37:25,950
average search time of the whole data

00:37:22,559 --> 00:37:28,380
structure in our case we can simply use

00:37:25,950 --> 00:37:32,190
a numeric based conversion to achieve

00:37:28,380 --> 00:37:34,890
this goal so let's choose a base based

00:37:32,190 --> 00:37:37,619
on the key universe which is the tone

00:37:34,890 --> 00:37:39,599
animal for keys thus who are the teeth

00:37:37,619 --> 00:37:41,920
root of the key in Versailles as the new

00:37:39,599 --> 00:37:44,710
base and then we convert the death

00:37:41,920 --> 00:37:48,640
number two the new base we treat each

00:37:44,710 --> 00:37:51,280
digits as the coordinates for the for

00:37:48,640 --> 00:37:53,410
the M the list so here we have a q

00:37:51,280 --> 00:37:55,780
numbers of 64 keys and the

00:37:53,410 --> 00:37:58,210
dimensionality of data structure is 3 so

00:37:55,780 --> 00:38:05,440
the new base is 4 when we convert the

00:37:58,210 --> 00:38:07,599
key 63 in decimal to 334 in base for we

00:38:05,440 --> 00:38:12,940
obtain the actual coordinates for that

00:38:07,599 --> 00:38:16,480
key the find operation an analyst while

00:38:12,940 --> 00:38:19,119
traverse recursively traverse 1 sub list

00:38:16,480 --> 00:38:22,329
on each dimension until it reached the

00:38:19,119 --> 00:38:25,119
target note so the green arrows they

00:38:22,329 --> 00:38:29,380
highlight a path from the root node to

00:38:25,119 --> 00:38:31,270
the node 3 3 2 when traversing each sub

00:38:29,380 --> 00:38:33,700
list it only compares one coordinate at

00:38:31,270 --> 00:38:36,040
a time so when the search function

00:38:33,700 --> 00:38:37,690
traversing the D 0 knows they only

00:38:36,040 --> 00:38:39,609
compare the first coordinates of the

00:38:37,690 --> 00:38:42,099
nodes with target notes and which

00:38:39,609 --> 00:38:45,040
travels in the Y notes they only compare

00:38:42,099 --> 00:38:46,839
the second coordinate now the worst case

00:38:45,040 --> 00:38:48,400
search time in this case is bounded by

00:38:46,839 --> 00:38:51,369
the total number of dimensions

00:38:48,400 --> 00:38:54,630
multiplied by the maximum number of keys

00:38:51,369 --> 00:38:56,619
on each dimension if we choose the

00:38:54,630 --> 00:38:58,569
dimensionality of this data structure it

00:38:56,619 --> 00:39:01,030
will logarithm of the key space view

00:38:58,569 --> 00:39:05,950
then we can obtain logarithm search time

00:39:01,030 --> 00:39:09,490
for the worst case now the insert

00:39:05,950 --> 00:39:12,339
function consists two steps the first

00:39:09,490 --> 00:39:15,250
step of note splicing actually connects

00:39:12,339 --> 00:39:17,470
the new node to predecessor node this is

00:39:15,250 --> 00:39:19,599
very similar to what you do with a lock

00:39:17,470 --> 00:39:22,119
free link list and this can be done with

00:39:19,599 --> 00:39:24,309
a cast pointer update so the producers

00:39:22,119 --> 00:39:26,049
quarry while we trimmed the produces

00:39:24,309 --> 00:39:29,470
node and the current child of the

00:39:26,049 --> 00:39:31,809
produces node and the dimension of the

00:39:29,470 --> 00:39:35,650
new node and new dimension of the

00:39:31,809 --> 00:39:39,369
current child now in this case we're

00:39:35,650 --> 00:39:43,119
inserting node 2 0 0 after a node 1 0 2

00:39:39,369 --> 00:39:47,020
the original child of 1 0 2 which is 2 0

00:39:43,119 --> 00:39:52,839
1 will be pushed back down to dimension

00:39:47,020 --> 00:39:54,520
2 in this case the child the child knows

00:39:52,839 --> 00:39:56,860
marked by the red arrow

00:39:54,520 --> 00:40:00,550
well no longer be accessible from node 2

00:39:56,860 --> 00:40:03,340
0 1 because this lot has been given to 2

00:40:00,550 --> 00:40:07,090
0 0 so we have to do an additional step

00:40:03,340 --> 00:40:10,360
to transform these to transfer these two

00:40:07,090 --> 00:40:12,400
child nodes back to the new nodes so the

00:40:10,360 --> 00:40:14,860
child adoption of process is only

00:40:12,400 --> 00:40:17,790
necessary if the dimensionality of the

00:40:14,860 --> 00:40:20,500
current child is changed during the

00:40:17,790 --> 00:40:23,860
insertion process which means that if

00:40:20,500 --> 00:40:25,510
the DC does not equal to DP so we we

00:40:23,860 --> 00:40:27,880
should transfer all the child in this

00:40:25,510 --> 00:40:29,650
range to the new node and since this is

00:40:27,880 --> 00:40:33,010
a process we require modification to

00:40:29,650 --> 00:40:35,530
multiple pointers on the current child

00:40:33,010 --> 00:40:38,170
node we use the descriptor to

00:40:35,530 --> 00:40:39,640
encapsulate the contacts and allow

00:40:38,170 --> 00:40:45,210
others rest you have finished this

00:40:39,640 --> 00:40:48,460
process if this process gets delayed now

00:40:45,210 --> 00:40:51,160
normally we could delete node from the

00:40:48,460 --> 00:40:54,010
data structure by doing the inverse work

00:40:51,160 --> 00:40:56,770
of an insert function but this wouldn't

00:40:54,010 --> 00:40:58,869
work for a lock free version because the

00:40:56,770 --> 00:41:01,000
insert function may promote or demote a

00:40:58,869 --> 00:41:02,830
node which is to increase its

00:41:01,000 --> 00:41:05,920
dimensionality and the delete function

00:41:02,830 --> 00:41:09,310
may promote node that is to decrease its

00:41:05,920 --> 00:41:11,560
dimensionality so during the due to the

00:41:09,310 --> 00:41:14,380
helping mechanism several threads may

00:41:11,560 --> 00:41:16,119
access the child adoption on the same

00:41:14,380 --> 00:41:18,850
node with without additional

00:41:16,119 --> 00:41:21,730
synchronization we cannot know if all of

00:41:18,850 --> 00:41:23,650
them have finished so there are races

00:41:21,730 --> 00:41:26,470
may arise among ongoing child adoption

00:41:23,650 --> 00:41:28,440
tasks and the child transfer process the

00:41:26,470 --> 00:41:32,590
solution is to keep dimension change

00:41:28,440 --> 00:41:34,810
unidirectional so we introduce a

00:41:32,590 --> 00:41:36,760
symmetrical delete which is the logical

00:41:34,810 --> 00:41:40,690
deletion process that coupled from the

00:41:36,760 --> 00:41:44,440
physical removal of the node so here we

00:41:40,690 --> 00:41:47,290
want to delete node 1 0 2 we first lot a

00:41:44,440 --> 00:41:49,180
bit mark the node to indicate as being

00:41:47,290 --> 00:41:51,670
logical to be deleted but the node is

00:41:49,180 --> 00:41:55,930
still in the data structure and is valid

00:41:51,670 --> 00:41:58,420
for rolling now we only purge the node

00:41:55,930 --> 00:42:00,970
from the data structure when a new node

00:41:58,420 --> 00:42:03,220
is inserted immediately in front of this

00:42:00,970 --> 00:42:05,380
node this way we can transfer our

00:42:03,220 --> 00:42:06,460
existing child nodes of the deleting

00:42:05,380 --> 00:42:08,560
nodes to the

00:42:06,460 --> 00:42:10,960
in certain notes using the same child

00:42:08,560 --> 00:42:12,790
adoption process this simplifies to help

00:42:10,960 --> 00:42:16,510
protocol and simplifies the

00:42:12,790 --> 00:42:18,910
synchronization so since we are using

00:42:16,510 --> 00:42:22,900
logical deletion the abstract states of

00:42:18,910 --> 00:42:25,180
the map becomes two parts we have a set

00:42:22,900 --> 00:42:27,609
of all nodes and a set of logical EDD

00:42:25,180 --> 00:42:34,000
you note the abstract state of the map

00:42:27,609 --> 00:42:35,950
becomes the and set em - set P so the

00:42:34,000 --> 00:42:37,510
linearization point for an inside

00:42:35,950 --> 00:42:40,570
function is when the cast updates

00:42:37,510 --> 00:42:42,460
predecessors child pointer and the

00:42:40,570 --> 00:42:44,650
linearization point for delete function

00:42:42,460 --> 00:42:46,690
is one cast marks the processor child

00:42:44,650 --> 00:42:48,369
pointer and linearization point for the

00:42:46,690 --> 00:42:53,320
find operation is when the child pointer

00:42:48,369 --> 00:42:56,410
is grid now we did our experiments on a

00:42:53,320 --> 00:42:58,960
sixth for for Numa system with for MMD

00:42:56,410 --> 00:43:02,859
CPUs and we compared our source code and

00:42:58,960 --> 00:43:06,670
and benchmark using GCC 4.7 with level 3

00:43:02,859 --> 00:43:10,869
optimization so for the experiment test

00:43:06,670 --> 00:43:12,849
emilich we randomly Iraq for the Mr lock

00:43:10,869 --> 00:43:18,160
and it's alternatives we randomly

00:43:12,849 --> 00:43:22,480
acquire up to 64 to 1024 resources and

00:43:18,160 --> 00:43:24,880
for the data structure obtained by oft T

00:43:22,480 --> 00:43:27,099
and M the list we test them under write

00:43:24,880 --> 00:43:30,790
dominate read dominate and mixed

00:43:27,099 --> 00:43:34,150
workloads so here are the alternatives

00:43:30,790 --> 00:43:35,890
which shows for the amarlok for two

00:43:34,150 --> 00:43:38,170
phase locking we use STD lock and

00:43:35,890 --> 00:43:40,450
together with this mutex and boost lock

00:43:38,170 --> 00:43:43,990
with boost new tax and for resource

00:43:40,450 --> 00:43:48,670
hierarchy we use it with the STD mutex

00:43:43,990 --> 00:43:50,260
as well as the Intel's TVB Q mutex now

00:43:48,670 --> 00:43:52,780
these two graphs here it shows the

00:43:50,260 --> 00:43:53,170
execution time for acquiring a million

00:43:52,780 --> 00:43:55,359
logs

00:43:53,170 --> 00:43:58,960
Aquarion releasing a million locks on 16

00:43:55,359 --> 00:44:03,010
threats on the Left we have the

00:43:58,960 --> 00:44:05,920
execution time for 64 resources what we

00:44:03,010 --> 00:44:08,940
can say is that the x-axis represents

00:44:05,920 --> 00:44:11,500
the resource contention level which is

00:44:08,940 --> 00:44:13,030
which is the number of requested

00:44:11,500 --> 00:44:16,540
resource divided by the number of total

00:44:13,030 --> 00:44:18,580
resources as we increase the number at

00:44:16,540 --> 00:44:20,240
the resource contention level the

00:44:18,580 --> 00:44:22,280
locking protocols

00:44:20,240 --> 00:44:24,820
including to the smokiest was hierarchy

00:44:22,280 --> 00:44:29,030
the extreme time all increases linearly

00:44:24,820 --> 00:44:31,640
but for analogue and extended TI TS lock

00:44:29,030 --> 00:44:34,130
the execution time keeps almost constant

00:44:31,640 --> 00:44:36,410
this is the major difference between

00:44:34,130 --> 00:44:40,089
batch locking and locking protocols is

00:44:36,410 --> 00:44:45,170
that the batch locking is insensitive to

00:44:40,089 --> 00:44:47,000
resource contention because whatever how

00:44:45,170 --> 00:44:49,900
many resources you are requesting is

00:44:47,000 --> 00:44:54,080
only handled in the same way

00:44:49,900 --> 00:44:56,359
overall the analog outperforms the best

00:44:54,080 --> 00:44:58,130
performing out alternative which is the

00:44:56,359 --> 00:45:01,550
resource hierarchy with a queue lock up

00:44:58,130 --> 00:45:04,430
by up to three times for a 64 resources

00:45:01,550 --> 00:45:09,619
and up to five times for a thousand 24

00:45:04,430 --> 00:45:11,330
resources one thing to note is that when

00:45:09,619 --> 00:45:12,109
the resource level contention level is

00:45:11,330 --> 00:45:14,089
very low

00:45:12,109 --> 00:45:17,030
actually the locking protocols they have

00:45:14,089 --> 00:45:18,920
a huge advantage because the batch

00:45:17,030 --> 00:45:22,730
locking they have a fixed start up

00:45:18,920 --> 00:45:24,530
overhead and here we show how the

00:45:22,730 --> 00:45:29,720
execution time scales with the

00:45:24,530 --> 00:45:32,839
increasing number of threat now the as

00:45:29,720 --> 00:45:36,680
we increase the number of threats the

00:45:32,839 --> 00:45:39,800
first the graph on the Left shows for

00:45:36,680 --> 00:45:42,859
resource contention 50% and 64 resources

00:45:39,800 --> 00:45:45,349
the extended t-80s lock actually

00:45:42,859 --> 00:45:47,540
performs really well with the small name

00:45:45,349 --> 00:45:50,240
of a threat but as the number of quest

00:45:47,540 --> 00:45:53,119
increases the contention level rises at

00:45:50,240 --> 00:45:56,300
a single memory location and eventually

00:45:53,119 --> 00:46:02,020
on 32 threats the amarlok begins to

00:45:56,300 --> 00:46:07,130
outperform the extended t-80s block now

00:46:02,020 --> 00:46:10,970
afford the thousand 24 resources the

00:46:07,130 --> 00:46:15,530
amarlok outperforms the alternatives all

00:46:10,970 --> 00:46:18,760
the time now the next benchmark is to

00:46:15,530 --> 00:46:21,349
test the transactional data structures

00:46:18,760 --> 00:46:23,030
we test two transactional data

00:46:21,349 --> 00:46:25,640
structures one transaction or skip list

00:46:23,030 --> 00:46:28,010
and one transactional link list for skip

00:46:25,640 --> 00:46:29,540
list we compare three versions the log

00:46:28,010 --> 00:46:32,000
for each transaction transformation and

00:46:29,540 --> 00:46:34,100
the transaction boosting as well as the

00:46:32,000 --> 00:46:36,110
object based STM

00:46:34,100 --> 00:46:37,730
and four linked lists we compare with

00:46:36,110 --> 00:46:42,500
the transaction boosting and a word

00:46:37,730 --> 00:46:46,010
based STM now in this graph we show the

00:46:42,500 --> 00:46:49,430
throughput of the of the data structures

00:46:46,010 --> 00:46:52,580
for mixed workload on a Numa system with

00:46:49,430 --> 00:46:55,820
1 million ki now the higher throughput

00:46:52,580 --> 00:46:58,370
the better so and also we append a

00:46:55,820 --> 00:47:00,470
number to the named to the name of the

00:46:58,370 --> 00:47:04,670
data structure to indicate the

00:47:00,470 --> 00:47:07,100
transaction sites so oft 2 means log 3

00:47:04,670 --> 00:47:10,310
transaction transformation with the true

00:47:07,100 --> 00:47:13,190
operation for transaction now overall

00:47:10,310 --> 00:47:15,440
the our FTP approach obtain the average

00:47:13,190 --> 00:47:17,300
of 60% speed-up over the boosting

00:47:15,440 --> 00:47:20,570
approach and 3 times speed up over the

00:47:17,300 --> 00:47:23,900
STM and we can see that as we increase

00:47:20,570 --> 00:47:25,970
the size of the transaction the overall

00:47:23,900 --> 00:47:28,220
throughput of all approaches drops

00:47:25,970 --> 00:47:30,800
slightly and this is because when we

00:47:28,220 --> 00:47:32,330
increase the size of the transaction if

00:47:30,800 --> 00:47:33,980
one operation fail with the larger

00:47:32,330 --> 00:47:39,710
transaction it four-phase actually a

00:47:33,980 --> 00:47:41,930
larger amount of work so this will cause

00:47:39,710 --> 00:47:44,870
the job in throughput when we increase

00:47:41,930 --> 00:47:46,850
the size of transaction and now the

00:47:44,870 --> 00:47:50,060
pollen half the graph actually shows the

00:47:46,850 --> 00:47:52,490
number of spurious reports for each of

00:47:50,060 --> 00:47:56,360
the approach the spurious boards means

00:47:52,490 --> 00:47:58,460
that the number of boards number of

00:47:56,360 --> 00:48:00,860
total ports not count in those caused by

00:47:58,460 --> 00:48:02,750
failed operations this is actually a

00:48:00,860 --> 00:48:06,020
measurement of the efficiency of the

00:48:02,750 --> 00:48:07,880
contention management scheme now since

00:48:06,020 --> 00:48:09,830
we are using helping so for skip list

00:48:07,880 --> 00:48:12,110
most of the time if we encounter a

00:48:09,830 --> 00:48:14,570
conflict that we're all set up to help

00:48:12,110 --> 00:48:19,570
each other hence there will be no abort

00:48:14,570 --> 00:48:23,840
but for STM and the boosting approach

00:48:19,570 --> 00:48:25,880
the number of posts is pretty high so

00:48:23,840 --> 00:48:28,970
here we show the throughput for a linked

00:48:25,880 --> 00:48:32,240
list with mixed workload with 10,000

00:48:28,970 --> 00:48:34,430
keys so for start-up with the small

00:48:32,240 --> 00:48:38,030
transactions we only obtain a very small

00:48:34,430 --> 00:48:39,710
advantage over the boosting approach but

00:48:38,030 --> 00:48:42,070
as we increase the size of the

00:48:39,710 --> 00:48:44,530
transaction we are getting more

00:48:42,070 --> 00:48:47,840
performance benefits over the boosting

00:48:44,530 --> 00:48:51,500
because in the case of larger

00:48:47,840 --> 00:48:53,840
for for the linked list it is actually

00:48:51,500 --> 00:48:56,090
very expensive to execute a Weaver's

00:48:53,840 --> 00:48:58,130
operation and a linked list then in a

00:48:56,090 --> 00:48:59,570
skip list so the performance of the

00:48:58,130 --> 00:49:02,300
boosting approach will chop

00:48:59,570 --> 00:49:03,770
significantly after we increase the site

00:49:02,300 --> 00:49:07,580
of the transaction where we encounter

00:49:03,770 --> 00:49:09,530
more upwards and retries now for the

00:49:07,580 --> 00:49:11,600
number of spurious ports we do encounter

00:49:09,530 --> 00:49:14,300
it's known a small number of spurious

00:49:11,600 --> 00:49:16,190
ports in this case for larger name of

00:49:14,300 --> 00:49:18,110
threat but it's still two to three

00:49:16,190 --> 00:49:22,490
orders of magnitude last and the

00:49:18,110 --> 00:49:24,680
alternative approaches so for the M the

00:49:22,490 --> 00:49:27,980
list based dictionary we compared

00:49:24,680 --> 00:49:31,100
against the log based and log 3 and our

00:49:27,980 --> 00:49:35,320
Cu based PST we also compare it with the

00:49:31,100 --> 00:49:37,610
three log free skip list implementations

00:49:35,320 --> 00:49:43,040
now this graph here shows the throughput

00:49:37,610 --> 00:49:46,790
of the Amulet of the all the maps with 1

00:49:43,040 --> 00:49:50,360
billion keys and 50% insert 50% delete

00:49:46,790 --> 00:49:53,690
operations so this is pretty white heavy

00:49:50,360 --> 00:49:55,700
workload and we can see that we obtain

00:49:53,690 --> 00:50:00,560
as much as a hundred percent speed-up

00:49:55,700 --> 00:50:02,630
for sixty resources because the MD list

00:50:00,560 --> 00:50:05,870
there has really optimized for right

00:50:02,630 --> 00:50:08,240
operation with only it only modifies on

00:50:05,870 --> 00:50:10,310
most two nodes so for this kind of

00:50:08,240 --> 00:50:13,580
workload is just performance the best

00:50:10,310 --> 00:50:15,860
and if we decrease the key size to a

00:50:13,580 --> 00:50:18,740
million and increase the read operation

00:50:15,860 --> 00:50:21,410
with our mixed workload we can see the

00:50:18,740 --> 00:50:24,050
MD lists can still obtain up to 50%

00:50:21,410 --> 00:50:28,100
speed up over the best-performing skip

00:50:24,050 --> 00:50:30,350
list now when we come to your case where

00:50:28,100 --> 00:50:32,810
we have a really small key wrench let's

00:50:30,350 --> 00:50:34,910
say a thousand key and the really read

00:50:32,810 --> 00:50:37,730
dominate workload with 90% fine

00:50:34,910 --> 00:50:41,150
operation this is when the BST really

00:50:37,730 --> 00:50:44,030
begins to shine because BST have really

00:50:41,150 --> 00:50:46,370
shallow depth in this scenario and for a

00:50:44,030 --> 00:50:48,440
thousand key the dimension which use for

00:50:46,370 --> 00:50:53,510
the MD list which is 16 is too large

00:50:48,440 --> 00:50:56,900
this has too many overhead so here this

00:50:53,510 --> 00:50:58,580
graph shows actually perimeter sweep on

00:50:56,900 --> 00:51:01,460
the dimensionality of the data structure

00:50:58,580 --> 00:51:04,310
as well as the number of threat

00:51:01,460 --> 00:51:07,040
the thing to know is that this is for a

00:51:04,310 --> 00:51:10,460
right to dominate workloads with 50%

00:51:07,040 --> 00:51:12,470
insert and 50% delete we can see that

00:51:10,460 --> 00:51:15,710
the maximum throughput of the end list

00:51:12,470 --> 00:51:17,930
always converges through the 20

00:51:15,710 --> 00:51:19,910
dimensions this means that the

00:51:17,930 --> 00:51:22,430
throughput of this data structure is

00:51:19,910 --> 00:51:24,500
independent from the concurrency level

00:51:22,430 --> 00:51:27,140
but dependent on actual work level so

00:51:24,500 --> 00:51:28,369
what we can do is that we can tween this

00:51:27,140 --> 00:51:30,920
data structure according to your

00:51:28,369 --> 00:51:32,329
specific workload and then when you add

00:51:30,920 --> 00:51:36,290
in more threads it will scale

00:51:32,329 --> 00:51:39,740
automatically so to summarize the

00:51:36,290 --> 00:51:41,450
performance characteristics the AMA log

00:51:39,740 --> 00:51:43,640
excels at medium to high levels of

00:51:41,450 --> 00:51:47,089
resource contention and is suitable for

00:51:43,640 --> 00:51:48,890
large pool of resources and the L of T T

00:51:47,089 --> 00:51:51,109
based transactional data structure

00:51:48,890 --> 00:51:52,700
excels at the larger transactions and it

00:51:51,109 --> 00:51:55,250
has improved the success rate with

00:51:52,700 --> 00:51:57,410
minimal sprigle support and M the list

00:51:55,250 --> 00:51:59,359
excels at high level of concurrency and

00:51:57,410 --> 00:52:03,079
a large key space it is optimized for

00:51:59,359 --> 00:52:07,130
write operations now here is no view of

00:52:03,079 --> 00:52:10,910
the Liberty Act II at the very bottom

00:52:07,130 --> 00:52:12,290
level we have three unmanaged containers

00:52:10,910 --> 00:52:14,480
which are log three data structures

00:52:12,290 --> 00:52:16,280
without memory management scheme these

00:52:14,480 --> 00:52:19,579
data structures will be used internally

00:52:16,280 --> 00:52:22,430
or externally by the memory management

00:52:19,579 --> 00:52:23,319
module and the log three profile or

00:52:22,430 --> 00:52:25,849
logger

00:52:23,319 --> 00:52:27,710
our management means that their memory

00:52:25,849 --> 00:52:29,869
will not be reclaimed until the data

00:52:27,710 --> 00:52:33,859
structure itself is being it's destroyed

00:52:29,869 --> 00:52:35,690
so on the upper level we have two

00:52:33,859 --> 00:52:37,520
strategies the analog we provide a

00:52:35,690 --> 00:52:39,980
resource mapper that allows you to map

00:52:37,520 --> 00:52:41,900
abstract logs into the pit set in

00:52:39,980 --> 00:52:43,670
yemalog to be used with the conventional

00:52:41,900 --> 00:52:46,250
log free our lock based data structures

00:52:43,670 --> 00:52:48,530
and the RTG we provide for data

00:52:46,250 --> 00:52:50,960
structure building lists and lists skip

00:52:48,530 --> 00:52:56,480
lists in the hash table with the two

00:52:50,960 --> 00:52:59,119
type of interfaces set in a map now in

00:52:56,480 --> 00:53:01,490
the future we plan to support wait free

00:52:59,119 --> 00:53:04,490
data structures the work to be done is

00:53:01,490 --> 00:53:06,500
to add width we add a procedure to

00:53:04,490 --> 00:53:08,900
support width we update on the note

00:53:06,500 --> 00:53:11,810
infrastructure and to have weight free

00:53:08,900 --> 00:53:13,280
memory management system we plan on to

00:53:11,810 --> 00:53:14,440
also support a non linked data

00:53:13,280 --> 00:53:16,300
structures

00:53:14,440 --> 00:53:18,640
but this way we have to associate the

00:53:16,300 --> 00:53:23,320
know the infrastructure actually with

00:53:18,640 --> 00:53:25,210
the HDL item and we may be also we can

00:53:23,320 --> 00:53:29,200
do automatic code transformation to

00:53:25,210 --> 00:53:31,270
actually obtain from a existing

00:53:29,200 --> 00:53:33,790
lock-free data structure just to apply a

00:53:31,270 --> 00:53:36,160
transformation allow you sir to identify

00:53:33,790 --> 00:53:37,930
the code blocks and we will keep the

00:53:36,160 --> 00:53:41,670
transactional data structure as an end

00:53:37,930 --> 00:53:44,590
result now here's some references to the

00:53:41,670 --> 00:53:47,500
contents and the link to the source code

00:53:44,590 --> 00:53:49,480
as well as our contact emails that

00:53:47,500 --> 00:53:52,050
concludes my talk and I'll take any

00:53:49,480 --> 00:53:52,050
questions you have

00:54:03,030 --> 00:54:07,020
all right questions

00:54:11,900 --> 00:54:14,900
yeah

00:54:25,910 --> 00:54:31,170
so the question was are we proposing

00:54:28,380 --> 00:54:34,140
anything for the C++ standard is that

00:54:31,170 --> 00:54:35,670
right and not right now no we are not

00:54:34,140 --> 00:54:38,130
proposing anything we're just building

00:54:35,670 --> 00:54:41,069
this as a experimental external library

00:54:38,130 --> 00:54:43,489
for user to use for experiments with

00:54:41,069 --> 00:54:43,489
concurrency

00:54:54,260 --> 00:54:57,820

YouTube URL: https://www.youtube.com/watch?v=uDNb8JL0vv8


