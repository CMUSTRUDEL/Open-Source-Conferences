Title: CppCon 2017: Carl Cook “When a Microsecond Is an Eternity: High Performance Trading Systems in C++”
Publication date: 2017-10-08
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
Automated trading involves submitting electronic orders rapidly when opportunities arise. But it’s harder than it seems: either your system is the fastest and you make the trade, or you get nothing. 

This is a considerable challenge for any C++ developer - the critical path is only a fraction of the total codebase, it is invoked infrequently and unpredictably, yet must execute quickly and without delay. Unfortunately we can’t rely on the help of compilers, operating systems and standard hardware, as they typically aim for maximum throughput and fairness across all processes. 

This talk describes how successful low latency trading systems can be developed in C++, demonstrating common coding techniques used to reduce execution times. While automated trading is used as the motivation for this talk, the topics discussed are equally valid to other domains such as game development and soft real-time processing.
— 
Carl Cook: Optiver, Software Engineer

Carl has a Ph.D. from the University of Canterbury, New Zealand, graduating in 2006. He currently works for Optiver, a global electronic market maker, where he is tasked with adding new trading features into the execution stack while continually reducing latencies. Carl is also an active member of SG14, making sure that requirements from the automated trading industry are represented. He is currently assisting with several proposals, including non-allocating standard functions, fast containers, and CPU affinity/cache control.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,357 --> 00:00:02,708
- Cool so yeah, good afternoon everyone.

00:00:02,708 --> 00:00:06,429
My name is Carl, I work for a company called Optiver.

00:00:06,429 --> 00:00:08,891
I work writing automated trading systems

00:00:08,891 --> 00:00:13,092
and I've done that for about 10 years in total.

00:00:13,092 --> 00:00:14,990
So today's talk is just going to be

00:00:14,990 --> 00:00:17,516
basically a day in the life of a developer

00:00:17,516 --> 00:00:20,349
of high frequency trading systems.

00:00:23,786 --> 00:00:25,953
I'm also a member of SG14,

00:00:25,953 --> 00:00:28,061
so I try to give a little bit of feedback

00:00:28,061 --> 00:00:31,897
from the trading community into their standards.

00:00:31,897 --> 00:00:35,167
What's important from the low latency perspective

00:00:35,167 --> 00:00:36,250
from trading.

00:00:37,128 --> 00:00:39,746
But I'm definitely not a C++ expert.

00:00:39,746 --> 00:00:42,032
There's people in this room who will know 10 times more

00:00:42,032 --> 00:00:44,443
about C++ than me.

00:00:44,443 --> 00:00:47,228
It's not necessarily a problem though.

00:00:47,228 --> 00:00:49,012
You just need to be really good at measurement.

00:00:49,012 --> 00:00:51,328
You need to be really good at applying scientific method.

00:00:51,328 --> 00:00:53,161
You need to understand the tools.

00:00:53,161 --> 00:00:54,712
If you can do that and you can measure

00:00:54,712 --> 00:00:56,300
and you can measure well, you can write

00:00:56,300 --> 00:00:58,620
very very fast software without knowing the standard

00:00:58,620 --> 00:00:59,537
inside out.

00:01:01,443 --> 00:01:05,610
So I'm gonna give a very quick overview of what we do.

00:01:08,076 --> 00:01:10,028
Won't go into that in much detail,

00:01:10,028 --> 00:01:12,256
'cuz that can take an hour or two just by itself.

00:01:12,256 --> 00:01:13,718
But I will talk about the technical challenges

00:01:13,718 --> 00:01:14,742
that we face.

00:01:14,742 --> 00:01:18,655
Those two topics are just an introductory,

00:01:18,655 --> 00:01:21,021
then we get into the real stuff,

00:01:21,021 --> 00:01:23,689
which is the actual techniques that I use on a daily basis

00:01:23,689 --> 00:01:26,272
for writing fast C++.

00:01:28,039 --> 00:01:30,082
Mention some surprises, some war stories,

00:01:30,082 --> 00:01:34,152
and then finally I'm gonna discuss how to measure C++

00:01:34,152 --> 00:01:35,945
and that's probably the most important few slides

00:01:35,945 --> 00:01:36,778
of the talk.

00:01:36,778 --> 00:01:40,945
It's really all about measurement if you want fast code.

00:01:48,693 --> 00:01:49,526
Cool.

00:01:50,911 --> 00:01:54,487
Another thing is that I would absolutely love to be able to

00:01:54,487 --> 00:01:57,760
discuss absolutely every optimization trick out there,

00:01:57,760 --> 00:01:58,699
but I can't.

00:01:58,699 --> 00:01:59,617
Just not enough time.

00:01:59,617 --> 00:02:01,487
I just did a run through of my slides this morning,

00:02:01,487 --> 00:02:03,773
threw away about half of them.

00:02:03,773 --> 00:02:05,276
So yeah, this is only a very very brief

00:02:05,276 --> 00:02:09,443
sample into optimization techniques for low latency.

00:02:10,392 --> 00:02:13,718
So low latency, not high frequent,

00:02:13,718 --> 00:02:17,143
not trying to process 60, 100 frames a second,

00:02:17,143 --> 00:02:18,994
but just trying to be very very fast

00:02:18,994 --> 00:02:21,077
when you need to be fast.

00:02:23,145 --> 00:02:24,562
So what do we do?

00:02:25,921 --> 00:02:29,473
Well most market makers are providing liquidity

00:02:29,473 --> 00:02:30,306
to the market.

00:02:30,306 --> 00:02:34,342
That basically means we're providing prices to the market

00:02:34,342 --> 00:02:37,647
to say we're willing to buy this instrument for

00:02:37,647 --> 00:02:39,543
a certain amount, we're willing to sell it

00:02:39,543 --> 00:02:41,041
for a certain amount more than that.

00:02:41,041 --> 00:02:43,695
By instruments I mean a tradable product,

00:02:43,695 --> 00:02:45,780
a stock, a future option.

00:02:45,780 --> 00:02:48,671
It's just pure coincidence that there's instruments

00:02:48,671 --> 00:02:49,782
on the table over there.

00:02:49,782 --> 00:02:52,365
Not those sorts of instruments.

00:02:53,484 --> 00:02:54,408
This has some challenges.

00:02:54,408 --> 00:02:57,137
You need to be moving your prices quite regularly.

00:02:57,137 --> 00:03:01,635
If you don't move your prices you'll miss out on the trade

00:03:01,635 --> 00:03:04,532
or even worse you'll get the trade but it wasn't

00:03:04,532 --> 00:03:06,755
what you're looking for 'cuz you didn't move your price

00:03:06,755 --> 00:03:10,922
fast enough and someone else has lifted you on that.

00:03:12,698 --> 00:03:15,306
So what market makers do is they try to just

00:03:15,306 --> 00:03:17,215
make the difference between buying and selling.

00:03:17,215 --> 00:03:21,423
So they want to buy a stock and sell it again.

00:03:21,423 --> 00:03:22,841
Sell it, buy it.

00:03:22,841 --> 00:03:24,942
That will hopefully balance itself out.

00:03:24,942 --> 00:03:27,973
So market makers aren't quantitative funds

00:03:27,973 --> 00:03:30,915
or hedge funds or anything like that.

00:03:30,915 --> 00:03:33,035
They're not looking to build a position

00:03:33,035 --> 00:03:35,993
with the intention that the market's going to go up or down,

00:03:35,993 --> 00:03:39,031
they're really just trying to make a small amount of profit

00:03:39,031 --> 00:03:42,364
by buying and selling fairly frequently.

00:03:43,727 --> 00:03:46,601
Ultimately this always comes down to two things:

00:03:46,601 --> 00:03:49,080
buying low, selling high.

00:03:49,080 --> 00:03:50,949
Yeah there's some complex maths in there.

00:03:50,949 --> 00:03:54,391
Sometimes it gets a little bit complex,

00:03:54,391 --> 00:03:57,473
but ultimately that's what all trading algorithms

00:03:57,473 --> 00:03:58,556
come down to.

00:03:59,987 --> 00:04:03,715
And success means being just slightly faster

00:04:03,715 --> 00:04:05,792
than your competitors.

00:04:05,792 --> 00:04:08,271
It doesn't need to be by a second,

00:04:08,271 --> 00:04:12,032
it could be a picosecond, just as long as you're faster.

00:04:12,032 --> 00:04:13,931
I think this is quite a nice picture actually

00:04:13,931 --> 00:04:16,408
because it gives you an idea.

00:04:16,408 --> 00:04:19,079
No one really remembers who comes second or third or fourth

00:04:19,079 --> 00:04:20,009
in a race right?

00:04:20,009 --> 00:04:23,538
And that's kind of the same thing with electronic

00:04:23,538 --> 00:04:24,914
market making.

00:04:24,914 --> 00:04:28,497
You really need to be the guy out in front.

00:04:29,354 --> 00:04:31,233
But, safety first.

00:04:31,233 --> 00:04:34,602
A lot can go wrong in a very very short period of time.

00:04:34,602 --> 00:04:38,554
So you need to be able to detect errors and get out,

00:04:38,554 --> 00:04:40,388
not the other way around.

00:04:40,388 --> 00:04:41,696
You don't want to think about things

00:04:41,696 --> 00:04:42,555
then get out of the market,

00:04:42,555 --> 00:04:44,832
you want to get out then figure out what's gone wrong,

00:04:44,832 --> 00:04:49,324
because a lot can go wrong in a few seconds.

00:04:49,324 --> 00:04:51,991
That's best automated obviously.

00:04:57,970 --> 00:04:59,637
That's a good quote.

00:05:04,810 --> 00:05:07,027
Okay, so I'm gonna talk about the hotpath a lot,

00:05:07,027 --> 00:05:11,194
sometimes I'll say fastpath, but it's the same thing.

00:05:12,166 --> 00:05:15,760
This is the code which sends a message

00:05:15,760 --> 00:05:18,488
from the exchange, decodes it, figures out that

00:05:18,488 --> 00:05:20,503
this is an interesting event, this is a price change,

00:05:20,503 --> 00:05:25,134
this is a trade, this is something, a market open event.

00:05:25,134 --> 00:05:27,805
Executes an autotrading strategy,

00:05:27,805 --> 00:05:29,816
decides it wants to trade, runs the risk checks,

00:05:29,816 --> 00:05:32,767
makes sure that that's okay,

00:05:32,767 --> 00:05:35,489
and then finally sends an order to the exchange

00:05:35,489 --> 00:05:37,433
to say I'd like to trade or I'd like to change my price

00:05:37,433 --> 00:05:38,530
or something like that.

00:05:38,530 --> 00:05:40,162
That's the hotpath.

00:05:40,162 --> 00:05:42,978
Characteristics of the hotpath are quite interesting.

00:05:42,978 --> 00:05:47,058
Probably one to five percent of the total code base

00:05:47,058 --> 00:05:50,008
and it gets executed very very infrequently.

00:05:50,008 --> 00:05:54,347
Maybe .01% of the time you'll actually have that hotpath

00:05:54,347 --> 00:05:55,514
fully execute.

00:05:57,133 --> 00:05:59,641
Now that's at odds with the operating system,

00:05:59,641 --> 00:06:02,570
the hardware, the network, basically everything in the room

00:06:02,570 --> 00:06:06,749
is at odds with trying to execute a hotpath.

00:06:06,749 --> 00:06:11,036
Not very frequently, but when it executes very very fast

00:06:11,036 --> 00:06:13,155
very little latency.

00:06:13,155 --> 00:06:16,075
So even networks for example are based on fairness,

00:06:16,075 --> 00:06:16,908
which makes sense.

00:06:16,908 --> 00:06:19,802
They want to cache up bytes until there's enough

00:06:19,802 --> 00:06:21,333
for the packet to be fully sent,

00:06:21,333 --> 00:06:23,900
then the packets sent and make sure that no one sender

00:06:23,900 --> 00:06:25,364
is saturating the network.

00:06:25,364 --> 00:06:28,684
The same with operating systems, they're all about

00:06:28,684 --> 00:06:31,729
multitasking and cooperative multitasking.

00:06:31,729 --> 00:06:33,051
These are things that you don't actually want

00:06:33,051 --> 00:06:35,798
from a low latency perspective.

00:06:35,798 --> 00:06:38,491
One final point: jitter.

00:06:38,491 --> 00:06:40,451
You do want to be fast most of the time,

00:06:40,451 --> 00:06:42,318
but sometimes you're slow.

00:06:42,318 --> 00:06:44,222
So if you're fast four times out of five, that's great,

00:06:44,222 --> 00:06:45,713
but if that fifth time you're so slow

00:06:45,713 --> 00:06:47,989
that you end up with a bad trade because you didn't

00:06:47,989 --> 00:06:50,056
move your price or you missed out,

00:06:50,056 --> 00:06:52,318
well if you miss out you miss an opportunity,

00:06:52,318 --> 00:06:55,318
but if you get hit on a bad order or bad trade,

00:06:55,318 --> 00:06:56,969
that can be quite expensive.

00:06:56,969 --> 00:06:59,052
So it's very frustrating.

00:07:00,011 --> 00:07:03,744
You can have code which runs faster on average

00:07:03,744 --> 00:07:07,957
a lower median, but the standard deviation is too wide

00:07:07,957 --> 00:07:09,554
and that's actually not a good thing.

00:07:09,554 --> 00:07:13,721
You prefer a tighter deviation than an actual faster median.

00:07:17,117 --> 00:07:20,147
So where does C++ come into this?

00:07:20,147 --> 00:07:23,115
Well it's the language of choice within trading companies

00:07:23,115 --> 00:07:25,282
and that's no coincidence.

00:07:29,773 --> 00:07:30,940
It's low cost.

00:07:32,066 --> 00:07:35,531
You get basically zero cost abstractions.

00:07:35,531 --> 00:07:37,353
You can get your hands on the hardware reasonably,

00:07:37,353 --> 00:07:39,275
but it's still a relatively abstract language.

00:07:39,275 --> 00:07:42,055
That's great, that's what we want.

00:07:42,055 --> 00:07:44,396
But there's some catches.

00:07:44,396 --> 00:07:47,384
Your compiler, and compiler version, build link flags,

00:07:47,384 --> 00:07:48,773
different libraries that you're using,

00:07:48,773 --> 00:07:50,451
the machine architecture, these are all going

00:07:50,451 --> 00:07:52,005
to impact your code.

00:07:52,005 --> 00:07:53,974
And they will unfortunately have interactions

00:07:53,974 --> 00:07:55,940
with each other as well.

00:07:55,940 --> 00:07:58,302
So you really need to not just know C++ well,

00:07:58,302 --> 00:08:02,142
but figure out what the compiler is actually generating.

00:08:02,142 --> 00:08:04,475
Anyone know an app for that?

00:08:07,517 --> 00:08:08,464
Very very useful.

00:08:08,464 --> 00:08:11,073
No coincidence that this comes from a trading company.

00:08:11,073 --> 00:08:15,150
You'll learn more about that on Friday I believe.

00:08:15,150 --> 00:08:16,643
Tools like this are great.

00:08:16,643 --> 00:08:19,457
You can change the compiler flags, see what happens,

00:08:19,457 --> 00:08:22,335
have a look at what the actual resulting assembly is,

00:08:22,335 --> 00:08:25,757
change your versions, compare client to GCC,

00:08:25,757 --> 00:08:26,590
it's great.

00:08:26,590 --> 00:08:31,050
See which version of the compiler actually made a change

00:08:31,050 --> 00:08:34,383
to your code if you notice a regression.

00:08:35,875 --> 00:08:37,674
Now I should point out as well,

00:08:37,674 --> 00:08:40,651
I don't know how to write assembly code.

00:08:40,651 --> 00:08:43,053
But I do know how to read a little bit of assembly code

00:08:43,053 --> 00:08:44,285
and that's great.

00:08:44,285 --> 00:08:46,016
So from a performance point of view,

00:08:46,016 --> 00:08:49,161
I know that a call or a jump is going to be expensive.

00:08:49,161 --> 00:08:52,738
I know that more machine code is probably more expensive

00:08:52,738 --> 00:08:54,979
than less machine code.

00:08:54,979 --> 00:08:58,450
I know that if I have fewer jumps, more conditional moves,

00:08:58,450 --> 00:09:02,617
and things like that, we're onto a good thing here.

00:09:05,457 --> 00:09:09,503
Now just to talk a tiny bit about system tuning.

00:09:09,503 --> 00:09:11,632
This is a C++ conference, so I can't go into this

00:09:11,632 --> 00:09:14,569
in too much detail, but it's worth just pointing this out.

00:09:14,569 --> 00:09:16,397
So you can have exactly the same hardware,

00:09:16,397 --> 00:09:17,803
exactly the same operating system,

00:09:17,803 --> 00:09:20,019
the same binary, the same everything,

00:09:20,019 --> 00:09:23,821
but put them on a slightly different configuration

00:09:23,821 --> 00:09:28,143
from a BIOS point of view or a Linux kernel tuning

00:09:28,143 --> 00:09:31,783
point of view, you get different results.

00:09:31,783 --> 00:09:33,283
So the blue server

00:09:36,494 --> 00:09:39,372
is a machine that is not running hyper threading.

00:09:39,372 --> 00:09:40,383
The orange server is a machine

00:09:40,383 --> 00:09:42,628
that is running hyper threading.

00:09:42,628 --> 00:09:46,795
This is sorting different size vectors of random integers.

00:09:50,570 --> 00:09:51,947
Why is the hyper threading ...

00:09:51,947 --> 00:09:53,186
Oh this is a single threaded (mumbles)

00:09:53,186 --> 00:09:54,764
running on one CPU.

00:09:54,764 --> 00:09:58,764
Why would the hyper threading slow us down here?

00:10:04,320 --> 00:10:08,787
What's common between the two threads on a core?

00:10:08,787 --> 00:10:11,194
If you're hyper threading.

00:10:11,194 --> 00:10:12,868
Cache. Exactly.

00:10:12,868 --> 00:10:14,377
So you're sharing the same cache.

00:10:14,377 --> 00:10:17,915
So your cache either goes down to 50% or more likely

00:10:17,915 --> 00:10:20,575
it goes down to zero because there's something else running

00:10:20,575 --> 00:10:22,325
on the second thread.

00:10:23,872 --> 00:10:24,993
Hyper threading is great,

00:10:24,993 --> 00:10:26,903
I'm not here to bag on hyper threading,

00:10:26,903 --> 00:10:29,493
I use it at home on my desktop, why not?

00:10:29,493 --> 00:10:31,519
It turns four cores into eight.

00:10:31,519 --> 00:10:33,406
But for a production server running low latency

00:10:33,406 --> 00:10:38,122
trading software, things like that will catch you up.

00:10:38,122 --> 00:10:40,190
You don't need to know about this from a C++

00:10:40,190 --> 00:10:42,012
development point of view, but you better hope

00:10:42,012 --> 00:10:43,326
that there's someone in your company

00:10:43,326 --> 00:10:45,317
who can take care of that for you,

00:10:45,317 --> 00:10:49,360
otherwise you've already lost unfortunately.

00:10:49,360 --> 00:10:51,027
So how fast is fast?

00:10:52,487 --> 00:10:55,987
The Burj Khalifa in Dubai 2,700 feet tall.

00:10:58,472 --> 00:11:01,951
Very tall building, tallest in the world at the moment.

00:11:01,951 --> 00:11:04,544
Speed of light coincidentally, is around about one foot

00:11:04,544 --> 00:11:05,794
per nanosecond.

00:11:06,913 --> 00:11:10,111
So if we developed a autotrading system

00:11:10,111 --> 00:11:13,623
that ran end to end, wire to wire, from seeing the packet in

00:11:13,623 --> 00:11:16,196
to performing of the logic and sending a packet back out

00:11:16,196 --> 00:11:20,363
to the exchange in say two and a half microseconds,

00:11:21,623 --> 00:11:23,676
well that's actually less time than it takes for light

00:11:23,676 --> 00:11:26,262
to travel from the top of the spire of this building

00:11:26,262 --> 00:11:27,429
to the ground.

00:11:29,686 --> 00:11:31,769
That's not a lot of time.

00:11:33,627 --> 00:11:35,892
So when you go oh well it's alright,

00:11:35,892 --> 00:11:37,756
the machine might be swapping a little bit at the moment,

00:11:37,756 --> 00:11:39,337
or we'll go out to main memory or it doesn't matter

00:11:39,337 --> 00:11:41,446
that someone else is using our cache at the moment,

00:11:41,446 --> 00:11:42,279
well it does.

00:11:42,279 --> 00:11:44,048
All bets are off at this point.

00:11:44,048 --> 00:11:45,376
You have lost the game at this point.

00:11:45,376 --> 00:11:49,543
There's such little time to actually execute your code.

00:11:55,746 --> 00:11:56,579
Okay so,

00:12:00,022 --> 00:12:01,887
that's the introductions done.

00:12:01,887 --> 00:12:05,614
I've got about 10 to 15 coding examples

00:12:05,614 --> 00:12:07,838
of just low latency techniques that have actually helped me

00:12:07,838 --> 00:12:10,142
in production make the code faster.

00:12:10,142 --> 00:12:12,671
This isn't just micro benchmarking,

00:12:12,671 --> 00:12:14,287
it's and yeah this appears faster,

00:12:14,287 --> 00:12:17,935
this is actual code that's been tested in production

00:12:17,935 --> 00:12:20,919
and it's made a serious difference.

00:12:20,919 --> 00:12:22,669
Most of these will be

00:12:25,324 --> 00:12:27,431
somewhat obvious to some people in the room,

00:12:27,431 --> 00:12:29,069
other people maybe not.

00:12:29,069 --> 00:12:31,010
So don't be too worried if some things you already know.

00:12:31,010 --> 00:12:35,281
Hopefully there's a few surprises in there for you still.

00:12:35,281 --> 00:12:39,355
So this gave me about a 100, 200 nanosecond speed up

00:12:39,355 --> 00:12:41,255
a couple months ago when I did it,

00:12:41,255 --> 00:12:43,976
which was removing the code on the left

00:12:43,976 --> 00:12:46,474
and replacing it with the code on the right.

00:12:46,474 --> 00:12:48,557
Why would this be faster?

00:12:55,495 --> 00:12:56,951
Mhm.

00:12:56,951 --> 00:12:59,847
So indeed, the compiler can optimize the hotpath better.

00:12:59,847 --> 00:13:00,680
What else?

00:13:02,764 --> 00:13:05,906
Yes, two answers at once and they were both super

00:13:05,906 --> 00:13:07,605
and they were both correct.

00:13:07,605 --> 00:13:09,842
One said cache production, one said branch production

00:13:09,842 --> 00:13:10,675
I think.

00:13:10,675 --> 00:13:11,992
Well cache and branching, yeah so,

00:13:11,992 --> 00:13:13,827
branch production, there's fewer branches

00:13:13,827 --> 00:13:16,468
on the right hand side for the hardware branch predictor

00:13:16,468 --> 00:13:18,091
to deal with.

00:13:18,091 --> 00:13:20,091
And cache, this is good,

00:13:21,492 --> 00:13:23,064
on the right is it better for your instruction cache

00:13:23,064 --> 00:13:25,466
or your data cache?

00:13:25,466 --> 00:13:26,299
Both?

00:13:28,601 --> 00:13:31,717
I don't think he said both but (laughs)

00:13:31,717 --> 00:13:33,501
I claim it's both.

00:13:33,501 --> 00:13:35,179
Yeah, you've got fewer instructions,

00:13:35,179 --> 00:13:36,813
less pressure on the instruction cache.

00:13:36,813 --> 00:13:39,009
And also who knows what these error handling functions

00:13:39,009 --> 00:13:40,779
are actually doing?

00:13:40,779 --> 00:13:42,609
Maybe they're trampling your data cache as well.

00:13:42,609 --> 00:13:45,776
Whereas now we've only got one integer

00:13:46,678 --> 00:13:49,145
that we're actually reading and writing to

00:13:49,145 --> 00:13:51,540
and machine architectures are incredibly good

00:13:51,540 --> 00:13:55,207
at testing if an integer is zero or nonzero.

00:13:57,522 --> 00:14:00,441
Does this seem obvious or ... ?

00:14:00,441 --> 00:14:02,432
After I did it, it was like oh why am I not doing this

00:14:02,432 --> 00:14:03,513
all the time?

00:14:03,513 --> 00:14:04,676
But yeah, just things like this can really

00:14:04,676 --> 00:14:06,176
make a difference.

00:14:07,391 --> 00:14:08,224
Hi.

00:14:13,411 --> 00:14:14,273
Yeah yeah yeah.

00:14:14,273 --> 00:14:16,180
So where is the error checking code?

00:14:16,180 --> 00:14:18,856
So the error checking code is behind the scenes.

00:14:18,856 --> 00:14:20,189
It's miles away.

00:14:21,306 --> 00:14:24,201
But if there's any flag that has been set by anything

00:14:24,201 --> 00:14:28,406
before heading this hotpath, we'll set a flag

00:14:28,406 --> 00:14:30,807
to say do not go any further.

00:14:30,807 --> 00:14:31,701
Yeah.

00:14:31,701 --> 00:14:36,189
Then in this handle error, that can figure out

00:14:36,189 --> 00:14:38,628
what the actual error is, deal with it.

00:14:38,628 --> 00:14:40,555
Does that kind of answer your question?

00:14:40,555 --> 00:14:41,388
Yeah.

00:14:42,947 --> 00:14:45,263
Okay another quite simple example.

00:14:45,263 --> 00:14:48,559
I see this quite a bit, which is template-based

00:14:48,559 --> 00:14:50,174
configuration.

00:14:50,174 --> 00:14:53,533
Well, the lack of template-based configuration.

00:14:53,533 --> 00:14:56,180
Sometimes you don't know everything at compile time,

00:14:56,180 --> 00:14:58,386
so maybe you can read from configuration

00:14:58,386 --> 00:15:00,897
and based on that you're going to instantiate

00:15:00,897 --> 00:15:02,513
one type of class for another type of class,

00:15:02,513 --> 00:15:05,584
but you don't know what class you're actually going

00:15:05,584 --> 00:15:06,834
to instantiate.

00:15:08,723 --> 00:15:12,839
Most people will use virtual functions for this.

00:15:12,839 --> 00:15:14,841
Virtual functions are absolutely fine,

00:15:14,841 --> 00:15:19,008
if you don't know what class you're going to instantiate.

00:15:20,770 --> 00:15:24,656
If you don't know all of the possible classes.

00:15:24,656 --> 00:15:27,267
If you know the complete set of things

00:15:27,267 --> 00:15:29,596
that may be instantiated,

00:15:29,596 --> 00:15:31,162
then you don't really need virtual functions.

00:15:31,162 --> 00:15:33,675
You can just use templates for this.

00:15:33,675 --> 00:15:35,343
And again this is a very simple example,

00:15:35,343 --> 00:15:38,505
but I think it's often overlooked and I see virtual

00:15:38,505 --> 00:15:41,152
calls going all the way through code and into the hotpath

00:15:41,152 --> 00:15:44,874
and it is slow and it is not required.

00:15:44,874 --> 00:15:46,326
You see this all the time in the ASDL

00:15:46,326 --> 00:15:47,860
and it works really really well.

00:15:47,860 --> 00:15:50,360
So here we've got two classes,

00:15:51,350 --> 00:15:54,008
and both send orders; order sender one, order sender two,

00:15:54,008 --> 00:15:54,841
A and B.

00:15:56,922 --> 00:15:58,546
And we've got an order manager which uses

00:15:58,546 --> 00:15:59,830
one of those two types.

00:15:59,830 --> 00:16:02,379
We don't know which one and it doesn't know which one,

00:16:02,379 --> 00:16:05,462
it's templated at this point in time.

00:16:06,464 --> 00:16:07,786
Now there is a virtual function core in there,

00:16:07,786 --> 00:16:09,836
the mail loop, but that's just to start the program

00:16:09,836 --> 00:16:11,282
so that virtual call is gonna disappear

00:16:11,282 --> 00:16:12,861
before you know about it.

00:16:12,861 --> 00:16:16,278
But the actual calls to this order sender

00:16:17,668 --> 00:16:20,085
and nonvirtual concrete time.

00:16:21,243 --> 00:16:22,422
How do you hook this up?

00:16:22,422 --> 00:16:24,341
Just use the factory function, yeah?

00:16:24,341 --> 00:16:26,774
Look at your configuration and then instantiate

00:16:26,774 --> 00:16:29,862
the correct type, pass it in, and you're done.

00:16:29,862 --> 00:16:32,389
No need for virtuals.

00:16:32,389 --> 00:16:34,708
Again, this is not a ...

00:16:34,708 --> 00:16:36,794
I won't win any awards with this slide,

00:16:36,794 --> 00:16:39,400
but so many times I've noticed that people miss these

00:16:39,400 --> 00:16:40,677
sorts of things.

00:16:40,677 --> 00:16:41,903
You know everything at compile time,

00:16:41,903 --> 00:16:44,020
you know the complete set of things that can happen,

00:16:44,020 --> 00:16:46,611
use that information to make your code fast.

00:16:46,611 --> 00:16:49,432
No virtuals, faster code, more compact code,

00:16:49,432 --> 00:16:51,748
more chance for the optimizer to optimize,

00:16:51,748 --> 00:16:53,061
less pressure on the instruction cache,

00:16:53,061 --> 00:16:54,842
less pressure on the data cache.

00:16:54,842 --> 00:16:56,342
It's a good thing.

00:16:58,609 --> 00:17:00,897
And the next one, Lambda functions,

00:17:00,897 --> 00:17:02,685
this will not give you a speed up.

00:17:02,685 --> 00:17:04,147
This slide will not give you a speed up,

00:17:04,147 --> 00:17:07,677
but it's a nice example of how C++ can be fast and powerful,

00:17:07,677 --> 00:17:11,010
but still relatively expressive as well.

00:17:11,886 --> 00:17:15,429
So on the left we've got a function called send message,

00:17:15,429 --> 00:17:18,070
takes in a lambda, prepares a message,

00:17:18,070 --> 00:17:21,767
invokes the lambda, sends the message.

00:17:21,767 --> 00:17:23,556
On the right is the actual usage.

00:17:23,556 --> 00:17:25,168
So here we're calling it send message,

00:17:25,168 --> 00:17:26,987
we need to pass it a lambda, so here's the lambda,

00:17:26,987 --> 00:17:29,154
the lambda takes a message

00:17:30,864 --> 00:17:33,637
and then it just populates this.

00:17:33,637 --> 00:17:37,509
Now, this could very well be as follows:

00:17:37,509 --> 00:17:39,693
send message, on the left, the prepare message

00:17:39,693 --> 00:17:41,857
isn't allocating memory or anything like that,

00:17:41,857 --> 00:17:45,068
it's just getting the network cards buffer,

00:17:45,068 --> 00:17:47,829
via DMA, and returning that.

00:17:47,829 --> 00:17:52,355
The lambda will definitely get inlined at two moves,

00:17:52,355 --> 00:17:54,706
which is the slow level as you can get.

00:17:54,706 --> 00:17:58,741
Send could be as simple as flipping one single bit

00:17:58,741 --> 00:18:02,063
on the network card to tell the network card to send.

00:18:02,063 --> 00:18:05,018
So that's very very low level, very powerful, very fast,

00:18:05,018 --> 00:18:07,137
and in fact, the entire function there on the left,

00:18:07,137 --> 00:18:09,353
send message, will probably get inlined as well,

00:18:09,353 --> 00:18:11,447
depending on what you're doing with it.

00:18:11,447 --> 00:18:13,492
So it's very very nice, very very fast,

00:18:13,492 --> 00:18:16,097
but still relatively high level language.

00:18:16,097 --> 00:18:18,050
So you can see why C++ is somewhat

00:18:18,050 --> 00:18:20,383
the language of choice here.

00:18:22,911 --> 00:18:24,903
Quick show of hands, who knows that memory allocation

00:18:24,903 --> 00:18:25,736
is slow?

00:18:26,575 --> 00:18:27,408
Good.

00:18:30,114 --> 00:18:33,686
So that should be no surprise to you.

00:18:33,686 --> 00:18:35,519
Use a pool of objects.

00:18:37,117 --> 00:18:39,274
I don't care how it's done, you can use an allocator,

00:18:39,274 --> 00:18:41,231
you can do it by hand, I don't mind.

00:18:41,231 --> 00:18:45,833
But definitely try to reuse and recycle objects.

00:18:45,833 --> 00:18:48,365
Be careful with distracting objects as well,

00:18:48,365 --> 00:18:50,265
this can get expensive.

00:18:50,265 --> 00:18:52,285
Underneath all of this will be a call to free.

00:18:52,285 --> 00:18:54,924
Free's actually relatively expensive.

00:18:54,924 --> 00:18:56,805
If you go and have a look through glibc,

00:18:56,805 --> 00:18:58,084
it's about 400 lines of code.

00:18:58,084 --> 00:19:00,214
Yeah you're not going to hit every single line of code

00:19:00,214 --> 00:19:03,277
each time, but even the act of deleting objects

00:19:03,277 --> 00:19:07,110
can be relatively expensive, so do watch that.

00:19:10,731 --> 00:19:12,308
Okay, exceptions.

00:19:12,308 --> 00:19:13,975
Are exceptions slow?

00:19:18,449 --> 00:19:20,782
If you don't drop into them.

00:19:25,677 --> 00:19:26,908
And throw, yeah.

00:19:26,908 --> 00:19:31,807
So absolutely if you have an exception that's thrown,

00:19:31,807 --> 00:19:34,478
from my measurement it's one or two micros,

00:19:34,478 --> 00:19:36,421
but as far as I can tell, and I've measured this a lot,

00:19:36,421 --> 00:19:37,584
I've really tried to measure this

00:19:37,584 --> 00:19:39,500
in many many different ways.

00:19:39,500 --> 00:19:42,582
Exceptions claims to be a zero cost thing as long as

00:19:42,582 --> 00:19:43,415
they don't throw.

00:19:43,415 --> 00:19:46,070
As far as I can tell, that is absolutely true.

00:19:46,070 --> 00:19:48,025
So don't be afraid of exceptions.

00:19:48,025 --> 00:19:49,731
Of course, if they throw, it's gonna be expensive,

00:19:49,731 --> 00:19:52,166
but if an exception is thrown you've got a problem anyway.

00:19:52,166 --> 00:19:56,166
You're not going to be sending an order anymore.

00:19:57,004 --> 00:20:00,098
So don't use exceptions for control flow, yeah?

00:20:00,098 --> 00:20:03,131
One, it's gonna be slow, and two, your code's gonna look

00:20:03,131 --> 00:20:04,631
really really bad.

00:20:08,454 --> 00:20:12,826
This is a little bit similar to a previous slide.

00:20:12,826 --> 00:20:15,826
You don't really need if statements.

00:20:17,172 --> 00:20:18,236
Particularly in your hotpath.

00:20:18,236 --> 00:20:20,726
Branching is bad, branching is expensive,

00:20:20,726 --> 00:20:23,571
and you can remove quite a bit of it.

00:20:23,571 --> 00:20:26,555
So here we've got a branching approach of running a strategy

00:20:26,555 --> 00:20:29,498
where we calculate a price, check some risk limits,

00:20:29,498 --> 00:20:31,393
and then send the order.

00:20:31,393 --> 00:20:32,583
And if you look at the implementation

00:20:32,583 --> 00:20:35,512
of one of these methods, here's our branching at the side,

00:20:35,512 --> 00:20:38,263
and so by then we want to ask for a bit of a discount,

00:20:38,263 --> 00:20:40,777
otherwise it must be a sale, so we're asking

00:20:40,777 --> 00:20:42,452
for a little bit of a premium.

00:20:42,452 --> 00:20:44,137
That's pretty simple.

00:20:44,137 --> 00:20:46,804
This should run relatively fast.

00:20:48,069 --> 00:20:50,405
But we know at compile time,

00:20:50,405 --> 00:20:52,723
it can either only be a buy or a sell,

00:20:52,723 --> 00:20:53,762
there's no third value.

00:20:53,762 --> 00:20:56,788
And we know what we want to do in both cases.

00:20:56,788 --> 00:20:58,837
We know that we want to either ask for a premium

00:20:58,837 --> 00:21:00,798
or ask for a discount.

00:21:00,798 --> 00:21:04,306
So you can just template specialize this completely away

00:21:04,306 --> 00:21:06,738
to the point where you have just completely deterministic,

00:21:06,738 --> 00:21:08,238
nonbranching code.

00:21:09,946 --> 00:21:11,392
Don't go crazy with this.

00:21:11,392 --> 00:21:13,449
Don't remove every single if from your code,

00:21:13,449 --> 00:21:14,858
it will look really bad, and it'll probably end up

00:21:14,858 --> 00:21:16,590
slowing your code down.

00:21:16,590 --> 00:21:18,622
But in the hotpath you can go a long way

00:21:18,622 --> 00:21:20,677
with absolutely removing branches completely

00:21:20,677 --> 00:21:22,236
from your code.

00:21:22,236 --> 00:21:23,069
Hi.

00:21:24,546 --> 00:21:28,713
- [Audience Member] (distant voice)

00:21:31,387 --> 00:21:35,528
- The question was, can't the optimizer optimize this out?

00:21:35,528 --> 00:21:38,107
In theory, I guess it probably can.

00:21:38,107 --> 00:21:40,857
In practice, it normally doesn't.

00:21:41,850 --> 00:21:44,363
And actually there is a very very good point.

00:21:44,363 --> 00:21:48,835
So much of this is just guaranteeing that the compiler

00:21:48,835 --> 00:21:51,412
is going to do what you expect it to do.

00:21:51,412 --> 00:21:53,065
You're kind of locking the compiler in to say

00:21:53,065 --> 00:21:55,882
hey I really want you to know that there's only

00:21:55,882 --> 00:21:59,458
two particular branches here or two particular conditions.

00:21:59,458 --> 00:22:02,945
So a lot of it is just basically taking the risk out of it

00:22:02,945 --> 00:22:06,021
and being very very concrete with telling the compiler

00:22:06,021 --> 00:22:07,688
what you want to do.

00:22:10,841 --> 00:22:12,174
Multi-threading.

00:22:19,543 --> 00:22:22,253
I don't like multi-threading.

00:22:22,253 --> 00:22:24,666
I know that you need it sometimes.

00:22:24,666 --> 00:22:26,190
I know that you don't want to be doing everything

00:22:26,190 --> 00:22:27,023
in the hotpath.

00:22:27,023 --> 00:22:28,500
I know that you need to sometimes

00:22:28,500 --> 00:22:30,480
maybe have a second thread on a different CPU

00:22:30,480 --> 00:22:34,556
or a different process on a second CPU, second core,

00:22:34,556 --> 00:22:36,896
doing some heavy lifting, doing some calculations,

00:22:36,896 --> 00:22:39,183
responding to messages and things like that.

00:22:39,183 --> 00:22:40,655
Okay I get that, you need multi-threading,

00:22:40,655 --> 00:22:44,582
but keep the data that you share between the hotpath

00:22:44,582 --> 00:22:47,239
and everything else to the absolute minimum.

00:22:47,239 --> 00:22:50,857
If you can do that, you're gonna get away with faster code

00:22:50,857 --> 00:22:52,107
than otherwise.

00:22:55,512 --> 00:22:57,498
So consider not even sharing data.

00:22:57,498 --> 00:22:59,506
Consider just throwing copies of data

00:22:59,506 --> 00:23:03,720
across from the producer to the consumer.

00:23:03,720 --> 00:23:08,189
May be a lot for a single writer single reader queue.

00:23:08,189 --> 00:23:09,677
Yeah you're still sharing a little bit there,

00:23:09,677 --> 00:23:12,501
you're sharing the atomic read and write pointers

00:23:12,501 --> 00:23:14,371
for the queue, but that's actually

00:23:14,371 --> 00:23:15,789
not much data to share.

00:23:15,789 --> 00:23:18,872
I think that's a reasonable approach.

00:23:19,795 --> 00:23:21,153
Of course sometimes you don't even need to worry

00:23:21,153 --> 00:23:22,440
about multithreading, sometimes ...

00:23:22,440 --> 00:23:25,731
Sorry, protection over multithreading.

00:23:25,731 --> 00:23:28,084
Sometimes receiving out of order is actually okay,

00:23:28,084 --> 00:23:29,843
it's not the end of the world.

00:23:29,843 --> 00:23:32,162
Sometimes the machine architecture is so strong

00:23:32,162 --> 00:23:35,462
that it'll actually avoid the risk conditions

00:23:35,462 --> 00:23:37,287
that you've read about anyway.

00:23:37,287 --> 00:23:40,454
So it's worth experimenting with that.

00:23:42,953 --> 00:23:44,281
So we're about halfway through now I think

00:23:44,281 --> 00:23:46,864
of the tips and tricks section.

00:23:48,712 --> 00:23:50,394
The next one is data lookups.

00:23:50,394 --> 00:23:53,402
So if you read a software engineering textbook,

00:23:53,402 --> 00:23:56,374
it'll probably suggest that if you have an instrument

00:23:56,374 --> 00:23:59,904
that you want to trade on, and a market,

00:23:59,904 --> 00:24:01,911
and each instrument has one market,

00:24:01,911 --> 00:24:03,867
and a market can have many instruments.

00:24:03,867 --> 00:24:07,228
So you can see the ER diagram forming.

00:24:07,228 --> 00:24:09,731
You'll do a lookup, so you go okay,

00:24:09,731 --> 00:24:11,095
this code here would make sense,

00:24:11,095 --> 00:24:13,203
so you create your message that you want to send

00:24:13,203 --> 00:24:15,363
to the exchange and you'll go look up the market

00:24:15,363 --> 00:24:17,476
and you'll get the information out of the market

00:24:17,476 --> 00:24:20,712
and that will be part of your computation.

00:24:20,712 --> 00:24:21,769
So that's fine.

00:24:21,769 --> 00:24:23,519
That's not a problem.

00:24:25,347 --> 00:24:29,474
But you can do things a lot faster than that.

00:24:29,474 --> 00:24:31,929
If you're going to read the instrument,

00:24:31,929 --> 00:24:35,353
if you're going to read the current price of the instrument,

00:24:35,353 --> 00:24:38,270
how many bytes have you read there?

00:24:40,135 --> 00:24:41,802
How big's the float?

00:24:43,041 --> 00:24:44,155
Four bytes?

00:24:44,155 --> 00:24:47,238
Okay, so how many bytes have we read?

00:24:49,066 --> 00:24:51,066
64. Why have we read 64?

00:24:53,657 --> 00:24:55,323
Because we've read a cacheline.

00:24:55,323 --> 00:24:56,659
We can't do anything about that,

00:24:56,659 --> 00:24:59,076
that's how the machine works.

00:25:01,743 --> 00:25:04,680
So, why not put the information that you need

00:25:04,680 --> 00:25:06,798
into that cacheline?

00:25:06,798 --> 00:25:08,982
So if you know that you always need to read

00:25:08,982 --> 00:25:11,566
say for example the quantity multiplier from the market

00:25:11,566 --> 00:25:13,788
and that hardly ever changes, well may not change at all

00:25:13,788 --> 00:25:16,077
that day, put it into the instrument.

00:25:16,077 --> 00:25:18,565
You're going to be reading that cacheline anyway,

00:25:18,565 --> 00:25:20,022
and then you get that for free.

00:25:20,022 --> 00:25:22,984
No further lookups, you're done.

00:25:22,984 --> 00:25:25,728
It's denormalized, might violate a few

00:25:25,728 --> 00:25:28,451
software engineering principles, but that's okay.

00:25:28,451 --> 00:25:30,868
You're gonna get faster code.

00:25:35,129 --> 00:25:36,046
Containers.

00:25:37,399 --> 00:25:40,330
Containers are worth a handful of slides,

00:25:40,330 --> 00:25:44,247
so I've got about four slides on unordered map.

00:25:46,370 --> 00:25:49,806
So you need something to do lookups on.

00:25:49,806 --> 00:25:51,883
Sometimes you are gonna have to look up data.

00:25:51,883 --> 00:25:53,005
So that's fine.

00:25:53,005 --> 00:25:57,150
So this is your likely implementation of an unordered map.

00:25:57,150 --> 00:25:58,985
You have multiple buckets, you have a key,

00:25:58,985 --> 00:26:02,235
the key gets hashed, the hash wall corresponds to one

00:26:02,235 --> 00:26:05,820
and only one bucket, and your key and value pair

00:26:05,820 --> 00:26:07,820
will go into the bucket.

00:26:08,888 --> 00:26:12,173
And if you get multiple pairs of keys and values

00:26:12,173 --> 00:26:16,289
that map to the same bucket, then you'll get a chain effect.

00:26:16,289 --> 00:26:20,456
So this will be one link list underneath the hood.

00:26:21,535 --> 00:26:23,189
Default max load factor of one.

00:26:23,189 --> 00:26:27,139
So on average, there will be one item per bucket.

00:26:27,139 --> 00:26:29,889
Order one insert, order one find.

00:26:31,910 --> 00:26:33,774
Unordered map is kind of your go to map

00:26:33,774 --> 00:26:36,119
if you want fast containers within C++

00:26:36,119 --> 00:26:39,382
and you want to just use CSDL.

00:26:39,382 --> 00:26:41,474
And by the way, the original proposal

00:26:41,474 --> 00:26:43,737
was in the bottom right, really worth the read.

00:26:43,737 --> 00:26:46,430
It's a really good description of how unordered maps

00:26:46,430 --> 00:26:49,347
and unordered sets are implemented.

00:26:51,365 --> 00:26:54,947
Link lists are not guaranteed to be in contiguous memory.

00:26:54,947 --> 00:26:56,664
So you really don't want to be jumping around

00:26:56,664 --> 00:26:58,198
to this link list looking for your key.

00:26:58,198 --> 00:27:01,115
You want to find it the first time.

00:27:02,900 --> 00:27:06,267
So, I did an experiment recently

00:27:06,267 --> 00:27:08,184
where I took 10,000 ...

00:27:10,389 --> 00:27:13,819
Well I generated 10,000 keys and I generated them

00:27:13,819 --> 00:27:18,088
from the range of zero through to one trillion.

00:27:18,088 --> 00:27:22,171
Then I inserted all of them into a unordered map.

00:27:23,161 --> 00:27:27,328
Then I asked H-node how many other nodes are in your bucket?

00:27:29,930 --> 00:27:32,960
And this is the distribution that I got back.

00:27:32,960 --> 00:27:35,962
Most nodes are in a bucket all by themselves,

00:27:35,962 --> 00:27:37,102
but only just.

00:27:37,102 --> 00:27:38,436
You will get collisions.

00:27:38,436 --> 00:27:41,103
This was a uniform distribution,

00:27:43,401 --> 00:27:45,150
you are going to get collisions.

00:27:45,150 --> 00:27:47,471
Unless you have a perfect hashing function.

00:27:47,471 --> 00:27:48,986
Which in this case I deliberately didn't have,

00:27:48,986 --> 00:27:50,791
but I had a pretty good hashing function.

00:27:50,791 --> 00:27:54,362
Used standard uniform int distribution.

00:27:54,362 --> 00:27:56,965
So if we go back, you will from time to time

00:27:56,965 --> 00:28:00,780
have to run through your nodes to find the key.

00:28:00,780 --> 00:28:04,280
It's not always going to be the first one.

00:28:06,722 --> 00:28:08,939
So let's time this, this is Google Benchmark

00:28:08,939 --> 00:28:11,569
for the top half of the slide.

00:28:11,569 --> 00:28:13,635
Anyone use Google Benchmark?

00:28:13,635 --> 00:28:15,331
Yeah it's good. Good to see.

00:28:15,331 --> 00:28:16,444
If you haven't already, check it out

00:28:16,444 --> 00:28:20,926
it's fantastic for paramaterizing micro benchmarks.

00:28:20,926 --> 00:28:23,191
14 nanoseconds through to 24 nanoseconds,

00:28:23,191 --> 00:28:24,839
so you can see that's reasonable.

00:28:24,839 --> 00:28:28,756
And you can see that speed of a find is broadly

00:28:29,744 --> 00:28:34,127
related to the number of elements in the collection.

00:28:34,127 --> 00:28:36,426
There's the Linux Perf output in the second half

00:28:36,426 --> 00:28:38,231
of the slide there.

00:28:38,231 --> 00:28:39,439
Nothing too surprising in there.

00:28:39,439 --> 00:28:41,562
Instructions per cycle was a little bit low,

00:28:41,562 --> 00:28:43,954
but there's also quite a few bit of stalling

00:28:43,954 --> 00:28:45,871
going on there as well.

00:28:47,175 --> 00:28:49,847
And if you do the math, that means that broadly

00:28:49,847 --> 00:28:52,190
one out of 500 cache references

00:28:52,190 --> 00:28:54,094
is actually a cache miss.

00:28:54,094 --> 00:28:54,927
So so.

00:28:57,643 --> 00:29:01,045
Or you could use something like Google's dense hash map.

00:29:01,045 --> 00:29:01,878
Fast.

00:29:02,931 --> 00:29:05,390
A little bit of complexity around management of collisions,

00:29:05,390 --> 00:29:06,723
but that's okay.

00:29:08,503 --> 00:29:11,586
So, given the choice I'd go for this.

00:29:13,137 --> 00:29:17,159
Or you can ignore Bjarne's suggestion this morning

00:29:17,159 --> 00:29:20,240
if not reimplementing anything in ECR relation

00:29:20,240 --> 00:29:23,278
to hash tables and take a hybrid,

00:29:23,278 --> 00:29:24,838
take the best of both worlds.

00:29:24,838 --> 00:29:26,151
That's what we do at Optiver,

00:29:26,151 --> 00:29:28,006
for a lot of our (mumbles) instead of code,

00:29:28,006 --> 00:29:29,939
is we have a slightly different hash table.

00:29:29,939 --> 00:29:31,997
Lots of different implementations of hash tables,

00:29:31,997 --> 00:29:34,535
plenty on the internet, pick your best one.

00:29:34,535 --> 00:29:35,947
What I'm going to do is just quickly describe

00:29:35,947 --> 00:29:39,236
what we use at Optiver, 'cuz it's quite a neat approach.

00:29:39,236 --> 00:29:41,432
Maybe you do something similar at your company,

00:29:41,432 --> 00:29:42,515
I don't know.

00:29:44,879 --> 00:29:47,212
But we have something like this.

00:29:47,212 --> 00:29:50,428
So the keys and values are just anywhere,

00:29:50,428 --> 00:29:52,236
they could be in a separate collection,

00:29:52,236 --> 00:29:54,335
or it could just be heap allocated.

00:29:54,335 --> 00:29:56,351
It really doesn't matter too much.

00:29:56,351 --> 00:29:58,470
The table itself is actually a precomputed hash

00:29:58,470 --> 00:30:00,803
and a pointer to the object.

00:30:02,120 --> 00:30:03,142
That's your hash table.

00:30:03,142 --> 00:30:05,870
So it's like a hash table of metadata.

00:30:05,870 --> 00:30:09,121
Importantly, the hash, eight bytes.

00:30:09,121 --> 00:30:10,429
The pointer, eight bytes.

00:30:10,429 --> 00:30:12,226
So each pair is 16 bytes.

00:30:12,226 --> 00:30:16,225
You can fit four of those in a cacheline.

00:30:16,225 --> 00:30:17,932
Which means that when you read one,

00:30:17,932 --> 00:30:21,465
you've read another three more than likely as well.

00:30:21,465 --> 00:30:24,095
Which means that you've already pre fetched in

00:30:24,095 --> 00:30:26,126
any subsequent data that you might need

00:30:26,126 --> 00:30:28,181
to resolve the conflict.

00:30:28,181 --> 00:30:31,318
So we take an example, if we look for key 73,

00:30:31,318 --> 00:30:34,859
if this is an integer key, then well the hash is just 73

00:30:34,859 --> 00:30:38,949
as well, maybe this index is to index one.

00:30:38,949 --> 00:30:41,134
Go to index one, the hash is 73.

00:30:41,134 --> 00:30:42,991
Okay great, we probably found what we're looking for,

00:30:42,991 --> 00:30:45,522
follow the pointer, yeah the key is 73 as well.

00:30:45,522 --> 00:30:47,522
Okay, great, we're done.

00:30:48,934 --> 00:30:51,726
If we take another example, key 12, maps to hash 12,

00:30:51,726 --> 00:30:55,893
maybe this thing happens to index slot number three,

00:30:57,708 --> 00:31:00,065
based on the implementation.

00:31:00,065 --> 00:31:02,009
Lets check the hash, the has is 98,

00:31:02,009 --> 00:31:04,337
ah okay, so there's a collision.

00:31:04,337 --> 00:31:07,292
That's alright, we'll just follow it to the right,

00:31:07,292 --> 00:31:08,488
what's the next hash value?

00:31:08,488 --> 00:31:10,178
Okay, that's hash value 12, great that's what we're

00:31:10,178 --> 00:31:13,414
looking for, follow that pointer, yeah, there's our key,

00:31:13,414 --> 00:31:14,914
there's our value.

00:31:16,307 --> 00:31:18,384
Very cache efficient.

00:31:18,384 --> 00:31:19,813
Don't take my word for it.

00:31:19,813 --> 00:31:23,145
So if we micro benchmark this, twice as fast,

00:31:23,145 --> 00:31:27,484
seems to be less susceptible to the size of the collection.

00:31:27,484 --> 00:31:29,681
Instructions per cycle are looking a lot healthier now,

00:31:29,681 --> 00:31:31,502
double if not triple.

00:31:31,502 --> 00:31:33,864
And only that one out of 1,500 cache references

00:31:33,864 --> 00:31:35,371
is actually a cache miss.

00:31:35,371 --> 00:31:37,921
So just a more cache friendly implementation

00:31:37,921 --> 00:31:39,254
of a hash table.

00:31:41,247 --> 00:31:45,124
There may be a talk later today about hash tables like this

00:31:45,124 --> 00:31:46,049
possibly by ...

00:31:46,049 --> 00:31:47,051
- [Audience Member] (indistinct)

00:31:47,051 --> 00:31:47,884
- Sorry?

00:31:47,884 --> 00:31:48,873
- [Audience Member] Wednesday.

00:31:48,873 --> 00:31:49,714
- It's Wednesday.

00:31:49,714 --> 00:31:51,435
- [Audience Member] Yeah.

00:31:51,435 --> 00:31:52,536
- Cheers.

00:31:52,536 --> 00:31:56,543
There may be a talk on Wednesday about this.

00:31:56,543 --> 00:31:59,419
Okay a couple more to go.

00:31:59,419 --> 00:32:01,165
Always inline and no inline.

00:32:01,165 --> 00:32:03,213
The inline keyword is incredibly confusing.

00:32:03,213 --> 00:32:06,522
Inline does not mean please inline, inline means

00:32:06,522 --> 00:32:08,659
there may be multiple definitions, that's okay,

00:32:08,659 --> 00:32:10,616
link it please, overlook it.

00:32:10,616 --> 00:32:11,911
If you want something to be inlined,

00:32:11,911 --> 00:32:15,174
you're far better to use GCC and Clang's attributes

00:32:15,174 --> 00:32:17,920
to say force this to be inlined please.

00:32:17,920 --> 00:32:20,524
Or force this to not be inlined.

00:32:20,524 --> 00:32:22,360
Now, dragons lie here.

00:32:22,360 --> 00:32:24,067
You need to be careful about this.

00:32:24,067 --> 00:32:25,599
Inlining can make your code faster,

00:32:25,599 --> 00:32:27,889
inlining can make your code slower.

00:32:27,889 --> 00:32:30,078
Not inlining can also give you

00:32:30,078 --> 00:32:31,761
either of those two alternatives.

00:32:31,761 --> 00:32:33,856
Again, you need to measure.

00:32:33,856 --> 00:32:37,358
Look at the disassembly, but even better, actually measure

00:32:37,358 --> 00:32:38,608
end production.

00:32:40,135 --> 00:32:42,597
So just a trivial example here,

00:32:42,597 --> 00:32:45,230
but we go to check the market

00:32:45,230 --> 00:32:46,971
and if we decide that we're not going to send the order

00:32:46,971 --> 00:32:49,662
'cuz something's gone wrong, do some logging,

00:32:49,662 --> 00:32:52,013
otherwise send the order.

00:32:52,013 --> 00:32:55,694
Now if this is inlined, if the compiler goes hey that's

00:32:55,694 --> 00:32:56,839
not a bad function to inline,

00:32:56,839 --> 00:32:59,650
it doesn't take too many parameters,

00:32:59,650 --> 00:33:02,413
yeah sure lets inline it, why not?

00:33:02,413 --> 00:33:04,372
You're gonna pollute your instruction cache.

00:33:04,372 --> 00:33:06,938
You've got all of this logging code,

00:33:06,938 --> 00:33:09,638
which you don't really want in the hotpath,

00:33:09,638 --> 00:33:12,020
right there in the middle of the hotpath.

00:33:12,020 --> 00:33:14,423
So this is an easy one, just no inline this,

00:33:14,423 --> 00:33:16,716
that'll drop out, that'll turn to a call,

00:33:16,716 --> 00:33:19,479
chances are the branch predictor will ...

00:33:19,479 --> 00:33:23,344
Well two things, chances are the compiler will

00:33:23,344 --> 00:33:25,674
pick the right branch, and even better

00:33:25,674 --> 00:33:27,619
at runtime the branch predictor will probably

00:33:27,619 --> 00:33:30,225
be picking the right branch all the time for you.

00:33:30,225 --> 00:33:31,868
Gonna talk about the branch predictor more

00:33:31,868 --> 00:33:33,573
very soon by the way.

00:33:33,573 --> 00:33:36,073
The hardware branch predictor.

00:33:37,368 --> 00:33:40,178
I'm gonna talk about the hardware branch predictor right now

00:33:40,178 --> 00:33:41,511
as it turns out.

00:33:43,142 --> 00:33:45,813
Okay, this is the most important slide

00:33:45,813 --> 00:33:49,174
out of the 15 or so tips and tricks.

00:33:49,174 --> 00:33:53,341
This gives me a five microsecond speed up on my code.

00:33:55,544 --> 00:33:58,179
Or in other words, whenever I screw this up,

00:33:58,179 --> 00:34:00,919
the systems run five microseconds slower.

00:34:00,919 --> 00:34:04,145
So much in fact, I have a flow chart on my desk

00:34:04,145 --> 00:34:05,691
which basically goes oh you have five markers

00:34:05,691 --> 00:34:06,750
you're going slower.

00:34:06,750 --> 00:34:08,182
Yes. Did you screw up?

00:34:08,182 --> 00:34:09,601
Dump your orders. Yes.

00:34:09,601 --> 00:34:12,597
Fix it, and go back to the start.

00:34:12,597 --> 00:34:15,731
So this is what happens in practice.

00:34:15,731 --> 00:34:19,387
You hardly ever execute the fastpath, the hotpath,

00:34:19,387 --> 00:34:21,351
and I said this right at the start.

00:34:21,351 --> 00:34:25,518
Gonna say it again, the fastpath is very seldomly executed.

00:34:28,289 --> 00:34:30,217
Maybe you get close and then you decide actually I can't

00:34:30,217 --> 00:34:32,249
because of risk limits, the next market data

00:34:32,249 --> 00:34:34,559
that comes in isn't interesting,

00:34:34,559 --> 00:34:37,230
the next is, but again the strategy decides

00:34:37,230 --> 00:34:39,265
I've already got enough of that instrument,

00:34:39,265 --> 00:34:40,318
I don't want to trade it.

00:34:40,318 --> 00:34:42,330
Then eventually, at some point ah great there's an order

00:34:42,330 --> 00:34:45,496
we want to go for, let's shoot for it.

00:34:45,496 --> 00:34:46,996
Now on top of this there's other things going on

00:34:46,996 --> 00:34:49,970
which I haven't put on the board, on the graph,

00:34:49,970 --> 00:34:54,109
things like the unordered map that's trampling

00:34:54,109 --> 00:34:56,124
all over cache and other things that are trampling

00:34:56,124 --> 00:34:57,884
all over our cache, handling other messages

00:34:57,884 --> 00:35:01,884
that are coming in, doing background processing.

00:35:03,643 --> 00:35:04,979
So how do we fix this?

00:35:04,979 --> 00:35:07,396
How do we keep the cache hot?

00:35:08,679 --> 00:35:11,754
Well, we pretend we live in a different universe

00:35:11,754 --> 00:35:14,046
where everything that we do results in an order

00:35:14,046 --> 00:35:16,815
being sent to the exchange.

00:35:16,815 --> 00:35:20,030
Here's a tip, you really don't want to send

00:35:20,030 --> 00:35:21,853
everything to the exchange.

00:35:21,853 --> 00:35:25,133
They'd get very annoyed with you very quickly.

00:35:25,133 --> 00:35:26,102
But you can pretend.

00:35:26,102 --> 00:35:27,205
So as long as you've got confidence

00:35:27,205 --> 00:35:30,793
that you can stop this before it gets to the exchange

00:35:30,793 --> 00:35:34,199
within your own software, within your own control,

00:35:34,199 --> 00:35:38,449
then pick a number somewhere between 1,000 to 10,000.

00:35:38,449 --> 00:35:39,737
That's gonna be the number of times

00:35:39,737 --> 00:35:43,904
that you simulate sending an order through your system.

00:35:45,722 --> 00:35:47,824
If you're using a low latency network card

00:35:47,824 --> 00:35:50,024
such as Mellanox or Solar Flare

00:35:50,024 --> 00:35:52,333
chances are even the card will allow you to do this.

00:35:52,333 --> 00:35:53,743
This is industry practice,

00:35:53,743 --> 00:35:58,090
it understands that people want to push data onto the card

00:35:58,090 --> 00:35:59,077
but not send it.

00:35:59,077 --> 00:36:00,870
It's just warming the card.

00:36:00,870 --> 00:36:03,783
So network cards will support this, so that's great.

00:36:03,783 --> 00:36:05,770
So basically saturate your system with

00:36:05,770 --> 00:36:07,238
I'm going to send an order, I'm going to send an order,

00:36:07,238 --> 00:36:12,161
I'm going to send an order, bang, I did send an order.

00:36:12,161 --> 00:36:15,411
Keeps your instruction cache hot, yeah?

00:36:16,679 --> 00:36:19,145
Nothing's gonna get evicted if you're doing that.

00:36:19,145 --> 00:36:21,098
Probably keeps your data cache hot

00:36:21,098 --> 00:36:22,910
if you're picking the right data,

00:36:22,910 --> 00:36:25,214
and it trains your hardware branch predictor.

00:36:25,214 --> 00:36:28,459
Your branch predictor, if it's just done the same thing

00:36:28,459 --> 00:36:30,542
10,000 times before in the last second,

00:36:30,542 --> 00:36:34,709
chances are it's gonna pick the right path this time.

00:36:36,109 --> 00:36:36,942
Hey.

00:36:38,127 --> 00:36:41,213
- [Audience Member] Do you use profile-guided optimization?

00:36:41,213 --> 00:36:42,079
- Ah yeah, good question.

00:36:42,079 --> 00:36:43,839
Do I use profile-guided optimization?

00:36:43,839 --> 00:36:46,851
Yeah, sometimes, in many different ways.

00:36:46,851 --> 00:36:49,747
Yeah, so profile-guided optimization

00:36:49,747 --> 00:36:52,331
is kind of a substitute for this,

00:36:52,331 --> 00:36:56,666
but again that's only something you can do at compile time

00:36:56,666 --> 00:37:00,434
and it's not gonna help with the hardware runtime.

00:37:00,434 --> 00:37:03,979
But indeed, yeah profile-guided optimization is great

00:37:03,979 --> 00:37:06,919
except you can overfit the model.

00:37:06,919 --> 00:37:10,246
So profile-guided optimization is effectively,

00:37:10,246 --> 00:37:14,413
you run a simulation, or maybe even run a production,

00:37:16,009 --> 00:37:19,259
where you're writing to a profiling file

00:37:19,259 --> 00:37:22,116
of what's actually happening with your system,

00:37:22,116 --> 00:37:24,604
then you recompile with that information

00:37:24,604 --> 00:37:26,840
and then the compiler can use that for branch prediction

00:37:26,840 --> 00:37:28,343
hints and reordering.

00:37:28,343 --> 00:37:29,715
So that works relatively well,

00:37:29,715 --> 00:37:33,231
but you need to make sure that your profiling

00:37:33,231 --> 00:37:36,588
that you're doing matches production.

00:37:36,588 --> 00:37:40,397
And sometimes that means that you're very fast sometimes

00:37:40,397 --> 00:37:42,846
and way off the mark other times.

00:37:42,846 --> 00:37:45,592
So it's kind of this thing that looks really great

00:37:45,592 --> 00:37:48,506
and it is some of the time.

00:37:48,506 --> 00:37:49,673
Good question.

00:37:51,989 --> 00:37:54,322
Okay, this is a Xeon E5 CPU.

00:37:57,845 --> 00:38:00,345
How many cores does this have?

00:38:04,778 --> 00:38:05,945
I claim eight.

00:38:07,014 --> 00:38:09,312
And the cores are on the left and the right.

00:38:09,312 --> 00:38:11,508
And what's in the middle?

00:38:11,508 --> 00:38:14,175
Cache. Glorious, glorious cache.

00:38:16,106 --> 00:38:19,052
L3 cache. Probably about 50 megs worth of cache.

00:38:19,052 --> 00:38:21,802
Very fast, this is what you want.

00:38:23,352 --> 00:38:26,709
How many cores get access to this cache?

00:38:26,709 --> 00:38:27,709
All of them.

00:38:29,533 --> 00:38:31,642
Well that sounds like a problem.

00:38:31,642 --> 00:38:34,166
Because I want this cache all to myself.

00:38:34,166 --> 00:38:34,999
Yeah?

00:38:36,540 --> 00:38:40,412
I turn off all but one of your cores.

00:38:40,412 --> 00:38:43,461
Now you get the cache all to yourself.

00:38:43,461 --> 00:38:45,933
It's a neat trick, it's not very respectful

00:38:45,933 --> 00:38:49,266
to any intel engineers in the room today

00:38:50,900 --> 00:38:53,721
and as a teenager I couldn't believe I'd be

00:38:53,721 --> 00:38:56,066
having eight cores and working for a company

00:38:56,066 --> 00:38:59,166
where I could turn all but one of them off.

00:38:59,166 --> 00:39:02,039
Or 22 cores and turning all of one of them off.

00:39:02,039 --> 00:39:05,261
It's a really effective way of increasing the average

00:39:05,261 --> 00:39:06,761
L3 cache per core.

00:39:07,850 --> 00:39:08,890
(audience laughs)

00:39:08,890 --> 00:39:09,723
Hey.

00:39:13,731 --> 00:39:14,564
Sorry?

00:39:18,986 --> 00:39:22,069
Have I tried using a single core CPU?

00:39:23,085 --> 00:39:24,749
Oh (laughs), interesting.

00:39:24,749 --> 00:39:26,811
So have I tried using a single core CPU

00:39:26,811 --> 00:39:29,644
or convincing someone to make one?

00:39:30,886 --> 00:39:35,053
So no I haven't, for the answer to both of your questions.

00:39:36,389 --> 00:39:38,062
We can talk about that afterwards a little bit,

00:39:38,062 --> 00:39:41,748
but I suspect that Intel are not necessarily going to be

00:39:41,748 --> 00:39:44,366
particularly interested in what I would want,

00:39:44,366 --> 00:39:48,533
because my domain is such a small part of Intel's market.

00:39:49,826 --> 00:39:51,826
That's just my gut feel.

00:39:54,627 --> 00:39:57,794
Oh incidentally, if I can go back one.

00:39:59,740 --> 00:40:02,411
Yeah if you do run multiple cores,

00:40:02,411 --> 00:40:04,715
normally you won't disable your cores,

00:40:04,715 --> 00:40:07,536
but just be careful about what other processors are running

00:40:07,536 --> 00:40:09,472
on other cores that share that cache.

00:40:09,472 --> 00:40:11,748
If they're noisy it's gonna cost you.

00:40:11,748 --> 00:40:13,356
If you've got noisy neighbors, get rid of them,

00:40:13,356 --> 00:40:16,450
put them on a completely different CPU somewhere else.

00:40:16,450 --> 00:40:19,701
It will probably help you out.

00:40:19,701 --> 00:40:20,701
Great quote.

00:40:23,727 --> 00:40:26,560
(audience laughs)

00:40:31,786 --> 00:40:36,115
Okay this is nitpicking, but this caught me out

00:40:36,115 --> 00:40:37,502
and it caught people from other trading companies

00:40:37,502 --> 00:40:38,335
out as well.

00:40:38,335 --> 00:40:42,502
Placement new can be slightly inefficient. Slightly.

00:40:43,720 --> 00:40:46,803
So if you use any of these compilers,

00:40:47,886 --> 00:40:52,282
placement new will be doing a null pointer check.

00:40:52,282 --> 00:40:56,442
And if the pointer that you pass into placement new is null

00:40:56,442 --> 00:40:59,193
it won't construct the object, it won't destruct the object,

00:40:59,193 --> 00:41:03,286
and it will return null back to the caller.

00:41:03,286 --> 00:41:04,312
Why?

00:41:04,312 --> 00:41:07,395
Because that's what regular new does.

00:41:08,510 --> 00:41:10,210
And the spec was a little bit ambiguous

00:41:10,210 --> 00:41:12,562
as to what placement new was meant to do.

00:41:12,562 --> 00:41:14,495
So most compiler implementers just did the same thing

00:41:14,495 --> 00:41:16,461
as regular new.

00:41:16,461 --> 00:41:17,766
But it's actually really inefficient.

00:41:17,766 --> 00:41:20,650
Why would C++ do an additional check for you?

00:41:20,650 --> 00:41:22,479
That's not the idea of C++,

00:41:22,479 --> 00:41:24,971
you don't pay for what you don't get.

00:41:24,971 --> 00:41:27,320
You don't pay for what you don't use.

00:41:27,320 --> 00:41:29,712
If you're going to pass a pointer into a function

00:41:29,712 --> 00:41:31,937
you should make sure that it's null or not null.

00:41:31,937 --> 00:41:32,770
Yeah, so.

00:41:34,189 --> 00:41:38,201
Marc Glisse and Jonathan Wakely got the spec updated,

00:41:38,201 --> 00:41:41,984
the standard updated, so that's now undefined behavior

00:41:41,984 --> 00:41:44,043
to pass null on to placement new.

00:41:44,043 --> 00:41:45,997
That means that the compiler can stop doing this null

00:41:45,997 --> 00:41:47,535
pointer check.

00:41:47,535 --> 00:41:51,667
Does a single null pointer check make a difference?

00:41:51,667 --> 00:41:52,910
Actually it does, yeah.

00:41:52,910 --> 00:41:55,793
So if you were right on the border of this function

00:41:55,793 --> 00:41:57,965
being inlined and not being inlined and this additional

00:41:57,965 --> 00:42:00,693
instruction could push it over the top.

00:42:00,693 --> 00:42:05,193
Also with GCC, GCC was particularly sensitive to this,

00:42:05,193 --> 00:42:10,085
and actually gave up on a whole bunch of optimizations.

00:42:10,085 --> 00:42:12,142
So yeah, just a small thing, but it actually made

00:42:12,142 --> 00:42:13,997
quite a big difference, enough that several

00:42:13,997 --> 00:42:16,593
trading companies picked up on this.

00:42:16,593 --> 00:42:18,973
If you can't use a modern version of the compiler

00:42:18,973 --> 00:42:22,124
that has the effects, there's a quick workaround

00:42:22,124 --> 00:42:25,624
that will work around it, at least in GCC.

00:42:26,853 --> 00:42:31,004
Okay another thing that caught me out is

00:42:31,004 --> 00:42:35,171
GCC five and below or below GCC 5.1, standard string

00:42:36,472 --> 00:42:38,544
it wasn't a great implementation.

00:42:38,544 --> 00:42:40,868
Had copy on right semantics and null small string

00:42:40,868 --> 00:42:42,699
optimization that wasn't there.

00:42:42,699 --> 00:42:46,157
So great, we moved to later versions of GCC,

00:42:46,157 --> 00:42:47,732
started using strings all over the place,

00:42:47,732 --> 00:42:50,582
but what do you know, it was slow.

00:42:50,582 --> 00:42:54,826
Because we use a standard distribution of Redhat and Centos

00:42:54,826 --> 00:42:58,362
and the way that GCC is packaged is it maintains

00:42:58,362 --> 00:43:00,730
ABI backwards compatibility.

00:43:00,730 --> 00:43:04,334
So it doesn't do the standard string breaking change,

00:43:04,334 --> 00:43:07,370
which means that we're still stuck with the old

00:43:07,370 --> 00:43:09,755
standard string implementation.

00:43:09,755 --> 00:43:13,100
And that was first noticed as being a bit questionable

00:43:13,100 --> 00:43:17,013
by Herb Sutter in 1999, and today in 2017

00:43:17,013 --> 00:43:20,596
I'm still stuck with copy-on-write strings.

00:43:24,269 --> 00:43:28,436
Another small nitpick, but C++11 has this

00:43:29,900 --> 00:43:32,528
static local variable initialization

00:43:32,528 --> 00:43:34,896
guarantees that static will be initialized once

00:43:34,896 --> 00:43:37,368
and only once even if multithreaded.

00:43:37,368 --> 00:43:40,691
Even if an exception comes in and blows away that routine

00:43:40,691 --> 00:43:43,861
it'll go okay we're not initialized yet.

00:43:43,861 --> 00:43:48,534
So very useful, but of course, that doesn't come for free.

00:43:48,534 --> 00:43:50,650
So every time that you refer to that static variable

00:43:50,650 --> 00:43:53,510
inside that function there's going to be a slight cost,

00:43:53,510 --> 00:43:54,868
which is checking the guard variable

00:43:54,868 --> 00:43:57,055
to see if that has been set or not.

00:43:57,055 --> 00:43:58,565
So again it's a minor nitpick,

00:43:58,565 --> 00:44:01,268
but if you are absolutely micro optimizing code,

00:44:01,268 --> 00:44:04,518
this will have a slight impact as well.

00:44:05,751 --> 00:44:09,174
Who knows that standard function might allocate?

00:44:09,174 --> 00:44:11,091
Most people? Yep, cool.

00:44:13,452 --> 00:44:15,318
So that can be a little bit surprising,

00:44:15,318 --> 00:44:17,070
because sometimes it doesn't allocate.

00:44:17,070 --> 00:44:18,853
It depends on the implementation.

00:44:18,853 --> 00:44:22,140
You can do a small function optimization in there

00:44:22,140 --> 00:44:24,640
around about 16 bytes I think.

00:44:25,586 --> 00:44:30,556
So this is with GCC 7.2, this is a silly trivial example,

00:44:30,556 --> 00:44:33,283
but here we are, we're not actually doing anything,

00:44:33,283 --> 00:44:36,450
we're capturing 24 bytes worth of data

00:44:37,321 --> 00:44:38,893
but we're not doing anything with it.

00:44:38,893 --> 00:44:42,183
Clang will just optimize this right out, GCC doesn't.

00:44:42,183 --> 00:44:44,859
So you'll actually see that there's an allocation

00:44:44,859 --> 00:44:46,579
for this code and actually deallocation,

00:44:46,579 --> 00:44:49,046
which I didn't print on the screen.

00:44:49,046 --> 00:44:49,879
That gets a little bit annoying,

00:44:49,879 --> 00:44:54,384
because standard functions are actually very very useful.

00:44:54,384 --> 00:44:56,536
So you'll see that several people have implemented

00:44:56,536 --> 00:44:57,673
nonallocating versions.

00:44:57,673 --> 00:44:59,642
I believe there's talks on this,

00:44:59,642 --> 00:45:02,470
again I would possibly claim later on today,

00:45:02,470 --> 00:45:04,637
but maybe I'm wrong again.

00:45:05,883 --> 00:45:07,349
But yeah if you want to,

00:45:07,349 --> 00:45:08,378
this is a shameless plug by the way,

00:45:08,378 --> 00:45:11,103
but go check out SG14's inplace function,

00:45:11,103 --> 00:45:12,542
which doesn't allocate.

00:45:12,542 --> 00:45:15,341
It will allocate inplace, which means that if you declare

00:45:15,341 --> 00:45:18,431
this function on the stack, the buffer for the closure

00:45:18,431 --> 00:45:22,013
will be on the stack with compile time static asserts.

00:45:22,013 --> 00:45:25,513
It also runtime checks if need be as well.

00:45:27,568 --> 00:45:31,225
Okay, the very last point for this section,

00:45:31,225 --> 00:45:34,259
then we've only got a small section to go,

00:45:34,259 --> 00:45:36,711
is glibc can be particularly evil,

00:45:36,711 --> 00:45:40,413
if you're looking to do low latency stuff.

00:45:40,413 --> 00:45:44,580
So standard power, if you call it with one to the power

00:45:45,697 --> 00:45:48,278
of 1.4, you'll get the right answer.

00:45:48,278 --> 00:45:51,528
If you call it 1.0 to the power of 1.5,

00:45:53,063 --> 00:45:55,174
you're also gonna get the right answer,

00:45:55,174 --> 00:45:58,253
but one's gonna be way slower than the other.

00:45:58,253 --> 00:46:01,870
It's not quite 1.0 by the way, it's 1.0 with a little bit

00:46:01,870 --> 00:46:04,204
of rounding error in there.

00:46:04,204 --> 00:46:06,042
But if you look at these timings,

00:46:06,042 --> 00:46:10,209
if we go on to 1.4, 53 nanos on both an old version

00:46:11,851 --> 00:46:15,528
of glibc and a new version of glibc, great.

00:46:15,528 --> 00:46:18,118
If you do it with 1.5 the pathological case,

00:46:18,118 --> 00:46:19,102
you're in trouble.

00:46:19,102 --> 00:46:22,602
Half a millisecond to calculate one single

00:46:23,759 --> 00:46:26,266
computation of power.

00:46:26,266 --> 00:46:27,878
The reason is the function's transcendental,

00:46:27,878 --> 00:46:30,428
it'll try to be accurate fast,

00:46:30,428 --> 00:46:33,174
if not it'll try to be accurate slow.

00:46:33,174 --> 00:46:35,491
So this has caught us out, it's effectively crashed

00:46:35,491 --> 00:46:37,610
our autotraders, because if you try to calculate this

00:46:37,610 --> 00:46:42,062
a thousand times quickly, everything kind of stops.

00:46:42,062 --> 00:46:46,229
Yeah, relatively surprising, you can upgrade to glibc 2.21

00:46:47,227 --> 00:46:49,429
which is a couple of years old now I think,

00:46:49,429 --> 00:46:52,741
that'll help you out, but most standard distros of Linux

00:46:52,741 --> 00:46:54,684
will be packaged with 2.17.

00:46:54,684 --> 00:46:57,767
So again, something to watch out for.

00:46:59,770 --> 00:47:02,423
While I'm on this topic, another thing

00:47:02,423 --> 00:47:04,383
which isn't on my slides but I should mention,

00:47:04,383 --> 00:47:06,415
is try to avoid system calls.

00:47:06,415 --> 00:47:07,983
System calls will just kill you.

00:47:07,983 --> 00:47:10,968
So you don't want to be going to the kernel,

00:47:10,968 --> 00:47:15,135
at all, you want just your C++ code to be run.

00:47:16,634 --> 00:47:19,560
You wanna get all interrupts absolutely away

00:47:19,560 --> 00:47:20,955
from the call that you're running.

00:47:20,955 --> 00:47:24,842
You want your hotpath to be running on a single call.

00:47:24,842 --> 00:47:26,879
Nothing else needs to know about that single call.

00:47:26,879 --> 00:47:29,107
Any sort of system calls, like a call to select

00:47:29,107 --> 00:47:33,333
or a call to kernal space tcp or anything

00:47:33,333 --> 00:47:35,722
that might invoke a system call whatsoever,

00:47:35,722 --> 00:47:38,510
you want to get rid of that.

00:47:38,510 --> 00:47:42,677
Sorry, just popped into my head, I thought I should say it.

00:47:46,962 --> 00:47:50,129
Not particularly funny, but very true.

00:47:51,954 --> 00:47:54,055
So the final section, only a couple of slides actually.

00:47:54,055 --> 00:47:55,888
Hey! I got a reminder.

00:47:56,747 --> 00:47:58,797
Talk, yes, that is right.

00:47:58,797 --> 00:48:01,797
(audience chuckles)

00:48:03,343 --> 00:48:05,760
That could've gone far worse.

00:48:12,644 --> 00:48:16,811
Okay, there's two ways to kind of measure things really,

00:48:18,228 --> 00:48:19,887
or two common approaches.

00:48:19,887 --> 00:48:24,303
One is to profile, see what your code is doing.

00:48:24,303 --> 00:48:26,953
The other one's to actually benchmark and time it.

00:48:26,953 --> 00:48:29,885
These two things are subtly different.

00:48:29,885 --> 00:48:32,811
So profiling is good, it can show you hey look

00:48:32,811 --> 00:48:34,993
you're spending most of your time in Malloc.

00:48:34,993 --> 00:48:38,906
Why are you even in Malloc? This makes no sense.

00:48:38,906 --> 00:48:41,889
Whereas benchmarking is actually start, stop,

00:48:41,889 --> 00:48:44,078
right this is how fast we are.

00:48:44,078 --> 00:48:47,375
If you make an improvement to your code based on profiling,

00:48:47,375 --> 00:48:50,386
that's great, but maybe your code's not faster,

00:48:50,386 --> 00:48:51,288
maybe it's slower.

00:48:51,288 --> 00:48:54,643
You know, you're just guessing. You need to measure.

00:48:54,643 --> 00:48:57,441
Once you've finished measuring, measure again,

00:48:57,441 --> 00:49:01,191
and then measure one more time, a third time.

00:49:02,496 --> 00:49:05,413
Okay so gprof, a sampling profiler.

00:49:06,323 --> 00:49:08,923
Gprof is great for checking out what your code is doing,

00:49:08,923 --> 00:49:10,740
but it's not going to work if most of the time

00:49:10,740 --> 00:49:12,517
your code is doing nothing and then

00:49:12,517 --> 00:49:14,761
for 300 or 400 nanoseconds, all of a sudden

00:49:14,761 --> 00:49:17,177
it's doing something and then it goes back to being idle.

00:49:17,177 --> 00:49:18,124
Gprof is going to tell you

00:49:18,124 --> 00:49:19,721
that you're doing absolutely nothing.

00:49:19,721 --> 00:49:23,427
It's gonna miss the actual events that you care about.

00:49:23,427 --> 00:49:24,677
Gprofs are out.

00:49:25,819 --> 00:49:28,402
Valgrind, callgrind, fantastic.

00:49:29,369 --> 00:49:31,226
Can tell you actually what your code is doing,

00:49:31,226 --> 00:49:33,644
which functions it's going into,

00:49:33,644 --> 00:49:35,531
how much memory you're using.

00:49:35,531 --> 00:49:37,165
But it's basically a virtual machine,

00:49:37,165 --> 00:49:40,299
it's simulating the CPU, it doesn't even take into account

00:49:40,299 --> 00:49:44,082
I/O characteristics, that's gone as well.

00:49:44,082 --> 00:49:45,744
It's too intrusive, it's not going to give you

00:49:45,744 --> 00:49:48,161
accurate performance results.

00:49:49,627 --> 00:49:52,094
I've been talking about microbenchmarking a lot.

00:49:52,094 --> 00:49:55,135
It's kind of okay, but don't rely on it.

00:49:55,135 --> 00:49:56,785
Because if you're sitting there spinning in an absolute

00:49:56,785 --> 00:50:00,340
tightloop, doing map lookups, that's a very different

00:50:00,340 --> 00:50:02,537
characteristic from what your code is gonna be doing

00:50:02,537 --> 00:50:05,881
in production when it's doing a one off map lookup

00:50:05,881 --> 00:50:08,502
once every eight trillion cycles.

00:50:08,502 --> 00:50:11,257
So benchmarks are kind of nice for map shootouts

00:50:11,257 --> 00:50:13,053
a little bit and things like that,

00:50:13,053 --> 00:50:15,559
but you can only take them so far.

00:50:15,559 --> 00:50:17,040
Now all of these tools are really useful,

00:50:17,040 --> 00:50:19,630
don't get me wrong I use all of them, they're great,

00:50:19,630 --> 00:50:22,010
but not necessarily for that very last step

00:50:22,010 --> 00:50:24,427
of microoptimizing your code.

00:50:27,385 --> 00:50:29,552
So we need something else.

00:50:30,717 --> 00:50:34,392
So this is what I use, and it is by far the most accurate

00:50:34,392 --> 00:50:37,277
benchmarking system I can come up with.

00:50:37,277 --> 00:50:41,660
What you do is you actually set up a production system,

00:50:41,660 --> 00:50:45,937
the server on the left here is replaying market data

00:50:45,937 --> 00:50:47,020
in real time.

00:50:48,038 --> 00:50:51,147
Across a network, my code is running on the server

00:50:51,147 --> 00:50:53,797
on the right, picking up the market data

00:50:53,797 --> 00:50:55,759
and then from time to time sending a trade

00:50:55,759 --> 00:50:58,720
or a request to trade back across the wire

00:50:58,720 --> 00:51:00,508
to where it thinks is the exchange.

00:51:00,508 --> 00:51:02,812
Of course it's just a fake exchange.

00:51:02,812 --> 00:51:04,798
What I've got in the middle is a switch,

00:51:04,798 --> 00:51:06,632
and the switch is tapped into the network

00:51:06,632 --> 00:51:10,382
and it's unobtrusively capturing every packet

00:51:11,280 --> 00:51:14,062
that's going across the wire and recording it,

00:51:14,062 --> 00:51:16,672
and also putting a timestamp into either the header

00:51:16,672 --> 00:51:20,413
or the footer of the evernet packet.

00:51:20,413 --> 00:51:22,913
So actually recording that true time

00:51:22,913 --> 00:51:25,817
that that packet went across the wire.

00:51:25,817 --> 00:51:29,343
Probably within an accuracy of five nanoseconds

00:51:29,343 --> 00:51:30,904
or something like that.

00:51:30,904 --> 00:51:33,529
Then afterwards, once you've done an hour

00:51:33,529 --> 00:51:37,030
or a days worth of simulated trading,

00:51:37,030 --> 00:51:38,440
then you can analyze the packets,

00:51:38,440 --> 00:51:41,976
look at which packets triggered an event,

00:51:41,976 --> 00:51:44,459
compare the timestamp of the corresponding packet

00:51:44,459 --> 00:51:46,406
that got sent back out to exchange, do a difference

00:51:46,406 --> 00:51:48,936
of the timestamps and that will tell you the actual

00:51:48,936 --> 00:51:53,160
true time that it's taking for your system to run.

00:51:53,160 --> 00:51:57,095
And if you see a speedup in this lab setup,

00:51:57,095 --> 00:51:58,762
you'll see a speedup in production.

00:51:58,762 --> 00:52:00,456
If you see a slowdown in the lab,

00:52:00,456 --> 00:52:02,837
you'll see a slowdown in production.

00:52:02,837 --> 00:52:06,833
Incredibly hard to set up, you have to get this perfect,

00:52:06,833 --> 00:52:10,034
all the time you'll forget ...

00:52:10,034 --> 00:52:12,565
You forgot to turn off particular interwraps

00:52:12,565 --> 00:52:14,243
or you forgot to set CPU affinity,

00:52:14,243 --> 00:52:16,158
or someone decided it might be a good idea

00:52:16,158 --> 00:52:17,760
to build on that server because they found

00:52:17,760 --> 00:52:19,448
that it was free and very few people were on it

00:52:19,448 --> 00:52:22,550
so they ran GCC on 24 cores.

00:52:22,550 --> 00:52:25,440
But this is a real pain, it's not easy to set up,

00:52:25,440 --> 00:52:27,345
but it's by far the most accurate way

00:52:27,345 --> 00:52:29,262
to do true measurement.

00:52:39,730 --> 00:52:42,037
Okay, so the brief summary

00:52:42,037 --> 00:52:45,415
is you don't need to be an expert about C++,

00:52:45,415 --> 00:52:46,807
but you do need to know it relatively well

00:52:46,807 --> 00:52:48,711
if you want fast low latency software.

00:52:48,711 --> 00:52:50,355
You have to understand your compiler.

00:52:50,355 --> 00:52:52,786
You have to understand what your compiler's trying to do

00:52:52,786 --> 00:52:56,141
and help your compiler along from time to time.

00:52:56,141 --> 00:52:58,552
Knowing the machine architecture will help.

00:52:58,552 --> 00:53:03,526
Knowing the impact of cache, the impact of other processes,

00:53:03,526 --> 00:53:08,234
it's gonna help you make create design decisions.

00:53:08,234 --> 00:53:12,151
Ultimately, aim for really really simple logic.

00:53:16,289 --> 00:53:19,273
Not just simple logic, but get rid of your logic if you can,

00:53:19,273 --> 00:53:23,155
go for static asserts, cons expressions, templates,

00:53:23,155 --> 00:53:26,599
just try to make your compilers life as easy as possible

00:53:26,599 --> 00:53:29,193
in that respect or reduce your code as much.

00:53:29,193 --> 00:53:31,573
The best thing about this is compilers optimize

00:53:31,573 --> 00:53:33,547
simple code the best by far.

00:53:33,547 --> 00:53:35,594
Simple code runs faster than complex code.

00:53:35,594 --> 00:53:38,011
It's just a fact, just as ...

00:53:39,155 --> 00:53:42,091
Sometimes you don't need perfect precision,

00:53:42,091 --> 00:53:43,978
sometimes an approximation is absolutely fine

00:53:43,978 --> 00:53:46,868
and if the approximation is fast, that's great.

00:53:46,868 --> 00:53:49,399
There's no point calculating a theoretical price

00:53:49,399 --> 00:53:54,241
to 12 significant digits if really only two digits would do.

00:53:54,241 --> 00:53:56,058
And if you need to do expensive work,

00:53:56,058 --> 00:53:58,501
don't do it on the hotpath, just do it when you think

00:53:58,501 --> 00:54:00,001
it's a quiet time.

00:54:01,471 --> 00:54:03,501
And ultimately it's about measurement.

00:54:03,501 --> 00:54:06,174
If you can't measure, you're basically just guessing.

00:54:06,174 --> 00:54:08,190
It's incredible, every time that I think

00:54:08,190 --> 00:54:09,589
this code is gonna be faster,

00:54:09,589 --> 00:54:11,871
do do do do, measure it, it's always slower.

00:54:11,871 --> 00:54:15,038
I always get it wrong, it's difficult.

00:54:16,689 --> 00:54:18,417
Okay, so that's me.

00:54:18,417 --> 00:54:20,994
This was quite lightweight, not entirely scientific,

00:54:20,994 --> 00:54:23,214
it's more just a hey this is the sort of things

00:54:23,214 --> 00:54:24,664
that we do.

00:54:24,664 --> 00:54:27,321
I hope you found that somewhat useful.

00:54:27,321 --> 00:54:29,981
Did people learn something?

00:54:29,981 --> 00:54:31,869
Little bit? So-so?

00:54:31,869 --> 00:54:33,329
Okay, cool.

00:54:33,329 --> 00:54:34,537
I'm pleased. Thank you.

00:54:34,537 --> 00:54:38,204
Cheers. (audience applauds)

00:54:41,697 --> 00:54:43,552
We've got a couple minutes for questions

00:54:43,552 --> 00:54:45,452
if anyone feels like it.

00:54:45,452 --> 00:54:46,285
Hey.

00:54:47,752 --> 00:54:49,577
- [Audience Member] Hi, can you comment

00:54:49,577 --> 00:54:51,940
in the use cases when you were only using one core

00:54:51,940 --> 00:54:56,331
on the multicore chip, do you know if Intel's Turbo Boost

00:54:56,331 --> 00:54:59,164
was helping you out at all or not?

00:55:00,822 --> 00:55:02,595
- Yeah, that's a good question.

00:55:02,595 --> 00:55:04,540
So I could spend another hour or two talking

00:55:04,540 --> 00:55:07,394
about the actual system tuning type of it.

00:55:07,394 --> 00:55:08,939
Won't go into too much detail here,

00:55:08,939 --> 00:55:12,171
but effectively you need to be very careful

00:55:12,171 --> 00:55:15,058
of Turbo Boost and different states,

00:55:15,058 --> 00:55:16,734
you just want things to be constant.

00:55:16,734 --> 00:55:18,336
You don't want things to be jumping up and down

00:55:18,336 --> 00:55:19,584
and yeah ...

00:55:19,584 --> 00:55:20,417
Cool.

00:55:21,752 --> 00:55:22,585
Hey.

00:55:22,585 --> 00:55:24,637
Oh, sorry, we'll go there.

00:55:24,637 --> 00:55:25,838
Hi.

00:55:25,838 --> 00:55:28,526
- [Audience Member] Hi, so one quick thing,

00:55:28,526 --> 00:55:30,689
the actual question is on newer architectures,

00:55:30,689 --> 00:55:32,854
you can actually force the L3 cache to be partitioned?

00:55:32,854 --> 00:55:34,126
- Yeah. - [Audience Member] Okay.

00:55:34,126 --> 00:55:36,916
You already know, but I'm talking about other people.

00:55:36,916 --> 00:55:39,076
- I'll just recap in case no one heard that,

00:55:39,076 --> 00:55:40,078
or I just cut you off.

00:55:40,078 --> 00:55:42,060
Yeah, you don't just have to turn off CPUs,

00:55:42,060 --> 00:55:44,870
you can actually lock the cache with latest versions

00:55:44,870 --> 00:55:46,838
of Intel CPUs.

00:55:46,838 --> 00:55:48,310
- [Audience Member] Right.

00:55:48,310 --> 00:55:51,736
When you describe the hash table,

00:55:51,736 --> 00:55:54,392
the layout in memory, you had an array,

00:55:54,392 --> 00:55:57,262
and you had the full hash, which you keep

00:55:57,262 --> 00:56:01,651
and then a pointer, but usually in HFT you only really care

00:56:01,651 --> 00:56:04,054
about access, not insertion.

00:56:04,054 --> 00:56:05,941
Insertion's often outside of the critical path.

00:56:05,941 --> 00:56:06,774
- Correct, yeah. - [Audience Member] Not always

00:56:06,774 --> 00:56:07,607
but often.

00:56:07,607 --> 00:56:10,332
So why not instead have ...

00:56:10,332 --> 00:56:12,663
If you have the key values in an array as well,

00:56:12,663 --> 00:56:15,460
you can just use the index of the hash itself

00:56:15,460 --> 00:56:16,528
to access that.

00:56:16,528 --> 00:56:19,925
Now you're only storing the hashes in the first lookup array

00:56:19,925 --> 00:56:21,345
so you get eight instead of four.

00:56:21,345 --> 00:56:22,262
So why not?

00:56:24,169 --> 00:56:26,037
- If I understand the question correctly,

00:56:26,037 --> 00:56:28,553
won't that put more pressure on your cache?

00:56:28,553 --> 00:56:32,720
Because you'd actually have less hash values per cache line.

00:56:34,038 --> 00:56:35,062
- [Audience Member] You'll have more, because you're not

00:56:35,062 --> 00:56:37,379
storing the pointer at all.

00:56:37,379 --> 00:56:39,969
The first layer of the lookup is just into an array

00:56:39,969 --> 00:56:41,781
with nothing but the hashes.

00:56:41,781 --> 00:56:42,614
- Oh I see what you're saying.

00:56:42,614 --> 00:56:43,447
- [Audience Member] And then if you verified

00:56:43,447 --> 00:56:45,988
that the hash is correct, you just use the index

00:56:45,988 --> 00:56:47,874
and go into the key value array.

00:56:47,874 --> 00:56:50,591
- Yeah you could do that, but that's going to also,

00:56:50,591 --> 00:56:52,430
as long as you're very confident that you're not getting

00:56:52,430 --> 00:56:54,843
too many collisions.

00:56:54,843 --> 00:56:55,925
So I think you're going to get a tradeoff.

00:56:55,925 --> 00:56:58,345
I see what you're saying, and yeah you could do that,

00:56:58,345 --> 00:57:02,175
but that's assuming a pretty damn good hash function

00:57:02,175 --> 00:57:03,157
I think.

00:57:03,157 --> 00:57:05,193
But there's always ...

00:57:05,193 --> 00:57:07,668
There's no right and wrong answer I think for hash tables

00:57:07,668 --> 00:57:10,907
there's always a bit of a sliding scale.

00:57:10,907 --> 00:57:12,026
- [Audience Member] Alright.

00:57:12,026 --> 00:57:13,174
- Thank you, cheers.

00:57:13,174 --> 00:57:14,507
Hey.

00:57:14,507 --> 00:57:17,549
- [Audience Member] So I'm interested about

00:57:17,549 --> 00:57:20,825
where you talked about using only one CPU

00:57:20,825 --> 00:57:22,491
unlocking the outlet.

00:57:22,491 --> 00:57:23,692
Others are one core.

00:57:23,692 --> 00:57:26,877
Do you mean that you're going to use one thread as well?

00:57:26,877 --> 00:57:28,078
- Yeah.

00:57:28,078 --> 00:57:29,094
- [Audience Member] And how does that work

00:57:29,094 --> 00:57:32,130
when you have multiple data sources that you need to

00:57:32,130 --> 00:57:33,088
look at and everything?

00:57:33,088 --> 00:57:34,151
Like ...

00:57:34,151 --> 00:57:34,984
- Yeah.

00:57:34,984 --> 00:57:35,817
- [Audience Member] Can you explain a bit about that?

00:57:35,817 --> 00:57:38,623
- Yeah sure, so this is getting away from the C++ side,

00:57:38,623 --> 00:57:40,275
which I purposely didn't want to talk about,

00:57:40,275 --> 00:57:42,548
being a C++ conference, but very very briefly,

00:57:42,548 --> 00:57:46,048
yeah exactly, one thread on that one core.

00:57:47,226 --> 00:57:49,694
That's it, that's the fast thread.

00:57:49,694 --> 00:57:54,262
But of course you can from time to time pull data coming in

00:57:54,262 --> 00:57:56,248
once every few hundred milliseconds,

00:57:56,248 --> 00:57:58,967
then pull the data, and statistically you should be okay.

00:57:58,967 --> 00:58:01,062
It all very much depends on how much data's coming in,

00:58:01,062 --> 00:58:02,508
if you've got a lot of data coming in,

00:58:02,508 --> 00:58:04,527
then no you'd need a second thread.

00:58:04,527 --> 00:58:06,065
But yeah, effectively ...

00:58:06,065 --> 00:58:07,917
- [Audience Member] I mean though the data that you need

00:58:07,917 --> 00:58:11,142
to respond quickly, the data from the market.

00:58:11,142 --> 00:58:14,038
So you can't actually pull them, because then you reduce

00:58:14,038 --> 00:58:15,466
your latency.

00:58:15,466 --> 00:58:18,673
- Ah yeah, true, but you could also have something

00:58:18,673 --> 00:58:20,741
ahead of you doing a little bit of a low pass filter

00:58:20,741 --> 00:58:23,324
or discarding more of the data.

00:58:24,515 --> 00:58:28,426
Yeah, you definitely have a component ahead

00:58:28,426 --> 00:58:29,763
filtering a lot of that out for you,

00:58:29,763 --> 00:58:31,016
otherwise yeah you'd be saturated.

00:58:31,016 --> 00:58:32,453
- [Audience Member] I see. - Yeah.

00:58:32,453 --> 00:58:33,286
- [Audience Member] Thank you.

00:58:33,286 --> 00:58:34,119
- Yeah, you're welcome.

00:58:34,119 --> 00:58:34,952
Hi.

00:58:34,952 --> 00:58:35,785
- [Audience Member] Just a small point.

00:58:35,785 --> 00:58:38,883
You said consider deleting large objects in another thread,

00:58:38,883 --> 00:58:40,174
one of the things I've been burned by

00:58:40,174 --> 00:58:43,621
is that we'll drain the TC Malluc size class for that

00:58:43,621 --> 00:58:47,430
and then cause global Mutex locks across all your threads

00:58:47,430 --> 00:58:50,214
when you allocate or deallocate in that size class.

00:58:50,214 --> 00:58:51,166
- Yeah, I was wondering if someone

00:58:51,166 --> 00:58:53,310
would pick up on that (laughs).

00:58:53,310 --> 00:58:54,678
True.

00:58:54,678 --> 00:58:57,200
We actually often will allocate on that separate thread

00:58:57,200 --> 00:58:59,421
as well and just pass the pointer across.

00:58:59,421 --> 00:59:00,254
Yeah, good point.

00:59:00,254 --> 00:59:03,485
I was praying no one would pick up on that.

00:59:03,485 --> 00:59:04,776
Hi.

00:59:04,776 --> 00:59:06,388
- [Audience Member] Hi, do you use things like

00:59:06,388 --> 00:59:10,104
built in expect to mark branches slightly or unlikely?

00:59:10,104 --> 00:59:12,322
- Yeah, so this will have to be the last question correct?

00:59:12,322 --> 00:59:15,926
Yeah, and I deleted those from my slides this morning

00:59:15,926 --> 00:59:18,927
because I ran out of time, as we have just done as well.

00:59:18,927 --> 00:59:21,311
I mean a built in expect, definitely for those who

00:59:21,311 --> 00:59:24,173
don't know about it, it's just a macro that's

00:59:24,173 --> 00:59:26,913
will tell the compiler hey this is the branch

00:59:26,913 --> 00:59:28,190
that I want you to take.

00:59:28,190 --> 00:59:31,552
Absolutely, you can use that, but it's not a silver bullet.

00:59:31,552 --> 00:59:33,853
Everything is about getting that hardware branch predictor

00:59:33,853 --> 00:59:34,744
trained up.

00:59:34,744 --> 00:59:36,613
In saying that, if you have a function

00:59:36,613 --> 00:59:39,187
which is called very very irregularly

00:59:39,187 --> 00:59:41,596
and you want that function to be fast when it's called.

00:59:41,596 --> 00:59:44,418
The hardware branch predictor has zero information.

00:59:44,418 --> 00:59:48,261
In that case, yeah, your likely true and likely false

00:59:48,261 --> 00:59:51,081
branch prediction hits are exactly what you need

00:59:51,081 --> 00:59:54,100
to make sure that correct path is hit.

00:59:54,100 --> 00:59:55,601
Good question, thank you.

00:59:55,601 --> 00:59:57,390
So we've gotta ...

00:59:57,390 --> 01:00:00,439
We can catch up afterwards, if maybe we have to.

01:00:00,439 --> 01:00:02,767
Times up now. We can talk afterwards.

01:00:02,767 --> 01:00:03,926
Yeah.

01:00:03,926 --> 01:00:04,793
Great, thank you.

01:00:04,793 --> 01:00:06,635

YouTube URL: https://www.youtube.com/watch?v=NH1Tta7purM


