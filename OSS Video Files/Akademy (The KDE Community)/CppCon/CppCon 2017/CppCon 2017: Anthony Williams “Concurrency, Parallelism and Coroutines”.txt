Title: CppCon 2017: Anthony Williams “Concurrency, Parallelism and Coroutines”
Publication date: 2017-10-13
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
C++17 is adding parallel overloads of most of the Standard Library algorithms. There is a TS for Concurrency in C++ already published, and a TS for Coroutines in C++ and a second TS for Concurrency in C++ in the works. 

What does all this mean for programmers? How are they all related? How do coroutines help with parallelism? 

This session will attempt to answer these questions and more. We will look at the implementation of parallel algorithms, and how continuations, coroutines and work-stealing fit together. We will also look at how this meshes with the Grand Unified Executors Proposal, and how you will be able to take advantage of all this as an application developer. 
— 
Anthony Williams: Just Software Solutions Ltd

Anthony Williams is the author of C++ Concurrency in Action.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,089 --> 00:00:02,850
okay well good afternoon everybody I

00:00:01,740 --> 00:00:10,500
hope that you've all been enjoying the

00:00:02,850 --> 00:00:11,730
conference so far excellent okay okay so

00:00:10,500 --> 00:00:14,130
I'm going to talk to you about a few

00:00:11,730 --> 00:00:15,960
things it's afternoon first off I'm

00:00:14,130 --> 00:00:17,730
going to talk to you about the support

00:00:15,960 --> 00:00:21,960
for parallelism that we've introduced

00:00:17,730 --> 00:00:24,000
with the C++ 17 standard then I'm going

00:00:21,960 --> 00:00:27,060
to do a very brief section on the curve

00:00:24,000 --> 00:00:33,090
routines TS and likewise on the

00:00:27,060 --> 00:00:34,739
concurrency TS then a bit about how care

00:00:33,090 --> 00:00:37,649
routines and parallel algorithms can be

00:00:34,739 --> 00:00:42,390
related and finally I'm going to finish

00:00:37,649 --> 00:00:46,739
off with a brief summary of the grand

00:00:42,390 --> 00:00:50,730
unified executives proposal so that's

00:00:46,739 --> 00:00:52,739
the ordered play lots of the things from

00:00:50,730 --> 00:00:55,140
TSS are all in the standard experimental

00:00:52,739 --> 00:00:57,420
namespace that is very long-winded for

00:00:55,140 --> 00:00:59,550
slides so we're going to assume that

00:00:57,420 --> 00:01:02,780
we've got this named face alias in

00:00:59,550 --> 00:01:16,280
effect for that all the sides sdd exp

00:01:02,780 --> 00:01:18,990
instead so parallelism in C++ 17 C++ 17

00:01:16,280 --> 00:01:20,939
provides us with a whole new set of

00:01:18,990 --> 00:01:24,920
overloads for just about every single

00:01:20,939 --> 00:01:29,009
one of the standard library algorithms

00:01:24,920 --> 00:01:34,710
we have a new first parameter this is a

00:01:29,009 --> 00:01:36,570
magic execution policy so hey for each

00:01:34,710 --> 00:01:38,640
you have execution policy and then I'll

00:01:36,570 --> 00:01:43,439
see your begin and end iterators and the

00:01:38,640 --> 00:01:45,840
function you're trying to pass on this

00:01:43,439 --> 00:01:47,640
execution policy tells the library what

00:01:45,840 --> 00:01:53,220
you're wanting out of your parallel

00:01:47,640 --> 00:01:54,810
execution it can be a sequential

00:01:53,220 --> 00:01:57,270
execution that you're asking for in

00:01:54,810 --> 00:02:00,570
which case it's just going to run it on

00:01:57,270 --> 00:02:02,130
the same thread just as usual with a few

00:02:00,570 --> 00:02:05,340
different constraints because now it is

00:02:02,130 --> 00:02:08,879
a parallel execution even though it's

00:02:05,340 --> 00:02:10,080
the sequential policy and so it has the

00:02:08,879 --> 00:02:13,790
same constraints as a parallel execution

00:02:10,080 --> 00:02:13,790
and we'll look at those in just a second

00:02:14,060 --> 00:02:21,060
you might have the parallel policy

00:02:17,370 --> 00:02:24,209
execution par which means that hopefully

00:02:21,060 --> 00:02:26,099
your compiler and library are going to

00:02:24,209 --> 00:02:31,230
split up your the work that you've given

00:02:26,099 --> 00:02:33,389
it across multiple threads and it's

00:02:31,230 --> 00:02:35,069
going to divide up the work in some

00:02:33,389 --> 00:02:39,719
random order which might vary between

00:02:35,069 --> 00:02:42,810
runs but each individual invocation of a

00:02:39,719 --> 00:02:45,450
user-supplied function will run in its

00:02:42,810 --> 00:02:47,280
entirety on a single thread in some

00:02:45,450 --> 00:02:49,049
relative order compared to the other

00:02:47,280 --> 00:02:50,549
part so if you've got for each you're

00:02:49,049 --> 00:02:52,139
going to run your single function the

00:02:50,549 --> 00:02:55,470
function you supplied for each element

00:02:52,139 --> 00:02:59,189
and on a given thread it might run a

00:02:55,470 --> 00:03:02,340
random set of elements right on 5 4 7 27

00:02:59,189 --> 00:03:03,689
3 million some random order and it was

00:03:02,340 --> 00:03:06,000
an unspecified order it might vary

00:03:03,689 --> 00:03:07,950
between runs but on any given thread you

00:03:06,000 --> 00:03:13,200
can say it did these elements in this

00:03:07,950 --> 00:03:15,629
sequence this is different from parallel

00:03:13,200 --> 00:03:20,040
on sequenced the final standard supplied

00:03:15,629 --> 00:03:24,949
execution policy because that one allows

00:03:20,040 --> 00:03:27,269
your functions to overlap now this is

00:03:24,949 --> 00:03:29,540
sometimes hard to get your head round

00:03:27,269 --> 00:03:31,199
but if you think of it like

00:03:29,540 --> 00:03:32,819
vectorization if you've got a

00:03:31,199 --> 00:03:34,979
vectorization instruction if you know

00:03:32,819 --> 00:03:38,579
you're running for reach and what you're

00:03:34,979 --> 00:03:40,439
doing is adding 5 to every element if

00:03:38,579 --> 00:03:42,810
you your processor has vectorization

00:03:40,439 --> 00:03:44,970
instructions it can probably know if

00:03:42,810 --> 00:03:46,739
your elements integers then it can

00:03:44,970 --> 00:03:51,199
probably manage to add 5 to quite a few

00:03:46,739 --> 00:03:54,329
integers in one instruction and so

00:03:51,199 --> 00:03:55,620
you're hoping therefore that the

00:03:54,329 --> 00:03:56,970
compiler and library are going to work

00:03:55,620 --> 00:03:59,159
together to do that for you

00:03:56,970 --> 00:04:00,900
so not only or your functions running on

00:03:59,159 --> 00:04:02,280
multiple threads but it's overlapping

00:04:00,900 --> 00:04:04,439
the execution and running them together

00:04:02,280 --> 00:04:08,220
simultaneously on a single thread if

00:04:04,439 --> 00:04:10,500
possible but it's not just those

00:04:08,220 --> 00:04:13,250
vectorization instructions this is it's

00:04:10,500 --> 00:04:16,609
generally uncie quenched so if you like

00:04:13,250 --> 00:04:20,760
if you're pushing things onto the GPU

00:04:16,609 --> 00:04:23,490
then the things that the GPU can do as a

00:04:20,760 --> 00:04:25,889
vectorization is much more

00:04:23,490 --> 00:04:28,770
there's much more scope there than what

00:04:25,889 --> 00:04:32,220
there is just on a plain CPU and it also

00:04:28,770 --> 00:04:33,900
allows the compiler to look at your

00:04:32,220 --> 00:04:37,500
function if you pass it something that's

00:04:33,900 --> 00:04:40,020
in line like a lambda to your x2 to the

00:04:37,500 --> 00:04:42,569
parallel algorithm then it has a lot

00:04:40,020 --> 00:04:44,729
more scope then for looking at that the

00:04:42,569 --> 00:04:46,650
library can unroll a loop and the

00:04:44,729 --> 00:04:50,759
compiler can then reorder things as an

00:04:46,650 --> 00:04:53,069
optimization and know it gives it much

00:04:50,759 --> 00:04:55,380
scope for running faster but you can't

00:04:53,069 --> 00:04:59,430
rely on each individual function having

00:04:55,380 --> 00:05:02,310
completed so yes you say no thread one

00:04:59,430 --> 00:05:05,009
is running elements 5 1 the 17 3 million

00:05:02,310 --> 00:05:07,169
and 23 but it's all mash them up

00:05:05,009 --> 00:05:08,159
together so you can't say which order it

00:05:07,169 --> 00:05:09,990
did them in because it did them all

00:05:08,159 --> 00:05:11,430
together it started all of them then it

00:05:09,990 --> 00:05:14,490
did a bit more and then it finished them

00:05:11,430 --> 00:05:18,560
all off and this has further

00:05:14,490 --> 00:05:20,819
consequences and then of course your

00:05:18,560 --> 00:05:22,380
your runtime library might supply you

00:05:20,819 --> 00:05:25,979
with some additional implementation

00:05:22,380 --> 00:05:27,599
defined policies anyone who went to the

00:05:25,979 --> 00:05:29,039
talk by the code play guys this morning

00:05:27,599 --> 00:05:30,870
will have seen they have an

00:05:29,039 --> 00:05:32,190
implementation defined policy for how to

00:05:30,870 --> 00:05:39,980
ship stuff on to the GPU

00:05:32,190 --> 00:05:41,930
I said

00:05:39,980 --> 00:05:44,150
a lot most of the pot most of the

00:05:41,930 --> 00:05:46,970
algorithms are there it really is

00:05:44,150 --> 00:05:48,410
there's over 100 I've put in bold if

00:05:46,970 --> 00:05:51,490
anyone can read them there's a few key

00:05:48,410 --> 00:05:56,950
ones they're copy count for each reduce

00:05:51,490 --> 00:05:56,950
transform transform reduce know sort

00:05:57,970 --> 00:06:03,680
most of them there so a few things

00:06:00,860 --> 00:06:05,180
aren't accumulate isn't that because the

00:06:03,680 --> 00:06:09,170
specification of accumulate is

00:06:05,180 --> 00:06:12,560
inherently in order have to do do each

00:06:09,170 --> 00:06:13,250
element having done the thing on the

00:06:12,560 --> 00:06:15,260
previous ones

00:06:13,250 --> 00:06:17,600
however the parallel equivalent is

00:06:15,260 --> 00:06:22,070
reduce reduces and out of order

00:06:17,600 --> 00:06:23,900
generalized sum and so if you can cope

00:06:22,070 --> 00:06:24,830
with things being out of order which if

00:06:23,900 --> 00:06:26,180
you're dealing with floating-point

00:06:24,830 --> 00:06:29,540
numbers then you might get a different

00:06:26,180 --> 00:06:31,430
answer because the the precision will

00:06:29,540 --> 00:06:32,600
change things know if you are if you

00:06:31,430 --> 00:06:34,610
have a load of small numbers and then

00:06:32,600 --> 00:06:37,670
one big one then it can depend which

00:06:34,610 --> 00:06:39,470
order you add things up if you use

00:06:37,670 --> 00:06:41,750
reduce you can cut you need to cope with

00:06:39,470 --> 00:06:44,750
that accumulate it has a specified order

00:06:41,750 --> 00:06:47,630
but no that's for you to decide if

00:06:44,750 --> 00:06:53,090
you're trying to paralyze things the

00:06:47,630 --> 00:06:55,970
option is there so yeah for the most

00:06:53,090 --> 00:06:57,320
part it's easy to paralyze your code if

00:06:55,970 --> 00:07:00,500
you works with a standard library

00:06:57,320 --> 00:07:05,440
algorithm you just have an execution

00:07:00,500 --> 00:07:08,540
policy of course you need to ensure that

00:07:05,440 --> 00:07:10,370
there is thread safety because the

00:07:08,540 --> 00:07:12,470
library doesn't know what your function

00:07:10,370 --> 00:07:18,740
does so it's going to make some

00:07:12,470 --> 00:07:20,330
assumptions so obviously if you if

00:07:18,740 --> 00:07:24,920
you're running things as sequential

00:07:20,330 --> 00:07:26,480
policy then it's sequential there's no

00:07:24,920 --> 00:07:28,790
additional thread safety requirements

00:07:26,480 --> 00:07:30,740
because it is all run on the same thread

00:07:28,790 --> 00:07:35,600
that you actually called you algorithm

00:07:30,740 --> 00:07:39,370
from if you specify the parallel

00:07:35,600 --> 00:07:44,570
execution policy execution power then

00:07:39,370 --> 00:07:48,470
it's up to you to ensure that if the

00:07:44,570 --> 00:07:51,770
library runs you use the supplied code

00:07:48,470 --> 00:07:53,120
so now with a for each example then the

00:07:51,770 --> 00:07:53,449
function that you've supplied if your

00:07:53,120 --> 00:07:58,099
drinks

00:07:53,449 --> 00:07:59,300
then a comparison or a swap know these

00:07:58,099 --> 00:08:02,479
things

00:07:59,300 --> 00:08:05,240
provided that the elements being worked

00:08:02,479 --> 00:08:09,860
on in different threads are in different

00:08:05,240 --> 00:08:11,749
elements then each know those

00:08:09,860 --> 00:08:13,339
implications should be okay it's up to

00:08:11,749 --> 00:08:14,300
you to ensure they're okay so this is

00:08:13,339 --> 00:08:16,999
just like you know if you've got an

00:08:14,300 --> 00:08:18,979
integer no two threads each working on

00:08:16,999 --> 00:08:20,569
separate integers fine two threads

00:08:18,979 --> 00:08:23,150
working on the same integer that's

00:08:20,569 --> 00:08:26,360
potentially a problem it can be a beta

00:08:23,150 --> 00:08:30,020
race an undefined behavior or and so

00:08:26,360 --> 00:08:32,149
this is a just a general no rule that so

00:08:30,020 --> 00:08:34,070
for the most part you don't need to do

00:08:32,149 --> 00:08:36,860
anything to do to deal with this

00:08:34,070 --> 00:08:38,120
constraint it's only if your code does

00:08:36,860 --> 00:08:40,490
anything fancy in terms of

00:08:38,120 --> 00:08:45,889
synchronization itself that you need to

00:08:40,490 --> 00:08:49,399
worry if you using the parallel on

00:08:45,889 --> 00:08:50,870
sequence policy not only does your do

00:08:49,399 --> 00:08:53,470
your operations need to be thread safe

00:08:50,870 --> 00:08:55,820
but they must not use any

00:08:53,470 --> 00:09:00,079
synchronization mechanisms so no new

00:08:55,820 --> 00:09:02,779
taxes no Atomics nothing that imposes a

00:09:00,079 --> 00:09:04,880
synchronization overhead because not

00:09:02,779 --> 00:09:06,199
only might they be interleaved which

00:09:04,880 --> 00:09:07,130
obviously is you've got two things and

00:09:06,199 --> 00:09:08,779
they're both trying to acquire the same

00:09:07,130 --> 00:09:10,550
mutex and they're both on the same

00:09:08,779 --> 00:09:11,660
thread then and it's trying to

00:09:10,550 --> 00:09:15,139
interleave them and they you can

00:09:11,660 --> 00:09:17,209
deadlock your own thread but also the

00:09:15,139 --> 00:09:19,819
whole concept of which thread they

00:09:17,209 --> 00:09:22,010
running on might be somewhat know

00:09:19,819 --> 00:09:23,600
disjoint and they might actually in some

00:09:22,010 --> 00:09:25,370
look as if they're migrating across

00:09:23,600 --> 00:09:26,870
threads now particularly if things have

00:09:25,370 --> 00:09:29,899
been moved to the GPU which doesn't have

00:09:26,870 --> 00:09:33,380
the same concept of thread as the host

00:09:29,899 --> 00:09:36,019
CPU does then the mapping might be

00:09:33,380 --> 00:09:37,760
somewhat arbitrary and you can't

00:09:36,019 --> 00:09:39,079
therefore rely on them being on the same

00:09:37,760 --> 00:09:41,870
thread so synchronization is just

00:09:39,079 --> 00:09:42,920
completely out but again and for lots of

00:09:41,870 --> 00:09:43,970
things it doesn't matter if you're

00:09:42,920 --> 00:09:45,139
trying to add five to a bunch of

00:09:43,970 --> 00:09:47,740
integers then you don't care which

00:09:45,139 --> 00:09:47,740
thread it's running on

00:09:51,690 --> 00:09:56,170
and the other thing that that really

00:09:54,700 --> 00:09:57,760
changes with these execution and

00:09:56,170 --> 00:10:00,850
parallel execution policies is their

00:09:57,760 --> 00:10:04,720
behavior with exceptions if you're doing

00:10:00,850 --> 00:10:06,220
a normal for each and your function for

00:10:04,720 --> 00:10:09,580
the millionth entry throws an exception

00:10:06,220 --> 00:10:11,230
that's fine it will just not process any

00:10:09,580 --> 00:10:14,800
more entries and propagate the exception

00:10:11,230 --> 00:10:19,090
out and you can catch it in your code if

00:10:14,800 --> 00:10:22,360
you're running this parallel and you one

00:10:19,090 --> 00:10:24,370
of the instances throws an exception it

00:10:22,360 --> 00:10:26,680
has no means of doing that because what

00:10:24,370 --> 00:10:28,510
no say its spawn it's you're running on

00:10:26,680 --> 00:10:31,660
a computer cluster you've got a thousand

00:10:28,510 --> 00:10:32,800
cause and the operations running on each

00:10:31,660 --> 00:10:33,220
of those thousand cause throws an

00:10:32,800 --> 00:10:34,750
exception

00:10:33,220 --> 00:10:37,990
you can't propagate a thousand

00:10:34,750 --> 00:10:41,680
exceptions out so in the original

00:10:37,990 --> 00:10:43,840
proposal for before C++ 17 had a sort of

00:10:41,680 --> 00:10:45,520
aggregate exception list of all the

00:10:43,840 --> 00:10:47,230
sections that got thrown that all get

00:10:45,520 --> 00:10:48,850
gathered together and the committee

00:10:47,230 --> 00:10:51,190
decided no no no no we don't like that

00:10:48,850 --> 00:10:53,740
if you're good if you're going to make

00:10:51,190 --> 00:10:55,210
this parallel don't throw exceptions so

00:10:53,740 --> 00:10:56,770
if you throw an exception from a

00:10:55,210 --> 00:11:00,810
parallel algorithm even if you choose

00:10:56,770 --> 00:11:03,520
the sequential policy it will terminate

00:11:00,810 --> 00:11:05,340
obviously if the implementation provides

00:11:03,520 --> 00:11:07,870
an additional policy it's up to them

00:11:05,340 --> 00:11:14,410
because it's only the standard policies

00:11:07,870 --> 00:11:16,990
that's they're forced this so yes so for

00:11:14,410 --> 00:11:21,190
lots of code you can just add execution

00:11:16,990 --> 00:11:25,330
par as the first parameter and it will

00:11:21,190 --> 00:11:28,660
paralyze your code but the thing is this

00:11:25,330 --> 00:11:31,300
is an optimization and like all

00:11:28,660 --> 00:11:32,590
optimizations you need to profile to

00:11:31,300 --> 00:11:34,480
make sure that it's worth it

00:11:32,590 --> 00:11:35,980
there is overhead to parallelizing stuff

00:11:34,480 --> 00:11:37,300
it's got a synchronized between the

00:11:35,980 --> 00:11:40,030
different threads it's got to divide up

00:11:37,300 --> 00:11:41,110
the work in the first place again for

00:11:40,030 --> 00:11:42,850
those people that were at the code play

00:11:41,110 --> 00:11:45,400
demonstration this morning they actually

00:11:42,850 --> 00:11:48,430
demonstrated in real time what that is

00:11:45,400 --> 00:11:50,200
no they showed they powerless to allow a

00:11:48,430 --> 00:11:51,730
stuff on the GPU with the smallest size

00:11:50,200 --> 00:11:53,830
they had in their demonstration it was

00:11:51,730 --> 00:11:55,030
slower but by the time they got to the

00:11:53,830 --> 00:11:57,640
biggest size in their demonstration it

00:11:55,030 --> 00:12:00,220
was five times faster and this was just

00:11:57,640 --> 00:12:01,670
on the built-in GPU on no on on tip on

00:12:00,220 --> 00:12:04,490
the CPU that no

00:12:01,670 --> 00:12:06,700
in most of the computers that you bought

00:12:04,490 --> 00:12:10,250
you buy these days have an on-chip GPU

00:12:06,700 --> 00:12:12,440
so there is potential for optimization

00:12:10,250 --> 00:12:14,780
there but you've got to make sure that

00:12:12,440 --> 00:12:16,310
your problem size is big enough now if

00:12:14,780 --> 00:12:18,680
you try and sort 10 elements there's no

00:12:16,310 --> 00:12:20,660
point trying to make it parallel but

00:12:18,680 --> 00:12:21,770
actually there might not be any point

00:12:20,660 --> 00:12:28,990
even if you've got a hundred thousand

00:12:21,770 --> 00:12:35,450
elements it's worth checking so yeah so

00:12:28,990 --> 00:12:38,390
coyotes fundamentally a CO routine is an

00:12:35,450 --> 00:12:40,490
extension of a function the function

00:12:38,390 --> 00:12:42,860
might be suspended mid execution and

00:12:40,490 --> 00:12:47,090
then resumed at a later state later time

00:12:42,860 --> 00:12:48,380
and it will resume directly where it

00:12:47,090 --> 00:12:49,970
left off in the middle with all the

00:12:48,380 --> 00:12:51,140
local variables having exactly the same

00:12:49,970 --> 00:12:56,360
values that they were when it got

00:12:51,140 --> 00:12:57,710
suspended and this includes any function

00:12:56,360 --> 00:12:59,540
parameters that will pass by value

00:12:57,710 --> 00:13:01,430
obviously their reference then the

00:12:59,540 --> 00:13:04,460
reference is still got saying refers to

00:13:01,430 --> 00:13:09,830
the same thing but the refer to thing

00:13:04,460 --> 00:13:12,830
might have changed but no does mean that

00:13:09,830 --> 00:13:14,360
then when you resume then it really

00:13:12,830 --> 00:13:15,500
doesn't just resume as if it was

00:13:14,360 --> 00:13:17,600
sequential code from the point of view

00:13:15,500 --> 00:13:23,900
of in the co routine it everything just

00:13:17,600 --> 00:13:26,150
carries on conceptually there are two

00:13:23,900 --> 00:13:27,710
types of co-routines stack full

00:13:26,150 --> 00:13:30,620
co-routines that store the entire

00:13:27,710 --> 00:13:33,710
execution stack and stack Lascaux

00:13:30,620 --> 00:13:36,230
routines the TS is about stack Lascaux

00:13:33,710 --> 00:13:37,850
routines so it's only the local function

00:13:36,230 --> 00:13:40,060
that is the co routine itself they get

00:13:37,850 --> 00:13:40,060
saved

00:13:44,590 --> 00:13:49,510
this does have a strong advantage it

00:13:47,530 --> 00:13:51,370
means everything is localized it means

00:13:49,510 --> 00:13:53,590
that there is minimal memory allocation

00:13:51,370 --> 00:13:55,120
and so you can have millions of

00:13:53,590 --> 00:13:56,530
in-flight care routines if you try to

00:13:55,120 --> 00:13:58,330
have millions of in flight Chloe teens

00:13:56,530 --> 00:14:00,010
with stack full co-routines

00:13:58,330 --> 00:14:02,620
that means you've got millions of full

00:14:00,010 --> 00:14:05,050
execution snacks and unless your

00:14:02,620 --> 00:14:10,690
execution stack is tiny that's going to

00:14:05,050 --> 00:14:13,480
require a heck of a lot of RAM and it

00:14:10,690 --> 00:14:16,450
does mean that the compiler can look at

00:14:13,480 --> 00:14:18,610
the function and the whole everything

00:14:16,450 --> 00:14:20,440
that it needs to worry about about this

00:14:18,610 --> 00:14:21,730
co-routine is just there it doesn't need

00:14:20,440 --> 00:14:23,080
to care what the rest of the stack does

00:14:21,730 --> 00:14:26,710
how it was called where it comes from

00:14:23,080 --> 00:14:28,750
and so it can then make optimizations it

00:14:26,710 --> 00:14:30,610
can eliminate that memory allocation if

00:14:28,750 --> 00:14:33,400
you're not doing anything that requires

00:14:30,610 --> 00:14:35,260
it to keep it around Gore's done

00:14:33,400 --> 00:14:36,820
presentations he calls this the magic

00:14:35,260 --> 00:14:40,540
disappearing care routines where all the

00:14:36,820 --> 00:14:42,490
overhead disappears no so no there is

00:14:40,540 --> 00:14:43,900
lots of potential for the compiler to

00:14:42,490 --> 00:14:48,850
optimize your co-routines because we're

00:14:43,900 --> 00:14:51,460
using stackless ones there are

00:14:48,850 --> 00:14:54,070
disadvantages Amin doesn't mean you can

00:14:51,460 --> 00:14:57,310
only suspend the current co-routine you

00:14:54,070 --> 00:15:00,760
can't suspend the entire stack which no

00:14:57,310 --> 00:15:03,160
when use Co await to suspend your Co

00:15:00,760 --> 00:15:04,480
routine it means that only the current

00:15:03,160 --> 00:15:08,770
function suspends and the call is

00:15:04,480 --> 00:15:10,780
mmediately returned to the caller which

00:15:08,770 --> 00:15:11,980
can have no knock-on consequences about

00:15:10,780 --> 00:15:14,440
the way that you structure your code so

00:15:11,980 --> 00:15:16,930
you need to be aware of that is in some

00:15:14,440 --> 00:15:18,700
ways some cases it's not what you would

00:15:16,930 --> 00:15:21,430
ideally have chosen but there are ways

00:15:18,700 --> 00:15:24,420
around it and you can have to design

00:15:21,430 --> 00:15:24,420
your code with that in mind

00:15:28,860 --> 00:15:34,950
so in C++ what is occurring to look like

00:15:33,130 --> 00:15:37,990
it's just a normal function

00:15:34,950 --> 00:15:39,339
the crucial is it must use in its

00:15:37,990 --> 00:15:41,290
implementation in the body of the

00:15:39,339 --> 00:15:42,580
function directly they're not in some

00:15:41,290 --> 00:15:44,200
nested function call but directly

00:15:42,580 --> 00:15:46,089
they're in the body of the function use

00:15:44,200 --> 00:15:47,830
one of the three magic keywords they

00:15:46,089 --> 00:15:52,960
start with Co Co keywords and make

00:15:47,830 --> 00:15:55,510
co-routines it can be weight Co a weight

00:15:52,960 --> 00:15:59,020
which will conceptually wait for

00:15:55,510 --> 00:16:00,910
something Co yield which conceptually

00:15:59,020 --> 00:16:01,240
returns an intermediate value to the

00:16:00,910 --> 00:16:03,400
caller

00:16:01,240 --> 00:16:06,700
but then allows your Co routine to be

00:16:03,400 --> 00:16:09,010
resumed and Co return which is and I'm

00:16:06,700 --> 00:16:12,279
done if you resume me then bad things

00:16:09,010 --> 00:16:15,460
will happen now a final final result

00:16:12,279 --> 00:16:17,290
coming out of your car routine and then

00:16:15,460 --> 00:16:20,040
you then the return type of the co

00:16:17,290 --> 00:16:22,870
routine must be something that is

00:16:20,040 --> 00:16:24,760
designed as a co routine return type and

00:16:22,870 --> 00:16:28,600
you must have a corresponding co-routine

00:16:24,760 --> 00:16:30,580
promise for that which is an internal

00:16:28,600 --> 00:16:33,190
data structure you specify by

00:16:30,580 --> 00:16:40,060
specializing the compilers co-routine

00:16:33,190 --> 00:16:40,959
traits so I mean Co return looks like a

00:16:40,060 --> 00:16:42,850
return statement

00:16:40,959 --> 00:16:47,650
okay return some value and that returns

00:16:42,850 --> 00:16:50,550
the final value Co await you pass in

00:16:47,650 --> 00:16:53,650
some away table thing and you're telling

00:16:50,550 --> 00:16:57,880
the compiler and library that you're

00:16:53,650 --> 00:17:01,480
going to wait on that thing to be ready

00:16:57,880 --> 00:17:03,160
and if the way to bully is not ready

00:17:01,480 --> 00:17:05,319
then you're going to suspend the current

00:17:03,160 --> 00:17:07,209
Co routine and return to the caller who

00:17:05,319 --> 00:17:08,860
will then hopefully do something that

00:17:07,209 --> 00:17:10,929
then and then your Cobra team will

00:17:08,860 --> 00:17:17,829
resume at a later point back right

00:17:10,929 --> 00:17:20,410
they're off to be awake all Co yield is

00:17:17,829 --> 00:17:22,959
similar but in the intent is that you

00:17:20,410 --> 00:17:24,910
yield with a value and so that value

00:17:22,959 --> 00:17:28,030
then gets somehow propagated out to your

00:17:24,910 --> 00:17:30,070
caller within with the provider you'll

00:17:28,030 --> 00:17:31,600
expect your carotene is now expecting to

00:17:30,070 --> 00:17:33,190
resume on the next line so this is

00:17:31,600 --> 00:17:36,730
different from a return it's expecting

00:17:33,190 --> 00:17:38,320
to resume and it will then carry on

00:17:36,730 --> 00:17:48,399
straight away from the net from

00:17:38,320 --> 00:17:51,700
from the following line now I mentioned

00:17:48,399 --> 00:17:55,659
carotene promise types a promise type is

00:17:51,700 --> 00:18:00,190
how you specify how the co-routine

00:17:55,659 --> 00:18:08,220
internals get that correspond to the

00:18:00,190 --> 00:18:11,289
return value it specifies what the

00:18:08,220 --> 00:18:13,990
compiler and library will do when you

00:18:11,289 --> 00:18:16,870
put cook in at those Co return Co awaits

00:18:13,990 --> 00:18:20,889
and Co yield no locations inside your

00:18:16,870 --> 00:18:23,169
coyote how it deals with suspension what

00:18:20,889 --> 00:18:28,120
happens how all the return value gets

00:18:23,169 --> 00:18:30,610
handled and then they also have this

00:18:28,120 --> 00:18:32,559
concept of runaway table the away table

00:18:30,610 --> 00:18:39,519
is something that you can wait for with

00:18:32,559 --> 00:18:41,740
kill await and often they will be

00:18:39,519 --> 00:18:43,090
related types so you might have

00:18:41,740 --> 00:18:45,190
something that is both in the weight of

00:18:43,090 --> 00:18:48,580
all you can wait for it we with no koa

00:18:45,190 --> 00:18:50,799
weight and has a corresponding

00:18:48,580 --> 00:18:54,490
co-routine promise type so that you can

00:18:50,799 --> 00:18:56,259
use that type as a return value form

00:18:54,490 --> 00:19:01,179
your co-routines but they're not

00:18:56,259 --> 00:19:03,460
necessarily the same so as an example of

00:19:01,179 --> 00:19:07,000
something that is going to be the same

00:19:03,460 --> 00:19:10,200
no under or you might expect to be the

00:19:07,000 --> 00:19:14,980
same it's something like a future from

00:19:10,200 --> 00:19:17,620
the code either from the C++ standard

00:19:14,980 --> 00:19:20,860
directly or the co-routines they

00:19:17,620 --> 00:19:23,679
concurrent CTS type futures then

00:19:20,860 --> 00:19:25,299
obviously a future is a value that is a

00:19:23,679 --> 00:19:27,850
placeholder for a value that's going to

00:19:25,299 --> 00:19:29,769
be available at some point and so you

00:19:27,850 --> 00:19:33,190
might have some function that's going to

00:19:29,769 --> 00:19:35,379
give you that future and some value and

00:19:33,190 --> 00:19:37,929
so then inside your Co routine you can

00:19:35,379 --> 00:19:40,539
Co await on a function that's going to

00:19:37,929 --> 00:19:42,070
generate your value and it will suspend

00:19:40,539 --> 00:19:45,070
your Co routine until that value is

00:19:42,070 --> 00:19:49,940
ready but then you could also return

00:19:45,070 --> 00:19:54,270
from a co routine a future and so then

00:19:49,940 --> 00:19:55,740
when you do then it will wake up

00:19:54,270 --> 00:19:57,210
anything that's waiting on it now if

00:19:55,740 --> 00:19:59,190
these are just standard futures or

00:19:57,210 --> 00:20:00,330
standard experimental futures then you

00:19:59,190 --> 00:20:03,960
can then use them with all the rest of

00:20:00,330 --> 00:20:05,400
your code and if it's if your code if it

00:20:03,960 --> 00:20:07,620
was a Co routine that returned that

00:20:05,400 --> 00:20:08,700
future and the co routine got suspended

00:20:07,620 --> 00:20:10,460
because the future wasn't ready then

00:20:08,700 --> 00:20:14,370
when you wait for the future then it

00:20:10,460 --> 00:20:16,440
will know check depending on how the

00:20:14,370 --> 00:20:18,750
library's been implemented how to and

00:20:16,440 --> 00:20:20,309
make sure that the Co routine gets gets

00:20:18,750 --> 00:20:22,620
resumed and when the co routine is reka

00:20:20,309 --> 00:20:24,480
finally finished and the data has been

00:20:22,620 --> 00:20:28,830
returned with that co route Co return

00:20:24,480 --> 00:20:31,500
call then the future will become ready

00:20:28,830 --> 00:20:33,510
and over the code that's waiting for it

00:20:31,500 --> 00:20:37,470
will finally return so this is provides

00:20:33,510 --> 00:20:39,600
a means of meshing co-routines with code

00:20:37,470 --> 00:20:46,110
that uses futures no light from standard

00:20:39,600 --> 00:20:48,450
async or from a standard promise whether

00:20:46,110 --> 00:20:50,220
or not your library provides that the

00:20:48,450 --> 00:20:52,830
standard load B provides that

00:20:50,220 --> 00:20:55,650
integration varies I think the visual

00:20:52,830 --> 00:20:59,190
geo 2017 implementation does provide

00:20:55,650 --> 00:21:01,500
that for standard future but the clang 5

00:20:59,190 --> 00:21:06,690
version doesn't yet if I remember

00:21:01,500 --> 00:21:08,130
rightly but they might then add it you

00:21:06,690 --> 00:21:12,120
could also write your own future like

00:21:08,130 --> 00:21:14,850
type no that was no my lib future

00:21:12,120 --> 00:21:16,590
instead of standard future and then you

00:21:14,850 --> 00:21:19,220
could implement mat yourself it's not

00:21:16,590 --> 00:21:19,220
particularly complicated

00:21:22,809 --> 00:21:29,029
now stackless co-routines worked nicely

00:21:26,239 --> 00:21:30,710
if everything down your stack is

00:21:29,029 --> 00:21:33,499
encouraging because actually then you

00:21:30,710 --> 00:21:35,119
can say koa wait on something that I'm

00:21:33,499 --> 00:21:37,970
waiting for and it returns up to the

00:21:35,119 --> 00:21:39,649
caller and the caller well immediately

00:21:37,970 --> 00:21:41,090
it doesn't care about what result you're

00:21:39,649 --> 00:21:43,940
returning so you've got some form of

00:21:41,090 --> 00:21:45,289
future value coming through and then you

00:21:43,940 --> 00:21:46,519
do some more code and then that point it

00:21:45,289 --> 00:21:47,779
will say oh well actually now I do need

00:21:46,519 --> 00:21:50,840
the value so I'm going to Co await on

00:21:47,779 --> 00:21:53,059
that future that I got back and suspend

00:21:50,840 --> 00:21:54,789
that care routine which obviously they

00:21:53,059 --> 00:21:58,519
it's nice to propagate all the way up

00:21:54,789 --> 00:22:00,379
and actually you could then so if you're

00:21:58,519 --> 00:22:01,999
doing that and you want to work with

00:22:00,379 --> 00:22:03,889
parallel algorithms then you could

00:22:01,999 --> 00:22:07,190
actually the implementation could

00:22:03,889 --> 00:22:09,679
provide a custom execution policy for

00:22:07,190 --> 00:22:11,239
your parallel algorithms that makes your

00:22:09,679 --> 00:22:15,409
parallel algorithm work as a KO routine

00:22:11,239 --> 00:22:17,989
so you could have no standards for each

00:22:15,409 --> 00:22:21,109
parallelize co-routine from begin to end

00:22:17,989 --> 00:22:23,690
do stuff and then you can then Co await

00:22:21,109 --> 00:22:26,690
on that returned future that you get

00:22:23,690 --> 00:22:32,119
from this invocation of parallel for

00:22:26,690 --> 00:22:33,769
each so that's the first first way that

00:22:32,119 --> 00:22:36,499
we could potentially integrate

00:22:33,769 --> 00:22:39,340
co-routines and parallelism is because

00:22:36,499 --> 00:22:45,039
by having this custom execution policy

00:22:39,340 --> 00:22:45,039
that allows us directly to mesh the two

00:22:49,129 --> 00:22:54,830
okay so we'll leave co-routines aside

00:22:52,710 --> 00:22:59,309
for a second and we'll have a look at

00:22:54,830 --> 00:23:02,789
our the concurrent CTS which is no

00:22:59,309 --> 00:23:04,049
extensions to futures it's probably the

00:23:02,789 --> 00:23:05,820
core bit and so that's what I'm

00:23:04,049 --> 00:23:06,690
primarily going to focus on here there's

00:23:05,820 --> 00:23:09,330
also other things

00:23:06,690 --> 00:23:14,700
there's also latches and barriers which

00:23:09,330 --> 00:23:16,919
are synchronization objects for events

00:23:14,700 --> 00:23:18,869
across multiple threads waiting for five

00:23:16,919 --> 00:23:21,210
five events to happen across a load of

00:23:18,869 --> 00:23:22,320
threads you have a latch and then when

00:23:21,210 --> 00:23:27,210
all the events have happened that latch

00:23:22,320 --> 00:23:29,580
becomes ready barriers are similar but

00:23:27,210 --> 00:23:31,349
they're reusable they are for when

00:23:29,580 --> 00:23:33,840
you've got a bunch of threads that are

00:23:31,349 --> 00:23:35,580
working together and they all need to be

00:23:33,840 --> 00:23:37,739
in lockstep in some fashion so they all

00:23:35,580 --> 00:23:39,779
do their first chunk of code work and

00:23:37,739 --> 00:23:41,759
then they hit the end and they all wait

00:23:39,779 --> 00:23:43,169
for the barrier and then they loop

00:23:41,759 --> 00:23:45,869
around again they do the next chunk of

00:23:43,169 --> 00:23:47,759
work know each thread doing its own

00:23:45,869 --> 00:23:50,789
relevant parts and then they will wait

00:23:47,759 --> 00:23:52,320
at the barrier again and so forth now

00:23:50,789 --> 00:23:53,580
the latches and barriers are actually

00:23:52,320 --> 00:23:56,369
now being targeted

00:23:53,580 --> 00:23:58,859
hopefully for C++ 20 no they've been

00:23:56,369 --> 00:24:00,809
pulled out with the co-routines TS and

00:23:58,859 --> 00:24:02,999
the and the committee has said yeah we

00:24:00,809 --> 00:24:04,470
like these pretty much as is there might

00:24:02,999 --> 00:24:08,519
be a few minor tweaks so they're going

00:24:04,470 --> 00:24:11,549
ahead with C++ 20 we've also got atomic

00:24:08,519 --> 00:24:15,899
smart pointers so atomic shared pointer

00:24:11,549 --> 00:24:20,999
an atomic weak pointer these allow a

00:24:15,899 --> 00:24:23,539
means of no okay so see standard C++

00:24:20,999 --> 00:24:26,549
shared pointer that we all know and love

00:24:23,539 --> 00:24:28,529
it works across multiple threads in the

00:24:26,549 --> 00:24:30,059
sense that you can have each thread can

00:24:28,529 --> 00:24:31,080
have its own shared pointer instance and

00:24:30,059 --> 00:24:33,210
they can all be referencing the same

00:24:31,080 --> 00:24:35,940
object and the reference count all works

00:24:33,210 --> 00:24:37,379
nicely now what we get from atomic

00:24:35,940 --> 00:24:39,509
shared pointer is that we can have a

00:24:37,379 --> 00:24:41,479
single atomic shared pointer instance

00:24:39,509 --> 00:24:46,739
which is reference from multiple threads

00:24:41,479 --> 00:24:48,059
possibly because it's like the a pointer

00:24:46,739 --> 00:24:49,320
to a master data structure and so

00:24:48,059 --> 00:24:51,299
everyone has to come and say can I have

00:24:49,320 --> 00:24:52,470
that have the pointer please but then

00:24:51,299 --> 00:24:54,509
somebody might want to come and update

00:24:52,470 --> 00:24:57,899
it and so rather than having a mutex

00:24:54,509 --> 00:25:00,239
lock round for protecting that master

00:24:57,899 --> 00:25:01,770
pointer then you can just make it an

00:25:00,239 --> 00:25:04,500
atomic shared pointer

00:25:01,770 --> 00:25:07,140
then that each thread can do that and it

00:25:04,500 --> 00:25:11,250
is inherently safe and that's also being

00:25:07,140 --> 00:25:13,290
targeted for C++ 20 now and that was the

00:25:11,250 --> 00:25:15,510
spelling has changed in the TS it was

00:25:13,290 --> 00:25:18,930
atomic underscore shared underscore

00:25:15,510 --> 00:25:21,980
pointer of T and now it is atomic angle

00:25:18,930 --> 00:25:24,690
brackets shared pointer of T but hey

00:25:21,980 --> 00:25:27,090
what's a underscore to an angle brackets

00:25:24,690 --> 00:25:28,350
between frames anyway so I'm not gonna

00:25:27,090 --> 00:25:31,020
talk anymore about latches and barriers

00:25:28,350 --> 00:25:32,310
and Atomics smart pointers if you want

00:25:31,020 --> 00:25:33,960
to talk more about them then come catch

00:25:32,310 --> 00:25:36,120
me later but for now I'm just going to

00:25:33,960 --> 00:25:40,560
focus on the futures part because that

00:25:36,120 --> 00:25:50,730
is what is relevant to parallelism and

00:25:40,560 --> 00:25:54,870
co-routines so a continuation is a new

00:25:50,730 --> 00:25:57,510
task that you want to run when the

00:25:54,870 --> 00:25:59,040
future is ready so you've got a future

00:25:57,510 --> 00:26:01,530
which is some task that's going to

00:25:59,040 --> 00:26:03,240
generate some value for you and you say

00:26:01,530 --> 00:26:06,110
well when that's ready then I want to do

00:26:03,240 --> 00:26:09,960
another calculation with that value and

00:26:06,110 --> 00:26:12,990
so you can either wait for a future and

00:26:09,960 --> 00:26:15,390
then spawn the task possibly earn as a

00:26:12,990 --> 00:26:16,710
new way synchronous tasks now using

00:26:15,390 --> 00:26:21,120
standard a sink or a thread pool or

00:26:16,710 --> 00:26:23,010
something but alternatively with the

00:26:21,120 --> 00:26:26,850
concurrency TS we now have this new

00:26:23,010 --> 00:26:32,010
member function on the future then so

00:26:26,850 --> 00:26:36,780
you can say when the task when the value

00:26:32,010 --> 00:26:38,970
is ready then do this and the library

00:26:36,780 --> 00:26:41,430
will take care of automatically

00:26:38,970 --> 00:26:43,740
scheduling your new tasks when that

00:26:41,430 --> 00:26:46,050
value is ready and you don't and you can

00:26:43,740 --> 00:26:48,210
specify all this upfront as soon as

00:26:46,050 --> 00:26:51,110
you've launched off the first task you

00:26:48,210 --> 00:26:53,700
can specify what to do next

00:26:51,110 --> 00:27:00,990
without having to wait for it to be

00:26:53,700 --> 00:27:03,300
ready now obviously futures as in

00:27:00,990 --> 00:27:04,920
directly no standard future and

00:27:03,300 --> 00:27:08,310
therefore standard experimental future

00:27:04,920 --> 00:27:12,660
which is based on the same are one-shot

00:27:08,310 --> 00:27:14,130
things no you get so you can only use it

00:27:12,660 --> 00:27:14,620
once you can only get the value out and

00:27:14,130 --> 00:27:17,680
once

00:27:14,620 --> 00:27:19,090
so if you specify a continuation then

00:27:17,680 --> 00:27:20,610
you can't get the value out in the other

00:27:19,090 --> 00:27:24,760
way

00:27:20,610 --> 00:27:28,330
that value must and now can only be

00:27:24,760 --> 00:27:32,380
retrieved by the continuation and so the

00:27:28,330 --> 00:27:34,860
source is no longer valid and secondly

00:27:32,380 --> 00:27:38,590
the continuation itself will get a

00:27:34,860 --> 00:27:40,780
future as the parameter because the

00:27:38,590 --> 00:27:43,930
future might contain either a value or

00:27:40,780 --> 00:27:46,840
an exception and so rather than having

00:27:43,930 --> 00:27:48,970
to have different continuations for the

00:27:46,840 --> 00:27:50,470
different options no well there's one

00:27:48,970 --> 00:27:51,820
function that we get called if we've got

00:27:50,470 --> 00:27:55,150
a value and another one that gets called

00:27:51,820 --> 00:27:57,190
if you've got an exception then we

00:27:55,150 --> 00:27:58,990
already have this packaged up thing of a

00:27:57,190 --> 00:28:01,120
future value in an exception it's the

00:27:58,990 --> 00:28:03,309
future so that future itself gets passed

00:28:01,120 --> 00:28:07,030
in to the continuation when that future

00:28:03,309 --> 00:28:08,440
is ready so the a continuation always

00:28:07,030 --> 00:28:09,670
receives a future but it always receives

00:28:08,440 --> 00:28:11,740
a future of these already ready if you

00:28:09,670 --> 00:28:13,570
try and wait it will not have to spend

00:28:11,740 --> 00:28:15,490
any time waiting because it's guaranteed

00:28:13,570 --> 00:28:19,630
to be ready when the continuation is

00:28:15,490 --> 00:28:20,890
invoked and it doesn't consequently mean

00:28:19,630 --> 00:28:25,000
of course that you can only have a one

00:28:20,890 --> 00:28:27,520
continuation her future though of course

00:28:25,000 --> 00:28:30,309
that continuation itself will return a

00:28:27,520 --> 00:28:33,240
new future so you can chain them up have

00:28:30,309 --> 00:28:37,570
a whole series but only one at a time

00:28:33,240 --> 00:28:38,710
so some look at some code we have some

00:28:37,570 --> 00:28:41,980
function that's going to find us the

00:28:38,710 --> 00:28:44,950
answer it's going to return us span the

00:28:41,980 --> 00:28:46,330
experimental future we have another

00:28:44,950 --> 00:28:49,090
function that's going to process that

00:28:46,330 --> 00:28:53,050
returned result and that takes a future

00:28:49,090 --> 00:28:54,940
as a parameter so we spawn our function

00:28:53,050 --> 00:28:57,940
we call find the answer it gives us a

00:28:54,940 --> 00:29:02,050
future back Auto F and then we chain on

00:28:57,940 --> 00:29:04,690
our continuation of process result by

00:29:02,050 --> 00:29:07,270
calling F dot then so f2 on this slide

00:29:04,690 --> 00:29:08,940
will be a experimental future for a

00:29:07,270 --> 00:29:12,929
string because the return result of

00:29:08,940 --> 00:29:12,929
process result is a string

00:29:17,539 --> 00:29:24,630
if the future contains an exception then

00:29:22,080 --> 00:29:26,399
the exception gets stored in the future

00:29:24,630 --> 00:29:29,279
that's passed to the continuation and

00:29:26,399 --> 00:29:34,409
like normal futures if you then call

00:29:29,279 --> 00:29:35,909
gates he will throw so the concurrence

00:29:34,409 --> 00:29:37,769
ETS has this nice function make

00:29:35,909 --> 00:29:39,299
exceptional future which directly stores

00:29:37,769 --> 00:29:41,880
the future in so you don't have to try

00:29:39,299 --> 00:29:43,830
and catch an and store a value directly

00:29:41,880 --> 00:29:45,480
or use promise not set exception or

00:29:43,830 --> 00:29:49,230
anything it you can just have a future

00:29:45,480 --> 00:29:50,909
that already contains an exception so we

00:29:49,230 --> 00:29:52,529
have we call fail and that's going to

00:29:50,909 --> 00:29:55,799
give us a future that holds an exception

00:29:52,529 --> 00:29:58,010
we put on the continuation with a call

00:29:55,799 --> 00:30:01,830
of N and passing in our next function

00:29:58,010 --> 00:30:04,169
and that will throw that know will then

00:30:01,830 --> 00:30:12,149
throw when it is called

00:30:04,169 --> 00:30:14,549
there by the runtime library and no so

00:30:12,149 --> 00:30:16,980
yes so the F the call to get inside Nate

00:30:14,549 --> 00:30:18,720
will throw which means that the content

00:30:16,980 --> 00:30:22,380
the future that is returned from our

00:30:18,720 --> 00:30:27,179
continuation that F in fear in foo will

00:30:22,380 --> 00:30:29,519
in hold an exception so when on the next

00:30:27,179 --> 00:30:31,860
line info we call F gate that will

00:30:29,519 --> 00:30:33,659
itself throw so it is the same no our

00:30:31,860 --> 00:30:35,820
runtime error that we created at the top

00:30:33,659 --> 00:30:38,250
is then propagated across the chain and

00:30:35,820 --> 00:30:40,740
then comes out the final with the final

00:30:38,250 --> 00:30:44,460
gate of course next could catch and

00:30:40,740 --> 00:30:46,080
handle that exception in which case then

00:30:44,460 --> 00:30:50,690
it wouldn't be propagated it would be

00:30:46,080 --> 00:30:53,100
kept caught and handled and then they

00:30:50,690 --> 00:30:55,169
called to get at the bottom there in

00:30:53,100 --> 00:30:57,720
food would then not be throwing an

00:30:55,169 --> 00:31:00,260
exception because no it was handled

00:30:57,720 --> 00:31:00,260
inside next

00:31:03,560 --> 00:31:07,040
if you just want the exception to

00:31:05,720 --> 00:31:11,510
propagate and you don't want to handle

00:31:07,040 --> 00:31:14,360
them and having a function for your

00:31:11,510 --> 00:31:18,470
continuation that takes a future as the

00:31:14,360 --> 00:31:20,180
result as the parameter is undesirable

00:31:18,470 --> 00:31:21,290
for your circumstance possibly because

00:31:20,180 --> 00:31:22,520
you already have a function that you

00:31:21,290 --> 00:31:24,230
want to use that doesn't take your

00:31:22,520 --> 00:31:28,010
future as a parameter then it's easy

00:31:24,230 --> 00:31:30,620
enough to wrap it in a lambda I've

00:31:28,010 --> 00:31:32,330
specified out the lambda function

00:31:30,620 --> 00:31:33,560
parameter there now is an experimental

00:31:32,330 --> 00:31:36,620
future but of course you could just like

00:31:33,560 --> 00:31:41,360
Auto okay our lambda that takes auto f

00:31:36,620 --> 00:31:42,890
and then returns the function a call

00:31:41,360 --> 00:31:44,780
from what we're actually wanting to and

00:31:42,890 --> 00:31:46,160
that f dot get will throw the exception

00:31:44,780 --> 00:31:47,930
that there was one which will then be

00:31:46,160 --> 00:31:50,330
propagated out on court so you don't

00:31:47,930 --> 00:31:56,360
have to worry about what happens on the

00:31:50,330 --> 00:31:58,460
exceptional case and if you don't like

00:31:56,360 --> 00:32:00,920
writing lambdas there then you can just

00:31:58,460 --> 00:32:03,200
write this unwrapped function which

00:32:00,920 --> 00:32:05,750
contains a lambda to deal with it for

00:32:03,200 --> 00:32:07,850
you know so then you can say unwrapped

00:32:05,750 --> 00:32:10,370
process result and that will unwrap the

00:32:07,850 --> 00:32:14,830
value and propagate the exceptions if

00:32:10,370 --> 00:32:17,600
there were any and then and call the the

00:32:14,830 --> 00:32:21,730
value that he got from the future they

00:32:17,600 --> 00:32:24,560
call called the supply continuation so

00:32:21,730 --> 00:32:26,240
that's there as a helper obviously it's

00:32:24,560 --> 00:32:28,040
not part of the TS you'd have to write

00:32:26,240 --> 00:32:30,980
it yourself now you can copy off this

00:32:28,040 --> 00:32:33,470
slide know that the PDF will be

00:32:30,980 --> 00:32:35,270
available from no cpp con at some point

00:32:33,470 --> 00:32:36,350
so that you can copy that out and call

00:32:35,270 --> 00:32:43,970
it what you like if you don't like the

00:32:36,350 --> 00:32:47,360
name unwrapped continuations also work

00:32:43,970 --> 00:32:51,560
this shared features but they have some

00:32:47,360 --> 00:32:53,540
changes now shared futures allow you to

00:32:51,560 --> 00:32:56,000
call get multiple times but you get a

00:32:53,540 --> 00:32:59,150
conference back and so likewise you can

00:32:56,000 --> 00:33:03,170
schedule multiple continuations with

00:32:59,150 --> 00:33:05,570
shared futures they can treat can't take

00:33:03,170 --> 00:33:08,210
a few a single us know a normal future

00:33:05,570 --> 00:33:10,340
as the parameter because otherwise no

00:33:08,210 --> 00:33:11,420
you might remove the value that was then

00:33:10,340 --> 00:33:13,070
trying to be propagated to all the

00:33:11,420 --> 00:33:16,280
others so they take a shared future as

00:33:13,070 --> 00:33:17,390
the value and the source teacher remains

00:33:16,280 --> 00:33:19,220
valid because

00:33:17,390 --> 00:33:21,799
hey there's no reason not to make it and

00:33:19,220 --> 00:33:23,630
so you can't contain chain multiple

00:33:21,799 --> 00:33:28,059
continuations on the same future either

00:33:23,630 --> 00:33:28,059
directly there or across all the copies

00:33:29,590 --> 00:33:34,750
so we have some code we've got some

00:33:32,600 --> 00:33:36,590
function to find the answer and to

00:33:34,750 --> 00:33:40,700
continuations we want to use this time

00:33:36,590 --> 00:33:42,320
makes one the next two so we call find

00:33:40,700 --> 00:33:44,750
the answers it's the future we share it

00:33:42,320 --> 00:33:47,630
to give us a shared future and then we

00:33:44,750 --> 00:33:49,820
can chain two continuations each of

00:33:47,630 --> 00:33:53,570
which will get the value from find the

00:33:49,820 --> 00:33:55,130
answer and of course they will have

00:33:53,570 --> 00:33:56,780
different types because the continuation

00:33:55,130 --> 00:33:58,610
functions return different things so f2

00:33:56,780 --> 00:34:01,730
will return an experimental future of

00:33:58,610 --> 00:34:04,700
void and f3 will be an experimental

00:34:01,730 --> 00:34:07,179
future event and that's just how you

00:34:04,700 --> 00:34:07,179
might expect

00:34:12,970 --> 00:34:18,070
alongside continuations we also have

00:34:15,429 --> 00:34:21,070
this concept of being able to wait for

00:34:18,070 --> 00:34:22,450
multiple futures because futures are all

00:34:21,070 --> 00:34:24,970
very well but if you've got a whole

00:34:22,450 --> 00:34:27,520
bunch of them then sometimes you want to

00:34:24,970 --> 00:34:29,649
say well when one of them is ready and I

00:34:27,520 --> 00:34:33,220
don't care which one then I want to do

00:34:29,649 --> 00:34:34,899
some stuff I want to process the first

00:34:33,220 --> 00:34:37,119
result that comes back now that might be

00:34:34,899 --> 00:34:39,159
because you're actually using you you've

00:34:37,119 --> 00:34:41,109
got a whole bunch of scores available

00:34:39,159 --> 00:34:43,889
and you're not doing anything meaningful

00:34:41,109 --> 00:34:46,060
with them so you can say well actually I

00:34:43,889 --> 00:34:47,649
don't know what the best way to

00:34:46,060 --> 00:34:49,359
calculate this result is so I'm going to

00:34:47,649 --> 00:34:50,710
launch off five threads that's going to

00:34:49,359 --> 00:34:53,290
each going to calculate the result in

00:34:50,710 --> 00:34:54,909
some different fashion and then when one

00:34:53,290 --> 00:34:56,200
of them comes back the first one that

00:34:54,909 --> 00:35:00,010
comes back with result I'm going to use

00:34:56,200 --> 00:35:03,940
that and so when any of them are ready

00:35:00,010 --> 00:35:05,410
then I'll do some processing and there's

00:35:03,940 --> 00:35:07,420
two ways of doing it you can either pass

00:35:05,410 --> 00:35:09,670
when any and pass the futures one at a

00:35:07,420 --> 00:35:11,710
time as prop function parameters or you

00:35:09,670 --> 00:35:13,480
can have some container that contains a

00:35:11,710 --> 00:35:15,359
load of futures and you can just pass

00:35:13,480 --> 00:35:17,890
the beginning and end of the container

00:35:15,359 --> 00:35:19,180
and then depending what you do you

00:35:17,890 --> 00:35:23,470
either get back a future that holds a

00:35:19,180 --> 00:35:26,020
tuple of the futures or you have comes

00:35:23,470 --> 00:35:29,380
back something that holds a vector of

00:35:26,020 --> 00:35:33,210
the containing the futures that you

00:35:29,380 --> 00:35:35,470
passed through and in the case this is I

00:35:33,210 --> 00:35:38,260
contains a that's stored inside the

00:35:35,470 --> 00:35:39,580
structure of a when any result which the

00:35:38,260 --> 00:35:41,470
key thing about that is that it tells

00:35:39,580 --> 00:35:44,320
you which one of these set of futures it

00:35:41,470 --> 00:35:45,700
was that was the one that woke you up so

00:35:44,320 --> 00:35:46,900
you don't have to go through and say are

00:35:45,700 --> 00:35:49,630
you ready are you ready are you ready

00:35:46,900 --> 00:35:51,640
I'll wait that one if you've got a whole

00:35:49,630 --> 00:35:53,560
set of 100 then that's a bit messy and

00:35:51,640 --> 00:35:57,070
so you actually just tells you it was

00:35:53,560 --> 00:36:01,810
the 97th okay fine I can take the 97th

00:35:57,070 --> 00:36:03,119
and use that obviously you get the

00:36:01,810 --> 00:36:05,589
futures back because if they were

00:36:03,119 --> 00:36:07,780
standard experimental future that you

00:36:05,589 --> 00:36:09,220
got passed in it's a one-shot thing so

00:36:07,780 --> 00:36:11,320
you just passed it into the what your

00:36:09,220 --> 00:36:13,210
when any and so therefore you don't have

00:36:11,320 --> 00:36:20,050
that anymore so you need to get it back

00:36:13,210 --> 00:36:21,940
in in the result so yeah it's great for

00:36:20,050 --> 00:36:24,100
specular tasks

00:36:21,940 --> 00:36:25,420
if you've got something it's like a well

00:36:24,100 --> 00:36:27,070
when the first of these is ready then

00:36:25,420 --> 00:36:29,470
I'm going to do something so so you're

00:36:27,070 --> 00:36:30,670
writing no web browser and you've got no

00:36:29,470 --> 00:36:32,530
download five images in the background

00:36:30,670 --> 00:36:33,520
when the first one comes back then I'm

00:36:32,530 --> 00:36:35,620
going to actually update

00:36:33,520 --> 00:36:37,090
the rendering of the page I don't care

00:36:35,620 --> 00:36:38,500
which one it is I'm just I'm going to do

00:36:37,090 --> 00:36:39,700
that processing and then I'll do

00:36:38,500 --> 00:36:42,930
something else and wait for the next one

00:36:39,700 --> 00:36:45,880
or whatever no so that's great

00:36:42,930 --> 00:36:48,850
so we have two functions that created us

00:36:45,880 --> 00:36:50,650
from futures foo and bar we then moved

00:36:48,850 --> 00:36:52,740
those futures into NNE this is moved

00:36:50,650 --> 00:36:55,920
because it's passed by value and

00:36:52,740 --> 00:36:59,800
experimental future is a move any title

00:36:55,920 --> 00:37:01,060
and then we can know it just returns an

00:36:59,800 --> 00:37:08,350
experimental feature so you can chain a

00:37:01,060 --> 00:37:10,200
continuation so f3 don't then instead of

00:37:08,350 --> 00:37:13,690
just one you can wait for all of them

00:37:10,200 --> 00:37:15,880
this time it's slightly simpler because

00:37:13,690 --> 00:37:16,990
the return value doesn't need to tell

00:37:15,880 --> 00:37:19,860
you which one it was that wake you up

00:37:16,990 --> 00:37:22,060
they were already that's why you woke up

00:37:19,860 --> 00:37:27,040
but again we've got the two overloads

00:37:22,060 --> 00:37:34,630
and so we can spawn three features and

00:37:27,040 --> 00:37:37,420
then we can wait for all of them so how

00:37:34,630 --> 00:37:39,790
does that relate to co-routines know

00:37:37,420 --> 00:37:41,530
we've got our routines from the

00:37:39,790 --> 00:37:43,630
co-routines TS we've got continuations

00:37:41,530 --> 00:37:45,730
from the concurrency TS how does that

00:37:43,630 --> 00:37:48,310
work well in some sense feature so

00:37:45,730 --> 00:37:49,630
ideally suited for co-routines they hold

00:37:48,310 --> 00:37:51,430
a value which is going to be available

00:37:49,630 --> 00:37:53,860
at some point you can already wait on

00:37:51,430 --> 00:37:56,980
them in normal code they can represent

00:37:53,860 --> 00:37:58,630
asynchronous tasks and you can create

00:37:56,980 --> 00:38:02,100
one or read that directly holds a value

00:37:58,630 --> 00:38:06,570
so these map very nicely to co-routines

00:38:02,100 --> 00:38:09,250
and so you might hope that your

00:38:06,570 --> 00:38:13,690
implementation of futures would work

00:38:09,250 --> 00:38:15,130
nicely with care routines so the

00:38:13,690 --> 00:38:17,200
compiler needs to know what to do with

00:38:15,130 --> 00:38:19,630
it and so the library has to specialize

00:38:17,200 --> 00:38:20,890
co-routine traits to say yeah we can

00:38:19,630 --> 00:38:23,200
deal with care routines you just deal

00:38:20,890 --> 00:38:24,460
with it like this if you're if you're

00:38:23,200 --> 00:38:26,260
trying to wait on futures that works

00:38:24,460 --> 00:38:27,880
fine and then operate a koa wait which

00:38:26,260 --> 00:38:32,230
is how do we wait on the future when

00:38:27,880 --> 00:38:35,430
we're in a curry team but once it's done

00:38:32,230 --> 00:38:35,430
those two things it all just works

00:38:37,400 --> 00:38:50,940
and so the there's a nifty feature in in

00:38:48,060 --> 00:38:53,160
the continuations part which is if your

00:38:50,940 --> 00:38:56,100
continuation function returns a future

00:38:53,160 --> 00:38:59,310
rather than a value then that future

00:38:56,100 --> 00:39:03,030
just gets subsumed into the final return

00:38:59,310 --> 00:39:04,770
value of the continuation so you know if

00:39:03,030 --> 00:39:07,140
you have a if your continuation function

00:39:04,770 --> 00:39:10,860
returns an int then the result of your

00:39:07,140 --> 00:39:12,390
then call is a future holding an int if

00:39:10,860 --> 00:39:15,390
your continuation function holds a

00:39:12,390 --> 00:39:17,010
future to an int then the return

00:39:15,390 --> 00:39:18,150
terminal then call is still a future of

00:39:17,010 --> 00:39:19,500
an inch rather than a future or our

00:39:18,150 --> 00:39:23,460
future of an inn because otherwise that

00:39:19,500 --> 00:39:25,770
just gets to missing and so if you have

00:39:23,460 --> 00:39:27,240
a Co routine then if you're trying to

00:39:25,770 --> 00:39:30,870
use it with you just pay routine your

00:39:27,240 --> 00:39:32,930
care routine will return a future so but

00:39:30,870 --> 00:39:36,990
that's okay because you can use it as a

00:39:32,930 --> 00:39:39,390
continuation and then the that Co rooty

00:39:36,990 --> 00:39:42,600
return of a future will then just get

00:39:39,390 --> 00:39:44,700
nicely mapped through so in this case

00:39:42,600 --> 00:39:47,910
where continuation returns a future

00:39:44,700 --> 00:39:49,650
holding a result and then the final

00:39:47,910 --> 00:39:52,680
result of our continuation is again any

00:39:49,650 --> 00:39:54,240
future holding a result and it doesn't

00:39:52,680 --> 00:39:56,550
matter than that continuation was a KO

00:39:54,240 --> 00:40:02,730
routine the IB doesn't care it's just

00:39:56,550 --> 00:40:04,230
something that we turned a future what

00:40:02,730 --> 00:40:07,080
about Co routines and parallel

00:40:04,230 --> 00:40:09,830
algorithms and I mentioned now allows Co

00:40:07,080 --> 00:40:16,500
routine as a possible execution policy

00:40:09,830 --> 00:40:18,600
but know what else is that and so if

00:40:16,500 --> 00:40:20,520
we're writing power levels and we're

00:40:18,600 --> 00:40:22,770
using parallel algorithms then one thing

00:40:20,520 --> 00:40:26,670
we care about specifically is processor

00:40:22,770 --> 00:40:29,070
utilization now we've got a thousand

00:40:26,670 --> 00:40:31,260
cause and we're trying to stick low to

00:40:29,070 --> 00:40:35,250
work on them and what we don't want is

00:40:31,260 --> 00:40:37,950
for those cores to be sat blocking doing

00:40:35,250 --> 00:40:39,780
nothing if we can help it because that

00:40:37,950 --> 00:40:43,200
is just wasting process available

00:40:39,780 --> 00:40:44,790
processor resources so if we have

00:40:43,200 --> 00:40:46,980
blocking operations which you get

00:40:44,790 --> 00:40:50,550
normally with futures

00:40:46,980 --> 00:40:53,160
then they hurt they complicate the

00:40:50,550 --> 00:40:54,660
scheduling may occupy a thread they can

00:40:53,160 --> 00:40:57,000
force us to do a context switch which is

00:40:54,660 --> 00:40:59,280
expensive as far as the process the OS

00:40:57,000 --> 00:41:03,930
is concerned and just generally saps our

00:40:59,280 --> 00:41:05,880
overall performance so co-routines allow

00:41:03,930 --> 00:41:09,480
us to turn blocking operations into non

00:41:05,880 --> 00:41:11,460
blocking ones because we say Khoa wait

00:41:09,480 --> 00:41:14,070
on something and this suspends our

00:41:11,460 --> 00:41:17,190
current care routine and returns to a

00:41:14,070 --> 00:41:19,020
caller and the carotene can then be

00:41:17,190 --> 00:41:21,300
resumed when that waited for thing is

00:41:19,020 --> 00:41:23,280
ready and but in the meantime the

00:41:21,300 --> 00:41:27,450
current thread because do what can still

00:41:23,280 --> 00:41:30,900
do something so this works well with

00:41:27,450 --> 00:41:32,490
parallel algorithms the issue is of

00:41:30,900 --> 00:41:35,190
course that if the suspension is a

00:41:32,490 --> 00:41:37,410
nested call then a stack Lascaux routine

00:41:35,190 --> 00:41:39,990
like we have with the carotenes TS just

00:41:37,410 --> 00:41:42,570
moves the blocking up Millea so if F

00:41:39,990 --> 00:41:44,280
calls G which calls H then if H is a Co

00:41:42,570 --> 00:41:46,470
routine and uses Co await to wait for

00:41:44,280 --> 00:41:49,410
something then that just bazooms

00:41:46,470 --> 00:41:51,810
execution in G but if that then needs

00:41:49,410 --> 00:41:55,530
the result of H then it has to block

00:41:51,810 --> 00:41:58,290
unless it's also a Co routine and can

00:41:55,530 --> 00:42:02,040
use Co await and then likewise all the

00:41:58,290 --> 00:42:06,420
way up the call stack so if we make

00:42:02,040 --> 00:42:09,060
everything a Co routine we're fine and

00:42:06,420 --> 00:42:11,550
we have H which the bottom level thing

00:42:09,060 --> 00:42:14,700
returns a future we cook the result we

00:42:11,550 --> 00:42:16,350
get from Co waiting on something and

00:42:14,700 --> 00:42:17,609
processing it and then we Co use Co

00:42:16,350 --> 00:42:20,040
return because we're trying to return

00:42:17,609 --> 00:42:25,260
the value it's a refuge from a care

00:42:20,040 --> 00:42:27,660
routine and then likewise in G and F and

00:42:25,260 --> 00:42:29,820
that means that then we can use that

00:42:27,660 --> 00:42:35,369
when we're building a parallel algorithm

00:42:29,820 --> 00:42:36,750
as an implementer know so the like if

00:42:35,369 --> 00:42:38,220
you're trying to implement something

00:42:36,750 --> 00:42:39,990
with a parallel last her routine

00:42:38,220 --> 00:42:42,359
algorithm then you provide an overload

00:42:39,990 --> 00:42:44,970
but then internally will look something

00:42:42,359 --> 00:42:47,190
like this it takes no it uses something

00:42:44,970 --> 00:42:48,690
to divide up your data and Co awaits on

00:42:47,190 --> 00:42:51,750
it so that that can be a parallel task

00:42:48,690 --> 00:42:53,990
and then when it's ready and it comes

00:42:51,750 --> 00:42:53,990
back

00:42:54,030 --> 00:43:00,180
then we're going to do recursive calls

00:42:57,570 --> 00:43:01,500
but know that no this is a recursive

00:43:00,180 --> 00:43:02,940
call to this function which is itself

00:43:01,500 --> 00:43:06,090
occur routine and is going to somehow be

00:43:02,940 --> 00:43:08,400
parallel and then we're going to need to

00:43:06,090 --> 00:43:11,250
cooperate on those results in order to

00:43:08,400 --> 00:43:12,270
combine them somehow which again we want

00:43:11,250 --> 00:43:13,860
to be parallel and then we're going to

00:43:12,270 --> 00:43:20,160
wait for that to finish in order to

00:43:13,860 --> 00:43:22,620
return our final result so then the way

00:43:20,160 --> 00:43:24,360
that all these subtasks get scheduled by

00:43:22,620 --> 00:43:25,740
the by the is then an implementation

00:43:24,360 --> 00:43:27,540
details are the library and you don't

00:43:25,740 --> 00:43:29,610
have to look at your code and say now

00:43:27,540 --> 00:43:31,650
where doesn't know how do i how do i

00:43:29,610 --> 00:43:33,450
post all the tasks on to my thread pool

00:43:31,650 --> 00:43:35,250
that's running this or whatever or how

00:43:33,450 --> 00:43:36,030
do i post it up to the GPU this is

00:43:35,250 --> 00:43:39,270
completely hidden

00:43:36,030 --> 00:43:42,600
the runtime library in its co-routine

00:43:39,270 --> 00:43:44,970
traits deals with making sure that when

00:43:42,600 --> 00:43:47,550
you co await on something inside your

00:43:44,970 --> 00:43:52,620
parallel algorithm it gets passed to the

00:43:47,550 --> 00:43:55,560
thread pool and so from a user's

00:43:52,620 --> 00:43:58,320
perspective it looks like magic but

00:43:55,560 --> 00:44:00,690
although all that magic goes in the

00:43:58,320 --> 00:44:07,860
operator Co await overload and the cocoa

00:44:00,690 --> 00:44:15,600
routine traits specializations okay so

00:44:07,860 --> 00:44:17,370
the final thing executive what do we

00:44:15,600 --> 00:44:22,320
mean by an executor I mean the core

00:44:17,370 --> 00:44:23,730
concept of something is it of an

00:44:22,320 --> 00:44:28,590
executor is it something that's going to

00:44:23,730 --> 00:44:32,840
run our tasks controls where and how

00:44:28,590 --> 00:44:34,890
they get executed but quite the details

00:44:32,840 --> 00:44:37,710
different people have different use

00:44:34,890 --> 00:44:39,630
cases and can lead to quite different

00:44:37,710 --> 00:44:46,920
approaches for how you implement and

00:44:39,630 --> 00:44:50,180
specify an executor what kind of tasks

00:44:46,920 --> 00:44:52,200
have we got where do we want them to run

00:44:50,180 --> 00:44:53,610
what relationships are there between

00:44:52,200 --> 00:44:57,210
tasks are they just completely

00:44:53,610 --> 00:45:00,600
independent or oh no are they in some

00:44:57,210 --> 00:45:03,000
crucial way dependent on each other can

00:45:00,600 --> 00:45:05,580
they synchronize with each other or or

00:45:03,000 --> 00:45:08,190
not no can

00:45:05,580 --> 00:45:09,450
can they truly run concurrently order

00:45:08,190 --> 00:45:11,280
some of them have to run sequentially

00:45:09,450 --> 00:45:13,590
each other but they can run concurrently

00:45:11,280 --> 00:45:15,150
with some other batch over there can

00:45:13,590 --> 00:45:18,180
they run into leave like with a parallel

00:45:15,150 --> 00:45:21,210
on sequinsed no can we move them around

00:45:18,180 --> 00:45:23,970
across threads can they in turn spawn

00:45:21,210 --> 00:45:24,750
new tasks on the same executor can they

00:45:23,970 --> 00:45:29,040
wait for each other

00:45:24,750 --> 00:45:31,860
all these are important questions and we

00:45:29,040 --> 00:45:34,200
want to allow as many of the know yes

00:45:31,860 --> 00:45:36,420
and know as different answers for all

00:45:34,200 --> 00:45:40,830
these questions without fundamentally

00:45:36,420 --> 00:45:42,480
compromising our framework and then we

00:45:40,830 --> 00:45:44,700
have other questions we know can an

00:45:42,480 --> 00:45:47,580
execute it be copyable no what would

00:45:44,700 --> 00:45:51,240
that mean are they composable can use

00:45:47,580 --> 00:45:54,720
somehow know build a composite executive

00:45:51,240 --> 00:45:57,150
all out of individual parts if you have

00:45:54,720 --> 00:45:59,130
a task that you know is running on them

00:45:57,150 --> 00:46:01,530
executable and you have some form of

00:45:59,130 --> 00:46:02,880
handle to that task can you get the

00:46:01,530 --> 00:46:05,370
executor back in order to run another

00:46:02,880 --> 00:46:08,430
task on the same one or if you are

00:46:05,370 --> 00:46:10,260
running a task right now and you know

00:46:08,430 --> 00:46:12,360
that this function that I am in has been

00:46:10,260 --> 00:46:15,270
called from an executor can I get an

00:46:12,360 --> 00:46:17,550
executor that was to find out which

00:46:15,270 --> 00:46:21,300
executor that was in order to run a new

00:46:17,550 --> 00:46:23,070
task now some form of continuation of

00:46:21,300 --> 00:46:28,890
the current one so these are again

00:46:23,070 --> 00:46:31,500
important questions there have been I

00:46:28,890 --> 00:46:32,960
think I counted something like 23 papers

00:46:31,500 --> 00:46:36,390
on this topic over the last few years

00:46:32,960 --> 00:46:42,960
from the C++ committee of which the

00:46:36,390 --> 00:46:46,830
latest is P 433 r2 that's the default

00:46:42,960 --> 00:46:48,360
the third the second revision and

00:46:46,830 --> 00:46:50,310
therefore the third third paper with

00:46:48,360 --> 00:46:52,140
this number of a unified executives

00:46:50,310 --> 00:46:54,240
proposal for C++ which provides us with

00:46:52,140 --> 00:46:56,550
many customisation points in order to

00:46:54,240 --> 00:47:02,400
allow as many possible answers to the

00:46:56,550 --> 00:47:04,410
previous questions no it has a basic

00:47:02,400 --> 00:47:05,970
concept of what we mean by an executor

00:47:04,410 --> 00:47:07,440
in this case it is something that is

00:47:05,970 --> 00:47:09,510
copy constructor bangla quality

00:47:07,440 --> 00:47:11,040
comparable so you can say is this the

00:47:09,510 --> 00:47:15,510
same executor that I had over there and

00:47:11,040 --> 00:47:19,000
I can freely copy it around it provides

00:47:15,510 --> 00:47:21,190
a context member function which tell

00:47:19,000 --> 00:47:22,690
see what the execution context is so for

00:47:21,190 --> 00:47:24,220
example if you if you're trying to

00:47:22,690 --> 00:47:25,660
implement a thread pool then your

00:47:24,220 --> 00:47:27,550
execution context is the spread pool

00:47:25,660 --> 00:47:30,340
itself and then the executor is

00:47:27,550 --> 00:47:33,340
something that says something tells you

00:47:30,340 --> 00:47:34,870
to run the task on that thread pool so

00:47:33,340 --> 00:47:36,970
you can pass around your executor

00:47:34,870 --> 00:47:40,210
because it's just a handle and you can

00:47:36,970 --> 00:47:42,460
copy that around willy-nilly and no any

00:47:40,210 --> 00:47:45,670
any thread that has a cop has a hand has

00:47:42,460 --> 00:47:47,200
a copy of that executor can then launch

00:47:45,670 --> 00:47:50,020
a task on that thread pool which is what

00:47:47,200 --> 00:47:53,110
the context fundamentally is but the

00:47:50,020 --> 00:47:56,650
details of what a context is is specific

00:47:53,110 --> 00:47:58,390
to the executor time and then

00:47:56,650 --> 00:48:01,150
fundamentally you somehow have an exact

00:47:58,390 --> 00:48:03,910
execute function whether that's a member

00:48:01,150 --> 00:48:09,250
function or free function in order to

00:48:03,910 --> 00:48:11,350
actually gone to asked and the way that

00:48:09,250 --> 00:48:14,550
we get to all the different properties

00:48:11,350 --> 00:48:17,590
that we wanted to know can come they can

00:48:14,550 --> 00:48:19,930
things run concurrently can we don't

00:48:17,590 --> 00:48:22,720
execute things whatever we have this

00:48:19,930 --> 00:48:25,090
require member function which provides

00:48:22,720 --> 00:48:28,660
overloads so if you need a specific

00:48:25,090 --> 00:48:32,350
property from the executor then you say

00:48:28,660 --> 00:48:34,690
I require this property and either it

00:48:32,350 --> 00:48:36,160
will compile and work because it can

00:48:34,690 --> 00:48:39,060
provide you that property or it will

00:48:36,160 --> 00:48:41,380
fail to compile because it doesn't and

00:48:39,060 --> 00:48:45,670
the framework and then Lyle you to build

00:48:41,380 --> 00:48:47,860
everything else from there so let's have

00:48:45,670 --> 00:48:50,110
a look at some code we have some

00:48:47,860 --> 00:48:51,520
executor my executor and we trying to

00:48:50,110 --> 00:48:54,910
fund a function so you can just like

00:48:51,520 --> 00:48:56,530
execute some function that works but we

00:48:54,910 --> 00:48:59,380
don't know quite what it's going to do

00:48:56,530 --> 00:49:02,710
in terms of how it's going to schedule

00:48:59,380 --> 00:49:05,350
an executor function so you might think

00:49:02,710 --> 00:49:08,890
ok well we we need something specific we

00:49:05,350 --> 00:49:13,030
need some properties on that so I say

00:49:08,890 --> 00:49:15,640
I'm going to require that from my

00:49:13,030 --> 00:49:17,350
executor I need it to be two-way

00:49:15,640 --> 00:49:21,400
communication so I need to know where my

00:49:17,350 --> 00:49:23,800
task is done but and I'm only going to

00:49:21,400 --> 00:49:26,410
execute one function at a time so and

00:49:23,800 --> 00:49:29,530
then that gives us a combined executor

00:49:26,410 --> 00:49:31,930
from our source one that has those

00:49:29,530 --> 00:49:33,460
properties and if it

00:49:31,930 --> 00:49:35,920
doesn't if we can't do that it will fail

00:49:33,460 --> 00:49:38,560
to compile and then we think all to a

00:49:35,920 --> 00:49:40,500
execute of our function and that will

00:49:38,560 --> 00:49:43,210
definitely then give us a future back

00:49:40,500 --> 00:49:47,980
which we can then wait on in order to

00:49:43,210 --> 00:49:51,520
know when the task has been done there's

00:49:47,980 --> 00:49:56,140
a whole bunch of properties that you can

00:49:51,520 --> 00:49:57,940
require know you can launch your tasks

00:49:56,140 --> 00:49:59,470
to say well I don't need to know because

00:49:57,940 --> 00:50:02,380
I've got some other mechanism of knowing

00:49:59,470 --> 00:50:06,640
knowing when it's done or I want a

00:50:02,380 --> 00:50:08,260
future back or this function that I'm

00:50:06,640 --> 00:50:10,060
passing you now is a continuation of

00:50:08,260 --> 00:50:12,190
this other of this other function which

00:50:10,060 --> 00:50:14,580
I'm passing your future for No

00:50:12,190 --> 00:50:20,080
so this works with the continuations

00:50:14,580 --> 00:50:21,580
from the concurrency TS I only want to

00:50:20,080 --> 00:50:23,350
pass in one future at a time one

00:50:21,580 --> 00:50:25,150
function at a time I want to pass in a

00:50:23,350 --> 00:50:27,910
whole bunch of functions at a time and

00:50:25,150 --> 00:50:30,550
get a name and in in order to bulk

00:50:27,910 --> 00:50:32,650
execute things because then you don't

00:50:30,550 --> 00:50:38,770
have the overhead of filling up your

00:50:32,650 --> 00:50:41,920
task queue one at a time I won the

00:50:38,770 --> 00:50:43,690
guarantee that I will never block the

00:50:41,920 --> 00:50:45,040
execute function is never gonna block

00:50:43,690 --> 00:50:46,780
it's always going to run the task in the

00:50:45,040 --> 00:50:48,160
background on some other thread or add

00:50:46,780 --> 00:50:53,650
it's the Sun cue for running later or

00:50:48,160 --> 00:50:55,660
something or I will need to always block

00:50:53,650 --> 00:50:58,390
because when I call it I wanted to run

00:50:55,660 --> 00:51:00,130
the function right now or possibly you

00:50:58,390 --> 00:51:04,480
want it to do whichever is most

00:51:00,130 --> 00:51:05,890
appropriate for it possibly block or you

00:51:04,480 --> 00:51:07,270
want to say well the function functions

00:51:05,890 --> 00:51:10,030
that when passing in or in some

00:51:07,270 --> 00:51:12,040
continuations of the current task so

00:51:10,030 --> 00:51:15,100
when the current task is done that then

00:51:12,040 --> 00:51:17,770
start these new functions and not before

00:51:15,100 --> 00:51:19,300
or attentively there will them

00:51:17,770 --> 00:51:20,410
independent things so I don't bother

00:51:19,300 --> 00:51:22,330
waiting for me you can start them

00:51:20,410 --> 00:51:24,010
straightaway on some other thread and

00:51:22,330 --> 00:51:25,720
then there are more there's a whole

00:51:24,010 --> 00:51:30,520
bunch of properties that you can query

00:51:25,720 --> 00:51:33,430
and the framework is extensible so in

00:51:30,520 --> 00:51:37,150
theory you could add extra properties

00:51:33,430 --> 00:51:38,470
for the the continuations that you as an

00:51:37,150 --> 00:51:41,770
impetus a library implementer are

00:51:38,470 --> 00:51:43,690
providing so when these go when the this

00:51:41,770 --> 00:51:45,520
finally gets widespread implementation

00:51:43,690 --> 00:51:47,350
then you might find that each implement

00:51:45,520 --> 00:51:57,490
provides a few extra properties than

00:51:47,350 --> 00:51:59,260
adjust for them this can then interact

00:51:57,490 --> 00:52:02,070
with the parallel algorithms because you

00:51:59,260 --> 00:52:05,920
can provide a custom execution policy

00:52:02,070 --> 00:52:13,330
that says spawn the tasks for this

00:52:05,920 --> 00:52:16,000
parallel execution on my executor it's

00:52:13,330 --> 00:52:18,790
also natural to what to do to think that

00:52:16,000 --> 00:52:22,480
you can do the same with the current

00:52:18,790 --> 00:52:24,880
concurrency ts continuations so I want

00:52:22,480 --> 00:52:27,070
to call this continuation and do it on

00:52:24,880 --> 00:52:29,140
an executor now that's actually worked

00:52:27,070 --> 00:52:31,420
quite well because the current the TS

00:52:29,140 --> 00:52:34,330
are specified runs the continuation on

00:52:31,420 --> 00:52:35,800
an unspecified threat you don't know

00:52:34,330 --> 00:52:38,410
whether it's a new threat you don't know

00:52:35,800 --> 00:52:40,540
whether it's a pool thread in theory it

00:52:38,410 --> 00:52:41,830
could be your GUI thread when you don't

00:52:40,540 --> 00:52:46,060
want anything is run on the GUI thread

00:52:41,830 --> 00:52:48,400
unless you're intending it to no no any

00:52:46,060 --> 00:52:50,050
any thread in the system so it's with

00:52:48,400 --> 00:52:50,680
the concurrency TS which is this is

00:52:50,050 --> 00:52:54,430
deliberate

00:52:50,680 --> 00:52:56,170
it's a TS we're asking for its

00:52:54,430 --> 00:53:00,070
implemented like this in order to get

00:52:56,170 --> 00:53:02,170
maximum feedback is and allow library

00:53:00,070 --> 00:53:06,130
implementers maximum scope for seeing

00:53:02,170 --> 00:53:07,930
what their users want but with an

00:53:06,130 --> 00:53:10,330
executive framework then it makes sense

00:53:07,930 --> 00:53:13,500
to be able to say I want this task to

00:53:10,330 --> 00:53:13,500
run on this executor

00:53:17,330 --> 00:53:22,950
so there's different availability for

00:53:20,100 --> 00:53:24,300
these different parts there is not a

00:53:22,950 --> 00:53:27,420
shipping implementation that aware off

00:53:24,300 --> 00:53:31,590
that supplies all of them Visual Studio

00:53:27,420 --> 00:53:34,680
provides the co-routines TS clang 5 if

00:53:31,590 --> 00:53:39,330
you use it with lib C++ 5 also provides

00:53:34,680 --> 00:53:41,610
the care routines ts the hpx team have

00:53:39,330 --> 00:53:42,990
got parallel algorithms and futures with

00:53:41,610 --> 00:53:45,120
continuations which look like the

00:53:42,990 --> 00:53:47,100
concurrency TS ones and they have some

00:53:45,120 --> 00:53:51,390
executive support but it's not quite the

00:53:47,100 --> 00:53:53,570
same as this proposal mine just rode pro

00:53:51,390 --> 00:53:57,720
library provides the concurrent CTS and

00:53:53,570 --> 00:54:00,630
integrates with Co routines and I'm

00:53:57,720 --> 00:54:03,060
working on parallel algorithms but the

00:54:00,630 --> 00:54:05,280
whole lot is not in any one

00:54:03,060 --> 00:54:06,360
implementation so at the moment you're

00:54:05,280 --> 00:54:07,950
going to have to choose piecemeal or

00:54:06,360 --> 00:54:16,290
actually help work on one of the

00:54:07,950 --> 00:54:17,670
open-source implementations my book

00:54:16,290 --> 00:54:19,140
cover stuff on the parallel algorithms

00:54:17,670 --> 00:54:21,840
and they can cover CTS there's nothing

00:54:19,140 --> 00:54:23,610
on ko routines in there the early access

00:54:21,840 --> 00:54:26,100
Edition is available for the second

00:54:23,610 --> 00:54:27,210
edition and I believe there's a 50%

00:54:26,100 --> 00:54:29,460
discount code that you should all have

00:54:27,210 --> 00:54:32,460
received by email and for being at the

00:54:29,460 --> 00:54:37,080
conference if you're actually interested

00:54:32,460 --> 00:54:40,590
in just a pro then go check out a

00:54:37,080 --> 00:54:42,890
website it's a commercial library of an

00:54:40,590 --> 00:54:42,890
open-source

00:54:46,950 --> 00:54:52,900
so we have a few minutes left has anyone

00:54:50,650 --> 00:55:00,540
got any questions there's a couple of

00:54:52,900 --> 00:55:00,540
mics how far

00:55:00,980 --> 00:55:08,720
that one so does that mean that if

00:55:06,680 --> 00:55:11,750
you're using it in your main function

00:55:08,720 --> 00:55:14,330
cover teens in your main function that

00:55:11,750 --> 00:55:18,980
you have to make your main function

00:55:14,330 --> 00:55:21,470
return a ko ko' routine or I don't think

00:55:18,980 --> 00:55:30,260
the co-routines TS supports main being a

00:55:21,470 --> 00:55:33,070
carotene a lot of slides back you had a

00:55:30,260 --> 00:55:36,230
shared future yes

00:55:33,070 --> 00:55:39,080
continue at two continuations using dot

00:55:36,230 --> 00:55:42,890
then twice yes how will that we've run

00:55:39,080 --> 00:55:45,980
will that run one and then the other or

00:55:42,890 --> 00:55:48,140
in parallel or is there any it's okay so

00:55:45,980 --> 00:55:50,720
if you if you said your multiple

00:55:48,140 --> 00:55:52,340
continuations on a shared future then is

00:55:50,720 --> 00:55:55,190
there any guarantee what order they run

00:55:52,340 --> 00:55:56,750
and well know is that the TS things run

00:55:55,190 --> 00:55:59,060
on unspecified threads and they run one

00:55:56,750 --> 00:56:01,010
suicides in unspecified times and order

00:55:59,060 --> 00:56:02,390
so if you schedule multiple they might

00:56:01,010 --> 00:56:03,740
run concurrently on set of threads they

00:56:02,390 --> 00:56:07,130
might run sequentially on one thread in

00:56:03,740 --> 00:56:11,150
some order you don't know that's a

00:56:07,130 --> 00:56:14,150
deliberate again decision to allow TS

00:56:11,150 --> 00:56:17,990
implement is maximum scope and that will

00:56:14,150 --> 00:56:19,550
change if the executor comes in because

00:56:17,990 --> 00:56:20,990
then you if it's someone then then you

00:56:19,550 --> 00:56:24,680
can then say what does the executives

00:56:20,990 --> 00:56:27,170
require they will specify for that and

00:56:24,680 --> 00:56:31,220
the other question was you mentioned to

00:56:27,170 --> 00:56:33,770
wait there when any so you you can wait

00:56:31,220 --> 00:56:36,680
for any of the continuation or any of

00:56:33,770 --> 00:56:39,290
the tasks will be ready what if you

00:56:36,680 --> 00:56:42,530
would want to wait for the next how

00:56:39,290 --> 00:56:44,990
could have achieved okay so you call

00:56:42,530 --> 00:56:46,700
wait any and in case it tells you that

00:56:44,990 --> 00:56:48,680
one of them was ready yeah now when you

00:56:46,700 --> 00:56:50,330
get the result back from that then

00:56:48,680 --> 00:56:53,060
you've got the full set of futures that

00:56:50,330 --> 00:56:55,340
you waited for so you can then pass

00:56:53,060 --> 00:56:56,630
those back into a subsequent when any

00:56:55,340 --> 00:56:58,880
call if you want to wait for the next

00:56:56,630 --> 00:57:02,700
one oh you get more yes you get them all

00:56:58,880 --> 00:57:04,859
back thank you okay

00:57:02,700 --> 00:57:07,710
you had some sample code there for a

00:57:04,859 --> 00:57:09,780
parallel func that the that called

00:57:07,710 --> 00:57:12,030
parallel divided and then parallel

00:57:09,780 --> 00:57:14,520
combined at the end it was sprinkled

00:57:12,030 --> 00:57:18,210
with but he's koa wait ko returned yes

00:57:14,520 --> 00:57:19,829
and I looked at how that would look if

00:57:18,210 --> 00:57:21,420
you did it with promises and futures

00:57:19,829 --> 00:57:23,550
instead and you just chain them together

00:57:21,420 --> 00:57:27,570
with then and at the end you when all

00:57:23,550 --> 00:57:29,730
then parallel combined yes and it seems

00:57:27,570 --> 00:57:31,320
like about the same amount of code but

00:57:29,730 --> 00:57:33,480
the futures promises one seemed to be

00:57:31,320 --> 00:57:35,250
more refactor of all in that you didn't

00:57:33,480 --> 00:57:36,900
have these syntactic constraints of

00:57:35,250 --> 00:57:38,190
having to put the key words everywhere

00:57:36,900 --> 00:57:42,480
and that breaking when you pulled out a

00:57:38,190 --> 00:57:44,790
piece of it is have you seen good

00:57:42,480 --> 00:57:48,240
examples of places where co-routines

00:57:44,790 --> 00:57:50,190
really give a big benefit over just

00:57:48,240 --> 00:57:51,329
using futures with an executor that

00:57:50,190 --> 00:57:53,130
would be completely synchronous

00:57:51,329 --> 00:57:58,050
something where when you schedule on an

00:57:53,130 --> 00:58:01,589
executor it just runs the thing yeah I

00:57:58,050 --> 00:58:03,000
mean there's plenty of examples in when

00:58:01,589 --> 00:58:06,900
you're looking at real code obviously

00:58:03,000 --> 00:58:08,430
this is code to go on a slide the

00:58:06,900 --> 00:58:10,740
advantage if you use something like that

00:58:08,430 --> 00:58:12,390
where you're using co-routines in our

00:58:10,740 --> 00:58:14,670
parallel algorithm then it means that

00:58:12,390 --> 00:58:19,560
the whole thing is a KO routine and and

00:58:14,670 --> 00:58:21,810
and you can disable the parallelism

00:58:19,560 --> 00:58:23,310
outside of that by put in by putting

00:58:21,810 --> 00:58:27,180
where it what's the context that it's

00:58:23,310 --> 00:58:30,420
running on and so it pumps that

00:58:27,180 --> 00:58:32,369
decisions no out of the out of the the

00:58:30,420 --> 00:58:34,109
code that's right there into the library

00:58:32,369 --> 00:58:35,730
that's dealing with the parallelism at

00:58:34,109 --> 00:58:37,310
the back which could then put it all on

00:58:35,730 --> 00:58:40,050
one on the single thread and just

00:58:37,310 --> 00:58:43,980
interleave all that all the calls as

00:58:40,050 --> 00:58:45,569
various bits become ready but in in some

00:58:43,980 --> 00:58:47,490
circumstances then just changing it with

00:58:45,569 --> 00:58:49,680
promises and futures is just the way to

00:58:47,490 --> 00:58:52,349
go I know this is it isn't a panacea

00:58:49,680 --> 00:58:54,030
it's not an answer to everything some

00:58:52,349 --> 00:58:57,450
sometimes it makes the code clearer and

00:58:54,030 --> 00:59:00,599
at other times it doesn't help say so in

00:58:57,450 --> 00:59:02,099
the curry tin case who's deciding it

00:59:00,599 --> 00:59:05,339
like which part of the program decides

00:59:02,099 --> 00:59:07,140
whether it's like multi-thread or not is

00:59:05,339 --> 00:59:08,520
it co-routine traits yeah as care

00:59:07,140 --> 00:59:10,500
routine traits an operator color white

00:59:08,520 --> 00:59:13,530
witch and those are global like operator

00:59:10,500 --> 00:59:15,330
new Sumi I work operator caraway is is

00:59:13,530 --> 00:59:17,990
overloaded

00:59:15,330 --> 00:59:20,100
they've so you specialized it for your

00:59:17,990 --> 00:59:25,770
speaks the right overload of whatever

00:59:20,100 --> 00:59:27,620
type it is and then but it works

00:59:25,770 --> 00:59:30,870
together with care routine traits

00:59:27,620 --> 00:59:33,720
it's rather nifty inside so when you if

00:59:30,870 --> 00:59:35,280
you're carotene traits allow you to have

00:59:33,720 --> 00:59:36,510
an await transform if you co wait for

00:59:35,280 --> 00:59:37,980
something then it transforms it into

00:59:36,510 --> 00:59:40,590
something else before it looks up

00:59:37,980 --> 00:59:42,900
operator Co wait so it's no it's really

00:59:40,590 --> 00:59:45,600
quite a nifty and I was a lot of scope

00:59:42,900 --> 00:59:47,520
for changing things around it took me a

00:59:45,600 --> 00:59:50,550
while to get my head round the scope of

00:59:47,520 --> 00:59:54,150
co-routines TS and how actually really

00:59:50,550 --> 00:59:58,590
good it is and the the the scope that we

00:59:54,150 --> 01:00:03,860
have for no completely messing up your

00:59:58,590 --> 01:00:06,420
code but in a good way yeah thank you

01:00:03,860 --> 01:00:09,060
you should have several questions first

01:00:06,420 --> 01:00:11,160
of all you mentioned GPU so please

01:00:09,060 --> 01:00:13,590
clarified that means that if I use a

01:00:11,160 --> 01:00:16,620
parallel version of an STL algorithm it

01:00:13,590 --> 01:00:18,270
can actually flow some work on GPU yes

01:00:16,620 --> 01:00:21,210
and the code by example this morning did

01:00:18,270 --> 01:00:25,460
just that well how exactly this happens

01:00:21,210 --> 01:00:27,690
I mean I think their implementation

01:00:25,460 --> 01:00:30,870
their compiler is a wrapper for two

01:00:27,690 --> 01:00:33,750
compilers and so you ship you pass it

01:00:30,870 --> 01:00:37,050
your CPP file it compiles it for the CPU

01:00:33,750 --> 01:00:38,910
and for the GPU and then at runtime it

01:00:37,050 --> 01:00:41,370
depending on where it gets its shipped

01:00:38,910 --> 01:00:44,070
some of the work over to the GPU okay so

01:00:41,370 --> 01:00:48,210
now developers of the C runtime and

01:00:44,070 --> 01:00:51,180
compilers actually required to adapt

01:00:48,210 --> 01:00:54,030
some GPU operations for this particular

01:00:51,180 --> 01:00:56,160
thing yeah inside yes

01:00:54,030 --> 01:00:58,350
every good their implementation requires

01:00:56,160 --> 01:01:01,440
compiler support no so they shipping a

01:00:58,350 --> 01:01:04,080
compiler that does it say okay yeah

01:01:01,440 --> 01:01:07,590
that's pretty cool actually yeah I think

01:01:04,080 --> 01:01:11,610
so well this brings to another question

01:01:07,590 --> 01:01:15,030
I was wondering is this this seems like

01:01:11,610 --> 01:01:17,910
a really really hard to walk and big

01:01:15,030 --> 01:01:20,010
amount for work how how it is actually

01:01:17,910 --> 01:01:24,390
applicable I mean usually when you

01:01:20,010 --> 01:01:26,850
paralyze things you separate your walk

01:01:24,390 --> 01:01:28,310
into tasks and then you make touch base

01:01:26,850 --> 01:01:31,220
parallelism

01:01:28,310 --> 01:01:32,840
it is it seems like a very corner case

01:01:31,220 --> 01:01:35,900
when you want to paralyze actually one

01:01:32,840 --> 01:01:38,480
algorithm because usually you have many

01:01:35,900 --> 01:01:41,180
many algorithms and you have a some kind

01:01:38,480 --> 01:01:45,140
of pipeline and maybe you have a

01:01:41,180 --> 01:01:47,750
dependency graph but paralyzation level

01:01:45,140 --> 01:01:51,470
of particular algorithm looks like just

01:01:47,750 --> 01:01:53,180
some corner case yeah in some way okay

01:01:51,470 --> 01:01:57,700
so sometimes that is what your problem

01:01:53,180 --> 01:02:00,350
is and you have a big problem which is I

01:01:57,700 --> 01:02:03,050
processed this large chunk of data all

01:02:00,350 --> 01:02:05,770
the same and you can use one of the STL

01:02:03,050 --> 01:02:10,280
algorithms to it lots of problems of

01:02:05,770 --> 01:02:11,810
other varieties you can rewrite as they

01:02:10,280 --> 01:02:13,340
call to transform reduce transform

01:02:11,810 --> 01:02:15,380
reduce is so general that you can do a

01:02:13,340 --> 01:02:18,980
heck of a lot with it and so parallel

01:02:15,380 --> 01:02:20,990
transform reduce you can do a whole

01:02:18,980 --> 01:02:22,040
bunch of things but sometimes that still

01:02:20,990 --> 01:02:23,960
doesn't work because yeah you have a

01:02:22,040 --> 01:02:25,340
pipeline in which case this doesn't

01:02:23,960 --> 01:02:25,550
solve your problem you need something

01:02:25,340 --> 01:02:28,240
else

01:02:25,550 --> 01:02:32,240
there are pipeline proposals before the

01:02:28,240 --> 01:02:34,970
C++ committee but they're not in any TSS

01:02:32,240 --> 01:02:36,530
or in the standards draft yet so

01:02:34,970 --> 01:02:38,540
basically the committee decided that

01:02:36,530 --> 01:02:42,710
this particular approach is applicable

01:02:38,540 --> 01:02:44,780
to many situations in real life yes it

01:02:42,710 --> 01:02:47,090
really is in practice there have been

01:02:44,780 --> 01:02:49,910
libraries shipping that do just this for

01:02:47,090 --> 01:02:52,580
a long time and so we finally got a

01:02:49,910 --> 01:02:56,240
standard API for it in the C++ standard

01:02:52,580 --> 01:02:59,170
rather than having to use the vendor

01:02:56,240 --> 01:03:01,550
specific implementations and their API

01:02:59,170 --> 01:03:04,850
key great things and the last questions

01:03:01,550 --> 01:03:07,850
you mentioned that when we throw an

01:03:04,850 --> 01:03:09,680
exception from a from a function that is

01:03:07,850 --> 01:03:12,590
executed in parallel you know for

01:03:09,680 --> 01:03:14,450
example for each we terminate right but

01:03:12,590 --> 01:03:16,540
we could have accumulated all exception

01:03:14,450 --> 01:03:23,300
but Committee decided not to do that yes

01:03:16,540 --> 01:03:25,900
why because it's messy and I what would

01:03:23,300 --> 01:03:28,340
the user do with a thousand exceptions

01:03:25,900 --> 01:03:30,290
well we have nested session exception

01:03:28,340 --> 01:03:32,150
process we have all kind of stuff I are

01:03:30,290 --> 01:03:33,680
pretty sure we might come up with some

01:03:32,150 --> 01:03:36,020
solution so there should be some

01:03:33,680 --> 01:03:38,720
fundamental reasons why we don't want to

01:03:36,020 --> 01:03:40,650
do that right it's it's messy it's

01:03:38,720 --> 01:03:42,810
complicated it makes the implement a

01:03:40,650 --> 01:03:45,900
of parallelograms really hard super

01:03:42,810 --> 01:03:47,400
house place right yeah but it but it

01:03:45,900 --> 01:03:49,050
also adds overhead because there has to

01:03:47,400 --> 01:03:50,520
be synchronization this direct this Rand

01:03:49,050 --> 01:03:56,100
list has to be synchronized across the

01:03:50,520 --> 01:03:57,690
things it's it's something that the the

01:03:56,100 --> 01:03:59,430
implement have said we don't want to do

01:03:57,690 --> 01:04:03,630
this if you're on the GPU and exceptions

01:03:59,430 --> 01:04:05,250
are not the same so it it really it

01:04:03,630 --> 01:04:08,490
complicated like the task of shipping

01:04:05,250 --> 01:04:10,230
stuff the GPU so know if you wish to

01:04:08,490 --> 01:04:12,510
write yourself a parallel algorithm that

01:04:10,230 --> 01:04:13,980
propagates exceptions feel free no one's

01:04:12,510 --> 01:04:18,870
gonna stop you and you already did

01:04:13,980 --> 01:04:20,550
actually and they alternatively badger

01:04:18,870 --> 01:04:23,280
somebody else who is already writing one

01:04:20,550 --> 01:04:31,940
an open source one and get them to do

01:04:23,280 --> 01:04:37,340
one thanks okay thank you for whenever

01:04:31,940 --> 01:04:45,060
presentation I have a question I have a

01:04:37,340 --> 01:04:52,920
concurrent TS similar Microsoft was PPR

01:04:45,060 --> 01:04:56,690
tasks I think Microsoft PPR tasks is is

01:04:52,920 --> 01:05:01,500
Microsoft PPR tasks compare entities

01:04:56,690 --> 01:05:05,520
concurrent tiers in some ways I believe

01:05:01,500 --> 01:05:07,770
they have some some of the same sort of

01:05:05,520 --> 01:05:12,870
concepts and tasks and continuations and

01:05:07,770 --> 01:05:16,320
features yeah if you I don't use people

01:05:12,870 --> 01:05:17,310
the Microsoft ppl so if you want to know

01:05:16,320 --> 01:05:18,600
about that then you're probably best

01:05:17,310 --> 01:05:22,050
lost nothing one of the Microsoft guys

01:05:18,600 --> 01:05:24,440
there's quite a few here today so yes

01:05:22,050 --> 01:05:24,440

YouTube URL: https://www.youtube.com/watch?v=JvHZ_OECOFU


