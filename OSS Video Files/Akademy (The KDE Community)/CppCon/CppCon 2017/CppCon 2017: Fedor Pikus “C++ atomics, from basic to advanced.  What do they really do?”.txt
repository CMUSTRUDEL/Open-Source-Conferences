Title: CppCon 2017: Fedor Pikus “C++ atomics, from basic to advanced.  What do they really do?”
Publication date: 2017-10-09
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
C++11 introduced atomic operations. They allowed C++ programmers to express a lot of control over how memory is used in concurrent programs and made portable lock-free concurrency possible. They also allowed programmers to ask a lot of questions about how memory is used in concurrent programs and made a lot of subtle bugs possible.

This talk analyzes C++ atomic features from two distinct points of view: what do they allow the programmer to express? what do they really do? The programmer always has two audiences: the people who will read the code, and the compilers and machines which will execute it. This distinction is, unfortunately, often missed. For lock-free programming, the difference between the two viewpoints is of particular importance: every time an explicit atomic operation is present, the programmer is saying to the reader of the program "pay attention, something very unusual is going on here." Do we have the tools in the language to precisely describe what is going on and in what way it is unusual? At the same time, the programmer is saying to the compiler and the hardware "this needs to be done exactly as I say, and with maximum efficiency since I went to all this trouble."

This talk starts from the basics, inasmuch as this term can be applied to lock-free programming. We then explore how the C++ lock-free constructs are used to express programmer's intent clearly (and when they get in the way of clarity). Of course, there will be code to look at and to be confused by. At the same time, we never lose track of the fact that the atomics are one of the last resorts of efficiency, and the question of what happens in hardware and how fast does it happen is of paramount importance. Of course, the first rule of performance — "never guess about performance!" — applies, and any claim about speed must be supported by benchmarks.

If you never used C++ atomics but want to learn, this is the talk for you. If you think you know C++ atomics but are unclear on few details, come to fill these few gaps in your knowledge. If you really do know C++ atomics, come to feel good (or to be surprised, and then feel even better).
— 
Fedor Pikus: Mentor Graphics - Siemens business, Chief Scientist

Fedor G Pikus is a Chief Engineering Scientist in the Design to Silicon division of Mentor Graphics Corp. His earlier positions included a Senior Software Engineer at Google and a Chief Software Architect for Calibre PERC, LVS, DFM at Mentor Graphics. He joined Mentor Graphics in 1998 when he made a switch from academic research in computational physics to software industry. His responsibilities as a Chief Scientist include planning long-term technical direction of Calibre products, directing and training the engineers who work on these products, design and architecture of the software, and research in new design and software technologies. Fedor has over 25 patents and over 90 papers and conference presentations on physics, EDA, software design, and C++ language.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,718 --> 00:00:01,718
- All right.

00:00:03,625 --> 00:00:05,042
Welcome, come in.

00:00:06,077 --> 00:00:10,244
Welcome to the Defense Against the Dark Arts class,

00:00:11,479 --> 00:00:14,396
where you will learn about the more

00:00:19,082 --> 00:00:21,999
radioactive parts of C++.

00:00:25,901 --> 00:00:27,852
For this class, I have several goals.

00:00:27,852 --> 00:00:29,228
One is just educational.

00:00:29,228 --> 00:00:30,884
If you don't know about atomics,

00:00:30,884 --> 00:00:33,488
I will introduce them from the beginning

00:00:33,488 --> 00:00:36,495
and all the way to where I want you to be.

00:00:36,495 --> 00:00:38,783
But there will be things that,

00:00:38,783 --> 00:00:42,116
for those who know something about them,

00:00:45,490 --> 00:00:46,481
might be interesting.

00:00:46,481 --> 00:00:47,727
More importantly, there will be things

00:00:47,727 --> 00:00:51,668
for those who think they know something about them.

00:00:51,668 --> 00:00:55,031
Okay, so let's start about C++ atomics.

00:00:55,031 --> 00:00:59,519
Now, if you're here on a class about atomic operations,

00:00:59,519 --> 00:01:01,840
it's probably because you want to learn

00:01:01,840 --> 00:01:03,479
something about lock-free programming

00:01:03,479 --> 00:01:04,721
or you want to learn something useful

00:01:04,721 --> 00:01:05,943
for lock-free programming.

00:01:05,943 --> 00:01:07,184
And if you want to learn something useful

00:01:07,184 --> 00:01:09,582
for lock-free programming, it's because

00:01:09,582 --> 00:01:12,473
lock-free programming is fast, right?

00:01:12,473 --> 00:01:14,723
Is everybody here for that?

00:01:16,594 --> 00:01:18,735
Okay, so lock-free programming is fast.

00:01:18,735 --> 00:01:20,735
And to demonstrate that,

00:01:22,613 --> 00:01:24,402
everybody was at Bjarne's keynote, right?

00:01:24,402 --> 00:01:26,721
Bjarne said you should start your presentation

00:01:26,721 --> 00:01:27,891
with an application.

00:01:27,891 --> 00:01:29,880
So I put in the application.

00:01:29,880 --> 00:01:32,317
I have two programs that do exactly the same thing,

00:01:32,317 --> 00:01:33,920
and they do it correctly.

00:01:33,920 --> 00:01:34,753
There are no tricks.

00:01:34,753 --> 00:01:36,250
There are no wait loops.

00:01:36,250 --> 00:01:38,232
It's straight up computation.

00:01:38,232 --> 00:01:41,289
One of the programs is mutex-based,

00:01:41,289 --> 00:01:42,986
and the other one actually

00:01:42,986 --> 00:01:44,888
is slightly better than lock-free.

00:01:44,888 --> 00:01:47,189
It's going to be wait-free.

00:01:47,189 --> 00:01:51,218
And this is the speedup of the two programs,

00:01:51,218 --> 00:01:53,662
number of threads versus the speedup,

00:01:53,662 --> 00:01:55,989
meaning this is a genuine speedup,

00:01:55,989 --> 00:01:57,905
not CPU time divided by real time.

00:01:57,905 --> 00:01:59,967
This is really how much faster, in real time,

00:01:59,967 --> 00:02:01,467
the job gets done.

00:02:02,427 --> 00:02:03,260
So the

00:02:06,723 --> 00:02:08,473
lock-free program is,

00:02:10,460 --> 00:02:13,460
did everybody expect that to happen?

00:02:15,813 --> 00:02:17,789
Anybody didn't?

00:02:17,789 --> 00:02:21,460
Okay, you weren't in my last talk. (laughs)

00:02:21,460 --> 00:02:23,908
So this is the code.

00:02:23,908 --> 00:02:26,768
Let's see why this is happening.

00:02:26,768 --> 00:02:27,922
Okay, everybody saw what's happening.

00:02:27,922 --> 00:02:29,877
Pay attention to the legend.

00:02:29,877 --> 00:02:33,448
The wait-free program is way down on the bottom.

00:02:33,448 --> 00:02:35,314
It doesn't speed up at all.

00:02:35,314 --> 00:02:37,498
And the lock-free program, despite being locked,

00:02:37,498 --> 00:02:39,665
speeds up almost linearly.

00:02:40,566 --> 00:02:42,147
So this is the code.

00:02:42,147 --> 00:02:45,414
What the program actually does, it computes a sum.

00:02:45,414 --> 00:02:47,831
The wait-free program uses atomic sum

00:02:47,831 --> 00:02:50,608
and adds to it atomically.

00:02:50,608 --> 00:02:51,605
This works.

00:02:51,605 --> 00:02:54,366
There's no cheating; this actually works.

00:02:54,366 --> 00:02:58,766
And the one with the lock computes its portion

00:02:58,766 --> 00:03:01,704
of the sum using a local accumulator

00:03:01,704 --> 00:03:05,704
and then, under lock, adds it to the global sum.

00:03:07,519 --> 00:03:09,274
Well, I showed you speedup,

00:03:09,274 --> 00:03:12,228
but what about the actual real, just the runtime?

00:03:12,228 --> 00:03:13,651
The runtime is even worse.

00:03:13,651 --> 00:03:17,818
Not only the wait-free program actually scales worse,

00:03:19,617 --> 00:03:20,867
it runs slower.

00:03:23,321 --> 00:03:25,121
The main conclusion from that is

00:03:25,121 --> 00:03:27,892
before you do anything else, look to your algorithms.

00:03:27,892 --> 00:03:29,892
Algorithms rule supreme.

00:03:32,257 --> 00:03:34,512
This is a general conclusion,

00:03:34,512 --> 00:03:37,549
which I like to bring up because people delve

00:03:37,549 --> 00:03:41,344
into details of implementation way too soon.

00:03:41,344 --> 00:03:43,966
Your algorithm is going to dominate

00:03:43,966 --> 00:03:48,056
everything else you can do to the implementation.

00:03:48,056 --> 00:03:51,637
Assuming you picked the best algorithm,

00:03:51,637 --> 00:03:55,527
now we're into details of lock-free programming.

00:03:55,527 --> 00:03:58,214
And one thing we can see is wait-free

00:03:58,214 --> 00:04:01,280
actually has nothing to do with time.

00:04:01,280 --> 00:04:03,373
And atomic operations, by themselves,

00:04:03,373 --> 00:04:06,968
do not guarantee good performance.

00:04:06,968 --> 00:04:11,007
And you have to know what you're doing.

00:04:11,007 --> 00:04:13,171
There is no substitute for that,

00:04:13,171 --> 00:04:15,008
except there is this class.

00:04:15,008 --> 00:04:19,027
In one hour, you will be good for that.

00:04:19,027 --> 00:04:22,610
So now let's go and understand the atomics.

00:04:23,545 --> 00:04:26,830
So first of all, what in general is the atomic operation?

00:04:26,830 --> 00:04:29,953
It's an operation that executes as a single transaction.

00:04:29,953 --> 00:04:32,611
If you have other threads, they will see

00:04:32,611 --> 00:04:36,264
your state of the system as it was before the operation

00:04:36,264 --> 00:04:38,539
or as it was after the operation,

00:04:38,539 --> 00:04:42,779
but not in any intermediate, partially-completed state.

00:04:42,779 --> 00:04:44,875
In the context of C++, we're usually talking

00:04:44,875 --> 00:04:47,605
about atomics as low-level, individual operations

00:04:47,605 --> 00:04:49,766
where there is some hardware assist.

00:04:49,766 --> 00:04:51,902
But it's a more general concept.

00:04:51,902 --> 00:04:55,709
So databases may have atomic operations on them,

00:04:55,709 --> 00:04:59,799
which have thousands of instructions or more.

00:04:59,799 --> 00:05:03,758
But the clients are guaranteed to see the database

00:05:03,758 --> 00:05:06,264
either before the transaction or after the transaction

00:05:06,264 --> 00:05:07,871
but not during the transaction.

00:05:07,871 --> 00:05:10,145
So the concept of atomicity scales

00:05:10,145 --> 00:05:11,477
all the way from a single instruction

00:05:11,477 --> 00:05:13,227
to the whole program.

00:05:14,788 --> 00:05:19,731
So why didn't our increment work well with atomics?

00:05:19,731 --> 00:05:22,314
Well, let's first, before we...

00:05:25,251 --> 00:05:26,760
Is Scott Meyers here?

00:05:26,760 --> 00:05:28,479
Okay, I'll steal his quote, then.

00:05:28,479 --> 00:05:32,199
Before you learn to crawl on broken glass,

00:05:32,199 --> 00:05:34,691
let's learn to crawl.

00:05:34,691 --> 00:05:35,524
So

00:05:37,166 --> 00:05:39,479
before we learn atomics, let's just look

00:05:39,479 --> 00:05:44,451
at your basic, garden-variety increment on two threads.

00:05:44,451 --> 00:05:47,925
You have a variable, you increment it by one

00:05:47,925 --> 00:05:50,876
in two threads, and what happens?

00:05:50,876 --> 00:05:54,303
Well, increment is a read-modify-write operation.

00:05:54,303 --> 00:05:56,606
So you're reading on two threads from memory,

00:05:56,606 --> 00:06:01,063
you're adding one, you're writing the results to memory.

00:06:01,063 --> 00:06:02,303
That's a data race.

00:06:02,303 --> 00:06:04,708
That's undefined behavior,

00:06:04,708 --> 00:06:08,458
which means anything can happen, technically,

00:06:09,424 --> 00:06:13,484
such as the demons could fly out of your nose.

00:06:13,484 --> 00:06:17,097
In reality, something like this will happen.

00:06:17,097 --> 00:06:20,077
Both threads will read the value,

00:06:20,077 --> 00:06:22,510
both will increment, and both will write it.

00:06:22,510 --> 00:06:26,996
You will probably lose the result of one of the increments.

00:06:26,996 --> 00:06:28,628
Why?

00:06:28,628 --> 00:06:30,486
Well, we have to go back to the hardware.

00:06:30,486 --> 00:06:32,475
I'm not going to go back in as much details

00:06:32,475 --> 00:06:34,176
as Paul did in his talk.

00:06:34,176 --> 00:06:36,319
Fortunately, he did it before me.

00:06:36,319 --> 00:06:39,569
So you have your CPUs at the top level.

00:06:40,830 --> 00:06:42,393
You have memory way down here.

00:06:42,393 --> 00:06:43,754
Both CPUs access the memory,

00:06:43,754 --> 00:06:44,943
but they don't do it directly.

00:06:44,943 --> 00:06:49,788
There is these levels of caches between the CPUs.

00:06:49,788 --> 00:06:51,317
Let's say three levels.

00:06:51,317 --> 00:06:53,234
This is an X86 picture.

00:06:54,850 --> 00:06:59,456
The CPU itself directly interacts with the closest cache.

00:06:59,456 --> 00:07:02,787
The variable initially sits all the way down in memory,

00:07:02,787 --> 00:07:05,549
so how does it get from memory to the cache?

00:07:05,549 --> 00:07:08,375
Well, the caches fetch it from memory

00:07:08,375 --> 00:07:10,496
and then store it back.

00:07:10,496 --> 00:07:14,368
So if we don't have any synchronization whatsoever,

00:07:14,368 --> 00:07:16,501
each CPU reads the value from its cache.

00:07:16,501 --> 00:07:18,533
It doesn't know that it needs to do anything else.

00:07:18,533 --> 00:07:21,931
There is no synchronization here, after all.

00:07:21,931 --> 00:07:25,308
It does the increment, writes it back.

00:07:25,308 --> 00:07:28,268
Eventually the value will propagate back to main memory.

00:07:28,268 --> 00:07:29,885
Well, one of them will.

00:07:29,885 --> 00:07:31,906
But they both did increments without knowing

00:07:31,906 --> 00:07:33,495
anything about the other, so one of them

00:07:33,495 --> 00:07:36,151
just stomped on the other value.

00:07:36,151 --> 00:07:38,234
They both wrote back one.

00:07:40,637 --> 00:07:44,300
Well, one is only one possibility.

00:07:44,300 --> 00:07:48,168
If you have another register, another CPU,

00:07:48,168 --> 00:07:50,998
that one could see zero because it could read X

00:07:50,998 --> 00:07:53,695
before any of the incremented values

00:07:53,695 --> 00:07:56,500
trickle down from the cache.

00:07:56,500 --> 00:07:58,428
Now, on X86 we're spoiled.

00:07:58,428 --> 00:08:00,903
In general, nobody said that reads and writes are atomic,

00:08:00,903 --> 00:08:03,010
which means you could actually see anything.

00:08:03,010 --> 00:08:07,793
Undefined behavior really means undefined behavior.

00:08:07,793 --> 00:08:09,735
Okay, so now that I've scared you enough

00:08:09,735 --> 00:08:14,175
with this example, how do you safely access data

00:08:14,175 --> 00:08:16,826
from multiple threads?

00:08:16,826 --> 00:08:18,278
That's where atomics come in.

00:08:18,278 --> 00:08:22,116
Before C++11, life was easy.

00:08:22,116 --> 00:08:24,699
The answer was what's a thread?

00:08:25,570 --> 00:08:27,677
C++11 introduced threads, so it had to do

00:08:27,677 --> 00:08:28,964
something about it.

00:08:28,964 --> 00:08:31,547
And what it did was std atomic.

00:08:33,043 --> 00:08:36,543
So the syntax, it's a template-like thing.

00:08:37,921 --> 00:08:40,432
You give it a type, you make the type atomic,

00:08:40,432 --> 00:08:41,804
and you can initialize it.

00:08:41,804 --> 00:08:45,202
By the way, notice that it has to be direct initialization.

00:08:45,202 --> 00:08:47,938
That's just how they did it.

00:08:47,938 --> 00:08:50,551
So increment is now atomic.

00:08:50,551 --> 00:08:53,119
Just by wrapping int into std atomic,

00:08:53,119 --> 00:08:56,202
I made the plus plus operator atomic.

00:08:57,196 --> 00:09:00,389
So now if I do exactly the same code, this works.

00:09:00,389 --> 00:09:01,910
And the result is guaranteed to be two

00:09:01,910 --> 00:09:05,743
because both threads will see the final state,

00:09:07,033 --> 00:09:11,200
one of them will do the atomic first and will see zero.

00:09:12,043 --> 00:09:13,226
The other one does the atomic second,

00:09:13,226 --> 00:09:15,309
is guaranteed to see one.

00:09:17,092 --> 00:09:17,925
Sorry?

00:09:22,889 --> 00:09:25,056
Yeah, X is now atomic int.

00:09:27,000 --> 00:09:29,947
Okay, so what's really going on now

00:09:29,947 --> 00:09:32,959
in the hardware, that I changed my non-atomic int

00:09:32,959 --> 00:09:34,126
to atomic int?

00:09:35,021 --> 00:09:37,590
Well, each CPU does the increment

00:09:37,590 --> 00:09:39,328
as a single atomic operation.

00:09:39,328 --> 00:09:42,829
There is a read-modify-write atomic, no interruptions.

00:09:42,829 --> 00:09:45,823
Okay, what's really going on deeper?

00:09:45,823 --> 00:09:47,018
Well, what's really going on deeper

00:09:47,018 --> 00:09:51,185
is the core gets exclusive access onto the memory.

00:09:53,276 --> 00:09:56,834
The value trickles up and down through the caches

00:09:56,834 --> 00:09:57,917
if necessary.

00:09:59,534 --> 00:10:00,771
Now, that's a simplified picture.

00:10:00,771 --> 00:10:02,878
They may instead be talking to each other's caches.

00:10:02,878 --> 00:10:04,545
But that's the idea.

00:10:06,448 --> 00:10:08,889
Whether the hardware actually implements it

00:10:08,889 --> 00:10:10,673
in terms of communicating between different caches

00:10:10,673 --> 00:10:12,977
of different CPUs or through the main memory

00:10:12,977 --> 00:10:15,268
doesn't matter for our purposes.

00:10:15,268 --> 00:10:18,830
It will matter for exactly how slow it runs, but

00:10:18,830 --> 00:10:21,947
conceptually, you can't tell the difference.

00:10:21,947 --> 00:10:25,657
So this is really how that atomic operation is built up.

00:10:25,657 --> 00:10:28,366
And if there is another core accessing it,

00:10:28,366 --> 00:10:30,798
it's guaranteed to not see the intermediate state,

00:10:30,798 --> 00:10:33,631
as long as everybody does atomics.

00:10:34,751 --> 00:10:37,841
Okay, so which types can be made atomic?

00:10:37,841 --> 00:10:39,406
Which operations can be made atomic?

00:10:39,406 --> 00:10:41,112
What can you do with atomics?

00:10:41,112 --> 00:10:42,655
And how fast is atomic?

00:10:42,655 --> 00:10:45,786
Well, any trivially copyable type can be made atomic.

00:10:45,786 --> 00:10:47,686
Trivially copyable means you can copy it

00:10:47,686 --> 00:10:49,878
with memcpy, basically.

00:10:49,878 --> 00:10:51,795
So int, double is okay.

00:10:54,491 --> 00:10:57,301
That struct of two longs actually is also okay.

00:10:57,301 --> 00:11:01,041
It's trivially copyable, can be made atomic.

00:11:01,041 --> 00:11:03,574
Okay, what can you do with them?

00:11:03,574 --> 00:11:06,306
You can read and write them, assignment operator,

00:11:06,306 --> 00:11:07,818
copy constructor, always.

00:11:07,818 --> 00:11:10,373
There are some special atomic operations we'll go through.

00:11:10,373 --> 00:11:12,564
And depending on the type T of the atomic,

00:11:12,564 --> 00:11:15,897
there may be some additional operations.

00:11:17,332 --> 00:11:19,749
So here is an atomic integer,

00:11:20,641 --> 00:11:22,062
which is initialized with zero.

00:11:22,062 --> 00:11:25,085
And there is a bunch of things you can do to it.

00:11:25,085 --> 00:11:27,066
One of these is different from all the others.

00:11:27,066 --> 00:11:28,562
Which one?

00:11:28,562 --> 00:11:29,395
Anybody?

00:11:31,375 --> 00:11:32,208
(man talking quietly)

00:11:32,208 --> 00:11:33,042
Sorry?

00:11:33,042 --> 00:11:34,748
(man talking quietly)

00:11:34,748 --> 00:11:36,248
Okay, that one is,

00:11:40,018 --> 00:11:41,328
there is one more just like it.

00:11:41,328 --> 00:11:44,100
But there is one that is very different.

00:11:44,100 --> 00:11:45,001
Sorry?

00:11:45,001 --> 00:11:46,364
(man talking quietly)

00:11:46,364 --> 00:11:47,494
No.

00:11:47,494 --> 00:11:48,327
That one.

00:11:50,661 --> 00:11:52,911
There is no atomic multiply.

00:11:52,911 --> 00:11:56,411
So C++ wouldn't let you do this.

00:11:59,291 --> 00:12:01,484
There is atomic add, two lines above,

00:12:01,484 --> 00:12:04,637
but there is no atomic multiply in most hardware systems.

00:12:04,637 --> 00:12:07,168
So this actually will not compile.

00:12:07,168 --> 00:12:10,073
If it compiled, it would have to be done atomically,

00:12:10,073 --> 00:12:14,448
so C++ refuses to let you do that.

00:12:14,448 --> 00:12:16,608
Okay, with these two taken out,

00:12:16,608 --> 00:12:20,453
there are two more that are not like the others.

00:12:20,453 --> 00:12:23,552
And one of them was already named.

00:12:23,552 --> 00:12:24,719
And these two,

00:12:26,697 --> 00:12:28,062
they both compile.

00:12:28,062 --> 00:12:31,475
But they are not atomic, at least not in the sense

00:12:31,475 --> 00:12:35,640
that you would expect looking at the expression.

00:12:35,640 --> 00:12:36,473
Why?

00:12:37,358 --> 00:12:39,447
Well, they are atomic in the sense

00:12:39,447 --> 00:12:43,195
that each operation on the atomic variable is atomic.

00:12:43,195 --> 00:12:46,896
But look at what operations are actually being done

00:12:46,896 --> 00:12:48,813
on the atomic variable.

00:12:50,047 --> 00:12:51,577
The first one is atomic increment,

00:12:51,577 --> 00:12:52,823
the second one is atomic increment,

00:12:52,823 --> 00:12:54,844
the third one is atomic increment.

00:12:54,844 --> 00:12:56,941
There is atomic bit set, yes.

00:12:56,941 --> 00:13:00,916
It's in the standard, and it's in hardware.

00:13:00,916 --> 00:13:04,067
But when you get to the bottom, there is an atomic read,

00:13:04,067 --> 00:13:06,375
and then there is an atomic write.

00:13:06,375 --> 00:13:07,823
But they're separate transactions.

00:13:07,823 --> 00:13:10,695
So we do atomic read from X,

00:13:10,695 --> 00:13:12,689
store it in the register, add one to it,

00:13:12,689 --> 00:13:14,965
do atomic write of the result.

00:13:14,965 --> 00:13:18,135
Any other thread can come in between the read and the write

00:13:18,135 --> 00:13:21,665
and modify the X, cause there are two atomic operations

00:13:21,665 --> 00:13:23,755
with a big gap between them.

00:13:23,755 --> 00:13:26,494
There is no synchronization, no exclusivity

00:13:26,494 --> 00:13:27,511
during this big gap.

00:13:27,511 --> 00:13:30,055
There is a read, and it goes onto the register,

00:13:30,055 --> 00:13:31,807
something happens to it, something happens to it,

00:13:31,807 --> 00:13:33,060
and then there is a write.

00:13:33,060 --> 00:13:35,262
The read is atomic, sure.

00:13:35,262 --> 00:13:36,342
The write is atomic.

00:13:36,342 --> 00:13:38,829
Nobody will see, you know, half.

00:13:38,829 --> 00:13:40,043
It's either zero or one.

00:13:40,043 --> 00:13:42,075
In this case it's a one or two.

00:13:42,075 --> 00:13:43,408
But nobody will,

00:13:48,296 --> 00:13:50,105
anybody can change it to seven in the middle,

00:13:50,105 --> 00:13:53,400
and then your write just writes it back to one.

00:13:53,400 --> 00:13:57,465
So there is this catch with the overloaded operators

00:13:57,465 --> 00:14:00,187
that you should be aware of.

00:14:00,187 --> 00:14:01,687
For integer types,

00:14:03,848 --> 00:14:06,594
std atomic provides overload operators

00:14:06,594 --> 00:14:08,999
for all the math operations.

00:14:08,999 --> 00:14:09,832
Well, not all.

00:14:09,832 --> 00:14:12,351
You can't do multiply.

00:14:12,351 --> 00:14:16,825
It doesn't compile if the operation doesn't support it,

00:14:16,825 --> 00:14:18,308
which is a good thing.

00:14:18,308 --> 00:14:22,308
It will compile if the expression is not atomic,

00:14:23,632 --> 00:14:25,699
which is not necessarily a good thing.

00:14:25,699 --> 00:14:28,616
It's pretty easy to make mistakes.

00:14:28,616 --> 00:14:30,223
So the three increments, plus plus,

00:14:30,223 --> 00:14:33,201
plus equal, and the X equal X plus one,

00:14:33,201 --> 00:14:35,235
all mean the same thing,

00:14:35,235 --> 00:14:39,588
except if the variable is atomic, then they don't.

00:14:39,588 --> 00:14:41,335
Now, the compiler knows the difference.

00:14:41,335 --> 00:14:44,535
The compiler will not optimize plus plus X

00:14:44,535 --> 00:14:46,924
as X equal X plus one.

00:14:46,924 --> 00:14:49,189
Do you always know the difference?

00:14:49,189 --> 00:14:51,522
So there is slightly danger.

00:14:54,464 --> 00:14:56,547
Okay, so we covered that.

00:14:59,259 --> 00:15:01,342
Oh, it's going backwards.

00:15:04,896 --> 00:15:07,926
Now, what else can you do with atomic types?

00:15:07,926 --> 00:15:10,533
You can do assignments, reads, and writes

00:15:10,533 --> 00:15:13,595
for all types that you can put into atomic,

00:15:13,595 --> 00:15:15,749
both built-in and user-defined.

00:15:15,749 --> 00:15:18,743
For raw pointers, you can do increment and decrement.

00:15:18,743 --> 00:15:21,350
For integers, you can do increment,

00:15:21,350 --> 00:15:24,932
decrement, and bitwise operations.

00:15:24,932 --> 00:15:27,590
Atomic of bool will compile.

00:15:27,590 --> 00:15:29,650
There are no special operations for it.

00:15:29,650 --> 00:15:31,309
So it reads and writes.

00:15:31,309 --> 00:15:33,920
Atomic of double will compile,

00:15:33,920 --> 00:15:36,274
reads and writes, no atomic increments

00:15:36,274 --> 00:15:39,441
or multiplies or anything of the sort.

00:15:43,577 --> 00:15:47,130
Okay, in addition to overloaded operators,

00:15:47,130 --> 00:15:49,401
it has a lot of member functions.

00:15:49,401 --> 00:15:52,381
There is load and store, which is the same as assignment,

00:15:52,381 --> 00:15:53,917
but it's explicit.

00:15:53,917 --> 00:15:56,256
Here is a new one, exchange.

00:15:56,256 --> 00:15:58,475
Exchange is an atomic swap.

00:15:58,475 --> 00:16:01,114
It's a read-modify-write done atomically.

00:16:01,114 --> 00:16:05,151
Both reads the old value, stores in the new value,

00:16:05,151 --> 00:16:09,318
and guarantees that nobody can get a word in in between.

00:16:12,233 --> 00:16:16,400
Compare-and-swap, which is a conditional exchange.

00:16:18,403 --> 00:16:21,093
So you can say compare-exchange,

00:16:21,093 --> 00:16:23,625
and ignore the strong for a moment.

00:16:23,625 --> 00:16:26,639
You give it two values: what you expect it to be,

00:16:26,639 --> 00:16:30,556
that's Y, and what you want it to be, that's Z.

00:16:31,647 --> 00:16:35,393
If it's not what you expect it to be, nothing happens.

00:16:35,393 --> 00:16:36,614
It returns false.

00:16:36,614 --> 00:16:39,566
If it is what you expect it to be,

00:16:39,566 --> 00:16:42,019
it's replaced by what you want it to be.

00:16:42,019 --> 00:16:46,996
So if X equals to Y, X becomes Z, true is returned.

00:16:46,996 --> 00:16:51,204
Otherwise, you get back the current value of X.

00:16:51,204 --> 00:16:53,110
Note that you get it through the first parameter.

00:16:53,110 --> 00:16:57,526
The first parameter is a non-const reference.

00:16:57,526 --> 00:16:59,869
Returns false; X doesn't change.

00:16:59,869 --> 00:17:01,641
This little operation is the key

00:17:01,641 --> 00:17:04,193
to most lock-free algorithms.

00:17:04,193 --> 00:17:05,665
What's that strong thing?

00:17:05,665 --> 00:17:06,498
Hold that thought.

00:17:06,498 --> 00:17:07,998
We'll get to that.

00:17:09,043 --> 00:17:10,841
So why is this compare-and-swap

00:17:10,841 --> 00:17:13,295
key to most lock-free algorithms?

00:17:13,295 --> 00:17:14,712
Well, here is how

00:17:17,160 --> 00:17:19,254
pretty much every lock-free algorithm

00:17:19,254 --> 00:17:23,822
is centered around this loop here that you see

00:17:23,822 --> 00:17:26,739
right here in the second paragraph.

00:17:28,353 --> 00:17:29,186
So

00:17:30,618 --> 00:17:32,589
we want to increment X.

00:17:32,589 --> 00:17:33,950
Of course, we have an increment for that,

00:17:33,950 --> 00:17:36,257
but we could also do a multiply,

00:17:36,257 --> 00:17:38,395
for which we don't have an atomic operation.

00:17:38,395 --> 00:17:41,895
So we say, I expect X to not have changed.

00:17:42,729 --> 00:17:45,208
So first of all, I'll read the atomic value,

00:17:45,208 --> 00:17:47,563
store it in a local X0.

00:17:47,563 --> 00:17:49,687
I hope nobody got to X before me.

00:17:49,687 --> 00:17:51,029
X hasn't changed.

00:17:51,029 --> 00:17:54,141
If that's true, I'm going to change it

00:17:54,141 --> 00:17:56,119
atomically to the new value that I want to be.

00:17:56,119 --> 00:17:59,551
That could be incremented or multiplied.

00:17:59,551 --> 00:18:01,968
And if nobody else changed X,

00:18:03,009 --> 00:18:05,621
then I did my increment atomically.

00:18:05,621 --> 00:18:10,421
If somebody did change X, compare-exchange fails,

00:18:10,421 --> 00:18:13,171
returns the new value into, whoa.

00:18:16,905 --> 00:18:18,056
Something wasn't atomic.

00:18:18,056 --> 00:18:20,306
(laughing)

00:18:22,934 --> 00:18:25,355
Returns the new value into X0

00:18:25,355 --> 00:18:29,220
so I don't have to do the read again.

00:18:29,220 --> 00:18:30,954
And I go in the next iteration of the loop.

00:18:30,954 --> 00:18:32,886
And I keep trying until my compare-and-swap

00:18:32,886 --> 00:18:35,249
beats everybody else's compare-and-swap

00:18:35,249 --> 00:18:37,499
and gets that increment in.

00:18:38,534 --> 00:18:40,617
Lock-free, not wait-free.

00:18:41,781 --> 00:18:45,727
So that's what's special about compare-and-swap.

00:18:45,727 --> 00:18:47,684
Some lock-free algorithms can be done

00:18:47,684 --> 00:18:49,931
with simpler instructions, but this one

00:18:49,931 --> 00:18:52,098
allows you to do anything.

00:18:53,936 --> 00:18:54,769
Okay.

00:18:55,665 --> 00:19:00,566
In addition to the operator overloads like plus equals,

00:19:00,566 --> 00:19:01,930
there are member functions.

00:19:01,930 --> 00:19:04,975
Fetch-add is the equivalent of plus equal.

00:19:04,975 --> 00:19:09,142
Now you can do, if you just call fetch-add, it adds.

00:19:10,154 --> 00:19:11,920
What's that fetch part?

00:19:11,920 --> 00:19:14,848
Well, it doesn't just add atomically.

00:19:14,848 --> 00:19:16,976
So it increments atomically,

00:19:16,976 --> 00:19:18,186
which is a read-modify-write,

00:19:18,186 --> 00:19:19,990
but it also returns you the old value.

00:19:19,990 --> 00:19:21,405
That's the fetch.

00:19:21,405 --> 00:19:24,322
So it returns you the old value

00:19:24,322 --> 00:19:28,072
and adds the increment, all of it atomically.

00:19:29,919 --> 00:19:31,269
Well, actually, it returns the new value.

00:19:31,269 --> 00:19:33,961
If you want the old value, it's,

00:19:33,961 --> 00:19:35,012
I'm sorry, it returns the old value.

00:19:35,012 --> 00:19:37,836
So yeah, if you want the new value,

00:19:37,836 --> 00:19:38,847
you have to add it yourself.

00:19:38,847 --> 00:19:39,867
But you know what it would be.

00:19:39,867 --> 00:19:41,440
So yeah, it returns the old value.

00:19:41,440 --> 00:19:44,607
Same thing for subtract and the bitwise,

00:19:44,607 --> 00:19:45,690
and, or, xor.

00:19:47,780 --> 00:19:49,742
This is obviously more verbose

00:19:49,742 --> 00:19:51,066
than your overloaded operators,

00:19:51,066 --> 00:19:53,235
but it's also less error-prone.

00:19:53,235 --> 00:19:55,922
You're less likely to write a complex expression

00:19:55,922 --> 00:19:58,653
and not notice that you're breaking up

00:19:58,653 --> 00:20:00,315
an individual atomic operation,

00:20:00,315 --> 00:20:01,492
that the expression is not atomic.

00:20:01,492 --> 00:20:03,493
If you have multiple of these function calls,

00:20:03,493 --> 00:20:04,404
you hopefully would see that you

00:20:04,404 --> 00:20:06,590
have multiple function calls, each one is atomic,

00:20:06,590 --> 00:20:08,340
the composite is not.

00:20:11,313 --> 00:20:15,396
So overloaded operators make writing code easier.

00:20:16,238 --> 00:20:20,405
Unfortunately, they also make making mistakes easier.

00:20:22,481 --> 00:20:25,958
As a programmer, you speak to two different audiences.

00:20:25,958 --> 00:20:27,476
You speak to the machines,

00:20:27,476 --> 00:20:31,089
and you speak to other programmers.

00:20:31,089 --> 00:20:34,410
The compilers always understand what you mean.

00:20:34,410 --> 00:20:36,091
It's not necessarily what you meant to say,

00:20:36,091 --> 00:20:39,176
but they always understand what you actually said.

00:20:39,176 --> 00:20:40,843
Speaking to other programmers,

00:20:40,843 --> 00:20:43,426
that's the more difficult part,

00:20:44,459 --> 00:20:47,704
in the well that makes you understood.

00:20:47,704 --> 00:20:50,371
So we're going to focus on that.

00:20:51,221 --> 00:20:54,275
The member functions help because they call attention,

00:20:54,275 --> 00:20:57,483
something weird is going on here.

00:20:57,483 --> 00:21:01,878
They highlight what's being done atomically.

00:21:01,878 --> 00:21:03,587
They may be longer to write,

00:21:03,587 --> 00:21:07,754
but they focus attention of the reader at the right place.

00:21:09,249 --> 00:21:10,220
And the programmers tend to read

00:21:10,220 --> 00:21:11,223
what they think it going on,

00:21:11,223 --> 00:21:12,488
not what's really going on.

00:21:12,488 --> 00:21:14,383
X equal X plus one, I know what's going on.

00:21:14,383 --> 00:21:16,941
It increments by one.

00:21:16,941 --> 00:21:17,774
Well, no.

00:21:20,783 --> 00:21:23,702
Okay, now, before we talk to the programmers,

00:21:23,702 --> 00:21:25,594
the whole point of this is doing things fast.

00:21:25,594 --> 00:21:28,601
So how fast are atomic operations?

00:21:28,601 --> 00:21:30,155
How fast are atomic operations?

00:21:30,155 --> 00:21:33,599
The first rule of performance is what?

00:21:33,599 --> 00:21:35,527
Always measure the performance.

00:21:35,527 --> 00:21:38,668
There are two caveats that come in with atomics.

00:21:38,668 --> 00:21:40,721
You should still always measure the performance,

00:21:40,721 --> 00:21:43,911
but you should be careful when deducing

00:21:43,911 --> 00:21:46,646
from the measurements what the hardware

00:21:46,646 --> 00:21:49,068
is actually doing and what it's supposed to be doing,

00:21:49,068 --> 00:21:52,848
because not all hardware implements all of the atomics

00:21:52,848 --> 00:21:54,418
the same way.

00:21:54,418 --> 00:21:56,720
So you can get some results that, for example,

00:21:56,720 --> 00:21:58,228
tell you that this atomic operation

00:21:58,228 --> 00:22:00,616
and that atomic operation run at exactly the same speed.

00:22:00,616 --> 00:22:04,575
That may be the artifact of this particular hardware.

00:22:04,575 --> 00:22:06,652
And we'll see some of that.

00:22:06,652 --> 00:22:10,446
Also, if you're comparing atomics with non-atomics,

00:22:10,446 --> 00:22:11,916
you're not really comparing the same thing.

00:22:11,916 --> 00:22:14,665
Atomic operation does more.

00:22:14,665 --> 00:22:16,228
You wouldn't use atomic if you could get away

00:22:16,228 --> 00:22:17,555
with non-atomic.

00:22:17,555 --> 00:22:21,490
That being said, let's see what the cost is.

00:22:21,490 --> 00:22:24,792
Okay, who expects atomic operations to be a lot slower?

00:22:24,792 --> 00:22:26,010
By show of hands?

00:22:26,010 --> 00:22:29,726
Who expects to be about the same speed?

00:22:29,726 --> 00:22:31,220
All right, here it is.

00:22:31,220 --> 00:22:32,137
That's X86.

00:22:34,788 --> 00:22:37,100
At the top of the line, there is a read and atomic read,

00:22:37,100 --> 00:22:40,183
and atomic read is marginally slower.

00:22:41,682 --> 00:22:44,300
The next one is atomic write and, I mean,

00:22:44,300 --> 00:22:47,321
non-atomic write, and atomic write is way down there.

00:22:47,321 --> 00:22:51,460
And the light blue in the middle is non-atomic increment,

00:22:51,460 --> 00:22:56,184
and atomic increment is way down there with atomic write.

00:22:56,184 --> 00:22:59,052
Now, as I said, it's not entirely fair

00:22:59,052 --> 00:23:00,618
because if I could use non-atomic increment,

00:23:00,618 --> 00:23:03,962
I wouldn't use atomic increment, right?

00:23:03,962 --> 00:23:05,379
So let's compare.

00:23:09,906 --> 00:23:12,807
Let's now compare with a lock.

00:23:12,807 --> 00:23:14,801
Atomics and locks do generally the same thing.

00:23:14,801 --> 00:23:17,647
They provide a thread-safe way of doing these operations.

00:23:17,647 --> 00:23:19,681
Let's focus on increment.

00:23:19,681 --> 00:23:23,598
So here is atomic increment, and here is mutex.

00:23:26,717 --> 00:23:29,760
That's a normal C++ mutex.

00:23:29,760 --> 00:23:32,677
Quite a bit slower, so that's good.

00:23:36,683 --> 00:23:39,580
There are more to locks than there are mutexes.

00:23:39,580 --> 00:23:42,322
I didn't say which lock.

00:23:42,322 --> 00:23:45,266
So let me add another lock, which is a spin lock

00:23:45,266 --> 00:23:48,599
that I wrote and tuned for this machine.

00:23:51,991 --> 00:23:56,102
Spin lock and atomic run at practically the same speed.

00:23:56,102 --> 00:23:58,035
This machine, what's this machine,

00:23:58,035 --> 00:24:00,446
and why am I going to 128 threads?

00:24:00,446 --> 00:24:03,529
Well, it's 120-core Broadwell server.

00:24:04,757 --> 00:24:08,391
At this point, anybody wants to say something?

00:24:08,391 --> 00:24:12,110
I know somebody wants to say something.

00:24:12,110 --> 00:24:13,346
Come on, don't be shy?

00:24:13,346 --> 00:24:16,709
Anybody want to accuse me of cheating?

00:24:16,709 --> 00:24:19,064
Okay, Fedor, you're cheating!

00:24:19,064 --> 00:24:20,814
Nobody has 120 cores.

00:24:24,212 --> 00:24:26,481
Everybody knows that spin lock is a lot slower

00:24:26,481 --> 00:24:29,865
if you run more threads than you have cores.

00:24:29,865 --> 00:24:31,615
Everybody knows that?

00:24:34,746 --> 00:24:38,139
Okay, this was done on my desktop,

00:24:38,139 --> 00:24:38,972
at home.

00:24:52,696 --> 00:24:55,613
Well, anybody doesn't believe that?

00:24:59,531 --> 00:25:00,645
What's wrong with this audience.

00:25:00,645 --> 00:25:02,645
Everybody believes that.

00:25:06,693 --> 00:25:10,104
Maybe four cores are still too many.

00:25:10,104 --> 00:25:12,187
This laptop only has two.

00:25:17,365 --> 00:25:19,509
I'm going to run that spin lock test.

00:25:19,509 --> 00:25:22,269
This is a Google benchmark, runs the increment

00:25:22,269 --> 00:25:23,936
of a 64-bit integer,

00:25:25,809 --> 00:25:30,442
up to 1024 threads, with atomic and with a spin lock.

00:25:30,442 --> 00:25:32,071
The number on the right is how many increments

00:25:32,071 --> 00:25:34,238
per second it can process.

00:25:37,642 --> 00:25:40,645
Starts off a little slower, but spin lock catches up

00:25:40,645 --> 00:25:43,012
at large number of threads.

00:25:43,012 --> 00:25:45,179
So this is pretty generic.

00:25:52,099 --> 00:25:55,822
Now, this is just in the benchmark configuration.

00:25:55,822 --> 00:25:57,943
In your application, you would see variations,

00:25:57,943 --> 00:26:01,139
depending on what else is going on with the cache,

00:26:01,139 --> 00:26:02,632
what else is going on the with memory.

00:26:02,632 --> 00:26:04,552
Spin lock may be faster, may be slower.

00:26:04,552 --> 00:26:07,144
You have to measure in context.

00:26:07,144 --> 00:26:10,727
But basically, don't be sure that lock-free

00:26:12,453 --> 00:26:14,812
is necessarily faster.

00:26:14,812 --> 00:26:16,612
Remember we talked about compare-and-swap.

00:26:16,612 --> 00:26:18,670
So I can do increment with compare-and-swap.

00:26:18,670 --> 00:26:19,752
I've shown you on one of the slides

00:26:19,752 --> 00:26:21,902
how to increment with compare-and-swap.

00:26:21,902 --> 00:26:22,955
Compare-and-swap would go somewhere

00:26:22,955 --> 00:26:26,205
between the mutex and atomic increment.

00:26:32,446 --> 00:26:34,702
In addition to the fact that atomics

00:26:34,702 --> 00:26:37,891
aren't necessarily the fastest thing in the universe,

00:26:37,891 --> 00:26:39,991
there is a huge secret they're hiding.

00:26:39,991 --> 00:26:43,212
They aren't even always lock-free.

00:26:43,212 --> 00:26:47,045
So here is an atomic type, three atomic types.

00:26:49,610 --> 00:26:53,294
It's a struct with one long, two longs, and three longs.

00:26:53,294 --> 00:26:55,360
And because they're all trivially copyable,

00:26:55,360 --> 00:26:58,193
I can put them all into an atomic.

00:26:59,113 --> 00:27:01,863
Which ones of them are lock-free?

00:27:04,291 --> 00:27:05,541
A is lock-free.

00:27:08,650 --> 00:27:10,658
(man talking quietly)

00:27:10,658 --> 00:27:11,667
B might be okay.

00:27:11,667 --> 00:27:14,058
X86 is the platform, let's say.

00:27:14,058 --> 00:27:16,658
I'm compiling for X86.

00:27:16,658 --> 00:27:17,741
Okay, yes, so

00:27:20,523 --> 00:27:22,992
A is definitely lock-free.

00:27:22,992 --> 00:27:24,843
C is definitely not.

00:27:24,843 --> 00:27:25,926
And B may be.

00:27:28,515 --> 00:27:30,432
Fortunately, there is a way to find out.

00:27:30,432 --> 00:27:34,206
And there is a member function, is-lock-free,

00:27:34,206 --> 00:27:35,442
that you can call.

00:27:35,442 --> 00:27:37,321
And it, at runtime, will tell you

00:27:37,321 --> 00:27:39,933
if this variable is lock-free or not.

00:27:39,933 --> 00:27:41,317
Why at runtime?

00:27:41,317 --> 00:27:43,618
Why not at compile time?

00:27:43,618 --> 00:27:47,752
Well, C++17 adds a compile time function

00:27:47,752 --> 00:27:49,872
that is called is-always-lock-free,

00:27:49,872 --> 00:27:51,209
but it's a subset.

00:27:51,209 --> 00:27:53,658
So you could get is-always-lock-free false,

00:27:53,658 --> 00:27:56,202
which means sometimes it is and sometimes it isn't.

00:27:56,202 --> 00:27:57,631
And at runtime, it will tell you

00:27:57,631 --> 00:28:01,079
for this particular variable if it is or it isn't lock-free.

00:28:01,079 --> 00:28:04,209
And the reason is mostly alignment.

00:28:04,209 --> 00:28:06,126
Depending on alignment,

00:28:07,589 --> 00:28:10,045
the variable may or may not be lock-free,

00:28:10,045 --> 00:28:12,848
cause some platforms have alignment requirements

00:28:12,848 --> 00:28:13,696
for atomics.

00:28:13,696 --> 00:28:17,279
So if you have a, for example, 16-byte long

00:28:18,903 --> 00:28:21,963
like this struct B, it will only be atomic

00:28:21,963 --> 00:28:24,296
if it has 16-byte alignment.

00:28:26,199 --> 00:28:29,616
So on X86, the two longs is indeed atomic

00:28:31,117 --> 00:28:32,518
if it has 16-byte alignment,

00:28:32,518 --> 00:28:36,423
because the move into mmx register is atomic.

00:28:36,423 --> 00:28:37,781
And if you look at the assembly code

00:28:37,781 --> 00:28:39,082
generated by the compiler, you'll see

00:28:39,082 --> 00:28:40,487
that that's what the compiler does

00:28:40,487 --> 00:28:43,654
if you ask it to load that atomically.

00:28:44,844 --> 00:28:46,657
The last one is greater than 16 bytes.

00:28:46,657 --> 00:28:49,703
There are no atomic instructions on X86,

00:28:49,703 --> 00:28:51,615
at least on the processor that I used,

00:28:51,615 --> 00:28:54,569
and is-lock-free returns false.

00:28:54,569 --> 00:28:57,175
What about the middle two, the C and D?

00:28:57,175 --> 00:28:59,092
They are the same size.

00:29:00,131 --> 00:29:01,464
Are they atomic?

00:29:04,717 --> 00:29:08,183
Who thinks that C and D are atomic?

00:29:08,183 --> 00:29:09,264
Lock-free, I mean.

00:29:09,264 --> 00:29:10,586
They can be both atomic.

00:29:10,586 --> 00:29:13,336
Who thinks C and D are lock-free?

00:29:14,371 --> 00:29:17,278
Who thinks they're not lock-free?

00:29:17,278 --> 00:29:19,356
Who thinks it's a trick question?

00:29:19,356 --> 00:29:22,142
(laughs)

00:29:22,142 --> 00:29:26,309
C is actually lock-free because it's not really 12 bytes.

00:29:27,479 --> 00:29:29,002
It's 16.

00:29:29,002 --> 00:29:32,134
And the same atomic move to mmx handles it.

00:29:32,134 --> 00:29:35,884
D is indeed 12 bytes, and it's not lock-free.

00:29:39,493 --> 00:29:42,993
Padding matters, in addition to alignment.

00:29:43,874 --> 00:29:46,957
Now, atomic operations are lock-free,

00:29:49,076 --> 00:29:51,404
maybe when wait-free.

00:29:51,404 --> 00:29:54,807
It doesn't mean they don't wait on each other.

00:29:54,807 --> 00:29:58,974
So let's compare atomic increment of an atomic variable

00:30:00,484 --> 00:30:05,221
versus incrementing two atomic variables that are different.

00:30:05,221 --> 00:30:06,964
So one thread has its own, the other thread has its own.

00:30:06,964 --> 00:30:09,446
This way they don't have to wait on each other.

00:30:09,446 --> 00:30:13,341
And let's run the benchmark for different number of threads.

00:30:13,341 --> 00:30:16,351
And it's basically the same thing.

00:30:16,351 --> 00:30:19,865
There is a slight blip at over 16 threads,

00:30:19,865 --> 00:30:22,948
but basically there is no difference.

00:30:25,124 --> 00:30:27,236
Can we conclude that atomic operations

00:30:27,236 --> 00:30:30,153
don't wait on each other from this?

00:30:32,733 --> 00:30:34,066
Not necessarily.

00:30:34,912 --> 00:30:39,119
What's going on here is what's called cache line sharing.

00:30:39,119 --> 00:30:42,512
So the two operations are in the same cache line.

00:30:42,512 --> 00:30:44,369
When I said that the variable trickles up and down

00:30:44,369 --> 00:30:46,418
through caches from main memory

00:30:46,418 --> 00:30:49,898
to the CPU and back, I lied to you.

00:30:49,898 --> 00:30:51,490
It's not the variable that trickles up and down.

00:30:51,490 --> 00:30:54,363
It's the whole cache line on X86, 64 bytes.

00:30:54,363 --> 00:30:56,380
You want one variable from that cache line,

00:30:56,380 --> 00:30:59,270
the entire 64-byte chunk will go up

00:30:59,270 --> 00:31:01,910
through the cache and down.

00:31:01,910 --> 00:31:05,719
And if two different CPUs want two different variables

00:31:05,719 --> 00:31:07,804
that are within the same 64 bytes,

00:31:07,804 --> 00:31:10,982
they have to wait as if it was the same variable

00:31:10,982 --> 00:31:13,298
because you don't get a lower granularity

00:31:13,298 --> 00:31:16,948
on cache access than 64 bytes on X86, again.

00:31:16,948 --> 00:31:21,115
So exclusive access is granted to this entire cache line.

00:31:23,213 --> 00:31:27,025
And that's the reason, remember this graph

00:31:27,025 --> 00:31:30,042
from the beginning, the wait-free program that didn't scale,

00:31:30,042 --> 00:31:32,422
well, because they do wait on each other.

00:31:32,422 --> 00:31:35,307
So if I put them on separate cache lines,

00:31:35,307 --> 00:31:36,499
then there is no problem.

00:31:36,499 --> 00:31:39,221
Then they don't have to wait on anything.

00:31:39,221 --> 00:31:40,676
So atomic operations wait on each other,

00:31:40,676 --> 00:31:42,217
in particular the writes.

00:31:42,217 --> 00:31:43,550
The reads don't.

00:31:47,567 --> 00:31:48,734
So here it is.

00:31:51,145 --> 00:31:53,171
I can prove it with a benchmark.

00:31:53,171 --> 00:31:56,997
The top one, the one that scales, is truly not shared.

00:31:56,997 --> 00:31:59,887
These two variables are now 64 bytes apart or more.

00:31:59,887 --> 00:32:01,985
They're on different cache lines.

00:32:01,985 --> 00:32:04,852
And the two ones on the bottom,

00:32:04,852 --> 00:32:07,000
one of them is truly shared, meaning I'm hammering

00:32:07,000 --> 00:32:09,315
at the same atomic variable, and the other one

00:32:09,315 --> 00:32:13,049
is false shared, meaning they're different atomic variables,

00:32:13,049 --> 00:32:16,063
but they are within the same cache line.

00:32:16,063 --> 00:32:19,751
Now, why there is a blip at 16 threads,

00:32:19,751 --> 00:32:21,936
because at 16 threads I run out of cache line,

00:32:21,936 --> 00:32:23,009
I run out of 64 bytes.

00:32:23,009 --> 00:32:27,320
I can't give eight-byte variable per thread

00:32:27,320 --> 00:32:30,320
anymore to each thread, so there is,

00:32:31,728 --> 00:32:33,437
some of them are false shared and some of them

00:32:33,437 --> 00:32:34,894
are actually not shared at all.

00:32:34,894 --> 00:32:37,811
So that's why it starts to pick up.

00:32:39,276 --> 00:32:42,145
So atomic operations do wait on a cache line access.

00:32:42,145 --> 00:32:46,312
And that's what you pay for data sharing without races.

00:32:47,173 --> 00:32:50,727
If you don't need to access shared data,

00:32:50,727 --> 00:32:55,062
make sure you're not accessing the same cache line.

00:32:55,062 --> 00:32:58,768
On more complex hardware, it may be more than that.

00:32:58,768 --> 00:33:02,351
May be even memory pages on newer machines.

00:33:05,489 --> 00:33:06,322
Okay.

00:33:08,263 --> 00:33:12,096
We did see the compare-exchange-strong before.

00:33:12,935 --> 00:33:15,189
You know, there is also in C++ standard

00:33:15,189 --> 00:33:16,592
compare-exchange-weak.

00:33:16,592 --> 00:33:19,494
And the standard says that compare-exchange-weak

00:33:19,494 --> 00:33:23,297
can spuriously fail, and even when the current value

00:33:23,297 --> 00:33:25,230
is the same as the expected value,

00:33:25,230 --> 00:33:27,503
it can still return false

00:33:27,503 --> 00:33:29,836
and fail to do the exchange.

00:33:31,552 --> 00:33:34,034
Everybody knows the difference?

00:33:34,034 --> 00:33:36,568
Okay, who thinks they know why there are two

00:33:36,568 --> 00:33:38,818
and what the difference is?

00:33:40,165 --> 00:33:42,267
Well, yes, it depends on hardware.

00:33:42,267 --> 00:33:44,783
And the most common explanation that I read

00:33:44,783 --> 00:33:46,783
is something along the lines of

00:33:46,783 --> 00:33:48,139
you have these different cores,

00:33:48,139 --> 00:33:49,569
and they have to communicate to each other,

00:33:49,569 --> 00:33:51,606
and it's kind of like timing out on a socket.

00:33:51,606 --> 00:33:53,936
You wait, and if you couldn't get the answer

00:33:53,936 --> 00:33:56,443
from the other core in some timeout,

00:33:56,443 --> 00:33:58,276
you just return false.

00:33:59,851 --> 00:34:03,601
Okay, is that consistent with what you heard?

00:34:06,162 --> 00:34:09,769
And I read that, and I thought I understood it.

00:34:09,769 --> 00:34:10,937
And then I thought about it some more,

00:34:10,937 --> 00:34:14,057
and I realized I don't understand it.

00:34:14,057 --> 00:34:14,890
So

00:34:16,723 --> 00:34:20,067
what does it return when the failure happens,

00:34:20,067 --> 00:34:21,476
when the false failure happens?

00:34:21,476 --> 00:34:24,809
Well, it returns the current value of X.

00:34:26,123 --> 00:34:28,675
It still has to return the correct current value of X

00:34:28,675 --> 00:34:29,898
because that's what you're going to start

00:34:29,898 --> 00:34:31,951
on the next iteration.

00:34:31,951 --> 00:34:34,974
It's actually going to return

00:34:34,974 --> 00:34:36,740
the value that you expect, because that's the definition

00:34:36,740 --> 00:34:37,841
of the false failure.

00:34:37,841 --> 00:34:40,333
The value was there, the expected value,

00:34:40,333 --> 00:34:42,092
but it didn't do the swap.

00:34:42,092 --> 00:34:44,098
That's what weak means.

00:34:44,098 --> 00:34:48,612
So it knew that the current value was the expected value.

00:34:48,612 --> 00:34:50,649
If it knew that, why didn't it do swap?

00:34:50,649 --> 00:34:52,182
And if it wasn't sure that the current value

00:34:52,182 --> 00:34:54,629
was the expected value, how can it return

00:34:54,629 --> 00:34:55,796
the value that

00:34:58,061 --> 00:35:00,104
may not be there, cause what are you going to do

00:35:00,104 --> 00:35:02,482
on the next iteration?

00:35:02,482 --> 00:35:05,649
So I had to do some more figuring out.

00:35:07,824 --> 00:35:09,741
Is it worth explaining?

00:35:10,687 --> 00:35:13,770
Okay, so this is how compare-and-swap

00:35:15,338 --> 00:35:18,070
might be done in hardware.

00:35:18,070 --> 00:35:20,240
This is pseudocode.

00:35:20,240 --> 00:35:21,073
So

00:35:22,181 --> 00:35:23,679
the lock here means we get some sort

00:35:23,679 --> 00:35:26,335
of exclusive access, whatever that means in that hardware.

00:35:26,335 --> 00:35:28,805
So we get the exclusive access to the variable,

00:35:28,805 --> 00:35:30,390
we read the value.

00:35:30,390 --> 00:35:33,754
If it's the expected value, we swap it and return true.

00:35:33,754 --> 00:35:37,032
If it's not the expected value, we just return false.

00:35:37,032 --> 00:35:37,865
Okay.

00:35:39,312 --> 00:35:42,220
Now, remember our benchmark.

00:35:42,220 --> 00:35:44,069
Reading shared variables is a lot faster

00:35:44,069 --> 00:35:45,922
than writing into them.

00:35:45,922 --> 00:35:48,943
So we could do, at the hardware level,

00:35:48,943 --> 00:35:51,982
again, this is pseudocode, an optimization.

00:35:51,982 --> 00:35:54,611
First, we read the current value.

00:35:54,611 --> 00:35:56,602
This is faster than writing.

00:35:56,602 --> 00:35:59,450
If the value isn't what's expected, we're done.

00:35:59,450 --> 00:36:01,908
We're just returning false.

00:36:01,908 --> 00:36:04,226
If the value is what's expected,

00:36:04,226 --> 00:36:07,530
we have to now get the exclusive access.

00:36:07,530 --> 00:36:09,371
Oh, by the way, before we got the exclusive access,

00:36:09,371 --> 00:36:11,260
the value may have changed.

00:36:11,260 --> 00:36:13,797
We have to read it again.

00:36:13,797 --> 00:36:15,379
Check that it hasn't changed,

00:36:15,379 --> 00:36:19,727
and if it hasn't changed, now we can do the swap.

00:36:19,727 --> 00:36:22,495
That's double-check locking pattern, back for you

00:36:22,495 --> 00:36:23,962
from the grave.

00:36:23,962 --> 00:36:27,046
Only the hardware can actually do it.

00:36:27,046 --> 00:36:30,492
Well, what could happen is exclusive access

00:36:30,492 --> 00:36:33,439
on the particular platform may be hard to get.

00:36:33,439 --> 00:36:36,623
Read isn't, but the exclusive access is.

00:36:36,623 --> 00:36:41,378
So instead of doing a lock, you may do a timed lock.

00:36:41,378 --> 00:36:44,711
So you did the read, and if the value you got,

00:36:44,711 --> 00:36:46,237
the current value isn't what's expected,

00:36:46,237 --> 00:36:49,147
you just return false right away.

00:36:49,147 --> 00:36:52,046
So the value, current value, is what's expected.

00:36:52,046 --> 00:36:53,975
You're going to try to do a timed lock.

00:36:53,975 --> 00:36:57,558
If you timed out on the lock at this point,

00:36:58,596 --> 00:37:01,544
well, you didn't do the swap, you timed out on the lock,

00:37:01,544 --> 00:37:02,455
you returned false.

00:37:02,455 --> 00:37:04,084
That's a spurious failure.

00:37:04,084 --> 00:37:06,386
So the result of the read was correct.

00:37:06,386 --> 00:37:10,553
There is really, the expected value of X is there.

00:37:12,144 --> 00:37:14,211
So you didn't time out on the read.

00:37:14,211 --> 00:37:15,598
You got the read.

00:37:15,598 --> 00:37:20,550
You timed out on trying to get the exclusive lock.

00:37:20,550 --> 00:37:22,860
So the hardware platforms that play games like this

00:37:22,860 --> 00:37:24,458
with compare-and-swap, they may have a weak

00:37:24,458 --> 00:37:25,787
compare-and-swap.

00:37:25,787 --> 00:37:28,452
X86, by the way, doesn't.

00:37:28,452 --> 00:37:31,610
X86 has a straight, brute-force,

00:37:31,610 --> 00:37:35,777
lock it and hold it until we're done compare-and-swap.

00:37:39,558 --> 00:37:43,246
Now, this is all well, except it actually

00:37:43,246 --> 00:37:45,893
doesn't really let you do anything.

00:37:45,893 --> 00:37:48,772
So it's good to understand,

00:37:48,772 --> 00:37:51,958
but it's not enough by a mile.

00:37:51,958 --> 00:37:54,978
And the reason is you very rarely use

00:37:54,978 --> 00:37:57,174
all atomic operations across your program.

00:37:57,174 --> 00:37:59,320
You never use all atomic operations across your program

00:37:59,320 --> 00:38:01,549
unless you have just one variable

00:38:01,549 --> 00:38:02,839
in your program, and that's atomic.

00:38:02,839 --> 00:38:04,906
Typically you have some atomic operations,

00:38:04,906 --> 00:38:08,406
and most of your variables are non-atomic.

00:38:09,980 --> 00:38:12,849
So let's look, for example, at atomic queue.

00:38:12,849 --> 00:38:17,106
Here is a lock-free queue implemented with an atomic.

00:38:17,106 --> 00:38:21,158
There is an array of slots that are not atomic.

00:38:21,158 --> 00:38:23,561
They're just plain integers, queue of integers.

00:38:23,561 --> 00:38:27,573
There is an atomic variable for front index.

00:38:27,573 --> 00:38:30,124
And what we are going to do is,

00:38:30,124 --> 00:38:31,192
in order to push on the queue,

00:38:31,192 --> 00:38:35,123
we're going to atomically increment the

00:38:35,123 --> 00:38:36,804
index of the front slot.

00:38:36,804 --> 00:38:41,171
Let's not worry about multiple writers yet.

00:38:41,171 --> 00:38:43,298
So I'm going to atomically increment,

00:38:43,298 --> 00:38:46,103
which means it's atomic operation.

00:38:46,103 --> 00:38:48,226
Nobody else can see the intermediate value.

00:38:48,226 --> 00:38:50,344
So if somebody incremented before me,

00:38:50,344 --> 00:38:52,986
I would see the incremented value.

00:38:52,986 --> 00:38:54,802
So let's say it starts from zero.

00:38:54,802 --> 00:38:56,053
And I do atomic increment.

00:38:56,053 --> 00:39:00,901
If I got back one, it means I own slot number zero.

00:39:00,901 --> 00:39:02,510
Nobody else can get to slot number zero,

00:39:02,510 --> 00:39:03,629
because it's atomic.

00:39:03,629 --> 00:39:05,753
If somebody got before me, I wouldn't get back zero.

00:39:05,753 --> 00:39:07,931
I wouldn't get zero plus one.

00:39:07,931 --> 00:39:10,170
I would get one plus one, two.

00:39:10,170 --> 00:39:13,833
If I got to the zero first, nobody else will see zero,

00:39:13,833 --> 00:39:14,714
never again.

00:39:14,714 --> 00:39:16,614
They'll see one, what I put in.

00:39:16,614 --> 00:39:18,187
That's what atomic means.

00:39:18,187 --> 00:39:22,136
So now, as a writer, I can store my integer

00:39:22,136 --> 00:39:24,469
into my slot anytime I want.

00:39:25,471 --> 00:39:28,989
Now, this avoids the, we haven't talked about

00:39:28,989 --> 00:39:31,044
how to synchronize with the reads,

00:39:31,044 --> 00:39:32,034
because the readers have to know

00:39:32,034 --> 00:39:33,183
when you're finished writing.

00:39:33,183 --> 00:39:34,816
Let's ignore that problem for now.

00:39:34,816 --> 00:39:37,983
The point here is that my atomic front

00:39:39,192 --> 00:39:42,589
is used as an index to a non-atomic memory.

00:39:42,589 --> 00:39:44,926
And that's how pretty much all lock-free

00:39:44,926 --> 00:39:46,593
data structures are.

00:39:47,534 --> 00:39:48,951
May not be index,

00:39:50,188 --> 00:39:51,605
may be a pointer.

00:39:52,656 --> 00:39:54,096
Here is a lock-free list.

00:39:54,096 --> 00:39:57,861
It has an atomic pointer to the head of the list.

00:39:57,861 --> 00:39:59,704
How do you push a new node onto the list?

00:39:59,704 --> 00:40:01,372
You create a node on the side.

00:40:01,372 --> 00:40:02,205
Nobody can see it.

00:40:02,205 --> 00:40:03,721
There is nothing to worry about.

00:40:03,721 --> 00:40:04,985
You get the current head.

00:40:04,985 --> 00:40:07,480
Again, nothing to worry about.

00:40:07,480 --> 00:40:09,312
You put the current head as the next pointer

00:40:09,312 --> 00:40:10,635
on your new node.

00:40:10,635 --> 00:40:11,868
Nobody can see your new node.

00:40:11,868 --> 00:40:13,659
There is no race conditions here.

00:40:13,659 --> 00:40:15,664
Now you're going to do an atomic swap,

00:40:15,664 --> 00:40:17,347
except you have to worry about the possibility

00:40:17,347 --> 00:40:19,346
that somebody else did the atomic swap before you,

00:40:19,346 --> 00:40:22,280
and the head is no longer the head.

00:40:22,280 --> 00:40:23,999
Okay, so you're going to do conditional swap,

00:40:23,999 --> 00:40:25,272
compare-and-swap.

00:40:25,272 --> 00:40:27,813
If nobody changed the head, you're going to enqueue,

00:40:27,813 --> 00:40:30,875
to store your new node as the head of the list

00:40:30,875 --> 00:40:33,530
and atomically replace the head pointer.

00:40:33,530 --> 00:40:36,531
If somebody did change the head pointer,

00:40:36,531 --> 00:40:38,001
you get the new head pointer back,

00:40:38,001 --> 00:40:40,411
and you loop all over again.

00:40:40,411 --> 00:40:44,123
The important point is that the atomic node star pointer,

00:40:44,123 --> 00:40:48,356
the head, points to non-atomic memory of the list.

00:40:48,356 --> 00:40:52,663
And that's, all lock-free algorithms are like that.

00:40:52,663 --> 00:40:54,891
Okay, so basically you have this.

00:40:54,891 --> 00:40:59,576
You have atomic pointer that points to memory location,

00:40:59,576 --> 00:41:02,826
and you grab it into your exclusive use

00:41:05,403 --> 00:41:08,677
and swap atomic pointer to some other memory location.

00:41:08,677 --> 00:41:10,923
Or you have some new memory location

00:41:10,923 --> 00:41:13,649
that you want to reveal to the world.

00:41:13,649 --> 00:41:15,438
You swap some atomic pointer to point

00:41:15,438 --> 00:41:18,070
to your new memory location, atomically.

00:41:18,070 --> 00:41:21,070
But the memory itself is not atomic.

00:41:22,120 --> 00:41:25,205
So what guarantees do we have

00:41:25,205 --> 00:41:29,372
that the non-atomic memory is still read correctly,

00:41:31,123 --> 00:41:33,066
without race conditions, by all the other threads?

00:41:33,066 --> 00:41:36,399
We spoke about atomic operations on atomic variables.

00:41:36,399 --> 00:41:37,325
We have guarantees for that.

00:41:37,325 --> 00:41:39,102
Atomic read, atomic write, atomic swap,

00:41:39,102 --> 00:41:39,935
that's all atomic.

00:41:39,935 --> 00:41:42,086
But that's on the atomic variable itself.

00:41:42,086 --> 00:41:45,586
What guarantees do we have for the memory?

00:41:48,404 --> 00:41:49,577
And we need these guarantees,

00:41:49,577 --> 00:41:50,832
we need guarantees of the sort,

00:41:50,832 --> 00:41:54,122
like if I'm publishing a new node,

00:41:54,122 --> 00:41:57,426
all other threads must see that memory

00:41:57,426 --> 00:41:59,591
of the new node in its final state.

00:41:59,591 --> 00:42:03,212
I'm done preparing it, I did the atomic swap.

00:42:03,212 --> 00:42:07,014
I must have a guarantee that my non-atomic writes

00:42:07,014 --> 00:42:09,357
to the list node have completed

00:42:09,357 --> 00:42:11,387
and become visible to everybody else.

00:42:11,387 --> 00:42:15,257
Otherwise, this atomic pointer is worthless.

00:42:15,257 --> 00:42:17,202
Now, that's done by the memory barriers.

00:42:17,202 --> 00:42:20,219
And memory barriers really go hand-in-hand with atomics.

00:42:20,219 --> 00:42:22,010
You really can't understand atomics

00:42:22,010 --> 00:42:23,826
if you don't understand memory barriers.

00:42:23,826 --> 00:42:26,487
So let me show you the memory barriers.

00:42:26,487 --> 00:42:29,654
Memory barriers control how changes to memory

00:42:29,654 --> 00:42:34,068
made by one of the CPUs become visible to the other CPUs.

00:42:34,068 --> 00:42:36,421
And the need for them is because if you don't have

00:42:36,421 --> 00:42:39,604
any memory barriers or anything equivalent to them,

00:42:39,604 --> 00:42:41,128
you have no guarantee of visibility.

00:42:41,128 --> 00:42:44,328
By default, there is no guarantee of visibility whatsoever.

00:42:44,328 --> 00:42:45,178
You have two CPUs.

00:42:45,178 --> 00:42:48,118
They are both modifying their caches here and here.

00:42:48,118 --> 00:42:50,031
Main memory doesn't have to change at all.

00:42:50,031 --> 00:42:53,673
There's no guarantees that anybody can see anything.

00:42:53,673 --> 00:42:56,587
That's not how X86 works, but that's how

00:42:56,587 --> 00:42:58,087
some systems work.

00:42:59,524 --> 00:43:02,833
So it's a global control of visibility

00:43:02,833 --> 00:43:04,519
across multiple CPUs.

00:43:04,519 --> 00:43:06,359
Has to be supported in the hardware.

00:43:06,359 --> 00:43:09,194
In practice, it's often not a special instruction,

00:43:09,194 --> 00:43:10,377
although it can be.

00:43:10,377 --> 00:43:14,544
It's often an attribute on some other instruction.

00:43:15,727 --> 00:43:17,060
Okay, thank you.

00:43:18,476 --> 00:43:21,643
So, again, C++03, life was very simple.

00:43:21,643 --> 00:43:23,342
No memory barriers.

00:43:23,342 --> 00:43:27,185
C++11 has memory barriers, and in the standard,

00:43:27,185 --> 00:43:29,221
the concept that they use is memory order.

00:43:29,221 --> 00:43:31,804
The two things are closely related.

00:43:31,804 --> 00:43:34,188
So the way it looks in the language is

00:43:34,188 --> 00:43:37,078
like what you see here in the last line.

00:43:37,078 --> 00:43:39,269
Remember, I told you before that there is

00:43:39,269 --> 00:43:40,530
assignment operator.

00:43:40,530 --> 00:43:43,314
And it's equivalent to a store function.

00:43:43,314 --> 00:43:45,625
Well, I simplified things for you.

00:43:45,625 --> 00:43:46,458
I lied.

00:43:48,137 --> 00:43:49,930
Store function is more complex than that.

00:43:49,930 --> 00:43:52,902
And a store function has the second argument

00:43:52,902 --> 00:43:57,649
that is the memory barrier, or the memory order.

00:43:57,649 --> 00:44:01,123
So if I say, store one with the memory-order-release,

00:44:01,123 --> 00:44:05,263
it means that I put a release-memory-barrier on that store.

00:44:05,263 --> 00:44:06,484
What's the release-memory-barrier?

00:44:06,484 --> 00:44:09,152
Well, let's start with a simple one.

00:44:09,152 --> 00:44:10,375
What's no memory barrier?

00:44:10,375 --> 00:44:12,616
No memory barrier means I can reorder

00:44:12,616 --> 00:44:14,584
reads and writes any way I want.

00:44:14,584 --> 00:44:17,923
So here is my atomic X in the middle,

00:44:17,923 --> 00:44:20,695
and I have A, B, and C, some non-atomic variables.

00:44:20,695 --> 00:44:23,311
In the program order, I write to A,

00:44:23,311 --> 00:44:27,146
then I write to B, then I write to C, then I write to X.

00:44:27,146 --> 00:44:31,146
In the actual order, anything is possible,

00:44:31,146 --> 00:44:32,992
no restrictions whatsoever.

00:44:32,992 --> 00:44:37,159
And that's, in C++ standard, called memory-order-relaxed.

00:44:38,338 --> 00:44:40,755
There are no guarantees here.

00:44:41,839 --> 00:44:46,211
Acquire, that's the first barrier with some guarantees.

00:44:46,211 --> 00:44:50,378
Well, there is also consume, but let's not talk about that.

00:44:52,407 --> 00:44:55,663
So acquire, acquire is what's called a half barrier

00:44:55,663 --> 00:44:58,007
sometimes, guarantees that all the memory operations

00:44:58,007 --> 00:45:01,407
that are scheduled after the barrier in the program order

00:45:01,407 --> 00:45:02,800
become visible after the barrier.

00:45:02,800 --> 00:45:06,395
Now, all operations means both reads and writes,

00:45:06,395 --> 00:45:08,895
not just reads or just writes.

00:45:09,861 --> 00:45:12,607
And all operations means not just operations

00:45:12,607 --> 00:45:17,150
on that atomic variable, but all reads and all writes.

00:45:17,150 --> 00:45:18,766
So how does it look like?

00:45:18,766 --> 00:45:20,099
Well, here is my

00:45:21,564 --> 00:45:23,901
read, load, on the atomic variable,

00:45:23,901 --> 00:45:26,068
with memory-order-acquire.

00:45:27,422 --> 00:45:30,289
And before it and after it in program order,

00:45:30,289 --> 00:45:34,102
I have some other operations on non-atomic variables.

00:45:34,102 --> 00:45:37,167
Some reordering is allowed, and some is not.

00:45:37,167 --> 00:45:40,500
Nothing that was after the load can move

00:45:41,767 --> 00:45:43,399
in front of it.

00:45:43,399 --> 00:45:47,365
Anything that was before can move after.

00:45:47,365 --> 00:45:49,256
That's the acquire barrier.

00:45:49,256 --> 00:45:53,085
Makes sense that the release barrier is the reverse.

00:45:53,085 --> 00:45:55,798
Nothing that was before release barrier

00:45:55,798 --> 00:46:00,344
in program order can be observed after that store operation.

00:46:00,344 --> 00:46:03,710
But things that were done after the store

00:46:03,710 --> 00:46:04,971
can be observed before.

00:46:04,971 --> 00:46:07,554
That's the second half barrier.

00:46:10,050 --> 00:46:12,800
Now, they're often used together,

00:46:14,542 --> 00:46:15,875
this acquire-release protocol.

00:46:15,875 --> 00:46:17,170
Why are they often used together?

00:46:17,170 --> 00:46:18,340
Well, think about it.

00:46:18,340 --> 00:46:20,668
So you have two threads, and the first one

00:46:20,668 --> 00:46:23,515
writes the atomic variable with the release barrier.

00:46:23,515 --> 00:46:25,720
It guarantees that all the writes done before

00:46:25,720 --> 00:46:27,049
become visible.

00:46:27,049 --> 00:46:30,595
But the second thread has to also use a barrier.

00:46:30,595 --> 00:46:32,004
If the second thread doesn't use a barrier,

00:46:32,004 --> 00:46:33,864
the guarantees don't apply.

00:46:33,864 --> 00:46:38,552
The second thread reads it with acquire barrier.

00:46:38,552 --> 00:46:42,013
All the reads, now I have the guarantee on the acquire side.

00:46:42,013 --> 00:46:45,315
All the reads that are done after the barrier

00:46:45,315 --> 00:46:47,707
in program order have to be done after the barrier

00:46:47,707 --> 00:46:49,920
in actual real execution order.

00:46:49,920 --> 00:46:53,389
So now all the writes that were done before the barrier

00:46:53,389 --> 00:46:54,442
have become visible.

00:46:54,442 --> 00:46:57,269
And all the reads that were done after the barrier

00:46:57,269 --> 00:47:00,352
are reading that new, updated memory.

00:47:04,967 --> 00:47:07,416
Has to be the same atomic variable.

00:47:07,416 --> 00:47:09,787
That's very important.

00:47:09,787 --> 00:47:12,312
So the language acquire and release here

00:47:12,312 --> 00:47:15,105
has to do with this concept of, I have some data,

00:47:15,105 --> 00:47:17,948
I prepared it, it's my private data.

00:47:17,948 --> 00:47:21,256
Finally, I am ready to publish it, I release it

00:47:21,256 --> 00:47:22,970
to the general consumption.

00:47:22,970 --> 00:47:26,458
Another thread wants to sync to make sure

00:47:26,458 --> 00:47:28,989
what I released is what it's going to read.

00:47:28,989 --> 00:47:30,739
It acquires the data.

00:47:32,435 --> 00:47:33,897
It acquires the atomic variable

00:47:33,897 --> 00:47:36,018
and using acquire fence, it's guaranteed

00:47:36,018 --> 00:47:40,271
that it will acquire the data that I published.

00:47:40,271 --> 00:47:44,372
So here is how acquire-release protocol works.

00:47:44,372 --> 00:47:46,496
I have a release-store, and I have some data

00:47:46,496 --> 00:47:50,296
that is all written before the release-store.

00:47:50,296 --> 00:47:53,461
That data is published to the reading thread

00:47:53,461 --> 00:47:55,819
that does the acquire-load and reads it

00:47:55,819 --> 00:47:57,502
after the acquire-load.

00:47:57,502 --> 00:48:00,792
And I'm guaranteed, even though my A, B, and C

00:48:00,792 --> 00:48:02,579
are not atomic, only X is atomic,

00:48:02,579 --> 00:48:06,580
I have the guarantee that the memory I published

00:48:06,580 --> 00:48:10,330
will be the memory the other thread will see.

00:48:12,807 --> 00:48:14,599
Okay, let's skip the locks,

00:48:14,599 --> 00:48:17,682
cause actually, in the previous talk,

00:48:18,632 --> 00:48:20,539
they covered the locks.

00:48:20,539 --> 00:48:23,215
C++ has two more barriers: acquire-release,

00:48:23,215 --> 00:48:25,755
it's just acquire and release in one place,

00:48:25,755 --> 00:48:29,218
bidirectional barrier, but only works if

00:48:29,218 --> 00:48:32,718
both threads use the same atomic variable.

00:48:34,047 --> 00:48:36,400
The total sequential consistency barrier,

00:48:36,400 --> 00:48:39,498
the highest, the most strict barrier,

00:48:39,498 --> 00:48:43,639
that removes that requirement of one atomic variable.

00:48:43,639 --> 00:48:46,717
Now if you have sequential consistent separations

00:48:46,717 --> 00:48:48,420
on two different atomic variables,

00:48:48,420 --> 00:48:52,565
you now have the ordering guaranteed between them.

00:48:52,565 --> 00:48:54,531
Compare-and-swap is the only member function

00:48:54,531 --> 00:48:57,042
that actually has two memory order arguments.

00:48:57,042 --> 00:48:58,010
Why two?

00:48:58,010 --> 00:49:00,268
Well, remember that double-check locking implementation.

00:49:00,268 --> 00:49:02,527
You have a load, and after load it may decide

00:49:02,527 --> 00:49:05,337
to just bail out with a false.

00:49:05,337 --> 00:49:07,938
And there is a memory order on that.

00:49:07,938 --> 00:49:12,184
But if after load it got the expected value,

00:49:12,184 --> 00:49:15,032
then it has to get the exclusive access and do a swap.

00:49:15,032 --> 00:49:17,677
And the swap is another memory transaction,

00:49:17,677 --> 00:49:19,031
which has its own memory order.

00:49:19,031 --> 00:49:20,280
So I could have two different memory orders

00:49:20,280 --> 00:49:22,572
on compare-and-swap,

00:49:22,572 --> 00:49:24,072
the one for just reading it

00:49:24,072 --> 00:49:27,072
and the one for actually writing it.

00:49:28,219 --> 00:49:29,365
If you don't specify anything,

00:49:29,365 --> 00:49:31,042
what's the default memory order?

00:49:31,042 --> 00:49:32,616
Well, the answer is the strongest memory order.

00:49:32,616 --> 00:49:37,050
They really guard you from accidentally making mistakes.

00:49:37,050 --> 00:49:38,717
That's a good thing,

00:49:40,085 --> 00:49:40,918
except for

00:49:43,343 --> 00:49:47,510
the strongest memory order can be really expensive.

00:49:48,445 --> 00:49:52,189
Again, remember we're talking to two audiences here,

00:49:52,189 --> 00:49:54,496
to the hardware and to the people.

00:49:54,496 --> 00:49:56,792
The strongest memory order may not be what you want to say,

00:49:56,792 --> 00:50:00,959
and it may not be what you need for best performance.

00:50:02,082 --> 00:50:04,621
Well, let's deal with the performance first,

00:50:04,621 --> 00:50:05,791
because you are all here to write

00:50:05,791 --> 00:50:08,313
the fastest code possible.

00:50:08,313 --> 00:50:12,077
So the top line is a non-atomic write

00:50:12,077 --> 00:50:14,077
or atomic relaxed write.

00:50:15,409 --> 00:50:17,472
They're basically running at exactly the same speed.

00:50:17,472 --> 00:50:20,394
The bottom line, one and a half orders of magnitude

00:50:20,394 --> 00:50:23,797
slower, is the sequential consistency write,

00:50:23,797 --> 00:50:25,110
which incidentally happens to be

00:50:25,110 --> 00:50:27,138
what you get when you write assignment operator

00:50:27,138 --> 00:50:28,851
on an atomic.

00:50:28,851 --> 00:50:33,018
So if you didn't need the sequential consistency order,

00:50:34,764 --> 00:50:36,682
you're paying for being lazy,

00:50:36,682 --> 00:50:40,421
for not writing store memory-order-relaxed,

00:50:40,421 --> 00:50:42,698
for just writing assignment, one character,

00:50:42,698 --> 00:50:44,448
instead of the whole big member function.

00:50:44,448 --> 00:50:48,115
You're losing about 15 times in performance.

00:50:50,545 --> 00:50:53,567
So memory barriers are expensive.

00:50:53,567 --> 00:50:54,400
How expensive?

00:50:54,400 --> 00:50:56,041
That varies from platform to platform.

00:50:56,041 --> 00:50:57,897
On ARM, really expensive.

00:50:57,897 --> 00:51:00,623
On X86, all loads are acquire-loads,

00:51:00,623 --> 00:51:02,075
so acquire barriers are free.

00:51:02,075 --> 00:51:04,502
All stores are release-stores, so those are free.

00:51:04,502 --> 00:51:08,626
But the other barrier, the release on read

00:51:08,626 --> 00:51:11,789
or the acquire on write, are really expensive.

00:51:11,789 --> 00:51:14,554
That's what we saw on the previous slide.

00:51:14,554 --> 00:51:17,404
All read-modify-write operations like exchange

00:51:17,404 --> 00:51:18,702
have bidirectional barriers,

00:51:18,702 --> 00:51:20,190
and there's no difference between acquire-release

00:51:20,190 --> 00:51:24,617
and sequential consistency barriers, same thing on X86.

00:51:24,617 --> 00:51:26,661
Doesn't mean that's always the case.

00:51:26,661 --> 00:51:28,517
Even if you're writing for X86,

00:51:28,517 --> 00:51:30,854
it's very important to say what you mean.

00:51:30,854 --> 00:51:33,713
Why it's important to say what you mean?

00:51:33,713 --> 00:51:36,270
Lock-free code is really hard to write.

00:51:36,270 --> 00:51:38,888
It's hard to write if you don't mind occasional bugs.

00:51:38,888 --> 00:51:41,888
It's really hard to write if you do.

00:51:43,292 --> 00:51:44,929
It's hard to read.

00:51:44,929 --> 00:51:46,459
It's hard to read for other people.

00:51:46,459 --> 00:51:48,221
It's hard to read for yourself.

00:51:48,221 --> 00:51:49,518
And why would you read your own code?

00:51:49,518 --> 00:51:52,872
Well, to reason that you have done it correctly.

00:51:52,872 --> 00:51:54,908
And memory order specification expresses

00:51:54,908 --> 00:51:57,568
the intent of what you want it to do.

00:51:57,568 --> 00:52:00,698
So let's look at how this works.

00:52:00,698 --> 00:52:04,629
You wrote, atomic count, fetch add one,

00:52:04,629 --> 00:52:06,823
with memory order relaxed.

00:52:06,823 --> 00:52:08,845
What did you want to say by this?

00:52:08,845 --> 00:52:11,578
At least, as a reader, what do I infer

00:52:11,578 --> 00:52:13,411
when I read this code?

00:52:14,345 --> 00:52:17,357
Count is incremented concurrently but not used.

00:52:17,357 --> 00:52:18,513
Why incremented concurrently?

00:52:18,513 --> 00:52:19,539
Because you made it atomic.

00:52:19,539 --> 00:52:20,660
If it wasn't incremented concurrently,

00:52:20,660 --> 00:52:21,930
it wouldn't make it atomic.

00:52:21,930 --> 00:52:24,257
But it's not used to index any memory.

00:52:24,257 --> 00:52:25,123
It's just a count.

00:52:25,123 --> 00:52:27,203
You want to know how many times something happened.

00:52:27,203 --> 00:52:29,116
But you're not using it to index an array.

00:52:29,116 --> 00:52:30,242
How do I know that?

00:52:30,242 --> 00:52:34,075
Because you didn't put a memory barrier on it.

00:52:36,343 --> 00:52:38,988
Now, fetch-add on X86 is actually

00:52:38,988 --> 00:52:40,838
memory order acquire-release.

00:52:40,838 --> 00:52:43,622
So you might be tempted to cut corners.

00:52:43,622 --> 00:52:44,495
Don't.

00:52:44,495 --> 00:52:46,984
First of all, it confuses your readers, including yourself.

00:52:46,984 --> 00:52:48,568
Second, the compilers, although currently

00:52:48,568 --> 00:52:50,369
I'm not aware of any compiler that does

00:52:50,369 --> 00:52:53,844
such an optimization, may, as a reader of your program,

00:52:53,844 --> 00:52:56,163
realize that hey, you said memory-order-relaxed.

00:52:56,163 --> 00:52:59,218
I can reorder things across this.

00:52:59,218 --> 00:53:00,548
It wouldn't be forbidden.

00:53:00,548 --> 00:53:03,720
It's difficult to do, for the compiler writers.

00:53:03,720 --> 00:53:05,904
It wouldn't be forbidden.

00:53:05,904 --> 00:53:08,549
So do write what you mean to say.

00:53:08,549 --> 00:53:10,308
Okay, let's look at something else.

00:53:10,308 --> 00:53:13,962
Same count, but now I'm saying memory-order-release.

00:53:13,962 --> 00:53:18,273
As a reader of your code, what do I infer from this?

00:53:18,273 --> 00:53:20,493
Now, as opposed to previous slide,

00:53:20,493 --> 00:53:23,406
now I'm saying your count indexes some memory.

00:53:23,406 --> 00:53:25,910
And that memory was prepared for my consumption

00:53:25,910 --> 00:53:28,123
prior to your increment.

00:53:28,123 --> 00:53:30,167
So you did something to that memory, you did some writes,

00:53:30,167 --> 00:53:31,863
you did some initialization.

00:53:31,863 --> 00:53:35,613
And then you released it to me with this add.

00:53:36,647 --> 00:53:39,664
And in order to properly consume that memory,

00:53:39,664 --> 00:53:43,478
I have to load with an acquire barrier.

00:53:43,478 --> 00:53:46,978
That's what I infer when I read this code.

00:53:51,930 --> 00:53:54,347
Okay, what if you wrote this?

00:53:58,757 --> 00:54:01,296
Well, there are two possibilities.

00:54:01,296 --> 00:54:05,463
One, there are more than one atomic variable here,

00:54:06,320 --> 00:54:08,582
so you needed sequential consistency,

00:54:08,582 --> 00:54:10,491
cause that's the default memory barrier,

00:54:10,491 --> 00:54:11,957
or memory order, actually.

00:54:11,957 --> 00:54:13,576
Sequential consistency's not a memory barrier,

00:54:13,576 --> 00:54:14,718
it's a memory order.

00:54:14,718 --> 00:54:17,694
So you needed sequential consistency memory order,

00:54:17,694 --> 00:54:21,474
which means you have multiple atomic variables in play.

00:54:21,474 --> 00:54:22,307
Thank you.

00:54:23,993 --> 00:54:27,127
There is another possibility.

00:54:27,127 --> 00:54:29,150
And what's that?

00:54:29,150 --> 00:54:32,433
You didn't think about what you were doing.

00:54:32,433 --> 00:54:35,600
And that is unfortunately more likely.

00:54:39,519 --> 00:54:41,757
Or you had a bug and you couldn't figure out why,

00:54:41,757 --> 00:54:45,924
and you kept adding memory order until the bug went away.

00:54:49,576 --> 00:54:50,409
Now,

00:54:52,682 --> 00:54:55,671
if there is one thing you take away from this class,

00:54:55,671 --> 00:54:57,504
please, please, please

00:54:58,378 --> 00:55:00,128
don't take away this.

00:55:02,938 --> 00:55:04,465
Sequential consistency's fine.

00:55:04,465 --> 00:55:06,503
It makes your programs much easier to understand,

00:55:06,503 --> 00:55:08,836
much easier to reason about.

00:55:10,262 --> 00:55:11,900
It's just not always necessary

00:55:11,900 --> 00:55:14,099
to make every atomic operation memory order

00:55:14,099 --> 00:55:15,905
sequential consistent in order to get sequential

00:55:15,905 --> 00:55:18,739
consistency in your code.

00:55:18,739 --> 00:55:21,241
Consider that locks, lock-based programs,

00:55:21,241 --> 00:55:22,535
usually are sequentially consistent,

00:55:22,535 --> 00:55:23,702
because they can be.

00:55:23,702 --> 00:55:27,284
And they actually use acquire and release barriers.

00:55:27,284 --> 00:55:28,979
Acquiring a lock has acquire barrier,

00:55:28,979 --> 00:55:30,343
releasing lock has a release barrier.

00:55:30,343 --> 00:55:34,510
There is no sequential consistency memory order in the lock.

00:55:38,380 --> 00:55:41,474
Now, wouldn't be a talk on C++

00:55:41,474 --> 00:55:42,953
if I haven't complained at least once

00:55:42,953 --> 00:55:45,098
about the C++ standard.

00:55:45,098 --> 00:55:47,065
Just isn't done.

00:55:47,065 --> 00:55:49,398
So let's say you wrote this.

00:55:50,281 --> 00:55:53,473
You have a class with an atomic data member,

00:55:53,473 --> 00:55:56,087
and in the destructor, you need to know

00:55:56,087 --> 00:55:59,003
how many of something you need to clean up.

00:55:59,003 --> 00:56:01,437
And you're doing a load with memory-order-relaxed

00:56:01,437 --> 00:56:05,246
to get that out of the atomic variable.

00:56:05,246 --> 00:56:09,329
What did you just say to me by writing this code?

00:56:11,025 --> 00:56:12,692
You said, be afraid.

00:56:14,276 --> 00:56:15,779
Why did you say that?

00:56:15,779 --> 00:56:19,797
Because somebody else is reading that atomic variable

00:56:19,797 --> 00:56:22,338
at the same time on a different thread,

00:56:22,338 --> 00:56:24,803
while the object is being destroyed.

00:56:24,803 --> 00:56:25,671
That's what you said.

00:56:25,671 --> 00:56:29,668
That's why you're doing an atomic read.

00:56:29,668 --> 00:56:31,185
Well, of course there is another possibility.

00:56:31,185 --> 00:56:32,962
You couldn't do any other way of reading it.

00:56:32,962 --> 00:56:35,141
You really wanted to say non-atomic read.

00:56:35,141 --> 00:56:36,808
And there isn't one.

00:56:49,365 --> 00:56:53,144
Okay, so with that mandatory gripe about the standard

00:56:53,144 --> 00:56:57,141
out of the way, what do we know about atomics?

00:56:57,141 --> 00:56:59,801
Well, I covered what the syntax is,

00:56:59,801 --> 00:57:03,170
what the member function and operators are,

00:57:03,170 --> 00:57:05,170
what the performance is,

00:57:06,605 --> 00:57:07,924
the connection with memory barriers,

00:57:07,924 --> 00:57:09,779
atomics and memory barriers really two sides

00:57:09,779 --> 00:57:11,093
of the same coin.

00:57:11,093 --> 00:57:14,259
If you are reaching out for atomics

00:57:14,259 --> 00:57:16,237
for lock-free programming, you're doing it

00:57:16,237 --> 00:57:19,645
to get the best possible performance.

00:57:19,645 --> 00:57:21,570
I've shown you some benchmarks against a spin lock,

00:57:21,570 --> 00:57:24,598
which tells you, not necessarily tell you

00:57:24,598 --> 00:57:28,362
that you will always fail to get a better performance

00:57:28,362 --> 00:57:29,195
with lock-free.

00:57:29,195 --> 00:57:30,250
That's not true.

00:57:30,250 --> 00:57:33,566
You will get better performance, often.

00:57:33,566 --> 00:57:35,390
But you have to work for it.

00:57:35,390 --> 00:57:37,782
You have to pull all the stops.

00:57:37,782 --> 00:57:39,666
You almost always have to actually go

00:57:39,666 --> 00:57:41,987
and use the right memory barriers.

00:57:41,987 --> 00:57:44,794
You can't afford the default memory barriers

00:57:44,794 --> 00:57:46,692
if you want maximum performance.

00:57:46,692 --> 00:57:48,432
You have to think about what you need.

00:57:48,432 --> 00:57:50,552
You have to use the right memory order

00:57:50,552 --> 00:57:51,714
on every atomic operation.

00:57:51,714 --> 00:57:53,392
Sometimes it will be sequential consistency,

00:57:53,392 --> 00:57:55,225
and often it won't be.

00:57:56,910 --> 00:57:58,994
It affects the performance of your code,

00:57:58,994 --> 00:58:00,086
it affects the clarity of your code,

00:58:00,086 --> 00:58:02,085
when you're saying exactly what you need,

00:58:02,085 --> 00:58:03,590
the restrictions that you need

00:58:03,590 --> 00:58:06,340
on each of the atomic operations.

00:58:07,668 --> 00:58:11,835
So sometimes you still need to use atomic in your C++ code.

00:58:14,637 --> 00:58:16,058
Often you don't, but sometimes you do

00:58:16,058 --> 00:58:17,435
for maximum performance.

00:58:17,435 --> 00:58:19,966
There are data structures that are really expensive to do

00:58:19,966 --> 00:58:22,742
with locks, and that's mainly data structures

00:58:22,742 --> 00:58:25,634
that allow distributed sort of access, like lists.

00:58:25,634 --> 00:58:27,593
Ever tried to do lists with a lock?

00:58:27,593 --> 00:58:30,275
Well, when you don't want to lock the entire list.

00:58:30,275 --> 00:58:32,194
You can put a spin lock on every node.

00:58:32,194 --> 00:58:33,886
That's not very expensive.

00:58:33,886 --> 00:58:37,864
You're going to deadlock really quickly.

00:58:37,864 --> 00:58:41,682
Not impossible, but hard to do correctly with locks.

00:58:41,682 --> 00:58:45,849
Not that hard to do with lock-free, with compare-and-swap.

00:58:48,943 --> 00:58:50,841
There are other drawbacks to locks.

00:58:50,841 --> 00:58:52,201
Locks are non-composable.

00:58:52,201 --> 00:58:54,168
You try to compose locks, you get deadlocks.

00:58:54,168 --> 00:58:56,244
You get priority inversion, you get latency problems

00:58:56,244 --> 00:58:57,376
with locks.

00:58:57,376 --> 00:59:00,903
Lock-free algorithms exist to solve all of those problems.

00:59:00,903 --> 00:59:05,514
If that's what you want, use lock-free algorithms.

00:59:05,514 --> 00:59:06,347
But again,

00:59:08,234 --> 00:59:10,983
learn the impact on the performance

00:59:10,983 --> 00:59:12,566
of all the details.

00:59:13,537 --> 00:59:17,826
Be very explicit about your atomic programming,

00:59:17,826 --> 00:59:19,033
what you do, why you do.

00:59:19,033 --> 00:59:21,770
State the actual restrictions and guarantees

00:59:21,770 --> 00:59:22,603
that you need.

00:59:22,603 --> 00:59:26,149
If you need a release, say that you need a release.

00:59:26,149 --> 00:59:28,399
Don't be sloppy about this.

00:59:29,311 --> 00:59:30,830
It helps me to read it;

00:59:30,830 --> 00:59:34,247
it helps you to get the best performance.

00:59:35,085 --> 00:59:39,140
If you have a concurrent synchronization protocol

00:59:39,140 --> 00:59:42,473
that can be achieved without atomic swap

00:59:43,967 --> 00:59:46,566
or compare-and-swap or exchange,

00:59:46,566 --> 00:59:49,899
you can get better performance by a mile

00:59:50,734 --> 00:59:52,506
than any other synchronization schema.

00:59:52,506 --> 00:59:54,450
If you can only use reads and writes.

00:59:54,450 --> 00:59:56,251
When can you use reads and writes

00:59:56,251 --> 00:59:58,038
in your atomic programming?

00:59:58,038 --> 01:00:02,185
Well, I have a talk on that on Wednesday.

01:00:02,185 --> 01:00:05,518
And with that, let's open for questions.

01:00:06,665 --> 01:00:10,082
In sequentially consistent order, please.

01:00:12,766 --> 01:00:15,016
(applause)

01:00:24,950 --> 01:00:25,783
Yes?

01:00:25,783 --> 01:00:27,582
- [Man] So I've done far more reading

01:00:27,582 --> 01:00:30,168
about lock-free programing than successfully

01:00:30,168 --> 01:00:34,004
programmed lock-free, but from my understanding

01:00:34,004 --> 01:00:36,671
of the weak versus strong write,

01:00:38,379 --> 01:00:41,330
in certain circumstances, in certain architectures,

01:00:41,330 --> 01:00:43,502
which have not been fully spelled out,

01:00:43,502 --> 01:00:45,268
other people have made the claim

01:00:45,268 --> 01:00:48,491
that the weak version may succeed

01:00:48,491 --> 01:00:50,639
faster than the strong one would,

01:00:50,639 --> 01:00:54,733
besides just spuriously returning false.

01:00:54,733 --> 01:00:57,587
- Okay, so the difference between the weak,

01:00:57,587 --> 01:00:59,080
remember that I showed you the difference

01:00:59,080 --> 01:01:00,311
between the weak and the strong.

01:01:00,311 --> 01:01:02,894
So the weak one will do a read,

01:01:04,665 --> 01:01:07,331
and that may require communication across cores.

01:01:07,331 --> 01:01:09,061
That part has to happen.

01:01:09,061 --> 01:01:12,770
But the getting exclusive access part,

01:01:12,770 --> 01:01:15,987
which is the global locking down of the entire,

01:01:15,987 --> 01:01:18,834
of everybody's access to the cache line

01:01:18,834 --> 01:01:20,151
while you're doing your write,

01:01:20,151 --> 01:01:24,282
that, on some hardware platforms, is very expensive.

01:01:24,282 --> 01:01:27,487
And yes, it may be better, basically,

01:01:27,487 --> 01:01:30,012
for this core to give up its write

01:01:30,012 --> 01:01:33,179
to the exclusive access and let somebody else

01:01:33,179 --> 01:01:35,012
do their update first.

01:01:36,074 --> 01:01:39,530
Essentially, if you're doing this kind of timed lock,

01:01:39,530 --> 01:01:40,774
and if you're waiting for a long time,

01:01:40,774 --> 01:01:42,194
it means somebody else is really trying

01:01:42,194 --> 01:01:44,839
to do something to that same cache line

01:01:44,839 --> 01:01:46,404
or the same memory location,

01:01:46,404 --> 01:01:47,785
and they're in a better position.

01:01:47,785 --> 01:01:50,409
It's local, maybe it's local cache to them.

01:01:50,409 --> 01:01:51,966
So okay, let them do it.

01:01:51,966 --> 01:01:56,312
So on some platforms, giving up on that exclusive access

01:01:56,312 --> 01:01:58,979
and essentially going for a spin

01:02:00,543 --> 01:02:02,499
on the compare-and-swap loop

01:02:02,499 --> 01:02:05,023
will result in a faster performance overall,

01:02:05,023 --> 01:02:07,898
across all threads, because you let another thread

01:02:07,898 --> 01:02:11,981
that can do it faster, you let that one go first.

01:02:13,946 --> 01:02:16,863
It requires that you have platforms

01:02:17,726 --> 01:02:20,003
that have a hardware that basically

01:02:20,003 --> 01:02:23,085
plays something like this double-check locking trick.

01:02:23,085 --> 01:02:24,702
It's not the only implementation that does it,

01:02:24,702 --> 01:02:27,546
but something along these lines.

01:02:27,546 --> 01:02:29,682
On other platforms, so if you look at X86,

01:02:29,682 --> 01:02:31,947
you can just look at assembler-generated code.

01:02:31,947 --> 01:02:33,618
Compare-and-exchange, strong compare-and-exchange

01:02:33,618 --> 01:02:36,767
would generate the same instructions.

01:02:36,767 --> 01:02:38,894
So X86 just doesn't have enough tricks

01:02:38,894 --> 01:02:41,477
in its bag to make use of that.

01:02:43,406 --> 01:02:46,094
Now, you can still say compare-exchange weak,

01:02:46,094 --> 01:02:48,719
and you wouldn't be wrong.

01:02:48,719 --> 01:02:51,962
Just because it can spuriously fail doesn't mean it will.

01:02:51,962 --> 01:02:55,628
Your program obviously has to handle if it might fail,

01:02:55,628 --> 01:02:57,460
but what if it never fails?

01:02:57,460 --> 01:02:59,672
Your program has to work for that condition.

01:02:59,672 --> 01:03:02,306
So it's not wrong to use it if you're just

01:03:02,306 --> 01:03:04,680
spinning on it and hammering the atomic

01:03:04,680 --> 01:03:06,904
over and over in this compare-exchange loop,

01:03:06,904 --> 01:03:09,859
it's usually better to do weak

01:03:09,859 --> 01:03:11,918
just in case you're on one of those platforms

01:03:11,918 --> 01:03:15,784
where getting a long-range lock is expensive

01:03:15,784 --> 01:03:19,951
and it's better to let the owner of that location go first.

01:03:24,592 --> 01:03:28,259
- [Man] You say that memory-order-relaxed is

01:03:31,485 --> 01:03:33,331
tricky to use.

01:03:33,331 --> 01:03:36,581
But do you know any practical use cases

01:03:38,516 --> 01:03:40,599
for this memory ordering?

01:03:41,701 --> 01:03:44,868
(man talking quietly)

01:03:48,163 --> 01:03:51,894
- Well, so relaxed, if you have a true relaxed memory order,

01:03:51,894 --> 01:03:53,423
which let's assume your hardware platform

01:03:53,423 --> 01:03:56,959
has a true relaxed, so like ARM has a true relaxed.

01:03:56,959 --> 01:03:59,825
It means that you have no guarantees

01:03:59,825 --> 01:04:03,242
on other variables, atomic or non-atomic,

01:04:05,705 --> 01:04:07,654
with respect to their writes.

01:04:07,654 --> 01:04:09,223
Now, what's the use?

01:04:09,223 --> 01:04:11,586
The use is, I have this atomic variable,

01:04:11,586 --> 01:04:15,392
and I'm only interested in that atomic variable.

01:04:15,392 --> 01:04:17,761
It's not an index into any array,

01:04:17,761 --> 01:04:20,511
it's not a pointer to any memory.

01:04:21,401 --> 01:04:23,492
I just want to know how many,

01:04:23,492 --> 01:04:25,469
I'm printing something in the log

01:04:25,469 --> 01:04:29,991
across my program, and I want to print 10 lines.

01:04:29,991 --> 01:04:33,574
So I want to know how many times I printed.

01:04:34,891 --> 01:04:36,574
I'm accumulating a sum.

01:04:36,574 --> 01:04:41,016
I have a bunch of integers sitting on my multiple cores.

01:04:41,016 --> 01:04:42,700
I just want to know the total sum.

01:04:42,700 --> 01:04:45,383
Well, it's faster to do the local accumulation first,

01:04:45,383 --> 01:04:46,520
but then eventually you're going to go

01:04:46,520 --> 01:04:48,824
and commit to the global accumulator.

01:04:48,824 --> 01:04:50,036
You just want to know the total sum.

01:04:50,036 --> 01:04:53,015
Do a relaxed atomic increment.

01:04:53,015 --> 01:04:54,985
You don't care about any other variable.

01:04:54,985 --> 01:04:59,513
You just want that atomic to be atomically incremented.

01:04:59,513 --> 01:05:01,588
So I have two threads.

01:05:01,588 --> 01:05:02,655
One wants to add three.

01:05:02,655 --> 01:05:04,206
One wants to add five.

01:05:04,206 --> 01:05:06,726
The only guarantee I want, I want to get eight at the end.

01:05:06,726 --> 01:05:08,476
Memory-order-relaxed,

01:05:09,762 --> 01:05:11,413
cause you're not using that eight to index

01:05:11,413 --> 01:05:13,163
into anybody's array.

01:05:14,568 --> 01:05:17,774
So that's memory-order-relaxed.

01:05:17,774 --> 01:05:19,691
Now, this is by itself.

01:05:20,706 --> 01:05:24,480
Now, when it gets tricky is you have multiple

01:05:24,480 --> 01:05:26,314
atomic variables.

01:05:26,314 --> 01:05:28,984
And you're doing some kind of communications for them,

01:05:28,984 --> 01:05:30,180
so you are pre-building.

01:05:30,180 --> 01:05:31,983
So you first increment this atomic variable.

01:05:31,983 --> 01:05:34,355
And that doesn't really signal anything.

01:05:34,355 --> 01:05:35,928
Then you increment that atomic variable,

01:05:35,928 --> 01:05:37,193
and that doesn't really mean anything either.

01:05:37,193 --> 01:05:39,721
Then there's a third atomic variable, some flag,

01:05:39,721 --> 01:05:40,994
which you change from zero to one,

01:05:40,994 --> 01:05:43,843
and that means, okay, I'm done, go.

01:05:43,843 --> 01:05:45,347
So the first two can be relaxed,

01:05:45,347 --> 01:05:49,014
and the last one is sequentially consistent.

01:05:51,189 --> 01:05:54,951
You don't need the barriers on the ones

01:05:54,951 --> 01:05:57,114
you used earlier because nothing is synchronized by them.

01:05:57,114 --> 01:06:00,097
You just want their final values to be known

01:06:00,097 --> 01:06:01,597
on the other side.

01:06:02,540 --> 01:06:04,710
So if you have a synchronization schema that,

01:06:04,710 --> 01:06:07,373
like a queue, queue has a front and an end.

01:06:07,373 --> 01:06:08,557
So two atomic variables,

01:06:08,557 --> 01:06:10,638
and you're atomically updating both of them.

01:06:10,638 --> 01:06:15,629
And you may go, okay, I cannot dequeue if they are

01:06:15,629 --> 01:06:17,507
the same place, but if there's a gap between them,

01:06:17,507 --> 01:06:19,362
then it means there is some data in the queue.

01:06:19,362 --> 01:06:20,427
I can dequeue.

01:06:20,427 --> 01:06:23,138
So you have two atomic variables,

01:06:23,138 --> 01:06:25,953
and you may not, depending on,

01:06:25,953 --> 01:06:27,782
there are different synchronization protocols.

01:06:27,782 --> 01:06:29,642
If you were in Magit's talk before,

01:06:29,642 --> 01:06:31,571
he's shown you two different schemas

01:06:31,571 --> 01:06:33,337
on synchronizing a queue.

01:06:33,337 --> 01:06:36,763
One of them just used basically memory-order-relaxed

01:06:36,763 --> 01:06:39,150
on front and tail, and atomic was somewhere else.

01:06:39,150 --> 01:06:41,648
So you have multiple atomics,

01:06:41,648 --> 01:06:43,681
very often you don't need a barrier on all of them,

01:06:43,681 --> 01:06:46,189
because you're using the last one

01:06:46,189 --> 01:06:48,069
to actually publish or synchronize,

01:06:48,069 --> 01:06:51,569
and everything else just has to be atomic.

01:06:56,444 --> 01:06:58,576
- [Man] So building on top of that,

01:06:58,576 --> 01:07:01,580
at the very beginning when you showed the demonstration

01:07:01,580 --> 01:07:05,230
of adding a whole bunch of integers,

01:07:05,230 --> 01:07:08,691
and you showed that the lock-free one was,

01:07:08,691 --> 01:07:11,758
of course, much slower in your benchmark,

01:07:11,758 --> 01:07:15,262
is that primarily due to the sequential consistent

01:07:15,262 --> 01:07:16,408
memory barriers?

01:07:16,408 --> 01:07:19,664
- Well, on X86, it's really hard to answer

01:07:19,664 --> 01:07:23,379
because increment or fetch-add is sequentially consistent

01:07:23,379 --> 01:07:25,850
on X86, and there is nothing I can do about it.

01:07:25,850 --> 01:07:28,041
I could say fetch-add memory-order-relaxed,

01:07:28,041 --> 01:07:29,339
and it wouldn't make any difference

01:07:29,339 --> 01:07:33,392
because the xaddl instruction that is emitted

01:07:33,392 --> 01:07:35,321
is acquire-release.

01:07:35,321 --> 01:07:36,908
On X86 there is no difference

01:07:36,908 --> 01:07:38,818
between acquire-release and sequentially consistent.

01:07:38,818 --> 01:07:42,343
So xaddl is a bidirectional barrier.

01:07:42,343 --> 01:07:44,171
So there is nothing I can do about it.

01:07:44,171 --> 01:07:45,004
Now,

01:07:47,886 --> 01:07:51,638
in this case it's not really because of the barrier so much.

01:07:51,638 --> 01:07:54,233
It's because of the cache line bouncing.

01:07:54,233 --> 01:07:56,900
So even if I had a true relaxed,

01:07:58,966 --> 01:08:03,008
let's assume that I had, on X86 I had the true relaxed,

01:08:03,008 --> 01:08:06,838
meaning I increment it, and I get no additional guarantees

01:08:06,838 --> 01:08:09,149
about any other memory except for this atomic variable

01:08:09,149 --> 01:08:10,216
that I'm incrementing.

01:08:10,216 --> 01:08:12,029
It would still do the cache line bounce

01:08:12,029 --> 01:08:13,866
from core to core as I'm incrementing

01:08:13,866 --> 01:08:16,428
cause I want to write into this variable, says this core.

01:08:16,428 --> 01:08:18,769
Okay, send me the cache line.

01:08:18,769 --> 01:08:20,852
I'm going to lock this cache line and increment.

01:08:20,852 --> 01:08:22,334
Oh, I want to write into this.

01:08:22,334 --> 01:08:24,719
Send me the cache line all the way back.

01:08:24,719 --> 01:08:26,093
I'm going to lock it and increment.

01:08:26,093 --> 01:08:27,577
And the cache line keeps bouncing back and forth

01:08:27,577 --> 01:08:30,632
between the cores, because each core

01:08:30,632 --> 01:08:34,795
has to have exclusive access in order to do its increment.

01:08:34,795 --> 01:08:37,294
So even if you had true relaxed,

01:08:37,294 --> 01:08:39,461
it wouldn't really make it

01:08:41,565 --> 01:08:45,020
anywhere close in performance to what you

01:08:45,020 --> 01:08:47,353
saw in that, the lock-based,

01:08:48,555 --> 01:08:50,963
the mutex there is a gimmick.

01:08:50,963 --> 01:08:54,464
The bulk of this algorithm is adding

01:08:54,464 --> 01:08:56,670
into the local sum accumulator.

01:08:56,670 --> 01:08:59,313
After I've added thousands of values,

01:08:59,313 --> 01:09:02,698
yeah, at the end I go and do one add into the global,

01:09:02,698 --> 01:09:04,392
and nobody cares how I do it.

01:09:04,392 --> 01:09:06,302
It happens so infrequently.

01:09:06,302 --> 01:09:09,980
It's once per entire computation, the lock-based.

01:09:09,980 --> 01:09:12,721
Mutex, spin lock, atomic, whatever.

01:09:12,721 --> 01:09:16,294
I do thousands of accumulations locally,

01:09:16,294 --> 01:09:19,600
in my thread local variable, the one on the stack,

01:09:19,600 --> 01:09:21,317
millions of accumulations, however long

01:09:21,317 --> 01:09:22,351
you want to run the benchmark.

01:09:22,351 --> 01:09:25,345
Cause I knew I do one atomic commit.

01:09:25,345 --> 01:09:28,721
Under lock, do whatever you want, at this point.

01:09:28,721 --> 01:09:29,554
Doesn't matter.

01:09:29,554 --> 01:09:32,140
So lock is really a gimmick there.

01:09:32,140 --> 01:09:33,896
You have to have a lock, but it doesn't matter

01:09:33,896 --> 01:09:34,767
what you do with it.

01:09:34,767 --> 01:09:36,504
- [Man] I think I simply missed the local variable

01:09:36,504 --> 01:09:40,579
on the slide with the code, so that's what confused me.

01:09:40,579 --> 01:09:41,412
Thank you.

01:09:41,412 --> 01:09:43,255
- Yeah, so the lock is necessary,

01:09:43,255 --> 01:09:45,687
but performance of the lock is utterly unimportant.

01:09:45,687 --> 01:09:49,854
The atomic lets you write correct code without that lock.

01:09:51,268 --> 01:09:53,821
Correct, doesn't mean it's not stupid.

01:09:53,821 --> 01:09:55,568
It's just correct. (laughs)

01:09:55,568 --> 01:09:57,946
- [Man] Right, so if you had removed the lock

01:09:57,946 --> 01:10:00,120
and replaced it with an atomic plus-equals

01:10:00,120 --> 01:10:02,727
in the second one but kept the algorithm the same,

01:10:02,727 --> 01:10:03,560
it would have been-

01:10:03,560 --> 01:10:05,015
- It would make absolutely no difference,

01:10:05,015 --> 01:10:07,265
assuming you actually have,

01:10:08,584 --> 01:10:11,507
so you run the loop into the local accumulator.

01:10:11,507 --> 01:10:13,049
Let's say it's 1,000 additions.

01:10:13,049 --> 01:10:15,064
And then you do one atomic or lock or whatever.

01:10:15,064 --> 01:10:16,858
It's not measurable.

01:10:16,858 --> 01:10:20,694
Now, if you're local accumulator was like three adds,

01:10:20,694 --> 01:10:22,359
and then you do the commit,

01:10:22,359 --> 01:10:25,735
because you only have six elements

01:10:25,735 --> 01:10:27,428
in the array to add, total,

01:10:27,428 --> 01:10:29,244
it's questionable whether you want to do six adds

01:10:29,244 --> 01:10:31,077
on two threads or not,

01:10:31,930 --> 01:10:33,078
but if that's what you had to do,

01:10:33,078 --> 01:10:34,647
you would start seeing the difference on the lock

01:10:34,647 --> 01:10:37,843
versus atomic, cause three adds plus a mutex,

01:10:37,843 --> 01:10:40,431
versus three adds plus an atomic,

01:10:40,431 --> 01:10:42,310
that's a difference.

01:10:42,310 --> 01:10:44,623
- [Man] Okay, so, okay, I understand, thanks.

01:10:44,623 --> 01:10:45,456
- Yeah.

01:10:46,701 --> 01:10:47,960
Any other questions?

01:10:47,960 --> 01:10:48,793
- [Man] Yes.

01:10:48,793 --> 01:10:49,626
- Oh, okay, yep?

01:10:49,626 --> 01:10:53,503
- [Man] Hello, so the question that I have is that,

01:10:53,503 --> 01:10:56,756
can you give a hint of how the memory barriers

01:10:56,756 --> 01:10:59,806
are actually implemented in hardware?

01:10:59,806 --> 01:11:02,522
And is there a scope, like do they have anything

01:11:02,522 --> 01:11:05,689
to do with cache flush and cache load?

01:11:06,572 --> 01:11:08,322
- Well, they have to,

01:11:09,948 --> 01:11:11,798
so first of all, how it's implemented,

01:11:11,798 --> 01:11:14,232
from your point of view, it's either a special instruction

01:11:14,232 --> 01:11:16,732
like mfence or lfence on Spark

01:11:19,074 --> 01:11:22,290
or mfence on X86, or it's a modifier

01:11:22,290 --> 01:11:24,540
on an existing instruction.

01:11:29,557 --> 01:11:32,889
What's actually going on in the hardware,

01:11:32,889 --> 01:11:35,211
yeah, there is some sort of global coherency

01:11:35,211 --> 01:11:36,953
being established.

01:11:36,953 --> 01:11:38,700
What exactly is being established

01:11:38,700 --> 01:11:40,800
depends on what it has to establish.

01:11:40,800 --> 01:11:44,436
So first of all, pretty much everybody,

01:11:44,436 --> 01:11:46,403
even the systems with very strict memory model

01:11:46,403 --> 01:11:48,995
like X86 have store buffers.

01:11:48,995 --> 01:11:51,818
So that's the first thing being affected

01:11:51,818 --> 01:11:52,845
by the memory barrier.

01:11:52,845 --> 01:11:56,910
It has to basically flush the store buffers.

01:11:56,910 --> 01:12:01,053
Because on X86, actually it wouldn't have

01:12:01,053 --> 01:12:03,731
out of order writes if you didn't have the store buffers.

01:12:03,731 --> 01:12:06,552
The caches themselves are DSO,

01:12:06,552 --> 01:12:08,007
double store ordering.

01:12:08,007 --> 01:12:10,494
But you have store buffers, so the stores

01:12:10,494 --> 01:12:13,577
on the individual CPU are accumulated

01:12:15,378 --> 01:12:16,696
in the store buffer.

01:12:16,696 --> 01:12:20,300
So while the CPU waits for the cache line to be

01:12:20,300 --> 01:12:24,000
sent from another CPU and locked for exclusive modification,

01:12:24,000 --> 01:12:27,834
the values themselves, the ones that you want to write,

01:12:27,834 --> 01:12:30,834
are accumulated in the store buffer,

01:12:31,947 --> 01:12:35,525
which means other CPUs can't see them at all.

01:12:35,525 --> 01:12:36,819
So if you have a memory barrier,

01:12:36,819 --> 01:12:38,295
the first thing that must happen,

01:12:38,295 --> 01:12:41,276
everything in that store buffer has to be committed

01:12:41,276 --> 01:12:44,619
before other CPUs will be allowed to proceed.

01:12:44,619 --> 01:12:45,702
- [Man] Okay.

01:12:47,217 --> 01:12:48,765
All right, thank you.

01:12:48,765 --> 01:12:50,029
- Beyond that,

01:12:50,029 --> 01:12:50,946
it depends.

01:12:52,773 --> 01:12:54,121
So if you're on the same socket,

01:12:54,121 --> 01:12:58,036
typically you have cache-to-cache communication protocols,

01:12:58,036 --> 01:13:00,021
meaning you don't have to go through main memory.

01:13:00,021 --> 01:13:01,568
If you're on different sockets,

01:13:01,568 --> 01:13:05,401
you go through the UPI link or its equivalent.

01:13:06,781 --> 01:13:08,024
So you don't go through main memory,

01:13:08,024 --> 01:13:10,731
but there is basically like a socket

01:13:10,731 --> 01:13:12,814
on the microscopic level.

01:13:14,352 --> 01:13:16,383
- [Man] Okay, so basically it's not trivial.

01:13:16,383 --> 01:13:17,510
- It's not trivial.

01:13:17,510 --> 01:13:21,769
It heavily depends on what hardware you're using.

01:13:21,769 --> 01:13:26,171
And the performance characteristics will depend a lot,

01:13:26,171 --> 01:13:28,507
again, on the hardware.

01:13:28,507 --> 01:13:32,648
So if you benchmark the same code on X86 and on ARM,

01:13:32,648 --> 01:13:36,572
you will see, and for example you measure how it scales,

01:13:36,572 --> 01:13:39,247
you start taking memory barriers and see

01:13:39,247 --> 01:13:42,663
basically how much the contention cost you.

01:13:42,663 --> 01:13:44,794
You would notice totally different,

01:13:44,794 --> 01:13:46,961
on X86 it might basically,

01:13:49,830 --> 01:13:51,762
it starts off really fast, and then as soon as

01:13:51,762 --> 01:13:54,189
it kicks into contention, it just slows down

01:13:54,189 --> 01:13:55,275
to sequential.

01:13:55,275 --> 01:13:57,572
And on ARM, you might see that it starts much slower,

01:13:57,572 --> 01:13:59,329
but it keeps scaling almost perfectly

01:13:59,329 --> 01:14:01,523
for a lot more threads before you start

01:14:01,523 --> 01:14:02,845
noticing slowdown.

01:14:02,845 --> 01:14:05,137
And that has to do with, well,

01:14:05,137 --> 01:14:06,768
the cores themselves being slower,

01:14:06,768 --> 01:14:10,018
but the memory model being more relaxed

01:14:11,520 --> 01:14:13,766
on ARM than on X86.

01:14:13,766 --> 01:14:16,183
- [Man] All right, thank you.

01:14:17,078 --> 01:14:18,682
- Any more questions?

01:14:18,682 --> 01:14:19,554
No?

01:14:19,554 --> 01:14:20,387
Okay, thank you.

01:14:20,387 --> 01:14:21,617

YouTube URL: https://www.youtube.com/watch?v=ZQFzMfHIxng


