Title: CppCon 2017: Pan Deng “An Introduction To Shogun Machine Learning Toolbox One For All”
Publication date: 2017-10-27
Playlist: CppCon 2017 Lightning Talks
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
Lightning Talk
— 
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,220
I like to comment on that but I think

00:00:02,610 --> 00:00:08,160
I'm out of time okay so now that I'm

00:00:05,220 --> 00:00:09,719
going to introduce you the Sigma third

00:00:08,160 --> 00:00:12,630
base machine learning tool box called

00:00:09,719 --> 00:00:15,389
sugar sugar was first established in

00:00:12,630 --> 00:00:17,550
1999 and so far it has around three

00:00:15,389 --> 00:00:20,699
hundred thousand lines of code on github

00:00:17,550 --> 00:00:24,180
it is well established and well

00:00:20,699 --> 00:00:28,230
commented and most not only is that show

00:00:24,180 --> 00:00:32,309
going it's mostly written in C++ and how

00:00:28,230 --> 00:00:34,620
can she go survive or survive today as I

00:00:32,309 --> 00:00:37,320
have mentioned in my title which you may

00:00:34,620 --> 00:00:39,600
not have noticed showed is a highly

00:00:37,320 --> 00:00:41,850
integrated library which provides users

00:00:39,600 --> 00:00:44,879
abundant resources was unified interface

00:00:41,850 --> 00:00:47,789
and today I'm going to introduce you to

00:00:44,879 --> 00:00:50,579
features of sugar interfacing diverse

00:00:47,789 --> 00:00:53,090
machine learning library and multiple

00:00:50,579 --> 00:00:56,100
languages to support my statement

00:00:53,090 --> 00:00:58,199
so before I start let's take a step back

00:00:56,100 --> 00:00:58,940
to look at a simple example of machine

00:00:58,199 --> 00:01:01,530
learning

00:00:58,940 --> 00:01:02,969
imagine you have two types of thoughts

00:01:01,530 --> 00:01:05,309
on the plane and you want

00:01:02,969 --> 00:01:08,970
computers to learn how to distinguish

00:01:05,309 --> 00:01:11,729
them and predict the types of the new

00:01:08,970 --> 00:01:14,640
thoughts coming in so machine learning

00:01:11,729 --> 00:01:17,939
can do this of course with the classic

00:01:14,640 --> 00:01:21,689
support vector machine method or SVM

00:01:17,939 --> 00:01:25,259
method we can draw a line based on the

00:01:21,689 --> 00:01:27,000
distance between the two groups and the

00:01:25,259 --> 00:01:29,040
message can also provide us the

00:01:27,000 --> 00:01:31,979
prediction of different thoughts based

00:01:29,040 --> 00:01:34,439
on the line in true so that seems very

00:01:31,979 --> 00:01:36,710
simple right but things can get

00:01:34,439 --> 00:01:36,710
complicated

00:01:37,070 --> 00:01:44,369
apart from linear separators or kernels

00:01:41,130 --> 00:01:48,500
we can also have nonlinear kernels for

00:01:44,369 --> 00:01:52,590
more complex or higher dimensional cases

00:01:48,500 --> 00:01:55,710
there are tens of publicly available C++

00:01:52,590 --> 00:01:58,860
implementation of as we am out there and

00:01:55,710 --> 00:02:02,189
each of them have a subset of kernel

00:01:58,860 --> 00:02:04,560
functions with their unique benefits and

00:02:02,189 --> 00:02:07,110
the effects of course if you are a

00:02:04,560 --> 00:02:10,280
newcomer to machine learning field how

00:02:07,110 --> 00:02:10,280
could you choose from them

00:02:10,690 --> 00:02:17,510
well you know want to try them out but

00:02:14,480 --> 00:02:20,120
good news is they all come was there or

00:02:17,510 --> 00:02:23,360
interpreters as well this is where

00:02:20,120 --> 00:02:26,510
Sheldon comes to handy so as the code

00:02:23,360 --> 00:02:29,660
shows sugar could allow you to

00:02:26,510 --> 00:02:32,930
initialize an icbm class with different

00:02:29,660 --> 00:02:35,690
types of SVM algorithm you like by

00:02:32,930 --> 00:02:38,780
specifying the template and some there

00:02:35,690 --> 00:02:41,050
is the shared pointer of Shogun and you

00:02:38,780 --> 00:02:43,550
can also specify the server type and

00:02:41,050 --> 00:02:46,010
just train the model of by calling the

00:02:43,550 --> 00:02:49,430
trim method and you get the trade model

00:02:46,010 --> 00:02:52,340
and once you apply your test data to it

00:02:49,430 --> 00:02:54,530
you will get a prediction so what's this

00:02:52,340 --> 00:02:57,170
interface it's it is pretty easy to

00:02:54,530 --> 00:03:00,710
compare and try our different kind of

00:02:57,170 --> 00:03:04,250
machine learning method okay let's move

00:03:00,710 --> 00:03:07,670
on to the next point though mostly

00:03:04,250 --> 00:03:10,730
written in c++ Shogun actually supports

00:03:07,670 --> 00:03:14,150
multiple high level languages pretty

00:03:10,730 --> 00:03:16,850
well by a suite a software enables

00:03:14,150 --> 00:03:19,760
bi-directional communication between C++

00:03:16,850 --> 00:03:22,340
and its target languages it basically

00:03:19,760 --> 00:03:24,740
means it basically allows one to use

00:03:22,340 --> 00:03:28,520
children library without knowing any C++

00:03:24,740 --> 00:03:32,570
you may ask I am a zipless of hacker why

00:03:28,520 --> 00:03:35,570
do I care about us so imagine this

00:03:32,570 --> 00:03:37,880
situation you may be so genius that you

00:03:35,570 --> 00:03:41,860
developed an algorithm to classify the

00:03:37,880 --> 00:03:46,250
dots perfectly without overfitting and

00:03:41,860 --> 00:03:49,760
you won't break it out however it is

00:03:46,250 --> 00:03:54,350
implemented in C++ how could the rest of

00:03:49,760 --> 00:03:56,690
or know what we have done it's become

00:03:54,350 --> 00:03:58,940
pretty easy with shoulder you could just

00:03:56,690 --> 00:04:02,209
expose your algorithm to multiple

00:03:58,940 --> 00:04:04,040
community of different languages by very

00:04:02,209 --> 00:04:08,330
little changing or maybe just minor

00:04:04,040 --> 00:04:11,060
changes to adapt children here are some

00:04:08,330 --> 00:04:14,530
examples showing the linear SVM method

00:04:11,060 --> 00:04:19,850
in play of children implemented in C++

00:04:14,530 --> 00:04:22,220
Python C sharp and Java so due to the

00:04:19,850 --> 00:04:23,840
time limit our just stop here and if

00:04:22,220 --> 00:04:26,660
your interest in sugar

00:04:23,840 --> 00:04:29,810
could have a look at the shoulders of ml

00:04:26,660 --> 00:04:33,919
with multiple shotgun usage and machine

00:04:29,810 --> 00:04:35,840
learning methods and she has become part

00:04:33,919 --> 00:04:38,060
of them focus earlier this year and

00:04:35,840 --> 00:04:40,880
we're definitely interested to get more

00:04:38,060 --> 00:04:42,620
developers involved and we have all

00:04:40,880 --> 00:04:45,229
different goals achieved including

00:04:42,620 --> 00:04:47,630
multiple other interfaces for libraries

00:04:45,229 --> 00:04:53,030
and languages and of course take lots of

00:04:47,630 --> 00:04:55,880
17 migration and here's a website and

00:04:53,030 --> 00:04:57,950
source code of sugar and you can also

00:04:55,880 --> 00:05:01,389
pin us on our C if you are interested

00:04:57,950 --> 00:05:01,389

YouTube URL: https://www.youtube.com/watch?v=fe1sq8M21uI


