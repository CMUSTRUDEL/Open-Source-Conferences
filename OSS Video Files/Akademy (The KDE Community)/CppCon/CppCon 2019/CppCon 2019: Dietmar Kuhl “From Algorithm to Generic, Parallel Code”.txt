Title: CppCon 2019: Dietmar Kuhl “From Algorithm to Generic, Parallel Code”
Publication date: 2019-09-27
Playlist: CppCon 2019
Description: 
	http://CppCon.org
—
Discussion & Comments: https://www.reddit.com/r/cpp/
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2019
—
This presentation starts with a parallel algorithm as it is described in books and turns it into a generic implementation. Multiple options for running the algorithm concurrently based on different technologies (OpenMP, Threading Building Blocks, C++ standard-only) are explored.

Using parallel algorithms seems like an obvious way to improve the performance of operations. However, to utilize more processsing power often requires additional work to be done and depending on available resources and the size of the problem the parallel version may actually take longer than a sequential version. Looking at the actual implementation for an algorithm should clarify some of the tradeoffs.

Showing how a parallel algorithm can be implemented should also demonstrate how such an algorithm can be done when there is no suitable implementation available from the [standard C++] library. As the implementation of a parallel algorithms isn't trivial it should also become clear that using a readily available implementation is much preferable.
— 
Dietmar Kuhl
Bloomberg LP
Engineer
Dietmar Kühl is a senior software developer at Bloomberg L.P. working on the data distribution environment used both internally and by enterprise installations at clients. Before joining Bloomberg he has done mainly consulting for software projects in the finance area. He is a regular attendee of the ANSI/ISO C++ standards committee, presents at conferences, and he used to be a moderator of the newsgroup comp.lang.c++.moderated. He frequently answers questions on Stackoverflow.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:08,240 --> 00:00:14,929
so okay hello everybody my name is Sigma

00:00:13,110 --> 00:00:17,510
cool I

00:00:14,929 --> 00:00:19,460
and the first thing I want to mention as

00:00:17,510 --> 00:00:22,640
some other have mentioned in debt works

00:00:19,460 --> 00:00:25,250
as well I'm by no means an expert on

00:00:22,640 --> 00:00:28,189
that stuff I'm a dude I've played with

00:00:25,250 --> 00:00:33,160
that if you expect me to talk about GPU

00:00:28,189 --> 00:00:35,900
stuff sorry won't come I think I'm I

00:00:33,160 --> 00:00:38,720
know some stuff of that but I also see

00:00:35,900 --> 00:00:40,280
some people in here whom I expect

00:00:38,720 --> 00:00:40,760
actually know more about that stuff than

00:00:40,280 --> 00:00:43,010
I do

00:00:40,760 --> 00:00:46,460
I hope they would at best correct me

00:00:43,010 --> 00:00:49,610
where I'm entirely wrong and otherwise

00:00:46,460 --> 00:00:53,839
present next time to actually tell

00:00:49,610 --> 00:00:54,979
people how to do it properly so what I

00:00:53,839 --> 00:00:59,239
started all over here is basically

00:00:54,979 --> 00:01:00,650
something which I did for a long time

00:00:59,239 --> 00:01:02,570
for other things as well and that is I'm

00:01:00,650 --> 00:01:04,580
interested in how does a standard

00:01:02,570 --> 00:01:07,760
library how does C++ work actually

00:01:04,580 --> 00:01:10,909
another under the covers so let's say I

00:01:07,760 --> 00:01:13,009
always seems everybody knows about

00:01:10,909 --> 00:01:14,210
iostream so everybody as you said but

00:01:13,009 --> 00:01:14,900
hardly anybody knows how it is

00:01:14,210 --> 00:01:16,640
implemented

00:01:14,900 --> 00:01:17,990
implementing it is actually kind of

00:01:16,640 --> 00:01:20,600
interesting and you learn a lot about

00:01:17,990 --> 00:01:22,130
that stuff like Wes espero algorithms I

00:01:20,600 --> 00:01:22,520
would like to know how do they actually

00:01:22,130 --> 00:01:25,460
work

00:01:22,520 --> 00:01:27,110
can I just knock it up and do it the

00:01:25,460 --> 00:01:29,510
other part of that aside from cooking

00:01:27,110 --> 00:01:31,400
I'm not it it is the sunnat' lab we just

00:01:29,510 --> 00:01:33,790
provide a number of parallel algorithms

00:01:31,400 --> 00:01:35,840
but it is not necessarily complete and

00:01:33,790 --> 00:01:37,940
sometimes it may be necessary or useful

00:01:35,840 --> 00:01:39,770
to implement your own peril algorithm

00:01:37,940 --> 00:01:42,650
and you may or may not be able to

00:01:39,770 --> 00:01:45,920
leverage what it exists so maybe it's

00:01:42,650 --> 00:01:47,479
useful to know or to get an idea of what

00:01:45,920 --> 00:01:49,250
can be done to actually make things work

00:01:47,479 --> 00:01:52,130
that's roughly what I cannot talk about

00:01:49,250 --> 00:01:55,490
and my implementation I hope you're not

00:01:52,130 --> 00:01:57,890
too disappointed out that won't meet

00:01:55,490 --> 00:02:00,670
what the standard library does I think

00:01:57,890 --> 00:02:04,190
it gets reasonably well but not as fast

00:02:00,670 --> 00:02:05,900
okay so what I'm going to talk about is

00:02:04,190 --> 00:02:08,590
actually a retrofit straightforward

00:02:05,900 --> 00:02:11,090
parallel algorithm not super

00:02:08,590 --> 00:02:15,709
straightforward I'm not just talking

00:02:11,090 --> 00:02:19,340
about something like the reduce which is

00:02:15,709 --> 00:02:24,349
massively parallel inclusive scan which

00:02:19,340 --> 00:02:25,670
is not as obviously concurrently where

00:02:24,349 --> 00:02:26,989
is actually one of the important

00:02:25,670 --> 00:02:28,790
building blocks of other parallel

00:02:26,989 --> 00:02:32,920
algorithms so that you don't have to

00:02:28,790 --> 00:02:35,239
who all that leverage for these things I

00:02:32,920 --> 00:02:38,180
basically start off this and version

00:02:35,239 --> 00:02:41,599
from a textbook from introduction to

00:02:38,180 --> 00:02:42,560
algorithms and work on that to basically

00:02:41,599 --> 00:02:44,810
make it work

00:02:42,560 --> 00:02:47,870
I've want to provide versions using a

00:02:44,810 --> 00:02:52,459
few concurrency sings systems which are

00:02:47,870 --> 00:02:54,920
not standard or at least not I system as

00:02:52,459 --> 00:02:57,799
part of C++ are not like an open P and

00:02:54,920 --> 00:03:03,440
building blocks and then I built also

00:02:57,799 --> 00:03:07,930
version which entirely builds just on so

00:03:03,440 --> 00:03:11,030
let's start of this the simple thing of

00:03:07,930 --> 00:03:13,609
scan in the standard library

00:03:11,030 --> 00:03:17,150
it used to be available as partial sum

00:03:13,609 --> 00:03:18,980
or if it was partial sum but the

00:03:17,150 --> 00:03:21,200
parallel version and a new word

00:03:18,980 --> 00:03:24,019
inclusive scan what it does is basically

00:03:21,200 --> 00:03:28,459
it just computes as on some of the

00:03:24,019 --> 00:03:31,790
elements and unlike accumulate it also

00:03:28,459 --> 00:03:34,250
stores all the intermediate values so

00:03:31,790 --> 00:03:35,930
over here the input the output let's go

00:03:34,250 --> 00:03:37,519
and look at how it is computed is very

00:03:35,930 --> 00:03:39,620
simple for the first element we just

00:03:37,519 --> 00:03:41,750
copy for the next one you basically take

00:03:39,620 --> 00:03:44,690
the previous one and at the next one and

00:03:41,750 --> 00:03:47,389
we keep doing that and if you look at as

00:03:44,690 --> 00:03:49,940
kind of all right each value depends on

00:03:47,389 --> 00:03:51,739
the one before that's slightly annoying

00:03:49,940 --> 00:03:54,430
we cannot just compute things nicely in

00:03:51,739 --> 00:03:56,949
power we need to have the values before

00:03:54,430 --> 00:03:59,419
before going into the parallel version

00:03:56,949 --> 00:04:02,120
the sequential version is really really

00:03:59,419 --> 00:04:05,870
straightforward here's an simple

00:04:02,120 --> 00:04:06,530
implementation it's basically split into

00:04:05,870 --> 00:04:10,129
two parts

00:04:06,530 --> 00:04:12,500
to get all the necessary overloads the

00:04:10,129 --> 00:04:14,359
first version is basically just taking

00:04:12,500 --> 00:04:18,769
an input range and output range and

00:04:14,359 --> 00:04:20,570
operation and just computes and

00:04:18,769 --> 00:04:23,000
basically all it does is it Atilla gates

00:04:20,570 --> 00:04:25,039
two different algorithm that's fairly

00:04:23,000 --> 00:04:26,750
common practice you get if you went to

00:04:25,039 --> 00:04:28,810
Connors talk they said all right all

00:04:26,750 --> 00:04:32,270
these algorithms are actually

00:04:28,810 --> 00:04:33,919
effectively just read use this is a

00:04:32,270 --> 00:04:36,770
similar case over here

00:04:33,919 --> 00:04:39,349
we delegate inclusive scan of one

00:04:36,770 --> 00:04:41,210
version to a different word and the

00:04:39,349 --> 00:04:42,630
other one just doesn't computation see

00:04:41,210 --> 00:04:44,400
the key difference is

00:04:42,630 --> 00:04:49,320
we have an initial value and we can just

00:04:44,400 --> 00:04:50,880
deal this initial value so simpie does

00:04:49,320 --> 00:04:52,620
basically yeah okay we have a special

00:04:50,880 --> 00:04:54,600
case for valves there's nothing to do

00:04:52,620 --> 00:04:56,820
and we don't do that and otherwise we

00:04:54,600 --> 00:05:00,570
can we now have an initial value we can

00:04:56,820 --> 00:05:02,760
work off but this is the reason the

00:05:00,570 --> 00:05:05,760
initial value is the cause of a lot of

00:05:02,760 --> 00:05:07,650
+1 sin and some of my code to basically

00:05:05,760 --> 00:05:11,520
say I write B set that off and we passed

00:05:07,650 --> 00:05:13,290
the initial value or the first value and

00:05:11,520 --> 00:05:16,890
the Bank as the initial value so we

00:05:13,290 --> 00:05:20,190
don't need to deal with that and then it

00:05:16,890 --> 00:05:23,120
just it right so okay that doesn't look

00:05:20,190 --> 00:05:25,470
super suitable for parallelization and

00:05:23,120 --> 00:05:28,170
indeed when you do paralyze this

00:05:25,470 --> 00:05:32,100
algorithm you actually do end up using

00:05:28,170 --> 00:05:34,680
doing more work that is if we if that

00:05:32,100 --> 00:05:35,970
were a sequential approach you would

00:05:34,680 --> 00:05:38,280
actually spend more time

00:05:35,970 --> 00:05:40,200
however because things are parallel and

00:05:38,280 --> 00:05:42,630
we can actually use more processors to

00:05:40,200 --> 00:05:44,820
to do more work you may bring down the

00:05:42,630 --> 00:05:48,000
overall time by actually spending

00:05:44,820 --> 00:05:50,760
instead more work this is the base so

00:05:48,000 --> 00:05:53,100
yeah rough ideas yeah we decompose the

00:05:50,760 --> 00:05:55,740
algorithm the sequences into two and

00:05:53,100 --> 00:05:58,260
keep doing that and we compute basically

00:05:55,740 --> 00:06:02,280
the initial values of what we need to

00:05:58,260 --> 00:06:04,350
store of intermediate steps and then we

00:06:02,280 --> 00:06:08,520
just distribute that or to write it

00:06:04,350 --> 00:06:10,560
differently so we can compute if you

00:06:08,520 --> 00:06:13,500
have the middle let's say we have the

00:06:10,560 --> 00:06:15,690
middle value of initial sum we can

00:06:13,500 --> 00:06:17,820
basically fill the two sequences and

00:06:15,690 --> 00:06:19,320
parallel this works nicely and then

00:06:17,820 --> 00:06:22,200
obviously if you do that instead of

00:06:19,320 --> 00:06:25,710
doing it was just two we can do that was

00:06:22,200 --> 00:06:28,110
4 or 8 or 16 depending on how many

00:06:25,710 --> 00:06:30,180
processes we have we can do that nicely

00:06:28,110 --> 00:06:33,090
in power so all we need to achieve is

00:06:30,180 --> 00:06:35,420
compute things quickly and we can do

00:06:33,090 --> 00:06:39,450
that in log n time such as effectively

00:06:35,420 --> 00:06:42,240
multiple accumulates running and then we

00:06:39,450 --> 00:06:45,240
can also in log n time distribute things

00:06:42,240 --> 00:06:47,340
so all in all the algorithm spends

00:06:45,240 --> 00:06:50,550
orphaned work but actually just

00:06:47,340 --> 00:06:53,190
overlooked in time assuming we have

00:06:50,550 --> 00:06:55,169
infinite processes okay so the

00:06:53,190 --> 00:06:56,370
implementation or the original algorithm

00:06:55,169 --> 00:06:59,490
of that

00:06:56,370 --> 00:07:01,470
I've taken from from this book when I

00:06:59,490 --> 00:07:03,630
mentioned the name nobody recognizes I

00:07:01,470 --> 00:07:08,729
hope the picture makes it recognize for

00:07:03,630 --> 00:07:10,080
most people opals I know it just three

00:07:08,729 --> 00:07:13,050
horses but I don't think there was a

00:07:10,080 --> 00:07:14,750
parallel algorithm in there and this is

00:07:13,050 --> 00:07:19,620
how the algorithm looks over there

00:07:14,750 --> 00:07:21,360
there's basically there - I wouldn't

00:07:19,620 --> 00:07:24,120
being called a scan app which basically

00:07:21,360 --> 00:07:26,250
computes intermediate sums and then P

00:07:24,120 --> 00:07:29,190
scan down which distributes them and

00:07:26,250 --> 00:07:33,810
this is basically the initial algorithm

00:07:29,190 --> 00:07:37,410
which basically does orchestrate that so

00:07:33,810 --> 00:07:38,490
implementing that algorithm and so the

00:07:37,410 --> 00:07:40,620
first step is actually taking the

00:07:38,490 --> 00:07:42,990
algorithm and translating it to C++

00:07:40,620 --> 00:07:45,330
well that's really straightforward this

00:07:42,990 --> 00:07:47,550
is actually just the same steps and it's

00:07:45,330 --> 00:07:48,870
not super interesting it gets a little

00:07:47,550 --> 00:07:51,150
bit more interesting if you look at

00:07:48,870 --> 00:07:52,830
something like piece came up because

00:07:51,150 --> 00:07:55,680
there's actually concurrency happening

00:07:52,830 --> 00:07:59,699
so this pidgin language over here

00:07:55,680 --> 00:08:01,620
basically has a spawn operation to kick

00:07:59,699 --> 00:08:04,349
off a thread so basically the spawn

00:08:01,620 --> 00:08:06,690
execute concurrently to the non-sport

00:08:04,349 --> 00:08:10,260
version and that adds and sewing we

00:08:06,690 --> 00:08:12,599
basically expect both of lis or the

00:08:10,260 --> 00:08:14,760
other threads which was born to come

00:08:12,599 --> 00:08:19,700
back and after sewing we have basically

00:08:14,760 --> 00:08:22,229
the results of both of these in place so

00:08:19,700 --> 00:08:24,360
translating that to C++ is also fairly

00:08:22,229 --> 00:08:26,580
straightforward and I'm using something

00:08:24,360 --> 00:08:30,270
that was sort of being removed and not

00:08:26,580 --> 00:08:32,039
being removed stood acing so for what

00:08:30,270 --> 00:08:33,690
people told me yeah no suit async is a

00:08:32,039 --> 00:08:36,209
way to go if you want to sing some

00:08:33,690 --> 00:08:38,099
parallel use stood a sink and then they

00:08:36,209 --> 00:08:39,719
go well it doesn't quite have the right

00:08:38,099 --> 00:08:41,400
interface it produces the wrong future

00:08:39,719 --> 00:08:45,120
as far as I understand it that's fine

00:08:41,400 --> 00:08:47,640
let's use to tasting and we basically

00:08:45,120 --> 00:08:49,529
kick off a thread by rotating and we

00:08:47,640 --> 00:08:53,250
just wait for it and then the other

00:08:49,529 --> 00:08:56,570
thing same same thing the wave works

00:08:53,250 --> 00:08:59,640
effectively is it basically stores

00:08:56,570 --> 00:09:02,820
things over again these temp variable

00:08:59,640 --> 00:09:06,350
which is basically an intermediate stage

00:09:02,820 --> 00:09:08,700
and then actually distributes the

00:09:06,350 --> 00:09:10,260
algorithm if you look at it over here

00:09:08,700 --> 00:09:14,040
actually has a

00:09:10,260 --> 00:09:15,660
these two internal arrays why and he to

00:09:14,040 --> 00:09:17,250
basically store the intermediate stage

00:09:15,660 --> 00:09:21,330
it would be nice not to have

00:09:17,250 --> 00:09:23,430
intermediate memory that are saying to

00:09:21,330 --> 00:09:26,060
make a algorithm actually effective

00:09:23,430 --> 00:09:30,570
there will be some extra memory needed

00:09:26,060 --> 00:09:32,790
okay so this is a one direction the

00:09:30,570 --> 00:09:34,770
other Keo direction is actually just the

00:09:32,790 --> 00:09:38,430
same thing it basically just decomposes

00:09:34,770 --> 00:09:41,130
everything nicely and spawn threats and

00:09:38,430 --> 00:09:43,700
then say and again we do that with async

00:09:41,130 --> 00:09:46,650
and wait and that should be fine so

00:09:43,700 --> 00:09:49,950
basically this is a literal translation

00:09:46,650 --> 00:09:51,540
of Yahoo if anybody has experience was

00:09:49,950 --> 00:09:53,910
taking a textbook and translating an

00:09:51,540 --> 00:09:56,130
algorithm and trying to run it you would

00:09:53,910 --> 00:09:58,470
not expect it to run particularly fast

00:09:56,130 --> 00:10:01,230
I guess so let's see how bad that one's

00:09:58,470 --> 00:10:04,260
first off it's important to mention on

00:10:01,230 --> 00:10:07,410
what kind of machine I'm executing

00:10:04,260 --> 00:10:09,210
things the stuff is executed on a

00:10:07,410 --> 00:10:11,430
machinist actually has quite a number of

00:10:09,210 --> 00:10:13,860
course 64 courses and Knights lending

00:10:11,430 --> 00:10:17,490
machine they're all on one chip it has

00:10:13,860 --> 00:10:19,950
16 gigs of relatively fast access memory

00:10:17,490 --> 00:10:22,080
on that ship in addition to whatever

00:10:19,950 --> 00:10:24,300
caches this has so it has basically and

00:10:22,080 --> 00:10:26,910
then as four times I first read it so if

00:10:24,300 --> 00:10:28,980
you run a parallel program over there

00:10:26,910 --> 00:10:32,100
son at libraries typically report that

00:10:28,980 --> 00:10:34,770
it has 256 Hardware threads so it should

00:10:32,100 --> 00:10:37,050
be actually massively faster now we all

00:10:34,770 --> 00:10:39,330
know that memory is in so contention and

00:10:37,050 --> 00:10:44,370
actually make running the sing on let's

00:10:39,330 --> 00:10:47,510
say just reduce and inclusive scan on

00:10:44,370 --> 00:10:50,340
integers actually doesn't give a lot of

00:10:47,510 --> 00:10:51,990
benefit at least not as massive as I

00:10:50,340 --> 00:10:54,030
would have hoped for that

00:10:51,990 --> 00:10:56,490
so I basically compute something which

00:10:54,030 --> 00:11:01,080
is made a problem let's say we compute a

00:10:56,490 --> 00:11:03,450
sequence of four four times four float

00:11:01,080 --> 00:11:05,910
matrices just computed against each

00:11:03,450 --> 00:11:08,280
other they're more interesting use cases

00:11:05,910 --> 00:11:10,680
which are just harder to explain and I

00:11:08,280 --> 00:11:12,360
think people understand what how matrix

00:11:10,680 --> 00:11:13,710
multiplication looks like they actually

00:11:12,360 --> 00:11:15,810
doesn't number of operations on that

00:11:13,710 --> 00:11:18,180
it's just intended to not make it

00:11:15,810 --> 00:11:22,440
entirely memory though and then then I

00:11:18,180 --> 00:11:23,370
will show graphs and low is good high as

00:11:22,440 --> 00:11:26,220
bad

00:11:23,370 --> 00:11:31,080
so all of the graphs show something like

00:11:26,220 --> 00:11:35,610
that you see it maybe you basically see

00:11:31,080 --> 00:11:39,180
at the bottom the light blue line this

00:11:35,610 --> 00:11:40,890
is the benchmark this is what I mean

00:11:39,180 --> 00:11:43,500
with benchmark this is a sequential

00:11:40,890 --> 00:11:45,600
version and the parallel version we just

00:11:43,500 --> 00:11:50,340
implemented this at orange is that

00:11:45,600 --> 00:11:52,650
violet line that is not good so we just

00:11:50,340 --> 00:11:54,150
did some work and we made it all ice and

00:11:52,650 --> 00:11:58,050
parallel and probably use quite a bit of

00:11:54,150 --> 00:12:00,750
CPU power and it get something in the

00:11:58,050 --> 00:12:03,480
order of thousand five hundred times

00:12:00,750 --> 00:12:05,610
slower you want to have it a lot faster

00:12:03,480 --> 00:12:07,950
at the bottom you see a number of other

00:12:05,610 --> 00:12:10,470
guys I would like to find those so the

00:12:07,950 --> 00:12:13,820
first step is the algorithm actually

00:12:10,470 --> 00:12:15,060
creates a lot of tiny tasks for each

00:12:13,820 --> 00:12:17,480
element

00:12:15,060 --> 00:12:20,250
it basically factors before it creates

00:12:17,480 --> 00:12:21,810
well not for all of them but for half of

00:12:20,250 --> 00:12:24,270
the elements it basically creates at

00:12:21,810 --> 00:12:26,570
least and or well actually it's more

00:12:24,270 --> 00:12:30,450
than that it creates a lot of these

00:12:26,570 --> 00:12:32,910
acing futures so huge amount of futures

00:12:30,450 --> 00:12:34,740
generated we don't necessarily need to

00:12:32,910 --> 00:12:37,080
do that once we actually have a range

00:12:34,740 --> 00:12:40,260
which is reasonably small we can reason

00:12:37,080 --> 00:12:42,540
when we decide let's stop trying to do

00:12:40,260 --> 00:12:45,120
something concurrently over here and we

00:12:42,540 --> 00:12:48,360
just do it sequentially on that level

00:12:45,120 --> 00:12:50,730
and then we execute these and power so

00:12:48,360 --> 00:12:54,380
effectively you change the algorithm you

00:12:50,730 --> 00:12:54,380
main change over here is just on that

00:12:55,040 --> 00:13:00,690
epaulet line to basically say instead of

00:12:57,900 --> 00:13:03,750
you stop at one when we have moved down

00:13:00,690 --> 00:13:06,420
to the smallest bit we stopped at some

00:13:03,750 --> 00:13:08,880
min size whatever min slices and I

00:13:06,420 --> 00:13:10,800
basically try it as a few variations and

00:13:08,880 --> 00:13:12,570
then once we have reached that level we

00:13:10,800 --> 00:13:15,680
basically just to accumulate because the

00:13:12,570 --> 00:13:19,650
first guy is just computing the sums of

00:13:15,680 --> 00:13:22,140
subsections and likewise in the second

00:13:19,650 --> 00:13:23,940
case because no we cannot actually go we

00:13:22,140 --> 00:13:25,800
need to basically change these things in

00:13:23,940 --> 00:13:28,230
lockstep because otherwise we wouldn't

00:13:25,800 --> 00:13:30,840
compute all the sums we need this guy we

00:13:28,230 --> 00:13:32,190
need to actually use the same blocking

00:13:30,840 --> 00:13:33,930
to basically figure out what are the

00:13:32,190 --> 00:13:37,139
values we then need to distribute and so

00:13:33,930 --> 00:13:39,779
we do inclusive scans on the

00:13:37,139 --> 00:13:41,670
submarino's and that improves things

00:13:39,779 --> 00:13:43,470
quite a lot as you could see from that

00:13:41,670 --> 00:13:48,329
graph this is all at the bottom over

00:13:43,470 --> 00:13:54,379
here so is that the computation is let's

00:13:48,329 --> 00:13:56,549
say only 200 times slower okay well okay

00:13:54,379 --> 00:14:00,269
I'm not going to stop here that would be

00:13:56,549 --> 00:14:02,129
bad right so the thing is over here I'm

00:14:00,269 --> 00:14:04,199
just using AC which was something which

00:14:02,129 --> 00:14:07,049
mice as nicely provided but it's not

00:14:04,199 --> 00:14:09,629
lovely but actually I just need to get

00:14:07,049 --> 00:14:11,730
some drops scheduled and async is

00:14:09,629 --> 00:14:14,609
certainly one way to do that there are

00:14:11,730 --> 00:14:17,910
other ways to do it and one way we could

00:14:14,609 --> 00:14:20,129
do is let's say I use thread building

00:14:17,910 --> 00:14:22,019
drugs the reason I use red building

00:14:20,129 --> 00:14:23,639
blocks is basically just I know that for

00:14:22,019 --> 00:14:26,160
long that time the other libraries which

00:14:23,639 --> 00:14:30,359
do something very similar in that space

00:14:26,160 --> 00:14:31,739
I just haven't used them so instead of

00:14:30,359 --> 00:14:34,429
building blocks as something like at a

00:14:31,739 --> 00:14:38,179
school the task group basically

00:14:34,429 --> 00:14:41,040
aggregates drops you stick jobs in and

00:14:38,179 --> 00:14:43,319
eventually it was paid for all the jobs

00:14:41,040 --> 00:14:46,160
to finish and this was basically the

00:14:43,319 --> 00:14:50,489
equivalent of the synchronization same

00:14:46,160 --> 00:14:52,019
tasks would run is the same thing as

00:14:50,489 --> 00:14:53,759
basically you had scheduled that job and

00:14:52,019 --> 00:14:55,829
run it elsewhere they actually didn't do

00:14:53,759 --> 00:14:56,429
a massive change in the structure of

00:14:55,829 --> 00:14:58,470
that thing

00:14:56,429 --> 00:15:02,399
instead of using async in the future I

00:14:58,470 --> 00:15:04,799
use a pass group and the right thing not

00:15:02,399 --> 00:15:07,980
not dramatic and the same over here just

00:15:04,799 --> 00:15:10,290
use the task group again run run a job

00:15:07,980 --> 00:15:17,009
and the job is just the same as before

00:15:10,290 --> 00:15:21,959
and then and hopefully this makes things

00:15:17,009 --> 00:15:23,429
better and actually it does so I'm not

00:15:21,959 --> 00:15:25,319
excluding the possibility unless

00:15:23,429 --> 00:15:28,139
actually potentially quite likely that

00:15:25,319 --> 00:15:29,970
I've just missed used async and I

00:15:28,139 --> 00:15:32,939
correctly used the task groups to make

00:15:29,970 --> 00:15:37,230
it work but the task group made really

00:15:32,939 --> 00:15:40,169
easy to make that right so over here you

00:15:37,230 --> 00:15:43,999
can see the blue line this is basically

00:15:40,169 --> 00:15:46,949
the fret lionesses it's just the

00:15:43,999 --> 00:15:51,049
benchmarking the corresponding sizes so

00:15:46,949 --> 00:15:51,049
basically on the going form

00:15:51,110 --> 00:15:57,260
left-right it's basically smaller

00:15:53,790 --> 00:16:01,620
sequences up to bigger sequences and

00:15:57,260 --> 00:16:04,380
basically the the blue line is doing it

00:16:01,620 --> 00:16:06,350
was sequentially so in the beginning I

00:16:04,380 --> 00:16:08,550
don't make any benefit but once the

00:16:06,350 --> 00:16:12,420
sequences become big enough there is at

00:16:08,550 --> 00:16:15,300
least an overall benefit so that's a

00:16:12,420 --> 00:16:20,810
good start there's actually a potential

00:16:15,300 --> 00:16:24,720
to to actually get the responses faster

00:16:20,810 --> 00:16:27,170
so the next stage of that is to say well

00:16:24,720 --> 00:16:30,060
what is he actually instead of

00:16:27,170 --> 00:16:33,240
reformulated as kind of which is natural

00:16:30,060 --> 00:16:36,870
for a nice divide and conquer algorithm

00:16:33,240 --> 00:16:41,220
instead of decomposing it basically and

00:16:36,870 --> 00:16:43,200
to in a recursive hierarchy or a very

00:16:41,220 --> 00:16:44,430
basic instead of doing recursion every

00:16:43,200 --> 00:16:45,990
time you recurse over if you don't

00:16:44,430 --> 00:16:48,960
really request but we actually just

00:16:45,990 --> 00:16:51,780
start another drop instead of doing that

00:16:48,960 --> 00:16:54,000
you could actually turn the algorithm a

00:16:51,780 --> 00:16:56,490
little bit around and basically make it

00:16:54,000 --> 00:16:59,750
iterative the nice thing of doing that

00:16:56,490 --> 00:17:02,730
is that tools like openmp which can

00:16:59,750 --> 00:17:04,890
paralyze loops so if you actually have a

00:17:02,730 --> 00:17:07,170
nice loop which does all the which needs

00:17:04,890 --> 00:17:08,850
to be paralyzed we would be in a

00:17:07,170 --> 00:17:12,510
position to use that also at building

00:17:08,850 --> 00:17:14,550
blocks also and loop constructed which

00:17:12,510 --> 00:17:17,910
make it easier the standard library

00:17:14,550 --> 00:17:19,470
algorithms unfortunately not so much at

00:17:17,910 --> 00:17:21,720
least not on that level there's

00:17:19,470 --> 00:17:23,579
obviously and something like an parallel

00:17:21,720 --> 00:17:25,350
for each but the parallel for each

00:17:23,579 --> 00:17:27,480
actually chop things up internally

00:17:25,350 --> 00:17:30,660
without your control and basically you

00:17:27,480 --> 00:17:32,310
don't know where did it split but we

00:17:30,660 --> 00:17:33,000
would like to know all right I actually

00:17:32,310 --> 00:17:35,550
want to

00:17:33,000 --> 00:17:37,260
I have already chunk things up and I

00:17:35,550 --> 00:17:39,660
want to schedule these things so

00:17:37,260 --> 00:17:42,930
standard lab we didn't really help me as

00:17:39,660 --> 00:17:45,570
much over them but anyway let's turn the

00:17:42,930 --> 00:17:50,790
algorithm into inclusive scan and make

00:17:45,570 --> 00:17:53,160
that basically an iterative approach it

00:17:50,790 --> 00:17:55,920
doesn't fit on one slide so I basically

00:17:53,160 --> 00:17:58,920
have this these things basically say it

00:17:55,920 --> 00:18:01,890
consists of stages the first stages

00:17:58,920 --> 00:18:04,300
let's set up some auxiliary stuff which

00:18:01,890 --> 00:18:07,000
is basically just some memory then

00:18:04,300 --> 00:18:09,250
we compute the sum of subsequences then

00:18:07,000 --> 00:18:12,610
instead of actually recursing on the on

00:18:09,250 --> 00:18:15,070
the thing I just use an inclusive scan

00:18:12,610 --> 00:18:17,560
effectively to compute the actual

00:18:15,070 --> 00:18:20,080
elements we need so basically first car

00:18:17,560 --> 00:18:23,050
is just computing the elements of the

00:18:20,080 --> 00:18:25,900
ranges concurrently then we do an

00:18:23,050 --> 00:18:28,780
inclusive scan to compute the the things

00:18:25,900 --> 00:18:30,280
that could be parallel as well but I

00:18:28,780 --> 00:18:32,800
don't think we have actually big enough

00:18:30,280 --> 00:18:36,250
computers for that to make it worthwhile

00:18:32,800 --> 00:18:39,400
to make that power and then we combined

00:18:36,250 --> 00:18:41,050
them again so the auxiliary stuff is

00:18:39,400 --> 00:18:44,790
basically just a number of type deaths

00:18:41,050 --> 00:18:46,720
and the intermediate storage

00:18:44,790 --> 00:18:48,820
intermediate storage over here I'm a

00:18:46,720 --> 00:18:51,280
little bit more careful about the value

00:18:48,820 --> 00:18:54,070
type I get out of years of one part of

00:18:51,280 --> 00:18:56,440
that degree is all just computing the

00:18:54,070 --> 00:18:58,240
value type once I get the value type I

00:18:56,440 --> 00:18:59,830
make the assumption that I actually

00:18:58,240 --> 00:19:02,680
don't know much about that value type

00:18:59,830 --> 00:19:05,410
for example I don't necessarily assume

00:19:02,680 --> 00:19:06,940
that it can be defaults on structures so

00:19:05,410 --> 00:19:08,740
I just create a container where I can

00:19:06,940 --> 00:19:10,900
stick things in which cannot be default

00:19:08,740 --> 00:19:13,360
constructed and I can just set them

00:19:10,900 --> 00:19:15,430
later the normal approach and other

00:19:13,360 --> 00:19:16,660
generic coders you typically wait until

00:19:15,430 --> 00:19:18,340
you have the object and you stick it in

00:19:16,660 --> 00:19:20,620
the container at that point in time that

00:19:18,340 --> 00:19:22,060
doesn't work comparable particularly

00:19:20,620 --> 00:19:25,090
well because you never know which thread

00:19:22,060 --> 00:19:27,700
is ready when so basically distributing

00:19:25,090 --> 00:19:31,600
that stuff ahead of time and creating

00:19:27,700 --> 00:19:35,080
space is kind of necessary okay

00:19:31,600 --> 00:19:38,020
so that said computing the sums of sub

00:19:35,080 --> 00:19:39,520
sequences well all we do over here and

00:19:38,020 --> 00:19:41,800
print principle this is obviously a

00:19:39,520 --> 00:19:43,900
sequential version of office stuff and

00:19:41,800 --> 00:19:47,950
we just compute the sums of the sub

00:19:43,900 --> 00:19:52,570
sequences there's one highlighted the

00:19:47,950 --> 00:19:54,940
reason for that is for the last chunk we

00:19:52,570 --> 00:19:57,420
actually don't need to compute the sum

00:19:54,940 --> 00:20:00,310
anymore you only need this initial value

00:19:57,420 --> 00:20:01,900
so we don't need to deal with the end

00:20:00,310 --> 00:20:03,580
condition that there's a basically

00:20:01,900 --> 00:20:06,250
arrange which may not be the right size

00:20:03,580 --> 00:20:07,750
we only deal this the chunks up to the

00:20:06,250 --> 00:20:12,130
that thing they're all the same size

00:20:07,750 --> 00:20:13,720
that makes that loop relatively easy and

00:20:12,130 --> 00:20:17,120
all it does is it basically uses the

00:20:13,720 --> 00:20:19,789
integer and computes a

00:20:17,120 --> 00:20:23,029
this loop is nicely accessible to openmp

00:20:19,789 --> 00:20:26,779
we want to get that parallel we just

00:20:23,029 --> 00:20:29,149
slop OpenMP pragma OMP power 4 on top of

00:20:26,779 --> 00:20:33,049
it and then open people actually nicely

00:20:29,149 --> 00:20:35,360
go yep I can paralyze that we can do

00:20:33,049 --> 00:20:37,249
pretty much the same as I did before

00:20:35,360 --> 00:20:39,200
there's a task group to basically say

00:20:37,249 --> 00:20:40,970
alright I can actually schedule these

00:20:39,200 --> 00:20:44,749
things in parallel as well using at a

00:20:40,970 --> 00:20:47,690
school and start building blocks as in

00:20:44,749 --> 00:20:53,379
for each construct that looks like that

00:20:47,690 --> 00:20:57,320
which actually basically provides and

00:20:53,379 --> 00:20:59,749
the function this an index and executes

00:20:57,320 --> 00:21:01,580
these things and empower and that's

00:20:59,749 --> 00:21:04,580
quite nice because it promises that each

00:21:01,580 --> 00:21:07,399
index as well it somehow

00:21:04,580 --> 00:21:09,169
paralyzes on the indices I probably

00:21:07,399 --> 00:21:11,330
kicks them off in power in some shape or

00:21:09,169 --> 00:21:14,629
form they don't really document that and

00:21:11,330 --> 00:21:18,559
foddy how that is done but it seems to

00:21:14,629 --> 00:21:20,960
be nicely purl so we can do that and to

00:21:18,559 --> 00:21:25,659
do something similar like like that in

00:21:20,960 --> 00:21:27,799
an in a kind of for each algorithm well

00:21:25,659 --> 00:21:30,110
there's something slightly annoying in

00:21:27,799 --> 00:21:32,210
there and that is to actually store

00:21:30,110 --> 00:21:33,860
things into that intermediate array I

00:21:32,210 --> 00:21:36,399
actually would like to get an index I

00:21:33,860 --> 00:21:38,690
would like to know the chunk number and

00:21:36,399 --> 00:21:40,129
yeah I can implement that in the

00:21:38,690 --> 00:21:43,429
algorithm but then the algorithm becomes

00:21:40,129 --> 00:21:45,559
somewhat convoluted so I basically am a

00:21:43,429 --> 00:21:47,600
busy idea to say alright I actually have

00:21:45,559 --> 00:21:49,549
a for each and I do the for each for

00:21:47,600 --> 00:21:51,409
each sub range and what that Singh

00:21:49,549 --> 00:21:53,539
produces as you can basically see it or

00:21:51,409 --> 00:21:57,379
from the signature of the green lambda

00:21:53,539 --> 00:21:59,629
it produces an index and a begin and end

00:21:57,379 --> 00:22:02,690
of the sub range so basically get all

00:21:59,629 --> 00:22:05,840
right this is the ice-sub range and this

00:22:02,690 --> 00:22:08,240
is what the range actually is and over

00:22:05,840 --> 00:22:10,159
later showed how that is implemented so

00:22:08,240 --> 00:22:12,200
it's basically chunking up the

00:22:10,159 --> 00:22:16,039
intermediate commercial computation

00:22:12,200 --> 00:22:18,799
that's relatively straightforward then

00:22:16,039 --> 00:22:20,360
the intermediate over here I actually

00:22:18,799 --> 00:22:22,549
should have changed partial sum over

00:22:20,360 --> 00:22:23,749
here to include an inclusive scan but it

00:22:22,549 --> 00:22:26,389
doesn't matter it's just the same thing

00:22:23,749 --> 00:22:29,169
it's just a sequential version of let's

00:22:26,389 --> 00:22:30,390
compute things in that immediate area

00:22:29,169 --> 00:22:32,460
once

00:22:30,390 --> 00:22:35,070
which is slightly annoying over here as

00:22:32,460 --> 00:22:37,620
the intermediate array actually is

00:22:35,070 --> 00:22:40,170
optionals they would all be populated by

00:22:37,620 --> 00:22:43,800
the time we get here but we need to have

00:22:40,170 --> 00:22:46,140
that I didn't highlight that we need to

00:22:43,800 --> 00:22:48,090
dereference the elements we get to get

00:22:46,140 --> 00:22:49,980
the actual value of the optional rather

00:22:48,090 --> 00:22:54,380
than the option we want to apply the

00:22:49,980 --> 00:22:54,380
operation which we get to the

00:22:54,500 --> 00:22:58,800
and then the other end is more of the

00:22:58,350 --> 00:23:00,810
same

00:22:58,800 --> 00:23:04,380
you basically again go over the entire

00:23:00,810 --> 00:23:06,870
sequence this time we need to process

00:23:04,380 --> 00:23:09,270
all sequences so there's actually

00:23:06,870 --> 00:23:12,000
special handling of the end sequence of

00:23:09,270 --> 00:23:14,370
the end chunk so most of the chance or

00:23:12,000 --> 00:23:16,950
equal sized but the last ones may have a

00:23:14,370 --> 00:23:22,500
smaller size this is a reason for for

00:23:16,950 --> 00:23:24,210
the green stuff at the back and then the

00:23:22,500 --> 00:23:27,210
next stage is to basically compute an

00:23:24,210 --> 00:23:29,100
initial value of the inclusive skin so

00:23:27,210 --> 00:23:30,960
that from then on everything works

00:23:29,100 --> 00:23:32,670
nicely and I do the same thing as I did

00:23:30,960 --> 00:23:35,070
to see

00:23:32,670 --> 00:23:37,440
so you basically you write I take the

00:23:35,070 --> 00:23:41,010
value I get from the intermediate thing

00:23:37,440 --> 00:23:43,440
this is the star temp of the I minus I

00:23:41,010 --> 00:23:48,090
minus one thing he together with the

00:23:43,440 --> 00:23:50,070
current position and basically compute

00:23:48,090 --> 00:23:52,160
compute that there's a little bit of

00:23:50,070 --> 00:23:54,600
triggering triggering about the C

00:23:52,160 --> 00:23:56,340
iterators over here because I actually

00:23:54,600 --> 00:23:58,290
have two sequences which are parallel

00:23:56,340 --> 00:24:00,870
the the two sequence we need to write

00:23:58,290 --> 00:24:02,910
things to and the source sequence we get

00:24:00,870 --> 00:24:05,730
sings from so I basically need to

00:24:02,910 --> 00:24:09,390
compute the start position twice and

00:24:05,730 --> 00:24:11,060
that happens in both of these cases but

00:24:09,390 --> 00:24:15,060
this is relatively straightforward

00:24:11,060 --> 00:24:18,240
translation of the original so and then

00:24:15,060 --> 00:24:21,900
again making that parallel because that

00:24:18,240 --> 00:24:26,010
does no loop we can make almost our

00:24:21,900 --> 00:24:28,230
suits or as parallel for or again

00:24:26,010 --> 00:24:30,660
although this becomes again little bit

00:24:28,230 --> 00:24:33,060
more complicated this is for each sub

00:24:30,660 --> 00:24:35,220
range and the for each sub range would

00:24:33,060 --> 00:24:36,810
basically execute things and there's

00:24:35,220 --> 00:24:38,550
something in in there which I didn't

00:24:36,810 --> 00:24:41,340
mention before I basically pretend I

00:24:38,550 --> 00:24:43,770
know what an executor is the Senate

00:24:41,340 --> 00:24:44,370
committee is in discussions what an

00:24:43,770 --> 00:24:49,350
execute

00:24:44,370 --> 00:24:51,390
since probably 10 years and hopefully

00:24:49,350 --> 00:24:53,760
we'll discuss only another two years or

00:24:51,390 --> 00:24:55,169
so so we get into 23 and we actually get

00:24:53,760 --> 00:24:57,929
these things into the standard library

00:24:55,169 --> 00:24:59,760
it would be really nice but all I need

00:24:57,929 --> 00:25:01,440
over here is something I assume the

00:24:59,760 --> 00:25:06,630
saying has an operation execute and I

00:25:01,440 --> 00:25:10,740
execute a function okay so was that I

00:25:06,630 --> 00:25:14,419
have implementations and the to

00:25:10,740 --> 00:25:21,690
basically run things and browser and

00:25:14,419 --> 00:25:24,419
this is how the graph looks like and

00:25:21,690 --> 00:25:25,830
what that does and basically it hides a

00:25:24,419 --> 00:25:29,000
little bit what's going on over there so

00:25:25,830 --> 00:25:32,549
basically this is the same graph this

00:25:29,000 --> 00:25:35,970
cut off to make it look a little bit

00:25:32,549 --> 00:25:38,010
nicer so one of the guy the green one

00:25:35,970 --> 00:25:41,250
actually became slower there's no

00:25:38,010 --> 00:25:44,610
surprise this is actually basically the

00:25:41,250 --> 00:25:47,640
sequential algorithm I don't know what

00:25:44,610 --> 00:25:49,200
is actually kind of the same speed at

00:25:47,640 --> 00:25:51,779
the beginning rusty is a little bit

00:25:49,200 --> 00:25:54,240
above but actually you don't even see it

00:25:51,779 --> 00:25:58,289
in the bid for the first couple of

00:25:54,240 --> 00:26:00,149
sequences I guess it is the reason for

00:25:58,289 --> 00:26:03,510
that is most of the stuff is in the end

00:26:00,149 --> 00:26:05,309
stood as memory access and it just gets

00:26:03,510 --> 00:26:06,690
away with accessing things in cache and

00:26:05,309 --> 00:26:10,380
doesn't really matter that much that we

00:26:06,690 --> 00:26:13,919
run over things twice but I don't know

00:26:10,380 --> 00:26:17,610
why that white only pops up later

00:26:13,919 --> 00:26:19,679
the other ones are the so the yellow one

00:26:17,610 --> 00:26:21,360
which does best this is why the part of

00:26:19,679 --> 00:26:23,970
the reason why I'm interested in OpenMP

00:26:21,360 --> 00:26:27,559
it's actually the OpenMP implementation

00:26:23,970 --> 00:26:33,270
that gets paralyzed the things over here

00:26:27,559 --> 00:26:36,179
fastest and the other ones are the the

00:26:33,270 --> 00:26:39,330
blue so in the red one which goes down

00:26:36,179 --> 00:26:46,490
as the disk using those red building

00:26:39,330 --> 00:26:49,409
blocks version and the violet one is oh

00:26:46,490 --> 00:26:52,470
and the violet one is using the red

00:26:49,409 --> 00:26:54,000
building blocks for each the executables

00:26:52,470 --> 00:26:57,690
are not there I first want to explain

00:26:54,000 --> 00:27:00,570
how they work actually sold

00:26:57,690 --> 00:27:03,870
okay so this is actually quite a bit of

00:27:00,570 --> 00:27:07,620
speed up on that workload at least okay

00:27:03,870 --> 00:27:09,509
so the algorithm which I'm kind of

00:27:07,620 --> 00:27:11,909
missing from the standard library so far

00:27:09,509 --> 00:27:13,350
and maybe we get something like that to

00:27:11,909 --> 00:27:16,860
help implementing other parallel

00:27:13,350 --> 00:27:20,039
algorithms is something like for each

00:27:16,860 --> 00:27:22,529
sub range the idea of a sub range as I

00:27:20,039 --> 00:27:25,200
mentioned as basically we basically

00:27:22,529 --> 00:27:27,480
produce an index of the current range

00:27:25,200 --> 00:27:29,639
and we basically also say this is begin

00:27:27,480 --> 00:27:31,799
at the end of the range I think there's

00:27:29,639 --> 00:27:35,759
a quite reasonable interface for

00:27:31,799 --> 00:27:37,409
something if you want to implement over

00:27:35,759 --> 00:27:39,450
here's an complete implementation of

00:27:37,409 --> 00:27:41,519
that it starts off as well if there's

00:27:39,450 --> 00:27:43,350
nothing to it doesn't do anything it

00:27:41,519 --> 00:27:45,720
uses the latch to basically determine

00:27:43,350 --> 00:27:50,519
when we are done so your idea of a latch

00:27:45,720 --> 00:27:52,799
is you basically initialize at once as

00:27:50,519 --> 00:27:56,960
once you thingy you initialize it this

00:27:52,799 --> 00:28:00,769
how many things you expect to arrive and

00:27:56,960 --> 00:28:03,000
they and basically once you have count

00:28:00,769 --> 00:28:05,490
people are calling a write on that see

00:28:03,000 --> 00:28:08,340
anybody who's waiting or trying to wait

00:28:05,490 --> 00:28:10,620
on that ledge is let's rule if you

00:28:08,340 --> 00:28:12,929
basically evade before everybody has

00:28:10,620 --> 00:28:15,059
arrived you basically wait and as get

00:28:12,929 --> 00:28:17,309
blocked over there so basically the

00:28:15,059 --> 00:28:20,129
latches which are saying right we expect

00:28:17,309 --> 00:28:22,889
that number of workloads to be finished

00:28:20,129 --> 00:28:26,330
this is actually it's my own

00:28:22,889 --> 00:28:28,580
implementation but there's ledge coming

00:28:26,330 --> 00:28:31,139
definitely in the standard C++ library

00:28:28,580 --> 00:28:35,789
and I did that at some point I just

00:28:31,139 --> 00:28:37,590
didn't have it available so the other

00:28:35,789 --> 00:28:39,539
part is basically the execute I already

00:28:37,590 --> 00:28:41,909
mentioned that we that are basically

00:28:39,539 --> 00:28:44,009
assumed executors of an interface face

00:28:41,909 --> 00:28:48,690
it says execute and they get one

00:28:44,009 --> 00:28:52,590
function object the sunnat' library or

00:28:48,690 --> 00:28:54,600
they come the paralyzation guys are

00:28:52,590 --> 00:28:58,440
discussing what kind of variations on

00:28:54,600 --> 00:29:00,419
execute we want to have this this

00:28:58,440 --> 00:29:03,570
interface is actually geared only

00:29:00,419 --> 00:29:06,169
towards running things on a cpu but once

00:29:03,570 --> 00:29:09,330
people actually want to run things on an

00:29:06,169 --> 00:29:11,280
on a GPU as well they want more

00:29:09,330 --> 00:29:15,000
something like better execute

00:29:11,280 --> 00:29:17,330
singie as well and but unfortunately the

00:29:15,000 --> 00:29:20,160
entire discussion around how how the

00:29:17,330 --> 00:29:22,560
executor actually looks like exactly is

00:29:20,160 --> 00:29:23,480
blocking the availability or at least

00:29:22,560 --> 00:29:26,700
something simple

00:29:23,480 --> 00:29:29,790
okay there's one one thing I found

00:29:26,700 --> 00:29:31,500
actually in in production code this is

00:29:29,790 --> 00:29:34,080
no production code but I'm also using

00:29:31,500 --> 00:29:38,940
letters in what I do in a day to day

00:29:34,080 --> 00:29:41,820
work and we basically had scheduled and

00:29:38,940 --> 00:29:43,830
cued sings function objects into some

00:29:41,820 --> 00:29:46,740
kind of thread queue and that got full

00:29:43,830 --> 00:29:48,840
and the function object that brought but

00:29:46,740 --> 00:29:51,600
people didn't expect that to happen but

00:29:48,840 --> 00:29:53,580
once a function object got dropped the

00:29:51,600 --> 00:29:56,580
arrive inside the function object which

00:29:53,580 --> 00:29:59,610
may have been called was not caught so

00:29:56,580 --> 00:30:02,040
it was basically a leak and basically we

00:29:59,610 --> 00:30:03,630
had a problem of the latch locking in

00:30:02,040 --> 00:30:05,970
indefinitely because things didn't

00:30:03,630 --> 00:30:08,490
arrive and this is the idea of the

00:30:05,970 --> 00:30:11,060
letter driver over here basically in the

00:30:08,490 --> 00:30:14,850
lambda function I pair I pass in I

00:30:11,060 --> 00:30:16,260
capture and let arriver even if that

00:30:14,850 --> 00:30:18,570
function object gets dropped on the

00:30:16,260 --> 00:30:20,520
floor without ever being executed the

00:30:18,570 --> 00:30:22,950
arrival will be destroyed and the

00:30:20,520 --> 00:30:27,120
arrival in its destructor actually calls

00:30:22,950 --> 00:30:31,200
arrive so there's no kind of leak of

00:30:27,120 --> 00:30:33,570
people expected to arrive that's a tool

00:30:31,200 --> 00:30:37,200
which probably would be also nice to

00:30:33,570 --> 00:30:41,880
have as a complimentary four letters

00:30:37,200 --> 00:30:44,790
possibly for for all the other that's

00:30:41,880 --> 00:30:47,310
called four barriers as well so to

00:30:44,790 --> 00:30:52,800
basically just say right we have a safe

00:30:47,310 --> 00:30:54,720
way to wife or whatever else okay and

00:30:52,800 --> 00:30:56,940
then inside was saying all it does is

00:30:54,720 --> 00:30:59,610
basically in calls a function object

00:30:56,940 --> 00:31:01,260
function object to see the parameter

00:30:59,610 --> 00:31:03,150
should have highlighted that to

00:31:01,260 --> 00:31:06,090
basically say what is being executed and

00:31:03,150 --> 00:31:08,480
this is basically executed once in a

00:31:06,090 --> 00:31:11,670
loop where we actually deal this

00:31:08,480 --> 00:31:15,950
equally-sized sequences and then the

00:31:11,670 --> 00:31:19,380
last guy gets X gets executed

00:31:15,950 --> 00:31:21,480
independently because the last chunk may

00:31:19,380 --> 00:31:24,280
be smaller so we don't have to special

00:31:21,480 --> 00:31:26,320
case inside the loop

00:31:24,280 --> 00:31:29,110
so this basically the four he trains

00:31:26,320 --> 00:31:31,480
thing it hinges on having an executor so

00:31:29,110 --> 00:31:34,030
so far the things which is missing is

00:31:31,480 --> 00:31:36,520
OpenMP and thread building blocks have

00:31:34,030 --> 00:31:40,390
nice tasks at us they actually kind of

00:31:36,520 --> 00:31:42,100
know how how we schedule tasks they I

00:31:40,390 --> 00:31:44,020
don't know how I pop my he does it under

00:31:42,100 --> 00:31:46,300
the hood they must have something like a

00:31:44,020 --> 00:31:48,190
task scheduler start building blocks

00:31:46,300 --> 00:31:49,990
does have a task Elliott definitely and

00:31:48,190 --> 00:31:52,360
they use apparently something like a

00:31:49,990 --> 00:31:54,640
drop scheme stealing a scheduler which

00:31:52,360 --> 00:31:56,620
is very good for certain workloads I

00:31:54,640 --> 00:32:00,750
don't think this workload actually is

00:31:56,620 --> 00:32:03,580
particular geared towards the task al

00:32:00,750 --> 00:32:07,030
the drop stealing's thing because

00:32:03,580 --> 00:32:08,890
actually the tasks themselves don't and

00:32:07,030 --> 00:32:11,230
queue drops again

00:32:08,890 --> 00:32:13,080
it's basically walked us execute and

00:32:11,230 --> 00:32:16,300
queued from the top level rather than

00:32:13,080 --> 00:32:18,280
the tasks and queuing things themselves

00:32:16,300 --> 00:32:21,190
and the drops stealing stuff benefits

00:32:18,280 --> 00:32:24,180
from basically having a deck where the

00:32:21,190 --> 00:32:26,290
task itself pushes through the back and

00:32:24,180 --> 00:32:28,090
consumes directly from there and it

00:32:26,290 --> 00:32:31,060
normally doesn't have any contention on

00:32:28,090 --> 00:32:33,150
that one whereas other drops the drop

00:32:31,060 --> 00:32:35,800
ceiling guys would look at the other end

00:32:33,150 --> 00:32:38,380
over here the workload this whole jobs

00:32:35,800 --> 00:32:42,640
just come into the system to start with

00:32:38,380 --> 00:32:45,340
there's no local processing okay so but

00:32:42,640 --> 00:32:49,680
the missing missing bit is how do we

00:32:45,340 --> 00:32:51,970
actually schedule this thing and the

00:32:49,680 --> 00:32:53,740
that's it has created a really simple

00:32:51,970 --> 00:32:56,260
executor just to get the idea of what an

00:32:53,740 --> 00:32:57,400
executor does and that is this executor

00:32:56,260 --> 00:32:59,650
actually doesn't do things concurrently

00:32:57,400 --> 00:33:02,290
it just schedules the job on the same

00:32:59,650 --> 00:33:04,150
threat if that blocks a little bit of a

00:33:02,290 --> 00:33:06,190
problem which is however actually an

00:33:04,150 --> 00:33:08,500
interesting aspect as well because

00:33:06,190 --> 00:33:10,270
actually if you have an an algorithm

00:33:08,500 --> 00:33:13,330
which blocks if you give it just once

00:33:10,270 --> 00:33:14,680
read and and deadlocks the algorithm

00:33:13,330 --> 00:33:18,400
probably wants to be implemented

00:33:14,680 --> 00:33:20,350
differently so this guy basically just

00:33:18,400 --> 00:33:22,480
all it does is whenever executors call

00:33:20,350 --> 00:33:24,790
it just executes the function object and

00:33:22,480 --> 00:33:26,890
that returns there's not particularly

00:33:24,790 --> 00:33:29,200
interesting but should be invalid

00:33:26,890 --> 00:33:29,770
implementation and then for the other

00:33:29,200 --> 00:33:31,450
guy is

00:33:29,770 --> 00:33:33,520
well we actually probably want a suspect

00:33:31,450 --> 00:33:37,270
pool there's a simple implementation of

00:33:33,520 --> 00:33:38,169
a spread pool I just put it on to

00:33:37,270 --> 00:33:39,940
basically

00:33:38,169 --> 00:33:42,249
a for completeness and then there's some

00:33:39,940 --> 00:33:46,210
interest some talking points about it

00:33:42,249 --> 00:33:48,850
one of them is and Hugh J sweats which

00:33:46,210 --> 00:33:53,379
is basically just joining sweats all

00:33:48,850 --> 00:33:55,659
they do is unlike the stood threads to

00:33:53,379 --> 00:33:57,580
trace read basically joins then being

00:33:55,659 --> 00:33:58,989
destroyed for the other guys you base if

00:33:57,580 --> 00:34:01,779
you always need to make sure you join

00:33:58,989 --> 00:34:03,399
them I didn't have an implementation of

00:34:01,779 --> 00:34:05,379
japheth available as well so I'm not

00:34:03,399 --> 00:34:07,500
using a stop talking and said I have

00:34:05,379 --> 00:34:11,169
just a boolean flag which is checked

00:34:07,500 --> 00:34:13,359
once my strap will show the store and

00:34:11,169 --> 00:34:17,619
the other interesting bit is over here

00:34:13,359 --> 00:34:19,540
says function function this function I'm

00:34:17,619 --> 00:34:21,040
using over here is not stood function

00:34:19,540 --> 00:34:25,929
the reason it's not stood function is

00:34:21,040 --> 00:34:29,260
the the arrival I have is kind of not

00:34:25,929 --> 00:34:30,819
really copyable I could make it copyable

00:34:29,260 --> 00:34:32,859
but then I would need to have reference

00:34:30,819 --> 00:34:35,829
counting around it it's only moveable

00:34:32,859 --> 00:34:39,490
and stood function as we evidence and

00:34:35,829 --> 00:34:42,730
library is unfortunately not not for

00:34:39,490 --> 00:34:46,720
function objects which are moved only so

00:34:42,730 --> 00:34:48,399
basically stood function if you try to

00:34:46,720 --> 00:34:52,389
copy or move the store function it

00:34:48,399 --> 00:34:54,879
basically and well or in general

00:34:52,389 --> 00:34:57,369
actually it needs to have an copyable a

00:34:54,879 --> 00:34:59,559
function object that is reasonably easy

00:34:57,369 --> 00:35:04,440
to create a function object which is

00:34:59,559 --> 00:35:06,760
only requires moved construction ok so

00:35:04,440 --> 00:35:08,440
once I have that there are a few

00:35:06,760 --> 00:35:10,480
interesting operations I'm not going to

00:35:08,440 --> 00:35:12,339
go over all the operations because most

00:35:10,480 --> 00:35:15,430
of them are boring the first one is we

00:35:12,339 --> 00:35:17,559
just create threads so thread pool I do

00:35:15,430 --> 00:35:19,510
a look at the start create all the

00:35:17,559 --> 00:35:20,980
threads and then let it go the reason

00:35:19,510 --> 00:35:22,690
for that is if you actually have a

00:35:20,980 --> 00:35:24,760
thread form you start using it while

00:35:22,690 --> 00:35:27,309
it's still being constructed there may

00:35:24,760 --> 00:35:29,770
be some funny effects of whatever your

00:35:27,309 --> 00:35:32,710
pool does under load so I didn't want

00:35:29,770 --> 00:35:36,040
that so basically just create them and

00:35:32,710 --> 00:35:37,930
kick things off and creating threads

00:35:36,040 --> 00:35:41,849
basically just amounts to write a create

00:35:37,930 --> 00:35:44,109
thread in my pool and they give it work

00:35:41,849 --> 00:35:47,260
scheduling some saying there's nothing

00:35:44,109 --> 00:35:50,799
magic in here at all you basically block

00:35:47,260 --> 00:35:51,850
the Q implies a function object and then

00:35:50,799 --> 00:35:54,250
we we sing

00:35:51,850 --> 00:35:55,900
in case any sweaters rating to to

00:35:54,250 --> 00:35:58,420
execute things doing normal execution

00:35:55,900 --> 00:36:01,270
hopefully we don't need signal but

00:35:58,420 --> 00:36:02,830
whatever and then the other side is work

00:36:01,270 --> 00:36:04,360
the work is a little bit more

00:36:02,830 --> 00:36:06,550
complicated because you needed a little

00:36:04,360 --> 00:36:10,660
bit further on the first thing is to

00:36:06,550 --> 00:36:13,420
basically well potentially wait if

00:36:10,660 --> 00:36:16,600
something is on and I need a unique lock

00:36:13,420 --> 00:36:18,310
to actually use weight and then actually

00:36:16,600 --> 00:36:21,490
the next step is to extract the function

00:36:18,310 --> 00:36:23,860
object and the slight pitfall which this

00:36:21,490 --> 00:36:26,890
however really quickly noticed is to

00:36:23,860 --> 00:36:28,930
actually keep the lock while executing

00:36:26,890 --> 00:36:31,150
the function obviously you need to first

00:36:28,930 --> 00:36:32,440
release the lock and then execute the

00:36:31,150 --> 00:36:35,520
function because otherwise you'd kill

00:36:32,440 --> 00:36:37,660
all the concurrency okay so this is

00:36:35,520 --> 00:36:41,020
straightforward and then based on that

00:36:37,660 --> 00:36:44,980
we can actually create a really fairly

00:36:41,020 --> 00:36:48,430
simple full executor basically it just

00:36:44,980 --> 00:36:51,550
takes the pool kicks one of these things

00:36:48,430 --> 00:36:56,160
up and every time it calls executed just

00:36:51,550 --> 00:36:58,600
steaks function into that pool the

00:36:56,160 --> 00:37:01,470
executor design at the moment assumes

00:36:58,600 --> 00:37:05,890
that executors are efficiently copyable

00:37:01,470 --> 00:37:07,090
that is may or may not be the well it

00:37:05,890 --> 00:37:08,980
needs to be the case it could be other

00:37:07,090 --> 00:37:12,010
designs they actually have the executor

00:37:08,980 --> 00:37:14,670
keep it as a reference but the this line

00:37:12,010 --> 00:37:17,350
is not lobbyists execute us are

00:37:14,670 --> 00:37:19,570
lightweight and you just copy them over

00:37:17,350 --> 00:37:23,170
here I'm just used create as executor

00:37:19,570 --> 00:37:25,120
and keeps share pointer to the thread

00:37:23,170 --> 00:37:27,610
pool they could also be at the sine-wave

00:37:25,120 --> 00:37:30,340
basically keep us one thread pool

00:37:27,610 --> 00:37:34,510
program sitting somewhere and you keep

00:37:30,340 --> 00:37:35,920
that this is I don't have a huge amount

00:37:34,510 --> 00:37:38,050
of experience what is the right approach

00:37:35,920 --> 00:37:39,520
over there this is part of the reason

00:37:38,050 --> 00:37:41,410
why I would like people who actually

00:37:39,520 --> 00:37:44,800
understand that stuff to implement

00:37:41,410 --> 00:37:48,580
executors so I can just use them okay

00:37:44,800 --> 00:37:51,310
and then yeah this is basically all

00:37:48,580 --> 00:37:53,560
there goes into implementing an executor

00:37:51,310 --> 00:37:55,990
and then I get a graph which looks like

00:37:53,560 --> 00:37:58,930
that annoyingly at the beginning again

00:37:55,990 --> 00:38:01,750
it makes things a lot slower so I

00:37:58,930 --> 00:38:03,370
basically before I get effective with my

00:38:01,750 --> 00:38:05,020
implementation I need to have a minimal

00:38:03,370 --> 00:38:07,830
size so I cut off

00:38:05,020 --> 00:38:11,320
to make it actually more visible the

00:38:07,830 --> 00:38:13,660
green stuff is still the sequential

00:38:11,320 --> 00:38:16,480
executor so you would expect that to be

00:38:13,660 --> 00:38:19,360
actually slower about half half as slow

00:38:16,480 --> 00:38:22,360
as the sequential version because it

00:38:19,360 --> 00:38:24,490
runs all the sequence twice and that is

00:38:22,360 --> 00:38:27,369
basically melee was a little bit better

00:38:24,490 --> 00:38:29,230
but otherwise a chief and then the the

00:38:27,369 --> 00:38:33,100
various executors which I actually show

00:38:29,230 --> 00:38:35,200
over here is basically the the pool

00:38:33,100 --> 00:38:38,230
executors which I just described as a

00:38:35,200 --> 00:38:42,160
yellow version I didn't want to actually

00:38:38,230 --> 00:38:44,830
at the entire details for an executor

00:38:42,160 --> 00:38:47,619
which does multiple pools but this is

00:38:44,830 --> 00:38:50,710
the is basically all it does is the

00:38:47,619 --> 00:38:53,170
executors its uses the same thread pool

00:38:50,710 --> 00:38:55,720
as before but splits up the threads kind

00:38:53,170 --> 00:38:58,750
of evenly between n number of pools and

00:38:55,720 --> 00:39:01,540
the executor just goes over them and

00:38:58,750 --> 00:39:05,740
basically every time I take in Q so drop

00:39:01,540 --> 00:39:10,750
it goes to the next one this would be

00:39:05,740 --> 00:39:11,920
the this is he the red version which was

00:39:10,750 --> 00:39:14,050
kind of nice

00:39:11,920 --> 00:39:16,930
I did try to create a drop stealing

00:39:14,050 --> 00:39:18,880
executor myself but I entirely failed I

00:39:16,930 --> 00:39:26,020
think well I didn't entirely fail this

00:39:18,880 --> 00:39:29,680
is he was one of the the gray version is

00:39:26,020 --> 00:39:32,109
basically using the the task group on

00:39:29,680 --> 00:39:33,820
threat building blocks again the

00:39:32,109 --> 00:39:35,980
interesting bit over there is a straight

00:39:33,820 --> 00:39:38,590
building block version I would have

00:39:35,980 --> 00:39:41,040
expected to be better but this basically

00:39:38,590 --> 00:39:43,930
the reason it starts getting good or

00:39:41,040 --> 00:39:45,940
reasonable relatively late must be in

00:39:43,930 --> 00:39:47,800
the algorithm so they're still things

00:39:45,940 --> 00:39:50,770
and how do I actually implement the

00:39:47,800 --> 00:39:54,070
algorithm properly which should make it

00:39:50,770 --> 00:39:57,160
hopefully better okay

00:39:54,070 --> 00:39:58,270
there's all that said let's go to how we

00:39:57,160 --> 00:40:03,130
actually do that

00:39:58,270 --> 00:40:06,910
and this is this was getting to the end

00:40:03,130 --> 00:40:09,580
but so the the way we actually want to

00:40:06,910 --> 00:40:11,859
use inclusive scan is none of that stuff

00:40:09,580 --> 00:40:14,500
he the entire implementation I showed

00:40:11,859 --> 00:40:16,030
icing is relatively complicated and

00:40:14,500 --> 00:40:17,560
certainly more complicated than I would

00:40:16,030 --> 00:40:18,820
like it to be I would like to have just

00:40:17,560 --> 00:40:21,490
an inclusive scan and say

00:40:18,820 --> 00:40:23,710
yet just executed please and the

00:40:21,490 --> 00:40:25,780
standard library already has that this

00:40:23,710 --> 00:40:28,900
is basically I just use an algorithm to

00:40:25,780 --> 00:40:30,550
show how how what kind of steps are

00:40:28,900 --> 00:40:31,960
thinking you need to go to it to if you

00:40:30,550 --> 00:40:33,370
want to implement a different algorithm

00:40:31,960 --> 00:40:36,880
and practice we're going to use

00:40:33,370 --> 00:40:39,610
inclusive scan with either power which

00:40:36,880 --> 00:40:42,730
is basically the execution policy to say

00:40:39,610 --> 00:40:45,790
execute things concurrently on different

00:40:42,730 --> 00:40:47,920
threats and you promise that your

00:40:45,790 --> 00:40:50,380
element access functions on whatever

00:40:47,920 --> 00:40:52,690
usage rate those function objects can be

00:40:50,380 --> 00:40:54,910
executed without that locking or we want

00:40:52,690 --> 00:40:57,880
to use power on seek this is basically

00:40:54,910 --> 00:41:00,790
the version way go well not only are the

00:40:57,880 --> 00:41:03,160
the things a thread safe but you can

00:41:00,790 --> 00:41:05,500
actually execute multiple these things

00:41:03,160 --> 00:41:07,780
kind of on the same thread this is for

00:41:05,500 --> 00:41:10,690
something like views as in simply

00:41:07,780 --> 00:41:12,820
operations or schedule things on an GPU

00:41:10,690 --> 00:41:15,820
like saying we're actually the same

00:41:12,820 --> 00:41:17,440
thread may execute multiple things so

00:41:15,820 --> 00:41:19,540
you cannot even have a look inside any

00:41:17,440 --> 00:41:22,810
of these guys and then inclusive scan

00:41:19,540 --> 00:41:25,240
see as the the interesting case they go

00:41:22,810 --> 00:41:27,220
well let's try out before going anything

00:41:25,240 --> 00:41:29,830
concurrently actually try whether the

00:41:27,220 --> 00:41:31,150
algorithm works in sequence just by the

00:41:29,830 --> 00:41:33,370
way one of the things I didn't mention

00:41:31,150 --> 00:41:35,440
so far while implementing all of these

00:41:33,370 --> 00:41:37,660
things the first implementation ID for

00:41:35,440 --> 00:41:40,510
all of these guys was using a sequential

00:41:37,660 --> 00:41:43,870
algorithm and also the the next step is

00:41:40,510 --> 00:41:46,300
alright I used chunks using big chunks

00:41:43,870 --> 00:41:48,580
and in parallel settings may cause

00:41:46,300 --> 00:41:50,560
problems but first I tried those tiny

00:41:48,580 --> 00:41:52,510
chunks let's say four elements per chunk

00:41:50,560 --> 00:41:55,420
so you basically make sure that the

00:41:52,510 --> 00:41:57,880
chunking business works if you use 8,000

00:41:55,420 --> 00:42:00,220
as a chunk and you try in your test case

00:41:57,880 --> 00:42:02,110
it's 100 elements you never get into the

00:42:00,220 --> 00:42:05,260
chunking so you basically want to make

00:42:02,110 --> 00:42:06,940
sure that you hit things at least doing

00:42:05,260 --> 00:42:09,670
development there's reasonably small

00:42:06,940 --> 00:42:11,110
things okay so all that said the

00:42:09,670 --> 00:42:13,390
parallel version I do the same

00:42:11,110 --> 00:42:16,420
benchmarking was a parallel version the

00:42:13,390 --> 00:42:22,240
violet thing over here was the best

00:42:16,420 --> 00:42:23,980
algorithm I came up with the the green

00:42:22,240 --> 00:42:27,340
version was obviously the sequential one

00:42:23,980 --> 00:42:32,020
and then the yellow yellow is the porn

00:42:27,340 --> 00:42:32,800
the red is a Ponzi the reason I'm an

00:42:32,020 --> 00:42:36,250
entire show

00:42:32,800 --> 00:42:39,610
the Han seekers actually relatively

00:42:36,250 --> 00:42:41,800
consistently faster because I don't

00:42:39,610 --> 00:42:43,480
think the on that system the algorithms

00:42:41,800 --> 00:42:47,800
actually have a lot of opportunity to

00:42:43,480 --> 00:42:49,660
take advantage of anything to make it

00:42:47,800 --> 00:42:54,250
faster over there but apparently they

00:42:49,660 --> 00:43:00,430
did okay so this is pretty much what I

00:42:54,250 --> 00:43:03,220
wanted to talk about and so the the

00:43:00,430 --> 00:43:04,990
conclusions I get from that is the

00:43:03,220 --> 00:43:06,340
parallel algorithms actually do speed

00:43:04,990 --> 00:43:08,590
things up at least once you have large

00:43:06,340 --> 00:43:14,020
workloads on tiny workloads they

00:43:08,590 --> 00:43:16,420
probably don't and and there's even true

00:43:14,020 --> 00:43:17,860
if you don't just have massively

00:43:16,420 --> 00:43:20,440
parallel algorithms but basically

00:43:17,860 --> 00:43:24,310
something like there's a dependency

00:43:20,440 --> 00:43:26,290
between elements as in scan and then the

00:43:24,310 --> 00:43:29,950
other part is although I didn't achieve

00:43:26,290 --> 00:43:31,390
the standard library thing I think the

00:43:29,950 --> 00:43:33,640
implementation of parallel algorithms

00:43:31,390 --> 00:43:35,410
isn't magic either we can Atari do it

00:43:33,640 --> 00:43:39,970
and if we want to or need to implement

00:43:35,410 --> 00:43:41,680
an algorithm we can do that however this

00:43:39,970 --> 00:43:43,360
is my experience not just as parallel

00:43:41,680 --> 00:43:45,700
reasons but with algorithms from books

00:43:43,360 --> 00:43:47,380
in general the algorithm actually needs

00:43:45,700 --> 00:43:49,900
some work to actually make it worthwhile

00:43:47,380 --> 00:43:51,580
to go and be effective so before you

00:43:49,900 --> 00:43:53,800
actually arrive at a good implementation

00:43:51,580 --> 00:43:55,750
you typically need to benchmark and

00:43:53,800 --> 00:43:58,030
verify that actually your algorithm is

00:43:55,750 --> 00:43:59,830
good if you just go yeah let's take that

00:43:58,030 --> 00:44:02,890
algorithm and it says in the book it's

00:43:59,830 --> 00:44:05,590
good and then you run it you will

00:44:02,890 --> 00:44:07,900
probably get a slow implementation and

00:44:05,590 --> 00:44:10,000
then the other thing is all of that

00:44:07,900 --> 00:44:11,680
would be a thing easier if you had these

00:44:10,000 --> 00:44:13,150
in thread pools but in any case you

00:44:11,680 --> 00:44:15,370
would want to use a standard C++ library

00:44:13,150 --> 00:44:17,140
versions of algorithms which are there

00:44:15,370 --> 00:44:19,540
and the nice thing is if you're already

00:44:17,140 --> 00:44:21,400
using algorithms making them parallel

00:44:19,540 --> 00:44:24,690
just advance to Chungking in an

00:44:21,400 --> 00:44:27,490
execution policy and flight with that

00:44:24,690 --> 00:44:34,470
okay thank you very much

00:44:27,490 --> 00:44:39,000
[Applause]

00:44:34,470 --> 00:44:49,210
so any questions or discussions comments

00:44:39,000 --> 00:44:52,930
yes Oh to paralyze beckoned I largely

00:44:49,210 --> 00:44:54,550
domed aside so the question is what is

00:44:52,930 --> 00:44:57,250
Bloomberg actually using to paralyze the

00:44:54,550 --> 00:45:00,070
backend the answer to that is I don't

00:44:57,250 --> 00:45:01,840
know I've worked in one particular area

00:45:00,070 --> 00:45:04,630
of Bloomberg which is the data

00:45:01,840 --> 00:45:06,760
distribution which is fairly concurrent

00:45:04,630 --> 00:45:08,470
but we don't use any parallel algorithms

00:45:06,760 --> 00:45:10,930
because of workloads are typically

00:45:08,470 --> 00:45:13,450
alright here is one price and we need to

00:45:10,930 --> 00:45:15,190
process that through changes and over

00:45:13,450 --> 00:45:16,630
there we do use thread pools and some

00:45:15,190 --> 00:45:19,930
shape or form and message passing and

00:45:16,630 --> 00:45:24,760
all of that but none of that stuff no

00:45:19,930 --> 00:45:38,950
parallel reasons or something anything

00:45:24,760 --> 00:45:40,960
else thank you very much ok the reason I

00:45:38,950 --> 00:45:43,630
take the lock on the mutexes I want

00:45:40,960 --> 00:45:46,660
these threads all be started before I

00:45:43,630 --> 00:45:49,960
let start processing any anything else

00:45:46,660 --> 00:45:51,640
if you do not take the lock you may

00:45:49,960 --> 00:45:55,089
already have a partially working thread

00:45:51,640 --> 00:45:57,910
pool which if you include things at off

00:45:55,089 --> 00:46:00,030
time possibly from from elsewhere starts

00:45:57,910 --> 00:46:03,190
running and I didn't want to do that

00:46:00,030 --> 00:46:07,619
this is roughly things although yeah how

00:46:03,190 --> 00:46:07,619
do I get all of the construction singing

00:46:10,830 --> 00:46:17,640
yeah ok ok you're right maybe I don't

00:46:13,480 --> 00:46:20,050
need that mutex it doesn't harm either

00:46:17,640 --> 00:46:22,900
well it's basically just doing startup

00:46:20,050 --> 00:46:27,240
being extra busy part as scheduling

00:46:22,900 --> 00:46:27,240
sings ok

00:46:38,960 --> 00:46:46,740
you mean you mean white at the beginning

00:46:41,450 --> 00:46:51,059
the so going all the way back to you

00:46:46,740 --> 00:46:53,970
mean when does that graph hit sings I

00:46:51,059 --> 00:46:56,759
didn't bother running it beyond that

00:46:53,970 --> 00:47:00,839
size because it already took ages so the

00:46:56,759 --> 00:47:04,499
this is a relative performance so it

00:47:00,839 --> 00:47:07,440
still takes 3,500 times I sing longer

00:47:04,499 --> 00:47:09,480
than the than the sequential version and

00:47:07,440 --> 00:47:15,329
the sequential version already starts

00:47:09,480 --> 00:47:16,859
running lengthy so it may eventually cut

00:47:15,329 --> 00:47:19,019
down but I don't know at which slice

00:47:16,859 --> 00:47:38,910
that what would happen I got bored of

00:47:19,019 --> 00:47:40,920
waiting so the comment is Jemison is

00:47:38,910 --> 00:47:42,900
that a sin basically creates a different

00:47:40,920 --> 00:47:44,999
thread every time I would hope that a

00:47:42,900 --> 00:47:46,650
sin doesn't do that but that a sing

00:47:44,999 --> 00:47:50,549
actually under the would keeps us red

00:47:46,650 --> 00:47:52,019
pool and basically pulls all the thread

00:47:50,549 --> 00:47:54,059
and runs that week at least that would

00:47:52,019 --> 00:47:56,789
be a sensible implementation off of that

00:47:54,059 --> 00:47:58,920
but even so even if it did that I would

00:47:56,789 --> 00:48:01,410
expect it to be relatively slow because

00:47:58,920 --> 00:48:03,420
you create a future for every individual

00:48:01,410 --> 00:48:05,519
object over here there's no chunking

00:48:03,420 --> 00:48:08,279
happening and I think chunking is

00:48:05,519 --> 00:48:13,999
essential to avoid communication for

00:48:08,279 --> 00:48:13,999
tiny workloads eventually okay

00:48:19,380 --> 00:48:24,070
okay so the question is which compiler

00:48:22,390 --> 00:48:25,900
did I actually use or which system did I

00:48:24,070 --> 00:48:27,940
used for the for the last assignment

00:48:25,900 --> 00:48:31,120
library stuff at the moment I actually

00:48:27,940 --> 00:48:32,890
only have access to a sonnet library

00:48:31,120 --> 00:48:35,740
implementation for the parallel grooves

00:48:32,890 --> 00:48:37,450
from a typical C++ I don't know whether

00:48:35,740 --> 00:48:39,430
any other compiler actually ships the

00:48:37,450 --> 00:48:40,360
parallel algorithms yet what least I

00:48:39,430 --> 00:48:42,460
don't have access to it

00:48:40,360 --> 00:48:44,560
I sing Microsoft's compiler is shipping

00:48:42,460 --> 00:48:50,450
but I don't have Microsoft compiler on

00:48:44,560 --> 00:48:54,510
mine my machine okay thank you

00:48:50,450 --> 00:48:54,510

YouTube URL: https://www.youtube.com/watch?v=RrG_A7Ybzo4


