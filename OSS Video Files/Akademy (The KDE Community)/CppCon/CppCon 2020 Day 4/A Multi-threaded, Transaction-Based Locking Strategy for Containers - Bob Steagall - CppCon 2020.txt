Title: A Multi-threaded, Transaction-Based Locking Strategy for Containers - Bob Steagall - CppCon 2020
Publication date: 2020-10-03
Playlist: CppCon 2020 Day 4
Description: 
	https://cppcon.org/
https://github.com/CppCon/CppCon2020/blob/main/Presentations/a_transactionbased_multithreaded_locking_strategy_for_containers/a_transactionbased_multithreaded_locking_strategy_for_containers__bob_steagall__cppcon_2020.pdf
---
With the concurrency tools available in the modern C++ standard library, it is easier than ever to create multi-threaded programs. When we write such applications, there are sometimes cases in which a container simply must be shared among multiple threads. Of course, sharing is trivial if the only operations on the container are reads. In the case where reads greatly outnumber writes, acceptable performance is often attainable with a reader/writer mutex type, like std::shared_mutex. But suppose that the number of writes is similar to, or even greater than, the number of reads -- how does one then perform simultaneous reads and writes on a single container?

One common usage pattern is that, for a given operation, sets of related records are read and updated together. In order to prevent data races and inconsistent views of the data, such sets must be locked together as a unit before any of them can actually be read or updated. Further, it is very easy to accidentally create deadlocks by choosing a seemingly correct locking order. In order to avoid these problems, we would like a locking algorithm that provides three important properties: atomicity, consistency, and isolation.

This talk will describe an algorithm, implemented in C++, that performs such locking based on the concept of strict timestamp ordering. Using only facilities from the C++17 standard library, it employs a straightforward approach to multi-threaded, transactional record locking that requires minimal spatial overhead and yet fulfils the requirements of atomicity, consistency, and isolation. We'll discuss the pros, cons, and limitations of the algorithm, and provide some performance measurements. 

---
Bob Steagall
Chief Cook and Bottle Washer, KEWB Computing

---
Streamed & Edited by Digital Medium Ltd - events.digital-medium.co.uk
events@digital-medium.co.uk
Captions: 
	00:00:09,679 --> 00:00:12,559
good morning everyone

00:00:10,639 --> 00:00:13,759
thank you for coming my name is bob

00:00:12,559 --> 00:00:15,360
stegall

00:00:13,759 --> 00:00:16,880
and i'm here today to talk about a

00:00:15,360 --> 00:00:19,439
little bit of code i wrote

00:00:16,880 --> 00:00:21,680
an experiment i performed for doing

00:00:19,439 --> 00:00:24,400
multi-threaded transaction based locking

00:00:21,680 --> 00:00:25,840
on actually on vectors and i've

00:00:24,400 --> 00:00:27,760
developed an algorithm

00:00:25,840 --> 00:00:29,599
i'm not sure that it's new but i think

00:00:27,760 --> 00:00:32,719
it's new and

00:00:29,599 --> 00:00:34,399
this talk is about that algorithm so

00:00:32,719 --> 00:00:36,079
we've got a lot to go through i'll jump

00:00:34,399 --> 00:00:38,160
right into it

00:00:36,079 --> 00:00:39,760
so sometimes when we're doing work we

00:00:38,160 --> 00:00:41,600
have a multi-threaded application and we

00:00:39,760 --> 00:00:44,879
need to share a container

00:00:41,600 --> 00:00:47,760
amongst multiple threads

00:00:44,879 --> 00:00:51,120
so i'll talk about the motivation i'll

00:00:47,760 --> 00:00:53,039
talk about some solutions

00:00:51,120 --> 00:00:54,640
i will then describe my solution which

00:00:53,039 --> 00:00:57,440
is based on a concept

00:00:54,640 --> 00:00:59,440
called strip time stamp ordering i'll

00:00:57,440 --> 00:01:03,520
talk a little bit about the testing

00:00:59,440 --> 00:01:06,000
and show some test results and summarize

00:01:03,520 --> 00:01:07,680
so sometimes we have a multi-threaded

00:01:06,000 --> 00:01:09,600
application and we absolutely need to

00:01:07,680 --> 00:01:11,040
share it amongst threads

00:01:09,600 --> 00:01:12,560
and if we need to write to that

00:01:11,040 --> 00:01:14,400
container we want to avoid race

00:01:12,560 --> 00:01:16,159
conditions we don't want to have corrupt

00:01:14,400 --> 00:01:17,920
data

00:01:16,159 --> 00:01:20,159
so for the algorithm i'm about to

00:01:17,920 --> 00:01:21,520
describe i'm going to assume that

00:01:20,159 --> 00:01:23,360
elements themselves

00:01:21,520 --> 00:01:24,560
elements in the vector are themselves

00:01:23,360 --> 00:01:26,400
unsynchronized

00:01:24,560 --> 00:01:27,840
they are susceptible to races if two

00:01:26,400 --> 00:01:31,040
threads try to write to one

00:01:27,840 --> 00:01:32,400
element simultaneously i'm going to

00:01:31,040 --> 00:01:35,200
assume that we want to

00:01:32,400 --> 00:01:38,479
uh update an element only one thread can

00:01:35,200 --> 00:01:40,479
update an element at any given time

00:01:38,479 --> 00:01:42,079
and that no other thread can read an

00:01:40,479 --> 00:01:43,920
element while the writing thread is

00:01:42,079 --> 00:01:46,799
updating it

00:01:43,920 --> 00:01:47,680
however uh more than one thread can read

00:01:46,799 --> 00:01:49,600
an element

00:01:47,680 --> 00:01:51,840
when that element is not being written

00:01:49,600 --> 00:01:51,840
to

00:01:52,399 --> 00:01:56,560
in modern c plus we have a lot of

00:01:54,720 --> 00:01:57,759
concurrency tools that are now available

00:01:56,560 --> 00:02:00,479
to us

00:01:57,759 --> 00:02:01,439
and i think that writing multi-threaded

00:02:00,479 --> 00:02:05,680
applications is

00:02:01,439 --> 00:02:05,680
easier than ever maybe easier

00:02:06,320 --> 00:02:10,479
so if we have this situation where

00:02:08,640 --> 00:02:12,640
multiple threads trying to

00:02:10,479 --> 00:02:13,840
manipulate a container if all of the

00:02:12,640 --> 00:02:15,520
threads or reader

00:02:13,840 --> 00:02:18,560
are readers then we don't need any

00:02:15,520 --> 00:02:18,560
locking on the container

00:02:18,959 --> 00:02:22,080
and if we have a situation where we're

00:02:20,640 --> 00:02:23,680
writing to the container

00:02:22,080 --> 00:02:26,080
and the number of threads that need to

00:02:23,680 --> 00:02:28,000
read the container is much larger

00:02:26,080 --> 00:02:29,280
than the number of containers the number

00:02:28,000 --> 00:02:30,720
of threads that need to write to the

00:02:29,280 --> 00:02:32,400
container

00:02:30,720 --> 00:02:36,080
then we might be able to use a reader's

00:02:32,400 --> 00:02:38,959
writer lock like stood shared mutex

00:02:36,080 --> 00:02:39,680
however the situation sometimes arises

00:02:38,959 --> 00:02:41,200
where

00:02:39,680 --> 00:02:43,599
most of the operations that you want to

00:02:41,200 --> 00:02:45,280
perform on the container are right

00:02:43,599 --> 00:02:47,680
operations

00:02:45,280 --> 00:02:49,120
now in this case when you start thinking

00:02:47,680 --> 00:02:50,160
about the design that you want to

00:02:49,120 --> 00:02:52,480
implement

00:02:50,160 --> 00:02:54,640
you might think that a per element mutex

00:02:52,480 --> 00:02:56,319
strategy might work

00:02:54,640 --> 00:02:58,000
and it might work if a given write

00:02:56,319 --> 00:03:01,519
operation requires

00:02:58,000 --> 00:03:01,519
locking exactly one element

00:03:02,319 --> 00:03:05,920
but what about the case when all the

00:03:04,239 --> 00:03:07,599
operations are rights

00:03:05,920 --> 00:03:08,959
sometimes you have a situation and i was

00:03:07,599 --> 00:03:12,159
working in a situation

00:03:08,959 --> 00:03:13,840
uh a year or so ago where all of the

00:03:12,159 --> 00:03:14,319
operations that were performed upon

00:03:13,840 --> 00:03:17,599
these

00:03:14,319 --> 00:03:20,640
set of containers were rights

00:03:17,599 --> 00:03:23,440
so what if they're all rights

00:03:20,640 --> 00:03:25,120
and each element that you want to update

00:03:23,440 --> 00:03:27,120
is also related to some

00:03:25,120 --> 00:03:29,360
other set of elements that are members

00:03:27,120 --> 00:03:32,640
of the same container

00:03:29,360 --> 00:03:33,120
so let's call this set uh the related

00:03:32,640 --> 00:03:35,120
group

00:03:33,120 --> 00:03:36,720
of e in other words you have an element

00:03:35,120 --> 00:03:37,680
e and there's a group of related

00:03:36,720 --> 00:03:39,840
elements

00:03:37,680 --> 00:03:40,959
and let's define the update group of e

00:03:39,840 --> 00:03:44,720
as being the union

00:03:40,959 --> 00:03:47,519
of e's related group and e itself

00:03:44,720 --> 00:03:48,959
so in graphical form suppose that we

00:03:47,519 --> 00:03:52,879
have a set of elements

00:03:48,959 --> 00:03:53,680
here i've just listed 16 and i want to

00:03:52,879 --> 00:03:56,720
update e

00:03:53,680 --> 00:04:00,879
and this is transaction n

00:03:56,720 --> 00:04:03,840
well e8 which is the you know is the

00:04:00,879 --> 00:04:04,640
index of the element uh is related to

00:04:03,840 --> 00:04:07,680
element two

00:04:04,640 --> 00:04:09,200
and element eleven and element two is

00:04:07,680 --> 00:04:12,720
related to element one and

00:04:09,200 --> 00:04:15,760
element five so

00:04:12,720 --> 00:04:20,239
by transitivity the related group

00:04:15,760 --> 00:04:23,440
for e8 consists of e1 e2 e5 and e11

00:04:20,239 --> 00:04:25,280
in other words if i need to update e8

00:04:23,440 --> 00:04:27,199
these are all the other elements i may

00:04:25,280 --> 00:04:30,240
need to touch that i may need to update

00:04:27,199 --> 00:04:32,560
in order to successfully update e8

00:04:30,240 --> 00:04:33,600
so the update group for e8 is going to

00:04:32,560 --> 00:04:38,000
be that set of

00:04:33,600 --> 00:04:40,000
related elements plus e8 itself

00:04:38,000 --> 00:04:43,840
so going back to our conditions now that

00:04:40,000 --> 00:04:43,840
we know what an update group is

00:04:44,080 --> 00:04:49,840
suppose also that one of the elements

00:04:47,280 --> 00:04:51,520
in ease related group also has to be

00:04:49,840 --> 00:04:53,360
updated at the same time

00:04:51,520 --> 00:04:54,880
consistently with e not only read but

00:04:53,360 --> 00:04:58,080
updated

00:04:54,880 --> 00:05:00,479
and the number of elements

00:04:58,080 --> 00:05:02,800
in the related group varies from

00:05:00,479 --> 00:05:06,160
transaction to transaction

00:05:02,800 --> 00:05:08,840
and the membership of the related group

00:05:06,160 --> 00:05:10,160
can possibly vary from transaction to

00:05:08,840 --> 00:05:13,600
transaction

00:05:10,160 --> 00:05:15,840
in other words in transaction n

00:05:13,600 --> 00:05:17,840
i have a related and up update group for

00:05:15,840 --> 00:05:20,000
e that looks like this

00:05:17,840 --> 00:05:21,759
but if i go to transaction n plus one

00:05:20,000 --> 00:05:24,400
perhaps even the next transaction

00:05:21,759 --> 00:05:26,400
and i need to update e8 again perhaps

00:05:24,400 --> 00:05:27,120
the membership of the related group has

00:05:26,400 --> 00:05:28,639
changed

00:05:27,120 --> 00:05:31,520
because of the update that i did in

00:05:28,639 --> 00:05:31,520
transaction n

00:05:32,720 --> 00:05:39,199
so why did i try to solve this problem

00:05:36,160 --> 00:05:41,520
and why is it interesting

00:05:39,199 --> 00:05:43,440
well suppose that i have a thing that i

00:05:41,520 --> 00:05:44,560
choose to label a reactive message

00:05:43,440 --> 00:05:46,880
processor

00:05:44,560 --> 00:05:48,800
it's a it's a box it's a program that

00:05:46,880 --> 00:05:50,880
receives a continuous stream of input

00:05:48,800 --> 00:05:52,560
messages perhaps from kafka or something

00:05:50,880 --> 00:05:54,639
like that

00:05:52,560 --> 00:05:56,080
and it listens to these messages and it

00:05:54,639 --> 00:05:57,919
generates output

00:05:56,080 --> 00:05:59,440
perhaps it sends that output to another

00:05:57,919 --> 00:06:02,240
kafka instance somewhere

00:05:59,440 --> 00:06:04,800
when something interesting happens in

00:06:02,240 --> 00:06:06,319
the stream of input messages

00:06:04,800 --> 00:06:08,080
so how does it know if something

00:06:06,319 --> 00:06:09,759
interesting has happened well

00:06:08,080 --> 00:06:11,280
when messages come in the stream

00:06:09,759 --> 00:06:13,360
processor uh

00:06:11,280 --> 00:06:14,880
the message processor has to keep a

00:06:13,360 --> 00:06:17,360
history that has

00:06:14,880 --> 00:06:20,319
pertinent information about the messages

00:06:17,360 --> 00:06:22,720
and these each message comes in

00:06:20,319 --> 00:06:23,759
the message processor takes the incoming

00:06:22,720 --> 00:06:25,680
message

00:06:23,759 --> 00:06:27,199
compares it against the relative aspects

00:06:25,680 --> 00:06:28,560
of the history and decides whether or

00:06:27,199 --> 00:06:28,960
not something interesting has happened

00:06:28,560 --> 00:06:31,440
so

00:06:28,960 --> 00:06:32,720
i can send a message out in this

00:06:31,440 --> 00:06:34,960
particular case

00:06:32,720 --> 00:06:37,280
i don't actually need to read the state

00:06:34,960 --> 00:06:38,880
of the history

00:06:37,280 --> 00:06:40,560
every message that comes into the

00:06:38,880 --> 00:06:44,800
message processor requires

00:06:40,560 --> 00:06:44,800
a right operation into the history

00:06:45,039 --> 00:06:48,720
and the history as i mentioned in the

00:06:47,360 --> 00:06:51,360
work i was doing before

00:06:48,720 --> 00:06:53,599
can be one or more containers in the

00:06:51,360 --> 00:06:55,280
code that i'll show you today

00:06:53,599 --> 00:06:58,639
it's really i'm just using a single

00:06:55,280 --> 00:07:01,120
vector to demonstrate the algorithm

00:06:58,639 --> 00:07:02,560
so here's a diagram of our reactive

00:07:01,120 --> 00:07:04,160
message processor

00:07:02,560 --> 00:07:05,840
we have our input source we have

00:07:04,160 --> 00:07:08,560
messages that come in

00:07:05,840 --> 00:07:09,440
we process the messages we look into the

00:07:08,560 --> 00:07:11,840
history

00:07:09,440 --> 00:07:14,160
we pull some data back out we decide

00:07:11,840 --> 00:07:16,880
whether or not we need to evaluate

00:07:14,160 --> 00:07:18,240
and in the process of evaluation whether

00:07:16,880 --> 00:07:20,400
or not we've created

00:07:18,240 --> 00:07:21,919
an interesting message if something an

00:07:20,400 --> 00:07:22,720
interesting change has happened and if

00:07:21,919 --> 00:07:26,000
it has

00:07:22,720 --> 00:07:26,000
we send it out to the sync

00:07:26,720 --> 00:07:31,360
this is pretty easy to do in the single

00:07:28,720 --> 00:07:34,800
threaded case

00:07:31,360 --> 00:07:35,599
however in the work that i was doing

00:07:34,800 --> 00:07:37,520
previously

00:07:35,599 --> 00:07:40,319
we needed this to scale to multiple

00:07:37,520 --> 00:07:43,520
threads and not just a couple of threads

00:07:40,319 --> 00:07:46,080
on the order of a few tens

00:07:43,520 --> 00:07:47,680
so it really sort of looked like this

00:07:46,080 --> 00:07:49,360
here we had our source

00:07:47,680 --> 00:07:51,199
feeding messages into a number of

00:07:49,360 --> 00:07:52,720
threads in each thread running its own

00:07:51,199 --> 00:07:55,599
message processor

00:07:52,720 --> 00:07:55,919
and each message processor was putting

00:07:55,599 --> 00:07:58,560
me

00:07:55,919 --> 00:08:00,240
putting changes into the history and

00:07:58,560 --> 00:08:02,160
seeing if anything interesting happened

00:08:00,240 --> 00:08:05,120
and sending it out to the soar

00:08:02,160 --> 00:08:05,840
of the sink now the the interesting

00:08:05,120 --> 00:08:07,919
thing here

00:08:05,840 --> 00:08:09,520
is that there are multiple instances of

00:08:07,919 --> 00:08:11,280
the message processor each running in

00:08:09,520 --> 00:08:12,800
its own thread but there's only one

00:08:11,280 --> 00:08:16,080
instance of the history

00:08:12,800 --> 00:08:17,599
which was in in the same process and of

00:08:16,080 --> 00:08:18,960
course all threads in a process share

00:08:17,599 --> 00:08:20,400
the same address space

00:08:18,960 --> 00:08:22,319
but the history was in effect a

00:08:20,400 --> 00:08:26,639
singleton and all of the threads were

00:08:22,319 --> 00:08:29,039
reading to and from it

00:08:26,639 --> 00:08:30,800
so what do we want our solution to to be

00:08:29,039 --> 00:08:31,599
able to do what are the properties we

00:08:30,800 --> 00:08:35,039
look for

00:08:31,599 --> 00:08:38,320
well first we want uh updates

00:08:35,039 --> 00:08:41,440
uh to be atomic and when an update group

00:08:38,320 --> 00:08:42,719
is modified we want the modification of

00:08:41,440 --> 00:08:45,760
that group to be treated

00:08:42,719 --> 00:08:47,760
atomically as a transaction and as a

00:08:45,760 --> 00:08:49,680
transaction it either

00:08:47,760 --> 00:08:52,240
succeeds completely or it fails

00:08:49,680 --> 00:08:52,240
completely

00:08:52,320 --> 00:08:55,519
the state of the history the state of

00:08:53,680 --> 00:08:57,519
the container has to be consistent from

00:08:55,519 --> 00:09:00,399
transaction to transaction

00:08:57,519 --> 00:09:00,880
meaning that when a transaction occurs

00:09:00,399 --> 00:09:02,959
it can

00:09:00,880 --> 00:09:05,200
only take the update group from one

00:09:02,959 --> 00:09:07,440
valid state to another

00:09:05,200 --> 00:09:08,640
and of course if the transaction is

00:09:07,440 --> 00:09:11,120
rolled back because the

00:09:08,640 --> 00:09:13,120
the update can't be performed then i'm

00:09:11,120 --> 00:09:16,080
going from the previous valid state

00:09:13,120 --> 00:09:16,080
back to that state

00:09:16,240 --> 00:09:19,760
and we also want transactions updates to

00:09:18,560 --> 00:09:22,080
be isolated

00:09:19,760 --> 00:09:24,560
in other words every time a transaction

00:09:22,080 --> 00:09:24,560
occurs

00:09:25,279 --> 00:09:28,399
that transaction has to occur and leave

00:09:27,680 --> 00:09:30,720
the

00:09:28,399 --> 00:09:31,839
leave the container in such a state that

00:09:30,720 --> 00:09:34,320
might have been obtained

00:09:31,839 --> 00:09:34,880
as if all the transactions were executed

00:09:34,320 --> 00:09:38,240
in some

00:09:34,880 --> 00:09:41,680
unknown but valid sequential order so

00:09:38,240 --> 00:09:44,720
multi-threading and concurrency must not

00:09:41,680 --> 00:09:47,600
uh affect uh the atomic nature

00:09:44,720 --> 00:09:48,720
or the consistent nature of of the state

00:09:47,600 --> 00:09:51,839
of the container

00:09:48,720 --> 00:09:51,839
at the end of each transaction

00:09:52,800 --> 00:09:56,320
so what are some ways that we can solve

00:09:54,959 --> 00:09:57,680
this problem

00:09:56,320 --> 00:10:00,480
well the first thing that might come to

00:09:57,680 --> 00:10:03,200
mind is a per container mutex

00:10:00,480 --> 00:10:03,600
we create a container we create a mutex

00:10:03,200 --> 00:10:06,240
for

00:10:03,600 --> 00:10:07,920
for the container and we perform updates

00:10:06,240 --> 00:10:10,640
in a single critical section that's

00:10:07,920 --> 00:10:13,920
guarded by the mutex

00:10:10,640 --> 00:10:16,079
the upside to this is that it works

00:10:13,920 --> 00:10:17,600
it's easy to understand and it's easy to

00:10:16,079 --> 00:10:20,320
reason about

00:10:17,600 --> 00:10:22,640
the downside however is this does not

00:10:20,320 --> 00:10:24,399
scale

00:10:22,640 --> 00:10:25,680
another solution that people look to in

00:10:24,399 --> 00:10:28,320
situations like this

00:10:25,680 --> 00:10:30,079
is called sharding and sharding is where

00:10:28,320 --> 00:10:30,720
you divide the set of elements in the

00:10:30,079 --> 00:10:33,600
container

00:10:30,720 --> 00:10:35,120
into individual in a sense

00:10:33,600 --> 00:10:38,720
self-contained groups called

00:10:35,120 --> 00:10:41,360
shards and each shard has the members

00:10:38,720 --> 00:10:41,760
of every related group of every element

00:10:41,360 --> 00:10:44,320
that

00:10:41,760 --> 00:10:47,040
is in the shard so if there's some

00:10:44,320 --> 00:10:49,040
element e of n that's in the shard

00:10:47,040 --> 00:10:50,800
all of its related group is part of the

00:10:49,040 --> 00:10:51,839
shard and that relationship holds for

00:10:50,800 --> 00:10:55,200
every element that's

00:10:51,839 --> 00:10:56,959
in the shard so

00:10:55,200 --> 00:10:58,959
in this case you could have a situation

00:10:56,959 --> 00:11:00,000
where updating the shard is performed by

00:10:58,959 --> 00:11:02,560
a single thread that's

00:11:00,000 --> 00:11:05,120
dedicated to servicing that shard and in

00:11:02,560 --> 00:11:06,880
effect there is no locking because

00:11:05,120 --> 00:11:08,800
there's no need to lock because we know

00:11:06,880 --> 00:11:10,480
by design that only one thread is going

00:11:08,800 --> 00:11:13,920
to be writing to

00:11:10,480 --> 00:11:15,200
or updating that particular shard the

00:11:13,920 --> 00:11:17,680
upside to this is

00:11:15,200 --> 00:11:18,959
you can get good performance however the

00:11:17,680 --> 00:11:20,959
downside

00:11:18,959 --> 00:11:22,640
is that it increases the complexity of

00:11:20,959 --> 00:11:24,240
your solution you have to

00:11:22,640 --> 00:11:26,160
manage your containers and manage your

00:11:24,240 --> 00:11:28,720
data appropriately

00:11:26,160 --> 00:11:29,839
and also it works if and only if the

00:11:28,720 --> 00:11:33,279
data itself

00:11:29,839 --> 00:11:33,279
is amenable to sharding

00:11:34,079 --> 00:11:37,519
another idea another idea that you might

00:11:36,959 --> 00:11:40,720
have

00:11:37,519 --> 00:11:41,120
is having a per element mutex in other

00:11:40,720 --> 00:11:43,839
words

00:11:41,120 --> 00:11:44,560
every single element has its own mutex

00:11:43,839 --> 00:11:46,959
and

00:11:44,560 --> 00:11:47,760
in order to perform an update on an

00:11:46,959 --> 00:11:50,000
update group

00:11:47,760 --> 00:11:51,839
you acquire all the mutexes for all the

00:11:50,000 --> 00:11:53,600
elements in that update group

00:11:51,839 --> 00:11:55,760
you do the updates and then you release

00:11:53,600 --> 00:11:57,760
them all at once

00:11:55,760 --> 00:11:59,120
the upside to this is it seems like it

00:11:57,760 --> 00:12:01,600
ought to work

00:11:59,120 --> 00:12:03,360
and it at least it seems like it should

00:12:01,600 --> 00:12:05,120
work for the case where the update group

00:12:03,360 --> 00:12:08,639
has exactly one element which

00:12:05,120 --> 00:12:10,079
is e the element you're trying to update

00:12:08,639 --> 00:12:12,399
it's a little more difficult to

00:12:10,079 --> 00:12:14,720
understand and think about

00:12:12,399 --> 00:12:17,920
the downside however is some mutex

00:12:14,720 --> 00:12:21,279
implementations are not small

00:12:17,920 --> 00:12:24,079
posix p thread mutexes are pretty big

00:12:21,279 --> 00:12:24,639
but the the tricky situation is what

00:12:24,079 --> 00:12:26,880
happens

00:12:24,639 --> 00:12:28,079
if you have a an element e naught that

00:12:26,880 --> 00:12:30,639
you want to update

00:12:28,079 --> 00:12:31,839
and its related group contains element

00:12:30,639 --> 00:12:34,240
e1

00:12:31,839 --> 00:12:35,120
and concurrently another thread is

00:12:34,240 --> 00:12:37,760
trying to update

00:12:35,120 --> 00:12:39,519
element e1 and e1's related group

00:12:37,760 --> 00:12:41,760
contains e naught

00:12:39,519 --> 00:12:43,360
well now you're going to have an order

00:12:41,760 --> 00:12:46,880
of operations where

00:12:43,360 --> 00:12:49,440
you end up with a deadlock

00:12:46,880 --> 00:12:50,560
obviously we don't want this to occur so

00:12:49,440 --> 00:12:52,240
the

00:12:50,560 --> 00:12:53,760
per element mutex is kind of like a

00:12:52,240 --> 00:12:55,200
mirage in the desert

00:12:53,760 --> 00:12:58,720
you think it might work you think it's

00:12:55,200 --> 00:13:00,800
gonna it's there but it really isn't

00:12:58,720 --> 00:13:01,760
so as i was trying to solve this problem

00:13:00,800 --> 00:13:03,600
i came across

00:13:01,760 --> 00:13:04,800
and having done a tiny amount of work

00:13:03,600 --> 00:13:07,519
with some databases

00:13:04,800 --> 00:13:08,480
in the past i remember this thing called

00:13:07,519 --> 00:13:11,120
timestamps

00:13:08,480 --> 00:13:12,160
so i looked and found an algorithm

00:13:11,120 --> 00:13:14,079
algorithms

00:13:12,160 --> 00:13:16,240
based on the concept of something called

00:13:14,079 --> 00:13:19,360
strict timestamp ordering

00:13:16,240 --> 00:13:22,000
now strict timestamp ordering or sto

00:13:19,360 --> 00:13:23,440
is one of a large variety of database

00:13:22,000 --> 00:13:25,279
concurrency

00:13:23,440 --> 00:13:27,040
control algorithms there's a whole

00:13:25,279 --> 00:13:29,440
wealth of them out there and sto

00:13:27,040 --> 00:13:31,120
is one of the first ones they teach you

00:13:29,440 --> 00:13:34,480
uh when you learn about

00:13:31,120 --> 00:13:38,320
transaction processing in databases

00:13:34,480 --> 00:13:40,399
so it's a transactional algorithm

00:13:38,320 --> 00:13:42,320
and as the name suggests it is time

00:13:40,399 --> 00:13:47,199
stamped based

00:13:42,320 --> 00:13:49,920
so what's a time stamp well a timestamp

00:13:47,199 --> 00:13:51,600
is a monotonically increasing value

00:13:49,920 --> 00:13:55,279
usually an integer value

00:13:51,600 --> 00:13:57,839
that indicates the age of a transaction

00:13:55,279 --> 00:13:58,560
so if you have a timestamp value that's

00:13:57,839 --> 00:14:01,519
lower

00:13:58,560 --> 00:14:02,000
that means it's an older transaction if

00:14:01,519 --> 00:14:04,639
you have

00:14:02,000 --> 00:14:05,120
a transaction value timestamp value

00:14:04,639 --> 00:14:07,360
that's

00:14:05,120 --> 00:14:08,959
that's greater that means it's a newer

00:14:07,360 --> 00:14:12,320
transaction

00:14:08,959 --> 00:14:14,839
in essence the timestamp value is very

00:14:12,320 --> 00:14:17,040
much like the birthday of the

00:14:14,839 --> 00:14:20,720
transaction

00:14:17,040 --> 00:14:23,920
so the idea behind sto is that uses

00:14:20,720 --> 00:14:24,959
time stamps uh as the basis of

00:14:23,920 --> 00:14:27,680
serializing

00:14:24,959 --> 00:14:28,720
concurrent transactions so that we avoid

00:14:27,680 --> 00:14:30,560
race conditions

00:14:28,720 --> 00:14:34,079
and that we fill we fulfill the

00:14:30,560 --> 00:14:34,079
conditions of atomicity

00:14:34,160 --> 00:14:38,079
consistency and isolation

00:14:38,880 --> 00:14:46,000
so in general when a transaction begins

00:14:42,800 --> 00:14:48,399
it's assigned a unique timestamp and

00:14:46,000 --> 00:14:50,079
for this to work correctly the timestamp

00:14:48,399 --> 00:14:50,800
has to come from an authoritative

00:14:50,079 --> 00:14:53,199
universal

00:14:50,800 --> 00:14:55,360
timestamp source every entity that's

00:14:53,199 --> 00:14:58,639
using a timestamp has to agree on where

00:14:55,360 --> 00:15:01,199
they're getting timestamps from

00:14:58,639 --> 00:15:02,000
now if two transactions are trying to

00:15:01,199 --> 00:15:04,000
update

00:15:02,000 --> 00:15:06,240
or write to the same update group at the

00:15:04,000 --> 00:15:08,560
same time the principle is

00:15:06,240 --> 00:15:10,480
the transaction with the lower timestamp

00:15:08,560 --> 00:15:13,920
the the older transaction

00:15:10,480 --> 00:15:16,480
goes first younger transactions always

00:15:13,920 --> 00:15:18,880
wait for their elders

00:15:16,480 --> 00:15:20,320
and older transactions never wait for

00:15:18,880 --> 00:15:23,360
younger transactions

00:15:20,320 --> 00:15:27,120
instead they roll back they give up

00:15:23,360 --> 00:15:30,720
and they try again so

00:15:27,120 --> 00:15:32,959
very smart people have proven that sto

00:15:30,720 --> 00:15:33,759
operation schedules in other words the

00:15:32,959 --> 00:15:37,199
order of

00:15:33,759 --> 00:15:40,639
operations in which updates are done

00:15:37,199 --> 00:15:43,519
in sto based systems are serializable

00:15:40,639 --> 00:15:45,120
and deadlock free which are you know

00:15:43,519 --> 00:15:48,560
great properties to have for our

00:15:45,120 --> 00:15:50,639
our problem however

00:15:48,560 --> 00:15:51,600
as probably apparent from the previous

00:15:50,639 --> 00:15:53,839
bullet

00:15:51,600 --> 00:15:54,959
the price that you pay for this power in

00:15:53,839 --> 00:15:58,800
this freedom

00:15:54,959 --> 00:16:00,399
is that it's possible that a thread

00:15:58,800 --> 00:16:01,920
who's trying to do an update could

00:16:00,399 --> 00:16:04,959
starve and

00:16:01,920 --> 00:16:06,639
have to restart the transaction and you

00:16:04,959 --> 00:16:08,720
might be able to get into a situation

00:16:06,639 --> 00:16:11,440
where a thread trying to do an update

00:16:08,720 --> 00:16:12,160
keeps keeps trying to update by the time

00:16:11,440 --> 00:16:14,160
it's

00:16:12,160 --> 00:16:16,320
its turn realizes that a younger

00:16:14,160 --> 00:16:17,600
transaction has actually already updated

00:16:16,320 --> 00:16:19,279
and has to go back

00:16:17,600 --> 00:16:22,959
so there could be some starvation in

00:16:19,279 --> 00:16:25,199
this case

00:16:22,959 --> 00:16:26,000
so talking about the algorithm in the

00:16:25,199 --> 00:16:28,240
general case

00:16:26,000 --> 00:16:30,399
in just a little more detail let's

00:16:28,240 --> 00:16:33,199
suppose that we have some element e

00:16:30,399 --> 00:16:34,720
and associated with e we have a a read

00:16:33,199 --> 00:16:37,600
time stamp

00:16:34,720 --> 00:16:39,440
and a write timestamp value and tsv is

00:16:37,600 --> 00:16:40,160
is the nomenclature i use here to

00:16:39,440 --> 00:16:43,519
indicate

00:16:40,160 --> 00:16:44,560
uh timestamp value so if you have a

00:16:43,519 --> 00:16:46,800
transaction

00:16:44,560 --> 00:16:48,320
uh t let's call it tx1 and has a

00:16:46,800 --> 00:16:50,320
timestamp tsv

00:16:48,320 --> 00:16:51,680
let's assume that it has functions begin

00:16:50,320 --> 00:16:53,759
commit and roll back

00:16:51,680 --> 00:16:55,440
and there's a facility for transactions

00:16:53,759 --> 00:16:57,199
to acquire elements

00:16:55,440 --> 00:16:58,639
acquire might be a function or it might

00:16:57,199 --> 00:17:00,240
not but there has to be a

00:16:58,639 --> 00:17:02,320
there has to be a way for a transaction

00:17:00,240 --> 00:17:05,679
to say this element is mine

00:17:02,320 --> 00:17:07,600
right now let's assume that's part of

00:17:05,679 --> 00:17:09,280
our business logic we have a function to

00:17:07,600 --> 00:17:11,280
update e

00:17:09,280 --> 00:17:13,360
and we have a function that reading from

00:17:11,280 --> 00:17:15,199
e

00:17:13,360 --> 00:17:18,559
so let's look at the read case

00:17:15,199 --> 00:17:21,839
transaction wants to read the element

00:17:18,559 --> 00:17:24,720
if if the transaction t1

00:17:21,839 --> 00:17:25,439
finds that it's its timestamp is less

00:17:24,720 --> 00:17:28,000
than

00:17:25,439 --> 00:17:29,840
the last write timestamp in the element

00:17:28,000 --> 00:17:30,960
that means a younger transaction is

00:17:29,840 --> 00:17:35,120
already written

00:17:30,960 --> 00:17:36,799
to e so the the transaction t1 rolls

00:17:35,120 --> 00:17:39,039
back

00:17:36,799 --> 00:17:39,840
if transaction finds that its timestamp

00:17:39,039 --> 00:17:42,559
is older

00:17:39,840 --> 00:17:44,640
than the last write that me that could

00:17:42,559 --> 00:17:46,000
possibly mean that an older transaction

00:17:44,640 --> 00:17:48,640
is in progress

00:17:46,000 --> 00:17:50,400
so transaction tx1 will wait until all

00:17:48,640 --> 00:17:53,200
the older transactions

00:17:50,400 --> 00:17:55,440
in our example here tx0 are done then

00:17:53,200 --> 00:17:58,320
tx1 will acquire the element

00:17:55,440 --> 00:17:59,600
it will assign its timestamp value to

00:17:58,320 --> 00:18:01,919
the read timestamp

00:17:59,600 --> 00:18:05,280
and it will read ease contents it then

00:18:01,919 --> 00:18:07,200
releases e and goes on its way

00:18:05,280 --> 00:18:08,559
writing is a little more difficult in

00:18:07,200 --> 00:18:11,440
the sa

00:18:08,559 --> 00:18:11,919
as with reading if i'm trying to write

00:18:11,440 --> 00:18:15,200
and

00:18:11,919 --> 00:18:18,160
a younger transaction is already written

00:18:15,200 --> 00:18:19,440
has already read from e that means i

00:18:18,160 --> 00:18:21,840
have to roll back

00:18:19,440 --> 00:18:23,039
i can't write to a transaction i can't

00:18:21,840 --> 00:18:25,120
write to an element

00:18:23,039 --> 00:18:28,240
if a younger transaction has already

00:18:25,120 --> 00:18:31,039
read from it

00:18:28,240 --> 00:18:31,440
if my time stamp value is less than e's

00:18:31,039 --> 00:18:33,200
right

00:18:31,440 --> 00:18:35,600
timestamp that means a younger

00:18:33,200 --> 00:18:37,679
transaction has already written to e

00:18:35,600 --> 00:18:38,720
so again if a younger transaction is

00:18:37,679 --> 00:18:41,840
already written

00:18:38,720 --> 00:18:42,559
i have to roll back and start again and

00:18:41,840 --> 00:18:46,720
as before

00:18:42,559 --> 00:18:48,480
if my uh if my transaction value

00:18:46,720 --> 00:18:50,160
timestamp value is greater than the

00:18:48,480 --> 00:18:52,000
right timestamp for e

00:18:50,160 --> 00:18:54,320
that means an older transaction might be

00:18:52,000 --> 00:18:56,000
in progress i wait until all the older

00:18:54,320 --> 00:18:59,360
transactions are done

00:18:56,000 --> 00:19:01,840
and then i acquire e i update ease write

00:18:59,360 --> 00:19:03,679
timestamp with my timestamp and i update

00:19:01,840 --> 00:19:07,679
the contents i release e

00:19:03,679 --> 00:19:09,520
and i go in our case

00:19:07,679 --> 00:19:11,120
we only have to worry about writes

00:19:09,520 --> 00:19:11,840
because that's a that's part of the

00:19:11,120 --> 00:19:13,520
problem here

00:19:11,840 --> 00:19:15,440
all of the all of the changes the

00:19:13,520 --> 00:19:17,760
container are update operations

00:19:15,440 --> 00:19:18,799
so we only need one time stamp per

00:19:17,760 --> 00:19:21,200
element

00:19:18,799 --> 00:19:21,840
so in this case every operation i want

00:19:21,200 --> 00:19:24,000
to perform

00:19:21,840 --> 00:19:25,120
is an update so when our transaction

00:19:24,000 --> 00:19:26,960
calls update

00:19:25,120 --> 00:19:29,120
if a younger transaction is already

00:19:26,960 --> 00:19:29,600
written uh has already written to the

00:19:29,120 --> 00:19:32,880
element

00:19:29,600 --> 00:19:35,200
we roll back and as before if an

00:19:32,880 --> 00:19:36,240
older if an older transaction has

00:19:35,200 --> 00:19:38,320
written to it

00:19:36,240 --> 00:19:40,240
it might be in progress we wait for the

00:19:38,320 --> 00:19:43,200
older transaction to complete

00:19:40,240 --> 00:19:45,280
we acquire e we assign to it we assign

00:19:43,200 --> 00:19:45,919
our timestamp to ease timestamp and we

00:19:45,280 --> 00:19:49,200
update e

00:19:45,919 --> 00:19:51,120
and go on so

00:19:49,200 --> 00:19:52,799
in order to do this we need some basic

00:19:51,120 --> 00:19:53,039
synchronization components which modern

00:19:52,799 --> 00:19:55,840
c

00:19:53,039 --> 00:19:57,600
plus plus provides we need a mutex we

00:19:55,840 --> 00:19:59,919
need to condition variables

00:19:57,600 --> 00:20:02,159
we need atomic pointers and we need the

00:19:59,919 --> 00:20:04,559
ability to do atomic comparing exchange

00:20:02,159 --> 00:20:06,320
on pointers

00:20:04,559 --> 00:20:07,919
we need a class that represents a

00:20:06,320 --> 00:20:11,600
lockable item i.e

00:20:07,919 --> 00:20:13,679
the element that i've mentioned and

00:20:11,600 --> 00:20:15,200
we need a class that manages

00:20:13,679 --> 00:20:18,240
transactions

00:20:15,200 --> 00:20:19,200
on behalf of each thread and in this

00:20:18,240 --> 00:20:21,600
design

00:20:19,200 --> 00:20:22,400
transactions are not actually objects

00:20:21,600 --> 00:20:24,320
transactions

00:20:22,400 --> 00:20:25,679
are ephemeral things they don't live

00:20:24,320 --> 00:20:28,080
very long

00:20:25,679 --> 00:20:30,480
and it just turned out i found to be

00:20:28,080 --> 00:20:31,520
easier to have a manager class that is

00:20:30,480 --> 00:20:33,760
long-lived

00:20:31,520 --> 00:20:36,880
that has the life of the thread and that

00:20:33,760 --> 00:20:39,280
is responsible for managing transactions

00:20:36,880 --> 00:20:41,440
transactions themselves are represented

00:20:39,280 --> 00:20:43,360
by a timestamp value

00:20:41,440 --> 00:20:46,000
so when a transaction is in progress

00:20:43,360 --> 00:20:49,200
that has that timestamp value

00:20:46,000 --> 00:20:51,679
in a sense that transaction is alive

00:20:49,200 --> 00:20:53,919
when the transaction manager's timestamp

00:20:51,679 --> 00:20:55,039
value changes to the next timestamp that

00:20:53,919 --> 00:20:57,280
it requires

00:20:55,039 --> 00:21:00,640
the previous transaction is done and the

00:20:57,280 --> 00:21:00,640
new transaction begins

00:21:01,200 --> 00:21:07,360
so threads share

00:21:04,640 --> 00:21:09,200
a containers or containers that hold the

00:21:07,360 --> 00:21:10,880
lockable items

00:21:09,200 --> 00:21:12,960
each thread initiates its own

00:21:10,880 --> 00:21:16,080
transactions and each thread owns the

00:21:12,960 --> 00:21:18,960
transactions that it initiates

00:21:16,080 --> 00:21:20,320
each transaction's key operations

00:21:18,960 --> 00:21:22,640
beginning the transaction

00:21:20,320 --> 00:21:24,320
acquiring the transaction committing and

00:21:22,640 --> 00:21:27,120
rolling back the transaction

00:21:24,320 --> 00:21:27,440
can only be performed by the thread that

00:21:27,120 --> 00:21:29,280
that

00:21:27,440 --> 00:21:32,320
owns the transaction in other words the

00:21:29,280 --> 00:21:34,799
transaction manager

00:21:32,320 --> 00:21:36,480
the transaction manager acquires the

00:21:34,799 --> 00:21:39,600
lockable items it locks them

00:21:36,480 --> 00:21:41,840
on behalf of the owning thread and then

00:21:39,600 --> 00:21:43,360
other logic the business logic that

00:21:41,840 --> 00:21:46,480
actually does the process

00:21:43,360 --> 00:21:47,919
of determining the update group updating

00:21:46,480 --> 00:21:51,120
the update group

00:21:47,919 --> 00:21:54,480
is part of a code that lies

00:21:51,120 --> 00:21:57,760
outside the transaction manager

00:21:54,480 --> 00:21:59,360
in any transaction we have to acquire

00:21:57,760 --> 00:22:00,880
because we may need to write to every

00:21:59,360 --> 00:22:02,880
element of the update group

00:22:00,880 --> 00:22:04,720
we have to acquire all of the members of

00:22:02,880 --> 00:22:06,640
the update group before you can do any

00:22:04,720 --> 00:22:08,799
right operation to any member of the

00:22:06,640 --> 00:22:10,480
group

00:22:08,799 --> 00:22:12,480
so here's some pseudo code that i'd like

00:22:10,480 --> 00:22:14,559
to walk to through very quickly

00:22:12,480 --> 00:22:16,880
that describes the algorithm at a high

00:22:14,559 --> 00:22:18,240
level before we jump into real code

00:22:16,880 --> 00:22:20,240
so let's assume that we're starting a

00:22:18,240 --> 00:22:21,919
thread function and one of the arguments

00:22:20,240 --> 00:22:23,919
to that thread function is a reference

00:22:21,919 --> 00:22:25,600
to our shared collection

00:22:23,919 --> 00:22:27,600
the very first thing we do in the thread

00:22:25,600 --> 00:22:29,280
function is we create the transaction

00:22:27,600 --> 00:22:31,919
manager

00:22:29,280 --> 00:22:32,480
once the transaction manager starts we

00:22:31,919 --> 00:22:34,559
begin

00:22:32,480 --> 00:22:37,039
looking for work to do so let's assume

00:22:34,559 --> 00:22:38,400
that there's work that needs to be done

00:22:37,039 --> 00:22:40,080
all right there's work to left to be

00:22:38,400 --> 00:22:43,760
done we're going to begin a new

00:22:40,080 --> 00:22:46,480
transaction all right

00:22:43,760 --> 00:22:48,000
so as part of that transaction we've

00:22:46,480 --> 00:22:48,559
determined that there's some target

00:22:48,000 --> 00:22:50,400
element

00:22:48,559 --> 00:22:52,159
in the shared collection that we're

00:22:50,400 --> 00:22:56,000
interested in

00:22:52,159 --> 00:22:57,840
so the first thing we'll do is we will

00:22:56,000 --> 00:22:59,520
try to determine the membership of the

00:22:57,840 --> 00:23:01,520
update group to the greatest extent

00:22:59,520 --> 00:23:04,240
possible

00:23:01,520 --> 00:23:05,520
then we will in a sense recursively or

00:23:04,240 --> 00:23:07,840
iteratively

00:23:05,520 --> 00:23:08,880
uh look through the update group so

00:23:07,840 --> 00:23:11,120
while update group

00:23:08,880 --> 00:23:12,400
members remain unacquired there could be

00:23:11,120 --> 00:23:14,640
more update groups in

00:23:12,400 --> 00:23:16,240
a transitive relationship and all of the

00:23:14,640 --> 00:23:18,880
acquisitions that we've performed have

00:23:16,240 --> 00:23:21,360
succeeded so far

00:23:18,880 --> 00:23:22,000
we will start to acquire the next

00:23:21,360 --> 00:23:24,159
unacquired

00:23:22,000 --> 00:23:25,679
member of the group we revise the

00:23:24,159 --> 00:23:27,919
membership of the group

00:23:25,679 --> 00:23:29,679
and we go back up again and so what this

00:23:27,919 --> 00:23:30,480
loop is indicating is that you start

00:23:29,679 --> 00:23:32,400
with e

00:23:30,480 --> 00:23:34,000
and you keep iterating until you find

00:23:32,400 --> 00:23:35,919
all the members

00:23:34,000 --> 00:23:37,440
you compute the set closure of all the

00:23:35,919 --> 00:23:39,280
elements that are part of the update

00:23:37,440 --> 00:23:42,960
group of e

00:23:39,280 --> 00:23:43,520
once that's done if we are able to

00:23:42,960 --> 00:23:45,919
acquire

00:23:43,520 --> 00:23:47,919
all the members of the update group then

00:23:45,919 --> 00:23:48,640
we apply our write operations to that

00:23:47,919 --> 00:23:51,520
group

00:23:48,640 --> 00:23:52,720
we commit the transaction and we go do

00:23:51,520 --> 00:23:55,200
new work

00:23:52,720 --> 00:23:57,200
otherwise if we were not able to acquire

00:23:55,200 --> 00:23:59,760
all the elements of the update group

00:23:57,200 --> 00:24:00,720
we roll back the transaction we loop

00:23:59,760 --> 00:24:05,520
back to the top

00:24:00,720 --> 00:24:07,919
and we try again so

00:24:05,520 --> 00:24:10,400
let's look at some prerequisites uh for

00:24:07,919 --> 00:24:10,400
the code

00:24:11,520 --> 00:24:15,360
so here's a little bit of the header and

00:24:13,760 --> 00:24:16,799
as we go through the code

00:24:15,360 --> 00:24:18,400
you'll see that there's some areas that

00:24:16,799 --> 00:24:20,000
i've highlighted in yellow and those are

00:24:18,400 --> 00:24:20,880
the things i'd like you to pay attention

00:24:20,000 --> 00:24:23,039
to

00:24:20,880 --> 00:24:24,799
so of particular importance here in our

00:24:23,039 --> 00:24:26,159
forward declarations header are the time

00:24:24,799 --> 00:24:28,880
stamp value type

00:24:26,159 --> 00:24:31,520
which is an unsigned 64 in a forward

00:24:28,880 --> 00:24:33,840
declaration of the transaction manager

00:24:31,520 --> 00:24:36,080
and a forward declaration of lockable

00:24:33,840 --> 00:24:36,080
item

00:24:36,640 --> 00:24:42,159
so let's look at the lockable item class

00:24:40,480 --> 00:24:43,840
so here's the class declaration it's

00:24:42,159 --> 00:24:44,559
intended to be used in these demo

00:24:43,840 --> 00:24:48,159
programs

00:24:44,559 --> 00:24:50,000
as a base class so

00:24:48,159 --> 00:24:51,840
it can return its id which is not

00:24:50,000 --> 00:24:53,600
relevant to our discussion it can return

00:24:51,840 --> 00:24:55,279
the last time stamp value

00:24:53,600 --> 00:24:57,200
but of particular interest is the fact

00:24:55,279 --> 00:24:58,480
that it's friend class is a transaction

00:24:57,200 --> 00:25:02,080
manager

00:24:58,480 --> 00:25:04,720
and it's interested in an atomic pointer

00:25:02,080 --> 00:25:06,159
an atomic pointer of type transaction

00:25:04,720 --> 00:25:09,200
manager star

00:25:06,159 --> 00:25:11,279
it it maintains

00:25:09,200 --> 00:25:12,240
an atomic pointer to a transaction

00:25:11,279 --> 00:25:15,120
manager

00:25:12,240 --> 00:25:16,080
and it also maintains the timestamp

00:25:15,120 --> 00:25:19,440
value of its

00:25:16,080 --> 00:25:21,120
last owner and this is important uh

00:25:19,440 --> 00:25:22,720
this is how we know this is how we

00:25:21,120 --> 00:25:25,440
determine who is last

00:25:22,720 --> 00:25:28,159
who is the last updater of this element

00:25:25,440 --> 00:25:30,720
and this is how an element knows

00:25:28,159 --> 00:25:31,520
the atomic pointer tells the element who

00:25:30,720 --> 00:25:33,600
owns me

00:25:31,520 --> 00:25:36,799
right now at this moment or who has

00:25:33,600 --> 00:25:36,799
locked me at this moment

00:25:36,880 --> 00:25:40,000
and here's some of the code for that

00:25:38,720 --> 00:25:41,760
very simple class

00:25:40,000 --> 00:25:43,679
i only bring it up because here's the

00:25:41,760 --> 00:25:45,039
interesting stuff we start with a

00:25:43,679 --> 00:25:48,400
timestamp value

00:25:45,039 --> 00:25:50,240
of zero so every time stamp

00:25:48,400 --> 00:25:52,000
no timestamp can be zero zero is the

00:25:50,240 --> 00:25:53,520
null timestamp the first valid time

00:25:52,000 --> 00:25:56,559
stamp is one

00:25:53,520 --> 00:25:58,320
and we we initialize the atomic pointer

00:25:56,559 --> 00:25:59,039
to the transaction manager with a null

00:25:58,320 --> 00:26:00,880
pointer

00:25:59,039 --> 00:26:02,400
and we're using the convention here that

00:26:00,880 --> 00:26:05,279
a null pointer means

00:26:02,400 --> 00:26:08,320
i am currently unlocked no one currently

00:26:05,279 --> 00:26:08,320
owns me at the moment

00:26:09,360 --> 00:26:14,400
so now let's dig into the transaction

00:26:11,120 --> 00:26:14,400
manager class a little bit

00:26:14,880 --> 00:26:18,000
this is a little bit longer and it

00:26:16,880 --> 00:26:20,960
contains uh

00:26:18,000 --> 00:26:22,480
member functions that you might expect

00:26:20,960 --> 00:26:26,240
it to have

00:26:22,480 --> 00:26:29,279
it has a begin a commit a roll back

00:26:26,240 --> 00:26:31,120
and it has acquire an element

00:26:29,279 --> 00:26:32,960
and here we're passing a reference to a

00:26:31,120 --> 00:26:34,720
lockable item as i mentioned lockable

00:26:32,960 --> 00:26:35,440
item is intended to be used as a base

00:26:34,720 --> 00:26:37,360
class

00:26:35,440 --> 00:26:38,559
so we acquire it with a reference to the

00:26:37,360 --> 00:26:40,400
base class because

00:26:38,559 --> 00:26:43,120
the base class ever has everything in it

00:26:40,400 --> 00:26:44,880
that we need in order to do the locking

00:26:43,120 --> 00:26:47,120
these member functions which are part of

00:26:44,880 --> 00:26:48,559
the transaction manager are part of its

00:26:47,120 --> 00:26:51,360
public interface

00:26:48,559 --> 00:26:52,559
and they are intended to be used by the

00:26:51,360 --> 00:26:54,320
business logic

00:26:52,559 --> 00:26:56,480
in the rest of the thread that's

00:26:54,320 --> 00:26:58,799
attempting to to do the updates

00:26:56,480 --> 00:27:00,080
to acquire the update group to update

00:26:58,799 --> 00:27:02,559
them appropriately

00:27:00,080 --> 00:27:06,720
and then commit or roll back as the case

00:27:02,559 --> 00:27:09,520
may be

00:27:06,720 --> 00:27:11,279
so let's look at the the uh dig in a

00:27:09,520 --> 00:27:13,679
little bit to the implementation

00:27:11,279 --> 00:27:15,120
here in i've highlighted in yellow just

00:27:13,679 --> 00:27:15,760
some simple type depths that i've

00:27:15,120 --> 00:27:17,679
created

00:27:15,760 --> 00:27:20,320
to make the code a little easier for me

00:27:17,679 --> 00:27:23,200
to read

00:27:20,320 --> 00:27:25,600
and here are the relevant data members

00:27:23,200 --> 00:27:29,279
relevant to this discussion

00:27:25,600 --> 00:27:32,080
we have a timestamp value this indicates

00:27:29,279 --> 00:27:33,840
the timestamp that i that is current uh

00:27:32,080 --> 00:27:37,039
this indicates the transaction that is

00:27:33,840 --> 00:27:40,399
currently in progress

00:27:37,039 --> 00:27:43,039
i have a list of pointers to items

00:27:40,399 --> 00:27:44,480
so here is my vector of pointers to

00:27:43,039 --> 00:27:48,320
lockable items

00:27:44,480 --> 00:27:50,000
and so this this vector of pointers to

00:27:48,320 --> 00:27:50,880
lockable items to elements this

00:27:50,000 --> 00:27:54,000
indicates

00:27:50,880 --> 00:27:56,159
the members of the update group

00:27:54,000 --> 00:27:57,120
i have a mutex which i mentioned before

00:27:56,159 --> 00:28:00,399
i will use for

00:27:57,120 --> 00:28:02,320
locking and a condition variable

00:28:00,399 --> 00:28:04,000
that i'll use for waking threads that

00:28:02,320 --> 00:28:05,520
are waiting for a transaction to

00:28:04,000 --> 00:28:08,799
complete

00:28:05,520 --> 00:28:11,200
and finally i have uh here

00:28:08,799 --> 00:28:12,000
here i've defined an atomic tsv it's an

00:28:11,200 --> 00:28:14,640
atomic

00:28:12,000 --> 00:28:15,520
unsigned 64-bit integer and here i've

00:28:14,640 --> 00:28:18,240
defined

00:28:15,520 --> 00:28:18,799
a static member which is the generator

00:28:18,240 --> 00:28:21,200
this is

00:28:18,799 --> 00:28:23,360
in effect the authoritative universal

00:28:21,200 --> 00:28:26,399
source of new transaction

00:28:23,360 --> 00:28:26,399
timestamp values

00:28:26,960 --> 00:28:31,360
so the picture i'd like you to keep in

00:28:28,559 --> 00:28:33,039
your mind is this

00:28:31,360 --> 00:28:34,799
here i have two different threads in my

00:28:33,039 --> 00:28:36,640
very simple diagram

00:28:34,799 --> 00:28:39,520
each thread has its own transaction

00:28:36,640 --> 00:28:41,919
manager and each transaction manager

00:28:39,520 --> 00:28:43,440
has the list of lockable items which are

00:28:41,919 --> 00:28:46,640
currently

00:28:43,440 --> 00:28:48,080
members of the update group the list of

00:28:46,640 --> 00:28:50,159
lockable items

00:28:48,080 --> 00:28:52,080
has pointers which point to the relevant

00:28:50,159 --> 00:28:53,600
elements that are members of the update

00:28:52,080 --> 00:28:56,559
group

00:28:53,600 --> 00:28:58,240
and each lockable item points back to

00:28:56,559 --> 00:29:00,880
the transaction manager

00:28:58,240 --> 00:29:01,679
so that the element that's being locked

00:29:00,880 --> 00:29:04,720
knows

00:29:01,679 --> 00:29:06,720
which transaction has locked it

00:29:04,720 --> 00:29:08,480
and here i've just drawn it in a simple

00:29:06,720 --> 00:29:10,720
way uh

00:29:08,480 --> 00:29:12,640
such that the you know the threads are

00:29:10,720 --> 00:29:15,039
the the ownership

00:29:12,640 --> 00:29:16,720
directions don't overlap with each other

00:29:15,039 --> 00:29:18,480
but you can imagine this

00:29:16,720 --> 00:29:20,159
played out over a larger number of

00:29:18,480 --> 00:29:22,799
threads and

00:29:20,159 --> 00:29:24,559
with these ownership relationships from

00:29:22,799 --> 00:29:25,200
the lockable items to the actual

00:29:24,559 --> 00:29:29,840
elements

00:29:25,200 --> 00:29:29,840
being spread over a large array of items

00:29:32,159 --> 00:29:35,919
so here's our here's our implementation

00:29:34,880 --> 00:29:37,679
of the static

00:29:35,919 --> 00:29:39,360
variable that's our timestamp value

00:29:37,679 --> 00:29:42,320
generator

00:29:39,360 --> 00:29:43,919
here's our constructor and so what i'm

00:29:42,320 --> 00:29:45,600
doing here this is the important and

00:29:43,919 --> 00:29:47,360
interesting initialization

00:29:45,600 --> 00:29:49,200
i'm setting the timestamp value i'm

00:29:47,360 --> 00:29:50,799
initializing that atomic integer to be

00:29:49,200 --> 00:29:54,880
zero

00:29:50,799 --> 00:29:57,279
i am uh i am going to uh initialize my

00:29:54,880 --> 00:29:58,799
array of pointers to items and in the

00:29:57,279 --> 00:30:00,640
body of the constructor i'm going to

00:29:58,799 --> 00:30:02,480
reserve 100 because it just seems like a

00:30:00,640 --> 00:30:06,240
reasonable number

00:30:02,480 --> 00:30:09,039
in the work that i was doing before uh

00:30:06,240 --> 00:30:09,919
the the levels of transitivity in those

00:30:09,039 --> 00:30:11,360
particular rate

00:30:09,919 --> 00:30:13,360
relationships were never really more

00:30:11,360 --> 00:30:15,120
than three or four and the total number

00:30:13,360 --> 00:30:16,640
of members in an update group is really

00:30:15,120 --> 00:30:19,919
never more than 20 in

00:30:16,640 --> 00:30:22,399
in practical circumstances uh

00:30:19,919 --> 00:30:22,960
here's my mutex that i'm going to use to

00:30:22,399 --> 00:30:25,200
guard

00:30:22,960 --> 00:30:26,240
shared state and here's the condition

00:30:25,200 --> 00:30:27,360
variable i'm going to use for

00:30:26,240 --> 00:30:30,720
broadcasting the

00:30:27,360 --> 00:30:30,720
completion of a transaction

00:30:30,880 --> 00:30:36,320
so returning the id and times timestamp

00:30:33,679 --> 00:30:41,200
value those are not very interesting

00:30:36,320 --> 00:30:44,559
so here's my begin and this is important

00:30:41,200 --> 00:30:47,919
when i begin the transaction

00:30:44,559 --> 00:30:50,000
i'm going to acquire the lock

00:30:47,919 --> 00:30:52,399
in my own mutex and the reason for this

00:30:50,000 --> 00:30:53,919
will become obvious later in the code

00:30:52,399 --> 00:30:55,520
i'm going to acquire the lock of the

00:30:53,919 --> 00:30:58,000
mutex that i own

00:30:55,520 --> 00:31:00,159
then i'm going to atomically update the

00:30:58,000 --> 00:31:02,159
timestamp value

00:31:00,159 --> 00:31:03,919
i want this to be atomic because if some

00:31:02,159 --> 00:31:04,320
other transaction is doing at the same

00:31:03,919 --> 00:31:06,320
time

00:31:04,320 --> 00:31:08,320
i want the two transactions to receive

00:31:06,320 --> 00:31:10,720
two different distinct values

00:31:08,320 --> 00:31:11,760
once i've updated my timestamp value i'm

00:31:10,720 --> 00:31:14,320
going to unlock

00:31:11,760 --> 00:31:19,440
the mutex that i own that's a member in

00:31:14,320 --> 00:31:20,960
my data here's the logic for commit

00:31:19,440 --> 00:31:24,159
it's a little more complicated but it's

00:31:20,960 --> 00:31:26,399
very it's it's analogous

00:31:24,159 --> 00:31:27,600
before i can commit a transaction i have

00:31:26,399 --> 00:31:30,720
to lock the mutex

00:31:27,600 --> 00:31:34,320
that's part of part of my member data

00:31:30,720 --> 00:31:37,519
and then while i still have elements

00:31:34,320 --> 00:31:40,399
that are members of of my

00:31:37,519 --> 00:31:41,039
uh while i still have items that i own

00:31:40,399 --> 00:31:45,840
that are

00:31:41,039 --> 00:31:47,760
members of the update group i'm going to

00:31:45,840 --> 00:31:51,039
i'm going to

00:31:47,760 --> 00:31:52,880
set the owning pointer

00:31:51,039 --> 00:31:54,880
in each of those elements i'm going to

00:31:52,880 --> 00:31:58,880
atomically set it back to null

00:31:54,880 --> 00:32:00,240
meaning this element is no longer owned

00:31:58,880 --> 00:32:02,000
and then i'm going to pop it off the

00:32:00,240 --> 00:32:04,880
back of the array

00:32:02,000 --> 00:32:05,840
so at the bottom of this array for all

00:32:04,880 --> 00:32:08,000
of the elements

00:32:05,840 --> 00:32:09,760
that were part of the update group i

00:32:08,000 --> 00:32:10,880
have set their transaction manager

00:32:09,760 --> 00:32:12,720
pointer

00:32:10,880 --> 00:32:14,000
back to the null pointer atomically

00:32:12,720 --> 00:32:16,880
indicating that

00:32:14,000 --> 00:32:19,200
no one these these elements are no

00:32:16,880 --> 00:32:22,399
longer locked

00:32:19,200 --> 00:32:25,519
and finally at the bottom i will

00:32:22,399 --> 00:32:27,679
broadcast on my condition variable

00:32:25,519 --> 00:32:29,440
i will notify them all in effect

00:32:27,679 --> 00:32:30,880
releasing them if anyone is waiting for

00:32:29,440 --> 00:32:32,880
this transaction to be done

00:32:30,880 --> 00:32:34,480
this notifies them the transaction is

00:32:32,880 --> 00:32:36,240
done and they can go back

00:32:34,480 --> 00:32:38,320
and try to acquire one or more members

00:32:36,240 --> 00:32:40,080
of that update group

00:32:38,320 --> 00:32:42,559
and of course as i drop out the bottom

00:32:40,080 --> 00:32:46,080
here uh the lock

00:32:42,559 --> 00:32:48,960
the the lock guarded lock

00:32:46,080 --> 00:32:51,360
unique lock destroys and the mutex is

00:32:48,960 --> 00:32:51,360
released

00:32:51,679 --> 00:32:57,279
so acquire is

00:32:55,440 --> 00:32:58,640
the function that's was most difficult

00:32:57,279 --> 00:32:59,919
and trickiest to implement

00:32:58,640 --> 00:33:01,360
and so what i'm going to do in the next

00:32:59,919 --> 00:33:02,880
few sections is i'm going to show you

00:33:01,360 --> 00:33:04,240
pseudo code for the algorithm

00:33:02,880 --> 00:33:05,679
and i'm going to interleave it with the

00:33:04,240 --> 00:33:07,360
real code and hopefully it will make

00:33:05,679 --> 00:33:10,399
sense as i do so

00:33:07,360 --> 00:33:12,240
so i am i've

00:33:10,399 --> 00:33:13,440
initiated transaction i've begun a

00:33:12,240 --> 00:33:15,919
transaction

00:33:13,440 --> 00:33:16,720
i have my first element i know what my

00:33:15,919 --> 00:33:19,840
first element is

00:33:16,720 --> 00:33:21,519
and now i need to acquire it right and i

00:33:19,840 --> 00:33:23,039
need to subsequently acquire elements in

00:33:21,519 --> 00:33:27,440
the update group

00:33:23,039 --> 00:33:29,440
so i'll loop if i can acquire an item

00:33:27,440 --> 00:33:31,120
meaning no one else has locked it i'm

00:33:29,440 --> 00:33:32,080
going to add that item to the list of

00:33:31,120 --> 00:33:36,080
items that i own

00:33:32,080 --> 00:33:38,320
list of lockable items if my timestamp

00:33:36,080 --> 00:33:39,200
having acquired this if my timestamp is

00:33:38,320 --> 00:33:42,000
newer

00:33:39,200 --> 00:33:45,360
meaning it's higher than the lockable

00:33:42,000 --> 00:33:47,760
item's last timestamp

00:33:45,360 --> 00:33:49,120
i'm sorry if my timestamp is newer that

00:33:47,760 --> 00:33:51,679
means that the

00:33:49,120 --> 00:33:53,679
that an older transaction has written it

00:33:51,679 --> 00:33:57,360
so if my timestamp is newer

00:33:53,679 --> 00:34:00,240
the older transactions are done i set

00:33:57,360 --> 00:34:01,360
item's timestamp to my timestamp i mark

00:34:00,240 --> 00:34:04,840
this as a success

00:34:01,360 --> 00:34:07,120
and i keep going otherwise if my

00:34:04,840 --> 00:34:09,119
timestamp is older

00:34:07,120 --> 00:34:11,200
than the item's timestamp that means a

00:34:09,119 --> 00:34:14,000
younger transaction has updated it

00:34:11,200 --> 00:34:16,079
and i roll back and go back to the top

00:34:14,000 --> 00:34:18,000
now if i don't acquire the item if i'm

00:34:16,079 --> 00:34:20,240
not able to acquire the item

00:34:18,000 --> 00:34:22,240
it could be that i already own the item

00:34:20,240 --> 00:34:22,800
if i'm computing the set closure it

00:34:22,240 --> 00:34:25,440
could be

00:34:22,800 --> 00:34:27,440
that for some traversal over elements i

00:34:25,440 --> 00:34:29,520
will encounter the same element twice

00:34:27,440 --> 00:34:32,079
so if i already own the element well

00:34:29,520 --> 00:34:34,800
then that's success and i go back

00:34:32,079 --> 00:34:36,560
otherwise if i'm not able to acquire it

00:34:34,800 --> 00:34:38,399
and i don't already own it

00:34:36,560 --> 00:34:39,599
i wait for the current owner to release

00:34:38,399 --> 00:34:41,839
the release the

00:34:39,599 --> 00:34:44,639
release the element and then i go back

00:34:41,839 --> 00:34:44,639
and try again

00:34:46,159 --> 00:34:50,000
so what does this look like in real life

00:34:49,119 --> 00:34:51,679
all right well here's

00:34:50,000 --> 00:34:54,320
some real c plus plus code and this is

00:34:51,679 --> 00:34:55,679
in the demo code

00:34:54,320 --> 00:34:57,359
this code that i'm going to show you

00:34:55,679 --> 00:34:59,359
corresponds to this part part

00:34:57,359 --> 00:35:01,440
if i acquire an item i add it to the

00:34:59,359 --> 00:35:03,359
list of items that i own

00:35:01,440 --> 00:35:05,040
so i've got a pointer to a transaction

00:35:03,359 --> 00:35:07,040
manager this is the

00:35:05,040 --> 00:35:09,359
i will use this pointer to represent the

00:35:07,040 --> 00:35:11,440
current transaction manager

00:35:09,359 --> 00:35:12,720
that owns the item in other words when i

00:35:11,440 --> 00:35:14,640
acquire an item

00:35:12,720 --> 00:35:16,000
if i'm looking at the item someone else

00:35:14,640 --> 00:35:19,359
might own the item

00:35:16,000 --> 00:35:21,440
so i'll do a an atomic compare and

00:35:19,359 --> 00:35:25,440
exchange

00:35:21,440 --> 00:35:27,760
with the null pointer and

00:35:25,440 --> 00:35:30,240
with a pointer to myself because i'm a

00:35:27,760 --> 00:35:32,079
transaction manager

00:35:30,240 --> 00:35:34,000
so for those of you unfamiliar with it

00:35:32,079 --> 00:35:35,440
atomic comparing exchange is a special

00:35:34,000 --> 00:35:36,960
atomic operation

00:35:35,440 --> 00:35:38,240
and here we're very fortunate that it's

00:35:36,960 --> 00:35:39,839
implemented for pointers and we're

00:35:38,240 --> 00:35:43,200
taking advantage of that

00:35:39,839 --> 00:35:45,200
so paraphrasing cppreference.com

00:35:43,200 --> 00:35:47,200
uh compare atomic compare and exchange

00:35:45,200 --> 00:35:50,079
compares the representation

00:35:47,200 --> 00:35:51,599
uh that's stored in the atomic pointer

00:35:50,079 --> 00:35:54,320
with the representation that's

00:35:51,599 --> 00:35:56,880
unexpected the first argument and if

00:35:54,320 --> 00:36:00,000
those are bitwise equal

00:35:56,880 --> 00:36:01,520
it will replace

00:36:00,000 --> 00:36:03,359
the representation that's part of the

00:36:01,520 --> 00:36:06,800
atomic pointer itself

00:36:03,359 --> 00:36:09,440
with the value of

00:36:06,800 --> 00:36:10,800
replaces the representation of this the

00:36:09,440 --> 00:36:11,359
representation that's in the atomic

00:36:10,800 --> 00:36:14,480
pointer

00:36:11,359 --> 00:36:16,640
it replaces it with desired otherwise it

00:36:14,480 --> 00:36:18,880
loads the actual value

00:36:16,640 --> 00:36:21,440
that's stored in the atomic pointer into

00:36:18,880 --> 00:36:23,119
expected and it returns false

00:36:21,440 --> 00:36:25,040
so if we go back and look at this what

00:36:23,119 --> 00:36:26,560
this really means is

00:36:25,040 --> 00:36:29,119
i'm going to call compare exchange

00:36:26,560 --> 00:36:31,280
strong what i'm hoping for

00:36:29,119 --> 00:36:32,240
what i expect is going to be the value

00:36:31,280 --> 00:36:35,040
in that pointer

00:36:32,240 --> 00:36:37,599
is the null pointer i expect that that

00:36:35,040 --> 00:36:40,960
element is going to be unowned

00:36:37,599 --> 00:36:42,960
and if this returns true that means that

00:36:40,960 --> 00:36:45,440
my expectation was met

00:36:42,960 --> 00:36:46,000
the current value or the value that was

00:36:45,440 --> 00:36:48,079
in that

00:36:46,000 --> 00:36:49,839
element was the null pointer and so

00:36:48,079 --> 00:36:52,400
compare exchange strong will now

00:36:49,839 --> 00:36:53,440
swap that value it will swap the value

00:36:52,400 --> 00:36:55,760
of a pointer to me

00:36:53,440 --> 00:36:58,320
into that and it does that compare and

00:36:55,760 --> 00:37:01,760
exchange in one atomic operation

00:36:58,320 --> 00:37:03,440
so if this succeeds then the

00:37:01,760 --> 00:37:05,200
pointer to the transaction manager

00:37:03,440 --> 00:37:08,480
that's stored in the lockable item

00:37:05,200 --> 00:37:10,800
points to me i own this item now

00:37:08,480 --> 00:37:12,320
now that i'm sure that i own the item

00:37:10,800 --> 00:37:14,400
i'm going to push that pointer back onto

00:37:12,320 --> 00:37:16,000
the back of my list

00:37:14,400 --> 00:37:17,280
i then go to this part i'm going to

00:37:16,000 --> 00:37:19,839
check the value i'm going to look at the

00:37:17,280 --> 00:37:22,160
value and make sure it's a good value

00:37:19,839 --> 00:37:24,880
so if my timestamp value is greater than

00:37:22,160 --> 00:37:26,720
the timestamp value in that element

00:37:24,880 --> 00:37:28,640
i'm going to set the timestamp value to

00:37:26,720 --> 00:37:31,520
be me

00:37:28,640 --> 00:37:33,200
if not i'm going to return false if i

00:37:31,520 --> 00:37:35,040
return true that means i acquired the

00:37:33,200 --> 00:37:36,640
item if i return false that means i did

00:37:35,040 --> 00:37:39,599
not

00:37:36,640 --> 00:37:42,079
okay well suppose that i did not acquire

00:37:39,599 --> 00:37:45,119
the item suppose that the null pointer

00:37:42,079 --> 00:37:46,560
was not the expected value right the

00:37:45,119 --> 00:37:48,400
pointer that was stored in the lockable

00:37:46,560 --> 00:37:50,560
item was pointing to something else

00:37:48,400 --> 00:37:53,040
i don't know what that something else is

00:37:50,560 --> 00:37:56,079
so i need to determine whether i own it

00:37:53,040 --> 00:37:58,800
already or somebody else owns it

00:37:56,079 --> 00:38:00,000
so i drop down here if that pointer if i

00:37:58,800 --> 00:38:02,480
already own the object

00:38:00,000 --> 00:38:04,160
that's great i return true because i've

00:38:02,480 --> 00:38:06,720
acquired it already

00:38:04,160 --> 00:38:08,560
otherwise this is where things get

00:38:06,720 --> 00:38:11,200
subtle

00:38:08,560 --> 00:38:11,680
so what i'm going to do i know at this

00:38:11,200 --> 00:38:14,960
point

00:38:11,680 --> 00:38:17,119
that the pointer in the lockable item

00:38:14,960 --> 00:38:18,000
is actually pointing to a transaction

00:38:17,119 --> 00:38:19,599
manager but it's not

00:38:18,000 --> 00:38:21,760
pointing to me it's not pointing the

00:38:19,599 --> 00:38:24,640
transaction manager in my thread

00:38:21,760 --> 00:38:26,800
so i'm going to try and lock the

00:38:24,640 --> 00:38:28,160
internal state of that other transaction

00:38:26,800 --> 00:38:31,200
manager i'm going to

00:38:28,160 --> 00:38:34,000
try to acquire his mutex when i do

00:38:31,200 --> 00:38:37,200
acquire his mutex

00:38:34,000 --> 00:38:40,320
i'm going to look and see

00:38:37,200 --> 00:38:44,640
um i'm going to look and see

00:38:40,320 --> 00:38:47,839
in the item if it's still owned by that

00:38:44,640 --> 00:38:49,520
by that uh by that transaction so i know

00:38:47,839 --> 00:38:50,320
what the item is it was an argument to

00:38:49,520 --> 00:38:52,000
the function

00:38:50,320 --> 00:38:53,599
i'm going to lock that other transaction

00:38:52,000 --> 00:38:56,160
manager i'm going to

00:38:53,599 --> 00:38:57,280
see if that transaction manager still

00:38:56,160 --> 00:39:00,320
owns

00:38:57,280 --> 00:39:01,200
uh that item it could be between the

00:39:00,320 --> 00:39:03,920
time

00:39:01,200 --> 00:39:04,720
i did this comparison and the time i got

00:39:03,920 --> 00:39:07,200
here

00:39:04,720 --> 00:39:09,040
that the other transaction released that

00:39:07,200 --> 00:39:12,079
lockable item so i'm going to check

00:39:09,040 --> 00:39:14,800
to make sure that he still owns it

00:39:12,079 --> 00:39:16,400
if he does still own it i'm going to

00:39:14,800 --> 00:39:18,480
look at the timestamp value and if he

00:39:16,400 --> 00:39:20,400
was a younger transaction than me

00:39:18,480 --> 00:39:22,400
then i'm going to return false because a

00:39:20,400 --> 00:39:26,720
younger transaction is updated

00:39:22,400 --> 00:39:30,000
and i have to i have to try again but

00:39:26,720 --> 00:39:32,640
if it is not a younger transaction then

00:39:30,000 --> 00:39:33,920
i'm going to wait for him to complete

00:39:32,640 --> 00:39:36,000
i'm going to wait for him to complete

00:39:33,920 --> 00:39:37,359
the transaction

00:39:36,000 --> 00:39:39,119
and i'm going to wait on the condition

00:39:37,359 --> 00:39:42,720
variable that i know he has

00:39:39,119 --> 00:39:46,480
because i have one too so there

00:39:42,720 --> 00:39:50,160
i i check the ownership i look to see if

00:39:46,480 --> 00:39:52,320
the timestamp value is is is valid for

00:39:50,160 --> 00:39:55,760
my purposes if not i return

00:39:52,320 --> 00:39:57,920
otherwise i wait now remember

00:39:55,760 --> 00:40:00,160
this is this is what commit looks like

00:39:57,920 --> 00:40:02,320
and let's pretend that this commit

00:40:00,160 --> 00:40:04,640
is being called by a transaction manager

00:40:02,320 --> 00:40:07,680
in a different thread than me

00:40:04,640 --> 00:40:09,280
well he is busy committing and when he

00:40:07,680 --> 00:40:10,000
commits he's going to release all of

00:40:09,280 --> 00:40:11,520
those guys

00:40:10,000 --> 00:40:13,040
all of those items by setting their

00:40:11,520 --> 00:40:14,800
ownership pointers to null

00:40:13,040 --> 00:40:16,160
and then he's going to broadcast on that

00:40:14,800 --> 00:40:18,079
condition variable

00:40:16,160 --> 00:40:19,440
when he broadcasts on that condition

00:40:18,079 --> 00:40:22,240
variable

00:40:19,440 --> 00:40:22,960
i'm waiting on him i will go back up to

00:40:22,240 --> 00:40:25,680
the top

00:40:22,960 --> 00:40:27,440
now while i'm in this section i'm still

00:40:25,680 --> 00:40:28,560
while i'm in this loop i'm still in the

00:40:27,440 --> 00:40:30,560
critical section

00:40:28,560 --> 00:40:32,160
from this mutex so there's no

00:40:30,560 --> 00:40:35,599
possibility of a race condition

00:40:32,160 --> 00:40:35,599
looking at his internal state

00:40:36,839 --> 00:40:44,240
so so i run this

00:40:40,640 --> 00:40:46,800
and if i don't succeed then i go back up

00:40:44,240 --> 00:40:46,800
to the top

00:40:47,280 --> 00:40:51,680
or succeed uh i go back up to the top

00:40:50,000 --> 00:40:52,880
and retry that's the point i was trying

00:40:51,680 --> 00:40:56,000
to make

00:40:52,880 --> 00:40:58,319
so let's look at how we test this

00:40:56,000 --> 00:40:59,839
and i'm going to show you some some test

00:40:58,319 --> 00:41:00,240
code i know but nobody likes to look at

00:40:59,839 --> 00:41:02,960
this

00:41:00,240 --> 00:41:04,640
and some some performance graphs the

00:41:02,960 --> 00:41:06,640
interesting part of the test graph

00:41:04,640 --> 00:41:08,880
test code i'm going to show you show

00:41:06,640 --> 00:41:11,280
some some business logic some business

00:41:08,880 --> 00:41:14,720
logic that i had to write

00:41:11,280 --> 00:41:15,520
to support the idea of figuring out the

00:41:14,720 --> 00:41:18,480
update group

00:41:15,520 --> 00:41:21,680
updating the update group and then

00:41:18,480 --> 00:41:26,480
committing the transaction

00:41:21,680 --> 00:41:28,400
so let's outline our test strategy

00:41:26,480 --> 00:41:29,920
so we need to create some functions that

00:41:28,400 --> 00:41:32,640
are going to update a collection of

00:41:29,920 --> 00:41:32,640
shared items

00:41:32,839 --> 00:41:37,760
right want to make sure

00:41:35,200 --> 00:41:39,839
that having done this that we write our

00:41:37,760 --> 00:41:42,880
data in such a way that

00:41:39,839 --> 00:41:46,079
race conditions are possible if

00:41:42,880 --> 00:41:46,560
our algorithm is invalid so we want to

00:41:46,079 --> 00:41:50,240
make it

00:41:46,560 --> 00:41:50,240
easy to detect race conditions

00:41:51,200 --> 00:41:54,880
when we're doing measurements we want to

00:41:52,880 --> 00:41:57,200
measure single threaded updates

00:41:54,880 --> 00:41:58,960
in other words if only a single thread

00:41:57,200 --> 00:41:59,520
is going out and doing the same business

00:41:58,960 --> 00:42:02,880
logic

00:41:59,520 --> 00:42:04,640
how fast does that run and then we want

00:42:02,880 --> 00:42:07,839
to measure

00:42:04,640 --> 00:42:08,800
the multi-threaded updates in a

00:42:07,839 --> 00:42:11,920
situation

00:42:08,800 --> 00:42:13,440
where we're doing the same logic as the

00:42:11,920 --> 00:42:15,200
single threaded updates

00:42:13,440 --> 00:42:17,040
but we're doing it in multiple threads

00:42:15,200 --> 00:42:18,480
we're not guarding we're not guarding

00:42:17,040 --> 00:42:21,599
access to the elements

00:42:18,480 --> 00:42:24,240
and we're trying to cause data races

00:42:21,599 --> 00:42:25,200
and i've designed the test element in

00:42:24,240 --> 00:42:28,839
such a way that

00:42:25,200 --> 00:42:31,839
causing a data race uh won't cause a

00:42:28,839 --> 00:42:31,839
crash

00:42:32,079 --> 00:42:36,000
for comparison purposes we also want to

00:42:34,079 --> 00:42:38,160
measure multi-threaded updates that are

00:42:36,000 --> 00:42:39,839
guarded by a single critical section

00:42:38,160 --> 00:42:41,359
and in this case because we're not

00:42:39,839 --> 00:42:42,960
distinguishing between readers and

00:42:41,359 --> 00:42:43,680
writers we don't really need a shared

00:42:42,960 --> 00:42:45,680
mutex

00:42:43,680 --> 00:42:49,520
we can just use a regular mutex because

00:42:45,680 --> 00:42:49,520
every operation is a write operation

00:42:49,760 --> 00:42:54,079
and then finally we want to test the

00:42:51,200 --> 00:42:55,599
algorithm's performance by measuring

00:42:54,079 --> 00:42:59,280
transactional updates in a

00:42:55,599 --> 00:43:02,800
multi-threaded environment

00:42:59,280 --> 00:43:05,920
so the system that i ran this on uh

00:43:02,800 --> 00:43:08,400
was an eight core intel cpu

00:43:05,920 --> 00:43:10,079
so eight cores hyper threading enabled

00:43:08,400 --> 00:43:14,240
16 hyper cores

00:43:10,079 --> 00:43:16,079
running ubuntu 1804 and i used gcc 10.2

00:43:14,240 --> 00:43:18,880
the most recent release

00:43:16,079 --> 00:43:20,560
to build the demo program and run some

00:43:18,880 --> 00:43:21,520
test driver scripts that collected the

00:43:20,560 --> 00:43:23,119
results

00:43:21,520 --> 00:43:26,560
that led to the graphs you'll see in a

00:43:23,119 --> 00:43:28,319
few minutes

00:43:26,560 --> 00:43:29,599
all right so i'll walk through some of

00:43:28,319 --> 00:43:33,200
this code quickly

00:43:29,599 --> 00:43:36,720
uh here are the prerequisites

00:43:33,200 --> 00:43:37,680
and of importance here i'm going to

00:43:36,720 --> 00:43:41,520
create

00:43:37,680 --> 00:43:44,079
uh a list of items

00:43:41,520 --> 00:43:45,200
and i'm going to use random number

00:43:44,079 --> 00:43:46,480
generation

00:43:45,200 --> 00:43:48,720
i'm going to use random number

00:43:46,480 --> 00:43:51,040
generation to generate data

00:43:48,720 --> 00:43:52,480
and i'm going to use hashing to value to

00:43:51,040 --> 00:43:54,960
evaluate

00:43:52,480 --> 00:43:56,319
to verify the validity of the data that

00:43:54,960 --> 00:43:58,240
i've created

00:43:56,319 --> 00:44:01,040
so that i can detect a race condition

00:43:58,240 --> 00:44:01,040
when it occurs

00:44:02,480 --> 00:44:05,680
so let's look at our test item as i

00:44:04,880 --> 00:44:07,440
mentioned before

00:44:05,680 --> 00:44:09,040
lockable item was intended to be used as

00:44:07,440 --> 00:44:11,200
a base class and so that's what i'm

00:44:09,040 --> 00:44:14,400
doing here

00:44:11,200 --> 00:44:16,720
i'm going to use a buffer size of 32 and

00:44:14,400 --> 00:44:18,400
in my item i'm going to contain a buffer

00:44:16,720 --> 00:44:19,760
of 32 characters

00:44:18,400 --> 00:44:21,599
this is the thing that's going to get

00:44:19,760 --> 00:44:24,400
updated

00:44:21,599 --> 00:44:25,920
i'm going to have two member functions

00:44:24,400 --> 00:44:29,359
single threaded update

00:44:25,920 --> 00:44:29,359
and transactional update

00:44:30,800 --> 00:44:37,359
so single threaded update is a little

00:44:32,640 --> 00:44:39,200
bit simpler let's look at it first

00:44:37,359 --> 00:44:40,880
when i do a single threaded update i'm

00:44:39,200 --> 00:44:43,680
going to create a local

00:44:40,880 --> 00:44:45,200
buffer of characters and then for ease

00:44:43,680 --> 00:44:46,480
of use and because i was playing around

00:44:45,200 --> 00:44:48,720
with stream view

00:44:46,480 --> 00:44:49,599
i'm going to create a string view that

00:44:48,720 --> 00:44:51,359
that looks

00:44:49,599 --> 00:44:53,440
that refers to my local buffer of

00:44:51,359 --> 00:44:54,960
characters and i'm going to create a

00:44:53,440 --> 00:44:56,640
string view that looks

00:44:54,960 --> 00:44:58,240
refers to the to the character of

00:44:56,640 --> 00:45:01,680
buffers that are

00:44:58,240 --> 00:45:03,280
member data of this item so i have

00:45:01,680 --> 00:45:05,359
member data which is possibly shared

00:45:03,280 --> 00:45:08,480
amongst threads and i have local data

00:45:05,359 --> 00:45:11,440
which is only visible in this thread

00:45:08,480 --> 00:45:12,079
and so what i'm going to do is i'm going

00:45:11,440 --> 00:45:15,680
to

00:45:12,079 --> 00:45:16,800
update the local data and the data

00:45:15,680 --> 00:45:20,000
that's member data

00:45:16,800 --> 00:45:22,800
at the same time in this simple loop now

00:45:20,000 --> 00:45:25,440
in the single threaded case

00:45:22,800 --> 00:45:27,760
this will this will work correctly and

00:45:25,440 --> 00:45:29,760
when i hash the data in the shared view

00:45:27,760 --> 00:45:31,680
and i hash the data in the local view i

00:45:29,760 --> 00:45:34,079
will get the same answer

00:45:31,680 --> 00:45:36,160
but when i don't get the answer i the

00:45:34,079 --> 00:45:38,720
same answer i found a raise

00:45:36,160 --> 00:45:40,480
and the reason i found the race is while

00:45:38,720 --> 00:45:42,720
i was doing this

00:45:40,480 --> 00:45:43,920
and before or even while i'm computing

00:45:42,720 --> 00:45:46,160
the hash value

00:45:43,920 --> 00:45:47,760
of the shared view the shared view has

00:45:46,160 --> 00:45:48,800
changed and its hash value will be

00:45:47,760 --> 00:45:51,280
different

00:45:48,800 --> 00:45:53,119
so this is how i detect race conditions

00:45:51,280 --> 00:45:56,000
by creating a local copy

00:45:53,119 --> 00:46:00,240
hashing it and looking at the hash value

00:45:56,000 --> 00:46:02,000
for the shared copy

00:46:00,240 --> 00:46:03,839
tx update which is used in the

00:46:02,000 --> 00:46:09,280
transactional case

00:46:03,839 --> 00:46:13,280
is very similar it's almost identical

00:46:09,280 --> 00:46:15,920
except it is uh it is passing a value

00:46:13,280 --> 00:46:17,760
a reference to the transaction but the

00:46:15,920 --> 00:46:18,960
transaction manager sorry that's a typo

00:46:17,760 --> 00:46:20,560
it's passing in a reference to the

00:46:18,960 --> 00:46:23,040
transaction manager

00:46:20,560 --> 00:46:24,240
and it's passed that in so that when it

00:46:23,040 --> 00:46:27,359
does logging

00:46:24,240 --> 00:46:29,359
it can it can report the id

00:46:27,359 --> 00:46:30,800
of the transaction manager where the

00:46:29,359 --> 00:46:33,040
race was found

00:46:30,800 --> 00:46:35,040
and because that id is a const member

00:46:33,040 --> 00:46:36,560
function of the transaction manager

00:46:35,040 --> 00:46:38,160
was which is only created when the

00:46:36,560 --> 00:46:40,400
transaction manager is

00:46:38,160 --> 00:46:42,319
instantiated there's no possibility of a

00:46:40,400 --> 00:46:44,640
race condition by reporting the

00:46:42,319 --> 00:46:47,359
transaction manager's id

00:46:44,640 --> 00:46:48,720
so once again local copy of data string

00:46:47,359 --> 00:46:51,280
view that points to that a string view

00:46:48,720 --> 00:46:54,079
that points to the shared member data

00:46:51,280 --> 00:46:55,359
update both at the same time compute the

00:46:54,079 --> 00:46:56,880
hashes of each

00:46:55,359 --> 00:46:58,720
and if the hashes are the same there's

00:46:56,880 --> 00:47:01,359
no problem the hashes differ

00:46:58,720 --> 00:47:01,359
there's a race

00:47:02,560 --> 00:47:07,839
okay so let's look at the actual test

00:47:05,280 --> 00:47:12,000
that's being performed here

00:47:07,839 --> 00:47:14,319
this test function tx access test

00:47:12,000 --> 00:47:16,079
this is a thread function this is the

00:47:14,319 --> 00:47:17,839
function that's running in a thread

00:47:16,079 --> 00:47:19,119
that i mentioned before previously in

00:47:17,839 --> 00:47:21,359
the algorithm

00:47:19,119 --> 00:47:22,880
so here i'm setting up some data that

00:47:21,359 --> 00:47:24,079
i'm going to use to implement my

00:47:22,880 --> 00:47:26,160
business logic

00:47:24,079 --> 00:47:27,520
including some random number generation

00:47:26,160 --> 00:47:29,200
here

00:47:27,520 --> 00:47:31,359
i'm going to have a stopwatch to do

00:47:29,200 --> 00:47:34,880
timing i'm going to keep some data which

00:47:31,359 --> 00:47:34,880
represents my update group

00:47:36,839 --> 00:47:41,920
importantly uh

00:47:38,800 --> 00:47:45,440
this uh this index list

00:47:41,920 --> 00:47:48,000
uh this is a list of uh of this is how

00:47:45,440 --> 00:47:50,240
it will form my update group

00:47:48,000 --> 00:47:51,200
and here i'm creating a transaction

00:47:50,240 --> 00:47:55,119
manager

00:47:51,200 --> 00:47:57,520
so here i'm giving him uh i'm

00:47:55,119 --> 00:47:58,800
forgotten what the one refers to that

00:47:57,520 --> 00:48:00,240
might

00:47:58,800 --> 00:48:02,240
yeah forgotten what that means anyways

00:48:00,240 --> 00:48:04,240
i'm creating the transaction manager

00:48:02,240 --> 00:48:06,160
and i'm keeping a boolean value that

00:48:04,240 --> 00:48:08,720
will effectively keep the summation

00:48:06,160 --> 00:48:09,760
of oh sorry the one is the log level the

00:48:08,720 --> 00:48:12,400
logging level

00:48:09,760 --> 00:48:13,920
so i'm keeping a boolean value to keep

00:48:12,400 --> 00:48:15,359
track of the totality of whether or not

00:48:13,920 --> 00:48:16,960
i've acquired all the elements in the

00:48:15,359 --> 00:48:20,160
update group

00:48:16,960 --> 00:48:22,240
so well let me go back

00:48:20,160 --> 00:48:24,400
and i forgot to highlight this i'm going

00:48:22,240 --> 00:48:26,160
to start the transaction

00:48:24,400 --> 00:48:28,000
so i move past this i start the

00:48:26,160 --> 00:48:30,880
transaction

00:48:28,000 --> 00:48:31,440
okay i'm going to clear out the data

00:48:30,880 --> 00:48:34,559
that i use

00:48:31,440 --> 00:48:35,760
to compute the update group

00:48:34,559 --> 00:48:37,440
and then i'm going to compute the

00:48:35,760 --> 00:48:38,319
membership of the elements that prize

00:48:37,440 --> 00:48:40,160
the update group

00:48:38,319 --> 00:48:42,480
and i do this by generating random

00:48:40,160 --> 00:48:45,200
numbers that correspond to elements that

00:48:42,480 --> 00:48:47,760
are in the array of data

00:48:45,200 --> 00:48:49,440
so the update group is basically always

00:48:47,760 --> 00:48:53,040
the same size in this test

00:48:49,440 --> 00:48:57,040
but the membership of that update group

00:48:53,040 --> 00:48:57,040
varies from transaction to transaction

00:48:57,680 --> 00:49:04,000
so begin the transaction

00:49:00,800 --> 00:49:05,760
i acquire the data or i set my acquired

00:49:04,000 --> 00:49:09,680
flag as being true

00:49:05,760 --> 00:49:12,160
and then what i do is i start trying to

00:49:09,680 --> 00:49:14,319
acquire the data that's in my list of

00:49:12,160 --> 00:49:16,880
of elements that are in the update group

00:49:14,319 --> 00:49:19,359
so i formed a list of elements that

00:49:16,880 --> 00:49:21,280
comprise the update group now i'm going

00:49:19,359 --> 00:49:23,280
to iterate over that list and i'm going

00:49:21,280 --> 00:49:25,760
to try and acquire each one

00:49:23,280 --> 00:49:27,520
if i successfully acquire it i will loop

00:49:25,760 --> 00:49:31,200
through and i'll keep going

00:49:27,520 --> 00:49:33,920
until either i fail to acquire or

00:49:31,200 --> 00:49:35,440
i've acquired every member whatever the

00:49:33,920 --> 00:49:36,240
number of references the number of

00:49:35,440 --> 00:49:39,040
members that i've

00:49:36,240 --> 00:49:39,839
i've decided i want to acquire i step

00:49:39,040 --> 00:49:43,119
out of the loop

00:49:39,839 --> 00:49:43,119
so if i've acquired it

00:49:43,520 --> 00:49:47,520
then for each item that's part of the

00:49:46,480 --> 00:49:50,160
update group

00:49:47,520 --> 00:49:51,520
i'm going to call tx update the the

00:49:50,160 --> 00:49:53,359
transactional update

00:49:51,520 --> 00:49:54,880
function that i just showed you above

00:49:53,359 --> 00:49:56,400
that's going to write data into the

00:49:54,880 --> 00:49:59,200
local buffer and the shared buffer and

00:49:56,400 --> 00:50:01,359
compare the hashes and look for a race

00:49:59,200 --> 00:50:04,160
i'm going to do that until i'm done then

00:50:01,359 --> 00:50:06,640
i will commit the transaction

00:50:04,160 --> 00:50:08,319
if i can't do that if i am unable to

00:50:06,640 --> 00:50:08,960
acquire all the elements in the update

00:50:08,319 --> 00:50:13,359
group

00:50:08,960 --> 00:50:15,119
i will roll back

00:50:13,359 --> 00:50:17,119
at the end of this at the end of the

00:50:15,119 --> 00:50:18,960
lifetime of this function

00:50:17,119 --> 00:50:20,240
i'll then print out the statistics of

00:50:18,960 --> 00:50:23,119
how much time i spent

00:50:20,240 --> 00:50:23,119
running in the function

00:50:23,839 --> 00:50:28,960
so let's look at some test results

00:50:30,079 --> 00:50:35,520
so here's a test that i ran

00:50:33,119 --> 00:50:38,240
where the number of elements in the

00:50:35,520 --> 00:50:40,400
lockable items collection was 10 million

00:50:38,240 --> 00:50:41,839
so there were 10 million items that i

00:50:40,400 --> 00:50:45,040
could that were part of the

00:50:41,839 --> 00:50:48,319
uh part of the shared collection

00:50:45,040 --> 00:50:50,960
i ran 100 000 transactions each

00:50:48,319 --> 00:50:53,200
transaction

00:50:50,960 --> 00:50:55,119
each transaction had a an update group

00:50:53,200 --> 00:50:59,359
that had 20 members in it

00:50:55,119 --> 00:51:01,839
and i ran it from 1 to 16 threads

00:50:59,359 --> 00:51:04,640
now you may look at this graph and you

00:51:01,839 --> 00:51:08,000
may say well what does this 0 mean

00:51:04,640 --> 00:51:10,240
so the 0 is the special case

00:51:08,000 --> 00:51:12,400
zero is the baseline case in all of

00:51:10,240 --> 00:51:12,960
these graphs zero represents the case

00:51:12,400 --> 00:51:15,200
where

00:51:12,960 --> 00:51:16,559
i was only doing a single threaded test

00:51:15,200 --> 00:51:18,079
in a single thread

00:51:16,559 --> 00:51:20,319
there was no multi-threading no

00:51:18,079 --> 00:51:23,760
transaction involved it should be the

00:51:20,319 --> 00:51:24,319
fastest possible result and basically it

00:51:23,760 --> 00:51:27,760
is across

00:51:24,319 --> 00:51:30,880
all of these then i ran the test for one

00:51:27,760 --> 00:51:32,480
through 16 threads using using the

00:51:30,880 --> 00:51:34,880
transactional algorithm

00:51:32,480 --> 00:51:36,319
so you can see that there is a little

00:51:34,880 --> 00:51:38,559
bit of overhead here

00:51:36,319 --> 00:51:39,440
doing the transactional algorithm in one

00:51:38,559 --> 00:51:41,440
thread

00:51:39,440 --> 00:51:44,400
versus doing the same operations in a

00:51:41,440 --> 00:51:47,359
thread that has no transactions

00:51:44,400 --> 00:51:47,760
so i did it for one to 16 threads and

00:51:47,359 --> 00:51:51,040
here

00:51:47,760 --> 00:51:53,680
are the number of uh sorry this is

00:51:51,040 --> 00:51:56,079
uh this is the test that i ran where i

00:51:53,680 --> 00:51:58,400
was deliberately trying to

00:51:56,079 --> 00:51:59,520
find race conditions so it's labeled

00:51:58,400 --> 00:52:03,040
races

00:51:59,520 --> 00:52:04,720
so i misspoke this particular graph

00:52:03,040 --> 00:52:06,960
shows the case of attempting to

00:52:04,720 --> 00:52:08,559
find race conditions so in this case

00:52:06,960 --> 00:52:10,319
with 10 million elements

00:52:08,559 --> 00:52:12,319
for each number of threads the number of

00:52:10,319 --> 00:52:15,359
races that i would find

00:52:12,319 --> 00:52:16,880
where a transaction two transactions

00:52:15,359 --> 00:52:18,000
were trying to update the same element

00:52:16,880 --> 00:52:23,680
the same time was very

00:52:18,000 --> 00:52:25,520
small the next case was

00:52:23,680 --> 00:52:28,160
running for a single mutex in other

00:52:25,520 --> 00:52:30,480
words if i was only using a single mutex

00:52:28,160 --> 00:52:32,000
to guard all accesses from 1 to 16

00:52:30,480 --> 00:52:33,599
threads what does the performance look

00:52:32,000 --> 00:52:35,040
like how long does it take

00:52:33,599 --> 00:52:36,640
so you can see again from the single

00:52:35,040 --> 00:52:38,160
threaded case to the

00:52:36,640 --> 00:52:39,920
to the multi-threaded case where

00:52:38,160 --> 00:52:41,599
everything is locked with only one

00:52:39,920 --> 00:52:42,800
thread there's a little bit of overhead

00:52:41,599 --> 00:52:45,280
difference here

00:52:42,800 --> 00:52:46,000
and then you get a nice linear

00:52:45,280 --> 00:52:47,920
progression here

00:52:46,000 --> 00:52:49,200
because every thread is doing 100 000

00:52:47,920 --> 00:52:51,680
transactions

00:52:49,200 --> 00:52:53,359
because there's sequential access to the

00:52:51,680 --> 00:52:55,520
to the collection you would expect this

00:52:53,359 --> 00:52:58,000
to be linear or close to it

00:52:55,520 --> 00:52:59,520
we get up around 13 threads on this

00:52:58,000 --> 00:53:02,319
particular computer

00:52:59,520 --> 00:53:02,880
i saw this very strange behavior where

00:53:02,319 --> 00:53:05,119
00:53:02,880 --> 00:53:06,400
15 and 16 threads would actually update

00:53:05,119 --> 00:53:08,559
more quickly

00:53:06,400 --> 00:53:10,480
when there was a single mutex and this

00:53:08,559 --> 00:53:12,240
behavior was somewhat consistent across

00:53:10,480 --> 00:53:14,640
all of the runs on this particular

00:53:12,240 --> 00:53:16,240
computer i can't explain it i don't know

00:53:14,640 --> 00:53:18,640
what's causing it

00:53:16,240 --> 00:53:19,680
but it might perhaps my theory is it has

00:53:18,640 --> 00:53:22,880
something to do with

00:53:19,680 --> 00:53:24,160
hyper threading because this machine had

00:53:22,880 --> 00:53:27,440
eight physical cores

00:53:24,160 --> 00:53:28,000
but 16 hyper cores so perhaps hyper

00:53:27,440 --> 00:53:31,839
cores had

00:53:28,000 --> 00:53:31,839
something to do with this phenomenon

00:53:31,920 --> 00:53:36,400
so finally for this case of 10 million

00:53:34,000 --> 00:53:37,520
items and 100 000 transactions and 16

00:53:36,400 --> 00:53:40,160
threads

00:53:37,520 --> 00:53:42,160
here is uh the timing so you can see

00:53:40,160 --> 00:53:45,200
there's a little more overhead for

00:53:42,160 --> 00:53:48,319
a transactional case with one thread

00:53:45,200 --> 00:53:50,880
versus no transactional processing

00:53:48,319 --> 00:53:52,800
but the amount of time that was taken to

00:53:50,880 --> 00:53:54,960
to do a hundred thousand transactions in

00:53:52,800 --> 00:53:58,000
each thread

00:53:54,960 --> 00:54:01,280
has only multiplied by a factor of about

00:53:58,000 --> 00:54:03,040
uh about two so i've increased the

00:54:01,280 --> 00:54:06,079
amount of work that needs to be done

00:54:03,040 --> 00:54:06,880
by 16 times when i go from one to 16

00:54:06,079 --> 00:54:08,720
threads

00:54:06,880 --> 00:54:11,200
but the amount of time the amount of

00:54:08,720 --> 00:54:14,800
wall clock time taken to do that work

00:54:11,200 --> 00:54:17,040
has only increased by a factor of about

00:54:14,800 --> 00:54:17,040
two

00:54:18,319 --> 00:54:21,839
now the subsequent graphs i'll show you

00:54:20,400 --> 00:54:23,839
i'm decreasing the

00:54:21,839 --> 00:54:25,119
size of the collection in order to form

00:54:23,839 --> 00:54:27,680
to try and force

00:54:25,119 --> 00:54:29,599
more collisions in the data so here

00:54:27,680 --> 00:54:31,280
again i'm looking for more races

00:54:29,599 --> 00:54:33,359
except i've got a smaller collection of

00:54:31,280 --> 00:54:35,599
items a million items

00:54:33,359 --> 00:54:38,079
and here you can see with fewer items

00:54:35,599 --> 00:54:40,480
the probability of causing a race

00:54:38,079 --> 00:54:42,240
has improved has increased and so you

00:54:40,480 --> 00:54:43,839
see more races

00:54:42,240 --> 00:54:45,359
for each number of threads and you kind

00:54:43,839 --> 00:54:47,119
of expect this behavior

00:54:45,359 --> 00:54:48,880
as there's less data in the collection

00:54:47,119 --> 00:54:49,520
the probability of a race condition

00:54:48,880 --> 00:54:52,000
occurring

00:54:49,520 --> 00:54:53,359
improves or increases and so that's what

00:54:52,000 --> 00:54:57,040
this graph

00:54:53,359 --> 00:54:58,720
shows us again in the case of locking

00:54:57,040 --> 00:55:01,359
the data with a single thread

00:54:58,720 --> 00:55:03,520
the performance curve is very similar or

00:55:01,359 --> 00:55:05,520
blocking the data with a single mutex

00:55:03,520 --> 00:55:08,319
the performance curve is very similar

00:55:05,520 --> 00:55:12,319
when we get out around 13

00:55:08,319 --> 00:55:16,079
around 13 threads

00:55:12,319 --> 00:55:16,079
the behavior is strange

00:55:16,319 --> 00:55:20,559
and the transactional case so again in

00:55:18,880 --> 00:55:24,400
the transactional case

00:55:20,559 --> 00:55:25,280
i would expect that i would expect that

00:55:24,400 --> 00:55:27,200
the

00:55:25,280 --> 00:55:29,680
uh the number of rollbacks would also

00:55:27,200 --> 00:55:30,880
increase as the number of collisions

00:55:29,680 --> 00:55:32,079
or as the number of elements in the

00:55:30,880 --> 00:55:33,119
collection decrease because the

00:55:32,079 --> 00:55:35,760
probability

00:55:33,119 --> 00:55:37,760
of trying of two transactions using

00:55:35,760 --> 00:55:38,799
having a given element be the member of

00:55:37,760 --> 00:55:39,599
update group for two different

00:55:38,799 --> 00:55:41,440
transactions

00:55:39,599 --> 00:55:43,359
increases so we see the number of

00:55:41,440 --> 00:55:44,240
rollbacks in absolute terms is a little

00:55:43,359 --> 00:55:47,200
bit higher

00:55:44,240 --> 00:55:49,920
but the performance delta is still i'm

00:55:47,200 --> 00:55:51,839
doing 16 times as much work in 16

00:55:49,920 --> 00:55:54,160
threads but my wall clock time is only

00:55:51,839 --> 00:55:54,160
doubled

00:55:54,480 --> 00:55:58,480
and similarly for a hundred thousand

00:55:57,040 --> 00:55:59,680
items doing races

00:55:58,480 --> 00:56:01,760
for a hundred thousand items and a

00:55:59,680 --> 00:56:03,280
hundred thousand transactions the number

00:56:01,760 --> 00:56:04,960
of races goes up

00:56:03,280 --> 00:56:07,839
and the threat uh the curve starts to

00:56:04,960 --> 00:56:09,520
get a little smoother

00:56:07,839 --> 00:56:11,359
for the case of guarding with a single

00:56:09,520 --> 00:56:13,520
mutex for 100 000 items

00:56:11,359 --> 00:56:14,640
again the same strange behavior out

00:56:13,520 --> 00:56:18,799
around 13 14

00:56:14,640 --> 00:56:21,839
15 16 threads

00:56:18,799 --> 00:56:24,720
and for the transactional behavior with

00:56:21,839 --> 00:56:26,960
smaller number of items you can see that

00:56:24,720 --> 00:56:28,720
the curve for the number of rollbacks is

00:56:26,960 --> 00:56:29,920
now starting to climb

00:56:28,720 --> 00:56:31,680
and that means that there are more

00:56:29,920 --> 00:56:33,440
collisions more collisions means more

00:56:31,680 --> 00:56:35,359
rollbacks which means more work for the

00:56:33,440 --> 00:56:37,119
transactional algorithm to do

00:56:35,359 --> 00:56:38,640
so you can see that there's an

00:56:37,119 --> 00:56:40,079
inflection point here

00:56:38,640 --> 00:56:42,160
and the time that takes for the

00:56:40,079 --> 00:56:42,880
transaction actually starts to climb a

00:56:42,160 --> 00:56:44,799
little more

00:56:42,880 --> 00:56:46,240
the performance is still not bad it's

00:56:44,799 --> 00:56:48,400
still now maybe

00:56:46,240 --> 00:56:49,599
two and a quarter times as much to go

00:56:48,400 --> 00:56:51,839
from one thread

00:56:49,599 --> 00:56:52,799
up to 16 threads but the shape of the

00:56:51,839 --> 00:56:55,839
curve has changed

00:56:52,799 --> 00:56:57,520
starting to tail up at the end

00:56:55,839 --> 00:56:59,839
and finally the last test was ten

00:56:57,520 --> 00:57:01,839
thousand items really trying to look for

00:56:59,839 --> 00:57:02,880
races really pushing the limit here and

00:57:01,839 --> 00:57:04,640
you can see

00:57:02,880 --> 00:57:07,200
i was getting large number of races and

00:57:04,640 --> 00:57:10,240
this curve starts to smooth out

00:57:07,200 --> 00:57:11,920
single mutex for ten thousand items

00:57:10,240 --> 00:57:14,240
the behavior is a little better here at

00:57:11,920 --> 00:57:16,079
the end but i still don't understand it

00:57:14,240 --> 00:57:18,319
and finally the transactional case and

00:57:16,079 --> 00:57:20,079
in this case the number of rollbacks is

00:57:18,319 --> 00:57:21,599
very high the probability of collision

00:57:20,079 --> 00:57:23,599
and rollback is very high

00:57:21,599 --> 00:57:26,079
and so you can see the curve for the

00:57:23,599 --> 00:57:27,599
transaction algorithm performance is

00:57:26,079 --> 00:57:28,319
starting to take a little bit of a turn

00:57:27,599 --> 00:57:30,400
and now

00:57:28,319 --> 00:57:31,440
the performance is instead of a factor

00:57:30,400 --> 00:57:34,640
of two

00:57:31,440 --> 00:57:35,040
is a factor of three so 16 times as much

00:57:34,640 --> 00:57:37,280
work

00:57:35,040 --> 00:57:38,480
now takes three times as much wall clock

00:57:37,280 --> 00:57:40,559
time

00:57:38,480 --> 00:57:42,400
and finally just for giggles here's a

00:57:40,559 --> 00:57:43,599
graph that compares the performance for

00:57:42,400 --> 00:57:46,079
10 000 items

00:57:43,599 --> 00:57:47,440
this two graphs the previous two graphs

00:57:46,079 --> 00:57:49,119
on one graph

00:57:47,440 --> 00:57:50,880
here's what it looks like for a single

00:57:49,119 --> 00:57:52,000
critical section with a single mutex and

00:57:50,880 --> 00:57:55,119
here's what it looks like for the

00:57:52,000 --> 00:57:58,160
transactional algorithm

00:57:55,119 --> 00:57:59,760
so finally these

00:57:58,160 --> 00:58:01,680
simple tools that i've done they operate

00:57:59,760 --> 00:58:02,960
on containers and elements and

00:58:01,680 --> 00:58:04,799
they don't require changing the

00:58:02,960 --> 00:58:06,799
container itself i'm not changing the

00:58:04,799 --> 00:58:08,559
number of elements in the vector

00:58:06,799 --> 00:58:10,319
there's an assumption that the internal

00:58:08,559 --> 00:58:12,720
structure of the container is unchanged

00:58:10,319 --> 00:58:14,720
while transactions are in progress

00:58:12,720 --> 00:58:16,240
consider the case of trying to resize a

00:58:14,720 --> 00:58:17,920
vector while transactions are in

00:58:16,240 --> 00:58:19,280
progress i haven't figured out how to do

00:58:17,920 --> 00:58:20,799
that yet

00:58:19,280 --> 00:58:22,559
consider the case if you were trying to

00:58:20,799 --> 00:58:24,480
apply this to a map based on a hash

00:58:22,559 --> 00:58:26,720
table or a red black tree

00:58:24,480 --> 00:58:27,760
the relationship amongst nodes in those

00:58:26,720 --> 00:58:30,960
linked structures

00:58:27,760 --> 00:58:33,040
and how transactions would affect them

00:58:30,960 --> 00:58:35,040
could this be handled better by a per

00:58:33,040 --> 00:58:38,400
container shared mutex

00:58:35,040 --> 00:58:40,000
i don't know so far i've only done these

00:58:38,400 --> 00:58:42,160
experiments with standard vector and the

00:58:40,000 --> 00:58:44,000
number of elements is pre-allocated and

00:58:42,160 --> 00:58:46,000
the vector itself in a sense is static

00:58:44,000 --> 00:58:47,280
there's no resizes

00:58:46,000 --> 00:58:49,119
if you wanted to create a container

00:58:47,280 --> 00:58:50,000
that's resizable and i actually did this

00:58:49,119 --> 00:58:52,160
in the work i mentioned

00:58:50,000 --> 00:58:54,079
before i built a homegrown hash table

00:58:52,160 --> 00:58:56,079
using stood vector

00:58:54,079 --> 00:58:57,280
each element of the vector was itself a

00:58:56,079 --> 00:58:59,040
hash bucket

00:58:57,280 --> 00:59:00,640
and the hash buckets contained the

00:58:59,040 --> 00:59:02,480
actual lockable items

00:59:00,640 --> 00:59:04,079
and the hash buckets had member

00:59:02,480 --> 00:59:06,000
functions for adding and finding and

00:59:04,079 --> 00:59:08,079
erasing elements

00:59:06,000 --> 00:59:10,079
the hash buckets themselves were locked

00:59:08,079 --> 00:59:11,040
during transactions so that elements

00:59:10,079 --> 00:59:13,280
within them

00:59:11,040 --> 00:59:14,079
could be updated and with a good hash

00:59:13,280 --> 00:59:15,920
function

00:59:14,079 --> 00:59:17,920
and a good sized hash table the lookup

00:59:15,920 --> 00:59:19,760
was fast and there wasn't a whole lot of

00:59:17,920 --> 00:59:24,319
contention

00:59:19,760 --> 00:59:26,880
from competing transactions rolling back

00:59:24,319 --> 00:59:28,640
finally downsides of this technique some

00:59:26,880 --> 00:59:30,799
threads could starve some transactions

00:59:28,640 --> 00:59:32,559
could become stale

00:59:30,799 --> 00:59:34,240
other container types might be amenable

00:59:32,559 --> 00:59:35,839
but i've not had time to explore that

00:59:34,240 --> 00:59:37,599
yet and of course there's always lots of

00:59:35,839 --> 00:59:39,839
room for more work

00:59:37,599 --> 00:59:40,880
so i'm a little bit over time thank you

00:59:39,839 --> 00:59:42,640
for attending

00:59:40,880 --> 00:59:45,920
i'm going to put the slides up on my

00:59:42,640 --> 00:59:48,079
github site within the next couple hours

00:59:45,920 --> 00:59:49,200
and there's my blog so i'm going to move

00:59:48,079 --> 00:59:53,040
to remo and

00:59:49,200 --> 00:59:53,040
take a couple questions very quickly

00:59:54,640 --> 00:59:58,640
so the first question is what happens

00:59:57,040 --> 00:59:59,839
when you roll back do you start over

00:59:58,640 --> 01:00:02,559
with a new timestamp

00:59:59,839 --> 01:00:04,240
yes by rolling back you're saying i am

01:00:02,559 --> 01:00:05,119
ending this transaction i'm no longer

01:00:04,240 --> 01:00:06,720
interested in it

01:00:05,119 --> 01:00:08,880
i want to do the same amount of work but

01:00:06,720 --> 01:00:10,799
i'm going to create a new transaction

01:00:08,880 --> 01:00:13,040
the creation of a transaction is

01:00:10,799 --> 01:00:15,839
synonymous with getting a new timestamp

01:00:13,040 --> 01:00:15,839
value

01:00:18,400 --> 01:00:22,160
next question is it safe to wait on the

01:00:20,480 --> 01:00:23,040
condition variable of the other

01:00:22,160 --> 01:00:25,040
transaction

01:00:23,040 --> 01:00:26,960
while still owning the other elements

01:00:25,040 --> 01:00:28,960
and not releasing them

01:00:26,960 --> 01:00:30,319
yes because while you're waiting on the

01:00:28,960 --> 01:00:32,400
condition

01:00:30,319 --> 01:00:33,920
while you're waiting on the condition

01:00:32,400 --> 01:00:34,640
you wait on that condition because what

01:00:33,920 --> 01:00:36,559
you want to do

01:00:34,640 --> 01:00:38,079
is you want to update shared state in

01:00:36,559 --> 01:00:41,280
the other transactions

01:00:38,079 --> 01:00:42,079
member data and while you're waiting on

01:00:41,280 --> 01:00:45,119
the

01:00:42,079 --> 01:00:46,640
condition variable uh it's safe to do

01:00:45,119 --> 01:00:47,920
that you don't have access to it you

01:00:46,640 --> 01:00:50,400
can't do anything

01:00:47,920 --> 01:00:52,079
when the condition variable is released

01:00:50,400 --> 01:00:54,160
and remember the condition variable is

01:00:52,079 --> 01:00:55,599
associated with the mutex

01:00:54,160 --> 01:00:58,000
when you're released from the condition

01:00:55,599 --> 01:01:00,000
variable you are back in the mutex that

01:00:58,000 --> 01:01:01,359
you've already acquired and that mutex

01:01:00,000 --> 01:01:02,400
was a data member of the other

01:01:01,359 --> 01:01:04,160
transaction

01:01:02,400 --> 01:01:06,000
because you're back in the mutex you've

01:01:04,160 --> 01:01:08,559
already you already have the mutex

01:01:06,000 --> 01:01:09,200
it is safe for you to look at the shared

01:01:08,559 --> 01:01:12,880
data

01:01:09,200 --> 01:01:12,880
that's part of the other transaction

01:01:14,000 --> 01:01:18,160
next question what happens if two

01:01:15,520 --> 01:01:20,640
transactions have identical timestamp

01:01:18,160 --> 01:01:22,240
uh that can't happen because the

01:01:20,640 --> 01:01:24,720
creation of a timestamp is

01:01:22,240 --> 01:01:26,480
is implemented by incrementing an atomic

01:01:24,720 --> 01:01:28,480
integer

01:01:26,480 --> 01:01:30,079
and each transaction begins by

01:01:28,480 --> 01:01:32,160
incrementing it and getting the value

01:01:30,079 --> 01:01:34,240
it's impossible for two transactions to

01:01:32,160 --> 01:01:36,839
receive the same value from the

01:01:34,240 --> 01:01:39,599
operation of incrementing that atomic

01:01:36,839 --> 01:01:44,240
integer

01:01:39,599 --> 01:01:44,240
uh slide 89

01:01:44,480 --> 01:01:56,319
let me just get to the slides here

01:01:52,720 --> 01:01:58,079
okay the question is slide 89

01:01:56,319 --> 01:01:59,440
what happens if you flow past the while

01:01:58,079 --> 01:02:02,559
loop well

01:01:59,440 --> 01:02:04,839
if you flow past the while loop uh

01:02:02,559 --> 01:02:07,760
that means that the owner of the current

01:02:04,839 --> 01:02:09,839
element the element's current owner

01:02:07,760 --> 01:02:11,599
is not the other transaction the other

01:02:09,839 --> 01:02:14,000
transaction has released it

01:02:11,599 --> 01:02:15,280
so you exit the while loop when you act

01:02:14,000 --> 01:02:16,160
remember when you're at the top of the

01:02:15,280 --> 01:02:18,000
while loop

01:02:16,160 --> 01:02:20,079
you own the mutex you own the shared

01:02:18,000 --> 01:02:21,520
state in the other transaction

01:02:20,079 --> 01:02:22,880
because perhaps you've fallen out of

01:02:21,520 --> 01:02:24,400
this condition variable and when you

01:02:22,880 --> 01:02:26,000
follow the condition variable you have

01:02:24,400 --> 01:02:28,559
the mutex again

01:02:26,000 --> 01:02:29,920
if this guy is not the same as what the

01:02:28,559 --> 01:02:32,079
element thinks he has

01:02:29,920 --> 01:02:33,440
you drop out of the while loop and thus

01:02:32,079 --> 01:02:35,200
you release the lock

01:02:33,440 --> 01:02:37,039
when the lock is destroyed the mutex is

01:02:35,200 --> 01:02:37,680
released you go back up to the top of

01:02:37,039 --> 01:02:40,799
the loop

01:02:37,680 --> 01:02:42,319
and you start a new transaction and the

01:02:40,799 --> 01:02:44,240
last question very quickly do we need to

01:02:42,319 --> 01:02:45,680
account for tsv rolling over in our

01:02:44,240 --> 01:02:47,920
comparisons

01:02:45,680 --> 01:02:49,119
well i'm using an unsigned 64-bit

01:02:47,920 --> 01:02:51,280
integer so

01:02:49,119 --> 01:02:52,640
i expect the answer to that is probably

01:02:51,280 --> 01:02:55,920
no

01:02:52,640 --> 01:02:55,920
so anyway

01:02:58,000 --> 01:03:01,520
so thank you everybody for coming i

01:03:00,160 --> 01:03:03,359
appreciate your attention

01:03:01,520 --> 01:03:17,839
have a good conference and maybe i'll

01:03:03,359 --> 01:03:17,839
see you in the hallway thank you

01:03:24,880 --> 01:03:26,960

YouTube URL: https://www.youtube.com/watch?v=Pi243hGxDyA


