Title: Halide: A Language for Fast, Portable Computation on Images and Tensors - Alex Reinking - CppCon 20
Publication date: 2020-10-15
Playlist: CppCon 2020 Day 5
Description: 
	https://cppcon.org/
https://github.com/CppCon/CppCon2020/blob/main/Presentations/halide_a_language_for_fast_portable_computation_on_images_and_tensors/halide_a_language_for_fast_portable_computation_on_images_and_tensors__alex_reinking__cppcon_2020.pptx
---

Halide is an open-source, domain-specific language for optimizing image processing, machine learning, and general array processing. It is used by major companies like Google, Adobe, and Qualcomm to optimize performance-critical software. It processes every photo taken with a Pixel phone, composites layers in every Photoshop document, and handles video processing at scale at YouTube. 

We will explore how Halide achieves top-tier performance in a fraction of the development time by separating the algorithm (what to compute) from the schedule (how to optimize it). Halide's schedules determine trade-offs between parallelism, vectorization, cache locality, and memory management in a simple, modular way. Programmers can easily target different CPUs and GPUs by writing multiple schedules. Halide integrates tightly with C++ and provides both a JIT and a C++ compatible ahead-of-time compiler.

---
Alex is a Ph.D. student at UC Berkeley working with Jonathan Ragan-Kelley on designing domain-specific languages for high-performance computing. He has previously worked at Microsoft Research and Facebook AI+R. He holds an MS in Computer Science from UC Berkeley and a BS in Computer Science and Mathematics from Yale.

---
Streamed & Edited by Digital Medium Ltd - events.digital-medium.co.uk
events@digital-medium.co.uk
Captions: 
	00:00:08,559 --> 00:00:12,160
cool

00:00:09,040 --> 00:00:12,160
uh hi everyone

00:00:12,400 --> 00:00:14,639
um

00:00:15,599 --> 00:00:19,600
uh thank you all for coming i'm alex

00:00:17,680 --> 00:00:21,199
reinking and i'm very excited and

00:00:19,600 --> 00:00:23,039
honored to be presenting at my very

00:00:21,199 --> 00:00:25,039
first cpp con

00:00:23,039 --> 00:00:27,199
uh if we haven't already met in the

00:00:25,039 --> 00:00:30,080
hallway track you might be wondering

00:00:27,199 --> 00:00:31,279
who i am and what i do well i'm a

00:00:30,080 --> 00:00:34,880
programming languages

00:00:31,279 --> 00:00:36,960
and compilers researcher at uc berkeley

00:00:34,880 --> 00:00:38,160
and i'm still working on my phd but in a

00:00:36,960 --> 00:00:40,320
nutshell i

00:00:38,160 --> 00:00:41,440
design and analyze domain-specific

00:00:40,320 --> 00:00:45,840
languages or

00:00:41,440 --> 00:00:45,840
dsls with high performance in mind

00:00:45,920 --> 00:00:49,680
so if you aren't sure what i mean by

00:00:47,680 --> 00:00:51,600
that just know that dsls

00:00:49,680 --> 00:00:53,520
are programming languages that are

00:00:51,600 --> 00:00:55,360
specialized and restricted

00:00:53,520 --> 00:00:56,640
to best meet the needs of a particular

00:00:55,360 --> 00:00:59,199
domain

00:00:56,640 --> 00:01:01,199
the goal of a dsl is to exploit those

00:00:59,199 --> 00:01:02,079
restrictions to give programmers faster

00:01:01,199 --> 00:01:04,879
performance

00:01:02,079 --> 00:01:06,000
easier maintenance safety guarantees and

00:01:04,879 --> 00:01:09,040
so on

00:01:06,000 --> 00:01:11,119
uh importantly dsls won't let you write

00:01:09,040 --> 00:01:12,080
just anything i don't think you can

00:01:11,119 --> 00:01:14,400
write a web server

00:01:12,080 --> 00:01:15,759
in cmake and if by chance you can then

00:01:14,400 --> 00:01:18,880
you probably shouldn't

00:01:15,759 --> 00:01:20,720
uh instead dsls really focus on making

00:01:18,880 --> 00:01:22,080
some part of a larger software system

00:01:20,720 --> 00:01:23,840
easier to handle

00:01:22,080 --> 00:01:25,520
it's nicer and more portable to write

00:01:23,840 --> 00:01:27,040
cmake than raw shell scripts and batch

00:01:25,520 --> 00:01:28,560
files

00:01:27,040 --> 00:01:30,400
it's also more efficient to use a

00:01:28,560 --> 00:01:31,920
well-engineered parser generator

00:01:30,400 --> 00:01:33,200
than to read a bunch of textbooks and

00:01:31,920 --> 00:01:35,280
write down parsing tables for your

00:01:33,200 --> 00:01:37,040
language yourself

00:01:35,280 --> 00:01:38,560
in the programming languages world we

00:01:37,040 --> 00:01:40,479
draw a distinction between

00:01:38,560 --> 00:01:42,320
internal dsls that borrow the host

00:01:40,479 --> 00:01:44,880
languages syntax and tool chain

00:01:42,320 --> 00:01:46,640
and external dsls that supply their own

00:01:44,880 --> 00:01:47,840
but in either case you've probably used

00:01:46,640 --> 00:01:51,119
a few

00:01:47,840 --> 00:01:53,520
um eigen3 and boost spirit

00:01:51,119 --> 00:01:55,119
are popular c plus internal dsls for

00:01:53,520 --> 00:01:56,399
linear algebra and writing parsers

00:01:55,119 --> 00:01:59,920
respectively

00:01:56,399 --> 00:02:02,399
and then c make make files uh bison

00:01:59,920 --> 00:02:04,159
sql and even regular expressions are

00:02:02,399 --> 00:02:08,000
external dsls that most of us have

00:02:04,159 --> 00:02:09,440
encountered at some point or another

00:02:08,000 --> 00:02:11,200
and that brings us to halide which is

00:02:09,440 --> 00:02:13,840
the subject of today's talk

00:02:11,200 --> 00:02:16,160
so halite is a hybrid of the internal

00:02:13,840 --> 00:02:18,800
and external dsl approaches

00:02:16,160 --> 00:02:21,200
it's embedded in c plus so in that sense

00:02:18,800 --> 00:02:23,520
it's just a library like eigen is

00:02:21,200 --> 00:02:24,400
but on the other hand it executes code

00:02:23,520 --> 00:02:27,040
by jitting

00:02:24,400 --> 00:02:27,520
or by producing c api compatible object

00:02:27,040 --> 00:02:30,080
files

00:02:27,520 --> 00:02:31,680
so in that sense it's more external the

00:02:30,080 --> 00:02:33,920
problem that it helps you solve

00:02:31,680 --> 00:02:35,680
is optimizing dense numerical kernels

00:02:33,920 --> 00:02:36,720
like those found in image processing

00:02:35,680 --> 00:02:38,640
computer vision

00:02:36,720 --> 00:02:40,160
machine learning and artificial

00:02:38,640 --> 00:02:41,519
intelligence

00:02:40,160 --> 00:02:43,920
it does this through a powerful

00:02:41,519 --> 00:02:46,080
application of separation of concerns

00:02:43,920 --> 00:02:46,959
in halide you define what you want to

00:02:46,080 --> 00:02:48,840
compute

00:02:46,959 --> 00:02:50,800
separately from how you want to optimize

00:02:48,840 --> 00:02:53,280
it

00:02:50,800 --> 00:02:55,280
so before i accidentally take all the

00:02:53,280 --> 00:02:57,040
credit for halide i want to acknowledge

00:02:55,280 --> 00:02:59,680
the incredible open source team

00:02:57,040 --> 00:03:01,599
backing halide my advisor jonathan

00:02:59,680 --> 00:03:02,400
reagan kelly and my colleague andrew

00:03:01,599 --> 00:03:04,640
adams

00:03:02,400 --> 00:03:05,519
created halide in 2011 when they were at

00:03:04,640 --> 00:03:07,360
mit

00:03:05,519 --> 00:03:08,879
my own personal role in the team is

00:03:07,360 --> 00:03:11,440
twofold so first

00:03:08,879 --> 00:03:13,360
as part of my phd research i'm working

00:03:11,440 --> 00:03:15,360
on publishing a mathematical proof

00:03:13,360 --> 00:03:17,599
that halide programs will never crash

00:03:15,360 --> 00:03:19,440
and that its optimizations are safe

00:03:17,599 --> 00:03:21,519
and then second i work on the build

00:03:19,440 --> 00:03:24,800
system testing continuous integration

00:03:21,519 --> 00:03:24,800
and the release cycle for the team

00:03:25,280 --> 00:03:29,280
i also want to emphasize that this isn't

00:03:27,360 --> 00:03:30,319
a talk about some new or unproven

00:03:29,280 --> 00:03:32,000
technology

00:03:30,319 --> 00:03:34,239
the amazing people that i work with have

00:03:32,000 --> 00:03:36,159
made halide into the system of choice

00:03:34,239 --> 00:03:38,480
for the google pixel camera and the

00:03:36,159 --> 00:03:40,560
adobe photoshop layer compositor

00:03:38,480 --> 00:03:42,239
when you snap a photo on a pixel phone

00:03:40,560 --> 00:03:44,640
or edit a photoshop document

00:03:42,239 --> 00:03:46,319
you're using halide we've implemented

00:03:44,640 --> 00:03:47,280
faster neural network layers than in

00:03:46,319 --> 00:03:50,239
tensorflow

00:03:47,280 --> 00:03:51,760
faster blurs than opencv faster matrix

00:03:50,239 --> 00:03:54,959
multiplication than eigen

00:03:51,760 --> 00:03:56,799
and a faster fourier transform than fftw

00:03:54,959 --> 00:03:58,959
which styles itself as the fastest

00:03:56,799 --> 00:04:01,439
fourier transform in the west

00:03:58,959 --> 00:04:03,200
in fact because youtube uses halide at

00:04:01,439 --> 00:04:05,280
scale to re-encode video

00:04:03,200 --> 00:04:06,400
when this talk and all of the other cpp

00:04:05,280 --> 00:04:08,239
contacts this year

00:04:06,400 --> 00:04:11,280
get uploaded to youtube they will be

00:04:08,239 --> 00:04:11,280
processed by haline

00:04:11,599 --> 00:04:16,560
so with that out of the way let me

00:04:14,319 --> 00:04:18,000
briefly summarize the rest of the talk

00:04:16,560 --> 00:04:20,560
we're going to discuss the main

00:04:18,000 --> 00:04:22,479
challenges in fast array processing

00:04:20,560 --> 00:04:23,919
what matters for performance what

00:04:22,479 --> 00:04:25,919
trade-offs we make

00:04:23,919 --> 00:04:27,520
and how halide helps you optimize for

00:04:25,919 --> 00:04:29,360
those factors

00:04:27,520 --> 00:04:30,720
then we're going to talk a bit about

00:04:29,360 --> 00:04:32,400
halide's architecture

00:04:30,720 --> 00:04:34,160
and how it integrates into your programs

00:04:32,400 --> 00:04:35,680
and builds

00:04:34,160 --> 00:04:38,000
but we're not going to be diving deep

00:04:35,680 --> 00:04:40,479
into architecture-specific details

00:04:38,000 --> 00:04:41,919
or picking apart assembly sequences

00:04:40,479 --> 00:04:42,880
we're also not going to cover every

00:04:41,919 --> 00:04:44,720
feature in halide

00:04:42,880 --> 00:04:46,560
we only have an hour together so i want

00:04:44,720 --> 00:04:49,440
you to take away something from this

00:04:46,560 --> 00:04:50,960
that applies outside of halide 2. there

00:04:49,440 --> 00:04:52,080
will also be a few times when i will

00:04:50,960 --> 00:04:56,400
pause your questions

00:04:52,080 --> 00:04:58,960
uh please ask them this uh topic is

00:04:56,400 --> 00:05:00,560
subtle and sometimes complicated and no

00:04:58,960 --> 00:05:02,000
question is sort of too small

00:05:00,560 --> 00:05:04,479
uh to make sure that everybody's on the

00:05:02,000 --> 00:05:04,479
same page

00:05:05,199 --> 00:05:10,080
now without even thinking about digital

00:05:08,240 --> 00:05:11,360
signal processors and other specialized

00:05:10,080 --> 00:05:14,240
accelerator hardware

00:05:11,360 --> 00:05:15,199
i want you to think about your cpu it is

00:05:14,240 --> 00:05:18,560
incredibly

00:05:15,199 --> 00:05:20,639
complex to optimize your programs for it

00:05:18,560 --> 00:05:21,600
you have to reason about multi-level

00:05:20,639 --> 00:05:24,160
caches

00:05:21,600 --> 00:05:25,520
vector instructions multiple cores and

00:05:24,160 --> 00:05:27,360
the costs and benefits of hyper

00:05:25,520 --> 00:05:29,039
threading on those cores

00:05:27,360 --> 00:05:30,639
you have to think about super scalar and

00:05:29,039 --> 00:05:32,639
out of order execution

00:05:30,639 --> 00:05:34,160
branch prediction and the list just goes

00:05:32,639 --> 00:05:36,000
on and on

00:05:34,160 --> 00:05:38,160
if you do throw a coprocessor into the

00:05:36,000 --> 00:05:39,680
mix then you have to think not only

00:05:38,160 --> 00:05:41,520
about how to optimize the code

00:05:39,680 --> 00:05:43,680
that runs on the other device taking

00:05:41,520 --> 00:05:44,800
into account all of its architectural

00:05:43,680 --> 00:05:46,880
details

00:05:44,800 --> 00:05:48,560
but you then also have to think about

00:05:46,880 --> 00:05:49,039
how to optimize the communication

00:05:48,560 --> 00:05:52,160
between

00:05:49,039 --> 00:05:54,160
it and the cpu and if there's anything

00:05:52,160 --> 00:05:56,639
we know to be true in computer science

00:05:54,160 --> 00:05:57,280
it's that programmers even experts even

00:05:56,639 --> 00:05:59,440
me

00:05:57,280 --> 00:06:00,479
were notoriously bad at predicting

00:05:59,440 --> 00:06:02,639
performance

00:06:00,479 --> 00:06:04,400
half the time our optimization efforts

00:06:02,639 --> 00:06:07,840
do nothing and it seems that the other

00:06:04,400 --> 00:06:07,840
half they make things worse

00:06:08,880 --> 00:06:12,160
still there are some go-to strategies we

00:06:11,120 --> 00:06:14,319
can look at

00:06:12,160 --> 00:06:16,560
so imagine that each one of these

00:06:14,319 --> 00:06:18,479
squares is some unit of work that your

00:06:16,560 --> 00:06:20,639
program has to do

00:06:18,479 --> 00:06:22,560
one of the simplest optimizations is to

00:06:20,639 --> 00:06:25,680
divide independent work

00:06:22,560 --> 00:06:27,680
across independent cpu cores

00:06:25,680 --> 00:06:29,120
of course you need to have opportunities

00:06:27,680 --> 00:06:30,479
for this in the first place

00:06:29,120 --> 00:06:32,880
and there are issues with hidden

00:06:30,479 --> 00:06:35,280
dependencies like false sharing

00:06:32,880 --> 00:06:36,960
but by and large if you add processors

00:06:35,280 --> 00:06:38,479
to a problem with enough independent

00:06:36,960 --> 00:06:40,160
work to keep them all busy

00:06:38,479 --> 00:06:42,319
you should expect a proportional speed

00:06:40,160 --> 00:06:42,319
up

00:06:43,440 --> 00:06:48,000
another optimization that might be less

00:06:45,280 --> 00:06:49,919
intuitive is to recompute values instead

00:06:48,000 --> 00:06:51,919
of loading them from memory

00:06:49,919 --> 00:06:53,919
remember that each level of cache is

00:06:51,919 --> 00:06:55,680
slower than the last until you hit dram

00:06:53,919 --> 00:06:57,840
or main memory

00:06:55,680 --> 00:06:58,880
so if your values aren't cached it can

00:06:57,840 --> 00:07:01,360
be a net win

00:06:58,880 --> 00:07:03,520
to do some extra work this is actually a

00:07:01,360 --> 00:07:05,440
very old idea and it goes all the way

00:07:03,520 --> 00:07:08,160
back to re-materialization work

00:07:05,440 --> 00:07:09,759
in register allocators in the 1970s but

00:07:08,160 --> 00:07:13,199
it's actually still very relevant to

00:07:09,759 --> 00:07:15,599
today's vector instruction sets

00:07:13,199 --> 00:07:17,759
so suppose you're modifying some array

00:07:15,599 --> 00:07:19,599
using vectors of width 4.

00:07:17,759 --> 00:07:22,080
if the array's length isn't an even

00:07:19,599 --> 00:07:22,880
multiple of 4 like this one which has 14

00:07:22,080 --> 00:07:27,440
elements

00:07:22,880 --> 00:07:30,560
then you'll step 1 2

00:07:27,440 --> 00:07:31,680
3 vectors out before finally reaching

00:07:30,560 --> 00:07:34,319
the end

00:07:31,680 --> 00:07:34,800
now with only two squares left you can

00:07:34,319 --> 00:07:36,960
either

00:07:34,800 --> 00:07:38,080
shift inwards and re-compute a small

00:07:36,960 --> 00:07:39,680
amount of overlap

00:07:38,080 --> 00:07:41,520
shown here in bright yellow with a

00:07:39,680 --> 00:07:44,080
single vector instruction

00:07:41,520 --> 00:07:45,199
or you can switch to serial code and

00:07:44,080 --> 00:07:47,440
execute two

00:07:45,199 --> 00:07:48,479
slower instructions in a different code

00:07:47,440 --> 00:07:50,479
path

00:07:48,479 --> 00:07:52,080
most of the time we'll prefer the former

00:07:50,479 --> 00:07:53,599
because the inward shift can be computed

00:07:52,080 --> 00:07:59,599
without branches and without bloating

00:07:53,599 --> 00:08:01,360
the code

00:07:59,599 --> 00:08:02,960
finally we get to the last big

00:08:01,360 --> 00:08:05,919
optimization opportunity

00:08:02,960 --> 00:08:07,440
cash locality suppose you have two

00:08:05,919 --> 00:08:09,360
stages of work

00:08:07,440 --> 00:08:11,280
where the tasks are independent in each

00:08:09,360 --> 00:08:12,319
stage but there are some dependencies

00:08:11,280 --> 00:08:15,280
between the tasks

00:08:12,319 --> 00:08:16,240
as shown by the arrows here we could go

00:08:15,280 --> 00:08:18,479
breadth first

00:08:16,240 --> 00:08:21,199
and compute all of the values in step

00:08:18,479 --> 00:08:22,800
one before any of the values in step two

00:08:21,199 --> 00:08:24,479
committing those all to memory in

00:08:22,800 --> 00:08:27,199
between

00:08:24,479 --> 00:08:29,280
or we could interleave the computations

00:08:27,199 --> 00:08:31,199
of step one and step two

00:08:29,280 --> 00:08:32,959
and then the memory written by step one

00:08:31,199 --> 00:08:35,599
will still be fresh in the cache

00:08:32,959 --> 00:08:36,560
when it's read but in step two

00:08:35,599 --> 00:08:40,320
ultimately

00:08:36,560 --> 00:08:42,399
locality is a function of reuse distance

00:08:40,320 --> 00:08:44,800
we want to reuse values as quickly as

00:08:42,399 --> 00:08:46,800
possible to optimize for the cache

00:08:44,800 --> 00:08:49,120
on the other hand it isn't obvious how

00:08:46,800 --> 00:08:50,160
you can parallelize this fused two-step

00:08:49,120 --> 00:08:51,839
process

00:08:50,160 --> 00:08:53,680
that interleaving looks pretty hopeless

00:08:51,839 --> 00:08:56,000
from a parallelization perspective

00:08:53,680 --> 00:08:58,959
we can't just pragma on parallel 4 our

00:08:56,000 --> 00:08:58,959
way to victory here

00:08:59,440 --> 00:09:04,480
instead what we can do is break the

00:09:02,000 --> 00:09:06,959
dependency chain

00:09:04,480 --> 00:09:08,240
by introducing some redundant work the

00:09:06,959 --> 00:09:10,080
red squares here

00:09:08,240 --> 00:09:11,279
mark where work was duplicated from the

00:09:10,080 --> 00:09:12,640
other side

00:09:11,279 --> 00:09:14,880
and we could continue to break this

00:09:12,640 --> 00:09:16,720
problem down by introducing only a small

00:09:14,880 --> 00:09:18,959
amount of redundant overhead per thread

00:09:16,720 --> 00:09:20,640
that we want to use

00:09:18,959 --> 00:09:23,440
and then this gives us the best of all

00:09:20,640 --> 00:09:24,240
three worlds we can keep multiple cpus

00:09:23,440 --> 00:09:26,320
busy

00:09:24,240 --> 00:09:27,360
each one accessing its caches in a nice

00:09:26,320 --> 00:09:28,959
local way

00:09:27,360 --> 00:09:31,360
and we didn't have to introduce very

00:09:28,959 --> 00:09:34,720
much redundant work to do it

00:09:31,360 --> 00:09:35,519
trading off between locality parallelism

00:09:34,720 --> 00:09:37,360
and redundant

00:09:35,519 --> 00:09:39,920
recomputation is at the heart of

00:09:37,360 --> 00:09:42,240
optimizing any numeric pipeline

00:09:39,920 --> 00:09:44,240
the gen the general principle is to

00:09:42,240 --> 00:09:45,600
strive for locality within threads

00:09:44,240 --> 00:09:48,880
and introduce redundant work

00:09:45,600 --> 00:09:50,800
strategically to break dependencies

00:09:48,880 --> 00:09:52,480
it's a delicate balancing act with no

00:09:50,800 --> 00:09:55,680
simple heuristics but it's the one that

00:09:52,480 --> 00:09:55,680
halide helps you perform

00:09:55,760 --> 00:09:59,760
so you might be thinking this is great

00:09:58,240 --> 00:10:01,680
knowledge for library writers

00:09:59,760 --> 00:10:03,600
but application developers can still get

00:10:01,680 --> 00:10:05,360
great performance by composing routines

00:10:03,600 --> 00:10:07,040
from highly optimized libraries

00:10:05,360 --> 00:10:08,720
like intel's math kernel library or

00:10:07,040 --> 00:10:11,519
opencv right

00:10:08,720 --> 00:10:13,519
well the short answer is no because

00:10:11,519 --> 00:10:15,279
there's no way to fuse across the stages

00:10:13,519 --> 00:10:17,200
of computation

00:10:15,279 --> 00:10:18,959
every time you finish calling a function

00:10:17,200 --> 00:10:20,720
you sync all of your data back to ram

00:10:18,959 --> 00:10:22,640
before calling the next one

00:10:20,720 --> 00:10:24,720
this is slow for the same reason we just

00:10:22,640 --> 00:10:26,160
saw with a locality example

00:10:24,720 --> 00:10:28,000
there's no way to rearrange the

00:10:26,160 --> 00:10:30,079
computation between the stages and get

00:10:28,000 --> 00:10:32,160
small reuse distances

00:10:30,079 --> 00:10:35,519
in general optimized kernels compose

00:10:32,160 --> 00:10:35,519
into inefficient pipelines

00:10:35,839 --> 00:10:39,680
okay so why not make c plus compilers do

00:10:38,640 --> 00:10:42,079
this work for us

00:10:39,680 --> 00:10:43,519
there are a few good reasons the first

00:10:42,079 --> 00:10:44,560
is that the space of possible

00:10:43,519 --> 00:10:47,279
optimizations

00:10:44,560 --> 00:10:48,959
is extraordinarily large and complicated

00:10:47,279 --> 00:10:49,680
making one decision that looks good

00:10:48,959 --> 00:10:51,519
locally

00:10:49,680 --> 00:10:53,920
might make the optimizer miss a much

00:10:51,519 --> 00:10:56,240
better opportunity down the line

00:10:53,920 --> 00:10:58,000
experts optimizing code by hand try

00:10:56,240 --> 00:11:00,720
countless alternatives before landing on

00:10:58,000 --> 00:11:03,120
the one that works the best

00:11:00,720 --> 00:11:04,079
second some important optimizations are

00:11:03,120 --> 00:11:06,399
not legal in c

00:11:04,079 --> 00:11:08,160
plus plus compilers aren't allowed to

00:11:06,399 --> 00:11:10,399
simply change your malloc sizes

00:11:08,160 --> 00:11:13,120
or reorder calls to external functions

00:11:10,399 --> 00:11:14,480
or try to optimize your memory layouts

00:11:13,120 --> 00:11:16,959
they have to obey the rules of the

00:11:14,480 --> 00:11:18,079
language the most beneficial

00:11:16,959 --> 00:11:19,839
optimizations

00:11:18,079 --> 00:11:21,839
are those that carefully fuse many

00:11:19,839 --> 00:11:22,880
stages of composition of computation

00:11:21,839 --> 00:11:25,279
together

00:11:22,880 --> 00:11:26,880
c plus compilers are not typically able

00:11:25,279 --> 00:11:29,360
to discover these sorts of global

00:11:26,880 --> 00:11:31,360
reorganizations on their own

00:11:29,360 --> 00:11:33,760
and finally from a theoretical

00:11:31,360 --> 00:11:35,360
standpoint c plus plus is rightly turing

00:11:33,760 --> 00:11:36,800
complete

00:11:35,360 --> 00:11:40,079
that means that c plus plus can do

00:11:36,800 --> 00:11:42,079
absolutely anything but by contrast

00:11:40,079 --> 00:11:44,000
all halide can do is express feed

00:11:42,079 --> 00:11:47,279
forward array programs and that makes it

00:11:44,000 --> 00:11:47,279
much easier to analyze

00:11:48,880 --> 00:11:52,800
so for the rest of this talk we're going

00:11:50,959 --> 00:11:53,760
to use this running example that sort of

00:11:52,800 --> 00:11:55,519
typifies

00:11:53,760 --> 00:11:57,839
the kind of program that halide helps

00:11:55,519 --> 00:11:57,839
you optimize

00:11:58,320 --> 00:12:02,560
it's called the two stage blur and it's

00:12:00,800 --> 00:12:04,560
a particular instance of the abstract

00:12:02,560 --> 00:12:06,320
example that we just saw

00:12:04,560 --> 00:12:08,560
as the name suggests it's going to take

00:12:06,320 --> 00:12:09,680
an input image and make it blurry so no

00:12:08,560 --> 00:12:11,600
surprises there

00:12:09,680 --> 00:12:12,959
but what might be a bit unconventional

00:12:11,600 --> 00:12:16,079
is that it's going to first

00:12:12,959 --> 00:12:18,160
blur the image horizontally in code

00:12:16,079 --> 00:12:19,920
we blur a pixel by computing an average

00:12:18,160 --> 00:12:21,839
of the pixels around it

00:12:19,920 --> 00:12:24,240
in this first step we compute the

00:12:21,839 --> 00:12:25,839
horizontal blur by averaging a pixel

00:12:24,240 --> 00:12:27,440
with its neighbors to the left and to

00:12:25,839 --> 00:12:29,839
the right that gives us this

00:12:27,440 --> 00:12:32,639
intermediate step here on the right

00:12:29,839 --> 00:12:34,240
then we do the same thing on the

00:12:32,639 --> 00:12:36,240
horizontally blurred image

00:12:34,240 --> 00:12:37,839
but vertically so now we average the

00:12:36,240 --> 00:12:40,240
neighbors above and below the final

00:12:37,839 --> 00:12:42,639
pixel in the horizontally blurred image

00:12:40,240 --> 00:12:45,279
in the end this is effectively the same

00:12:42,639 --> 00:12:46,959
as blurring the full 3x3 box around a

00:12:45,279 --> 00:12:48,959
pixel in one giant step

00:12:46,959 --> 00:12:52,959
but splitting it up into two stages like

00:12:48,959 --> 00:12:54,720
this does less arithmetic overall

00:12:52,959 --> 00:12:57,120
now as with all things in computer

00:12:54,720 --> 00:12:58,959
science there are edge cases

00:12:57,120 --> 00:13:00,880
but in image processing they happen to

00:12:58,959 --> 00:13:02,720
be very literal

00:13:00,880 --> 00:13:04,240
if you're trying to blur the pixels

00:13:02,720 --> 00:13:06,639
around the edge of the image

00:13:04,240 --> 00:13:08,079
you'll find that some values are missing

00:13:06,639 --> 00:13:10,000
you could skip those pixels

00:13:08,079 --> 00:13:12,000
but it would crop a one pixel border off

00:13:10,000 --> 00:13:12,800
of the image since we don't want that to

00:13:12,000 --> 00:13:14,800
happen

00:13:12,800 --> 00:13:16,560
we just extend the boundary by repeating

00:13:14,800 --> 00:13:17,120
the outer edge to keep the image size

00:13:16,560 --> 00:13:18,480
the same

00:13:17,120 --> 00:13:21,839
that's highlighted in orange in the

00:13:18,480 --> 00:13:24,079
second step here

00:13:21,839 --> 00:13:25,680
so let's try to write this in c plus

00:13:24,079 --> 00:13:27,600
plus

00:13:25,680 --> 00:13:30,560
imagine that we've already got a struct

00:13:27,600 --> 00:13:32,639
called image that wraps a stud vector

00:13:30,560 --> 00:13:34,800
and exposes a two-dimensional operator

00:13:32,639 --> 00:13:36,399
parens to access its elements according

00:13:34,800 --> 00:13:38,160
to the width and height

00:13:36,399 --> 00:13:39,199
this is definitely slide code and i

00:13:38,160 --> 00:13:40,560
don't want to dwell on the

00:13:39,199 --> 00:13:43,519
implementation of this

00:13:40,560 --> 00:13:44,880
uh image struct too much but just assume

00:13:43,519 --> 00:13:49,440
that the image struct does the right

00:13:44,880 --> 00:13:51,680
thing for the purposes of this example

00:13:49,440 --> 00:13:54,079
the input will be represented by a const

00:13:51,680 --> 00:13:56,399
reference to such an image struct

00:13:54,079 --> 00:13:57,680
then we can get started with a boundary

00:13:56,399 --> 00:13:59,279
extending step

00:13:57,680 --> 00:14:01,199
we create an intermediate image that's

00:13:59,279 --> 00:14:03,600
got extra space for the border

00:14:01,199 --> 00:14:05,360
we make it you in 16 instead of u and

00:14:03,600 --> 00:14:08,000
eight because we don't want the addition

00:14:05,360 --> 00:14:10,480
in the averages to overflow later

00:14:08,000 --> 00:14:12,000
then we loop over all the uninitialized

00:14:10,480 --> 00:14:14,560
points in the padded image

00:14:12,000 --> 00:14:16,240
and assign them to the closest values in

00:14:14,560 --> 00:14:18,720
space from the input

00:14:16,240 --> 00:14:20,240
c plus plus 17 makes that easy since it

00:14:18,720 --> 00:14:20,880
adds the clamp algorithm that we use

00:14:20,240 --> 00:14:22,720
here

00:14:20,880 --> 00:14:24,399
if you aren't familiar with it clamp

00:14:22,720 --> 00:14:25,440
simply compares the first value to the

00:14:24,399 --> 00:14:27,760
next two arguments

00:14:25,440 --> 00:14:29,279
which are a lower and an upper bound if

00:14:27,760 --> 00:14:30,320
it's less than the lower bound then it

00:14:29,279 --> 00:14:31,839
uses the lower bound

00:14:30,320 --> 00:14:33,600
and if it's greater than the upper bound

00:14:31,839 --> 00:14:35,199
then it uses the upper bound instead

00:14:33,600 --> 00:14:36,639
it's a very helpful algorithm for

00:14:35,199 --> 00:14:39,199
ensuring that you don't read an array

00:14:36,639 --> 00:14:39,199
out of bounds

00:14:39,279 --> 00:14:44,160
then in the horizontal blur step

00:14:42,320 --> 00:14:46,000
we create another intermediate image

00:14:44,160 --> 00:14:47,279
this time a bit narrower than the padded

00:14:46,000 --> 00:14:48,480
image to account for the shrinking

00:14:47,279 --> 00:14:50,480
boundary

00:14:48,480 --> 00:14:51,839
we loop over all the points again and

00:14:50,480 --> 00:14:53,120
average the neighbors like i explained

00:14:51,839 --> 00:14:55,199
before

00:14:53,120 --> 00:14:57,440
pay careful attention to the loop bounds

00:14:55,199 --> 00:14:59,519
we iterate from x equals 1 to

00:14:57,440 --> 00:15:00,480
x equals the width of the image since

00:14:59,519 --> 00:15:03,920
we're always writing

00:15:00,480 --> 00:15:05,680
to x minus 1 in the body of the loop

00:15:03,920 --> 00:15:09,040
and finally we do the same sort of

00:15:05,680 --> 00:15:09,040
pattern for the vertical blur

00:15:10,320 --> 00:15:13,839
now gluing all of this together isn't

00:15:12,480 --> 00:15:16,800
too much trouble and i'll give you a

00:15:13,839 --> 00:15:16,800
second to take it all in

00:15:18,160 --> 00:15:23,839
now we're writing c plus

00:15:21,440 --> 00:15:26,399
so this should be fast right we take the

00:15:23,839 --> 00:15:28,639
input by reference so no copy there

00:15:26,399 --> 00:15:30,399
we ought to get an rvo for the result so

00:15:28,639 --> 00:15:32,240
no copy there either

00:15:30,399 --> 00:15:33,680
we've got loops that can probably be

00:15:32,240 --> 00:15:37,040
automatically vectorized

00:15:33,680 --> 00:15:39,839
and so the star should align for us

00:15:37,040 --> 00:15:40,480
but life isn't so simple if the image

00:15:39,839 --> 00:15:42,800
struct

00:15:40,480 --> 00:15:43,600
uses a row major memory layout so that

00:15:42,800 --> 00:15:46,160
adding 1

00:15:43,600 --> 00:15:47,839
to x adds 1 to the pointer into the stud

00:15:46,160 --> 00:15:49,759
vector in blur x

00:15:47,839 --> 00:15:51,519
then this code will write a pixel value

00:15:49,759 --> 00:15:52,880
to a different cache line on every

00:15:51,519 --> 00:15:54,639
iteration

00:15:52,880 --> 00:15:56,399
this is about the worst performance you

00:15:54,639 --> 00:15:59,279
can get on this example without going

00:15:56,399 --> 00:16:01,040
out of your way to do something weird

00:15:59,279 --> 00:16:03,040
on my machine running this on a 4

00:16:01,040 --> 00:16:06,800
megapixel image takes about 100

00:16:03,040 --> 00:16:06,800
milliseconds which is not very fast at

00:16:06,839 --> 00:16:09,839
all

00:16:10,240 --> 00:16:14,000
fortunately there's a simple fix just

00:16:12,720 --> 00:16:15,519
swap the loops

00:16:14,000 --> 00:16:16,959
you don't need to make any changes to

00:16:15,519 --> 00:16:18,720
the assignment since all the loop

00:16:16,959 --> 00:16:20,720
iterations are independent

00:16:18,720 --> 00:16:24,240
now you'll access consecutive cache

00:16:20,720 --> 00:16:26,000
lines which is much better for the cpu

00:16:24,240 --> 00:16:27,920
if you do this then it drops to eight

00:16:26,000 --> 00:16:30,160
milliseconds on the same image which is

00:16:27,920 --> 00:16:33,440
a 12 to 13x speed up a full

00:16:30,160 --> 00:16:35,040
order of magnitude if you imagine memory

00:16:33,440 --> 00:16:36,720
being laid out left to right and top to

00:16:35,040 --> 00:16:38,160
bottom then you can clearly see the

00:16:36,720 --> 00:16:39,600
issue with the traversal order

00:16:38,160 --> 00:16:41,279
this is one of the few times that you

00:16:39,600 --> 00:16:41,920
can tell something will be slow at a

00:16:41,279 --> 00:16:44,240
glance

00:16:41,920 --> 00:16:46,800
since it never accesses memory in a nice

00:16:44,240 --> 00:16:46,800
local way

00:16:47,279 --> 00:16:51,040
if you continue down this path and keep

00:16:49,040 --> 00:16:52,480
on optimizing you might eventually end

00:16:51,040 --> 00:16:53,440
up with a mess that looks like this

00:16:52,480 --> 00:16:54,880
program

00:16:53,440 --> 00:16:56,480
you'll have to trust me that it does the

00:16:54,880 --> 00:16:58,880
same thing but

00:16:56,480 --> 00:16:59,839
it does it with parallelism from openmp

00:16:58,880 --> 00:17:02,800
that's the pragma

00:16:59,839 --> 00:17:05,520
parallel 4 at the top and sse intrinsics

00:17:02,800 --> 00:17:08,079
inside the loops

00:17:05,520 --> 00:17:09,839
it's also locality optimized by fusing

00:17:08,079 --> 00:17:11,679
the stages together and tiling so that

00:17:09,839 --> 00:17:13,919
we can cut down on memory usage

00:17:11,679 --> 00:17:15,679
and have simpler for loop extents but

00:17:13,919 --> 00:17:18,880
rather than pick this code apart i want

00:17:15,679 --> 00:17:22,400
you to think about three questions

00:17:18,880 --> 00:17:22,400
first is it portable

00:17:22,640 --> 00:17:26,480
and i don't mean across compilers or cpu

00:17:24,959 --> 00:17:27,760
architectures because we already know

00:17:26,480 --> 00:17:30,080
that it isn't

00:17:27,760 --> 00:17:32,080
it uses x86 and compiler specific

00:17:30,080 --> 00:17:32,880
intrinsics and openmp none of which are

00:17:32,080 --> 00:17:36,320
standard c

00:17:32,880 --> 00:17:40,480
plus but is it portable across different

00:17:36,320 --> 00:17:40,480
x86 cpus with the same compiler

00:17:40,640 --> 00:17:44,400
the answer here is also no different

00:17:43,679 --> 00:17:46,160
cpus

00:17:44,400 --> 00:17:47,679
even in the same generation will have

00:17:46,160 --> 00:17:50,720
different cache sizes

00:17:47,679 --> 00:17:53,440
what's fast on a consumer grade intel i5

00:17:50,720 --> 00:17:55,440
will not be optimal for a xeon released

00:17:53,440 --> 00:17:58,799
in the same year

00:17:55,440 --> 00:18:02,000
next is it correct

00:17:58,799 --> 00:18:04,000
how could you tell there's actually a

00:18:02,000 --> 00:18:04,799
subtle rounding error here and did you

00:18:04,000 --> 00:18:06,640
spot it

00:18:04,799 --> 00:18:09,919
what would it take for you to notice and

00:18:06,640 --> 00:18:13,360
how long would it take to fix

00:18:09,919 --> 00:18:15,360
finally is this code maintainable

00:18:13,360 --> 00:18:17,520
based on the previous two questions i

00:18:15,360 --> 00:18:19,360
hope you'll agree that the answer is no

00:18:17,520 --> 00:18:22,000
this code has actually already bit

00:18:19,360 --> 00:18:24,480
rotted it's using sse not avx

00:18:22,000 --> 00:18:26,640
it has fixed tile sizes when it was

00:18:24,480 --> 00:18:28,160
written in 2011 it was pretty good

00:18:26,640 --> 00:18:30,880
but you shouldn't copy it into your own

00:18:28,160 --> 00:18:30,880
program today

00:18:32,960 --> 00:18:36,799
what sets halide apart is that it has

00:18:34,960 --> 00:18:37,440
good answers to all three of those

00:18:36,799 --> 00:18:39,039
questions

00:18:37,440 --> 00:18:41,120
and it does it through a really clever

00:18:39,039 --> 00:18:42,640
separation of concerns

00:18:41,120 --> 00:18:44,799
the first thing you do when writing a

00:18:42,640 --> 00:18:47,440
halide program is to define the

00:18:44,799 --> 00:18:48,799
algorithm which specifies what to

00:18:47,440 --> 00:18:50,720
compute

00:18:48,799 --> 00:18:53,760
it's a declarative specification of what

00:18:50,720 --> 00:18:55,600
to do not code that will immediately run

00:18:53,760 --> 00:18:56,880
as such it doesn't care about loop

00:18:55,600 --> 00:18:59,200
bounds or memory

00:18:56,880 --> 00:19:01,520
or traversal orders or anything else it

00:18:59,200 --> 00:19:03,679
just serves as a single source of truth

00:19:01,520 --> 00:19:05,840
for what your pipeline should return

00:19:03,679 --> 00:19:06,720
later on you'll specify how to you want

00:19:05,840 --> 00:19:10,400
to optimize it

00:19:06,720 --> 00:19:12,240
and then you get a chance to run it

00:19:10,400 --> 00:19:14,080
so let's see in action on the blur

00:19:12,240 --> 00:19:17,280
pipeline first

00:19:14,080 --> 00:19:19,440
we'll declare a func called input a func

00:19:17,280 --> 00:19:21,840
in halide is conceptually a mapping

00:19:19,440 --> 00:19:23,600
from array locations to values but with

00:19:21,840 --> 00:19:25,360
indeterminate bounds

00:19:23,600 --> 00:19:27,679
for the functional programmers among you

00:19:25,360 --> 00:19:29,679
note that every function halide is pure

00:19:27,679 --> 00:19:31,360
that is it has no side effects and

00:19:29,679 --> 00:19:33,280
always returns the same values at the

00:19:31,360 --> 00:19:36,080
same points

00:19:33,280 --> 00:19:37,919
so the boundary extending stage looks

00:19:36,080 --> 00:19:38,160
almost identical to what we saw earlier

00:19:37,919 --> 00:19:40,320
in

00:19:38,160 --> 00:19:41,760
plain z plus plus only it's just the

00:19:40,320 --> 00:19:44,160
assignment statement

00:19:41,760 --> 00:19:45,919
again we use clamp to keep the inside of

00:19:44,160 --> 00:19:48,240
the bounds of the original input

00:19:45,919 --> 00:19:49,760
but remember that the input func is

00:19:48,240 --> 00:19:51,280
still a placeholder

00:19:49,760 --> 00:19:53,120
the eventual width and height will be

00:19:51,280 --> 00:19:54,320
determined when we actually feed an

00:19:53,120 --> 00:19:57,440
image through the pipeline

00:19:54,320 --> 00:19:58,000
not right now similarly the variables x

00:19:57,440 --> 00:20:00,000
and y

00:19:58,000 --> 00:20:01,760
here are symbolic variables that don't

00:20:00,000 --> 00:20:04,080
have specific integer values at this

00:20:01,760 --> 00:20:04,080
point

00:20:04,880 --> 00:20:08,720
it happens to be that repeating the edge

00:20:06,880 --> 00:20:10,400
to extend the boundary is such a common

00:20:08,720 --> 00:20:12,640
thing to do in image processing

00:20:10,400 --> 00:20:14,240
the halide provides a helper for it so

00:20:12,640 --> 00:20:16,640
we'll just go ahead and use that here to

00:20:14,240 --> 00:20:18,720
make things easier to read

00:20:16,640 --> 00:20:19,760
now we'll define the funcs for the

00:20:18,720 --> 00:20:22,320
horizontal

00:20:19,760 --> 00:20:24,000
and vertical blurs the horizontal one

00:20:22,320 --> 00:20:25,280
reads from the upcast image

00:20:24,000 --> 00:20:29,039
and the vertical one reads from the

00:20:25,280 --> 00:20:29,039
horizontal image same as before

00:20:29,600 --> 00:20:34,960
excuse me one more once more

00:20:32,720 --> 00:20:36,720
the glue code is fairly minimal nothing

00:20:34,960 --> 00:20:38,400
new besides the variable declarations

00:20:36,720 --> 00:20:39,679
and the cast back down to 8-bit in the

00:20:38,400 --> 00:20:41,440
result func

00:20:39,679 --> 00:20:42,720
remember that since halide is just a

00:20:41,440 --> 00:20:45,600
c-plus plus library

00:20:42,720 --> 00:20:47,360
this is valid c plus 2. we're just using

00:20:45,600 --> 00:20:48,559
operator overloading to get a natural

00:20:47,360 --> 00:20:50,880
syntax

00:20:48,559 --> 00:20:52,720
the funk and var types are normal c plus

00:20:50,880 --> 00:20:55,919
classes

00:20:52,720 --> 00:20:57,919
so now is a great time to stop

00:20:55,919 --> 00:20:59,200
and answer any questions you might have

00:20:57,919 --> 00:21:01,200
because we're about to jump into the

00:20:59,200 --> 00:21:02,640
deep end optimizing this

00:21:01,200 --> 00:21:04,320
i'll wait just a minute for some

00:21:02,640 --> 00:21:05,520
questions to roll in if there aren't any

00:21:04,320 --> 00:21:07,200
and then we can move on

00:21:05,520 --> 00:21:17,840
and i'll also wait a little bit to

00:21:07,200 --> 00:21:17,840
account for the delay on remote

00:21:21,360 --> 00:21:24,480
so we have one question that says on

00:21:23,360 --> 00:21:26,720
slide 28

00:21:24,480 --> 00:21:28,880
you said that clamp from c plus 17 is

00:21:26,720 --> 00:21:30,080
used what if the company uses visual

00:21:28,880 --> 00:21:32,640
studio 2008

00:21:30,080 --> 00:21:34,000
and there's no support for c plus 17. is

00:21:32,640 --> 00:21:37,200
there any work around

00:21:34,000 --> 00:21:38,240
so on slide 28 the code that i was

00:21:37,200 --> 00:21:40,320
showing

00:21:38,240 --> 00:21:42,640
was the c plus code i'll pull up the

00:21:40,320 --> 00:21:42,640
slide

00:21:43,280 --> 00:21:48,480
that i was using to demonstrate how you

00:21:45,120 --> 00:21:48,480
would write this in c plus

00:21:48,799 --> 00:21:51,840
this code isn't something you would

00:21:50,080 --> 00:21:53,760
actually want to use and you can always

00:21:51,840 --> 00:21:56,799
replace clamp by an equivalent

00:21:53,760 --> 00:22:00,159
uh set of min and max

00:21:56,799 --> 00:22:03,679
computations instructions or macros

00:22:00,159 --> 00:22:05,760
the uh use in halide um is again

00:22:03,679 --> 00:22:08,240
just sort of tangential i've used clamp

00:22:05,760 --> 00:22:09,520
here to both show off a cool c plus plus

00:22:08,240 --> 00:22:13,440
17 feature

00:22:09,520 --> 00:22:17,360
and um to make the code fit on the slide

00:22:13,440 --> 00:22:20,080
um however i will point out

00:22:17,360 --> 00:22:20,720
that halide as a dsl and as a c plus

00:22:20,080 --> 00:22:23,679
library

00:22:20,720 --> 00:22:26,000
does require c plus 11 and we don't test

00:22:23,679 --> 00:22:28,880
it on visual studio versions prior to

00:22:26,000 --> 00:22:31,760
2015 anymore uh so your mileage may vary

00:22:28,880 --> 00:22:34,000
if you're stuck on 2008.

00:22:31,760 --> 00:22:36,480
i got another question does the dsl

00:22:34,000 --> 00:22:38,720
follow some sort of c plus plus standard

00:22:36,480 --> 00:22:40,080
it is for all intents and purposes a

00:22:38,720 --> 00:22:42,640
normal c plus 11

00:22:40,080 --> 00:22:43,360
library you include its headers you link

00:22:42,640 --> 00:22:45,760
to it

00:22:43,360 --> 00:22:47,360
a link to its shared library and you

00:22:45,760 --> 00:22:49,919
just write normal c plus plus code

00:22:47,360 --> 00:22:49,919
surrounding it

00:22:50,960 --> 00:22:55,840
so let's continue along

00:22:56,400 --> 00:22:59,919
so the other side of the halide coin is

00:22:58,559 --> 00:23:01,679
the schedule

00:22:59,919 --> 00:23:04,000
the relationship between a schedule and

00:23:01,679 --> 00:23:05,600
an algorithm is somewhat similar to the

00:23:04,000 --> 00:23:08,559
relationship between a css

00:23:05,600 --> 00:23:10,720
style sheet and an html document a

00:23:08,559 --> 00:23:12,880
schedule is a companion to the algorithm

00:23:10,720 --> 00:23:15,120
and it defines an order of execution by

00:23:12,880 --> 00:23:17,039
referring to the funcs inside of it

00:23:15,120 --> 00:23:18,159
as we'll see shortly it controls

00:23:17,039 --> 00:23:22,159
vectorization

00:23:18,159 --> 00:23:23,840
tiling inlining parallelism and more

00:23:22,159 --> 00:23:25,360
the scheduling language provides some

00:23:23,840 --> 00:23:27,120
strong guarantees

00:23:25,360 --> 00:23:29,120
applying a scheduling directive will

00:23:27,120 --> 00:23:31,440
never change your program's output

00:23:29,120 --> 00:23:36,400
and without manually disabling checks

00:23:31,440 --> 00:23:38,720
your program will never crash either

00:23:36,400 --> 00:23:39,919
so keeping this code in mind let's talk

00:23:38,720 --> 00:23:41,760
about how to schedule it

00:23:39,919 --> 00:23:44,159
and we're going to focus in particular

00:23:41,760 --> 00:23:47,200
on the input 16 func

00:23:44,159 --> 00:23:48,400
the blur x func and the result func for

00:23:47,200 --> 00:23:51,600
the purposes of this

00:23:48,400 --> 00:23:55,840
think of the result funk as being blur y

00:23:51,600 --> 00:23:55,840
plus the downcast back to uint8

00:23:57,039 --> 00:24:00,559
the next few slides are all going to

00:23:58,880 --> 00:24:02,400
follow this same format

00:24:00,559 --> 00:24:04,320
on the bottom left we have the halide

00:24:02,400 --> 00:24:05,520
schedule code that's applied to our blur

00:24:04,320 --> 00:24:07,440
algorithm

00:24:05,520 --> 00:24:09,440
on the right is the imperative pseudo

00:24:07,440 --> 00:24:11,840
code corresponding to what halide will

00:24:09,440 --> 00:24:13,600
generate based on that schedule

00:24:11,840 --> 00:24:15,600
on the top is a visualization of the

00:24:13,600 --> 00:24:17,520
programming running on a smaller image

00:24:15,600 --> 00:24:19,520
and at the very top we have performance

00:24:17,520 --> 00:24:20,799
numbers for running it on a 4 megapixel

00:24:19,520 --> 00:24:22,720
image

00:24:20,799 --> 00:24:24,080
the checkerboard pattern indicates an

00:24:22,720 --> 00:24:26,240
uninitialized buffer

00:24:24,080 --> 00:24:27,679
and the blue and yellow triangles track

00:24:26,240 --> 00:24:30,159
where the pipeline is reading and

00:24:27,679 --> 00:24:32,080
writing respectively

00:24:30,159 --> 00:24:34,159
this schedule is equivalent to the bad

00:24:32,080 --> 00:24:38,080
c-plus plus code we saw earlier

00:24:34,159 --> 00:24:41,440
it uses two directives compute root

00:24:38,080 --> 00:24:43,120
and reorder compute root

00:24:41,440 --> 00:24:44,480
means that halides should compute all

00:24:43,120 --> 00:24:46,480
the values of that func

00:24:44,480 --> 00:24:49,279
and store them in a big array before

00:24:46,480 --> 00:24:52,320
anything else tries to read from it

00:24:49,279 --> 00:24:53,200
then the reorder directive sets the loop

00:24:52,320 --> 00:24:56,320
order to the

00:24:53,200 --> 00:24:58,159
to be y innermost and x outermost

00:24:56,320 --> 00:25:00,000
uh we can actually see the program run

00:24:58,159 --> 00:25:00,640
for a minute to see what it's doing so

00:25:00,000 --> 00:25:04,080
i'll let it

00:25:00,640 --> 00:25:04,080
run back around to the start here

00:25:08,240 --> 00:25:15,279
so first we can see input get copied

00:25:11,600 --> 00:25:16,880
padded and up cast into input 16

00:25:15,279 --> 00:25:18,960
and it does this one sort of one pixel

00:25:16,880 --> 00:25:20,880
at a time

00:25:18,960 --> 00:25:22,640
and it's going in the slow against the

00:25:20,880 --> 00:25:25,120
cache line direction

00:25:22,640 --> 00:25:26,159
then we can watch as wide rectangles of

00:25:25,120 --> 00:25:28,159
input 16

00:25:26,159 --> 00:25:30,799
are read to compute single pixels in

00:25:28,159 --> 00:25:30,799
blur x

00:25:32,080 --> 00:25:35,600
finally we can watch as tall rectangles

00:25:34,640 --> 00:25:37,679
of blur x

00:25:35,600 --> 00:25:39,679
are red and down cast to compute the

00:25:37,679 --> 00:25:43,360
final result the pixels and blur

00:25:39,679 --> 00:25:46,559
the pixels in result

00:25:43,360 --> 00:25:48,640
so now this schedule

00:25:46,559 --> 00:25:50,080
is the same as the faster c plus code we

00:25:48,640 --> 00:25:50,960
saw earlier with the corrected loop

00:25:50,080 --> 00:25:53,440
order

00:25:50,960 --> 00:25:55,279
because halide either knows or controls

00:25:53,440 --> 00:25:57,120
the memory layout for every func

00:25:55,279 --> 00:25:58,640
it gets traversal orders right by

00:25:57,120 --> 00:26:01,360
default

00:25:58,640 --> 00:26:03,760
again we can watch as input gets copied

00:26:01,360 --> 00:26:05,679
and padded into input 16 but this time

00:26:03,760 --> 00:26:07,919
in the cache friendly order and we could

00:26:05,679 --> 00:26:09,440
watch input 16 get blurred horizontally

00:26:07,919 --> 00:26:11,840
into blur x

00:26:09,440 --> 00:26:13,279
and then finally we can watch blur x get

00:26:11,840 --> 00:26:14,880
blurred vertically all using this

00:26:13,279 --> 00:26:16,400
straightforward memory pattern

00:26:14,880 --> 00:26:18,640
and again we see that this is eight

00:26:16,400 --> 00:26:22,080
milliseconds or a 13x speed up over the

00:26:18,640 --> 00:26:22,080
naive code we just saw in the previous

00:26:22,840 --> 00:26:25,840
slide

00:26:26,559 --> 00:26:30,080
so if you choose not to schedule your

00:26:28,480 --> 00:26:32,000
pipeline at all

00:26:30,080 --> 00:26:33,120
then every funk will be inlined into the

00:26:32,000 --> 00:26:34,720
result

00:26:33,120 --> 00:26:36,640
because this pipeline is short and

00:26:34,720 --> 00:26:39,120
memory bound this is actually not the

00:26:36,640 --> 00:26:40,559
worst schedule it's ever so slightly

00:26:39,120 --> 00:26:42,400
faster than the previous one taking

00:26:40,559 --> 00:26:43,919
seven and a half milliseconds instead of

00:26:42,400 --> 00:26:46,159
eight

00:26:43,919 --> 00:26:48,400
we use inlining as a default schedule in

00:26:46,159 --> 00:26:50,159
halide because it encourages users to

00:26:48,400 --> 00:26:52,320
break down their algorithms into small

00:26:50,159 --> 00:26:54,240
funks with zero overhead

00:26:52,320 --> 00:26:55,679
this enables scheduling only the funks

00:26:54,240 --> 00:26:57,200
that seem relevant up front

00:26:55,679 --> 00:26:59,279
but still allows for incremental

00:26:57,200 --> 00:27:00,640
scheduling without being slow by default

00:26:59,279 --> 00:27:03,440
which would be the case if we took

00:27:00,640 --> 00:27:04,960
compute root as the default schedule

00:27:03,440 --> 00:27:07,200
notice that in this schedule the

00:27:04,960 --> 00:27:09,600
intermediate buffers for input 16

00:27:07,200 --> 00:27:11,760
and blur x have disappeared completely

00:27:09,600 --> 00:27:13,760
now 9 pixels are red for each pixel of

00:27:11,760 --> 00:27:15,360
the result which means considerably more

00:27:13,760 --> 00:27:20,240
computation will be done here

00:27:15,360 --> 00:27:22,320
than in other schedules we'll examine

00:27:20,240 --> 00:27:24,240
halide provides another directive that

00:27:22,320 --> 00:27:25,200
automatically vectorizes loops for a

00:27:24,240 --> 00:27:28,240
given vector width

00:27:25,200 --> 00:27:29,679
helpfully named vectorize in this case

00:27:28,240 --> 00:27:32,480
we mark every inner loop to be

00:27:29,679 --> 00:27:34,120
vectorized with a 32 element wide vector

00:27:32,480 --> 00:27:36,799
remember that our funks operate on

00:27:34,120 --> 00:27:39,840
uh-16s which are two bytes wide

00:27:36,799 --> 00:27:40,720
and avx-512 has 64-bit wide vector

00:27:39,840 --> 00:27:44,000
registers so

00:27:40,720 --> 00:27:46,159
64 divided by 2 is 32.

00:27:44,000 --> 00:27:47,679
halide transparently handles alignment

00:27:46,159 --> 00:27:50,720
and instruction selection issues for

00:27:47,679 --> 00:27:52,640
every supported architecture

00:27:50,720 --> 00:27:55,919
notice in the animation that for tail

00:27:52,640 --> 00:27:57,840
cases halide will do some redundant work

00:27:55,919 --> 00:27:59,279
uh when padding and converting the input

00:27:57,840 --> 00:28:01,279
to the 16-bit form

00:27:59,279 --> 00:28:02,720
the vector shifts inwards and overlaps

00:28:01,279 --> 00:28:04,320
the previous one on each row

00:28:02,720 --> 00:28:06,159
this is the same optimization i

00:28:04,320 --> 00:28:08,799
described the beginning of the talk

00:28:06,159 --> 00:28:09,520
but for free because these are vector

00:28:08,799 --> 00:28:11,200
instructions

00:28:09,520 --> 00:28:13,120
this is more efficient than switching to

00:28:11,200 --> 00:28:15,360
serial code for the tail case

00:28:13,120 --> 00:28:16,640
because that would end up being up to 32

00:28:15,360 --> 00:28:18,880
or 31

00:28:16,640 --> 00:28:20,320
serial instructions the generated

00:28:18,880 --> 00:28:23,039
assembly has fewer branches

00:28:20,320 --> 00:28:23,760
as a result and is smaller overall this

00:28:23,039 --> 00:28:25,919
schedule

00:28:23,760 --> 00:28:27,760
is already pretty good and it clocks in

00:28:25,919 --> 00:28:29,840
around three milliseconds

00:28:27,760 --> 00:28:31,360
and a 30x speed boost over the most

00:28:29,840 --> 00:28:32,799
naive code we've considered

00:28:31,360 --> 00:28:34,640
this is the kind of performance you

00:28:32,799 --> 00:28:36,799
would expect to get if you were using a

00:28:34,640 --> 00:28:40,320
simd template library in plain c plus

00:28:36,799 --> 00:28:40,320
plus to optimize the inner loops

00:28:41,440 --> 00:28:46,080
but there still aren't any locality

00:28:44,080 --> 00:28:47,520
optimizations in that schedule

00:28:46,080 --> 00:28:49,120
the first halide directive we're going

00:28:47,520 --> 00:28:53,200
to use to achieve locality

00:28:49,120 --> 00:28:56,320
is called tile it breaks up the x and y

00:28:53,200 --> 00:29:00,399
loops into four loops x y

00:28:56,320 --> 00:29:02,799
x i and y i where i is short for inner

00:29:00,399 --> 00:29:03,600
notice in the imperative code that x i

00:29:02,799 --> 00:29:05,600
and y i

00:29:03,600 --> 00:29:08,320
have known extents prescribed by the

00:29:05,600 --> 00:29:10,559
arguments to the tile call

00:29:08,320 --> 00:29:12,880
halide will figure out the right bounds

00:29:10,559 --> 00:29:15,840
for the plain x and y variables

00:29:12,880 --> 00:29:17,360
so that they iterate over the tiles here

00:29:15,840 --> 00:29:18,080
i chose to chop up the loops for the

00:29:17,360 --> 00:29:22,000
result

00:29:18,080 --> 00:29:23,679
into 128 by 24 pixel tiles

00:29:22,000 --> 00:29:25,760
you might wonder how did i pick that

00:29:23,679 --> 00:29:27,039
tile size and the answer is that i did

00:29:25,760 --> 00:29:28,399
it by trial and error

00:29:27,039 --> 00:29:30,480
which is quick and easy to do with

00:29:28,399 --> 00:29:32,960
halide i tried square tiles

00:29:30,480 --> 00:29:36,240
tall tiles wide tiles and i manually

00:29:32,960 --> 00:29:36,240
tuned until i landed on this

00:29:37,360 --> 00:29:42,000
but tiling alone isn't useful until we

00:29:39,919 --> 00:29:43,520
fuse the computation of earlier funks

00:29:42,000 --> 00:29:45,440
into those tiles

00:29:43,520 --> 00:29:46,799
halide provides a directive called

00:29:45,440 --> 00:29:48,399
compute at

00:29:46,799 --> 00:29:50,000
that enables us to interleave the

00:29:48,399 --> 00:29:52,000
computations of two functs

00:29:50,000 --> 00:29:54,159
by moving the loops for the producing

00:29:52,000 --> 00:29:55,039
func into the loops of the consuming

00:29:54,159 --> 00:29:58,080
fung

00:29:55,039 --> 00:29:58,480
so here we move the blur x func into the

00:29:58,080 --> 00:30:01,200
x

00:29:58,480 --> 00:30:02,000
loop for the result and then halide will

00:30:01,200 --> 00:30:04,000
automatically

00:30:02,000 --> 00:30:05,200
disambiguate the variable names to avoid

00:30:04,000 --> 00:30:08,320
clashes

00:30:05,200 --> 00:30:11,440
we can see that blur x that's right here

00:30:08,320 --> 00:30:13,840
is computed just before the tile loops

00:30:11,440 --> 00:30:14,640
for the result and halide will use this

00:30:13,840 --> 00:30:17,120
information

00:30:14,640 --> 00:30:19,279
to compute only the values of blur x

00:30:17,120 --> 00:30:22,799
that are needed for those two

00:30:19,279 --> 00:30:25,600
subsequent innermost loops then

00:30:22,799 --> 00:30:26,320
we can apply the same directive to input

00:30:25,600 --> 00:30:28,399
00:30:26,320 --> 00:30:30,240
and we can see that it moves before the

00:30:28,399 --> 00:30:32,399
blur x loops

00:30:30,240 --> 00:30:33,919
and after disambiguating variable names

00:30:32,399 --> 00:30:36,000
again

00:30:33,919 --> 00:30:37,360
like before only what is needed for the

00:30:36,000 --> 00:30:42,240
current computation

00:30:37,360 --> 00:30:42,240
of blur x will be computed in input 16.

00:30:42,880 --> 00:30:46,480
as a bonus halide's bounds inference

00:30:44,880 --> 00:30:48,559
engine is able to determine

00:30:46,480 --> 00:30:50,399
that fixed size allocations are

00:30:48,559 --> 00:30:52,880
appropriate for the intermediate buffers

00:30:50,399 --> 00:30:54,159
because the tile size is fixed it will

00:30:52,880 --> 00:30:55,919
mallock them at the start of the

00:30:54,159 --> 00:30:56,480
pipeline but because they are constant

00:30:55,919 --> 00:30:57,919
size

00:30:56,480 --> 00:31:00,399
you could ask for them to be placed on

00:30:57,919 --> 00:31:02,320
the stack with a special directive

00:31:00,399 --> 00:31:03,919
i'll let this play for a minute you

00:31:02,320 --> 00:31:05,679
should notice that the overall pattern

00:31:03,919 --> 00:31:08,080
of computation resembles the abstracted

00:31:05,679 --> 00:31:10,640
one we saw earlier on in the talk

00:31:08,080 --> 00:31:12,080
within each tile we go left to right and

00:31:10,640 --> 00:31:16,480
top to bottom by vectors

00:31:12,080 --> 00:31:16,480
and overall we process tiles in the same

00:31:16,840 --> 00:31:21,600
way

00:31:19,519 --> 00:31:24,799
this schedule is as fast as i could find

00:31:21,600 --> 00:31:27,039
for a single core on my workstation cpu

00:31:24,799 --> 00:31:29,440
it processes an image in 1.2

00:31:27,039 --> 00:31:32,799
milliseconds or 86 times faster than the

00:31:29,440 --> 00:31:35,840
worst schedule we saw

00:31:32,799 --> 00:31:35,840
let that finish up

00:31:37,200 --> 00:31:41,519
and now finally this is the fastest

00:31:40,000 --> 00:31:42,480
schedule that i could find for four

00:31:41,519 --> 00:31:45,200
cores

00:31:42,480 --> 00:31:46,000
it processes rows of tiles in parallel

00:31:45,200 --> 00:31:48,640
remember that

00:31:46,000 --> 00:31:50,399
the results y indexes tiles and this is

00:31:48,640 --> 00:31:54,240
a big optimization

00:31:50,399 --> 00:31:56,720
we also move the blur x computation

00:31:54,240 --> 00:31:57,600
closer to the usage site this improves

00:31:56,720 --> 00:31:59,600
locality

00:31:57,600 --> 00:32:01,120
at the cost of some more compute this

00:31:59,600 --> 00:32:02,000
one was a particularly small

00:32:01,120 --> 00:32:03,760
optimization

00:32:02,000 --> 00:32:05,360
but i think it's due to the multiple

00:32:03,760 --> 00:32:06,960
threads accessing the same memory at the

00:32:05,360 --> 00:32:09,120
tile boundaries

00:32:06,960 --> 00:32:11,519
and now we can process this image in a

00:32:09,120 --> 00:32:14,159
mere 0.4 milliseconds or

00:32:11,519 --> 00:32:14,960
fully 251 times faster than what we

00:32:14,159 --> 00:32:18,240
started with

00:32:14,960 --> 00:32:20,480
that's three orders of magnitude

00:32:18,240 --> 00:32:22,320
notice that this schedule makes use of

00:32:20,480 --> 00:32:24,240
every optimization we have discussed

00:32:22,320 --> 00:32:25,840
at the beginning of the talk it uses

00:32:24,240 --> 00:32:28,480
redundant recomputation

00:32:25,840 --> 00:32:30,159
both to avoid slow serial code paths and

00:32:28,480 --> 00:32:32,240
to break dependencies

00:32:30,159 --> 00:32:34,559
the independent tiles are computed in

00:32:32,240 --> 00:32:37,039
parallel and the access patterns within

00:32:34,559 --> 00:32:39,519
those tiles are cache friendly

00:32:37,039 --> 00:32:41,360
the reason this code runs so fast is

00:32:39,519 --> 00:32:42,960
because it's found a sweet spot

00:32:41,360 --> 00:32:45,519
in the three-way trade-off between

00:32:42,960 --> 00:32:47,360
compute locality and parallelism that's

00:32:45,519 --> 00:32:49,039
specific to my cpu

00:32:47,360 --> 00:32:51,600
it might be the case that a different

00:32:49,039 --> 00:32:53,919
schedule will be better on your cpu

00:32:51,600 --> 00:32:56,159
but this schedule code is only nine

00:32:53,919 --> 00:32:57,279
lines long that's not pseudo code on the

00:32:56,159 --> 00:32:58,799
left

00:32:57,279 --> 00:33:00,960
it isn't a large maintenance burden

00:32:58,799 --> 00:33:04,320
anymore to write many schedules each

00:33:00,960 --> 00:33:04,320
optimized for different hardware

00:33:07,200 --> 00:33:10,960
now finding these schedules is a bit of

00:33:09,360 --> 00:33:12,640
an art form

00:33:10,960 --> 00:33:14,240
and does require some expert knowledge

00:33:12,640 --> 00:33:17,200
to understand what is fast

00:33:14,240 --> 00:33:18,960
or slow about a schedule but a nice

00:33:17,200 --> 00:33:19,679
feature of having a decoupled scheduling

00:33:18,960 --> 00:33:22,000
language

00:33:19,679 --> 00:33:23,200
is that we can define a search space

00:33:22,000 --> 00:33:25,039
over those schedules

00:33:23,200 --> 00:33:27,120
and write programs called auto schedules

00:33:25,039 --> 00:33:28,960
to go through them automatically

00:33:27,120 --> 00:33:30,640
the current release of halide has three

00:33:28,960 --> 00:33:32,159
auto schedulers and they're all named

00:33:30,640 --> 00:33:35,279
after the academic papers

00:33:32,159 --> 00:33:37,120
that uh describe their algorithms one

00:33:35,279 --> 00:33:39,519
that's been around for a while

00:33:37,120 --> 00:33:41,919
is called mulipudi 2016 and it gives you

00:33:39,519 --> 00:33:43,279
a decent schedule very quickly

00:33:41,919 --> 00:33:44,799
it's good to use while you're still

00:33:43,279 --> 00:33:46,320
developing the algorithm and making sure

00:33:44,799 --> 00:33:49,519
that it's correct

00:33:46,320 --> 00:33:51,679
the next one lee 2018 is optimized for

00:33:49,519 --> 00:33:53,360
machine learning and optimization tasks

00:33:51,679 --> 00:33:55,039
where you use halide's automatic

00:33:53,360 --> 00:33:56,559
differentiation features to perform

00:33:55,039 --> 00:33:58,480
gradient descent

00:33:56,559 --> 00:34:00,000
this is a more advanced use case but

00:33:58,480 --> 00:34:01,600
it's also currently the only auto

00:34:00,000 --> 00:34:04,080
scheduler that searches for gpu

00:34:01,600 --> 00:34:06,720
schedules

00:34:04,080 --> 00:34:08,480
finally there's the atoms 2019 auto

00:34:06,720 --> 00:34:09,919
scheduler that uses deep learning to

00:34:08,480 --> 00:34:12,480
produce cpu schedules that are

00:34:09,919 --> 00:34:14,320
competitive with human experts in x86

00:34:12,480 --> 00:34:16,639
there's ongoing work to bring it to arm

00:34:14,320 --> 00:34:19,599
and gpus too

00:34:16,639 --> 00:34:20,000
on the blur pipeline adams 2019 finds a

00:34:19,599 --> 00:34:22,560
schedule

00:34:20,000 --> 00:34:24,720
that runs in 0.6 milliseconds instead of

00:34:22,560 --> 00:34:26,800
the ever so slightly better 0.4

00:34:24,720 --> 00:34:30,079
millisecond schedule i found manually

00:34:26,800 --> 00:34:32,079
but we got it for zero effort this isn't

00:34:30,079 --> 00:34:32,480
quite fair to the atoms auto scheduler

00:34:32,079 --> 00:34:34,079
either

00:34:32,480 --> 00:34:36,639
because this is just the first schedule

00:34:34,079 --> 00:34:38,800
it returned it can be run in a tuning

00:34:36,639 --> 00:34:41,200
or profile guided mode where it learns

00:34:38,800 --> 00:34:42,960
by observing your pipeline

00:34:41,200 --> 00:34:44,480
this enables you to trade significant

00:34:42,960 --> 00:34:45,119
compile time for better runtime

00:34:44,480 --> 00:34:47,040
performance

00:34:45,119 --> 00:34:48,879
but it's actually already competitive on

00:34:47,040 --> 00:34:51,119
its first attempt

00:34:48,879 --> 00:34:53,119
also i haven't analyzed this schedule in

00:34:51,119 --> 00:34:54,720
detail but i do notice that it uses

00:34:53,119 --> 00:34:57,520
three levels of tiling

00:34:54,720 --> 00:34:58,720
rather than the one that i used earlier

00:34:57,520 --> 00:35:00,880
in all likelihood

00:34:58,720 --> 00:35:02,720
this schedule will beat mine on a larger

00:35:00,880 --> 00:35:04,720
image because it will optimize the dram

00:35:02,720 --> 00:35:05,599
to l3 cache interface better a four

00:35:04,720 --> 00:35:07,839
megapixel

00:35:05,599 --> 00:35:09,040
image happens to fit just inside my l3

00:35:07,839 --> 00:35:11,119
cache

00:35:09,040 --> 00:35:12,240
it also makes uh better use of some

00:35:11,119 --> 00:35:13,839
scheduling features

00:35:12,240 --> 00:35:15,839
some that we don't have time to discuss

00:35:13,839 --> 00:35:17,760
like the manual loop unrolling directive

00:35:15,839 --> 00:35:21,680
or the store at directive which enables

00:35:17,760 --> 00:35:21,680
finer grained reuse distance trade-offs

00:35:23,760 --> 00:35:27,280
so we're a bit more than halfway through

00:35:25,280 --> 00:35:28,960
the talk and before we dive into

00:35:27,280 --> 00:35:32,240
halide's implementation let me pause

00:35:28,960 --> 00:35:32,240
again and answer some questions

00:35:33,760 --> 00:35:38,480
so does halide let users specify

00:35:37,680 --> 00:35:40,640
different ways

00:35:38,480 --> 00:35:42,160
eg precisions in which to perform

00:35:40,640 --> 00:35:42,960
intermediate fixed or floating point

00:35:42,160 --> 00:35:45,119
computations

00:35:42,960 --> 00:35:46,240
separately for the same kernel uh the

00:35:45,119 --> 00:35:49,440
short answer to that

00:35:46,240 --> 00:35:52,480
is yes because halide is staged

00:35:49,440 --> 00:35:54,880
in c plus plus it's possible to have

00:35:52,480 --> 00:35:56,720
a c plus program automatically generate

00:35:54,880 --> 00:35:58,320
the variance for the different types

00:35:56,720 --> 00:35:59,200
that you would want to use we have

00:35:58,320 --> 00:36:01,440
official

00:35:59,200 --> 00:36:04,240
api support for this that i'll describe

00:36:01,440 --> 00:36:04,240
in the next section

00:36:04,960 --> 00:36:09,599
another person asks can i plug in my own

00:36:07,440 --> 00:36:11,520
allocator for the intermediate buffers

00:36:09,599 --> 00:36:13,520
i'll describe the runtime in more detail

00:36:11,520 --> 00:36:14,320
in the next section but the short answer

00:36:13,520 --> 00:36:18,720
for that is yes

00:36:14,320 --> 00:36:21,280
you can another question

00:36:18,720 --> 00:36:21,760
is tile followed by parallel different

00:36:21,280 --> 00:36:24,240
than

00:36:21,760 --> 00:36:25,040
parallel followed by tile that is can

00:36:24,240 --> 00:36:27,760
you specify

00:36:25,040 --> 00:36:28,960
parallel either within or across tiles

00:36:27,760 --> 00:36:32,640
yes you can

00:36:28,960 --> 00:36:34,640
um parallel applies to a variable name

00:36:32,640 --> 00:36:35,680
and if you use tile to break it apart

00:36:34,640 --> 00:36:37,680
into more than one var

00:36:35,680 --> 00:36:39,040
into multiple variables and introduce

00:36:37,680 --> 00:36:41,839
new ones to your

00:36:39,040 --> 00:36:43,200
to your funk then you would have to call

00:36:41,839 --> 00:36:46,720
parallel on those

00:36:43,200 --> 00:36:48,640
tile um on those tiles variables later

00:36:46,720 --> 00:36:51,839
but if you if you're doing it on an

00:36:48,640 --> 00:36:51,839
earlier one it should just commute

00:36:52,079 --> 00:36:56,640
slide 53 you tried for four cores does

00:36:54,880 --> 00:36:57,119
that mean we can do it faster using more

00:36:56,640 --> 00:37:00,480
cores

00:36:57,119 --> 00:37:00,800
what about gpu support um i tested on

00:37:00,480 --> 00:37:03,280
four

00:37:00,800 --> 00:37:05,040
cores because i believe that testing on

00:37:03,280 --> 00:37:06,880
my ten core workstation wouldn't

00:37:05,040 --> 00:37:09,680
necessarily be believable

00:37:06,880 --> 00:37:10,640
um to the audience i'm sure it would run

00:37:09,680 --> 00:37:12,960
perfectly well

00:37:10,640 --> 00:37:15,359
on my 10 core workstation as long as i

00:37:12,960 --> 00:37:17,680
used a large enough image

00:37:15,359 --> 00:37:19,359
and regarding gpu support it is

00:37:17,680 --> 00:37:20,240
perfectly possible to write a gpu

00:37:19,359 --> 00:37:21,920
schedule for this

00:37:20,240 --> 00:37:24,480
i just didn't do it because i didn't

00:37:21,920 --> 00:37:27,440
want to assume that my audience had gpu

00:37:24,480 --> 00:37:27,440
um background

00:37:27,920 --> 00:37:30,800
i'll just wait one more minute if

00:37:29,200 --> 00:37:32,960
there's any last minute questions yes

00:37:30,800 --> 00:37:34,800
does the auto gpu scheduler consider

00:37:32,960 --> 00:37:35,839
upload and download time i believe that

00:37:34,800 --> 00:37:38,079
it does

00:37:35,839 --> 00:37:40,400
i'm not involved with the implementation

00:37:38,079 --> 00:37:43,200
of the lee 2018 auto scheduler

00:37:40,400 --> 00:37:45,440
but it's pretty well tuned to its use

00:37:43,200 --> 00:37:45,440
case

00:37:47,680 --> 00:37:51,760
once the atoms 2019 auto scheduler gets

00:37:50,000 --> 00:37:54,079
gpu features it should perform even

00:37:51,760 --> 00:37:54,079
better

00:37:58,079 --> 00:38:02,560
okay i think that's it for questions for

00:38:00,720 --> 00:38:04,720
now

00:38:02,560 --> 00:38:05,839
just one more second account for the

00:38:04,720 --> 00:38:08,079
delay

00:38:05,839 --> 00:38:08,079
cool

00:38:09,839 --> 00:38:14,800
so halide leans heavily on the llvm

00:38:13,200 --> 00:38:16,320
compiler infrastructure

00:38:14,800 --> 00:38:18,160
so you can expect all of the good

00:38:16,320 --> 00:38:21,040
research that's gone into clang

00:38:18,160 --> 00:38:22,960
to apply to halide code as well we take

00:38:21,040 --> 00:38:24,880
the halide algorithm and the schedule

00:38:22,960 --> 00:38:27,440
and combine those into a single function

00:38:24,880 --> 00:38:29,599
in our intermediate representation

00:38:27,440 --> 00:38:31,680
after some optimization passes this is

00:38:29,599 --> 00:38:34,800
translated to llvm's ir

00:38:31,680 --> 00:38:36,480
to compile to vectorized x86 or arm code

00:38:34,800 --> 00:38:38,400
or to graphs of cuda kernels

00:38:36,480 --> 00:38:40,960
and the cpu code required to launch and

00:38:38,400 --> 00:38:40,960
manage them

00:38:42,160 --> 00:38:46,400
the full picture is a bit messier we

00:38:44,560 --> 00:38:47,680
keep adding back ends not all of which

00:38:46,400 --> 00:38:49,040
go through llvm

00:38:47,680 --> 00:38:50,960
but at this point if there's a

00:38:49,040 --> 00:38:52,880
commercial architecture you can think of

00:38:50,960 --> 00:38:54,240
we either support it already or it would

00:38:52,880 --> 00:38:55,680
be easy to add support

00:38:54,240 --> 00:38:58,160
either through a custom back end to

00:38:55,680 --> 00:38:59,920
halide or by extending llvm

00:38:58,160 --> 00:39:04,000
most recently we've been working on risk

00:38:59,920 --> 00:39:04,000
5 and direct 3d 12 as back ends

00:39:05,440 --> 00:39:09,839
we've seen a good bit of the front end

00:39:07,440 --> 00:39:10,720
already the basic way that it works is

00:39:09,839 --> 00:39:12,960
that we overload

00:39:10,720 --> 00:39:14,000
operators to simplify the creation of

00:39:12,960 --> 00:39:16,800
syntax trees for

00:39:14,000 --> 00:39:17,760
specifying the algorithms because halide

00:39:16,800 --> 00:39:19,760
code doesn't run

00:39:17,760 --> 00:39:21,359
until you schedule it from the point of

00:39:19,760 --> 00:39:23,520
view of a halide algorithm

00:39:21,359 --> 00:39:24,960
the whole c plus language is const

00:39:23,520 --> 00:39:27,599
expert

00:39:24,960 --> 00:39:29,359
for loops in c plus g unrolled into

00:39:27,599 --> 00:39:31,280
halide syntax fragments

00:39:29,359 --> 00:39:32,960
if statements in c plus plus let you

00:39:31,280 --> 00:39:34,000
conditionally include code in the

00:39:32,960 --> 00:39:37,359
pipeline

00:39:34,000 --> 00:39:39,200
and so on and so forth using c

00:39:37,359 --> 00:39:40,880
plus as a host language means that we

00:39:39,200 --> 00:39:42,960
get all of the benefits of c plus plus

00:39:40,880 --> 00:39:44,720
for debugging halide programs

00:39:42,960 --> 00:39:46,079
this is a good sort of design for any

00:39:44,720 --> 00:39:48,079
domain-specific language

00:39:46,079 --> 00:39:50,560
because it avoids the chicken egg issue

00:39:48,079 --> 00:39:52,400
of tooling and developer communities

00:39:50,560 --> 00:39:54,320
developers don't want to use languages

00:39:52,400 --> 00:39:56,000
without good tools and no one wants to

00:39:54,320 --> 00:39:59,119
write tools for a language that nobody

00:39:56,000 --> 00:40:01,119
uses staging halide in c plus plus means

00:39:59,119 --> 00:40:02,480
that most of the time i get to work in c

00:40:01,119 --> 00:40:04,720
lion or visual studio

00:40:02,480 --> 00:40:08,560
and use the fantastic debugging code

00:40:04,720 --> 00:40:08,560
completion and static analysis tools

00:40:08,839 --> 00:40:13,040
there

00:40:10,880 --> 00:40:13,920
once you've scheduled your pipelines we

00:40:13,040 --> 00:40:16,480
translate it to an

00:40:13,920 --> 00:40:18,240
internal uh representation which is

00:40:16,480 --> 00:40:19,520
suitable for bounds inference

00:40:18,240 --> 00:40:21,599
and then we apply some more

00:40:19,520 --> 00:40:24,240
domain-specific optimizations before

00:40:21,599 --> 00:40:26,560
moving on to llvm

00:40:24,240 --> 00:40:28,400
the halide library contains llvm inside

00:40:26,560 --> 00:40:30,240
it so you don't need to install it or

00:40:28,400 --> 00:40:33,280
deploy it yourself

00:40:30,240 --> 00:40:35,200
we then either jit compile excuse me or

00:40:33,280 --> 00:40:36,640
output an object file and header for

00:40:35,200 --> 00:40:38,800
future linking

00:40:36,640 --> 00:40:40,400
because we're using llvm halide

00:40:38,800 --> 00:40:42,160
pipelines are already compatible with

00:40:40,400 --> 00:40:43,839
all of the popular sanitizers

00:40:42,160 --> 00:40:46,480
including thread memory and address

00:40:43,839 --> 00:40:46,480
sanitizers

00:40:47,440 --> 00:40:51,440
so let's talk a bit about jit mode llvm

00:40:50,480 --> 00:40:54,480
has a very robust

00:40:51,440 --> 00:40:56,560
jip module called mcgit that we use it's

00:40:54,480 --> 00:40:58,800
cross-platform and well-tested

00:40:56,560 --> 00:41:00,640
but jip mode is mostly useful when the

00:40:58,800 --> 00:41:01,280
structure of your pipeline might vary at

00:41:00,640 --> 00:41:03,920
run time

00:41:01,280 --> 00:41:06,400
or perhaps for rapid development this is

00:41:03,920 --> 00:41:07,119
because llvm's optimization passes are

00:41:06,400 --> 00:41:08,400
slow

00:41:07,119 --> 00:41:10,960
and this introduces a good bit of

00:41:08,400 --> 00:41:12,079
latency we have a caching mechanism that

00:41:10,960 --> 00:41:14,079
offsets this a bit

00:41:12,079 --> 00:41:15,920
but if you don't need dynamic pipelines

00:41:14,079 --> 00:41:16,960
then the ahead of time compilation mode

00:41:15,920 --> 00:41:20,800
i'll discuss in a minute

00:41:16,960 --> 00:41:23,359
might better suit your needs

00:41:20,800 --> 00:41:24,720
so this is a fully working halide

00:41:23,359 --> 00:41:25,839
program for the blur we looked at

00:41:24,720 --> 00:41:27,599
earlier

00:41:25,839 --> 00:41:29,359
i left out the schedule to fit it on the

00:41:27,599 --> 00:41:31,119
slide but this should give you a sense

00:41:29,359 --> 00:41:32,560
of how little code is needed to get

00:41:31,119 --> 00:41:35,040
something working

00:41:32,560 --> 00:41:36,640
you really do just include a few headers

00:41:35,040 --> 00:41:38,880
check your command line arguments and

00:41:36,640 --> 00:41:40,319
then get off to the races

00:41:38,880 --> 00:41:42,640
there's some code in here for loading

00:41:40,319 --> 00:41:44,560
and converting images halide supports a

00:41:42,640 --> 00:41:48,240
wide variety of image formats

00:41:44,560 --> 00:41:50,640
including png jpeg tiff and even matlab

00:41:48,240 --> 00:41:52,720
and i think numpy arrays

00:41:50,640 --> 00:41:54,079
then most of the rest of the code is the

00:41:52,720 --> 00:41:55,599
same as we saw before

00:41:54,079 --> 00:41:57,280
you define the variables that you're

00:41:55,599 --> 00:41:57,920
going to use the function you're going

00:41:57,280 --> 00:42:00,240
to use

00:41:57,920 --> 00:42:03,839
define those and then i'll just give you

00:42:00,240 --> 00:42:03,839
a chance to take all this in

00:42:06,960 --> 00:42:10,960
the one point that we haven't seen

00:42:09,119 --> 00:42:12,800
before and that i wanted to call out

00:42:10,960 --> 00:42:14,319
is this bit of code that actually runs

00:42:12,800 --> 00:42:17,760
the pipeline

00:42:14,319 --> 00:42:19,119
first we call compile jit to invoke llvm

00:42:17,760 --> 00:42:21,040
but it's on the second line that we

00:42:19,119 --> 00:42:23,520
actually run the pipeline

00:42:21,040 --> 00:42:25,280
the realize function takes a window of

00:42:23,520 --> 00:42:27,200
the output to compute

00:42:25,280 --> 00:42:28,400
halide pipelines have a demand driven

00:42:27,200 --> 00:42:30,480
execution model

00:42:28,400 --> 00:42:31,839
you'll notice that we didn't explicitly

00:42:30,480 --> 00:42:34,240
pass the input func

00:42:31,839 --> 00:42:35,760
to the result func but instead built the

00:42:34,240 --> 00:42:38,079
whole pipeline around it

00:42:35,760 --> 00:42:39,760
and then just used its size information

00:42:38,079 --> 00:42:44,240
to determine what window of the result

00:42:39,760 --> 00:42:46,560
to compute

00:42:44,240 --> 00:42:47,440
so if you don't need to pay the jit

00:42:46,560 --> 00:42:48,800
overhead

00:42:47,440 --> 00:42:51,119
you can use the ahead of time

00:42:48,800 --> 00:42:53,040
compilation mode conceptually

00:42:51,119 --> 00:42:55,520
this is more like a traditional compiler

00:42:53,040 --> 00:42:57,839
invocation but since halide is just c

00:42:55,520 --> 00:42:59,920
plus and has llvm inside of it you have

00:42:57,839 --> 00:43:02,480
to do it in two stages

00:42:59,920 --> 00:43:04,400
first you write a generator subclass and

00:43:02,480 --> 00:43:05,200
link to a special main function that we

00:43:04,400 --> 00:43:07,200
provide

00:43:05,200 --> 00:43:10,160
this is a fairly similar setup to google

00:43:07,200 --> 00:43:11,119
test this gives you a generator binary

00:43:10,160 --> 00:43:14,000
that you run

00:43:11,119 --> 00:43:14,960
to get an object file and a header that

00:43:14,000 --> 00:43:16,960
object file

00:43:14,960 --> 00:43:18,640
can be linked to your final application

00:43:16,960 --> 00:43:20,480
included in a static library

00:43:18,640 --> 00:43:22,400
or linked to a generic runner that

00:43:20,480 --> 00:43:24,400
sniffs metadata about your pipeline to

00:43:22,400 --> 00:43:27,040
aid in benchmarking and testing

00:43:24,400 --> 00:43:28,079
the object files use the plane c abi so

00:43:27,040 --> 00:43:32,640
as not to run into

00:43:28,079 --> 00:43:32,640
c plus api incompatibility concerns

00:43:33,599 --> 00:43:37,040
the generator code for this pipeline is

00:43:36,079 --> 00:43:39,040
even shorter

00:43:37,040 --> 00:43:40,560
because it can exclude the image loading

00:43:39,040 --> 00:43:43,119
and saving boiler plate that's provided

00:43:40,560 --> 00:43:45,440
for us by the generic runner

00:43:43,119 --> 00:43:47,119
again there's very little magic here we

00:43:45,440 --> 00:43:48,800
have to name the input and output

00:43:47,119 --> 00:43:49,920
function explicitly as fields of the

00:43:48,800 --> 00:43:52,079
generator

00:43:49,920 --> 00:43:54,079
there's some c plus template magic going

00:43:52,079 --> 00:43:56,240
on here that allows the generator base

00:43:54,079 --> 00:43:58,560
class to locate those variables

00:43:56,240 --> 00:44:00,240
notice that the struct blur is passed as

00:43:58,560 --> 00:44:01,599
a template argument to generator from

00:44:00,240 --> 00:44:03,520
which it derives

00:44:01,599 --> 00:44:04,640
this is called the curiously recurrent

00:44:03,520 --> 00:44:07,280
template pattern

00:44:04,640 --> 00:44:08,640
and it's really really cool especially

00:44:07,280 --> 00:44:09,440
for implementing domain-specific

00:44:08,640 --> 00:44:11,520
languages

00:44:09,440 --> 00:44:13,680
if you haven't seen it before definitely

00:44:11,520 --> 00:44:15,680
google it after my talk

00:44:13,680 --> 00:44:17,599
the only other bit of magic here is the

00:44:15,680 --> 00:44:19,440
halide register generator macro

00:44:17,599 --> 00:44:20,640
that communicates the class name and the

00:44:19,440 --> 00:44:22,560
intended cabi

00:44:20,640 --> 00:44:24,000
function name to the special main

00:44:22,560 --> 00:44:25,359
function that you link against

00:44:24,000 --> 00:44:27,040
so i'll just give you one more second to

00:44:25,359 --> 00:44:29,200
look at this

00:44:27,040 --> 00:44:30,640
generators can also take command line

00:44:29,200 --> 00:44:32,720
parameters

00:44:30,640 --> 00:44:33,920
from the generator binary to produce

00:44:32,720 --> 00:44:36,160
variants

00:44:33,920 --> 00:44:37,119
so you can put if statements in your

00:44:36,160 --> 00:44:39,119
generate

00:44:37,119 --> 00:44:40,319
function here that read those parameters

00:44:39,119 --> 00:44:41,520
to generate different

00:44:40,319 --> 00:44:44,000
versions of pipelines so that should

00:44:41,520 --> 00:44:48,480
address the question that came earlier

00:44:44,000 --> 00:44:48,480
about using different types in the

00:44:48,839 --> 00:44:52,640
middle

00:44:50,960 --> 00:44:54,079
now from a build perspective there's

00:44:52,640 --> 00:44:55,359
nothing up our sleeves

00:44:54,079 --> 00:44:57,119
you just need to step through and

00:44:55,359 --> 00:44:58,079
compile the special main and the

00:44:57,119 --> 00:45:00,480
generator

00:44:58,079 --> 00:45:01,280
link them together into halide run that

00:45:00,480 --> 00:45:02,960
program

00:45:01,280 --> 00:45:04,560
and then link the resulting registration

00:45:02,960 --> 00:45:06,560
file to our special runner

00:45:04,560 --> 00:45:08,640
these commands are for a gcc compatible

00:45:06,560 --> 00:45:10,400
compiler but the process is essentially

00:45:08,640 --> 00:45:12,800
the same for msbc

00:45:10,400 --> 00:45:14,640
imagine that the environment variable hl

00:45:12,800 --> 00:45:15,280
root points to the halide installation

00:45:14,640 --> 00:45:18,640
path

00:45:15,280 --> 00:45:20,720
for this

00:45:18,640 --> 00:45:22,720
the only real pain point here in most

00:45:20,720 --> 00:45:24,160
build systems is that you need to run a

00:45:22,720 --> 00:45:26,400
binary that you built

00:45:24,160 --> 00:45:27,920
during your build to get a binary that

00:45:26,400 --> 00:45:29,359
you'll eventually link against during

00:45:27,920 --> 00:45:31,280
the same build

00:45:29,359 --> 00:45:33,359
also since halide is a natural cross

00:45:31,280 --> 00:45:34,720
compiler we have to specify the target

00:45:33,359 --> 00:45:36,400
to be the same as the host

00:45:34,720 --> 00:45:38,079
but we could also put a different target

00:45:36,400 --> 00:45:40,560
here we support cross

00:45:38,079 --> 00:45:41,440
os and cross architecture builds you can

00:45:40,560 --> 00:45:43,839
compile two

00:45:41,440 --> 00:45:46,079
arm from x86 or vice versa you can

00:45:43,839 --> 00:45:47,760
compile to linux from windows too

00:45:46,079 --> 00:45:49,520
your machines don't need to have cuda

00:45:47,760 --> 00:45:53,119
installed or even a gpu

00:45:49,520 --> 00:45:56,240
to generate cuda code and so on

00:45:53,119 --> 00:45:58,000
now this is not a cmake talk

00:45:56,240 --> 00:45:59,920
but if that sounded scary i do want to

00:45:58,000 --> 00:46:01,359
put your mind at ease i've personally

00:45:59,920 --> 00:46:03,119
put a lot of effort

00:46:01,359 --> 00:46:05,520
into making the build story as seamless

00:46:03,119 --> 00:46:06,079
as possible halide itself is built with

00:46:05,520 --> 00:46:08,560
cmake

00:46:06,079 --> 00:46:10,319
and we provide a c make package too

00:46:08,560 --> 00:46:10,720
generators are still a bit of a square

00:46:10,319 --> 00:46:12,880
peg

00:46:10,720 --> 00:46:14,000
in a round hole for cmake but for most

00:46:12,880 --> 00:46:17,599
common cases it's quite

00:46:14,000 --> 00:46:19,359
easy all you do is create a generator

00:46:17,599 --> 00:46:21,119
executable by linking the halide

00:46:19,359 --> 00:46:22,560
generator target to your sources in the

00:46:21,119 --> 00:46:25,680
usual way

00:46:22,560 --> 00:46:27,119
then you call our special uh add halide

00:46:25,680 --> 00:46:29,440
library function that our package

00:46:27,119 --> 00:46:32,000
supplies it will create a static library

00:46:29,440 --> 00:46:33,760
for you from your generator executable

00:46:32,000 --> 00:46:35,280
it has a wide variety of options for

00:46:33,760 --> 00:46:37,040
more advanced build scenarios

00:46:35,280 --> 00:46:38,560
including running an auto scheduler on

00:46:37,040 --> 00:46:42,160
your pipeline but

00:46:38,560 --> 00:46:45,200
this is all you need for standard cases

00:46:42,160 --> 00:46:46,560
finally we we link the generated blur

00:46:45,200 --> 00:46:48,319
library to the runner

00:46:46,560 --> 00:46:50,079
the add halide library function

00:46:48,319 --> 00:46:51,520
populates a variable with a path to the

00:46:50,079 --> 00:46:52,560
generated registration code for the

00:46:51,520 --> 00:46:54,000
runner

00:46:52,560 --> 00:46:55,520
and then you just invoke cmake in the

00:46:54,000 --> 00:46:58,160
normal way and point it to your haloid

00:46:55,520 --> 00:47:01,280
installation directory

00:46:58,160 --> 00:47:02,800
finally uh the last bit of halide's

00:47:01,280 --> 00:47:05,520
engineering that i want to talk about

00:47:02,800 --> 00:47:06,160
is its run time many domain specific

00:47:05,520 --> 00:47:08,240
languages

00:47:06,160 --> 00:47:09,359
have heavy runtime requirements garbage

00:47:08,240 --> 00:47:11,760
collectors and the like

00:47:09,359 --> 00:47:13,359
halide has none of that it simply

00:47:11,760 --> 00:47:14,880
requires malloc and a threading

00:47:13,359 --> 00:47:16,800
implementation

00:47:14,880 --> 00:47:18,240
all the pieces of the runtime can be

00:47:16,800 --> 00:47:20,160
overridden bit by bit

00:47:18,240 --> 00:47:22,079
either through wink either through weak

00:47:20,160 --> 00:47:24,160
linking on supported platforms

00:47:22,079 --> 00:47:26,319
or explicitly calling a function that

00:47:24,160 --> 00:47:28,079
overwrites a function pointer

00:47:26,319 --> 00:47:29,920
the malloc that it uses is a system one

00:47:28,079 --> 00:47:32,319
by default but it can be replaced by

00:47:29,920 --> 00:47:33,680
your own implementation if you so choose

00:47:32,319 --> 00:47:36,319
for each operating system that we

00:47:33,680 --> 00:47:36,720
support we provide a highly tuned thread

00:47:36,319 --> 00:47:38,079
pool

00:47:36,720 --> 00:47:39,359
let's optimize for the types of

00:47:38,079 --> 00:47:41,040
contention patterns that halide

00:47:39,359 --> 00:47:42,960
pipelines encounter

00:47:41,040 --> 00:47:44,559
clients have tried to replace it with

00:47:42,960 --> 00:47:45,760
for example intel's thread building

00:47:44,559 --> 00:47:47,200
blocks thread pool

00:47:45,760 --> 00:47:49,520
but so far we haven't encountered a

00:47:47,200 --> 00:47:51,520
faster one finally

00:47:49,520 --> 00:47:53,119
the runtime is sufficiently minimal that

00:47:51,520 --> 00:47:54,160
it is used in industry in embedded

00:47:53,119 --> 00:47:55,920
scenarios

00:47:54,160 --> 00:47:57,599
qualcomm actually provides an older

00:47:55,920 --> 00:47:59,440
version of halide as part of their

00:47:57,599 --> 00:48:02,160
official hexagon dsp software

00:47:59,440 --> 00:48:02,160
development kit

00:48:03,280 --> 00:48:07,920
and that about wraps things up i

00:48:06,319 --> 00:48:09,599
hope you take away these things from my

00:48:07,920 --> 00:48:11,599
talk though first

00:48:09,599 --> 00:48:12,880
performance in numerical pipelines is a

00:48:11,599 --> 00:48:15,200
complex trade-off

00:48:12,880 --> 00:48:17,839
between parallelism locality and

00:48:15,200 --> 00:48:19,839
redundant recomputation

00:48:17,839 --> 00:48:21,440
performance is counterintuitive and to

00:48:19,839 --> 00:48:22,720
achieve maximum performance

00:48:21,440 --> 00:48:24,400
you'll have to try many different

00:48:22,720 --> 00:48:27,119
strategies most of which are either

00:48:24,400 --> 00:48:28,960
pointless or pessimizing

00:48:27,119 --> 00:48:30,720
i also hope you learned about some of

00:48:28,960 --> 00:48:32,079
the benefits halide provides

00:48:30,720 --> 00:48:34,000
by separating algorithms from their

00:48:32,079 --> 00:48:34,720
schedules halide code is easier to

00:48:34,000 --> 00:48:36,960
maintain

00:48:34,720 --> 00:48:39,440
faster to iterate on and more portable

00:48:36,960 --> 00:48:41,599
than manually optimizing c plus plus

00:48:39,440 --> 00:48:44,000
also just this week we released halide

00:48:41,599 --> 00:48:45,920
10 which includes the atoms 2019

00:48:44,000 --> 00:48:48,800
auto scheduler i showed earlier i do

00:48:45,920 --> 00:48:51,040
hope you give halide a try

00:48:48,800 --> 00:48:53,160
um thanks so much for listening to my

00:48:51,040 --> 00:48:54,400
talk you can learn more at

00:48:53,160 --> 00:48:57,119
halidelang.org

00:48:54,400 --> 00:48:58,800
reach me via email at alex underscore

00:48:57,119 --> 00:49:00,240
ranking berkeley.edu

00:48:58,800 --> 00:49:02,559
or on linkedin or twitter where my

00:49:00,240 --> 00:49:04,960
username is just alex reinking on both

00:49:02,559 --> 00:49:06,240
the same is true of github 2 actually

00:49:04,960 --> 00:49:08,480
i'll use the remaining time

00:49:06,240 --> 00:49:09,920
to answer questions but afterwards i'll

00:49:08,480 --> 00:49:12,720
be hanging out in the hallway track

00:49:09,920 --> 00:49:14,160
ideally i'd like to get floor 4 table 2

00:49:12,720 --> 00:49:16,800
but i'll post the exact table in the

00:49:14,160 --> 00:49:20,160
slack when i get there and thanks again

00:49:16,800 --> 00:49:23,760
okay so i'm seeing

00:49:20,160 --> 00:49:25,680
a few questions here so the first one is

00:49:23,760 --> 00:49:27,440
many industry inspection machines snap

00:49:25,680 --> 00:49:29,200
high resolution pictures even as big as

00:49:27,440 --> 00:49:31,119
20 megabytes for a single picture

00:49:29,200 --> 00:49:32,960
is it possible to take advantage of cash

00:49:31,119 --> 00:49:35,359
for such cases the answer is

00:49:32,960 --> 00:49:38,880
absolutely yes and the bigger your image

00:49:35,359 --> 00:49:42,240
is the more important it gets

00:49:38,880 --> 00:49:44,640
another question i see is is halide oh

00:49:42,240 --> 00:49:45,760
uh got an upvote sorry halite is written

00:49:44,640 --> 00:49:48,079
with images in mind

00:49:45,760 --> 00:49:50,880
but can it be used with other 2d data or

00:49:48,079 --> 00:49:52,800
is it a square peg in a circular hole

00:49:50,880 --> 00:49:54,559
halide was originally written with

00:49:52,800 --> 00:49:55,839
images in mind but it has since crossed

00:49:54,559 --> 00:49:58,880
over to the machine learning

00:49:55,839 --> 00:49:59,599
and scientific computing domains too a

00:49:58,880 --> 00:50:02,319
domain

00:49:59,599 --> 00:50:03,440
is a sort of fuzzy concept uh in

00:50:02,319 --> 00:50:05,760
computer science

00:50:03,440 --> 00:50:06,880
what halide really does is it optimizes

00:50:05,760 --> 00:50:09,440
computations over

00:50:06,880 --> 00:50:10,880
dense multi-dimensional arrays so if

00:50:09,440 --> 00:50:13,119
your

00:50:10,880 --> 00:50:13,920
program fits into that domain more

00:50:13,119 --> 00:50:15,760
likely than not

00:50:13,920 --> 00:50:18,000
halide will be a good fit for some part

00:50:15,760 --> 00:50:18,000
of it

00:50:18,880 --> 00:50:22,160
i get another question is halide used

00:50:20,880 --> 00:50:23,920
for commercial cases

00:50:22,160 --> 00:50:25,359
if yes what happens if there is any

00:50:23,920 --> 00:50:27,040
issue in code that results in a

00:50:25,359 --> 00:50:30,559
commercial loss

00:50:27,040 --> 00:50:31,760
um i'm not sure what that question

00:50:30,559 --> 00:50:34,720
means so what would you mean by

00:50:31,760 --> 00:50:38,160
commercial cases um

00:50:34,720 --> 00:50:40,160
halide is used in many devices that are

00:50:38,160 --> 00:50:41,839
put into consumers hands including all

00:50:40,160 --> 00:50:45,280
of the google pixel phones

00:50:41,839 --> 00:50:46,800
uh i'm not sure what happens if there's

00:50:45,280 --> 00:50:47,280
any issue in code that results in a

00:50:46,800 --> 00:50:48,559
commercial

00:50:47,280 --> 00:50:50,319
i guess i'm not sure what commercial

00:50:48,559 --> 00:50:52,960
means for this question sorry

00:50:50,319 --> 00:50:53,839
um any specific tutorial you recommend

00:50:52,960 --> 00:50:55,559
to learn halide

00:50:53,839 --> 00:50:57,359
yes there is an official one on

00:50:55,559 --> 00:51:00,319
halidlang.org there's a link to the

00:50:57,359 --> 00:51:02,480
tutorial at the top of the page

00:51:00,319 --> 00:51:03,920
uh finally what are the best resources

00:51:02,480 --> 00:51:05,599
to learn more about the specific

00:51:03,920 --> 00:51:06,160
optimizations one can incorporate into

00:51:05,599 --> 00:51:09,200
halide

00:51:06,160 --> 00:51:10,319
the expert's knowledge you mentioned so

00:51:09,200 --> 00:51:13,760
our doxygen

00:51:10,319 --> 00:51:16,559
um document contains most of the

00:51:13,760 --> 00:51:18,720
uh optimizations that are actually all

00:51:16,559 --> 00:51:20,240
of the optimizations that halide exposes

00:51:18,720 --> 00:51:22,079
and those are good enough to get

00:51:20,240 --> 00:51:23,680
state-of-the-art performance on a wide

00:51:22,079 --> 00:51:26,319
variety of pipelines

00:51:23,680 --> 00:51:27,839
um what it really comes down to is that

00:51:26,319 --> 00:51:28,960
trade-off that i talked about earlier

00:51:27,839 --> 00:51:32,240
between compute

00:51:28,960 --> 00:51:35,119
parallelism and locality llvm

00:51:32,240 --> 00:51:36,160
and most compilers are so good at what

00:51:35,119 --> 00:51:39,280
they do

00:51:36,160 --> 00:51:42,160
that time basically

00:51:39,280 --> 00:51:44,400
your time is best spent reasoning about

00:51:42,160 --> 00:51:48,559
the sorts of global reorganizations that

00:51:44,400 --> 00:51:49,680
halide uh helps you helps you make um

00:51:48,559 --> 00:51:51,599
that that's just kind of the state of

00:51:49,680 --> 00:51:53,599
things

00:51:51,599 --> 00:51:55,440
so for production use cases are the

00:51:53,599 --> 00:51:57,040
generated binary shipped devices or are

00:51:55,440 --> 00:52:00,480
they generated on devices

00:51:57,040 --> 00:52:02,720
typically the aot compiled haline uh

00:52:00,480 --> 00:52:03,520
binaries are shipped in or shipped to

00:52:02,720 --> 00:52:05,920
the devices

00:52:03,520 --> 00:52:06,880
you don't typically deploy the whole lib

00:52:05,920 --> 00:52:08,640
halide

00:52:06,880 --> 00:52:10,079
to uh embedded device you would just

00:52:08,640 --> 00:52:12,640
deploy the small

00:52:10,079 --> 00:52:14,720
uh cabi object file that you generated

00:52:12,640 --> 00:52:16,319
on your build machine

00:52:14,720 --> 00:52:19,839
is c plus plus the best language to

00:52:16,319 --> 00:52:19,839
define the pipeline absolutely

00:52:21,040 --> 00:52:25,839
of course it is um

00:52:24,319 --> 00:52:28,960
i mean what language would we want to

00:52:25,839 --> 00:52:31,520
use besides c plus

00:52:28,960 --> 00:52:32,720
that said we do have python bindings so

00:52:31,520 --> 00:52:34,480
if you're in a

00:52:32,720 --> 00:52:35,680
like a machine learning uh scenario

00:52:34,480 --> 00:52:37,280
where you really want to integrate

00:52:35,680 --> 00:52:39,119
deeply with uh numpy

00:52:37,280 --> 00:52:40,960
or scikit-learn or something like that

00:52:39,119 --> 00:52:42,400
then you can use halide from python

00:52:40,960 --> 00:52:43,920
that might be one reason you wouldn't

00:52:42,400 --> 00:52:44,839
want to use the perfect language that is

00:52:43,920 --> 00:52:47,839
c

00:52:44,839 --> 00:52:47,839
plus

00:52:48,160 --> 00:52:52,720
how easy is it to use halide from say c

00:52:50,960 --> 00:52:54,559
is this a supported use case i guess you

00:52:52,720 --> 00:52:57,599
mentioned python now already

00:52:54,559 --> 00:53:01,040
yes um you can't define

00:52:57,599 --> 00:53:03,280
and compile a halide pipeline from c

00:53:01,040 --> 00:53:05,599
but you can absolutely use a generated

00:53:03,280 --> 00:53:07,760
halide pipeline in c

00:53:05,599 --> 00:53:09,839
also we have an output mode in the

00:53:07,760 --> 00:53:12,400
generator that generates plane

00:53:09,839 --> 00:53:13,359
c code and this is for use when we don't

00:53:12,400 --> 00:53:15,440
have a

00:53:13,359 --> 00:53:16,800
back end available and you want to use

00:53:15,440 --> 00:53:19,280
some proprietary

00:53:16,800 --> 00:53:21,599
uh c compiler for some proprietary

00:53:19,280 --> 00:53:21,599
target

00:53:22,880 --> 00:53:29,839
and that is actually also available from

00:53:24,559 --> 00:53:29,839
cmake that is a supported scenario

00:53:31,040 --> 00:53:35,359
of course halide is open source it's on

00:53:33,200 --> 00:53:36,160
github and it's mit licensed oh sorry

00:53:35,359 --> 00:53:39,040
the question was

00:53:36,160 --> 00:53:39,520
can we contribute back to halide we very

00:53:39,040 --> 00:53:41,760
much

00:53:39,520 --> 00:53:44,240
uh welcome and encourage uh pull

00:53:41,760 --> 00:53:44,240
requests

00:53:44,559 --> 00:53:47,839
show us weird algorithms please i'm

00:53:46,400 --> 00:53:49,680
sorry i don't have bonus slides for

00:53:47,839 --> 00:53:50,640
weird algorithms but you can find a

00:53:49,680 --> 00:53:53,280
whole host of

00:53:50,640 --> 00:53:55,040
full complete halide applications inside

00:53:53,280 --> 00:53:56,000
our git repository under the top level

00:53:55,040 --> 00:53:57,920
apps folder

00:53:56,000 --> 00:53:59,040
you'll find our are faster than

00:53:57,920 --> 00:54:01,440
tensorflow uh

00:53:59,040 --> 00:54:03,680
depth wise separable convolution layer

00:54:01,440 --> 00:54:03,680
there

00:54:04,240 --> 00:54:07,920
about memory allocation can i optimize

00:54:06,480 --> 00:54:10,319
for memory size

00:54:07,920 --> 00:54:11,200
yes the way that you do that is by

00:54:10,319 --> 00:54:14,640
controlling

00:54:11,200 --> 00:54:16,880
um basically the compute at locations

00:54:14,640 --> 00:54:18,319
the tiling sizes and then we also have

00:54:16,880 --> 00:54:18,960
this feature called the store at

00:54:18,319 --> 00:54:20,640
location

00:54:18,960 --> 00:54:22,960
so if you're willing to trade off more

00:54:20,640 --> 00:54:24,720
memory you can set a store location

00:54:22,960 --> 00:54:26,319
higher than the compute location and

00:54:24,720 --> 00:54:28,160
that will keep those values around

00:54:26,319 --> 00:54:30,319
longer as opposed to just every time it

00:54:28,160 --> 00:54:32,160
hits the compute loop

00:54:30,319 --> 00:54:33,680
does it work with consumer grade cpus

00:54:32,160 --> 00:54:37,040
like nvidia gtx

00:54:33,680 --> 00:54:38,640
1660 ti gtx 1070 and those

00:54:37,040 --> 00:54:41,839
i personally develop halide on a

00:54:38,640 --> 00:54:43,760
workstation with two gtx 1080 ti's in it

00:54:41,839 --> 00:54:45,839
so yeah absolutely our build

00:54:43,760 --> 00:54:49,359
infrastructure uses actually some older

00:54:45,839 --> 00:54:49,359
consumer grade gtx chips

00:54:49,520 --> 00:54:53,440
i think fortran is known for taking

00:54:51,520 --> 00:54:55,760
advantage of stronger aliasing stuff

00:54:53,440 --> 00:54:56,640
and llvm less so because cnc plus does

00:54:55,760 --> 00:54:59,760
not have those

00:54:56,640 --> 00:55:00,559
how does halide deal with that so i'm

00:54:59,760 --> 00:55:02,240
not

00:55:00,559 --> 00:55:03,839
too familiar with a background code but

00:55:02,240 --> 00:55:06,160
from what i understand llvm

00:55:03,839 --> 00:55:07,119
actually does provide ways for you to

00:55:06,160 --> 00:55:10,559
inform

00:55:07,119 --> 00:55:11,440
uh the inform its code generator and its

00:55:10,559 --> 00:55:14,799
optimizer

00:55:11,440 --> 00:55:18,000
about um aliasing restrictions

00:55:14,799 --> 00:55:21,200
uh in the ir itself so it's actually

00:55:18,000 --> 00:55:22,720
easier to get those types of uh to get

00:55:21,200 --> 00:55:24,559
those types of optimizations

00:55:22,720 --> 00:55:28,079
by going through llvm directly than it

00:55:24,559 --> 00:55:28,079
is through standard sequels plus code

00:55:28,640 --> 00:55:32,880
um are super computing centers planning

00:55:31,119 --> 00:55:35,440
on deploying halide installs

00:55:32,880 --> 00:55:36,799
if so are they worried potentially

00:55:35,440 --> 00:55:38,640
irrationally about just-in-time

00:55:36,799 --> 00:55:40,240
compilation

00:55:38,640 --> 00:55:41,920
i am not personally involved in any

00:55:40,240 --> 00:55:43,440
talks with super computing centers but i

00:55:41,920 --> 00:55:45,280
would love to see it deployed

00:55:43,440 --> 00:55:46,480
i do know that people who do scientific

00:55:45,280 --> 00:55:48,559
computing will

00:55:46,480 --> 00:55:49,599
load the halide binaries themselves and

00:55:48,559 --> 00:55:53,520
generate

00:55:49,599 --> 00:55:55,760
aot pipelines and deploy the generated

00:55:53,520 --> 00:55:57,040
object files to their super computing

00:55:55,760 --> 00:55:59,520
jobs

00:55:57,040 --> 00:56:03,119
i've personally done that as part of a

00:55:59,520 --> 00:56:05,839
class i ta

00:56:03,119 --> 00:56:05,839
but yeah

00:56:07,440 --> 00:56:10,640
well it looks like we have a one minute

00:56:09,119 --> 00:56:11,839
left so i

00:56:10,640 --> 00:56:13,839
want to just thank you all again for

00:56:11,839 --> 00:56:15,680
listening to my talk engaging uh

00:56:13,839 --> 00:56:17,280
engaging with me here and hopefully i'll

00:56:15,680 --> 00:56:31,839
catch some more of you

00:56:17,280 --> 00:56:31,839
out of the conference

00:56:38,720 --> 00:56:40,799

YouTube URL: https://www.youtube.com/watch?v=1ir_nEfKQ7A


