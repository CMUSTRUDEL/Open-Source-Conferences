Title: Berlin Buzzwords 2018: Robert Rodger â€“ Learned Indexes: a New Idea for Efficient Data Access #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Indexes are what make efficient access for our data storage systems possible. Though traditionally implemented with highly-optimized tree-based data structures, this past December a group from Google proposed a novel idea: replace certain types of index structures with trained machine learning algorithms. 

After all, an index is nothing other than a model that maps a key to the position of a record; in this light, exchanging, say, your B-tree search with a deep neural network prediction seems at least possible, if not practical. Surprisingly, doing so can often lead to significant performance improvements, in terms of both time and memory consumption.

In this talk we discuss how learned indexes accomplish this. We focus on neural networks, and in particular how recent trends in processor architecture design make them computationally competitive against tree search. We then have a look at how machine learning algorithms can be applied to the task of range indexation, how they can deliver error bound guarantees, and how their accuracy can be honed by layering them recursively. We finish with a review of the Google group's results on three realistic datasets and a brief mention of how machine learning can be applied to other indexation tasks.

Read more:
https://2018.berlinbuzzwords.de/18/session/learned-indexes-new-idea-efficient-data-access

About Robert Rodger:
https://2018.berlinbuzzwords.de/users/robert-rodger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,670 --> 00:00:11,280
hello everyone and thanks for coming to

00:00:07,950 --> 00:00:11,639
my talk my name is Thomas ed is Robert

00:00:11,280 --> 00:00:15,180
Roger

00:00:11,639 --> 00:00:18,360
I am a American but I am based in

00:00:15,180 --> 00:00:19,710
Amsterdam where I work for a small data

00:00:18,360 --> 00:00:23,010
science and engineering consultancy

00:00:19,710 --> 00:00:25,650
called go data driven and now of the two

00:00:23,010 --> 00:00:28,440
of forementioned career tracks at my

00:00:25,650 --> 00:00:31,620
place of employment I am a data

00:00:28,440 --> 00:00:37,050
scientist which is to say I'm not an

00:00:31,620 --> 00:00:39,559
engineer so what then is data scientists

00:00:37,050 --> 00:00:42,839
like me doing on the store track stage

00:00:39,559 --> 00:00:45,390
well the answer is this paper the case

00:00:42,839 --> 00:00:48,420
for learned index structures written by

00:00:45,390 --> 00:00:50,159
Tim Kroshka who I think at the time was

00:00:48,420 --> 00:00:52,739
a brown took a year off went to Google

00:00:50,159 --> 00:00:54,809
is now at MIT along with a team of

00:00:52,739 --> 00:00:57,420
collaborators over at Google it made a

00:00:54,809 --> 00:00:59,550
splash at the data science water-cooler

00:00:57,420 --> 00:01:03,059
back in December because it proposed

00:00:59,550 --> 00:01:05,280
what I think is a really novel idea that

00:01:03,059 --> 00:01:07,680
machine learning something we data

00:01:05,280 --> 00:01:09,950
scientists no one care a great deal

00:01:07,680 --> 00:01:13,500
about I had the potential to replace

00:01:09,950 --> 00:01:16,680
indexes and certain types of database

00:01:13,500 --> 00:01:19,259
systems something I think and hope you

00:01:16,680 --> 00:01:21,030
hardcore storage systems engineers and

00:01:19,259 --> 00:01:24,270
the audience all know and care a great

00:01:21,030 --> 00:01:26,460
deal about now not only was this paper

00:01:24,270 --> 00:01:28,740
interesting in a general sense because

00:01:26,460 --> 00:01:30,539
at first glance the two use cases don't

00:01:28,740 --> 00:01:33,119
really seem to match up on the one hand

00:01:30,539 --> 00:01:35,219
you have statistical inference which is

00:01:33,119 --> 00:01:36,780
for learning patterns for the purposes

00:01:35,219 --> 00:01:38,909
of making predictions based on the

00:01:36,780 --> 00:01:41,369
future input and on the other you have

00:01:38,909 --> 00:01:43,200
data structures which are useful for

00:01:41,369 --> 00:01:44,969
optimized lookup but that's of

00:01:43,200 --> 00:01:47,100
information which you've already seen

00:01:44,969 --> 00:01:51,240
but it was also interesting for me

00:01:47,100 --> 00:01:52,619
personally because while I feel like I

00:01:51,240 --> 00:01:53,780
know a fair amount about machine

00:01:52,619 --> 00:01:55,859
learning

00:01:53,780 --> 00:01:57,509
despite their crucially in my daily

00:01:55,859 --> 00:02:00,240
professional life I know next to nothing

00:01:57,509 --> 00:02:02,789
about database internals and wanting to

00:02:00,240 --> 00:02:04,829
understand this paper gave me a really

00:02:02,789 --> 00:02:07,079
good excuse to dive in and learn all I

00:02:04,829 --> 00:02:09,780
could about the subject so I want to

00:02:07,079 --> 00:02:11,760
share with you today this idea and let's

00:02:09,780 --> 00:02:13,590
hope that Doug Turnbull had it right

00:02:11,760 --> 00:02:16,019
yesterday when he said that these sorts

00:02:13,590 --> 00:02:17,220
of autodidactic experiences tend to lead

00:02:16,019 --> 00:02:20,190
to great talks

00:02:17,220 --> 00:02:23,180
whether or not that's true here's my

00:02:20,190 --> 00:02:25,680
plan so first I'm going to talk about

00:02:23,180 --> 00:02:27,450
database indexes and while I'm sure this

00:02:25,680 --> 00:02:29,850
will be old hats and most of you in the

00:02:27,450 --> 00:02:31,820
room I want to do it anyway both to

00:02:29,850 --> 00:02:35,040
ensure that everyone is on the same page

00:02:31,820 --> 00:02:39,080
but also to reframe how we think about

00:02:35,040 --> 00:02:41,760
just what it is that database indexes do

00:02:39,080 --> 00:02:43,680
secondly I'm going to talk on again

00:02:41,760 --> 00:02:45,480
relatively high level about what it is

00:02:43,680 --> 00:02:47,190
that machine learning tries to

00:02:45,480 --> 00:02:51,540
accomplish and how this can be adapted

00:02:47,190 --> 00:02:53,460
to the database index domain and lastly

00:02:51,540 --> 00:02:55,830
I'm going to show how machine learning

00:02:53,460 --> 00:02:57,990
can be utilized to replace database

00:02:55,830 --> 00:03:02,040
indexes and three different types of

00:02:57,990 --> 00:03:04,830
tasks one range index is 2 point lookups

00:03:02,040 --> 00:03:09,480
and 3 existence checks so for all ready

00:03:04,830 --> 00:03:12,150
let's go let's start with analogy a

00:03:09,480 --> 00:03:14,100
database is like a haystack and I hope

00:03:12,150 --> 00:03:16,740
this analogy is not too english centric

00:03:14,100 --> 00:03:18,780
for the crowd but as these databases

00:03:16,740 --> 00:03:21,270
consist of thousands millions or even

00:03:18,780 --> 00:03:23,340
billions of records I think it would not

00:03:21,270 --> 00:03:24,810
be unfair to say that the challenge of

00:03:23,340 --> 00:03:27,600
finding specific records in our database

00:03:24,810 --> 00:03:28,170
is very much like finding needles in a

00:03:27,600 --> 00:03:30,239
haystack

00:03:28,170 --> 00:03:32,850
and if we were to follow the naive

00:03:30,239 --> 00:03:34,530
approach every time we need to look up a

00:03:32,850 --> 00:03:36,959
record in our database we would have to

00:03:34,530 --> 00:03:40,350
comb through our haystack one entry at a

00:03:36,959 --> 00:03:42,840
time until we either find what we are

00:03:40,350 --> 00:03:44,730
looking for or convince ourselves that

00:03:42,840 --> 00:03:46,860
what we're looking for isn't to be found

00:03:44,730 --> 00:03:49,769
and this might be acceptable for

00:03:46,860 --> 00:03:51,660
databases with a few hundred records but

00:03:49,769 --> 00:03:53,100
even in moderate scales waiting for

00:03:51,660 --> 00:03:55,860
answers from our database whether it's

00:03:53,100 --> 00:03:57,720
me doing some sort of analysis or you

00:03:55,860 --> 00:03:59,820
and the audience trying to populate the

00:03:57,720 --> 00:04:01,980
catalog of your online retail shop that

00:03:59,820 --> 00:04:04,290
would take far too much time and what we

00:04:01,980 --> 00:04:06,750
would rather have is some means of

00:04:04,290 --> 00:04:08,489
knowing precisely where our desired

00:04:06,750 --> 00:04:11,280
record is stored in the database and

00:04:08,489 --> 00:04:14,989
then skipping over all the other records

00:04:11,280 --> 00:04:17,669
to just such that one so because of this

00:04:14,989 --> 00:04:20,640
database systems often compared with an

00:04:17,669 --> 00:04:22,620
ancillary system called an index whose

00:04:20,640 --> 00:04:25,650
function it is to tell us more or less

00:04:22,620 --> 00:04:28,289
where in the database our desired record

00:04:25,650 --> 00:04:31,169
is and to do it in an efficient manner

00:04:28,289 --> 00:04:33,389
and how it works is roughly as follows

00:04:31,169 --> 00:04:36,689
say each record in our database is

00:04:33,389 --> 00:04:38,969
identified by a unique key an index is a

00:04:36,689 --> 00:04:40,590
black box or and this is the insight

00:04:38,969 --> 00:04:43,590
we're going to need for part two of my

00:04:40,590 --> 00:04:45,689
talk a model where the unique key goes

00:04:43,590 --> 00:04:50,099
in and a prediction for the position of

00:04:45,689 --> 00:04:52,259
the Associated record comes out by the

00:04:50,099 --> 00:04:55,259
way this is very much like how card

00:04:52,259 --> 00:04:56,969
catalogs in the library used to work at

00:04:55,259 --> 00:04:59,370
least for those of you like me old

00:04:56,969 --> 00:05:02,729
enough to remember having to do high

00:04:59,370 --> 00:05:05,009
school research projects using one now

00:05:02,729 --> 00:05:06,800
that's a rather abstract view so we

00:05:05,009 --> 00:05:09,479
should ask ourselves now if we were

00:05:06,800 --> 00:05:12,240
actually to implement such a system what

00:05:09,479 --> 00:05:14,520
types of black boxes or models could be

00:05:12,240 --> 00:05:17,430
used and to begin this discussion with a

00:05:14,520 --> 00:05:20,400
concrete task what could we used to

00:05:17,430 --> 00:05:22,560
handle say range requests that is

00:05:20,400 --> 00:05:26,069
locating all the records whose keys fall

00:05:22,560 --> 00:05:27,990
between two specific values now anyone

00:05:26,069 --> 00:05:30,060
who's ever taken an algorithms course or

00:05:27,990 --> 00:05:32,669
perhaps recently sat for a tech

00:05:30,060 --> 00:05:35,430
interview would probably think well if I

00:05:32,669 --> 00:05:36,839
can order my records by these keys then

00:05:35,430 --> 00:05:39,000
I can probably use some sort of binary

00:05:36,839 --> 00:05:41,370
search to find each of the endpoint

00:05:39,000 --> 00:05:45,270
records and then grab everything in

00:05:41,370 --> 00:05:46,919
between at least as a first guess and so

00:05:45,270 --> 00:05:48,990
let's follow up on that hunch we want to

00:05:46,919 --> 00:05:53,069
facilitate binary search and so for our

00:05:48,990 --> 00:05:54,659
model let's use a binary search tree now

00:05:53,069 --> 00:05:55,050
for those of you who haven't seen one

00:05:54,659 --> 00:05:57,599
before

00:05:55,050 --> 00:06:00,060
the way a binary search tree works is as

00:05:57,599 --> 00:06:02,279
follows for each record key you make a

00:06:00,060 --> 00:06:04,919
node and that's uniquely identified by

00:06:02,279 --> 00:06:06,330
the same ID as the record and also

00:06:04,919 --> 00:06:08,610
carries the position of that record in

00:06:06,330 --> 00:06:10,439
the database and then these nodes are

00:06:08,610 --> 00:06:13,139
stored in the tree structure where every

00:06:10,439 --> 00:06:15,029
node can have at most two children which

00:06:13,139 --> 00:06:18,000
themselves are trees and you have

00:06:15,029 --> 00:06:20,099
requirement that all the keys and the

00:06:18,000 --> 00:06:21,990
left child tree will be smaller than the

00:06:20,099 --> 00:06:23,939
current node ski and those in the right

00:06:21,990 --> 00:06:26,819
child tree will be larger than the

00:06:23,939 --> 00:06:28,889
current note ski and if you further use

00:06:26,819 --> 00:06:31,199
one of the variations on a binary search

00:06:28,889 --> 00:06:32,909
tree that also do their best to keep the

00:06:31,199 --> 00:06:37,620
bottommost nodes all in the same depth

00:06:32,909 --> 00:06:39,029
then you get a guarantee as well this is

00:06:37,620 --> 00:06:42,149
supposed to be an animation oh there we

00:06:39,029 --> 00:06:43,979
go is attempting to demonstrate that the

00:06:42,149 --> 00:06:44,949
maximum number of nodes that you have to

00:06:43,979 --> 00:06:46,930
examine during a look

00:06:44,949 --> 00:06:49,180
is on the order of log base two of n

00:06:46,930 --> 00:06:51,669
where n is the number of keys in your

00:06:49,180 --> 00:06:53,889
index so for example if you had a

00:06:51,669 --> 00:06:55,419
million keys this number turns out to be

00:06:53,889 --> 00:06:59,530
around 20 and for a billion you're only

00:06:55,419 --> 00:07:02,050
dealing with 30 and this seems a great

00:06:59,530 --> 00:07:03,699
improvement over brute search and you

00:07:02,050 --> 00:07:05,770
might rightfully ask yourself well why

00:07:03,699 --> 00:07:08,229
stop at two children per node or even

00:07:05,770 --> 00:07:09,909
one keeper knows and if you continue

00:07:08,229 --> 00:07:11,650
with that line of thinking and again

00:07:09,909 --> 00:07:14,020
think of some clever rebalancing rules

00:07:11,650 --> 00:07:16,539
to maintain and even tree depth you'll

00:07:14,020 --> 00:07:21,819
eventually discover improvement to the B

00:07:16,539 --> 00:07:24,430
tree seen here ignoring the costs of

00:07:21,819 --> 00:07:26,919
scanning the keys inside an individual

00:07:24,430 --> 00:07:28,629
node which well because the number of

00:07:26,919 --> 00:07:30,039
keys and the note is much much smaller

00:07:28,629 --> 00:07:32,860
than a total number of keys in our

00:07:30,039 --> 00:07:35,139
database should be justifiable we've now

00:07:32,860 --> 00:07:38,500
guaranteed ourselves lookup times on the

00:07:35,139 --> 00:07:40,960
order of log base K of n where K is the

00:07:38,500 --> 00:07:45,159
number or the maximum number of children

00:07:40,960 --> 00:07:46,990
allowed for node which means if we go

00:07:45,159 --> 00:07:49,449
back to a previous figures and we say we

00:07:46,990 --> 00:07:50,860
allow 100 children per node now four

00:07:49,449 --> 00:07:52,690
million records were down to a tree

00:07:50,860 --> 00:07:55,750
depth of three and for a billion we have

00:07:52,690 --> 00:07:58,060
a solid five and with this we seem to

00:07:55,750 --> 00:07:59,889
optimize the look of complexity of the

00:07:58,060 --> 00:08:01,870
problem but having a node for every

00:07:59,889 --> 00:08:04,060
record also can perhaps be seen as

00:08:01,870 --> 00:08:06,190
overkill and likely there's room for

00:08:04,060 --> 00:08:08,710
optimization in terms of space

00:08:06,190 --> 00:08:10,479
requirements and this light it's s to

00:08:08,710 --> 00:08:13,779
the third and final improvement we can

00:08:10,479 --> 00:08:16,210
make which works as follows so we take

00:08:13,779 --> 00:08:17,949
our sorted records and we divide them up

00:08:16,210 --> 00:08:20,319
into continuous groups which we call

00:08:17,949 --> 00:08:22,839
pages of a fixed size which we call the

00:08:20,319 --> 00:08:26,110
page size and then we store in our index

00:08:22,839 --> 00:08:28,659
not every key of every record but only

00:08:26,110 --> 00:08:30,039
the first key of every page and then

00:08:28,659 --> 00:08:33,579
this allows a great deal of space

00:08:30,039 --> 00:08:35,949
efficiency and though our index now only

00:08:33,579 --> 00:08:38,320
points to a page and not to a record

00:08:35,949 --> 00:08:40,329
meaning that after we finally descended

00:08:38,320 --> 00:08:44,440
the index we still have to perform a

00:08:40,329 --> 00:08:46,990
search on the page since we choose the

00:08:44,440 --> 00:08:48,670
page size to be again tiny in comparison

00:08:46,990 --> 00:08:51,519
to the size of the number of records on

00:08:48,670 --> 00:08:53,770
the balance of increased computation

00:08:51,519 --> 00:08:55,930
versus decreased storage we still come

00:08:53,770 --> 00:08:58,000
out in the black and with this third

00:08:55,930 --> 00:08:58,660
improvement we now have modulo some

00:08:58,000 --> 00:09:01,690
optimization

00:08:58,660 --> 00:09:03,550
involving caches arguably them the most

00:09:01,690 --> 00:09:06,430
common type of reined index out in the

00:09:03,550 --> 00:09:09,370
wild and it's this last improvement

00:09:06,430 --> 00:09:11,830
making our index sparse I think really

00:09:09,370 --> 00:09:14,530
makes the analogy of index as model of

00:09:11,830 --> 00:09:16,540
work and the reason is when we think of

00:09:14,530 --> 00:09:17,800
the word model we really we typically

00:09:16,540 --> 00:09:19,900
think of something that makes

00:09:17,800 --> 00:09:22,870
predictions and then these predictions

00:09:19,900 --> 00:09:24,670
have associated errors and with binary

00:09:22,870 --> 00:09:28,270
search trees and beech trees we had the

00:09:24,670 --> 00:09:30,250
notion of prediction that is where we

00:09:28,270 --> 00:09:32,860
could find the record but now with

00:09:30,250 --> 00:09:36,130
sparsity a model predictions gained also

00:09:32,860 --> 00:09:38,590
the notion of error as the prediction is

00:09:36,130 --> 00:09:41,830
not of the exact location of the record

00:09:38,590 --> 00:09:44,800
but of its page further these errors can

00:09:41,830 --> 00:09:46,450
come with hard guarantees after all the

00:09:44,800 --> 00:09:49,570
record if it's in the database is

00:09:46,450 --> 00:09:51,370
definitely not to the left of the first

00:09:49,570 --> 00:09:54,160
record on the page and it's definitely

00:09:51,370 --> 00:09:57,700
no more than page saves page size

00:09:54,160 --> 00:09:59,920
records to the right and all in all we

00:09:57,700 --> 00:10:01,450
have now a very nice system and that

00:09:59,920 --> 00:10:04,510
we've got hard guarantees both on

00:10:01,450 --> 00:10:07,120
prediction complexity and sorry compute

00:10:04,510 --> 00:10:09,340
complexity and also on error magnitudes

00:10:07,120 --> 00:10:13,000
and seeing as how B trees have been

00:10:09,340 --> 00:10:15,130
around since 1971 surely one might think

00:10:13,000 --> 00:10:17,020
no nothing could really work better

00:10:15,130 --> 00:10:19,150
because if there were then that newer

00:10:17,020 --> 00:10:21,160
technology would have long ago replaced

00:10:19,150 --> 00:10:24,370
the B tree as the model of choice for

00:10:21,160 --> 00:10:25,990
range indexes except they're not

00:10:24,370 --> 00:10:28,570
necessarily the best option out there

00:10:25,990 --> 00:10:30,580
and here's a simple counter example

00:10:28,570 --> 00:10:32,890
which I should mention also comes

00:10:30,580 --> 00:10:35,170
straight from the paper say your records

00:10:32,890 --> 00:10:37,840
were of a fixed size and the keys were

00:10:35,170 --> 00:10:40,480
to be the continuous integers between

00:10:37,840 --> 00:10:43,240
let's say one and a hundred million then

00:10:40,480 --> 00:10:45,130
we could have a constant time lookup in

00:10:43,240 --> 00:10:49,240
our database simply by using the key as

00:10:45,130 --> 00:10:51,970
an offset perhaps minus one and of

00:10:49,240 --> 00:10:53,290
course is not the most realistic example

00:10:51,970 --> 00:10:55,720
out there but it serves illustrate the

00:10:53,290 --> 00:10:58,480
following point that the reason why B

00:10:55,720 --> 00:11:00,450
trees are so widespread and generally

00:10:58,480 --> 00:11:02,800
available database systems is not

00:11:00,450 --> 00:11:05,350
because of the best model for fitting

00:11:02,800 --> 00:11:06,570
your data distribution but because

00:11:05,350 --> 00:11:08,860
they're the best model for fitting

00:11:06,570 --> 00:11:12,070
unknown or what's called the average

00:11:08,860 --> 00:11:12,430
data distribution of course if your

00:11:12,070 --> 00:11:15,160
database

00:11:12,430 --> 00:11:17,830
engineers were to know your exact data

00:11:15,160 --> 00:11:19,870
buta data distribution they could

00:11:17,830 --> 00:11:21,730
engineer a tailored index but this

00:11:19,870 --> 00:11:24,010
engineering costs would likely be too

00:11:21,730 --> 00:11:25,870
great for your project and would also be

00:11:24,010 --> 00:11:27,610
unrealistic to expect from a database

00:11:25,870 --> 00:11:29,730
available for general use so Thank You

00:11:27,610 --> 00:11:32,230
Redis is thank your Postgres is etc

00:11:29,730 --> 00:11:35,770
which leads us to the following wanted

00:11:32,230 --> 00:11:37,870
ads what we want we want an index

00:11:35,770 --> 00:11:40,200
structure tailored to your particular

00:11:37,870 --> 00:11:42,399
data distribution which can be

00:11:40,200 --> 00:11:45,490
automatically synthesized to avoid

00:11:42,399 --> 00:11:49,120
engineering costs and which comes with

00:11:45,490 --> 00:11:51,399
hard error guarantees as otherwise the

00:11:49,120 --> 00:11:54,670
performance gains we get at prediction

00:11:51,399 --> 00:11:58,420
time might be lost at seek time so what

00:11:54,670 --> 00:11:59,709
could fit the bill well as you might

00:11:58,420 --> 00:12:04,300
have guessed the answer according to

00:11:59,709 --> 00:12:06,790
Koshka at all is machine learning and as

00:12:04,300 --> 00:12:09,670
for the general reason why recall that

00:12:06,790 --> 00:12:11,580
what we want the index to learn is the

00:12:09,670 --> 00:12:14,500
distribution of keys in the key space

00:12:11,580 --> 00:12:16,020
this distribution is a function and it

00:12:14,500 --> 00:12:19,630
turns out that machine learning is

00:12:16,020 --> 00:12:20,950
really good at learning functions in

00:12:19,630 --> 00:12:23,290
particular the class of machine learning

00:12:20,950 --> 00:12:25,209
algorithms called neural nets which form

00:12:23,290 --> 00:12:26,680
the basis of all the deep learning

00:12:25,209 --> 00:12:29,110
you've been hearing about for the past

00:12:26,680 --> 00:12:30,730
four years in fact that machine learning

00:12:29,110 --> 00:12:33,400
algorithms are so good at learning

00:12:30,730 --> 00:12:35,470
functions that data scientists and other

00:12:33,400 --> 00:12:38,110
practitioners of machine learning

00:12:35,470 --> 00:12:39,910
typically introduce restrictions on the

00:12:38,110 --> 00:12:41,980
allowed complexity of trained machine

00:12:39,910 --> 00:12:44,860
learning models simply to keep them from

00:12:41,980 --> 00:12:48,300
learning the data too well and here's an

00:12:44,860 --> 00:12:51,430
example of what I mean say the function

00:12:48,300 --> 00:12:54,520
you want to learn is here represented by

00:12:51,430 --> 00:12:57,100
the blue curve and machine learning

00:12:54,520 --> 00:12:59,110
problems you don't know a priori what

00:12:57,100 --> 00:13:01,000
that function is but you can make

00:12:59,110 --> 00:13:04,420
observations of that function here

00:13:01,000 --> 00:13:06,850
represented by the yellow dots by the

00:13:04,420 --> 00:13:09,100
way the reason why those yellow dots

00:13:06,850 --> 00:13:12,040
don't actually coincide with the blue

00:13:09,100 --> 00:13:14,770
curve is because these observations are

00:13:12,040 --> 00:13:17,350
in machine learning problems corrupted

00:13:14,770 --> 00:13:19,930
by some sort of noise and if we can sort

00:13:17,350 --> 00:13:22,150
of make this grounded in reality here's

00:13:19,930 --> 00:13:25,000
an example let's say we're trying to fit

00:13:22,150 --> 00:13:25,290
the function of apartment prices here in

00:13:25,000 --> 00:13:27,389
Berlin

00:13:25,290 --> 00:13:29,759
and our observations would include

00:13:27,389 --> 00:13:32,250
information about the number of each

00:13:29,759 --> 00:13:33,660
rooms or the area in square meters or

00:13:32,250 --> 00:13:36,060
the distance to the metro for each of

00:13:33,660 --> 00:13:37,920
these apartments and then together with

00:13:36,060 --> 00:13:40,259
the actual historical sale prices of

00:13:37,920 --> 00:13:42,630
those apartments whose deviations from

00:13:40,259 --> 00:13:45,120
this latent price function could be

00:13:42,630 --> 00:13:47,160
explained by let's say the prejudices of

00:13:45,120 --> 00:13:49,259
the buyer sorry the seller or time

00:13:47,160 --> 00:13:52,860
pressures experienced by potential

00:13:49,259 --> 00:13:55,410
buyers etc so now our machine learning

00:13:52,860 --> 00:13:58,529
algorithm would then make a guess about

00:13:55,410 --> 00:14:00,269
what this function could be then used

00:13:58,529 --> 00:14:02,850
the observations together with some

00:14:00,269 --> 00:14:04,980
measure of loss to calculate an error on

00:14:02,850 --> 00:14:06,779
that guess and then consequently use the

00:14:04,980 --> 00:14:08,519
error to make a better guess and so on

00:14:06,779 --> 00:14:10,920
until the change in error between

00:14:08,519 --> 00:14:14,220
guesses falls below some tolerance

00:14:10,920 --> 00:14:15,959
thresholds so we try to fit curves of

00:14:14,220 --> 00:14:21,480
varying complexity to these observations

00:14:15,959 --> 00:14:23,550
the most simple shown here in blue well

00:14:21,480 --> 00:14:25,740
we see that even with the best selection

00:14:23,550 --> 00:14:28,050
of the parameters for that function the

00:14:25,740 --> 00:14:30,620
resulting curve is unable to approach

00:14:28,050 --> 00:14:32,940
the vast majority of observations

00:14:30,620 --> 00:14:34,589
machine learning practitioner would say

00:14:32,940 --> 00:14:37,110
that in this case the model is under

00:14:34,589 --> 00:14:39,029
fitting and this arises when the allowed

00:14:37,110 --> 00:14:40,860
complexity of the model is not

00:14:39,029 --> 00:14:44,130
sufficient to describe the function

00:14:40,860 --> 00:14:47,069
underlying the observations the most

00:14:44,130 --> 00:14:48,600
complex curve here the green one this is

00:14:47,069 --> 00:14:49,199
also doing a terrible job but for a

00:14:48,600 --> 00:14:51,360
different reason

00:14:49,199 --> 00:14:53,670
this one is trying to pass through all

00:14:51,360 --> 00:14:56,880
of the points no matter how illogical

00:14:53,670 --> 00:14:58,620
the resulting shape and this phenomenon

00:14:56,880 --> 00:14:59,790
is called overfitting and what's

00:14:58,620 --> 00:15:02,100
happening is that the machine learning

00:14:59,790 --> 00:15:03,959
algorithm is fitting not to the latent

00:15:02,100 --> 00:15:06,240
function but to the noisy observations

00:15:03,959 --> 00:15:08,760
of that function and remember this

00:15:06,240 --> 00:15:10,410
because that'll be important later so we

00:15:08,760 --> 00:15:12,000
need some curve whose complexity

00:15:10,410 --> 00:15:14,310
somewhere in between the blue curve and

00:15:12,000 --> 00:15:17,339
the green curve which is here shown in

00:15:14,310 --> 00:15:19,529
orange and actually finding that perfect

00:15:17,339 --> 00:15:21,300
balance between under fitting and over

00:15:19,529 --> 00:15:24,930
fitting is one of the hardest parts

00:15:21,300 --> 00:15:26,130
about doing machine learning ok so that

00:15:24,930 --> 00:15:28,860
was an example of using machine learning

00:15:26,130 --> 00:15:30,269
algorithm to fit a simple function but

00:15:28,860 --> 00:15:32,519
actually machine learning algorithms are

00:15:30,269 --> 00:15:34,769
capable of fitting immensely complicated

00:15:32,519 --> 00:15:36,119
functions of hundreds of millions of

00:15:34,769 --> 00:15:39,089
parameters

00:15:36,119 --> 00:15:41,339
Google translates Facebook's facial

00:15:39,089 --> 00:15:43,829
recognition software and deepmind's

00:15:41,339 --> 00:15:46,259
alphago these are these all boiled down

00:15:43,829 --> 00:15:50,449
to machine learning systems that have

00:15:46,259 --> 00:15:52,409
learned incredibly complicated functions

00:15:50,449 --> 00:15:54,359
so we see that machine learning is

00:15:52,409 --> 00:15:56,609
useful for learning functions how do we

00:15:54,359 --> 00:15:57,829
apply this to the database domain so

00:15:56,609 --> 00:16:01,470
let's say the situation is the following

00:15:57,829 --> 00:16:03,869
one our database records each have a

00:16:01,470 --> 00:16:07,649
unique key and all and the collection of

00:16:03,869 --> 00:16:09,749
all these keys is orderable to our set

00:16:07,649 --> 00:16:12,449
of records is held in memory sorted by

00:16:09,749 --> 00:16:14,699
key and it's static whereby static I

00:16:12,449 --> 00:16:16,589
mean actually static or perhaps only

00:16:14,699 --> 00:16:19,319
updated infrequently so we have let's

00:16:16,589 --> 00:16:23,099
say a cold storage or a data warehouse

00:16:19,319 --> 00:16:24,809
or something like this and lastly we are

00:16:23,099 --> 00:16:27,179
interested in read access in this

00:16:24,809 --> 00:16:30,479
database and we're interested in range

00:16:27,179 --> 00:16:32,669
queries so given these conditions here's

00:16:30,479 --> 00:16:34,109
another function learning situation more

00:16:32,669 --> 00:16:38,519
along the lines of what we're interested

00:16:34,109 --> 00:16:40,409
in doing say we have our keys a little

00:16:38,519 --> 00:16:41,819
difficult to see here we have these pink

00:16:40,409 --> 00:16:43,849
lines representing keys and key space

00:16:41,819 --> 00:16:46,769
and they're spread out in some way

00:16:43,849 --> 00:16:48,659
amongst the allowed values what we're

00:16:46,769 --> 00:16:51,359
learning is interested in learning is

00:16:48,659 --> 00:16:53,789
this it's the key distribution and

00:16:51,359 --> 00:16:56,459
please forgive the lack of rigor in my

00:16:53,789 --> 00:16:58,409
illustration so now machine learning

00:16:56,459 --> 00:17:00,419
algorithms could learn this naked

00:16:58,409 --> 00:17:01,769
distribution perfectly well it's a

00:17:00,419 --> 00:17:03,989
machine learning task called density

00:17:01,769 --> 00:17:05,909
estimation but actually from an

00:17:03,989 --> 00:17:08,519
engineering point of view the function

00:17:05,909 --> 00:17:12,240
we would rather learn is the cumulative

00:17:08,519 --> 00:17:15,990
feet key distribution that is a say we

00:17:12,240 --> 00:17:17,459
want to give our model a key and we want

00:17:15,990 --> 00:17:20,399
to have it predict say that this

00:17:17,459 --> 00:17:22,039
particular key is greater than 25% of

00:17:20,399 --> 00:17:24,809
all the keys according to their ordering

00:17:22,039 --> 00:17:27,929
because this way we immediately know

00:17:24,809 --> 00:17:29,549
that we would skip the first 25% of the

00:17:27,929 --> 00:17:34,200
records in our database to retrieve the

00:17:29,549 --> 00:17:35,669
record associated with that key now what

00:17:34,200 --> 00:17:37,860
I just described about learning

00:17:35,669 --> 00:17:40,019
distributions could be termed normal

00:17:37,860 --> 00:17:41,909
machine learning however there's a very

00:17:40,019 --> 00:17:44,460
important difference between our

00:17:41,909 --> 00:17:45,990
database indexing Aereo and normal

00:17:44,460 --> 00:17:48,870
machine learning in normal machine

00:17:45,990 --> 00:17:50,820
learning you learn a function based

00:17:48,870 --> 00:17:52,830
annoys the observations of the function

00:17:50,820 --> 00:17:55,590
and then make predictions for input

00:17:52,830 --> 00:17:57,480
values that you haven't seen before so

00:17:55,590 --> 00:18:01,020
for instance going back to our Berlin

00:17:57,480 --> 00:18:03,600
apartment pricing model we were fitting

00:18:01,020 --> 00:18:05,430
this model based on historical prices of

00:18:03,600 --> 00:18:07,830
sold apartments but actually the reason

00:18:05,430 --> 00:18:10,020
we want to use this model is not to

00:18:07,830 --> 00:18:11,700
explain apartment prices in the past but

00:18:10,020 --> 00:18:14,640
rather to make predictions of the price

00:18:11,700 --> 00:18:16,290
of an apartment in the future whose

00:18:14,640 --> 00:18:21,120
exact combination of features we haven't

00:18:16,290 --> 00:18:23,100
seen before but with an index model not

00:18:21,120 --> 00:18:24,440
only are you observe a shion's the keys

00:18:23,100 --> 00:18:26,370
noise-free

00:18:24,440 --> 00:18:28,380
but when it comes time to make

00:18:26,370 --> 00:18:31,200
predictions you're actually going to

00:18:28,380 --> 00:18:33,390
make predictions on inputs the model has

00:18:31,200 --> 00:18:35,850
already seen before namely the keys

00:18:33,390 --> 00:18:37,980
themselves and then this break with

00:18:35,850 --> 00:18:42,809
normal machine learning methodology

00:18:37,980 --> 00:18:45,270
means in fact that the situation the in

00:18:42,809 --> 00:18:46,530
this situation our observations and the

00:18:45,270 --> 00:18:48,780
underlying function we're trying to

00:18:46,530 --> 00:18:50,550
learn are one in the same that is

00:18:48,780 --> 00:18:53,010
there's nothing really distinguishing

00:18:50,550 --> 00:18:54,300
the blue curve and the yellow dots which

00:18:53,010 --> 00:18:56,070
in turn means that in our previous

00:18:54,300 --> 00:18:58,380
example we actually would have preferred

00:18:56,070 --> 00:19:00,660
the highly overfitting model that wildly

00:18:58,380 --> 00:19:02,340
jumps about because it always predicts

00:19:00,660 --> 00:19:04,410
what it had seen before and because

00:19:02,340 --> 00:19:07,640
there are no values of the function that

00:19:04,410 --> 00:19:07,640
the model hadn't seen before

00:19:08,150 --> 00:19:13,110
additionally this break with traditional

00:19:11,010 --> 00:19:15,510
methodology gives us hard error

00:19:13,110 --> 00:19:17,820
guarantees on our predictions because

00:19:15,510 --> 00:19:19,860
after training our model will only be

00:19:17,820 --> 00:19:22,290
making predictions on what the model has

00:19:19,860 --> 00:19:24,690
already seen and because the training

00:19:22,290 --> 00:19:26,670
data doesn't change to calculate our

00:19:24,690 --> 00:19:29,160
error guarantees all that we have to do

00:19:26,670 --> 00:19:30,630
is to remember the worst errors that the

00:19:29,160 --> 00:19:34,710
model makes on the training data and

00:19:30,630 --> 00:19:36,750
that's it now I mentioned earlier that a

00:19:34,710 --> 00:19:39,480
machine learning algorithm particularly

00:19:36,750 --> 00:19:42,360
adept at overfitting is the neural

00:19:39,480 --> 00:19:45,000
network so to test their idea the

00:19:42,360 --> 00:19:47,580
researchers took a test set of 200

00:19:45,000 --> 00:19:49,500
million web server log records trained

00:19:47,580 --> 00:19:52,950
in neural network index over their time

00:19:49,500 --> 00:19:53,580
stamps and examine the results and what

00:19:52,950 --> 00:19:55,679
did they find

00:19:53,580 --> 00:19:58,560
well they found that the model did

00:19:55,679 --> 00:20:01,530
terribly in comparison with the standard

00:19:58,560 --> 00:20:03,210
b-tree index 2 in 2 orders of magnitude

00:20:01,530 --> 00:20:05,279
slower for making

00:20:03,210 --> 00:20:07,610
addictions and two to three times slower

00:20:05,279 --> 00:20:10,470
for searching the error margins

00:20:07,610 --> 00:20:12,960
now the author's offer a number of

00:20:10,470 --> 00:20:14,880
reasons for the poor performance much of

00:20:12,960 --> 00:20:17,789
it could be attributed to their choice

00:20:14,880 --> 00:20:19,320
of machine learning framework for both

00:20:17,789 --> 00:20:24,240
training and for making predictions

00:20:19,320 --> 00:20:27,149
namely pythons tensorflow tensorflow was

00:20:24,240 --> 00:20:29,309
optimized for big models and as a result

00:20:27,149 --> 00:20:31,710
has a significant invocation overhead

00:20:29,309 --> 00:20:35,309
that just killed the performance of his

00:20:31,710 --> 00:20:36,330
relatively small neural network this

00:20:35,309 --> 00:20:38,669
problem however could be

00:20:36,330 --> 00:20:41,279
straightforwardly dealt with you simply

00:20:38,669 --> 00:20:43,620
train the model with tensorflow and then

00:20:41,279 --> 00:20:45,960
you export the optimized parameter

00:20:43,620 --> 00:20:48,210
values and recreate the model

00:20:45,960 --> 00:20:50,279
architecture using a faster language

00:20:48,210 --> 00:20:53,789
let's say C++ for actually making

00:20:50,279 --> 00:20:56,600
predictions but there's another problem

00:20:53,789 --> 00:20:59,789
less straightforward which was that

00:20:56,600 --> 00:21:02,640
though neural networks are comparably

00:20:59,789 --> 00:21:05,070
good in terms of CPU and space

00:21:02,640 --> 00:21:07,169
efficiency at overfitting to the general

00:21:05,070 --> 00:21:09,899
shape of the cumulative data

00:21:07,169 --> 00:21:12,390
distribution they lose their competitive

00:21:09,899 --> 00:21:15,179
advantage over B trees when going the

00:21:12,390 --> 00:21:18,000
last mile so to say a fine tuning their

00:21:15,179 --> 00:21:20,669
predictions so put another way with a

00:21:18,000 --> 00:21:22,350
sufficient number of keys from 10,000

00:21:20,669 --> 00:21:25,679
feet up in the air the cumulative

00:21:22,350 --> 00:21:30,899
distribution looks relatively smooth but

00:21:25,679 --> 00:21:32,549
when you zoom in and you see that the

00:21:30,899 --> 00:21:35,820
distribution appears relatively grainy

00:21:32,549 --> 00:21:37,260
now the former situation when the curve

00:21:35,820 --> 00:21:39,659
appears moved that's really good for

00:21:37,260 --> 00:21:41,909
machine learning but when it's quite

00:21:39,659 --> 00:21:45,570
grainy like this on the right that's

00:21:41,909 --> 00:21:48,779
quite bad so the solution of the authors

00:21:45,570 --> 00:21:50,909
was to replace that single monolithic

00:21:48,779 --> 00:21:54,299
model with something that looks like

00:21:50,909 --> 00:21:59,039
this which they called their recursive

00:21:54,299 --> 00:22:02,220
model index the idea is to build a model

00:21:59,039 --> 00:22:04,740
of experts such that the models at the

00:22:02,220 --> 00:22:08,669
bottom are extremely knowledgeable about

00:22:04,740 --> 00:22:11,279
a small localized part of key space and

00:22:08,669 --> 00:22:13,200
the models above them are really good at

00:22:11,279 --> 00:22:15,900
steering queries to the appropriate

00:22:13,200 --> 00:22:18,030
expert below

00:22:15,900 --> 00:22:20,730
no by the way that this is not a tree

00:22:18,030 --> 00:22:23,400
structure multiple models at one level

00:22:20,730 --> 00:22:26,460
can indeed point to the same model at

00:22:23,400 --> 00:22:31,770
the level below now this architecture

00:22:26,460 --> 00:22:34,620
has three principal benefits one instead

00:22:31,770 --> 00:22:37,350
of training a single model based on its

00:22:34,620 --> 00:22:39,900
accuracy across the entire key space you

00:22:37,350 --> 00:22:42,030
now train multiple models each

00:22:39,900 --> 00:22:44,190
accountable only for a small region of

00:22:42,030 --> 00:22:48,260
the key space which has the net effect

00:22:44,190 --> 00:22:51,270
of decreasing overall loss number two

00:22:48,260 --> 00:22:53,760
complex and expensive models which are

00:22:51,270 --> 00:22:56,130
better capturing the overall general

00:22:53,760 --> 00:22:58,350
shape can be used at the first level of

00:22:56,130 --> 00:23:00,720
experts while simple and cheap models

00:22:58,350 --> 00:23:03,840
can be used on the smaller mini domains

00:23:00,720 --> 00:23:05,640
so in this case we can use a neural

00:23:03,840 --> 00:23:08,130
network at the top to make the first

00:23:05,640 --> 00:23:09,510
initial assignment and then afterwards

00:23:08,130 --> 00:23:13,830
beginning something similar like linear

00:23:09,510 --> 00:23:15,480
regression and three there's no search

00:23:13,830 --> 00:23:19,080
process required in between the stages

00:23:15,480 --> 00:23:20,190
like in a beech tree remember I sort of

00:23:19,080 --> 00:23:21,990
glossed over this but when you're

00:23:20,190 --> 00:23:23,520
searching a beech tree every time you

00:23:21,990 --> 00:23:24,780
hit a node you still have to search the

00:23:23,520 --> 00:23:28,260
keys in that no before you figure out

00:23:24,780 --> 00:23:30,090
what child you have to go do which means

00:23:28,260 --> 00:23:32,670
that model outputs are simply offsets

00:23:30,090 --> 00:23:35,330
and as a result the entire index can be

00:23:32,670 --> 00:23:37,950
expressed as a sparse matrix

00:23:35,330 --> 00:23:40,470
multiplication which means that predicts

00:23:37,950 --> 00:23:43,950
occur with constant complexity instead

00:23:40,470 --> 00:23:47,190
of on the order of log sub K of the

00:23:43,950 --> 00:23:48,210
number of keys in your index so I should

00:23:47,190 --> 00:23:49,560
mention up to now that we've been

00:23:48,210 --> 00:23:52,230
discussing the beech tree database

00:23:49,560 --> 00:23:54,270
indices as though they were strictly for

00:23:52,230 --> 00:23:56,280
looking up individual records and while

00:23:54,270 --> 00:23:58,620
they are adept at that their true

00:23:56,280 --> 00:24:00,990
ability lies in accessing ranges of

00:23:58,620 --> 00:24:02,490
records remember that our records are

00:24:00,990 --> 00:24:05,340
sorted sort of free predict the

00:24:02,490 --> 00:24:07,320
petitions of the two end points of our

00:24:05,340 --> 00:24:09,240
range of interest we very quickly know

00:24:07,320 --> 00:24:12,180
the locations of all the extras of the

00:24:09,240 --> 00:24:14,340
records we'd like to retrieve so a

00:24:12,180 --> 00:24:17,730
logical follow-up question could then be

00:24:14,340 --> 00:24:19,110
are there other index structures where

00:24:17,730 --> 00:24:21,930
our machine learning could also play a

00:24:19,110 --> 00:24:24,990
role and that's the subject of the third

00:24:21,930 --> 00:24:27,000
section of this talk like to now talk

00:24:24,990 --> 00:24:28,260
about two additional types of index

00:24:27,000 --> 00:24:30,600
structures

00:24:28,260 --> 00:24:34,080
hashmaps and bloom filters so let's

00:24:30,600 --> 00:24:37,200
start with hashmaps in contrast to tree

00:24:34,080 --> 00:24:39,120
based sorry be tree based indexes which

00:24:37,200 --> 00:24:41,490
can be used to locate individual records

00:24:39,120 --> 00:24:43,800
but whose strength is really to quickly

00:24:41,490 --> 00:24:46,560
discover records associated with a range

00:24:43,800 --> 00:24:48,720
of keys the hash map is an index

00:24:46,560 --> 00:24:52,050
structure whose sole purpose is to

00:24:48,720 --> 00:24:55,050
assign individual records to and loader

00:24:52,050 --> 00:24:59,280
later locate in an array so let's call

00:24:55,050 --> 00:25:02,460
it a point index viewed as a model we

00:24:59,280 --> 00:25:04,680
again have the situation where key goes

00:25:02,460 --> 00:25:07,380
into the black box and record location

00:25:04,680 --> 00:25:10,050
comes out but whereas in the previous

00:25:07,380 --> 00:25:13,050
case the records were all sorted and

00:25:10,050 --> 00:25:15,330
adjacent to one another in the point

00:25:13,050 --> 00:25:18,870
index case the location of the keys in

00:25:15,330 --> 00:25:22,860
this array is assigned randomly I'll be

00:25:18,870 --> 00:25:25,260
the deterministic hash function so what

00:25:22,860 --> 00:25:27,210
typically happens is that multiple keys

00:25:25,260 --> 00:25:30,960
are assigned to the same location a

00:25:27,210 --> 00:25:32,910
situation known as a conflict thus what

00:25:30,960 --> 00:25:35,670
the model points you may not in fact be

00:25:32,910 --> 00:25:38,730
a record of all but let's say a list of

00:25:35,670 --> 00:25:40,890
records that needs to be traversed and

00:25:38,730 --> 00:25:43,710
now an ideal situation there are no

00:25:40,890 --> 00:25:46,590
conflicts and then lookup becomes a

00:25:43,710 --> 00:25:48,060
constant time operation and no extra

00:25:46,590 --> 00:25:50,460
space needs to be reserved for the

00:25:48,060 --> 00:25:52,830
overflow but in the situation where a

00:25:50,460 --> 00:25:55,200
number of keys equals the number of

00:25:52,830 --> 00:25:57,660
array slots simply because of statistics

00:25:55,200 --> 00:26:00,480
collisions are unavoidable using the

00:25:57,660 --> 00:26:03,150
naive hashing strategies and collision

00:26:00,480 --> 00:26:06,720
avoidance strategies cost either memory

00:26:03,150 --> 00:26:09,240
or they cost time so what we want from

00:26:06,720 --> 00:26:11,430
our hashing model is to make location

00:26:09,240 --> 00:26:13,520
assignments as efficiently as possible

00:26:11,430 --> 00:26:18,030
there's say we want to fill up every

00:26:13,520 --> 00:26:20,550
available slot in our array so to do so

00:26:18,030 --> 00:26:22,860
the proposal of Kroshka at all is to

00:26:20,550 --> 00:26:25,230
again have the machine learning model

00:26:22,860 --> 00:26:28,380
learn the cumulative D key distribution

00:26:25,230 --> 00:26:31,710
that is to say the model predicts that a

00:26:28,380 --> 00:26:33,900
particular key is greater than say 25%

00:26:31,710 --> 00:26:36,960
of all keys and then the hashing index

00:26:33,900 --> 00:26:40,740
tries to insert it 25% of the way along

00:26:36,960 --> 00:26:41,970
the available array slots and of course

00:26:40,740 --> 00:26:43,950
should there be a closed

00:26:41,970 --> 00:26:47,220
which is bound to happen if there are

00:26:43,950 --> 00:26:49,110
fewer slots then keys the regular

00:26:47,220 --> 00:26:52,259
collision of resolution techniques could

00:26:49,110 --> 00:26:54,600
still be applied the point is that by

00:26:52,259 --> 00:26:57,120
avoiding empty array slots in the first

00:26:54,600 --> 00:26:59,250
place these costly collision resolution

00:26:57,120 --> 00:27:03,299
techniques will have to be useless

00:26:59,250 --> 00:27:06,149
frequently so that's hashmaps moving on

00:27:03,299 --> 00:27:07,879
to bloom filters we are now interested

00:27:06,149 --> 00:27:10,470
in an index structure that indicates

00:27:07,879 --> 00:27:13,440
record existence so let's call it an

00:27:10,470 --> 00:27:16,500
existence index specifically a bloom

00:27:13,440 --> 00:27:18,299
filter is a model which predicts whether

00:27:16,500 --> 00:27:21,120
or not a particular key is stored in the

00:27:18,299 --> 00:27:24,539
database with the additional requirement

00:27:21,120 --> 00:27:27,629
that a prediction of no have an error of

00:27:24,539 --> 00:27:30,059
0 and a prediction of yes have some

00:27:27,629 --> 00:27:32,429
error but this air can be deterministic

00:27:30,059 --> 00:27:34,590
we mitigated typically by giving the

00:27:32,429 --> 00:27:38,519
model access to either additional

00:27:34,590 --> 00:27:40,019
compute and/or additional memory from a

00:27:38,519 --> 00:27:42,600
machine learning perspective this seems

00:27:40,019 --> 00:27:45,629
like a job for a binary classifier that

00:27:42,600 --> 00:27:49,799
is a model which predicts a percentage

00:27:45,629 --> 00:27:53,070
between 0 and 100 and has a threshold

00:27:49,799 --> 00:27:55,379
value such that predictions above that

00:27:53,070 --> 00:27:58,049
number are classified as being in the

00:27:55,379 --> 00:28:02,330
database and predictions below are

00:27:58,049 --> 00:28:05,399
classified as not being in the database

00:28:02,330 --> 00:28:08,039
and so just as in the range and point

00:28:05,399 --> 00:28:09,629
index scenarios we have to break with

00:28:08,039 --> 00:28:11,399
standard machine learning methodology

00:28:09,629 --> 00:28:15,480
but this time we do it in a different

00:28:11,399 --> 00:28:18,179
way specifically usually when we train a

00:28:15,480 --> 00:28:20,879
binary classifier we feed the model

00:28:18,179 --> 00:28:23,129
examples of both classes but in this

00:28:20,879 --> 00:28:24,870
case we only have examples of the

00:28:23,129 --> 00:28:27,809
positive class that is to say the keys

00:28:24,870 --> 00:28:31,440
which are in a database so the first

00:28:27,809 --> 00:28:34,529
trick we have to use is to just make up

00:28:31,440 --> 00:28:36,960
some fake keys that is values which come

00:28:34,529 --> 00:28:39,389
from the allowed key space but are not

00:28:36,960 --> 00:28:41,399
actually used by our records and in

00:28:39,389 --> 00:28:43,440
these fake keys we add to the collection

00:28:41,399 --> 00:28:45,710
of real keys and then we use this

00:28:43,440 --> 00:28:48,600
combined data set to train our models

00:28:45,710 --> 00:28:50,970
the second trick we use is to adjust the

00:28:48,600 --> 00:28:53,970
threshold value to match our desired

00:28:50,970 --> 00:28:55,830
false positive rate remember the set of

00:28:53,970 --> 00:28:57,240
keys is static so

00:28:55,830 --> 00:29:00,330
we just keep changing that threshold

00:28:57,240 --> 00:29:02,039
until we reach that desired number now

00:29:00,330 --> 00:29:04,799
well of course still be left with a

00:29:02,039 --> 00:29:07,230
false negative rate which remember we

00:29:04,799 --> 00:29:11,190
need to get down to zero so trick three

00:29:07,230 --> 00:29:13,190
is to actually just make a separate

00:29:11,190 --> 00:29:15,899
bloom filter a traditional one which

00:29:13,190 --> 00:29:17,669
will be applied to all the keys

00:29:15,899 --> 00:29:19,919
predicted by the machine learning model

00:29:17,669 --> 00:29:21,600
to belong to the negative class as a

00:29:19,919 --> 00:29:23,460
double check just to ensure that we get

00:29:21,600 --> 00:29:26,129
that negative false sorry that false

00:29:23,460 --> 00:29:28,049
negative rate equal to zero and while

00:29:26,129 --> 00:29:29,879
that might be seen as a sort of cop-out

00:29:28,049 --> 00:29:35,519
and talk about using machine learning to

00:29:29,879 --> 00:29:37,379
replace database structures we still we

00:29:35,519 --> 00:29:39,570
still greatly reduce the resources

00:29:37,379 --> 00:29:42,330
required to implement our existence

00:29:39,570 --> 00:29:44,669
index in particular because bloom

00:29:42,330 --> 00:29:47,309
filters scale linearly with the number

00:29:44,669 --> 00:29:50,369
of keys they were responsible for and

00:29:47,309 --> 00:29:52,139
given that the number of keys our

00:29:50,369 --> 00:29:54,269
overflow bloom filter will be

00:29:52,139 --> 00:29:55,740
responsible for scales with the false

00:29:54,269 --> 00:29:57,929
negative rate of our machine learning

00:29:55,740 --> 00:30:00,570
model even if the binary classifier has

00:29:57,929 --> 00:30:02,369
a let's say 50% false negative rate

00:30:00,570 --> 00:30:07,649
we've managed to reduce the size or

00:30:02,369 --> 00:30:09,029
bloom Fister we need by half so I've

00:30:07,649 --> 00:30:11,480
told you about machine learning models

00:30:09,029 --> 00:30:16,379
and how they can be used to supplant or

00:30:11,480 --> 00:30:19,499
complement B trees hash maps and bloom

00:30:16,379 --> 00:30:22,889
filters for the purposes of range point

00:30:19,499 --> 00:30:24,450
and existence indexing respectively what

00:30:22,889 --> 00:30:26,610
I haven't told you is how well machine

00:30:24,450 --> 00:30:31,070
learning based index systems held up

00:30:26,610 --> 00:30:33,419
against their classical counterparts so

00:30:31,070 --> 00:30:36,570
now how were crash Grinnell's

00:30:33,419 --> 00:30:39,570
benchmarking results all the results

00:30:36,570 --> 00:30:43,080
were good at least according to Google

00:30:39,570 --> 00:30:44,970
and while I don't have time today to go

00:30:43,080 --> 00:30:48,059
into the details of the benchmarks and

00:30:44,970 --> 00:30:49,889
nor am i as a data scientist really on

00:30:48,059 --> 00:30:52,110
solid enough footing to be able to

00:30:49,889 --> 00:30:54,659
evaluate the appropriateness of the

00:30:52,110 --> 00:30:56,610
tests that they performed I think I can

00:30:54,659 --> 00:30:58,409
confidently say that this idea has

00:30:56,610 --> 00:31:00,899
opened up the possibility for new

00:30:58,409 --> 00:31:02,970
research programs which especially given

00:31:00,899 --> 00:31:05,340
the increased likelihood for the

00:31:02,970 --> 00:31:07,139
inclusion of machine friendly machine

00:31:05,340 --> 00:31:09,600
learning friendly GPUs and potentially

00:31:07,139 --> 00:31:11,970
even GPUs in commodity hardware

00:31:09,600 --> 00:31:13,889
may very well result in the adoption by

00:31:11,970 --> 00:31:17,460
future database systems of the ideas I

00:31:13,889 --> 00:31:20,970
talked about today rounding up I'd like

00:31:17,460 --> 00:31:23,669
to thank Tim Kroshka Alex bill tell EDG

00:31:20,970 --> 00:31:26,789
deft teen and Niklas Bowie Soltis for

00:31:23,669 --> 00:31:28,919
their novel idea I'd like to thank my

00:31:26,789 --> 00:31:31,559
employer go data-driven for flying out

00:31:28,919 --> 00:31:34,229
here and let me speak to you on company

00:31:31,559 --> 00:31:36,119
time and I'd like to thank you the

00:31:34,229 --> 00:31:37,529
audience for your attention and now if

00:31:36,119 --> 00:31:46,889
there's any time for questions I'd be

00:31:37,529 --> 00:31:48,979
more than happy to feel them we've got

00:31:46,889 --> 00:31:51,419
some time for questions are there ones

00:31:48,979 --> 00:31:59,879
there's one here in the Nova and it's

00:31:51,419 --> 00:32:02,429
not here hello texting interesting talk

00:31:59,879 --> 00:32:05,340
so one of the things you mentioned is

00:32:02,429 --> 00:32:07,830
that your models and all the things in

00:32:05,340 --> 00:32:08,429
your database but in a usual production

00:32:07,830 --> 00:32:10,289
use case

00:32:08,429 --> 00:32:11,519
you're always inserting records what

00:32:10,289 --> 00:32:13,619
does that mean every time you insert a

00:32:11,519 --> 00:32:15,749
new record you need to be retraining

00:32:13,619 --> 00:32:17,759
your model that's correct

00:32:15,749 --> 00:32:20,039
so the scenario given in this talk is

00:32:17,759 --> 00:32:21,929
that as you say that the it's it's

00:32:20,039 --> 00:32:23,929
static for some time frame the authors

00:32:21,929 --> 00:32:26,460
of the paper do address the concern of

00:32:23,929 --> 00:32:32,489
doing either inserts in the middle or

00:32:26,460 --> 00:32:35,729
pens on the end yes when you have new

00:32:32,489 --> 00:32:37,440
data you have to retrain your model so

00:32:35,729 --> 00:32:39,059
they propose a couple solutions one you

00:32:37,440 --> 00:32:40,739
either have place or like an overflow

00:32:39,059 --> 00:32:43,379
buffer and then only periodically update

00:32:40,739 --> 00:32:45,629
to your database in the first place only

00:32:43,379 --> 00:32:47,940
needed to be updated let's say one time

00:32:45,629 --> 00:32:51,929
at night and so that wouldn't really

00:32:47,940 --> 00:32:54,809
interrupt your system and three they

00:32:51,929 --> 00:32:57,119
also consider the idea that you could

00:32:54,809 --> 00:32:58,950
insert but then you don't just insert a

00:32:57,119 --> 00:33:00,809
single record like you do in a tree you

00:32:58,950 --> 00:33:03,330
have to insert let's say some additional

00:33:00,809 --> 00:33:05,369
block of space which you determine based

00:33:03,330 --> 00:33:08,580
on your previously calculated cumulative

00:33:05,369 --> 00:33:10,590
key distribution so you make the

00:33:08,580 --> 00:33:12,090
assumption that this distribution is the

00:33:10,590 --> 00:33:14,720
same regardless of how many keys you

00:33:12,090 --> 00:33:14,720
enter in the future

00:33:20,660 --> 00:33:32,700
can you please pass this down Thanks so

00:33:30,240 --> 00:33:34,710
a little bit similar question the data

00:33:32,700 --> 00:33:36,419
set was static but what happens with a

00:33:34,710 --> 00:33:37,710
cold start so you still need to have

00:33:36,419 --> 00:33:39,870
some sort of distribution at the

00:33:37,710 --> 00:33:41,549
beginning to train the model and usually

00:33:39,870 --> 00:33:44,070
when you have a use case I'm starting I

00:33:41,549 --> 00:33:45,630
have a business model but no data how I

00:33:44,070 --> 00:33:48,360
will build the index without the data

00:33:45,630 --> 00:33:49,950
the point is that you don't so this

00:33:48,360 --> 00:33:51,360
particular use case is when you already

00:33:49,950 --> 00:33:55,679
have some body of data and then want to

00:33:51,360 --> 00:33:57,390
build an index on top of it nothing

00:33:55,679 --> 00:33:58,890
the authors don't propose that this is

00:33:57,390 --> 00:34:00,390
the solution for every you case they

00:33:58,890 --> 00:34:03,929
just say to here the specific you cases

00:34:00,390 --> 00:34:05,010
where it could be advantageous so maybe

00:34:03,929 --> 00:34:07,590
I'll ask another question

00:34:05,010 --> 00:34:09,389
is all of this still purely theoretical

00:34:07,590 --> 00:34:12,990
or was they'll be like a database

00:34:09,389 --> 00:34:14,520
outdoor and development that will be

00:34:12,990 --> 00:34:18,240
able to take this kind of like modeled

00:34:14,520 --> 00:34:19,950
index yeah put it to use so I said this

00:34:18,240 --> 00:34:24,510
paper came out in December so December

00:34:19,950 --> 00:34:25,919
2017 to the extent to which I own I know

00:34:24,510 --> 00:34:27,629
and the public knows this is all

00:34:25,919 --> 00:34:31,169
relatively theoretical aside from their

00:34:27,629 --> 00:34:33,300
their tests and then part of what they

00:34:31,169 --> 00:34:35,310
encourage in the papers and believe that

00:34:33,300 --> 00:34:38,790
this should become a research Avenue

00:34:35,310 --> 00:34:39,929
that said it's Google and I wouldn't

00:34:38,790 --> 00:34:42,990
think that they would release such

00:34:39,929 --> 00:34:44,520
information so lightly so my assumption

00:34:42,990 --> 00:34:46,109
is that there's at least some

00:34:44,520 --> 00:34:47,879
experimentations going on deep in the

00:34:46,109 --> 00:34:50,159
bowels of Google trying to implement

00:34:47,879 --> 00:34:53,399
this in production but that's just the

00:34:50,159 --> 00:34:58,500
speculation on my part all right there's

00:34:53,399 --> 00:35:03,960
one more here what kind of objects were

00:34:58,500 --> 00:35:05,520
used for the keys right so the they had

00:35:03,960 --> 00:35:09,210
a number of test data sets which they

00:35:05,520 --> 00:35:11,820
spoke of they used let me see if I

00:35:09,210 --> 00:35:13,109
remember so they made up a set of

00:35:11,820 --> 00:35:14,520
integers based on a log normal

00:35:13,109 --> 00:35:16,530
distribution they have a set of

00:35:14,520 --> 00:35:18,780
longitudes based on some map data they

00:35:16,530 --> 00:35:21,900
had a set of document IDs which were

00:35:18,780 --> 00:35:26,850
strings and they had a set of time

00:35:21,900 --> 00:35:27,930
stamps which were also integers now I

00:35:26,850 --> 00:35:29,250
didn't want to talk too much about the

00:35:27,930 --> 00:35:30,600
results but the results show that for

00:35:29,250 --> 00:35:32,580
integers this performs my

00:35:30,600 --> 00:35:34,800
better strings are when you have

00:35:32,580 --> 00:35:37,670
difficulties and that comes largely

00:35:34,800 --> 00:35:40,910
because of the challenges in

00:35:37,670 --> 00:35:44,790
representing variable length strings as

00:35:40,910 --> 00:35:47,160
keys and then uniform way but yet so

00:35:44,790 --> 00:35:49,050
direct answer is index and strings what

00:35:47,160 --> 00:35:51,690
they talk about integers and strings

00:35:49,050 --> 00:35:54,470
okay let's take one more question and

00:35:51,690 --> 00:35:54,470
that is here to the left

00:35:54,920 --> 00:36:02,340
so those model and indices they are

00:35:59,820 --> 00:36:04,050
being calculated on top of the data like

00:36:02,340 --> 00:36:06,390
from the data but has there been any

00:36:04,050 --> 00:36:08,790
research of actuary let's say mapping an

00:36:06,390 --> 00:36:11,850
existing classic like B 3 index to a

00:36:08,790 --> 00:36:17,700
model like try to create a model based

00:36:11,850 --> 00:36:20,840
on existing indices no tomatoes now all

00:36:17,700 --> 00:36:20,840
right let's think this peak again

00:36:25,500 --> 00:36:27,560

YouTube URL: https://www.youtube.com/watch?v=0q9mxMekBeE


