Title: Berlin Buzzwords 2018: Marcin Szymaniuk – DataFrames in Apache Spark - the analysts perspective
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Are you a data analyst who works with Apache Spark and often gets confused by failures you don’t understand? Have you seen a bunch of presentations or blog posts about Apache Spark performance but you are still not certain how to apply the hints you have been given in practice?

Apache Spark is commonly used by people who are not experts in programming but they know SQL and sometimes basic Python. They treat Spark as a tool for getting business value from the the data. And that is how it should be! Although it’s common that queries they run do not work for any obvious reason. This talk is designed for such Spark users and will be focused on common problems with Spark (especially DataFrames and SQL) which can be solved by anyone familiar with SQL. You don’t need to read bytecode to understand the techniques presented and apply them in practice!

This talk will be a case study of multiple DataFrame queries in Apache Spark which initially do not work. I will not only explain how to fix them, but we will go through the solution step-by-step so you will learn what to pay attention to and how to apply similar techniques to your codebase!

Read more:
https://2018.berlinbuzzwords.de/18/session/dataframes-spark-analysts-perspective

About Marcin Szymaniuk:
https://2018.berlinbuzzwords.de/users/marcin-szymaniuk

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,360 --> 00:00:12,389
hello my name is marching today we are

00:00:08,850 --> 00:00:16,139
going to talk about data analysts

00:00:12,389 --> 00:00:19,160
perspective on data frames

00:00:16,139 --> 00:00:21,330
I'm a data engineer at Santos data

00:00:19,160 --> 00:00:24,210
Atlantis data will help clients with

00:00:21,330 --> 00:00:26,340
basically everything data related so

00:00:24,210 --> 00:00:28,230
from plaster installations through

00:00:26,340 --> 00:00:31,700
application architecture development

00:00:28,230 --> 00:00:35,280
training supporting teams of data

00:00:31,700 --> 00:00:36,660
analysts and data engineers and I have

00:00:35,280 --> 00:00:39,930
been doing all of that but on daily

00:00:36,660 --> 00:00:45,360
basis I act as data architect and data

00:00:39,930 --> 00:00:47,550
data developer during this talk first of

00:00:45,360 --> 00:00:50,250
all I am going to introduce you to what

00:00:47,550 --> 00:00:53,399
I mean by data analysts or data

00:00:50,250 --> 00:00:56,249
scientists perspective what and why they

00:00:53,399 --> 00:00:59,219
are using spark then we will have a

00:00:56,249 --> 00:01:03,570
quick walk through spark execution model

00:00:59,219 --> 00:01:06,330
so how SPARC execute the program in the

00:01:03,570 --> 00:01:09,540
distributed environment and then we will

00:01:06,330 --> 00:01:12,659
have a walk through multiple use cases

00:01:09,540 --> 00:01:15,390
multiple problematic queries which fails

00:01:12,659 --> 00:01:17,750
initially and then we will have a look

00:01:15,390 --> 00:01:21,990
at why they fail so at the end you know

00:01:17,750 --> 00:01:25,140
how to fix such kind of queries and the

00:01:21,990 --> 00:01:27,630
query selection is based on the fact

00:01:25,140 --> 00:01:30,120
that they are very common so I will show

00:01:27,630 --> 00:01:33,180
you very common failures but at the same

00:01:30,120 --> 00:01:38,340
time there are they will be quite easy

00:01:33,180 --> 00:01:40,980
to solve by by anybody who is not who is

00:01:38,340 --> 00:01:43,320
not very good at networking or garbage

00:01:40,980 --> 00:01:46,110
collection so that makes it perfect use

00:01:43,320 --> 00:01:51,120
case for for data scientists data

00:01:46,110 --> 00:01:52,710
analysts and at the end I will very

00:01:51,120 --> 00:01:56,580
quickly summarize the lesson learned

00:01:52,710 --> 00:01:59,790
from these problematic use cases so

00:01:56,580 --> 00:02:01,740
let's start with data analysts left

00:01:59,790 --> 00:02:04,380
scientist perspective so what do they do

00:02:01,740 --> 00:02:07,140
they look at the data and they try to

00:02:04,380 --> 00:02:10,349
explain what has happened and why and

00:02:07,140 --> 00:02:12,660
the next step would be to use that

00:02:10,349 --> 00:02:14,910
knowledge in order to improve the

00:02:12,660 --> 00:02:18,120
product in order to drive a decisions to

00:02:14,910 --> 00:02:20,430
be made by a company and even steps

00:02:18,120 --> 00:02:22,349
further would be to build machine

00:02:20,430 --> 00:02:25,349
learning models so you can automate

00:02:22,349 --> 00:02:26,370
these stuff so you can you can make it

00:02:25,349 --> 00:02:29,730
better

00:02:26,370 --> 00:02:32,700
and this kind of things has been done

00:02:29,730 --> 00:02:35,130
for a very long time so the traditional

00:02:32,700 --> 00:02:38,670
approach is that you take the data from

00:02:35,130 --> 00:02:41,160
the source you move it to maybe some our

00:02:38,670 --> 00:02:43,560
server or to your local machine you do

00:02:41,160 --> 00:02:45,840
the analysis and you get the result but

00:02:43,560 --> 00:02:47,700
if you have large data set you early you

00:02:45,840 --> 00:02:51,180
have to you have to limit it you have to

00:02:47,700 --> 00:02:54,209
either take a sample of your data or an

00:02:51,180 --> 00:02:57,480
aggregate of of your data set and that

00:02:54,209 --> 00:03:00,720
means you might end up with these blind

00:02:57,480 --> 00:03:02,670
men and an elephant problem so a few

00:03:00,720 --> 00:03:04,769
blind men are touching an elephant and

00:03:02,670 --> 00:03:07,860
they get completely different idea about

00:03:04,769 --> 00:03:09,480
what an elephant is and something very

00:03:07,860 --> 00:03:12,390
similar might happen to your data if

00:03:09,480 --> 00:03:16,019
you're looking at just limited data set

00:03:12,390 --> 00:03:18,890
on the other hand if you use spark you

00:03:16,019 --> 00:03:21,870
can analyze the whole data set because

00:03:18,890 --> 00:03:23,760
because it distributes the load you can

00:03:21,870 --> 00:03:25,920
analyze the data set faster you get

00:03:23,760 --> 00:03:30,630
faster feedback loop and also since

00:03:25,920 --> 00:03:32,370
spark kind of brings the the analysis to

00:03:30,630 --> 00:03:34,709
the data not the other way around you

00:03:32,370 --> 00:03:37,230
don't end up with extra data copies and

00:03:34,709 --> 00:03:39,920
that might be quite important from the

00:03:37,230 --> 00:03:44,489
point of view of data privacy gdpr and

00:03:39,920 --> 00:03:48,989
stuff like that so coming back to data

00:03:44,489 --> 00:03:52,709
analysts perspective the end result of

00:03:48,989 --> 00:03:55,079
what you do is probably something quite

00:03:52,709 --> 00:03:57,420
quite sophisticated algorithm something

00:03:55,079 --> 00:03:59,819
might be quite complex but the reality

00:03:57,420 --> 00:04:02,489
is that in order to achieve that you

00:03:59,819 --> 00:04:05,010
really have to run a lot of ad-hoc

00:04:02,489 --> 00:04:08,010
queries to understand the data to come

00:04:05,010 --> 00:04:10,920
up with come come up with reports and so

00:04:08,010 --> 00:04:14,639
on and also to do the data cleanup so

00:04:10,920 --> 00:04:18,120
you come up with with good feature set

00:04:14,639 --> 00:04:20,789
for your machine learning model so this

00:04:18,120 --> 00:04:23,070
is a diagram I have found in the

00:04:20,789 --> 00:04:27,960
internet and it says over 50 percent of

00:04:23,070 --> 00:04:30,180
machine data scientists time is spent on

00:04:27,960 --> 00:04:33,780
data cleanup probably you don't like

00:04:30,180 --> 00:04:35,610
that but what you would like to achieve

00:04:33,780 --> 00:04:39,020
at least is that you run these kind of

00:04:35,610 --> 00:04:42,620
ad hoc queries as fast as possible

00:04:39,020 --> 00:04:44,600
so spark lets you express all kind of

00:04:42,620 --> 00:04:46,610
ad-hoc queries so even if you are not

00:04:44,600 --> 00:04:48,260
familiar with it if you are familiar

00:04:46,610 --> 00:04:51,110
with sequel then you see group by

00:04:48,260 --> 00:04:54,440
drawing select you already know what

00:04:51,110 --> 00:04:59,150
this program is doing but the problem is

00:04:54,440 --> 00:05:01,100
that sometimes queries just fail for not

00:04:59,150 --> 00:05:03,800
obvious reason so imagine you are going

00:05:01,100 --> 00:05:07,190
to the office on Monday and you have

00:05:03,800 --> 00:05:08,900
like 26 queries you would like to run so

00:05:07,190 --> 00:05:10,970
we understand the data better and you go

00:05:08,900 --> 00:05:13,490
further from there and you hope to run

00:05:10,970 --> 00:05:16,100
them by the end of the day but then you

00:05:13,490 --> 00:05:19,010
realize okay it's Wednesday already and

00:05:16,100 --> 00:05:20,630
I'm stuck with query number 6 and it's

00:05:19,010 --> 00:05:23,060
not a cluster problem because all the

00:05:20,630 --> 00:05:25,670
other guys around me are running them so

00:05:23,060 --> 00:05:29,890
what is going on what do you do with

00:05:25,670 --> 00:05:32,480
queries which fail or which never finish

00:05:29,890 --> 00:05:36,170
so what we are going to do we are going

00:05:32,480 --> 00:05:38,750
to do a quick deep dive into how spark

00:05:36,170 --> 00:05:41,960
executes the program so you have better

00:05:38,750 --> 00:05:44,930
disaster recovery plan than this so

00:05:41,960 --> 00:05:47,810
asking for help is completely fine but

00:05:44,930 --> 00:05:51,110
sometimes is good if you if you can

00:05:47,810 --> 00:05:53,570
solve the problem so let's do the deep

00:05:51,110 --> 00:05:55,700
dive the very first thing you have to

00:05:53,570 --> 00:05:57,260
keep in mind that when you work with

00:05:55,700 --> 00:05:59,030
data frames it feels that you are

00:05:57,260 --> 00:06:00,800
working with some local collection you

00:05:59,030 --> 00:06:03,170
are not exposed to any details about

00:06:00,800 --> 00:06:06,830
machines and stuff like that but the

00:06:03,170 --> 00:06:09,110
data set is actually partitioned and it

00:06:06,830 --> 00:06:15,980
lives in multiple machines of your

00:06:09,110 --> 00:06:18,920
cluster and SPARC do two types of

00:06:15,980 --> 00:06:21,470
transformations on top of on top of the

00:06:18,920 --> 00:06:24,020
data frame it do narrow transformations

00:06:21,470 --> 00:06:26,360
which work on single partition at the

00:06:24,020 --> 00:06:30,860
time so the simple simplest operation

00:06:26,360 --> 00:06:33,290
ever and it does wide transformations

00:06:30,860 --> 00:06:35,150
where it has to exchange the data over

00:06:33,290 --> 00:06:38,090
the network it has to exchange the data

00:06:35,150 --> 00:06:40,970
between multiple partitions so just very

00:06:38,090 --> 00:06:44,320
quick example let's say you want to

00:06:40,970 --> 00:06:48,800
calculate hash of a user ID and then

00:06:44,320 --> 00:06:51,710
reverse a column string

00:06:48,800 --> 00:06:56,180
and maybe some up to different columns

00:06:51,710 --> 00:06:58,610
and you will end up with such an

00:06:56,180 --> 00:07:00,800
execution plan so what that means is

00:06:58,610 --> 00:07:04,099
pretty much that spark will read the

00:07:00,800 --> 00:07:05,930
input data it will squeeze all these

00:07:04,099 --> 00:07:08,659
operations together it will do the magic

00:07:05,930 --> 00:07:11,889
code generation to run it in a

00:07:08,659 --> 00:07:14,389
performant way and it will write the

00:07:11,889 --> 00:07:16,639
write the result so the point is that

00:07:14,389 --> 00:07:19,220
these kind of operations can be squeezed

00:07:16,639 --> 00:07:23,150
together and they're kind of executed

00:07:19,220 --> 00:07:26,000
locally to each partition and your

00:07:23,150 --> 00:07:29,319
execution plan might look slightly more

00:07:26,000 --> 00:07:32,300
complex but the point is as long as

00:07:29,319 --> 00:07:35,599
these operations are in context of

00:07:32,300 --> 00:07:39,440
single stage they will be executed kind

00:07:35,599 --> 00:07:41,719
of locally but sometimes you want to do

00:07:39,440 --> 00:07:44,030
something more complex so for instance

00:07:41,719 --> 00:07:46,539
you would like to group your events

00:07:44,030 --> 00:07:50,659
based on user the ID and maybe do some

00:07:46,539 --> 00:07:53,449
further analysis so what spark will have

00:07:50,659 --> 00:07:57,050
to do it will have to pull all the

00:07:53,449 --> 00:07:59,659
events with user ID one to one place all

00:07:57,050 --> 00:08:04,180
the events for user ID two to another

00:07:59,659 --> 00:08:07,099
place and so on so we have many arrows

00:08:04,180 --> 00:08:09,889
nodes are talking to each other and a

00:08:07,099 --> 00:08:13,490
lot of a lot of data is being exchanged

00:08:09,889 --> 00:08:16,460
here so here are samples of operations

00:08:13,490 --> 00:08:18,440
which require to exchange data between

00:08:16,460 --> 00:08:20,690
partitions here our sample of operations

00:08:18,440 --> 00:08:22,490
which requires to do a shuffle and we

00:08:20,690 --> 00:08:23,569
are going to focus on join and

00:08:22,490 --> 00:08:28,479
repartition

00:08:23,569 --> 00:08:32,180
so I will show you a few use cases where

00:08:28,479 --> 00:08:34,279
where where we use them so this is like

00:08:32,180 --> 00:08:37,579
just to give you an overview what it

00:08:34,279 --> 00:08:39,709
will look like in in spark UI when you

00:08:37,579 --> 00:08:42,829
write a program which does a drawing of

00:08:39,709 --> 00:08:46,779
users and events this is what Sparky Y

00:08:42,829 --> 00:08:49,459
would look like and these arrows are

00:08:46,779 --> 00:08:54,170
indicators that the shuffle happens

00:08:49,459 --> 00:08:55,250
these URLs indicates that that spark

00:08:54,170 --> 00:08:59,199
will exchange the data between

00:08:55,250 --> 00:09:01,120
partitions so in order to understand how

00:08:59,199 --> 00:09:03,829
spark

00:09:01,120 --> 00:09:06,860
paralyzed the load I'll show you the

00:09:03,829 --> 00:09:12,860
simplest scenario ever first so we reach

00:09:06,860 --> 00:09:15,950
some events from from from HDFS and

00:09:12,860 --> 00:09:18,680
these events contains some timestamp :

00:09:15,950 --> 00:09:21,170
and out of this timestamp column you

00:09:18,680 --> 00:09:24,380
would like to calculate the Year month

00:09:21,170 --> 00:09:28,610
and the day when the event happened and

00:09:24,380 --> 00:09:31,209
spark will squeeze all the calculation

00:09:28,610 --> 00:09:34,220
all these three column calculation into

00:09:31,209 --> 00:09:38,120
into single operation inside one task

00:09:34,220 --> 00:09:40,760
and one task will be reading a single

00:09:38,120 --> 00:09:43,040
block from HDFS or a portion of your

00:09:40,760 --> 00:09:45,380
data and eventually it will write out

00:09:43,040 --> 00:09:49,130
the result and if you have let's say

00:09:45,380 --> 00:09:53,060
your data set in 8,000 blocks in HDFS it

00:09:49,130 --> 00:09:55,730
will create 8,000 tasks like that so we

00:09:53,060 --> 00:09:58,279
will have many of them all of them will

00:09:55,730 --> 00:10:00,470
look exactly the same all of them are

00:09:58,279 --> 00:10:02,930
independent there are no arrows between

00:10:00,470 --> 00:10:04,850
them there is really nothing go going on

00:10:02,930 --> 00:10:08,180
between them so it's very easy to

00:10:04,850 --> 00:10:12,230
parallelize so it's very simple so let's

00:10:08,180 --> 00:10:14,449
have a look at small extension of this

00:10:12,230 --> 00:10:19,490
use case so we would like to organize

00:10:14,449 --> 00:10:25,209
our data set by the date so we have we

00:10:19,490 --> 00:10:27,709
get better better performing queries so

00:10:25,209 --> 00:10:32,120
if you are doing a lookup in your not

00:10:27,709 --> 00:10:34,670
organized data set based on the Year

00:10:32,120 --> 00:10:36,860
spark will have to scan the whole the

00:10:34,670 --> 00:10:39,740
whole data set and then and filter

00:10:36,860 --> 00:10:41,990
filter out the events you are interested

00:10:39,740 --> 00:10:44,930
in as on the other hand if you organize

00:10:41,990 --> 00:10:48,949
your data by the years of 2015 goes to

00:10:44,930 --> 00:10:51,800
one directory 2016 to another one and so

00:10:48,949 --> 00:10:55,850
on you will end up with structure like

00:10:51,800 --> 00:10:58,279
this and the queries for a given year

00:10:55,850 --> 00:11:00,230
will perform much better and of course

00:10:58,279 --> 00:11:03,139
it's not a rocket science it has been

00:11:00,230 --> 00:11:04,970
there for quite a long time and what you

00:11:03,139 --> 00:11:08,209
would like to do you'd maybe like to

00:11:04,970 --> 00:11:10,990
organize it by not by year by but by a

00:11:08,209 --> 00:11:14,680
day and since this is very well known

00:11:10,990 --> 00:11:16,779
approach to to optimizing the queries

00:11:14,680 --> 00:11:20,589
Parc supports that out-of-the-box you

00:11:16,779 --> 00:11:22,750
can call partition by you specify which

00:11:20,589 --> 00:11:24,399
columns you want to partition the data

00:11:22,750 --> 00:11:27,490
by your partitioning it here by day

00:11:24,399 --> 00:11:29,830
month and a year and in many cases it

00:11:27,490 --> 00:11:33,880
works but I'll show you an example where

00:11:29,830 --> 00:11:38,589
it does not so you run a program like

00:11:33,880 --> 00:11:40,660
this then you go to spark UI you can see

00:11:38,589 --> 00:11:45,279
that there is only one stage going on

00:11:40,660 --> 00:11:47,589
and spark tells you that all the tasks

00:11:45,279 --> 00:11:49,600
has finished and you don't trust it so

00:11:47,589 --> 00:11:54,450
we would like to go deeper and look at

00:11:49,600 --> 00:11:58,089
the task overview and you can see many

00:11:54,450 --> 00:12:00,910
many tasks and all of them are marked as

00:11:58,089 --> 00:12:02,500
succeeded but the job is still up and

00:12:00,910 --> 00:12:04,839
running it's still marked as up and

00:12:02,500 --> 00:12:09,430
running but you don't trust me we think

00:12:04,839 --> 00:12:13,779
the UI is somehow off so you go to HDFS

00:12:09,430 --> 00:12:16,690
and you list the directory where you

00:12:13,779 --> 00:12:18,850
expect the the output to list the

00:12:16,690 --> 00:12:21,010
content of this directory for instance

00:12:18,850 --> 00:12:25,870
and you want to count the number of

00:12:21,010 --> 00:12:28,450
files there and you get resolved 22 and

00:12:25,870 --> 00:12:30,880
like one minute later you run it and you

00:12:28,450 --> 00:12:33,279
notice that there is a difference is 25

00:12:30,880 --> 00:12:35,200
and after some time you run it and and

00:12:33,279 --> 00:12:39,100
you can see that basically the number of

00:12:35,200 --> 00:12:44,800
tasks in the output directory is still

00:12:39,100 --> 00:12:46,690
growing so what is going on in order to

00:12:44,800 --> 00:12:49,570
understand what's going on you need to

00:12:46,690 --> 00:12:53,410
understand how the partition by method

00:12:49,570 --> 00:12:58,029
is implemented so let's say you have 90

00:12:53,410 --> 00:13:02,490
days of data in your in your in your

00:12:58,029 --> 00:13:05,800
input directory each task will read

00:13:02,490 --> 00:13:09,640
block from HDFS it will calculate the

00:13:05,800 --> 00:13:12,010
columns and it will write out single

00:13:09,640 --> 00:13:15,959
file file pipe per day because that's

00:13:12,010 --> 00:13:19,959
how you wanted it to partition by and

00:13:15,959 --> 00:13:21,850
then you will have many of these tasks

00:13:19,959 --> 00:13:24,640
because you have many blocks as an input

00:13:21,850 --> 00:13:27,040
data and by the end of the day if you

00:13:24,640 --> 00:13:28,390
have 90 days of the data and like 8,000

00:13:27,040 --> 00:13:32,140
blocks in HDFS

00:13:28,390 --> 00:13:35,590
you will end up with these many blocks

00:13:32,140 --> 00:13:38,260
as an output so you do the math but the

00:13:35,590 --> 00:13:40,660
point is it's quite quite large number

00:13:38,260 --> 00:13:42,700
and all of these files will be small so

00:13:40,660 --> 00:13:44,920
it doesn't make that much sense and the

00:13:42,700 --> 00:13:48,010
consequence will be that which will be

00:13:44,920 --> 00:13:51,760
slow it will be slow because HDFS is not

00:13:48,010 --> 00:13:53,200
really optimized for for handling many

00:13:51,760 --> 00:13:55,540
small files is right it's rather

00:13:53,200 --> 00:13:58,690
optimized for for batch workload and

00:13:55,540 --> 00:14:00,520
what has happened here is that spark has

00:13:58,690 --> 00:14:02,830
created the output in temporary

00:14:00,520 --> 00:14:07,240
directory but now it's kind of moving

00:14:02,830 --> 00:14:09,430
the temporary directory to the actual

00:14:07,240 --> 00:14:14,050
output location and it's hammering name

00:14:09,430 --> 00:14:15,520
node kind of a central piece of of HDFS

00:14:14,050 --> 00:14:18,040
it's hammering the name node and

00:14:15,520 --> 00:14:20,290
everything is just just slow but in

00:14:18,040 --> 00:14:23,140
extreme cases you might even kill the

00:14:20,290 --> 00:14:26,280
cluster and this is what your cluster

00:14:23,140 --> 00:14:29,620
admin will look like when he noticed and

00:14:26,280 --> 00:14:34,870
the problem is he actually knows who you

00:14:29,620 --> 00:14:36,640
are so we want to fix that so mmm the

00:14:34,870 --> 00:14:39,460
way to fix that is actually quite simple

00:14:36,640 --> 00:14:42,190
there is a method called repartition and

00:14:39,460 --> 00:14:45,390
that can reorganize our data based on

00:14:42,190 --> 00:14:49,960
what we want to but without touching

00:14:45,390 --> 00:14:52,360
HDFS so we call the repartition we

00:14:49,960 --> 00:14:57,070
specify the exact same columns we used

00:14:52,360 --> 00:15:01,060
for partition by and then when we look

00:14:57,070 --> 00:15:04,320
at spark UI it will be slightly more

00:15:01,060 --> 00:15:08,500
complex we'll have two stages now and

00:15:04,320 --> 00:15:10,930
what these stages will do is stage

00:15:08,500 --> 00:15:14,650
number one a task in stage number one

00:15:10,930 --> 00:15:17,230
we'll read the data from HDFS it will do

00:15:14,650 --> 00:15:20,560
the current calculation and it will

00:15:17,230 --> 00:15:24,100
organize the data by day but it will

00:15:20,560 --> 00:15:26,050
save it just to the local disk and all

00:15:24,100 --> 00:15:30,340
the other tasks will do exactly the same

00:15:26,050 --> 00:15:31,960
and the rule here is that if task if day

00:15:30,340 --> 00:15:34,420
number three is going to the green

00:15:31,960 --> 00:15:38,650
bucket it will always go to the green

00:15:34,420 --> 00:15:41,650
bucket so it's consistent so task number

00:15:38,650 --> 00:15:43,019
one in stage two can pull all the data

00:15:41,650 --> 00:15:46,449
for day one

00:15:43,019 --> 00:15:49,959
task number two will pool day two and so

00:15:46,449 --> 00:15:53,290
on so here we are processing single day

00:15:49,959 --> 00:15:56,230
or maybe multiple days but the whole day

00:15:53,290 --> 00:15:59,230
is processed by the same task as the end

00:15:56,230 --> 00:16:04,839
result so we will end up with just one

00:15:59,230 --> 00:16:07,839
file in HDFS per day so we have limited

00:16:04,839 --> 00:16:10,389
number of files initially when we when

00:16:07,839 --> 00:16:12,759
we were running just partition by we the

00:16:10,389 --> 00:16:15,490
job was really slow so you have to know

00:16:12,759 --> 00:16:18,790
how the partition by works but once you

00:16:15,490 --> 00:16:20,800
know that you definitely should consider

00:16:18,790 --> 00:16:23,290
we partitioning the data when you are

00:16:20,800 --> 00:16:26,620
writing it to HDFS and in order to

00:16:23,290 --> 00:16:31,079
decide whether to use it or not you need

00:16:26,620 --> 00:16:34,779
to know your data distribution so

00:16:31,079 --> 00:16:38,559
repartition is the rescue here but is it

00:16:34,779 --> 00:16:41,170
some kind of a silver bullet well no

00:16:38,559 --> 00:16:43,149
because I'm going to show you another

00:16:41,170 --> 00:16:47,949
use case where it arc when it actually

00:16:43,149 --> 00:16:50,829
fails after we partition in so let's say

00:16:47,949 --> 00:16:53,949
we run exactly the same code just on

00:16:50,829 --> 00:16:56,679
different data set so I will show you my

00:16:53,949 --> 00:17:00,399
small example I have run it

00:16:56,679 --> 00:17:02,379
I have run it on some small data set the

00:17:00,399 --> 00:17:05,649
the query plan looks exactly the same

00:17:02,379 --> 00:17:07,929
and when the stage number one has

00:17:05,649 --> 00:17:11,319
finished so it has organized the data by

00:17:07,929 --> 00:17:13,390
day it's a it's stored it locally but

00:17:11,319 --> 00:17:15,569
then you go to stage number two and you

00:17:13,390 --> 00:17:21,399
want to see what is going on there and

00:17:15,569 --> 00:17:24,339
what you can see here after sorting your

00:17:21,399 --> 00:17:27,189
tasks based on the amount of data they

00:17:24,339 --> 00:17:30,100
have read you can see that three of your

00:17:27,189 --> 00:17:33,610
tasks have read like three gigabytes of

00:17:30,100 --> 00:17:35,230
data but the other ones have read

00:17:33,610 --> 00:17:38,649
nothing and they are already succeeded

00:17:35,230 --> 00:17:40,840
so they are just so lazy and then you

00:17:38,649 --> 00:17:44,140
refresh the page and you can see these

00:17:40,840 --> 00:17:50,620
three tasks are growing and growing and

00:17:44,140 --> 00:17:54,850
growing so the reason for that is I in

00:17:50,620 --> 00:17:56,230
my data set had just three days of data

00:17:54,850 --> 00:17:59,350
and I'm repartition

00:17:56,230 --> 00:18:02,070
by day the rule is that the whole day is

00:17:59,350 --> 00:18:04,630
processed by single tasks after

00:18:02,070 --> 00:18:07,000
reporting and you end up with such kind

00:18:04,630 --> 00:18:12,130
of problems so imagine you have 100 gigs

00:18:07,000 --> 00:18:15,720
of data per day then your execution plan

00:18:12,130 --> 00:18:18,160
we will look exactly as before except

00:18:15,720 --> 00:18:21,190
single tasks will have to process

00:18:18,160 --> 00:18:24,820
hundred bits of data and most often this

00:18:21,190 --> 00:18:27,100
is not what you want to do most often

00:18:24,820 --> 00:18:30,400
you will end up with one of these

00:18:27,100 --> 00:18:33,970
problems so first of all spark might try

00:18:30,400 --> 00:18:36,630
to avoid out of memory and it will spill

00:18:33,970 --> 00:18:39,850
the data to disk but it will be slow and

00:18:36,630 --> 00:18:42,070
there are other possible problems which

00:18:39,850 --> 00:18:44,669
you can end up with depending on what

00:18:42,070 --> 00:18:46,900
exactly you're doing after the partition

00:18:44,669 --> 00:18:49,330
so what what could you do

00:18:46,900 --> 00:18:52,780
you could try to split the data further

00:18:49,330 --> 00:18:57,100
so you could try to repartition by hour

00:18:52,780 --> 00:18:59,140
and that will what what that will do so

00:18:57,100 --> 00:19:01,929
if we are if you are partitioning the

00:18:59,140 --> 00:19:04,750
data by day then that means day number

00:19:01,929 --> 00:19:07,809
one will go to one task and day number

00:19:04,750 --> 00:19:09,910
two will go to another one but if

00:19:07,809 --> 00:19:12,340
instead you are reproducing the data by

00:19:09,910 --> 00:19:16,540
something which splits the data further

00:19:12,340 --> 00:19:21,309
if you have such a common our number

00:19:16,540 --> 00:19:23,890
eleven day one will go to one task our

00:19:21,309 --> 00:19:25,450
number one will go to another one and so

00:19:23,890 --> 00:19:28,059
on sometimes it might be a little bit

00:19:25,450 --> 00:19:31,480
skewed but still you are better off with

00:19:28,059 --> 00:19:34,000
doing that and after running my job I

00:19:31,480 --> 00:19:36,370
can see many more tasks which are

00:19:34,000 --> 00:19:40,480
actually doing something and this is

00:19:36,370 --> 00:19:42,520
what I want to achieve so when you are

00:19:40,480 --> 00:19:44,950
dealing with the partitioning of your

00:19:42,520 --> 00:19:49,480
data first of all you really need to

00:19:44,950 --> 00:19:52,000
have you really really need to know what

00:19:49,480 --> 00:19:54,820
your data looks like how the keys are

00:19:52,000 --> 00:19:58,799
distributed so you you do it smart you

00:19:54,820 --> 00:20:02,320
you make sure your executors are getting

00:19:58,799 --> 00:20:06,549
enough load and what you want to achieve

00:20:02,320 --> 00:20:08,920
you really want to have to see all your

00:20:06,549 --> 00:20:11,260
tasks doing something at least so

00:20:08,920 --> 00:20:17,140
so you benefit from the fact that spark

00:20:11,260 --> 00:20:20,050
is a distributed system okay so we have

00:20:17,140 --> 00:20:22,960
an idea of how shuffle works for

00:20:20,050 --> 00:20:25,630
repartition so let's have a look at join

00:20:22,960 --> 00:20:29,820
something slightly more complex so you

00:20:25,630 --> 00:20:35,970
want to join events with users based on

00:20:29,820 --> 00:20:39,250
user ID : spark will create three stages

00:20:35,970 --> 00:20:45,190
stage number one will organize your

00:20:39,250 --> 00:20:48,790
users based on the user ID stage number

00:20:45,190 --> 00:20:50,980
two will organize the events based on

00:20:48,790 --> 00:20:53,830
the user ID so the key you are using for

00:20:50,980 --> 00:20:56,170
the join and it will do it in a

00:20:53,830 --> 00:20:59,110
consistent way because it's consistent

00:20:56,170 --> 00:21:02,650
then stage number three will pull the

00:20:59,110 --> 00:21:05,710
bucket one for users and the bucket one

00:21:02,650 --> 00:21:10,150
for events so they are matching and it

00:21:05,710 --> 00:21:12,040
will perform the partial join it will

00:21:10,150 --> 00:21:14,860
just perform the partial join here

00:21:12,040 --> 00:21:19,180
start task number two will perform

00:21:14,860 --> 00:21:22,150
another part of the join and so on so

00:21:19,180 --> 00:21:24,670
this is what it will look like in the

00:21:22,150 --> 00:21:27,880
spark UI and I would like you to pay

00:21:24,670 --> 00:21:31,750
attention to this number we have 200

00:21:27,880 --> 00:21:34,390
here and that means the the join will be

00:21:31,750 --> 00:21:37,690
processed in two hundred tasks so there

00:21:34,390 --> 00:21:40,150
will be two hundred tasks which will be

00:21:37,690 --> 00:21:45,220
pulling the data and and processing part

00:21:40,150 --> 00:21:47,680
of it and when I run some program

00:21:45,220 --> 00:21:50,170
performing the join I can see that my

00:21:47,680 --> 00:21:52,210
tasks are all of them are kind of busy

00:21:50,170 --> 00:21:54,250
and all of them are reading some data

00:21:52,210 --> 00:21:56,200
but I am concerned about the amount of

00:21:54,250 --> 00:21:59,560
the data I can see already almost ten

00:21:56,200 --> 00:22:04,690
gigs and I'm just doing simple join and

00:21:59,560 --> 00:22:08,530
again this might be might be problematic

00:22:04,690 --> 00:22:11,950
so let's assume you are processing ten

00:22:08,530 --> 00:22:14,470
terabytes of events which are uniformly

00:22:11,950 --> 00:22:17,560
distributed across your users and you

00:22:14,470 --> 00:22:21,470
are joining it with just one gigs of a

00:22:17,560 --> 00:22:25,720
few tzer's or some small user data set

00:22:21,470 --> 00:22:30,799
the join will look exactly the same and

00:22:25,720 --> 00:22:33,740
the question will be how many tasks in

00:22:30,799 --> 00:22:36,320
stage 3 you will have how many tasks you

00:22:33,740 --> 00:22:38,600
will split the data into here is the

00:22:36,320 --> 00:22:41,360
answer it's controlled by this sparks

00:22:38,600 --> 00:22:46,129
equal shuffle partitions parameter and

00:22:41,360 --> 00:22:49,399
that's why you saw 200 there and if you

00:22:46,129 --> 00:22:52,100
are dealing with 10 terabytes of events

00:22:49,399 --> 00:22:54,019
you performing the join and if we do a

00:22:52,100 --> 00:22:58,129
quick back of the envelope calculation

00:22:54,019 --> 00:23:00,830
10 terabytes split into 200 tasks is 50

00:22:58,129 --> 00:23:04,970
gigs per task this is too much you want

00:23:00,830 --> 00:23:07,700
to keep your executors small for for

00:23:04,970 --> 00:23:11,659
this kind of jobs so this is too much so

00:23:07,700 --> 00:23:13,370
how do we how do we solve that

00:23:11,659 --> 00:23:16,029
so this is what what it will look like

00:23:13,370 --> 00:23:18,830
each task will get 50 gigs of the data

00:23:16,029 --> 00:23:21,710
we end up with 1 of these problems again

00:23:18,830 --> 00:23:23,480
one of these shuffle problems so first

00:23:21,710 --> 00:23:25,580
of all you have to understand your data

00:23:23,480 --> 00:23:27,529
you have to know the size of your data

00:23:25,580 --> 00:23:29,330
and do the actually do the back of the

00:23:27,529 --> 00:23:33,289
envelope calculation so you understand

00:23:29,330 --> 00:23:35,269
why it happens to you but then once you

00:23:33,289 --> 00:23:38,179
understand that it is actually quite

00:23:35,269 --> 00:23:40,100
simple you can just control the level of

00:23:38,179 --> 00:23:42,500
parallelism so use this parameter and

00:23:40,100 --> 00:23:44,450
increase the number of tasks so you have

00:23:42,500 --> 00:23:46,730
more tasks each of them will be

00:23:44,450 --> 00:23:50,389
processing place data because they are

00:23:46,730 --> 00:23:54,620
uniformly distributed a similar problem

00:23:50,389 --> 00:23:57,409
might happen to when you do any other

00:23:54,620 --> 00:23:58,610
shuffle shuffle operations so if you are

00:23:57,409 --> 00:24:01,730
doing free partition you can also

00:23:58,610 --> 00:24:05,659
control number of partitions it will it

00:24:01,730 --> 00:24:10,039
will create so as the result after

00:24:05,659 --> 00:24:16,090
changing the the parameter we can see

00:24:10,039 --> 00:24:16,090
two thousand tasks here oh my god really

00:24:17,380 --> 00:24:22,809
we can see 2,000 tasks here and each of

00:24:20,470 --> 00:24:28,660
them will be just processing smaller

00:24:22,809 --> 00:24:30,850
amount of data but what if our data

00:24:28,660 --> 00:24:32,830
what if our events are not really

00:24:30,850 --> 00:24:35,890
uniformly distributed so let's have a

00:24:32,830 --> 00:24:49,360
look at case number form for the skewed

00:24:35,890 --> 00:24:54,190
join sometimes it happens that when you

00:24:49,360 --> 00:24:56,590
run your job on this on this like

00:24:54,190 --> 00:25:00,580
overview level it looks exactly the same

00:24:56,590 --> 00:25:04,120
as before but when I click here I can

00:25:00,580 --> 00:25:05,740
see that many of my tasks are not doing

00:25:04,120 --> 00:25:08,620
that much they are just processing some

00:25:05,740 --> 00:25:12,039
reasonable amount of data but there is

00:25:08,620 --> 00:25:14,830
this one task which read much more data

00:25:12,039 --> 00:25:17,770
this is one gig so far it's not bad but

00:25:14,830 --> 00:25:21,520
it's still growing and eventually it

00:25:17,770 --> 00:25:26,230
explodes so what if you are about to

00:25:21,520 --> 00:25:30,610
join ten terabytes of events with one

00:25:26,230 --> 00:25:33,520
user who produced one terabyte of events

00:25:30,610 --> 00:25:35,470
so one user has produced most of your

00:25:33,520 --> 00:25:39,520
events and you are joining by by the

00:25:35,470 --> 00:25:44,169
user ID again the way it will be done is

00:25:39,520 --> 00:25:48,460
exactly the same but one of your tasks

00:25:44,169 --> 00:25:52,840
will have to process one terabyte of

00:25:48,460 --> 00:25:55,210
data and this will definitely explode so

00:25:52,840 --> 00:25:59,590
what do you do if you have a skewed

00:25:55,210 --> 00:26:02,169
joint problem first of all check if your

00:25:59,590 --> 00:26:05,740
data is correct it might be that the

00:26:02,169 --> 00:26:07,990
data delivery mechanism is off and it's

00:26:05,740 --> 00:26:09,700
producing some now IDs and so on so that

00:26:07,990 --> 00:26:14,320
means you have to change your data

00:26:09,700 --> 00:26:16,720
delivery then check if the logic you you

00:26:14,320 --> 00:26:19,120
just created may be the way you are

00:26:16,720 --> 00:26:20,830
building the data frame is just off

00:26:19,120 --> 00:26:22,530
maybe you have a bug in your logic and

00:26:20,830 --> 00:26:25,990
that means you have fixed your logic but

00:26:22,530 --> 00:26:28,870
sometimes it's completely fine you have

00:26:25,990 --> 00:26:30,880
data like this and you would like to

00:26:28,870 --> 00:26:34,060
still perform the join

00:26:30,880 --> 00:26:36,670
so how do we do that on the left hand

00:26:34,060 --> 00:26:39,340
side you can see events on the right

00:26:36,670 --> 00:26:43,210
hand side you can see users and this

00:26:39,340 --> 00:26:45,010
user ID number one happens to be happens

00:26:43,210 --> 00:26:49,780
to appear quite often even we would like

00:26:45,010 --> 00:26:53,350
to split that because you will go to to

00:26:49,780 --> 00:26:57,720
the same to the same task so what we do

00:26:53,350 --> 00:27:00,130
is we generate a salt column where we

00:26:57,720 --> 00:27:01,840
randomly children and narrate some value

00:27:00,130 --> 00:27:05,470
here we are just generating three

00:27:01,840 --> 00:27:09,100
different values and what we do with

00:27:05,470 --> 00:27:12,310
users is we duplicate all of them with

00:27:09,100 --> 00:27:13,090
every single possible salt value so from

00:27:12,310 --> 00:27:16,200
one two three

00:27:13,090 --> 00:27:20,260
each user appears three times and now

00:27:16,200 --> 00:27:24,820
when you do the join you join by not

00:27:20,260 --> 00:27:28,870
only user ID but also less salt so user

00:27:24,820 --> 00:27:31,990
ID one salt one goes to one place salt 2

00:27:28,870 --> 00:27:35,980
is going to another task and salt three

00:27:31,990 --> 00:27:38,320
is going to one more task so we have

00:27:35,980 --> 00:27:40,180
splitted that into three buckets usually

00:27:38,320 --> 00:27:43,960
what you want to split it into into more

00:27:40,180 --> 00:27:46,690
so how do you do that first of all you

00:27:43,960 --> 00:27:49,810
have to calculate the salt at events so

00:27:46,690 --> 00:27:53,400
the the events with salt : and you do

00:27:49,810 --> 00:27:56,320
that by adding an extra column where you

00:27:53,400 --> 00:28:01,090
basically create a hundred different

00:27:56,320 --> 00:28:05,770
random values then you need to calculate

00:28:01,090 --> 00:28:07,870
the users users salted so first you

00:28:05,770 --> 00:28:10,480
create some dummy data frame with

00:28:07,870 --> 00:28:13,840
hundred different values and then you do

00:28:10,480 --> 00:28:18,970
Cartesian join of users and these

00:28:13,840 --> 00:28:23,800
hundred dummy values and then you join

00:28:18,970 --> 00:28:26,980
the event salted with user salted not

00:28:23,800 --> 00:28:31,390
only based on user ID but also based on

00:28:26,980 --> 00:28:33,880
the salt value so at the end you won't

00:28:31,390 --> 00:28:37,000
have your task won't have to process

00:28:33,880 --> 00:28:41,590
this much data you limit the amount of

00:28:37,000 --> 00:28:43,810
data per task and this might sound us a

00:28:41,590 --> 00:28:44,710
little bit artificial problem for you

00:28:43,810 --> 00:28:46,630
but this is something

00:28:44,710 --> 00:28:48,520
can see very often when working with

00:28:46,630 --> 00:28:51,070
clients and it's actually quite powerful

00:28:48,520 --> 00:28:54,010
technique and of course you can optimize

00:28:51,070 --> 00:28:56,549
it you can generate salt for only only

00:28:54,010 --> 00:28:59,799
the user which is problematic and so on

00:28:56,549 --> 00:29:02,679
but but make sure you keep keep in mind

00:28:59,799 --> 00:29:07,360
that these kind of techniques so the

00:29:02,679 --> 00:29:09,669
quick summary of the use cases I have I

00:29:07,360 --> 00:29:11,409
have shown you is that first of all you

00:29:09,669 --> 00:29:13,450
have to know your data and that's really

00:29:11,409 --> 00:29:15,850
important size of your data the

00:29:13,450 --> 00:29:19,330
distribution of your keys you are using

00:29:15,850 --> 00:29:20,860
when when joining and so on then you

00:29:19,330 --> 00:29:24,159
really have to understand how the

00:29:20,860 --> 00:29:25,809
operators you are using are working and

00:29:24,159 --> 00:29:30,279
you should pay attention to those ones

00:29:25,809 --> 00:29:33,010
which are triggering shuffle so join is

00:29:30,279 --> 00:29:35,409
then the most common one and what you

00:29:33,010 --> 00:29:38,049
want to achieve in general is you want

00:29:35,409 --> 00:29:39,820
to keep all your tasks busy so we

00:29:38,049 --> 00:29:42,970
benefit from the fact SPARC is a

00:29:39,820 --> 00:29:45,250
distributed system and you want to make

00:29:42,970 --> 00:29:48,669
sure that your tasks are not processing

00:29:45,250 --> 00:29:50,559
too much too much data so so maybe

00:29:48,669 --> 00:29:56,010
controlling the level of parallelize

00:29:50,559 --> 00:29:58,210
will help and that there are many other

00:29:56,010 --> 00:30:01,360
challenges but I would really recommend

00:29:58,210 --> 00:30:03,010
you to go back to these examples and

00:30:01,360 --> 00:30:05,710
make sure you understand them maybe play

00:30:03,010 --> 00:30:07,270
around with them because for instance

00:30:05,710 --> 00:30:09,010
when you are when you want to do

00:30:07,270 --> 00:30:11,260
broadcast variable the whole point of

00:30:09,010 --> 00:30:13,500
broadcasting is that you avoid shuffle

00:30:11,260 --> 00:30:16,750
so you have to know when shuffle happens

00:30:13,500 --> 00:30:18,460
and what is the consequence but if you

00:30:16,750 --> 00:30:21,610
would like to discuss any of these of

00:30:18,460 --> 00:30:24,690
these problems or any problems part I

00:30:21,610 --> 00:30:29,980
would be happy to do that afterwards and

00:30:24,690 --> 00:30:32,049
the last comment I would like to I would

00:30:29,980 --> 00:30:35,409
like to make is that if you are a data

00:30:32,049 --> 00:30:39,190
scientist or data analyst you definitely

00:30:35,409 --> 00:30:42,220
want to know how to fix the most common

00:30:39,190 --> 00:30:46,390
problems but on the other hand there are

00:30:42,220 --> 00:30:49,809
problems with spark which which are not

00:30:46,390 --> 00:30:52,210
really related to due to your expertise

00:30:49,809 --> 00:30:55,330
which might be related to what happens

00:30:52,210 --> 00:30:57,350
in the cluster with the general cluster

00:30:55,330 --> 00:30:59,270
state or it might be bugging with

00:30:57,350 --> 00:31:01,730
so if you feel that you are completely

00:30:59,270 --> 00:31:04,730
stuck and it's going outside of your

00:31:01,730 --> 00:31:07,309
expertise make sure you know where to go

00:31:04,730 --> 00:31:09,620
to make sure you know where the data

00:31:07,309 --> 00:31:14,990
engineers are or where your dev ops are

00:31:09,620 --> 00:31:18,350
so day they unblock you I would be happy

00:31:14,990 --> 00:31:21,110
to answer your questions now I have

00:31:18,350 --> 00:31:25,610
actually posted the slides on my on my

00:31:21,110 --> 00:31:28,700
Twitter and I I also have have posted a

00:31:25,610 --> 00:31:32,120
blog post about one of the use cases I

00:31:28,700 --> 00:31:33,860
will be describing all of them soon so

00:31:32,120 --> 00:31:44,720
follow me on Twitter if you are

00:31:33,860 --> 00:31:54,049
interested so we have ten minutes

00:31:44,720 --> 00:31:56,780
question interesting hook thank you what

00:31:54,049 --> 00:31:58,940
happens if you put some of sparks equal

00:31:56,780 --> 00:32:01,100
inside your data frame is it good or bad

00:31:58,940 --> 00:32:04,220
to do that tied to doing things by hand

00:32:01,100 --> 00:32:07,090
well so behind the scene it will do

00:32:04,220 --> 00:32:10,280
exactly the same it will use the same

00:32:07,090 --> 00:32:13,280
the same optimization engine and so on I

00:32:10,280 --> 00:32:17,000
like myself I'm not a fan of sequel

00:32:13,280 --> 00:32:18,799
because it's just much easier for the

00:32:17,000 --> 00:32:22,780
test for me and it's much easier to

00:32:18,799 --> 00:32:26,179
split that into into the coding blocks

00:32:22,780 --> 00:32:28,669
but I know like if your data scientist

00:32:26,179 --> 00:32:31,190
data data analyst a they really many of

00:32:28,669 --> 00:32:33,620
them really like do it but and the point

00:32:31,190 --> 00:32:35,780
is behind the scene it will work exactly

00:32:33,620 --> 00:32:40,490
the same so if you are writing a sequel

00:32:35,780 --> 00:32:42,679
and and if you are performing a join the

00:32:40,490 --> 00:32:45,200
same thing will happen so whether you're

00:32:42,679 --> 00:32:49,100
using this interface or sequel it's

00:32:45,200 --> 00:32:51,610
still the same it's still the same

00:32:49,100 --> 00:32:51,610
pretty much

00:33:05,450 --> 00:33:11,400
hi so what is your usual debugging

00:33:09,390 --> 00:33:13,350
workflow when you notice that a task is

00:33:11,400 --> 00:33:15,600
taking way too long longer than it

00:33:13,350 --> 00:33:18,090
should be how do you best in your

00:33:15,600 --> 00:33:21,930
experience go about debugging it okay so

00:33:18,090 --> 00:33:25,050
I actually try to in a very quick way

00:33:21,930 --> 00:33:29,550
reproduce it here so first of all I look

00:33:25,050 --> 00:33:31,890
at at the spark UI and I see an overview

00:33:29,550 --> 00:33:35,180
of this stage which is problematic then

00:33:31,890 --> 00:33:37,860
I try to narrow down how this stage

00:33:35,180 --> 00:33:41,160
matches the code and which part of the

00:33:37,860 --> 00:33:42,930
code is is the problematic one then if I

00:33:41,160 --> 00:33:45,510
suspect for instance we have a skew

00:33:42,930 --> 00:33:48,870
I actually run an analysis on top of on

00:33:45,510 --> 00:33:51,030
top of it and see if it's if it's really

00:33:48,870 --> 00:33:55,050
a problem also it's worth knowing that

00:33:51,030 --> 00:33:59,430
where your logs are so like if you are

00:33:55,050 --> 00:34:01,500
using spark on top of HDFS the dialog

00:33:59,430 --> 00:34:03,600
should be automatically aggregated if

00:34:01,500 --> 00:34:06,000
you want to really go deeper and deeper

00:34:03,600 --> 00:34:08,910
but for I would say for most for most

00:34:06,000 --> 00:34:11,240
queries and knowing your your data

00:34:08,910 --> 00:34:13,560
distribution and understanding

00:34:11,240 --> 00:34:16,700
understanding how to read the data from

00:34:13,560 --> 00:34:19,590
the from the SPARQL how to read some

00:34:16,700 --> 00:34:21,720
some hints should be enough at least

00:34:19,590 --> 00:34:25,590
four for a data scientist if you want to

00:34:21,720 --> 00:34:27,990
go deeper definitely looking looking

00:34:25,590 --> 00:34:31,260
into logs looking into into the query

00:34:27,990 --> 00:34:34,950
plan spark shows you how it will go it

00:34:31,260 --> 00:34:38,130
will execute the query but yet generally

00:34:34,950 --> 00:34:41,000
it depends on on the use case but this

00:34:38,130 --> 00:34:41,000
is the most common one

00:34:45,329 --> 00:34:50,250
still have some time I believe so

00:34:53,159 --> 00:35:01,929
okay thank you

00:34:55,760 --> 00:35:01,929

YouTube URL: https://www.youtube.com/watch?v=4Lngj22dFiA


