Title: Berlin Buzzwords 2018: Igor Mazor - Deep Learning Recommender Architecture: Master Thesis to ...
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Igor Mazor talking about "Deep Learning Recommender Architecture â€“ From a Master Thesis to Production".
 
The vast majority of the recommender systems nowadays are based on algorithms which are good at representing linear relations between users and items, or items and items. With the growing popularity of deep learning applications, it was quite natural for us to experiment with a more advanced recommendation engine which could represent more complex, non-linear relations. With this we were able to generate much more relevant recommendations to the users.

The new recommendation engine was developed by a student as part of his master thesis, lead by a PHD colleague from the mobile.de data team. The engine is based on deep learning and combines 3 sub neural networks. The engine showed really good results compared to our current recommendation engine, what brought us to the decision of trying to deploy it into our production system. Once we started to review the new engine, the following main challenges were raised:

 - We are using Java/Scala in our production system: Is there a possibility to deploy a Tensorflow model, which was trained in python, in Java/Scala?
- How would we be able to deploy new models into the production system, without any  downtimes ?
- The model contains 3 sub neural networks, each sub network is responsible for different  functionality: Would it be possible to isolate each of those sub networks and deploy it  separately in order to scale out the entire engine ?
- How could we generate recommendations in real-time without pre-calculating the recommendations for each user/item over night ?
- Does such complex system can scale at all?

The main focus of this talk would be on our journey towards the design and implementation of a scalable architecture, giving all the mentioned above requirements, which could support the deployment of the new Deep Learning Recommender in production. During the talk I would try to present the different architecture components and how each component helps in solving the mentioned above challenges. In addition, I would try to describe how the different components of the Deep Learning Recommender could be reused and help us to improve also our search functionalities.

Read more:
https://2018.berlinbuzzwords.de/18/session/deep-learning-recommender-architecture-master-thesis-production

About Igor Mazor:
https://2018.berlinbuzzwords.de/users/igor-mazor-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,560 --> 00:00:09,240
so my name is Igor I am working for

00:00:07,200 --> 00:00:10,710
Villa in the last almost two years I

00:00:09,240 --> 00:00:12,300
join will be left for building

00:00:10,710 --> 00:00:14,969
specifically as a real-time

00:00:12,300 --> 00:00:16,680
infrastructure and a few months ago I

00:00:14,969 --> 00:00:19,080
got a really interesting project which

00:00:16,680 --> 00:00:20,790
is around deep learning recommender the

00:00:19,080 --> 00:00:23,180
project started as a master thesis by

00:00:20,790 --> 00:00:26,940
Marcel Karofsky and I was leading by

00:00:23,180 --> 00:00:30,000
florian vedyam a PhD employee from one

00:00:26,940 --> 00:00:31,260
of my team colleagues and they build

00:00:30,000 --> 00:00:33,690
something really interesting but it was

00:00:31,260 --> 00:00:35,190
part of his master thesis and they came

00:00:33,690 --> 00:00:37,170
to me and asked me ok how can we make it

00:00:35,190 --> 00:00:39,179
to production and then we started a

00:00:37,170 --> 00:00:41,160
really interesting journey with a lot of

00:00:39,179 --> 00:00:43,260
challenges which I want to discuss with

00:00:41,160 --> 00:00:45,449
you in short today how we actually were

00:00:43,260 --> 00:00:49,170
able eventually to bring this master

00:00:45,449 --> 00:00:50,370
Jesus into production so today a bit was

00:00:49,170 --> 00:00:52,679
the agenda I will start a bit with

00:00:50,370 --> 00:00:54,569
motivation about the recommender why we

00:00:52,679 --> 00:00:56,640
needed in mobile specifically some of

00:00:54,569 --> 00:00:58,260
the components and the deep learning

00:00:56,640 --> 00:01:05,970
approach and architectures that we were

00:00:58,260 --> 00:01:08,640
using so a little bit numbers for

00:01:05,970 --> 00:01:10,799
motivation mobilize Germany largest

00:01:08,640 --> 00:01:12,810
online vehicle marketplace we have

00:01:10,799 --> 00:01:15,360
almost around 2 million different ads

00:01:12,810 --> 00:01:15,930
and its keep changing each day updated

00:01:15,360 --> 00:01:18,390
constantly

00:01:15,930 --> 00:01:20,549
we have around 43,000 dealers which is

00:01:18,390 --> 00:01:22,979
covers almost all the dealers in Germany

00:01:20,549 --> 00:01:26,399
so we are really the leading platform

00:01:22,979 --> 00:01:29,189
for that we have almost around 13 and I

00:01:26,399 --> 00:01:31,469
have 14 million unique visitors unique

00:01:29,189 --> 00:01:33,750
visit visitors here users that come in

00:01:31,469 --> 00:01:38,909
each day and we allowed a lot of changes

00:01:33,750 --> 00:01:40,649
to the items on the other hand we have

00:01:38,909 --> 00:01:43,170
also a lot of interactions of the users

00:01:40,649 --> 00:01:44,609
we have almost around 100 million events

00:01:43,170 --> 00:01:46,799
that users generate each day by

00:01:44,609 --> 00:01:49,740
interactions with the platform events

00:01:46,799 --> 00:01:52,799
such as item views such as searches

00:01:49,740 --> 00:01:55,979
saving to favorite list ok but when you

00:01:52,799 --> 00:01:59,340
look on that so 100 million events for

00:01:55,979 --> 00:02:00,509
us it's quite a lot but separately each

00:01:59,340 --> 00:02:02,909
of those events do not tell us anything

00:02:00,509 --> 00:02:05,100
and we had to come up with a way that

00:02:02,909 --> 00:02:06,990
you can reduce those 100 million events

00:02:05,100 --> 00:02:09,080
to something more useful something more

00:02:06,990 --> 00:02:11,880
meaningful that can help us to somehow

00:02:09,080 --> 00:02:13,830
help the users to find a better car

00:02:11,880 --> 00:02:16,050
something that fit for their preferences

00:02:13,830 --> 00:02:18,300
so eventually the idea was to create

00:02:16,050 --> 00:02:20,430
something in the form of user profile

00:02:18,300 --> 00:02:22,710
so each day we take all of those 100

00:02:20,430 --> 00:02:24,030
million events we crunch them we

00:02:22,710 --> 00:02:25,890
calculate them and we reduce them

00:02:24,030 --> 00:02:31,140
eventually to around 2 million user

00:02:25,890 --> 00:02:34,980
preferences so as you can imagine on one

00:02:31,140 --> 00:02:36,330
hand we have 2 million items and on the

00:02:34,980 --> 00:02:38,760
other hand we have millions of different

00:02:36,330 --> 00:02:40,710
user preferences so then the question

00:02:38,760 --> 00:02:43,170
and the real challenge is eventually how

00:02:40,710 --> 00:02:45,780
can we bring get together how can we use

00:02:43,170 --> 00:02:48,210
the part of the user preferences and the

00:02:45,780 --> 00:02:50,190
part of the items to match between them

00:02:48,210 --> 00:02:52,380
and to help find the users the sort of

00:02:50,190 --> 00:02:55,050
the perfect car so if you look for

00:02:52,380 --> 00:02:56,550
example on this firefighter the perfect

00:02:55,050 --> 00:02:59,340
car that will be for him this is the

00:02:56,550 --> 00:03:00,870
firefighter truck right so this is what

00:02:59,340 --> 00:03:02,610
we want to do for all of the users to

00:03:00,870 --> 00:03:08,160
find help some findings or perfect car

00:03:02,610 --> 00:03:09,780
find perfect match so we try different

00:03:08,160 --> 00:03:11,340
approaches and one of the approaches

00:03:09,780 --> 00:03:13,860
that we tried was using deep learning

00:03:11,340 --> 00:03:15,240
and you may ask yourself okay but why

00:03:13,860 --> 00:03:17,370
deep learnings are many different ways

00:03:15,240 --> 00:03:19,920
of recommender and we tried other

00:03:17,370 --> 00:03:21,690
recommenders as well but deep learning

00:03:19,920 --> 00:03:23,100
if you think about it it can first of

00:03:21,690 --> 00:03:24,480
all capture a nonlinear relationship

00:03:23,100 --> 00:03:25,830
which is really important because most

00:03:24,480 --> 00:03:27,540
of the algorithms today for

00:03:25,830 --> 00:03:29,190
recommendations whether is matrix

00:03:27,540 --> 00:03:31,260
factorization or any other type of

00:03:29,190 --> 00:03:36,000
recommenders usually have some limits

00:03:31,260 --> 00:03:37,860
about linear relationships second of all

00:03:36,000 --> 00:03:39,570
it until some degree reducing

00:03:37,860 --> 00:03:41,220
engineering effort feature engineering

00:03:39,570 --> 00:03:43,020
effort because deep learning in the way

00:03:41,220 --> 00:03:44,790
how it build it it can just take and

00:03:43,020 --> 00:03:46,320
combine different features together and

00:03:44,790 --> 00:03:47,760
you get in something yeah the only

00:03:46,320 --> 00:03:49,950
problems that it's harder than to

00:03:47,760 --> 00:03:51,750
understand exactly what is output why we

00:03:49,950 --> 00:03:53,000
getting some specific results but in

00:03:51,750 --> 00:03:56,970
general it should improve also

00:03:53,000 --> 00:03:58,440
predictive capability so this was a

00:03:56,970 --> 00:04:03,750
motivation for us to try deep learning

00:03:58,440 --> 00:04:04,470
approach so speaking a bit about the

00:04:03,750 --> 00:04:06,390
components

00:04:04,470 --> 00:04:08,220
what kind of components we have that are

00:04:06,390 --> 00:04:11,519
part of the major deep learning

00:04:08,220 --> 00:04:12,900
recommender first of all our items so as

00:04:11,519 --> 00:04:16,440
I mentioned we have two million items

00:04:12,900 --> 00:04:19,140
and each item have more than 100

00:04:16,440 --> 00:04:21,180
attributes and just to make clear item

00:04:19,140 --> 00:04:23,280
for us it's some sort of car it can be a

00:04:21,180 --> 00:04:25,770
car a motorcycle it could be a truck

00:04:23,280 --> 00:04:27,930
okay so each item have more than 100

00:04:25,770 --> 00:04:30,390
different attributes such as price color

00:04:27,930 --> 00:04:31,830
a number of doors number of previous

00:04:30,390 --> 00:04:34,950
owners condition seats

00:04:31,830 --> 00:04:37,800
many many we're currently using only 20

00:04:34,950 --> 00:04:39,120
attributes and so we examining the model

00:04:37,800 --> 00:04:41,040
only with twenty attributes but

00:04:39,120 --> 00:04:42,300
definitely in the future we will try to

00:04:41,040 --> 00:04:44,760
see how we can take more of those

00:04:42,300 --> 00:04:46,410
attributes and leverage them and our

00:04:44,760 --> 00:04:47,970
items are updated constantly by the

00:04:46,410 --> 00:04:50,190
dealers most of the time it's the price

00:04:47,970 --> 00:04:52,650
but still it's really dynamic it's not

00:04:50,190 --> 00:04:55,290
like items for example in our B&B or

00:04:52,650 --> 00:04:57,540
ebooks or Spotify that they have some

00:04:55,290 --> 00:04:59,190
something that is more static that once

00:04:57,540 --> 00:05:00,960
user upload the song most of the time

00:04:59,190 --> 00:05:03,320
nothing will be changed for that song

00:05:00,960 --> 00:05:03,320
anymore

00:05:04,490 --> 00:05:08,850
the other component that I mentioned is

00:05:06,810 --> 00:05:10,500
the use of preferences and in our case

00:05:08,850 --> 00:05:12,780
user preferences is actually modeled

00:05:10,500 --> 00:05:14,400
through the bayesian statistics so

00:05:12,780 --> 00:05:16,350
actually we take all the interactions of

00:05:14,400 --> 00:05:18,480
the user such as for example save to

00:05:16,350 --> 00:05:21,120
favorite least contact specific dealer

00:05:18,480 --> 00:05:23,190
view search and so on and we try to

00:05:21,120 --> 00:05:25,140
build the statistics about that user we

00:05:23,190 --> 00:05:28,080
try to see what his preferences towards

00:05:25,140 --> 00:05:31,560
specific item attributes for example you

00:05:28,080 --> 00:05:34,860
can see that user Express 30% blue

00:05:31,560 --> 00:05:39,360
colors and 40% red cars and that user

00:05:34,860 --> 00:05:41,280
prefer 50% BMW and 40% Aldi and so on

00:05:39,360 --> 00:05:44,550
and so on and the price distribution is

00:05:41,280 --> 00:05:46,230
around 20,000 plus minus 1.5 K so we try

00:05:44,550 --> 00:05:48,330
to build that kind of statistical model

00:05:46,230 --> 00:05:50,610
and in addition we calculate also the

00:05:48,330 --> 00:05:52,500
average user the preferences of the

00:05:50,610 --> 00:05:55,440
average user and eventually using the

00:05:52,500 --> 00:05:57,210
Bayesian statistics we weighting the

00:05:55,440 --> 00:06:03,060
specific use of references against the

00:05:57,210 --> 00:06:06,060
average user preferences okay but how

00:06:03,060 --> 00:06:08,970
does that really help us then to build

00:06:06,060 --> 00:06:11,520
our recommender so imagine if we would

00:06:08,970 --> 00:06:13,830
had a way that we can represent our user

00:06:11,520 --> 00:06:16,410
preferences and our item in some

00:06:13,830 --> 00:06:18,300
abstraction some sort of maybe

00:06:16,410 --> 00:06:21,390
mathematical abstraction way and then we

00:06:18,300 --> 00:06:23,670
can take the abstraction representation

00:06:21,390 --> 00:06:26,430
of the user preferences and abstraction

00:06:23,670 --> 00:06:28,620
of the item put them in some sort of

00:06:26,430 --> 00:06:30,810
black box and the black box will be

00:06:28,620 --> 00:06:33,000
smart enough to give us a ranking and

00:06:30,810 --> 00:06:35,610
that ranking will be sort of probability

00:06:33,000 --> 00:06:37,590
of user to interact with a specific item

00:06:35,610 --> 00:06:39,420
so based on that probabilities we can

00:06:37,590 --> 00:06:42,930
select the items the next items that we

00:06:39,420 --> 00:06:44,760
want to show to that user and that black

00:06:42,930 --> 00:06:45,840
box is actually as some of you already

00:06:44,760 --> 00:06:47,910
guessed this

00:06:45,840 --> 00:06:49,889
is a deep learning approach so how

00:06:47,910 --> 00:06:52,260
actually that the black box looks inside

00:06:49,889 --> 00:06:54,180
so he have a deep learning model which

00:06:52,260 --> 00:06:57,690
is trained with SAP with three sub

00:06:54,180 --> 00:07:00,030
different networks user net item net and

00:06:57,690 --> 00:07:02,100
rent net the user net is actually

00:07:00,030 --> 00:07:05,100
responsible to taking those user

00:07:02,100 --> 00:07:08,220
preferences and converting them to some

00:07:05,100 --> 00:07:10,650
sort of embedding in another world the

00:07:08,220 --> 00:07:14,070
embedding is a vector of 77 floats that

00:07:10,650 --> 00:07:16,220
mathematically represent that user the

00:07:14,070 --> 00:07:20,040
item net on the other hand taking the

00:07:16,220 --> 00:07:22,380
attributes of the car and calculates

00:07:20,040 --> 00:07:24,030
also embeddings which is also a float a

00:07:22,380 --> 00:07:25,650
vector of 77 floats

00:07:24,030 --> 00:07:28,320
actually in the training process what we

00:07:25,650 --> 00:07:30,960
try to do is to bring the item

00:07:28,320 --> 00:07:32,700
embeddings of the the item in banks and

00:07:30,960 --> 00:07:34,560
the user in bindings vectors to be on

00:07:32,700 --> 00:07:36,750
the same space because that will allow

00:07:34,560 --> 00:07:39,840
us later to do a direct comparison for

00:07:36,750 --> 00:07:41,729
example using equality and distance once

00:07:39,840 --> 00:07:43,020
we have the item in banks and the user

00:07:41,729 --> 00:07:45,539
in benning's we can pass them together

00:07:43,020 --> 00:07:47,400
through the rank net and what the rank

00:07:45,539 --> 00:07:50,100
net do is actually give a score between

00:07:47,400 --> 00:07:52,200
0 and 1 and we using in the rank net

00:07:50,100 --> 00:07:53,639
also negative sampling method so

00:07:52,200 --> 00:07:55,169
actually what the rent net give us is

00:07:53,639 --> 00:08:01,740
the probability of the user to interact

00:07:55,169 --> 00:08:03,210
with a specific item for a higher level

00:08:01,740 --> 00:08:05,039
view imagine that we have the user

00:08:03,210 --> 00:08:07,560
preferences we have multiple different

00:08:05,039 --> 00:08:09,450
items we convert them we take the user

00:08:07,560 --> 00:08:11,700
embed ink we take multiple item in

00:08:09,450 --> 00:08:14,220
benning's we pass it everything through

00:08:11,700 --> 00:08:15,690
the rank net we give we get scores and

00:08:14,220 --> 00:08:17,370
then based on the scores we can decide

00:08:15,690 --> 00:08:22,260
which item we're going to present to the

00:08:17,370 --> 00:08:25,919
user all right so our first challenge

00:08:22,260 --> 00:08:28,110
started with our data scientist our data

00:08:25,919 --> 00:08:30,599
scientist are really smart guys but as

00:08:28,110 --> 00:08:32,070
most of our data sign many other data

00:08:30,599 --> 00:08:35,400
scientists today they prefer to work

00:08:32,070 --> 00:08:37,800
with cerebus Python and I wish Scala

00:08:35,400 --> 00:08:39,180
with Python and most of the

00:08:37,800 --> 00:08:41,940
proof-of-concept that they build are

00:08:39,180 --> 00:08:43,770
with Python and they were using

00:08:41,940 --> 00:08:46,500
tensorflow to train our deep learning

00:08:43,770 --> 00:08:48,540
models and obviously they done into this

00:08:46,500 --> 00:08:49,800
Python so when they came to me and they

00:08:48,540 --> 00:08:51,270
told me ok we need to bring it to

00:08:49,800 --> 00:08:53,550
production we had a bit and a problem

00:08:51,270 --> 00:08:55,740
because our production system is only

00:08:53,550 --> 00:08:57,630
Java or Scala this is the policy

00:08:55,740 --> 00:08:59,670
currently it's not looking very we not

00:08:57,630 --> 00:09:01,710
that we have anything against Python

00:08:59,670 --> 00:09:03,240
I actually think that we should try to

00:09:01,710 --> 00:09:05,250
bring some Python stuff to production

00:09:03,240 --> 00:09:07,320
but most of our experience of the

00:09:05,250 --> 00:09:08,970
developers and of the DevOps are around

00:09:07,320 --> 00:09:11,820
Java and this way we didn't want to have

00:09:08,970 --> 00:09:13,080
any sort of surprises now with Python so

00:09:11,820 --> 00:09:16,320
we had to think how actually we can take

00:09:13,080 --> 00:09:17,910
the Python stuff put it in Scala because

00:09:16,320 --> 00:09:20,190
it is my team works mostly with Scala

00:09:17,910 --> 00:09:21,810
without rewriting the code from scratch

00:09:20,190 --> 00:09:25,710
and this is what's really important

00:09:21,810 --> 00:09:28,110
requirement after doing some search and

00:09:25,710 --> 00:09:29,790
research I found out different blocks

00:09:28,110 --> 00:09:32,190
that most of them actually were more in

00:09:29,790 --> 00:09:33,480
the Python but I had enough ideas and

00:09:32,190 --> 00:09:35,880
understanding how we can convert it to

00:09:33,480 --> 00:09:37,890
the Scala world so the interesting

00:09:35,880 --> 00:09:39,690
things that tensorflow comes with it

00:09:37,890 --> 00:09:42,480
stands for floss serving which is a

00:09:39,690 --> 00:09:45,510
specific C++ implementation that allow

00:09:42,480 --> 00:09:47,010
us to load tensor flow models tensor

00:09:45,510 --> 00:09:49,410
flow models which can be exported and

00:09:47,010 --> 00:09:51,660
saved via protocol buffers and tens are

00:09:49,410 --> 00:09:54,180
serving is the C++ implementation that

00:09:51,660 --> 00:09:57,630
can simply load the model and you can

00:09:54,180 --> 00:09:58,950
use it to serve predictions or do

00:09:57,630 --> 00:10:01,410
anything that you want around this

00:09:58,950 --> 00:10:03,900
tensorflow models the only problem is

00:10:01,410 --> 00:10:06,120
the tensor flow model was sorry viz the

00:10:03,900 --> 00:10:07,800
tensor flow servant was that because

00:10:06,120 --> 00:10:09,960
it's C++ and because it's Google and

00:10:07,800 --> 00:10:12,660
Google always special they using only

00:10:09,960 --> 00:10:14,580
the protocol which is called G RPC G RPC

00:10:12,660 --> 00:10:17,130
communicate only via protocol buffer

00:10:14,580 --> 00:10:19,410
which means that you cannot you cannot

00:10:17,130 --> 00:10:21,480
call tensorflow serving with a simple

00:10:19,410 --> 00:10:24,270
rest comma with a simple rest service or

00:10:21,480 --> 00:10:26,400
with a simple JSON interface okay but

00:10:24,270 --> 00:10:27,660
all our ecosystem around the services is

00:10:26,400 --> 00:10:29,940
the rest obviously so we had to think

00:10:27,660 --> 00:10:31,680
how we overcome it so the idea was to

00:10:29,940 --> 00:10:34,980
take a docker container which allow us

00:10:31,680 --> 00:10:36,960
to ice with resources and to deploy in

00:10:34,980 --> 00:10:39,270
the same docker container the tensorflow

00:10:36,960 --> 00:10:41,160
serving and side by side a scale

00:10:39,270 --> 00:10:43,290
application right the scarification

00:10:41,160 --> 00:10:44,790
would be able locally inside the docker

00:10:43,290 --> 00:10:47,490
to communicate with the tensorflow

00:10:44,790 --> 00:10:49,470
serving and other services that need to

00:10:47,490 --> 00:10:51,510
get prediction we will call actually the

00:10:49,470 --> 00:10:53,400
Scala gateway and in such way we were

00:10:51,510 --> 00:10:54,950
able to scale it quite well quite well

00:10:53,400 --> 00:10:56,790
as well because we didn't had to deploy

00:10:54,950 --> 00:11:00,420
separately any sort of tensorflow

00:10:56,790 --> 00:11:02,070
serving cluster the applications that we

00:11:00,420 --> 00:11:03,480
scaled was each application have a

00:11:02,070 --> 00:11:07,110
docker and inside the Scala together

00:11:03,480 --> 00:11:09,450
with tensorflow serving once we figure

00:11:07,110 --> 00:11:11,940
out how to make Scala and tensorflow

00:11:09,450 --> 00:11:13,470
work together we could build we could

00:11:11,940 --> 00:11:15,540
start thinking about the

00:11:13,470 --> 00:11:19,740
flows how we can divide our problem to

00:11:15,540 --> 00:11:23,010
multiple parts so we can so we can put

00:11:19,740 --> 00:11:25,950
everything together in production so the

00:11:23,010 --> 00:11:27,180
first workflow so before the first world

00:11:25,950 --> 00:11:29,070
who actually is the general concept

00:11:27,180 --> 00:11:31,560
would be as following we have a tensor

00:11:29,070 --> 00:11:34,020
flow modules that we train in Python

00:11:31,560 --> 00:11:37,650
once we finishing to train it in Python

00:11:34,020 --> 00:11:39,240
we can export the the module to protocol

00:11:37,650 --> 00:11:42,480
buffers file which is really a simple

00:11:39,240 --> 00:11:43,980
file once we have the protocol buffer in

00:11:42,480 --> 00:11:45,930
some place that assumes some sort of

00:11:43,980 --> 00:11:47,910
object storage okay it doesn't important

00:11:45,930 --> 00:11:49,710
git repository we can actually then

00:11:47,910 --> 00:11:51,390
build a docker image and once you build

00:11:49,710 --> 00:11:53,010
this docker image the docker image will

00:11:51,390 --> 00:11:55,410
include inside the scala gateways that

00:11:53,010 --> 00:11:57,540
we built the that Scala gateway is able

00:11:55,410 --> 00:12:00,180
to communicate locally via G RPC

00:11:57,540 --> 00:12:03,240
protocol with tensorflow serving and we

00:12:00,180 --> 00:12:05,940
will be able also to load the protocol

00:12:03,240 --> 00:12:07,830
buffers model to that docker and what

00:12:05,940 --> 00:12:10,680
nice with tensorflow serving that is

00:12:07,830 --> 00:12:12,240
actually have a place a library sorry a

00:12:10,680 --> 00:12:15,000
directory where it listens for new

00:12:12,240 --> 00:12:16,440
incoming protocol buffer files so if you

00:12:15,000 --> 00:12:19,380
have a new version of the model you can

00:12:16,440 --> 00:12:21,480
just push it to that specific folder and

00:12:19,380 --> 00:12:23,190
tensorflow serving automatically will

00:12:21,480 --> 00:12:24,780
catch up and start serve it so it's

00:12:23,190 --> 00:12:26,700
actually allow you first of all easily

00:12:24,780 --> 00:12:27,480
push new models updates without

00:12:26,700 --> 00:12:29,370
downtimes

00:12:27,480 --> 00:12:31,230
and it allows you actually if you will

00:12:29,370 --> 00:12:33,600
call to the tens of serving with a

00:12:31,230 --> 00:12:35,550
specific module version to test it we in

00:12:33,600 --> 00:12:44,940
a be really easily different model

00:12:35,550 --> 00:12:46,740
versions ok so for the next part we had

00:12:44,940 --> 00:12:48,870
the challenge of our items as I

00:12:46,740 --> 00:12:50,910
mentioned our items are constantly keep

00:12:48,870 --> 00:12:52,740
updated the dealers constantly update

00:12:50,910 --> 00:12:54,240
the items sometimes they change small

00:12:52,740 --> 00:12:55,680
attributes most of the time to change

00:12:54,240 --> 00:12:57,420
the price but we need to catch up

00:12:55,680 --> 00:12:59,430
constantly we need to be able to

00:12:57,420 --> 00:13:02,250
calculate those item embeddings based on

00:12:59,430 --> 00:13:04,170
the attributes of the item constantly in

00:13:02,250 --> 00:13:05,880
real-time we cannot allow to ourself do

00:13:04,170 --> 00:13:08,490
it in batch again as I mentioned because

00:13:05,880 --> 00:13:10,320
we are not like Airbnb for example as 45

00:13:08,490 --> 00:13:11,940
or somebody upload department or

00:13:10,320 --> 00:13:13,710
somebody upload the song or a book and

00:13:11,940 --> 00:13:15,840
this is quite statically and it will not

00:13:13,710 --> 00:13:17,820
change once the dealer constantly keep

00:13:15,840 --> 00:13:20,310
updates or deleting for example

00:13:17,820 --> 00:13:22,080
uploading new ads we need to be able to

00:13:20,310 --> 00:13:24,000
catch it because otherwise if we will do

00:13:22,080 --> 00:13:25,680
it calculation of the embeddings once a

00:13:24,000 --> 00:13:27,210
day it could be that we will serve

00:13:25,680 --> 00:13:27,570
something that's already deleted and not

00:13:27,210 --> 00:13:30,870
up

00:13:27,570 --> 00:13:33,180
to date also our dealers are able to

00:13:30,870 --> 00:13:34,680
solve to sell cars quite fast so it

00:13:33,180 --> 00:13:36,390
could be a situation that we have a new

00:13:34,680 --> 00:13:37,740
car we didn't catch it up we will

00:13:36,390 --> 00:13:39,960
calculate the embedding only for the

00:13:37,740 --> 00:13:41,640
next day but that car already gone so we

00:13:39,960 --> 00:13:43,620
want to be able to catch up as fast as

00:13:41,640 --> 00:13:45,860
we can so the idea was that we have a

00:13:43,620 --> 00:13:49,470
kafka topic that includes all our items

00:13:45,860 --> 00:13:51,510
and we constantly consuming that kafka

00:13:49,470 --> 00:13:53,370
topic and using the configuration of the

00:13:51,510 --> 00:13:55,260
Dockers that we have the Scala get away

00:13:53,370 --> 00:13:57,930
with the tensile serving we can actually

00:13:55,260 --> 00:14:00,150
constantly keep generating item

00:13:57,930 --> 00:14:02,160
embeddings and we push them to a new

00:14:00,150 --> 00:14:04,590
Kafka topic by pushing those item in

00:14:02,160 --> 00:14:06,960
banks plus some specific item attributes

00:14:04,590 --> 00:14:08,940
to a new Kafka topic we can also allow

00:14:06,960 --> 00:14:11,190
other teams later to leverage it and to

00:14:08,940 --> 00:14:12,600
use it for any use case that they want

00:14:11,190 --> 00:14:14,790
we can then later to persist those

00:14:12,600 --> 00:14:16,710
embeddings to elasticsearch to radius or

00:14:14,790 --> 00:14:24,120
whatever and I will actually touch that

00:14:16,710 --> 00:14:27,090
point the third challenge which was one

00:14:24,120 --> 00:14:28,950
of the biggest actually is how we can

00:14:27,090 --> 00:14:33,330
actually generate the recommendations

00:14:28,950 --> 00:14:35,460
themself so having the process of being

00:14:33,330 --> 00:14:37,770
able to calculate items embeddings in

00:14:35,460 --> 00:14:39,870
real time it's cool and after that we

00:14:37,770 --> 00:14:42,750
figure out how we can combine Scala and

00:14:39,870 --> 00:14:45,120
G RPC and the tens of serving in the

00:14:42,750 --> 00:14:47,340
same docker container it was also cool

00:14:45,120 --> 00:14:49,500
but the major challenge remained

00:14:47,340 --> 00:14:51,990
actually how can we eventually generate

00:14:49,500 --> 00:14:54,960
those kind of recommendations so ideally

00:14:51,990 --> 00:14:56,640
in a perfect world scenario we have only

00:14:54,960 --> 00:14:58,920
two million but imagine if you had five

00:14:56,640 --> 00:15:01,860
million so ideally each time we getting

00:14:58,920 --> 00:15:04,560
a user input we're getting from some

00:15:01,860 --> 00:15:07,020
user preferences service we can get also

00:15:04,560 --> 00:15:08,220
the Preferences of the user and then in

00:15:07,020 --> 00:15:10,800
real time we need to be able to

00:15:08,220 --> 00:15:12,300
calculate the user embedding so imagine

00:15:10,800 --> 00:15:15,540
we could do it without a problem but

00:15:12,300 --> 00:15:18,150
then we need actually to rank that usery

00:15:15,540 --> 00:15:19,530
beddings against all our item in banks

00:15:18,150 --> 00:15:22,170
against all the two million item in

00:15:19,530 --> 00:15:24,330
banks to get the rank for each item and

00:15:22,170 --> 00:15:26,580
then to take for example the top ten of

00:15:24,330 --> 00:15:30,420
the top 20 so actually for each user

00:15:26,580 --> 00:15:32,130
request ranking through the rank net 2

00:15:30,420 --> 00:15:33,810
million item embedding sits it will not

00:15:32,130 --> 00:15:35,220
scale it will not work it will take us

00:15:33,810 --> 00:15:38,010
more than a minute or two minutes to

00:15:35,220 --> 00:15:39,690
submit recommendations for the user and

00:15:38,010 --> 00:15:41,689
I guess nobody will be really much happy

00:15:39,690 --> 00:15:44,399
with such performance

00:15:41,689 --> 00:15:46,800
so the idea is that we need first to

00:15:44,399 --> 00:15:49,680
filter those items and we need first to

00:15:46,800 --> 00:15:52,889
try to find out some way that will allow

00:15:49,680 --> 00:15:53,639
us to select the more relevant items for

00:15:52,889 --> 00:15:55,709
that user

00:15:53,639 --> 00:15:58,529
for example let's filter out and select

00:15:55,709 --> 00:16:00,660
approximately only 200 and till 300 item

00:15:58,529 --> 00:16:02,579
candidates and based on those candidates

00:16:00,660 --> 00:16:04,620
we will use only the embeddings of those

00:16:02,579 --> 00:16:06,990
candidates to pass them through through

00:16:04,620 --> 00:16:09,569
the rank Network and rank on is those

00:16:06,990 --> 00:16:11,819
200 and till 300 candidates once we have

00:16:09,569 --> 00:16:14,209
done that we can take the top 10 of the

00:16:11,819 --> 00:16:16,439
top 20 from that but here's a problem

00:16:14,209 --> 00:16:18,689
actually how actually we can do that

00:16:16,439 --> 00:16:26,129
kind of filtering especially with two

00:16:18,689 --> 00:16:29,610
million so our first approach was to try

00:16:26,129 --> 00:16:31,769
this elastic search so as I mentioned we

00:16:29,610 --> 00:16:35,129
have the kafka topic right and we can

00:16:31,769 --> 00:16:37,439
use a Kafka topic in order to write some

00:16:35,129 --> 00:16:39,300
sort of elastic search indexer and we

00:16:37,439 --> 00:16:41,790
would index our item embeddings to

00:16:39,300 --> 00:16:44,009
elastic search together with some of the

00:16:41,790 --> 00:16:46,410
attributes so for example we will in

00:16:44,009 --> 00:16:48,209
indexed elastic search the curve price

00:16:46,410 --> 00:16:50,129
and the color and number of previous

00:16:48,209 --> 00:16:52,500
owners and so on but in addition we will

00:16:50,129 --> 00:16:53,939
index L so the item embeddings and to

00:16:52,500 --> 00:16:54,959
index the item in Bennigsen elastic

00:16:53,939 --> 00:16:57,329
search it's actually quite

00:16:54,959 --> 00:17:03,059
straightforward it's just a vector a

00:16:57,329 --> 00:17:04,980
list of 77 floats in our case then each

00:17:03,059 --> 00:17:06,959
time when we get a request from a user

00:17:04,980 --> 00:17:08,699
we need to calculate the user we need to

00:17:06,959 --> 00:17:10,470
get the user preferences we will need

00:17:08,699 --> 00:17:13,380
also to have some of the user embedding

00:17:10,470 --> 00:17:15,270
and we will execute based on the user

00:17:13,380 --> 00:17:16,770
preferences a simple query against

00:17:15,270 --> 00:17:18,750
elastic search which will do filtering

00:17:16,770 --> 00:17:21,000
based on those preferences so for

00:17:18,750 --> 00:17:24,000
example if we have a user that prefer

00:17:21,000 --> 00:17:25,980
red or yellow we will ask a sick search

00:17:24,000 --> 00:17:28,559
to filter all the items that have red or

00:17:25,980 --> 00:17:31,200
yellow and we will feel we will ask

00:17:28,559 --> 00:17:33,299
elastic search to filter items in a

00:17:31,200 --> 00:17:36,990
specific price range okay in a specific

00:17:33,299 --> 00:17:40,110
car model for example BMW or Audi once

00:17:36,990 --> 00:17:43,559
we will do that we will take the top

00:17:40,110 --> 00:17:46,289
hundred the sorry the top 200 of the Riz

00:17:43,559 --> 00:17:48,690
of the result from elastic search we

00:17:46,289 --> 00:17:50,610
take on his item embeddings of those top

00:17:48,690 --> 00:17:53,190
200 and we pass them through docker

00:17:50,610 --> 00:17:55,850
container which have inside the ability

00:17:53,190 --> 00:17:58,290
to run it through the rank net

00:17:55,850 --> 00:18:00,630
so some of the problems with that

00:17:58,290 --> 00:18:03,210
approach first of all our item

00:18:00,630 --> 00:18:05,310
embeddings and our user preferences

00:18:03,210 --> 00:18:06,510
already contain implicitly inside all

00:18:05,310 --> 00:18:08,490
the information about the user

00:18:06,510 --> 00:18:10,440
preferences about the color about the

00:18:08,490 --> 00:18:13,230
price and so on it's just that the deep

00:18:10,440 --> 00:18:13,970
network learned it in a more obstructive

00:18:13,230 --> 00:18:16,530
way in a more mathematical

00:18:13,970 --> 00:18:18,510
representation so actually if we will do

00:18:16,530 --> 00:18:21,570
that kind of filtering before it's until

00:18:18,510 --> 00:18:24,540
some degree I will not say it's resident

00:18:21,570 --> 00:18:26,880
but then it's it's useless and to use

00:18:24,540 --> 00:18:28,860
the deep learning because the the real

00:18:26,880 --> 00:18:29,820
power of making better recommendations

00:18:28,860 --> 00:18:31,560
is actually to use the embeddings

00:18:29,820 --> 00:18:32,970
directly and if I do this kind of

00:18:31,560 --> 00:18:35,040
filtering it could be that I'm actually

00:18:32,970 --> 00:18:37,380
missing something and I will remain with

00:18:35,040 --> 00:18:40,380
items that are really not the one that

00:18:37,380 --> 00:18:42,330
fit for that user so this is why it was

00:18:40,380 --> 00:18:46,740
a bit problematic second of all and the

00:18:42,330 --> 00:18:48,500
biggest problem scalability so that

00:18:46,740 --> 00:18:50,910
approach is not scanning really well

00:18:48,500 --> 00:18:52,920
executing thousands of those queries per

00:18:50,910 --> 00:18:54,240
second against elastic search to be able

00:18:52,920 --> 00:18:57,030
to do this kind of filtering and then

00:18:54,240 --> 00:18:58,650
taking only the 200 make elastic search

00:18:57,030 --> 00:19:01,470
works really hard and we saw a lot of

00:18:58,650 --> 00:19:03,510
issues and okay currently we have our

00:19:01,470 --> 00:19:06,120
elastic search I think is around 25

00:19:03,510 --> 00:19:08,250
knots cluster but still we had a lot of

00:19:06,120 --> 00:19:10,860
issues to scale it like beyond already

00:19:08,250 --> 00:19:15,480
200 requests per second so this was not

00:19:10,860 --> 00:19:19,710
the ideal solution for us then we

00:19:15,480 --> 00:19:21,720
decided to try a more crazy idea and I

00:19:19,710 --> 00:19:24,840
think eventually it's actually also

00:19:21,720 --> 00:19:28,050
remained only in the matter of of idea

00:19:24,840 --> 00:19:32,160
so the idea is as following I will try

00:19:28,050 --> 00:19:34,920
to explain it clearly so we have all the

00:19:32,160 --> 00:19:37,650
item Ebanks item Ebanks are just vectors

00:19:34,920 --> 00:19:39,210
right so the operations that we can do

00:19:37,650 --> 00:19:41,610
on top of those item embeddings is

00:19:39,210 --> 00:19:44,250
clustering using even the simple k-means

00:19:41,610 --> 00:19:46,050
so we can cluster all the two million

00:19:44,250 --> 00:19:48,240
item embeddings two different clusters

00:19:46,050 --> 00:19:51,210
let's assume we will cluster it to 20

00:19:48,240 --> 00:19:53,670
until 30 clusters once we have the

00:19:51,210 --> 00:19:55,680
clusters we can assign each item to

00:19:53,670 --> 00:19:57,720
which cluster it belongs based on the

00:19:55,680 --> 00:19:59,190
distance calculation between the item

00:19:57,720 --> 00:20:01,260
embedding and the centroid of the

00:19:59,190 --> 00:20:03,390
cluster so then what we can do in the

00:20:01,260 --> 00:20:04,170
next step when we index our items into

00:20:03,390 --> 00:20:06,930
elasticsearch

00:20:04,170 --> 00:20:08,750
we can also index together with the item

00:20:06,930 --> 00:20:11,270
embedding the item attributes

00:20:08,750 --> 00:20:13,820
the century the cluster to which that

00:20:11,270 --> 00:20:16,310
item belongs and why it's good how it

00:20:13,820 --> 00:20:18,140
can help us so on the next step when

00:20:16,310 --> 00:20:21,050
we're getting a request from a user to

00:20:18,140 --> 00:20:23,990
generate recommendations for we can

00:20:21,050 --> 00:20:25,730
generate the user embed ink and then for

00:20:23,990 --> 00:20:28,070
that user embedding we will do almost

00:20:25,730 --> 00:20:30,560
something similar we will calculate a

00:20:28,070 --> 00:20:33,080
karidian distance between the user

00:20:30,560 --> 00:20:35,480
embedding vector and all the centroids

00:20:33,080 --> 00:20:38,450
of the clusters that we calculated

00:20:35,480 --> 00:20:40,250
before and then we will take for example

00:20:38,450 --> 00:20:44,090
two or three clusters which are the most

00:20:40,250 --> 00:20:45,470
closest to that user okay so once we

00:20:44,090 --> 00:20:48,110
have the user embedding we can calculate

00:20:45,470 --> 00:20:50,540
what are the cluster centroids that

00:20:48,110 --> 00:20:53,060
represent item embeddings closest to

00:20:50,540 --> 00:20:55,430
that user embedding and we can use it as

00:20:53,060 --> 00:20:56,900
a filter in elasticsearch so then in

00:20:55,430 --> 00:20:58,940
elasticsearch instead of filtering by

00:20:56,900 --> 00:21:00,890
user preferences we actually will say

00:20:58,940 --> 00:21:03,290
okay give us all the items that belong

00:21:00,890 --> 00:21:04,820
to cluster one two and five because we

00:21:03,290 --> 00:21:06,470
believe that this is the clusters that

00:21:04,820 --> 00:21:09,140
have the items most closest to that user

00:21:06,470 --> 00:21:12,350
okay but that not enough we want still

00:21:09,140 --> 00:21:14,630
to leverage our embeddings even more so

00:21:12,350 --> 00:21:17,060
we could also on the Spree filtered

00:21:14,630 --> 00:21:18,980
results to use a quality and distance to

00:21:17,060 --> 00:21:23,240
calculate the distance between user

00:21:18,980 --> 00:21:24,920
embedding and item in veining now it's

00:21:23,240 --> 00:21:26,540
not really a quality and distance it's

00:21:24,920 --> 00:21:27,680
more approximation of a quality and

00:21:26,540 --> 00:21:29,690
distance because then you don't need to

00:21:27,680 --> 00:21:32,660
calculate the route square and then it's

00:21:29,690 --> 00:21:34,070
make life easier from elasticsearch also

00:21:32,660 --> 00:21:35,990
because we don't really need the precise

00:21:34,070 --> 00:21:37,370
distance for the quality alone we just

00:21:35,990 --> 00:21:39,500
need something that give us relative

00:21:37,370 --> 00:21:40,880
sorting and the approximation of

00:21:39,500 --> 00:21:43,910
equivalent distance is good enough to

00:21:40,880 --> 00:21:45,740
give relative sorting in order to

00:21:43,910 --> 00:21:47,300
implement the quality and distance

00:21:45,740 --> 00:21:49,010
calculation in lastik searched was not

00:21:47,300 --> 00:21:51,050
so straightforward because elasticsearch

00:21:49,010 --> 00:21:52,700
out-of-the-box support all kind of those

00:21:51,050 --> 00:21:55,790
functions only with vectors for two

00:21:52,700 --> 00:21:59,000
dimensions like Gyo Gyo queries but not

00:21:55,790 --> 00:22:01,280
with 77 floats so luckily for us from

00:21:59,000 --> 00:22:03,080
elasticsearch 5 and above there is a new

00:22:01,280 --> 00:22:04,850
scripting language called painless and

00:22:03,080 --> 00:22:06,530
that language is eventually when you

00:22:04,850 --> 00:22:08,300
write a script is compiled directly to

00:22:06,530 --> 00:22:10,460
Java bytes so actually the execution is

00:22:08,300 --> 00:22:13,370
as fastest as it can be for native

00:22:10,460 --> 00:22:14,660
elasticsearch functions already so we

00:22:13,370 --> 00:22:17,180
actually had to write a really simple

00:22:14,660 --> 00:22:19,220
function which was just really a simple

00:22:17,180 --> 00:22:21,530
loop that you would switch a item in

00:22:19,220 --> 00:22:22,600
each of the vectors and just do sass

00:22:21,530 --> 00:22:25,470
obstruction and some

00:22:22,600 --> 00:22:27,580
chillie another way to do it is with

00:22:25,470 --> 00:22:29,620
plugins and we actually thought about

00:22:27,580 --> 00:22:32,140
maybe leveraging some of the Java more

00:22:29,620 --> 00:22:34,360
linear algebra core computational

00:22:32,140 --> 00:22:36,549
numerical libraries I tried it I done a

00:22:34,360 --> 00:22:40,000
benchmark but for this kind of vectors

00:22:36,549 --> 00:22:42,280
substraction of 77 floats actually loop

00:22:40,000 --> 00:22:44,140
in Java is was the fastest really so

00:22:42,280 --> 00:22:45,240
going for numerical libraries was

00:22:44,140 --> 00:22:48,070
overkill

00:22:45,240 --> 00:22:49,840
so once we have that we could execute

00:22:48,070 --> 00:22:51,520
sort of a risk or query inside

00:22:49,840 --> 00:22:53,919
elasticsearch is calculating equality

00:22:51,520 --> 00:22:55,390
and distance and then taking on his

00:22:53,919 --> 00:22:58,120
items which have the closest one for

00:22:55,390 --> 00:22:59,950
example the top 200 but again we try to

00:22:58,120 --> 00:23:01,780
do it a bit we saw that this also have a

00:22:59,950 --> 00:23:03,250
lot of problems you need to reduce the

00:23:01,780 --> 00:23:05,620
month and number of items quite

00:23:03,250 --> 00:23:07,510
drastically because the calculation of

00:23:05,620 --> 00:23:10,090
the quality and distance since it's not

00:23:07,510 --> 00:23:11,710
supported natively by elasticsearch was

00:23:10,090 --> 00:23:15,640
quite problematic and it was not scaling

00:23:11,710 --> 00:23:18,039
as well so finally the approach that we

00:23:15,640 --> 00:23:20,440
came up was the nearest neighbors search

00:23:18,039 --> 00:23:22,690
approach approximation actually and we

00:23:20,440 --> 00:23:24,280
found a really good library enoy which

00:23:22,690 --> 00:23:26,140
is coming from Spotify and what if I

00:23:24,280 --> 00:23:28,120
using this library for some of the

00:23:26,140 --> 00:23:30,179
recommendations and the full name is

00:23:28,120 --> 00:23:33,280
approximate nearest neighbors per year

00:23:30,179 --> 00:23:35,409
and the name is funny but the library is

00:23:33,280 --> 00:23:36,909
really good and what it was really

00:23:35,409 --> 00:23:40,120
actually good in that libraries that it

00:23:36,909 --> 00:23:42,789
is a C++ implementation and it's allow

00:23:40,120 --> 00:23:44,950
searching points in spaces that are

00:23:42,789 --> 00:23:47,230
close to a given query right so for

00:23:44,950 --> 00:23:49,990
example we have the user embedding 77

00:23:47,230 --> 00:23:52,360
floats all our items are also 77 floats

00:23:49,990 --> 00:23:54,820
so the user embeddings will be the input

00:23:52,360 --> 00:23:57,309
query for our nearest neighbor search

00:23:54,820 --> 00:24:00,280
and then it it will help us to find the

00:23:57,309 --> 00:24:02,590
most closest item embeddings which we

00:24:00,280 --> 00:24:06,640
can then later tag and pass through the

00:24:02,590 --> 00:24:08,289
rand network well it was extra really

00:24:06,640 --> 00:24:12,370
fast the library works really fast

00:24:08,289 --> 00:24:13,690
it keeps an index mam it keeps an index

00:24:12,370 --> 00:24:15,520
file in memories actually it have a

00:24:13,690 --> 00:24:17,230
special technique that I will not go too

00:24:15,520 --> 00:24:18,549
deep into it which use random

00:24:17,230 --> 00:24:21,669
projections and it created a special

00:24:18,549 --> 00:24:23,530
index 3 and inside index trees this is

00:24:21,669 --> 00:24:25,750
they use some sort of approximation to

00:24:23,530 --> 00:24:27,880
find the nearest neighbors what was the

00:24:25,750 --> 00:24:29,530
best that is support Scala and Python as

00:24:27,880 --> 00:24:30,700
I mentioned we didn't want to rewrite

00:24:29,530 --> 00:24:32,559
the code so we want to have something

00:24:30,700 --> 00:24:34,720
that the data science can check on his

00:24:32,559 --> 00:24:36,040
site from Python perspective and we can

00:24:34,720 --> 00:24:37,480
check in scar

00:24:36,040 --> 00:24:40,180
and that you have exactly the same

00:24:37,480 --> 00:24:42,250
behavior and the fact that the memory

00:24:40,180 --> 00:24:44,230
that the index file isn't memories also

00:24:42,250 --> 00:24:46,240
helped a lot to boost up the performance

00:24:44,230 --> 00:24:47,800
especially since at least in our case we

00:24:46,240 --> 00:24:49,930
have only two million so putting it in

00:24:47,800 --> 00:24:52,060
memory it was like around 125 gigabyte

00:24:49,930 --> 00:24:53,410
which is definitely not a problem today

00:24:52,060 --> 00:24:56,230
in any environment in any production

00:24:53,410 --> 00:24:57,940
environment I know a library supports

00:24:56,230 --> 00:24:59,950
multiple different distance calculation

00:24:57,940 --> 00:25:02,590
like a collodion Manhattan and Harmon

00:24:59,950 --> 00:25:05,350
concern we had to we needed on his equal

00:25:02,590 --> 00:25:07,840
ideon and it works best with vectors

00:25:05,350 --> 00:25:09,400
under 100 dimensions but actually it

00:25:07,840 --> 00:25:12,790
performs quite well also these vectors

00:25:09,400 --> 00:25:15,670
up until 1000 immense since we have 77

00:25:12,790 --> 00:25:17,830
dimensions it was perfect for us so how

00:25:15,670 --> 00:25:19,810
we make it to work eventually so back to

00:25:17,830 --> 00:25:21,910
the flaws that we had about how we

00:25:19,810 --> 00:25:24,340
update in real time our item embeddings

00:25:21,910 --> 00:25:26,290
right so we have still the Kafka topic

00:25:24,340 --> 00:25:28,030
with item embeddings and we have the

00:25:26,290 --> 00:25:30,790
docker configuration with Scala gateway

00:25:28,030 --> 00:25:33,610
and tensorflow serving we calculate the

00:25:30,790 --> 00:25:35,140
item embeddings but eventually since

00:25:33,610 --> 00:25:36,400
we're putting them as remember I told

00:25:35,140 --> 00:25:38,680
you since we put them in a separate

00:25:36,400 --> 00:25:40,090
Kafka topic somebody else can just take

00:25:38,680 --> 00:25:42,220
that topic consume it and to do whatever

00:25:40,090 --> 00:25:43,950
you want so in the first attempt we try

00:25:42,220 --> 00:25:46,630
to consume from the topic and put in

00:25:43,950 --> 00:25:47,980
indexing in elasticsearch on the second

00:25:46,630 --> 00:25:49,840
attempt what we're doing is actually

00:25:47,980 --> 00:25:52,630
we're running a schedule job in jenkins

00:25:49,840 --> 00:25:55,120
each hour and that job each hour consume

00:25:52,630 --> 00:25:56,830
all the items from that Kafka topic and

00:25:55,120 --> 00:25:58,900
again since we have only two million we

00:25:56,830 --> 00:26:00,700
can afford it and it takes around 15

00:25:58,900 --> 00:26:03,790
minutes to consume the entire topic and

00:26:00,700 --> 00:26:05,650
it builds that kind of annoy index file

00:26:03,790 --> 00:26:07,660
once we are finishing to build that

00:26:05,650 --> 00:26:09,760
annoy index file we need to push it to

00:26:07,660 --> 00:26:12,580
some object storage which will be used

00:26:09,760 --> 00:26:14,920
later by some the core application to

00:26:12,580 --> 00:26:21,250
load it into memory and to use for

00:26:14,920 --> 00:26:25,730
submitting recommendations anything

00:26:21,250 --> 00:26:28,730
chemists know okay

00:26:25,730 --> 00:26:31,549
so then the next question after we

00:26:28,730 --> 00:26:33,230
solved how we select candidates using

00:26:31,549 --> 00:26:37,130
the nearest approximate neighbors

00:26:33,230 --> 00:26:38,990
approach and how and how we were able to

00:26:37,130 --> 00:26:40,880
calculate item in banks and update them

00:26:38,990 --> 00:26:43,279
in real time and that we figure out how

00:26:40,880 --> 00:26:45,230
we're dealing with the problem of Python

00:26:43,279 --> 00:26:47,539
and Scala the next question is actually

00:26:45,230 --> 00:26:49,820
okay but how do you eventually generate

00:26:47,539 --> 00:26:51,590
those recommendations and again we need

00:26:49,820 --> 00:26:53,570
to be able to generate recommendations

00:26:51,590 --> 00:26:55,340
in real time we don't want to generate

00:26:53,570 --> 00:26:57,559
those recommendations in a batch once a

00:26:55,340 --> 00:26:59,059
day so each time a user getting a

00:26:57,559 --> 00:27:00,559
request we want to calculate his

00:26:59,059 --> 00:27:03,799
embeddings and we want to calculate what

00:27:00,559 --> 00:27:05,990
items can feed him for that for that

00:27:03,799 --> 00:27:08,440
point the most updated items not waiting

00:27:05,990 --> 00:27:11,480
for the night to do that kind of magic

00:27:08,440 --> 00:27:14,600
so we have another docker container

00:27:11,480 --> 00:27:16,070
which is the main the core applications

00:27:14,600 --> 00:27:18,289
the core deep learning application and

00:27:16,070 --> 00:27:19,820
that docker container have three major

00:27:18,289 --> 00:27:22,130
components so it will have the Scala

00:27:19,820 --> 00:27:23,750
gateway that have all the logic and all

00:27:22,130 --> 00:27:25,700
the coordination between all the

00:27:23,750 --> 00:27:27,950
different pieces and networks it have

00:27:25,700 --> 00:27:30,830
tens of serving tens of serving again

00:27:27,950 --> 00:27:32,630
the same configuration but this time we

00:27:30,830 --> 00:27:34,639
have two different nets that we are

00:27:32,630 --> 00:27:36,200
holding in that docker we have the user

00:27:34,639 --> 00:27:38,929
net in order to be able to calculate

00:27:36,200 --> 00:27:40,970
user embeddings in real time and we have

00:27:38,929 --> 00:27:42,919
the rank net in order to make the

00:27:40,970 --> 00:27:45,260
ranking between the user net and the

00:27:42,919 --> 00:27:47,539
item in benning's and actually we also

00:27:45,260 --> 00:27:49,789
need the annoy index file so the scale

00:27:47,539 --> 00:27:52,010
application can look inside that annoy

00:27:49,789 --> 00:27:55,519
index to find the nearest neighbors the

00:27:52,010 --> 00:27:57,080
candidate so let's have a look on the

00:27:55,519 --> 00:27:58,940
flow what's happening when we getting a

00:27:57,080 --> 00:28:03,230
request of a user and we need generate

00:27:58,940 --> 00:28:05,960
recommendations so first of all we need

00:28:03,230 --> 00:28:07,760
to get the user preferences and the user

00:28:05,960 --> 00:28:09,799
preferences we can get from the user

00:28:07,760 --> 00:28:11,659
preferences service it's a completely

00:28:09,799 --> 00:28:15,049
separate service and actually I gave a

00:28:11,659 --> 00:28:16,940
talk about it last last year I also

00:28:15,049 --> 00:28:18,620
recommend you if you want to see how you

00:28:16,940 --> 00:28:19,039
can calculate user preferences in real

00:28:18,620 --> 00:28:21,019
time

00:28:19,039 --> 00:28:23,179
the trick is sync with the user

00:28:21,019 --> 00:28:25,309
preferences is like item embeddings user

00:28:23,179 --> 00:28:27,559
preferences are calculated in real time

00:28:25,309 --> 00:28:30,230
so when some user come to our platform

00:28:27,559 --> 00:28:33,799
he search he view his he saved something

00:28:30,230 --> 00:28:36,350
to his favorites list his preferences

00:28:33,799 --> 00:28:38,120
updated almost immediately which means

00:28:36,350 --> 00:28:39,230
that the profit that the Prados

00:28:38,120 --> 00:28:41,600
preferences are not

00:28:39,230 --> 00:28:42,889
statics are dynamic and that means that

00:28:41,600 --> 00:28:44,389
we really cannot calculate user

00:28:42,889 --> 00:28:45,710
embeddings overnight as well because

00:28:44,389 --> 00:28:47,570
it's keep changing constantly

00:28:45,710 --> 00:28:49,700
this is why we need to take the zero

00:28:47,570 --> 00:28:52,820
user preferences those statistics of 20

00:28:49,700 --> 00:28:55,639
20 % blue-collar and price distribution

00:28:52,820 --> 00:28:57,049
of 20,000 plus minus 1 25k and from

00:28:55,639 --> 00:28:59,600
those in real time to be able to

00:28:57,049 --> 00:29:01,549
calculate the user embedding so once we

00:28:59,600 --> 00:29:03,169
get into the Preferences the next step

00:29:01,549 --> 00:29:06,440
will be to calculate the user embedding

00:29:03,169 --> 00:29:09,019
user using the user net once we have the

00:29:06,440 --> 00:29:10,519
user embedding we continue to the

00:29:09,019 --> 00:29:13,039
following step which is finding the

00:29:10,519 --> 00:29:14,899
k-nearest items using the Neue library

00:29:13,039 --> 00:29:16,760
right there are no index files the

00:29:14,899 --> 00:29:19,460
nearest neighbors approach we take we

00:29:16,760 --> 00:29:21,679
take approximately now I think today 200

00:29:19,460 --> 00:29:23,149
candidates once we have those 200

00:29:21,679 --> 00:29:26,570
candidates which are we believe the most

00:29:23,149 --> 00:29:29,929
closest to that user we using the other

00:29:26,570 --> 00:29:31,909
net rent net to give a score for each of

00:29:29,929 --> 00:29:34,730
those item embeddings and sort of saying

00:29:31,909 --> 00:29:39,110
this is a probability of that user to

00:29:34,730 --> 00:29:41,480
interact with that items and finally

00:29:39,110 --> 00:29:43,940
from sauce we take top 10 or top 20

00:29:41,480 --> 00:29:45,380
depend on the sec holder in the service

00:29:43,940 --> 00:29:47,840
you can define I want 10 recommendations

00:29:45,380 --> 00:29:54,950
5 recommendations so this would be

00:29:47,840 --> 00:29:56,360
eventually our recommendations ok so the

00:29:54,950 --> 00:29:58,370
general view how everything looks

00:29:56,360 --> 00:30:00,740
eventually in production so we have one

00:29:58,370 --> 00:30:02,450
side that is consuming items

00:30:00,740 --> 00:30:05,480
calculated constantly the item

00:30:02,450 --> 00:30:07,510
embeddings putting them to Kafka topic

00:30:05,480 --> 00:30:10,429
which have those embeddings we have a

00:30:07,510 --> 00:30:12,470
job that runs each hour calculate

00:30:10,429 --> 00:30:14,690
generates a new annoying index file and

00:30:12,470 --> 00:30:16,700
push it to some object storage

00:30:14,690 --> 00:30:19,039
something like a 3 but we have our own

00:30:16,700 --> 00:30:21,740
private cloud so it's something based on

00:30:19,039 --> 00:30:24,320
OpenStack on the other hand we have

00:30:21,740 --> 00:30:26,539
another docker which is the core deep

00:30:24,320 --> 00:30:28,100
learning application it is able to

00:30:26,539 --> 00:30:29,750
communicate with the user preferences

00:30:28,100 --> 00:30:31,700
service to get the Preferences each time

00:30:29,750 --> 00:30:33,139
it's needed it's able to calculate the

00:30:31,700 --> 00:30:35,630
user embeddings in real time each time

00:30:33,139 --> 00:30:39,320
it's needed and to do the ranking and

00:30:35,630 --> 00:30:41,539
also he need to load as a no index file

00:30:39,320 --> 00:30:43,010
from that object storage so when we

00:30:41,539 --> 00:30:45,559
deploy this application on the

00:30:43,010 --> 00:30:47,600
production at the beginning there is a

00:30:45,559 --> 00:30:49,279
sort of part of the code that knows ok

00:30:47,600 --> 00:30:50,570
now before I start the application and

00:30:49,279 --> 00:30:52,770
it's available for all the stakeholders

00:30:50,570 --> 00:30:55,890
I need first to go and grab

00:30:52,770 --> 00:30:57,540
I know index from the object storage the

00:30:55,890 --> 00:30:59,400
other question is how actually but then

00:30:57,540 --> 00:31:00,840
you put each hour the new update is

00:30:59,400 --> 00:31:04,200
annoying index file to that application

00:31:00,840 --> 00:31:06,090
and here's our multiple ways one of the

00:31:04,200 --> 00:31:08,460
ways is for example to build a mechanism

00:31:06,090 --> 00:31:10,500
inside your application that constantly

00:31:08,460 --> 00:31:12,150
keep checking each minute for example

00:31:10,500 --> 00:31:14,310
the object storage to see if there is a

00:31:12,150 --> 00:31:16,560
new file and if yes it's loaded and it's

00:31:14,310 --> 00:31:18,360
doing swap in memory on the fly so

00:31:16,560 --> 00:31:21,360
really without any kind of down times

00:31:18,360 --> 00:31:23,880
this is one way another option is to

00:31:21,360 --> 00:31:25,500
send actually notifications to the

00:31:23,880 --> 00:31:27,690
application saying hey I have a

00:31:25,500 --> 00:31:30,210
calculated a new file please grab it

00:31:27,690 --> 00:31:32,700
from the please please get it from the

00:31:30,210 --> 00:31:34,350
object storage and this you can do in

00:31:32,700 --> 00:31:36,450
two ways either you have a Kafka topic

00:31:34,350 --> 00:31:38,100
which the application consumes messages

00:31:36,450 --> 00:31:39,420
from Kafka topic and through the Kafka

00:31:38,100 --> 00:31:41,010
topic you keep sending messages hey I

00:31:39,420 --> 00:31:43,650
have a new file and then the application

00:31:41,010 --> 00:31:45,030
will know to grab it either actually

00:31:43,650 --> 00:31:46,800
what we decided to do is a bit more

00:31:45,030 --> 00:31:49,310
easier we have something called console

00:31:46,800 --> 00:31:53,160
from Hoshi Corp it's it's a specific

00:31:49,310 --> 00:31:55,320
software that allow self discovery of

00:31:53,160 --> 00:31:57,420
services but it have also a lightweight

00:31:55,320 --> 00:31:59,460
key value storage and it's actually have

00:31:57,420 --> 00:32:01,500
a specific functionalities that you can

00:31:59,460 --> 00:32:03,930
listen to changes in that specific key

00:32:01,500 --> 00:32:06,690
value so each time we actually push a

00:32:03,930 --> 00:32:08,460
new index file to the object storage we

00:32:06,690 --> 00:32:10,440
update that key value the other

00:32:08,460 --> 00:32:12,480
application constantly listening to

00:32:10,440 --> 00:32:15,360
changes in that key value and once there

00:32:12,480 --> 00:32:17,490
is a change it will take the new URL and

00:32:15,360 --> 00:32:19,470
will download the new index file and

00:32:17,490 --> 00:32:26,060
keep it in and switch switch it in

00:32:19,470 --> 00:32:28,200
memory all right so for some final notes

00:32:26,060 --> 00:32:31,530
bring data scientists down to earth

00:32:28,200 --> 00:32:32,850
really important because when I came to

00:32:31,530 --> 00:32:34,830
start working with the data scientist

00:32:32,850 --> 00:32:36,360
about this project they had the Python

00:32:34,830 --> 00:32:38,220
and they had this implementation and

00:32:36,360 --> 00:32:39,510
there was some exotic library here and

00:32:38,220 --> 00:32:41,760
exotic libraries there and many

00:32:39,510 --> 00:32:43,980
requirements and it's all look guys this

00:32:41,760 --> 00:32:45,750
is all nice it works fine it was really

00:32:43,980 --> 00:32:47,040
beautiful in your thesis but we need to

00:32:45,750 --> 00:32:48,840
bring it to production production is

00:32:47,040 --> 00:32:50,880
completely different environment we

00:32:48,840 --> 00:32:53,430
cannot allow ourselves too much playing

00:32:50,880 --> 00:32:55,710
there and failing so you need to see

00:32:53,430 --> 00:32:56,940
what data science bring you and then you

00:32:55,710 --> 00:32:59,730
need to conform with what is actually

00:32:56,940 --> 00:33:01,260
possible in reality ok but don't start

00:32:59,730 --> 00:33:01,950
from but don't stop from yourself to

00:33:01,260 --> 00:33:03,990
being creative

00:33:01,950 --> 00:33:05,640
don't say hey this is really hard to

00:33:03,990 --> 00:33:06,480
implement I don't know this exotic I

00:33:05,640 --> 00:33:08,100
don't know how to put it in

00:33:06,480 --> 00:33:11,040
the Scala world so I will not put it

00:33:08,100 --> 00:33:13,530
know try to listen and try to to see

00:33:11,040 --> 00:33:15,030
what you can do this is why it died you

00:33:13,530 --> 00:33:16,860
need to take time to do the research and

00:33:15,030 --> 00:33:18,299
read it took me some time to read

00:33:16,860 --> 00:33:21,090
different blogs and researchers

00:33:18,299 --> 00:33:23,190
especially even not related to Scala and

00:33:21,090 --> 00:33:24,600
but more to Python to get to get idea

00:33:23,190 --> 00:33:27,750
how we can actually do it in Scala as

00:33:24,600 --> 00:33:28,980
well for that project actually I think

00:33:27,750 --> 00:33:30,690
that basic understanding of deep

00:33:28,980 --> 00:33:32,700
learning concept was enough I am really

00:33:30,690 --> 00:33:34,350
not a deep learning expert as you see

00:33:32,700 --> 00:33:36,330
this talk was not about how to choose a

00:33:34,350 --> 00:33:38,070
proper deep learning model or framework

00:33:36,330 --> 00:33:40,500
or whatever but how you can put the

00:33:38,070 --> 00:33:42,179
pieces working together in production

00:33:40,500 --> 00:33:44,610
and basic concepts understanding is

00:33:42,179 --> 00:33:47,040
quite important really important at

00:33:44,610 --> 00:33:49,110
least for our case was to find sky

00:33:47,040 --> 00:33:50,730
equivalent libraries to Python because

00:33:49,110 --> 00:33:51,600
we didn't want to do code rewrite since

00:33:50,730 --> 00:33:53,910
we have already is that kind of

00:33:51,600 --> 00:33:55,320
experience from previous projects so for

00:33:53,910 --> 00:33:57,240
me it's really important otherwise you

00:33:55,320 --> 00:33:59,429
have a really he'll maintain two

00:33:57,240 --> 00:34:01,500
different code bases and as I mentioned

00:33:59,429 --> 00:34:03,750
create creativity and out-of-the-box

00:34:01,500 --> 00:34:06,390
thinking it's quite important don't give

00:34:03,750 --> 00:34:07,679
up because you think something is too

00:34:06,390 --> 00:34:09,899
new or too exotic

00:34:07,679 --> 00:34:12,020
maybe there are still some really nice

00:34:09,899 --> 00:34:23,550
solution somewhere that you can leverage

00:34:12,020 --> 00:34:29,359
thank you so we have five minutes

00:34:23,550 --> 00:34:31,830
question so hi

00:34:29,359 --> 00:34:34,649
you mentioned that you're running an

00:34:31,830 --> 00:34:37,470
hourly job to recalculate the index for

00:34:34,649 --> 00:34:39,440
the no library right does it mean that

00:34:37,470 --> 00:34:42,810
you are still running into the risk of

00:34:39,440 --> 00:34:47,399
having like outdated index basically huh

00:34:42,810 --> 00:34:50,310
yes but we did some analytics and we

00:34:47,399 --> 00:34:52,500
tried to see what are the risk for that

00:34:50,310 --> 00:34:54,899
and we saw that they are constantly

00:34:52,500 --> 00:34:57,270
items that are updated but still we

00:34:54,899 --> 00:35:00,119
decided that during that hour we can

00:34:57,270 --> 00:35:01,740
sort of take that risk because otherwise

00:35:00,119 --> 00:35:03,960
it was between not doing it at all or

00:35:01,740 --> 00:35:05,730
doing it sort of you can say just in

00:35:03,960 --> 00:35:09,950
time right so real-time definitions are

00:35:05,730 --> 00:35:09,950
what is real time for use just in time

00:35:13,400 --> 00:35:18,450
I'm a I'm curious I'm curious if you

00:35:16,800 --> 00:35:20,670
looked at the elasticsearch learning to

00:35:18,450 --> 00:35:21,900
rank plugin for implementing crank net

00:35:20,670 --> 00:35:24,420
because they have a rank net

00:35:21,900 --> 00:35:26,880
implementation again the there's a

00:35:24,420 --> 00:35:28,470
elasticsearch learning to rank yeah we

00:35:26,880 --> 00:35:31,560
had a look on that but this is actually

00:35:28,470 --> 00:35:32,730
not the one this it works a bit

00:35:31,560 --> 00:35:35,250
differently than that one it will not

00:35:32,730 --> 00:35:36,570
help us actually I didn't had time to

00:35:35,250 --> 00:35:39,570
put it on the slides here but we had

00:35:36,570 --> 00:35:40,920
resently workshop with West I don't know

00:35:39,570 --> 00:35:43,920
if you heard about it it's open source

00:35:40,920 --> 00:35:45,540
presently from Yahoo and actually we are

00:35:43,920 --> 00:35:48,360
now evaluating it to see if this can

00:35:45,540 --> 00:35:53,940
replace entirely that stuff because it's

00:35:48,360 --> 00:35:57,180
exactly for that use case when we and

00:35:53,940 --> 00:36:00,090
well we evaluated the skull and the

00:35:57,180 --> 00:36:02,970
peyten implementation of an OE we found

00:36:00,090 --> 00:36:05,730
that the skull implementation was very

00:36:02,970 --> 00:36:07,470
slow did you have any did you look into

00:36:05,730 --> 00:36:09,690
the performance of this maybe we use the

00:36:07,470 --> 00:36:13,320
wrong library so you mean scallion

00:36:09,690 --> 00:36:14,610
permutation of tensorflow I know II I'm

00:36:13,320 --> 00:36:16,950
using this color implementation of an IE

00:36:14,610 --> 00:36:18,780
and did you do any evaluation of the

00:36:16,950 --> 00:36:23,100
performance versus the python

00:36:18,780 --> 00:36:24,510
implementation we I don't remember if

00:36:23,100 --> 00:36:26,130
you've done any comparison but it was

00:36:24,510 --> 00:36:27,360
working come in fine and we compare

00:36:26,130 --> 00:36:30,210
these data science to see that you

00:36:27,360 --> 00:36:32,580
getting the same results and there is

00:36:30,210 --> 00:36:34,590
actually two implementations one is a

00:36:32,580 --> 00:36:37,200
scholar pure implementation and one is a

00:36:34,590 --> 00:36:41,150
pinning to C++ that using Zano itself so

00:36:37,200 --> 00:36:41,150
it's depend which one you take thank you

00:36:44,960 --> 00:36:49,710
hi I'd be interested to know how you

00:36:48,360 --> 00:36:51,330
measure the improvement of your

00:36:49,710 --> 00:36:54,000
recommendation system after you

00:36:51,330 --> 00:36:55,470
implemented this introduction good

00:36:54,000 --> 00:36:57,090
question unfortunately currently I

00:36:55,470 --> 00:36:59,910
cannot answer it because this is still

00:36:57,090 --> 00:37:01,470
going to the a/b test scenario and most

00:36:59,910 --> 00:37:03,570
of the evaluation was done actually in

00:37:01,470 --> 00:37:05,460
sort of offline manner this is why we

00:37:03,570 --> 00:37:06,840
really try to bring it now properly to

00:37:05,460 --> 00:37:08,550
production with proper rate so it's in

00:37:06,840 --> 00:37:10,350
production is just waiting for a B tests

00:37:08,550 --> 00:37:12,150
to run properly so we can know the

00:37:10,350 --> 00:37:14,460
result so it's quite fresh out of the

00:37:12,150 --> 00:37:16,410
oven so but the answer would be a be

00:37:14,460 --> 00:37:17,580
testing in that case so currently we try

00:37:16,410 --> 00:37:19,200
to ib test against the current

00:37:17,580 --> 00:37:20,250
recommender but in general the idea is

00:37:19,200 --> 00:37:22,140
to build the system that keeps

00:37:20,250 --> 00:37:24,000
constantly monitoring the performance of

00:37:22,140 --> 00:37:27,620
that so then it's also can trigger the

00:37:24,000 --> 00:37:27,620
training of a new model when it's needed

00:37:31,950 --> 00:37:39,670
hi one question so tensorflow serving is

00:37:36,099 --> 00:37:42,190
now out when version 1 since 2017

00:37:39,670 --> 00:37:44,109
something so what's your if you look

00:37:42,190 --> 00:37:47,050
back having it in production for real

00:37:44,109 --> 00:37:49,450
you case with a lot of data how is it

00:37:47,050 --> 00:37:54,490
working how it's monitoring capabilities

00:37:49,450 --> 00:37:55,599
how stable is it so with part of the

00:37:54,490 --> 00:37:57,369
item in benning's which will calculate

00:37:55,599 --> 00:37:58,780
from Kafka constant it's in production I

00:37:57,369 --> 00:38:00,400
think already for three or four months s

00:37:58,780 --> 00:38:02,710
the other part took us a bit more time

00:38:00,400 --> 00:38:03,880
to bring to production I didn't show any

00:38:02,710 --> 00:38:06,130
problems I keep monitoring the

00:38:03,880 --> 00:38:08,080
performance a quite good sum from two

00:38:06,130 --> 00:38:11,440
until three milliseconds for making me a

00:38:08,080 --> 00:38:13,330
prediction on the 95 percentage the

00:38:11,440 --> 00:38:15,760
problems that I do know this tensor

00:38:13,330 --> 00:38:17,920
serving is that you can submit sort of

00:38:15,760 --> 00:38:20,109
for example 200 items to be ranked at

00:38:17,920 --> 00:38:22,060
the same time this will be fine but if

00:38:20,109 --> 00:38:23,650
you will try to submit thousands of them

00:38:22,060 --> 00:38:25,270
then terms of serving start to be a

00:38:23,650 --> 00:38:27,520
problem and this is for example why

00:38:25,270 --> 00:38:35,770
Vespa decided to sort of rewrite their

00:38:27,520 --> 00:38:38,140
own terms of serving alright thanks so a

00:38:35,770 --> 00:38:40,420
question about rank net so the two

00:38:38,140 --> 00:38:44,140
inputs is one the embedding of the item

00:38:40,420 --> 00:38:46,150
and embed embedding of the user so and

00:38:44,140 --> 00:38:47,710
then rank rank that is kind of black box

00:38:46,150 --> 00:38:50,920
which could learn any function but

00:38:47,710 --> 00:38:53,859
further you use as assumption that the

00:38:50,920 --> 00:38:56,470
Euclidean distance is like a good thing

00:38:53,859 --> 00:38:59,470
to filter for so how the question is how

00:38:56,470 --> 00:39:02,619
do you make sure that this assumption is

00:38:59,470 --> 00:39:04,180
is true okay I will try to answer it

00:39:02,619 --> 00:39:05,859
because as I mentioned I am NOT coming

00:39:04,180 --> 00:39:07,630
more from the data science part but

00:39:05,859 --> 00:39:09,339
first of all the rank net is more using

00:39:07,630 --> 00:39:11,200
a sinc function similar to logistic

00:39:09,339 --> 00:39:12,640
regression some sigmoid to do the

00:39:11,200 --> 00:39:16,390
classification with some negative

00:39:12,640 --> 00:39:19,869
sampling as this one the second thing we

00:39:16,390 --> 00:39:21,849
we try to punish the training of the

00:39:19,869 --> 00:39:23,680
model between the item embeddings and

00:39:21,849 --> 00:39:25,780
the user embeddings i think is using

00:39:23,680 --> 00:39:27,640
causing distance so he tried to do some

00:39:25,780 --> 00:39:29,440
sort of punishment over there in order

00:39:27,640 --> 00:39:31,260
to force the item embeddings and the

00:39:29,440 --> 00:39:33,490
user embeddings to be in the same space

00:39:31,260 --> 00:39:35,050
so this is why we believe that the

00:39:33,490 --> 00:39:38,020
collision distance can be a good enough

00:39:35,050 --> 00:39:39,520
approximation to find close items we

00:39:38,020 --> 00:39:41,320
also tried different methods

00:39:39,520 --> 00:39:42,820
to do the evaluation and if you thought

00:39:41,320 --> 00:39:45,010
actually that collinear distance works

00:39:42,820 --> 00:39:48,250
quite well this combination with the

00:39:45,010 --> 00:39:50,230
rent network but yeah user preferences

00:39:48,250 --> 00:39:52,780
are dynamically changing the source are

00:39:50,230 --> 00:39:55,720
quite stochastic the item embedding are

00:39:52,780 --> 00:39:57,810
quite static so it's it's based on the

00:39:55,720 --> 00:40:00,190
same attributes rights of the car but

00:39:57,810 --> 00:40:02,260
from one point we try to bring it into

00:40:00,190 --> 00:40:03,850
the same space from another point a car

00:40:02,260 --> 00:40:05,550
born to be read but user have some

00:40:03,850 --> 00:40:09,780
preferences which is not always read so

00:40:05,550 --> 00:40:25,150
but still the performance was quite well

00:40:09,780 --> 00:40:27,070
thanks last question yep I was curious

00:40:25,150 --> 00:40:29,140
if there is an advantage to rank net

00:40:27,070 --> 00:40:33,040
versus a gradient boosting method if you

00:40:29,140 --> 00:40:34,630
looked at that so I do you know what

00:40:33,040 --> 00:40:38,160
really much understand the second one

00:40:34,630 --> 00:40:38,160
this probably because I don't know it I

00:40:40,980 --> 00:40:51,870
can ask one of the data scientist and

00:40:43,270 --> 00:40:59,680
tell you I am purely in that engineering

00:40:51,870 --> 00:41:02,620
so no more question last one how do you

00:40:59,680 --> 00:41:05,710
know that you profile the same users you

00:41:02,620 --> 00:41:09,760
don't have to login into your site to

00:41:05,710 --> 00:41:14,050
perform the search and since users don't

00:41:09,760 --> 00:41:15,700
have to outright authenticate how do you

00:41:14,050 --> 00:41:17,740
distinguish different users from each

00:41:15,700 --> 00:41:21,820
other it's a kind of a basic questions

00:41:17,740 --> 00:41:22,960
but I'm curious how you do it GTP are

00:41:21,820 --> 00:41:27,610
related question not sure I can answer

00:41:22,960 --> 00:41:30,670
it was that lawyer but we just use the

00:41:27,610 --> 00:41:33,460
cookie ID and for those that are login

00:41:30,670 --> 00:41:37,510
it's cool but most of our users actually

00:41:33,460 --> 00:41:39,100
not login so it's just the best effort

00:41:37,510 --> 00:41:40,900
that we can do it could be situation

00:41:39,100 --> 00:41:44,110
that you are coming at mobile and

00:41:40,900 --> 00:41:46,150
tomorrow you'll be on a desktop and you

00:41:44,110 --> 00:41:49,270
will see you as two different users so

00:41:46,150 --> 00:41:52,110
we try to do the best as we can in you

00:41:49,270 --> 00:41:52,110
know in the limits of the law

00:41:53,609 --> 00:41:58,240
okay so thank you

00:41:55,779 --> 00:41:59,820
and you can enjoy your lunch next door

00:41:58,240 --> 00:42:03,349
[Applause]

00:41:59,820 --> 00:42:03,349

YouTube URL: https://www.youtube.com/watch?v=C6gB4ROjTXI


