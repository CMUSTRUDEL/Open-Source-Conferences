Title: Berlin Buzzwords 2018: Houston Putman â€“ Relevant Data Analysis: Apache Solr Analytics #bbuzz
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Data scientists now have access to vast amounts of data; therefore, it is important to ensure the data they analyze is meaningful and relevant. Search engines are used to find relevance in vast volumes of data; however, until recently, they could only do basic data analysis. We will investigate new features recently added to Apache Solr that make search an attractive option for data scientists. The talk will focus on the Solr Analytics Component and other complementary features offered by Solr.

This talk is presented by Bloomberg.

Read more:
https://2018.berlinbuzzwords.de/18/session/relevant-data-analysis-apache-solr-analytics

About Houston Putman:
https://2018.berlinbuzzwords.de/users/houston-putman

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,670 --> 00:00:09,540
hello everyone as you said my name is

00:00:07,560 --> 00:00:13,860
Houston Putman I'm a software engineer

00:00:09,540 --> 00:00:16,379
at Bloomberg and the creator and

00:00:13,860 --> 00:00:19,950
maintainer of the Apache Solr analytics

00:00:16,379 --> 00:00:21,690
component which is a control I'm here to

00:00:19,950 --> 00:00:24,540
talk to you today about relevant data

00:00:21,690 --> 00:00:26,730
analysis and unsurprisingly how the

00:00:24,540 --> 00:00:29,940
solar analytics component can solve your

00:00:26,730 --> 00:00:32,879
needs so first I'll just give a brief

00:00:29,940 --> 00:00:35,309
overview of what we do at Bloomberg we

00:00:32,879 --> 00:00:38,429
are a provider of financial governmental

00:00:35,309 --> 00:00:41,460
law and news data for a variety of

00:00:38,429 --> 00:00:43,739
professionals our strength is accurately

00:00:41,460 --> 00:00:47,219
and quickly providing this very relevant

00:00:43,739 --> 00:00:50,569
information to these clients and since

00:00:47,219 --> 00:00:53,850
we have professionals and finance any

00:00:50,569 --> 00:00:57,510
instability can really cost our clients

00:00:53,850 --> 00:01:00,149
money so stability is key we employ over

00:00:57,510 --> 00:01:03,420
5,000 software engineers and many of

00:01:00,149 --> 00:01:06,300
these are tasked with providing this

00:01:03,420 --> 00:01:09,329
very relevant data and since we have so

00:01:06,300 --> 00:01:12,740
much data this data isn't necessarily

00:01:09,329 --> 00:01:15,360
useful to the clients raw so relevant

00:01:12,740 --> 00:01:16,680
analyzed data I mean

00:01:15,360 --> 00:01:21,770
it kind of ties into what we're talking

00:01:16,680 --> 00:01:21,770
in here that is not what I wanted to do

00:01:22,549 --> 00:01:27,270
there yeah so today we're gonna start

00:01:25,500 --> 00:01:30,479
off talking about relevance and

00:01:27,270 --> 00:01:33,359
analytics and kind of how to merge these

00:01:30,479 --> 00:01:35,520
two worlds and then talk about how solar

00:01:33,359 --> 00:01:39,869
analytics is one approach to this

00:01:35,520 --> 00:01:41,939
problem and then how the solar analytics

00:01:39,869 --> 00:01:44,490
works in a distributed mode because it's

00:01:41,939 --> 00:01:45,719
not as trivial as I would think after

00:01:44,490 --> 00:01:48,719
that we'll talk about performance

00:01:45,719 --> 00:01:50,729
considerations which unsurprisingly ties

00:01:48,719 --> 00:01:53,039
into the distributed nature of the

00:01:50,729 --> 00:01:54,899
component and then additional features

00:01:53,039 --> 00:01:57,390
that have been introduced over the last

00:01:54,899 --> 00:01:59,369
year and at the end we'll briefly talk

00:01:57,390 --> 00:02:01,619
about kind of how this is used within

00:01:59,369 --> 00:02:07,140
Bloomberg and what use cases this is

00:02:01,619 --> 00:02:09,899
really at that solving so relevance and

00:02:07,140 --> 00:02:12,900
analytics as I said Bloomberg and many

00:02:09,899 --> 00:02:15,480
other people really have this use of

00:02:12,900 --> 00:02:18,330
finding relevant data and then analyzing

00:02:15,480 --> 00:02:20,190
it and there are a lot of establish

00:02:18,330 --> 00:02:22,320
search engines such as elastic and solar

00:02:20,190 --> 00:02:25,470
and a lot of established analytics

00:02:22,320 --> 00:02:27,560
engine such as spark and Hadoop but they

00:02:25,470 --> 00:02:30,750
don't really play together that well

00:02:27,560 --> 00:02:33,150
there are solutions to take data out of

00:02:30,750 --> 00:02:35,640
solar after a query and then ingest that

00:02:33,150 --> 00:02:38,220
into sparker dupe to perform data

00:02:35,640 --> 00:02:40,380
analysis but really the benefits of

00:02:38,220 --> 00:02:43,620
these external analytics engines come

00:02:40,380 --> 00:02:45,090
when you are processing a vast amount of

00:02:43,620 --> 00:02:47,310
data so like hundreds of millions of

00:02:45,090 --> 00:02:48,840
records billions of records and taking

00:02:47,310 --> 00:02:53,280
all of that data out of search engines

00:02:48,840 --> 00:02:56,430
is pretty slow yet sparking I do have a

00:02:53,280 --> 00:02:58,860
very like a vast kind of universe of

00:02:56,430 --> 00:03:01,110
tools around them because Dana

00:02:58,860 --> 00:03:02,490
scientists have built them over the

00:03:01,110 --> 00:03:05,250
years because they needed them with

00:03:02,490 --> 00:03:09,360
these systems so they do solve a lot of

00:03:05,250 --> 00:03:12,600
problems solar analytics is trying to

00:03:09,360 --> 00:03:15,570
take a chunk of these problems and make

00:03:12,600 --> 00:03:17,730
a better solution so instead of having

00:03:15,570 --> 00:03:19,950
this external search in this search

00:03:17,730 --> 00:03:21,810
engine talked to this analytics engine

00:03:19,950 --> 00:03:24,060
and kind of having to manage both of

00:03:21,810 --> 00:03:26,790
them have an analytics engine reside

00:03:24,060 --> 00:03:29,730
within of the search engine which kind

00:03:26,790 --> 00:03:31,650
of fixes a couple of problems one taking

00:03:29,730 --> 00:03:33,000
all the data out of solar doesn't really

00:03:31,650 --> 00:03:36,150
need to do that because if you have your

00:03:33,000 --> 00:03:38,010
analytics within solar the analytics

00:03:36,150 --> 00:03:42,300
engine can read directly from the index

00:03:38,010 --> 00:03:44,940
which is obviously much faster and since

00:03:42,300 --> 00:03:47,220
no one really likes managing too many

00:03:44,940 --> 00:03:49,950
dependencies just relying on a search

00:03:47,220 --> 00:03:52,680
engine is much easier than managing a

00:03:49,950 --> 00:03:55,500
search engine and analytics engine and

00:03:52,680 --> 00:03:58,470
how they talk to each other kind of the

00:03:55,500 --> 00:04:00,420
last point is that solar's is live as

00:03:58,470 --> 00:04:03,209
the data ingested into it and therefore

00:04:00,420 --> 00:04:05,670
a lot of clients need analytics to be on

00:04:03,209 --> 00:04:08,880
this live data and if you're using an

00:04:05,670 --> 00:04:12,180
external analytics engine it's pretty I

00:04:08,880 --> 00:04:13,709
know it's pretty common to cache results

00:04:12,180 --> 00:04:16,169
from solar cache the results these

00:04:13,709 --> 00:04:18,180
analytics which causes your results to

00:04:16,169 --> 00:04:19,350
not be as live as they could be if you

00:04:18,180 --> 00:04:23,760
were getting your analytics straight

00:04:19,350 --> 00:04:25,229
from solar so let's bring it back a

00:04:23,760 --> 00:04:27,030
little bit and say it kind of asked the

00:04:25,229 --> 00:04:31,320
question why do we need to analytics and

00:04:27,030 --> 00:04:32,270
how did this guide the creation of the

00:04:31,320 --> 00:04:35,780
solar I know

00:04:32,270 --> 00:04:37,490
so kind of a warning I am a very big

00:04:35,780 --> 00:04:39,080
baseball fan I'm sure most feel don't

00:04:37,490 --> 00:04:40,910
know anything about baseball but all the

00:04:39,080 --> 00:04:42,919
examples will be in baseball and you

00:04:40,910 --> 00:04:45,259
don't need to know anything about than

00:04:42,919 --> 00:04:47,630
the fact that baseball data is very

00:04:45,259 --> 00:04:49,580
easily broken down into individual

00:04:47,630 --> 00:04:54,169
events of documents in the search

00:04:49,580 --> 00:04:55,639
engines and we'll be talking about the

00:04:54,169 --> 00:04:58,400
Astros who won their first World Series

00:04:55,639 --> 00:05:00,949
in 2017 so a lot of the examples are

00:04:58,400 --> 00:05:02,960
from that so let's say that we have a

00:05:00,949 --> 00:05:06,740
search engine that's full of baseball

00:05:02,960 --> 00:05:09,620
data and this is very convenient because

00:05:06,740 --> 00:05:14,210
we can search for results for certain

00:05:09,620 --> 00:05:15,680
team a certain year a player etc but

00:05:14,210 --> 00:05:17,990
however like when we get these results

00:05:15,680 --> 00:05:21,710
back it doesn't really tell us much

00:05:17,990 --> 00:05:24,020
because as most it's just a decisions no

00:05:21,710 --> 00:05:27,080
one piece of data doesn't give you

00:05:24,020 --> 00:05:29,720
insight into like the whole corpus of

00:05:27,080 --> 00:05:31,280
data so even the worst hitters will hit

00:05:29,720 --> 00:05:34,069
a homerun and even the best hitters will

00:05:31,280 --> 00:05:36,440
strike out in order to find meaning

00:05:34,069 --> 00:05:38,630
within this data we need to analyze it

00:05:36,440 --> 00:05:40,729
this means combining it a grenading it

00:05:38,630 --> 00:05:43,490
and we do this through analytical

00:05:40,729 --> 00:05:45,849
expressions you can see these throughout

00:05:43,490 --> 00:05:47,900
your life we use them every day probably

00:05:45,849 --> 00:05:51,650
but they've been doing this in baseball

00:05:47,900 --> 00:05:53,210
for over 150 years so we have statistics

00:05:51,650 --> 00:05:54,860
called on-base percentage and batting

00:05:53,210 --> 00:05:57,289
average which are probably the two most

00:05:54,860 --> 00:06:02,479
popular ways of expressing how good a

00:05:57,289 --> 00:06:05,569
player is batting and these are very old

00:06:02,479 --> 00:06:07,789
concepts that not very easily into

00:06:05,569 --> 00:06:13,300
analytical expresses a very new concept

00:06:07,789 --> 00:06:13,300
so analytical expressions are a very

00:06:13,750 --> 00:06:19,159
good way of mapping real-world problems

00:06:17,449 --> 00:06:21,560
into problems that can be solved with

00:06:19,159 --> 00:06:23,840
the analyst component so let's just get

00:06:21,560 --> 00:06:25,729
an example out there since we're using

00:06:23,840 --> 00:06:27,349
solar we need to first query for some

00:06:25,729 --> 00:06:29,690
results let's ask for some Astros

00:06:27,349 --> 00:06:33,620
results which gives us a list of players

00:06:29,690 --> 00:06:35,330
in their plate appearance back first

00:06:33,620 --> 00:06:36,740
we're trying to calculate on-base

00:06:35,330 --> 00:06:38,000
percentage as you can see on the right

00:06:36,740 --> 00:06:40,460
and we're going to build up an

00:06:38,000 --> 00:06:43,250
expression to calculate this so first we

00:06:40,460 --> 00:06:45,770
need to map data within each document

00:06:43,250 --> 00:06:46,130
together and Sylla looks as I said

00:06:45,770 --> 00:06:48,350
before

00:06:46,130 --> 00:06:52,130
a MapReduce framework much like sparking

00:06:48,350 --> 00:06:54,530
Hadoop so the first step of any alpha

00:06:52,130 --> 00:06:56,150
analytical expressions taking the fields

00:06:54,530 --> 00:06:59,570
from your solar index and mapping them

00:06:56,150 --> 00:07:01,130
together to combine data so for our

00:06:59,570 --> 00:07:03,080
addition function we're trying to add

00:07:01,130 --> 00:07:05,990
all the times that a player got on base

00:07:03,080 --> 00:07:08,090
per plate appearance once the mapping

00:07:05,990 --> 00:07:10,820
has been done we need to then reduce the

00:07:08,090 --> 00:07:14,510
data because that's where the analytical

00:07:10,820 --> 00:07:18,170
magic happens we reduce data across all

00:07:14,510 --> 00:07:20,000
the documents to find the aggregation so

00:07:18,170 --> 00:07:22,010
we take the count of plate appearances

00:07:20,000 --> 00:07:23,870
sum up all the times that they've been

00:07:22,010 --> 00:07:26,960
on base and then we finally do our last

00:07:23,870 --> 00:07:28,820
mapping to map the results of these

00:07:26,960 --> 00:07:31,310
reductions together to get an overall

00:07:28,820 --> 00:07:36,350
result of on-base percentage which is

00:07:31,310 --> 00:07:37,940
0.6 hundred so as you can see this

00:07:36,350 --> 00:07:39,800
mapping reducing framework has kind of

00:07:37,940 --> 00:07:42,440
built on the same principles as Dupin

00:07:39,800 --> 00:07:44,240
SPARC and allows for as much

00:07:42,440 --> 00:07:47,720
parallelization as possible for these

00:07:44,240 --> 00:07:49,460
things so mapping can be done say per

00:07:47,720 --> 00:07:52,790
shard because it's done on a per

00:07:49,460 --> 00:07:55,250
document basis the reduction can be done

00:07:52,790 --> 00:07:58,550
using groupings such as spark and a dupe

00:07:55,250 --> 00:07:58,850
and kind of aggregated together at the

00:07:58,550 --> 00:08:02,780
end

00:07:58,850 --> 00:08:05,210
so this MapReduce framework really makes

00:08:02,780 --> 00:08:08,210
the component as extensible as possible

00:08:05,210 --> 00:08:09,920
to further parallelization so I've

00:08:08,210 --> 00:08:13,370
showed you building up the on-base

00:08:09,920 --> 00:08:15,830
percentage animal expression now I'll

00:08:13,370 --> 00:08:17,480
show you average as well so all these

00:08:15,830 --> 00:08:19,850
integral expressions start with

00:08:17,480 --> 00:08:24,620
constants and fields which are from your

00:08:19,850 --> 00:08:26,090
index from these fields and constants

00:08:24,620 --> 00:08:29,060
you can map them together on a per

00:08:26,090 --> 00:08:31,130
document basis and then reduce those

00:08:29,060 --> 00:08:34,160
values together and after the reduction

00:08:31,130 --> 00:08:37,280
has been done across all documents map

00:08:34,160 --> 00:08:41,570
the results of those to get your overall

00:08:37,280 --> 00:08:43,430
values so as you can see we can

00:08:41,570 --> 00:08:45,860
calculate analytics over entire result

00:08:43,430 --> 00:08:48,650
sets but how often do people do that

00:08:45,860 --> 00:08:51,410
very rarely because the thing you want

00:08:48,650 --> 00:08:54,440
to do with these values is compare them

00:08:51,410 --> 00:08:57,620
I don't really care the batting average

00:08:54,440 --> 00:08:59,059
of everyone in 2018 I care about the

00:08:57,620 --> 00:09:01,639
batting average

00:08:59,059 --> 00:09:04,129
of each player in 2018 so we need to

00:09:01,639 --> 00:09:06,829
break up the data calculate results

00:09:04,129 --> 00:09:07,579
right based on those groupings and then

00:09:06,829 --> 00:09:09,589
return them

00:09:07,579 --> 00:09:12,079
so in solar we how I already have this

00:09:09,589 --> 00:09:15,649
idea facets using the fast component and

00:09:12,079 --> 00:09:18,289
so we can extend that into the analytics

00:09:15,649 --> 00:09:20,509
component to break up the data and

00:09:18,289 --> 00:09:22,579
baseball you can I think of groupings

00:09:20,509 --> 00:09:25,249
such as group by to see which players

00:09:22,579 --> 00:09:27,799
are the best and worst which years in a

00:09:25,249 --> 00:09:30,169
player's career were they good which

00:09:27,799 --> 00:09:32,179
were years were they bad which teams are

00:09:30,169 --> 00:09:34,489
good which teams are bad

00:09:32,179 --> 00:09:35,929
basically as many ways you can break up

00:09:34,489 --> 00:09:38,359
the data the analytics component

00:09:35,929 --> 00:09:40,149
lets you do that so we'll go through the

00:09:38,359 --> 00:09:43,339
different types of fastest available

00:09:40,149 --> 00:09:45,919
first we have value facets which are an

00:09:43,339 --> 00:09:48,499
extension of solar field facets which

00:09:45,919 --> 00:09:50,539
let you group the data by the value of a

00:09:48,499 --> 00:09:53,599
field or a mapping expression will go

00:09:50,539 --> 00:09:54,769
further into the value facets later but

00:09:53,599 --> 00:09:57,049
here on the right you can see that

00:09:54,769 --> 00:09:58,639
players have been grouped the data has

00:09:57,049 --> 00:10:00,709
been grouped by players so that we can

00:09:58,639 --> 00:10:03,559
calculate analytics on a per player

00:10:00,709 --> 00:10:04,999
basis next we have range facets which

00:10:03,559 --> 00:10:08,179
work exactly the same as they do in the

00:10:04,999 --> 00:10:10,569
FAFSA component which you can break it

00:10:08,179 --> 00:10:13,609
down by dates such as May and June or

00:10:10,569 --> 00:10:16,189
value such as innings 1 through 3 or

00:10:13,609 --> 00:10:18,289
innings 4 through 6 as you can see there

00:10:16,189 --> 00:10:20,299
we have data from inning 7 and that's

00:10:18,289 --> 00:10:23,329
not included because it wasn't in the

00:10:20,299 --> 00:10:28,249
defined set of ranges provided to the in

00:10:23,329 --> 00:10:30,319
the request solar also allows for query

00:10:28,249 --> 00:10:32,869
facets so the Analects component also

00:10:30,319 --> 00:10:35,089
provides query facets so you can ask for

00:10:32,869 --> 00:10:36,709
cold games hot games if you have an

00:10:35,089 --> 00:10:39,379
additional query to send a solar to

00:10:36,709 --> 00:10:41,179
break of the data by so I mentioned

00:10:39,379 --> 00:10:43,059
value facets earlier and how they're

00:10:41,179 --> 00:10:45,469
kind of an extension of field facets but

00:10:43,059 --> 00:10:49,309
using some of that analytics magic in

00:10:45,469 --> 00:10:50,899
their value fastest let you give a

00:10:49,309 --> 00:10:52,579
mapping expression and when we were

00:10:50,899 --> 00:10:54,499
breaking down these expressions earlier

00:10:52,579 --> 00:10:56,209
that's any expression that doesn't have

00:10:54,499 --> 00:10:59,239
a reduction function in it because you

00:10:56,209 --> 00:11:01,489
can't really break down data by the

00:10:59,239 --> 00:11:05,379
reduced value it doesn't make any sense

00:11:01,489 --> 00:11:08,419
and so for an example if we didn't have

00:11:05,379 --> 00:11:11,089
what stadium played appearance at

00:11:08,419 --> 00:11:12,800
curtain we could say that I make an

00:11:11,089 --> 00:11:16,700
expression that says if that play

00:11:12,800 --> 00:11:19,610
at home at their home stadium returned

00:11:16,700 --> 00:11:21,589
that team or else returned the opposing

00:11:19,610 --> 00:11:24,980
team this kind of gives you that data

00:11:21,589 --> 00:11:27,260
without having to store it Valley facets

00:11:24,980 --> 00:11:30,740
also allow for more complex sorting than

00:11:27,260 --> 00:11:33,350
a given in the FASTA component and solar

00:11:30,740 --> 00:11:36,890
so you can sort by multiply multiple

00:11:33,350 --> 00:11:39,910
criteria be it expression or facet value

00:11:36,890 --> 00:11:42,500
and then set a limited offset of course

00:11:39,910 --> 00:11:45,130
so this is just an example of how to

00:11:42,500 --> 00:11:48,500
express it in the analytics framework

00:11:45,130 --> 00:11:49,910
and this is what I explained earlier I'm

00:11:48,500 --> 00:11:53,660
not gonna get too far into it

00:11:49,910 --> 00:11:55,370
pivot facets are what kind of the same

00:11:53,660 --> 00:11:59,230
ideas they are in the fast component

00:11:55,370 --> 00:12:01,790
allowing drill down of different

00:11:59,230 --> 00:12:04,640
fascinating drill down over multiple

00:12:01,790 --> 00:12:07,640
mapping expressions and so it's like the

00:12:04,640 --> 00:12:09,290
solar and the solar fasting component

00:12:07,640 --> 00:12:11,149
pivot facets but instead of fields

00:12:09,290 --> 00:12:14,360
mapping expressions much like valley

00:12:11,149 --> 00:12:16,370
facets complex sorting is enabled for

00:12:14,360 --> 00:12:18,410
each pivot individual individually

00:12:16,370 --> 00:12:20,839
unlike the fasten component and then

00:12:18,410 --> 00:12:23,839
results are calculated at each pivot

00:12:20,839 --> 00:12:26,120
level let's go over an example so say

00:12:23,839 --> 00:12:32,750
that we wanted to calculate how each

00:12:26,120 --> 00:12:34,940
team did a compensable league so we can

00:12:32,750 --> 00:12:37,970
see the Astra get the results from the

00:12:34,940 --> 00:12:40,070
Astros calculate their analytics then

00:12:37,970 --> 00:12:42,620
see how they did against the Yankee's

00:12:40,070 --> 00:12:45,470
against the Angels see how the Yankees

00:12:42,620 --> 00:12:48,410
did against the Astros the Red Sox and

00:12:45,470 --> 00:12:49,640
the Blue Jays and since the Dodgers

00:12:48,410 --> 00:12:54,680
haven't played anyone yet they won't

00:12:49,640 --> 00:12:56,540
have any children pivot to make this is

00:12:54,680 --> 00:13:00,199
just how you would express it in the

00:12:56,540 --> 00:13:02,329
analytics language so how does this

00:13:00,199 --> 00:13:06,680
mapping and reducing framework work with

00:13:02,329 --> 00:13:08,240
facets here required for the same set of

00:13:06,680 --> 00:13:11,260
assets results and we're calculating

00:13:08,240 --> 00:13:13,880
on-base percentage again we first map

00:13:11,260 --> 00:13:16,130
the documents with it the values within

00:13:13,880 --> 00:13:17,959
each document and this is unchanged from

00:13:16,130 --> 00:13:20,750
before since we haven't started to

00:13:17,959 --> 00:13:23,120
reduce so once we reduce instead of

00:13:20,750 --> 00:13:26,600
having one reduction at the end we need

00:13:23,120 --> 00:13:30,230
to reduce for each value of

00:13:26,600 --> 00:13:32,300
the facet that we're calculating so

00:13:30,230 --> 00:13:35,720
we're gonna in this example break up the

00:13:32,300 --> 00:13:38,060
data for each player once the reduction

00:13:35,720 --> 00:13:40,370
has been done we can then map the result

00:13:38,060 --> 00:13:42,560
from each of these facet values to get

00:13:40,370 --> 00:13:45,980
an on-base percentage for each of the

00:13:42,560 --> 00:13:47,690
players pretty straightforward so I've

00:13:45,980 --> 00:13:48,650
given you a lot of baseball examples and

00:13:47,690 --> 00:13:50,630
I understand it must be L don't know

00:13:48,650 --> 00:13:52,700
baseball here's an example of how we use

00:13:50,630 --> 00:13:54,680
this app Bloomberg this is the screen

00:13:52,700 --> 00:13:56,780
merchant mergers and acquisitions which

00:13:54,680 --> 00:13:59,390
basically tells you about companies

00:13:56,780 --> 00:14:01,070
buying other companies and here all the

00:13:59,390 --> 00:14:02,780
data is provided by the solar analytics

00:14:01,070 --> 00:14:05,120
component and you can see here we have

00:14:02,780 --> 00:14:07,190
analytics expressions just the count of

00:14:05,120 --> 00:14:11,030
deals the minimum maximum deals eyes

00:14:07,190 --> 00:14:13,820
median and then volume deal count etc

00:14:11,030 --> 00:14:16,280
and then facets that this data is broken

00:14:13,820 --> 00:14:19,730
up by such as payment type being cash

00:14:16,280 --> 00:14:23,300
doc equity and whatever target multiples

00:14:19,730 --> 00:14:25,670
is I'm not really sure so as you can see

00:14:23,300 --> 00:14:28,910
this real world applications of this

00:14:25,670 --> 00:14:31,420
another component so distributed

00:14:28,910 --> 00:14:34,730
analytics why is it needed

00:14:31,420 --> 00:14:37,430
so in solar collections can have

00:14:34,730 --> 00:14:40,240
billions of documents however shards are

00:14:37,430 --> 00:14:45,490
limited to do two billion documents and

00:14:40,240 --> 00:14:49,460
since a lot of users need to be able to

00:14:45,490 --> 00:14:52,660
analyze lots of data that two billion

00:14:49,460 --> 00:14:57,020
documents per shard really limits

00:14:52,660 --> 00:14:59,210
users and so in order to solve this we

00:14:57,020 --> 00:15:03,230
need to have data spread across multiple

00:14:59,210 --> 00:15:06,590
shards and this causes some problems

00:15:03,230 --> 00:15:09,230
when try to compute analytics what is

00:15:06,590 --> 00:15:11,150
that issue so obviously let's go back to

00:15:09,230 --> 00:15:14,120
our MapReduce framework mapping is

00:15:11,150 --> 00:15:16,040
really easy since you can just map on a

00:15:14,120 --> 00:15:21,530
per document basis and documents are on

00:15:16,040 --> 00:15:24,230
one shard each so mapping isn't affected

00:15:21,530 --> 00:15:26,990
by this shard egg however when you start

00:15:24,230 --> 00:15:30,890
to reduce the data down you'll get one

00:15:26,990 --> 00:15:32,840
set of reductions per shard because the

00:15:30,890 --> 00:15:34,460
data you can't really when you're doing

00:15:32,840 --> 00:15:39,710
this reduction you can't talk across

00:15:34,460 --> 00:15:40,490
machines very easily so what do we do

00:15:39,710 --> 00:15:42,260
with these three

00:15:40,490 --> 00:15:46,310
that's a reduction at the end to get one

00:15:42,260 --> 00:15:48,020
overall set of reductions for some

00:15:46,310 --> 00:15:49,970
things is a very easy task so

00:15:48,020 --> 00:15:52,550
associative reduction functions such as

00:15:49,970 --> 00:15:54,080
sum count min and Max this is trivial

00:15:52,550 --> 00:15:55,880
because you can take the sum of each

00:15:54,080 --> 00:15:58,640
chart and then take the sum of all those

00:15:55,880 --> 00:16:00,380
results and you get the overall sum but

00:15:58,640 --> 00:16:02,270
for non-associative reduction functions

00:16:00,380 --> 00:16:03,980
is pretty hard because you require all

00:16:02,270 --> 00:16:06,310
the data to be in one place such as

00:16:03,980 --> 00:16:08,630
percentile and median and unique

00:16:06,310 --> 00:16:10,550
percentile median require a sorted list

00:16:08,630 --> 00:16:12,620
of all the data and unique requires a

00:16:10,550 --> 00:16:14,839
unique set of all the data unless you

00:16:12,620 --> 00:16:19,610
want to if you want accurate information

00:16:14,839 --> 00:16:21,050
so the solution is since every reduction

00:16:19,610 --> 00:16:22,760
function needs a different set of data

00:16:21,050 --> 00:16:25,279
sent across shards the reduction

00:16:22,760 --> 00:16:27,649
functions are in charge of exporting and

00:16:25,279 --> 00:16:31,790
merging that data in the original node

00:16:27,649 --> 00:16:34,160
and so for men the min function sins all

00:16:31,790 --> 00:16:36,260
the minimums from each shard and can

00:16:34,160 --> 00:16:38,690
define the minimum of all those values

00:16:36,260 --> 00:16:40,070
at the end unique since the unique set

00:16:38,690 --> 00:16:43,040
from each shard creating an overall

00:16:40,070 --> 00:16:45,430
unique set medians and sorted lists

00:16:43,040 --> 00:16:51,050
creates an overall sorted list and some

00:16:45,430 --> 00:16:53,270
you understand so as you can see each

00:16:51,050 --> 00:16:55,520
function here is in charge of sinning

00:16:53,270 --> 00:16:57,470
and merging its own data making it very

00:16:55,520 --> 00:17:00,800
easy to create new functions in this

00:16:57,470 --> 00:17:03,230
framework so let's just briefly go over

00:17:00,800 --> 00:17:05,720
how a distributed request is sent inside

00:17:03,230 --> 00:17:09,290
of a solar cloud the request is sent to

00:17:05,720 --> 00:17:11,209
an analog component which finds one shot

00:17:09,290 --> 00:17:14,000
one replica of each shard to send the

00:17:11,209 --> 00:17:15,589
request to the request isn't fair all

00:17:14,000 --> 00:17:18,709
the mapping is done on each of those

00:17:15,589 --> 00:17:21,410
shards and the initial set of reductions

00:17:18,709 --> 00:17:23,390
is calculated that reduction data is

00:17:21,410 --> 00:17:26,740
sent back to the originating shard which

00:17:23,390 --> 00:17:29,210
is then merged and then the facets are

00:17:26,740 --> 00:17:32,780
sorted and limited and then the response

00:17:29,210 --> 00:17:34,970
is sent out so the takeaways from this

00:17:32,780 --> 00:17:37,100
is that distributed analytics does let

00:17:34,970 --> 00:17:39,679
you speed up aggregations a lot because

00:17:37,100 --> 00:17:41,210
that mapping phase can be paralyzed

00:17:39,679 --> 00:17:43,730
across the shard and done at the same

00:17:41,210 --> 00:17:45,260
time and for associative reductions

00:17:43,730 --> 00:17:48,679
which are really easy to distribute

00:17:45,260 --> 00:17:51,320
across shards you can really speed up

00:17:48,679 --> 00:17:53,180
your aggregation by as many shards as

00:17:51,320 --> 00:17:55,700
you have in your cluster

00:17:53,180 --> 00:17:58,580
meaning that if you have say 12 shards

00:17:55,700 --> 00:18:00,050
it will perform roughly 12 times as fast

00:17:58,580 --> 00:18:03,170
it's just having all of your data in one

00:18:00,050 --> 00:18:04,940
shard and since we need we need the

00:18:03,170 --> 00:18:06,260
request interface to be the same across

00:18:04,940 --> 00:18:09,380
single charted and multi search

00:18:06,260 --> 00:18:10,850
collections so that no features are lost

00:18:09,380 --> 00:18:14,690
in between and the users don't really

00:18:10,850 --> 00:18:16,000
care where their data is so let's talk

00:18:14,690 --> 00:18:18,920
about some performance considerations

00:18:16,000 --> 00:18:20,570
but first let's talk about how a request

00:18:18,920 --> 00:18:24,290
is processed in the analytics component

00:18:20,570 --> 00:18:28,220
and basically how we can speed things up

00:18:24,290 --> 00:18:29,840
so first obviously the query that sent

00:18:28,220 --> 00:18:31,700
to the Analects component is executed

00:18:29,840 --> 00:18:34,430
and a result set to calculate the

00:18:31,700 --> 00:18:38,600
analytics over is found for each of

00:18:34,430 --> 00:18:39,980
those documents it's looped the analyst

00:18:38,600 --> 00:18:42,830
component loops over each of those

00:18:39,980 --> 00:18:47,720
documents reading it from the document

00:18:42,830 --> 00:18:50,570
from the index and filling the reduction

00:18:47,720 --> 00:18:53,810
data for expressions and pivot and value

00:18:50,570 --> 00:18:56,510
facets since query and range facets are

00:18:53,810 --> 00:18:57,710
done by sending extra queries to solar

00:18:56,510 --> 00:18:59,900
those have to be done in a different

00:18:57,710 --> 00:19:01,910
step which is a new step in so the

00:18:59,900 --> 00:19:03,710
queries are sent the queries are

00:19:01,910 --> 00:19:07,190
executed which kind of returns to sub1

00:19:03,710 --> 00:19:10,400
and populates additional reduction data

00:19:07,190 --> 00:19:13,760
for those facet values once all the

00:19:10,400 --> 00:19:17,600
reduction data is found it's sent back

00:19:13,760 --> 00:19:20,840
to the originating node and the overall

00:19:17,600 --> 00:19:22,430
the results are calculated the facet

00:19:20,840 --> 00:19:26,150
results are filtered and the results are

00:19:22,430 --> 00:19:28,550
sent back to the user so what things can

00:19:26,150 --> 00:19:31,070
we do to make this faster so a lot of

00:19:28,550 --> 00:19:32,990
the times and these request you'll sit

00:19:31,070 --> 00:19:35,750
you'll send some of the same sub

00:19:32,990 --> 00:19:37,520
expressions multiple times so in these

00:19:35,750 --> 00:19:39,920
set of four expressions you can see

00:19:37,520 --> 00:19:42,340
multiple things that are referenced at

00:19:39,920 --> 00:19:46,400
more than once such as count PA is

00:19:42,340 --> 00:19:48,860
reference twice add homerun and BB walks

00:19:46,400 --> 00:19:51,080
is reference twice and homerun is

00:19:48,860 --> 00:19:54,260
reference three times instead of

00:19:51,080 --> 00:19:57,470
calculating all these individually we

00:19:54,260 --> 00:19:59,800
can I basically share overlapping

00:19:57,470 --> 00:20:02,600
expressions so that we only had to read

00:19:59,800 --> 00:20:05,000
plate appearances once from the index we

00:20:02,600 --> 00:20:07,190
only have to read home runs once we only

00:20:05,000 --> 00:20:10,220
had to add home runs and locks once

00:20:07,190 --> 00:20:12,740
and this really speed things up the

00:20:10,220 --> 00:20:16,160
speeds things up for large analyst

00:20:12,740 --> 00:20:18,440
requests um let's go back to the

00:20:16,160 --> 00:20:19,990
distributed reduction solution and see

00:20:18,440 --> 00:20:23,360
how we can speed things up there since

00:20:19,990 --> 00:20:25,010
as you can see if you're trying to

00:20:23,360 --> 00:20:27,410
calculate the median over a billion

00:20:25,010 --> 00:20:29,120
values that can be pretty slow because

00:20:27,410 --> 00:20:30,890
you have to send billions of values

00:20:29,120 --> 00:20:35,420
across the network to the originating

00:20:30,890 --> 00:20:36,800
node um so what we can do is not reduce

00:20:35,420 --> 00:20:38,120
that amount of data because to get

00:20:36,800 --> 00:20:39,830
accurate results you have to send all

00:20:38,120 --> 00:20:41,690
the data back but we can make sure that

00:20:39,830 --> 00:20:43,400
we're not sending the same data multiple

00:20:41,690 --> 00:20:45,650
times for example if we're trying to

00:20:43,400 --> 00:20:47,450
calculate the median of inning the 20th

00:20:45,650 --> 00:20:48,410
percentile inning in the 60th percentile

00:20:47,450 --> 00:20:51,050
inning

00:20:48,410 --> 00:20:53,000
all of those reduction functions require

00:20:51,050 --> 00:20:55,730
a sorted list of innings so instead of

00:20:53,000 --> 00:20:58,850
sending that sort of list three times

00:20:55,730 --> 00:21:01,610
once for each function we can have these

00:20:58,850 --> 00:21:04,910
functions share the data and we do this

00:21:01,610 --> 00:21:07,250
through reduction data and so instead of

00:21:04,910 --> 00:21:09,830
having reduction functions merge and

00:21:07,250 --> 00:21:12,460
export data we have reduction functions

00:21:09,830 --> 00:21:16,040
Reserve Reduction data which manage

00:21:12,460 --> 00:21:18,920
collection exporting and merging so here

00:21:16,040 --> 00:21:21,950
we can see that median and percentile

00:21:18,920 --> 00:21:24,590
would reserve a sorted list of their

00:21:21,950 --> 00:21:27,470
expression and unique set with a unique

00:21:24,590 --> 00:21:29,450
would reserve a unique set and this can

00:21:27,470 --> 00:21:32,900
really give performance improvements for

00:21:29,450 --> 00:21:35,360
non-charged collections so let's go to

00:21:32,900 --> 00:21:37,040
the let's give an example of this and

00:21:35,360 --> 00:21:39,230
say that we want to find the median of

00:21:37,040 --> 00:21:41,150
inning twenty percent of inning sixty

00:21:39,230 --> 00:21:43,250
percent of inning some of home run and

00:21:41,150 --> 00:21:45,190
mean of home run what reduction data

00:21:43,250 --> 00:21:47,660
would these reduction functions require

00:21:45,190 --> 00:21:49,670
and as you can see they only require

00:21:47,660 --> 00:21:51,530
three sets of reduction data that's

00:21:49,670 --> 00:21:54,620
count of home runs sum of home run and

00:21:51,530 --> 00:21:57,260
sort of list of inning once this has

00:21:54,620 --> 00:21:58,970
been found out we can just ask each

00:21:57,260 --> 00:22:02,900
shard to collect these three bits of

00:21:58,970 --> 00:22:04,280
data and then merge and export export

00:22:02,900 --> 00:22:07,100
and then merge that in the originating

00:22:04,280 --> 00:22:09,050
node and once that's happened we each

00:22:07,100 --> 00:22:11,240
production function can take the data

00:22:09,050 --> 00:22:16,640
from the reduction data it's reserved

00:22:11,240 --> 00:22:18,980
and calculate its value instantly so as

00:22:16,640 --> 00:22:21,260
you can see here all five of these

00:22:18,980 --> 00:22:23,900
functions only require three

00:22:21,260 --> 00:22:26,900
it's a reduction data which vastly

00:22:23,900 --> 00:22:28,730
improves the performance of sending data

00:22:26,900 --> 00:22:32,900
across the notes especially with the

00:22:28,730 --> 00:22:34,970
slow Network cool some other performance

00:22:32,900 --> 00:22:38,480
various performance considerations think

00:22:34,970 --> 00:22:41,660
of our adding new expressions don't

00:22:38,480 --> 00:22:45,290
necessarily increase the amount of time

00:22:41,660 --> 00:22:49,100
the query takes to calculate and I would

00:22:45,290 --> 00:22:50,570
very much suggest that people add the

00:22:49,100 --> 00:22:52,430
things that they want to be calculated

00:22:50,570 --> 00:22:55,360
and see if it affects performance

00:22:52,430 --> 00:22:59,300
because the way that the analog

00:22:55,360 --> 00:23:00,590
component kind of solves the overlapping

00:22:59,300 --> 00:23:02,810
expressions and the sharing of

00:23:00,590 --> 00:23:06,020
production data can really mitigate a

00:23:02,810 --> 00:23:08,030
lot of fears of calculating the

00:23:06,020 --> 00:23:10,790
performance overhead of calculating

00:23:08,030 --> 00:23:13,280
expressions and it should also be noted

00:23:10,790 --> 00:23:15,290
that the non-associative reductions such

00:23:13,280 --> 00:23:17,450
as median and unique in percentile

00:23:15,290 --> 00:23:19,730
require a significant amount of memory

00:23:17,450 --> 00:23:21,680
for large results at sizes so obviously

00:23:19,730 --> 00:23:23,660
if you're trying to compute the median

00:23:21,680 --> 00:23:26,690
of a billion values starting this

00:23:23,660 --> 00:23:31,640
billion values and memory to is a hefty

00:23:26,690 --> 00:23:33,410
hefty charge and then all fields used in

00:23:31,640 --> 00:23:35,450
these expressions must have dock values

00:23:33,410 --> 00:23:36,770
enabled that's just kind of a caveat

00:23:35,450 --> 00:23:39,410
because that's how it reads from the

00:23:36,770 --> 00:23:41,330
index and if you use the old analytics

00:23:39,410 --> 00:23:43,280
component the new one that has a much

00:23:41,330 --> 00:23:45,500
lower memory consumption for high

00:23:43,280 --> 00:23:49,100
cardinality facets so facets with

00:23:45,500 --> 00:23:50,450
hundreds of thousands of values so some

00:23:49,100 --> 00:23:51,620
additional features I'll just breathe

00:23:50,450 --> 00:23:54,530
through these have been added in the

00:23:51,620 --> 00:23:58,180
last year so expressions over multi

00:23:54,530 --> 00:24:02,720
valued fields are now supported by the

00:23:58,180 --> 00:24:04,580
component so for example expressions

00:24:02,720 --> 00:24:06,170
functions that used to take in a single

00:24:04,580 --> 00:24:10,100
value and return a single value such as

00:24:06,170 --> 00:24:12,320
log in negate now take in multiple field

00:24:10,100 --> 00:24:15,430
with multiple values and return multiple

00:24:12,320 --> 00:24:18,140
values for each value in that input and

00:24:15,430 --> 00:24:20,930
you can see how the mapping works it's

00:24:18,140 --> 00:24:22,940
pretty straightforward functions have

00:24:20,930 --> 00:24:24,500
taken a single VAT two single valued

00:24:22,940 --> 00:24:26,930
parameters and return a single value are

00:24:24,500 --> 00:24:30,110
a little harder to map this multi valued

00:24:26,930 --> 00:24:32,990
expression world but it works pretty

00:24:30,110 --> 00:24:34,460
simply so you can take a single value in

00:24:32,990 --> 00:24:35,149
the first parameter and the multi value

00:24:34,460 --> 00:24:38,269
in the second program

00:24:35,149 --> 00:24:42,379
it returns a multi multiple values

00:24:38,269 --> 00:24:44,299
one for each mapping mapping the first

00:24:42,379 --> 00:24:46,700
parameter which each with each value of

00:24:44,299 --> 00:24:48,919
the second parameter and this works the

00:24:46,700 --> 00:24:51,469
other way around taking a multiple for a

00:24:48,919 --> 00:24:52,909
multi valued parameter in the first slot

00:24:51,469 --> 00:24:57,349
and then a single value parameter in the

00:24:52,909 --> 00:24:58,849
second these functions also can take in

00:24:57,349 --> 00:25:01,820
a variable length parameter so just

00:24:58,849 --> 00:25:04,279
concatenate add and multiply on this

00:25:01,820 --> 00:25:06,679
very easy to see work in a multi valued

00:25:04,279 --> 00:25:08,539
expression world by just taking in one

00:25:06,679 --> 00:25:10,580
multi valued parameter and returning

00:25:08,539 --> 00:25:14,839
that single value so instead of taking

00:25:10,580 --> 00:25:16,669
in like add a one and two just say add a

00:25:14,839 --> 00:25:20,839
value that contains one in two and it

00:25:16,669 --> 00:25:22,820
returns that's same exact value we have

00:25:20,839 --> 00:25:25,099
new supported mapping functions such as

00:25:22,820 --> 00:25:28,149
logical ones comparison and conditional

00:25:25,099 --> 00:25:32,149
and you can you've seen these in the

00:25:28,149 --> 00:25:33,889
examples I've given previously so

00:25:32,149 --> 00:25:36,759
variable functions are a new feature

00:25:33,889 --> 00:25:39,589
enabled too and so they allow you to

00:25:36,759 --> 00:25:42,799
kind of put business logic into your

00:25:39,589 --> 00:25:44,839
request and not have to write out the

00:25:42,799 --> 00:25:47,149
same thing over and over again since we

00:25:44,839 --> 00:25:51,229
know copy and pasting can lead to lots

00:25:47,149 --> 00:25:53,479
of errors so using variable functions

00:25:51,229 --> 00:25:56,059
you give a variable name a function name

00:25:53,479 --> 00:25:59,539
with the variables it takes in and then

00:25:56,059 --> 00:26:02,019
it uses and then an expression that uses

00:25:59,539 --> 00:26:04,219
those parameters so if mean wasn't

00:26:02,019 --> 00:26:06,139
enabled in the Analects component you

00:26:04,219 --> 00:26:08,719
could say mean of a is the division of

00:26:06,139 --> 00:26:10,849
some of they encountered a and then use

00:26:08,719 --> 00:26:14,119
me as if it was a built-in function and

00:26:10,849 --> 00:26:17,119
solar it also accepts variable length

00:26:14,119 --> 00:26:19,369
parameters since analog functions also

00:26:17,119 --> 00:26:22,460
take in variable links parameters so if

00:26:19,369 --> 00:26:25,460
you put in CSV of one and two it would

00:26:22,460 --> 00:26:28,009
expand out to concatenates can cats set

00:26:25,460 --> 00:26:33,889
of comma with one and two which results

00:26:28,009 --> 00:26:36,589
in one comma two we so in the first

00:26:33,889 --> 00:26:40,339
example I gave it wrapped a with sum of

00:26:36,589 --> 00:26:42,549
a and count today not all if you were

00:26:40,339 --> 00:26:46,279
given a variable length parameter

00:26:42,549 --> 00:26:47,960
wrapping it could not work because if

00:26:46,279 --> 00:26:49,390
you wanted to like fill missing each

00:26:47,960 --> 00:26:52,190
value within

00:26:49,390 --> 00:26:54,440
just a feeling filled missing a with na

00:26:52,190 --> 00:26:56,540
doesn't work because that would film

00:26:54,440 --> 00:26:58,640
missing just takes two parameters so we

00:26:56,540 --> 00:27:00,800
have a framework of kind of lambda

00:26:58,640 --> 00:27:04,340
functions that say for each value of a

00:27:00,800 --> 00:27:07,010
fill missing that value within a and so

00:27:04,340 --> 00:27:09,410
if you put CSV of 1 and null it would

00:27:07,010 --> 00:27:11,180
map out to can cats F of fill missing

00:27:09,410 --> 00:27:14,570
one and a and then fill missing in

00:27:11,180 --> 00:27:15,980
Noland a yeah if you have any additional

00:27:14,570 --> 00:27:18,560
questions about this I know it's a

00:27:15,980 --> 00:27:20,330
little confusing there's much more in

00:27:18,560 --> 00:27:24,050
the solar reference guide about it and

00:27:20,330 --> 00:27:26,270
you could ask me after the talk so on to

00:27:24,050 --> 00:27:28,970
use cases when does the solar I know its

00:27:26,270 --> 00:27:31,550
component work for you and as you can is

00:27:28,970 --> 00:27:34,250
you've probably seen here solar Analects

00:27:31,550 --> 00:27:37,010
only provides first-order analytics and

00:27:34,250 --> 00:27:39,580
first-order analytics are analytics that

00:27:37,010 --> 00:27:41,930
are based off of the underlying data set

00:27:39,580 --> 00:27:43,580
second-order analytics are analytics

00:27:41,930 --> 00:27:45,580
that rely on that underlying data set

00:27:43,580 --> 00:27:49,700
and first-order analytics and

00:27:45,580 --> 00:27:51,140
third-order etc so if you right now the

00:27:49,700 --> 00:27:53,750
analytic component only supports

00:27:51,140 --> 00:27:55,820
first-order analytics there are plans to

00:27:53,750 --> 00:27:59,930
support in order analytics in the future

00:27:55,820 --> 00:28:01,370
but for now if you're use case most use

00:27:59,930 --> 00:28:04,430
cases really only need first-order

00:28:01,370 --> 00:28:07,280
analytics so this shouldn't be a problem

00:28:04,430 --> 00:28:10,430
but it's something to keep in mind so

00:28:07,280 --> 00:28:12,620
how do we use this in Bloomberg I start

00:28:10,430 --> 00:28:14,120
this off by saying that solar Analects

00:28:12,620 --> 00:28:15,770
doesn't fit all use cases that what I'm

00:28:14,120 --> 00:28:17,660
trying to say Bloomberg still uses a

00:28:15,770 --> 00:28:19,730
dupe and spark heavily and there are

00:28:17,660 --> 00:28:21,380
several teams that use other analytic

00:28:19,730 --> 00:28:24,190
internal analytics engines such as

00:28:21,380 --> 00:28:26,780
streaming expressions and JSON facets

00:28:24,190 --> 00:28:29,000
solar analytics is used heavily within

00:28:26,780 --> 00:28:30,590
our team sun as a member of the search

00:28:29,000 --> 00:28:33,860
infrastructure team with hundreds of

00:28:30,590 --> 00:28:35,630
clients within the company we know what

00:28:33,860 --> 00:28:37,520
teams are using what features and the

00:28:35,630 --> 00:28:39,830
analyst component is used heavily within

00:28:37,520 --> 00:28:42,140
Bloomberg and it has some place many

00:28:39,830 --> 00:28:44,420
high priced external solutions and

00:28:42,140 --> 00:28:47,780
custom in-house code that we can now

00:28:44,420 --> 00:28:50,450
deprecated the use cases ranged a lot

00:28:47,780 --> 00:28:53,720
like from analyzing a hundreds of values

00:28:50,450 --> 00:28:55,880
to hundreds of millions of results using

00:28:53,720 --> 00:28:58,070
one shard versus dozens of shards and

00:28:55,880 --> 00:28:59,570
then not using any facets to using

00:28:58,070 --> 00:29:01,730
facets with hundreds of thousands of

00:28:59,570 --> 00:29:02,960
values so this has been test very

00:29:01,730 --> 00:29:06,770
heavily using a

00:29:02,960 --> 00:29:08,240
I eighty of different use cases there's

00:29:06,770 --> 00:29:10,669
a lot of future work that we want to

00:29:08,240 --> 00:29:12,440
enable such as as I mentioned early

00:29:10,669 --> 00:29:15,409
supporting in ordered expressions

00:29:12,440 --> 00:29:17,090
natively we also want to add integration

00:29:15,409 --> 00:29:19,640
with streaming expressions

00:29:17,090 --> 00:29:20,919
since both calculated analytics would be

00:29:19,640 --> 00:29:23,390
nice if they could talk to each other

00:29:20,919 --> 00:29:25,520
also the ability to pivot over different

00:29:23,390 --> 00:29:28,789
types of facets so say you want to value

00:29:25,520 --> 00:29:31,789
facet pivoted by query facets much like

00:29:28,789 --> 00:29:33,770
it works in JSON facets and then finally

00:29:31,789 --> 00:29:35,210
the ability to add custom functions in

00:29:33,770 --> 00:29:37,340
the schema so that you can write your

00:29:35,210 --> 00:29:39,950
own functionality and not have to

00:29:37,340 --> 00:29:44,029
actually modify solar source code and

00:29:39,950 --> 00:29:47,270
build your own package in conclusion on

00:29:44,029 --> 00:29:49,669
the Analects component provides complex

00:29:47,270 --> 00:29:53,870
data introspection without spending the

00:29:49,669 --> 00:29:55,850
resources on managing external analytics

00:29:53,870 --> 00:29:58,070
engines it's running in production at

00:29:55,850 --> 00:30:00,620
scale at Bloomberg and it's available

00:29:58,070 --> 00:30:01,970
starting this all the features that I've

00:30:00,620 --> 00:30:04,279
talked about are available starting with

00:30:01,970 --> 00:30:07,460
the cellar 7 release it's been in solar

00:30:04,279 --> 00:30:10,600
since solar 5 but the distributed and

00:30:07,460 --> 00:30:12,919
new features are in solar 7

00:30:10,600 --> 00:30:15,950
documentation in important bug fixes

00:30:12,919 --> 00:30:18,409
were included in solar 7 too and

00:30:15,950 --> 00:30:21,470
basically if you have a solar cloud and

00:30:18,409 --> 00:30:22,940
need analytics check out solar analytics

00:30:21,470 --> 00:30:26,179
because it could possibly fit your needs

00:30:22,940 --> 00:30:28,159
very easily if you have any questions

00:30:26,179 --> 00:30:31,100
there's a section on the reference guide

00:30:28,159 --> 00:30:32,750
about it we are hiring at Bloomberg for

00:30:31,100 --> 00:30:34,220
search professionals so please can talk

00:30:32,750 --> 00:30:35,240
to us if you have any questions about

00:30:34,220 --> 00:30:37,190
that

00:30:35,240 --> 00:30:38,890
we have some current work of moving the

00:30:37,190 --> 00:30:42,110
Analects component from the country as a

00:30:38,890 --> 00:30:44,120
contribute to moving it to core and we

00:30:42,110 --> 00:30:46,029
have a history of the different solar

00:30:44,120 --> 00:30:49,630
tickets if you're interested in that

00:30:46,029 --> 00:30:49,630
cool any questions

00:30:57,960 --> 00:31:06,910
hello you talked about the concat

00:31:01,960 --> 00:31:09,910
function for multivalued yields but do

00:31:06,910 --> 00:31:13,810
you have for example a count function

00:31:09,910 --> 00:31:16,540
yes most so those are just like examples

00:31:13,810 --> 00:31:18,340
in the reference guide it gives all the

00:31:16,540 --> 00:31:20,050
default functions and what type of

00:31:18,340 --> 00:31:22,960
parameters they allow multi-diode or

00:31:20,050 --> 00:31:25,210
single valued oh I think all the

00:31:22,960 --> 00:31:27,190
reduction functions allow single or

00:31:25,210 --> 00:31:29,140
multi valued fields like it doesn't

00:31:27,190 --> 00:31:31,120
really care so if you're doing a count

00:31:29,140 --> 00:31:33,190
you can do a count of documents which is

00:31:31,120 --> 00:31:34,630
like if you have a multi valued field it

00:31:33,190 --> 00:31:37,060
would just count the number of documents

00:31:34,630 --> 00:31:38,770
that have a value or count the number of

00:31:37,060 --> 00:31:42,670
values in each field if that makes sense

00:31:38,770 --> 00:31:46,060
so you counted the number of values in a

00:31:42,670 --> 00:31:52,020
multi valued field and after that can

00:31:46,060 --> 00:31:56,020
you sort of your results on this base

00:31:52,020 --> 00:31:57,790
are you mean sorting like facets you get

00:31:56,020 --> 00:32:00,610
you can search facets on that count this

00:31:57,790 --> 00:32:03,370
yeah so you would like to see the

00:32:00,610 --> 00:32:07,300
results in your facets with the number

00:32:03,370 --> 00:32:09,520
of v.i.c seven values and six yeah you

00:32:07,300 --> 00:32:12,760
can sort by any analytical expression

00:32:09,520 --> 00:32:21,910
that you give it and counting is yeah

00:32:12,760 --> 00:32:25,000
one of those Thanks all right great talk

00:32:21,910 --> 00:32:26,200
Thanks my question is about I've come

00:32:25,000 --> 00:32:27,940
across a couple of use cases where

00:32:26,200 --> 00:32:29,110
people want dynamic facets so they don't

00:32:27,940 --> 00:32:30,670
know the buckets at the start and they

00:32:29,110 --> 00:32:32,500
want to derive it from the data as they

00:32:30,670 --> 00:32:35,020
go through have you thought of any of

00:32:32,500 --> 00:32:35,980
those use cases so her in the original

00:32:35,020 --> 00:32:38,230
analytics component

00:32:35,980 --> 00:32:40,720
did support that but it was a bit tricky

00:32:38,230 --> 00:32:42,880
to do given the just like whenever we

00:32:40,720 --> 00:32:44,650
had a distributed support because you

00:32:42,880 --> 00:32:46,840
need to calculate all the results then

00:32:44,650 --> 00:32:48,970
go back out to the shards to get more

00:32:46,840 --> 00:32:55,180
data so that's not supported right now

00:32:48,970 --> 00:32:58,060
but when we add in order and the in

00:32:55,180 --> 00:32:59,980
order expressions that should be it I

00:32:58,060 --> 00:33:01,510
would imagine that would be enabled

00:32:59,980 --> 00:33:03,990
using the results of previous

00:33:01,510 --> 00:33:11,280
expressions to facet over the new ones

00:33:03,990 --> 00:33:11,280
yeah questions

00:33:14,929 --> 00:33:20,559
okay if none thanks houston for the talk

00:33:20,650 --> 00:33:23,789

YouTube URL: https://www.youtube.com/watch?v=2N94JVacGaE


