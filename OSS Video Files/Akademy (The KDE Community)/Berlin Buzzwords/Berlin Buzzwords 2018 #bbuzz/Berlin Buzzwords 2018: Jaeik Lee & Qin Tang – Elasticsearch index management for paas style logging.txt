Title: Berlin Buzzwords 2018: Jaeik Lee & Qin Tang â€“ Elasticsearch index management for paas style logging
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Jaeik Lee and Qin Tang talking about "Elasticsearch index management for paas style logging system".

In this session, we will introduce large-scale log management system called NELO used in Naver corp and mainly discuss how to maintain Elasticsearch indices for paas style logging system. Naver Corporation is an Internet content service company which operates Korea's top search engine Naver and manages global mobile services such as the mobile messenger LINE, video messenger Snow, and group communication service BAND. 

NELO is handling various different kinds of logs and more than 3 billions of logs are incoming every day. As backend storage and search engine, we are heavily depending on Elasticsearch. Because the number of logs and variety of logs is increasing, managing indices in Elasticsearch clusters are more and more complicated. In the beginning, we only created one index every day, but as scales are growing, we suffered mapping explosion issues and performance issues. By introducing index management service inside NELO, now we have resolved mapping explosion issues and supported custom type and custom retention time, etc. 

In this session we will explain our first and recent index model and how to resolve mapping explosion and how to support custom type. From this information, users will be able to understand difficulties of maintaining large scale Elasticsearch cluster and index model for multi-tenant log management system which can cover many different kinds of logs with different mappings.

Read more:
https://2018.berlinbuzzwords.de/18/session/elasticsearch-index-management-paas-style-logging-system

About Qin Tang:
https://2018.berlinbuzzwords.de/users/qin-tang

About Jaeik Lee:
https://2018.berlinbuzzwords.de/users/jaeik-lee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,890 --> 00:00:10,950
good morning everyone this is Jay and

00:00:07,870 --> 00:00:14,350
working for neighbor in Korea

00:00:10,950 --> 00:00:22,869
it's good money I am counting also

00:00:14,350 --> 00:00:26,830
working for neighboring China so today

00:00:22,869 --> 00:00:30,330
we will talk about elastic elastic

00:00:26,830 --> 00:00:36,760
search index management for pass style

00:00:30,330 --> 00:00:40,600
logging system first I'll briefly

00:00:36,760 --> 00:00:44,170
introduce nello and how we use elastic

00:00:40,600 --> 00:00:48,789
search in yellow and some problem or

00:00:44,170 --> 00:00:54,039
first design and then how we resolve the

00:00:48,789 --> 00:01:04,629
issues in second page and index manager

00:00:54,039 --> 00:01:07,920
details is design so nello is in house

00:01:04,629 --> 00:01:12,189
logging system in neighbor for neighbor

00:01:07,920 --> 00:01:15,700
developers and some we provide some

00:01:12,189 --> 00:01:18,900
asset case to port logs and collecting

00:01:15,700 --> 00:01:22,810
logs through some various protocols and

00:01:18,900 --> 00:01:27,009
we provide a lot in your time and

00:01:22,810 --> 00:01:31,360
scheduled way so some users if there's a

00:01:27,009 --> 00:01:36,060
Magister a lot if some rules of a lot

00:01:31,360 --> 00:01:40,000
meet then we send some notification and

00:01:36,060 --> 00:01:43,899
we also provide kind of crash slope this

00:01:40,000 --> 00:01:48,460
implication and the of obvious case of

00:01:43,899 --> 00:01:53,380
you stating that means for example in

00:01:48,460 --> 00:01:56,670
mobile application sometimes application

00:01:53,380 --> 00:01:59,860
crashes then it generates crash log

00:01:56,670 --> 00:02:05,950
basically that crash slow is that Shuman

00:01:59,860 --> 00:02:09,190
lead over so in our peg and some for

00:02:05,950 --> 00:02:14,070
example if user register their binary

00:02:09,190 --> 00:02:16,060
and some when there is some crazy log

00:02:14,070 --> 00:02:19,879
some

00:02:16,060 --> 00:02:24,049
generated in applications an application

00:02:19,879 --> 00:02:28,610
for to our system through our SDK then

00:02:24,049 --> 00:02:32,780
in our system we some transform that not

00:02:28,610 --> 00:02:37,010
human kneadable some crash log to human

00:02:32,780 --> 00:02:40,310
readable format and we developed own web

00:02:37,010 --> 00:02:44,180
application and we provide dashboard

00:02:40,310 --> 00:02:48,349
just given a dashboard and some case

00:02:44,180 --> 00:02:51,440
user want to customize their some law

00:02:48,349 --> 00:02:56,629
with stored in yellow so we provide open

00:02:51,440 --> 00:03:04,430
API to make some data available to those

00:02:56,629 --> 00:03:08,209
kinds of users we have to some vein

00:03:04,430 --> 00:03:12,139
design considerations first one is we

00:03:08,209 --> 00:03:16,569
need to support various log format in

00:03:12,139 --> 00:03:20,410
neighbor we support some we provide many

00:03:16,569 --> 00:03:24,680
applications and those applications are

00:03:20,410 --> 00:03:28,069
developed based on some various platform

00:03:24,680 --> 00:03:30,799
such as Android and iOS and Mac OSX

00:03:28,069 --> 00:03:33,379
windows and some depending on

00:03:30,799 --> 00:03:35,000
publication they also some use some

00:03:33,379 --> 00:03:38,269
frame of to Saint Louis

00:03:35,000 --> 00:03:41,090
so depending on platform or frame ups

00:03:38,269 --> 00:03:46,099
and some terrorism common part for some

00:03:41,090 --> 00:03:50,480
different hard talk local format we need

00:03:46,099 --> 00:03:53,480
to cover those cases and also meanwhile

00:03:50,480 --> 00:03:56,540
some application they also want to

00:03:53,480 --> 00:04:01,819
define some own some custom for some

00:03:56,540 --> 00:04:04,790
field so basically we have some inside

00:04:01,819 --> 00:04:08,989
of our companies and some hours and

00:04:04,790 --> 00:04:13,010
client who want to some pro by some

00:04:08,989 --> 00:04:15,250
their own local format so we need to

00:04:13,010 --> 00:04:15,250
cover

00:04:18,370 --> 00:04:25,479
this is a current our scale Ovilus

00:04:21,549 --> 00:04:30,190
search we have four hundred ninety s and

00:04:25,479 --> 00:04:33,669
note and eleven clusters in nine

00:04:30,190 --> 00:04:36,820
instances and a total number of logs

00:04:33,669 --> 00:04:41,070
seven point six terabyte and total size

00:04:36,820 --> 00:04:44,710
of logs currently one point six petabyte

00:04:41,070 --> 00:04:47,590
at the beginning of our product we just

00:04:44,710 --> 00:04:52,410
start from training data node herbalist

00:04:47,590 --> 00:04:57,280
search and we just have some

00:04:52,410 --> 00:05:00,360
half-billion laws so we our some product

00:04:57,280 --> 00:05:07,479
itself is keep growing so we need to

00:05:00,360 --> 00:05:12,520
concern of of scalability so this is our

00:05:07,479 --> 00:05:15,850
overall architecture so in client-side

00:05:12,520 --> 00:05:20,639
we support many different kind of sdk

00:05:15,850 --> 00:05:23,530
including some open source logging agent

00:05:20,639 --> 00:05:28,720
client applications and log to our

00:05:23,530 --> 00:05:32,380
collector and in collector size if some

00:05:28,720 --> 00:05:37,270
incoming logs are growing then we can

00:05:32,380 --> 00:05:39,490
easily add some collector and some some

00:05:37,270 --> 00:05:42,610
registers some collector addressed to

00:05:39,490 --> 00:05:47,380
the air for then it is very easy to

00:05:42,610 --> 00:05:51,039
scare out and most of data stored in

00:05:47,380 --> 00:05:55,780
Africa and some depending on some local

00:05:51,039 --> 00:05:58,470
type and event we defined some some

00:05:55,780 --> 00:06:02,530
different topics and in the backend

00:05:58,470 --> 00:06:05,710
depending on this topic we process and

00:06:02,530 --> 00:06:11,710
most of some back-end processing we

00:06:05,710 --> 00:06:15,310
implemented on turbo stone and crash

00:06:11,710 --> 00:06:19,810
collector is aggregation crash data and

00:06:15,310 --> 00:06:24,800
store to last asserts and most over logs

00:06:19,810 --> 00:06:27,830
lead from Kafka and some

00:06:24,800 --> 00:06:30,830
some in-depth through sand storm

00:06:27,830 --> 00:06:32,979
topology and for allotting we use

00:06:30,830 --> 00:06:35,870
populate the feature

00:06:32,979 --> 00:06:38,440
simulator I already explained you but

00:06:35,870 --> 00:06:41,680
implemented it on top of stone

00:06:38,440 --> 00:06:44,990
previously we use a different types of

00:06:41,680 --> 00:06:47,569
different kind of frame oh but for

00:06:44,990 --> 00:06:54,440
maintainability we unified to use a

00:06:47,569 --> 00:06:57,860
stone and some most of our features are

00:06:54,440 --> 00:07:00,880
available and featuring some data is

00:06:57,860 --> 00:07:03,319
available through open API and users are

00:07:00,880 --> 00:07:08,659
basically interact through our web

00:07:03,319 --> 00:07:15,169
application and today I remain nice and

00:07:08,659 --> 00:07:18,830
talk of elasticsearch so enforced faith

00:07:15,169 --> 00:07:22,060
this is a elasticsearch closed

00:07:18,830 --> 00:07:30,710
architecture we had three dedicated

00:07:22,060 --> 00:07:33,530
master node and client node and so most

00:07:30,710 --> 00:07:37,819
of some such requests or indexing

00:07:33,530 --> 00:07:42,190
Roquette always some cones routes and

00:07:37,819 --> 00:07:42,190
coordinating those to the data node and

00:07:42,310 --> 00:07:52,279
so we use some kind of hot ohm

00:07:47,719 --> 00:07:57,909
architecture so using some SSD machine

00:07:52,279 --> 00:08:02,479
we solve some hot data and through some

00:07:57,909 --> 00:08:07,599
HDD machine we some solve some warm data

00:08:02,479 --> 00:08:15,169
I think is kinda popular architecture

00:08:07,599 --> 00:08:20,560
and for index motor at the beginning we

00:08:15,169 --> 00:08:25,190
defined someone in depth Pole and day so

00:08:20,560 --> 00:08:30,580
every data for one day means all some

00:08:25,190 --> 00:08:30,580
project data stored in one in depth and

00:08:30,800 --> 00:08:37,120
and each project we define some

00:08:33,560 --> 00:08:39,709
different mapping and to support

00:08:37,120 --> 00:08:44,870
retention time basically some old

00:08:39,709 --> 00:08:47,690
project in this structure all projects

00:08:44,870 --> 00:08:52,550
have same detention time because they

00:08:47,690 --> 00:08:59,750
are sharing some index so if there is

00:08:52,550 --> 00:09:02,660
some requirement for example some some

00:08:59,750 --> 00:09:06,490
organization want to keep some data for

00:09:02,660 --> 00:09:16,220
two years or five years that we splitted

00:09:06,490 --> 00:09:22,310
instances and how we search or in depth

00:09:16,220 --> 00:09:27,740
and to optimize search performance we

00:09:22,310 --> 00:09:32,209
use a custom routing so for example for

00:09:27,740 --> 00:09:35,750
small project we like to store data only

00:09:32,209 --> 00:09:40,040
why short but for big project to

00:09:35,750 --> 00:09:45,260
paralyse processing we store some data

00:09:40,040 --> 00:09:51,740
to all chart in in depth so for example

00:09:45,260 --> 00:09:55,820
here that - actually this decide whether

00:09:51,740 --> 00:10:01,279
the project is more or at the moment we

00:09:55,820 --> 00:10:06,339
just use some static value some 50

00:10:01,279 --> 00:10:10,850
millions some logs per day if some

00:10:06,339 --> 00:10:13,940
projects have dead much off-site then we

00:10:10,850 --> 00:10:18,230
some consider it as big project some

00:10:13,940 --> 00:10:21,860
others and project have lower value then

00:10:18,230 --> 00:10:26,120
we consider it as a small project so

00:10:21,860 --> 00:10:32,000
here's a line application is a big

00:10:26,120 --> 00:10:34,940
project so let's say is more than 50

00:10:32,000 --> 00:10:36,079
million data is coming every day then we

00:10:34,940 --> 00:10:42,040
store some data

00:10:36,079 --> 00:10:44,710
- all short and for Nate music and

00:10:42,040 --> 00:10:55,060
some map application we store only

00:10:44,710 --> 00:10:59,470
tutors one specific chart but in this

00:10:55,060 --> 00:11:03,670
model determinism problems first problem

00:10:59,470 --> 00:11:06,700
was a so-called mapping explosion more

00:11:03,670 --> 00:11:10,300
projects created that means more mapping

00:11:06,700 --> 00:11:14,110
critic creatives as well so for example

00:11:10,300 --> 00:11:18,310
in neighbor instance we have more than

00:11:14,110 --> 00:11:22,420
3000 project that means everyday 3000

00:11:18,310 --> 00:11:26,560
mapping the pint in one in depth size

00:11:22,420 --> 00:11:29,110
was on more than six megabytes basically

00:11:26,560 --> 00:11:33,010
you know last search synchronized the

00:11:29,110 --> 00:11:37,570
mapping of an index among all node with

00:11:33,010 --> 00:11:40,360
single thread so that means some size of

00:11:37,570 --> 00:11:44,140
mapping increases and number of a node

00:11:40,360 --> 00:11:51,100
increase then some takes more time to

00:11:44,140 --> 00:11:54,520
update mapping so sometimes entire cloth

00:11:51,100 --> 00:11:58,240
blocked by this mapping update mapping

00:11:54,520 --> 00:12:03,060
event so for example this was a kind of

00:11:58,240 --> 00:12:09,100
log so some some update food mapping

00:12:03,060 --> 00:12:16,690
took some more than five minute so if we

00:12:09,100 --> 00:12:21,130
see this graph of index root food so if

00:12:16,690 --> 00:12:25,330
we see some lead arrow every some some

00:12:21,130 --> 00:12:27,880
moment some kind of Sun in the same

00:12:25,330 --> 00:12:32,800
group for dropped because of that some

00:12:27,880 --> 00:12:37,930
update mapping event and another problem

00:12:32,800 --> 00:12:41,250
is a imbalance of short sides because we

00:12:37,930 --> 00:12:45,130
are using the routing custom loading so

00:12:41,250 --> 00:12:47,770
you know velocity search decide the

00:12:45,130 --> 00:12:51,640
chart based on hash value of that

00:12:47,770 --> 00:12:52,910
loading value so if some project have

00:12:51,640 --> 00:12:57,410
same

00:12:52,910 --> 00:13:02,120
some hash value then some those projects

00:12:57,410 --> 00:13:05,350
stored in same chart that means

00:13:02,120 --> 00:13:10,220
sometimes some chart bigger than other

00:13:05,350 --> 00:13:14,449
chart and it means - art is busier than

00:13:10,220 --> 00:13:17,000
others so in this graph this is came

00:13:14,449 --> 00:13:19,850
from the earth data actually or other

00:13:17,000 --> 00:13:23,449
problem is a shared site itself quite

00:13:19,850 --> 00:13:28,160
big so here's a most of shared size is

00:13:23,449 --> 00:13:33,889
around between 40 to 60 but some charge

00:13:28,160 --> 00:13:38,120
is more than 100 kilobytes so it can

00:13:33,889 --> 00:13:42,940
affect some entires and close the

00:13:38,120 --> 00:13:49,069
performance and another thing is a

00:13:42,940 --> 00:13:53,540
impact of big project so basically some

00:13:49,069 --> 00:13:57,560
among more than 3,000 project less than

00:13:53,540 --> 00:14:02,269
10% of project some configured as big

00:13:57,560 --> 00:14:06,949
size big project and they send but they

00:14:02,269 --> 00:14:10,579
are sending more than 80% of logs so you

00:14:06,949 --> 00:14:15,439
know some because they are stored in

00:14:10,579 --> 00:14:18,740
same in depth some some so I shard has

00:14:15,439 --> 00:14:21,170
shared together so that means some

00:14:18,740 --> 00:14:24,579
shirring lizards together so all

00:14:21,170 --> 00:14:29,290
paintings and Obamas of remaining some

00:14:24,579 --> 00:14:29,290
90% of small project

00:14:31,509 --> 00:14:42,589
another one is a as our product is

00:14:38,480 --> 00:14:47,660
service keep growing some organization

00:14:42,589 --> 00:14:50,870
keep asking us to some kind of custom

00:14:47,660 --> 00:14:54,380
mutation like one or two years or

00:14:50,870 --> 00:14:59,240
sometimes almost forever they want to

00:14:54,380 --> 00:15:01,850
kill data in this case basically we

00:14:59,240 --> 00:15:04,749
don't support readings and ones and less

00:15:01,850 --> 00:15:08,259
third cluster essen

00:15:04,749 --> 00:15:13,449
and their different retention time in

00:15:08,259 --> 00:15:17,230
our structure so at the moment we need

00:15:13,449 --> 00:15:20,949
to split instance means different

00:15:17,230 --> 00:15:28,870
elasticsearch cluster and I mean some

00:15:20,949 --> 00:15:32,769
some different entire narrow so it means

00:15:28,870 --> 00:15:35,620
we need to some give some mores and

00:15:32,769 --> 00:15:39,459
maintainability cost because we need

00:15:35,620 --> 00:15:48,749
boards and some note and we need to

00:15:39,459 --> 00:15:51,639
maintain more servers another problem is

00:15:48,749 --> 00:15:56,410
at the beginning honest we support

00:15:51,639 --> 00:15:56,889
string time but you know in dashboard -

00:15:56,410 --> 00:16:00,339
oh great

00:15:56,889 --> 00:16:04,180
k theta numeric data have more valuable

00:16:00,339 --> 00:16:07,839
to some show some very different kind of

00:16:04,180 --> 00:16:13,749
application result but you know from

00:16:07,839 --> 00:16:16,920
last 32 point s even if some some

00:16:13,749 --> 00:16:20,829
mapping is different but if some

00:16:16,920 --> 00:16:26,439
mappings and this resides in within one

00:16:20,829 --> 00:16:34,689
I mean same in this then if field name

00:16:26,439 --> 00:16:39,939
is same type also should be same so for

00:16:34,689 --> 00:16:45,129
example let's say I want to use side

00:16:39,939 --> 00:16:49,240
field but side field with some integer

00:16:45,129 --> 00:16:52,449
some type but other project already is

00:16:49,240 --> 00:16:59,649
and defined the side field as string

00:16:52,449 --> 00:17:04,260
then there can be some type conflict so

00:16:59,649 --> 00:17:08,679
here question is to define new some

00:17:04,260 --> 00:17:11,220
design do we need to create separate in

00:17:08,679 --> 00:17:16,120
dice for every project

00:17:11,220 --> 00:17:20,939
in the case we have 3000 project means

00:17:16,120 --> 00:17:30,779
we need to define 3000 indices every day

00:17:20,939 --> 00:17:34,779
means any 3000 charge per day created so

00:17:30,779 --> 00:17:38,320
if some if there are two leaf flickers

00:17:34,779 --> 00:17:47,230
that means hands out on the chart every

00:17:38,320 --> 00:17:50,190
day created so it's not scalable to

00:17:47,230 --> 00:17:57,639
resolve this problem we improved our

00:17:50,190 --> 00:18:01,950
architecture we have several instance

00:17:57,639 --> 00:18:05,620
but there are some some big instance

00:18:01,950 --> 00:18:10,139
special neighbor and line instance so

00:18:05,620 --> 00:18:14,320
for those instance we splitted some

00:18:10,139 --> 00:18:21,429
cluster to keep some some metadata

00:18:14,320 --> 00:18:24,190
cluster smaller and we splitted indices

00:18:21,429 --> 00:18:28,809
per day previously we defined only one

00:18:24,190 --> 00:18:33,429
in this we defined multiple some indices

00:18:28,809 --> 00:18:38,980
but some none like some some define

00:18:33,429 --> 00:18:41,740
entities for every project with by

00:18:38,980 --> 00:18:44,169
splitting some indices with sub we can

00:18:41,740 --> 00:18:48,490
support some custom type and the change

00:18:44,169 --> 00:18:52,779
question retention time and previously

00:18:48,490 --> 00:18:58,559
we only some using some static already

00:18:52,779 --> 00:19:02,769
some specifies and chart number but in

00:18:58,559 --> 00:19:06,759
new model we sent island it dynamically

00:19:02,769 --> 00:19:13,149
asked me some number of shard some by

00:19:06,759 --> 00:19:17,460
some history of some log size so we keep

00:19:13,149 --> 00:19:20,340
organized on number of shard and cluster

00:19:17,460 --> 00:19:22,590
so

00:19:20,340 --> 00:19:26,040
in terms of elastic search across the

00:19:22,590 --> 00:19:29,220
architecture we introduced it at rhyme

00:19:26,040 --> 00:19:33,150
note you know try note with try note you

00:19:29,220 --> 00:19:37,740
can search with for some multi cluster

00:19:33,150 --> 00:19:41,490
so what I mean by that is even if some

00:19:37,740 --> 00:19:42,299
data is stored in different charged with

00:19:41,490 --> 00:19:46,770
criminal

00:19:42,299 --> 00:19:48,510
you can search together and but there is

00:19:46,770 --> 00:19:52,980
some limitation with trying knows you

00:19:48,510 --> 00:19:56,820
cannot update some metadata so from six

00:19:52,980 --> 00:19:57,570
point X alas search oh fisheries and

00:19:56,820 --> 00:20:00,150
support

00:19:57,570 --> 00:20:03,000
cross cross desert so if we use some

00:20:00,150 --> 00:20:07,470
recent button we can use cross cross the

00:20:03,000 --> 00:20:11,850
search and in previous model we just

00:20:07,470 --> 00:20:14,700
have one cluster even for hot here and

00:20:11,850 --> 00:20:19,590
one here we just use some comment to

00:20:14,700 --> 00:20:23,309
move data from hot to warm but it with

00:20:19,590 --> 00:20:29,640
fretted clusters so we in our case we

00:20:23,309 --> 00:20:33,270
use a HDFS snapshot so when we move some

00:20:29,640 --> 00:20:37,230
data we first some store snapshot from

00:20:33,270 --> 00:20:45,360
this hot cluster and restore to some one

00:20:37,230 --> 00:20:49,860
cluster and for index mode we separate

00:20:45,360 --> 00:20:55,350
indices to different kind for small

00:20:49,860 --> 00:20:58,640
project still we store some common

00:20:55,350 --> 00:21:03,450
indices means one in this for one day

00:20:58,640 --> 00:21:07,679
because 90% of some project still stored

00:21:03,450 --> 00:21:14,400
in one in depth but for big project we

00:21:07,679 --> 00:21:18,080
stored in dedicated indices so we wide

00:21:14,400 --> 00:21:22,350
this way because we try to some

00:21:18,080 --> 00:21:26,130
optimized on the number of chart so in

00:21:22,350 --> 00:21:28,670
this example some be project like band

00:21:26,130 --> 00:21:32,900
or neighboring or line they

00:21:28,670 --> 00:21:37,940
some some have some dedicated indices

00:21:32,900 --> 00:21:45,490
but others have some share some common

00:21:37,940 --> 00:21:50,870
index in this model to surgeon in depth

00:21:45,490 --> 00:21:57,049
is more simplified because we you start

00:21:50,870 --> 00:22:00,710
using areas here so previously for

00:21:57,049 --> 00:22:03,020
searching in there's some we need to

00:22:00,710 --> 00:22:03,740
aware about whether the project is big

00:22:03,020 --> 00:22:06,350
or small

00:22:03,740 --> 00:22:09,290
because for big project we don't specify

00:22:06,350 --> 00:22:11,720
loading and just use default routing but

00:22:09,290 --> 00:22:17,450
possible small project we need to

00:22:11,720 --> 00:22:20,929
specify some routing value but here some

00:22:17,450 --> 00:22:25,130
some for indexing searching don't need

00:22:20,929 --> 00:22:28,370
to care about it just use just need to

00:22:25,130 --> 00:22:33,350
aware about an alias name alias name nor

00:22:28,370 --> 00:22:41,679
is project name and some date of loss

00:22:33,350 --> 00:22:46,429
and for indexing name for dedicated

00:22:41,679 --> 00:22:51,890
project we give some post piece some

00:22:46,429 --> 00:22:56,169
project name so application only care

00:22:51,890 --> 00:23:00,169
about areas but within areas it just

00:22:56,169 --> 00:23:04,540
knows and point to specification L in

00:23:00,169 --> 00:23:08,120
depth or specific chart with routing

00:23:04,540 --> 00:23:11,870
this kind of job is done by index

00:23:08,120 --> 00:23:16,100
measure so touching can you introduce

00:23:11,870 --> 00:23:20,600
about in the Spencer thanks Kay okay

00:23:16,100 --> 00:23:24,140
next I will describe was a detail of how

00:23:20,600 --> 00:23:26,210
we managing our new index models and the

00:23:24,140 --> 00:23:28,850
component of which is rooster tomb

00:23:26,210 --> 00:23:30,580
performance a management work is called

00:23:28,850 --> 00:23:34,610
the index manager

00:23:30,580 --> 00:23:37,400
what is index manager it is responsible

00:23:34,610 --> 00:23:39,570
for manic is a life circle of all

00:23:37,400 --> 00:23:43,499
indices in a newest is

00:23:39,570 --> 00:23:46,200
every day Yasuda pre create all the

00:23:43,499 --> 00:23:49,710
indices and the new SES for our project

00:23:46,200 --> 00:23:53,779
and it needed to delete all expired

00:23:49,710 --> 00:23:58,559
English is and the Angels is everything

00:23:53,779 --> 00:24:01,499
besides of this as we have multiple

00:23:58,559 --> 00:24:04,739
clusters so it will show the response

00:24:01,499 --> 00:24:07,919
Bravo transfer indices is among these

00:24:04,739 --> 00:24:12,119
hot tire clusters according to the index

00:24:07,919 --> 00:24:15,690
retention time besides all the scheduler

00:24:12,119 --> 00:24:17,970
task index manager also handles some

00:24:15,690 --> 00:24:20,820
instant the project a monumental event

00:24:17,970 --> 00:24:24,509
as this event they may bring some

00:24:20,820 --> 00:24:30,809
changes to our existing index such as a

00:24:24,509 --> 00:24:34,379
create project and a delete project so

00:24:30,809 --> 00:24:38,009
this is a modules and the tasks of the

00:24:34,379 --> 00:24:40,769
index manager inside the index manager

00:24:38,009 --> 00:24:44,639
some tasks that just needed to be

00:24:40,769 --> 00:24:48,269
scheduled or once every day but others

00:24:44,639 --> 00:24:51,330
they needed to be scheduled every time

00:24:48,269 --> 00:24:55,379
there is a real-time management event

00:24:51,330 --> 00:24:57,830
comes for scheduled the tasks they are

00:24:55,379 --> 00:25:01,739
responsible for maintaining the existing

00:24:57,830 --> 00:25:05,190
projects in this is an dangerous is they

00:25:01,739 --> 00:25:09,419
are for kind of scheduler tasks first is

00:25:05,190 --> 00:25:12,659
that is the creating index in angel

00:25:09,419 --> 00:25:15,809
strobe in this drop it is responsible

00:25:12,659 --> 00:25:18,090
for creators of all projects indices and

00:25:15,809 --> 00:25:21,539
Indian angels years of tomorrow

00:25:18,090 --> 00:25:24,210
and it also needed to transfer the oil

00:25:21,539 --> 00:25:29,399
indexes from one cluster to another

00:25:24,210 --> 00:25:33,090
cluster in the backup job related to a

00:25:29,399 --> 00:25:36,330
create indexes wrap short in HDFS and

00:25:33,090 --> 00:25:41,609
this is for use the fulfil over backup

00:25:36,330 --> 00:25:43,590
and in the clean up drop the index

00:25:41,609 --> 00:25:46,649
manager should be responsible for

00:25:43,590 --> 00:25:50,370
removing all the expiring indices and

00:25:46,649 --> 00:25:55,350
aeneas s according to the retention time

00:25:50,370 --> 00:25:57,690
so here is a scheduler task another kind

00:25:55,350 --> 00:25:59,730
of task is world ham task

00:25:57,690 --> 00:26:03,900
there are four kinds of a real-time task

00:25:59,730 --> 00:26:07,020
first when the user created a project as

00:26:03,900 --> 00:26:10,710
raloo web app then the create project

00:26:07,020 --> 00:26:14,580
handle et sudha and as projects a nurse

00:26:10,710 --> 00:26:17,669
to the existing in this then well

00:26:14,580 --> 00:26:19,950
Theatre Project event comes in each

00:26:17,669 --> 00:26:24,029
project handler should delete the

00:26:19,950 --> 00:26:30,120
project dedicated the index was angels

00:26:24,029 --> 00:26:33,330
from common index besides it means to

00:26:30,120 --> 00:26:36,570
delete all the history data of this

00:26:33,330 --> 00:26:41,399
project as vivid project editor is a

00:26:36,570 --> 00:26:45,960
time consuming pro pro the process so

00:26:41,399 --> 00:26:48,440
you to do it in a synchronous another

00:26:45,960 --> 00:26:53,789
you printer is a customized project

00:26:48,440 --> 00:26:57,330
mappings every time when this task

00:26:53,789 --> 00:27:02,220
receives the event it will read as the

00:26:57,330 --> 00:27:07,620
project mappings from we call the

00:27:02,220 --> 00:27:10,169
customized dub mitra indexes then we

00:27:07,620 --> 00:27:13,020
build as a program Baptist in updated

00:27:10,169 --> 00:27:15,690
index and the for displayed project

00:27:13,020 --> 00:27:18,870
event the handle release to create a

00:27:15,690 --> 00:27:22,890
dedicated a index for this project and

00:27:18,870 --> 00:27:30,059
it removes all angels and mappings from

00:27:22,890 --> 00:27:33,450
the coma antics here is a architectural

00:27:30,059 --> 00:27:36,570
view over the index manager as index

00:27:33,450 --> 00:27:39,390
manager manages all the indexes of a

00:27:36,570 --> 00:27:42,690
yellow system so we won't allow any

00:27:39,390 --> 00:27:45,779
downtime instead of a deploy on single

00:27:42,690 --> 00:27:48,960
road we deploy it into the index major

00:27:45,779 --> 00:27:52,799
cluster in this cluster we have one

00:27:48,960 --> 00:27:55,799
master and several snips the each node a

00:27:52,799 --> 00:27:58,530
is collected to the root keeper once the

00:27:55,799 --> 00:28:01,530
master is done remaining snails will

00:27:58,530 --> 00:28:02,750
elect a dual master and the continue to

00:28:01,530 --> 00:28:07,940
perform the Magnum

00:28:02,750 --> 00:28:10,940
to work as we described the in previous

00:28:07,940 --> 00:28:15,250
page the index mangers managing broker

00:28:10,940 --> 00:28:19,160
can be triggering in two ways first is

00:28:15,250 --> 00:28:24,080
the scheduler task it will be triggered

00:28:19,160 --> 00:28:25,970
by the timer every day and once the

00:28:24,080 --> 00:28:28,880
timer is triggered trigger that

00:28:25,970 --> 00:28:32,240
different task as a way of performing

00:28:28,880 --> 00:28:35,870
the monkey mental work differently for

00:28:32,240 --> 00:28:38,680
example for the pre create job the first

00:28:35,870 --> 00:28:42,410
stage will create all the projects

00:28:38,680 --> 00:28:49,120
angels and mappings in the hot cluster

00:28:42,410 --> 00:28:53,630
and as a nurse and for the transfer job

00:28:49,120 --> 00:28:57,500
it needed to transfer the indexes from

00:28:53,630 --> 00:29:02,080
the hot cluster and the tool on cluster

00:28:57,500 --> 00:29:05,450
and we use HDFS as is a story meter and

00:29:02,080 --> 00:29:08,390
for the clean up drop it needs to remove

00:29:05,450 --> 00:29:12,590
all expiry indices and any alleles are

00:29:08,390 --> 00:29:15,770
from all these clusters and the fourth

00:29:12,590 --> 00:29:21,280
backup job it also needed to create a

00:29:15,770 --> 00:29:25,130
snapshots of every indexes in HDFS

00:29:21,280 --> 00:29:28,760
Furious is a scheduled tasks another

00:29:25,130 --> 00:29:32,030
trickery is a real-time event every time

00:29:28,760 --> 00:29:36,560
there is a real-time event or from web

00:29:32,030 --> 00:29:39,800
app then the real-time handle a virtual

00:29:36,560 --> 00:29:42,620
event from the Kafka and handle the

00:29:39,800 --> 00:29:46,130
differently maybe create read edit take

00:29:42,620 --> 00:29:49,060
Katie the index were a dangerous to the

00:29:46,130 --> 00:29:56,360
coma Enix were updated projects map is

00:29:49,060 --> 00:29:59,600
like that after talking about these

00:29:56,360 --> 00:30:04,580
tasks this come to see the index manager

00:29:59,600 --> 00:30:08,000
config a stick worthy describe the

00:30:04,580 --> 00:30:11,120
previously we have a steering instance

00:30:08,000 --> 00:30:12,950
is each instance their skill is

00:30:11,120 --> 00:30:14,310
different their hardware may be also

00:30:12,950 --> 00:30:19,740
different

00:30:14,310 --> 00:30:22,770
and so and for example we need a

00:30:19,740 --> 00:30:25,050
different configuration for the shop

00:30:22,770 --> 00:30:28,590
number notice odd number the shorter

00:30:25,050 --> 00:30:31,590
size so and we also need to define

00:30:28,590 --> 00:30:34,050
different rotation days so this is our

00:30:31,590 --> 00:30:36,090
son Telemachus our meters we can

00:30:34,050 --> 00:30:42,960
configure the in is an index manager

00:30:36,090 --> 00:30:46,710
config besides for all of the tasks you

00:30:42,960 --> 00:30:49,050
can see we interact with many many

00:30:46,710 --> 00:30:54,380
component like a zookeeper years and

00:30:49,050 --> 00:30:57,900
HDFS any task can be failed to arena

00:30:54,380 --> 00:31:02,340
because accouting officer transaction so

00:30:57,900 --> 00:31:05,220
well if the transaction is fail and when

00:31:02,340 --> 00:31:10,440
index memories recovers and just work

00:31:05,220 --> 00:31:13,650
may be changer from from the older

00:31:10,440 --> 00:31:17,010
master to a low electric master so how

00:31:13,650 --> 00:31:19,980
we do how we know we recover from where

00:31:17,010 --> 00:31:22,410
so we recorded the index managers

00:31:19,980 --> 00:31:27,990
runtime information in your index code

00:31:22,410 --> 00:31:31,560
index manager meter this is also an area

00:31:27,990 --> 00:31:39,960
index which is used the tool stores the

00:31:31,560 --> 00:31:43,830
project's customized mappings okay next

00:31:39,960 --> 00:31:47,730
I will introduce how we perform some

00:31:43,830 --> 00:31:51,360
important tasks first a users creating

00:31:47,730 --> 00:31:54,180
index in Angels is a where they know how

00:31:51,360 --> 00:31:56,880
our new index model is but before we

00:31:54,180 --> 00:31:59,280
create into tomorrow's index I think

00:31:56,880 --> 00:32:05,150
there are several problems we needed to

00:31:59,280 --> 00:32:08,580
solve first is in our new index model we

00:32:05,150 --> 00:32:11,310
allocated a big project in dedicated

00:32:08,580 --> 00:32:11,760
index in the small project in common

00:32:11,310 --> 00:32:16,230
index

00:32:11,760 --> 00:32:20,220
the problem is what kind of project is a

00:32:16,230 --> 00:32:23,430
big and what kind project is a small can

00:32:20,220 --> 00:32:26,869
we decided to dynamically and not only

00:32:23,430 --> 00:32:32,070
use a static number

00:32:26,869 --> 00:32:35,639
and another problem is for a index

00:32:32,070 --> 00:32:38,339
prover snake we use a static a sharp

00:32:35,639 --> 00:32:41,369
number but currently can we determine it

00:32:38,339 --> 00:32:44,399
to the logically every day and what is

00:32:41,369 --> 00:32:47,009
the appropriate number and if the sub

00:32:44,399 --> 00:32:50,249
number is too big then it will overload

00:32:47,009 --> 00:32:55,070
you just the cluster and it is too small

00:32:50,249 --> 00:32:55,070
it will bring down our performance

00:32:55,369 --> 00:33:02,159
another problems for the Angels in

00:32:59,190 --> 00:33:04,979
common index which angels should have

00:33:02,159 --> 00:33:08,999
routine and which angels should have not

00:33:04,979 --> 00:33:11,940
routine how it is that how it decided it

00:33:08,999 --> 00:33:15,329
dynamically maybe come here you are they

00:33:11,940 --> 00:33:17,399
low when needed to estimate as a size we

00:33:15,329 --> 00:33:20,159
needed to estimate it as project styles

00:33:17,399 --> 00:33:22,319
and as the index test then compared with

00:33:20,159 --> 00:33:25,639
some threshold then we determine the

00:33:22,319 --> 00:33:30,029
dynamically here is how we could do it

00:33:25,639 --> 00:33:33,119
first we estimate is our project size

00:33:30,029 --> 00:33:33,749
using the average of a last Testament a

00:33:33,119 --> 00:33:38,489
snogs

00:33:33,749 --> 00:33:42,419
burn then based on these projects ties

00:33:38,489 --> 00:33:45,629
we compare it with a Spoleto style

00:33:42,419 --> 00:33:49,019
threshold then we determine whether this

00:33:45,629 --> 00:33:51,779
project is the allocated in common where

00:33:49,019 --> 00:33:53,999
did he take hidden index is if we

00:33:51,779 --> 00:33:56,129
exercise is a smaller than this ratio

00:33:53,999 --> 00:33:58,739
design you should begin common index

00:33:56,129 --> 00:34:03,749
only if it's bigger than that then it

00:33:58,739 --> 00:34:06,269
should be in dedicated index then for

00:34:03,749 --> 00:34:09,839
then we have multiple witnesses every

00:34:06,269 --> 00:34:12,839
day for each index will lead to decide

00:34:09,839 --> 00:34:16,200
as a short number the Angels is a nice

00:34:12,839 --> 00:34:19,799
markings for the short number as we were

00:34:16,200 --> 00:34:24,659
the nose protocol size in this index so

00:34:19,799 --> 00:34:28,769
we can get the stint exercise simply by

00:34:24,659 --> 00:34:32,129
some of the projects tests and the sub

00:34:28,769 --> 00:34:36,720
number we just estimate it using the

00:34:32,129 --> 00:34:38,149
number of index size divided a stroller

00:34:36,720 --> 00:34:41,779
style a shot

00:34:38,149 --> 00:34:44,869
and the for the agencies in the common

00:34:41,779 --> 00:34:47,149
index we also use as a project size to

00:34:44,869 --> 00:34:49,129
compare with other routines that is the

00:34:47,149 --> 00:34:54,859
threshold then we determine whether

00:34:49,129 --> 00:34:58,819
where you team were not so currently we

00:34:54,859 --> 00:35:01,670
only a disease ratios can figure a

00:34:58,819 --> 00:35:05,119
config file so maybe in the future we

00:35:01,670 --> 00:35:07,130
can determine it dynamically using some

00:35:05,119 --> 00:35:10,480
tool and you run it in our production

00:35:07,130 --> 00:35:16,579
environment then we can get this baby on

00:35:10,480 --> 00:35:21,049
her medical this is how it transferring

00:35:16,579 --> 00:35:24,999
in the exists in our new model for some

00:35:21,049 --> 00:35:27,980
very big instance we have two clusters

00:35:24,999 --> 00:35:32,960
for each cluster we have different

00:35:27,980 --> 00:35:36,049
orientation types of the index so this

00:35:32,960 --> 00:35:38,170
retention diet is configure the Inza

00:35:36,049 --> 00:35:41,589
index manager config file

00:35:38,170 --> 00:35:44,989
besides over this every times

00:35:41,589 --> 00:35:47,809
transparent drop works yellow will

00:35:44,989 --> 00:35:51,950
record as at last to transfer the index

00:35:47,809 --> 00:35:55,249
of each cluster then next time it runs

00:35:51,950 --> 00:35:59,119
from these two kinds of information it

00:35:55,249 --> 00:36:03,200
allows on each cluster which indices we

00:35:59,119 --> 00:36:06,880
need to move then for each index index

00:36:03,200 --> 00:36:10,249
we just create a snapshot in HDFS and

00:36:06,880 --> 00:36:13,130
store the index from this lab shortening

00:36:10,249 --> 00:36:16,700
the target cluster then when the latest

00:36:13,130 --> 00:36:25,579
snapshot and indexes in frost cluster

00:36:16,700 --> 00:36:29,930
after that we have updated metadata here

00:36:25,579 --> 00:36:34,460
is how we do backup the backup task is

00:36:29,930 --> 00:36:39,559
the almost same wheels as a transferring

00:36:34,460 --> 00:36:45,170
drop it also uses HTTP as is a storage

00:36:39,559 --> 00:36:49,249
media and every day the backup job it is

00:36:45,170 --> 00:36:51,589
includes it will finds which we see in

00:36:49,249 --> 00:36:55,069
Texas we should create a snare

00:36:51,589 --> 00:36:58,700
short as every time we record as an

00:36:55,069 --> 00:37:02,799
artist nap short date in the index mind

00:36:58,700 --> 00:37:06,499
emitter so we know where to start then

00:37:02,799 --> 00:37:09,680
below from the start date to yesterday

00:37:06,499 --> 00:37:11,630
we all needed to create indices and for

00:37:09,680 --> 00:37:18,319
every one we create a snapshot and

00:37:11,630 --> 00:37:24,229
update as a metadata this is how we

00:37:18,319 --> 00:37:26,630
removing indices and a newest s as the

00:37:24,229 --> 00:37:28,700
chick already state we have many

00:37:26,630 --> 00:37:31,430
projects through styling the one

00:37:28,700 --> 00:37:35,450
instance and every project they have

00:37:31,430 --> 00:37:38,359
different retention time so we live to

00:37:35,450 --> 00:37:40,339
clean this project data differently

00:37:38,359 --> 00:37:44,839
according to the recent change in time

00:37:40,339 --> 00:37:49,960
and in index magnetometer we also start

00:37:44,839 --> 00:37:53,299
each projects nested deleted angels were

00:37:49,960 --> 00:37:56,029
indexes so from these two kind of

00:37:53,299 --> 00:38:00,019
information we can find for each project

00:37:56,029 --> 00:38:06,079
which index were angels we needed to

00:38:00,019 --> 00:38:10,099
remove then for for the project's angels

00:38:06,079 --> 00:38:15,759
in the common index we just remove the

00:38:10,099 --> 00:38:19,460
angels but we didn't remove the data as

00:38:15,759 --> 00:38:22,849
deleted by quarry is very snowy years so

00:38:19,460 --> 00:38:25,660
we just leave the deleted data and -

00:38:22,849 --> 00:38:31,069
when the common indexing expired and

00:38:25,660 --> 00:38:36,170
when the see this is how it do is in the

00:38:31,069 --> 00:38:38,630
common index coordinator if the protocol

00:38:36,170 --> 00:38:42,979
is in dedicated the index then we just

00:38:38,630 --> 00:38:48,650
remove the index and the updated

00:38:42,979 --> 00:38:51,579
metadata okay this is what we wanted to

00:38:48,650 --> 00:38:51,579
see re thank you

00:38:54,299 --> 00:38:59,760
has anyone asked a question questions

00:39:01,589 --> 00:39:05,559
thanks very much for the presentation

00:39:03,730 --> 00:39:08,020
that was really nice to see how you

00:39:05,559 --> 00:39:13,089
scale to this amount of documents and

00:39:08,020 --> 00:39:17,589
and scaled many questions but one of the

00:39:13,089 --> 00:39:19,480
last ones that I occurred to me it it

00:39:17,589 --> 00:39:22,000
revolves around how you decide which

00:39:19,480 --> 00:39:24,480
index you're gonna use so if it's gonna

00:39:22,000 --> 00:39:26,740
be a big project or a small project and

00:39:24,480 --> 00:39:30,400
you said in order to make that decision

00:39:26,740 --> 00:39:31,869
then you look at seven days of data but

00:39:30,400 --> 00:39:34,569
that doesn't that mean that you've

00:39:31,869 --> 00:39:36,130
already made a decision and then you run

00:39:34,569 --> 00:39:38,260
the project to run for seven days and

00:39:36,130 --> 00:39:41,079
then you make another decision am I am i

00:39:38,260 --> 00:39:41,529
understanding that correctly yeah all

00:39:41,079 --> 00:39:44,470
right

00:39:41,529 --> 00:39:46,900
so you start with a small index yes I

00:39:44,470 --> 00:39:48,970
say okay and then you run you run it for

00:39:46,900 --> 00:39:51,460
seven days and then you make a decision

00:39:48,970 --> 00:39:54,400
okay now we need to upgrade it to a fee

00:39:51,460 --> 00:39:57,579
okay all right and my second question

00:39:54,400 --> 00:39:59,799
was with regards to the shards size

00:39:57,579 --> 00:40:02,380
threshold what is that can you say a few

00:39:59,799 --> 00:40:05,079
more words on that and currently we just

00:40:02,380 --> 00:40:08,770
restate it according to our experience

00:40:05,079 --> 00:40:12,809
for example in our big most a big

00:40:08,770 --> 00:40:18,250
instance we just just stated to 20

00:40:12,809 --> 00:40:20,859
gigabytes oh god yeah 20 actually is the

00:40:18,250 --> 00:40:23,650
pair it depends on some vacation and

00:40:20,859 --> 00:40:26,470
some machines power actually you need to

00:40:23,650 --> 00:40:29,279
some test about it actually there is

00:40:26,470 --> 00:40:31,450
some document of insanity searchable

00:40:29,279 --> 00:40:32,799
okay so there is observe the test that

00:40:31,450 --> 00:40:34,869
you can run in order to figure out what

00:40:32,799 --> 00:40:37,569
is the optimal size for the short story

00:40:34,869 --> 00:40:40,029
or the application yeah actually we need

00:40:37,569 --> 00:40:43,200
to improve that part or until a peace

00:40:40,029 --> 00:40:45,760
and justice specify some pace very short

00:40:43,200 --> 00:40:49,829
okay thank you very much thank you thank

00:40:45,760 --> 00:40:49,829

YouTube URL: https://www.youtube.com/watch?v=lM4Um50R7c4


